{"id": "1708.09667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Video Captioning with Guidance of Multimodal Latent Topics", "abstract": "The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&amp;M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&amp;M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.", "histories": [["v1", "Thu, 31 Aug 2017 11:18:28 GMT  (3559kb,D)", "http://arxiv.org/abs/1708.09667v1", "To appear in ACM Multimedia 2017"], ["v2", "Sat, 2 Sep 2017 15:34:44 GMT  (3559kb,D)", "http://arxiv.org/abs/1708.09667v2", "ACM Multimedia 2017"]], "COMMENTS": "To appear in ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["shizhe chen", "jia chen", "qin jin", "alexander hauptmann"], "accepted": false, "id": "1708.09667"}, "pdf": {"name": "1708.09667.pdf", "metadata": {"source": "META", "title": "Video Captioning with Guidance of Multimodal Latent Topics", "authors": ["Shizhe Chen", "Jia Chen", "Qin Jin", "Alexander Hauptmann"], "emails": ["cszhe1@ruc.edu.cn", "jiac@cs.cmu.edu", "qjin@ruc.edu.cn", "alex@cs.cmu.edu"], "sections": [{"heading": null, "text": "Keywords Video Captioning; Multimodal; Latent topics; Multi-task"}, {"heading": "1 Introduction", "text": "It is impossible to watch these overwhelming amounts of videos, so automated techniques for searching and analyzing video content are highly desirable. Although the topic of creating descriptions for video content (a.k.a. video labeling) is one of such important techniques for this challenge, there can be a wide range of applications such as supporting visually impaired people and improving the quality of online video themes. Although there have been a number of breakthroughs in captioning recently, video labeling remains very challenging due to the diversity and complexity of video content. E-domain videos contain a wide range of topics that correspond with authorities such as sports, music, food and so on, the results in very diverse vocabulary and expressive styles for describing video content."}, {"heading": "2 Related Work", "text": "In recent years, the number of people who are able to work in the US has multiplied; in recent years, the number of people who go to the US has multiplied, both in the US and in Europe."}, {"heading": "3 Problem Formulation", "text": "We want to use the underlying vocabulary and expressions within the topic to describe the underlying vocabulary and expressions in a video. Suppose we have a video system with a set of vocabulary and descriptions in which we have converted a word from the vocabulary W. We have transformed Video-ContentV into a multi-dimensional video representation. It started with a sentence from the multimodal video representation x. It started with a sentence from the vocabulary W. We have transformed Video-ContentV into a multi-dimensional video representation x. It started with a labeling of the sentence y with probability Pr."}, {"heading": "4 M&M Topic-Guided Model", "text": "In this section we present our solutions for topic-oriented video subtitling. Figure 1 illustrates the overall framework. Our proposed M & M TGM consists of three components: Topic Mining, Topic Predictor and Topic Awareness Decoder, which will be optimized by multi-task training. First, we will present the general multi-task training program and then present the three modules in detail."}, {"heading": "4.1 Multi-task Training Scheme", "text": "It is difficult to integrate the theme-conscious conditional sentence probability into eq (7) via the latent theme space, but since the latent theme is heavily dependent on the video content, the latent theme distribution of the video could be very skewed, with a massive likelihood of peaking. Therefore, it is reasonable that we use the maximum latent theme probability to approximate the summation via the latent theme space without losing much precision. e approximation for the theme eq (7) is as follows: Pr (y | x) = default z1,..., zK Pr (y | z1,.., zK, x) Pr (z1,.., zK | x) (12) Prediction Pr (y | z1) = z-z, z-z, z-z, z-z, z-z-z"}, {"heading": "4.2 Topic Mining and Prediction", "text": "We rsten brie y present two theme structures that have been adopted in previous work, and then propose our multimodal latent themes that we apply in the video forecast, and the theme of prediction that is shown in Figure 2 (a). Video uploaders can select one of the themes to organize their videos. Such themes can reflect visual theme diversity to some degree, but have the following disadvantages: 1) user-assigned themes are noisy with caption errors; 2) theme distributions are exclusive, which ignore the theme diversity within the video; and 3) there could be a visual variety of visual meanings and visual appearances within a theme that are suboptimal than latent themes. Textual latent themes: e annotated descriptions in video apostasets provide rich and accurate information about the video content, which can also be more appropriate latent theme distributions."}, {"heading": "4.3 Topic-aware Decoder", "text": "In this section we rst brie y (17) hCell (17) Cell. (1) Cell (16) Cell (16) Cell (16) Cell (16) Cell (18) Cell (18) Cell (18) Cell (18) Cell (18) Cell (18) Cell (18). (3) Cell (16) Cell (16) Cell (16). (3) Cell (14) F (14) F (14) F (14) F (14) F (14) F (14) F (14) F (14) F (14) F) E (14) F) E (14) D) E (14) D) E (14) E (14) D) E (14) D) E (14) D) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) 16 (16) E (16) E (16) E (16) E (16) E (16) E (16) E (16) 16 (16) E (16) E (16) E (16) E (16) E (16) E (16)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "To confirm the effectiveness, robustness and generalization of our proposed methods, we are conducting extensive experiments with two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].MSR-VTT: e MSR-VTT corpus [42] is currently the largest open domain captioning dataset for videos with a variety of video themes. It consists of 10,000 video clips with 20 human annotated captions per clip. Each clip also contains a noisy expert topic crawled by YouTube, the distribution of which is shown in Figure 3. Following the standard data split in the MM2016 challenge [31], we use 6,513 videos for training, 497 videos for validation and the remaining 2,990 for testing. Youtube2Text: e Youtube2Text corpus [11] contains 1970 YouTube clips with approximately 40 & 2 clips;"}, {"heading": "5.2 Implementation Details", "text": "Multimodality features: We extract features from image, motion and audio modalities. For image features, we extract activations from the penultimate layer of the Inceptions Resnet [36], which was pre-trained on the ImageNet, whose dimensionality is 1,536. For motion features, we extract activations from the last 3D folding layer of the C3D model [37], which was pre-trained on the Sports-1M dataset. We perform max pooling on the spatial dimension (width and height), which results in 512-dimensional features. For aural features, we extract the Mel Frequency Cepstral Coe cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] coding methods to create video-level features, with dimensionality of 1,024 and 624, respectively. We use zero video pad for audio without sound."}, {"heading": "5.3 Evaluation of M&M TGM", "text": "In this subsection, we present two baselines to show the effectiveness of the theme guidance and our multi-task learning concept for video caption. e rst baseline, Vanilla, consists of the multimodal encoder and the standard LSTM decoder. e rst baseline is TGM, a trimmed version of our M & M TGM. We discard the Multi-Task Joint Optimization Step (last step) in Section 4.1.To evaluate the effectiveness of the theme guidance, we compare the capture performance of the Vanilla and TGMs with di erent latent theme guides in Table 1. e Numbers of the textual and multimodal latent themes are optimized on the validation set (50 themes for MSR-VTT and 5 themes for Youtube2Text). We can see that TGMs perform the Vanilla model consistently on all four MetricsText MSR-TT2Text and Youtube2Text-Datasets."}, {"heading": "5.4 Comparison with the State-of-the-art", "text": "Table 3 presents our M & M TGM with several state-of-the-art methods applied to the two video labeling datasets. Our method performs significantly better than previous work on the MSRVTT dataset, for example, the BLEU @ 4 achieves a relative improvement of 7.08% over the previous best. METEOR and CIDERr achieve significantly better results for the Youtube2Text dataset, but our performance on BLEU @ 4 is lower than the SCN-LSTM. e BLEU @ 4 metric focuses on syntactical agreement, while METEOR and CIDER care more about semantic meanings. Therefore, we consider the sentence generated by M & M TGM to be more semantically relevant to video content. It is important to note that the semantic concepts used in SCN-LSTM are trained with additional image data, as they claim that Youtube2Text corpus is too small to be able to identify reliable subject matter, but easier to clarify for our MM-only conceptual reasons."}, {"heading": "5.5 Experimental Analysis", "text": "Generalization capability: To evaluate the generalization capability of our proposed method, we conduct the experiment with cross-sectional data. We train M & M TGM on the MSR-VTT dataset and test its performance on the Youtube2Text dataset. Results will be shown in Table 4. We can see that the proposed M & M TGM performs significantly better than the vanilla model in evaluating cross-sectional data in all four metrics, which shows that our method can not only improve captioning for videos in domains, but also generalize well on videos in the wild. Topic Loss Selection: We compare captioning performance with l2 distance or KL divergence as Ltopic in the TGM model. Table 5 presents the results. e l2 distance consistently exceeds KL divergence across datasets and metrics. Unless otherwise specified, we use l2 distances as the topics of the number of the prediction."}, {"heading": "5.6 alitative Analysis", "text": "To get an intuition on how to improve the generated video descriptions of the M & M TGM model, we present some video examples with the video description of the Vanilla model and M & M TGM using the MSR-VTT test kit. In Figure 6, we can see that M & M TGM can produce more detailed video descriptions than the Vanilla model, even though they share the same multimodal characteristics. In Figure 7, although the descriptions of Vanilla and M & M TGM are both correct, the M & M TGM model can provide more detailed information about the video content. We also observe that M & M TGM uses more unique words at 493 than the 391 unique words of the Vanilla model. The reason for these quality improvements may be that M & M TGM can narrow the saturation space according to the predicted latent surfaces."}, {"heading": "5.7 Human Interaction in Captioning", "text": "In addition to improving caption performance, our theme-driven model can provide an interface for users involved in caption creation, so users can easily assign a category tag to the uploaded videos, at minimal cost, to render the automatically generated video descriptions. To evaluate the performance gains through the human enhancement of our theme-driven model, we manually comment on the expert topics for the Youtub2Text dataset as a clean version and compare their caption performance with the loudly assigned topics. The results are presented in Table 6, which shows that manually correcting the topics at a low cost can make a huge profit."}, {"heading": "6 Conclusions", "text": "In this paper, we propose a novel topic-led subtitling model to address the challenge of diversity of topics for open domain video subtitling assignments. e-suggested models can predict the latent topics of videos and then generate topic-oriented video descriptions along with the topic guidance in an end-to-end way. e-topic-aware decoders can use the predicted topics to adjust their weighting to the topic-dependent sentence distributions. Our experimental results on two public video benchmark datasets show that the proposed model can generate more precise and detailed descriptions within specific topics and improve performance uniformly across all metrics of both datasets. Furthermore, we show that our model has very good generalization capabilities with regard to datasets. e-proposed topic-led subtitling models can be considered a general framework that could be integrated with other techniques such as video investigations in the future."}, {"heading": "7 Acknowledgments", "text": "is supported by the National Key Research and Development Plan under grant number 2016YFB1001202."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Generating Video Descriptions with Topic Guidance", "author": ["Shizhe Chen", "Jia Chen", "Qin Jin"], "venue": "In ICMR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing 28,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1980}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Inderjit S Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Improving Interpretability of Deep Neural Networks with Semantic Information", "author": ["Yinpeng Dong", "Hang Su", "Jun Zhu", "Bo Zhang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Semantic Compositional Networks for Visual Captioning", "author": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep Sparse Recti\u0080er Neural Networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Describing Videos using Multi-modal Fusion", "author": ["Qin Jin", "Jia Chen", "Shizhe Chen", "Yifan Xiong", "Alexander Hauptmann"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Video description generation using audio and visual cues", "author": ["Qin Jin", "Junwei Liang"], "venue": "In Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "In Proceedings of the 24th CVPR. Citeseer", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Meteor universal: Language speci\u0080c translation evaluation for any target language", "author": ["Michael Denkowski Alon Lavie"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Phrase-based Image Captioning", "author": ["R\u00e9mi Lebret", "Pedro H.O. Pinheiro", "Ronan Collobert"], "venue": "In ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "author": ["Stefan Lee", "Michael Cogswell", "Viresh Ranjan", "David Crandall", "Dhruv Batra"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Event speci\u0080c multimodal pa\u008aern mining for knowledge base construction", "author": ["Hongzhi Li", "Joseph G Ellis", "Heng Ji", "Shih-Fu Chang"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Unsupervised Learning of Image Transformations", "author": ["R Memisevic", "G Hinton"], "venue": "In CVPR", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "So\u0089ening quantization in bag-ofaudio-words", "author": ["Stephanie Pancoast", "Murat Akbacak"], "venue": "In ICASSP", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Multimodal Video Description", "author": ["Vasili Ramanishka", "Abir Das", "Dong Huk Park", "Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach", "Kate Saenko"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Self-critical Sequence Training for Image Captioning", "author": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Image classi\u0080cation with the \u0080sher vector: \u008ceory and practice", "author": ["Jorge S\u00e1nchez", "Florent Perronnin", "\u008comas Mensink", "Jakob Verbeek"], "venue": "International journal of computer vision 105,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Weakly Supervised Dense Video Captioning", "author": ["Zhiqiang Shen", "Jianguo Li", "Zhou Su", "Minjun Li", "Yurong Chen", "Yu-Gang Jiang", "Xiangyang Xue"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Frame-and segment-level features and candidate pool evaluation for video caption generation", "author": ["Rakshith She\u008ay", "Jorma Laaksonen"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": "In NIPS", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["Christian Szegedy", "Sergey Io\u0082e", "Vincent Vanhoucke", "Alex Alemi"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": "In ICCV", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Je\u0082 Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "Science", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Msr-v\u008a: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Show, a\u008aend and tell: Neural image caption generation with visual a\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Review networks for caption generation", "author": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W Cohen", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Image captioning with semantic a\u008aention", "author": ["\u008banzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 27, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 37, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 39, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 40, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 42, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 17, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 11, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 29, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 38, "context": "tion datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "tion datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "Early works are mainly based on rule systems [17, 19] and suffer from generating \u0083exible and accurate descriptions.", "startOffset": 45, "endOffset": 53}, {"referenceID": 16, "context": "Early works are mainly based on rule systems [17, 19] and suffer from generating \u0083exible and accurate descriptions.", "startOffset": 45, "endOffset": 53}, {"referenceID": 1, "context": "So more researches have been focusing on the encoder-decoder framework [2, 35] for caption generation.", "startOffset": 71, "endOffset": 78}, {"referenceID": 31, "context": "So more researches have been focusing on the encoder-decoder framework [2, 35] for caption generation.", "startOffset": 71, "endOffset": 78}, {"referenceID": 32, "context": "\u008ce deep convolutional neural networks (CNNs) [36] function as the encoder to transform image contents into dense vectors.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "\u008cen the decoder, typically the Long-short Term Memory [12] (LSTM), is utilized to generate sequential words conditioned on image features [41].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "\u008cen the decoder, typically the Long-short Term Memory [12] (LSTM), is utilized to generate sequential words conditioned on image features [41].", "startOffset": 138, "endOffset": 142}, {"referenceID": 39, "context": "[43] propose the spatial a\u008aention mechanism based on the basic encoder-decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[46] and Gan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] propose the semantic a\u008aention model and semantic compositional networks (SCN) respectively to exploit detected concepts from the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 41, "context": "[45] propose the temporal a\u008aention mechanism, and Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] utilize the hierarchical LSTM encoder to explore the temporal structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Most previous works only focus on the visual modality [26, 40], but recently Jin et al.", "startOffset": 54, "endOffset": 62}, {"referenceID": 36, "context": "Most previous works only focus on the visual modality [26, 40], but recently Jin et al.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "[13, 14] and Ramanishka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[13, 14] and Ramanishka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[29] have shown improvement of multimodal fusion for video captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "To promote the video captioning research for open-domain videos, several large-scale videos with various topic are collected such as MSR-VTT [42] and TGIF [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "To promote the video captioning research for open-domain videos, several large-scale videos with various topic are collected such as MSR-VTT [42] and TGIF [22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "[13] encode expert-de\u0080ned topics with multimodal features, which results in their winning of the MSR-VTT challenge [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] utilize the textual mined topics to learn interpretable features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33] train separate language decoders for each expert-de\u0080ned topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] explore the guidance from textual minded topics to generate topic-aware sentences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "\u008ce latent Dirichlet allocation (LDA) proposed by Blei [3] is one of the most classic models to automatically inference the latent topics for textual documents.", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": "[21] apply the association rule mining algorithm on image-caption pairs to discover the multimodal topics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u008cus, our previous work [4] mined topics from the annotated video captions on the training set.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "We use the kernel K-means algorithm [6] for clustering.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Topic Predictor: We take the teacher-student perspective [1] to train the topic predictor\u03c8z .", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "whereWi ,bi (i = 1, 2) are parameters, [\u00b7] denotes feature concatenation and \u03c6 is the RELU nonlinear function [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "[9], we extend each weight matrix of the conventional LSTM to be an ensemble of a set of topic-dependent weight matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "So the 3-way factorization method [16, 24] is used to share parameters.", "startOffset": 34, "endOffset": 42}, {"referenceID": 38, "context": "To validate the e\u0082ectiveness, robustness and generalization of our proposed methods, we conduct extensive experiments on two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "To validate the e\u0082ectiveness, robustness and generalization of our proposed methods, we conduct extensive experiments on two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 192, "endOffset": 196}, {"referenceID": 38, "context": "MSR-VTT: \u008ce MSR-VTT corpus [42] is currently the largest open-domain video captioning dataset with a wide variety of video topics.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "Youtube2Text: \u008ce Youtube2Text corpus [11] contains 1970 Youtube video clips with around 40 human annotated sentences per clip.", "startOffset": 37, "endOffset": 41}, {"referenceID": 41, "context": "[45], with 1,200 videos for training, 100 videos for validation and 670 videos for testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "For image features, we extract activations from the penultimate layer of the inception-resnet [36] pre-trained on the ImageNet, the dimensionality of which is 1,536.", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "For motion features, we extract activations from the last 3D convolution layer of the C3D model [37] pre-trained on the Sports-1M dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 77, "endOffset": 80}, {"referenceID": 24, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 108, "endOffset": 112}, {"referenceID": 28, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "5 on the input and output of LSTM and use ADAM algorithm [15] with learning rate of 10\u22124.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "Aalto [34] 39.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "v2t navigator [13] 40.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "dense caption [33] 41.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "LSTM-YT [40] 33.", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "S2VT [39] - 29.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "LSTM-I [8] 44.", "startOffset": 7, "endOffset": 10}, {"referenceID": 41, "context": "SA [45] 41.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "LSTM-E [26] 45.", "startOffset": 7, "endOffset": 11}, {"referenceID": 43, "context": "h-RNN decoder [47] 49.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "h-RNN encoder [25] 43.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "SCN-LSTM [9] 50.", "startOffset": 9, "endOffset": 12}], "year": 2017, "abstractText": "\u008ce topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an uni\u0080ed caption framework, M&M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-de\u0080ned topics, the mined multimodal topics are more semantically and visually coherent and can re\u0083ect the topic distribution of videos be\u008aer. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. \u008ce topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. \u008ce entire learning procedure is end-to-end and it optimizes both tasks simultaneously. \u008ce results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the e\u0082ectiveness of our proposed model. M&M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves be\u008aer generalization ability.", "creator": "LaTeX with hyperref package"}}}