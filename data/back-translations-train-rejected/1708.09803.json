{"id": "1708.09803", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation", "abstract": "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource one. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that while BPE and transfer learning perform inconsistently on their own, together they improve translation quality by up to 1.8 BLEU.", "histories": [["v1", "Thu, 31 Aug 2017 16:34:38 GMT  (78kb,D)", "http://arxiv.org/abs/1708.09803v1", null], ["v2", "Thu, 21 Sep 2017 17:04:20 GMT  (85kb,D)", "http://arxiv.org/abs/1708.09803v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["toan q nguyen", "david chiang"], "accepted": false, "id": "1708.09803"}, "pdf": {"name": "1708.09803.pdf", "metadata": {"source": "META", "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation", "authors": ["Toan Q. Nguyen"], "emails": ["tnguye28@nd.edu", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) (Sutskever et al., 2014), (Bahdanau et al., 2015) is quickly proving to be a strong competitor to other statistical machine translation methods. However, it is still lagging behind for very low-resource language pairs (Zoph et al., 2016).A common strategy for improving the learning of low-resource languages is the use of resources from related languages (Nakov and Ng, 2009). However, adaptation to these resources is not trivial. NMT offers some simple ways to do this. Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model that is further trained on a low-resource language pair. Specifically, they showed that a French-English model could be used to improve the low-resource translation model of languages."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attentional Model", "text": "For the purposes of this work, the most important detail of the model is that (as in many other models) thear Xiv: 170 8.09 803v 1 [cs.C L] 31 Aug 201 7 word types of both the source and target languages are mapped to vector representations called word embeddings, which are automatically learned with the rest of the model."}, {"heading": "2.2 Language transfer", "text": "We follow the approach of transfer learning proposed by Zoph et al. (2016). In their work, a parent model is first trained on a resource-intensive language pair, then the parameter values of the child model are copied from the parent model and matched to its resource-poor data. Source word embeddings are copied with the rest of the model, but since the mother and child embeddings have different vocabularies, parent word embeddings are effectively randomly assigned to the child embeddings. In other words, even if a word exists in both parent and child vocabulary, it is highly unlikely that it will be assigned the same embeddings in both models. As the target language is the same in both parent and child models, target word embeddings are frozen during fine tuning."}, {"heading": "2.3 Related languages", "text": "The experiments described below concern the translation of three Turkish languages into English. The family of Turkish languages is a group of related languages with a very large geographical spread from Turkey to north-east Siberia. Turkic languages are morphologically rich and show similarities in phonology, morphology and syntax. For example, in our analysis of training data we find many Turkish and Uzbek words that have the same root and meaning. Some examples are shown in Table 1."}, {"heading": "2.4 Byte Pair Encoding", "text": "BPE (Sennrich et al., 2016) is an efficient word segmentation algorithm. It first treats words as sequences of character tokens, then iteratively finds the most common character pair and merges them into one. It stops after a controllable number of operations or when no character pair appears more than once. At test times, the learned merge operations are applied to merge the strings in the test data into larger symbols."}, {"heading": "3 Method", "text": "The basic idea of our method is to extend the transmission method of Zoph et al. (2016) so that the source words of parents and children are shared, so that when transferring source words embedded, a word that occurs in both words retains its embedding. In order for this to work, parent and child languages must overlap, and if a word occurs in both languages, it often has a similar meaning in both languages. Therefore, we need to process the data so that these two assumptions hold as much as possible."}, {"heading": "3.1 Transliteration", "text": "Even if both use the same script, some transformation could be applied; for example, we could change the French -eur endings to Spanish - or. Here, we are taking a minimalist approach. Turkish and Uzbek are both written in Latin script, and we have not applied any transformations to them. Our Uighur data is written in Arabic script, so we translate it into Latin script using a standard translator. 1 Transliteration is a string homomorphism that replaces Arabic letters with English letters or consonant clusters regardless of the context."}, {"heading": "3.2 Segmentation", "text": "To increase the overlap between parent and child vocabulary, we use BPE to split up words1: https: / / cis.temple.edu / \u02dc anwar / code / latin2uyghur.htmlinto. So that the BPE rules not only find the common subwords between two source languages, but also ensure consistency between source and target segmentation between each language pair, we learn the rules from combining source and target data from both the parent and child model. The rules are then used to segment the corpora. It is important to note that this results in a single vocabulary that is used for both the source and target languages in both models."}, {"heading": "4 Experiments", "text": "We linked all the data to the Moses toolkit (Koehn et al., 2007); for Uzbek-English experiments, we also cracked the data. To save time, we trained the word-based models in the original Uyghur systems and translated the sprawling words into the Latin scripts. To save time, we fixed the word-based systems to the word-based systems and replaced the sprawling words with the sprawling values."}, {"heading": "5 Results and Analysis", "text": "According to Table 3, the Uzbek-English BPE base and transfer models perform better than the word-based models, and they also show that transferring word-based models with this resource-poor setting actually worsens performance. Our approach, on the other hand, not only exceeds the baseline, but also leads to further improvements in transmission from parent to parent. In Uyghur English, transferring the entire word-based model is of little help. However, setting target embeddings seems to worsen performance. Moreover, the use of BPE alone hardly helps. In contrast, the best BPE transfer model improves on both levels. In summary, our best BPE transfer approach achieves + 1.6 BLEU improvements over the word-based base model on the Uzbek-English dataset and + 1.8 BLEU on the Uzbek-English dataset. A similar pattern emerges when we examine the best BLEU values on the development table (Figure 1)."}, {"heading": "6 Conclusion", "text": "In this paper, we have shown that the transfer learning method used by Zoph et al. (2016), while appealing, does not always work in a resource-poor context. However, by combining it with BPE, we can improve the NMT performance of a low-resource language pair by exploiting its lexical similarity to another related, also resource-poor language. Our results show consistent improvements in two Turkic languages. Our approach, based on the segmentation of words into subwords, seems well suited for agglutinative languages; further research would be needed to confirm whether our method also works in other languages."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proc. EMNLP. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Improved statistical machine translation for resource-poor", "author": ["Preslav Nakov", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Nakov and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Nakov and Ng.", "year": 2009}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proc. ACL. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems 27. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701v1.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proc. EMNLP. pages 1568\u20131575.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Neural machine translation (NMT) (Sutskever et al., 2014), (Bahdanau et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.", "startOffset": 9, "endOffset": 32}, {"referenceID": 7, "context": "However, it still lags behind on very low-resource language pairs (Zoph et al., 2016).", "startOffset": 66, "endOffset": 85}, {"referenceID": 3, "context": "A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009).", "startOffset": 106, "endOffset": 126}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind on very low-resource language pairs (Zoph et al., 2016). A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009). However, adapting to these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.", "startOffset": 10, "endOffset": 478}, {"referenceID": 7, "context": "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of Zoph et al. (2016) fails, but it is still possible to use the parent model to considerably improve the child model.", "startOffset": 114, "endOffset": 133}, {"referenceID": 4, "context": "To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the union of the vocabularies to increase the number of common subwords.", "startOffset": 94, "endOffset": 117}, {"referenceID": 2, "context": "We use the 2-layer, 512-hidden-unit global attentional model with general scoring function and input feeding by Luong et al. (2015). For the purposes of this paper, the most important detail of the model is that (as in many other models) the ar X iv :1 70 8.", "startOffset": 112, "endOffset": 132}, {"referenceID": 7, "context": "We follow the transfer learning approach proposed by Zoph et al. (2016). In their work, a parent model is first trained on a high-resource language pair.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "BPE (Sennrich et al., 2016) is an efficient word segmentation algorithm.", "startOffset": 4, "endOffset": 27}, {"referenceID": 7, "context": "The basic idea of our method is to extend the transfer method of Zoph et al. (2016) to share the parent and child\u2019s source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.", "startOffset": 65, "endOffset": 84}, {"referenceID": 1, "context": "We tokenized all data using the Moses toolkit (Koehn et al., 2007); for Uzbek-English experiments, we also truecased the data.", "startOffset": 46, "endOffset": 66}, {"referenceID": 6, "context": "We trained using Adadelta (Zeiler, 2012), with a minibatch size of 32 and dropout with a dropout rate of 0.", "startOffset": 26, "endOffset": 40}, {"referenceID": 7, "context": "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of Zoph et al. (2016) (\u00a72.", "startOffset": 169, "endOffset": 188}, {"referenceID": 7, "context": "In this paper, we have shown that the transfer learning method of Zoph et al. (2016), while appealing, might not always work in low-resource context.", "startOffset": 66, "endOffset": 85}], "year": 2017, "abstractText": "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource one. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that while BPE and transfer learning perform inconsistently on their own, together they improve translation quality by up to 1.8 BLEU.", "creator": "LaTeX with hyperref package"}}}