{"id": "1709.00023", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering", "abstract": "In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that \"reads\" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance.", "histories": [["v1", "Thu, 31 Aug 2017 18:08:35 GMT  (213kb,D)", "http://arxiv.org/abs/1709.00023v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shuohang wang", "mo yu", "xiaoxiao guo", "zhiguo wang", "tim klinger", "wei zhang", "shiyu chang", "gerald tesauro", "bowen zhou", "jing jiang"], "accepted": false, "id": "1709.00023"}, "pdf": {"name": "1709.00023.pdf", "metadata": {"source": "CRF", "title": "R: Reinforced Reader-Ranker for Open-Domain Question Answering", "authors": ["Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerald Tesauro", "Bowen Zhou", "Jing Jiang"], "emails": ["shwang.2014@smu.edu.sg,", "yum@us.ibm.com,", "xiaoxiao.guo@ibm.com"], "sections": [{"heading": null, "text": "In this paper, we present a novel open domain QA system called Reinforced Ranker Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline of open domain QA with a Ranker component that learns to evaluate retrieved passages in terms of the likelihood of generating the down-to-earth answer to a given question. Second, we propose a novel method that trains the Ranker jointly. This work was carried out during the 1st author's internship at IBM. 1In the QA community, \"openness\" can be interpreted as referring either to the scope of question topics or to the breadth and universality of the knowledge source used to answer each question. Following Chen et al. 2017a, we adopt the latter definition, along with a response generation reader model based on reinforcement learning. We report on extensive experimental results that show that our open domain significantly improves the state of the art method for multiple domain sets of data."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes a few days to get there, to get to the point where a solution can be found."}, {"heading": "2 Framework", "text": "This year, it has come to the point where you feel you can put yourself at the top, in order to put yourself at the top."}, {"heading": "4 Experimental Settings", "text": "To evaluate our model, we selected five challenging datasets created using the following remote monitoring method (Chen et al., 2017a), including Quasar-T (Dhingra et al., 2017b), SQuAD (Rajpurkar et al., 2016), WikiMovie (Miller et al., 2016), CuratedTREC (Baudis and S-edivy, 2015) and WebQuestion (Berant et al., 2013). Based on these datasets, we compare three public baseline models: GA (Dhingra et al., 2017a, b), BiDAF (Seo et al., 2017) and DrQA (Chen et al., 2017a)."}, {"heading": "4.1 Datasets", "text": "We are experimenting with five different datasets, the statistics of which are presented in Table 2.Quasar-T (Dhingra et al., 2017b) is a dataset for SR-QA, with question-answer pairs from different Internet sources. Each question is compared with 100 sentence-level sentences retrieved by their IR model from the ClueWeb09 data source to extract the answer. The other four datasets we are looking at are: SQuAD (Rajpurkar et al., 2016), the Stanford QA dataset, where each question-answer pair of commentators is generated using a given Wikipedia paragraph; WikiMovie (Miller et al., 2016), which contains film-related questions from the OMDb and MovieLens databases and where the questions can be answered with Wikipedia pages; CuratedTREC (Baudis and S-Edivy, 2015), based on TREC (Voheores and Tice, 2000 and English)."}, {"heading": "4.2 Baselines", "text": "We are looking at three basic public models 8: GA (Dhingra et al., 2017a, b), a reader with gated attention to text comprehension; BiDAF (Seo et al., 2017), a reader with bidirectional attention flow to machine comprehension; and DrQA (Chen et al., 2017a), a document reader to answer questions. We are also comparing our model R3 with two internal baselines: Single Reader (SR) This model is designed in the same way as Chen et al. 2017a and Dhingra7https: / / lucene.apache.org / 8We are comparing only the results from the public papers. et al. 2017b. We find all response spans that exactly match the truth answers obtained from the passages obtained and train the reader with the goal of Eqn. (8)."}, {"heading": "4.3 Implementation Details", "text": "To increase the probability that the question-related context will be contained in the retrieved passages of the training data set, we combine the question with the answer to form the query for retrieval of information. For the test data set, we only use the question as a query and collect the 50 most important passages for the answer extraction. During the training, our R3 model is initially initialized by pre-training the model with the Simple Ranker Reader (R2) to promote convergence. As previously discussed, pre-processing and the matching layers, Eqn. (1-3), are shared by the ranker and reader. The LSTM layer in Eqn. (4) is optimized to 3 for the reader and 1 for the ranker.Our model is based on Adamax (Kingma and Ba, 2015). We use fixed GloVe (Pennington et al., 2014) word embedding. We set l to 300 to increase the probability of learning to 0.0002 and 0.000h."}, {"heading": "5 Results and Analysis", "text": "In this section, we show the performance of different models on five QA datasets and provide further analysis.9 Our code is published at https: / / github. com / shuohangwang / mprc."}, {"heading": "5.1 Overall Results", "text": "Our results are shown in Table 3. We use the F1 score and the Exact Match (EM) evaluation metrics10. We first find that the single reader can outperform the state of the art in Quasar-T. In addition, unlike DrQA, our models are all remotely monitored, and without pre-training on the original SQuAD dataset11, our single reader model still performs better on the WikiMovie and CuratedTREC datasets. Next, we find that the enhanced Ranker Reader (R3) on the Quasar-T, WikiMovies and CuratedTREC datasets performs best and performs significantly better than our internal base model Simple Ranker Reader (SR2) on all datasets except CuratedTREC. These results show the effectiveness of the joint training of RL for marker and reader both in comparison with competing approaches and with the non-Ranker value."}, {"heading": "5.2 Further Analysis", "text": "In this section, we first present an analysis of the improvement of both rankings and readers associated with our methodology, and then discuss any ideas for further improving the rankings."}, {"heading": "6 Related Work", "text": "The task of open domain question-answering dates back to as early as (Green Jr et al., 1961) and was popularized with TREC-8 (Voorhees, 1999), which is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), websites (Kwok et al., 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2017) An early consensus since TREC-8 has produced an approach with three important components: question-asking, retrieval and ranking, and answer-extraction. Although question analysis is relatively mature, answer-extraction and document-ranking represent significant challenges."}, {"heading": "7 Conclusion", "text": "We have proposed and evaluated R3, a new opendomain-based QA framework that combines IR with a deep learning-based Ranker and Reader. First, the IR model retrieves the uppermost N passages (N a hyperparameter) that are conditioned to the question. Afterwards, Ranker and Reader are jointly trained on the basis of reinforcement learning to directly optimize the expectation to extract the truth answer from the retrieved passages. To make predictions, the values calculated from Ranker and Reader are summed for the final response extraction. Our model surpasses a base model that trains Ranker and Reader together on the basis of supervised learning, and R3 achieves the best performance on multiple datasets."}], "references": [{"title": "Modeling of the question answering task in the yodaqa system", "author": ["Petr Baudi\u0161", "Jan \u0160ediv\u1ef3."], "venue": "International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222\u2013 228. Springer.", "citeRegEx": "Baudi\u0161 and \u0160ediv\u1ef3.,? 2015", "shortCiteRegEx": "Baudi\u0161 and \u0160ediv\u1ef3.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Reading Wikipedia to answer opendomain questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "Proceedings of the Conference on Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2017a", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Enhanced lstm for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2017b", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Coarse-to-fine question answering for long documents", "author": ["Eunsol Choi", "Daniel Hewlett", "Jakob Uszkoreit", "Illia Polosukhin", "Alexandre Lacoste", "Jonathan Berant."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "Proceedings of the Conference on Association for Computational Linguistics.", "citeRegEx": "Dhingra et al\\.,? 2017a", "shortCiteRegEx": "Dhingra et al\\.", "year": 2017}, {"title": "QUASAR: Datasets for question answering by search and reading", "author": ["Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen."], "venue": "arXiv preprint arXiv:1707.03904.", "citeRegEx": "Dhingra et al\\.,? 2017b", "shortCiteRegEx": "Dhingra et al\\.", "year": 2017}, {"title": "SearchQA: A new q&a dataset augmented with context from a search engine", "author": ["Matthew Dunn", "Levent Sagun", "Mike Higgins", "Ugur Guney", "Volkan Cirik", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1704.05179.", "citeRegEx": "Dunn et al\\.,? 2017", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Building watson: An overview of the deepqa project", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": "AI magazine,", "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Baseball: an automatic question-answerer", "author": ["Bert F Green Jr", "Alice K Wolf", "Carol Chomsky", "Kenneth Laughery."], "venue": "Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference, pages 219\u2013224. ACM.", "citeRegEx": "Jr et al\\.,? 1961", "shortCiteRegEx": "Jr et al\\.", "year": 1961}, {"title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension", "author": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Joshi et al\\.,? 2017", "shortCiteRegEx": "Joshi et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Scaling question answering to the web", "author": ["Cody Kwok", "Oren Etzioni", "Daniel S Weld."], "venue": "ACM Transactions on Information Systems (TOIS), 19(3):242\u2013 262.", "citeRegEx": "Kwok et al\\.,? 2001", "shortCiteRegEx": "Kwok et al\\.", "year": 2001}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "MS MARCO: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "The trec-8 question answering track report", "author": ["Ellen M. Voorhees."], "venue": "Trec, volume 99, pages 77\u201382.", "citeRegEx": "Voorhees.,? 1999", "shortCiteRegEx": "Voorhees.", "year": 1999}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207. ACM.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "A compareaggregate model for matching text sequences", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Wang and Jiang.,? 2017a", "shortCiteRegEx": "Wang and Jiang.", "year": 2017}, {"title": "Machine comprehension using match-LSTM and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Wang and Jiang.,? 2017b", "shortCiteRegEx": "Wang and Jiang.", "year": 2017}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."], "venue": "Proceedings of the Conference on Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine Learning.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Xiong et al\\.,? 2017", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Improved neural relation detection for knowledge base question answering", "author": ["Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the Conference on Association for Computational Linguistics.", "citeRegEx": "Yu et al\\.,? 2017", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1610.09996.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "These approaches have achieved state of the art results in simplified closeddomain settings1 such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted.", "startOffset": 111, "endOffset": 135}, {"referenceID": 3, "context": ", wikipedia) instead of a pre-selected passage (Chen et al., 2017a).", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "Knowledge sources can be knowledge bases (Berant et al., 2013) or structured or unstructured text passages (Ferrucci et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 10, "context": ", 2013) or structured or unstructured text passages (Ferrucci et al., 2010; Baudi\u0161 and \u0160ediv\u1ef3, 2015).", "startOffset": 52, "endOffset": 100}, {"referenceID": 0, "context": ", 2013) or structured or unstructured text passages (Ferrucci et al., 2010; Baudi\u0161 and \u0160ediv\u1ef3, 2015).", "startOffset": 52, "endOffset": 100}, {"referenceID": 3, "context": "Recent deep learning-based research has focused on open-domain QA based on large text corpora such as wikipedia, applying a two-stepprocess of information retrieval (IR) to extract passages and reading comprehension (RC) to select an answer phrase from them (Chen et al., 2017a; Dhingra et al., 2017b).", "startOffset": 258, "endOffset": 301}, {"referenceID": 8, "context": "Recent deep learning-based research has focused on open-domain QA based on large text corpora such as wikipedia, applying a two-stepprocess of information retrieval (IR) to extract passages and reading comprehension (RC) to select an answer phrase from them (Chen et al., 2017a; Dhingra et al., 2017b).", "startOffset": 258, "endOffset": 301}, {"referenceID": 27, "context": "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).", "startOffset": 165, "endOffset": 263}, {"referenceID": 34, "context": "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).", "startOffset": 165, "endOffset": 263}, {"referenceID": 29, "context": "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).", "startOffset": 165, "endOffset": 263}, {"referenceID": 31, "context": "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).", "startOffset": 165, "endOffset": 263}, {"referenceID": 28, "context": "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).", "startOffset": 165, "endOffset": 263}, {"referenceID": 20, "context": "In standard RC model training, passages are manually selected to guarantee that groundtruth answers are contained and annotated within the passage (Rajpurkar et al., 2016).", "startOffset": 147, "endOffset": 171}, {"referenceID": 3, "context": "trast, in SR-QA approaches such as (Chen et al., 2017a; Dhingra et al., 2017b), the model is given only QA-pairs and uses an IR component to retrieve passages similar to the question from a large corpus.", "startOffset": 35, "endOffset": 78}, {"referenceID": 8, "context": "trast, in SR-QA approaches such as (Chen et al., 2017a; Dhingra et al., 2017b), the model is given only QA-pairs and uses an IR component to retrieve passages similar to the question from a large corpus.", "startOffset": 35, "endOffset": 78}, {"referenceID": 30, "context": "The Ranker is trained using REINFORCE (Williams, 1992) with a reward de-", "startOffset": 38, "endOffset": 54}, {"referenceID": 24, "context": "Passage ranking models for non-factoid QA (Wang et al., 2007; Yang et al., 2015) are able to learn to rank these passages; but these models are trained using human annotated answer labels, which are not available here.", "startOffset": 42, "endOffset": 80}, {"referenceID": 32, "context": "Passage ranking models for non-factoid QA (Wang et al., 2007; Yang et al., 2015) are able to learn to rank these passages; but these models are trained using human annotated answer labels, which are not available here.", "startOffset": 42, "endOffset": 80}, {"referenceID": 25, "context": "We discuss the Ranker-Reader model in detail in the next section but briefly, the Ranker and Reader are implemented as variants of MatchLSTM models (Wang and Jiang, 2016).", "startOffset": 148, "endOffset": 170}, {"referenceID": 25, "context": "In this section, we first review the MatchLSTM (Wang and Jiang, 2016) which provides input for both the Reader and Ranker.", "startOffset": 47, "endOffset": 69}, {"referenceID": 26, "context": "is the column concatenation of matrices; Elementwise operations (\u00b7 \u2299 \u00b7) and (\u00b7 \u2212 \u00b7) are also used to represent word-level matching (Wang and Jiang, 2017a; Chen et al., 2017b).", "startOffset": 131, "endOffset": 174}, {"referenceID": 4, "context": "is the column concatenation of matrices; Elementwise operations (\u00b7 \u2299 \u00b7) and (\u00b7 \u2212 \u00b7) are also used to represent word-level matching (Wang and Jiang, 2017a; Chen et al., 2017b).", "startOffset": 131, "endOffset": 174}, {"referenceID": 3, "context": "vision setting following (Chen et al., 2017a)\u2019s work, including Quasar-T (Dhingra et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 8, "context": ", 2017a)\u2019s work, including Quasar-T (Dhingra et al., 2017b), SQuAD (Rajpurkar et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 20, "context": ", 2017b), SQuAD (Rajpurkar et al., 2016), WikiMovie (Miller et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 16, "context": ", 2016), WikiMovie (Miller et al., 2016), CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), and WebQuestion (Berant et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": ", 2016), CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), and WebQuestion (Berant et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": ", 2016), CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), and WebQuestion (Berant et al., 2013).", "startOffset": 64, "endOffset": 85}, {"referenceID": 21, "context": ", 2017a,b), BiDAF (Seo et al., 2017), and DrQA (Chen et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 3, "context": ", 2017), and DrQA (Chen et al., 2017a).", "startOffset": 18, "endOffset": 38}, {"referenceID": 8, "context": "Quasar-T (Dhingra et al., 2017b) is a dataset for SR-QA, with question-answer pairs from various internet sources.", "startOffset": 9, "endOffset": 32}, {"referenceID": 20, "context": "The other four datasets we consider are: SQuAD (Rajpurkar et al., 2016), the Stanford QA dataset, where each question-answer pair is generated by annotators using a given Wikipedia paragraph; WikiMovie (Miller et al.", "startOffset": 47, "endOffset": 71}, {"referenceID": 16, "context": ", 2016), the Stanford QA dataset, where each question-answer pair is generated by annotators using a given Wikipedia paragraph; WikiMovie (Miller et al., 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant", "startOffset": 138, "endOffset": 159}, {"referenceID": 0, "context": ", 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant", "startOffset": 160, "endOffset": 185}, {"referenceID": 23, "context": ", 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant", "startOffset": 201, "endOffset": 226}, {"referenceID": 3, "context": "For these four datasets under the distant supervision setting, no candidate passages are provided so we build a similar sentence-level Search Index based on English Wikipedia, following (Chen et al., 2017a).", "startOffset": 186, "endOffset": 206}, {"referenceID": 21, "context": ", 2017a,b), a gated-attention reader for text comprehension; BiDAF (Seo et al., 2017), a reader with bidirectional attention flow for machine comprehension; and DrQA (Chen et al.", "startOffset": 67, "endOffset": 85}, {"referenceID": 3, "context": ", 2017), a reader with bidirectional attention flow for machine comprehension; and DrQA (Chen et al., 2017a), a document reader for question answering.", "startOffset": 88, "endOffset": 108}, {"referenceID": 13, "context": "Our model is optimized using Adamax (Kingma and Ba, 2015).", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "We use fixed GloVe (Pennington et al., 2014) word embeddings.", "startOffset": 19, "endOffset": 44}, {"referenceID": 7, "context": "GA (Dhingra et al., 2017a) 26.", "startOffset": 3, "endOffset": 26}, {"referenceID": 21, "context": "4 - - - - - - - BiDAF (Seo et al., 2017) 28.", "startOffset": 22, "endOffset": 40}, {"referenceID": 3, "context": "9 - - - - - - - DrQA (Chen et al., 2017a) - - - 28.", "startOffset": 21, "endOffset": 41}, {"referenceID": 3, "context": "DrQA-MTL (Chen et al., 2017a) - - - 29.", "startOffset": 9, "endOffset": 29}, {"referenceID": 0, "context": "7 YodaQA (Baudi\u0161 and \u0160ediv\u1ef3, 2015) - - - - - - - 31.", "startOffset": 9, "endOffset": 34}, {"referenceID": 20, "context": "Evaluation tooling is from SQuAD (Rajpurkar et al., 2016).", "startOffset": 33, "endOffset": 57}, {"referenceID": 22, "context": "was popularized with TREC-8 (Voorhees, 1999).", "startOffset": 28, "endOffset": 44}, {"referenceID": 22, "context": "The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al.", "startOffset": 90, "endOffset": 106}, {"referenceID": 14, "context": "The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al., 2001) or structured knowledge bases (Berant et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 1, "context": ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).", "startOffset": 38, "endOffset": 97}, {"referenceID": 2, "context": ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).", "startOffset": 38, "endOffset": 97}, {"referenceID": 33, "context": ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).", "startOffset": 38, "endOffset": 97}, {"referenceID": 18, "context": "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).", "startOffset": 209, "endOffset": 312}, {"referenceID": 3, "context": "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).", "startOffset": 209, "endOffset": 312}, {"referenceID": 12, "context": "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).", "startOffset": 209, "endOffset": 312}, {"referenceID": 9, "context": "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).", "startOffset": 209, "endOffset": 312}, {"referenceID": 8, "context": "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).", "startOffset": 209, "endOffset": 312}, {"referenceID": 15, "context": "For example, (Lei et al., 2016) propose to first extract informative text fragments then feed them to text classification and question retrieval models.", "startOffset": 13, "endOffset": 31}, {"referenceID": 5, "context": "(Cheng and Lapata, 2016) and (Choi et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "(Cheng and Lapata, 2016) and (Choi et al., 2017) proposed coarse-to-fine frameworks with an additional sentence-level prediction followed by the original word-level prediction for text summarization and reading comprehension, respectively.", "startOffset": 29, "endOffset": 48}], "year": 2017, "abstractText": "In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closeddomain settings1 such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that \u201creads\u201d the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel opendomain QA system called Reinforced Ranker-Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker \u2217This work has been done during the 1st author\u2019s internship with IBM. In the QA community, \u201copenness\u201d can be interpreted as referring either to the scope of question topics or to the breadth and generality of the knowledge source used to answer each question. Following Chen et al. 2017a we adopt the latter definition. along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.", "creator": "LaTeX with hyperref package"}}}