{"id": "1709.00028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Glyph-aware Embedding of Chinese Characters", "abstract": "Given the advantage and recent success of English character-level and subword-unit models in several NLP tasks, we consider the equivalent modeling problem for Chinese. Chinese script is logographic and many Chinese logograms are composed of common substructures that provide semantic, phonetic and syntactic hints. In this work, we propose to explicitly incorporate the visual appearance of a character's glyph in its representation, resulting in a novel glyph-aware embedding of Chinese characters. Being inspired by the success of convolutional neural networks in computer vision, we use them to incorporate the spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In the context of two basic Chinese NLP tasks of language modeling and word segmentation, the model learns to represent each character's task-relevant semantic and syntactic information in the character-level embedding.", "histories": [["v1", "Thu, 31 Aug 2017 18:19:08 GMT  (280kb)", "http://arxiv.org/abs/1709.00028v1", "Workshop on Subword and Character level models in NLP at EMNLP 2017. Source code available"]], "COMMENTS": "Workshop on Subword and Character level models in NLP at EMNLP 2017. Source code available", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["falcon z dai", "zheng cai"], "accepted": false, "id": "1709.00028"}, "pdf": {"name": "1709.00028.pdf", "metadata": {"source": "CRF", "title": "Glyph-aware Embedding of Chinese Characters", "authors": ["Falcon Z. Dai", "Zheng Cai"], "emails": ["dai@ttic.edu,", "jontsai@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to follow the rules that they have imposed on themselves. (...) In fact, it is as if they are able to determine themselves what they want. (...) In fact, it is as if they are able to determine themselves. (...) In fact, it is as if they are able to determine themselves. (...) In fact, it is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) (...) It is as if they are able to determine themselves. (...). (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves."}, {"heading": "2 Hypotheses", "text": "We believe that the semantic and syntactical information of sub-glyphs structures can help improve character embedding and thus improve performance in Chinese NLP tasks. Intuitively, the representation of each character by its own ID alone implies that each pair of characters is as different as any other pair. This ignores all common sub-glyphs structures shared by the characters. Therefore, we should be able to generalize knowledge of a character about its common sub-glyphs structures. However, this hypothesis is not trivial, as there are many Chinese characters that share a strikingly similar visual appearance, but not their meaning. For example: (soil) \u2012 (roughly expressed means -he as a fighter) and (person) \u2012 (enter). By identifying a character with only its visual appearance, we are vulnerable to this new source of ambiguity that can damage performance."}, {"heading": "3 Method", "text": "To compare the proposed glyphs-conscious embedding with the glyphs-unconscious embedding, we will leave the recurring neural network architecture (RNN) unchanged and only change the embedding in our experiments. Given that there are many different layouts for sub-glyphs 4, and the same radical embedding may appear at different positions5, we think that the most promising representation that preserves both the identities and the spatial arrangement of the sub-structures is to use the raw pixels of a glyphs."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Chinese language modeling", "text": "Following the common approach of speech modeling (LM), we model the probability of a senteline 4,000 hidden U layer, which has a vertical arrangement, a horizontal and a compound. 5The radical I (mouth) may appear on the left, top, bottom, inside. 6We used Google's free Noto font (Google Inc.) during this work, including the Chinese characters represented in this paper.7http: / / github.com / falcondai / chinese-char-lmasp (c1, \u00b7, cn) = p (c1) n, i = 2p (ci | c1, \u00b7, ci \u2212 1) where ci is the i-th character in a set of n characters. The conditional distribution of p (ci | c1, \u00b7, ci \u2212 1) is the second row we model as a gated recurrent unit (GRU) (Chung et al., 2014) together with a bedder."}, {"heading": "Chinese word segmentation", "text": "We use the Peking University (PKU) dataset and the Microsoft Research dataset (MSR) from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005) to compare the proposed CNN embedder with the ID embedder. We formulated the segmentation task as a structured prediction problem when predicting whether a word boundary should be inserted behind a letter for each character since the entire input set is given, for example: The sequence prediction models in our experiments (RNN segmentor) and bidirectional long-term memory (LSTM) are recurring networks (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997). (see Table. 2 and Table. 3)."}, {"heading": "5 Analysis", "text": "In contrast to a numerical class in MNIST (LeCun et al., 2010), which has 6,000 training examples, one character has only one glyph and each sub-glyphs structure appears in an average of only about 40 characters. Therefore, we suspect that the variability of input to CNN is too limited. Following the traditional image magnification technique (Krizhevsky et al., 2012), we applied random jitters, i.e. 2D translations with a high-resolution x, an image magnification technique (Krizhevsky et al., 2012), to an image magnification technique (Krizhevsky et al., 2012)."}, {"heading": "6 Discussion", "text": "It should be noted that the number of parameters of the proposed CNN embedder is different from that of the ID embedder. Suppose the dimensionality of the embedding vectors is K, and the vocabulary size is N, the CNN embedder has O (N + K) many parameters: O (K) many trainable parameters and O (N) glyphs rendered from a font file. In contrast, the ID embedder O (N K) has many parameters, all traceable. This means that the CNN embedder represents a more compact representation with competitive performance than the ID embedder."}, {"heading": "Related work", "text": "Shi et al. (2015) represented a character by its radicals based on the Wubi input method, but this ignores the scales and spatial arrangement of each individual radical present in our rendered glyphs. It came to our late attention that Liu et al. (2017) independently looked at the same problem of character step modeling and experimented with vanilla CNN models that are almost identical to ours. They evaluated their method using a new document classification task instead of the frequently viewed tasks or benchmarks that we considered in this work. In line with their results, we also observed similar effects of CNN embedder, ID embedder and mixed embedder in our tasks. Our mixed embedding roughly corresponds to their early fusion model. Costa-juss\u00e0 et al. (2017) also considered including Chinese glyphs as additional features in their Chinese-Spanish embedder translation system and their translation inscription system."}, {"heading": "Future work", "text": "We hope to delve deeper into the cause of the CNN embedder's poor performance in the LM task. Specifically, we want to experiment with using the Bagof stroke prediction in a multi-task loss to provide CNN with additional supervision during training. Furthermore, we have only studied two NLP tasks that emphasize semantic and syntactic information in this work. In the future, we hope to explore tasks that require more phonetic information to work well, such as phoneme prediction."}, {"heading": "7 Conclusion", "text": "Our experiments show that glyphs-aware embedding can improve performance in some Chinese NLP tasks, particularly the task of word segmentation. Further studies are needed to understand the usefulness of glyphs in a more comprehensive way. However, given the visual ambiguity inherent in Chinese characters and the difficulty of interpreting neural network models, any further research using glyphs features and deep learning methods should exercise caution when measuring and verifying the contribution of glyphs features."}, {"heading": "Acknowledgement", "text": "We thank Kevin Gimpel, Matthew Walter and Karen Livescu for their kind support and helpful comments during our investigation and the anonymous reviewers for their constructive suggestions."}], "references": [{"title": "Neural word segmentation learning for chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "CoRR.", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Chinese\u2013spanish neural machine translation enhanced with character and word bitmap fonts", "author": ["Marta R Costa-juss\u00e0", "David Ald\u00f3n", "Jos\u00e9 AR Fonollosa."], "venue": "Machine Translation, pages 1\u201313.", "citeRegEx": "Costa.juss\u00e0 et al\\.,? 2017", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2017}, {"title": "Second international chinese word segmentation bakeoff", "author": ["Tom Emerson."], "venue": "http://sighan.cs. uchicago.edu/bakeoff2005/. Accessed: 201704-15.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "CoRR.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Mnist handwritten digit database", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges."], "venue": "AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2.", "citeRegEx": "LeCun et al\\.,? 2010", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Learning Character-level Compositionality with Visual Features", "author": ["Frederick Liu", "Han Lu", "Chieh Lo", "Graham Neubig."], "venue": "CoRR. ArXiv: 1704.04859.", "citeRegEx": "Liu et al\\.,? 2017", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Radical embedding: Delving deeper to chinese radicals", "author": ["Xinlei Shi", "Junjie Zhai", "Xudong Yang", "Zehua Xie", "Chao Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Shi et al\\.,? 2015", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Some comments on Zipf\u2019s law for the Chinese language", "author": ["S. Shtrikman."], "venue": "Journal of Information Science, 20(2):142\u2013143.", "citeRegEx": "Shtrikman.,? 1994", "shortCiteRegEx": "Shtrikman.", "year": 1994}, {"title": "Table of general standard chinese characters\u2014wikipedia, the free encyclopedia", "author": ["Wikipedia."], "venue": "[Online; accessed 21-July-2017].", "citeRegEx": "Wikipedia.,? 2017", "shortCiteRegEx": "Wikipedia.", "year": 2017}, {"title": "On the Applicability of Zipf\u2019s Law in Chinese Word Frequency Distribution", "author": ["Hang Xiao."], "venue": "Journal of Chinese Language and Computing, 18(1):33\u201346.", "citeRegEx": "Xiao.,? 2008", "shortCiteRegEx": "Xiao.", "year": 2008}, {"title": "The psychology of language", "author": ["George K Zipf."], "venue": "NY Houghton-Mifflin.", "citeRegEx": "Zipf.,? 1935", "shortCiteRegEx": "Zipf.", "year": 1935}], "referenceMentions": [{"referenceID": 15, "context": "the most common) characters and more than 8,105 characters in total (Wikipedia, 2017).", "startOffset": 68, "endOffset": 85}, {"referenceID": 17, "context": "At the same time, it is not correct to treat Chinese characters as equivalent to English words because the distribution of Chinese characters deviate markedly from Zipf\u2019s law (Zipf, 1935; Shtrikman, 1994).", "startOffset": 175, "endOffset": 204}, {"referenceID": 14, "context": "At the same time, it is not correct to treat Chinese characters as equivalent to English words because the distribution of Chinese characters deviate markedly from Zipf\u2019s law (Zipf, 1935; Shtrikman, 1994).", "startOffset": 175, "endOffset": 204}, {"referenceID": 16, "context": "Furthermore, there is evidence suggesting that segmented Chinese words, - some of them are unigrams -, distribute according to Zipf\u2019s law (Xiao, 2008).", "startOffset": 138, "endOffset": 150}, {"referenceID": 9, "context": "2Since then, we discovered two independent, concurrent studies with approaches similar to ours by Liu et al. (2017) and Costa-juss\u00e0 et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 2, "context": "(2017) and Costa-juss\u00e0 et al. (2017). 3A character\u2019s visual appearance is essential in solving hand-writing recognition tasks which are challenges in computer vision.", "startOffset": 11, "endOffset": 37}, {"referenceID": 7, "context": ", 1995) in learning feature representation in computer vision (Krizhevsky et al., 2012), we used CNN to implement the embedder (see Figure 1).", "startOffset": 62, "endOffset": 87}, {"referenceID": 1, "context": "The conditional distribution of p(ci|c1, \u00b7 \u00b7 \u00b7 , ci\u22121) is modeled as a gated recurrent unit (GRU) (Chung et al., 2014) together with an embedder.", "startOffset": 98, "endOffset": 118}, {"referenceID": 11, "context": "For all the layers, we use ReLU non-linearity throughout (Nair and Hinton, 2010).", "startOffset": 57, "endOffset": 80}, {"referenceID": 3, "context": "We experimented with language modeling on the Microsoft Research dataset (MSR) from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005).", "startOffset": 143, "endOffset": 158}, {"referenceID": 3, "context": "We use Peking University dataset (PKU) and Microsoft Research dataset (MSR) from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005) to compare the proposedCNN embedder with the ID embedder.", "startOffset": 140, "endOffset": 155}, {"referenceID": 4, "context": "We experimented with both single-directional GRU and bidirectional long short-term memory (LSTM) recurrent networks (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) as the sequence pre-", "startOffset": 116, "endOffset": 208}, {"referenceID": 5, "context": "We experimented with both single-directional GRU and bidirectional long short-term memory (LSTM) recurrent networks (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) as the sequence pre-", "startOffset": 116, "endOffset": 208}, {"referenceID": 12, "context": "We experimented with both single-directional GRU and bidirectional long short-term memory (LSTM) recurrent networks (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) as the sequence pre-", "startOffset": 116, "endOffset": 208}, {"referenceID": 0, "context": "15 NWS (Cai and Zhao, 2016) 95.", "startOffset": 7, "endOffset": 27}, {"referenceID": 0, "context": "43 NWS (Cai and Zhao, 2016) 96.", "startOffset": 7, "endOffset": 27}, {"referenceID": 6, "context": "We use Adam (Kingma and Ba, 2014) optimizer throughout all our experiments.", "startOffset": 12, "endOffset": 33}, {"referenceID": 9, "context": "Unlike a digit class in MNIST (LeCun et al., 2010) which has 6,000 training examples, given one font, a character only has one glyph and every sub-glyph structure appears on average in only about 40 characters.", "startOffset": 30, "endOffset": 50}, {"referenceID": 7, "context": "Modeling after common image augmentation technique (Krizhevsky et al., 2012), we applied random jitters, i.", "startOffset": 51, "endOffset": 76}, {"referenceID": 10, "context": "It came to our late attention that independently, Liu et al. (2017) considered the same characterlevel modeling problem and experimented with vanilla CNNmodels almost identical to ours.", "startOffset": 50, "endOffset": 68}, {"referenceID": 2, "context": "Costa-juss\u00e0 et al. (2017) also considered incorporating Chinese glyphs as additional features in their Chinese-Spanish machine translation system and their modeling approach corresponds roughly to our linear embedder.", "startOffset": 0, "endOffset": 26}], "year": 2017, "abstractText": "Given the advantage and recent success of English character-level and subword-unit models in several NLP tasks, we consider the equivalent modeling problem for Chinese. Chinese script is logographic and many Chinese logograms are composed of common substructures that provide semantic, phonetic and syntactic hints. In this work, we propose to explicitly incorporate the visual appearance of a character\u2019s glyph in its representation, resulting in a novel glyph-aware embedding of Chinese characters. Being inspired by the success of convolutional neural networks in computer vision, we use them to incorporate the spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In the context of two basic Chinese NLP tasks of language modeling and word segmentation, the model learns to represent each character\u2019s task-relevant semantic and syntactic information in the character-level embedding.", "creator": "LaTeX with hyperref package"}}}