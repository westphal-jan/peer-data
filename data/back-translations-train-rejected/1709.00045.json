{"id": "1709.00045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "On Security and Sparsity of Linear Classifiers for Adversarial Settings", "abstract": "Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks. This can be considered a relevant problem, as linear classifiers have been increasingly used in embedded systems and mobile devices for their low processing time and memory requirements. We exploit recent findings in robust optimization to investigate the link between regularization and security of linear classifiers, depending on the type of attack. We also analyze the relationship between the sparsity of feature weights, which is desirable for reducing processing cost, and the security of linear classifiers. We further propose a novel octagonal regularizer that allows us to achieve a proper trade-off between them. Finally, we empirically show how this regularizer can improve classifier security and sparsity in real-world application examples including spam and malware detection.", "histories": [["v1", "Thu, 31 Aug 2017 19:11:56 GMT  (150kb,D)", "http://arxiv.org/abs/1709.00045v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["ambra demontis", "paolo russu", "battista biggio", "giorgio fumera", "fabio roli"], "accepted": false, "id": "1709.00045"}, "pdf": {"name": "1709.00045.pdf", "metadata": {"source": "CRF", "title": "On Security and Sparsity of Linear Classifiers for Adversarial Settings", "authors": ["Ambra Demontis", "Paolo Russu", "Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "emails": ["ambra.demontis@diee.unica.it", "paolo.russu@diee.unica.it", "battista.biggio@diee.unica.it", "fumera@diee.unica.it", "roli@diee.unica.it"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are not \"normal\" people, but people who are able to move."}, {"heading": "2 Background", "text": "In this section, we summarize the attack model previously proposed in [8,9,10,11] and the link between regulation and robustness in [13,14,15]."}, {"heading": "2.1 Attacker\u2019s Model", "text": "In order to analyze possible attacks against machine learning and to design principled countermeasures, a formal model of classification is usually divided into two categories: in defining its goal (e.g., circumventing detection in the test phase), in which knowledge of the classifier and the ability to manipulate the input data play a role. Among the possible goals, we focus on evasive attacks in which the goal is to modify a single malicious sample (e.g., a spam email) in order to have it misclassified as legitimate (with the greatest confidence) by the classifier. Attacker's knowledge may have different levels of knowledge about the intended classifier; it may have limited or perfect knowledge of the training data, the scope of functions, and the classification algorithm [8,9]. In this work, we focus on perfect knowledge (worst-case)."}, {"heading": "2.2 Robustness and Regularization", "text": "The aim of this section is to clarify the relationship between regularization and input data uncertainty, based on the most recent results in [13,14,15]. In particular, Xu and al. [13] have considered the following robust optimization problem: min w, b max u1,... to define the SVM uncertainty i = 1 (1 \u2212 yi (w > (xi \u2212 ui) + b) +, (3) where (z) + equals z > R if z > 0 and 0 otherwise, u1,..., in order to define the SVM uncertainty a set of limited disturbances in training data {xi, yi} mi = 1 \u2012 Rm \u00d7 {\u2212 1, + 1} m, and the so-called uncertainty U = {(u1,..., um)."}, {"heading": "3 Security and Sparsity", "text": "Here we discuss the most important contributions of this paper. The result discussed in the previous section, similar to the one independently reported in [15], helps un-2 point out that the \"1 norm is the dual norm of the\" 3 norm \"and vice versa, while the\" 2 norm is the dual norm of itself. Derstanding the security properties of linear classifiers in adversarial settings, in terms of the relationship between security and sparseness. In fact, what is discussed in the previous section confirms not only the intuition in [4,7], i.e. that more uniform feature weighting schemes should improve classifier security by forcing the attacker to manipulate more feature values in order to circumvent detection. The result in [13,14,15] also clarifies the importance of the uniformity of feature weights. If a \"1 (sparse) tenarifiers, which is faced with higher costs in modifying other features, turns out to be the norm given more optimal.\""}, {"heading": "4 Experimental Analysis", "text": "We will first consider a paired version of the infinity norm, for the purpose of selecting (correlates) this theme to illustrate its weathering and its impact on the images."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we have shed light on the theoretical and practical implications of scarcity and security in linear classifiers. We have shown in opposing real-world applications that the choice of a suitable regulator is critical. In fact, infinity standard SVMs can drastically outperform the security of standard SVMs in sparse attacks. We believe this is an important result, since (standard) SVMs are widely used in security tasks without taking too much into account the risk of counter-attacks. In addition, we propose a new octagonal regulator that allows trading in scarcity for a marginal loss of security in sparse ID attacks, which is extremely useful in applications where scarcity and computing efficiency are critical at test time. If denser attacks are deemed more likely, instead, the standard SVM can be retained as a good compromise. In this case, if sparseness is required, one can trade a certain level of security for sparseness by using the Estic-29,M (Finally, we can trade the Estic-29,M for Easy Classifier)."}], "references": [{"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "10th Int\u2019l Conf. Knowl. Disc. Data Mining (KDD), Seattle, WA, USA, ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "11th Int\u2019l Conf. Knowl. Disc. Data Mining (KDD), Chicago, IL, USA, ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "2nd Conf. Email and Anti-Spam (CEAS), Mountain View, CA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "6th Conf. Email and Anti-Spam (CEAS), Mountain View, CA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "LEET \u201908, Berkeley, CA, USA, USENIX Association", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. Mach. Learn. Cyb. 1(1)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "In Blockeel, H. et al., eds.: ECML-PKDD, Part III, vol. 8190, LNCS, Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 26(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition systems under attack: Design issues and research challenges", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. Patt. Rec. Artif. Intell. 28(7)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "4th ACM Workshop Artif. Intell. and Sec. (AISec), Chicago, IL, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Adversarial feature selection against evasion attacks", "author": ["F. Zhang", "P. Chan", "B. Biggio", "D. Yeung", "F. Roli"], "venue": "IEEE Trans. Cyb. 46(3)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "J. Mach. Learn. Res. 10", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimization for Machine Learning", "author": ["S. Sra", "S. Nowozin", "S.J. Wright"], "venue": "The MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple geometric interpretation of SVM using stochastic adversaries", "author": ["R. Livni", "K. Crammer", "A. Globerson", "Edmond", "E.i.", "L. Safra"], "venue": "JMLR W&CP - vol. 22 of AISTATS \u201912.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On sparse feature attacks in adversarial learning", "author": ["F. Wang", "W. Liu", "S. Chawla"], "venue": "IEEE Int\u2019l Conf. on Data Mining (ICDM), IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn. 20", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Duality and geometry in SVM classifiers", "author": ["K.P. Bennett", "E.J. Bredensteiner"], "venue": "17th ICML, Morgan Kaufmann Publishers Inc.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "1-norm support vector machines", "author": ["J. Zhu", "S. Rosset", "R. Tibshirani", "T.J. Hastie"], "venue": "In Thrun, S., Saul, L., Sch\u00f6lkopf, B., eds.: NIPS 16, MIT Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR", "author": ["Bondell", "Reich"], "venue": "Biometrics 64", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y LeCun"], "venue": "Int\u2019l Conf. Artif. Neural Networks.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "TREC 2007 spam track overview", "author": ["G.V. Cormack"], "venue": "In Voorhees, E.M., Buckland, L.P., eds.: TREC. Volume Special Publication 500-274., NIST", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surv. 34", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A pattern recognition system for malicious PDF files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "In Perner, P., ed.: Mach. Learn. and Data Mining in Patt. Rec. vol. 7376, LNCS, Springer Berlin Heidelberg", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society, Series B 67(2)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Poisoning attacks against SVMs", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In Langford, J., Pineau, J., eds.: 29th ICML, Omnipress", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Is feature selection secure against training data poisoning", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "eds.: 32nd ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 2, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 3, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 4, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 5, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 6, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 7, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 8, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 9, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 10, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 5, "context": "For instance, the widely-used SpamAssassin anti-spam filter exploits a linear classifier [7,5].", "startOffset": 89, "endOffset": 94}, {"referenceID": 4, "context": "For instance, the widely-used SpamAssassin anti-spam filter exploits a linear classifier [7,5].", "startOffset": 89, "endOffset": 94}, {"referenceID": 3, "context": "investigated the security of linear classifiers to evasion attacks [4,7], suggesting the use of more evenly-distributed feature weights as a mean to improve their security.", "startOffset": 67, "endOffset": 72}, {"referenceID": 5, "context": "investigated the security of linear classifiers to evasion attacks [4,7], suggesting the use of more evenly-distributed feature weights as a mean to improve their security.", "startOffset": 67, "endOffset": 72}, {"referenceID": 11, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 12, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 13, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 11, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 12, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 13, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 6, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 7, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 8, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 9, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 11, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 12, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 13, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 6, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 7, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 8, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 9, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 6, "context": ", a spam email) to have it misclassified as legitimate (with the largest confidence) by the classifier [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "The attacker can have different levels of knowledge of the targeted classifier; she may have limited or perfect knowledge about the training data, the feature set, and the classification algorithm [8,9].", "startOffset": 197, "endOffset": 202}, {"referenceID": 7, "context": "The attacker can have different levels of knowledge of the targeted classifier; she may have limited or perfect knowledge about the training data, the feature set, and the classification algorithm [8,9].", "startOffset": 197, "endOffset": 202}, {"referenceID": 14, "context": "As discussed in [16], two kinds of constraints have been mostly used when modeling real-world adversarial settings, leading one to define sparse (`1) and dense (`2) attacks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "This amounts to (slightly) blurring the image, instead of obtaining a salt-and-pepper noise effect (as the one produced by sparse attacks) [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 1, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 6, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 7, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 10, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 11, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 12, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 13, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 11, "context": "[13] have considered the following robust optimization problem:", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Typical examples of uncertainty sets according to the above definition include `1 and `2 balls [13,14].", "startOffset": 95, "endOffset": 102}, {"referenceID": 12, "context": "Typical examples of uncertainty sets according to the above definition include `1 and `2 balls [13,14].", "startOffset": 95, "endOffset": 102}, {"referenceID": 11, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 12, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 13, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 11, "context": "3 in [13]):", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "This means that, if the `2 norm is chosen as the dual norm characterizing the uncertainty set U , then w is regularized with the `2 norm, and the above problem is equivalent to a standard Support Vector Machine (SVM) [17].", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "The result discussed in the previous section, similar to that reported independently in [15], helps un2 Note that the `1 norm is the dual norm of the `\u221e norm, and vice-versa, while the `2 norm is the dual norm of itself.", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "In fact, what discussed in the previous section does not only confirm the intuition in [4,7], i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 5, "context": "In fact, what discussed in the previous section does not only confirm the intuition in [4,7], i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 11, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 12, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 13, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 19, "context": "To visually show how evasion attacks work, we perform sparse and dense attacks on the MNIST digit data [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "As in [8], we simulate an adversarial classification problem where the digits 8 and 9 correspond to the legitimate and malicious class, respectively.", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "3 Note that octagonal regularization has been previously proposed also in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "For our experiments we use the TREC 2007 spam track data, consisting of about 25000 legitimate and 50000 spam emails [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "We extract a dictionary of terms (features) from the first 5000 emails (in chronological order) using the same parsing mechanism of SpamAssassin, and then select the 200 most discriminant features according to the information gain criterion [23].", "startOffset": 241, "endOffset": 245}, {"referenceID": 2, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 3, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 6, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 7, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 10, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 22, "context": "We represent every file using the 114 features that are described in [24].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "This is the standard SVM learning algorithm [17].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": ",d |wj | [20]:", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Its learning algorithm is defined as [19]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "We use here the elastic-net regularizer [25], combined with the hinge loss to obtain an SVM formulation with tunable sparsity:", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "To evaluate security of linear classifiers, we define a measure E of weight evenness, similarly to [4,7], based on the ratio of the `1 and `\u221e norm:", "startOffset": 99, "endOffset": 104}, {"referenceID": 5, "context": "To evaluate security of linear classifiers, we define a measure E of weight evenness, similarly to [4,7], based on the ratio of the `1 and `\u221e norm:", "startOffset": 99, "endOffset": 104}, {"referenceID": 6, "context": "These modified digits are obtained by solving Problem (1)-(2) through a simple projected gradient-descent algorithm, as in [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}, {"referenceID": 24, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}, {"referenceID": 25, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}], "year": 2017, "abstractText": "Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks. This can be considered a relevant problem, as linear classifiers have been increasingly used in embedded systems and mobile devices for their low processing time and memory requirements. We exploit recent findings in robust optimization to investigate the link between regularization and security of linear classifiers, depending on the type of attack. We also analyze the relationship between the sparsity of feature weights, which is desirable for reducing processing cost, and the security of linear classifiers. We further propose a novel octagonal regularizer that allows us to achieve a proper trade-off between them. Finally, we empirically show how this regularizer can improve classifier security and sparsity in real-world application examples including spam and malware detection.", "creator": "LaTeX with hyperref package"}}}