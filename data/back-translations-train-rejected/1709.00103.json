{"id": "1709.00103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "abstract": "A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 87726 hand-annotated examples of questions and SQL queries distributed across 26375 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 60.3% and logical form accuracy from 23.4% to 49.2%.", "histories": [["v1", "Thu, 31 Aug 2017 23:12:15 GMT  (1543kb,D)", "http://arxiv.org/abs/1709.00103v1", "10 pages, 5 figures"], ["v2", "Wed, 20 Sep 2017 19:14:04 GMT  (223kb,D)", "http://arxiv.org/abs/1709.00103v2", "12 pages, 5 figures"], ["v3", "Wed, 18 Oct 2017 01:34:46 GMT  (195kb,D)", "http://arxiv.org/abs/1709.00103v3", "12 pages, 5 figures"], ["v4", "Thu, 26 Oct 2017 21:13:56 GMT  (270kb,D)", "http://arxiv.org/abs/1709.00103v4", "12 pages, 5 figures"], ["v5", "Tue, 31 Oct 2017 20:49:31 GMT  (270kb,D)", "http://arxiv.org/abs/1709.00103v5", "12 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["victor zhong", "caiming xiong", "richard socher"], "accepted": false, "id": "1709.00103"}, "pdf": {"name": "1709.00103.pdf", "metadata": {"source": "CRF", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "authors": ["Victor Zhong", "Caiming Xiong"], "emails": ["vzhong@salesforce.com", "cmxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 Introduction", "text": "An enormous amount of today's information is stored in relational databases, which form the basis for important applications such as medical records [Hillestad et al., 2005], financial markets [Beck et al., 2000], and customer relationship management [Ngai et al., 2009]. However, accessing information in relational databases requires an understanding of query languages such as SQL, which attempt to provide tools for interacting with computers through the use of natural language processing and human-computer interactions."}, {"heading": "2 Related Work", "text": "However, the question we are asking is not only a question of semantically answering our questions, but also a question of semantically answering our questions, which in 2017 have focused in semantic terms based on exploring and answering local forms, without relying on annotated logical forms, through the use of conversation protocols [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], remote surveillance [Cai and Yates, 2013, Reddy et al., 2014], and fragmentary pairs of answers [Liang et al al al al, 2011]. Recently, Pasupat and Liang [2015] we have introduced a floating parser to address the large amount of variations in utterances and tables for semantic parsing on web tables."}, {"heading": "3 Model", "text": "The Seq2SQL model generates an SQL query from a question of natural language and a table schema. [1] A powerful base model is the Seq2Seq model used for machine translation. However, the output space of the standard Softmax in a Seq2Seq model is unnecessarily large for this task. In particular, we note that the problem can be dramatically simplified by restricting the output space of the generated sequence to merging the table schema, the question and the SQL keywords; the resulting model is similar to a pointer network [Vinyals et al., 2015] with extended inputs. We first show this extended pointer network model and then address its limitations, especially with respect to generating disordered query conditions, in our description of Seq2SQL."}, {"heading": "3.1 Augmented Pointer Network", "text": "In our case, the input sequence is the concatenation of column names used for the selection column and condition columns of the query, the question required for the conditions of the query, and the limited vocabulary of the SQL language such as SELECT, COUNT, etc. In the example shown in Figure 2, the column caption consists of \"Pick,\" \"#,\" CFL, \"\" Team, \"etc. The question tokens consists of\" How, \"\" many, \"\" CFL, \"etc.; the SQL tokens consist of SELECT, WHERE, COUNT, MAX, etc. With this augmented input sequence, the pointer network can fully generate the SQL query by selecting only from the column."}, {"heading": "4.1 Evaluation", "text": "Let N specify the total number of examples in the row, Nex the number of queries that, when executed, lead to the correct result, and Nlf the number of queries has an exact match of the string with the query used to capture the paraphrase. We evaluate the use of Accex = NexN accuracy metric. A disadvantage of Accex is that it is possible to construct an SQL query that does not correspond to the question but nevertheless produces the same result. For example, we use the two queries SELECT COUNT (Name) WHERE SSN = 123 and SELECT COUNT (SSN) WHERE SSN = 123 if no two people with different names produce the same result."}, {"heading": "5 Experiments", "text": "The data set is tokenized with the help of Stanford CoreNLP [Manning et al., 2014]. Specifically, we use the normalized tokens for training and return them to their original gloss before issuing the query. In this way, the queries are composed of tokens from the original gloss and are therefore executable in the database. We use GloVe word embedding [Pennington et al., 2014] and character-n-gram embedding [Hashimoto et al., 2016]. Let wgx denote the GloVe embedding and w c x the character embedding for word x. Here, wcx is the mean value for embedding all character-n-grams in x. For a given word x, we define its embedding wx = [wgx; w c x]. For words that have neither word nor character embedding, we assign the zero vector to. All networks are executed for a maximum of 100 units, with an early QM execution."}, {"heading": "5.1 Result", "text": "We compare the results with an attentive sequence to the sequence baseline based on the neural semantic parser proposed by Dong and Lapata [2016], which achieves state-of-the-art results based on a variety of semantic parsing datasets. We modify the model by adding the table schema to the input so that the model can be generalized to new tables. This model is described in detail in Section 2 of the Appendix. The performance of the three models is shown in Table 2.Reducing output space by using the extended pointer network significantly improves attentive sequences to the sequence model by 16.9%. Furthermore, the use of the structure of SQL queries leads to a further significant 4.8% improvement, as demonstrated by the performance of Seq2SQL without RL compared to the extended pointer network. Finally, the full Seq2SQL model, trained on the basis of amplification learning on the data base, leads to a further significant 2.7% increase in final performance from the https."}, {"heading": "5.2 Analysis", "text": "In fact, it's a reactionary thing, but it's not a reactionary thing, it's a reactionary thing, \"he said.\" We, \"he said,\" have been able to complain, \"he said.\" We, \"he said,\" have been able to complain, \"he said.\" We, \"he said,\" have been able to complain, have been able to get angry. \""}, {"heading": "6 Conclusion", "text": "We proposed Seq2SQL, an in-depth neural network for translating questions into SQL queries. Our model uses the structure of SQL queries to reduce the output space of the model. As part of Seq2SQL, we applied query execution in the loop to learn a policy for generating the conditions of the SQL query, which is inherently disordered and unsuitable for optimization through cross-entropy loss. We also introduced WikiSQL, a set of questions and SQL queries that is an order of magnitude larger than comparable records. Finally, we showed that Seq2SQL exceeds the attentive sequences of sequence models on WikiSQL, improving execution accuracy from 35.9% to 60.3% and logical form accuracy from 23.4% to 49.2%."}, {"heading": "A Collection of WikiSQL", "text": "In the paraphrase phase, we use tables extracted from Bhagavatula and al. [Bhagavatula et al., 2013] and remove small tables according to the following criteria: \u2022 the number of cells in each row is not equal \u2022 the content in a cell exceeds 50 characters \u2022 a header is empty \u2022 the table has fewer than 5 rows or 5 columns \u2022 over 40% of the cells in a row contain identical content. We also remove the last row of a table because a large amount of HTML tables tend to have summary statistics in the last row, and the last row does not adhere to the table scheme defined by the header."}, {"heading": "B Attentional Seq2Seq Baseline", "text": "This model was proposed by Dong et al. for neural semantic parsing and achieves state-of-the-art results on a variety of semantic parsing datasets. We implement a variant using OpenNMT and a global encoder decoder architecture (with input) described by Luong et al. we use the same two-layer, bi-directional, stacked LSTM encoder as described above. The decoder is a two-layer, unidirectional, stacked LSTM. The LSTM decoder is almost identical to the one described in Equation 2 of the paper, the only difference being derived from the input input. gs = LSTM ([emb (ys \u2212 1); e.g. dec s \u2212 1], gs \u2212 1) (8), where the dhid dimensional decoders decode the context via the input sequence during the decoding step, the highest attention is decoded (decoded) (10)."}, {"heading": "C Predictions by Seq2SQL", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Natural language interfaces to databases - an introduction", "author": ["I. Androutsopoulos", "G. Ritchie", "P. Thanisch"], "venue": null, "citeRegEx": "Androutsopoulos et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 1995}, {"title": "Bootstrapping semantic parsers from conversations", "author": ["Y. Artzi", "L.S. Zettlemoyer"], "venue": "In EMNLP,", "citeRegEx": "Artzi and Zettlemoyer.,? \\Q2011\\E", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L.S. Zettlemoyer"], "venue": "TACL, 1:49\u201362,", "citeRegEx": "Artzi and Zettlemoyer.,? \\Q2013\\E", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A new database on the structure and development of the financial sector", "author": ["T. Beck", "A. Demirg\u00fc\u00e7-Kunt", "R. Levine"], "venue": "The World Bank Economic Review,", "citeRegEx": "Beck et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2000}, {"title": "Neural combinatorial optimization with reinforcement learning", "author": ["I. Bello", "H. Pham", "Q.V. Le", "M. Norouzi", "S. Bengio"], "venue": "ICRL,", "citeRegEx": "Bello et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bello et al\\.", "year": 2017}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Methods for exploring and mining tables on wikipedia", "author": ["C. Bhagavatula", "T. Noraset", "D. Downey"], "venue": "In IDEA@KDD,", "citeRegEx": "Bhagavatula et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bhagavatula et al\\.", "year": 2013}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Q. Cai", "A. Yates"], "venue": "In ACL,", "citeRegEx": "Cai and Yates.,? \\Q2013\\E", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": null, "citeRegEx": "Dong and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Translating questions to SQL queries with generative parsers discriminatively reranked", "author": ["A. Giordani", "A. Moschitti"], "venue": "In COLING,", "citeRegEx": "Giordani and Moschitti.,? \\Q2012\\E", "shortCiteRegEx": "Giordani and Moschitti.", "year": 2012}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["J. Gu", "Z. Lu", "H. Li", "V.O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "author": ["K. Hashimoto", "C. Xiong", "Y. Tsuruoka", "R. Socher"], "venue": "arXiv, cs.CL", "citeRegEx": "Hashimoto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "Can electronic medical record systems transform health care? potential health benefits, savings, and costs", "author": ["R. Hillestad", "J. Bigelow", "A. Bower", "F. Girosi", "R. Meili", "R. Scoville", "R. Taylor"], "venue": "Health affairs,", "citeRegEx": "Hillestad et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hillestad et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning a neural semantic parser from user feedback", "author": ["S. Iyer", "I. Konstas", "A. Cheung", "J. Krishnamurthy", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Iyer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Association for Computational Linguistics (ACL) System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": null, "citeRegEx": "Merity et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2017}, {"title": "Learning a natural language interface with neural programmer", "author": ["A. Neelakantan", "Q.V. Le", "M. Abadi", "A. McCallum", "D. Amodei"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2017}, {"title": "Application of data mining techniques in customer relationship management: A literature review and classification", "author": ["E.W. Ngai", "L. Xiu", "D.C. Chau"], "venue": "Expert systems with applications,", "citeRegEx": "Ngai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ngai et al\\.", "year": 2009}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "In ACL,", "citeRegEx": "Pasupat and Liang.,? \\Q2015\\E", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["A.-M. Popescu", "O. Etzioni", "H. Kautz"], "venue": "In Proceedings of the 8th International Conference on Intelligent User Interfaces,", "citeRegEx": "Popescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P.J. Price"], "venue": null, "citeRegEx": "Price.,? \\Q1990\\E", "shortCiteRegEx": "Price.", "year": 1990}, {"title": "Large-scale semantic parsing without question-answer", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": "pairs. TACL,", "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["J. Schulman", "N. Heess", "T. Weber", "P. Abbeel"], "venue": "In NIPS,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M.J. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Joint learning of ontology and semantic parser from text", "author": ["J. Starc", "D. Mladenic"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Starc and Mladenic.,? \\Q2017\\E", "shortCiteRegEx": "Starc and Mladenic.", "year": 2017}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["L.R. Tang", "R.J. Mooney"], "venue": "In ECML,", "citeRegEx": "Tang and Mooney.,? \\Q2001\\E", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In ACL,", "citeRegEx": "Wong and Mooney.,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Dynamic coattention networks for question answering", "author": ["C. Xiong", "V. Zhong", "R. Socher"], "venue": "ICRL,", "citeRegEx": "Xiong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "In AAAI/IAAI,", "citeRegEx": "Zelle and Mooney.,? \\Q1996\\E", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "A vast amount of today\u2019s information is stored in relational databases, which provide the foundation of crucial applications such as medical records [Hillestad et al., 2005], financial markets [Beck et al.", "startOffset": 149, "endOffset": 173}, {"referenceID": 4, "context": ", 2005], financial markets [Beck et al., 2000], and customer relations management [Ngai et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 21, "context": ", 2000], and customer relations management [Ngai et al., 2009].", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "A prominent research area at the intersection of natural language processing and human-computer interactions is the study of natural language interfaces (NLI) [Androutsopoulos et al., 1995], which seek to provide means for humans to interact with computers through the use of natural language.", "startOffset": 159, "endOffset": 189}, {"referenceID": 1, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al.", "startOffset": 140, "endOffset": 169}, {"referenceID": 2, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al.", "startOffset": 186, "endOffset": 215}, {"referenceID": 17, "context": ", 2014], and questionanswer pairs [Liang et al., 2011].", "startOffset": 34, "endOffset": 54}, {"referenceID": 31, "context": "Previous semantic parsing systems were typically designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery [Tang and Mooney, 2001] and ATIS [Price, 1990].", "startOffset": 163, "endOffset": 186}, {"referenceID": 25, "context": "Previous semantic parsing systems were typically designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery [Tang and Mooney, 2001] and ATIS [Price, 1990].", "startOffset": 196, "endOffset": 209}, {"referenceID": 30, "context": "Researchers have also investigated QA over subsets of large-scale knowledge graphs such as DBPedia [Starc and Mladenic, 2017] and Freebase [Cai and Yates, 2013, Berant et al.", "startOffset": 99, "endOffset": 125}, {"referenceID": 32, "context": "The dataset \u201cOvernight\u201d [Wang et al., 2015] uses a similar crowd-sourcing process to build a dataset of (natural language question, logical form) pairs, but it is comprised of only 8 domains.", "startOffset": 24, "endOffset": 43}, {"referenceID": 22, "context": "WikiTableQuestions [Pasupat and Liang, 2015] is a collection of question and answers, also over a large quantity of tables extracted from Wikipedia.", "startOffset": 19, "endOffset": 44}, {"referenceID": 3, "context": "Our baseline model is based on the attentional sequence to sequence model (Seq2Seq) proposed by [Bahdanau et al., 2015].", "startOffset": 96, "endOffset": 119}, {"referenceID": 19, "context": "This class of models has been successfully applied to tasks such as language modeling [Merity et al., 2017], summarization [Gu et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 11, "context": ", 2017], summarization [Gu et al., 2016], combinatorial optimization [Bello et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 5, "context": ", 2016], combinatorial optimization [Bello et al., 2017], and question answering [Seo et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 9, "context": "However, unlike [Dong and Lapata, 2016], we use pointer based generation instead of Seq2Seq generation and show that our approach achieves higher performance, especially for rare words and column names.", "startOffset": 16, "endOffset": 39}, {"referenceID": 20, "context": "Unlike [Neelakantan et al., 2017], our model does not require access to the table content during inference, which may be unavailable due to privacy concerns.", "startOffset": 7, "endOffset": 33}, {"referenceID": 1, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al., 2014], and questionanswer pairs [Liang et al., 2011]. Recently, Pasupat and Liang [2015] introduced a floating parser to address the large amount of variation in utterances and table schema for semantic parsing on web tables.", "startOffset": 141, "endOffset": 362}, {"referenceID": 24, "context": "CISE [Popescu et al., 2003], which translates questions to SQL queries and identifies questions that it is not confident about.", "startOffset": 5, "endOffset": 27}, {"referenceID": 10, "context": "Giordani and Moschitti [2012] proposed a system to translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels.", "startOffset": 0, "endOffset": 30}, {"referenceID": 10, "context": "Giordani and Moschitti [2012] proposed a system to translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels. Both of these approaches rely on a high quality grammar and are not suitable for tasks that require generalization to new schema. Iyer et al. [2017] also proposed a system to translate to SQL, but with a Seq2Seq model that is further improved with human feedback.", "startOffset": 0, "endOffset": 318}, {"referenceID": 3, "context": "1 One powerful baseline model is the Seq2Seq model utilized for machine translation [Bahdanau et al., 2015].", "startOffset": 84, "endOffset": 107}, {"referenceID": 14, "context": "The network first encodes x using a two-layer, bidirectional Long Short-Term Memory network [Hochreiter and Schmidhuber, 1997].", "startOffset": 92, "endOffset": 126}, {"referenceID": 14, "context": "out the LSTM equations, which are described by Hochreiter and Schmidhuber [1997]. We then apply a pointer network similar to that proposed by Vinyals et al.", "startOffset": 47, "endOffset": 81}, {"referenceID": 14, "context": "out the LSTM equations, which are described by Hochreiter and Schmidhuber [1997]. We then apply a pointer network similar to that proposed by Vinyals et al. [2015] to the input encodings h.", "startOffset": 47, "endOffset": 164}, {"referenceID": 27, "context": "As described by Schulman et al. [2015], the act of sampling during the forward pass of the network can be followed by the corresponding injection of a synthetic gradient, which is a function of the reward, during the backward pass in order to compute estimated parameter gradient.", "startOffset": 16, "endOffset": 39}, {"referenceID": 31, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 59, "endOffset": 72}, {"referenceID": 8, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 32, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al., 2015], WebQuestions [Berant et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 6, "context": ", 2015], WebQuestions [Berant et al., 2013], and WikiTableQuestions [Pasupat and Liang, 2015].", "startOffset": 22, "endOffset": 43}, {"referenceID": 22, "context": ", 2013], and WikiTableQuestions [Pasupat and Liang, 2015].", "startOffset": 32, "endOffset": 57}, {"referenceID": 18, "context": "The dataset is tokenized using Stanford CoreNLP [Manning et al., 2014].", "startOffset": 48, "endOffset": 70}, {"referenceID": 23, "context": "We use, and keep fixed, GloVe word embeddings [Pennington et al., 2014] and character n-gram embeddings [Hashimoto et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": ", 2014] and character n-gram embeddings [Hashimoto et al., 2016].", "startOffset": 40, "endOffset": 64}, {"referenceID": 16, "context": "We train using ADAM [Kingma and Ba, 2014] and regularize using dropout [Srivastava et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 9, "context": "We compare results against an attentional sequence to sequence baseline based on the neural semantic parser proposed by Dong and Lapata [2016], which achieved state of the art results on a variety of semantic parsing datasets.", "startOffset": 120, "endOffset": 143}], "year": 2017, "abstractText": "A significant amount of the world\u2019s knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-theloop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 87726 hand-annotated examples of questions and SQL queries distributed across 26375 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 60.3% and logical form accuracy from 23.4% to 49.2%.", "creator": "LaTeX with hyperref package"}}}