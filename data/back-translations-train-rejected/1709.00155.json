{"id": "1709.00155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Order-Planning Neural Text Generation From Structured Data", "abstract": "Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.", "histories": [["v1", "Fri, 1 Sep 2017 04:46:10 GMT  (1181kb,D)", "http://arxiv.org/abs/1709.00155v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR cs.LG", "authors": ["lei sha", "lili mou", "tianyu liu", "pascal poupart", "sujian li", "baobao chang", "zhifang sui"], "accepted": false, "id": "1709.00155"}, "pdf": {"name": "1709.00155.pdf", "metadata": {"source": "CRF", "title": "Order-Planning Neural Text Generation From Structured Data", "authors": ["Lei Sha", "Lili Mou", "Tianyu Liu", "Pascal Poupart", "Sujian Li", "Baobao Chang", "Zhifang Sui"], "emails": ["szf}@pku.edu.cn", "doublepower.mou@gmail.com,", "ppoupart@uwaterloo.ca"], "sections": [{"heading": "Introduction", "text": "Generating texts from structured data (e.g. a table) is important for various tasks of processing natural language, such as answering questions and dialogs. Table 1 shows an example of a Wikipedia infobox (which contains fields and values) and a summary of texts. In early years, text creation is usually achieved by man-made rules and templates (Green 2006; Turner, Sripada, and Reiter 2010), and therefore the texts generated are not flexible. Recently, researchers have been using neural networks to capture texts from structured data (Lebret, Grangier, and Auli 2016), with a neural encoder decoding table information and a recursive neural network (RNN) decoding this information into a set of natural language. Although such a neural network-based approach is able to capture text from structured language, and can be trained in end-to-end mode, it lacks exposing a modeling of a word to generate a specific order of contents, such as N."}, {"heading": "Table:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Text:", "text": "In other words, a neural network should model not only the order of words (as well captured by RNN), but also the order of content, i.e. fields in a table. We also observe from real summaries that table fields themselves provide illuminating clues and limitations to text generation. In the biography domain, for example, the nationality of a person is typically mentioned before the occupation. This could benefit from explicit planning of the order of contents during neural text creation. In this paper, we propose a method of ordering for table-to-text generation. Our model is based on the encoder decoder framework and uses RNN for text synthesis taking into account table entries. In contrast to exciting neural models, we design a table field linking mechanism inspired by temporal memory concatenation."}, {"heading": "Approach", "text": "The neural network has three main components: \u2022 An encoder captures table information; \u2022 A dispatcher - a hybrid content and link-based information mechanism about table contents - plans what to generate next; and \u2022 A decoder uses RNN to generate a summary of natural language, into which we also build a copy mechanism (Gu et al. 2016) to cope with rare words.The rest of this section elaborates on these components."}, {"heading": "Encoder: Table Representation", "text": "We design a neural encoder to display tabular information. As shown in Figure 1, the contents of each field are divided into individual words and the entire table is transformed into a large sequence. Then, we use a recursive neural network (RNN) with long-term short-term memory (LSTM) units (Hochreiter and Schmidhuber 1997) to read the contents and their corresponding field names. Specifically, we let C be the number of substantive words in a table; let ci and fi contain the embedding of a content and its associated field, respectively (i = 1 \u00b7 C. The input of LSTM-RNN is the concatenation of fi and ci, referred to as xi = [fi; ci], and the output, referred to as hi, is the encrypted information corresponding to a substantive word, i.e."}, {"heading": "Dispatcher: Planning What to Generate Next", "text": "During the decoding process, the RNN is expanded with a dispatcher who plans what is to be generated, while a dispatcher is an attention mechanism through tabular content. In our model, the dispatcher is a hybrid of content and left-based attention that is discussed in detail as a consequence. Traditionally, the calculation of attention is based on the content representation hi as well as on some states during decoding (Bahdanau, Cho, Bengio 2015; Mei, Bansal and Walter 2016). We call this content-based attention, which is also a component of our decoding."}, {"heading": "Decoder: Sentence Generation", "text": "We also have an attention mechanism (Bahdanau, Cho and Bengio 2015) that summarizes the source information, i.e. the table in our scenario by weighted sum, giving an attention vector at byat = C \u2211 i = 1 \u03b1hybridt, i hi (12), where hi is the hidden representation that the table encoder gets. As \u03b1hybrid, i is a probable distribution - determined by both content and link information - of content words, allowing the decoder RNN to focus on relevant information at one point in time, which serves as a job scheduling mechanism for generating table to text. Then, we link the attention vector to and embedding the last step word yt \u2212 1, and use a single-layer neural network to mix information before we enter the decoder RNN input and the embedding of the last step (1) of the trauma period."}, {"heading": "Decoder", "text": "The score function is calculated by sLSTMt = Wsh \u2032 t + bs (14), where h \u2032 t is the state of the RNN decoder. (Ws and bs are weights.) The score function can be considered as entering a softmax level for classification before it is normalized to a probabilistic distribution. We include a copy mechanism (Gu et al. 2016) in our approach, and the normalization is performed after taking into account a copy mechanism that is introduced as follows. The copy mechanism scores a content word ci by its hidden representation hi in the encoder page, indicating how likely the word ci is to be copied directly during target generation. That is, i = bumps (h > i Wc) h \u2032 t (15) and st, i is a real number for i = 1, \u00b7 \u00b7 \u00b7 C (the number of words containing a parameter that is a multiple)."}, {"heading": "Training Objective", "text": "Our training goal is the negative log probability of a set y1 \u00b7 \u00b7 \u00b7 yT in the training set.J = \u2212 T \u2211 t = 1log p (yt | y0 \u00b7 \u00b7 yt \u2212 1) (19), where p (yt | \u00b7) is calculated by Equation 18. Like most other studies, a \"2 penalty is added. As all of the components described above are differentiable, our entire model can be trained through feedback, and we use Adam (Kingma and Ba 2015) for optimization."}, {"heading": "Dataset", "text": "We used the newly published WIKIBIO dataset (Lebret, Grangier and Auli 2016), 2, which contains 728,321 biographies from WikiProject Biography3 (originally from Wikipedia, September 2015).Each sample contains an infobox table with pairs of field contents, which is the input of our system.The generating target is the first sentence in the biography, which is based on the setting in previous work (Lebret, Grangier and Auli 2016).Although only the first sentence is taken into account in the experiment, the sentence typically serves as the summary of the article. In fact, the target sentence has an average of 26.1 tokens, which is actually long. In addition, the sentence contains information covering several fields, so our ordering mechanism is useful in this scenario. 2https: / / github.com / DavidGrangier / wikipedia-Biography-dataset3https: / wipedia.org / EU Wikipedia / Project Wikipedia sample is based on the BLASH 1000% of the results conducted."}, {"heading": "Settings", "text": "We decapitalized all the words and maintained a vocabulary size of 20,000 for substantive words and generational candidates that followed previous work (Lebret, Grangier and Auli 2016). Even with this relatively large vocabulary size, we had more than 900k words outside the vocabulary, streamlining the use of the copying mechanism. For the names of table fields, we treated each as special characters. By removing nonsensical fields whose content is \"none\" and grouping fields that occur less than 100 times as \"unknown\" fields, we had a total of 1475 different field names. In our experiments, both word \"and table fields\" were 400-dimensional embeddings and LSTM layers 500-dimensional. Note that one field (e.g. \"name\") and one content / generational word (e.g. \"name\"), even with the same string of characters, were considered as different tokens; therefore, they had different embeddings, all of which were Adam, and Adam, and the simple ones."}, {"heading": "Baselines", "text": "We compared our model with previous results using either traditional language models or neural networks. \u2022 KN and Template KN (Heafield et al. 2013): Lebret, Grangier and Auli (2016) train an interpolated KneserNey (KN) language model for comparison with the KenLM toolkit. They also train a KN language model with templates. \u2022 Table NLM: Lebret, Grangier and Auli (2016) suggest an RNN-based model with attention and copying mechanisms. They have several model variants, and we cite the most reported results. We report on the performance of the model in relation to several metrics, namely BLEU-4, ROUGE-4 and NIST-4, which are calculated by standard software, NIST mteval-v13a.pl (for BLEU and NIST) and MSR rouge-1.5.5 (for ROUGE)."}, {"heading": "Results", "text": "We see that modern neural networks are much better than traditional CN models with or without templates. Furthermore, our base model (with content attention only) outperforms the results of Lebret, Grangier and Auli (2016) and shows our better technical efforts. After adding all the proposed components, we get + 2.5 BLEU and ROUGE improvements and + 0.3 NIST improvements, thus reaching a new state of the art. Table 3 provides a comprehensive ablation test to verify the effectiveness of each component in our model. The top half of the table shows the results without copying mechanism and the bottom half takes into account the copy results as described above. We observe that the copy mechanism is consistently effective with different types of attention. We then compare content-based attention and left-based attention and their hybrid attention (also Table 3)."}, {"heading": "Case Study and Visualization", "text": "We give an example in Table 5: With only satisfactory attention, the network is confused as to when the word \"American\" is appropriate in the sentence and corrupts the wording of the former governor of the Federal Reserve System as it appears in the reference. However, when left-based attention is added, the network is more aware of the order between the fields of \"nationality\" and \"occupation\" and generates the American nationality before occupation economics. This process could also be illustrated in Figure 4. At this point, we sketch the content-based attention of our model, left-based attention and its hybrid. (Content and left-based attention probabilities may differ from those trained separately in the ablation test.) After generating \"emmett john rice (December 21, 1919 - March 10, 2011),\" content-based attention skips nationality and focuses more on occupation. Left-based attention, on the other hand, provides a strong indication that the occupation first and then the occupation will produce the strong indication of nationality."}, {"heading": "Related Work", "text": "As a matter of fact, most of them are unable to abide by the rules they have imposed on themselves. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules. (7) Most of them are unable to abide by the rules."}, {"heading": "Conclusion and Future Work", "text": "In this essay, we propose a neural order planning network that generates texts from a table (Wikipedia Infobox). The process of generating text is based on an RNN, taking into account tabular content. In contrast to traditional content-based attention, we explicitly model the sequence of content through a linkage matrix on which we calculate left-based attention. Then, a self-adaptive gate balances the content-based and left-based attention mechanisms. We also integrate a copy mechanism into our model to deal with rare or invisible words. We evaluated our approach using a newly proposed large-scale data set, WIKIBIO. Experimental results show that we significantly exceed previous results in terms of BLEU, ROUGE and NIST values. In addition, we had extensive ablation tests that show the effectiveness of the copying mechanism, as well as hybrid attention of the contents and the possible information we would get to the improvement of the method by comparing the attention of the table with the other methods."}, {"heading": "Acknowledgments", "text": "We thank Jing He of AdeptMind.ai for helpful discussions about different ways of using field information."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Liang Angeli", "G. Klein 2010] Angeli", "P. Liang", "D. Klein"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Angeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2015] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "and Lapata", "author": ["R. Barzilay"], "venue": "M.", "citeRegEx": "Barzilay and Lapata 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "CORAL: Using natural language generation for navigational assistance", "author": ["Geldof Dale", "R. Prost 2003] Dale", "S. Geldof", "J.-P. Prost"], "venue": "In Proceedings of the 26th Australasian Computer Science Conference,", "citeRegEx": "Dale et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dale et al\\.", "year": 2003}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Graves"], "venue": "Nature", "citeRegEx": "Graves,? \\Q2016\\E", "shortCiteRegEx": "Graves", "year": 2016}, {"title": "V", "author": ["J. Gu", "Z. Lu", "H. Li", "Li"], "venue": "O.", "citeRegEx": "Gu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["K. Heafield", "I. Pouzyrevsky", "Clark"], "venue": "H.; and Koehn, P.", "citeRegEx": "Heafield et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural text generation from structured data with application to the biography domain", "author": ["Grangier Lebret", "R. Auli 2016] Lebret", "D. Grangier", "M. Auli"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lebret et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2016}, {"title": "M", "author": ["Liang, P.", "Jordan"], "venue": "I.; and Klein, D.", "citeRegEx": "Liang. Jordan. and Klein 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "M", "author": ["H. Mei", "M. Bansal", "Walter"], "venue": "R.", "citeRegEx": "Mei. Bansal. and Walter 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Choosing words in computer-generated weather forecasts", "author": ["Reiter"], "venue": "Artificial Intelligence", "citeRegEx": "Reiter,? \\Q2005\\E", "shortCiteRegEx": "Reiter", "year": 2005}, {"title": "Neural responding machine for short-text conversation", "author": ["Lu Shang", "L. Li 2015] Shang", "Z. Lu", "H. Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Trainable sentence planning for complex information presentations in spoken dialog systems", "author": ["Prassad Stent", "A. Walker 2004] Stent", "R. Prassad", "M. Walker"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,", "citeRegEx": "Stent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Abstractive document summarization with a graphbased attentional neural model", "author": ["Wan Tan", "J. Xiao 2017] Tan", "X. Wan", "J. Xiao"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Tan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2017}, {"title": "Generating approximate geographic descriptions", "author": ["Sripada Turner", "R. Reiter 2010] Turner", "S. Sripada", "E. Reiter"], "venue": "In Empirical Methods in Natural Language Generation,", "citeRegEx": "Turner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turner et al\\.", "year": 2010}, {"title": "Real versus template-based natural language generation: A false opposition? Computational Linguistics 31(1):15\u201324", "author": ["Theune Van Deemter", "K. Krahmer 2005] Van Deemter", "M. Theune", "E. Krahmer"], "venue": null, "citeRegEx": "Deemter et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Deemter et al\\.", "year": 2005}, {"title": "E", "author": ["S. William", "White"], "venue": "B.", "citeRegEx": "William and White 1999", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [], "year": 2017, "abstractText": "Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WIKIBIO dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.", "creator": "LaTeX with hyperref package"}}}