{"id": "1709.00224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Variational Inference for Logical Inference", "abstract": "Functional Distributional Semantics is a framework that aims to learn, from text, semantic representations which can be interpreted in terms of truth. Here we make two contributions to this framework. The first is to show how a type of logical inference can be performed by evaluating conditional probabilities. The second is to make these calculations tractable by means of a variational approximation. This approximation also enables faster convergence during training, allowing us to close the gap with state-of-the-art vector space models when evaluating on semantic similarity. We demonstrate promising performance on two tasks.", "histories": [["v1", "Fri, 1 Sep 2017 09:55:44 GMT  (31kb,D)", "http://arxiv.org/abs/1709.00224v1", "Conference on Logic and Machine Learning in Natural Language (LaML)"]], "COMMENTS": "Conference on Logic and Machine Learning in Natural Language (LaML)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1709.00224"}, "pdf": {"name": "1709.00224.pdf", "metadata": {"source": "CRF", "title": "Variational Inference for Logical Inference", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction and Background", "text": "In fact, it is not that we have agreed on a common denominator. (...) It is not that we have agreed on a common denominator. (...) It is not that we have agreed on a common denominator. (...) It is that we have agreed on a common denominator. (...) It is that we have agreed on a common denominator. \"(...) It is that we have agreed on a common denominator.\" (...) It is that we have agreed on a common denominator. \"(...) It is that we have agreed on a common denominator.\" (...) It is that we have agreed on a common denominator. \""}, {"heading": "2 Theoretical Contributions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Logical Inference", "text": "This year, it will be able to shake the aforementioned hreeisrcW until they are able to do so."}, {"heading": "2.2 Variational Inference", "text": "As explained in the previous section, we can express certain logical statements as conditional probabilities, but the calculation of these probabilities Q = Q = Q is precisely impracticable, since it is a summation over the whole space that grows exponentially with the number of dimensions. Furthermore, we must avoid similar conditional probabilities when we form the model in the first place. First, instead of adding over the whole space, the summation of the individual dimensions over a small number of carefully selected pixies, using a Markov-Chain Monte Carlo method, is required. However, this algorithm is slow for two reasons. First, many iterations of the Markov chain are required before the samples are useful. Second, even if we do not add over the whole space, many samples are needed because the discrete values result in high variance values. In this section, we introduce a variable inference algorithm in which we directly approximate the distribution over pixies that we need and then optimize this approximation."}, {"heading": "3.1 Lexical Semantic Similarity", "text": "In order to measure the similarity of two predicates a and b, we use the conditional probability described in \u00a7 2.1 and illustrate it in Figures 4 and 6. Since this is an asymmetric measure, we multiply the conditional probabilities in both directions, i.e. we calculate P (ta, x | tb, x) P (tb, x | ta, x).We evaluated two data sets that aim to capture similarities instead of recognizing correlations: SimLex999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2001), which split into similarity and correlations. Results are adjusted in Table 1.9 for each data set hyperparameters are matched to the remaining datasets. Since WordSim-353 is a noun-based dataset, it is possible that performance on SimLex-999 verbs could be optimized by hyperparameters."}, {"heading": "3.2 RELPRON", "text": "The RELPRON dataset was created by Rimell et al. (2016). It consists of \"terms\" (all nouns), each paired with up to ten \"properties.\" For example, a telescope is a device used by astronomers, and a saw is a device that cuts wood. All properties are of this form: a hypernym of the term that is modified by a relative clause with a transitive verb. For each term, the task is to identify the properties that apply to that term. Since each property follows only two patterns, we can focus on semantics instead of analyzing it. A model that captures kinship can perform well on that dataset - Rimell et al. found that the other argument of the verb was the best predictor of the term (e.g. astronomer predicts that the entirety of the term is telescope)."}, {"heading": "4 Conclusion", "text": "We can define probabilistic logical conclusions in functional distribution semantics and efficiently calculate them using variable conclusions. We can use this to improve the performance of the RELPRON dataset, suggesting that our model can learn structures that are not captured by vector space models."}, {"heading": "Acknowledgements", "text": "We would like to thank Emily Bender for the helpful discussion and feedback on an earlier draft, which was supported by a Schiff Foundation Studentship."}, {"heading": "A Logical equivalence", "text": "A quantified theorem of the form Q A = > A is B, where Q is a quantifier that has limitations on the magnitudes of the theorems A\\\\ B and A\\ B and does not say anything about the magnitude of B. From these definitions, we can use the standard theorem to prove all and only the valid syllogisms. To prove equivalence with our probability framework, we must first note that the magnitudes of the theorems form a measurement (the \"counting measure\"), and the probabilities also form a measurement. The above conditions are all constraints on the magnitudes of the theorems that are zero or zero, so that it is sufficient to show that the magnitudes and probabilities are equivalent in the theoretical sense of the measure."}, {"heading": "B Derivation of update rule", "text": "We try to optimize Q to minimize the KLdivergences of Q (x) to P (x | tc, x). We try to minimize the KLdivergences of Q (x) to P (x | tc, x): DKL (P | | Q) = p \u2212 P (x | tc, x) \u2212 P (x \u2212 tc, x \u2212) log P (x | tc, x) Q (x | tc, x) Q (x) = x (x | tc, x) (logP) \u2212 logQ (x) Note that the first term is independent of Q. To optimize a parameter at a time iteratively, we take the derivative: X (x) P (x | tc) qi qi qi qi qi qi qi qi qi qi qi (x) (x) qi qi qi qi (x qi qi qi qi) x (x) qi qi qi qi x (x) qi qi qi qi qi qi x) (x qi qi qi qi qi) (x qi qi qi qi) (x) (x) qi qi qi qi x (x) (x) qi qi qi qi (x)."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "Proceedings of the 2009 Conference of the North American", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Frege in space: A program of compositional distributional semantics", "author": ["Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli."], "venue": "Linguistic Issues in Language Technology 9.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Representing meaning with a combination of logical and distributional models", "author": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J Mooney."], "venue": "Computational Linguistics 42(4):763\u2013808.", "citeRegEx": "Beltagy et al\\.,? 2016", "shortCiteRegEx": "Beltagy et al\\.", "year": 2016}, {"title": "Monte Carlo Semantics: Robust inference and logical pattern processing with Natural Language text", "author": ["Richard Bergmair."], "venue": "Ph.D. thesis, University of Cambridge.", "citeRegEx": "Bergmair.,? 2010", "shortCiteRegEx": "Bergmair.", "year": 2010}, {"title": "Efficient parsing with largescale unification grammars", "author": ["Ulrich Callmeier."], "venue": "Master\u2019s thesis, Universit\u00e4t des Saarlandes, Saarbr\u00fccken, Germany.", "citeRegEx": "Callmeier.,? 2001", "shortCiteRegEx": "Callmeier.", "year": 2001}, {"title": "Efficiency in ambiguity: Two models of probabilistic semantics for natural language", "author": ["Daoud Clarke", "Bill Keller."], "venue": "Proceedings of the 11th International Conference on Computational Semantics (IWCS). pages 129\u2013139.", "citeRegEx": "Clarke and Keller.,? 2015", "shortCiteRegEx": "Clarke and Keller.", "year": 2015}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "Linguistic Analysis 36:345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Austinian truth, attitudes and type theory", "author": ["Robin Cooper."], "venue": "Research on Language and Computation 3(2-3):333\u2013362.", "citeRegEx": "Cooper.,? 2005", "shortCiteRegEx": "Cooper.", "year": 2005}, {"title": "Probabilistic type theory and natural language semantics", "author": ["Robin Cooper", "Simon Dobnik", "Staffan Larsson", "Shalom Lappin."], "venue": "LiLT (Linguistic Issues in Language Technology) 10.", "citeRegEx": "Cooper et al\\.,? 2015", "shortCiteRegEx": "Cooper et al\\.", "year": 2015}, {"title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go", "author": ["Ann Copestake."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. pages 1\u20139.", "citeRegEx": "Copestake.,? 2009", "shortCiteRegEx": "Copestake.", "year": 2009}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["Ann Copestake", "Guy Emerson", "Michael Wayne Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszy\u0144ska."], "venue": "Proceedings of the 10th International", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Lexicalised compositionality", "author": ["Ann Copestake", "Aur\u00e9lie Herbelot."], "venue": "Unpublished draft.", "citeRegEx": "Copestake and Herbelot.,? 2012", "shortCiteRegEx": "Copestake and Herbelot.", "year": 2012}, {"title": "Functional Distributional Semantics", "author": ["Guy Emerson", "Ann Copestake."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP (RepL4NLP). Association for Computational Linguistics, pages 40\u201352.", "citeRegEx": "Emerson and Copestake.,? 2016", "shortCiteRegEx": "Emerson and Copestake.", "year": 2016}, {"title": "Semantic composition via probabilistic model theory", "author": ["Guy Emerson", "Ann Copestake."], "venue": "Proceedings of the 12th International Conference on Computational Semantics (IWCS). Association for Computational Linguistics.", "citeRegEx": "Emerson and Copestake.,? 2017", "shortCiteRegEx": "Emerson and Copestake.", "year": 2017}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th International Conference on the World Wide Web. Asso-", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "On building a more efficient grammar by exploiting types", "author": ["Dan Flickinger."], "venue": "Natural Language Engineering 6(1):15\u201328.", "citeRegEx": "Flickinger.,? 2000", "shortCiteRegEx": "Flickinger.", "year": 2000}, {"title": "Accuracy vs", "author": ["Dan Flickinger."], "venue": "robustness in grammar engineering. In Emily M Bender and Jennifer E Arnold, editors, Language from a cognitive perspective: Grammar, usage, and processing, CSLI Publications, pages 31\u201350.", "citeRegEx": "Flickinger.,? 2011", "shortCiteRegEx": "Flickinger.", "year": 2011}, {"title": "WikiWoods: Syntacto-semantic annotation for English Wikipedia", "author": ["Dan Flickinger", "Stephan Oepen", "Gisle Ytrest\u00f8l."], "venue": "Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC). European Language Resources", "citeRegEx": "Flickinger et al\\.,? 2010", "shortCiteRegEx": "Flickinger et al\\.", "year": 2010}, {"title": "Integrating logical representations with probabilistic information using Markov logic", "author": ["Dan Garrette", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 9th International Conference on Computational Semantics (IWCS). Association for Computational", "citeRegEx": "Garrette et al\\.,? 2011", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette."], "venue": "Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics (*SEM). pages 1\u201310.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson."], "venue": "Journal of Logic and Computation 25(2):335\u2013369.", "citeRegEx": "Larsson.,? 2013", "shortCiteRegEx": "Larsson.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:211\u2013 225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 1st International Conference on Learning Representations.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["Thomas P Minka."], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence. pages 362\u2013369.", "citeRegEx": "Minka.,? 2001", "shortCiteRegEx": "Minka.", "year": 2001}, {"title": "Random positive-only projections: PPMI-enabled incremental semantic space construction", "author": ["Behrang QasemiZadeh", "Laura Kallmeyer."], "venue": "Proceedings of the 5th Joint Conference on Lexical and Computational Semantics (*SEM). pages 189\u2013198.", "citeRegEx": "QasemiZadeh and Kallmeyer.,? 2016", "shortCiteRegEx": "QasemiZadeh and Kallmeyer.", "year": 2016}, {"title": "RELPRON: A relative clause evaluation dataset for compositional distributional semantics", "author": ["Laura Rimell", "Jean Maillard", "Tamara Polajnar", "Stephen Clark."], "venue": "Computational Linguistics 42(4):661\u2013 701.", "citeRegEx": "Rimell et al\\.,? 2016", "shortCiteRegEx": "Rimell et al\\.", "year": 2016}, {"title": "Resolving references to objects in photographs using the words-as-classifiers model", "author": ["David Schlangen", "Sina Zarrie\u00df", "Casey Kennington."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. pages 1213\u20131223.", "citeRegEx": "Schlangen et al\\.,? 2016", "shortCiteRegEx": "Schlangen et al\\.", "year": 2016}, {"title": "Cardinality Restricted Boltzmann Machines", "author": ["Kevin Swersky", "Ilya Sutskever", "Daniel Tarlow", "Richard S Zemel", "Ruslan R Salakhutdinov", "Ryan P Adams."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS). pages 3293\u20133301.", "citeRegEx": "Swersky et al\\.,? 2012", "shortCiteRegEx": "Swersky et al\\.", "year": 2012}, {"title": "Stochastic HPSG parse selection using the Redwoods corpus", "author": ["Kristina Toutanova", "Christopher D Manning", "Stephan Oepen."], "venue": "Journal of Research on Language and Computation 3(1):83\u2013105.", "citeRegEx": "Toutanova et al\\.,? 2005", "shortCiteRegEx": "Toutanova et al\\.", "year": 2005}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Extracting and annotating Wikipedia subdomains", "author": ["Gisle Ytrest\u00f8l", "Stephan Oepen", "Dan Flickinger"], "venue": "In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories", "citeRegEx": "Ytrest\u00f8l et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ytrest\u00f8l et al\\.", "year": 2009}, {"title": "Is this a child, a girl or a car? Exploring the contribution of distributional similarity to learning referential word meanings", "author": ["Sina Zarrie\u00df", "David Schlangen."], "venue": "Proceedings of the 15th Annual Conference of the European Chapter of the Association", "citeRegEx": "Zarrie\u00df and Schlangen.,? 2017", "shortCiteRegEx": "Zarrie\u00df and Schlangen.", "year": 2017}], "referenceMentions": [{"referenceID": 31, "context": "Standard approaches to distributional semantics represent meanings as vectors, whether this is done using the more traditional count vectors (Turney and Pantel, 2010), or using embedding vectors trained with a neural network (Mikolov et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 24, "context": "Standard approaches to distributional semantics represent meanings as vectors, whether this is done using the more traditional count vectors (Turney and Pantel, 2010), or using embedding vectors trained with a neural network (Mikolov et al., 2013).", "startOffset": 225, "endOffset": 247}, {"referenceID": 14, "context": "Further discussion of model theory will be given in forthcoming work (Emerson and Copestake, 2017) (henceforth E&C-forth).", "startOffset": 69, "endOffset": 98}, {"referenceID": 4, "context": "Coecke et al. (2010) and Baroni et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure.", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus.", "startOffset": 11, "endOffset": 293}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al.", "startOffset": 11, "endOffset": 383}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another).", "startOffset": 11, "endOffset": 409}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another). This approach requires existing vectors, and assumes we can interpret similarity in terms of inference. In contrast to the above, Emerson and Copestake (2016) (henceforth E&C) introduced the framework of Functional Distributional Semantics, which represents the meaning of a predicate not as a vector, but as a function.", "startOffset": 11, "endOffset": 730}, {"referenceID": 7, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015).", "startOffset": 114, "endOffset": 149}, {"referenceID": 8, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015).", "startOffset": 114, "endOffset": 149}, {"referenceID": 7, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015). Working within TTR, Larsson (2013) argues in favour of representing perceptual concepts as classifiers of perceptual input.", "startOffset": 115, "endOffset": 186}, {"referenceID": 3, "context": "Our approach to logical inference is related to the work of Bergmair (2010) and Clarke and Keller (2015), who use fuzzy truth values and probabilistic truth values, respectively.", "startOffset": 60, "endOffset": 76}, {"referenceID": 3, "context": "Our approach to logical inference is related to the work of Bergmair (2010) and Clarke and Keller (2015), who use fuzzy truth values and probabilistic truth values, respectively.", "startOffset": 60, "endOffset": 105}, {"referenceID": 11, "context": "Our model can be trained on a corpus annotated with Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009).", "startOffset": 98, "endOffset": 139}, {"referenceID": 9, "context": "Our model can be trained on a corpus annotated with Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009).", "startOffset": 98, "endOffset": 139}, {"referenceID": 29, "context": "The joint distribution over pixies is given by a Cardinality Restricted Boltzmann Machine (CaRBM) (Swersky et al., 2012).", "startOffset": 98, "endOffset": 120}, {"referenceID": 9, "context": "As Copestake and Herbelot (2012) note, distinguishing synonyms and antonyms requires checking whether expressions are mutually exclusive.", "startOffset": 3, "endOffset": 33}, {"referenceID": 25, "context": "3 Minimising this quantity is also done in the Expectation Propagation algorithm (Minka, 2001).", "startOffset": 81, "endOffset": 94}, {"referenceID": 29, "context": "Firstly, in areas where the number of active units is wrong, Q is bound to be too high, but if we want to sample from Q, we can avoid these areas by using belief propagation, as explained by Swersky et al. (2012). Secondly, in areas where the number of active units is correct, Q will be much higher than P only if there is a dependence between dimensions that Q cannot capture, such as if P is a multi-modal distribution.", "startOffset": 191, "endOffset": 213}, {"referenceID": 4, "context": "(2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al.", "startOffset": 88, "endOffset": 129}, {"referenceID": 30, "context": "(2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al.", "startOffset": 88, "endOffset": 129}, {"referenceID": 32, "context": ", 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al., 2009).", "startOffset": 83, "endOffset": 106}, {"referenceID": 10, "context": "6 The WikiWoods corpus was pre-processed using the Python packages pydelphin7 (developed by Michael Goodman), and pydmrs8 (Copestake et al., 2016).", "startOffset": 122, "endOffset": 146}, {"referenceID": 12, "context": "This resource was produced by Flickinger et al. (2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 26, "context": "To speed up training, we initialised our model using random positive-only projections, a simple method for producing reduced-dimension count vectors (QasemiZadeh and Kallmeyer, 2016).", "startOffset": 149, "endOffset": 182}, {"referenceID": 23, "context": "As with normal PPMIbased count vectors, there are several hyperparameters that can be tuned (Levy et al., 2015) \u2013 however, as we are using these vectors as parameters for semantic functions, it should be noted that the optimal hyperparameter settings are not the same.", "startOffset": 92, "endOffset": 111}, {"referenceID": 24, "context": "Both use Mikolov et al. (2013)\u2019s skipgram algorithm with negative sampling.", "startOffset": 9, "endOffset": 31}, {"referenceID": 21, "context": "We evaluated on two datasets which aim to capture similarity, rather than relatedness: SimLex999 (Hill et al., 2015), and WordSim-353 (Finkelstein et al.", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": ", 2015), and WordSim-353 (Finkelstein et al., 2001), which Agirre et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 0, "context": ", 2001), which Agirre et al. (2009) split into similarity and relatedness subsets.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "Performance of Word2Vec on SimLex-999 is higher than reported by Hill et al. (2015). Despite correspondence with the authors, it is not clear why their figures are so low.", "startOffset": 65, "endOffset": 84}, {"referenceID": 27, "context": "The RELPRON dataset was produced by Rimell et al. (2016). It consists of \u2018terms\u2019 (all nouns), each paired with up to ten \u2018properties\u2019.", "startOffset": 36, "endOffset": 57}], "year": 2017, "abstractText": "Functional Distributional Semantics is a framework that aims to learn, from text, semantic representations which can be interpreted in terms of truth. Here we make two contributions to this framework. The first is to show how a type of logical inference can be performed by evaluating conditional probabilities. The second is to make these calculations tractable by means of a variational approximation. This approximation also enables faster convergence during training, allowing us to close the gap with state-of-the-art vector space models when evaluating on semantic similarity. We demonstrate promising performance on two tasks.", "creator": "LaTeX with hyperref package"}}}