{"id": "1709.00226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Semantic Composition via Probabilistic Model Theory", "abstract": "Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.", "histories": [["v1", "Fri, 1 Sep 2017 10:01:52 GMT  (26kb,D)", "http://arxiv.org/abs/1709.00226v1", "International Conference on Computational Semantics (IWCS)"]], "COMMENTS": "International Conference on Computational Semantics (IWCS)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1709.00226"}, "pdf": {"name": "1709.00226.pdf", "metadata": {"source": "CRF", "title": "Semantic Composition via Probabilistic Model Theory", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Vector space models of semantics are popular in NLP, as they are easy to work with, can be trained on unannotated corpora, and are useful in many tasks. They can be trained in many ways, including counting methods (Turney and Pantel, 2010) and neural embedding methods (Mikolov et al., 2013). In addition, they allow a natural and computer-efficient measurement of similarity, in the form of cosmic similarities. But even if we can form models that produce a good similarity, a vector space does not provide natural operations for other aspects of meaning."}, {"heading": "2 Related Work", "text": "An approach that is looking for new ways into the future is the search for a solution to the problems that have arisen in the past. (...) Another approach is the search for a solution to the problems that have arisen in the past. (...) Another approach is the search for a solution to the problems of the future. (...) Another approach is the search for a solution to the problems of the future. (...) Another approach is the search for a solution to the problems of the future. (...) Another approach is the search for a solution to the problems of the future. (...) Another approach is the search for a solution for the future. (...) Another approach is the search for a solution for the future. (...) Another approach is the search for a solution for the future. (...) Another approach is the search for a solution for the future. (...) Another approach is the search for a solution for the future. (...) Another approach is the search for a solution for the future. (...) A different approach for the future. (...) A different approach is the future."}, {"heading": "3 From Model Theory to Probability Theory", "text": "In this section, we will show how model theory can be reconciled in a probabilistic setting. The aim is not to present a complete probabilistic logic, but rather to show how we can define a family of probability distributions that capture traditional model structures as a special case and at the same time allow structured representations of the way they are used in machine learning. In this way, we will be able to view functional distribution semantics as a generalization of model theory."}, {"heading": "3.1 Background: Model Theory, Neo-Davidsonian Events, and Situations", "text": "A standard approach to formal semantics is to use an extended model structure (Cann, 1993; Allan, 2001; Kamp and Reyle, 2013). First, we define a series of \"individuals\" (or \"entities\") in the model. Then, we define the meaning of a predicate as its extension - the subset of individuals for whom the predicate is true. Expansion can also be characterized by a truth-based function - a functional assignment of individuals to truth values. Individuals in the extension of the predicate are assigned to true, and all other individuals to false. We adopt a neo-Davidsonian approach to event semantics (Davidson, 1967; Parsons, 1990). This treats events as individuals, and verbal predicates are single-place relationships that can be true for event individuals. Other participants in an event are indicated by dual-place relationships that link the event to the individual's argument, as an individual would relate a sentence and five stories."}, {"heading": "3.2 Model Structures as Probability Distributions", "text": "In this section, we generalize this notion of a model structure in two ways: First, instead of a set of situations, we look at a probability distribution over a set of situations; second, instead of deterministic truth-related functions, we look at probabilistic truth-related functions.A probability distribution over a set of situations is, of course, more general than the set itself, because it provides more information - just as knowing that a situation is in the set, we also know its probability. From a formal linguistic point of view, this may seem irrelevant to the concept of truth; but from the point of view of machine learning (and perhaps also from the point of view of acquisition), it is very helpful if our goal is not only to represent the truth, but also what is true, we do not know in advance which situations should be part of the model structure."}, {"heading": "3.3 Denotations versus Truth-Conditional Functions", "text": "If individuals are atomic elements without further structure, then denotations and truth-related functions have almost identical representations. A denotation is a subset of the group of individuals, while a truth-related function is the indicator function for this subset: individuals in the denotation are mapped to 1, and other individuals to 0. The conversion between these two representations is trivial. If individuals are structured objects, denotations and truth-related functions can have quite different representations. To represent the structure of individuals, we assume that we have a semantic space in which every point in space represents a possible individual, including information about all its characteristics. We will use the term \"pixie\" to refer to a point in semantic space, since it is intuitively a \"pixel\" of space. Note that E & C uses the term \"entity\" to refer to both individuals and their abilities."}, {"heading": "3.4 Functional Distributional Semantics as Model-Theoretic Semantics", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.5 Interpretation of Quantifiers", "text": "This year is the highest in the history of the country."}, {"heading": "4 From Conditional Dependence to Context Dependence", "text": "In the previous section, we have seen how a model structure can be generalized using probability distributions. In this section, we will show how this approach allows us to grasp context-dependent meanings in a natural way with conditional probabilities."}, {"heading": "4.1 Occasion Meaning versus Standing Meaning", "text": "In fact, it is a very strange situation, in which most of us feel able to solve our problems, and in which we feel able to get to grips with our problems."}, {"heading": "4.2 Context Dependence in Functional Distributional Semantics", "text": "This year, we will be able to find a solution that will enable us to achieve our goals, \"he said.\" We will be able to achieve our goals, \"he said.\" We will be able to achieve our goals, \"he said.\" We will be able to achieve our goals, \"he said."}, {"heading": "4.3 Semantic Composition using Context-Dependent Meanings", "text": "Vector space models are about mapping two or more vectors to a single vector. Specifically, if nearby vectors represent similar meanings, only the first few significant digits of each dimension are important, which limits the meaning of a vector. Even if we use economical vectors, we are \"consumed of all available dimensions.\" Therefore, the composition of vectors is generally impractical."}, {"heading": "4.4 Inference using Context-Dependent Meanings", "text": "In fact, it is the case that we will be able to solve the problems mentioned in order to solve and resolve them."}, {"heading": "5 Experimental Results", "text": "This year is the highest in the history of the country."}, {"heading": "5.1 Lexical Similarity", "text": "We evaluated our model using several lexical similarity data sets. Our goal is firstly to show that the performance of our model competes with state-of-the-art vector space models, and secondly to show that our model can specifically target similarity rather than kinship. For example, while the predicates of painting and painting are related, they apply to very different individuals. We used SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016), both aimed at measuring similarity, not kinship; MEN (Bruni et al., 2014); and WordSim-353 (Finkelstein et al., 2001), both aimed at measuring similarity, not kinship; MEN (Bruni et al., 2014); and WordSim-353 (Verd."}, {"heading": "5.2 Similarity in Context", "text": "Grefenstette and Sadrzadeh (2011) have created a dataset of pairs of SVO triples in which only the verb in the pair varies. Each pair was commented on because of the similarity. 11 For example, annotators had to evaluate the similarity of Word2Vec to SimLex-999. 11 The performance of Word2Vec to SimLex-999 is higher than that of Hill et al. (2015). Despite correspondence with the authors, it is not clear why their numbers are so low.For every three times we calculated the mean field vector for the verb due to all three predicates. We then calculated the probability that the predicate of the other verb correlates between the results of the system and the average annotations. For every three times we have the medium vector for the verb due to the three predicates, the best predicate."}, {"heading": "5.3 Composition of Relative Clauses", "text": "The RELPRON dataset was produced by Rimell et al. (2016). In fact, it consists of a series of \"terms,\" each paired with up to ten \"properties.\" Each property is a short phrase, consisting of a hypernym of the term, modified by a relative clause with a transitive verb. For example, a telescope is a device that astronomers use, and a saw is a device that cuts wood. The task is to identify the properties that apply to each term, constructed as a retrievable task: given a single term, and the complete set of properties, the goal is to rank the properties with the right properties at the top of the list. There are 65 terms and 518 terms that apply in the test phase, and 569 terms that will run property in the test phase."}, {"heading": "6 Conclusion", "text": "We can interpret Functional Distribution Semantics to learn a probable model structure that provides us with natural operations for composition, inference, and context dependence, with applications in both computer and formal semantics. Our experiments show that the additional structure of the model allows it to learn and use information that is not captured by vector space models."}, {"heading": "Acknowledgements", "text": "We would like to thank Emily Bender for the helpful discussion and detailed feedback on an earlier draft. This work was supported by a grant from the Schiff Foundation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.", "creator": "LaTeX with hyperref package"}}}