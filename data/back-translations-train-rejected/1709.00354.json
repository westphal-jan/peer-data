{"id": "1709.00354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Query-by-example Spoken Term Detection using Attention-based Multi-hop Networks", "abstract": "Retrieving spoken content with spoken queries, or query-by- example spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.", "histories": [["v1", "Fri, 1 Sep 2017 14:56:53 GMT  (2339kb,D)", "http://arxiv.org/abs/1709.00354v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.MM", "authors": ["chia-wei ao", "hung-yi lee"], "accepted": false, "id": "1709.00354"}, "pdf": {"name": "1709.00354.pdf", "metadata": {"source": "CRF", "title": "QUERY-BY-EXAMPLE SPOKEN TERM DETECTION USING ATTENTION-BASED MULTI-HOP NETWORKS", "authors": ["Chia-Wei Ao", "Hung-yi Lee"], "emails": ["r04942094@ntu.edu.tw,", "hungyilee@ntu.edu.tw"], "sections": [{"heading": null, "text": "Index Terms - Query for Examples, Attention-Based Multi-Hop Network"}, {"heading": "1. INTRODUCTION", "text": "The query of spoken content with spoken content is also known as query-by-example spoken term detection (STD). Because both the content and the queries are in the form of speech, it is possible to transmit the signals directly to the acoustic level without transforming them into phonemes or words. For low-level languages with short annotations, recognition is difficult, so it makes sense to bypass the need for speech recognition, which usually involves learning large amounts of spoken audio data."}, {"heading": "2. RELATED WORK", "text": "There are a number of approaches that have changed in recent years. Thus, for example, it is in the USA, but also in other countries, in which people are able to unfold. (...) In the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "3. FRAMEWORK AND TRAINING SCENARIO", "text": "The network architecture is described in the next section. Network input is the spoken query and an audio scenario in the database to be retrieved. However, both the spoken query and the audio scenario are represented by acoustic characteristics such as MFCCs. In this paper, the audio segments cover multiple utterances and as such are much longer than the spoken queries. The output of the network is a scalar. The scalar represents the confidence that the term exists in the spoken query in the audio segment. In the face of a spoken query, the system ranks the audio segments in the database according to the trust results and provides the search results. It may appear that the proposed approach only extracts the target audio segments rather than locating the time periods of the query in the segments. In fact, the time spans are determined on the basis of the attention mechanism in the network. This is shown in the experimental half of the network in which the training is performed."}, {"heading": "4. ATTENTION-BASED MULTI-HOP NETWORK", "text": "In this section we describe the model architecture of the attention-based multi-hop network."}, {"heading": "4.1. Query Representation", "text": "The input query is a sequence of T vectors, x1, x2,..., xT, each of which is an acoustic characteristic vector like MFCC. In this example, we use bidirectional LSTM, but it is trivial to replace bidirectional LSTM with unidirectional LSTM. In Figure 2 (A), the hidden layer output of the forward LSTM (blue rectangle) at the time index t is denoted by yft; that of the backward LSTM (pink rectangle) is denoted by ybt. After traversing all frames in the query, the query vector representation VQ or VQ = GRQ = the forward layer of the forward LSTM network at the time of the first and the forward layer of the forward LSTM (pink rectangle) is denoted as ybt."}, {"heading": "4.2. Audio Segment Representation with Attention", "text": "Fig. 2 (B) shows an audio segment (with multiple pronouncements) in the database to be retrieved. Although it is a lengthy acoustic feature sequence, for simplicity we only show eight features. The bidirectional LSTM in Fig. 2 (B) goes through the entire document and encodes each frame3. The vector representation of the tenth frame St is the concatenation of the hidden layer outputs of the forward and backward LSTM networks; i.e. St = [y f t] ybt. This process can be completed off-line before the spoken query is submitted. Then the attention value \u03b1t for each time model t is the cosmic similarity between the query vector VQ (A) and the vector representation St of each frame, \u03b1t = St VQ4. We normalize the attention values \u03b1t as \u03b1 \u2032 t. Then we normalize vectors from the bidirectional network LSTM for each fragment."}, {"heading": "4.3. Hopping", "text": "Fig. 3 illustrates the hopping: The entered spoken query is first converted by the module in Fig. 2 (A) into a vector VQ1, then 2Here the symbol [\u00b7 \u00b7 \u00b7 \u00b7] denotes the concatenation of two vectors. 3The bidirectional LSTMs used in Fig. 2 (A) and (B) are the same. 4Symbol denotes the cosinal similarity between two vectors that the module in (B) uses to calculate the attention values \u03b1t in order to obtain the storyvector VS1. Then, VQ1 and VS1 are added to form the new question vector VQ2. This process is the first hop (Hop 1). Output of the first hop VQ2 can be used to calculate the new attention values in order to obtain a new storyvector VS2: This can be considered a questionable expansion where the machine goes over the audiosector again to extract a new fragment to extract a new information."}, {"heading": "4.4. Keyword Detection", "text": "Finally, as shown in the upper half of Fig. 3, a detector determines the confidence value based on the query vector representation VQ and the VSn vector representation. Here, we use three methods to calculate this value: (1) we use the cosinal similarity between VQ and VSn as the result; (2) we use the detector - a networked feedback neuron network - using VQ and VSn as input and output a scalar as the confidence value; (3) we combine (1) and (2): A neural network gives the query vector VQ, the VSn vector and the cosine similarity as input and output a score."}, {"heading": "5. EXPERIMENTAL SETUP", "text": "We used the LibriSpeech corpus [49] as data for the experiments. \u2022 To train the attention-based multi-hop network, some query segment pairs were needed as training examples. 70,000 training examples were used in the experiments, including 500 different spoken queries; all audio segments came from the LibriSpeech train Clean 360 set. In the monitored scenario, the label for each example (a query segment pair) was specified to determine whether the audio segment contains the spoken query. 5 In the unattended scenario, each example is labeled with the score from the DTW algorithm [50]. There are three test sets. In all test sets, the audio segments were evaluated5This is easily determined with the manual transcriptions of the audio segments available from the LibriSpeech corpus."}, {"heading": "6. EXPERIMENTAL RESULTS", "text": "In Section 6.1, 6.2 and 6.3, we look at the monitored scenario. The results of the unattended scenario are presented in Section 6.4."}, {"heading": "6.1. Attention-based Model", "text": "The results of the study show that the number of people who are able, are able, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move."}, {"heading": "6.2. Attention analysis", "text": "In recognizing the spoken term, we try to determine not only whether the query term exists in the audio segments, but sometimes - if it exists - the time spans of those query terms within the segments. We find that the attention weights reveal the time spans of the query terms: Fig. 4 shows an example. An audio segment spectrogram is shown in Fig. 46. The blue curve is the normalized attention weights (sharpening). The time span of the query term in the audio segment is in Rot7. We find that larger attention weights usually appear at the end of the words (although we do not provide the model with information about word boundaries). In this test-6The transcription of the audio segment was \"withal one of the most beautiful girls ever seen as people naturally love their own loveliness this mother even spended on her old daughter and at the end of the same end.\""}, {"heading": "6.3. Multiple Hopping", "text": "Table 2 shows the results when using several hops for the production of audio segment representations. The results in line (B) are the results without several hops, which are also shown in line (C-1) of table 1; the results with 2 to 5 hops are those in lines (C) to (F). Several hops perform better than individual hops (lines (C) to (F) v.s. (B), with the exception of 1 and 3 hops in test set 1. This shows that hops improve the universality of the model as the training and test data do not match in test sets 2 and 3."}, {"heading": "6.4. Unsupervised Scenario", "text": "We use DTW as a teacher's approach and normalize the DTW similarity values between 0 and 1 as the target of regression; the results are shown in Table 3. The table shows that the performance of the attention-based network without multiple hops is comparable to DTW (lines (B) v.s. (A) and that the 3-hop network surpasses DTW in test sets 2 and 3 (lines (C) v.s. (A). At this point, we emphasize that the time complexity of the network during the test is much lower than DTW: with a document length of M and a query length of N, the time complexity of the DTW is O (M \u00d7 N), while the time complexity of the network is O (M \u00b7 n), where n is the number of hops 8, so it makes sense to replace DTW with a network learned from it."}, {"heading": "7. CONCLUSION", "text": "In this paper, we propose an end-to-end STD model based on an attention-based multi-hop network that has been successfully used in QA. The model can be trained either under supervision or unsupervised. In the monitored scenario, we show that attention and multiple hops are both very helpful, and that the attention weights of the proposed model reveal the time span of the input keyword. In the unsupervised environment, the neural network mimics the DTW behavior and achieves performance comparable to DTW at shorter runtimes. In the future, we will explore more new attention-based models and explore new models that output time spans directly instead of a confidence value. In addition, we will apply the attention-based multi-hop model to STD with text query. 8 We implemented DTW in C + + and the network using Tensorflow. On average, the proposed approach was faster than a TW without using seven GU."}, {"heading": "8. REFERENCES", "text": "[1] Yaodong Zhang and James R Glass, \"Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,\" in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398-403. [2] Gokhan Tur and Renato DeMori, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, Chapter 15, pp. 417-446, John Wiley & Sons Inc., 2011. [3] C. Chelba, T.J. Hazen, and Renato DeMori, and M. Saraclar \"Retrieval and browsing of Spoken Content,\" Signal Processing Magazine, IEEE, vol. 25, no. 3, pp. 39-49, May 2008. [4] Martha Larson and Gareth J. Jones, \"Spoken content retrieval: A survey of techniques and technologies,\" Trends."}], "references": [{"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Yaodong Zhang", "James R Glass"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013403.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "DeMori, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, chapter 15, pp. 417\u2013446", "author": ["Gokhan Tur", "Renato"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Retrieval and browsing of spoken content", "author": ["C. Chelba", "T.J. Hazen", "M. Saraclar"], "venue": "Signal Processing Magazine, IEEE, vol. 25, no. 3, pp. 39 \u201349, may 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Spoken content retrieval: A survey of techniques and technologies", "author": ["Martha Larson", "Gareth J.F. Jones"], "venue": "Found. Trends Inf. Retr., vol. 5, pp. 235\u2013422, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent developments in spoken term detection: a survey", "author": ["Anupam Mandal", "K.R. Prasanna Kumar", "Pabitra Mitra"], "venue": "International Journal of Speech Technology, pp. 1\u201316, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken content retrievalbeyond cascading speech recognition with text retrieval", "author": ["Lin-shan Lee", "James Glass", "Hung-yi Lee", "Chunan Chan"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 9, pp. 1389\u20131420, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "The spoken web search task at mediaeval 2011", "author": ["F. Metze", "N. Rajput", "X. Anguera", "M. Davel", "G. Gravier", "C. van Heerden", "G.V. Mantena", "A. Muscariello", "K. Prahallad", "I. Szoke", "J. Tejedor"], "venue": "ICASSP, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task", "author": ["F. Metze", "E. Barnard", "M. Davel", "C. Van Heerden", "X. Anguera", "G. Gravier", "N. Rajput."], "venue": "MediaEval 2012 Workshop, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task", "author": ["Xavier Anguera", "Florian Metze", "Andi Buzo", "Igor Szoke", "Luis Javier Rodriguez-Fuentes"], "venue": "MediaEval 2013 Workshop, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Query by example search on speech at mediaeval 2014", "author": ["Xavier Anguera", "Luis-Javier Rodriguez Fuentes", "Igor Szke", "Andi Buzo", "Florian Metze"], "venue": "MediaEval 2014 Workshop, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Query by example search on speech at mediaeval 2015", "author": ["Andi Buzo", "Xavier Anguera", "Florian Metze", "Jorge Proenca", "Martin Lojka", "Xiao Xiong"], "venue": ".", "citeRegEx": "11", "shortCiteRegEx": null, "year": 0}, {"title": "Language independent search in mediaeval\u2019s spoken web search task", "author": ["Florian Metze", "Xavier Anguera", "Etienne Barnard", "Marelie Davel", "Guillaume Gravier"], "venue": " Computer Speech & Language, vol. 28, no. 5, pp. 1066 \u2013 1082, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative articulatory models for spoken term detection in low-resource conversational settings", "author": ["Rohit Prabhavalkar", "Karen Livescu", "Eric Fosler-Lussier", "Joseph Keshet"], "venue": "ICASSP, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative spoken term detection with limited data", "author": ["Rohit Prabhavalkar", "Joseph Keshet", "Karen Livescu", "Eric Fosler-Lussier"], "venue": "MLSLP, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust discriminative keyword spotting for emotionally colored spontaneous speech using bidirectional LSTM networks", "author": ["M. Wollmer", "F. Eyben", "J. Keshet", "A. Graves", "B. Schuller", "G. Rigoll"], "venue": "ICASSP, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative keyword spotting", "author": ["Joseph Keshet", "David Grangier", "Samy Bengio"], "venue": "Speech Communication, vol. 51, pp. 317 \u2013 329, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative keyword spotting", "author": ["Joseph Keshet", "David Grangier", "Samy Bengio"], "venue": "NOLISP, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Endto-end asr-free keyword search from speech", "author": ["Kartik Audhkhasi", "Andrew Rosenberg", "Abhinav Sethy", "Bhuvana Ramabhadran", "Brian Kingsbury"], "venue": "arXiv preprint arXiv:1701.04313, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["Keith Levin", "Katharine Henry", "Aren Jansen", "Karen Livescu"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 410\u2013 415.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "ICASSP, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "venue": "ICASSP, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung- Yi Lee", "Lin-Shan Lee"], "venue": "arXiv preprint arXiv:1603.00982, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples", "author": ["Hung-Yi Lee", "Lin-Shan Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272\u20131284, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid HMM/DNN approach to keyword spotting of short words", "author": ["I.-F. Chen", "C.-H. Lee"], "venue": "INTER- SPEECH, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting discriminative point process models for spoken term detection", "author": ["A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Query-by-example keyword spotting using long shortterm memory networks", "author": ["Guoguo Chen", "Carolina Parada", "Tara N. Sainath"], "venue": "ICASSP, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Query-by-example search with discriminative neural acoustic word embeddings", "author": ["Shane Settle", "Keith Levin", "Herman Kamper", "Karen Livescu"], "venue": "arXiv preprint arXiv:1706.03818, 2017.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "CoRR, vol. abs/1410.3916, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "CoRR, vol. abs/1503.08895, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "CoRR, vol. abs/1506.07285, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "CoRR, vol. abs/1506.02075, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "CoRR, vol. abs/1603.01417, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "CoRR, vol. abs/1506.03340, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "CoRR, vol. abs/1511.07394, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "venue": "CoRR, vol. abs/1611.01604, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "venue": "CoRR, vol. abs/1611.01603, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "ABC-CNN: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "CVPR, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "CVPR, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards machine learning comprehension of spoken content: Initial toefl listening comprehension test by machine", "author": ["B.-H. Tseng", "S.-S. Shen", "H.-Y. Lee", "L.-S. Lee"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical attention model for improved machine comprehension of spoken content", "author": ["Wei Fang", "Jui-Yang Hsu", "Hung yi Lee", "Lin-Shan Lee"], "venue": "SLT, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "venue": "arXiv preprint arXiv:1611.01603, 2016.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "arXiv preprint arXiv:1511.05234, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "arXiv preprint arXiv:1410.3916, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Librispeech: an ASR corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206\u20135210.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Dynamic time warping (DTW) [1] is widely used in this case.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "The reader is referred to several tutorial chapters and papers [2\u20136].", "startOffset": 63, "endOffset": 68}, {"referenceID": 2, "context": "The reader is referred to several tutorial chapters and papers [2\u20136].", "startOffset": 63, "endOffset": 68}, {"referenceID": 3, "context": "The reader is referred to several tutorial chapters and papers [2\u20136].", "startOffset": 63, "endOffset": 68}, {"referenceID": 4, "context": "The reader is referred to several tutorial chapters and papers [2\u20136].", "startOffset": 63, "endOffset": 68}, {"referenceID": 5, "context": "The reader is referred to several tutorial chapters and papers [2\u20136].", "startOffset": 63, "endOffset": 68}, {"referenceID": 6, "context": "This direction exactly matches the target of the Spoken Web Search (SWS) task [7\u201311] 1, a part", "startOffset": 78, "endOffset": 84}, {"referenceID": 7, "context": "This direction exactly matches the target of the Spoken Web Search (SWS) task [7\u201311] 1, a part", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "This direction exactly matches the target of the Spoken Web Search (SWS) task [7\u201311] 1, a part", "startOffset": 78, "endOffset": 84}, {"referenceID": 9, "context": "This direction exactly matches the target of the Spoken Web Search (SWS) task [7\u201311] 1, a part", "startOffset": 78, "endOffset": 84}, {"referenceID": 10, "context": "This direction exactly matches the target of the Spoken Web Search (SWS) task [7\u201311] 1, a part", "startOffset": 78, "endOffset": 84}, {"referenceID": 11, "context": "A complete overview of the approaches developed in SWS in 2011 and 2012 is available [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14\u201318], in which the time spans corresponding to the queries in utterances are considered as hidden information.", "startOffset": 110, "endOffset": 117}, {"referenceID": 13, "context": "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14\u201318], in which the time spans corresponding to the queries in utterances are considered as hidden information.", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14\u201318], in which the time spans corresponding to the queries in utterances are considered as hidden information.", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14\u201318], in which the time spans corresponding to the queries in utterances are considered as hidden information.", "startOffset": 110, "endOffset": 117}, {"referenceID": 16, "context": "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14\u201318], in which the time spans corresponding to the queries in utterances are considered as hidden information.", "startOffset": 110, "endOffset": 117}, {"referenceID": 17, "context": "An end-to-end deep learning based system for text query STD has been proposed [19].", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "Query-by-example STD by representing each word segment as a vector [20\u201323] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].", "startOffset": 67, "endOffset": 74}, {"referenceID": 19, "context": "Query-by-example STD by representing each word segment as a vector [20\u201323] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].", "startOffset": 67, "endOffset": 74}, {"referenceID": 20, "context": "Query-by-example STD by representing each word segment as a vector [20\u201323] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].", "startOffset": 67, "endOffset": 74}, {"referenceID": 21, "context": "Query-by-example STD by representing each word segment as a vector [20\u201323] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].", "startOffset": 67, "endOffset": 74}, {"referenceID": 21, "context": "Query-by-example STD by representing each word segment as a vector [20\u201323] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].", "startOffset": 307, "endOffset": 311}, {"referenceID": 19, "context": "Several approaches have been successfully used in STD [21, 24\u201326], but these approaches were developed primarily in more heuristic ways, rather than deep learning.", "startOffset": 54, "endOffset": 65}, {"referenceID": 22, "context": "Several approaches have been successfully used in STD [21, 24\u201326], but these approaches were developed primarily in more heuristic ways, rather than deep learning.", "startOffset": 54, "endOffset": 65}, {"referenceID": 23, "context": "Several approaches have been successfully used in STD [21, 24\u201326], but these approaches were developed primarily in more heuristic ways, rather than deep learning.", "startOffset": 54, "endOffset": 65}, {"referenceID": 24, "context": "Several approaches have been successfully used in STD [21, 24\u201326], but these approaches were developed primarily in more heuristic ways, rather than deep learning.", "startOffset": 54, "endOffset": 65}, {"referenceID": 25, "context": "By learning RNN with an audio segment as the input and the corresponding word as the target, the outputs of the hidden layer at the last few time steps can be taken as the representation of the input segment [27].", "startOffset": 208, "endOffset": 212}, {"referenceID": 26, "context": "Audio segment embedding can also be jointly learned with their corresponding character sequences by multi-view approach [28].", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "Sequence-to-sequence Autoencoder is used to represent variable-length audio segments by vectors with fixed dimensionality, which is referred to as Audio Word2Vec [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 26, "context": "However, it was shown that neural embeddings learned from pre-segmented audio can be applied for embedding arbitrary segments [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 118, "endOffset": 126}, {"referenceID": 28, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 118, "endOffset": 126}, {"referenceID": 29, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 160, "endOffset": 164}, {"referenceID": 29, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 235, "endOffset": 242}, {"referenceID": 30, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 235, "endOffset": 242}, {"referenceID": 31, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 235, "endOffset": 242}, {"referenceID": 32, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 235, "endOffset": 242}, {"referenceID": 33, "context": "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31\u201335].", "startOffset": 235, "endOffset": 242}, {"referenceID": 34, "context": "Some new attention mechanisms [36, 37] recently proposed are shown to be helpful on reading comprehension dataset where the answer to every question is a segment of text.", "startOffset": 30, "endOffset": 38}, {"referenceID": 35, "context": "Some new attention mechanisms [36, 37] recently proposed are shown to be helpful on reading comprehension dataset where the answer to every question is a segment of text.", "startOffset": 30, "endOffset": 38}, {"referenceID": 36, "context": "In vidual question answering (VQA), attention-based configurable convolutional neural network (ABC-CNN) can learn question-guided attention [38], and it is shown that multiple hops yielded improved results compared to a single step [39].", "startOffset": 140, "endOffset": 144}, {"referenceID": 37, "context": "In vidual question answering (VQA), attention-based configurable convolutional neural network (ABC-CNN) can learn question-guided attention [38], and it is shown that multiple hops yielded improved results compared to a single step [39].", "startOffset": 232, "endOffset": 236}, {"referenceID": 38, "context": "Attention-based multihop networks were also applied on machine comprehension of spoken content [40], and hierarchical attention model (HAM) [41] further constructed tree-structured sentence representations for sentences from their parsing trees and estimate attention weights on different nodes of the hierarchies.", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "Attention-based multihop networks were also applied on machine comprehension of spoken content [40], and hierarchical attention model (HAM) [41] further constructed tree-structured sentence representations for sentences from their parsing trees and estimate attention weights on different nodes of the hierarchies.", "startOffset": 140, "endOffset": 144}, {"referenceID": 40, "context": "2 (A), a bidirectional long short-term memory (LSTM) network [42] takes one frame from the input spoken query sequentially at a time.", "startOffset": 61, "endOffset": 65}, {"referenceID": 41, "context": "To ensure a time complexity linear to the length of the input audio segment, we do not use more sophisticated attention models [43]; thus the approach is faster than DTW.", "startOffset": 127, "endOffset": 131}, {"referenceID": 42, "context": "in [44], there are two ways to normalize the scores: Sharpening: The score list is normalized using the softmax activation function:", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "This has been widely used in many existing neural attention frameworks [45\u201348], and works well with noisy data.", "startOffset": 71, "endOffset": 78}, {"referenceID": 44, "context": "This has been widely used in many existing neural attention frameworks [45\u201348], and works well with noisy data.", "startOffset": 71, "endOffset": 78}, {"referenceID": 45, "context": "This has been widely used in many existing neural attention frameworks [45\u201348], and works well with noisy data.", "startOffset": 71, "endOffset": 78}, {"referenceID": 46, "context": "This has been widely used in many existing neural attention frameworks [45\u201348], and works well with noisy data.", "startOffset": 71, "endOffset": 78}, {"referenceID": 47, "context": "We used the LibriSpeech corpus [49] as the data for the experiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 48, "context": "\u2022 The networks were trained for 100 epochs using ADAM [51] without momentum, with a fixed learning rate of 0.", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Retrieving spoken content with spoken queries, or query-byexample spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.", "creator": "LaTeX with hyperref package"}}}