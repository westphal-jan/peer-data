{"id": "1709.00387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge", "abstract": "In order to successfully annotate the Arabic speech con- tent found in open-domain media broadcasts, it is essential to be able to process a diverse set of Arabic dialects. For the 2017 Multi-Genre Broadcast challenge (MGB-3) there were two possible tasks: Arabic speech recognition, and Arabic Dialect Identification (ADI). In this paper, we describe our efforts to create an ADI system for the MGB-3 challenge, with the goal of distinguishing amongst four major Arabic dialects, as well as Modern Standard Arabic. Our research fo- cused on dialect variability and domain mismatches between the training and test domain. In order to achieve a robust ADI system, we explored both Siamese neural network models to learn similarity and dissimilarities among Arabic dialects, as well as i-vector post-processing to adapt domain mismatches. Both Acoustic and linguistic features were used for the final MGB-3 submissions, with the best primary system achieving 75% accuracy on the official 10hr test set.", "histories": [["v1", "Mon, 28 Aug 2017 14:20:02 GMT  (168kb,D)", "http://arxiv.org/abs/1709.00387v1", "Submitted to the 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2017)"]], "COMMENTS": "Submitted to the 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2017)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["suwon shon", "ahmed ali", "james glass"], "accepted": false, "id": "1709.00387"}, "pdf": {"name": "1709.00387.pdf", "metadata": {"source": "CRF", "title": "MIT-QCRI ARABIC DIALECT IDENTIFICATION SYSTEM FOR THE 2017 MULTI-GENRE BROADCAST CHALLENGE", "authors": ["Suwon Shon", "Ahmed Ali", "James Glass"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Dialect Recognition, Arabic, MGB Challenge, Siamese Network, Domain Adaptation"}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "2. MGB-3 ARABIC DIALECT IDENTIFICATION", "text": "For the MGB-3 ADI task, Challenge organizers provided 13,825 expressions (53.6 hours) for the training set (TRN), 1,524 expressions (10 hours) for a development set (DEV), and 1,492 expressions (10.1 hours) for a test set (TST). Each data set consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA). Detailed statistics of the ADI data set can be found in [24]. Table 1 shows some facts about the evaluation conditions and data properties. Note that the development set is relatively low compared to the2Of course, due to the acoustic discrepancy that may indirectly affect performance for linguistic skills, the development rate may be higher than the test set. Thus, the development set provides valuable information for adapting the test set or recording (in the domain)."}, {"heading": "3. DIALECT IDENTIFICATION TASK & SYSTEM", "text": "The MGB-3 FDI task asks participants to classify language as one of five dialects by specifying one dialect for each audio file they submit, and performance is judged on three indices: overall accuracy, average precision, and average memory of the five dialects."}, {"heading": "3.1. Baseline ADI System", "text": "The task consisted of 400 dimensional i-vector features for each audio file (based on bottleneck feature inputs for their frame-level acoustic representation) and lexical features using bigrams generated from transcriptions [24]. A multi-level support vector machine (SVM) was used to detect the basic dialect. Basic i-vector performance was 57.3%, 60.8%, and 58.0%, respectively, in terms of accuracy, precision, and retrieval. Dictionary features reached 48.4%, 51.0%, and 49.3%, respectively. While the audio-based features performed better than the lexical features, both systems achieved only about 50% accuracy, indicating that this ADI task is difficult given that there are only five classes to choose from."}, {"heading": "3.2. Siamese Neural Network-based ADI", "text": "To further distinguish language from different Arabic dialects and to make the language from the same dialect more similar, we chose a Siamese neural network architecture [25] based on an i-vector attribute space. Siamese neural network has two parallel revolutionary networks, GW, which have the same set of weights, W, as shown in Figure 1 (a). Let us consider spectrum 1 and spectrum 2 as appasions of i-vectors for which we want to calculate a distance. Let us take Y as a label for the pair, where Y = 1 if the i-vectors perspec1 and \u03c92 belong to the same dialect, and Y = \u2212 1. To optimize the network, let us use a euclidean distance loss function between the label and the cosinal distance, DW, where L (\u03c9j, Yij, Yij = | | \u03c92 belong to the same dialect, and Y = 1. Otherwise, let us use a cosmic distance loss function between the cosmic label and W."}, {"heading": "3.3. i-vector Post-Processing", "text": "In this section, we describe the domain adaptation techniques that we have studied using the development set to adapt our models to the test set."}, {"heading": "3.3.1. Interpolated i-vector Dialect Model", "text": "Although the baseline system used an SVM classifier, Cosine Distance Scoring (CDS) is a quick, easy and effective method to measure the similarity between a captured ivector model and a test value i-vector. Under CDS, the ZT standard or S standard can also be used to normalize the score [26]. The dialect classification can be achieved by i-vectors for each dialect and is called the i-vector dialect model. As we have two datasets for dialect classification, the I-vector dialect model is used: \u03c9d = (1 / nd)."}, {"heading": "3.3.2. Recursive Whitening Transformation", "text": "For i-vector-based loudspeaker and speech recognition approaches, white light transformation and length normalization are considered indispensable [27]. Since length normalization is by nature a non-linear, non-white operation, a recursive white light transformation was recently proposed to reduce remaining non-white components in the i-vector space, as illustrated in Figure 3 [15]. In this approach, the data subset that best corresponds to the test data is used for each iteration to calculate the white light transformation. In our ADI experiments, we applied 1 to 3 levels of recursive white light transformation using training and development data."}, {"heading": "3.4. Phoneme Features", "text": "Phoneme extraction consists of the extraction of the telephone sequence and the telephone duration statistics with four different speech detectors: Czech, Hungarian and Russian with narrowband model and English with broadband model [28]. We evaluated the four systems with a Support Vector Machine (SVM). Hyperparameters for the SVM are the distance from the hyperplane (C is 0.01) and the penalty l2. We used the training data for the training of the SVM and the development data for the test. Table 2 shows the results for the four phoneme detectors. The Hungarian phoneme detection achieved the best results, so we used it for the final system combination."}, {"heading": "3.5. Character Features", "text": "Word strings are extracted from speech to text using a state-of-the-art Arabic transcription system developed under the MGB-2 [29]. The system is a combination of a Time Delayed Neural Network (TDNN), a Long Short-Term Memory Recurrent Neural Network (LSTM), and LSTM bidirectional acoustic models, followed by 4-gram and Recurrent Neural Network (RNN) speech models. Our system uses a graph lexicon both during training and when decrypting. The acoustic models are trained on 1200 hours of Arabic transmission language. We also perform data augmentation (speed and volume disturbance), which gives us three times the original training data. For more details, see the system description [5]. We have retained the < UNK > system from the ASR system, which specifies words outside the vocabulary (OV) that we have replaced with a special symbol."}, {"heading": "3.6. Score Calibration", "text": "All values are calibrated between 0 and 1. Linear calibration is carried out using the Bosaris toolkit [30]. Fusion is also linear."}, {"heading": "4. ADI EXPERIMENTS", "text": "For experiments and evaluations, we use i-vectors and transcriptions provided by the challenge organizers. For descriptions of i-vector extraction and Arabic language-text configuration, see [24]."}, {"heading": "4.1. Using Training Data for Training", "text": "The first experiment we conducted used only the training data for the development of the ADI system. Therefore, the interpolated i-vector dialect model cannot be used for this experimental condition. Table 3 shows the performance of dimensionally reduced i-vectors using the Siamese network (Siam i-vector) and Linear Discrimination Analysis (LDA i-vector) compared to the base system. LDA reduces the i-vector of the 400 dimension to 4, while the Siamese network reduces it from 400 to 200. Since the Siamese network used a cosinal distance for the loss function, the Siam i-vector showed better performance with the CDS scoring method, while others achieved better performance with an SVM. Overall, the best system with Siam i-vector showed 10% better performance accuracy compared to the baseline."}, {"heading": "4.2. Using Training and Development Data for Training", "text": "For our second experiment, both the training and the development data were used for the training. For phonemes and character traits, we show experimental results of the development in Table 4. For i-vector experiments, we show results in Table 5. In the table, we see that the interpolated dialect model brought significant improvements in all three metrics. Recursive whitening resulted in slight improvements over the original i-vector, but not according to LDA and the Siamese network. The best system is the original i-vector with recursive whitening and an interpolated i-vector dialect model, which resulted in an accuracy improvement of over 20% over the baseline. While the Siamese i-vector network only helped with the training data, it shows no advantage over the baseline i-vector for this state. We suspect that this result is due to the composition of the data used for the training of the Siamese network."}, {"heading": "4.3. Performance Evaluation of Submission", "text": "Tables 6 and 7 show detailed performance assessments of our three submitted systems. System 1 was trained using only the training data shown in Table 6. Systems 2 and 3 were trained using both training and development kits, as shown in Table 7. We found that the best linear fusion weight based on System 1 was 0.7, 0.2 and 0.1 for ivector, character and phonetic values to prevent overmatch. We applied the same weights to Systems 2 and 3 for fusion. Table 6 shows that the Siamese network demonstrated its effectiveness both in development and in test kits, without using any information from the test domain. The interpolated i-vector dialect model also shows that it well reflects the information from the test domains as shown in Table 2 and 3. Although we expected that the linguistic features would not be affected by the mismatch of development and phonemon features, systems are - all features and useful for contributions."}, {"heading": "5. CONCLUSION", "text": "In this paper, we describe the MIT-QCRI ADI system using both audio and linguistic characteristics for the MGB-3 challenge. We examined several approaches to address dialect variability and domain mismatches between training and test sets. Without knowing the test area in which the system is to be applied, reducing i-vector dimensionality using a Siamese network proved useful, while an interpolated i-vector dialect model showed effectiveness with relatively small amounts of test domain information from development data. Under both conditions, the merging of audio and language characteristics guarantees significant improvements in dialect identification. As these approaches are not limited to dialect identification, we plan to investigate their usefulness for other speaker and speech recognition problems in the future."}, {"heading": "6. REFERENCES", "text": "[1] Najim Dehak, Patrick J Kenny, Reda Dehak, Pierre Dumouchel, and Pierre Ouellet, \"Front-End Factor Analysis for Speaker Verification,\" IEEE Trans. on Audio, Speech, and Lang. Process., vol. 19, no. 4, pp. 788-798, may 2011. [2] Fred Richardson, Douglas Reynolds, and Najim Dehak, \"A Unified Deep Neural Network for Speaker and Language Recognition,\" in Interspeech, 2015, pp. 1146- 1150,. [3] Yun Lei, Nicolas Scheffer, Luciana Ferrer, and Mitchell McLaren, \"A Novel Scheme for Speaker Recognition using a Phonetically-aware Deep Neural Network,\" in IEEE ICASSP, 2014, pp. [4] David Snyder, Daniel Garcia-Romero, and Daniel Povey. \""}], "references": [{"title": "Front-End Factor Analysis for Speaker Verification", "author": ["Najim Dehak", "Patrick J Kenny", "Reda Dehak", "Pierre Dumouchel", "Pierre Ouellet"], "venue": "IEEE Trans. on Audio, Speech, and Lang. Process., vol. 19, no. 4, pp. 788\u2013798, may 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A Unified Deep Neural Network for Speaker and Language Recognition", "author": ["Fred Richardson", "Douglas Reynolds", "Najim Dehak"], "venue": "Interspeech, 2015, pp. 1146\u2013 1150.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A Novel Scheme for Speaker Recognition using a Phonetically-aware Deep Neural Network", "author": ["Yun Lei", "Nicolas Scheffer", "Luciana Ferrer", "Mitchell McLaren"], "venue": "IEEE ICASSP, 2014, pp. 1714\u20131718.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Time delay deep neural network-based universal background models for speaker recognition", "author": ["David Snyder", "Daniel Garcia-Romero", "Daniel Povey"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2016, pp. 92\u201397.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "QCRI advanced transcription system (QATS) for the Arabic Multi-Dialect Broadcast media recognition: MGB-2 challenge", "author": ["Sameer Khurana", "Ahmed Ali"], "venue": "IEEE Workshop on Spoken Language Technology(SLT), 2016, pp. 292\u2013298.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised Clustering Approaches for Domain Adaptation in Speaker Recognition Systems", "author": ["Stephen Shum", "Douglas a. Reynolds", "Daniel Garcia- Romero", "Alan McCree"], "venue": "Proceedings of Odyssey - The Speaker and Language Recognition Workshop, 2014, pp. 265\u2013272.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Dataset-invariant covari-  ance normalization for out-domain PLDA speaker verification", "author": ["Md Hafizur Rahman", "Ahilan Kanagasundaram", "David Dean", "Sridha Sridharan"], "venue": "Interspeech, 2015, pp. 1017\u20131021.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain Mismatch Compensation for Speaker Recognition Using a Library of Whiteners", "author": ["Elliot Singer", "Douglas A. Reynolds"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 11, pp. 2000\u20132003, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Improving Speaker Recognition Performance in the Domain Adaptation Challenge Using Deep Neural Networks", "author": ["Daniel Garcia-Romero", "Xiaohui Zhang", "Alan McCree", "Daniel Povey"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2014, pp. 378\u2013383.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised domain adaptation for I-vector based speaker recognition", "author": ["Daniel Garcia-Romero", "Alan McCree"], "venue": "IEEE ICASSP, 2014, pp. 4047\u20134051.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised Domain Adaptation for I-Vector Speaker Recognition", "author": ["Daniel Garcia-Romero", "Alan McCree", "Stephen Shum", "Niko Brummer", "Carlos Vaquero"], "venue": "Proceedings of Odyssey - The Speaker and Language Recognition Workshop, 2014, pp. 260\u2013264.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation via within-class covariance correction in i-vector based speaker recognition systems", "author": ["Ondrej Glembek", "Jeff Ma", "Pavel Matejka", "Bing Zhang", "Oldrich Plchot", "Lukas Burget", "Spyros Matsoukas"], "venue": "IEEE ICASSP, 2014, pp. 4060\u20134064.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Inter dataset variability compensation for speaker recognition", "author": ["Hagai Aronowitz"], "venue": "IEEE ICASSP, 2014, pp. 4002\u20134006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Autoencoder based Domain Adaptation for Speaker Recognition under Insufficient Channel Information", "author": ["Suwon Shon", "Seongkyu Mun", "Wooil Kim", "Hanseok Ko"], "venue": "Interspeech, 2017, pp. 1014\u20131018.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition", "author": ["Suwon Shon", "Seongkyu Mun", "Hanseok Ko"], "venue": "Interspeech, 2017, pp. 2869\u20132873.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition for NIST 2016 Speaker Recognition Evaluation", "author": ["Suwon Shon", "Hanseok Ko"], "venue": "ArXiv e-prints arXiv:1702.00956, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Improving out-domain PLDA speaker verification using unsupervised inter-dataset variability compensation approach", "author": ["Ahilan Kanagasundaram", "David Dean", "Sridha Sridharan"], "venue": "IEEE ICASSP, 2015, pp. 4654\u2013 4658.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Compensating Inter-Dataset Variability in PLDA Hyper-Parameters for Robust Speaker Recognition", "author": ["Hagai Aronowitz"], "venue": "Proceedings of Odyssey - The Speaker  and Language Recognition Workshop, 2014, pp. 280\u2013 286.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic dialect detection in Arabic broadcast speech", "author": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass", "Peter Bell", "Steve Renals"], "venue": "Interspeech, 2016, vol. 08-12- Sept, pp. 2934\u20132938.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Identifying dialects with textual and acoustic cues", "author": ["Abualsoud Hanani", "Aziz Qaroush", "West Bank"], "venue": "VarDial, 2017, pp. 93\u2013101.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning to Identify Arabic and German Dialects using Multiple Kernels", "author": ["Radu Tudor Ionescu", "Andrei M Butnaru"], "venue": "VarDial, 2017, pp. 200\u2013209.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "T ubingen system in VarDial 2017 shared task : experiments with language identification and cross-lingual parsing", "author": ["Taraka Rama"], "venue": "VarDial, 2017, pp. 146\u2013 155.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Arabic Dialect Identification Using iVectors and ASR Transcripts", "author": ["Shervin Malmasi", "Marcos Zampieri"], "venue": "VarDial, 2017, number 2015, pp. 178\u2013183.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech Recognition Challenge in the Wild: ARABIC MGB-3 ( DRAFT )", "author": ["Ahmed Ali", "Stephan Vogel", "Steve Renals"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2017, p. to be appeared.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Signature Verification Using a Siamese Time Delay Neural Network", "author": ["Jane Bromley", "James W. Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann Lecun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 07, no. 04, pp. 669\u2013688, 1993.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Unsupervised Speaker Adaptation based on the Cosine Similarity for Text-Independent Speaker Verification", "author": ["Stephen Shum", "Najim Dehak", "Reda Dehak", "James R Glass"], "venue": "Proceedings of Odyssey - The Speaker and Language Recognition Workshop, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of i-vector Length Normalization in Speaker Recognition Systems", "author": ["Daniel Garcia-Romero", "Carol Y Espy-Wilson"], "venue": "Interspeech, 2011, pp. 249\u2013 252.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical structures of neural networks for phoneme recognition", "author": ["Petr Schwarz", "Pavel Matejka", "Jan Cernocky"], "venue": "IEEE ICASSP. IEEE, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "The mgb-2 challenge: Arabic multi-dialect broadcast media recognition", "author": ["Ahmed Ali", "Peter Bell", "James Glass", "Yacine Messaoui", "Hamdy Mubarak", "Steve Renals", "Yifan Zhang"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF", "author": ["Niko Br\u00fcmmer", "Edward de Villiers"], "venue": "NIST SRE\u201911 Analysis Workshop, apr 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The dominant approach, based on i-vector extraction, has proven to be very effective for both language and speaker recognition [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "Recently, phonetically aware deep neural models have also been found to be effective in combination with i-vectors [2, 3, 4].", "startOffset": 115, "endOffset": 124}, {"referenceID": 2, "context": "Recently, phonetically aware deep neural models have also been found to be effective in combination with i-vectors [2, 3, 4].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "Recently, phonetically aware deep neural models have also been found to be effective in combination with i-vectors [2, 3, 4].", "startOffset": 115, "endOffset": 124}, {"referenceID": 4, "context": "Participants were provided high-quality Aljazeera news broadcasts as well as transcriptions generated by a multi-dialect ASR system created from the MGB-2 dataset [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 6, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 7, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 8, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 9, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 10, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 11, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 12, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 13, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 14, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 15, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 16, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 17, "context": "In order to address performance degradation of speaker and language recognition systems due to domain mismatches, researchers have proposed various approaches to compensate for, and to adapt to the mismatch [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 207, "endOffset": 255}, {"referenceID": 18, "context": "The linguistic feature space is, naturally, completely different to the audio feature space, so a fusion of the results from both feature representations has been previously shown to be beneficial [19, 20, 21, 22, 23].", "startOffset": 197, "endOffset": 217}, {"referenceID": 19, "context": "The linguistic feature space is, naturally, completely different to the audio feature space, so a fusion of the results from both feature representations has been previously shown to be beneficial [19, 20, 21, 22, 23].", "startOffset": 197, "endOffset": 217}, {"referenceID": 20, "context": "The linguistic feature space is, naturally, completely different to the audio feature space, so a fusion of the results from both feature representations has been previously shown to be beneficial [19, 20, 21, 22, 23].", "startOffset": 197, "endOffset": 217}, {"referenceID": 21, "context": "The linguistic feature space is, naturally, completely different to the audio feature space, so a fusion of the results from both feature representations has been previously shown to be beneficial [19, 20, 21, 22, 23].", "startOffset": 197, "endOffset": 217}, {"referenceID": 22, "context": "The linguistic feature space is, naturally, completely different to the audio feature space, so a fusion of the results from both feature representations has been previously shown to be beneficial [19, 20, 21, 22, 23].", "startOffset": 197, "endOffset": 217}, {"referenceID": 23, "context": "Detailed statistics of the ADI dataset can be found in [24].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "The features consisted of 400 dimensional i-vector features for each audio file (based on bottleneck feature inputs for their frame-level acoustic representation), as well as lexical features using bigrams generated from transcriptions [24].", "startOffset": 236, "endOffset": 240}, {"referenceID": 24, "context": "To further distinguish speech from different Arabic dialects, while making speech from the same dialect more similar, we adopted a Siamese neural network architecture [25] based on an i-vector feature space.", "startOffset": 167, "endOffset": 171}, {"referenceID": 25, "context": "Under CDS, ZT-norm or S-norm can be also applied for score normalization [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "For i-vector-based speaker and language recognition approaches, a whitening transformation and length normalization is considered essential [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "Since length normalization is inherently a nonlinear, non-whitening operation, recently, a recursive whitening transformation has been proposed to reduce residual un-whitened components in the i-vector space, as illustrated in Figure 3 [15].", "startOffset": 236, "endOffset": 240}, {"referenceID": 27, "context": "Phoneme feature extraction consists of extracting the phone sequence, and phone duration statistics using four different speech recognizers: Czech, Hungarian, and Russian using narrowband model, and English using a broadband model [28].", "startOffset": 231, "endOffset": 235}, {"referenceID": 28, "context": "Word sequences are extracted using a state-of-the-art Arabic speech-to-text transcription system built as part of the MGB-2 [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "For more details see the system description paper [5].", "startOffset": 50, "endOffset": 53}, {"referenceID": 29, "context": "A linear calibration is done by the Bosaris toolkit [30].", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "Please refer to [24] for descriptions of i-vector extraction and Arabic speech-to-text configuration.", "startOffset": 16, "endOffset": 20}], "year": 2017, "abstractText": "In order to successfully annotate the Arabic speech content found in open-domain media broadcasts, it is essential to be able to process a diverse set of Arabic dialects. For the 2017 Multi-Genre Broadcast challenge (MGB-3) there were two possible tasks: Arabic speech recognition, and Arabic Dialect Identification (ADI). In this paper, we describe our efforts to create an ADI system for the MGB-3 challenge, with the goal of distinguishing amongst four major Arabic dialects, as well as Modern Standard Arabic. Our research focused on dialect variability and domain mismatches between the training and test domain. In order to achieve a robust ADI system, we explored both Siamese neural network models to learn similarity and dissimilarities among Arabic dialects, as well as i-vector post-processing to adapt domain mismatches. Both Acoustic and linguistic features were used for the final MGB-3 submissions, with the best primary system achieving 75% accuracy on the official 10hr test set.", "creator": "LaTeX with hyperref package"}}}