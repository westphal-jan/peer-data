{"id": "1709.00513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Learning Loss for Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student.", "histories": [["v1", "Sat, 2 Sep 2017 01:03:08 GMT  (356kb,D)", "http://arxiv.org/abs/1709.00513v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["zheng xu", "yen-chang hsu", "jiawei huang"], "accepted": false, "id": "1709.00513"}, "pdf": {"name": "1709.00513.pdf", "metadata": {"source": "META", "title": "Learning Loss for Knowledge Distillation with Conditional Adversarial Networks", "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are in a position, in which they are able to integrate themselves, and in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "2 Related work", "text": "The techniques can be roughly divided into three categories: low accuracy, truncation and factorization, and knowledge distillation; low precision methods use a limited number of bits to store and operate the network weights; the extreme case is also binary networks that use only 1-bit to represent each number (Rastegari et al. 2016); acceleration of these methods is sometimes conceptual, because the low precision support of the common GPU is still limited; networks can also be modified directly by pruning and factorizing the redundant weights, either as post-processing after training or as stage fine-tuning (Li et al. 2017b; Howard et al. 2017); these methods often take network weights, which are sparse and / or low, and target efficient networks of similar architecture with a reduced number of weights. Knowledge distillation is a principal approach to the formation of small neural weights."}, {"heading": "3 Learning loss for knowledge distillation", "text": "In this section, we present the approach of learning loss based on conditional hostile networks. We start with a summary of modern network architectures (Section 3.1) and then describe the dark knowledge that can be transferred from the teacher to the student network (Section 3.2). The GAN-based approach to learning loss is in Section 3.3.3.1 Neural networks with residual connectionThe modern neural networks are built by stacking basic components. For computer vision tasks, residual blocks (Heet al. 2016; Zagoruyko and Komodakis 2016) are based on the basic components for building deep neural networks in order to reach the state of the art. Both students and teachers in this essay are based on the residual conversion blocks shown in Figure 1 (left). The first layer consists of 16 filters with 3 x 3 conversion, followed by a stack of 6n layers, where n is the number of residual blocks, and each block contains two net conversion layers, each of which is double with the number of iva remaining."}, {"heading": "3.2 Knowledge distillation", "text": "The probability is generated by the Softmax layer of logits, which represents the output of the last fully connected layer. The dimension of the logits from student and teacher networks corresponds to both the number of categories. Rich information is embedded in the output probability of a teacher network (LS), and we can use logits to transfer knowledge to the student network (Bucilu, Caruana and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals and Dean 2015). We check the method in (Hinton, Vinyals and Dean 2015), which provides a metric between student and teacher logs that generalizes earlier methods of knowledge distillation. The Logit vector generated by pre-trained teacher networks is a logical vector for an input image xi, i = 1,..., N is represented by ti, where the dimension of vector = categories (soi, i-categories, i-categories) is generated."}, {"heading": "3.3 Learning loss with adversarial networks", "text": "The main idea of learning for the transfer of knowledge from teacher to student is presented in Figure 2. Instead of forcing the student to accurately mimic the teacher by minimizing KL divergence in L1 (F, T) of equation (4), the knowledge is transferred from teacher to student, so that the discrimination in our GAN-based approach cannot be distinguished. There are several advantages of the proposed method. First, the learned loss can be effective, as has already been demonstrated for several image translation tasks. (Isola et al. 2017) Furthermore, the GAN-based approach alleviates the pain for the hand technique."}, {"heading": "4 Experiments", "text": "In this section we present the experimental results, in Section 4.1 the implementation details and the experimental settings. In Section 4.2 we present the advantages of the proposed method for training a small student network with the help of a large teacher network. In Section 4.3 we then analyze the various components of the proposed methods. In Section 4.4 we present the effects of depth and breadth of the student network, followed by the discussion of the trade-off between classification accuracy and inference acceleration in Section 4.5. Finally, in Section 4.6 we show the qualitative visualization of output distribution for students, teachers and knowledge distillation."}, {"heading": "4.1 Experimental setting", "text": "We look at three benchmark image classification datasets: ImageNet32 (Chrabaszcz, Loshchilov, and Hutter 2017), CIFAR-10, and CIFAR-100 (Krizhevsky and Hinton 2009). ImageNet32 is a downsampled version of the ImageNet2012 Challenge dataset (Russakovsky et al. 2015), which contains 1.28M training images and 50K validation images for 1K classes; all images are sampled to 32 \u00d7 32. CIFAR datasets contain 50K training images and 10K validation images for 10 and 100 classes, respectively. The images are also 32 \u00d7 32. In all experiments, light data augmentation with horizontal flip, adding, and cropping is used for input images as in (He et al. 2016).We use wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) as students and teachers."}, {"heading": "4.2 Benefits of learning loss", "text": "First, we show that the proposed method is good for knowledge transfer from teacher to student. Table 2 represents the error rate of the classification based on the three benchmark datasets: the teacher is the deep and broad WRN-40-10. The student is much flatter and thinner, WRN-10-4 for CIFARs and WRN-22-4 for ImageNet32. We choose a larger student network for ImageNet32 because the dataset contains more training samples and categories. More discussion of the wise choice of student architecture can be found in sections 4.4 and 4.5. The first two rows of Table 2 show the performance of pure supervised learning for student and teacher networks, without knowledge transfer through student-teacher strategy. We compare the GAN-based approach with the knowledge distillation proposed and reviewed in sections 4.4 and 4.5 (Hinton, Vinyals and Dean 2015)."}, {"heading": "4.3 Analysis of the proposed method", "text": "Figure 3 shows the training curve of the small student network WRN-10-4 on the CIFAR-100 dataset. Loss of the discriminator (blue solid line) is gradually decreasing, indicating that the opposing training is progressing steadily; the error rate of the GAN-based method for both training and test data is decreasing; the error rate in testing the GAN-based method is consistently better than the purely supervised training of the student model and looks more stable between epochs 50-100. Surprisingly, the error rate in pure supervised learning is slightly better than in the GAN-based method, which suggests that the knowledge transfer is more advantageous for generalization.Next, we look at various components of the GAN-based approach, as shown in Table 3. By optimizing the Adversarial loss and categorized knowledge transfer LLP, the learned loss leads LGAN-based reasonably well."}, {"heading": "4.4 Do WRN need to be deep and wide?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10-2 0.32 0.14 33.22 32.74 32.1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10-4 1.22 0.32 28.52 27.16 25.75", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10-6 2.72 0.60 27.27 25.39 24.39", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10-8 4.81 0.82 26.23 24.31 23.38", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10-10 7.49 1.17 26.04 23.49 23.02", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "16-4 2.77 0.71 24.73 22.9 22.73", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "22-4 4.32 1.07 23.61 22.02 21.66", "text": "(Urban et al. 2017) asked similar questions for Convolutionary Neural Networks, claiming that the network should have at least a few layers of convolutions. We examine the modern architecture of WRN of residual blocks. Our empirical study suggests that even for the modern architecture of WRN the network must be deep and wide to some degree. Table 5 presents the results of pure supervised learning, knowledge distillation (Hinton, Vinyals and Dean 2015) and the GAN-based approach for various student networks on CIFAR-100. We first set the depth of the WRN as 10 and change the extended factor from 2 to 10. 10 is the minimum depth for our WRN architecture, as the depth must be 6n + 4. We then set the width as 4 and increase the depth from 10 to 34. The parameter size is in millions, and the inference time is in seconds per minibatch of 100 samples on CPU. If the student is very small, as WRN-104-2, the depth can be increased from 34 to 34."}, {"heading": "4.5 Training student for acceleration", "text": "We present the trade-off between error rate, inference time, and parameter size in Figure 4. The figure is derived from Table 5 by changing the architecture of the student network. A larger student network is more precise, but also slower. On networks similar in size to WRN-10 and WRN-34-4, a deep network achieves a lower error rate, while a broad network runs a little faster. Student teacher strategy can help improve the classification performance of the student network. If the student network is relatively large, like WRN-34-4, the student network trained using the GAN-based approach can have a competitive error rate compared to the teacher WRN-40-10. WRN-34-4 is 7x smaller and 5x faster than WRN-40-10, and the GANbased approach reduces the absolute error rate by 2.5%. (Student) (our) (Teacher)"}, {"heading": "4.6 Visualization of distribution", "text": "In the last section of the experimental results, we present qualitative visualization for the GAN-based approach. Figure 5 shows the scaled histogram for predicting category 85 in CIFAR-100. The histogram is counted on the 10K test samples, in which 100 samples come from category 85 and are marked as positive (green in the figure) and the other 9.9K as negative (blue in the figure).The histogram is scaled to one for positive or negative. The three diagrams represent the distribution predicted by the student network trained by pure learning, the teacher network and the student network trained by the GAN-based approach. The histogram in the center resembles the histogram on the right, which suggests that the GAN-based approach transfers knowledge from the teacher to the student."}, {"heading": "5 Conclusion and discussion", "text": "We demonstrate empirically that the GAN-based approach can improve the formation of student networks, especially when the student network is flat and thin. In addition, we empirically study the impact of capacity on a modern network as a student and provide guidelines for the smart selection of a student to compensate for error rate and acceleration. In a given environment, we can train a student who is 7x smaller and 5x faster than a teacher without loss of accuracy. The GAN-based approach is stable and easy to implement after applying several advanced techniques in the GAN literature. Current implementation uses the stored logos from teacher networks to save GPU memory and computing power. Generating teacher logs in a blink with dropouts can be more reliable for opposing training. Finally, the GAN-based approach can be naturally extended to the network as a teacher."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "ICML. Arpit, D.; Jastrzebski, S.; Ballas, N.; Krueger, D.; Bengio, E.; Kanwal, M. S.; Maharaj, T.; Fischer, A.; Courville, A.; Bengio, Y.; et al. 2017. A closer look at memorization in deep networks. ICML.", "citeRegEx": "Arjovsky et al\\.,? 2017", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "KDD, 535\u2013541. ACM.", "citeRegEx": "Bucilu et al\\.,? 2006", "shortCiteRegEx": "Bucilu et al\\.", "year": 2006}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "NIPS, 2172\u20132180.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Darkrank: Accelerating deep metric learning via cross sample similarities transfer", "author": ["Y. Chen", "N. Wang", "Z. Zhang"], "venue": "arXiv preprint arXiv:1707.01220.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "A downsampled variant of imagenet as an alternative to the cifar datasets", "author": ["P. Chrabaszcz", "I. Loshchilov", "F. Hutter"], "venue": "arXiv preprint arXiv:1707.08819.", "citeRegEx": "Chrabaszcz et al\\.,? 2017", "shortCiteRegEx": "Chrabaszcz et al\\.", "year": 2017}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "MCSS 2(4):303\u2013314.", "citeRegEx": "Cybenko,? 1989", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["R. Eldan", "O. Shamir"], "venue": "COLT, 907\u2013940.", "citeRegEx": "Eldan and Shamir,? 2016", "shortCiteRegEx": "Eldan and Shamir", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks 2(5):359\u2013366.", "citeRegEx": "Hornik et al\\.,? 1989", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "author": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"], "venue": "arXiv preprint arXiv:1704.04861.", "citeRegEx": "Howard et al\\.,? 2017", "shortCiteRegEx": "Howard et al\\.", "year": 2017}, {"title": "Like what you like: Knowledge distill via neuron selectivity transfer", "author": ["Z. Huang", "N. Wang"], "venue": "arXiv preprint arXiv:1707.01219.", "citeRegEx": "Huang and Wang,? 2017", "shortCiteRegEx": "Huang and Wang", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML, 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Image-toimage translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CVPR.", "citeRegEx": "Isola et al\\.,? 2017", "shortCiteRegEx": "Isola et al\\.", "year": 2017}, {"title": "Transferring knowledge to smaller network with class-distance loss", "author": ["S.W. Kim", "Kim", "H.-E."], "venue": "ICLR Workshop.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Training quantized nets: A deeper understanding", "author": ["H. Li", "S. De", "Z. Xu", "C. Studer", "H. Samet", "T. Goldstein"], "venue": "arXiv preprint arXiv:1706.02379.", "citeRegEx": "Li et al\\.,? 2017a", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Pruning filters for efficient convnets", "author": ["H. Li", "A. Kadav", "I. Durdanovic", "H. Samet", "H.P. Graf"], "venue": "ICLR.", "citeRegEx": "Li et al\\.,? 2017b", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Why deep neural networks for function approximation? ICLR", "author": ["S. Liang", "R. Srikant"], "venue": null, "citeRegEx": "Liang and Srikant,? \\Q2017\\E", "shortCiteRegEx": "Liang and Srikant", "year": 2017}, {"title": "Face model compression by distilling knowledge from neurons", "author": ["P. Luo", "Z. Zhu", "Z. Liu", "X. Wang", "X Tang"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y. Lau", "Z. Wang", "S.P. Smolley"], "venue": "arXiv preprint ArXiv:1611.04076.", "citeRegEx": "Mao et al\\.,? 2016", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784.", "citeRegEx": "Mirza and Osindero,? 2014", "shortCiteRegEx": "Mirza and Osindero", "year": 2014}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": "ICML.", "citeRegEx": "Odena et al\\.,? 2017", "shortCiteRegEx": "Odena et al\\.", "year": 2017}, {"title": "XNOR-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ECCV, 525\u2013542. Springer.", "citeRegEx": "Rastegari et al\\.,? 2016", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "ICML.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "ICLR.", "citeRegEx": "Romero et al\\.,? 2015", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Training constrained deconvolutional networks for road scene semantic segmentation", "author": ["G. Ros", "S. Stent", "P.F. Alcantarilla", "T. Watanabe"], "venue": "arXiv preprint arXiv:1604.01545.", "citeRegEx": "Ros et al\\.,? 2016", "shortCiteRegEx": "Ros et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Depth-width tradeoffs in approximating natural functions with neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "ICML, 2979\u2013 2987.", "citeRegEx": "Safran and Shamir,? 2017", "shortCiteRegEx": "Safran and Shamir", "year": 2017}, {"title": "Deep model compression: Distilling knowledge from noisy teachers", "author": ["B.B. Sau", "V.N. Balasubramanian"], "venue": "arXiv preprint arXiv:1610.09650.", "citeRegEx": "Sau and Balasubramanian,? 2016", "shortCiteRegEx": "Sau and Balasubramanian", "year": 2016}, {"title": "In teacher we trust: Learning compressed models for pedestrian detection", "author": ["J. Shen", "N. Vesdapunt", "V.N. Boddeti", "K.M. Kitani"], "venue": "arXiv preprint arXiv:1612.00478.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman,? 2014", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Distral: Robust multitask reinforcement learning", "author": ["Y.W. Teh", "V. Bapst", "W.M. Czarnecki", "J. Quan", "J. Kirkpatrick", "R. Hadsell", "N. Heess", "R. Pascanu"], "venue": "arXiv preprint arXiv:1707.04175.", "citeRegEx": "Teh et al\\.,? 2017", "shortCiteRegEx": "Teh et al\\.", "year": 2017}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485.", "citeRegEx": "Telgarsky,? 2016", "shortCiteRegEx": "Telgarsky", "year": 2016}, {"title": "Do deep convolutional nets really need to be deep and convolutional? ICLR", "author": ["G. Urban", "K.J. Geras", "S.E. Kahou", "O. Aslan", "S. Wang", "R. Caruana", "A. Mohamed", "M. Philipose", "M. Richardson"], "venue": null, "citeRegEx": "Urban et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2017}, {"title": "Deeply-fused nets", "author": ["J. Wang", "Z. Wei", "T. Zhang", "W. Zeng"], "venue": "arXiv preprint arXiv:1605.07716.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["S. Xie", "R. Girshick", "P. Doll\u00e1r", "Z. Tu", "K. He"], "venue": "CVPR.", "citeRegEx": "Xie et al\\.,? 2017", "shortCiteRegEx": "Xie et al\\.", "year": 2017}, {"title": "Stabilizing adversarial nets with prediction methods", "author": ["A. Yadav", "S. Shah", "Z. Xu", "D. Jacobs", "T. Goldstein"], "venue": "arXiv preprint arXiv:1705.07364.", "citeRegEx": "Yadav et al\\.,? 2017", "shortCiteRegEx": "Yadav et al\\.", "year": 2017}, {"title": "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning", "author": ["J. Yim", "D. Joo", "J. Bae", "J. Kim"], "venue": "CVPR.", "citeRegEx": "Yim et al\\.,? 2017", "shortCiteRegEx": "Yim et al\\.", "year": 2017}, {"title": "Learning from multiple teacher networks", "author": ["S. You", "C. Xu", "C. Xu", "D. Tao"], "venue": "KDD, 1285\u20131294. ACM.", "citeRegEx": "You et al\\.,? 2017", "shortCiteRegEx": "You et al\\.", "year": 2017}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146.", "citeRegEx": "Zagoruyko and Komodakis,? 2016", "shortCiteRegEx": "Zagoruyko and Komodakis", "year": 2016}, {"title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "ICLR.", "citeRegEx": "Zagoruyko and Komodakis,? 2017", "shortCiteRegEx": "Zagoruyko and Komodakis", "year": 2017}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "ICLR.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Rocket launching: A universal and efficient framework for training well-performing light net", "author": ["G. Zhou", "Y. Fan", "R. Cui", "W. Bian", "X. Zhu", "K. Gai"], "venue": "arXiv preprint arXiv:1708.04106.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "For one of the core applications in computer vision, large-scale image classification (Russakovsky et al. 2015), the accuracy reached by DNNs has become comparable to humans on several benchmark datasets.", "startOffset": 86, "endOffset": 111}, {"referenceID": 9, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 44, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 40, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 6, "context": "Do DNNs really need to be deep and wide? Early theoretical studies (Cybenko 1989; Hornik, Stinchcombe, and White 1989) suggest that shallow networks can approximate arbitrary functions.", "startOffset": 67, "endOffset": 118}, {"referenceID": 7, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 37, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 21, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 31, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 46, "context": "Moreover, the overparameterized and redundant networks, which can easily memorize and overfit the training data, surprisingly generalize well in practice (Zhang et al. 2017; Arpit et al. 2017).", "startOffset": 154, "endOffset": 192}, {"referenceID": 1, "context": "On the other hand, empirical studies suggest that the performance of shallow networks can be improved by learning from large networks following the student-teacher strategy (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 173, "endOffset": 288}, {"referenceID": 38, "context": "On the other hand, empirical studies suggest that the performance of shallow networks can be improved by learning from large networks following the student-teacher strategy (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 173, "endOffset": 288}, {"referenceID": 1, "context": "In the previous works, (Ba and Caruana 2014; Urban et al. 2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al.", "startOffset": 23, "endOffset": 63}, {"referenceID": 38, "context": "In the previous works, (Ba and Caruana 2014; Urban et al. 2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al.", "startOffset": 23, "endOffset": 63}, {"referenceID": 28, "context": "2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al. 2015) train a small deep and thin network to replace a shallow and wide network for acceleration, given the best teacher at that time is the shallow and wide VGGNet (Simonyan and Zisserman 2014).", "startOffset": 266, "endOffset": 286}, {"referenceID": 34, "context": "2015) train a small deep and thin network to replace a shallow and wide network for acceleration, given the best teacher at that time is the shallow and wide VGGNet (Simonyan and Zisserman 2014).", "startOffset": 165, "endOffset": 194}, {"referenceID": 9, "context": "ResNet (He et al. 2016) has significantly deepened the networks by introducing residual connections, and wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) suggest to widen the networks for better performance.", "startOffset": 7, "endOffset": 23}, {"referenceID": 44, "context": "2016) has significantly deepened the networks by introducing residual connections, and wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) suggest to widen the networks for better performance.", "startOffset": 117, "endOffset": 147}, {"referenceID": 15, "context": "Our learning loss approach is inspired by the recent success of conditional adversarial networks for various imageto-image translation applications (Isola et al. 2017).", "startOffset": 148, "endOffset": 167}, {"referenceID": 26, "context": "Low precision methods use limited number of bits to store and operate the network weights, and the extreme case is binary networks that only use 1-bit to represent each number (Rastegari et al. 2016).", "startOffset": 176, "endOffset": 199}, {"referenceID": 20, "context": "Networks can also be directly modified by pruning and factorizing the redundant weights, either as a post-processing after training, or as a fine-tuning stage (Li et al. 2017b; Howard et al. 2017).", "startOffset": 159, "endOffset": 196}, {"referenceID": 12, "context": "Networks can also be directly modified by pruning and factorizing the redundant weights, either as a post-processing after training, or as a fine-tuning stage (Li et al. 2017b; Howard et al. 2017).", "startOffset": 159, "endOffset": 196}, {"referenceID": 1, "context": "(Ba and Caruana 2014; Urban et al. 2017) trained shallow but wide student by learning from a deep teacher, which were not primarily designed for acceleration.", "startOffset": 0, "endOffset": 40}, {"referenceID": 38, "context": "(Ba and Caruana 2014; Urban et al. 2017) trained shallow but wide student by learning from a deep teacher, which were not primarily designed for acceleration.", "startOffset": 0, "endOffset": 40}, {"referenceID": 29, "context": "The variants of knowledge distillation has also been applied to many different tasks, such as semantic segmentation (Ros et al. 2016), pedestrian detection (Shen et al.", "startOffset": 116, "endOffset": 133}, {"referenceID": 33, "context": "2016), pedestrian detection (Shen et al. 2016), face recognition (Luo et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 22, "context": "2016), face recognition (Luo et al. 2016), metric learning(Chen, Wang, and Zhang 2017), reinforcement learning (Teh et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 36, "context": "2016), metric learning(Chen, Wang, and Zhang 2017), reinforcement learning (Teh et al. 2017) and for regularization(Sau and Balasubramanian 2016).", "startOffset": 75, "endOffset": 92}, {"referenceID": 32, "context": "2017) and for regularization(Sau and Balasubramanian 2016).", "startOffset": 28, "endOffset": 58}, {"referenceID": 28, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 39, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 45, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 42, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 13, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 47, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 43, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 8, "context": "Generative adversarial networks (GAN) has been extensively studied over recent years since (Goodfellow et al. 2014).", "startOffset": 91, "endOffset": 115}, {"referenceID": 24, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 15, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 27, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 44, "context": "Figure 1: Blocks with residual connection for convolutional neural networks (Zagoruyko and Komodakis 2016) (left) and multi-layer perceptron (right).", "startOffset": 76, "endOffset": 106}, {"referenceID": 14, "context": "The first layer is 16 filters of 3\u00d7 3 convolution, followed by a stack of 6n layers, where n is the number of residual blocks and each block contains two convolution layers equipped with batch normalization (Ioffe and Szegedy 2015), ReLU (Krizhevsky, Sutskever, and Hinton 2012) and dropout (Srivastava et al.", "startOffset": 207, "endOffset": 231}, {"referenceID": 35, "context": "The first layer is 16 filters of 3\u00d7 3 convolution, followed by a stack of 6n layers, where n is the number of residual blocks and each block contains two convolution layers equipped with batch normalization (Ioffe and Szegedy 2015), ReLU (Krizhevsky, Sutskever, and Hinton 2012) and dropout (Srivastava et al. 2014).", "startOffset": 291, "endOffset": 315}, {"referenceID": 44, "context": "Table 1: The architecture of wide residual networks (Zagoruyko and Komodakis 2016).", "startOffset": 52, "endOffset": 82}, {"referenceID": 1, "context": "Rich information is embedded in the output probability of a teacher network, and we can use logits to transfer the knowledge to student network (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 144, "endOffset": 259}, {"referenceID": 38, "context": "Rich information is embedded in the output probability of a teacher network, and we can use logits to transfer the knowledge to student network (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 144, "endOffset": 259}, {"referenceID": 15, "context": "First, the learned loss can be effective, as has already been demonstrated for several image to image translation tasks (Isola et al. 2017).", "startOffset": 120, "endOffset": 139}, {"referenceID": 8, "context": "The adversarial loss LA for knowledge distillation, which follows the original GAN (Goodfellow et al. 2014), faces two major issues.", "startOffset": 83, "endOffset": 107}, {"referenceID": 23, "context": "Even if we replace the log-likelihood with advanced techniques such as Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017) and Least Squares GAN (Mao et al. 2016), the training is still slow and unstable in our experiments.", "startOffset": 147, "endOffset": 164}, {"referenceID": 3, "context": "To tackle these problems, we modify the discriminator objective to also predict the class labels, inspired by (Chen et al. 2016; Odena, Olah, and Shlens 2017).", "startOffset": 110, "endOffset": 158}, {"referenceID": 17, "context": "We consider three benchmark datasets for image classification: ImageNet32 (Chrabaszcz, Loshchilov, and Hutter 2017), CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton 2009).", "startOffset": 140, "endOffset": 168}, {"referenceID": 30, "context": "ImageNet32 is a downsampled version of the ImageNet2012 challenge dataset (Russakovsky et al. 2015), which contains 1.", "startOffset": 74, "endOffset": 99}, {"referenceID": 9, "context": "In all the experiments, light data augmentation with horizontal flip, padding and cropping is used for input images as in (He et al. 2016).", "startOffset": 122, "endOffset": 138}, {"referenceID": 44, "context": "We use wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) as both student and teacher networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 38, "context": "(Urban et al. 2017) asked similar question for convolutional neural networks and claimed the network should at least has a few layers of convolutions.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "There is an increasing interest on accelerating neural networks for real-time applications. We study the studentteacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student.", "creator": "TeX"}}}