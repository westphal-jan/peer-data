{"id": "1709.00541", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "abstract": "Words in some natural languages can have a composite structure. Elements of this structure include the root (that could also be composite), prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character $n$-gram regularities. The selection is made using the pattern-based Conditional Random Field model with $l_1$ regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet's symbols confine a local statistical context stronger than the characters, therefore they allow better representations in ${\\mathbb{R}}^n$ and are better building blocks for word representation. In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.", "histories": [["v1", "Sat, 2 Sep 2017 07:00:22 GMT  (151kb,D)", "http://arxiv.org/abs/1709.00541v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rustem takhanov", "zhenisbek assylbekov"], "accepted": false, "id": "1709.00541"}, "pdf": {"name": "1709.00541.pdf", "metadata": {"source": "CRF", "title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "authors": ["Rustem Takhanov"], "emails": ["zhassylbekov}@nu.edu.kz"], "sections": [{"heading": null, "text": "Keywords: subword-conscious language modeling, pattern-based conditional random field, word representation, deep learning"}, {"heading": "1 Introduction", "text": "The goal of natural language modeling is, in the face of a corpus of texts from a particular language, to achieve a probable distribution of words / sentences."}, {"heading": "2 A new alphabet for words", "text": "In all the work, we shall use the following notation: If X is an alphabet, then we shall designate a series of words about X; for \u03b1, \u03b2, X, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II,"}, {"heading": "3 Subword-aware neural language model", "text": "The overall architecture of the subword-conscious neural language model is shown in Figure 3.It consists of three main parts: (i) subword-based word embedding model, (ii) word-level recurring neuronal network language model (RNLM), and (iii) softmax layer. (iii) Each part is described in more detail, and each state is defined by the sequence of its subwords s1..... snw \"X\". \"D\".D. \".D.\".D. \".D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D \"D.\" D \"D.\" D \"D.\" D \"D.\" D. \"D\" D. \"D\" D. \"D\" D. \"D\" D. \"D.\" D \"D\" D. \"D.\" D \"D\" D. \"D.\" D \"D.\" D \"D\" D. \"D.\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D."}, {"heading": "4 Experimental Setup", "text": "The results of the PTB study show that the two models are two different types. As the PTB is criticized for being small today, we also provide an evaluation of the WikiText-2 dataset, which is about two times larger than the PTB in size and three times larger in vocabulary. We do not use additional symbols at the end of each line in WikiText-2, but remove spaces between the equals in the sequences \"=\" and \"that occur in the section titled Hyperparameters: The regulation parameter C of (1) is set to 1600, which results in 883 unique patterns (= 883 unique patterns). We have the results in 883 unique patterns (= 1483 unique patterns) for the PTB dataset (cf. 48 unique characters) and 1440 unique patterns (0 = 1440 unique patterns)."}, {"heading": "5 Results", "text": "The results of the evaluation on PTB and WikiText-2 are described accordingly in Tables 1 and 2. As can be seen, models that use patterns consistently outperform those that use characters on small parameter budgets. However, the performance difference is less pronounced if we allow more parameters. Also, it is clear that patterns for simple models like Concat and Sum are more advantageous, but have less impact on the CNN model, which shrinks the gap between characters and patterns. This is quite natural, since patterns carry some information about character n-grams and can therefore be considered \"discrete convolutions,\" which makes CNN less efficient with patterns than CNN with regular characters. However, we note that in all cases a simple sum of pattern embedding (pat-sum) is on par with a more complex conversion of character embedding (char-CNN). Faster2 training of the pat-sum makes the patterns even more advantageous compared to the char-CNN."}, {"heading": "6 Conclusion", "text": "Regular characters tend to be uninformative when their embedding is concatenated or summed up to create word vectors, but patterns, on the contrary, contain enough information to make these methods work much better. Embedding swings over subwords capture regularities of n-grammars and thus make the difference between characters and patterns less noticeable. It is noteworthy that a simple and quick sum of embedding patterns equals more sophisticated and slower embedding swings over characters."}, {"heading": "7 Acknowledgments", "text": "Thank you for supporting NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": "arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Enriching word vectors with subword information", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.04606", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Compositional morphology for word representations and language modelling", "author": ["J. Botha", "P. Blunsom"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRE Transactions on information theory 2(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1956}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Identity matters in deep learning", "author": ["M. Hardt", "T. Ma"], "venue": "arXiv preprint arXiv:1611.04231", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A character-word compositional neural language model for finnish", "author": ["M. Lankinen", "H. Heikinheimo", "P. Takala", "T. Raiko", "J. Karhunen"], "venue": "arXiv preprint arXiv:1612.03266", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "C. Dyer", "A.W. Black", "I. Trancoso", "R. Fermandez", "S. Amir", "L. Marujo", "T. Luis"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics 19(2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": "Proceedings of ICLR 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech. Volume 2.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "H.S. Le", "S. Kombrink", "J. Cernocky"], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon", "W. Weaver"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Letter n-gram-based input encoding for continuous space language models", "author": ["H. Sperr", "J. Niehues", "A. Waibel"], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Inference algorithms for pattern-based crfs on sequence data", "author": ["R. Takhanov", "V. Kolmogorov"], "venue": "ICML (3).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-word lstm language models", "author": ["L. Verwimp", "J. Pelemans", "P Wambacq"], "venue": "Proceedings of EACL 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE 78(10)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields with high-order features for sequence labeling", "author": ["N. Ye", "W.S. Lee", "H.L. Chieu", "D. Wu"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Kout\u0144\u0131k", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "Proceedings of ICLR 2017.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 22, "context": "The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization.", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization.", "startOffset": 76, "endOffset": 83}, {"referenceID": 15, "context": "Historically, first approaches to the problem [16,4] were highly interpretable, involving syntax and morphology, i.", "startOffset": 46, "endOffset": 52}, {"referenceID": 3, "context": "Historically, first approaches to the problem [16,4] were highly interpretable, involving syntax and morphology, i.", "startOffset": 46, "endOffset": 52}, {"referenceID": 14, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 10, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 8, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 9, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 19, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 16, "context": "In [17] a word is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding; the word embeddings are then fed into neural machine translation models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22] a very similar technique is used and an evaluation on three other tasks (word similarity, sentence similarity, and part-of-speech tagging) is performed; they demonstrate that their method outperforms more complex architectures based on character-level recurrent and convolutional neural networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Probably closest to ours is an approach from [2] where a word representation is a sum of terms, each term corresponding to a certain n-gram that occurs in that word.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "A pattern-based conditional random field model for our language is the following probability distribution over A\u2217 [23,19]:", "startOffset": 114, "endOffset": 121}, {"referenceID": 18, "context": "A pattern-based conditional random field model for our language is the following probability distribution over A\u2217 [23,19]:", "startOffset": 114, "endOffset": 121}, {"referenceID": 18, "context": "The latter function is convex, an efficient computation of its value and gradient is described in [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "This approach was used by [3] to combine a word and its morpheme embeddings into a single word vector.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "\u2013 CNN: A convolutional model of [9]:", "startOffset": 32, "endOffset": 35}, {"referenceID": 17, "context": "To model interactions between subwords, we feed the resulting word embedding w into a stack of two highway layers [18] with dimensionality dHW per layer.", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "The most advanced recurrent neural architectures, at the time of this writing, are RHN [25] and NAS [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "The most advanced recurrent neural architectures, at the time of this writing, are RHN [25] and NAS [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "However, to make our results directly comparable to the previous work of [9] on character-level language modeling we select a more conventional architecture \u2013 a stack of two LSTM cells [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "However, to make our results directly comparable to the previous work of [9] on character-level language modeling we select a more conventional architecture \u2013 a stack of two LSTM cells [8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 11, "context": "Data sets: All models are trained and evaluated on the English PTB data set [12] utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with pre-processing by [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Data sets: All models are trained and evaluated on the English PTB data set [12] utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with pre-processing by [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Since the PTB is criticized for being small nowadays, we also provide an evaluation on the WikiText-2 data set [13], which is approximately two times larger than PTB in size and three times larger in vocabulary.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "CNN: In character-based models we choose the same values for hyperparameters as in the work of [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 1, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 2, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 3, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 4, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 5, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 0, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 1, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 2, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 4, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 5, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 23, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 8, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 4, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 20, "context": "which is typically done by truncated BPTT [21,6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "which is typically done by truncated BPTT [21,6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 4, "context": "For regularization we use variational dropout [5] with dropout rates for small/medium Concat, Sum/medium CNN models as follows: 0.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Hence the highway in Pat-Sum does less nonlinear work than in Char-CNN: In Pat-Sum it is almost an identical transformation, and such a simple highway is well-trained according to [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 0, "context": "Source code: All models were implemented in TensorFlow [1] and the source code for Pat-Sum is available at https://github.", "startOffset": 55, "endOffset": 58}], "year": 2017, "abstractText": "Words in some natural languages can have a composite structure. Elements of this structure include the root (that could also be composite), prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character n-gram regularities. The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet\u2019s symbols confine a local statistical context stronger than the characters, therefore they allow better representations in R and are better building blocks for word representation. In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.", "creator": "LaTeX with hyperref package"}}}