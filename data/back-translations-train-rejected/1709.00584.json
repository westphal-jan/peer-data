{"id": "1709.00584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Deep Learning-Guided Image Reconstruction from Incomplete Data", "abstract": "An approach to incorporate deep learning within an iterative image reconstruction framework to reconstruct images from severely incomplete measurement data is presented. Specifically, we utilize a convolutional neural network (CNN) as a quasi-projection operator within a least squares minimization procedure. The CNN is trained to encode high level information about the class of images being imaged; this information is utilized to mitigate artifacts in intermediate images produced by use of an iterative method. The structure of the method was inspired by the proximal gradient descent method, where the proximal operator is replaced by a deep CNN and the gradient descent step is generalized by use of a linear reconstruction operator. It is demonstrated that this approach improves image quality for several cases of limited-view image reconstruction and that using a CNN in an iterative method increases performance compared to conventional image reconstruction approaches. We test our method on several limited-view image reconstruction problems. Qualitative and quantitative results demonstrate state-of-the-art performance.", "histories": [["v1", "Sat, 2 Sep 2017 14:15:24 GMT  (6144kb,D)", "http://arxiv.org/abs/1709.00584v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["brendan kelly", "thomas p matthews", "mark a anastasio"], "accepted": false, "id": "1709.00584"}, "pdf": {"name": "1709.00584.pdf", "metadata": {"source": "CRF", "title": "Deep Learning-Guided Image Reconstruction from Incomplete Data", "authors": ["Brendan Kelly", "Thomas P. Matthews"], "emails": ["bmkelly@wustl.edu", "thomas.matthews@wustl.edu", "anastasio@wustl.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "Related Work", "text": "Extensive work has been done on the application of DL methods for image restoration tasks, such as denocialization or in-painting methods, which are considered independently of each other. [16, 30, 19] These tasks share similarities with image reconstruction tasks, but differ in the information provided as input. In image restoration, a degraded image is provided as input, while in image reconstruction measurements are provided that correspond to some operators applied to the image. Typically, these measurements are structurally significantly different from the images themselves. Furthermore, many image reconstruction methods require an initial assessment of the true image, which itself can be a degraded image. Image reconstruction methods can be combined with image reconstruction methods as a post-processing step to try to correct all artifacts in the reconstructed image. As these restoration methods only accept an image as input, it is not possible to consider this data during reworking."}, {"heading": "2 Statement of image reconstruction problem", "text": "Consider a reverse problem in which one tries to create an image f-Rn from a collection of measurements g-Rm for m < n. While the proposed approach is more general in order to facilitate discussion and interpretation, we will focus on the case in which the measured data is related to the true image viag = Hf, (1) in which H-Rm \u00b7 n is a linear operator describing the measurement system. Given some H, each object f can be broken down into two orthogonal components, the measurable component fmeas-Umeas and the zero-space component fnull-Unull, in which the Unull component {f-Hf = 0} and Umeas is its orthogonal completion. A key challenge is to restore the component of f, which is typically located in Unull. From the definition of Unull, it follows that the measured data g does not contain information about zero, which is invisible to the system."}, {"heading": "3 Approach", "text": "The structure of the proposed approach is inspired by that of the proximal Q (or projected) Q-Production Descent. In this approach, an initial guess for the true object f (0) is iteratively realized by first taking a step along the gradient of some cost functions C (f) and then applying a proximal operator P that maps the current iteration. In theory, this approach can be very effective. For example, P could be defined as a projection operator for all artifact-free images. In practice, however, it is difficult to determine a method for calculating P except for relatively simple cases. This limits the amount and complexity of a priority information that can be translated into projection. P will go beyond this constraint, P will be replaced by a quasi-projector."}, {"heading": "Network Architecture for Establishing Q", "text": "The operator Q represents a mapping of Rn to Rn. Using a DL model to implement this mapping, there is great freedom in selecting the appropriate network architecture. We base our selection on two main reasons: the first is that recent work has shown that deeper networks are more efficient than flatter networks with the same number of parameters [11], [25]. Lower-lying networks consist of a larger chain of nonlinear layers that provide a higher degree of abstraction; the second is that learning a residual mapping is an attractive alternative when there is a high correlation between input and output. As input and output are quite similar to this problem, learning the residual layer is intuitively easier than learning the direct mapping. The network architecture chosen for implementing Q corresponds to a deep residual mapping of CNN, inspired by [16]. The network contains 20 contiguous layers, with one ReLU layer present after each contiguous layer with the final exception."}, {"heading": "Training Q", "text": "Conventional training: In view of a training set of images F and their corresponding measurement data {g = Hf | f \u00b2 F}, weights w are determined by minimizing (i), except for the i-th training sample and f \u2212 k) R, (i) denotes the estimate of the image according to line 4 of Alg. 1 for the k-th iteration and i-th training sample. This corresponds to the optimization of CNN weights using reconstructed images generated by R for a first guess of all zeros. Two-step training: It has been observed that a CNN optimized by the conventional training scheme described above may not be generalized well to f \u00b2 (k) R inputs. (i) Training related to the k > 1. Training was introduced to regulate weights, a two-step training scheme."}, {"heading": "4 Computer-simulation studies", "text": "To demonstrate our approach, a canonical image reconstruction problem in X-ray computed tomography (CT) was considered, in which very limited measurements are used."}, {"heading": "Sample generation", "text": "Inspired by the Shepp-Logan Phantom [14], the generated images consist of a main ellipse and between 2 and 7 other small ellipses, and the number and properties of the ellipses were randomly selected. Additional details about the process of sample generation can be found in the supplementary materials. This special sample form enabled the use of a closed form to calculate the measured data, providing an independent way to calculate the measured data that did not depend on the numerical model. 256 x 256, with 7500 training images, 1000 validation images and 500 test images, the validation images were used to optimize hyperparameters, and the test images were used only once to evaluate performance."}, {"heading": "X-ray CT forward model", "text": "An idealized 2-D X-ray CT forward model is considered. A series of parallel X-rays is transmitted through a 2-D object, with the intensity of the X-rays decreasing as they travel through the object. The resulting intensity of the X-rays transmitted is recorded by a linear detector, which rotates around the object and this process is repeated for a collection of tomographic views. Typically, the detector is swept over an angular range of 180 \u043c. In practice, we consider the case of limited vision, where the angle coverage is much less than 180 m. There are two main sources of error in the reconstruction of images. The first source is the model error, where the assumed forward operator, H, does not correspond to the forward operator that generated the measured data. In practice, H cannot accurately capture all the properties of a real imaging system, so there is always a certain amount of model errors where we can use the same scenario without measuring the model errors in the simulations."}, {"heading": "Image reconstruction methods", "text": "In this thesis, three different formulations of the optimization problem of Eqn. 2 were considered for comparative purposes, which corresponded to different decisions of S and \u03a6 (f): 1. The smallest squares (LS): S = Rn, \u03a6 (f) = 0 2. Non-negative smallest squares (LS-NN): S = Rn \u2265 0, \u03a6 (f) = 03. The smallest squares with TV regulation (PLS-TV): S = Rn \u2265 0, \u03a6 (f) = VP-TV, where Rn \u2265 0 denotes the set of non-negative real numbers and vice versa semi-normalizes the total variation (TV). The proposed DL-assisted approach requires a certain selection for the approximate reconstruction operator R (line 4 in Alg. 1). In this thesis, the action of R was calculated by approximately minimizing the LS lens function or LS-NN lens function."}, {"heading": "5 Numerical experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Interpretation", "text": "The least challenging version of the reverse problem, the reverse crime case, was initially considered, although the operator Q was specified to calculate the solution to the LS optimization problem, which can be calculated by applying the Moore-Penrose pseudo-inverse to the measured data. Since the measured data is consistent and H \u2212 1MPH is the projection operator on Umeas, in this case R can restore the measurable component of the true object. The zero space component of the estimated object will remain unchanged from the original conjecture. Furthermore, any changes to the measurable component of Q can be corrected by ApplyingR, while the zero component remains unchanged. In this way, R and Q assume responsibility for estimating different components of the object: the measurable component is estimated by R and the zero component by Q. by applying each of these operators alternately, the information in the zero component is estimated."}, {"heading": "Results", "text": "The proposed approach was compared to a single-pass approach and PLS-TV method. In the DL-assisted approach, little to no increase in inperformance was observed after 5 iterations; thus, n = 5 employments.To compare with the single-pass approach (i.e., CNN-based image recovery), a CNN with the architecture described above was initially trained with the conventional training regimen; the performance of the single-pass CNN increased when trained with the two-step scheme. This performance increase can be explained by the additional training data acting in the capacity of the common data augmentation techniques, which increase CNN's ability to generalize new data. Therefore, the single-pass approach is fairly accurately compared with the proposed DL-assisted reconstruction approach. The PLS-TV optimization problem was solved with the FISTA."}, {"heading": "6 Conclusion", "text": "An approach was presented that embeds a conventional, deep CNN in an iterative framework for the task of image reconstruction. Due to its iterative nature, our approach balances the information available in the measured data with the a priori information learned within CNN. Experimental results and our analysis show that our approach achieves better results than a traditional, single-pass CNN-based image reconstruction approach and a modern reconstruction method that exploits scarcity to regulate. Our framework is broad enough to include other variants of R, such as those that appreciate a PLS TV solution, and can also be applied to areas outside of image reconstruction."}], "references": [{"title": "Deep learning for photoacoustic tomography from sparse data", "author": ["Stephan Antholzer", "Markus Haltmeier", "Johannes Schwab"], "venue": "arXiv preprint arXiv:1704.04587,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Foundations of image science", "author": ["H.H. Barrett", "K.J. Myers"], "venue": "Wiley series in pure and applied optics. Wiley-Interscience,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Compressed sensing using generative models", "author": ["Ashish Bora", "Ajil Jalal", "Eric Price", "Alexandros G Dimakis"], "venue": "arXiv preprint arXiv:1703.03208,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Emmanuel J. Cand\u00e8s", "Justin K. Romberg", "Terence Tao"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Low-dose ct with a residual encoder-decoder convolutional neural network (red-cnn)", "author": ["Hu Chen", "Yi Zhang", "Mannudeep K Kalra", "Feng Lin", "Peixi Liao", "Jiliu Zhou", "Ge Wang"], "venue": "arXiv preprint arXiv:1702.00288,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "arXiv preprint arXiv:1606.00915,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "International conference on artificial intelligence and statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A Deep Learning Architecture for Limited-Angle Computed Tomography Reconstruction, pages 92\u201397", "author": ["Kerstin Hammernik", "Tobias W\u00fcrfl", "Thomas Pock", "Andreas Maier"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["John R. Hershey", "Jonathan Le Roux", "Felix Weninger"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Globally and Locally Consistent Image Completion", "author": ["Satoshi Iizuka", "Edgar Simo-Serra", "Hiroshi Ishikawa"], "venue": "ACM Transactions on Graphics (Proc. of SIGGRAPH 2017),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Fundamentals of Digital Image Processing, page 439", "author": ["Anil K. Jain"], "venue": "Englewood Cliffs, NJ, Prentice,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Learning Optimal Nonlinearities for Iterative Thresholding Algorithms", "author": ["U.S. Kamilov", "H. Mansour"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Accurate image super-resolution using very deep convolutional networks", "author": ["Jiwon Kim", "Jung Kwon Lee", "Kyoung Mu Lee"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR Oral),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Reconnet: Non-iterative reconstruction of images from compressively sensed random measurements", "author": ["Kuldeep Kulkarni", "Suhas Lohit", "Pavan Turaga", "Ronan Kerviche", "Amit Ashok"], "venue": "arXiv preprint arXiv:1601.06892,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections", "author": ["Xiaojiao Mao", "Chunhua Shen", "Yu-Bin Yang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Learning proximal operators: Using denoising networks for regularizing inverse imaging problems", "author": ["Tim Meinhardt", "Michael M\u00f6ller", "Caner Hazirbas", "Daniel Cremers"], "venue": "arXiv preprint arXiv:1704.03488,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning the weight matrix for sparsity averaging in compressive imaging", "author": ["Dimitris Perdios", "Adrien Georges Jean Besson", "Philippe Rossinelli", "Jean-Philippe Thiran"], "venue": "In IEEE International Conference on Image Processing (ICIP 2017),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization", "author": ["Emil Y Sidky", "Xiaochuan Pan"], "venue": "Physics in medicine and biology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "A perspective on deep imaging", "author": ["Ge Wang"], "venue": "IEEE Access,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Accelerating magnetic resonance imaging via deep learning", "author": ["Shanshan Wang", "Zhenghang Su", "Leslie Ying", "Xi Peng", "Shun Zhu", "Feng Liang", "Dagan Feng", "Dong Liang"], "venue": "In Biomedical Imaging (ISBI),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Deep admm-net for compressive sensing mri", "author": ["Yan Yang", "Jian Sun", "Huibin Li", "Zongben Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["Raymond Yeh", "Chen Chen", "Teck Yian Lim", "Mark Hasegawa-Johnson", "Minh N Do"], "venue": "arXiv preprint arXiv:1607.07539,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 99, "endOffset": 107}, {"referenceID": 24, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 99, "endOffset": 107}, {"referenceID": 7, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 122, "endOffset": 128}, {"referenceID": 28, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 151, "endOffset": 159}, {"referenceID": 12, "context": "Such methods are now ubiquitous and routinely employed for tasks that include image classification [11, 26], segmentation [8, 2], and image completion [30, 13], to name a few.", "startOffset": 151, "endOffset": 159}, {"referenceID": 25, "context": "The next wave in DL methodologies for imaging applications is likely to address the important task of improving the quality of images produced by computed imaging systems [27].", "startOffset": 171, "endOffset": 175}, {"referenceID": 5, "context": "They are particularly effective when the measurement model satisfies the mathematical conditions prescribed by compressive sampling theory [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 20, "context": "Although recent advances in generative neural networks are exciting and encouraging [21] [23], the task of specifying an image prior that comprehensively describes the statistical properties of a specified ensemble of images remains a challenging task.", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "Although recent advances in generative neural networks are exciting and encouraging [21] [23], the task of specifying an image prior that comprehensively describes the statistical properties of a specified ensemble of images remains a challenging task.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "Extensive work has been performed on applying DL methods to image restoration tasks, such as denoising or in-painting [16, 30, 19].", "startOffset": 118, "endOffset": 130}, {"referenceID": 28, "context": "Extensive work has been performed on applying DL methods to image restoration tasks, such as denoising or in-painting [16, 30, 19].", "startOffset": 118, "endOffset": 130}, {"referenceID": 18, "context": "Extensive work has been performed on applying DL methods to image restoration tasks, such as denoising or in-painting [16, 30, 19].", "startOffset": 118, "endOffset": 130}, {"referenceID": 15, "context": "In [16], a deep residual network is proposed for the task of image super-resolution.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19], an autoencoder style CNN is proposed which makes use of residual-style layers [11], and is applied to the tasks of image denoising and super-resolution.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [19], an autoencoder style CNN is proposed which makes use of residual-style layers [11], and is applied to the tasks of image denoising and super-resolution.", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "Recently, several groups has have used DL techniques to remove artifacts and noise from images reconstructed from incomplete and/or inconsistent measurement data [7, 1, 28].", "startOffset": 162, "endOffset": 172}, {"referenceID": 0, "context": "Recently, several groups has have used DL techniques to remove artifacts and noise from images reconstructed from incomplete and/or inconsistent measurement data [7, 1, 28].", "startOffset": 162, "endOffset": 172}, {"referenceID": 26, "context": "Recently, several groups has have used DL techniques to remove artifacts and noise from images reconstructed from incomplete and/or inconsistent measurement data [7, 1, 28].", "startOffset": 162, "endOffset": 172}, {"referenceID": 11, "context": "In [12], inference algorithms are unfolded into a series of layers and the parameters of those layers are optimized by a neural network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "This technique has been expanded to incorporate many standard inference algorithms and applications [29, 22, 10, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 21, "context": "This technique has been expanded to incorporate many standard inference algorithms and applications [29, 22, 10, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 9, "context": "This technique has been expanded to incorporate many standard inference algorithms and applications [29, 22, 10, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 14, "context": "This technique has been expanded to incorporate many standard inference algorithms and applications [29, 22, 10, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 17, "context": "In [18], the original image is directly estimated by a CNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "In [5], the original image is estimated by gradient descent where the estimated image is constrained to lie in the range of a generative CNN.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "In [20], the proximal operator is replaced by a denoising CNN for an image reconstruction problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The first is that recent work has shown that deeper networks are more powerful than shallower networks with the same number of parameters [11],[25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "The chosen network architecture for implementing Q corresponds to a deep residual CNN, inspired by [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "Additional details can be found in [16].", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "All models were trained in the same way, using ADAM [17], a batchsize of 64 and minimizing the mean squared error loss as in Eqn.", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "The default parameters for ADAM [17] were used, except for a learning rate of 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "We employed the weight initialization strategy introduced in [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 13, "context": "Taking inspiration from the Shepp-Logan phantom [14], the generated images are made up of one main ellipse and between 2 and 7 other minor ellipses.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "The Moore-Penrose pseudoinverse was computed by first performing singular value decomposition of H and then employing the method described in [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "The PLS-TV optimization problem was solved using the FISTA [4] with an initial guess of all zeros.", "startOffset": 59, "endOffset": 62}, {"referenceID": 23, "context": "This approach represents a state-of-the-art method for sparse image reconstruction [24].", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "An approach to incorporate deep learning within an iterative image reconstruction framework to reconstruct images from severely incomplete measurement data is presented. Specifically, we utilize a convolutional neural network (CNN) as a quasi-projection operator within a least squares minimization procedure. The CNN is trained to encode high level information about the class of images being imaged; this information is utilized to mitigate artifacts in intermediate images produced by use of an iterative method. The structure of the method was inspired by the proximal gradient descent method, where the proximal operator is replaced by a deep CNN and the gradient descent step is generalized by use of a linear reconstruction operator. It is demonstrated that this approach improves image quality for several cases of limited-view image reconstruction and that using a CNN in an iterative method increases performance compared to conventional image reconstruction approaches. We test our method on several limited-view image reconstruction problems. Qualitative and quantitative results demonstrate state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}