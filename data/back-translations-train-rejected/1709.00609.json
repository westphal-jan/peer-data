{"id": "1709.00609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Security Evaluation of Pattern Classifiers under Attack", "abstract": "Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier's behavior in adversarial environments, and lead to better design choices.", "histories": [["v1", "Sat, 2 Sep 2017 17:38:45 GMT  (1204kb,D)", "http://arxiv.org/abs/1709.00609v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["battista biggio", "giorgio fumera", "fabio roli"], "accepted": false, "id": "1709.00609"}, "pdf": {"name": "1709.00609.pdf", "metadata": {"source": "CRF", "title": "Security Evaluation of Pattern Classifiers under Attack", "authors": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "emails": ["battista.biggio@diee.unica.it,", "fumera@diee.unica.it,", "roli@diee.unica.it,"], "sections": [{"heading": null, "text": "Index Terms - Model Classification, Enemy Classification, Performance Rating, Safety Rating, Robust Rating."}, {"heading": "1 INTRODUCTION", "text": "It is. It is. It is. It is. It is. It is. It is. Indeed, it is. Most people who stand up for the rights of men are not aware of themselves and their rights. (...) It is not as if they feel able to respect the rights of men. (...) It is not as if they disregard the rights of men. \"(...) It is as if they disregard the rights of men.\" (...) It is. \"(...) It is as if they disregard the rights of men.\" (...) \"It is as if they disregard the rights of men.\" (...) \"It is.\" (...) It is. \"(...) It is.\" (...) It is. \"It is.\" (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. (... It is. It is. It is. It is."}, {"heading": "2 BACKGROUND AND PREVIOUS WORK", "text": "At this point, we review previous work and highlight the concepts used in our framework."}, {"heading": "2.1 A taxonomy of attacks against pattern classifiers", "text": "A taxonomy of potential attacks against pattern classifiers was proposed in [11], [15] and subsequently expanded in [14], and we will exploit it within our framework in defining attack scenarios. Taxonomy is based on two main characteristics: the nature of the impact of attacks on the classifier and the type of security breach that they cause; the influence can be causal either by undermining the learning algorithm to cause subsequent misclassifications; or explorative if it uses the knowledge of the trained classifier to cause misclassifications without affecting the learning algorithm. Therefore, causal attacks can affect both training and test data or only training data, while exploratory attacks only affect test data. The security breach can be a targeted breach of integrity if it allows the opponent to access the service or resources protected by the classifier; an availability violation if it involves legitimate access to the user's private sphere or if the attacker is denied legitimate access to it."}, {"heading": "2.2 Limitations of classical performance evaluation methods in adversarial classification", "text": "The classical methods of performance assessment, such as k-fold cross-validation and bootstrapping, aim to estimate the performance of a classifier during operation by using data D collected during the classification process.2 These methods are based on the stationary assumption that the data seen in operation follow the same distribution as D. Accordingly, they resort to D to construct one or more pairs of training and test sets that ideally follow the same distribution as D. However, the presence of an intelligent and adaptive adversary makes the classification problem highly unstable and makes it difficult to predict how many and what types of attacks a classifier will be exposed to during operation, i.e. how the distribution of data will change. In particular, the test data processed by the trained classifier may be affected by both exploratory and causal attacks, while the training data may only be affected by causal attacks when the classifier is performed during operation [if the classifier is performed during both online and in other cases].11"}, {"heading": "2.3 Arms race and security by design", "text": "In fact, it is a purely reactionary project, which is about finding a solution that does justice to the interests of the individual."}, {"heading": "2.4 Previous work on security evaluation", "text": "Many authors implicitly conducted safety assessments as a what-if analysis, based on empirical simulation methods; however, they mainly focused on a specific application, classification, and attack, and developed ad hoc safety assessment procedures based on the use of problem knowledge and heuristic techniques [1] - [6], [15], [22] - [36]. Their goal was either to point out a previously unknown vulnerability or to evaluate security against a known attack. In some cases, specific countermeasures were also proposed, according to a proactive / security-by-design approach. Attacks were simulated by manipulating samples only according to application-specific criteria, without reference to more general guidelines; consequently, such techniques cannot be directly exploited by a system designer in more general cases. Some papers suggested analytical methods to evaluate the safety of learning algorithms or some classes of decision functions (e.g. linear), based on more general, independent criteria [12], in particular [the theory] 11, and [the] 11."}, {"heading": "2.5 Building on previous work", "text": "Here we summarize the three main concepts that have emerged more or less explicitly from previous work and are used in our framework for security assessment.1) Arms Race and Security by Construction: Since it is not possible to predict how many and what types of attacks a classifier will suffer during operation, the security of a classifier should be proactively evaluated using a what-if analysis by simulating potential attack scenarios. 2) Enemy modeling: Effective simulation of attack scenarios requires a formal model of the enemy under attack. 3) Data distribution: The distribution of test data may differ from the distribution of training data when the classifier."}, {"heading": "3 A FRAMEWORK FOR EMPIRICAL EVALUATION OF CLASSIFIER SECURITY", "text": "We propose a framework for empirical evaluation of the security of classifiers in adversarial environments that combines and builds on the three concepts highlighted in Section 2.5. Our main goal is to provide a quantitative and universal basis for applying the what-if analysis to the security evaluation of classifiers, based on the definition of potential attack scenarios. To this end, we propose: (i) an adversary model that allows us to define each attack scenario; (ii) an appropriate model of data distribution; and (iii) a method for generating training and test sets that are representative of data distribution and used for empirical performance evaluation."}, {"heading": "3.1 Attack scenario and model of the adversary", "text": "This year, it has come to the point that there will only be one time that there will be such a process, in which there will be such a process."}, {"heading": "3.2 A model of the data distribution", "text": "iSe rf\u00fc ide r\u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f \u00fc die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f der f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f der die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die die f die f die f die f die die f die die f die f die f die f die f die f die die f die f die f die f die die die die f die f die die f die f die f die die f die f die f die die f die f die die die f die f die f die die f die die f die f die die die die f die f die die die die f die f die die die die f die f die die f die f die f die die die f die f die die die f die f die die die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f"}, {"heading": "3.3 Training and testing set generation", "text": "Here we propose an algorithm to measure the performance of the classifier (TR) and to test (TS) sets of any desired size from the distributions ptr (X, Y) and pts (X, Y). We assume that k \u2265 1 different pairs of training and test sets (DiTR, DiTS), i = 1,., k, from D {{S} analyses were obtained using a classic resampling technique such as cross-validation or boot strapping. Accordingly, their samples follow the distribution pD (X, Y). In the following, we describe how we modify each of the sets DiTR to construct a training collection that follows the distribution ptr (X, Y). For the sake of simplicity, we will skip the superscript i. An identical procedure can be followed to construct a test set TSi from each of the DiTS. Safety evaluation is then performed using the classical method by checking the performance of the classifier for TRi."}, {"heading": "3.4 How to use our framework", "text": "We summarize here the steps that the designer of a pattern classifier should take to evaluate its security against our framework, for each attack scenario of interest. They extend the performance assessment step of the classical design cycle of [9], which is used as part of the model selection phase, and to evaluate the final classifier to be used.1) attack scenario. The attack scenario should be defined at the conceptual level by making specific assumptions about the target, knowledge (k.i-v) and capability of the adversary (c.i-iv) and defining the corresponding attack strategy (a.i-iii) according to the model of section 3.1.2) data model. According to the hypothetical attack scenario, the designer should define the distributions p (Y), p (A-Y) and p (X-Y, A)."}, {"heading": "4 APPLICATION EXAMPLES", "text": "While previous work has focused on a single application, here we look at three different application examples of our framework in the areas of spam filtering, biometric authentication and network intrusion detection. Our aim is to show how the designer of a pattern classifier can use our framework and what kind of additional information he can obtain through the security assessment. We will show that there is sometimes a trade-off between classification accuracy and security, and that this information can be used for several purposes, for example to improve the selection phase of the model by taking into account both classification accuracy and security."}, {"heading": "4.1 Spam filtering", "text": "Assuming that one would distinguish between legitimate and legitimate E-mails (X = Y). (D = Y = Y). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "4.2 Biometric authentication", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "4.3 Network intrusion detection", "text": "We assume that this type of attack is unable to detect ever-before-seen malicious activity or even variants of known activity. To overcome this problem, anomalous detection systems are proposed, building a statistical model of normal traffic using machine learning techniques, usually single-class classifiers (e.g. PAYL [49]), and setting off alarms when abnormal traffic is detected."}, {"heading": "5 SECURE DESIGN CYCLE: NEXT STEPS", "text": "The classic design cycle of a pattern classifier [9] consists of: data collection, data pre-processing, feature extraction and selection, model selection (including the choice of learning and classification algorithms and adjustment of their parameters) and performance evaluation. We would like to point out that this design cycle disregards the threats that may occur in the opposing settings and extends the performance evaluation to such settings. Reviewing the remaining steps under a security aspect remains a very interesting topic for future work. We briefly outline how this open problem can be addressed and assume that the opponent has some control over the data collected for classification training and parameter tuning, a filter step for detecting and removing attack samples."}, {"heading": "6 CONTRIBUTIONS, LIMITATIONS AND OPEN ISSUES", "text": "In this paper, we focused on the empirical safety assessment of pattern classifiers that need to be used in adversarial environments and suggested how to revise the classic design step of performance assessment, which is not suitable for this purpose. Our main contribution is based on an empirical safety assessment framework that formalizes and generalizes all attacks considered in previous work and can be applied to various classifiers, learning algorithms and classification tasks. It is based on a formal model of the adversary and on a model of data distribution that can represent all possible attacks; it provides a systematic method for generating training and test kits that allows for safety assessment; and it can accommodate application-specific techniques for attack simulation. This is a clear evolution in relation to previous work, as without a general framework, most proposed techniques (often tailored to a particular classification model) cannot be applied directly to other problems."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Davide Ariu, Gavin Brown, Pavel Laskov and Blaine Nelson for discussions and comments on an earlier version of this paper. This work was partially supported by a scholarship funded by Battista Biggio of the Autonomous Region of Sardegna, PO Sardegna FSE 2007-2013, L.R. 7 / 2007 \"Promoting Scientific Research and Technological Innovation in Sardinia,\" through the CRP-18293 project funded by the Autonomous Region of Sardegna, L.R. 7 / 2007, Bando 2009, and through the TABULA RASA project funded within the 7th Framework Programme of Research of the European Union."}], "references": [{"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["R.N. Rodrigues", "L.L. Ling", "V. Govindaraju"], "venue": "J. Vis. Lang. Comput., vol. 20, no. 3, pp. 169\u2013179, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal fusion vulnerability to non-zero effort (spoof) imposters", "author": ["P. Johnson", "B. Tan", "S. Schuckers"], "venue": "IEEE Int\u2019l Workshop on Inf. Forensics and Security, 2010, pp. 1\u20135.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proc. 15th Conf. on USENIX Security Symp. CA, USA: USENIX Association, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "On attacking statistical spam filters", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "1st Conf. on Email and Anti-Spam, CA, USA, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "2nd Conf. on Email and Anti-Spam, CA, USA, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "6th Conf. on Email and Anti-Spam, CA, USA, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Adversarial knowledge discovery", "author": ["D.B. Skillicorn"], "venue": "IEEE Intell. Syst., vol. 24, pp. 54\u201361, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Adversarial information retrieval: The manipulation of web content", "author": ["D. Fetterly"], "venue": "ACM Computing Reviews, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "10th ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining, WA, USA, 2004, pp. 99\u2013108.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "in Proc. Symp. Inf., Computer and Commun. Sec. (ASIACCS). NY, USA: ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Evaluation of classifiers: Practical considerations for security applications", "author": ["A.A. C\u00e1rdenas", "J.S. Baras"], "venue": "AAAI Workshop on Evaluation Methods for Machine Learning, MA, USA, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning in adversarial environments", "author": ["P. Laskov", "R. Lippmann"], "venue": "Machine Learning, vol. 81, pp. 115\u2013119, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "4th ACM Workshop on Artificial Intelligence and Security, IL, USA, 2011, pp. 43\u201357.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A. Joseph", "J. Tygar"], "venue": "Machine Learning, vol. 81, pp. 121\u2013148, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proc. 11th ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining, A. Press, Ed., IL, USA, 2005, pp. 641\u2013647.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "Proc. 2nd ACM Workshop on Security and Artificial Intelligence. NY, USA: ACM, 2009, pp. 1\u20134.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for generating data to simulate changing environments", "author": ["A.M. Narasimhamurthy", "L.I. Kuncheva"], "venue": "Artificial Intell. and Applications. IASTED/ACTA Press, 2007, pp. 415\u2013420.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "What-if analysis", "author": ["S. Rizzi"], "venue": "Enc. of Database Systems, pp. 3525\u2013 3529, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Paragraph: Thwarting signature learning by training maliciously", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "Recent Advances in Intrusion Detection, ser. LNCS. Springer, 2006, pp. 81\u2013105.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proc. 23rd Int\u2019l Conf. on Machine Learning. ACM, 2006, pp. 353\u2013360.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Using an ensemble of oneclass SVM classifiers to harden payload-based anomaly detection systems", "author": ["R. Perdisci", "G. Gu", "W. Lee"], "venue": "Int\u2019l Conf. Data Mining. IEEE CS, 2006, pp. 488\u2013498.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Advanced allergy attacks: does a corpus really help", "author": ["S.P. Chung", "A.K. Mok"], "venue": "Recent Advances in Intrusion Detection, ser. RAID \u201907. Berlin, Heidelberg: Springer-Verlag, 2007, pp. 236\u2013255.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A multiple instance learning strategy for combating good word attacks on spam filters", "author": ["Z. Jorgensen", "Y. Zhou", "M. Inge"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1115\u20131146, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Casting out demons: Sanitizing training data for anomaly sensors", "author": ["G.F. Cretu", "A. Stavrou", "M.E. Locasto", "S.J. Stolfo", "A.D. Keromytis"], "venue": "IEEE Symp. on Security and Privacy. CA, USA: IEEE CS, 2008, pp. 81\u201395.  14", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Proc. 1st Workshop on Large-Scale Exploits and Emergent Threats. CA, USA: USENIX Association, 2008, pp. 1\u20139.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Antidote: understanding and defending against poisoning of anomaly detectors", "author": ["B.I. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S.-h. Lau", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "Proc. 9th ACM SIGCOMM Internet Measurement Conf., ser. IMC \u201909. NY, USA: ACM, 2009, pp. 1\u201314.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proc. 13th Int\u2019l Conf. on Artificial Intell. and Statistics, 2010, pp. 405\u2013412.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to classify with missing and corrupted features", "author": ["O. Dekel", "O. Shamir", "L. Xiao"], "venue": "Machine Learning, vol. 81, pp. 149\u2013178, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, 2011, pp. 977\u2013982.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["\u2014\u2014"], "venue": "Int\u2019l Journal of Machine Learning and Cybernetics, vol. 1, no. 1, pp. 27\u201341, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial environments", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "Proc. 10th Int\u2019l Workshop on Multiple Classifier Systems, ser. LNCS, vol. 6713. Springer-Verlag, 2011, pp. 350\u2013359.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Poisoning adaptive biometric systems", "author": ["B. Biggio", "G. Fumera", "F. Roli", "L. Didaci"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition, ser. LNCS, vol. 7626. Springer, 2012, pp. 417\u2013425.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "Proc. 29th Int\u2019l Conf. on Machine Learning, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning in the presence of malicious errors", "author": ["M. Kearns", "M. Li"], "venue": "SIAM J. Comput., vol. 22, no. 4, pp. 807\u2013837, 1993.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1993}, {"title": "A framework for the evaluation of intrusion detection systems", "author": ["A.A. C\u00e1rdenas", "J.S. Baras", "K. Seamon"], "venue": "Proc. IEEE Symp. on Security and Privacy. DC, USA: IEEE CS, 2006, pp. 63\u201377.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple classifier systems for adversarial classification tasks", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Proc. 8th Int\u2019l Workshop on Multiple Classifier Systems, ser. LNCS, vol. 5519. Springer, 2009, pp. 132\u2013141.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 2617\u20132654, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Vulnerabilities in biometric encryption systems", "author": ["A. Adler"], "venue": "5th Int\u2019l Conf. on Audio- and Video-Based Biometric Person Authentication, ser. LNCS, vol. 3546. NY, USA: Springer, 2005, pp. 1100\u20131109.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1993}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Trans. on Neural Networks, vol. 10, no. 5, pp. 1048\u20131054, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surv., vol. 34, pp. 1\u201347, 2002.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "LibSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "2001. [Online]. Available: http://www.csie.ntu.edu. tw/\u223ccjlin/libsvm/", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Likelihood ratio-based biometric score fusion", "author": ["K. Nandakumar", "Y. Chen", "S.C. Dass", "A. Jain"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intell., vol. 30, pp. 342\u2013347, February 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Robustness of multi-modal biometric verification systems under realistic spoofing attacks", "author": ["B. Biggio", "Z. Akhtar", "G. Fumera", "G. Marcialis", "F. Roli"], "venue": "Int\u2019l Joint Conf. on Biometrics, 2011, pp. 1\u20136.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Security evaluation of biometric authentication systems under real spoofing attacks", "author": ["B. Biggio", "Z. Akhtar", "G. Fumera", "G.L. Marcialis", "F. Roli"], "venue": "IET Biometrics, vol. 1, no. 1, pp. 11\u201324, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Anomalous payload-based network intrusion detection", "author": ["K. Wang", "S.J. Stolfo"], "venue": "RAID, ser. LNCS, vol. 3224. Springer, 2004, pp. 203\u2013222.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural Comput., vol. 12, no. 5, pp. 1207\u20131245, 2000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Comparing anomaly detection techniques for http", "author": ["K. Ingham", "H. Inoue"], "venue": "Recent Advances in Intrusion Detection, ser. LNCS. Springer, 2007, pp. 42\u201362.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Spam filtering using inexact string matching in explicit feature space with on-line linear classifiers", "author": ["D. Sculley", "G. Wachman", "C.E. Brodley"], "venue": "15th Text Retrieval Conf. NIST, 2006.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 257, "endOffset": 260}, {"referenceID": 3, "context": ", by misspelling common spam words to avoid their detection) [4]\u2013[6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": ", by misspelling common spam words to avoid their detection) [4]\u2013[6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "Adversarial scenarios can also occur in intelligent data analysis [7] and information retrieval [8]; e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Adversarial scenarios can also occur in intelligent data analysis [7] and information retrieval [8]; e.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 306, "endOffset": 309}, {"referenceID": 10, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 311, "endOffset": 315}, {"referenceID": 14, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 317, "endOffset": 321}, {"referenceID": 15, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 323, "endOffset": 327}, {"referenceID": 0, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Although this emerging field is attracting growing interest [13], [18], [19], the above issues have only been sparsely addressed under different perspectives and to", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": ", in the case of concept drift [20].", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "[21], which is a common practice in security.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "As in [17], it is formulated as the optimization of an objective function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "We propose to define this function based on the desired security violation (integrity, availability, or privacy), and on the attack specificity (from targeted to indiscriminate), according to the taxonomy in [11], [14] (see Sect.", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "We propose to define this function based on the desired security violation (integrity, availability, or privacy), and on the attack specificity (from targeted to indiscriminate), according to the taxonomy in [11], [14] (see Sect.", "startOffset": 214, "endOffset": 218}, {"referenceID": 5, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 231, "endOffset": 235}, {"referenceID": 14, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 237, "endOffset": 241}, {"referenceID": 37, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 243, "endOffset": 247}, {"referenceID": 12, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 37, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "assumptions about what can be kept fully secret from the adversary should be done, as discussed in [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "i) the attack influence (either causative or exploratory), as defined in [11], [14];", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "i) the attack influence (either causative or exploratory), as defined in [11], [14];", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Our model can be easily extended to take into account concurrent attacks involving m > 1 different kinds of sample manipulations; for example, to model attacks against different classifiers in multimodal biometric systems [1], [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "Our model can be easily extended to take into account concurrent attacks involving m > 1 different kinds of sample manipulations; for example, to model attacks against different classifiers in multimodal biometric systems [1], [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 28, "context": ", see [32]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": "Accordingly, we can sample with replacement from D TR [42].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 2, "endOffset": 5}, {"referenceID": 24, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 39, "endOffset": 42}, {"referenceID": 24, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "appear in legitimate emails, and by obfuscating \u201cbad words\u201d that are typically present in spam [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "As in [6], [10], the adversary is assumed to have perfect knowledge of the classifier, i.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "As in [6], [10], the adversary is assumed to have perfect knowledge of the classifier, i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": ", she can insert or obfuscate any word), but up to a maximum number nmax of features in each spam email [6], [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": ", she can insert or obfuscate any word), but up to a maximum number nmax of features in each spam email [6], [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "As in [10], the generation of attack samples can be represented by a function A : X 7\u2192 X , and the number of modified words can be evaluated by the Hamming distance.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "We use DTR to construct TR and DTS to construct TS, as in [5], [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "We use DTR to construct TR and DTS to construct TS, as in [5], [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 40, "context": "Four feature subsets with size 1,000, 2,000, 10,000 and 20,000 have been selected using the information gain criterion [44].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "1] [6]: AUC10% = \u222b 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 41, "context": "SVMs are implemented with the LibSVM software [45].", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "To this end, we partially exploit the analysis in [1], [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "To this end, we partially exploit the analysis in [1], [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 11, "endOffset": 14}, {"referenceID": 42, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "As in [1], [2], we assume that each impostor", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "As in [1], [2], we assume that each impostor", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Here we follow the approach of [1], [2], in which the impostor is assumed to fabricate \u201cperfect\u201d fake traits,", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "Here we follow the approach of [1], [2], in which the impostor is assumed to fabricate \u201cperfect\u201d fake traits,", "startOffset": 36, "endOffset": 39}, {"referenceID": 43, "context": "of the distribution of the fake traits; however, this is a challenging research issue on its own, that is out of the scope of this work, and part of the authors\u2019 ongoing work [47], [48].", "startOffset": 175, "endOffset": 179}, {"referenceID": 44, "context": "of the distribution of the fake traits; however, this is a challenging research issue on its own, that is out of the scope of this work, and part of the authors\u2019 ongoing work [47], [48].", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "According to the above assumption, as in [1], [2], any spoofing attack is simulated by replacing", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "According to the above assumption, as in [1], [2], any spoofing attack is simulated by replacing", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "of the fingerprint matcher for the left index finger, and normalize them in [0, 1] using the min-max technique.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "We thus compute a maximum likelihood estimate of p(xfing, xface|Y ) from TR using a product of two Gamma distributions, as in [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "is unacceptable for security applications, and provides further support to the conclusions of [1], [2] against the common belief that multimodal biometric systems are intrinsically more robust than unimodal systems.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "is unacceptable for security applications, and provides further support to the conclusions of [1], [2] against the common belief that multimodal biometric systems are intrinsically more robust than unimodal systems.", "startOffset": 99, "endOffset": 102}, {"referenceID": 45, "context": ", PAYL [49]), and raise an alarm when anomalous traf-", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 224, "endOffset": 228}, {"referenceID": 25, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 236, "endOffset": 240}, {"referenceID": 32, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 242, "endOffset": 246}, {"referenceID": 45, "context": "designed, using a one-class \u03bd-SVM classifier with a radial basis function (RBF) kernel and the feature vector representation proposed in [49].", "startOffset": 137, "endOffset": 141}, {"referenceID": 46, "context": "[50]), and the \u03b3 value of the RBF kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "01 as suggested in [24], so that only \u03b3 has to be chosen.", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": "We focus on a causative attack similar to the ones considered in [38], aimed at forcing the learned model of normal traffic to include samples of intrusions to be attempted during operation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "We use the data set of [24].", "startOffset": 23, "endOffset": 27}, {"referenceID": 47, "context": "packets) collected by a web server during five days in 2006, and a publicly available set of 205 malicious samples coming from intrusions which exploit the HTTP protocol [51].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "1, and the fact that in this application all the malicious samples in D must be inserted in the testing set [27], [51].", "startOffset": 108, "endOffset": 112}, {"referenceID": 47, "context": "1, and the fact that in this application all the malicious samples in D must be inserted in the testing set [27], [51].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "The performance under attack is evaluated as a function of pmax, as in [17], [28], which reduces to the classical performance evaluation when pmax = 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "The performance under attack is evaluated as a function of pmax, as in [17], [28], which reduces to the classical performance evaluation when pmax = 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": ", the data sanitization method of [27]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 48, "context": "For instance, in [52] inexact string matching was proposed to counteract word obfuscation attacks in spam filtering.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 243, "endOffset": 246}, {"referenceID": 19, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 254, "endOffset": 258}, {"referenceID": 28, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 260, "endOffset": 264}, {"referenceID": 12, "context": "Finally, a secure system should also guarantee the privacy of its users, against attacks aimed at stealing confidential information [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 37, "context": "methods have been proposed in biometric recognition systems to protect the users against the so-called hillclimbing attacks, whose goal is to get information about the users\u2019 biometric traits [41], [53].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "preserve privacy in [14], [54].", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "We obtained encouraging preliminary results on this topic [32].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier\u2019s behavior in adversarial environments, and lead to better design choices.", "creator": "LaTeX with hyperref package"}}}