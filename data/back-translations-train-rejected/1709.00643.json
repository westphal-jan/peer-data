{"id": "1709.00643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Fast Image Processing with Fully-Convolutional Networks", "abstract": "We present an approach to accelerating a wide variety of image processing operators. Our approach uses a fully-convolutional network that is trained on input-output pairs that demonstrate the operator's action. After training, the original operator need not be run at all. The trained network operates at full resolution and runs in constant time. We investigate the effect of network architecture on approximation accuracy, runtime, and memory footprint, and identify a specific architecture that balances these considerations. We evaluate the presented approach on ten advanced image processing operators, including multiple variational models, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. All operators are approximated by the same model. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach. The results are shown in the supplementary video at", "histories": [["v1", "Sat, 2 Sep 2017 22:38:13 GMT  (5387kb,D)", "http://arxiv.org/abs/1709.00643v1", "Published at the International Conference on Computer Vision (ICCV 2017)"]], "COMMENTS": "Published at the International Conference on Computer Vision (ICCV 2017)", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["qifeng chen", "jia xu", "vladlen koltun"], "accepted": false, "id": "1709.00643"}, "pdf": {"name": "1709.00643.pdf", "metadata": {"source": "CRF", "title": "Fast Image Processing with Fully-Convolutional Networks", "authors": ["Qifeng Chen", "Jia Xu", "Vladlen Koltun"], "emails": [], "sections": [{"heading": null, "text": "We show that our models generalize across datasets and resolutions and examine a number of extensions of the approach presented. 1ar X"}, {"heading": "1. Introduction", "text": "In this context, it should be noted that the measures in question are measures that have been taken in recent years."}, {"heading": "2. Related Work", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "3. Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Preliminaries", "text": "Let me be an image represented in the RGB color space. Let's be an operator who converts the contents of an image without changing its dimensions: that is, me and f (me) have the same resolution. We will consider a variety of operators that use a wide range of algorithms. In addition, we will consider many operators, but our corresponding approximations require that they all share the same parameters: both the operator f and its approach to variable resolutions. We will consider many operators."}, {"heading": "3.2. Context aggregation networks", "text": "Our primary architecture is the multi-scale contextual aggregation network (CAN) developed in the context of semantic image analysis. [78] Its intermediate representations and output have the same resolution as input. Contextual information is gradually aggregated on ever larger scales, so that the computation of each output pixel takes into account all input pixels within a size window exponentially in the depth of the network. \u2212 This results in global information aggregation for high-resolution images with very compact parameterization. We will see that this architecture fills all the desiderata outlined above. We will now describe the parameterization in detail. \u2212 The data is created over several successive layers: {L0,., Ld}. The first and last layers L0, Ld we have dimensionality m \u00d7 n \u00d7 3. These represent the input and output images."}, {"heading": "3.3. Adaptive normalization", "text": "We have found that the use of batch normalization improves approximation accuracy in demanding machine vision operators such as style transfer and pencil drawing, but impairs performance in other machine vision operators. We therefore use an adaptive normalization that combines batch normalization and identity mapping: batch normalization is done by backpropagation depending on all other parameters of the network [67]. Learning these weights allows the model to adapt to the characteristics of the approximate operator and adjust the strengths of the identity branch and the batch normalization branch as needed."}, {"heading": "3.4. Training", "text": "The network is trained on a set of input-output pairs that contain images before and after the application of the original operator: D = {Ii, f (Ii)}. The parameters of the network are the core weights K = {Ksi, j} s, i, j and the distortions B = {bsi} s, i. These parameters are optimized to match the action of operator f over all images in the training environment. We train with an image-space regression loss: (K, B) = i1Ni f (Ii, B) \u2212 f (Ii)."}, {"heading": "4. Experiments", "text": "In fact, it is so that most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) In fact, it is so that they are able to outdo themselves. (...) \"(...)\" In fact, it is as if they are able to outdo themselves. (...) \"(...)\" (...) \"(...)\" (\") (((...)\" (...) (\") (((...)\") (((...) \"(...) ((...) (() (()) ((...) () () () (...) () () () () () () () () ()) (()) (())) (()) (()) (()) (()) (()) (())) (()) (() () ()) (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () ((() () () (() (() () () (() (() (() (() () (() ((() () (((() () (() (() ((() (() () () (() ((() (() (() ((() () ((() (() ((() (((()) ((((())) ((((()) ((("}, {"heading": "5. Extensions", "text": "We will now describe three extensions of the presented approach: the representation of parameterized operators representing multiple operators on a single network, and video processing. An image processor can have parameters that control its action. For example, variable image smoothing operators [66, 58, 73] often have a parameter that controls the relative strength of the regulator: higher proportions lead to more aggressive smoothing. To this end, we will add channels to the input layer that have several significant parameters that can be used to control the operator's action. Of course, our approach extends to the creation of parameterized approximators that expose these degrees of freedom. To this end, we will add channels to the input layer, we will add an input channel that is used to communicate the value of the parameter."}, {"heading": "6. Conclusion", "text": "We have presented an approach to approximate a wide range of machine vision operators, with all operators approached with the same parameterization and computational flow. We have shown that the presented approach significantly exceeds previous approximation schemes. We see the uniform and regular computational flow in the presented model as a major advantage. Although the model is already faster than baselines using generic implementation, we expect significant further acceleration to be achieved."}, {"heading": "A. Operators", "text": "In this appendix we describe in more detail the ten image processing operators used in our experiments. Our approach approaches all operators using the same model. Rudin-Osher-Fatemi (ROF) is a pioneering model for variational image restoration, which aims to eliminate noise while at the same time obtaining verifiable image characteristics by optimizing a variational lens over the image. Let me say that the images are a grayscale image. A restored image J: \"R\" can be calculated by minimizing the following lens: \"J\" (I \u2212 J) 2, (1) which is a free parameter controlling the smoothness of J. The first term is the total variation of the regulation and the second term \"J.\" (I \u2212 J) 2 is a data term that uses the L2 standard. Equation (1) is strictly convex, so there is a unique minimum global."}, {"heading": "B. Context Aggregation Networks", "text": "The context aggregation architecture is schematically illustrated in Figure 4. For this figure, we use depth d = 6 and width w = 8. Dilation is increased from r1 = 1 in L1 to r4 = 8 in L4. Corresponding growth in the receptive field of each element in each layer is shown in the figure. For Ld \u2212 1 (L5 in Figure 4), we do not use dilation. For the output layer Ld (L6 in the figure), we use a linear transformation (1 x 1 convolution without nonlinearity) that projects the final feature layer into the RGB color space. Figure 4 provides only a schematic visualization. The network we use is deeper and has a much larger receptive field. Table 3 provides a specification of the configuration of CAN32 that provides d = 10 and w = 32 and 5x 13 receptive fields."}, {"heading": "C. Alternative Fully-Convolutional Architectures", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "D. Accuracy and Runtime", "text": "The approximation accuracy of each approach for each operator is given in Table 4. These are the numerical results visualized in Figure 2 and summarized in Table 1 in the paper. The approximation accuracy for different CAN configurations and alternative fully revolutionary architectures is given in Table 5; these are the numerical results summarized in Table 2 in the paper. The runtime of each approach for each operator is in Table 6. Operators are arranged in the same order as in Table 4. Runtime was measured on a computer with an Intel i7-5960X 3.0 GHz CPU and a Nvidia Titan X GPU. Our approach is faster than BGU-opt by more than one order of magnitude. It is faster than BGU-fast for eight of the ten operators."}, {"heading": "E. Cross-Resolution Generalization", "text": "The results of the cross-resolution generalization of L0 smoothing are shown in Figure 5."}, {"heading": "F. Cross-Dataset Generalization", "text": "Similarly, we tested two models for each operator on the RAISE test set: one on the RAISE training set and one on the MIT-Adobe training set. Results for all operators are shown in Table 7. They show that the trained approximators generalize extremely well. Accuracy under appropriate conditions (e.g. MIT \u2192 MIT and RAISE \u2192 MIT) is practically identical. On the MIT test set, the SSIM achieved by models trained on RAISE is within 1% of the SSIM achieved by models trained on the MIT training set for all operators, and the same is true for the RAISE test set. This indicates that our approximators effectively reflect the underlying action of the reference operators."}, {"heading": "G. Ablation Studies", "text": "Here we report on the results of additional controlled experiments, which examine various aspects of the structure of our model and their effects on approximation accuracy. Results of these experiments are repeated with the smoothing operator L0 on the MIT-Adobe dataset. Depth. Let's start with training and testing the context network at different depths. Results are in Table 8. Note that smaller depth implies a smaller receiver field. As shown in the table, the results are good even for flat networks: Thus, even at depth, the model achieves 4 higher SSIM than BGU-opt. (At this depth, the runtime on 1080p images is 67 ms.) Accuracy continues to improve with depth and saturation at d = 9, width. We are now evaluating the effect of width (the number of characteristic maps in each interlayer) on approximation accuracy. The experimental setup is the same as in the previous experiment. The results are shown in Table 9. Accuracy is even at a relatively low network capacity of 84p."}, {"heading": "H. Parameterized Operators", "text": "We use the smoothing operator L0. We sample different hyperparameters \u03bb in Equation 3: \u03bb = \u03bb-Exp (x), where x is a random variable with uniform distribution U (\u2212 ln (10), ln (10) and \u03bb-Exp (0.01) is the default value. We train and test the approximator with randomly detected parameters \u03bb. The approximation accuracy achieved by our approach is 21.0 in MSE, 36.2 in PSNR and 0.984 in SSIM."}, {"heading": "I. Qualitative Results", "text": "Our method consistently outperforms other approaches. The most sophisticated evaluation / upsample scheme, BGU-opt, does not work well if the operator's action at high resolution cannot be recovered from its output at low resolution. In contrast, our method works directly at the original resolution. Our direct approach is also more precise than previous approaches that used deep networks."}], "references": [{"title": "Fast local Laplacian filters: Theory and applications", "author": ["M. Aubry", "S. Paris", "S.W. Hasinoff", "J. Kautz", "F. Durand"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Non-local image dehazing", "author": ["D. Berman", "T. Treibitz", "S. Avidan"], "venue": "In CVPR, 2016", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Bilateral guided upsampling", "author": ["J. Chen", "A. Adams", "N. Wadhwa", "S.W. Hasinoff"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Edgepreserving decompositions for multi-scale tone and detail manipulation", "author": ["Z. Farbman", "R. Fattal", "D. Lischinski", "R. Szeliski"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Stochastic relaxation, Gibbs distributions, and the bayesian restoration", "author": ["S. Geman", "D. Geman"], "venue": "of images. PAMI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "Deep joint demosaicking and denoising", "author": ["M. Gharbi", "G. Chaurasia", "S. Paris", "F. Durand"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Single image haze removal using dark channel", "author": ["K. He", "J. Sun", "X. Tang"], "venue": "prior. PAMI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"], "venue": "In CVPR, 2017", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": "In ECCV, 2016", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Learning recursive filters for low-level vision via a hybrid neural network", "author": ["S. Liu", "J. Pan", "M. Yang"], "venue": "In ECCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Combining sketch and tone for pencil drawing production", "author": ["C. Lu", "L. Xu", "J. Jia"], "venue": "In Non-Photorealistic Animation and Rendering,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Optimal approximations by piecewise smooth functions and associated variational problems", "author": ["D. Mumford", "J. Shah"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Fast and effective L0 gradient minimization by region fusion", "author": ["R.M.H. Nguyen", "M.S. Brown"], "venue": "In ICCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A variational approach to remove outliers and impulse noise", "author": ["M. Nikolova"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Local Laplacian filters: Edge-aware image processing with a Laplacian pyramid", "author": ["S. Paris", "S.W. Hasinoff", "J. Kautz"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "U-Net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "In MIC- CAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Fast partitioning of vectorvalued images", "author": ["M. Storath", "A. Weinmann"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Image smoothing via L0 gradient minimization", "author": ["L. Xu", "C. Lu", "Y. Xu", "J. Jia"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep edgeaware filters", "author": ["L. Xu", "J.S.J. Ren", "Q. Yan", "R. Liao", "J. Jia"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Structure extraction from texture via relative total variation", "author": ["L. Xu", "Q. Yan", "Y. Xia", "J. Jia"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "In ICLR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer\u2019s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].", "startOffset": 72, "endOffset": 92}, {"referenceID": 6, "context": "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer\u2019s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].", "startOffset": 156, "endOffset": 162}, {"referenceID": 4, "context": "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer\u2019s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].", "startOffset": 156, "endOffset": 162}, {"referenceID": 8, "context": "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer\u2019s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].", "startOffset": 278, "endOffset": 293}, {"referenceID": 13, "context": "One general approach to accelerating a broad range of image processing operators is well-known: downsample the image, execute the operator at low resolution, and upsample [45, 34, 14].", "startOffset": 171, "endOffset": 183}, {"referenceID": 13, "context": "For example, the PSNR of our approximators across the ten considered operators on the MIT-Adobe test set is 36 dB, compared to 25 dB for the high-accuracy variant of bilateral guided upsampling [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 20, "context": "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].", "startOffset": 96, "endOffset": 125}, {"referenceID": 14, "context": "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].", "startOffset": 96, "endOffset": 125}, {"referenceID": 1, "context": "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].", "startOffset": 96, "endOffset": 125}, {"referenceID": 0, "context": "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].", "startOffset": 96, "endOffset": 125}, {"referenceID": 7, "context": "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].", "startOffset": 96, "endOffset": 125}, {"referenceID": 5, "context": "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].", "startOffset": 64, "endOffset": 79}, {"referenceID": 12, "context": "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].", "startOffset": 64, "endOffset": 79}, {"referenceID": 16, "context": "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].", "startOffset": 64, "endOffset": 79}, {"referenceID": 22, "context": "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "A general approach to accelerating image processing operators is to downsample the image, evaluate the operator at low resolution, and upsample [45, 34, 14].", "startOffset": 144, "endOffset": 156}, {"referenceID": 10, "context": "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 2, "context": "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 9, "context": "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].", "startOffset": 73, "endOffset": 101}, {"referenceID": 18, "context": "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].", "startOffset": 73, "endOffset": 101}, {"referenceID": 21, "context": "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].", "startOffset": 168, "endOffset": 172}, {"referenceID": 9, "context": "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].", "startOffset": 148, "endOffset": 168}, {"referenceID": 19, "context": "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].", "startOffset": 148, "endOffset": 168}, {"referenceID": 15, "context": "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].", "startOffset": 148, "endOffset": 168}, {"referenceID": 19, "context": "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].", "startOffset": 194, "endOffset": 206}, {"referenceID": 3, "context": "Adversarial training is known to be unstable [4, 56, 16] and we found that it also did not increase the already excellent results that we were able to obtain with an appropriate network architecture and a direct image-space loss.", "startOffset": 45, "endOffset": 56}, {"referenceID": 15, "context": "Adversarial training is known to be unstable [4, 56, 16] and we found that it also did not increase the already excellent results that we were able to obtain with an appropriate network architecture and a direct image-space loss.", "startOffset": 45, "endOffset": 56}, {"referenceID": 23, "context": "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].", "startOffset": 223, "endOffset": 227}, {"referenceID": 4, "context": "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].", "startOffset": 295, "endOffset": 302}, {"referenceID": 4, "context": "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].", "startOffset": 355, "endOffset": 358}, {"referenceID": 8, "context": "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].", "startOffset": 405, "endOffset": 408}, {"referenceID": 11, "context": "We use two image processing datasets: MIT-Adobe 5K and RAISE [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "We use two image processing datasets: MIT-Adobe 5K and RAISE [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 13, "context": "Our primary baseline is bilateral guided upsampling (BGU) [14], the state-of-the-art form of the downsample-evaluate-upsample scheme for accelerating image processing operators.", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].", "startOffset": 239, "endOffset": 246}, {"referenceID": 8, "context": "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].", "startOffset": 266, "endOffset": 269}, {"referenceID": 4, "context": "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].", "startOffset": 350, "endOffset": 353}, {"referenceID": 13, "context": "BGU-fast [14] 521.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "BGU-opt [14] 413.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "(a) Input (b) Ours (c) BGU-opt [14] (d) Xu et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "For each operator, we show the input image, the result of the original reference operator, the result produced by our approximator, and results produced by BGU-opt [14], Xu et al.", "startOffset": 164, "endOffset": 168}, {"referenceID": 23, "context": "Other operators, such as multiscale tone manipulation, have multiple meaningful parameters that can be used to control the operator\u2019s effect [24].", "startOffset": 141, "endOffset": 145}], "year": 2017, "abstractText": "We present an approach to accelerating a wide variety of image processing operators. Our approach uses a fullyconvolutional network that is trained on input-output pairs that demonstrate the operator\u2019s action. After training, the original operator need not be run at all. The trained network operates at full resolution and runs in constant time. We investigate the effect of network architecture on approximation accuracy, runtime, and memory footprint, and identify a specific architecture that balances these considerations. We evaluate the presented approach on ten advanced image processing operators, including multiple variational \u2217Joint first authors models, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. All operators are approximated by the same model. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach. 1 ar X iv :1 70 9. 00 64 3v 1 [ cs .C V ] 2 S ep 2 01 7", "creator": "LaTeX with hyperref package"}}}