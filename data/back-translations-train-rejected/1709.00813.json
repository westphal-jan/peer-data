{"id": "1709.00813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "From Review to Rating: Exploring Dependency Measures for Text Classification", "abstract": "Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis.", "histories": [["v1", "Mon, 4 Sep 2017 05:38:35 GMT  (70kb,D)", "http://arxiv.org/abs/1709.00813v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel cunningham-nelson", "mahsa baktashmotlagh", "wageeh boles"], "accepted": false, "id": "1709.00813"}, "pdf": {"name": "1709.00813.pdf", "metadata": {"source": "CRF", "title": "From Review to Rating: Exploring Dependency Measures for Text Classification", "authors": ["Samuel Cunningham-Nelson", "Mahsa Baktashmotlagh", "Wageeh Boles"], "emails": ["samuel.cunninghamnelson@qut.edu.au", "m.baktashmotlagh@qut.edu.au", "w.boles@qut.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTIONStatistical analysis techniques are used to find patterns in actual data. Statistical dependence metrics such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and Randomized Dependence Coefficient (RDC) [3] have been widely used in the areas of pattern recognition and computer vision to find correlations between random variables. Here, we use state-of-the-art nonlinear dependence metrics for text classification. We first perform feature selection by maximizing the dependence of text comments and correlated results, and then train the classifier on the reduced features. Text analysis methods and techniques are applied to many actual data examples to obtain meaningful examples and results. Text data can be used when looking at product ratings to determine whether the film value consists of positive or negative evaluations or whether the product is satisfactory."}, {"heading": "II. LITERATURE REVIEW", "text": "Text analysis is a large area in which various methods and techniques are frequently used. Literary criticism discusses several common techniques, with an emphasis on those used in this work."}, {"heading": "A. Text Analysis and Bag of Words Representation", "text": "However, it is more difficult to extract the information mentioned than it was written by the critics."}, {"heading": "III. BACKGROUND", "text": "In general, the two probability distributions can be compared either by non-parametric models (e.g. estimation of nuclear density) or by parametric models (e.g. use of Gaussian mixing models) using two non-parametric approaches to calculate the distribution difference between multiple data sources: MMD (Maximum Mean Discrepancy) and RDC (Randomised Dependence Coefficient)."}, {"heading": "A. MMD (Maximum Mean Discrepancy)", "text": "Using the MMD criterion, we can determine whether p = q.Definition 1: [14] F should be a class of functions f: X. \u2212 R. Then the MMD and its empirical estimate are defined as: MMD (F, p, q) = sup f: F (Ex: p [f (x)] \u2212 Ey: q [f (y)]), MMD (F, Xp, Yq) = sup f: F 1 n: f (xip) \u2212 F 1 n: f (xip) \u2212 n: f (xip) \u2212 f: i (xip) \u2212 n: i = 1 f (xip) \u2212 1 n: n: j = 1 f (yjq) = 1 f (xip), (xip: [14] Let F: be a standard sphere in a reproducing kernel Hilbert Space (RKHS), defined on a compact metric space (xip) \u2212 1 c \u2212 j: k = (xip), x: 1 k (xip), x: 1 (xip)."}, {"heading": "B. RDC (Randomized Dependence Coefficient)", "text": "The Randomized Dependence Coefficient (RDC) statistic measures dependence between random samples X-Rp \u00b7 n and Y-Rq \u00b7 n by first applying a copula transformation to the random samples and projecting the copula through randomly selected nonlinear k-projections, and then finding the greatest canonical correlation between these nonlinear projections. In view of the random samples X-Rp \u00b7 n and Y-Rq \u00b7 n and the parameters k-N + and s-R +, the Randomized Dependence Coefficient between X and Y is defined as: rdc (X, Y; k, s): = sup \u03b1, \u03b2\u03c1 (P (X); k, s), \u03b2T\u0445 (P (Y); k, s), which is dependent coefficient. (X; k, s)."}, {"heading": "IV. PROPOSED APPROACH", "text": "This paper uses a combination of word vectors and techniques for reducing attributes, which are described in this section on how these methods were applied."}, {"heading": "A. Word2Vec", "text": "Word2Vec is a method for the formation of word vectors. The Google News dataset contains word vectors that were trained on about 100 billion words from articles by Google News. Some other common models are trained on datasets such as Wikipedia, but the Google News dataset continues to diffuse. Figure 1 below shows the neural network model in which Word2Vec is trained. The network has a single hidden layer. Both the output and output layers consist of the number of neurons that correspond to the number of unique words in the training vocabulary. The number of neurons in the hidden layer corresponds to the dimensionality of the word vector [15]. Figure 2 shows an example of how Word2Vec can model certain words that have once been reduced to two-dimensional space (in this case using PCA). You can see relationships between certain words that are represented in a vector form. As an equation, we can describe the relationship between this vectors (this equator - V) and the equation (the following equation \u2212 V)."}, {"heading": "B. Choosing Dimensionality", "text": "We validated the empirical performance of RDC and MMD on the basis of the evaluation data of the students q of the university subjects. The random dimension for RDC was set to 20, because we could not detect any improvements in the embedding of dimensions d \u2265 100. Furthermore, as mentioned in [16], we refer to the law of universality for the embedding dimension of [16], [17]: Theorem 1: A law of universality for the embedding dimension. In view of the n \u00b7 h random project \u03c6 with the parameters p > 4, we can refer to the law of universality for the embedding dimension of [16], [17]: Theorem 1: A law of universality for the embedding dimension. In view of the n \u00b7 h random project \u03c6 with the parameters p > 4, \u03bd 1, 1 (0, 1), and ctuo (0, 1) there is a number ee\u00df (%, N,%,%) for the following statement applies."}, {"heading": "C. Feature Reduction", "text": "The proposed feature reduction approach uses a greedy search method to calculate the correlation with both RDC and MMD. This was also compared with PCA for reference, and it was decided that the characteristics for each vector would be reduced from 300 to 20 with each method. For both RDC and MMD, we started with a feature for all words (N \u00d7 1) and found the maximum correlation between a feature and the output label. After selecting the optimal individual feature, this was then repeated by adding another feature to the input and maximizing this correlation until the 20 features were selected with the combined highest correlation. Figure 3 below shows an example of this. In this figure, N is the number of text reactions, C the correlation, and i the number of features to be tested (between 1 and 20)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. The Data Set", "text": "The data for this experiment was collected by Queensland University of Technology. QUT is conducting two student surveys entitled \"Pulse\" and \"Insight\" as part of the \"Reframe\" framework for evaluating learning and teaching. [18] The first survey, the Pulse survey, asks students for feedback during the first few weeks of the semester, and the second survey, entitled \"Insight,\" asks students at the end of the semester. In each of these surveys, students are asked to rate their views on three statements, but we focus on the final question: \"I am satisfied with this unit so far.\" Each statement was answered on the Likert scale, with 1 strictly rejecting and 5 strongly agreeing. Once these questions are answered, an open answer has been left open, allowing students to respond with feedback and suggestions asking them: \"Please provide further feedback on this unit.\" This feedback provides recommendations and suggestions for teachers to read and consider the work."}, {"heading": "B. Procedure", "text": "The procedure was divided into four important steps: data preparation and pre-processing, removal of excess data, reduction of dimensionality and application of different machine models (1) data preparation and pre-processing: The text data for each student reaction was pre-processed first \u2022 This means that all texts must be converted to lowercase letters. \u2022 Remove punctuation from the text data (i.e. no stopwords are used). Each word in the answer was converted to a vector, and the average of all of these vectors was then normalized before further analysis was carried out."}, {"heading": "C. Quantitative Results", "text": "1) Bag of Words and TF-IDF Accuracy: Bag of Words and TF-IDC are initially used to obtain initial accuracy.2) Word2Vec Accuracy: Presenting the words as vectors allows to uncover more information.These results show Word2Vec using the same machine learning models previously tested. As the Word2Vec model has a large dimensional space (300), various methods have been researched to reduce the complexity of the data. These vectors have been reduced to 20 dimensions following the aforementioned procedure. Table II shows some accuracies from both methods. The initial results from Table II show that the Gaussian Kernel - SVM achieves the highest accuracy (51.67%) in predicting satisfaction values."}, {"heading": "D. Qualitative Results", "text": "This year, it's like it's able to put itself at the top, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" But it's the way it is. \""}, {"heading": "E. Summary", "text": "This shows that RDC is able to select relevant feature vectors from the many feature vectors provided by Word2Vec. The qualitative results also show how important it is not to rely exclusively on accuracy values for these comments, especially since the student satisfaction value and comments may not match or provide all relevant information. Both the qualitative and quantitative results highlight different points of importance. The quantitative results show the utility of modelling students \"responses as vectors and show that the Gaussian SVM provides the best classified results."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we compared several text analysis techniques related to predicting student satisfaction. We showed a strong correlation between student satisfaction scores for a given unit. Word2Vec provided the best accuracy in predicting student satisfaction using a Gaussian SVM model. However, the Word2Vec model used contained a large number of dimensions, so we also investigated several functional reduction techniques. PCA delivered, as expected, the best numerical accuracy of the three techniques tested (PCA, RDC and MMD), but RDC was a close second. When looking at several responses qualitatively, it was found that RDC achieved results that were not necessarily correctly classified, but when reading the answer manually seemed to fit better into the category in which RDC placed it. These techniques allow those dimensions that have the greatest influence to be found. This work provides a comparison between different machine learning models and the accuracy of the result that we currently believe has been applied to a more accurate set of learning models and a more accurate set of results."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors thank QUT for providing the data and QUT students for completing the survey."}], "references": [{"title": "Kernel Independent Component Analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Copula-based Kernel Dependency Measures", "author": ["J. Schneider", "B. Poczos", "Z. Ghahramani"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp. 775\u2013782, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The randomized dependence coefficient", "author": ["D. Lopez-Paz", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems, 2013, pp. 1\u20139.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["P.D. Turney"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002, pp. 417\u2013424.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimodels of quality in education", "author": ["Y. Cheong Cheng", "W. Ming Tam"], "venue": "Quality Assurance in Education, vol. 5, no. 1, pp. 22\u201331, 1997. [Online]. Available: http://www.emeraldinsight.com/doi/10.1108/ 09684889710156558", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Movie Reviews and Revenues : An Experiment in Text Regression", "author": ["M. Joshi", "D. Das", "K. Gimpel", "N.A. Smith"], "venue": "no. June, pp. 293\u2013296, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Using TF-IDF to Determine Word Relevance in Document Queries", "author": ["J. Ramos", "J. Eden", "R. Edu"], "venue": "Proceedings of the first instructional conference on machine learning, vol. 242, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Nips, pp. 1\u20139, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "word2vec Explained: Deriving Mikolov et al.\u2019s Negative-Sampling Word-Embedding Method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722, no. 2, pp. 1\u20135, 2014. [Online]. Available: http: //arxiv.org/abs/1402.3722", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1532\u20131543, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Text comparison using word vector representations and dimensionality reduction", "author": ["H. Heuer"], "venue": "no. Euroscipy, pp. 13\u201316, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing Data using t-SNE", "author": ["L. Van Der Maaten", "G. Hinton", "G.H. van der Maaten"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A kernel method for the two-sample-problem", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems, vol. 19, no. 157, pp. 0\u201343, 2006. [Online]. Available: machinelearning.wustl.edu/mlpapers/paper{ }files/NIPS2006{ }583.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "author": ["K. Sugathadasa", "B. Ayesha", "N. de Silva", "A.S. Perera", "V. Jayawardana", "D. Lakmal", "M. Perera"], "venue": "arXiv preprint arXiv:1706.01967, 2017. [Online]. Available: http://arxiv.org/abs/1706. 01967", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Universality laws for randomized dimension reduction, with applications", "author": ["S. Oymak", "J.A. Tropp"], "venue": "no. November 2015, 2015. [Online]. Available: http://arxiv.org/abs/1511.09433", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust Domain Generalisation by Enforcing Distribution Invariance", "author": ["S.M. Erfani", "M. Baktashmotlagh", "M. Moshtaghi", "V. Nguyen", "C. Leckie", "J. Bailey", "K. Ramamohanarao"], "venue": "IJCAI, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Reframe: Queensland university of technology\u2019s evaluation framework", "author": ["Queensland University of Technology"], "venue": "May 2013. [Online]. Available: https://cms.qut.edu.au/{ }data/assets/pdf{ }file/ 0007/261718/reframe-qut-evaluation-framework{ }2013.pdf", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "Movie reviews consisting of solely text data have also been used to predict ratings or scores [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Part of teaching is fulfilling students\u2019 expectations, and feedback and ratings from evaluation allows that to be done [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Maas et al [6] show a generic approach to using words as a vector and their application to sentiment analysis.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "A study, that provides valuable insights into text data analysis, looks at predicting movie revenue from ratings written by critics [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "TF-IDF calculates values for every word which is a proportion of the word frequency in one document with respect to the frequency percentage of all documents the word appears in [8].", "startOffset": 178, "endOffset": 181}, {"referenceID": 8, "context": "1) Word2Vec: Developed in 2013, Word2Vec is one method of modelling words as vectors [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Other versions developed using similar processes exist, however this model was developed using a neural network training approach, and a negative skip gram model [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "2) Glove: Another alternative to Word2Vec is the word vector representation Glove [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "3) t-SNE: Visualising high-dimensional data is an important problem and needs to be considered carefully [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Using t-Distributed Stochastic Neighbour Embedding (t-SNE), data which previously has many dimensions can be reduced to just two or three for visualisation purposes [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "Definition 1: [14] Let F be a class of functions f : X \u2192 R.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Theorem 1: [14] Let F be a unit ball in a Reproducing Kernel Hilbert Space (RKHS), defined on a compact metric space X with associated kernel k(\u00b7, \u00b7).", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "Unlike the other non-linear dependence measures such as Kernel Canonical Correlation Analysis (KCCA) [1] and Copula Maximum Mean Discrepancy [2] which exhibit prohibitive running times on large scale data, RDC has low computational cost of O(n log n), where n is the number of samples.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Unlike the other non-linear dependence measures such as Kernel Canonical Correlation Analysis (KCCA) [1] and Copula Maximum Mean Discrepancy [2] which exhibit prohibitive running times on large scale data, RDC has low computational cost of O(n log n), where n is the number of samples.", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "The number of neurons in the hidden layer is equal to the dimentionality of the word vector [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "Word2Vec Model [15]", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Moreover, as noted in [16], the randomised embedding dimension should not be too much smaller than the original dimension D to prevent a point in the set from being mapped to the origin.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "To ensure this, we refer to the Universality Law for the Embedding Dimension from [16], [17]: Theorem 1: A Universality Law for the Embedding Dimension.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "To ensure this, we refer to the Universality Law for the Embedding Dimension from [16], [17]: Theorem 1: A Universality Law for the Embedding Dimension.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "QUT runs two student surveys titled \u2018Pulse\u2019 and \u2018Insight\u2019 as a part of the framework \u2018Reframe\u2019 for evaluating learning and teaching [18].", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis.", "creator": "LaTeX with hyperref package"}}}