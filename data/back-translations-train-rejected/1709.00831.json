{"id": "1709.00831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Hypothesis Testing based Intrinsic Evaluation of Word Embeddings", "abstract": "We introduce the cross-match test - an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.", "histories": [["v1", "Mon, 4 Sep 2017 06:29:36 GMT  (22kb)", "http://arxiv.org/abs/1709.00831v1", "Accepted to RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP"]], "COMMENTS": "Accepted to RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nishant gurnani"], "accepted": false, "id": "1709.00831"}, "pdf": {"name": "1709.00831.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ndgurnan@ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.00 831v 1 [cs.C L] 4S ep2 017Exact, non-distributed, high-dimensional hypotheses test as an intrinsic value for word embedding. We show that cross-match is an effective means of measuring the distribution similarity between different vector representations and evaluating the statistical significance of different vector embedding models. Furthermore, we find that cross-match can be used to provide a quantitative measure of linguistic similarity in the selection of bridge languages for machine translation. We show that the results of the hypotheses test are in line with our expectations and point out that the scope of two sample hypotheses tests is not limited to word embedding and can be extended to all vector representations."}, {"heading": "1 Introduction", "text": "This year, we have reached the stage where we feel we can put ourselves at the top, in the way that we feel we can put ourselves at the top."}, {"heading": "2 Cross-Match Test", "text": "The cross-match test (Rosenbaum, 2005) is a non-parametric quality-of-fit test of arbitrary dimensions. It is an exact, non-distributive two-sample hypotheses test that measures whether two distributions are equal or not. Formally, in two independent samples w1,...., wn \u0445 W and v1,...., cross-match tests use the zero hypothesis H0: W = V against the alternative hypotheses H1: W 6 = V. The test has traditionally been used in clinical settings where the goal is not to evaluate a treatment effect on a high-dimensional result between controls and treated subjects in a randomized experiment (Heller et al., 2010). In the case of word embedding, the goal is to test whether two sets of words embedding vectors were \"sampled\" from the same distribution."}, {"heading": "2.1 Definition of the Cross-Match Statistic", "text": "Suppose we get two sets of word vectors {w1,.., wn} \u0445 W and {v1,.., vm} \u0445 V. Assign the group names 0 and 1 to indicate from which sample the vectors originate, so that the data is organized as follows: {(0, w1),.., (0, w1)} and {(1, v1),. (1, vm). The match statistics C, is a function of the word vector sD = {w1,.., v1,., vn} and the group names G = {0,., 0, 1,..,.,.,..,.... vels, is a function of the word vos,."}, {"heading": "2.2 Null Distribution of the Cross-Match Statistic", "text": "One advantage of the cross-match test is that we can calculate the exact distribution of statistical C under the null hypothesis H0. For N2 pair vectors, c0 should indicate the observed number of pairs with group labels (0.0), for c1 the observed number of pairs with group labels (0.1) or (1.0) (this is our observed cross-match statistic), and finally c2 the observed number of pairs with group labels (1.1).The zero distribution of C in closed form is: f (c1) = P (C = c1) = 2c1n! (N) c0! c1! c2! where N2 = c0 + c1 + c2 the null distribution in closed form also allows us to calculate the exact p value for our observed cross-match statistics.The resulting p value is equal to F (c1), where F (c1) = P (C \u2264 1) c2, the zero distribution in closed form, would also allow us to calculate the exact p value for our observed cross-match statistics.The resulting p value is equal to F (c1), where F (c1) = P (C) = c1) would reject the v1 from a word that we would yield the v1 hypothesis."}, {"heading": "3 Experiments", "text": "In the following experiments, we show two different illustrative examples of cross-match testing. Our goal is to demonstrate the effectiveness of cross-match as a general tool for intrinsic evaluation of word embedding vectors."}, {"heading": "3.1 Embedding Similarity", "text": "A bridge language (also referred to as a pivot language) is an artificial or natural language used as an intermediary for the translation between two different languages. In machine translation, a bridge language is useful in situations with limited resources where a good parallel corpora is not available for the target language. In such cases, a resource-rich, linguistically similar language is used as a proxy to accomplish the required NLP task. For example, in Tsvetkov and Dyer (2015), the authors use Arabic, Italian and French as bridge languages to perform Swahili English, Maltese-English, and Romanian-English translations accordingly. Assessing whether languages are linguistically similar is a relatively difficult task and depends on the notion of similarity that is used (lexical, morphological, etc.) In this experiment, we use crosswords that we use to establish a quantitative similarity between the two languages in order to assess the similarity between languages."}, {"heading": "3.2 Embedding Evaluation", "text": "Despite the popularity of different embedding models (Mikolov et al., 2013a, b; Pennington et al., 2014), it is not always clear whether a model represents a statistically significant improvement over other existing models (it could be that they all capture broadly similar features of the text). We look at four popular embedding models: word2vec Skip-gram, word2vec CBOW, Glove, and fastText, all trained on the same English Wikipedia corpus. Once again, we take size 200 samples from each method, calculate the p value between two pairs of methods using cross-match, and then report the average p value over 500 repeated iterations. Results in 3 show low p values across all pairs of embedding methods, suggesting that they capture all different aspects of the corpus they model. In other words, with the help of cross-match, we have evidence that there is no hypothesis that there is agreement."}, {"heading": "4 Conclusion", "text": "In this paper, we introduced the cross-match test, an accurate, non-distributed, high-dimensional hypotheses test as an intrinsic assessment metric for word embedding. We were able to demonstrate from two vivid examples that the test works in line with our expectations and can potentially be a useful tool in assessing bridge languages for machine translation. Despite the initially promising results, much remains to be done to confirm the effectiveness of cross-match in the context of word embedding. We believe that our main contribution is the introduction of the hypotheses testing framework as a method for intrinsic evaluation of vector representations. We note that there is nothing remarkable about word embedding or the cross-match test and that our experiments could be extended to other vector representations (sentence, phrase, etc.)."}, {"heading": "Acknowledgments", "text": "We thank Ndapa Nakashole for several useful discussions on the formulation of the problem and the anonymous reviewers for their feedback."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 .", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai."], "venue": "Comput. Linguist. 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Problems with evaluation of word embeddings using word similarity tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer."], "venue": "CoRR abs/1605.02276.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola."], "venue": "J. Mach. Learn. Res. 13:723\u2013773.", "citeRegEx": "Gretton et al\\.,? 2012", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Sensitivity analysis for the cross-match test, with applications in genomics", "author": ["Ruth Heller", "Shane T. Jensen", "Paul R. Rosenbaum", "Dylan S. Small."], "venue": "Journal of the American Statistical Association 105(491):1005\u20131013.", "citeRegEx": "Heller et al\\.,? 2010", "shortCiteRegEx": "Heller et al\\.", "year": 2010}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An improved model of semantic similarity based on lexical co-occurence", "author": ["Douglas L.T. Rohde", "Laura M. Gonnerman", "David C. Plaut."], "venue": "COMMUNICATIONS OF THE ACM 8:627\u2013633.", "citeRegEx": "Rohde et al\\.,? 2006", "shortCiteRegEx": "Rohde et al\\.", "year": 2006}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["Paul R. Rosenbaum."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(4):515\u2013 530.", "citeRegEx": "Rosenbaum.,? 2005", "shortCiteRegEx": "Rosenbaum.", "year": 2005}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Cross-lingual bridges with models of lexical borrowing", "author": ["Yulia Tsvetkov", "Chris Dyer."], "venue": "J. Artif. Intell. Res. (JAIR) 55:63\u201393.", "citeRegEx": "Tsvetkov and Dyer.,? 2015", "shortCiteRegEx": "Tsvetkov and Dyer.", "year": 2015}, {"title": "Correlation-based intrinsic evaluation of word vector representations", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Chris Dyer"], "venue": "CoRR abs/1606.06710", "citeRegEx": "Tsvetkov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2016}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "EMNLP. ACL, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.", "startOffset": 48, "endOffset": 116}, {"referenceID": 9, "context": "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.", "startOffset": 48, "endOffset": 116}, {"referenceID": 7, "context": "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.", "startOffset": 48, "endOffset": 116}, {"referenceID": 0, "context": ", 2013a) or neural networks (Bengio et al., 2003) have been successfully used to address various natural language processing", "startOffset": 28, "endOffset": 49}, {"referenceID": 15, "context": "tasks (Vaswani et al., 2013; Soricut and Och, 2015).", "startOffset": 6, "endOffset": 51}, {"referenceID": 12, "context": "tasks (Vaswani et al., 2013; Soricut and Och, 2015).", "startOffset": 6, "endOffset": 51}, {"referenceID": 8, "context": "syntactic and semantic properties of natural language (Mikolov et al., 2013b).", "startOffset": 54, "endOffset": 77}, {"referenceID": 11, "context": "In this work, we use Cross-match (Rosenbaum, 2005) - an exact, distribution free, high-dimensional hypothesis test to propose a novel approach for intrinsic evaluation of word embeddings, one that provides insight on tasks that depend on linguistic similarity.", "startOffset": 33, "endOffset": 50}, {"referenceID": 10, "context": "In the case of word embeddings, these constraints have led to the development of dedicated evaluation tasks like similarity and analogy (Rohde et al., 2006; Levy et al., 2015) which are not directly related to training objectives or to downstream tasks.", "startOffset": 136, "endOffset": 175}, {"referenceID": 6, "context": "In the case of word embeddings, these constraints have led to the development of dedicated evaluation tasks like similarity and analogy (Rohde et al., 2006; Levy et al., 2015) which are not directly related to training objectives or to downstream tasks.", "startOffset": 136, "endOffset": 175}, {"referenceID": 3, "context": "Despite their ease of interpretability, Faruqui et al. (2016) have shown that these tasks do not correlate well with downstream performance.", "startOffset": 40, "endOffset": 62}, {"referenceID": 3, "context": "Despite their ease of interpretability, Faruqui et al. (2016) have shown that these tasks do not correlate well with downstream performance. In related work, Tsvetkov et al. (2016) propose an evaluation measure QVECCCA that is shown to correlate well with downstream semantic tasks where the objective is to", "startOffset": 40, "endOffset": 181}, {"referenceID": 11, "context": "In this work, we use the Cross-match hypothesis test (Rosenbaum, 2005) to measure distributional similarity between different word vec-", "startOffset": 53, "endOffset": 70}, {"referenceID": 1, "context": "First, we use pretrained word vectors (trained on Wikipedia using the skip-gram model in Bojanowski et al. (2016)) from Facebook\u2019s fastText library for several languages to calculate the cross-match statistic for several language pairs.", "startOffset": 89, "endOffset": 114}, {"referenceID": 11, "context": "The cross-match test (Rosenbaum, 2005) is a nonparametric goodness-of-fit test in arbitrary dimensions.", "startOffset": 21, "endOffset": 38}, {"referenceID": 5, "context": "The test has been traditionally used in clinical settings, where the goal is to assess no treatment effect on a high-dimensional outcome between control and treated subjects in a randomized experiment (Heller et al., 2010).", "startOffset": 201, "endOffset": 222}, {"referenceID": 13, "context": "For example in Tsvetkov and Dyer (2015) the authors use Arabic, Italian and French as bridge languages to perform Swahili-English, Maltese-English and Romanian-English translations respectively.", "startOffset": 15, "endOffset": 40}, {"referenceID": 9, "context": "Despite the popularity of various different embedding models (Mikolov et al., 2013a,b; Pennington et al., 2014) it is not always clear whether one model represents a statistically significant improvement to other existing models (it maybe that all of them capture largely similar features of the text).", "startOffset": 61, "endOffset": 111}, {"referenceID": 4, "context": ") using other modern two-sample hypothesis tests such as the popular maximum mean discrepancy (Gretton et al., 2012).", "startOffset": 94, "endOffset": 116}], "year": 2017, "abstractText": "We introduce the cross-match test an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.", "creator": "LaTeX with hyperref package"}}}