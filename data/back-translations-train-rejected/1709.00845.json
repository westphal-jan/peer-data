{"id": "1709.00845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction", "abstract": "This work presents a novel semi-supervised learning approach for data-driven modeling of asset failures when health status is only partially known in historical data. We combine a generative model parameterized by deep neural networks with non-linear embedding technique. It allows us to build prognostic models with the limited amount of health status information for the precise prediction of future asset reliability. The proposed method is evaluated on a publicly available dataset for remaining useful life (RUL) estimation, which shows significant improvement even when a fraction of the data with known health status is as sparse as 1% of the total. Our study suggests that the non-linear embedding based on a deep generative model can efficiently regularize a complex model with deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.", "histories": [["v1", "Mon, 4 Sep 2017 07:42:57 GMT  (3145kb,D)", "http://arxiv.org/abs/1709.00845v1", "9 pages, 6 figures, 1 table, KDD17 Workshop on Machine Learning for Prognostics and Health Management.August 13-17, 2017, Halifax, Nova Scotia - Canada"]], "COMMENTS": "9 pages, 6 figures, 1 table, KDD17 Workshop on Machine Learning for Prognostics and Health Management.August 13-17, 2017, Halifax, Nova Scotia - Canada", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andre s yoon", "taehoon lee", "yongsub lim", "deokwoo jung", "philgyun kang", "dongwon kim", "keuntae park", "yongjin choi"], "accepted": false, "id": "1709.00845"}, "pdf": {"name": "1709.00845.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction", "authors": ["Andre S. Yoon", "Taehoon Lee", "Yongsub Lim", "Deokwoo Jung", "Philgyun Kang", "Dongwon Kim", "Keuntae Park", "Yongjin Choi"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Data-based forecasting; semi-supervised learning; deep learning; deep generative models; predictive maintenance"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that one will be able to move to a place where one is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Statement", "text": "In our problem, we have multivariate data for the sensor measurement and its corresponding RUL data. Let x (i, j) denote the measurement value of the sensor j. Furthermore, let us denote x (i) a vector of the multivariate sensor measurement such that x (i) = [x) = [x, 1), \u00b7, x (i, m)], where m is the number of sensors. Formally, let us denote a sensor measurement matrix = {x (1), \u00b7, x (n)}, where x (n)}, where x (i)}, where x (i) and n is the number of observed measurements. We also denote the corresponding set of RUL after y (1), \u00b7 \u00b7, y (n)}, where y (n)}, where y (i) and R +. Comprehensively, let us define the data sets D after {(i), y (i)."}, {"heading": "2.2 Neural Network Overview", "text": "In this section we will briefly discuss the definition and architecture of the basic, deep neural network and its variants.2.1 Preliminary neural network We can formally describe a forward-looking neural network defined as follows: Let us leave the number of neurons at level l and the non-linear activation function of neurons at level l, or where the output dimension k is the dimension of a single sensor measurement. In our problem, k = 1 is the number of neurons at level l, and k is the dimension of a neuronal output. We can comfortably assume that the output dimension k is the dimension of a single sensor measurement. In our problem, k = 1, as each sensor measurement is a scalar value. Then layer l has two function vectors; fl = 1 \u00b7 \u00b7 \u00b7 \u00b7 fl T and gl = [gl] T."}, {"heading": "2.3 Variational Autoencoder (VAE)", "text": "An autoencoder is a neural network with an encoder-decoder architecture that is trained to reconstruct its own input. Faced with input x, the autoencoder, fenc (x), is encoded, and then the decoder, fdec (z), decodes the encoded input (a representation) back to reconstruct the input as accurately as possible by minimizing a loss function of the form [5]: L (x, x) = | x \u2212 fdec (fenc (x) | (5), where fenc and fdec are symmetrical in topology by minimizing a loss function of the form [5]: L (x, x) = | x \u2212 fdec (x). The autoencoder is formed using the backpropagation algorithm in the same way as in the advanced neural network in Section 2.2.1. However, when the architecture with many hidden layers of the autoencoder becomes deeply weighted, the initial coding is necessary."}, {"heading": "3. OUR APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "In this section, we describe our general approach to semi-supervised learning for RUL predictions using a neural network frame and a deep generative model described in Section 2.2 and 2.3. We describe the implemented architectures for RUL predictions and several important architectural decisions on design parameters described in Section 4.3.1.1, originally used in Section 4.3.1.1 to improve the generalization of a supervised model. In our approach, we use the VAE model to obtain non-linear embedding in a lower dimensional space. It is done by training the VAE as described in Section 2.3, using x from both marked DL data and unmarked DU data to learn a mapping function for latent variable z."}, {"heading": "9 end", "text": "Phase 2 Supervised Learning with Embedding Input: (xN, yN), (xN, yN), (xN,), (N,), (N,), (N,), (N,), (N,), (N,), (N,), (N,), (N,), (N, (N,), (N,), (N, (N,), (N,), (N, (N,), (N, (N,), (N), (N), (N), (N), (N), (N), (N), (N), (N), (N), (N), (N,), (N, (), (N), (), (N, (), (N), (), (N), (),"}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "This section presents the evaluation of our approach using a publicly available dataset, Turbofan Engine Dataset, provided by the Prognostics Center of Excellence in National Aeronautics and Space Administration (NASA) [26]. The dataset consists of simulated multi-sensor measurements of a fleet of aircraft engines of the same type, created using realistic damage propagation modeling based on C-MAPSS [27]. The dataset is further divided into training and test sets. We use the training data to create models for the RUL estimate, and use the test data to evaluate the performance of the models with different measurements. In evaluating the proposed approach, we consider the case where the true historical RUL is partially known in the training set, by randomly dropping RUL information for a fraction of the engines in the training samples. We vary the fraction to cover a variety of conditions."}, {"heading": "4.1 C-MAPSS Turbofan Engine Dataset", "text": "The FD001 data set consists of multiple multivariate (24 sensors including 3 operational settings) time series from a fleet of engines with varying degrees of initial wear and unknown manufacturing variance. Training and test kits each contain 100 such engines. Engines in the training set have time series histories leading to failure. While the engines in the test set have time series histories shortened before errors occur, true RUL values are reported in a separate validation file at the time of shortening, with the total number of cycles in the training set and test kit reaching approximately 20K and 13K, respectively."}, {"heading": "4.2 Evaluations", "text": "4.2.1 Overview We model the motor failure directly by parameterizing the RNN architecture in Equation 4, so that the input sensor readings are directly mapped to historical RUL. RUL is mapped to each sequence for a specific motor by simply counting the number of cycles left before the failure (by definition). In this way, we avoid making any domain-specific assumptions for the modeling process. We first build a reliability model in a purely monitored environment in which all training samples with their corresponding RULs are used. We then omit the RULs for a randomly selected set of motors in the training set and repeat the procedure. The proportion of motors for which the RUL information is dropped varies from 100% to 1% (f = 100, 80, 50, 30, 20, 10, 5 and 1%). To remove any selection bias, we repeat the random selection for each fraction five times."}, {"heading": "4.3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Reconstruction with Variational Autoencoder", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5. DISCUSSION", "text": "This approach describes a semi-monitored learning approach to predicting plant failures when relatively little label information is available. Our approach uses non-linear embedding based on a deep generative model. We used the variable autoencoder as a deep generative model selection because the available label information is relatively easy to follow based on its solid theoretical basis. We trained the approach unsupervised using all available data, whether labeled or unlabeled. With this approach, we pursued a good predictive accuracy, even if the available label information is very limited. The approach was comprehensively evaluated using a publicly available turbofan engine data set, varying the proportion of labels to up to 1%. The evaluation shows significant improvements in predictive accuracy compared to the monitored and a naive semi-monitored approach, i.e. the selfgenerated label is considered to be a fraction of 1% if the predictive label condition is not considered to be an extreme 1% improvement compared to the 3.6% accuracy of the predictive label."}, {"heading": "6. REFERENCES", "text": "[1] G. S. Babu, P. Zhao, and X.-L. Li. Deep convolutionalneural network based regression approach for estimation of remaining useful life. In International Conference on Database Systems for Advanced Applications, pp. 214-228. Springer, 2016. [2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. [4] K. Cho, B. Merrie \u00f1boer, C-Gu \u00b6 lc, Mar. 1994. [3] O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning."}], "references": [{"title": "Deep convolutional neural network based regression approach for estimation of remaining useful life", "author": ["G.S. Babu", "P. Zhao", "X.-L. Li"], "venue": "International Conference on Database Systems for Advanced Applications, pages 214\u2013228. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Transaction of Neural Network,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative Adversarial Networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Recurrent neural networks for remaining useful life estimation", "author": ["F.O. Heimes"], "venue": "Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20136. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "science, 313(5786):504\u2013507", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Journal of Machine Learning Research", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Ladder Variational Autoencoders", "author": ["C. Kaae S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Rul prediction based on a new similarity-instance based approach", "author": ["R. Khelif", "S. Malinowski", "B. Chebel-Morello", "N. Zerhouni"], "venue": "Industrial Electronics  (ISIE), 2014 IEEE 23rd International Symposium on, pages 2463\u20132468. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ArXiv e-prints,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "CoRR, abs/1406.5298", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Prognostics and health management: A review on data driven approaches", "author": ["Q.Z.Y.H. Kwok L. Tsui", "Nan Chen", "W. Wang"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Iot-based prognostics and systems health management for industrial applications", "author": ["D. Kwon", "M.R. Hodkiewicz", "J. Fan", "T. Shibutani", "M.G. Pecht"], "venue": "IEEE Access, 4:3659\u20133670", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, London, UK, UK", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Auxiliary Deep Generative Models", "author": ["L. Maal\u00f8e", "C. Kaae S\u00f8nderby", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Multi-sensor prognostics using an unsupervised health index based on LSTM encoder-decoder", "author": ["P. Malhotra", "V. TV", "A. Ramakrishnan", "G. Anand", "L. Vig", "P. Agarwal", "G. Shroff"], "venue": "CoRR, abs/1608.06154", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to predictive maintenance", "author": ["R.K. Mobley"], "venue": "Butterworth-Heinemann", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "A modified echo state network based remaining useful life estimation approach", "author": ["Y. Peng", "H. Wang", "J. Wang", "D. Liu", "X. Peng"], "venue": "Prognostics and Health Management (PHM), 2012 IEEE Conference on, pages 1\u20137. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigating computational geometry for failure prognostics", "author": ["E. Ramasso"], "venue": "International Journal of Prognostics and Health Management, 5(1):005", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "CoRR, abs/1507.02672", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Turbofan engine degradation simulation data set", "author": ["A. Saxena", "K. Goebel"], "venue": "NASA Ames Prognostics Data Repository", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Damage propagation modeling for aircraft engine run-to-failure simulation", "author": ["A. Saxena", "K. Goebel", "D. Simon", "N. Eklund"], "venue": "Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20139. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Remaining useful life estimation\u2013a review on the statistical data driven approaches", "author": ["X.-S. Si", "W. Wang", "C.-H. Hu", "D.-H. Zhou"], "venue": "European journal of operational research, 213(1):1\u201314", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Prognostic modelling options for remaining useful life estimation by industry", "author": ["J. Sikorska", "M. Hodkiewicz", "L. Ma"], "venue": "Mechanical Systems and Signal Processing, 25(5):1803\u20131836", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE, volume 78", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "Deep learning and its applications to machine health monitoring: A survey", "author": ["R. Zhao", "R. Yan", "Z. Chen", "K. Mao", "P. Wang", "R.X. Gao"], "venue": "arXiv preprint arXiv:1612.07640", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "With increasingly more sensor data available either transient in stream or permanent in database, numerous physical assets in different industrial sectors such as manufacturing, transportation, and energy are subjected to asset condition and health monitoring [17].", "startOffset": 260, "endOffset": 264}, {"referenceID": 15, "context": ", asset failure [16].", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "The concept of RUL, also referred to as an estimated time to failure, is widely used in the future reliability prediction with an important industrial application known as predictive maintenance (PdM) [21].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 27, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 28, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "The advent of deep learning has enabled us to capture the complicated functions; for example, [31] exhibited performance improvements with deep learning-based prognostic models to predict the RUL of given assets.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "This is also closely related to the problem of intermittent failures also known as No Fault Found (NFF) problem [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "recognized such difficulty and explored various techniques to make learning better generalized in the lack of sufficient labels [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Semi-supervised learning (SSL) is a class of techniques that falls between unsupervised learning and supervised learning (SL) [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "There exist many different approaches in SSL, which include heuristic approaches such as self-learning and approaches based on generative models, and low-density separation [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 18, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 24, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 14, "context": "Two most popular approaches of DGM rely on variational inference [15] and generative adversarial network (GAN) [6].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Two most popular approaches of DGM rely on variational inference [15] and generative adversarial network (GAN) [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "For the non-linear activation, we use a popular rectified linear unit (ReLU) [22] where gi(fi) = max{0, fi} for fi \u2208 R.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "the backpropagation algorithm [18] given D .", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "RNN is such kind of a model that can capture into model the influence of an arbitrary number of previous samples on the current [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 29, "context": "The RNN parameter \u03b8 can be trained using backpropagation through time (BPTT) algorithm [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "However, RNNs with the BPTT suffer from the vanishing and exploding gradient problem [2] because a recurrent network grows as deep as sequence length.", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "An LSTM [9] is a second-order recurrent network that has three additional gate layers called input, forget, and update gates.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "GRU [4] is a simpler variant of LSTM which combines forget input gate and read input gate into overwrite gate by setting an input gate to the inverse of forget gate, ,i.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "It is shown that the performance of GRU is on par with LSTM but more computationally efficient [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Given an input x, the autoencoder, fenc(x), encodes and then the decoder, fdec(z), decodes the encoded input (a representation) back to reconstruct the input as closely as possible by minimizing a loss function of the form [5]:", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "However, when the architecture becomes deep with many hidden layers, pretraining is necessary to set the initial weights of the autoencoder close to the final ones [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 14, "context": "It can be rewritten for individual data points, x as [15]: log p\u03b8(x ) = DKL(q\u03c6(z|x)||p\u03b8(z|x)) + L(\u03b8, \u03c6; x) (6) where DKL is Kullback-Leibeler (KL) divergence.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "The left-hand side of (7), which is called variational lowerbound, can further be rewritten using Bayes\u2019 rule and the definition of KL divergence as [15]:", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "8 requires the Monte Carlo estimate of the expectation, which is not easily differentiable [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "This problem is solved by introducing a reparameterization of z with a deterministic variable such that z = \u03bc+ \u03c3 , with \u223c N (0, 1), which is known as \u201creparameterization trick\u201d [15].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "0 100 200 300 400 500 600 Cycle (d) S en so r v al ue s in ra ng e [0 ,1 ]", "startOffset": 67, "endOffset": 74}, {"referenceID": 14, "context": "chosen as originally suggested in [15].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "model (M1), which shows the state-of-the-art performance in various classification tasks [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Self-learning also known as self-labeling is an iterative algorithm that uses supervised learning in repetition [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 14, "context": "The first phase follows the minibatch version of the AEVB algorithm [15]", "startOffset": 68, "endOffset": 72}, {"referenceID": 25, "context": "This section presents the evaluation of our approach on a publicly available dataset, Turbofan Engine Dataset, provided by Prognostics Center of Excellence in National Aeronautics and Space Administration (NASA) [26].", "startOffset": 212, "endOffset": 216}, {"referenceID": 26, "context": "The dataset is composed of simulated multi-sensor readings from a fleet of aircraft engines of the same type, created with a realistic damage propagation modeling based on C-MAPSS [27].", "startOffset": 180, "endOffset": 184}, {"referenceID": 19, "context": "For the RUL prediction, we limit the value below a maximum RUL of 140 cycles as suggested in [20, 7].", "startOffset": 93, "endOffset": 100}, {"referenceID": 6, "context": "For the RUL prediction, we limit the value below a maximum RUL of 140 cycles as suggested in [20, 7].", "startOffset": 93, "endOffset": 100}, {"referenceID": 26, "context": "This includes the mean squared error (MSE), mean absolute error (MAE), and the score metric proposed in [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": ", L > 2) [11].", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The model is trained end-to-end using the Adam optimizer [13] with a mini-batch size of 100.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "This may support the manifold hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 19, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 0, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 11, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 23, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}], "year": 2017, "abstractText": "This work presents a novel semi-supervised learning approach for data-driven modeling of asset failures when health status is only partially known in historical data. We combine a generative model parameterized by deep neural networks with non-linear embedding technique. It allows us to build prognostic models with the limited amount of health status information for the precise prediction of future asset reliability. The proposed method is evaluated on a publicly available dataset for remaining useful life (RUL) estimation, which shows significant improvement even when a fraction of the data with known health status is as sparse as 1% of the total. Our study suggests that the non-linear embedding based on a deep generative model can efficiently regularize a complex model with deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.", "creator": "LaTeX with hyperref package"}}}