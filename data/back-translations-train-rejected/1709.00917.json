{"id": "1709.00917", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Using Optimal Ratio Mask as Training Target for Supervised Speech Separation", "abstract": "Supervised speech separation uses supervised learning algorithms to learn a mapping from an input noisy signal to an output target. With the fast development of deep learning, supervised separation has become the most important direction in speech separation area in recent years. For the supervised algorithm, training target has a significant impact on the performance. Ideal ratio mask is a commonly used training target, which can improve the speech intelligibility and quality of the separated speech. However, it does not take into account the correlation between noise and clean speech. In this paper, we use the optimal ratio mask as the training target of the deep neural network (DNN) for speech separation. The experiments are carried out under various noise environments and signal to noise ratio (SNR) conditions. The results show that the optimal ratio mask outperforms other training targets in general.", "histories": [["v1", "Mon, 4 Sep 2017 12:25:18 GMT  (906kb)", "http://arxiv.org/abs/1709.00917v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["shasha xia", "hao li", "xueliang zhang"], "accepted": false, "id": "1709.00917"}, "pdf": {"name": "1709.00917.pdf", "metadata": {"source": "CRF", "title": "Using Optimal Ratio Mask as Training Target for Supervised Speech Separation", "authors": ["Shasha Xia", "Hao Li", "Xueliang Zhang"], "emails": ["cszxl@imu.edu.cn"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. DNN-BASED MONAURAL SPEECH SEPARATION", "text": "The framework of DNN-based monaural language separation used in this study is the same as in [9]. We use a series of complementary features, consisting of amplitude modulation spectrogram (AMS), relative spectral transformation and perceptual linear prediction (RASTA-PLP), and Mel frequency receiver coefficients (MFCC). The feature set used here is similar to that in [9]. Since useful information is transmitted over timeframes [13], a symmetrical context window of 5 frames is used to split adjacent frames into a single feature vector. The DNN consists of three hidden layers, each layer having 1024 equilibrated linear hidden units (ReLU) [13]. Return propagation with failure regulation (failure rate 0.2) [14] is used for network training. The adaptive linear descend function of AAPSI coupled with an AAPSI activation dynamic during 2017PA is the first phase."}, {"heading": "III. THE OPTIMAL RATIO MASK", "text": "In fact, it is as if it were a pure project, in which it is a pure project."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dateset", "text": "We use 600 randomly selected expressions from the female IEEE corpus as training expressions, and the rest of 120 expressions serve as a test set. Four sounds from the NOISEX dataset [17] are used as training and test sounds, including a speech sound (SSN), a babble sound (chatter), an operating sound (factory), and a destructive sound (engine). Except SNN, all other 3 sounds are not stationary. All sounds are about 4 minutes long. To create the training set, we use random 10 cuts from the first 2 minutes of each sound to mix with each expression from the training sounds at -3, 0, and 3dB. Thus, we have 72000 (600 expressions x 10 cuts x 4 cuts x 3 SNR). The test mixtures are constructed by mixing random cuts from the last 2 minutes of each sound with the test sounds at -3, 0, and 3 SNR."}, {"heading": "B. Evaluation Criteria", "text": "We use the Short-Time Objective Intelligibility (STOI) score [18] to measure objective intelligibility. STOI refers to a correlation of short-term time envelopes between clean and separated language and has been shown to correlate highly with the human speech intelligibility score. The value of the STOI is in the range of [0, 1]. We also evaluate objective speech quality using the Perceptual Evaluation of SpeechQuality (PESQ) score [19]. Like the STOI, the PESQ is calculated by comparing the separated language with the corresponding clean language. The STOI score ranges from 0 to 1, and the PESQ score ranges from -0.5 to 4.5."}, {"heading": "C. Results", "text": "The separating results of the five training objectives are shown in Table 1, 2, 3, each under the condition of - 3 dB, 3 dB SNR mixture. Bold face indicates the target that is best performed within a noise. IBM and IRM are two most widely used training objectives. Table 1-3 shows that IBM improves speech intelligibility but not speech quality. This may be due to the binary property of IBM, and the musical noise is produced during separation. Compared to IBM, speech intelligibility and speech quality improves significantly."}, {"heading": "VI. CONCLUSIONS", "text": "Monaural speech separation is always a challenging task in the field of speech recognition. Monitored speech separation integrates deep neural network technology, which has emerged as a new trend in recent years. Impressive improvements are achieved with low SNR mixing and non-stationary noise levels. IBM and IRM are the most popular training targets. However, IBM improves speech intelligibility, but not speech quality. IRM improves both speech intelligibility and speech quality on the assumption that clean speech is independent of noise. Recent research shows that phase information is important for speech separation, cIRM and PSM have been proposed. cIRM performs well in theory, while the structure of its conceptual component is not obvious and difficult to estimate. PSM significantly improves speech intelligibility and speech quality, outperforms other training objectives. In this essay, we adapt ORM as a training goal in terms of the correlation between clean speech and noise, the result shows that ORM may perform better in the context of speech separation than other communication objectives, because this is a larger training objective."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was partially supported by a grant from the China National Nature Science Foundation (No. 61365006)."}], "references": [{"title": "Suppression of acoustic noise in speech using spectral subtraction,", "author": ["S. Boll"], "venue": "IEEE Trans. on acoust., speech, and signal process.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1979}, {"title": "On Ideal Binary Mask as the Computational Goal of Auditory Scene Analysis", "author": ["D.L Wang"], "venue": "P. Divenyi (Ed.), Speech Separation by Humans and Machines, Kluwer Academic, Norwell MA, 2005:181-197, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "An algorithm that improves speech intelligibility in noise for normal-hearing listeners", "author": ["G. Kim", "Y. Lu", "Y. Hu", "P.C. Loizou"], "venue": "J. Acoust. Soc. Amer., vol. 126, pp. 1486\u20131494, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A classification based approach to speech segregation", "author": ["K. Han", "D.L. Wang"], "venue": "Journal of the Acoust. Society of Amer., vol. 132, pp. 3475- 3483, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Cocktail party processing via structured prediction", "author": ["Y. Wang", "D.L. Wang"], "venue": "Proceedings of NIPS-12, pp. 224-232, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A feature study for classificationbased speech separation at low signal-to-noise ratios", "author": ["J. Chen", "Y. Wang", "D.L. Wang"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Lang. Process., vol. 22, pp. 1993-2002, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Features for masking-based monaural speech separation in reverberant conditions", "author": ["M. Delfarah", "D.L. Wang"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Lang. Process., vol. 25, pp. 1085-1094, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "On training targets for supervised speech separation", "author": ["Y. Wang", "A. Narayanan", "D.L. Wang"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Lang. Process., vol. 22, pp. 1849-1858, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1849}, {"title": "Complex ratio masking for monaural speech separation", "author": ["D.S. Williamson", "Y. Wang", "D.L. Wang"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Lang. Process, vol. 24, pp. 483-492, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe"], "venue": "ICASSP, pp. 708-712, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "The optimal ratio time-frequency mask for speech separation in terms of the signal-to-noise ratio", "author": ["S. Liang S", "W. Liu", "W. Jiang"], "venue": "Journal of the Acoust. Society of Amer., vol. 134, pp.452-458, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proc. ICML 2010 pp. 807-814.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky"], "venue": "The Journal of Machine Learn. Resear., vol. 15, pp. 1929-1958, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1929}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121-2159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of phase in speech enhancement", "author": ["K. Paliwal", "K. W\u00f3jcicki", "B. Shannon"], "venue": "Speech Commun., vol. 53, pp. 465-494, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems", "author": ["A. Varga", "H. Steeneken"], "venue": "Speech Commun., vol. 12, pp. 247\u2013251, 1993.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "An algorithm for intelligibility prediction of time-frequency weighted noisy speech", "author": ["C. Taal", "R. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "IEEE Trans. Audio, Speech, Lang. Process., vol. 19, pp. 2125\u20132136, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs", "author": ["A. Rix", "J. Beerends", "M. Hollier", "A. Hekstra"], "venue": "Proc. ICASSP, pp. 749\u2013752, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "spectral subtraction [1], have stationary assumptions on background noise, which limit their application.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Computational auditory scene analysis (CASA) [2] stimulates human auditory mechanism and employs an Ideal binary mask (IBM) [3] as the computational goal.", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Gaussian mixture model (GMM) [4], support vector machine (SVM) [5] and Deep Neural Networks (DNN) [6].", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Gaussian mixture model (GMM) [4], support vector machine (SVM) [5] and Deep Neural Networks (DNN) [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Gaussian mixture model (GMM) [4], support vector machine (SVM) [5] and Deep Neural Networks (DNN) [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "In [7-8], extensive features are studied in noisy and reverberant conditions.", "startOffset": 3, "endOffset": 8}, {"referenceID": 6, "context": "In [7-8], extensive features are studied in noisy and reverberant conditions.", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "The IBM and Ideal Ratio Mask (IRM) [9] are commonly used targets.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Based on these researches, complex Ideal Ratio Mask (cIRM) [10] and Phase Sensitive Mask (PSM) [11] were proposed.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "Based on these researches, complex Ideal Ratio Mask (cIRM) [10] and Phase Sensitive Mask (PSM) [11] were proposed.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Recently, Optimal Ratio Mask (ORM) is proposed in [12], which can be viewed as an improved version of the IRM.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "The framework of the DNN-based monaural speech separation used in this study is same as in [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "The feature set used here is similar to the one in [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "The DNN is composed of three hidden layers, each layer has 1024 rectified linear hidden units (ReLU) [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "2) [14] is used for network training.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Adaptive gradient descent [15]", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "The sigmoid activation function is applied when target is in the range of [0, 1], otherwise linear activation function is applied.", "startOffset": 74, "endOffset": 80}, {"referenceID": 10, "context": "The definition for the ORM is derived by minimizing the mean square error (MSE) of clean speech and estimated target speech [12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "The ORM is proven to get better performance for speech separation in [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "So, we restrict the value of the ORM with a hyperbolic tangent as in [9]", "startOffset": 69, "endOffset": 72}, {"referenceID": 14, "context": "Recent research showed that the phase information is also important for speech separation [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "Four noises from the NOISEX dataset [17] are used as our training and test noises, including a speech-shaped noise (SSN), a babble noise (babble), a factory noise (factory) and a destroyer engine room noise (engine).", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "We use the Short-Time Objective Intelligibility (STOI) score [18] to measure the objective intelligibility.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "The value of STOI is in the range of [0, 1].", "startOffset": 37, "endOffset": 43}, {"referenceID": 17, "context": "We also evaluate objective speech quality using the Perceptual Evaluation of Speech Quality (PESQ) score [19].", "startOffset": 105, "endOffset": 109}], "year": 2017, "abstractText": "Supervised speech separation uses supervised learning algorithms to learn a mapping from an input noisy signal to an output target. With the fast development of deep learning, supervised separation has become the most important direction in speech separation area in recent years. For the supervised algorithm, training target has a significant impact on the performance. Ideal ratio mask is a commonly used training target, which can improve the speech intelligibility and quality of the separated speech. However, it does not take into account the correlation between noise and clean speech. In this paper, we use the optimal ratio mask as the training target of the deep neural network (DNN) for speech separation. The experiments are carried out under various noise environments and signal to noise ratio (SNR) conditions. The results show that the optimal ratio mask outperforms other training targets in general.", "creator": "Microsoft\u00ae Word 2013"}}}