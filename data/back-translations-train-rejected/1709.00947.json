{"id": "1709.00947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "abstract": "This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from over-sensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.", "histories": [["v1", "Mon, 4 Sep 2017 13:30:23 GMT  (178kb,D)", "http://arxiv.org/abs/1709.00947v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pedro saleiro", "lu\\'is sarmento", "eduarda mendes rodrigues", "carlos soares", "eug\\'enio oliveira"], "accepted": false, "id": "1709.00947"}, "pdf": {"name": "1709.00947.pdf", "metadata": {"source": "CRF", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "authors": ["Pedro Saleiro", "L\u00fa\u0131s Sarmento", "Eduarda Mendes Rodrigues", "Carlos Soares", "Eug\u00e9nio Oliveira"], "emails": ["pssc@fe.up.pt"], "sections": [{"heading": "1 Introduction", "text": "However, there are several related challenges in terms of calculating and distributing word components in terms of: - intrinsic properties of embedding. How many dimensions do we actually need to store all the \"useful\" semantic information? How large should the embedded vocabulary be to have any practical value? - Type of model used to generate the embedding, and it is not obvious which are the \"best\" semantic information. - The size and properties of the training data are related to each other. - Type of model used to generate the embedding."}, {"heading": "2 Related Work", "text": "There are several approaches to creating word embeddings. One can build models that explicitly aim to create word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models that implicitly include such word embeddings in the process of solving other language tasks. Word embeddings methods aim to represent words as real rated continuous vectors in a much lower dimensional space when compared with traditional words. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the statistics that enable the creation of such representations."}, {"heading": "3 Our Neural Word Embedding Model", "text": "The neural word embedding model that we use in our experiments is strongly inspired by what is described in [5], but ours is one layer deeper and is set to solve a slightly different word prediction. Given a sequence of 5 words - wi \u2212 2 wi \u2212 1 wi + 2, the task the model is trying to perform is to predict the middle word, wi, based on the two words on the left - wi \u2212 2 wi \u2212 1 - and the two words on the right - wi + 2: P (wi \u2212 1, wi + 1, wi + 2), which should produce embedding that closely capture the distributive similarity so that the words belong to the same semantic class, or the antagonyms from each other are embedded in \"narrow\" regions. \""}, {"heading": "4 Experimental Setup", "text": "We are interested in assessing two aspects of the word embedding process: on the one hand, we would like to evaluate the semantic quality of the embedded embeds produced, and on the other hand, we want to quantify how much computing power and training data is required to train the embedding model according to the size of the vocabulary we are trying to embed. These aspects have fundamental practical implications for deciding how we should try to create the large database 1 https: / / github.com / sarmento / syntagmaof embedings that we will provide in the future. All the resources developed in this work are publicly available.Apart from the size of the vocabulary to be processed (| V |), the hyperparameters of the model that we could potentially explore are i) the dimensions of the embedded words and ii) the dimensions of the output word embedings."}, {"heading": "4.1 Training Data", "text": "In this process, the entire text has been pushed into the background. To help anonymize the ngram information, we have replaced all Twitter actions with an artificial token. \"We have also replaced all HTTP links with the token,\" he says. \"We have replaced all HTTP links with the token.\" \"We have two special tokens to complete the 5 gram information generated by the first two words of the tweet, and we have added two more special tokens to complete the 5 gram link.\" \"We have all HTTP links with the token.\" \"We have two special tokens to complete the 5 gram link.\" \"We have two special tokens to complete the 5 gram link.\" \"We have two more special tokens to complete the 5 gram link.\""}, {"heading": "4.2 Metrics related with the Learning Process", "text": "We tracked indicators that related to the learning process itself, depending on the vocabulary size that was to be embedded | V | and the proportion of training data used (25%, 50%, 75% and 100%).For all possible configurations, we recorded the values of training and validation loss (cross entropy) after each epoch. Tracking these indicators serves as a minimalist test of reason: If the model is unable to solve the word prediction task with some success (e.g. if we do not observe a significant decrease in losses), one should not expect the embedding to capture any of the distribution information it is intended to capture."}, {"heading": "4.3 Tests and Gold-Standard Data for Intrinsic Evaluation", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Results and Analysis", "text": "We perform the training process and introduce the appropriate evaluation for 12 combinations of the size of the vocabulary that needs to be embedded and the volume of available training data that has been used. Table 2 presents some general statistics after training for 40 epochs. The average time per epoch increases first with the size of the vocabulary, then has more parameters."}, {"heading": "5.1 Intrinsic Evaluation", "text": "Table 3 presents results for the three different tests described in Section 4. The first (expected) result is that the coverage metrics increase with the size of the embedded vocabulary | V | = 32768 words we achieve a test coverage of almost 90%. On the other hand, for the test set class differentiation it is important to compile gold standard data directly from Twitter content if we want to perform a more precise evaluation of the test cases of each class in the test set class membership - we get very low coverage numbers. This suggests that it is not always possible to reuse previously compiled gold standard data, and that it will be important to compile gold standard data directly from Twitter content if we want to perform a more accurate evaluation. The effect of the variation in the cosmic similarity threshold when deciding between 0.70 and 0.80 for the test class membership V shows that the percentage of test cases classified as correct drops significantly."}, {"heading": "5.2 Further Analysis regarding Evaluation Metrics", "text": "Although we already provide interesting practical clues to our goal of embedding a larger vocabulary by using more of the training data available to us, these results have also shown that the intrinsic evaluation metrics we use are hypersensitive to their corresponding cosine similarity thresholds. This sensitivity poses serious challenges to further systematic research into the word embedding architectures and their corresponding hyperparameters, which has also been observed in other recent work [16]. By using these absolute thresholds as criteria for determining word similarity, we create a dependence between the evaluation metrics and the geometry of the embedded data. If we consider the embedding data as graphs, this means that metrics change when we apply scaling operations to certain parts of the graph, even if its structure (i.e. the relative position of the embedded words) does not change. For most practical purposes (including the formation of downstream ML models), metrics have little absolute significance."}, {"heading": "6 Conclusions", "text": "The production of word embedding from tweets is challenging due to the specifics of the vocabulary in the medium. We have implemented a neural word embedding model that embeds words based on N-gram information extracted from a sample of the Portuguese Twitter stream, which can be considered a flexible starting point for further experiments in the field. The work reported in this paper is a preliminary study of the attempt to find parameters for embedding training words from Twitter and adequate evaluation tests and gold standard data. Results show that the use of less than 50% of the available training examples for each vocabulary size could lead to overadjustment. The resulting embedding receives an interesting performance in intrinsic evaluation tests when a vocabulary with the 32768 most common words is formed in a relatively small Twitter sample size. Nevertheless, the resulting embedding receives an interesting performance in intrinsic evaluation tests with a relatively small number of available vocabulary examples for each 2768 training examples."}], "references": [{"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of machine learning research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Hlt-naacl,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Randwalk: A latent variable model approach to word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1502.03520,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Semeval-2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov"], "venue": "Proceedings of SemEval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Lx-dsemvectors: Distributional semantics models for portuguese", "author": ["Jo\u00e3o Rodrigues", "Ant\u00f3nio Branco", "Steven Neale", "Jo\u00e3o Silva"], "venue": "In International Conference on Computational Processing of the Portuguese Language,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Twitterecho: a distributed focused crawler to support open research with twitter data", "author": ["Matko Bo\u0161njak", "Eduardo Oliveira", "Jos\u00e9 Martins", "Eduarda Mendes Rodrigues", "L\u00fa\u0131s Sarmento"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "venue": "arXiv preprint arXiv:1412.6568,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Problems with evaluation of word embeddings using word similarity tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer"], "venue": "ACL 2016,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Intrinsic evaluations of word embeddings: What can we do better", "author": ["Anna Gladkova", "Aleksandr Drozd", "Computing Center"], "venue": "ACL 2016,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Not only the space of possibilities for each of these aspects is large, there are also challenges in performing a consistent large-scale evaluation of the resulting embeddings [1].", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks.", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks.", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 4, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 5, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 6, "context": "Levy and Goldberg [7] showed that the two approaches are closely related.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Although, word embeddings research go back several decades, it was the recent developments of Deep Learning and the word2vec framework [2] that captured the attention of the NLP community.", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": "[8] showed that embeddings trained using word2vec models (CBOW and Skip-gram) exhibit linear structure, allowing analogy questions of the form \u201cman:woman::king:??.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "For instance, in the paper describing GloVe [3] authors trained their model on five corpora of different sizes and built a vocabulary of 400K most frequent words.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "[8] trained with 82K vocabulary while Mikolov et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] was trained with 3M vocabulary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed a generative model for learning embeddings that tries to explain some theoretical justification for nonlinear models (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "SemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that participants either used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or \u201cfrom some sort of dataset\u201d [10].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "[11] created and distributed the first general purpose embeddings for Portuguese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The neural word embedding model we use in our experiments is heavily inspired in the one described in [5], but ours is one layer deeper and is set to solve a slightly different word prediction task.", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "The model was implemented using the Syntagma library which relies on Keras [12] for model development, and we train the model using the built-in ADAM [13] optimizer with the default parameters.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "We randomly sampled 5M tweets from a corpus of 300M tweets collected from the Portuguese Twitter community [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work [15, 16].", "startOffset": 182, "endOffset": 190}, {"referenceID": 14, "context": "We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work [15, 16].", "startOffset": 182, "endOffset": 190}, {"referenceID": 10, "context": "This data was collected from the evaluation data provided by [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "This sensitivity poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters, which was also observed in other recent works [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "We are in line with recent work [17], proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from oversensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.", "creator": "LaTeX with hyperref package"}}}