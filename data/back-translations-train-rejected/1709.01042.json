{"id": "1709.01042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Getting Reliable Annotations for Sarcasm in Online Dialogues", "abstract": "The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger's, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply.", "histories": [["v1", "Mon, 4 Sep 2017 16:54:35 GMT  (276kb,D)", "http://arxiv.org/abs/1709.01042v1", "International Conference on Language Resources and Evaluation (LREC 2014)"]], "COMMENTS": "International Conference on Language Resources and Evaluation (LREC 2014)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["reid swanson", "stephanie lukin", "luke eisenberg", "thomas chase corcoran", "marilyn a walker"], "accepted": false, "id": "1709.01042"}, "pdf": {"name": "1709.01042.pdf", "metadata": {"source": "CRF", "title": "Getting Reliable Annotations for Sarcasm in Online Dialogues", "authors": ["Reid Swanson", "Stephanie Lukin", "Luke Eisenberg", "Thomas Chase Corcoran", "Marilyn A. Walker"], "emails": ["reid@soe.ucsc.edu,", "maw@soe.ucsc.edu"], "sections": [{"heading": null, "text": "Keywords: sarcasm, crowdsourcing, reliability"}, {"heading": "1. Introduction", "text": "In fact, most of us are able to survive on our own."}, {"heading": "2. Sarcasm Corpus and Models of Reliability", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Sarcasm Corpus", "text": "The initial IAC note included 10,003 QuoteResponse (Q-R) pairs, in which mechanics were shown seven Q-R pairs and asked if the answer was sarcastic or not. Examples of Q-R contributions are in Fig. 1 and Fig. 2. Turks were not given additional definitions of the meaning of sarcasm, for example, we let Turks apply their native intuitions about what it means to be sarcastic for a contribution, as previous work suggested that non-specialists tend to collapse all forms of verbal irony under the term sarcastic (Bryant and Fox Tree, 2002; Gibbs, 2000). For each of these 10,003 Q-R pairs, we collected notes from 5 to 7 mechanical Turks. Previous work had tested the reliability of mechanical Turker notes compared to expertly trained annotators for five different NLP tasks."}, {"heading": "2.2. Reliability Measures", "text": "In fact, it is the case that most of us are able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "4. Results", "text": "We will use the 25 new annotations to compare the different reliability measurements of our gold standard data in terms of accuracy as a function of the number of comments made by the Turks. We will also examine the implications of this study for the L & W sarcasm data. Finally, we will examine the value of additional annotations and the impact on future similar studies. Most workers received about 23 of the gold standard questions correctly. There were slightly more annotations that fell below the peak, but there were also many who sought the comparison, while there were many unreliable annotations in yellow."}, {"heading": "3 18 0.53 0.60 19 0.48 0.44", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 14 0.53 0.58 6 0.48 0.41", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 7 0.51 0.54 10 0.47 0.45", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 10 0.52 0.56 2 0.50 0.50", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 9 0.51 0.53 4 0.49 0.48", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "13 6 0.51 0.52 5 0.48 0.47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "15 7 0.51 0.54 3 0.48 0.46", "text": "The return for the 15 comments above 10 is low, and some examples of this type of comment can be found in Fig. 2. Interestingly, there are also comments (R4 of Fig. 2) that remain ambiguous even at 30 comments. Table 3 also shows that the threshold we use for Karger's comments is much more conservative than the initial threshold of L & W. To get a better understanding of how many comments we should collect in the future before we reach a reduction in yields, we have also examined how ambiguous cases change comments as more comments are provided. Fig. 11 shows 101 positions that were most ambiguous after 3 comments from Karger's algorithm, with the same sampling method as Fig. 8 and then the traces of the comments change when more comments are provided."}, {"heading": "5. Discussion and Future Work", "text": "We report the results of a detailed annotation study on sarcasm in Mechanical Turk using different reliability measurements. Unfortunately, Question O1 is still open: while at the end of the study only 6 points remain unclear when using the voting method, the overlap between these and the L & W data is small (262 points). We do not have the results for the final vote on all 600 points in relation to the original L & W. We suspect that the answer to Question O3 is that sarcasm may be more difficult to obtain reliable annotations and that some statements may be intentionally sarcastic. However, it appears that 7 Turks are sufficient to agree on a label category, which means that the answer to Question O3 is no, as previous work has suggested that 7 comments for subjective tasks are sufficient to obtain reliable annotations (Snow et al., 2008; Callisonch, 2009), our results for approximately 7 categories conform to sarcasm."}, {"heading": "6. References", "text": "s Mechanical Turk. In Proc. of the 2009 Conf. on Empirical Methods in Natural Language Processing, p. 286-295. Dagan, I., O., and Magnini, B. (2006). The PASCAL recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, Notes in Computer Science 3944, p. 177-190. Daviv D., D. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment."}], "references": [{"title": "Politeness: Some universals in language usage", "author": ["P. Brown", "S. Levinson"], "venue": "Cambridge University Press.", "citeRegEx": "Brown and Levinson,? 1987", "shortCiteRegEx": "Brown and Levinson", "year": 1987}, {"title": "Recognizing verbal irony in spontaneous speech", "author": ["G. Bryant", "J. Fox Tree"], "venue": "Metaphor and symbol, 17(2):99\u2013119.", "citeRegEx": "Bryant and Tree,? 2002", "shortCiteRegEx": "Bryant and Tree", "year": 2002}, {"title": "Fast, cheap, and creative: Evaluating translation quality using Amazon\u2019s Mechanical Turk", "author": ["C. Callison-Burch"], "venue": "Proc. of the 2009 Conf. on Empirical Methods in Natural Language Processing, p. 286\u2013295.", "citeRegEx": "Callison.Burch,? 2009", "shortCiteRegEx": "Callison.Burch", "year": 2009}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, Lecture Notes in Computer Science", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Semisupervised recognition of sarcastic sentences in twitter and amazon", "author": ["D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "Proc. of the Fourteenth Conf. on Computational Natural Language Learning, p. 107\u2013116.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied Statistics, p. 20\u201328.", "citeRegEx": "Dawid and Skene,? 1979", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "The kappa statistic: A second look", "author": ["B. Di Eugenio", "M. Glass"], "venue": "Computational linguistics, 30(1):95\u2013 101.", "citeRegEx": "Eugenio and Glass,? 2004", "shortCiteRegEx": "Eugenio and Glass", "year": 2004}, {"title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing", "author": ["E. Filatova"], "venue": "Language Resources and Evaluation Conf. , LREC2012.", "citeRegEx": "Filatova,? 2012", "shortCiteRegEx": "Filatova", "year": 2012}, {"title": "Irony in talk among friends", "author": ["R. Gibbs"], "venue": "Metaphor and Symbol, 15(1):5\u201327.", "citeRegEx": "Gibbs,? 2000", "shortCiteRegEx": "Gibbs", "year": 2000}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["R. Gonz\u00e1lez-Ib\u00e1\u00f1ez", "S. Muresan", "N. Wacholder"], "venue": "Proc. of the 49th Annual Meeting of the Association for Computational Linguistics, volume 2, p. 581\u2013586.", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.", "year": 2011}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah"], "venue": "Advances in neural information processing systems, p. 1953\u20131961.", "citeRegEx": "Karger et al\\.,? 2011", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "Some pre-observations on the modelling of dialogue", "author": ["S.C. Levinson"], "venue": "Discourse Processes, 4:93\u2013116.", "citeRegEx": "Levinson,? 1981", "shortCiteRegEx": "Levinson", "year": 1981}, {"title": "What\u2019s special about conversational inference", "author": ["S.C. Levinson"], "venue": "1987 Linguistics Institute Packet.", "citeRegEx": "Levinson,? 1985", "shortCiteRegEx": "Levinson", "year": 1985}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler"], "venue": "Advances in Neural Information Processing Systems 25, p. 701\u2013709.", "citeRegEx": "Liu et al\\.,? 2012", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Automatic identification of general and specific sentences by leveraging discourse annotations", "author": ["A. Louis", "A. Nenkova"], "venue": "IJCNLP, p. 605\u2013613.", "citeRegEx": "Louis and Nenkova,? 2011", "shortCiteRegEx": "Louis and Nenkova", "year": 2011}, {"title": "Really? Well", "author": ["S. Lukin", "M. Walker"], "venue": "Apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. Language and Social Media Workshop, NAACL 2013, p. 30.", "citeRegEx": "Lukin and Walker,? 2013", "shortCiteRegEx": "Lukin and Walker", "year": 2013}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["E. Riloff", "A. Qadir", "P. Surve", "L. De Silva", "N. Gilbert", "R. Huang"], "venue": null, "citeRegEx": "Riloff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A. Ng"], "venue": "In Proc. of the Conf. on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "A corpus for research on deliberation and debate", "author": ["M. Walker", "P. Anand", "R. Abbott", "J.E. Fox Tree"], "venue": "In Language Resources and Evaluation", "citeRegEx": "Walker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Whether the aim is to engineer automatic methods for reliably detecting sarcasm or to further develop or test theories of sarcasm in dialogue (Bryant and Fox Tree, 2002; Gibbs, 2000), a major challenge is simply the difficulty of getting enough reliably labelled examples.", "startOffset": 142, "endOffset": 182}, {"referenceID": 18, "context": "In previous work, we released the Internet Argument Corpus (IAC), a large corpus of online social and political dialogues (Walker et al., 2012).", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "In our previous work on automatic sarcasm classification, henceforth referred to as L&W (Lukin and Walker, 2013), we used a threshhold defined as two annotators said it was sarcastic to define the subset of the corpus labeled as sarcastic.", "startOffset": 88, "endOffset": 112}, {"referenceID": 17, "context": "\u2022 O3: Do annotations of subjectivity in dialogue require more annotators to achieve reliability than the NLP tasks of previous work (Snow et al., 2008; CallisonBurch, 2009)?", "startOffset": 132, "endOffset": 172}, {"referenceID": 0, "context": "2 might have been deliberately constructed by the speaker to be ambiguous, in the same way that indirect speech acts may be constructed to be ambiguous (Brown and Levinson, 1987; Levinson, 1981; Levinson, 1985).", "startOffset": 152, "endOffset": 210}, {"referenceID": 11, "context": "2 might have been deliberately constructed by the speaker to be ambiguous, in the same way that indirect speech acts may be constructed to be ambiguous (Brown and Levinson, 1987; Levinson, 1981; Levinson, 1985).", "startOffset": 152, "endOffset": 210}, {"referenceID": 12, "context": "2 might have been deliberately constructed by the speaker to be ambiguous, in the same way that indirect speech acts may be constructed to be ambiguous (Brown and Levinson, 1987; Levinson, 1981; Levinson, 1985).", "startOffset": 152, "endOffset": 210}, {"referenceID": 5, "context": "We apply different reliability measures to the same data, including majority class, Karger\u2019s, Kappa and EM (Di Eugenio and Glass, 2004; Dawid and Skene, 1979; Karger et al., 2011).", "startOffset": 107, "endOffset": 179}, {"referenceID": 10, "context": "We apply different reliability measures to the same data, including majority class, Karger\u2019s, Kappa and EM (Di Eugenio and Glass, 2004; Dawid and Skene, 1979; Karger et al., 2011).", "startOffset": 107, "endOffset": 179}, {"referenceID": 9, "context": "Previous work on sarcasm in Twitter has mainly assumed that the user-generated #sarcasm tag reliably identifies sarcastic utterances (Gonz\u00e1lez-Ib\u00e1\u00f1ez et al., 2011; Davidov et al., 2010), although a recent study by (Riloff et al.", "startOffset": 133, "endOffset": 185}, {"referenceID": 4, "context": "Previous work on sarcasm in Twitter has mainly assumed that the user-generated #sarcasm tag reliably identifies sarcastic utterances (Gonz\u00e1lez-Ib\u00e1\u00f1ez et al., 2011; Davidov et al., 2010), although a recent study by (Riloff et al.", "startOffset": 133, "endOffset": 185}, {"referenceID": 16, "context": ", 2010), although a recent study by (Riloff et al., 2013) found that only 45% of the utterances tagged as #sarcasm in a large corpus of Twitter utterances were judged by human annotators to be sarcastic without any prior context.", "startOffset": 36, "endOffset": 57}, {"referenceID": 7, "context": "(Filatova, 2012) reports a crowdsourcing study for identifying sarcasm in product reviews on Amazon, and describes a procedure for achieving a corpus with highly reliable labels, but does not actually report reliability statistics.", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "we let Turkers use their native intuitions about what it means for a post to be sarcastic, since previous work suggests that non-specialists tend to collapse all forms of verbal irony under the term sarcastic (Bryant and Fox Tree, 2002; Gibbs, 2000).", "startOffset": 209, "endOffset": 249}, {"referenceID": 17, "context": "compared to expertly trained annotators for five different NLP tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation (Snow et al., 2008).", "startOffset": 194, "endOffset": 213}, {"referenceID": 14, "context": "does it make worse predictions on low confidence items (Louis and Nenkova, 2011)?", "startOffset": 55, "endOffset": 80}, {"referenceID": 10, "context": "(Karger et al., 2011) prove that majority voting is suboptimal and can be significantly improved upon.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Karger\u2019s algorithm (Karger et al., 2011) is an iterative message passing algorithm that attempts to address this issue.", "startOffset": 19, "endOffset": 40}, {"referenceID": 13, "context": "(Liu et al., 2012) have argued that Karger\u2019s method suffers several shortcomings that cause it to perform poorly on several real world NLP annotation tasks, such as the Recognizing Textual Entailment challenge (Dagan et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": ", 2012) have argued that Karger\u2019s method suffers several shortcomings that cause it to perform poorly on several real world NLP annotation tasks, such as the Recognizing Textual Entailment challenge (Dagan et al., 2006).", "startOffset": 199, "endOffset": 219}, {"referenceID": 5, "context": "An alternative probabilistic method was developed by Dawid and Skene (Dawid and Skene, 1979).", "startOffset": 69, "endOffset": 92}, {"referenceID": 15, "context": "2 provides examples of response posts that are initially categorized differently by Karger\u2019s and by the threshhold method of (Lukin and Walker, 2013).", "startOffset": 125, "endOffset": 149}, {"referenceID": 12, "context": "that the interpretative process to recognize sarcasm is similar to that for indirect speech acts (Levinson, 1985).", "startOffset": 97, "endOffset": 113}, {"referenceID": 17, "context": "This means that the answer to question O3 is no, because previous work suggested that for subjective tasks, 7 annotators is enough to achieve reliable annotations (Snow et al., 2008; CallisonBurch, 2009): our results that labelling categories for sarcasm converge at around 7 to 10 annotators.", "startOffset": 163, "endOffset": 203}], "year": 2017, "abstractText": "The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger\u2019s, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply.", "creator": "LaTeX with hyperref package"}}}