{"id": "1709.01058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "A Unified Query-based Generative Model for Question Generation and Question Answering", "abstract": "We propose a query-based generative model for solving both tasks of question generation (QG) and question an- swering (QA). The model follows the classic encoder- decoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple per- spectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major prob- lem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher per- formances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong ex- tractive QA system. In addition, our model shows bet- ter performance than the state-of-the-art baselines of the generative QA task.", "histories": [["v1", "Mon, 4 Sep 2017 17:54:49 GMT  (288kb,D)", "http://arxiv.org/abs/1709.01058v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "zhiguo wang", "wael hamza"], "accepted": false, "id": "1709.01058"}, "pdf": {"name": "1709.01058.pdf", "metadata": {"source": "CRF", "title": "A Unified Query-based Generative Model for Question Generation and Question Answering", "authors": ["Linfeng Song", "Zhiguo Wang", "Wael Hamza"], "emails": [], "sections": [{"heading": "Introduction", "text": "The task of the question generation (QA) is to generate a fluent and relevant question that gives a passage and a target answer, while the task of the question generation (QA) is to generate a correct answer to a passage and a question. However, both tasks have massive industrial values: QA has been used in industrial products such as search engines, while QG is helpful to improve the corresponding questions by automatically increasing the training data. It can also be used to generate questions for educational purposes such as language learning. For the QG task, the existing work is either completely ignored (Du, Shao, and Cardie, 2017) while generating the corresponding questions, or directly the hard codes term in the passage (Zhou et al., 2017; Yang et al.)., 2017; Subramanian et al., 2017; Tang et al., 2017; originator c."}, {"heading": "Model", "text": "Figure 1 shows the architecture of our model. The model takes two components as input: a passage P = (p1,..., pj,..., pN) of length N and a query Q = (q1,..., qi,..., qM) of length M, then generates word by word the output sequence X = (x1,..., xL). Specifically, the model follows the encoder decoder frame. The encoder matches each time step of the passage from several perspectives against all time steps of the query and encodes the matching result into a \"multi-perspective memory.\" In addition, the decoder generates the output sequence one word at a time based on the \"multi-perspective memory.\""}, {"heading": "Multi-Perspective Matching Encoder", "text": "The encoder initially represents all terms within the passage and the query with word embedding (Mikolov et al., 2013). However, to include contextual information in the representation of each time step of the passage or query, we use a bidirectional LSTM (BiLSTM) level to encode the passage and the source texture individually. \u2212 hqi = LSTM (\u2190 \u2212 hqi + 1, qi) \u2212 hqi = LSTM (\u2212 1, qi \u2212 qi) -hpj \u2212 hpj + 1, pj \u2212 hpj = hpj = LSTM (\u2212 hpj \u2212 1, pj \u2212 1), where Qi and pj incorporate the i-th word into the passage."}, {"heading": "LSTM Decoder", "text": "The right side of Figure 1 is our decoder. Essentially, it is an attention-based LSTM model = 1 = as far as 2016 (Bahdanau, Cho and Bengio, 2014) with copy and coverage mechanisms. The decoder takes the \"multi-perspective memory\" as attention memory and generates the output of a word at a given time. Specifically, the decoder considers five factors as input: (1) the \"multi-perspective memory\" H = {h0,..., hN}, where each vector aligns the i-th word in the passage, while the decoder considers the previous hidden state of the LSTM model to be \u2212 1; (3) the embedding of the previously generated word xt \u2212 1; (4) the previous context vector \u2212 1, which is calculated from the attention mechanism, where H is the attention mechanism."}, {"heading": "Policy Gradient Reinforcement Learning via Scheduled Sampling", "text": "A common method for creating a sequence generation model is to optimize the log probability of the gold standard output sequence Y * = y * 0,..., y * t,..., y * T with cross-entropy loss: lce = \u2212 T * t = 1log p (y * t | y * t \u2212 1,... y * 0, X *), where X is the model input and \u03b8 is the tractable model parameter.However, this method suffers from two main problems: First, during the training phase, the model must rely on the basic truth of the previous word y * t \u2212 1 as input in order to predict the probabilities of the next word yt. However, in the test phase, the basic truth y \u2212 1 is not available, and the previously generated word yt \u2212 1. If the model has chosen a different yt \u2212 1 as the basic truth y \u2212 1, the following generated sequence y \u2212 1 may differ from the gold standard sequence."}, {"heading": "12 end", "text": "Algorithm 1: Scheduled Sampling StrategySocher, 2017; Rennie et al., 2016). Formally, the loss function is defined as follows: lrl = (r (Y) \u2212 r (Y s))) T \u2211 t = 1log p (yst | yst \u2212 1,..., ys0, x; \u03b8), where Y s = ys0,..., y s T is the sampled sequence, Y is the sequence generated by a baseline, and the function r (Y) is the reward calculated on the evaluation metric. Intuitively, the loss function lrl increases the log probability of the sampled sequence Y s, if Y s is better than the baseline Y-words calculated by r (Y), or vice versa. In this work, for the QG task, we use the BLEU model sampling score (Papineni et al., 2002) as a reward, and for the QE, Y, the search, Y, the task we use, Y-GE-Y, and Y-strategy."}, {"heading": "Experimental Setup", "text": "In fact, the fact is that most of us are able to survive ourselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \""}, {"heading": "Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Question Generation", "text": "We compare our model with Du, Shao and Cardie al. (2017) and Zhou et al. (2017) to the question generation task, and show the results in Table 1. Since Zhou et al. (2017) adopts rich features (such as name entity tags and part-of-speech tags), we implement a version without these rich features (w / o rich feature) for fair comparison. We also implement two versions of our model: (1) MPQG is our model trained only with cross-entropy loss, and (2) MPQG + R is our model finely tuned with the political gradient enhancement learning algorithm according to pretraining.First, our MPQG model surpasses the comparison systems on both data columns, which show the effectiveness of our multi-perspective matching encoder encoder, and our model fine."}, {"heading": "Question Generation for Extractive QA", "text": "Table 3 shows the results to improve an extractive QA system with automatically generated questions. Here, F1 measures the overlap between the prediction and the reference in the form of tokens, and the exact match (EM) measures the percentage in which the prediction is identical to the reference (Rajpurkar et al., 2016). The baseline is trained only in the part in which gold questions are available, while the others are trained using the combination of the gold questions and the automatically generated questions, but using other methods for generating questions: (1) w / window, a strong baseline of Yang et al. (2017), uses the previous and the following 5 characters of the target answer as a pseudo question, and (2) w / MPQG + R generates questions with our MPQG + R model. First, we can see that w / MPQG + R exceeds the baseline values under all settings, but the values of MPG / R are not significantly better than the effectiveness of MPG / R, but that the results of MPR are from 24% of the questions to the effectiveness of MPG."}, {"heading": "Generative QA", "text": "For the generative QA experiment, we compare our model with the generative models in Nguyen et al. (2016) on the subset \"Description\" of the MS-MARCO dataset. Table 4 shows the corresponding performance. Of all the comparison methods, Best Passage selects the best passage with respect to the ROUGE-L score and obviously refers to the reference. Passage Ranking classifies the passage using a deeply structured semantic model from Huang et al. (2013). Sequence to Sequence is a vanilla sequence model (Sutskever, Vinyals and Le, 2014). Memory Network uses the end-to-end memory network (Sukhbaatar et al., 2015) as encoder and a vanilla RNN model as decoder. We also implement a base system \"Vanilla-Cosine,\" which only exceeds the vanilla-cosine similarity for the matching function fm in our encoder and a vanilla RNN model as decoder."}, {"heading": "Related Work", "text": "In addressing this issue (QG), our work extends to previous work (Du, Shao and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan and Trischler, 2017; Yuan et al., 2017) by performing query understanding. Tang et al. (2017); Yang et al. (2017) links the QG task with the QA task, but they still perform the QG task, the only difference being that they directly optimize QA performance rather than as a general metric (such as BLEU). On the other hand, our model can perform both QG and QA tasks, but most previous work (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; QA et al."}, {"heading": "Conclusion", "text": "In this paper, we presented a query-based generative model that can be used for both question generation and answer. Following the encoder decoder framework, a multi-perspective matching encoder was developed to perform query and passage understanding, and an LSTM model with coverage and copy mechanisms was used as a decoder to generate the target sequence. In addition, we used a policy gradient learning algorithm to mitigate the exposure bias problem that generative models suffer from when they are trained with cross-entropy loss. Experiments on both question generation and question-answer tasks show superior performance of our model, which exceeds the state-of-the-art models. From the results, we conclude that query understanding is important for question generation and that policy gradient is effective in addressing the problem of exposure training, which is cross-entropy resulting from loss."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.", "citeRegEx": "Banerjee and Lavie,? 2005", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "NIPS 2015, 1171\u20131179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Reading wikipedia to answer open-domain questions", "author": ["D. Chen", "A. Fisch", "J. Weston", "A. Bordes"], "venue": "Proceedings of ACL 2017.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Gated-attention readers for text comprehension", "author": ["B. Dhingra", "H. Liu", "Z. Yang", "W. Cohen", "R. Salakhutdinov"], "venue": "Proceedings of ACL 2017.", "citeRegEx": "Dhingra et al\\.,? 2017", "shortCiteRegEx": "Dhingra et al\\.", "year": 2017}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["X. Du", "J. Shao", "C. Cardie"], "venue": "arXiv preprint arXiv:1705.00106.", "citeRegEx": "Du et al\\.,? 2017", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["J. Gu", "Z. Lu", "H. Li", "V.O. Li"], "venue": "Proceedings of ACL 2017.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["C. Gulcehre", "S. Ahn", "R. Nallapati", "B. Zhou", "Y. Bengio"], "venue": "Proceedings of ACL 2017.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "Proceedings of CIKM 2013, 2333\u20132338.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["K. Lee", "S. Salant", "T. Kwiatkowski", "A. Parikh", "D. Das", "J. Berant"], "venue": "arXiv preprint arXiv:1611.01436.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain.", "citeRegEx": "Lin and C..Y.,? 2004", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "A coverage embedding model for neural machine translation", "author": ["H. Mi", "B. Sankaran", "Z. Wang", "A. Ittycheriah"], "venue": "EMNLP 2016.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "MS MARCO: A human generated machine reading comprehension dataset", "author": ["T. Nguyen", "M. Rosenberg", "X. Song", "J. Gao", "S. Tiwary", "R. Majumder", "L. Deng"], "venue": "arXiv preprint arXiv:1611.09268.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A deep reinforced model for abstractive summarization", "author": ["R. Paulus", "C. Xiong", "R. Socher"], "venue": "arXiv preprint arXiv:1705.04304.", "citeRegEx": "Paulus et al\\.,? 2017", "shortCiteRegEx": "Paulus et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Proceedings of EMNLP 2014, 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "Proceedings of EMNLP 2016, 2383\u20132392.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Self-critical sequence training for image captioning", "author": ["S.J. Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": "arXiv preprint arXiv:1612.00563.", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "Get to the point: Summarization with pointer-generator networks", "author": ["A. See", "P.J. Liu", "C.D. Manning"], "venue": "arXiv preprint arXiv:1704.04368.", "citeRegEx": "See et al\\.,? 2017", "shortCiteRegEx": "See et al\\.", "year": 2017}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "arXiv preprint arXiv:1611.01603.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Y. Shen", "P.-S. Huang", "J. Gao", "W. Chen"], "venue": "arXiv preprint arXiv:1609.05284.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Neural models for key phrase detection and question generation", "author": ["S. Subramanian", "T. Wang", "X. Yuan", "A. Trischler"], "venue": "arXiv preprint arXiv:1706.04560.", "citeRegEx": "Subramanian et al\\.,? 2017", "shortCiteRegEx": "Subramanian et al\\.", "year": 2017}, {"title": "End-toend memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R Fergus"], "venue": "NIPS", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS 2014, 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "S-net: From answer extraction to answer generation for machine reading comprehension", "author": ["C. Tan", "F. Wei", "N. Yang", "W. Lv", "M. Zhou"], "venue": "arXiv preprint arXiv:1706.04815.", "citeRegEx": "Tan et al\\.,? 2017", "shortCiteRegEx": "Tan et al\\.", "year": 2017}, {"title": "Question answering and question generation as dual tasks", "author": ["D. Tang", "N. Duan", "T. Qin", "M. Zhou"], "venue": "arXiv preprint arXiv:1706.02027.", "citeRegEx": "Tang et al\\.,? 2017", "shortCiteRegEx": "Tang et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "arXiv preprint arXiv:1601.04811.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["S. Wang", "J. Jiang"], "venue": "arXiv preprint arXiv:1608.07905.", "citeRegEx": "Wang and Jiang,? 2016", "shortCiteRegEx": "Wang and Jiang", "year": 2016}, {"title": "Multiperspective context matching for machine comprehension", "author": ["Z. Wang", "H. Mi", "W. Hamza", "R. Florian"], "venue": "arXiv preprint arXiv:1612.04211.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["W. Wang", "N. Yang", "F. Wei", "B. Chang", "M. Zhou"], "venue": "Proceedings of ACL 2017.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Bilateral multiperspective matching for natural language sentences", "author": ["Z. Wang", "W. Hamza", "R. Florian"], "venue": "IJCAI 2017.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "A joint model for question answering and question generation", "author": ["T. Wang", "X. Yuan", "A. Trischler"], "venue": "arXiv preprint arXiv:1706.01450.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Dynamic coattention networks for question answering", "author": ["C. Xiong", "V. Zhong", "R. Socher"], "venue": "arXiv preprint arXiv:1611.01604.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Semi-supervised qa with generative domainadaptive nets", "author": ["Z. Yang", "J. Hu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "arXiv preprint arXiv:1702.02206.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Neural generative question answering", "author": ["J. Yin", "X. Jiang", "Z. Lu", "L. Shang", "H. Li", "X. Li"], "venue": "arXiv preprint arXiv:1512.01337.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Y. Yu", "W. Zhang", "K. Hasan", "M. Yu", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1610.09996.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Machine comprehension by text-to-text neural question generation", "author": ["X. Yuan", "T. Wang", "C. Gulcehre", "A. Sordoni", "P. Bachman", "S. Subramanian", "S. Zhang", "A. Trischler"], "venue": "arXiv preprint arXiv:1705.02012.", "citeRegEx": "Yuan et al\\.,? 2017", "shortCiteRegEx": "Yuan et al\\.", "year": 2017}, {"title": "Neural question generation from text: A preliminary study", "author": ["Q. Zhou", "N. Yang", "F. Wei", "C. Tan", "H. Bao", "M. Zhou"], "venue": "arXiv preprint arXiv:1704.01792.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 31, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 23, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 32, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 3, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 22, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 11, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 39, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 4, "context": "For the QA task, most of the existing literatures (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, where they assume that the target answer occurs in the passage verbatim.", "startOffset": 50, "endOffset": 255}, {"referenceID": 7, "context": "Our model follows the classic encoder-decoder framework, where the encoder takes a passage and a query as input, then performs query understanding by matching the query with the passage from multiple perspectives, and the decoder is an attention-based LSTM model with copy (Gulcehre et al., 2016; Gu et al., 2016; See, Liu, and Manning, 2017) and coverage (Tu et al.", "startOffset": 273, "endOffset": 342}, {"referenceID": 6, "context": "Our model follows the classic encoder-decoder framework, where the encoder takes a passage and a query as input, then performs query understanding by matching the query with the passage from multiple perspectives, and the decoder is an attention-based LSTM model with copy (Gulcehre et al., 2016; Gu et al., 2016; See, Liu, and Manning, 2017) and coverage (Tu et al.", "startOffset": 273, "endOffset": 342}, {"referenceID": 29, "context": ", 2016; See, Liu, and Manning, 2017) and coverage (Tu et al., 2016; Mi et al., 2016) mechanisms.", "startOffset": 50, "endOffset": 84}, {"referenceID": 13, "context": ", 2016; See, Liu, and Manning, 2017) and coverage (Tu et al., 2016; Mi et al., 2016) mechanisms.", "startOffset": 50, "endOffset": 84}, {"referenceID": 2, "context": "In our policy-gradient reinforcement learning algorithm, we adopt a similar sampling strategy as the scheduled sampling strategy (Bengio et al., 2015) for generating the sampled output.", "startOffset": 129, "endOffset": 150}, {"referenceID": 19, "context": "We perform experiments on the SQuAD dataset (Rajpurkar et al., 2016) for the QG task, and on the \u201cdescription\u201d subset of the MSMARCO (Nguyen et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 15, "context": ", 2016) for the QG task, and on the \u201cdescription\u201d subset of the MSMARCO (Nguyen et al., 2016) dataset for the generative QA task.", "startOffset": 72, "endOffset": 93}, {"referenceID": 14, "context": "The encoder first represents all words within the passage and the query with word embeddings (Mikolov et al., 2013).", "startOffset": 93, "endOffset": 115}, {"referenceID": 8, "context": "In order to incorporate contextual information into the representation of each time-step of the passage or the query, we utilize a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) layer to encode the passage and the query individually:", "startOffset": 160, "endOffset": 194}, {"referenceID": 31, "context": "Inspired by Wang et al. (2016), we adopt the multi-perspective cosine matching function defined as:", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "On top of the LSTM decoder, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See, Liu, and Manning, 2017) to integrate the attention distribution into the final vocabulary distribution.", "startOffset": 56, "endOffset": 125}, {"referenceID": 6, "context": "On top of the LSTM decoder, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See, Liu, and Manning, 2017) to integrate the attention distribution into the final vocabulary distribution.", "startOffset": 56, "endOffset": 125}, {"referenceID": 35, "context": "Concretely, we adopt the \u201cREINFORCE with a baseline\u201d algorithm (Williams, 1992), a wellknown policy-gradient reinforcement learning algorithm, to train our model, because it has shown the effectiveness for several sequence generation tasks (Paulus, Xiong, and Data: gold-standard sequence Y \u2217", "startOffset": 63, "endOffset": 79}, {"referenceID": 16, "context": "In this work, for the QG task, we use the BLEU score (Papineni et al., 2002) as the reward, and for the QA task, we use the ROUGE score (Lin, 2004) as the reward.", "startOffset": 53, "endOffset": 76}, {"referenceID": 15, "context": "In this work, for the QG task, we use the BLEU score (Papineni et al., 2002) as the reward, and for the QA task, we use the ROUGE score (Lin, 2004) as the reward. Following Rennie et al. (2016), we take the greedy search result from the current model as the baseline sequence \u0176 .", "startOffset": 54, "endOffset": 194}, {"referenceID": 15, "context": "In this work, for the QG task, we use the BLEU score (Papineni et al., 2002) as the reward, and for the QA task, we use the ROUGE score (Lin, 2004) as the reward. Following Rennie et al. (2016), we take the greedy search result from the current model as the baseline sequence \u0176 . Rennie et al. (2016) generated the sampled sequence Y s according to the probability distribution of p(y t |y t\u22121, .", "startOffset": 54, "endOffset": 301}, {"referenceID": 2, "context": "Inspired by Bengio et al. (2015), we designed a new \u201cScheduled Sampling\u201d strategy to construct the sampled sequence Y s from both the gold-standard sequence Y \u2217 and the greedy search se-", "startOffset": 12, "endOffset": 33}, {"referenceID": 20, "context": "Our experiments show that sampling the sequence according to the model distribution, as Rennie et al. (2016) does, usually produces outputs worse than the greedy search sequence, so it does not help very much.", "startOffset": 88, "endOffset": 109}, {"referenceID": 41, "context": "75 \u2013 Zhou et al. (2017) \u2013 \u2013 \u2013 13.", "startOffset": 5, "endOffset": 24}, {"referenceID": 41, "context": "*There is no published scores for Zhou et al. (2017) without the rich features, so we re-implemented their system and show the result.", "startOffset": 34, "endOffset": 53}, {"referenceID": 16, "context": "Question Generation For the QG task, we evaluate the quality of generated questions with some automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as well as their effectiveness in improving an extractive QA system.", "startOffset": 136, "endOffset": 159}, {"referenceID": 19, "context": "We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016) by comparing our model with Du, Shao, and Cardie (2017) and Zhou et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 1, "context": "(2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE.", "startOffset": 32, "endOffset": 58}, {"referenceID": 15, "context": "Question Generation For the QG task, we evaluate the quality of generated questions with some automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as well as their effectiveness in improving an extractive QA system. We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016) by comparing our model with Du, Shao, and Cardie (2017) and Zhou et al.", "startOffset": 137, "endOffset": 377}, {"referenceID": 15, "context": "Question Generation For the QG task, we evaluate the quality of generated questions with some automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as well as their effectiveness in improving an extractive QA system. We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016) by comparing our model with Du, Shao, and Cardie (2017) and Zhou et al. (2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE.", "startOffset": 137, "endOffset": 400}, {"referenceID": 1, "context": "(2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE. The dataset contains 536 articles and over 100k questions related to the articles. Here, we follow Du, Shao, and Cardie (2017) and Zhou et al.", "startOffset": 33, "endOffset": 197}, {"referenceID": 1, "context": "(2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE. The dataset contains 536 articles and over 100k questions related to the articles. Here, we follow Du, Shao, and Cardie (2017) and Zhou et al. (2017) to conduct experiments on the accessible part as our entire dataset.", "startOffset": 33, "endOffset": 220}, {"referenceID": 1, "context": "(2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE. The dataset contains 536 articles and over 100k questions related to the articles. Here, we follow Du, Shao, and Cardie (2017) and Zhou et al. (2017) to conduct experiments on the accessible part as our entire dataset. Since Du, Shao, and Cardie (2017) and Zhou et al.", "startOffset": 33, "endOffset": 323}, {"referenceID": 1, "context": "(2017) in terms of BLEU, METEOR (Banerjee and Lavie, 2005) and ROUGE. The dataset contains 536 articles and over 100k questions related to the articles. Here, we follow Du, Shao, and Cardie (2017) and Zhou et al. (2017) to conduct experiments on the accessible part as our entire dataset. Since Du, Shao, and Cardie (2017) and Zhou et al. (2017) conducted their experiments using different training/dev/test split, we conduct experiments on both splits, and compare with their reported performance.", "startOffset": 33, "endOffset": 346}, {"referenceID": 31, "context": "The extractive QA system we choose is Wang et al. (2016), but our framework does not make any assumptions about the extractive QA systems being used.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "Generative QA For this task, we conduct experiments on the MS MARCO dataset (Nguyen et al., 2016), which contains around 100k queries and 1M passages.", "startOffset": 76, "endOffset": 97}, {"referenceID": 10, "context": "Adam (Kingma and Ba, 2014) is used for parameter optimization, and the learning rate is set to 0.", "startOffset": 5, "endOffset": 26}, {"referenceID": 14, "context": "Therefore, for the generative QA experiments, we follow Nguyen et al. (2016) to conduct experiments on the \u201cdescription\u201d subset, and compare with their reported results.", "startOffset": 56, "endOffset": 77}, {"referenceID": 41, "context": "We compare our model with Du, Shao, and Cardie (2017) and Zhou et al. (2017) on the question generation task, and show the results in Table 1.", "startOffset": 58, "endOffset": 77}, {"referenceID": 41, "context": "We compare our model with Du, Shao, and Cardie (2017) and Zhou et al. (2017) on the question generation task, and show the results in Table 1. Since Zhou et al. (2017) adopts rich features (such as name entity tags and part-of-speech tags), we re-implement a version without these rich features (w/o rich feature) for fair comparison.", "startOffset": 58, "endOffset": 168}, {"referenceID": 41, "context": "We compare our model with Du, Shao, and Cardie (2017) and Zhou et al. (2017) on the question generation task, and show the results in Table 1. Since Zhou et al. (2017) adopts rich features (such as name entity tags and part-of-speech tags), we re-implement a version without these rich features (w/o rich feature) for fair comparison. We also implement two versions of our model: (1) MPQG is our model only trained with the cross-entropy loss, and (2) MPQG+R is our model fine-tuned with the policy gradient reinforcement learning algorithm after pretraining. First, our MPQG model outperforms the comparing systems on both data splits, which shows the effectiveness of our multi-perspective matching encoder. Our MPQG model, which only takes word features, shows even better performance than the feature-rich (POS tag and NER) system of Zhou et al. (2017). Du, Shao, and Cardie (2017) utilized the sequence-to-sequence model to take the passage as input and then generate the questions, where they entirely ignored the", "startOffset": 58, "endOffset": 857}, {"referenceID": 41, "context": "We compare our model with Du, Shao, and Cardie (2017) and Zhou et al. (2017) on the question generation task, and show the results in Table 1. Since Zhou et al. (2017) adopts rich features (such as name entity tags and part-of-speech tags), we re-implement a version without these rich features (w/o rich feature) for fair comparison. We also implement two versions of our model: (1) MPQG is our model only trained with the cross-entropy loss, and (2) MPQG+R is our model fine-tuned with the policy gradient reinforcement learning algorithm after pretraining. First, our MPQG model outperforms the comparing systems on both data splits, which shows the effectiveness of our multi-perspective matching encoder. Our MPQG model, which only takes word features, shows even better performance than the feature-rich (POS tag and NER) system of Zhou et al. (2017). Du, Shao, and Cardie (2017) utilized the sequence-to-sequence model to take the passage as input and then generate the questions, where they entirely ignored the", "startOffset": 58, "endOffset": 886}, {"referenceID": 41, "context": "The baseline is our implementation of Zhou et al. (2017) without rich features.", "startOffset": 38, "endOffset": 57}, {"referenceID": 41, "context": "Zhou et al. (2017) hard-coded the target answer positions into the passage, and employed the sequence-to-sequence model to consume the positionencoded passages, then generate the questions.", "startOffset": 0, "endOffset": 19}, {"referenceID": 41, "context": "To better illustrate the advantage of our model, we show some comparative results of different models in Table 2, where the Baseline system is our implementation of Zhou et al. (2017) without rich features.", "startOffset": 165, "endOffset": 184}, {"referenceID": 41, "context": "To better illustrate the advantage of our model, we show some comparative results of different models in Table 2, where the Baseline system is our implementation of Zhou et al. (2017) without rich features. Generally, our MPQG model generates better questions than Zhou et al. (2017). Taking the first case as an example, the baseline fails to recognize that \u201c1856\u201d is the year when \u201cnikola tesla\u201d is born, while our MPQG learns that from the pattern \u201cday month year - day month year\u201d, which frequently occurs in the training data.", "startOffset": 165, "endOffset": 284}, {"referenceID": 19, "context": "Here F1 measures the overlap between the prediction and the reference in terms of bags of tokens, and exact match (EM) measures the percentage where the prediction is identical to the reference (Rajpurkar et al., 2016).", "startOffset": 194, "endOffset": 218}, {"referenceID": 19, "context": "Here F1 measures the overlap between the prediction and the reference in terms of bags of tokens, and exact match (EM) measures the percentage where the prediction is identical to the reference (Rajpurkar et al., 2016). The baseline is trained only on the part where gold questions are available, while the others are trained on the combination of the gold questions and the automatically generated questions, but with different methods of generating questions: (1) w/ window, a strong baseline from Yang et al. (2017), uses the previous and the following 5 tokens of the target answer as the pseudo question, and (2) w/ MPQG+R generates questions with our MPQG+R model.", "startOffset": 195, "endOffset": 519}, {"referenceID": 19, "context": "Here F1 measures the overlap between the prediction and the reference in terms of bags of tokens, and exact match (EM) measures the percentage where the prediction is identical to the reference (Rajpurkar et al., 2016). The baseline is trained only on the part where gold questions are available, while the others are trained on the combination of the gold questions and the automatically generated questions, but with different methods of generating questions: (1) w/ window, a strong baseline from Yang et al. (2017), uses the previous and the following 5 tokens of the target answer as the pseudo question, and (2) w/ MPQG+R generates questions with our MPQG+R model. First, we can see that w/ MPQG+R outperforms the baseline under all settings in terms of both F1 and EM scores, especially under the 10% setting, where we observe 3 and 5 points gains in terms of F1 and EM scores. This shows the effectiveness of our model. Second, the comparing results between w/ MPQG+R and w/ window show that the improvements of w/ MPQG+R are not due to simply enlarging the training data, but because of the higher quality of the generated questions. Yang et al. (2017) showed that w/ window can significantly improve their baseline, while it is not true in our experiment.", "startOffset": 195, "endOffset": 1162}, {"referenceID": 25, "context": "Memory Network adopts the end-to-end memory network (Sukhbaatar et al., 2015) as the encoder, and a vanilla RNN model as the decoder.", "startOffset": 52, "endOffset": 77}, {"referenceID": 14, "context": "For the generative QA experiment, we compare our model with the generative models in Nguyen et al. (2016) on the \u201cdescription\u201d subset of MS-MARCO dataset.", "startOffset": 85, "endOffset": 106}, {"referenceID": 9, "context": "Passage Ranking ranks the passage by a deep structured semantic model of Huang et al. (2013). Sequence to Sequence is a vanilla sequence-to-sequence model (Sutskever, Vinyals, and Le, 2014).", "startOffset": 73, "endOffset": 93}, {"referenceID": 41, "context": "For question generation (QG), our work extends previous work (Du, Shao, and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding.", "startOffset": 61, "endOffset": 224}, {"referenceID": 37, "context": "For question generation (QG), our work extends previous work (Du, Shao, and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding.", "startOffset": 61, "endOffset": 224}, {"referenceID": 24, "context": "For question generation (QG), our work extends previous work (Du, Shao, and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding.", "startOffset": 61, "endOffset": 224}, {"referenceID": 28, "context": "For question generation (QG), our work extends previous work (Du, Shao, and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding.", "startOffset": 61, "endOffset": 224}, {"referenceID": 40, "context": "For question generation (QG), our work extends previous work (Du, Shao, and Cardie, 2017; Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding.", "startOffset": 61, "endOffset": 224}, {"referenceID": 24, "context": ", 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding. Tang et al. (2017); Yang et al.", "startOffset": 8, "endOffset": 159}, {"referenceID": 24, "context": ", 2017; Subramanian et al., 2017; Tang et al., 2017; Wang, Yuan, and Trischler, 2017; Yuan et al., 2017) by performing query understanding. Tang et al. (2017); Yang et al. (2017) joins the QG task with the QA task, but they still conduct the QG task.", "startOffset": 8, "endOffset": 179}, {"referenceID": 30, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 31, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 23, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 32, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 3, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 22, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 11, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 39, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 4, "context": "For question answering (QA), most previous works (Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2016; Wang et al., 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer.", "startOffset": 49, "endOffset": 254}, {"referenceID": 3, "context": ", 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer. Obviously, they rely on the assumption that the answer can be exactly matched in the passage. On the other hand, our model performs generative QA, which generates the answer word-by-word, and does not rely on this assumption. The generative QA is valuable for studying, as we can not guarantee the assumption being true for all scenarios. Tan et al. (2017) claims to perform generative QA, but it still relies on an extractive QA system by generating answers from the extractive results.", "startOffset": 8, "endOffset": 591}, {"referenceID": 3, "context": ", 2017; Chen et al., 2017; Xiong, Zhong, and Socher, 2016; Seo et al., 2016; Lee et al., 2016; Yu et al., 2016; Dhingra et al., 2017) focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer. Obviously, they rely on the assumption that the answer can be exactly matched in the passage. On the other hand, our model performs generative QA, which generates the answer word-by-word, and does not rely on this assumption. The generative QA is valuable for studying, as we can not guarantee the assumption being true for all scenarios. Tan et al. (2017) claims to perform generative QA, but it still relies on an extractive QA system by generating answers from the extractive results. One notable exclusion is Yin et al. (2015), which generate factoid answers from a knowledge base (KB).", "startOffset": 8, "endOffset": 765}], "year": 2017, "abstractText": "We propose a query-based generative model for solving both tasks of question generation (QG) and question answering (QA). The model follows the classic encoderdecoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple perspectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major problem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher performances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong extractive QA system. In addition, our model shows better performance than the state-of-the-art baselines of the generative QA task.", "creator": "LaTeX with hyperref package"}}}