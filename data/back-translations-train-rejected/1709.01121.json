{"id": "1709.01121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Learning to parse from a semantic objective: It works. Is it syntax?", "abstract": "Recent work on reinforcement learning and other gradient estimators for latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) they do outperform baselines on sentence classification, but that (ii) their parsing strategies are not especially consistent across random restarts, (iii) the parses they produce tend to be shallower than PTB parses, and (iv) these do not resemble those of PTB or of any other recognizable semantic or syntactic grammar formalism.", "histories": [["v1", "Mon, 4 Sep 2017 19:05:39 GMT  (36kb)", "http://arxiv.org/abs/1709.01121v1", "13 pages, 6 figures, 4 tables, submitted to TACL"]], "COMMENTS": "13 pages, 6 figures, 4 tables, submitted to TACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adina williams", "andrew drozdov", "samuel r bowman"], "accepted": false, "id": "1709.01121"}, "pdf": {"name": "1709.01121.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Adina Williams", "Andrew Drozdov", "Samuel R. Bowman"], "emails": ["adinawilliams@nyu.edu", "andrew.drozdov@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.01 121v 1 [cs.C L] 4S epRecent work on reinforcement learning and other gradient estimators of latent tree learning has made it possible to train neural networks that learn to analyze both a sentence and the resulting parse to interpret the sentence, all without any contact with parse trees at the time of training. Surprisingly, these models often perform better sentence comprehension tasks than models that use parse trees from conventional parsers. The aim of this paper is to investigate what these latent tree learning models learn. We replicate two such models in a common code base and find that (i) they outperform baselines in sentence classification, but (ii) their parse strategies are not particularly uniform at random restarts, (iii) the parses they produce tend to be flatter than PTB parses, and (iv) they do not resemble those of other tactical or tactical TB parses."}, {"heading": "1 Introduction", "text": "However, this type of learning is effective in judging tasks such as sentiment analysis (Socher et al., 2013), textual engagement (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011) can also be trained to be produced on eBay, Inc.parse trees, which they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al.; Choi et al., 2017) has led to the development of new training methods for TreeRNs that allow them to learn without ever being given an example of a correct parasite tree."}, {"heading": "2 Background", "text": "This year it is more than ever before."}, {"heading": "3 Models and Methods", "text": "This year is the highest in the history of the country."}, {"heading": "4 Does latent tree learning help sentence understanding?", "text": "Table 1 shows the accuracy of all models on two test sets: SNLI (training on SNLI only, for comparison with previous work) and MultiNLI (training on both sets of data) Each figure represents the accuracy of the best run, selected using the development set, of five runs with different random initializations and hyperparameter values. On both SNLI and MultiNLI, we reproduce the main result of Choi et al., showing that the STGumbel model, which does not receive syntactic information at the time of training, surpasses SPINN-NC, which performs the composition in the same way but is explicitly trained to analyze it. This suggests that the latent trees are helpful for constructing semantic representations of sets, whether they resemble conventional parse trees or not. Our results with RL-SPINN are clearer than the results of the SPINN models, but not impacting the performance of the full SPINN model, which is equivalent to the effect of this INN model."}, {"heading": "5 Are these models consistent?", "text": "If it were the case that a latent tree learning model exceeds its basics by identifying a particular grammar for English that is better than the one used in the PTB and Stanford parser, then we would expect these models to detect roughly the same grammar via random restarts and minor configuration changes, and use that grammar to"}, {"heading": "300D SPINN 67.1 (1.0) 68.3 71.5", "text": "Table 2 shows two measures for the consistency of the four models that generate parses, and a simple random baseline that generates parses by randomly fusing pairs of adjacent words and phrases.First, we show the variation in the accuracy of MultiNLI development set over runs. While an outlier distorts these numbers for ST-Gumbel without GRU sheet, these numbers are roughly equal between the latent tree learning models and the baselines, suggesting that these models are not substantially more brittle or hyperparameter sensitive in their task performance. The second measurement shows the self-F1 for each model: the unlabeled binary F1 between the parses generated by two runs of the same model for the MultiNLI development group, averaged over all possible pairs of different runs. This measurement measures the degree to which the models reliably converge on the same parameters, and sheds some light on the behavior of the baseline F1, but the levels of the models themselves are relatively high below 965% consistency."}, {"heading": "6 Do these models learn PTB grammar?", "text": "In fact, most of them are able to play by the rules they have set themselves, and they are able to play by the rules they have set themselves."}, {"heading": "7 Analyzing the Learned Trees", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "8 Conclusion", "text": "The experiments and analyses presented in this paper show that the best models available for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics in a discernible way. Nevertheless, these models perform just as well or better than models with access to Penn Treebank-like parses when it comes to sentence comprehension - measured by the performance of MultiNLI. This result leaves us with a puzzle: What do these models - especially those based on the ST-Gumbel technique - learn that enables them to do so well? We have made some observations, but we do not have a fully satisfactory explanation for this. A thorough examination of this problem will probably require a search for new architectures for sentence encodings that draw different behaviors from the models trained in this work. This result also opens wide-ranging questions about grammar and sentence comprehension: Will the optimal grammars for sentence comprehension problems like NLI, when we explore the full space of grammars to find them - do not necessarily require perceptible sibilities with the structure of syntax?"}, {"heading": "Acknowledgments", "text": "This project benefited from financial support for SB from Google and Tencent Holdings and from a Titan X Pascal GPU donated by NVIDIA Corporation to AD. Jon Gauthier contributed to early discussions that motivated this work, and he, Nikita Nangia, Kelly Zhang and Cipta Herwana were on hand to provide advice."}], "references": [{"title": "Core syntax: A minimalist approach", "author": ["David Adger."], "venue": "Oxford University Press.", "citeRegEx": "Adger.,? 2003", "shortCiteRegEx": "Adger.", "year": 2003}, {"title": "What do neural machine translation models learn about morphology", "author": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "venue": "In Proceedings of the 55th Annual Meeting of the Association", "citeRegEx": "Belinkov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Bayesian grammar induction for language modeling", "author": ["Stanley F. Chen."], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Chen.,? 1995", "shortCiteRegEx": "Chen.", "year": 1995}, {"title": "Unsupervised learning of task-specific tree structures with tree-lstms", "author": ["Jihun Choi", "Kang Min Yoo", "Sang-goo Lee."], "venue": "ArXiv preprint 1707.02786.", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Aspects of the Theory of Syntax", "author": ["Noam Chomsky."], "venue": "MIT press.", "citeRegEx": "Chomsky.,? 1965", "shortCiteRegEx": "Chomsky.", "year": 1965}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio."], "venue": "ArXiv preprint 1609.01704.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Unsupervised structure prediction with non-parallel multilingual guidance", "author": ["Shay B Cohen", "Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Sreerupa Das", "C. Lee Giles", "Guo-Zheng Sun."], "venue": "Proceedings of The Fourteenth Annual Conference of Cognitive", "citeRegEx": "Das et al\\.,? 1992", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Pattern classification", "author": ["Richard O Duda", "Peter E Hart", "David G Stork."], "venue": "Wiley, New York.", "citeRegEx": "Duda et al\\.,? 1973", "shortCiteRegEx": "Duda et al\\.", "year": 1973}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "\u00dcber sinn und bedeutung", "author": ["Gottlob Frege."], "venue": "Wittgenstein Studien 1(1).", "citeRegEx": "Frege.,? 1892", "shortCiteRegEx": "Frege.", "year": 1892}, {"title": "Introduction to syntactic pattern recognition", "author": ["King Sun Fu."], "venue": "Syntactic Pattern Recognition, Applications, Springer Verlag, Berlin, pages 1\u201331.", "citeRegEx": "Fu.,? 1977", "shortCiteRegEx": "Fu.", "year": 1977}, {"title": "Lowresource semantic role labeling", "author": ["Matthew R. Gormley", "Margaret Mitchell", "Benjamin Van Durme", "Mark Dredze."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Gormley et al\\.,? 2014", "shortCiteRegEx": "Gormley et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Semantics in generative grammar", "author": ["Irene Heim", "Angelika Kratzer."], "venue": "Blackwell.", "citeRegEx": "Heim and Kratzer.,? 1998", "shortCiteRegEx": "Heim and Kratzer.", "year": 1998}, {"title": "Identifiability and unmixing of latent parse trees", "author": ["Daniel J. Hsu", "ShamM. Kakade", "Percy S. Liang."], "venue": "Advances in neural information processing systems. pages 1511\u20131519.", "citeRegEx": "Hsu et al\\.,? 2012", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Sapporo, Japan,", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Learning structured text representations", "author": ["Yang Liu", "Mirella Lapata."], "venue": "ArXiv preprint 1705.09207.", "citeRegEx": "Liu and Lapata.,? 2017", "shortCiteRegEx": "Liu and Lapata.", "year": 2017}, {"title": "Jointly learning sentence embeddings and syntax with unsupervised tree-lstms", "author": ["Jean Maillard", "Stephen Clark", "Dani Yogatama."], "venue": "ArXiv preprint 1705.09189.", "citeRegEx": "Maillard et al\\.,? 2017", "shortCiteRegEx": "Maillard et al\\.", "year": 2017}, {"title": "Linguistic theory and natu", "author": ["Ivan A. Sag"], "venue": null, "citeRegEx": "Sag.,? \\Q1991\\E", "shortCiteRegEx": "Sag.", "year": 1991}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Con-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "An introduction to syntactic analysis and theory", "author": ["Dominique Sportiche", "Hilda Koopman", "Edward Stabler."], "venue": "John Wiley & Sons.", "citeRegEx": "Sportiche et al\\.,? 2013", "shortCiteRegEx": "Sportiche et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research (JMLR) 15(1):1929\u2013", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "The neural network pushdown automaton: Model, stack and learning simulations", "author": ["G.Z. Sun", "C.L. Giles", "H.H. Chen", "Y.C. Lee."], "venue": "Technical Report UMIACS-TR-93-77/CSTR-3118, University of Maryland.", "citeRegEx": "Sun et al\\.,? 1993", "shortCiteRegEx": "Sun et al\\.", "year": 1993}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "author": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."], "venue": "ArXiv preprint 1704.05426.", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine learning 8(34):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Yogatama et al\\.,? 2017", "shortCiteRegEx": "Yogatama et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 27, "context": ", 2011)\u2014which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree\u2014have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al.", "startOffset": 223, "endOffset": 244}, {"referenceID": 3, "context": ", 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 11, "context": ", 2016), and translation (Eriguchi et al., 2016).", "startOffset": 25, "endOffset": 48}, {"referenceID": 3, "context": "Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce", "startOffset": 30, "endOffset": 72}, {"referenceID": 34, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 25, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 5, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 211}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 225}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 250}, {"referenceID": 26, "context": "This is well illustrated by syntactically ambiguous sentences like the one below, repeated from Sag (1991) a.", "startOffset": 96, "endOffset": 107}, {"referenceID": 2, "context": "(2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.", "startOffset": 134, "endOffset": 178}, {"referenceID": 32, "context": "(2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.", "startOffset": 134, "endOffset": 178}, {"referenceID": 29, "context": "In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 3, "context": "(2017) and Choi et al. (2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 5, "context": "We confirm that both types of model succeed at producing useful sentence representations, but find that only the stronger of the two models\u2014that of Choi et al. (2017)\u2014learns a nontrivial grammar or outperforms its baseline.", "startOffset": 148, "endOffset": 167}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 8, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 17, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977).", "startOffset": 79, "endOffset": 231}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.", "startOffset": 79, "endOffset": 245}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.", "startOffset": 79, "endOffset": 296}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here. In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.", "startOffset": 79, "endOffset": 492}, {"referenceID": 31, "context": "The remaining three models all use TreeLSTMs (Tai et al., 2015), and all train and evaluate both components on a shared semantic objective.", "startOffset": 45, "endOffset": 63}, {"referenceID": 2, "context": "All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective.", "startOffset": 76, "endOffset": 97}, {"referenceID": 33, "context": "(2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise receive gradients through backpropagation.", "startOffset": 122, "endOffset": 138}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al.", "startOffset": 40, "endOffset": 58}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al.", "startOffset": 40, "endOffset": 122}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al. (2015). Socher et al.", "startOffset": 40, "endOffset": 153}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al. (2015). Socher et al. (2011) present the first neural network model that we are aware of that uses the same learned representations to both parse a sentence and make semantic classification decisions on that sentence and the resulting parse.", "startOffset": 40, "endOffset": 175}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.", "startOffset": 0, "endOffset": 194}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work. We are only aware of four prior works on of latent tree learning for sentence understanding with neural networks. All four jointly train two model components\u2014a parser based on distributed representations of words and phrases and a TreeRNN of some kind that uses those parses\u2014but differ in the parsing strategies, TreeRNN parameterizations, and training objective used. Socher et al. (2011) use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair.", "startOffset": 0, "endOffset": 632}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work. We are only aware of four prior works on of latent tree learning for sentence understanding with neural networks. All four jointly train two model components\u2014a parser based on distributed representations of words and phrases and a TreeRNN of some kind that uses those parses\u2014but differ in the parsing strategies, TreeRNN parameterizations, and training objective used. Socher et al. (2011) use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair. They train their model on a sentiment analysis objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse. While this work shows good results on sentiment analysis, it does not evaluate the proposed model\u2019s ability to induce trees. There is neither any direct analysis of the induced trees nor any comparison with a baseline that uses trees from a conventionallytrained parser. The remaining three models all use TreeLSTMs (Tai et al., 2015), and all train and evaluate both components on a shared semantic objective. All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective. The models differ primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components. Yogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise receive gradients through backpropagation.", "startOffset": 0, "endOffset": 1709}, {"referenceID": 18, "context": ", but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016).", "startOffset": 63, "endOffset": 82}, {"referenceID": 20, "context": "Several models (Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using attention or related mechanisms, but do not propagate information up the trees as in typical compositional models.", "startOffset": 15, "endOffset": 81}, {"referenceID": 24, "context": "Several models (Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using attention or related mechanisms, but do not propagate information up the trees as in typical compositional models.", "startOffset": 15, "endOffset": 81}, {"referenceID": 7, "context": "Other models like that of Chung et al. (2016) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.", "startOffset": 26, "endOffset": 46}, {"referenceID": 22, "context": "Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "(2016) on language modeling and Belinkov et al. (2017) on translation.", "startOffset": 32, "endOffset": 55}, {"referenceID": 2, "context": "Figure 2: The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down, reproduced with permission from Bowman et al. (2016). The model parses a sentence by selecting a sequence of SHIFT and REDUCE transitions using the transition classifier (shown in blue) and simultaneously uses the resulting parse to build a vector representation of the sentence by using a TreeLSTM composition function (shown in green) during REDUCE transitions.", "startOffset": 143, "endOffset": 164}, {"referenceID": 5, "context": "Figure 3: The ST-Gumbel model in its first step of processing the sentence the cat sat down, based on a figure by Choi et al. (2017), used with permission.", "startOffset": 114, "endOffset": 133}, {"referenceID": 2, "context": "SPINN Variants All three of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016). Figure 2 shows and describes the architecture.", "startOffset": 128, "endOffset": 149}, {"referenceID": 34, "context": "RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss.", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2017) and is shown in Figure 3.", "startOffset": 47, "endOffset": 66}, {"referenceID": 5, "context": "ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2017) and is shown in Figure 3. The model takes a sequence of N \u2212 1 steps to build a tree over N words. At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector. A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose. This repeats until only two nodes remain, at which point they are composed and the tree is complete. This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies. Though the scoring function yields discrete decisions, the Straight-Through Gumbel-Softmax estimator of Jang et al. (2016) makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle reinforcement learning techniques.", "startOffset": 47, "endOffset": 1010}, {"referenceID": 32, "context": "Data Our primary experiments use the MultiGenre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017).", "startOffset": 82, "endOffset": 115}, {"referenceID": 29, "context": "We use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer.", "startOffset": 43, "endOffset": 68}, {"referenceID": 21, "context": "We train all models using the Adam optimizer (Kingma and Ba, 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 13, "context": "This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making the it easier to learn an effective composition function that faithfully encodes the contents of a sentence. This interpretation is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths.", "startOffset": 309, "endOffset": 447}, {"referenceID": 28, "context": "The object and the verb of a sentence are generally taken to form a constituent in mainstream syntactic theory (Adger 2003; Sportiche et al. 2013) for three reasons: (i) We can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like he shot his gun, but she didn\u2019t .", "startOffset": 111, "endOffset": 146}, {"referenceID": 0, "context": "Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013).", "startOffset": 107, "endOffset": 144}, {"referenceID": 28, "context": "Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013).", "startOffset": 107, "endOffset": 144}], "year": 2017, "abstractText": "Recent work on reinforcement learning and other gradient estimators for latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) they do outperform baselines on sentence classification, but that (ii) their parsing strategies are not especially consistent across random restarts, (iii) the parses they produce tend to be shallower than PTB parses, and (iv) these do not resemble those of PTB or of any other recognizable semantic or syntactic grammar formalism.", "creator": "LaTeX with hyperref package"}}}