{"id": "1709.01134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "WRPN: Wide Reduced-Precision Networks", "abstract": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "histories": [["v1", "Mon, 4 Sep 2017 19:56:48 GMT  (652kb)", "http://arxiv.org/abs/1709.01134v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["asit mishra", "eriko nurvitadhi", "jeffrey j cook", "debbie marr"], "accepted": false, "id": "1709.01134"}, "pdf": {"name": "1709.01134.pdf", "metadata": {"source": "CRF", "title": "WRPN: Wide Reduced-Precision Networks", "authors": ["Asit Mishra", "Eriko Nurvitadhi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 9.01 134v 1 [cs.C V] 4S ep2 017"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to survive on their own are also able to survive on their own; most people who are able to survive on their own are not able to survive on their own, and so are those who are able to survive on their own."}, {"heading": "3 WRPN scheme and studies on AlexNet", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "4 Studies on deeper networks", "text": "To this end, we examine ResNet-34 [8] and batchnormalized Inception [9] and note similar trends, in particular that 2-bit weight and 4-bit activations continue to provide consistent accuracy as baseline values. We use TensorFlow [2] and tensorpack [1] for all our evaluations, and use ILSVRC-12 tensile and val datasets for analyses.2"}, {"heading": "4.1 ResNet", "text": "ResNet-34 has 3x3 filters in each of its modular layers with a network activity of 1x1. The filter bank width changes from 64 to 512 in depth. We use the pre-activation variant of ResNet and the base activation of ResNet-34 implementations with a single precision of 73.59%. Binarizing weights and activations for all layers except the first and last layers in this network give a top-1 accuracy of 60.5%. For binarizing ResNet, we do not have new layers (as in XNOR-NET)."}, {"heading": "4.2 Batch-normalized Inception", "text": "We applied the WRPN scheme to the batch normalized inception network [9]. This network includes the batch normalization of all layers and is a variant of GoogleNet [20], where the 5x5 folding filters are replaced by two 3x3 folds with up to 128 wide filters. Table 5 shows the results of our analysis. By using 4-bit activations and 2-bit weight and doubling the number of filter banks in the network, a model is created that is almost equal in accuracy to the base network with uniform precision (0.02% accuracy loss). A wide network with binary weights and activations is within 6.6% of the fully precise base network."}, {"heading": "5 Hardware friendly quantization scheme", "text": "The question that arises is whether it is a matter of a way in which the people in the USA, Europe and the world maneuver themselves into the crisis, in which they maneuver themselves into the crisis, in which they maneuver themselves into the crisis, in which they maneuver themselves into the crisis, in which they maneuver themselves into the crisis, in which they maneuver themselves into the crisis, in which they lead, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the question, in the crisis, in the crisis, in the question, in the question, in the question, in the question, in the crisis, in the crisis, in the crisis, in the crisis, in the crisis, in the question, in the crisis, in the crisis, in the crisis, in the crisis,"}, {"heading": "5.1 Efficiency improvements of reduced-precision operations on GPU, FPGA and ASIC", "text": "In practice, most people are able to decide for themselves what they want and what they don't want. In practice, most people are able to decide what they want and what they don't want. In practice, most people are able to decide whether they want to or not. In practice, most people are able to decide whether they want to or not. In practice, most people are able to decide whether they want to or not."}, {"heading": "6 Related work", "text": "Works such as Binary connect (BC) [5], Ternary-weight networks (TWN) [12], fine-grained ternary quantization [14] and INQ [25] aim to reduce the precision of network weights while still using full-precision activations. Accuracy is almost always degraded in the quantization of weights. For AlexNet on Imagenet, TWN loses 5% top-1 accuracy. Schemes such as INQ, [18] and [14] show fine-tuning to quantify network weights and do not sacrifice as much accuracy as when quantifying bits, but are not applicable to training networks from the ground up. INQ shows promising results with 5-bits of precision. XNOR-NET [17], BNN [4], DoReFa [26] and TTQ [27] aim."}, {"heading": "7 Conclusions", "text": "In this scheme, the numerical precision of weights and activation phases in both countries is significantly lower than in other countries. \"We must adjust to the fact that we are able to compensate for the loss of information,\" he said. \"We must adjust to it.\" \"We must adjust to it.\" \"We must adjust to it.\" \"We must adjust.\" \"\" We must adjust. \"\" \"We must adjust.\" \"\" \"We must adjust.\" \"\" \"\" We must adjust. \"\" \"\" \"\" We must adjust. \"\" \"\". \"\" \"\". \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \".\" \".\" \".\" \".\" \"\". \"\". \".\" \"\". \".\" \"\". \"\" \".\". \".\". \"\" \".\". \"\" \".\" \"\". \"\". \".\". \"\" \".\". \".\". \"\" \"\". \".\". \".\" \"\". \"\" \"\" \".\". \".\" \"\". \".\". \"\". \".\" \"\". \".\" \"\" \"\" \".\". \".\". \".\". \"\" \"\". \".\". \"\" \".\". \"\" \".\". \"\" \".\" \"\" \".\". \".\" \".\" \".\". \".\" \".\". \"\" \"\". \".\" \"\". \".\" \".\" \"\". \"\" \".\" \"..\" \"\". \"\" \".\" \".\" \".\" \".\" \"\". \"\". \"\". \"\" \".\" \".\". \".\" \".\" \"...\" \".\" \"\" \"\" \".\" \"...\" \"\" \"....\" \"\" \"\"... \"\" \"\"... \"\" \"\" \"...\" \"\" \"\" \""}], "references": [{"title": "and X", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu"], "venue": "Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. L\u00e9onard", "A.C. Courville"], "venue": "CoRR, abs/1308.3432", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, abs/1602.02830", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": "CoRR, abs/1511.00363", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Low-precision batch-normalized activations", "author": ["B. Graham"], "venue": "CoRR, abs/1702.08231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "CoRR, abs/1502.02551", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, abs/1502.03167", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Ternary weight networks", "author": ["F. Li", "B. Liu"], "venue": "CoRR, abs/1605.04711", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "CoRR, abs/1510.03009", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ternary Neural Networks with Fine-Grained Quantization", "author": ["N. Mellempudi", "A. Kundu", "D. Mudigere", "D. Das", "B. Kaul", "P. Dubey"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["D. Miyashita", "E.H. Lee", "B. Murmann"], "venue": "CoRR, abs/1603.01025", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["E. Nurvitadhi", "G. Venkatesh", "J. Sim", "D. Marr", "R. Huang"], "venue": "Ong Gee Hock, Y. T. Liew, K. Srivatsan, D. Moss, S. Subhaschandra, and G. Boudoukh. Can fpgas beat gpus in accelerating next-generation deep neural networks? In Proceedings of the 2017 ACM/SIGDA International Symposium on Field- Programmable Gate Arrays, FPGA \u201917, pages 5\u201314, New York, NY, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "CoRR, abs/1603.05279", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Resiliency of deep neural networks under quantization", "author": ["W. Sung", "S. Shin", "K. Hwang"], "venue": "CoRR, abs/1511.06488", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Inception-v4", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, abs/1409.4842", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "FINN: A framework for fast", "author": ["Y. Umuroglu", "N.J. Fraser", "G. Gambardella", "M. Blott", "P.H.W. Leong", "M. Jahre", "K.A. Vissers"], "venue": "scalable binarized neural network inference. CoRR, abs/1612.07119", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["G. Venkatesh", "E. Nurvitadhi", "D. Marr"], "venue": "CoRR, abs/1610.00324", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "CoRR, abs/1605.07146", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["A. Zhou", "A. Yao", "Y. Guo", "L. Xu", "Y. Chen"], "venue": "CoRR, abs/1702.03044", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou"], "venue": "CoRR, abs/1606.06160", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Trained ternary quantization", "author": ["C. Zhu", "S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1612.01064", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 119, "endOffset": 138}, {"referenceID": 10, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 119, "endOffset": 138}, {"referenceID": 12, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 119, "endOffset": 138}, {"referenceID": 19, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 119, "endOffset": 138}, {"referenceID": 24, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 172, "endOffset": 184}, {"referenceID": 9, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 172, "endOffset": 184}, {"referenceID": 20, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 172, "endOffset": 184}, {"referenceID": 23, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 207, "endOffset": 225}, {"referenceID": 2, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 207, "endOffset": 225}, {"referenceID": 14, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 207, "endOffset": 225}, {"referenceID": 3, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 207, "endOffset": 225}, {"referenceID": 18, "context": "Due to such efficiency benefits, there are many existing works which propose low-precision deep neural networks (DNNs) [25, 13, 15, 7, 22], even down to 2-bit ternary mode [27, 12, 23] and 1-bit binary mode [26, 4, 17, 5, 21].", "startOffset": 207, "endOffset": 225}, {"referenceID": 8, "context": "We report results on AlexNet [11], batch-normalized Inception [9], and ResNet-34 [8] on ILSVRC-12 [11] dataset.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "We report results on AlexNet [11], batch-normalized Inception [9], and ResNet-34 [8] on ILSVRC-12 [11] dataset.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "We report results on AlexNet [11], batch-normalized Inception [9], and ResNet-34 [8] on ILSVRC-12 [11] dataset.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "We report results on AlexNet [11], batch-normalized Inception [9], and ResNet-34 [8] on ILSVRC-12 [11] dataset.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 24, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 23, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 20, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "[4, 27, 26, 23, 12, 5, 21]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 16, "context": "Figure 1 shows memory footprint of activation maps and filter maps as batch size changes for 4 different networks (AlexNet, Inception-Resnet-v2 [19], ResNet-50 and ResNet-101) during the training and inference steps.", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "However, a straightforward reduction in precision of activation maps leads to significant reduction in model accuracy [26, 17].", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "However, a straightforward reduction in precision of activation maps leads to significant reduction in model accuracy [26, 17].", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "This is similar to what is reported in [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "32bA and 2bW data-point in this table is using TTQ technique [27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "Our widening of filter maps is inspired fromWide ResNet [24] work where the depth of the network is reduced and width of each layer is increased (the operand precision is still FP32).", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "With binary weights and activations we better the accuracy of XNORNET [17] by 4%.", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Apart from other benefits of reduced precision activations as mentioned earlier, widening filter maps also improves the efficiency of underlying GEMM calls for convolution operations since compute accelerators are typically more efficient on a single kernel consisting of parallel computation on large data-structures as opposed to many small sized kernels [24].", "startOffset": 357, "endOffset": 361}, {"referenceID": 6, "context": "For this, we study ResNet-34 [8] and batchnormalized Inception [9] and find similar trends, particularly that 2-bits weight and 4-bits activations continue to provide at-par accuracy as baseline.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "For this, we study ResNet-34 [8] and batchnormalized Inception [9] and find similar trends, particularly that 2-bits weight and 4-bits activations continue to provide at-par accuracy as baseline.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "We use TensorFlow [2] and tensorpack [1] for all our evaluations and use ILSVRC-12 train and val dataset for analysis.", "startOffset": 18, "endOffset": 21}, {"referenceID": 14, "context": "As a reference, for ResNet-18, the gap between XNOR-NET (1b weights and activations) and full-precision network is 18% [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "To the best of our knowledge, our ResNet binary and ternary (with 2-bits or 4-bits activation) top-1 accuracies are state-of-the-art results in the literature including unpublished technical reports (with similar data augmentation [14]).", "startOffset": 231, "endOffset": 235}, {"referenceID": 7, "context": "We applied WRPN scheme to batch-normalized Inception network [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "This network includes batch normalization of all layers and is a variant of GoogleNet [20] where the 5x5 convolutional filters are replaced by two 3x3 convolutions with up to 128 wide filters.", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "We adopt the straight-through estimator (STE) approach in our work [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "For instance, TWN [12] uses a threshold and a scaling factor for each layer to quantize weights to ternary domain.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "In TTQ [27], the scaling factors are learned parameters.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "clip_by_val when using Tensorflow [2]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "When k = 1, for binary weights we use the BWN approach [5] where the binarized weight value is computed based on the sign of input value followed by scaling with the mean of absolute values.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "We collect performance numbers from both previously reported analysis [16] as well as our own experiments.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": ", on FPGA [16] and ASIC such as TPU [10]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Works like Binary connect (BC) [5], Ternary-weight networks (TWN) [12], fine-grained ternary quantization [14] and INQ [25] target reducing the precision of network weights while still using full-precision activations.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "Works like Binary connect (BC) [5], Ternary-weight networks (TWN) [12], fine-grained ternary quantization [14] and INQ [25] target reducing the precision of network weights while still using full-precision activations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "Works like Binary connect (BC) [5], Ternary-weight networks (TWN) [12], fine-grained ternary quantization [14] and INQ [25] target reducing the precision of network weights while still using full-precision activations.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Works like Binary connect (BC) [5], Ternary-weight networks (TWN) [12], fine-grained ternary quantization [14] and INQ [25] target reducing the precision of network weights while still using full-precision activations.", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Schemes like INQ, [18] and [14] do fine-tuning to quantize the network weights and do not sacrifice accuracy as much but are not applicable for training networks from scratch.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Schemes like INQ, [18] and [14] do fine-tuning to quantize the network weights and do not sacrifice accuracy as much but are not applicable for training networks from scratch.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "XNOR-NET [17], BNN [4], DoReFa [26] and TTQ [27] target training as well.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "XNOR-NET [17], BNN [4], DoReFa [26] and TTQ [27] target training as well.", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "XNOR-NET [17], BNN [4], DoReFa [26] and TTQ [27] target training as well.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "XNOR-NET [17], BNN [4], DoReFa [26] and TTQ [27] target training as well.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "Recent work in [6] targets low-precision activations and reports accuracy within 1% of baseline with 5-bits precision and logarithmic (with base \u221a 2) quantization.", "startOffset": 15, "endOffset": 18}], "year": 2017, "abstractText": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (networkweights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamicmemory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "creator": "LaTeX with hyperref package"}}}