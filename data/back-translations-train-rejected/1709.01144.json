{"id": "1709.01144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Information Theoretic Analysis of DNN-HMM Acoustic Modeling", "abstract": "We propose an information theoretic framework for quantitative assessment of acoustic modeling for hidden Markov model (HMM) based automatic speech recognition (ASR). Acoustic modeling yields the probabilities of HMM sub-word states for a short temporal window of speech acoustic features. We cast ASR as a communication channel where the input sub-word probabilities convey the information about the output HMM state sequence. The quality of the acoustic model is thus quantified in terms of the information transmitted through this channel. The process of inferring the most likely HMM state sequence from the sub-word probabilities is known as decoding. HMM based decoding assumes that an acoustic model yields accurate state-level probabilities and the data distribution given the underlying hidden state is independent of any other state in the sequence. We quantify 1) the acoustic model accuracy and 2) its robustness to mismatch between data and the HMM conditional independence assumption in terms of some mutual information quantities. In this context, exploiting deep neural network (DNN) posterior probabilities leads to a simple and straightforward analysis framework to assess shortcomings of the acoustic model for HMM based decoding. This analysis enables us to evaluate the Gaussian mixture acoustic model (GMM) and the importance of many hidden layers in DNNs without any need of explicit speech recognition. In addition, it sheds light on the contribution of low-dimensional models to enhance acoustic modeling for better compliance with the HMM based decoding requirements.", "histories": [["v1", "Tue, 29 Aug 2017 10:03:05 GMT  (1287kb,D)", "http://arxiv.org/abs/1709.01144v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["pranay dighe", "afsaneh asaei", "herv\\'e bourlard"], "accepted": false, "id": "1709.01144"}, "pdf": {"name": "1709.01144.pdf", "metadata": {"source": "CRF", "title": "Information Theoretic Analysis of DNN-HMM Acoustic Modeling", "authors": ["Pranay Dighe", "Afsaneh Asaei"], "emails": [], "sections": [{"heading": null, "text": "This year it is more than ever before in the history of the city."}, {"heading": "II. BACKGROUND AND PRIOR RESEARCH", "text": "Language is a complex temporally varying signal that is normally assumed as the result of a piecemeal stationary stochastic process, so that the observed signal can be modelled by a Hidden Markov Model (HMM). HMM is a probabilistic finite state model in which the state-specific emission probability distribution is assumed to be independent of previous states and previous observations and the transition probabilities follow a Markovian structure of the first order."}, {"heading": "A. Foundation of HMM based Speech Recognition", "text": "In a typical HMM-based ASR framework, the hypothetical word sequence W \u00b2 Q = Q \u00b2 P (Q \u00b2 P = Q \u00b2 P) is estimated from the sequence of acoustic characteristics X = {X1,... XT}, where Xt is a standard acoustic characteristic (e.g. MFCC with first and second order time derivatives) at a given time t, asW \u00b2 = argmax W P (W | X) = argmax W (X | W) P (W) (1), where p (W) is estimated the probability of the word sequence W from the language model, and p (X | W) the probability of the acoustic sequence P (W | X) is then calculated as: p (X | W)."}, {"heading": "B. Prior Research", "text": "This year is the highest in the history of the country."}, {"heading": "C. Our Approach and Contributions", "text": "This year it is more than ever before."}, {"heading": "A. Mutual Information", "text": "In information theory, the mutual information of two random variables quantifies the amount of information transmitted via one random variable by the other random variable. This concept is defined by the concept of entropy, which measures the amount of information contained in a random variable. [26] For a discrete random variable A that accepts values A, entropy H (A) = \u2212 Africa a (A = a) log (P (A (A = a)))) (6) Accordingly, conditional entropy H (A | B = b) of a random variable A (A | B) is defined for another random variable A (A = a | B) log (P (A = a | B) = \u2211 b) B (B) B (B = b) H (A | B = \u2212 b), where H (A | B = b) = \u2212 is illegal."}, {"heading": "B. Desired Acoustic Model Properties", "text": "The z-HMM ASR communication channel discussed in Section III is most efficient when 1) the observed input feature Zt and the underlying hidden state Qt, which is to be decrypted, the highest mutual information I (Zt; Qt) and 2) the HMM conditional assumption of independence is met, so that the mutual information between the feature and the former hidden state is minimized when the current hidden state is known, i.e. I (Zt; Qt \u2212 Qt) the amount of information transmitted by the acoustic model, we discuss the following two properties of an ideal acoustic model for best classification and compliance with the HMM structure: 1) High information transmission capacity (P1): In order to maximize the amount of information transmitted by the acoustic model for state decoding, it is desirable to have a high mutual information between the feature and compliance with the HM structure."}, {"heading": "C. Computational Procedure using Posterior Features", "text": "In this section, we will develop the method for quantifying the desired properties P1 and P2 using the posterior probability characteristics generated by the acoustic model (= 7 estimated properties).This approach relies on DNN's ability as an acoustic model to directly generate posterior probabilities for underlying senon classes. Assuming that the acoustic models are trained on some transcribed training data, we will perform this analysis using a separate development data set (available for transcription)."}, {"heading": "V. NUMERICAL EVALUATION AND ANALYSIS", "text": "Experiments are carried out on the demanding AMI corpus [28], consisting of conversation conversations (in intelligent meeting rooms) in accented English. We use different acoustic models 7 and evaluate the properties P1 and P2. The acoustic models considered are based on common Gaussian mixing models (GMM) or DNNs with different architectures, which are examined in Section V-B. Furthermore, we investigate the effects of sparse and low improvements, as proposed in [22], on DNN acoustic models in Section V-E."}, {"heading": "A. Experimental Setup", "text": "AMI corpus contains recordings of spontaneous conversations in multi-party meeting scenarios. We use recordings of single head microphones (IHM) consisting of approximately 67 hours of tension set, 9 hours of development time (dev) set and 7 hours of test set. All acoustic models are trained using the same training data. 10% of the training data is used for cross-validation during DNN training. Kaldi toolkit [29] is used for the training of GMM and DNN HMM systems. Input characteristics are 39 dimensional Mel frequency Cepstral coefficients (MFCC) together with their primary and secondary temporal derivatives, resulting in 39 x 9 = 351 dimensional inputs, while an output class room of dimension 4007 representing the senonal probability vector space (MFCC). A GMM-HMM system is used to obtain the traction distributions based on the exact traction distributions of truth and truth."}, {"heading": "B. Comparing DNN v/s GMM Acoustic Modeling", "text": "Here we compare the first two lines of Table II. Our primary observation is that DNN-based probabilities on Zt have a lower entropy H (Zt) than GMMs. This means that the degree of uncertainty in the DNN outputs is typically lower than the GMM values. In addition, we observe higher mutual information I (Zt; Qt) in the DNN acoustic models, which indicates higher information transmission through the ASR communication channel (see Figure 1 (c)) compared to GMMs. Thus, the capacity of the channel is higher and the DNN values are more accurate in discriminating between the underlying sensing classes. This confirms the well-known better modelling capability of DNN compared to GMM. Furthermore, the criterion of conditional independence of HMM by the DNN acoustic model is better met, as the conditional mutual information between the current observation and the previous state can be compensated if the current state (Zt) is compensated by Qt \u2212 language."}, {"heading": "C. Effect of Increasing Depth in DNN Acoustic Models", "text": "The results are listed in Table II. We compare DNN architectures in a manner similar to the study in [12], in which the number of hidden layers is increased with or without maintaining the total number of network parameters equal to the base parameter DNN. DNN with 0 hidden layers is simply a logistic regression model, in which the input layer is directly connected to the base layer. It is a noteworthy observation that the deeper architectures exhibit higher and higher mutual information I (Zt; Qt), which leads to higher accuracy of the acoustic model. This trend is observed in both cases - when the number of parameters is equal and when they are not. On the other hand, we observe a fairly negligible effect of the depth of DNN on robustness compared to the violation of the HMM conditioned assumption of independence. Regardless of this, we find that all DNN architectures exhibit a lower predictive value of I (Zt; Qt; Qt \u2212 QT | QM) than the very precise QM in the QM (essential QR)."}, {"heading": "D. Sparse and Low-rank Acoustic Model Enhancement", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "E. Learning Student Networks Using Sparse or Low-rank Soft Targets", "text": "Recently, in [22], [23] we proposed to train the student DNN acoustic models based on improved soft targets obtained from monitored sparse and low projection. Improved soft targets are essentially training data obtained from the run-up of a DNN trained on binary labels and then reconstructed using PCA or sparse coding in a supervised manner, as shown in Figure 2. The main idea of the teacher-student DNN training is that a \"student\" DNN can be trained with soft targets (instead of uniform labels) obtained from a more complex model serving as a \"teacher.\" In our approach, the student DNNs trained with improved soft targets try to improve the function modelled by the base year DNN, as well as 2) the reconstruction transformation performed by PCA or sparse coding. In Table III, the ASR models of the DNN implicit we support the low DNN performance of the sparse targets, as well as the low DNN implied by the low targets."}, {"heading": "F. Further Implications", "text": "The talking-hearing z-HMM perspective provides various conceptual insights into ASR. Assuming that the senone subspaces are independent, i.e. there is no correlation or contextual dependencies, a state Qt = qk should be deterministically assigned to the binary observation Zt = qk with the probability of one. However, as in [21], senone subspaces are correlated with each other and they have ranks higher than unit for real data. These correlations may arise from common parent nodes in senone decision trees or be the result of contextual dependencies of senones with each other, as in Figure 4 (a) and 4 (b)."}, {"heading": "VI. CONCLUSIONS", "text": "We presented an information theoretical approach to comparing the quality of different acoustic models in HMM-based ASR. We quantified the information transmitted through an ASR communication channel as these decode the sequence of hidden HMM states. Higher amounts of information transmitted through this channel indicate better modeling capability of the acoustic model and higher accuracy. The conditional assumption of independence of HMM is also evaluated with respect to the conditional mutual information between the current observation and the previous state when the current state is given. Lower value of conditional mutual information indicates better agreement with the HMM structure. Our experimental analysis provides quantitative measurements for these different dimensions of superiority of DNN-based acoustic modeling over GMM. In addition, we measured the contribution of a gradual increase in the depth of DNN in improving the quality of acoustic modeling. In addition, we demonstrated that low and moderate reconstruction of codispositions and additional local translation information can lead to further improvement of PCN."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The research that led to these results was funded by the Swiss NSF project \"Parsimonious Hierarchical Automatic Speech Recognition and Query Detection (PHASERQUAD)\" with funding number 200020-169398."}], "references": [{"title": "Continuous speech recognition by statistical methods", "author": ["Frederick Jelinek"], "venue": "Proceedings of the IEEE, vol. 64, no. 4, pp. 532\u2013556, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Statistical methods for speech recognition", "author": ["Frederick Jelinek"], "venue": "MIT press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Connectionist speech recognition: a hybrid approach, Springer", "author": ["Herve A Bourlard", "Nelson Morgan"], "venue": "Science & Business Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "What HMMs can do", "author": ["Jeff A Bilmes"], "venue": "IEICE Transactions on Information and Systems, vol. 89, no. 3, pp. 869\u2013891, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["Dan Gillick", "Larry Gillick", "Steven Wegmann"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2011, pp. 71\u201376.  11", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "What HMMs cant do", "author": ["Jeff A Bilmes"], "venue": "ATR Workshop, Invited paper and lecture, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring how deep neural networks form phonemic categories", "author": ["Tasha Nagamine", "Michael L Seltzer", "Nima Mesgarani"], "venue": "INTER- SPEECH, 2015, pp. 1912\u20131916.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A Comparative Analytic Study on the Gaussian Mixture and Context Dependent Deep Neural Network Hidden Markov Models", "author": ["Dong Yu Chaojun Liu Yifan Gong Yan Huang"], "venue": "Interspeech 2014, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "How neural network depth compensates for hmm conditional independence assumptions in dnnhmm acoustic models", "author": ["Suman Ravuri", "Steven Wegmann"], "venue": "Interspeech 2016, pp. 2736\u20132740, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Studies with fabricated switchboard data: Exploring sources of modeldata mismatch", "author": ["Don McAllaster", "Larry Gillick", "Francesco Scattone", "Mike Newman"], "venue": "Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Discriminative training for speech recognition is compensating for statistical dependence in the hmm framework", "author": ["Dan Gillick", "Steven Wegmann", "Larry Gillick"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 4745\u20134748.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "From hmm\u2019s to segment models: a unified view of stochastic modeling for speech recognition", "author": ["M. Ostendorf", "V.V. Digalakis", "O.A. Kimball"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 4, no. 5, pp. 360\u2013378, Sep 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "pp. 4945\u20134949, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 4960\u20134964.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Maximum mutual information based reduction strategies for cross-correlation based joint distributional modeling", "author": ["Jeff A Bilmes"], "venue": "Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on. IEEE, 1998, vol. 1, pp. 469\u2013472.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse modeling of neural network posterior probabilities for exemplar-based speech recognition", "author": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Speech Communication, vol. 76, pp. 230\u2013244, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting lowdimensional structures to enhance dnn based acoustic modeling in speech recognition", "author": ["P. Dighe", "G. Luyet", "A. Asaei", "H. Bourlard"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 5690\u2013 5694.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank and sparse soft targets to learn better dnn acoustic models", "author": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "2017.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Exploiting eigenposteriors for semi-supervised training of dnn acoustic models with sequence discrimination", "author": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Proceedings of Interspeech, 2017.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Lowrank representation of nearest neighbor phone posterior probabilities to enhance dnn acoustic modeling", "author": ["Gil Luyet", "Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Interspeech, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "On modeling context-dependent clustered states: Comparing hmm/gmm, hybrid hmm/ann and kl-hmm approaches", "author": ["Marzieh Razavi", "Ramya Rasipuram", "Mathew Magimai-Doss"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 7659\u20137663.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Entropy, relative entropy and mutual information", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": "1991.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning and generalization with the information bottleneck", "author": ["Ohad Shamir", "Sivan Sabato", "Naftali Tishby"], "venue": "International Conference on Algorithmic Learning Theory. Springer, 2008, pp. 92\u2013107.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "The ami meeting corpus", "author": ["Iain McCowan", "Jean Carletta", "W Kraaij", "S Ashby", "S Bourban", "M Flynn", "M Guillemot", "T Hain", "J Kadlec", "V Karaiskos"], "venue": "Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, 2005, vol. 88.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Luk\u00e1\u0161 Burget", "Ond\u0159ej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motl\u0131\u0301\u010dek", "Yanmin Qian", "Petr Schwarz"], "venue": "2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Using kl-based acoustic models in a large vocabulary recognition task", "author": ["Guillermo Aradilla", "Herve Bourlard", "Mathew Magimai-Doss"], "venue": "01 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Over the last 40 years, hidden Markov models (HMMs, and more recently DNN-HMM) have served as the backbone1 of virtually all large scale Automatic Speech Recognition (ASR) systems [1], [2], [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "Over the last 40 years, hidden Markov models (HMMs, and more recently DNN-HMM) have served as the backbone1 of virtually all large scale Automatic Speech Recognition (ASR) systems [1], [2], [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "Over the last 40 years, hidden Markov models (HMMs, and more recently DNN-HMM) have served as the backbone1 of virtually all large scale Automatic Speech Recognition (ASR) systems [1], [2], [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 3, "context": "However, HMMs are built upon several major assumptions which are well known and understood, yet often shattered in the speech community [4], [5], [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "However, HMMs are built upon several major assumptions which are well known and understood, yet often shattered in the speech community [4], [5], [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "However, HMMs are built upon several major assumptions which are well known and understood, yet often shattered in the speech community [4], [5], [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "Replacing GMM acoustic models by DNNs to achieve these hybrid speech recognition systems [4] has been the single largest source of ASR performance improvement in the last few years [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Replacing GMM acoustic models by DNNs to achieve these hybrid speech recognition systems [4] has been the single largest source of ASR performance improvement in the last few years [7].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Building upon work initiated in the early 90\u2019s [4], and fully exploiting the availability of larger amount of training data and processing power, DNNs are now recognised to outperform GMMs in HMM based ASR [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "Building upon work initiated in the early 90\u2019s [4], and fully exploiting the availability of larger amount of training data and processing power, DNNs are now recognised to outperform GMMs in HMM based ASR [7].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "1) Towards Better State Conditional Probabilities: A detailed discussion in [5], [8] argues that accurate sequence decoding using HMM requires the acoustic model to be structurally discriminative of the underlying classes and the model should capture their distinctive features.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "1) Towards Better State Conditional Probabilities: A detailed discussion in [5], [8] argues that accurate sequence decoding using HMM requires the acoustic model to be structurally discriminative of the underlying classes and the model should capture their distinctive features.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "success of DNNs highlighted their invariant representation learning power for class discrimination [7], [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "success of DNNs highlighted their invariant representation learning power for class discrimination [7], [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "In [10], it was found that the individual neurons in each of the DNN hidden layers learn to be selectively active in different ways towards distinct phone patterns.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "It was also confirmed analytically in [11] that DNNs are significantly better at phone classification compared to GMMs and, although robustness against unseen noise and data/channel mismatch is a challenge, they still outperform GMMs in these conditions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "In particular, the conditional independence assumption of HMM is often acknowledged as the number one limiting factor resulting in poor ASR performance and lack of robustness [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "To test this hypothesis, earlier works [13], [6] replaced real speech data with synthetic data which strictly follow HMM assumptions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "To test this hypothesis, earlier works [13], [6] replaced real speech data with synthetic data which strictly follow HMM assumptions.", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "Along this line, [12], [14] have studied how DNNs cope with violation of HMM assumptions.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "Along this line, [12], [14] have studied how DNNs cope with violation of HMM assumptions.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "For instance, the segmental models for stochastic modelling of speech [15] are employed to model a long duration of a speech segment as a unit instead of the frame-wise modeling procedure.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "More recently, end-to-end ASR systems like attention-based mechanisms using long short term memory implementation of the recurrent neural networks (LSTM-RNNs) [16], [17], [18] are notable efforts in this direction.", "startOffset": 159, "endOffset": 163}, {"referenceID": 16, "context": "More recently, end-to-end ASR systems like attention-based mechanisms using long short term memory implementation of the recurrent neural networks (LSTM-RNNs) [16], [17], [18] are notable efforts in this direction.", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "More recently, end-to-end ASR systems like attention-based mechanisms using long short term memory implementation of the recurrent neural networks (LSTM-RNNs) [16], [17], [18] are notable efforts in this direction.", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "Unlike previous mutual information estimation using GMM that requires expectation maximization (EM) to learn a joint probability distribution [19], the proposed method directly uses the DNN based acoustic models in a simple and efficient information calculation procedure.", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "It was shown in [20] that DNN based posterior probabilities live in a union of low-dimensional subspaces that can be characterized using", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "sparse [21] and low-rank representations [22], [23], [24].", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "sparse [21] and low-rank representations [22], [23], [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "sparse [21] and low-rank representations [22], [23], [24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "sparse [21] and low-rank representations [22], [23], [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "In other words, a one-to-one relationship holds between the hidden states and the senone observation [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "This concept is defined through the notion of entropy which measures the quantity of information held in a random variable [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "We rely on I(Zt;Qt) as the measure of the information transmitted by the acoustic model in the ASR communication channel [27].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Due to the data-processing inequality [26], I(Zt;Qt) \u2264 I(Xt;Qt) with the equality being achieved when Zt is the sufficient statistic of features Xt.", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "Experiments are conducted on the challenging AMI corpus [28], consisting of conversational speech (in smart meeting rooms) in accented English.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "Furthermore, we investigate the effect of sparse and low-rank enhancements, as proposed in [22], on DNN acoustic models in Section V-E.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Kaldi toolkit [29] is used for training of GMM and DNN-HMM systems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "2: (a) Supervised enhancement of DNN based posterior features is illustrated using low-rank and sparsity based reconstruction as proposed in [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "We compare DNN architectures in a manner similar to the study in [12] where number of hidden layers are increased with or without keeping the total number of network parameters equal to the baseline DNN.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "In [22], [23], we modify the forward pass outputs of the baseline DNN using (1) principal component analysis (PCA) based low-rank reconstruction and (2) overcomplete dictionary based sparse reconstruction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [22], [23], we modify the forward pass outputs of the baseline DNN using (1) principal component analysis (PCA) based low-rank reconstruction and (2) overcomplete dictionary based sparse reconstruction.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "Instead, we use a simple methodology based on teacher-student DNN framework (used in [22]) to evaluate the ASR performance of our approach on test data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Recently, in [22], [23], we proposed to train student DNN acoustic models using enhanced soft targets obtained from supervised sparse and low-rank projection.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Recently, in [22], [23], we proposed to train student DNN acoustic models using enhanced soft targets obtained from supervised sparse and low-rank projection.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Theoretical analysis provided in this work supports the importance of sparse and low-rank enhancements in improving DNN acoustic models which was confirmed in our previous works [21], [22], [23], [24] experimentally.", "startOffset": 178, "endOffset": 182}, {"referenceID": 21, "context": "Theoretical analysis provided in this work supports the importance of sparse and low-rank enhancements in improving DNN acoustic models which was confirmed in our previous works [21], [22], [23], [24] experimentally.", "startOffset": 184, "endOffset": 188}, {"referenceID": 22, "context": "Theoretical analysis provided in this work supports the importance of sparse and low-rank enhancements in improving DNN acoustic models which was confirmed in our previous works [21], [22], [23], [24] experimentally.", "startOffset": 190, "endOffset": 194}, {"referenceID": 23, "context": "Theoretical analysis provided in this work supports the importance of sparse and low-rank enhancements in improving DNN acoustic models which was confirmed in our previous works [21], [22], [23], [24] experimentally.", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "However, as shown in [21], [22], senone subspaces are correlated with each other and they have ranks higher than unity for real data.", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "However, as shown in [21], [22], senone subspaces are correlated with each other and they have ranks higher than unity for real data.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "Building on the theories elaborated in Section III and discussion above, the KL-HMM approach in the prior work [30], [25] can be seen as associating a distribution P (Zt|Qt) to each state.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Building on the theories elaborated in Section III and discussion above, the KL-HMM approach in the prior work [30], [25] can be seen as associating a distribution P (Zt|Qt) to each state.", "startOffset": 117, "endOffset": 121}], "year": 2017, "abstractText": "We propose an information theoretic framework for quantitative assessment of acoustic modeling for hidden Markov model (HMM) based automatic speech recognition (ASR). Acoustic modeling yields the probabilities of HMM sub-word states for a short temporal window of speech acoustic features. We cast ASR as a communication channel where the input sub-word probabilities convey the information about the output HMM state sequence. The quality of the acoustic model is thus quantified in terms of the information transmitted through this channel. The process of inferring the most likely HMM state sequence from the sub-word probabilities is known as decoding. HMM based decoding assumes that an acoustic model yields accurate state-level probabilities and the data distribution given the underlying hidden state is independent of any other state in the sequence. We quantify 1) the acoustic model accuracy and 2) its robustness to mismatch between data and the HMM conditional independence assumption in terms of some mutual information quantities. In this context, exploiting deep neural network (DNN) posterior probabilities leads to a simple and straightforward analysis framework to assess shortcomings of the acoustic model for HMM based decoding. This analysis enables us to evaluate the Gaussian mixture acoustic model (GMM) and the importance of many hidden layers in DNNs without any need of explicit speech recognition. In addition, it sheds light on the contribution of low-dimensional models to enhance acoustic modeling for better compliance with the HMM based decoding requirements.", "creator": "LaTeX with hyperref package"}}}