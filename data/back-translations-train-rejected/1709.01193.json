{"id": "1709.01193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Compositional Approaches for Representing Relations Between Words: A Comparative Study", "abstract": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, noun-modifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words. This study aims to compare different operations for creating relation representations from word-level representations. We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy. Moreover, we evaluate the different relation representations in a knowledge base completion task.", "histories": [["v1", "Mon, 4 Sep 2017 23:30:22 GMT  (1185kb,D)", "http://arxiv.org/abs/1709.01193v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huda hakami", "danushka bollegala"], "accepted": false, "id": "1709.01193"}, "pdf": {"name": "1709.01193.pdf", "metadata": {"source": "CRF", "title": "Compositional Approaches for Representing Relations Between Words: A Comparative Study", "authors": ["Huda Hakami", "Danushka Bollegala"], "emails": ["h.a.hakami@liv.ac.uk", "danushka.bollegala@liv.ac.uk"], "sections": [{"heading": null, "text": "Determining the relationships between words (or entities) is important for various tasks of natural language processing, such as relationship search, nounmodifier classification, and recognition of analogies. A popular approach to depicting relationships between pairs of words is to extract the patterns in which the words occur in common with a corpus, and assign a vector of pattern frequencies to each pair of words. Despite the simplicity of this approach, it suffers from a lack of data, information scalability, and linguistic creativity, as the model is unable to handle previously invisible pairs of words in a corpus. In contrast, a compositional approach to depicting relationships between words overcomes these problems by using the attributes of each word to indirectly represent the common relationships between the two words."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to move into a different world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2. Related work", "text": "In practice, however, this method generates high-dimensional and sparse vectors, which means that the words that appear in similar contexts have similar meanings. Traditionally, word representation counts the co-occurrences of a word with its adjacent words in a specific window size. In practice, however, this method generates high-dimensional and sparse vectors. Instead of counting the occurrences between words and contexts, machine learning techniques are used to directly distinguish the vocabulary from each other."}, {"heading": "3. Relation Composition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Compositional operators", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3.2. Input Word Embeddings", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. Evaluation methods", "text": "In previous work suggesting compositional operators such as Mult, Add, etc., their effectiveness has been evaluated using semantic compositional activities. Mitchell and Lapata [27, 18], for example, used a set of phrase similarities data. Firstly, a phrase is represented by applying a particular compositional operator to the constituent word representations. Secondly, the similarity between two phrases is calculated using a measure of similarity, e.g. the cosmic similarity between the corresponding phrase representations. Finally, the calculated similarity values are compared with human similarity evaluations using some correlation measurements such as the Spearman or Pearson correlation coefficients. If a particular compositional technician produces a higher agreement with human similarity evaluations, he is considered superior. However, our task in this paper is to measure the similarity between relationships and not phrases (or sentences). Therefore, this evaluation protocol is not directly relevant to us."}, {"heading": "4.1. Relational similarity prediction", "text": "Given two pairs of words (a, b) and (c, d), the task is to measure the similarity between the semantic relationships that exist between the two words in each pair. This type of similarity is often referred to as relational similarity in previous work [5]. The task is to measure the degree of relational similarity between two given pairs of words (a, b) and (c, d). We need a method that assigns a high degree of relational similarity when the first pair is in the same relationship as another pair. Two benchmark datasets have been widely used in previous work to evaluate relational similarity are SAT [34] Dataset and SemEval 2012-Task25 [35] Dataset. We briefly describe the protocol for evaluating relational similarity with these datasets.The Scholastic Aptitude Test (SAT) word analogy Dataset contains 374 multiple-choice questions in which each question contains a pair of words as trunk, the required categories."}, {"heading": "4.2. Relation classification", "text": "In relation to the classification of relationships, the problem is to classify a particular pair of words (w1, w2) to a specific relationship r in a predefined set of relationships R, according to the relationship that exists between w1 and w2. We use the DiffVecs dataset proposed by Vylomova et al. [26], which consists of 12,458 triples < w1, w2, r >, with the words w1 and w2 by a relationship r. The relationship R comprises 15 relationship types that include lexical semantic relations, morphosyntactic paradigm relations, and morphosemantic relations. 6We use the various compositional operators discussed in Section 3.1 to represent each word pair by a relational embedding. We then perform a 1-next adjacent word classification (1-NN) in this relational embedding zone to classify the test word pairs into the relationship types."}, {"heading": "4.3. Knowledge base completion", "text": "In the United States, the number of people residing in the United States has increased by more than half, in the United States by more than half, in the United States by more than half, in Europe by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, in the United States by more than half, \"in the United States by\" in the United States by \"in the\" in the \"in the\" in the \"in\" in the \"in\" in \"in the\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" the \"in\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" the \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in the\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in the \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\" in \"in\""}, {"heading": "5. Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Performance of Relational Similarity Task", "text": "In fact, it is as if most people are able to recognize themselves and understand how they have behaved. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they change the world. (...) It is as if they change the world. \""}, {"heading": "5.2. Effect of Dimensionality", "text": "The dimensions in which we move are very different. \"The dimensions in which we move are so different that we are unable to identify ourselves. The dimensions in which we move are so different that we are unable to identify ourselves.\" The dimensions in which we move are so different that we are unable to identify ourselves. \"The dimensions in which we move are so different that we are able to identify ourselves.\" The dimensions in which we move are so different. \"The dimensions in which we move are so different that we are unable to identify ourselves.\" The dimensions in which we move are so different. \""}, {"heading": "5.3. Performance on Knowledge Base Completion Task", "text": "Table 4 shows the performance on the compositional operators for the knowledge base completion task on the two knowledge diagrams WN18 and FB15k, where low middle rank and high hits @ 10 indicate better performance. As can be seen from the table, Mult Operator delivers the lowest middle rank and highest hit accuracy among other operators for the two knowledge bases. Given that PairDiff was the best operator for relational similarity tasks, it is surprising that Mult Operator outperforms PairDiff in both WN18 and FB15k datasets. Remember that knowledge base completion is the task where the (h, t) pair is related by a relationship r (as provided in the train order), which we must evaluate how likely that (h, t, t) this evaluation process is by measuring the inner product between f (h, t) and f (h, mm), where f represents a compositional function."}, {"heading": "5.4. Evaluating the Asymmetry of the PairDiff Operator", "text": "If two words a and b are related by a symmetrical relationship, then b may not necessarily be related to a relationship with the same relation r. Examples of asymmetrical relationships include synonyms and antonyms. As discussed in Section 5.1, PairDiff surpasses operators Add and Mult. Unlike Mult and Add, which are commutative operators, PairDiff is a non-commutative operator. Therefore, PairDiff should be able to detect the direction of a relationship between two words."}, {"heading": "6. Discussion and conclusion", "text": "Specifically, we looked at several compositional operators, such as PairDiff, Mult, Add, and Concat, to create a representation (embedding) for the relationship between two words, since their word embeddings exist as input. We used various pre-trained word embeddings and evaluated the operators \"performance in two tasks: measuring relational similarity and completing the knowledge base. We observed that PairDiff is the best operator for measuring relational similarity, while Mult-Operator is the best for completing the knowledge base. Subsequently, we investigated the effect of dimensionality on the performance of these two operators and showed that the scarcity of embeddings affects the operator Mult, not the negativity of the input word, as speculated in previous work. Our analysis in this paper was limited to unattended operators in the sense that there are no parameters in the planes that we need to learn from the operators\" performance on the basis of their work)."}], "references": [{"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "in: Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Using relational similarity between word pairs for latent relational search on the web", "author": ["N.T. Duc", "D. Bollegala", "M. Ishizuka"], "venue": "in: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Corpus-based learning of analogies and semantic relations", "author": ["P.D. Turney", "M.L. Littman"], "venue": "Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Cross-language latent relational search: Mapping knowledge across languages", "author": ["N.T. Duc", "D. Bollegala", "M. Ishizuka"], "venue": "in: Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Www sits the sat: Measuring relational similarity on the web", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka"], "venue": "in: ECAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Aritificial Intelligence Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Measuring semantic similarity by latent relational analysis, arXiv preprint", "author": ["P.D. Turney"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Distributional structure, The Philosophy of Linguistics", "author": ["Z. Harris"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of artificial intelligence research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Efficient estimation of word representation in vector space", "author": ["T. Mikolov", "K. Chen", "J. Dean"], "venue": "in: Proceedings of International Conference on Learning Representations,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Glove: global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "in: HLT-NAACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in: Interspeech,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "in: EMNLP\u201910,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A regression model of adjective-noun compositionality in distributional semantics, in: ACL\u201910", "author": ["E. Guevara"], "venue": "Workshop on Geometrical Models of Natural Language Semantics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["T. Linzen"], "venue": "arXiv preprint arXiv:1606.07736", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Word embeddings, analogies, and machine learning: Beyond king-man+ woman= queen", "author": ["A. Drozd", "A. Gladkova", "S. Matsuoka"], "venue": "in: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning, arXiv preprint", "author": ["E. Vylomova", "L. Rimmel", "T. Cohn", "T. Baldwin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "in: ACL-HLT\u201908,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Holographic embeddings of knowledge graphs", "author": ["M. Nickel", "L. Rosasco", "T. Poggio"], "venue": "in: Proc. of AAAI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning meta-embeddings by using ensembles of embedding sets", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "in: Proc. of ACL,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Language models based on semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english, in: Proceedings of the 4th Web as Corpus Workshop (WAC-4", "author": ["A. Ferraresi", "E. Zanchetta", "M. Baroni", "S. Bernardini"], "venue": "Can we beat Google,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Vector space models of lexical meaning, Handbook of Contemporary Semantic Theory, The (2015) 493\u2013522", "author": ["S. Clark"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Algorithms for non-negative matrix factorization, in: Advances in neural information processing", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["P. Turney", "M.L. Littman", "J. Bigham", "V. Shnayder"], "venue": "in: Proceedings of the Recent Advances in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "in: NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "in: AAAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 229, "endOffset": 232}, {"referenceID": 3, "context": "For example, given the relational search query Bill Gates is to Microsoft as Steve Jobs is to?, a relational search engine [4] is expected to return the result Apple Inc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 4, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 5, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 6, "context": "Following the Vector Space Model (VSM) [7], each pair of words is represented using a vector of pattern frequencies where the elements correspond to the number of times the two words in a given pair co-occur with a particular pattern.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "1% [5, 8].", "startOffset": 3, "endOffset": 9}, {"referenceID": 7, "context": "1% [5, 8].", "startOffset": 3, "endOffset": 9}, {"referenceID": 8, "context": "Different approaches have been proposed in the NLP community for representing the meaning of individual words based on the distributional hypothesis [9], which states that the meaning of a word can be predicted by the words that co-occur with it in different contexts.", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Counting-based approaches [10] represent the meaning of a word by a potentially high-dimensional sparse vector, where each dimension corresponds to a particular word that co-occurs with the word under consideration in some context.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "The values of the dimensions are computed using some word association measure such as the pointwise mutual information or log-likelihood ratio [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "Prediction-based approaches have also been used for representing the meanings of words using vectors [12, 13].", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "Prediction-based approaches have also been used for representing the meanings of words using vectors [12, 13].", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "[15] show that the learnt word embeddings using recurrent neural network language model [16] captures linguistic regularities by simply applying vector offset and addition operators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] show that the learnt word embeddings using recurrent neural network language model [16] captures linguistic regularities by simply applying vector offset and addition operators.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 16, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 17, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 18, "context": "In practice however this method generates high dimensional and sparse vectors [22, 11].", "startOffset": 78, "endOffset": 86}, {"referenceID": 10, "context": "In practice however this method generates high dimensional and sparse vectors [22, 11].", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "For example, skip-gram and continuous bag-of-words models learn vectors that maximise the likelihood of co-occurrence contexts in a corpus [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "[15] report that word embeddings capture relational information between words by simple linear offset between words vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Following this work, alternative methods have been proposed and compared with 3CosAdd for analogical reasoning [23, 24, 25].", "startOffset": 111, "endOffset": 123}, {"referenceID": 20, "context": "Following this work, alternative methods have been proposed and compared with 3CosAdd for analogical reasoning [23, 24, 25].", "startOffset": 111, "endOffset": 123}, {"referenceID": 21, "context": "[26] conduct a study to evaluate how well the offset method encodes relational information between pairs of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Many compositional operators have been proposed for the purpose of representing sentences [19, 27].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "Many compositional operators have been proposed for the purpose of representing sentences [19, 27].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "For example, Mitchell and Lapata [27] introduce additive and multiplicative models for sentence representations, whereas Nickel et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "[28] proposed circular correlation for relational composition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] for detecting syntactic and semantic analogies using the offset method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Similar relations have shown to produce parallel vectors in prior work on word embedding learning [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Such geometric regularities are useful for NLP tasks such as solving word analogies [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "In particular, vector concatenation has been found to be effective for combining multiple source embeddings to a single meta embedding [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "shown that element-wise multiplication to be an effective method for composing representations for larger lexical units such as phrases or sentences from elementary lexical units such as words [27].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "For example, Baroni and Zamparelli [19] report that word embeddings created via singular value decomposition performs poorly when composing phrase representations because of this sign-flipping issue.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Element-wise multiplication and addition have been evaluated in compositional semantics for composing phrase-level or sentence-level representations from word-level representations [30, 27].", "startOffset": 181, "endOffset": 189}, {"referenceID": 22, "context": "Element-wise multiplication and addition have been evaluated in compositional semantics for composing phrase-level or sentence-level representations from word-level representations [30, 27].", "startOffset": 181, "endOffset": 189}, {"referenceID": 11, "context": "We consider three widely used prediction-based word embedding methods namely, Continuous Bag-of Words (CBOW), Skip-gram (SG)[12] and Global Vector Prediction (GloVe) [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "We consider three widely used prediction-based word embedding methods namely, Continuous Bag-of Words (CBOW), Skip-gram (SG)[12] and Global Vector Prediction (GloVe) [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 26, "context": "2 billion words [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "This count-based statistical method for word representations is widely applied in NLP to produce semantic representations for words and documents [32, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 10, "context": "This count-based statistical method for word representations is widely applied in NLP to produce semantic representations for words and documents [32, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 28, "context": "As an alternative dimensionality reduction method, we use Nonnegative Matrix Factorisation (NMF) in our experiments [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "For example, Mitchell and Lapata [27, 18] used a crowd sourced dataset of phrase similarity.", "startOffset": 33, "endOffset": 41}, {"referenceID": 15, "context": "For example, Mitchell and Lapata [27, 18] used a crowd sourced dataset of phrase similarity.", "startOffset": 33, "endOffset": 41}, {"referenceID": 4, "context": "This type of similarity is often referred to as relational similarity in prior work [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 29, "context": "Two benchmark datasets have been used frequently in prior work for evaluating relational similarity measures are SAT [34] dataset and SemEval 2012-Task2 [35] dataset.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "For the analogy completion task we use two datasets: MSR [15], and Google analogy [36] datasets.", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "For the analogy completion task we use two datasets: MSR [15], and Google analogy [36] datasets.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "[26] that consists of 12,458 triples \u3008w1, w2, r\u3009, where word w1 and w2 are connected by a relation r.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The datasets and the source code that generates entity embeddings are publicly available [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "If the negativity was the only issue with Mult operator as previously suggested by [19], then Mult should have performed better with NMF.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, nounmodifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words. This study aims to compare different operations for creating relation representations from word-level representations. We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy. Moreover, we evaluate the different relation representations in a knowledge base completion task.", "creator": "LaTeX with hyperref package"}}}