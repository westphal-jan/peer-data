{"id": "1709.01423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "A heterogeneity based iterative clustering approach for obtaining samples with reduced bias", "abstract": "Medical and social sciences demand sampling techniques which are robust, reliable, replicable and give samples with the least bias. Majority of the applications of sampling use randomized sampling, albeit with stratification where applicable to lower the bias. The randomized technique is not consistent, and may provide different samples each time, and the different samples themselves may not be similar to each other. In this paper, we introduce a novel sampling technique called Wobbly Center Algorithm, which relies on iterative clustering based on maximizing heterogeneity to achieve samples which are consistent, and with low bias. The algorithm works on the principle of iteratively building clusters by finding the points with the maximal distance from the cluster center. The algorithm consistently gives a better result in lowering the bias by reducing the standard deviations in the means of each feature in a scaled data", "histories": [["v1", "Sat, 2 Sep 2017 17:26:15 GMT  (468kb)", "http://arxiv.org/abs/1709.01423v1", "Submitted to Pattern Recognition Letters (Elsevier)"], ["v2", "Mon, 16 Oct 2017 06:19:42 GMT  (474kb)", "http://arxiv.org/abs/1709.01423v2", "Submitted to IEEE Access"]], "COMMENTS": "Submitted to Pattern Recognition Letters (Elsevier)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chandrasekaran anirudh bhardwaj", "megha mishra", "kalyani desikan"], "accepted": false, "id": "1709.01423"}, "pdf": {"name": "1709.01423.pdf", "metadata": {"source": "CRF", "title": "A heterogeneity based iterative clustering approach for obtaining samples with reduced bias", "authors": ["Chandrasekaran Anirudh Bhardwaj", "Megha Mishra", "Kalyani Desikan"], "emails": [], "sections": [{"heading": null, "text": "An iterative clustering technique based on heterogeneity to obtain samples with reduced bias Chandrasekaran Anirudh Bhardwaj1, Megha Mishra1 and Kalyani Desikan21 B. Tech Computer Science, VIT University, Vandalur-Kelabmbakam Road, Chennai - 600127, India2 Faculty-School of Advanced Sciences, VIT University, Vandalur-Kelabmbakam Road, Chennai - 600127, IndiaMedical and Social Sciences require robust, reliable, replicable and least biased samples. The majority of sample applications use randomized sampling, albeit with stratification, to reduce bias. Randomized technique is not consistent and can provide different samples each time, and the different samples themselves may not be similar. In this essay, we are performing a novel sampling technique called Wentbly based on the principle of observing the center's algorithm."}, {"heading": "1. Introduction", "text": "This year, it has come to the point where there is such a process, where the question is to what extent there has been such a process until there has been such a process."}, {"heading": "2. Wobbly Center Algorithm", "text": "The wobble center algorithm works on the principle of iterative cluster building based on maximizing dissimilarity within each cluster as a selection criterion. The dataset is scaled first because the algorithm is a scale variant. A vector is created that contains the mean of each feature for the entire dataset. The generated vector is called the center of the dataset. A suitable distance metric is selected and the distances of each point from the mean of each point are calculated from the mean of the center. Next, the points closest to the center are used as starting points for cluster generation, ensuring faster convergence rates and better overall results. Trivially, one could also conclude that the starting points are also the centers of each cluster for the first iteration. Next, the distances of each point from the centers of the cluster are calculated for each cluster."}, {"heading": "3. Test Conditions", "text": "The data was first scaled by normalizing the Z-value using the Standard Scaler function in the Ski-Kit Learn (22) Library. This transformation ensured that the data had no normal mean and unit deviation because the Wobbly Center algorithm is a scale variant (based on the type of distance measurement used).On the normalized dataset, the Wobbly Center algorithm was tested for random samples taken from the train _ test _ split function in the Ski-Kit Learn Library. All datasets were sourced from the repository hosted by the University of California."}, {"heading": "5 Experimental Analysis and Results", "text": "If the null hypothesis is not defined as a statistical deviation for the two samples obtained by the sweep-centered algorithm and the random splitter algorithm using Ski-Kit Learn with a confidence interval of 95% and then the alpha as = 0.05, we can test whether the splits obtained match or not the probability distribution of the other by comparing the obtained novavalue with the alpha. We use the one-way analysis of variance [25], (ANOVA) to test the hypothesis. When testing the hypothesis for splits obtained by the sweep-centered algorithm, we do not succeed in rejecting the null hypothesis for each attribute in the data set."}, {"heading": "5. Discussion", "text": "In fact, it is the case that you will be able to follow the rules that you have set yourself and then put them into practice."}], "references": [{"title": "Algorithm AS 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1979}, {"title": "Clustering by means of medoids", "author": ["Kaufman", "L. Rousseeuw", "Peter"], "venue": "Statistical Data Analysis Based on the L1 Norm and Related Methods", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1987}, {"title": "Clustering Large Applications (Program CLARA), in Finding Groups in Data: An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "CLARANS: a method for clustering objects for spatial data mining", "author": ["R.T. Ng", "Jiawei Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 5, pp. 1003-1016, Sep/Oct 2002.doi: 10.1109/TKDE.2002.1033770", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern recognition letters,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "FCM: The fuzzy c-means clustering algorithm, Computers & Geosciences", "author": ["James C. Bezdek", "Robert Ehrlich", "William Full"], "venue": "Volume 10, Issue", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Analysis and implementation of CLARA algorithm on clustering", "author": ["G.F. ZHAO", "G.Q. QU"], "venue": "Journal of Shandong University of Technology (Science and Technology),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Fast algorithms for projected clustering", "author": ["C.C. Aggarwal", "J.L. Wolf", "P.S. Yu", "C. Procopiuc", "J.S. Park", "June"], "venue": "In ACM SIGMoD Record (Vol. 28,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Random sampling techniques for space efficient online computation of order statistics of large datasets", "author": ["Gurmeet Singh Manku", "Sridhar Rajagopalan", "Bruce G. Lindsay"], "venue": "In Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Simultaneous Gaussian Model-Based Clustering for Samples of Multiple Origins", "author": ["Alexandre Lourme", "Christophe Biernacki"], "venue": "Computational Statistics, Springer Verlag,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Improved Sampling\u2010Importance Resampling and Reduced Bias Importance Sampling", "author": ["\u00d8. Skare", "E. B\u00f8lviken", "L. Holden"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Random sampling from databases (Doctoral dissertation, University of California at Berkeley)", "author": ["F. Olken"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Sampling for qualitative research", "author": ["M.N. Marshall"], "venue": "Family practice,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Focus on research methods combining qualitative and quantitative sampling, data collection, and analysis techniques", "author": ["M. Sandelowski"], "venue": "Research in nursing & health,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "An Experimental Study of Quota Sampling.", "author": ["C.A. Moser", "A. Stuart"], "venue": "Journal of the Royal Statistical Society. Series A (General)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1953}, {"title": "Introductory statistics (Vol. 19690)", "author": ["T.H. Wonnacott", "R.J. Wonnacott"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1972}, {"title": "An evaluation of model-dependent and probability-sampling inferences in sample surveys", "author": ["M.H. Hansen", "W.G. Madow", "B.J. Tepping"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1983}, {"title": "Design, data analysis and sampling techniques for clinical research", "author": ["K. Suresh", "S.V. Thomas", "G. Suresh"], "venue": "Annals of Indian Academy of Neurology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "The management and control of quality", "author": ["J.R. Evans", "W.M. Lindsay"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Abalone Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/abalone]. Irvine, CA: University of California, School of Information and Computer Science [Dataset", "author": ["Sam Waugh"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Using multivariate statistics", "author": ["B.G. Tabachnick", "L.S. Fidell", "S.J. Osterlind"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Scipy: Open Source Scientific Tools for Python, 2001-, http://www.scipy.org/ [Online; accessed", "author": ["E Jones", "E Oliphant", "P Peterson"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Wine Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/wine]. Irvine, CA: University of California, School of Information and Computer Science [Dataset", "author": ["Stefan Aeberhard"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Cloud Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/cloud] Irvine, CA: University of California, School of Information and Computer Science [Dataset", "author": ["Philippe Collard"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Normal Approximations and the Central Limit Theorem. In: Fundamentals of Probability: A First Course. Springer Texts in Statistics. Springer, New York, NY, https://doi.org/10.1007/978-1-4419-5780-1_10 i The research is under consideration at Pattern Recognition Letters (Elsevier", "author": ["A. DasGupta"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "The K-means clustering algorithm gave rise to the similarity based clustering paradigm [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "Algorithms like Fuzzy c-means [7] introduced the concept of fuzziness coefficient and multiple membership to the clustering approach.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Scalability in clustering was introduced by CLARA [8] and CLARANS [9] by employing different sampling techniques and combining different clustering approaches.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Scalability in clustering was introduced by CLARA [8] and CLARANS [9] by employing different sampling techniques and combining different clustering approaches.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Techniques like one pass sampling [10] for large datasets can also been deployed for reducing the numerosity of data points.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "Simultaneous Gaussian Model-Based clustering [11] for samples of multiple origin involves splitting the input data into multiple origin sources which they may have originated from.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "The Sampling Importance Resampling commonly abbreviated as SIR [12] technique is objectively a better method of sampling as compared to simple randomized sampling [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "The Sampling Importance Resampling commonly abbreviated as SIR [12] technique is objectively a better method of sampling as compared to simple randomized sampling [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "As the random sampling error is inversely proportional to the square root of sample size [14], very small samples are inherently not suitable for generalization.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "There are principally two types of sampling, Quantitative sampling and Qualitative Sampling [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "Qualitative sampling tries to reduce the sample size from thousands of data points to a few tens of representative data points [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "There are many applications of reliable and consistent sampling technique such as Medical Testing [20], Human Resources [21] etc.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "There are many applications of reliable and consistent sampling technique such as Medical Testing [20], Human Resources [21] etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "All the datasets were procured from the repository hosted by the University of California, Irvine [23].", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "We use one-way Analysis of Variance [25], (ANOVA), to test the hypothesis.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Using ANOVA on the splits obtained by the Random Split Algorithm, we find that there exists at least one statistically deviant attribute for the dataset which is rejected by the null hypothesis ANOVA was performed using SCIPY [26] stats one_way_anova function.", "startOffset": 226, "endOffset": 230}, {"referenceID": 20, "context": "b represent the graph for distance of individual means of each cluster from the actual means of the data for Abalone [24] Dataset for the first 100 iterations of WCA", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "a represents the results calculated for Abalone [24] Dataset for Wobbly Center Algorithm for 2 clusters Attribute Mean(Cluster 1) Mean(Cluster 2) STD(Cluster 1) STD(Cluster 2) 1-way Anova", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "b represents the results calculated for Abalone [24] Dataset for Random Sampling Algorithm for 2 clusters Attribute Mean(Cluster 1) Mean(Cluster 2) STD(Cluster 1) STD(Cluster 2) 1-way Anova", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "The values obtained for other datasets, Wine Dataset [27] and Cloud Cover [28] Dataset, also indicate similar results as Abalone Dataset.", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "The values obtained for other datasets, Wine Dataset [27] and Cloud Cover [28] Dataset, also indicate similar results as Abalone Dataset.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "The Wobbly Center Algorithm tries to overcome a flaw that the Central Limit Theorem [29], henceforth abbreviated as CLT does not cover.", "startOffset": 84, "endOffset": 88}], "year": 2017, "abstractText": "Medical and social sciences demand sampling techniques which are robust, reliable, replicable and give samples with the least bias. Majority of the applications of sampling use randomized sampling, albeit with stratification where applicable to lower the bias. The randomized technique is not consistent, and may provide different samples each time, and the different samples themselves may not be similar to each other. In this paper, we introduce a novel sampling technique called Wobbly Center Algorithm, which relies on iterative clustering based on maximizing heterogeneity to achieve samples which are consistent, and with low bias. The algorithm works on the principle of iteratively building clusters by finding the points with the maximal distance from the cluster center. The algorithm consistently gives a better result in lowering the bias by reducing the standard deviations in the means of each feature in a scaled data.", "creator": "Microsoft\u00ae Word 2016"}}}