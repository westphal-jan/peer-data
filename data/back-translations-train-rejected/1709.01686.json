{"id": "1709.01686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks", "abstract": "Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with additional side branch classifiers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected less frequently, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.", "histories": [["v1", "Wed, 6 Sep 2017 06:30:51 GMT  (3736kb,D)", "http://arxiv.org/abs/1709.01686v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["surat teerapittayanon", "bradley mcdanel", "h t kung"], "accepted": false, "id": "1709.01686"}, "pdf": {"name": "1709.01686.pdf", "metadata": {"source": "CRF", "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks", "authors": ["Surat Teerapittayanon", "Bradley McDanel"], "emails": ["steerapi@seas.harvard.edu", "mcdanel@fas.harvard.edu", "kung@harvard.edu"], "sections": [{"heading": null, "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "II. BACKGROUND AND RELATED PRIOR WORK", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "A. Architecture", "text": "A BranchyNet network consists of an entry point and one or more exit points. A branch is a subset of the network that contains contiguous layers that do not overlap with other branches, followed by an exit point. The main branch can be considered the base network (original) before side branches are added. Starting from the lowest branch that moves to the highest branch, we number each branch and the associated exit point with increasing integers starting at one."}, {"heading": "B. Training BranchyNet", "text": "For a classification task, the softmax cross-entropy loss function is usually used as an optimization target. Here, we describe how BranchyNet uses this loss function. Let y be a unified ground-truth label vector, x be an input example, and C be the set of all possible labels. The objective function can be written asL (y, y; \u03b8) = \u2212 1 | C | \u2211 c-c yc log y-c, where f-x = softmax (z) = exp (z) \u2211 c-C exp (zc), andz = fexitn (x; \u03b8), where fexitn is the output of the n-th exit branch and \u03b8 is the parameters of the layers from entry point to exit point.The design goal of each exit branch is to minimize this loss function. To train the entire BranchyNet, we form a common optimization problem as a weighted sum of the loss functions of each branch exit branch Lynet."}, {"heading": "C. Fast Inference with BranchyNet", "text": "After training, BranchyNet can be used for quick conclusions by classifying samples at earlier stages of the network based on the algorithm in Figure 2. If the classifier has high confidence in the correct labeling of a test sample x at an exit point of a branch, the sample will exit and return a predicted label without the higher branches of the network performing any further calculations. We use entropy as a measure of how safe the classifier is at an exit point over the sample. Entropy is defined as asentropy, where y is a vector that contains calculated probabilities for all possible class designations and C is a set of all possible labels. To quickly draw conclusions about a particular BranchyNet network, we follow the procedure as described in Figure 2. The procedure requires T, a vector, where the n-th entry is the threshold used to determine whether we should terminate the end point at the exit point, and the end point at the exit point, respectively."}, {"heading": "IV. RESULTS", "text": "In this section, we will show the effectiveness of BranchyNet by connecting three widely studied Convolutionary Networks at the level of image classification: LeNet, AlexNet, and ResNet. We will evaluate Branchy-LeNet (B-LeNet) at the level of MNIST data, which we both select in terms of the exit threshold of Alexios Alexios (B-ResNet) and Branch-ResNet (B-ResNet) at the level of CIFAR10 data. We will present evaluation results for both the CPU and the GPU threshold. We will use a 3.0 GHz CPU with 20MB L3 cache and NVIDIA GeForce GITAN X (Maxwell) 12GB GPU. We will only describe the conventional and fully connected layers of each network. Generally, these networks also include maximum activation, non-linear activation, and non-linear functions (e.g. a reflected linear unit and sigmatic unit)."}, {"heading": "A. Hyperparameter Sensitivity", "text": "Two important BranchyNet hyperparameters are the weights in the joint optimization (section III-B) and the exit thresholds T for the fast inference algorithm described in Figure 2. In selecting the weight of each branch, we observed that a higher weighting of earlier branches improves the accuracy of later branches due to additional regulation. In a simplified version of BranchyAlexNet with only the first and last branches, weighting the first branch with 1.0 and the last branch with 0.3 leads to a 1% higher classification accuracy than the same weighting of all branches. A stronger weighting of earlier branches promotes more differentiated feature learning in early layers of the network and allows for early exit with high confidence. Figure 4 shows how the choice of T influences the number of samples taken from the first branch point."}, {"heading": "B. Tuning Entropy Thresholds", "text": "The results shown in Figure 3 provide the accuracy and runtime for a range of T values. These T values show how BranchyNet calculates the accuracy for a faster runtime as the entropy thresholds rise. In practice, however, we could automatically set T to meet a certain runtime or accuracy limit. One approach is to simply scan through Tas and select a setting that meets the constraints. We have provided code that is used to generate performance results that includes a method for performing this screening [24]. In addition, it may be possible to use a meta-recognition algorithm [19], [26] to estimate the properties of invisible test samples and automatically adjust T to maintain a specified runtime or accuracy target. A simple approach to creating such a meta meta recognition algorithm would be to set a small multilayer Perceptron (LP) for each starting point."}, {"heading": "C. Effects of Structure of Branches", "text": "Figure 5 shows the impact on the accuracy of the last branch by adding additional convex layers in an earlier branch for a modified version of B-AlexNet with only the first branch. We see that there is an optimal number of layers to improve the accuracy of the main branch, and that adding too many layers can actually affect overall accuracy. In addition to the convex layers, adding some fully bonded layers after convex layers to a branch is also helpful, as it can combine local and global features and form more differentiated features. The number of layers in a branch and the size of a branch should be chosen so that the total size of the branch requires less computing power to leave it at a later exit point. Generally, we find that earlier branch points should have more layers and later branch points fewer layers."}, {"heading": "D. Effects of cache", "text": "Since the majority of samples are left at early branching points, the later branches are used less frequently, allowing the weights at these early branches to be cached more efficiently. Figure 6 shows the effect of caching based on different T values for B-AlexNet. We see that the more aggressive T values have faster CPU runtime and lower cache failure rates. We could use this insight to select an industry structure that fits more effectively into a cache, potentially accelerating inference further."}, {"heading": "VI. CONCLUSION", "text": "We have proposed BranchyNet, a novel network architecture that promotes faster conclusions through early outputs from branches. By providing appropriate branching structures and exit criteria, as well as common optimization of loss functions for all exit points, the architecture is able to take advantage of the insight that many test patterns can be correctly classified at an early stage and therefore do not need the later network layers. We have evaluated this approach on several popular network architectures and demonstrated that BranchyNet can reduce the inference costs of deep neural networks and provide two- to six-fold acceleration on both the CPU and the GPU. BranchyNet is a toolbox that researchers can use on all deep network models for quick conclusions. BranchyNet can be used in conjunction with previous work such as network pruning and network compression [3], [5]. BranchyNet can be adapted to solve other types of problems such as image segmentation and is not limited to mere classification problems."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported in part by gifts from Intel Corporation and in part by the Naval Supply Systems Command, which advance the in-depth education of the international networks of intelligence. [1] He has the networks of the Naval Postgraduate School Agreements under the title 24ally 24ally 2015- 244-15-0050 and No. N00244-16-0018.REFERENCES [1] A. Bendale and T. Boult. Towards open set deep networks. arXiv preprint arXiv: 1511.06233, 2015. [2] A. Bendale and T. Boult. Towards open world recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1893-1902, 2015. [3] C. Bucilu, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge and data mining, 535-541 pages."}], "references": [{"title": "Towards open set deep networks", "author": ["A. Bendale", "T. Boult"], "venue": "arXiv preprint arXiv:1511.06233", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards open world recognition", "author": ["A. Bendale", "T. Boult"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1893\u20131902", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep compression: Compressing deep neural networks with pruning", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "trained quantization and huffman coding. arXiv preprint arXiv:1510.00149", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1026\u20131034", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint arXiv:1603.09382", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast algorithms for convolutional neural networks", "author": ["A. Lavin"], "venue": "arXiv preprint arXiv:1509.09308", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional deep learning for energy-efficient and enhanced pattern recognition", "author": ["P. Panda", "A. Sengupta", "K. Roy"], "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 475\u2013480. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Metarecognition: The theory and practice of recognition score analysis", "author": ["W.J. Scheirer", "A. Rocha", "R.J. Micheals", "T.E. Boult"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 33(8):1689\u2013 1695", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u2013 1958", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Branchynet: Fast inference via early exiting from deep neural networks", "author": ["S. Teerapittayanon", "B. McDanel", "H. Kung"], "venue": "https://gitlab.com/htkung/branchynet", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, volume 1, page 4", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Predicting failures of vision systems", "author": ["P. Zhang", "J. Wang", "A. Farhadi", "M. Hebert", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3566\u20133573", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "In recent years, advances in both hardware and learning techniques have emerged to train even deeper networks, which have improved classification performance further [4], [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "In recent years, advances in both hardware and learning techniques have emerged to train even deeper networks, which have improved classification performance further [4], [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 6, "context": "The ImageNet challenge exemplifies the trend to deeper networks, as the state of the art methods have advanced from 8 layers (AlexNet), to 19 layers (VGGNet), and to 152 layers (ResNet) in the span of four years [7], [13], [20].", "startOffset": 212, "endOffset": 215}, {"referenceID": 12, "context": "The ImageNet challenge exemplifies the trend to deeper networks, as the state of the art methods have advanced from 8 layers (AlexNet), to 19 layers (VGGNet), and to 152 layers (ResNet) in the span of four years [7], [13], [20].", "startOffset": 217, "endOffset": 221}, {"referenceID": 19, "context": "The ImageNet challenge exemplifies the trend to deeper networks, as the state of the art methods have advanced from 8 layers (AlexNet), to 19 layers (VGGNet), and to 152 layers (ResNet) in the span of four years [7], [13], [20].", "startOffset": 223, "endOffset": 227}, {"referenceID": 10, "context": "For example, experiments that compare VGGNet to AlexNet on a Titan X GPU have shown a factor of 20x increase in runtime and power consumption for a reduction in error rate of around 4% (from 11% to 7%) [11].", "startOffset": 202, "endOffset": 206}, {"referenceID": 14, "context": "LeNet-5 [15] introduced the standard convolutional neural networks (CNN) structure which is composed of stacked convolutional layers, optionally followed by contrast normalization and maxpooling, and then finally followed by one or more fully-connected layers.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "AlexNet [13], VGG [20], ResNet [7] and others have expanded on this structure with their own innovative approaches to make the network deeper and larger for improved classification accuracy.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "AlexNet [13], VGG [20], ResNet [7] and others have expanded on this structure with their own innovative approaches to make the network deeper and larger for improved classification accuracy.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "AlexNet [13], VGG [20], ResNet [7] and others have expanded on this structure with their own innovative approaches to make the network deeper and larger for improved classification accuracy.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "(2006) proposed a method of compressing a deep network into a smaller network that achieves a slightly reduced level of accuracy by retraining a smaller network on synthetic data generated from a deep network [3].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "(2015) have proposed a pruning approach that removes network connections with small contributions [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "However, while pruning approaches can significantly reduce the number of model parameters in each layer, converting that reduction into a significant speedup is difficult using standard GPU implementations due to the lack of high degrees of exploitable regularity and computation intensity in the resulting sparse connection structure [6].", "startOffset": 335, "endOffset": 338}, {"referenceID": 10, "context": "(2015) use a Tucker decomposition (a tensor extension of SVD) to extract shared information between convolutional layers and perform rank selection [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "(2011) explored code optimizations to speed up the execution of convolutional neural networks (CNNs) on CPUs [25].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "(2013) showed that convolution using FFT can be used to speed up training and inference for CNNs [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "(2015) have introduced faster algorithms specifically for 3x3 convolutional filters (which are used extensively in VGGNet and ResNet) [14].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "Dropout [21], L1 and L2 regularization and many other techniques have been used to regularize the network and prevent overfitting.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "(2015) introduced the concept of adding softmax branches in the middle layers of their inception module within deep networks as a way to regularize the main network [23].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "Several papers have introduced ideas to mitigate this issue including normalized network initialization [4], [16] and intermediate normalization layers [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Several papers have introduced ideas to mitigate this issue including normalized network initialization [4], [16] and intermediate normalization layers [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Several papers have introduced ideas to mitigate this issue including normalized network initialization [4], [16] and intermediate normalization layers [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Recently, new approaches such as Highway Networks [22], ResNet [7], and Deep Networks with Stochastic Depth [9] have been studied.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "Recently, new approaches such as Highway Networks [22], ResNet [7], and Deep Networks with Stochastic Depth [9] have been studied.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Recently, new approaches such as Highway Networks [22], ResNet [7], and Deep Networks with Stochastic Depth [9] have been studied.", "startOffset": 108, "endOffset": 111}, {"referenceID": 17, "context": "[18] propose Conditional Deep Learning (CDL) by iteratively adding linear classifiers to each convolutional layer, starting with the first layer, and monitoring the output to decide whether a sample can be exited early.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "For gradient descent, we use Adam algorithm [12], though other variants of Stochastic Gradient Descent (SGD) can also be used.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "For LeNet-5 [15] which consists of 3 convolutional layers and 2 fully-connected layers, we add a branch consisting of 1 convolutional layer and 1 fully-connected layer after the first convolutional layer of the main network.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "For AlexNet [13] which consists of 5 convolutional layers and 3 fully-connected layers, we add 2 branches.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "For ResNet-110 [7] which consists of 109 convolutional layers and 1 fully-connected layer, we add 2 branches.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "For this evaluation, we use batch size 1 as evaluated in [5], [11] in order to target real-time streaming applications.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "For this evaluation, we use batch size 1 as evaluated in [5], [11] in order to target real-time streaming applications.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "We provided code used to generate the performance results which includes a method for performing this screening [24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": "Additionally, it may be possible to use a Meta-Recognition algorithm [19], [26] to estimate the characteristics of unseen test samples and adjust T automatically in order to maintain a specified runtime or accuracy goal.", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "Additionally, it may be possible to use a Meta-Recognition algorithm [19], [26] to estimate the characteristics of unseen test samples and adjust T automatically in order to maintain a specified runtime or accuracy goal.", "startOffset": 75, "endOffset": 79}, {"referenceID": 1, "context": "More generally, this approach is closely related to the open world recognition problem [2], [1], which is interested in quantifying the uncertainty of a model for a particular set of unseen or out of set test samples.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "More generally, this approach is closely related to the open world recognition problem [2], [1], which is interested in quantifying the uncertainty of a model for a particular set of unseen or out of set test samples.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "We can expand on the MLP approach further by using a different formulation than SoftMax, such as OpenMax [2], which attempts to quantify the uncertainty directly in the probability vector \u0177 by adding an additional uncertain class.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "BranchyNet can be used in conjunction with prior works such as network pruning and network compression [3], [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "BranchyNet can be used in conjunction with prior works such as network pruning and network compression [3], [5].", "startOffset": 108, "endOffset": 111}], "year": 2017, "abstractText": "Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with additional side branch classifiers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected less frequently, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.", "creator": "LaTeX with hyperref package"}}}