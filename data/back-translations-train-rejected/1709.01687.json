{"id": "1709.01687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction Mention Extraction", "abstract": "Social media is an useful platform to share health-related information due to its vast reach. This makes it a good candidate for public-health monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from twitter. Medical information extraction from social media is challenging, mainly due to short and highly information nature of text, as compared to more technical and formal medical reports.", "histories": [["v1", "Wed, 6 Sep 2017 06:42:22 GMT  (1673kb)", "http://arxiv.org/abs/1709.01687v1", "Accepted at DTMBIO workshop, CIKM 2017. To appear in BMC Bioinformatics. Pls cite that version"]], "COMMENTS": "Accepted at DTMBIO workshop, CIKM 2017. To appear in BMC Bioinformatics. Pls cite that version", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["shashank gupta", "sachin pawar", "nitin ramrakhiyani", "girish palshikar", "vasudeva varma"], "accepted": false, "id": "1709.01687"}, "pdf": {"name": "1709.01687.pdf", "metadata": {"source": "META", "title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction Mention Extraction", "authors": ["Shashank Gupta", "Sachin Pawar", "Nitin Ramrakhiyani", "Girish Keshav Palshikar", "Vasudeva Varma"], "emails": ["shashank.gupta@research.iiit.ac.in", "sachin7.p@tcs.com", "nitin.ramrakhiyani@tcs.com", "gk.palshikhar@tcs.com", "vv@iiit.ac.in"], "sections": [{"heading": null, "text": "In fact, it is a good candidate for public relations, especially for pharmacovigilance. We are investigating the problem of extracting Adverse Drug Reaction (ADR) mentions from social media, especially from tweets. Extracting medical information from social media is a challenge, especially due to the short and highly informal nature of the text, compared to more technical and formal medical reports. Current methods in ADRD extraction rely on supervised learning methods affected by the labeled data shortage."}, {"heading": "1 INTRODUCTION", "text": "One study observed that the number of deaths in the range of (44,000-98,000) due to medical errors, 7,000 from ADRs.1. Drug monitoring in the postal market is therefore necessary to identify such potentially adverse reactions. Such platforms can be slow and ineffective. Studies show that 94% of ADRs are underrepresented [5].Social media is a useful platform to conduct such surveillance in the postal market, given the large audience and the vast reach of such platforms. Such platforms have been used for real-time information that is retrieval and tracks trends, including digital disease surveillance systems. The current study shows that two times more ADRs have been reported than have been reported by FDA. Of 61,000 tweets mentioning ADRs as ADRs, 1,400 are compared by FDA."}, {"heading": "2 RELATED WORK", "text": "The problem of ADRmention extraction falls into the category of sequence marking problem. The most modern method of sequence marking is Conditional Random Fields (CRFs) [10]. ADRMine [16] is a CRF-based model for ADR extraction tasks. It uses a variety of handcra ed features, including word context, ADR lexicon, POS tag and word embedding features as input into CRF.e word embedding features, which are trained on a large domain-specific tweet corpus. e problem with the above-mentioned approach is its dependence on handcra ed features, which requires time and effort. To work around this problem, a Long Short Term Memory (LSTM) network-based model is proposed [1]. Instead of using humanly constructed features, word embedding features are trained on a sequence sequence-based STM model to generate a CRF."}, {"heading": "3 ADR-MENTION EXTRACTION USING SEMI-SUPERVISED BI-DIRECTIONAL LSTM", "text": "In this section, we present our approach to ADR extraction, based on a semi-supervised learning method that works in two phases: 1) Unsupervised learning: At this stage, we train a bi-directional LSTM model to predict the drug name based on its context in the tweet. As training data for this task, we select tweets with a specific mention of a prescription drug. As we already know the drug name, there is no need for any comment. 2) Supervised learning: In this phase, we use the same bi-directional LSTM model from phase 1 and (train) it to predict the sequence of labels based on the tweet text."}, {"heading": "3.1 Unsupervised learning", "text": "For this phase, we select a novel task of predicting the drug name from its context in the tweet. For training data, we use a large collection of tweets with exactly one mention of the drug name in them. Since we predict the drug name from a tweet that already exists in the tweet, we use a trivial function to avoid using a dummy token to map the drug name in the input to the drug name in the output, without taking into account the context, the drug name in the tweet. For function extraction, we use a bi-directional LSTM model. The e-model takes as input a sequence of consecutive word vectors as input and predicts a corresponding sequence of word vectors as output tokens. e equations that determine the dynamics of LSTMs are defined as follows: \u00ae \u00a4ht = average LSTMs are plugged into the distribution (W-wired-wired-I)."}, {"heading": "3.2 Supervised Sequence Classification", "text": "For this phase, we use the bidirectional LSTM, which was trained from the previous phase after the setup, similar to the state of the art [1]. At each time step of the sequence, a so-called maximum layer is plotted, which predicts a probability distribution via sequence markers. Formally, yt = so f tmax (Wh + b) (4) here are W and b weight matrices for the so-called maximum layer. The final loss for the sequence marking is the sum of categorical cross entropy losses in each time. e hidden state h and the parameters Wu, Wf, Wo, Wc, Iu, If, Io, Ic are divided during the training of both phases. e The intuition around the unattended task is that the network can learn the textual context in which drug names appear, which can help in the identification of adverse drug reactions from drugs."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "The tweets were collected using 81 drug names as keyword search terms. In the original record, a total of 960 tweets with ADR mentions are commented on. Due to the license of Twitter's search APIs, only tweet IDs were published. Of the total of 960 tweets, we collected a total of 645 tweets with Python library weepy2. According to the default train test split, 470 tweets will be used for training and 170 tweets for testing. For the unmarked record, we used the Twi-ers search API 3 with the drug names used in the original study as keyword search terms 4. We searched the tweets over a period of two months. For simplicity, we deleted the tweets with more than one drug mention, resulting in a total of 0.1 million tweets."}, {"heading": "4.2 Implementation Details", "text": "We use Keras5 for the implementation. For the text preprocessing, we have applied several pre-processing steps, namely: \u2022 Normalization of HTML links and user mentions: We have replaced all HTML linker mentions with the token \"< LINK >.\" Similarly, we have removed all user handle mentions (e.g. @ JonDoe) with the token \"< USER >.\" \u2022 Special Character Removal: We have removed all punctuation marks and special symbols such as \"#\" from tweets. \u2022 Emoticons Removal: We have removed all emoticons, especially all non-ascii characters, which are special types of emoticons. \u2022 Stop-Word and Remove Remoticons rare words: We have removed all stop words and set the vocabulary size to top 15,000 most common words in the corpus.We have set the word2vec model Corviviviviconi characters from Worviviconi Worviviconi Words and Removiviconi Words."}, {"heading": "4.3 Results", "text": "To turn the ADR extraction problem into a sequence labeling problem, we need to assign commented units with appropriate tag representations. We follow the IO encoding scheme, in which each word belongs to one of the following categories: (1) I-ADR (within the ADR) (2) I-Indication (within the indication) (3) O (outside of any mention) (4) < PAD > (if the word is a padding token). It should be noted that, similar to the baseline method [1], we only report on performance on the ADR label, due to the very low number of indication comments in the # 6. An exemplary tweet associated with IO encoding: @ BLENDOSO LamictalO andO trileptalO andO seroquelO courseO seroquelO situelsequelO analysequelO becomes serotake O seroquelO."}, {"heading": "4.4 Analysis", "text": "4.4.1 Effect of the drug mask. For the unattended learning phase, we choose the task of predicting the drug name based on its context. To prevent the network from learning a degenerated function that maps the drug name to the drug name, we mask all the drug names in the input mask with a single symbol. To verify this, we will in training 16 in tests 7h ps: / / github.com / chop-dbhi / twi er-adr-blstmreport the accuracy results without the drug mask, i.e. with the drug name contained in the input mask, 16 in tests 7h ps: / github.com / chop-dbhi er-blstmreport the accuracy results without the drug-mask, i.e. with the drug name contained in the input mask. Result is presented in Table 2. It is clear that 35% of the drug mask is the final performance."}, {"heading": "5 CONCLUSIONS", "text": "We present a novel semi-supervised bidirectional LSTM-based model for ADR mention extraction. We evaluate our method using an annotated two-dimensional corpus. By using a potentially large, unlabeled corpus, our method outperforms the state-of-the-art method by 3.01% in the F1 score. We also show that word embeddings trained on a large domain agnostic two-dimensional corpus are better than more popular, Google News Corpus-trained word embeddings and surprisingly even better than medical, domain-specific word embeddings 9h ps: / / code.google.com / archive / p / word2vec / 10h ps: / / zenodo.org / record / 27354 #. WWWWYph1ekW4Atracted on Tweets, suggesting that language structure and semantics are more important in downstream information extraction tasks, if we know about drug side effects and in combination with DR."}], "references": [{"title": "Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twi\u008aer posts", "author": ["Anne Cocos", "Alexander G Fiks", "Aaron J Masino"], "venue": "Journal of the American Medical Informatics Association (2017),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Digital Drug Safety Surveillance: Monitoring Pharmaceutical Products in Twi\u008aer", "author": ["Clark C. Freifeld", "John S. Brownstein", "Christopher M. Menone", "Wenjie Bao", "Ross Filice", "Taha Kass-Hout", "Nabarun Dasgupta"], "venue": "Drug Safety 37,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Multimedia lab@ acl w-nut ner shared task: named entity recognition for twi\u008aer microposts using distributed word representations", "author": ["Fr\u00e9deric Godin", "Baptist Vandersmissen", "Wesley De Neve", "Rik Van de Walle"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1211.3711", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Under-reporting of Adverse Drug Reactions: A Systematic Review", "author": ["Lorna Hazell", "Saad Aw Shakir"], "venue": "Pharmacoepidemiology and Drug Safety", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Long short-termmemory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Adverse drug reaction classification with deep neural networks", "author": ["Trung Huynh", "Yulan He", "Allistair Willis", "Stefan R\u00fcger"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Adam: Amethod for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "author": ["Ji Young Lee", "FranckDernoncourt"], "venue": "In Proceedings of NAACL- HLT", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Mining social media streams to improve public health allergy surveillance", "author": ["Kathy Lee", "Ankit Agrawal", "Alok Choudhary"], "venue": "ASONAM", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adverse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks", "author": ["Kathy Lee", "Ashequl Qadir", "Sadid A Hasan", "Vivek Datla", "Aaditya Prakash", "Joey Liu", "Oladimeji Farri"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Adapting phrase-based machine translation to normalise medical terms in social media", "author": ["Nut Limsopatham", "Nigel Collier"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features", "author": ["Azadeh Nikfarjam", "Abeed Sarker", "Karen OfiConnor", "Rachel Ginn", "Graciela Gonzalez"], "venue": "Journal of the American Medical Informatics Association 22,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Various criteria in the evaluation of biomedical named entity recognition", "author": ["Richard Tzong-Han Tsai", "Shih-Hung Wu", "Wen-Chi Chou", "Yu-Chun Lin", "Ding He", "Jieh Hsiang", "Ting-Yi Sung", "Wen-Lian Hsu"], "venue": "BMC bioinformatics 7,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "Short-Term-Memory networks (LSTMs) [6].", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Studies show that 94% ADRs are underreported [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "surveillance system [12].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Out of 61,000 tweets collected, 4400 had mention of ADRs as compared to 1400 ADRs reported through FDA during the same time-period [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Recent work in deep learning has demonstrated its superiority over traditional hand-cra\u0089ed feature based machine learning models [8, 11].", "startOffset": 129, "endOffset": 136}, {"referenceID": 10, "context": "Recent work in deep learning has demonstrated its superiority over traditional hand-cra\u0089ed feature based machine learning models [8, 11].", "startOffset": 129, "endOffset": 136}, {"referenceID": 3, "context": "In this work, we present a novel semi-supervised Recurrent Neural Network (RNN) [4] based method for ADR mention extraction, specifically leveraging a relatively large unlabeled data.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "We demonstrate the effectiveness of our method through experimentation on ADRmention annotated tweet corpus [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "\u2022 Wepropose a novel semi-supervised sequence labelingmethod based onRNN, specifically Long-Short-Term-Memory Network [6] which are known to capture long-term dependencies be\u008aer than vanilla RNN.", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "\u2022 On the twi\u008aer dataset with ADR mentions annotated [1], our method achieves an F-score of 0.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "State-of-the-art method for sequence labeling problem is Conditional Random Fields (CRFs) [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "ADRMine [16], is a CRF-based model for ADR extraction task.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "A Long-Short-Term-Memory (LSTM) network based model is proposed [1] to get around this problem.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Some recent work also focuses on the problem of Adverse-DrugEvent (ADE) detection [7, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "Some recent work also focuses on the problem of Adverse-DrugEvent (ADE) detection [7, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 0, "context": "For this phase, we reuse the Bidirectional LSTM trained from the previous phase following the setup similar to state-of-the-art [1].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Baseline [1] 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "We used the word2vec [15] embeddings trained on a large generic twi\u008aer corpus [3] as input to the model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "We used the word2vec [15] embeddings trained on a large generic twi\u008aer corpus [3] as input to the model.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "BiLSTM parameters are set to the best reported se\u008aing from [1], with hidden unit\u2019s dimension equal to 500.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "io/ the supervised model, we use the adam optimizer [9] with batchsize equal to 1 and for training the unsupervised model, we used the batch adam optimizer [9] with batch-size set to 128.", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "io/ the supervised model, we use the adam optimizer [9] with batchsize equal to 1 and for training the unsupervised model, we used the batch adam optimizer [9] with batch-size set to 128.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "similar to the baseline method [1] we report the performance on the ADR label only.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].", "startOffset": 255, "endOffset": 259}, {"referenceID": 0, "context": "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].", "startOffset": 323, "endOffset": 330}, {"referenceID": 15, "context": "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].", "startOffset": 323, "endOffset": 330}, {"referenceID": 0, "context": "fers from the one used in baseline [1], we re-ran their model using the source-code released by them7.", "startOffset": 35, "endOffset": 38}, {"referenceID": 16, "context": "It should be noted that the original model used RMSProp [17] as an optimizer, so for a fair comparison we also report the baseline results with optimizer as adam instead of RMSProp.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "We also experiment withword embeddings trained on a largemedical-concept terms related tweet corpus10 [14].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "Social media is an useful platform to share health-related information due to its vast reach. \u008cis makes it a good candidate for publichealth monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from Twi\u008aer. Medical information extraction from social media is challenging, mainly due to short and highly informal nature of text, as compared to more technical and formal medical reports. Current methods in ADRmention extraction rely on supervised learning methods, which suffer from labeled data scarcity problem. \u008ce State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are LongShort-Term-Memory networks (LSTMs) [6]. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in the real-world, it is hard to get large labeled data, mainly due to the heavy cost associated with the manual annotation. To this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. \u008crough experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention", "creator": "LaTeX with hyperref package"}}}