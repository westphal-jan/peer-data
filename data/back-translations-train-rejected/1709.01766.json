{"id": "1709.01766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation Model", "abstract": "Even though sequence-to-sequence neural machine translation (NMT) model have achieved state-of-art performance in the recent fewer years, but it is widely concerned that the recurrent neural network (RNN) units are very hard to capture the long-distance state information, which means RNN can hardly find the feature with long term dependency as the sequence becomes longer. Similarly, convolutional neural network (CNN) is introduced into NMT for speeding recently, however, CNN focus on capturing the local feature of the sequence; To relieve this issue, we incorporate a relation network into the standard encoder-decoder framework to enhance information-propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. Experiments show that proposed framework outperforms the statistical MT model and the state-of-art NMT model significantly on two data sets with different scales.", "histories": [["v1", "Wed, 6 Sep 2017 11:13:50 GMT  (2308kb,D)", "http://arxiv.org/abs/1709.01766v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wen zhang", "jiawei hu", "yang feng", "qun liu"], "accepted": false, "id": "1709.01766"}, "pdf": {"name": "1709.01766.pdf", "metadata": {"source": "CRF", "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation Model", "authors": ["Wen Zhang", "Jiawei Hu", "Yang Feng", "Qun Liu"], "emails": ["zhangwen@ict.ac.cn", "hujiawei@ict.ac.cn", "fengyang@ict.ac.cn", "qliu@computing.dcu.ie"], "sections": [{"heading": "Introduction", "text": "In recent years, the neural machine Transaiton (NMT) (Cho et al. 2014b; Sutskever, Vinyals and Le 2014; Cho et al. 2014a) has achieved great success in some language pairs, beyond the state of the art statistical machine translation (SMT) (Koehn, Hoang and Birch 2007). The encoder decoder architecture is widely used for the NMT, the principle behind it is that: encrypting the meaning of input into a concept space and performing translations based on this encoding. This \"meaning\" leads to a deeper understanding and learning of the translation rules, and thus to a better translation than conventional statistical machine translation (SMT), which only takes into account surface forms, i.e., words and phrases based on this encoding."}, {"heading": "Background", "text": "Based on RNNSearch, we build our machine translation system; RNNSearch improved the attention mechanism of the attention-based NMT (Bahdanau, Cho and Bengio 2014), which first converts the initial sentence into a vector sequence through a bidirectional RNN encoder, then another RNN decoder learns to align and generate the target translation word for word, using a leading neural network to generate a dynamic alignment according to the representation of the source sentence, the previous target word and the previous hidden state of the decoder. We begin this section by describing the attention-based NMT model."}, {"heading": "Attention-based Neural Machine Translation", "text": "Figure 1 shows the framework of the attention-based NMT, which consists of three components: Hi-Hi-Hi-Hi-Hi sentence = \u2192 Hi-Hi-Hi set point and decoder.ar Xiv: 170 9.01-766v 1 [cs.C L] 6S ep2 017Generally, a source sequence contains x = {x1, \u00b7 \u00b7 \u00b7, xls} and the incomplete target sequence y < j = {y1, \u00b7 \u00b7 \u2212 \u2212, yj} that was generated. The model predicts the next word according to the following target word probability distribution: p (yj | x, y < j). Exp (f (yj \u2212 1, sj, aj) \u00b7 Wv) (1), where the function f \u2212 \u2212 \u2212 \u2212 \u2212 is a recursive neural network, Wv is a weighted matrix to classify the output of f into the dimension of the target < j, aj; j (j) < aj (1)."}, {"heading": "Improved Decoder", "text": "The improved decoder model is shown in Figure 2, we just replace the decoder part in Figure 1 with the improved counterpart and leave other modules unchanged. In the new decoder, a new hidden intermediate state s \u2032 j is introduced, and the new hidden intermediate state sj is calculated on the basis of s \u2032 j; so sj would not be calculated directly by yj \u2212 1, sj \u2212 1 and aj, formula (6) is broken down into two others: One GRU calculates the hidden intermediate state s \u2032 j according to yj \u2212 1 and sj \u2212 1s \u2032 j = GRU \u2032 (7), and then the other formula (1, sj \u2212 1) is used to calculate the next hidden intermediate state and attention j = GRU (s \u2032 j, aj) (8), with formula (4) and (5) also being updated, instead of sj \u2212 1, s \u2032 j is used to create a hidden state together with hi, as shown in Figure 2."}, {"heading": "Relational Attention Model", "text": "Based on an improved attention model in Figure 2, we integrate another layer of relationship between the attention layer and the bidirectional encoder of the NMT; as shown in Figure 3, the relationship layer consists of three components: \u2022 Convolutional Encoder (CENC) \u2022 Graph Propogation Layer (GPL) \u2022 Multi-Layer Perceptron Decoder (MLPDEC) CENC: the initial representation of the source sequence generated by the NMT encoder is unchanged to the dimension of sentence length by two layers of 1-dimensional neural network (CNN), so that the encoder prints d k-dimensional maps that represent a new revolutionary representation for each source."}, {"heading": "Convolution Gated Recurrent Unit", "text": "At this point, we present a Convolution Gated Recurrent Unit (CGRU) (Kaiser and Sutskever 2015) to replace the 1-dimensional Convolutionary Encoder Layer in Figure 3. CGRU is a unit similar to the standard GRU but combines the convolution operation to GRU (Cho et al. 2014b). With a few changes, the state of st \u2212 1 layer by layer in st is updated: st = ut \u0445 st \u2212 1 + (1 \u2212 ut). ct = tanh (Wc \u0445 st \u2212 1) + bc) ut = \u03c3 (Wu \u0445 st \u2212 1 + bu) rt = \u03c3 (Wr \u0445 st \u2212 1 + br), where W and b can be treated as weights and prestresses of the Constitutional Operation, they are learnable parameters. Just as operation Wu \u0445 st \u2212 1 represents a convolution process to the state st \u2212 1, so we call the sequential representation."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Data Preparation", "text": "We conduct experiments with Sino-English translation tasks using two different scale datasets, one of which is a smaller dataset and the other is a larger dataset. IWSLT data The training dataset of the IWSLT corpus consists of 44K records from the tourism and travel area. The development dataset consists of the ASR devset 1 and devset 2 of the IWSLT 2005, and the test dataset consists of the IWSLT testset.NIST data Training data consists of 1M Chinese parallel sets pairs with 19M source marks and 24M target marks from the LDC corpus of LDC2002E18, LDC2003E07, LDC2003E14 and Hansard's part of LDC2004T07, LDC2004T08 and LDC2005T06. and we use NIST 2002 test records 2003 as validation sets (IST-IST 16IST and 16IST-NIST)."}, {"heading": "Systems", "text": "We compare the translation performance of our system with four other basic translation systems: \u2022 Traditional statistical machine translation (MOSES) \u2022 The RNNsearch-based translation system (called GROUNDHOG), which we are re-implementing (Bahdanau, Cho and Bengio 2014), is carried out as our weaker basic model. \u2022 The improved RNNsearch-based translation system (referred to as RNSEARCH?), which we use as a stronger base model. \u2022 Google's new neural translation system (indicated by TRANSFOMER) without RNN units (Vaswani et al. 2017)."}, {"heading": "Evaluation Mitrcs", "text": "Without UNK replacement and detoxification, the translation is evaluated on the basis of a case-insensitive 4-gram BLEU score (Papineni et al. 2002) with a test of statistical significance (Collins, Koehn and Kuc erov\u00e1 2005) between the proposed model and the base models."}, {"heading": "Improvement to Translation Performance", "text": "In order to confirm the stability and effectiveness of the proposed model, we compare it with others on two different scales according to the BLEU score. Firstly, two NMT base models have no advantages over MOSES on small training data, as they achieve a similar BLEU score with MOSES on the test data we could derive from Tab 1. On the toy data set, we consistently improve the BLEU score by using different types of different relationship network configurations. Our best setting model performs significantly better than MOSES by 4.05 points and RNNSEARCH? by 4.02 points. Models with other settings all provide more than 1 points than three base models. On the 1M training data set, RNSEARCH? delivers an average of 3.47 + and 4.12 + BLEU scores comparable to MOSES and GROUNDHOG."}, {"heading": "Linguistic Analysis", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Related Works", "text": "(Meng et al. 2016) introduces a new attention mechanism to model the interaction between the decoder and the representation of the source sentence during translation by reading and writing memory, for the original Groundhog or RNNSearch model, the representation of the source sentence remains unchanged throughout the decoding process (Bahdanau, Cho and Bengio 2014), interactive attention mechanisms write a new source representation into the memory for each step of the decoding and could track the interaction history between the decoder and the representation of the source sentence during translation. (Tu et al.) Try to use a coding model to improve the standard attention mechanism by forcing the decoder to treat these source words untranslated and ignore the translated ones; (Bastings et al. 2017) you adopt the dependency tree of the source sentence, so that you can forget the generic decoding or the additional decoding coding of the target coders at the top layer of the relationship with our words."}, {"heading": "Conclusion", "text": "In the NMT model, we are introducing a relationship network layer on top of the standard bi-directional RNN encoder; experiments show that our model consistently improves translation performance on both toy and large data sets; analyses show that the relationship network layer makes the attention mechanism more specific without losing the universality of modeling sequence, which means that our model can learn which part of the process of generating each target word is more precise and attentive at the same time, so that the important source information for the long source sentence can be preserved."}, {"heading": "Hyper Parameters and Training Details", "text": "\"We have spent the past few years working in the fields of IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT"}], "references": [{"title": "Neural Machine Translation By Jointly Learning To Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Iclr 2015 1\u201315.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Graph Convolutional Encoders for Syntaxaware Neural Machine Translation. Emnlp", "author": ["J. Bastings", "I. Titov", "W. Aziz", "D. Marcheggiani", "K. Sima \u2019an"], "venue": null, "citeRegEx": "Bastings et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bastings et al\\.", "year": 2017}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL \u201905 (June):531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Scalable Modified Kneser-Ney Language Model Estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers) 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.08228.", "citeRegEx": "Kaiser and Sutskever,? 2015", "shortCiteRegEx": "Kaiser and Sutskever", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch"], "venue": "Proceedings of the 45th . . . (June):177\u2013180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Interactive Attention for Neural Machine Translation", "author": ["F. Meng", "Z. Lu", "H. Li", "Q. Liu"], "venue": "Coling-2016 2174\u2013 2185.", "citeRegEx": "Meng et al\\.,? 2016", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Annual Meeting of the ACL 1001(1):160.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A simple neural network module for relational reasoning", "author": ["A. Santoro", "D. Raposo", "D.G. Barrett", "M. Malinowski", "R. Pascanu", "P. Battaglia", "T. Lillicrap"], "venue": "Arxiv 1\u201316. Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. Advances in Neural", "citeRegEx": "Santoro et al\\.,? 2017", "shortCiteRegEx": "Santoro et al\\.", "year": 2017}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "Information Processing Systems", "citeRegEx": "Tu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "as dependency relationship) between some word and another in one sentence, recurrent neural network models like GRU or LSTM are very difficult to capture these kinds of relations between word-pairs in one sentence, thus, when making dynamic alignment, the attention model is also hard to recognize this relation; To address this problem, (Bastings et al. 2017) introduces graph-convolutinal networks (GCNs) into the encoder of attention-based NMT, which simultaneously take the representation and the syntactic dependency tree of the source sentence as input to produce another representation for each word, beneficially, these representations may be sensitive to their syntactic neighborhoods.", "startOffset": 338, "endOffset": 360}, {"referenceID": 12, "context": "In this paper, we propose to introduce another simple neural network called Relation Networks (RNs) (Santoro et al. 2017) into the attention module of attention-based NMT compatibly, to let the NMT model capture the relations between each word-pair in the source sentence during the process of dynamic alignment.", "startOffset": 100, "endOffset": 121}, {"referenceID": 7, "context": "Here, we introduce a Convolution Gated Recurrent Unit (CGRU) (Kaiser and Sutskever 2015) to replace the 1dimension convolutional encoder layer in Figure 3, CGRU is a unit which is similar with the standard GRU but combines the convolution operation into GRU (Cho et al.", "startOffset": 61, "endOffset": 88}, {"referenceID": 11, "context": "Without UNK replacement and de-tokenization, the translation is evaluated by using case-insensitive 4-gram BLEU score (Papineni et al. 2002) with test of statistical significance (Collins, Koehn, and Ku\u010derov\u00e1 2005) between the proposed model and baseline models.", "startOffset": 118, "endOffset": 140}, {"referenceID": 6, "context": "Table 1: Comparison among Systems on IWSLT data set, \u201cReLU\u201d, \u201cSigmoid\u201d and \u201cLR\u201d stands for the rectified linear, sigmoid and the leaky rectified linear activation function; \u201cRes\u201d represents the residual connection whose operations we used include addition (\u201cAdd\u201d) and multiplication (\u201cMul\u201d); while \u201cDC\u201d is the abbreviation of dense concatenation (Huang et al. 2016).", "startOffset": 346, "endOffset": 365}, {"referenceID": 9, "context": "(Meng et al. 2016) introduces a new attention mechanism to model the interaction between the decoder and the representation of the source sentence during translation by reading memory and writing memory, for original Groundhog or RNNSearch model, the representation of the source sentence keep unchanged during the whole decoding process (Bahdanau, Cho, and Bengio 2014) , interactive attention mechanism write a new source representation into memory for each step in decoding and could keep track of the interaction history between the decoder and the representation of source sentence during translation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": ") try to employ a coverage model to enhance the standard attention mechanism, enforcing decoder to tend to attend those source words untranslated and ignore those translated ones; (Bastings et al. 2017) adopt the dependency tree of the source sentence to restraint the decoder to focus on those words having dependency relation when generating each target words.", "startOffset": 180, "endOffset": 202}], "year": 2017, "abstractText": "Even though sequence-to-sequence neural machine translation (NMT) model have achieved state-of-art performance in the recent fewer years, but it is widely concerned that the recurrent neural network (RNN) units are very hard to capture the long-distance state information, which means RNN can hardly find the feature with long term dependency as the sequence becomes longer. Similarly, convolutional neural network (CNN) is introduced into NMT for speeding recently, however, CNN focus on capturing the local feature of the sequence; To relieve this issue, we incorporate a relation network into the standard encoder-decoder framework to enhance information-propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. Experiments show that proposed framework outperforms the statistical MT model and the state-ofart NMT model significantly on two data sets with different scales.", "creator": "TeX"}}}