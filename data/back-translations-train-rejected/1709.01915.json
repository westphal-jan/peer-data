{"id": "1709.01915", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Towards Neural Machine Translation with Latent Tree Attention", "abstract": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.", "histories": [["v1", "Wed, 6 Sep 2017 17:44:53 GMT  (662kb,D)", "http://arxiv.org/abs/1709.01915v1", "Presented at SPNLP 2017"]], "COMMENTS": "Presented at SPNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["james bradbury", "richard socher"], "accepted": false, "id": "1709.01915"}, "pdf": {"name": "1709.01915.pdf", "metadata": {"source": "CRF", "title": "Towards Neural Machine Translation with Latent Tree Attention", "authors": ["James Bradbury", "Richard Socher"], "emails": ["james.bradbury@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 Introduction", "text": "Many efforts to use the linguistic hierarchy in NLP tasks lead to the fact that the results of a study dealing with the question of the future of mankind are pushed into the background."}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Encoder/Decoder Architecture", "text": "The model consists of a coupled encoder and decoder, where the encoder is a modified stackonly recursive neural network grammar (Kuncoro et al., 2017) and the decoder is an RNG that is only extended at stack level with constitutive attention. An RNG is a top-to-bottom transition model that collectively builds a sentence representation and a parse tree that represents the parser state with a StackLSTM and therefore does not include the phrase type as an input in the composition function. Our implementation is detailed in Figure 1 and differs from Dyer et al. (2016b) in that it lacks separate newnonterminal tokens for different phrase types and therefore does not include the phrase type as an input in the composition function. Instead, the values of xi for the encoder are set to a constant state for the decoder, while the 2.2 is fixed by the method for the decoder."}, {"heading": "2.2 Attention", "text": "While the encoder uses a single token to represent a new nonterminal state, the decoder represents a new nonterminal state on the stack as a sum weighted according to the structural attention paid to phrase representations of all nonterminal tree nodes produced by the encoder. Specifically, we use the normalized dot products between the decoder stack representation sdecj and the stack representation on each encoder node sienc (i.e. the hidden state of the StackLSTM up to and including xencj, but not h enc j) as coefficients in a weighted sum of phrase embedding hienc that correspond to the encoder node: \u03b1ij = softmax all i (senci \u00b7 sdecj) xdecj = \u2211 i enc i enc ijh enc i i. (1) Since the dot products between the encoding and the encoding node are not a measure of the encoding state of the encoding or the encoding of the encoding of the encoding that exists within the encoder and the current state of the encoding of the encoding."}, {"heading": "2.3 Training", "text": "We formulate our model as a sum of three concepts of MSE: \"All this is a model in which it is only a question to what extent it is a model.\" (\"It is a question of whether it is a model.\") \"It is a question of whether it is a model.\" (\"It is a model.\") \"It is a question of whether it is a model.\" \"\" It is a question of whether it is a model. \"\" \"It is a question of whether it is a model.\" (\"It is a model.\") \"(\" It is a model. \")\" (\"It is a model.\") \"(\" It is a model. \")\" (\"It is a question.\") \"(Question.\" It is a question. \")\" (Question. \"It is a model.\") \"(Question.\" Question. \"It is a model.\") \"(Question.\" It is a model. \"It is a model.\") \"(Question.\" It is a model. \"It is a model.\") \"(Question.\" It is a model. \"It is a model.\" It is a model. \")\" (Question. \"It is a model.\" (Question. \"It is a model.\") \"(Question.\" It is a model. \")\" (Question. \"It is a model.\""}, {"heading": "3 Results", "text": "In the eeisn eeisn nlrteeeeiiugnngVnreeu nvo the eeisn eeisrmnlrteeVnlrteeu nvo the eeisn eeisrsrteeeVnlrteeeeeeVnlrrteeeeeeeeVnlrrteeeeeeeeeeeisn eeisn eeisn eeisn eeisrcnlhteeeeeeeeeeeeeeeeisn eeisrf\u00fc ide eeisrcnn eeisrrf\u00fc ide eeisrcnn eeisn eeisdne eeisdne, n \"i os os os rf\u00fc nde eeisrdne eeisn eeisn nn, nlrrf\u00fc the eeisne eeeeeeeeeadeisn, nso\" eadeiseisn, eadeisf\u00fc \"eeadeisnn, eadeisnnn, eeisnn, eeiseisnn, eeisnnnnnnn, eeisnn, eeisf\u00fc, eeisnn eeisn, eeiseisnn, eeeeisnnnnnnnn, eeisnnn, eeeeisrteeeisnnnn, nnnnnnnnn, nnnnnnnnnnnnnnnn, nnnnnnnn, nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}, {"heading": "4 Conclusion", "text": "Our experiments show that a small MT dataset contains sufficient training signals to deduce latent linguistic structures, and we are curious to see which models, such as the one presented here, can be discovered in full translation corpora. A particularly promising research approach is to use the inherently compositional phrases generated by the encoder for other NLP tasks. There are also many possible directions to improve the model itself and the training process. Value bases can replace exponential moving averages, pure amplification learning can replace teacher constraints, and beam searching can be used instead of greedy inferences. Solutions for the translation pathologies presented in Section 3 are likely to be more complex, although a possible approach would utilize varying inferences using a teacher model that recognizes the buffer and helps train a pure student model."}], "references": [{"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher Manning", "Christopher Potts."], "venue": "ACL.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Chung et al\\.,? 2017", "shortCiteRegEx": "Chung et al\\.", "year": 2017}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "EMNLP.", "citeRegEx": "Dyer et al\\.,? 2016a", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah Smith."], "venue": "NAACL.", "citeRegEx": "Dyer et al\\.,? 2016b", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "ACL.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "ACL.", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "IEEE International Conference on Neural Networks. IEEE, volume 1, pages 347\u2013352.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Neural machine translation with source-side latent graph parsing", "author": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "EMNLP.", "citeRegEx": "Hashimoto and Tsuruoka.,? 2017", "shortCiteRegEx": "Hashimoto and Tsuruoka.", "year": 2017}, {"title": "A syntax-directed translator with extended domain of locality", "author": ["Liang Huang", "Kevin Knight", "Aravind Joshi."], "venue": "CHPJI-NLP. Association for Computational Linguistics.", "citeRegEx": "Huang et al\\.,? 2006", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander Rush."], "venue": "ICLR.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "OpenNMT: Open-source toolkit for neural machine translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush."], "venue": "ArXiv preprint arXiv:1701.02810 .", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "What do recurrent neural network grammars learn about syntax? In EACL", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah Smith"], "venue": null, "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Syntax-aware neural machine translation using CCG", "author": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1702.01147 .", "citeRegEx": "Nadejde et al\\.,? 2017", "shortCiteRegEx": "Nadejde et al\\.", "year": 2017}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack."], "venue": "Artificial Intelligence 46(1):77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel."], "venue": "NIPS.", "citeRegEx": "Schulman et al\\.,? 2015", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "WMT .", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher Manning", "Andrew Ng."], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning.", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Simaan", "Desmond Elliott."], "venue": "WMT .", "citeRegEx": "Specia et al\\.,? 2016", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu."], "venue": "Computational linguistics 23(3):377\u2013403.", "citeRegEx": "Wu.,? 1997", "shortCiteRegEx": "Wu.", "year": 1997}, {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling."], "venue": "ICLR.", "citeRegEx": "Yogatama et al\\.,? 2017", "shortCiteRegEx": "Yogatama et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "Many efforts to exploit linguistic hierarchy in NLP tasks make use of the output of a self-contained parser system trained from a human-annotated treebank (Huang et al., 2006).", "startOffset": 155, "endOffset": 175}, {"referenceID": 22, "context": "An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005).", "startOffset": 231, "endOffset": 255}, {"referenceID": 1, "context": "An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005).", "startOffset": 231, "endOffset": 255}, {"referenceID": 16, "context": "Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K\u00fcchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al.", "startOffset": 175, "endOffset": 237}, {"referenceID": 7, "context": "Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K\u00fcchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al.", "startOffset": 175, "endOffset": 237}, {"referenceID": 20, "context": "Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K\u00fcchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al.", "startOffset": 175, "endOffset": 237}, {"referenceID": 19, "context": ", 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b).", "startOffset": 106, "endOffset": 168}, {"referenceID": 0, "context": ", 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b).", "startOffset": 106, "endOffset": 168}, {"referenceID": 4, "context": ", 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b).", "startOffset": 106, "endOffset": 168}, {"referenceID": 10, "context": "But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying inference over graph-based conditional random fields (Kim et al., 2017), the straight-through estimator (Chung et al.", "startOffset": 192, "endOffset": 210}, {"referenceID": 2, "context": ", 2017), the straight-through estimator (Chung et al., 2017), or policy gradient reinforcement learning (Yogatama et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 23, "context": ", 2017), or policy gradient reinforcement learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states.", "startOffset": 51, "endOffset": 74}, {"referenceID": 1, "context": "For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al.", "startOffset": 217, "endOffset": 231}, {"referenceID": 18, "context": "For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al.", "startOffset": 299, "endOffset": 348}, {"referenceID": 15, "context": "For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al.", "startOffset": 299, "endOffset": 348}, {"referenceID": 5, "context": ", 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al.", "startOffset": 35, "endOffset": 88}, {"referenceID": 8, "context": ", 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al.", "startOffset": 35, "endOffset": 88}, {"referenceID": 6, "context": ", 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines.", "startOffset": 68, "endOffset": 91}, {"referenceID": 0, "context": ", 2010; Bowman et al., 2016; Dyer et al., 2016b). But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying inference over graph-based conditional random fields (Kim et al., 2017), the straight-through estimator (Chung et al., 2017), or policy gradient reinforcement learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states. For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants of the RNNG model introduced by Dyer et al. (2016b), allowing tree structures of unconstrained arity, while text is ingested at the character level, allowing the model to discover and make use of structure within words.", "startOffset": 8, "endOffset": 1366}, {"referenceID": 13, "context": "The model consists of a coupled encoder and decoder, where the encoder is a modified stackonly recurrent neural network grammar (Kuncoro et al., 2017) and the decoder is a stack-only RNNG augmented with constituent-level attention.", "startOffset": 128, "endOffset": 150}, {"referenceID": 3, "context": "All three of these states can be computed with at most one LSTM step per parser transition using the StackLSTM algorithm (Dyer et al., 2016a).", "startOffset": 121, "endOffset": 141}, {"referenceID": 13, "context": "But such a baseline RNNG is actually outperformed by one which conditions the parser transitions only on the stack representation (Kuncoro et al., 2017).", "startOffset": 130, "endOffset": 152}, {"referenceID": 3, "context": "Our implementation is detailed in Figure 1, and differs from Dyer et al. (2016b) in that it lacks separate newnonterminal tokens for different phrase types, and thus does not include the phrase type as an input to the composition function.", "startOffset": 61, "endOffset": 81}, {"referenceID": 17, "context": "We formulate our model as a stochastic computation graph (Schulman et al., 2015), leading to a training paradigm that combines backpropagation (which provides the exact gradient through deterministic nodes) and vanilla policy gradient (which provides a Monte Carlo estimator for the gradient through stochastic nodes).", "startOffset": 57, "endOffset": 80}, {"referenceID": 21, "context": "tual component of the WMT Multimodal Translation shared task (Specia et al., 2016).", "startOffset": 61, "endOffset": 82}, {"referenceID": 12, "context": "An attentional sequence-to-sequence model with two layers and 384 hidden units from the OpenNMT project (Klein et al., 2017) was run at the character level as a baseline, obtaining 32.", "startOffset": 104, "endOffset": 124}, {"referenceID": 11, "context": "We implemented the model in PyTorch, benefiting from its strong support for dynamic and stochastic computation graphs, and trained with batch size 10 and the Adam optimizer (Kingma and Ba, 2015) with early stopping after 12 epochs.", "startOffset": 173, "endOffset": 194}, {"referenceID": 11, "context": "We implemented the model in PyTorch, benefiting from its strong support for dynamic and stochastic computation graphs, and trained with batch size 10 and the Adam optimizer (Kingma and Ba, 2015) with early stopping after 12 epochs. Character embeddings and the encoder\u2019s xenc embedding were initialized to random 384dimensional vectors. The value of \u03b3 and the decay constant for the baselines\u2019 exponential moving average were both set to 0.95. A random selection of translations is included in the supplemental material, while two attention plots are shown in Figure 2. Figure 2b demonstrates a common pathology of the model, where a phrasal encoder constituent would be attended to during decoding of the head word of the corresponding decoder constituent, while the head word of the encoder constituent would be attended to during decoding of the decoder constituent corresponding to the whole phrase. Another common pathology is repeated sentence fragments in the translation, which are likely generated because the model cannot condition future attention directly on past attention weights (the \u201cinput feeding\u201d approach introduced by Luong et al. (2015)).", "startOffset": 174, "endOffset": 1158}], "year": 2017, "abstractText": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.", "creator": "LaTeX with hyperref package"}}}