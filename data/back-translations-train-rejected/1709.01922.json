{"id": "1709.01922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "A Comparison on Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging", "abstract": "Deep neural networks (DNN) have been successfully applied for music classification tasks including music tagging. In this paper, we investigate the effect of audio preprocessing on music tagging with neural networks. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. We show that many commonly used input preprocessing techniques are redundant except magnitude compression.", "histories": [["v1", "Wed, 6 Sep 2017 12:44:01 GMT  (169kb,D)", "http://arxiv.org/abs/1709.01922v1", "5 pages, will be submitted to ICASSP 2017. arXiv admin note: substantial text overlap witharXiv:1706.02361"]], "COMMENTS": "5 pages, will be submitted to ICASSP 2017. arXiv admin note: substantial text overlap witharXiv:1706.02361", "reviews": [], "SUBJECTS": "cs.SD cs.CV cs.IR cs.LG", "authors": ["keunwoo choi", "george fazekas", "kyunghyun cho", "mark sandler"], "accepted": false, "id": "1709.01922"}, "pdf": {"name": "1709.01922.pdf", "metadata": {"source": "CRF", "title": "A COMPARISON ON AUDIO SIGNAL PREPROCESSING METHODS FOR DEEP NEURAL NETWORKS ON MUSIC TAGGING", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": null, "text": "In fact, it is so that it is a way in which the people in the most different constellations in the world move: from the USA via France to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA, from the USA to the USA to the USA, from the USA to the USA, from the USA to the USA to the USA, from the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA to the USA, from the USA to the USA to the USA to the USA, from the USA to the USA, from the USA to the USA to the USA to the USA to the USA to the USA, from the USA to the USA to the USA, from the USA to the USA to the USA to the USA to the USA, from the USA to the USA to the USA to the USA to the USA, from the USA to the USA to the USA to the USA to the USA to"}, {"heading": "2.1. Variance by different initialisation", "text": "In the field of deep learning, the use of K-fold cross-validation is not standard practice for two reasons. First, the model can be trained with sufficiently large data and a good distribution of traction, validation and test sets with low variance. Second, the cost of searching for hyperparameters is very high and it makes repeated experiments too expensive in practice. Therefore, we do not review ConvNet in this study. Instead, we present the results of repeated experiments with fixed line and tagging training hyperparameters: https: / / github.com / keunwoochoi / transfer _ learning _ music and https: / / github.com / keunwoochoi / MSD _ split _ for _ taggingtraining hyperparameters, such as training sample sequences and batch size. This experiment therefore measures the variance of the model introduced by different weight initializations of the convolutionary layers."}, {"heading": "2.2. Time-frequency representations", "text": "STFT and melting spectrogram were the most popular input representations for deep learning in music. Melting spectrograms provide an efficient and perceptual representation compared to STFT [14] and have shown that they perform well in various tasks [15-20]. However, a STFT closer to the original signal and neural networks may be able to learn a representation that is more optimal for the task. However, this requires large amounts of training data, as reported in [20], where the use of melting spectrograms exceeds STFT with smaller data. Figure 3 shows the AUC values obtained using melting spectrogram vs. STFT, while the size of the training data used is varied with logarithmic scaling. Although there are small differences in AUC values up to 0.007, none of them exceeds the others, especially if enough data is provided for HF values to HFT values. This refutes an earlier result in [20], because the HFT values have no clear advantage from HFT values to HFT values from HFT values to HFT values, even with a small size of HF to HFT values, HFT to HF values from HFT values can be predicted."}, {"heading": "2.3. Log-scaling of magnitudes", "text": "In this section, we will discuss how the logarithmic scaling of magnitudes, i.e. the decibel scaling, affects performance, which is considered standard pre-processing in the query of music information, motivated by the human perception of loudness [14], which has a logarithmic relationship with the physical energy of sound. Although learning a logarithmic function is a trivial task for neural networks, it can be difficult to implicitly learn an optimal nonlinear imaging when embedded in a complicated task. Nonlinear imaging has also shown that it affects performance in visual image recognition using neural networks [22]. Figure 4 compares the histograms of the magnitudes of time-frequency bins after the standardization of the ceromedic unit. On the left, a logarithmically compressed melt spectrogram shows an approximate distribution without extreme values."}, {"heading": "2.4. Analysis of scaling effects and frequency-axis weights", "text": "Finally, we will discuss the effects of order of magnitude manipulation. Preliminary experiments suggested that there may be two independent aspects that need to be investigated: i) frequency axis weights and ii) size scaling of each element in the training set. Our experiment serves to isolate these two effects. We tested two input representations log-signal spectrogram vs. signal spectrogram with three frequency weighting schemes perfrequency, A-weighting and bypass, as well as two scaling methods \u00d7 10 (an) and \u00d7 1 (out), which altogether yield 2 x 3 x 2 = 12 configurations. We summarize the mechanism of each block as follows. \u2022 Pro-frequency-Stdd: Calculation of means and standard deviations over time, i.e. per frequency and standardize each frequency band using these values. The average frequency response is flat (balanced). This method was used in the literature for tagging 20 [Singing], single tuning, and an overweight of STD."}, {"heading": "2.4.1. Frequency weighting", "text": "This process is related to volume, i.e. the human perception of sound energy [14], which is a function of frequency; the sensitivity of the human auditory system falls well below a few hundred Hz3, so music signals typically have a higher energy in the lower range to compensate for the attenuation; this is illustrated in Figure 6, where uncompensated average energy measurements corresponding to the bypass curve (shown in green) peak at low frequencies; this imbalance affects the activation of neural networks in the first layer, which can affect performance. To evaluate this effect, we tested three approaches to frequency weighting. Their typical profiles are shown in Figure 6. In all three strategies, partial standardization is used to mitigate scaling effects (see Section 2.4.2). 3See contours of equal volume, e.g. in ISO 226: 2003. Our test results show that networks that the group of AUC have significant differences within the second groups {1, the three significant differences}."}, {"heading": "2.4.2. Analysis of scaling effects", "text": "s learning rate is proportional to the order of magnitude of the input X. [5] Consider comparing the same-colored bars of {1 vs 1 s} and {2 vs 2 s}. Here, the scale factor is set on the illustration, but many possible values < 100 have been tested and showed similar results. In summary, this hypothesis is refuted - the scaling has no effect on the performance of ma."}], "references": [{"title": "Stacked convolutional and re-  current neural networks for music emotion recognition", "author": ["M. Malik", "S. Adavanne", "K. Drossos", "T. Virtanen", "D. Ticha", "R. Jarina"], "venue": "arXiv preprint arXiv:1706.02292, 2017.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning to pinpoint singing voice from weakly labeled examples", "author": ["J. Schl\u00fcter"], "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 44\u201350.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Samplelevel deep convolutional neural networks for music auto-tagging using raw waveforms", "author": ["J. Lee", "J. Park", "K.L. Kim", "J. Nam"], "venue": "arXiv preprint arXiv:1703.01789, 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural networks, vol. 4, no. 2, pp. 251\u2013257, 1991.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 9\u201348.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "2017 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "librosa: 0.4.1", "author": ["B. McFee", "M. McVicar", "C. Raffel", "D. Liang", "O. Nieto", "E. Battenberg", "J. Moore", "D. Ellis", "R. YAMAMOTO", "R. Bittner", "D. Repetto", "P. Viktorin", "J.F. Santos", "A. Holovaty"], "venue": "Oct. 2015. [Online]. Available: https://doi.org/10.5281/zenodo.32193", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras", "author": ["K. Choi", "D. Joo", "J. Kim"], "venue": "Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML, 2017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412. 6980", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras: Deep learning library for theano and tensorflow", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5590, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on  imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to the psychology of hearing", "author": ["B.C. Moore"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep content-based music recommendation", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2643\u20132651.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep bag-offeatures model for music auto-tagging", "author": ["J. Nam", "J. Herrera", "K. Lee"], "venue": "arXiv preprint arXiv:1508.04999, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "S. Bock"], "venue": "Acoustics, Speech and Signal Processing, IEEE International Conference on. IEEE, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Boundary detection in music structure analysis using convolutional neural networks", "author": ["K. Ullrich", "J. Schl\u00fcter", "T. Grill"], "venue": "Proceedings of the 15th International Society for Music Information Retrieval Conference (IS- MIR 2014), Taipei, Taiwan, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "The 17th International Society of Music Information Retrieval Conference, New York, USA. International Society of Music Information Retrieval, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Auditory toolbox", "author": ["M. Slaney"], "venue": "Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Understanding how image quality affects deep neural networks", "author": ["S.F. Dodge", "L.J. Karam"], "venue": "CoRR, vol. abs/1604.04004, 2016. [Online]. Available: http://arxiv.org/abs/1604.04004", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "An end-to-end neural network for polyphonic music transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "arXiv preprint arXiv:1508.01774, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.", "startOffset": 209, "endOffset": 212}, {"referenceID": 2, "context": "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.", "startOffset": 232, "endOffset": 235}, {"referenceID": 3, "context": "Although deep neural networks are known to be universal function approximators [4], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "Although deep neural networks are known to be universal function approximators [4], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [5].", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "This showed a good performance with efficient training in a prior benchmark [6], where the model we selected was denoted k2c2, indicating 2D kernels and convolution axes.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Exponential linear unit (ELU) is used as an activation function in all convolutional layers [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "The preprocessing is performed using Librosa [8] and Kapre [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "The preprocessing is performed using Librosa [8] and Kapre [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "For the acceleration of stochastic gradient descent, we use adaptive optimisation based on ADAM [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "The experiment is implemented in Python with Keras [11] and Theano [12] as deep learning frameworks.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "The experiment is implemented in Python with Keras [11] and Theano [12] as deep learning frameworks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "[13], which has been shown to yield a stable training procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 15, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 16, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 17, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 18, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 19, "context": "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15\u201320].", "startOffset": 153, "endOffset": 160}, {"referenceID": 19, "context": "This requires large amounts of training data however, as reported in [20] where using melspectrograms outperformed STFT with a smaller dataset.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "This rebuts a previous result in [20] because melspectrograms did not have a clear advantage here even with a small training data size.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "\u2022 STFT in [20]: 6000/129=46.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "\u2022 Melspectrogram in [20] and our work: 35.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "9 Hz for frequency < 1 kHz (96 mel-bins and by [21] and [8])", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "9 Hz for frequency < 1 kHz (96 mel-bins and by [21] and [8])", "startOffset": 56, "endOffset": 59}, {"referenceID": 19, "context": "In [20], the frequency resolution of the STFT was lower than that of the melspectrogram to enable comparing them with similar number of frequency bins.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The procedure is motivated by the human perception of loudness [14] which has logarithmic relationship with the physical energy of sound.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "A nonlinear mapping was also shown to affect the performance in visual image recognition using neural networks [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].", "startOffset": 86, "endOffset": 89}, {"referenceID": 22, "context": "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": ", human perception of sound energy [14], which is a function of frequency.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "This is due to batch normalization [24] which compensates for the different magnitudes by normalizing the activations of convolutional layers.", "startOffset": 35, "endOffset": 39}], "year": 2017, "abstractText": "Deep neural networks (DNN) have been successfully applied for music classification tasks including music tagging. In this paper, we investigate the effect of audio preprocessing on music tagging with neural networks. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. We show that many commonly used input preprocessing techniques are redundant except magnitude compression.", "creator": "LaTeX with hyperref package"}}}