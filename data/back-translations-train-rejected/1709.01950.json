{"id": "1709.01950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "\"Having 2 hours to write a paper is fun!\": Detecting Sarcasm in Numerical Portions of Text", "abstract": "Sarcasm occurring due to the presence of numerical portions in text has been quoted as an error made by automatic sarcasm detection approaches in the past. We present a first study in detecting sarcasm in numbers, as in the case of the sentence 'Love waking up at 4 am'. We analyze the challenges of the problem, and present Rule-based, Machine Learning and Deep Learning approaches to detect sarcasm in numerical portions of text. Our Deep Learning approach outperforms four past works for sarcasm detection and Rule-based and Machine learning approaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows that special attention to text containing numbers may be useful to improve state-of-the-art in sarcasm detection.", "histories": [["v1", "Wed, 6 Sep 2017 18:09:15 GMT  (266kb,D)", "http://arxiv.org/abs/1709.01950v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lakshya kumar", "arpan somani", "pushpak bhattacharyya"], "accepted": false, "id": "1709.01950"}, "pdf": {"name": "1709.01950.pdf", "metadata": {"source": "CRF", "title": "\u201cHaving 2 hours to write a paper is fun!\u201d: Detecting Sarcasm in Numerical Portions of Text", "authors": ["Lakshya Kumar", "Arpan Somani", "Pushpak Bhattacharyya"], "emails": ["lakshya@cse.iitb.ac.in", "somani@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the number of those who are able to shoot up has multiplied; in recent years, the number of those who are able to shoot up has skyrocketed; in recent years, the number of those who are able to shoot up has skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed and skyrocketed, skyrocketed and skyrocketed."}, {"heading": "2 Related Work", "text": "Sarcasm and irony recognition has been extensively studied in linguistic, psychological and cognitive sciences (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcascasm classifier with syntactic and pattern-based features. Carvalho et al. (2011) states that sarcascascasm transforms the polarity of an apparently positive or negative utterance."}, {"heading": "3 Motivation", "text": "Consider the following sentences: 1. This phone has a fantastic battery life of 38 hours. (Non-sarcastic).2. This phone has a fantastic battery life of 2 hours. (Sarcastic).3. This phone has a terrible battery life of 2 hours (non-sarcastic).The sentences 1 and 3 are not sarcastic, while sentence 2 is sarcastic. Consider sentences 1 and 2. The two sentences only differ in numerical value (\"38\" versus \"2\").The sarcasm in sentence 2 can be understood in terms of the incompatibility between the word \"genius\" and \"2 hours\" in the case of battery life. On the contrary, sentences 2 and 3 differ in a word with different surface feel (\"genius versus\" \"terrible\").Detecting sarcasm in sentences such as sentence 2 using information from sentence 1 and 3 in a dataset is the key idea of our rule sarcastic formation."}, {"heading": "4 Approaches", "text": "To detect numerical sarcasm, we implement two rules-based approaches as described in the following subsections."}, {"heading": "4.1 Approach-1: Noun phrase exact matching", "text": "In this approach, we first create two repositories, that is, sarcastic and non-sarcastic phrases obtained with the help of a training dataset. Each entry in the repository is of the following format: (Tweet Index No., Noun Phrase list, Mean of Number unit3, Std Dev of Number unit4, Number Unit5). The repositories are created as follows: For each sarcasm-designated tweet in the training dataset, we perform the following steps: \u2022 Step-1: Extract noun phrases in the tweet using a parser. \u2022 Step-2: Selected unit of numbers as the word that follows the word POS as a \"CD.\" Examples of number units are minutes, days, years, etc. \u2022 Step-3: We add an entry to the corresponding repository according to the name of the tweet."}, {"heading": "4.2 Approach-2: Noun phrase cosine similarity matching", "text": "Therefore, Approach-2 loosens the constraint by using cosine similarity to match the phrases in the test tweet and the repository entry. This approach also creates two repositories, i.e. sarcastic and non-sarcastic tweets as follows: \u2022 Step-1: We first convert the list of noun sentences into their vector representation. \u2022 Step-2: This vector representation is created by adding the 200-dimensional word \"embeddings7\" of the words included in the list of noun sentences, and then dividing them by their counting. \u2022 Step-3: Now the repositories are of the form: (Tweet index number, vector representation of the noun phrase list, means of the numerical unit, Std-Dev of the numerical unit, unit of numbers).Example: If there is a numerical sarcastic tweet, cosmic meetings are the best way to start the birthday phrase."}, {"heading": "4.3 Machine Learning based approach", "text": "To create a machine learning approach to numerical sarcasm detection, we train SVM, KNN and Random Forest classifiers that use different types of traits as described below: \u2022 Mood-based traits: These traits include number of positive words, number of negative words, number of highly emotional positive words, number of highly emotional negative words. Positive / negative word is considered highly emotional if its part-of-speech day is one of the following traits: \"JJ,\" \"JJR,\" \"JS,\" \"RB,\" \"RBR,\" \"RBS,\" \"VB,\" \"Tweet,\" \"VBD,\" \"VBG,\" \"VBN,\" \"VBP,\" \"VBZ.\" \u2022 Emoticon-based traits: These traits include positive emoticons, negative emoticons, contrast between word, i.e., a Boolean function that will be one when positive and negative words are present."}, {"heading": "4.4 Deep Learning based approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 CNN-FF Model", "text": "The architecture of the CNN-FF model is shown in Figure 2. There is an embedding matrix E, IR | V | \u00b7 d, where | V | is the vocabulary size and d is the tweet word that embeds the dimension. For the input tweet, we get an input matrix I, IR | S | \u00b7 d, where | S | is the length of the tweet including the padding, where Ii is the d-dimensional vector for the i-th word in the tweet in the input matrix. Let k be the length of the filter, and the vector f, IR | k | \u00b7 d is a filter for the folding process. For each position p in the input matrix I, there is a window wp of k consecutive words, which are referred to as: wp = [Ip, Ip + 1,..., Ip + k \u2212 1]."}, {"heading": "4.4.2 LSTM-FF Model", "text": "RNNs have demonstrated the ability to capture sequential information in a chain-like neural network. Standard RNNs will not be able to learn long-term dependencies when the gap between two time steps becomes large. We have adopted the standard architecture of LSTM proposed by Hochreiter and Schmidhuber. The LSTM architecture has a series of repetitive modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a series of gates in IRd depending on the old hidden state \u2212 1 and the input in the current time step text: the forget gate ft, the input gate it and the output gate ot. These gates collectively decide how to update the current hidden state in IRd. We use d to denote the memory dimension in the LSTM and all vectors in this architecture share the same dimension x."}, {"heading": "4.4.3 CNN-LSTM-FF Model", "text": "The representation of the input matrix for the tweet T is derived from the embedding matrix E, as described in Figure 4.4.1. Filters of size 5Xd, where d is the embedding dimension of the tweet, slide over the input matrix I of the tweet to extract the characteristics. We have passed the output of the Convolutionary Network through a pooling layer and maximum pooling is size 4. All filters are of the same dimension and after executing the pooling operation via their outputs we get a concatenated feature matrix, which is called: C = [c1; c2;... cn], where n in cn displays the total number of filters used in the architecture.The feature matrix C, IRl \u00d7 n, each ci, IRl, where l is the dimension obtained after the pooling. Leave xj, IR1 \u00d7 n, is a vector that is obtained from the input matrix for the Texj TexSp, where the ZZIP is the ZZIP."}, {"heading": "5 Experiment Setup", "text": "We use three datasets to perform experiments described in Table 1, and their creation details are described below: (A) Dataset-1: To create this dataset, we extract tweets from the Twitter API (https: / / dev. twitter.com). The tweets, the hashtags # sarcasm, # sarcastic, # BeingSarcastic, # BeingSarcastic, # nonsarcasm, # notsarcastic, are referred to as the non-sarcastic. We remove URLs, duplicate tweets, retweets, usernames and other non-ASCII characters in these tweets. (B) Dataset-2: We retain only those tweets that contain numeric characters to create Dataset-2. Additional processing is performed to remove irrelevant tweets, such as those that adjacent alphabet or special characters to a number such as Model34d, 4s, < 3 (heart smiley Dataset) We share these in 81."}, {"heading": "6 Results", "text": "In this section, we evaluate our approaches to detecting sarcasm in numerical parts of the text system, as illustrated in Table 2. F1 F1 F1 evaluates the performance of four previous approaches on Dataset-1 and Dataset-2. We see that three of the past four works at8http: / / scikit-learn.org / stable / modulesF1-score, which comes close to each other on Dataset-2. On Dataset-1 and Dataset-2, the best F1 score of 0.72 and 0.25, respectively, is achieved by using features from Joshi et al. (2015), there is a 47% degradation on Dataset-2, which contains only numerical tweets. This degradation clearly shows that these past approaches are unable to detect the sarcasm that results from numbers in the text. All of the past 4 approaches are built to detect the normal sarcasm in which the discrepancy originates from the text."}, {"heading": "7 Error Analysis", "text": "We classify the errors according to our approaches into the following categories: \u2022 Unit Mismatch / Unit Missing: There are some tweets where the unit of numbers is present in a very informal way, such as min for minutes and hr for hours. So these types of units caused a problem when performing the unit matching test. In some tweets, for example, the unit of numbers is missing: \"I love waking up at 545.\" Unit is not as present as time is in the wrong format. \u2022 Multiple number presence: Some tweets contain multiple numbers and this makes it harder to identify sarcasm based on numbers. For example: \"$34.04 for a 10-mile trip that takes 19 minutes? that makes sense.\" \u2022 Sarcasm due to the text but not the number: Some tweets contain numbers, but they are sarcastic, not because of the presence of numbers. For example: \"First asthma attack in 6 years. forget how much fun they are,\" in this tweet sarcasm arises because of the \"fun at the end.\""}, {"heading": "8 Conclusion & Future Work", "text": "Numerical sarcasm is a specific case of sarcasm where there are discrepancies between text-based and numerical content. It shows the deterioration in the performance of the past four approaches compared to the dataset containing numerical sarcasm tweets. In addition, we present rules-based machine learning and deep learning approaches to detecting numerical sarcasms, achieving the best total score of 0.93 from the CNN-FF model. In this work, we are trying to build a system for detecting numerical sarcasm in tweets. Our work opens up a new way of detecting sarcasm because previous approaches are unable to detect numerical sarcasm because they capture the clues of normal sarcasm and improve performance. In the future, all previous approaches to detecting sarcasm can benefit from our work by separating the normal sarcasm from numerical sarcasm and improving performance."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["Paul Tucker", "Vijay Vasudevan", "Pete Warden", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). pages 265\u2013", "citeRegEx": "Tucker et al\\.,? 2016", "shortCiteRegEx": "Tucker et al\\.", "year": 2016}, {"title": "Modelling context with user embeddings for sarcasm detection in social media", "author": ["Silvio Amir", "Byron C Wallace", "Hao Lyu", "Paula Carvalho M\u00e1rio J Silva."], "venue": "arXiv preprint arXiv:1607.00976 .", "citeRegEx": "Amir et al\\.,? 2016", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "A pattern-based approach for sarcasm detection on twitter", "author": ["Mondher Bouazizi", "Tomoaki Otsuki Ohtsuki."], "venue": "IEEE Access 4:5477\u20135488.", "citeRegEx": "Bouazizi and Ohtsuki.,? 2016", "shortCiteRegEx": "Bouazizi and Ohtsuki.", "year": 2016}, {"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Konstantin Buschmeier", "Philipp Cimiano", "Roman Klinger."], "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sen-", "citeRegEx": "Buschmeier et al\\.,? 2014", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Clues for detecting irony in user-generated contents: oh...!! it\u2019s so easy;", "author": ["Paula Carvalho", "Lu\u0131\u0301s Sarmento", "M\u00e1rio J Silva", "Eug\u00e9nio De Oliveira"], "venue": "In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for", "citeRegEx": "Carvalho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carvalho et al\\.", "year": 2009}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the fourteenth conference on computational natural language learning. Association for Computational Lin-", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Fracking sarcasm using neural network", "author": ["Aniruddha Ghosh", "Tony Veale."], "venue": "Proceedings of NAACL-HLT . pages 161\u2013169.", "citeRegEx": "Ghosh and Veale.,? 2016", "shortCiteRegEx": "Ghosh and Veale.", "year": 2016}, {"title": "On the psycholinguistics of sarcasm", "author": ["Raymond W Gibbs."], "venue": "Journal of Experimental Psychology: General 115(1):3.", "citeRegEx": "Gibbs.,? 1986", "shortCiteRegEx": "Gibbs.", "year": 1986}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Context incongruity and irony processing", "author": ["Stacey L Ivanko", "Penny M Pexman."], "venue": "Discourse Processes 35(3):241\u2013279.", "citeRegEx": "Ivanko and Pexman.,? 2003", "shortCiteRegEx": "Ivanko and Pexman.", "year": 2003}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 217\u2013226.", "citeRegEx": "Joachims.,? 2006", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Automatic sarcasm detection: A survey", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman."], "venue": "arXiv preprint arXiv:1602.03426 .", "citeRegEx": "Joshi et al\\.,? 2016a", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya."], "venue": "ACL (2). pages 757\u2013762.", "citeRegEx": "Joshi et al\\.,? 2015", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Harnessing sequence labeling for sarcasm detection in dialogue from tv series friends", "author": ["Aditya Joshi", "Vaibhav Tripathi", "Pushpak Bhattacharyya", "Mark Carman."], "venue": "CoNLL 2016 page 146.", "citeRegEx": "Joshi et al\\.,? 2016b", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["CC Liebrecht", "FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A deeper look into sarcastic tweets using deep convolutional neural networks", "author": ["Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Prateek Vij."], "venue": "arXiv preprint arXiv:1610.08815 .", "citeRegEx": "Poria et al\\.,? 2016", "shortCiteRegEx": "Poria et al\\.", "year": 2016}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Ashwin Rajadesingan", "Reza Zafarani", "Huan Liu."], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. ACM, pages 97\u2013106.", "citeRegEx": "Rajadesingan et al\\.,? 2015", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "EMNLP. volume 13, pages 704\u2013714.", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": " yeah right\u201d: sarcasm recognition for spoken dialogue systems", "author": ["Joseph Tepperman", "David R Traum", "Shrikanth Narayanan."], "venue": "INTERSPEECH.", "citeRegEx": "Tepperman et al\\.,? 2006", "shortCiteRegEx": "Tepperman et al\\.", "year": 2006}, {"title": "Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony", "author": ["Akira Utsumi."], "venue": "Journal of Pragmatics 32(12):1777\u2013 1806.", "citeRegEx": "Utsumi.,? 2000", "shortCiteRegEx": "Utsumi.", "year": 2000}, {"title": "Tweet sarcasm detection using deep neural network", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics. pages 2449\u20132460.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Computational detection of sarcasm has seen attention from the sentiment analysis community in the past few years (Joshi et al., 2016a).", "startOffset": 114, "endOffset": 135}, {"referenceID": 8, "context": "Past approaches for sarcasm detection report features related to sentiment (Gonz\u00e1lez-Ib\u00e1nez et al., 2011), author\u2019s historical context (Rajadesingan et al.", "startOffset": 75, "endOffset": 105}, {"referenceID": 18, "context": ", 2011), author\u2019s historical context (Rajadesingan et al., 2015), and conversational context (Joshi et al.", "startOffset": 37, "endOffset": 64}, {"referenceID": 14, "context": ", 2015), and conversational context (Joshi et al., 2016b).", "startOffset": 36, "endOffset": 57}, {"referenceID": 8, "context": "Past approaches for sarcasm detection report features related to sentiment (Gonz\u00e1lez-Ib\u00e1nez et al., 2011), author\u2019s historical context (Rajadesingan et al., 2015), and conversational context (Joshi et al., 2016b). Error analysis presented in many of these works has served as a motivation for future work. Our paper is based on an error observed by Joshi et al. (2015): \u2018Incongruity in numbers, resulting in sarcasm\u2019.", "startOffset": 76, "endOffset": 369}, {"referenceID": 7, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000).", "startOffset": 105, "endOffset": 132}, {"referenceID": 21, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000).", "startOffset": 105, "endOffset": 132}, {"referenceID": 5, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al.", "startOffset": 106, "endOffset": 272}, {"referenceID": 5, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.", "startOffset": 106, "endOffset": 297}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features.", "startOffset": 0, "endOffset": 136}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) states that sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite.", "startOffset": 0, "endOffset": 236}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) states that sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. Liebrecht et al. (2013) showed that sarcasm is often signaled by hyperbole, using intensifiers and exclamations; in contrast, nonhyperbolic sarcastic messages often receive an explicit marker.", "startOffset": 0, "endOffset": 371}, {"referenceID": 2, "context": "Buschmeier et al. (2014) provided the baseline for classification of ironic or sarcastic reviews.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Buschmeier et al. (2014) provided the baseline for classification of ironic or sarcastic reviews. They analyzed the impact of different features for the classification task. The work by Joshi et al. (2015) shows how sarcasm arises because of implicit or explicit incongruity in the sentence.", "startOffset": 0, "endOffset": 206}, {"referenceID": 2, "context": "Bouazizi and Ohtsuki (2016) proposed a pattern-based approach to detect sarcasm on Twitter.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Ghosh and Veale (2016) provides a neural network semantic model for sarcasm detection.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Ghosh and Veale (2016) provides a neural network semantic model for sarcasm detection. Their model composed of Convolution Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a Deep Neural Network(DNN). Poria et al. (2016) proposed a novel method to detect sarcasm using Convolution Neural Networks.", "startOffset": 0, "endOffset": 256}, {"referenceID": 1, "context": "Amir et al. (2016) proposed a deep-learning based architecture to automatically learn user embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Amir et al. (2016) proposed a deep-learning based architecture to automatically learn user embeddings. In their proposed approach they have used this user embeddings to provide contextual features, going beyond the lexical and syntactic cues for sarcasm. Zhang et al. (2016) used a bi-directional gated recurrent neural network followed by a pooling neural network to detect sarcasm.", "startOffset": 0, "endOffset": 275}, {"referenceID": 16, "context": ", 25-D, 50D, 100-D, 150-D, 200-D, 250-D, 300-D of tweet words using word2vec (Mikolov et al., 2013) tool on a large corpora of 6 million tweets.", "startOffset": 77, "endOffset": 99}, {"referenceID": 9, "context": "We adopted the standard architecture of LSTM proposed by (Hochreiter and Schmidhuber, 1997).", "startOffset": 57, "endOffset": 91}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al.", "startOffset": 33, "endOffset": 89}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al.", "startOffset": 33, "endOffset": 114}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al. (2015) for Sarcasm detection, to show the degradation in performance for Numeric-Sarcastic Dataset.", "startOffset": 33, "endOffset": 138}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al. (2015) for Sarcasm detection, to show the degradation in performance for Numeric-Sarcastic Dataset. We train classifiers for the features introduced by these approaches, using SVMperf by Joachims (2006) with RBF kernel.", "startOffset": 33, "endOffset": 334}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.", "startOffset": 29, "endOffset": 54}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.", "startOffset": 29, "endOffset": 94}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.68 0.15 Liebrecht et al. (2013) 0.", "startOffset": 29, "endOffset": 128}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.68 0.15 Liebrecht et al. (2013) 0.67 0.17 Joshi et al. (2015) 0.", "startOffset": 29, "endOffset": 158}, {"referenceID": 12, "context": "25 respectively are obtained by using features from Joshi et al. (2015), there is a degradation of 47% on Dataset-2 which contains only numerical tweets.", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": "We see an improvement of 68% in F1-score against the best performing past approach of Joshi et al. (2015).", "startOffset": 86, "endOffset": 106}], "year": 2017, "abstractText": "Sarcasm occurring due to the presence of numerical portions in text has been quoted as an error made by automatic sarcasm detection approaches in the past. We present a first study in detecting sarcasm in numbers, as in the case of the sentence \u2018Love waking up at 4 am\u2019. We analyze the challenges of the problem, and present Rulebased, Machine Learning and Deep Learning approaches to detect sarcasm in numerical portions of text. Our Deep Learning approach outperforms four past works for sarcasm detection and Rule-based and Machine learning approaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows that special attention to text containing numbers may be useful to improve state-of-the-art in sarcasm detection.", "creator": "LaTeX with hyperref package"}}}