{"id": "1709.02066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge", "abstract": "Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.", "histories": [["v1", "Thu, 7 Sep 2017 04:50:29 GMT  (870kb)", "http://arxiv.org/abs/1709.02066v1", "6 pages, 4 figures, conference"]], "COMMENTS": "6 pages, 4 figures, conference", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["pin wang", "ching-yao chan"], "accepted": false, "id": "1709.02066"}, "pdf": {"name": "1709.02066.pdf", "metadata": {"source": "CRF", "title": "Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge", "authors": ["Pin Wang", "Ching-Yao Chan"], "emails": ["pin_wang@berkeley.edu"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "A. LSTM", "text": "LSTM is a special recursive neural network designed to address long-term dependency issues [15]. LSTM has the ability to remember values for both long and short periods of time. As previously mentioned, the driving environment of the on-ramp merge scenario includes interactions with the surrounding vehicles that are not necessarily predictable from the ego vehicle's point of view. By using LSTM, the historical driving information can be integrated into an internal state that represents an adequate representation of the interactive environment. In other words, the internal state of an LSTM cell provides a compact and solid representation of the history that can be fed into the Q network. In our study, we train the LSTM model through supervisedlearning. The architecture of the LSTM model is represented in Figure 2. Each LSTM unit includes two modules, an observation module and a state module \"(the next step) to the internal observation module (the next observation module is used)."}, {"heading": "B. Deep Q-Learning", "text": "Less than a year ago, that was already the case."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Berkeley Deep Drive Program support and helpful discussion with Dr. Yi-Ta Chuang.REFERENCES [1] https: / / www.tesla.com / autopilot [2] https: / / www.google.com / selfdrivingcar / [3] http: / / www.volvocars.com / intl / about / our-innovationbrands / intellisafe / autonomous-driving / drive-me. [4] Davis, L.C. Effect of adaptive cruise control systems on mixed trafficflow near an-ramp. Physica: Statistical Mechanics and its Applications, Vol. 379, No. 274-290, 2007. [5] Marinescu, D., \u010curn, J., Bouroche, M., Cahill, V. On-ramp traffic merging using cooperative intelligent vehicles: a slot-based approach."}], "references": [{"title": "Effect of adaptive cruise control systems on mixed traffic flow near an on-ramp", "author": ["L.C. Davis"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "On-ramp traffic merging using cooperative intelligent vehicles: a slot-based approach", "author": ["D. Marinescu", "J. \u010curn", "M. Bouroche", "V. Cahill"], "venue": "15th International IEEE Conference on Intelligent Transportation Systems, Alaska,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Decision-making analysis during urban expressway ramp merge for autonomous vehicle", "author": ["X. Chen", "M. Jin", "C. Chan", "Y. Mao", "W. Gong"], "venue": "Annual Meeting of the Transportation Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Freeway Ramp-Metering Control based on Reinforcement Learning", "author": ["A. Fares", "W. Gomaa"], "venue": "IEEE International Conference on Control & Automation (ICCA),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Reinforcement learning ramp metering control for weaving sections in a connected vehicle environment", "author": ["H. Yang", "H. Rakha"], "venue": "Annual Meeting of the Transportation Research Board,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "End-to-End Deep Reinforcement Learning for Lane Keeping Assist", "author": ["A. Sallab", "M. Abdou", "E. Perot", "S. Yogamani"], "venue": "Conference on Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep Reinforcement Learning for Simulated Autonomous Vehicle Control", "author": ["A. Yu", "R. Palefsky-Smith", "R. Bedi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Longterm Planning by Short-term Prediction. 2016, arXiv preprint arXiv:1602.01580", "author": ["S. Shalev-Shwartz", "N. Ben-Zrihem", "A. Cohen", "A. Shashua"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Telerobotics, Automation, and Human Supervisory Control", "author": ["T.B. Sheridan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Cooperative Inverse Reinforcement Learning, 30", "author": ["D. Hadfield-Menell", "Dragan", "P.A. Abbeel", "S. Russell"], "venue": "Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Davis [4] presented a cooperative merging strategy, in which vehicles on the mainline always slow down to create enough gap space to let the on-ramp vehicle merge into.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "[5] proposed a slot-based merging algorithm, which defined a slot\u2019s occupancy status (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[6] used driving rules and a gap acceptance theory to model the decision-making process of the urban expressway on-ramp merge problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[8] designed a Reinforcement Learning based Density Control Agent (RLCA) to control the number of vehicles entering the mainline from the ramp merging area.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[9] developed a rampmetering control algorithm based on reinforcement learning to increase the capacity at weaving sections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For example, Google DeepMind [10] has successfully applied a Deep Q-network (a convolutional neural network, CNN) to play Atari games with only screen images and game scores as inputs.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "[11] used end-to-end deep reinforcement learning for lane-keeping assist on an open-source simulator for Racing called (TORCS).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[12] investigated the use of deep reinforcement learning for training an agent to control a simulated car running on the track in JavaScript Racer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Mobileye [13] used another approach that employed two supervised learning models to describe an interactive environment and a recurrent neural network based on these two models to learn an optimal policy.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "LSTM is a special recurrent neural network designed to handle long-term dependency problems [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "The \u201csupervisory control\u201d term per Sheridan [16], suggests that human-machine systems can exist in a spectrum of automation, and shift across the spectrum of control levels in real time to suit the situation at hand.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "To accommodate a balanced and common reward in the machine learning approach, a cooperative inverse reinforcement learning concept (CIRL) [17] may be considered and adopted into the framework.", "startOffset": 138, "endOffset": 142}], "year": 2017, "abstractText": "Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle\u2019s optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions. Keywords\u2014 Autonomous Driving; Highway On-Ramp Merge; Deep Reinforcement Learning; Long Short-Term Memory; Deep Q-Network; Control Policy", "creator": "'Certified by IEEE PDFeXpress at 07/31/2017 4:20:40 PM'"}}}