{"id": "1709.02076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Composition by Conversation", "abstract": "Most musical programming languages are developed purely for coding virtual instruments or algorithmic compositions. Although there has been some work in the domain of musical query languages for music information retrieval, there has been little attempt to unify the principles of musical programming and query languages with cognitive and natural language processing models that would facilitate the activity of composition by conversation. We present a prototype framework, called MusECI, that merges these domains, permitting score-level algorithmic composition in a text editor while also supporting connectivity to existing natural language processing frameworks.", "histories": [["v1", "Thu, 7 Sep 2017 05:39:00 GMT  (239kb,D)", "http://arxiv.org/abs/1709.02076v1", "6 pages, 8 figures, accepted to ICMC 2017"]], "COMMENTS": "6 pages, 8 figures, accepted to ICMC 2017", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.IR cs.PL", "authors": ["donya quick", "clayton t morrison"], "accepted": false, "id": "1709.02076"}, "pdf": {"name": "1709.02076.pdf", "metadata": {"source": "META", "title": "Composition by Conversation", "authors": ["Donya Quick", "Clayton T. Morrison"], "emails": ["dquick@stevens.edu", "claytonm@email.arizona.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "We have reached an age where natural voice interfaces are increasingly available to our technology, and these interfaces provide opportunities to interact more naturally with our devices. For example, it is becoming increasingly common for people to speak directly to mobile devices to perform tasks where no one is on the other end of the line. However, little work has been done to support conversations and interactions with musicians. Deep learning with neural networks is currently a popular strategy in the field of natural speech processing (NLP) and has been applied to the development of dialogue systems that attempt to have a conversation with a human."}, {"heading": "2. PROGRAMMING LANGUAGES", "text": "A central goal of the MusECI project is to create a language for the representation of music at score level that can be used in an algorithmic or automated compositional environment, while at the same time adhering closely to models of music cognition and expression in natural language. We accept the limitations that the data structures used to represent music have both a simple interpretation for editing and reproduction, and that they can also be easily queried to identify features based on natural language descriptions and manipulate them according to instructions.Many musical programming languages focus on the sound or signal aspects of music production. Examples of this are Csound [5], SuperCollider [6] 2, and Max / MSP [8]. While these languages are extreme, our implementation is at musica.ml4ai.org /. 2 SuperCollider represents another language paradigm known as live coding."}, {"heading": "2.1 Algorithmic Score Analysis and Generation", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2.2 Modeling Natural Language Semantics", "text": "A major limitation for MusECI is that it has to take into account the semantics of how music is described in natural language. To achieve this, we are inspired by computer languages that are designed to represent the natural language semantics of visual and physical environments such as VoxML. [4] The VoxML modeling language encodes semantic knowledge of objects in the real world that are presented as three-dimensional physical models, along with events and attributes that are related to each other and change through actions and interactions. The framework encodes background knowledge of how interactions between objects affect their properties, and allows simulation to supplement missing information in speech inputs. With this representation, expressions of natural language that describe scenes, events and actions on objects can be specified in such detail that they can be simulated and visualized. We also seek a language in which musical concepts can be expressed in building blocks that express in close harmony with natural language."}, {"heading": "3. MUSICAL PRIMITIVES", "text": "Our lowest level of representation for musical ECIs are called musical primitives: concepts that represent basic musical units. As we focus on music at the level of a paper score, our primitives are based on features that occur in scores: notes and rests."}, {"heading": "3.1 Symbolic Primitives", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3.2 Connecting Primitives", "text": "MusECI uses Euterpea-inspired representations to connect musical structures sequentially and in parallel. To minimize the problem of multiple tree structures associated with Euterpea's binary trees, we use n-ary trees. This allows notes in a melody or chord that have the same level of conceptual hierarchy to appear on the same level within the representation. We also define normal shapes for grouping structures from a score format such as MusicXML. The set of all possible sequential and parallel structures that we call M is recursively defined. Seq ({M | P} +, contexts) Par ({M | P} +, contexts) (7) We use square brackets notation [...] to denote lists of elements. Therefore, \"the three-tone melody in Part B\" could be referred to as: Seq ([N (), N (), \"Part B\"), (8) we will refer to the value of any music we can use in any of it."}, {"heading": "3.3 MIDI and MusicXML Conversion", "text": "MusECI is able to analyze both MIDI and MusicXML files in a normal form with the represented representations.5 In MusECI we measure and beat numbers both from zero. Therefore, \"bar 3\" generates a value of 2.50. We use the following algorithm to convert MIDI and MusicXML information into representations of our system: 1. Greedily we group notes with the same beginning and the same duration with Abs. 2. Greedily we group temporally adjacent structures (potentially including chords from step 1) with Abs. 3. We group all remaining temporally separated but still sequential structures with Seq and use pauses to fill time gaps. 4. Let us group all remaining elements under a global par.Importantly, this normal form for reading MusicXML is not the only way to represent musical characteristics. Other normal shapes are possible, and we hope to improve our normal shapes by learning later work, so that phrases are identified within achretical structures."}, {"heading": "4. SELECTION", "text": "Similar to the SELECT statement of SQL, which searches a database of tables for entries that match a query, we want to scan a musical data structure and return references to the correct parts of it. We use pattern matching as an integral part of our selection process, which checks whether specific definitions match with characteristics that match in an incomplete or abstract definition. MusECI defines the selection operation as a function that takes two music values: one as a pattern for searching (a query) and the other for matching with the pattern. The \"values match with a concrete value. For example, the select statement (N (,,...), m) selects all notes from the music value, m and returns a collection of references to the notes. Playing references in an object-oriented style allows m to be located immediately after relevant parts of the same: Query, Query = Query (can also be expressed via) a part."}, {"heading": "5. OPERATIONS", "text": "Support for creating and generating music requires the definition of certain standard operators for manipulating musical data structures. The following are some examples of operations in MusECI that we will use later in examples: \u2022 invert (music) and invertAt (pitch, music): generalized musical inversion (turning along the pitch axis) at the first pitch or at a certain pitch. \u2022 retrograde (music): inverting a music structure. \u2022 transpose (int. music): Transpose a music with a number of half-steps, either chromatic or by scale, depending on the given tonal context."}, {"heading": "6. INTEGRATION WITH TRIPS PARSER", "text": "In our work, we use the TRIPS natural language parser. TRIPS is a bottom-up diagram parser that integrates a grammar and a lexicon to encode both syntactic and semantic characteristics, which are used to decipher and generate a logical representation of the form that captures the semantic roles of terms in the utterance. [17] Figure 1 shows the logical shape graph that TRIPS uses after parsing the phrase \"Move B up an octave in bar 2.\" (Many details of the logical shape representation of TRIPS have been removed for presentation.) Labels on slurs (surrounded by \"<... >\") represent semantic roles, and bold face labels represent lexical terms that are preceded by their semantic type (e.g., a music note)."}, {"heading": "6.1 Resolving Ambiguities", "text": "Natural language is extremely sensitive to the history of conversation. Sentences like \"Give it this\" are therefore impossible to resolve semantically without context for assigning pronouns to entities. In a musical environment, references like \"the C beside\" also suffer from this problem. Even with enough context to resolve references like pronouns, it can be insufficiently detailed to find a precise part of a musical concept. Consider the pitch classes in the opening refrain of \"Twinkle Twinkle Little Star:\" C, C, G, G, G, A, G. The request to \"move the G half a step up\" creates an ambiguity that G should be moved over, since the selection (N (G,),...) will find ambiguities. However, \"the G\" implies that we only want to operate with one of them. A system for addressing referential ambiguity requires two features: a working memory and a working memory that investigates algorithms."}, {"heading": "6.2 Examples", "text": "Let's look at the two-bar melody shown in Figure 2 and assume we want to turn it into the melody shown in Figure 4, using natural voice commands. A precise way to describe this would be to \"move the F up by a whole step\" followed by \"move the C down by the first bar of bar two by a half-step.\" This leads to the following interactions between computer (C) and user (U). Conversation 1 C: m = the melody in Figure 2. U: \"Move the F up by a whole step.\" C: Transpose (((C), (1, 0),..., select (N (F,),,,,, m) U: \"Move the C around the first bar of two bars by a half-step down.\" C: Transpose (\u2212 1, select (C,), (1, 0), (...), select (1, 0), m), eloquently convert the data structure (3) to the beginning structure is shown."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "We have proposed the framework for modeling music in a way that is based on the analysis of natural language impressions. Our Python implementation of musical data structures supports normal algorithmic composition in a text editor in addition to composition, which can be easily integrated into the analysis of systems such as TRIPs. Our data structures are robust enough to represent complex structures. Currently, the overall system is a prototype limited to a relatively narrow collection of musical operations, and this collection of operations needs to be extended to include further tree manipulations, such as different strategies for removing and rearranging notes within a larger structure."}, {"heading": "8. REFERENCES", "text": "[1] I. V. Serban et al., \"Building End-to-end Dialogue Systems Using Generative Hierarchical Neural Network Models,\" in Proceedings of the AAAI Conference on Artificial Intelligence, 2016, pp. 3776-3783. [2] J. Mandler, The foundations of mind: Origins of conceptual thought. Oxford University Press, 2004. [3] R. St.Amant et al.], An Image Schema Language, \"in Proceedings of the International Conference on Cognitive Modeling, 2006. [4] J. Pustejovsky and N. Krishnaswamy,\" VoxML: A Vistualization Modeling Language, \"in Language Resources Evaluation Conference (LREC), 2016. [5] R. Boulanger, The Csound Book: Perspectives in Software Synthesis, Sound Design, Signal Processing, and Programming."}], "references": [{"title": "Building End-to-end Dialogue Systems Using Generative Hierarchical Neural Network Models,", "author": ["I.V. Serban"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "The foundations of mind: Origins of conceptual thought", "author": ["J. Mandler"], "venue": "Oxford University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "An Image Schema Language,", "author": ["R. St.Amant"], "venue": "Proceedings of the International Conference on Cognitive Modeling,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "VoxML: A Vistualization Modeling Language,", "author": ["J. Pustejovsky", "N. Krishnaswamy"], "venue": "in Language Resources Evaluation Conference (LREC),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "The Csound Book: Perspectives in Software Synthesis", "author": ["R. Boulanger"], "venue": "Sound Design, Signal Processing,and Programming. MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Rethinking the Computer Music Language: SuperCollider,", "author": ["J. McCartney"], "venue": "Computer Music Journal,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Interacting with Generative Music through Live Coding,", "author": ["A.R. Brown", "A. Sorensen"], "venue": "Contemporary Music Review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "LilyPond", "author": ["H. Nienhuys", "J. Nieuwenhuizen"], "venue": "a system for automated music engraving,\u201d in Proceedings of the Colloquium on Musical Informatics", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "MusicXML for Notation and Analysis,\u201d in The Virtual Score: Representation, Retrieval, Restoration", "author": ["M. Good"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "MuSQL: A Music Structured Query Language,", "author": ["C. Wang"], "venue": "Proceedings of the International Multimedia Modeling Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Making Music with Computers: Creative Programming in Python", "author": ["B. Manaris", "A. Brown"], "venue": "New York, NY, USA: Chapman and Hall", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Making Music with Java", "author": ["A. Brown"], "venue": "lulu.com", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Composing with Kulitta,", "author": ["D. Quick"], "venue": "Proceedings of International Computer Music Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "and W", "author": ["J.F. Allen", "M. Swift"], "venue": "de Beaumont, \u201cDeep Semantic Analysis for Text Processing,\u201d in Proceedings of the Symposium on Semantics in Systems for Text Processing", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning with neural networks is currently a popular strategy in the domain of natural language processing (NLP), and has been applied to developing dialog systems where a machine attempts to carry on a conversation with a human [1].", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "For example, there is some evidence to suggest that \u201csurface\u201d is one such concept that infants have and use to build other concepts, such as collections of surfaces forming objects and internal spaces [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "This style of representation has been proposed to underly human natural language processing tasks such as describing and referring to items and events in visual environments [3, 4].", "startOffset": 174, "endOffset": 180}, {"referenceID": 3, "context": "This style of representation has been proposed to underly human natural language processing tasks such as describing and referring to items and events in visual environments [3, 4].", "startOffset": 174, "endOffset": 180}, {"referenceID": 4, "context": "Examples of these include Csound [5], SuperCollider[6] 2 , and Max/MSP[8].", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Examples of these include Csound [5], SuperCollider[6] 2 , and Max/MSP[8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "2 SuperCollider also represents another language paradigm known as live coding[7], which allows compilation and execution of fragments of a larger program on demand.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Languages such as LilyPond [9] focus on type-setting aspects of a score, with emphasis on details of visual presentation and formatting and less on relationships between musical features that would be useful for modeling music cognition.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "MusicXML[10] is one such language.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "MuSQL [11] is a structured query language for operating on musical databases.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "The Jython Music library [13], a Python version of JMusic [14], favors list representations for constructing melodies and chords using notes and rests.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "The Jython Music library [13], a Python version of JMusic [14], favors list representations for constructing melodies and chords using notes and rests.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "The structures and operations we use here are also very similar to some representations used in Kulitta [16], which is a Haskell-based framework for automated composition in multiple styles.", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "To accomplish this, we take inspiration from computational languages designed for representing the natural language semantics of visual and physical environments, such as VoxML [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "These features are used to disambiguate and produce a logical form representation that captures the semantic roles of terms in the utterance [17].", "startOffset": 141, "endOffset": 145}], "year": 2017, "abstractText": "Most musical programming languages are developed purely for coding virtual instruments or algorithmic compositions. Although there has been some work in the domain of musical query languages for music information retrieval, there has been little attempt to unify the principles of musical programming and query languages with cognitive and natural language processing models that would facilitate the activity of composition by conversation. We present a prototype framework, called MusECI, that merges these domains, permitting score-level algorithmic composition in a text editor while also supporting connectivity to existing natural language processing frameworks.", "creator": "LaTeX with hyperref package"}}}