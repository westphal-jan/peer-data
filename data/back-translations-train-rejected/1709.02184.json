{"id": "1709.02184", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Translating Domain-Specific Expressions in Knowledge Bases with Neural Machine Translation", "abstract": "Our work presented in this paper focuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domain-specific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model.", "histories": [["v1", "Thu, 7 Sep 2017 11:31:09 GMT  (30kb)", "http://arxiv.org/abs/1709.02184v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mihael arcan", "paul buitelaar"], "accepted": false, "id": "1709.02184"}, "pdf": {"name": "1709.02184.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["firstname.lastname@insight-centre.org"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.02 184v 1 [cs.C L] 7S ep2 017cuses on the translation of domain-specific expressions represent in semantically structured resources, like ontologies or knowledge graphs. In order to make knowledge accessible beyond language boundaries, these resources need to be translated into different languages. The challenge in translating names or terminological expressions represented in ontologies lies in the highly specific vocabulary and lack of contextual information that can cause a machine translation system to translate ambiguous words into the target area. Due to the challenges we train and translate terminological expressions in the media and financial spheres with both statistical and neural machine translation methods. We evaluate the translation quality of domain-specific expressions with translation systems trained on the basis of a generic data set, and experiment with terminological expressions. In addition, we conduct experiments to inject external knowledge systems based on the SMD, we do not have a distinct advantage in these translation systems through the SMD method."}, {"heading": "1 Introduction", "text": "Most of the labels stored in semantically structured resources, i.e. ontologies, taxonomies or knowledge graphs, are represented only in English (Gracia et al., 2012). Therefore, applications in information retrieval, in which these monolingual resources are used, are limited to the language in which the information, i.e. terms or metadata, is stored. To allow access to knowledge of languages, these resources must be enriched with multilingual information. This enhancement can enable the extraction of information beyond English, e.g. for financial interlinguistic business intelligence (O'Riain et al., 2013)."}, {"heading": "2 Related Work", "text": "Most of them are able to surpass themselves by setting out in search of new paths that they are treading, most of them are able to set out in search of new paths that they are treading, most of them are able to set out in search of new paths that they are about to tread, most of them are able to set out in search of new paths that they are about to tread, and the others who are about to tread are able to set out in search of new paths that they are not about to tread."}, {"heading": "3 Methodology", "text": "We use the SMT and NMT approaches to translate domain-specific expressions, focusing specifically on the performance of NMT and how it handles the translation of expressions that are rarely found in parallel training data. Therefore, we examine how translation quality can benefit from using subword segmentation to help overcome the problem of vocabulary constraint. In addition, we perform domain customization with domain-specific expressions and experiment with various approaches and methods to bring external knowledge into the translation process. These methods are described in the following sections."}, {"heading": "3.1 Byte Pair Encoding", "text": "A common problem in machine translation in general is rare and unknown words, such as terminological expressions that the system has rarely or never seen. Thus, if the training method does not see a particular word or phrase several times during the training, it does not learn the correct translation. NMT is even more challenging because of the complexity associated with neural networks. Therefore, vocabulary is often limited to 50,000 or 100,000 words (compared to 200,000 or more words in a two million body). To overcome this limitation, various methods have been proposed, such as character-based NMT (Costa-Jussa and Fonollosa, 2016; Ling et al., 2015) or the use of subword units, such as byte pair encoding (BPE), which has been successfully adapted for word segmentation specifically for the NMT scenario Sennrich et al. (2015; Ling et al., 2015)."}, {"heading": "3.2 Domain Adaptation", "text": "To adjust the loglinear weights of the SMT system to the resource type and domain, we re-run MERT (Och, 2003) based on the development set of available resources. To assess whether preset weights have an impact on domain matching, we apply two strategies: First, we start with the retuning approach from the already adjusted weights of the various models, based on the generic development set (generic \u2192 domain specific in Table 3); second, we start with the adjustment of weights from the unadjusted, uniformed weights (domain specific); in addition, we perform a weight adjustment across different ranges to see if the text type can improve translation quality independently of the domain. For domain fitting within the NMT, we used models trained on generic data (generic data) or the small weight adjustment of individual resources and Wikipedia weights."}, {"heading": "3.3 Integration of Terminological Knowledge into Machine Translation", "text": "Due to the fact that domain-specific bilingual information is missing and cannot be learned from the parallel sentences, some of the terminological expressions cannot be automatically translated with an SMT or NMT system. Therefore, we use the information we get from Wikipedia or the in-domain data of ontology1 to improve the translation of expressions not known to the translation systems. XML markup within SMT Using the XML markup approach, external knowledge can be passed directly to the decoder by specifying the translation of a certain range of the source set. In the case of multiple translations of the same source range, a score can be used to indicate the association between the source and target formulation. In the case of using Wikipedia titles as external knowledge, we conduct two experiments. In the first experiment, we set all probabilities of the translation candidates to 1.0."}, {"heading": "4 Experimental Setting", "text": "In this section, we give an overview of the data sets and translation tools used in our experiment. In addition, we describe the existing approaches and provide insights into SMT and NMT evaluation techniques, taking into account the translation direction from English to German."}, {"heading": "4.1 Training Datasets", "text": "For a broader coverage of the generic training dataset required for the SMT system, we have merged parts of JRCAcquis 3.0 (Steinberger et al., 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), giving us a training corpus of nearly two million sentences with about 38M running words (Table 1). 6Due to the challenging task of terminology translation, we are conducting an additional experiment in which we combine the generic corpus of two million sentences with Wikipedia entries (about 876k), which have an interlingual link between the En-2an object designed as Container 3a for water transport. 4OpenNMT Option Phrase Table 5OpenNMT Option -replace unk 6For reproducibility and future evaluation, we take the first third of each corpus.glish and the German language in the Wikipedia repository."}, {"heading": "4.2 Evaluation Dataset", "text": "For our evaluation tasks, we use SMT and NMT translation models for different text types and domains. For a general overview and comparison of the performance of SMT and NMT, we use the English-German evaluation basis WMT157. In addition, we perform translations to domain-specific expression translations covering both generic and domain-specific expressions. ICD ontology For our experiments, we used the International Classification of Diseases (ICD) Ontology as the gold standard. http http: / Wikipedia titles covering both generic and domain-specific expressions. ICD ontology for our experiments, we used the International Classification of Diseases (ICD) Ontology as the gold standard. 8 With the designations, we orient ourselves between the English and German languages. ICD ontology, translated into 43 languages, is used to monitor diseases and report the general health situation of the population in a country."}, {"heading": "4.3 Machine Translation tools", "text": "Moses For our SMT translation task, we use the Moses statistical translation toolkit (Koehn et al., 2007), which uses the GIZA + + toolkit (Och and Ney, 2003) to generate word alignments, and the KenLM toolkit (Heafield, 2011), which was used to create the 5 gram language model.OpenNMT OpenNMT (Klein et al., 2017) is a general deep learning framework specializing mainly in sequence-to-sequence (seq2seq) models that cover a variety of tasks such as machine translation, summary, image text, and speech recognition. We used the standard OpenNMT parameters, i.e. 2 layers, 500 hidden LSTM units, input, stack size of 64, 0.3 failure probability, and dynamic learning rate."}, {"heading": "4.4 Evaluation Metrics", "text": "Automatic translation evaluation is based on the correspondence between the SMT output and the reference translation (gold standard), using the metrics BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovic \u0301, 2015). BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations, which range from 0 to 100 (perfect translation) and are then averaged over the entire evaluation dataset to obtain an estimate of the overall quality of the translation. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonious means of precision and memory, with memory being weighted higher than precision. Together with exact words (or phrases), the correspondence indicates additional features, i.e. metaphors and symbols, overlap with English."}, {"heading": "5 Evaluation", "text": "In this section, we report on a general quality comparison between SMT and NMT based on the WMT15 dataset. Furthermore, we evaluate the translation quality of ontology labels and Wikipedia titles using the SMT and NMT methods. Additionally, we conduct experiments with domain adaptation with in and out domain knowledge. The final experiment supports the translation system with in-domain lexical knowledge and knowledge identified in the Wikipedia repository."}, {"heading": "5.1 WMT15 Dataset Evaluation", "text": "We translate the WMT15 evaluation set to evaluate the generic performance of the trained SMT and NMT models, taking into account the translation generation from English to German. We trained the SMT and NMT translation models from the generic corpus, as well as the combination of generic corpus and Wikipedia entries. For NMT, we do not automatically limit the translation quality of the WMT15 data set to 50,000 and 100,000 words, respectively. In addition, by applying subword segmentation (BPE) and limiting the vocabulary to 32,000 (BPE32k) and 90,000 (BPE32k) units, we do not automatically improve the translation quality of the WMT15 data set in relation to BLEU. This is particularly evident in SMT, where the BLEU score of 16.02 (generic data) on Wikic data drops."}, {"heading": "5.2 Translation of Domain-Specific Expression Evaluation", "text": "In this evaluation section, we focus on the domain-specific vocabulary stored in ICD and IFRS ontology, as well as on Wikipedia titles. Unlike the WMT15 evaluation, entries in these records rarely appear in the training data and are translated in isolation, i.e. without contextual information, which can be helpful in disambiguating ambiguous expressions. Similar to the WMT15 evaluation experiment, adding Wikipedia knowledge to the generic corpus does not always improve translation quality when translating ontology labels (Table 2). Focusing on the SMT method improves performance in relation to ICD ontology labels (6.39 to 7.40), with the performance of IFRS ontology labels exceeding translation quality (10.51 vs. 9.03)."}, {"heading": "5.3 Domain Adaptation with Domain-Specific Expressions", "text": "When performing the domain matching experiment, we use the development kits of the aforementioned resources, which adjust the weights of the loglinear models in SMT or the weights in the architecture of the neural network. As shown in Table 3, the translation quality improves compared to the SMT system, trained and tuned to generic data. As an example, the BLEU score of ICD label translations is increased from 6.39 to 8.02 when the weights are adjusted to the target domain. Similarly, significant improvements are observed for the IFRS and Wikipedia evaluation datasets. Using the models with generic sentences and Wikipedia knowledge, the translation quality improves significantly for the ICD (9.11 vs 7.40) and IFRS labels for ontology (14.24 vs 9.25)."}, {"heading": "5.4 External Knowledge Injection into Machine Translation", "text": "As a final experiment, we compare the performance of injecting external knowledge into the SMT and NMT systems. To simulate a common scenario, we only use generic models and inject into the domain and Wikipedia knowledge as an external resource into the translation process. As mentioned in Section 3.3, we give all translation candidates a probability of 1.0. In the second setting, we adapt the probability depending on the similarity between the domain and the Wikipedia abstracts. We learned from Table 4 that the applied probabilities can be found in the voculary muscular vocabularies."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a performance comparison between SMT and NMT in the translation of highly domain-specific expressions, i.e. terminological expressions stored in ICD ontology, medicine, and IFRS in the financial field. In addition, we are conducting experiments to translate Wikipedia titles, which can be both domain and generic expressions. We show that the Wikipedia resource can be advantageous in the translation approach, but due to the lexical ambiguity of Wikipedia titles, the translation candidates should be classified according to the target domain. We show that domain matching with terminological expressions significantly improves translation quality, which is particularly evident when an existing generic neural network with the vocabulary of the target domain is retrained."}, {"heading": "Acknowledgments", "text": "This publication is the result of research funded by Science Foundation Ireland (SFI) under grant number SFI / 12 / RC / 2289 (Insight)."}], "references": [{"title": "Translating the FINREP Taxonomy using a Domainspecific Corpus", "author": ["Arcan et al.2013] Mihael Arcan", "SusanMarie Thomas", "Derek De Brandt", "Paul Buitelaar"], "venue": "In Machine Translation Summit XIV", "citeRegEx": "Arcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arcan et al\\.", "year": 2013}, {"title": "Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation", "author": ["Arcan et al.2014] Mihael Arcan", "Claudio Giuliano", "Marco Turchi", "Paul Buitelaar"], "venue": "In Proceedings of the 4th International Workshop on Com-", "citeRegEx": "Arcan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arcan et al\\.", "year": 2014}, {"title": "Knowledge portability with semantic expansion of ontology labels", "author": ["Arcan et al.2015] Mihael Arcan", "Marco Turchi", "Paul Buitelaar"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Arcan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arcan et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Guiding neural machine translation decoding with external knowledge", "author": ["Matteo Negri", "Marco Turchi", "Marcello Federico", "Lucia Specia", "Fr\u00e9d\u00e9ric Blain"], "venue": "In Proceedings of the Second Conference on Machine", "citeRegEx": "Chatterjee et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2017}, {"title": "An empirical comparison of domain adaptationmethods for neural machine translation", "author": ["Chu et al.2017] Chenhui Chu", "Raj Dabre", "Sadao Kurohashi"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-", "citeRegEx": "Chu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "A note on ontology localization", "author": ["Elena MontielPonsoda", "Paul Buitelaar", "Mauricio Espinoza", "Asunci\u00f3n G\u00f3mez-P\u00e9rez"], "venue": null, "citeRegEx": "Cimiano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2010}, {"title": "Character-based neural machine translation. CoRR, abs/1603.00810", "author": ["Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": null, "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Multilingual lexical semantic resources for ontology translation", "author": ["Declerck", "Asunci\u00f3n G\u00f3mez P\u00e9rez", "Ovidiu Vela", "Zeno Gantner", "David Manzano."], "venue": "In Proceedings of the 5th International Con-", "citeRegEx": "Declerck et al\\.,? 2006", "shortCiteRegEx": "Declerck et al\\.", "year": 2006}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Improving the extraction of bilingual terminology from wikipedia", "author": ["Kotaro Nakayama", "Takahiro Hara", "Shojiro Nishio"], "venue": "ACM Trans. Multimedia Comput. Commun. Appl.,", "citeRegEx": "Erdmann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erdmann et al\\.", "year": 2009}, {"title": "Ontology localization", "author": ["Elena Montiel-Ponsoda", "Asunci\u00f3n G\u00f3mez-P\u00e9rez"], "venue": "In Proceedings of the Fifth International Conference on Knowledge Capture,", "citeRegEx": "Espinoza et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Espinoza et al\\.", "year": 2009}, {"title": "Measuring user productivity in machine translation enhanced computer assisted translation", "author": ["Alessandro Cattelan", "Marco Trombetti"], "venue": "In Proceedings of the Tenth Conference of the Association for Machine", "citeRegEx": "Federico et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2012}, {"title": "Cross-lingual ontology mapping - an investigation of the impact of machine translation", "author": ["Fu et al.2009] Bo Fu", "Rob Brennan", "Declan O\u2019Sullivan"], "venue": null, "citeRegEx": "Fu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2009}, {"title": "A new algorithm for data compression", "author": ["Philip Gage"], "venue": "C Users J.,", "citeRegEx": "Gage.,? \\Q1994\\E", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Challenges for the multilingual web of data. Web Semantics: Science, Services and Agents on the World Wide", "author": ["Gracia et al.2012] Jorge Gracia", "Elena MontielPonsoda", "Philipp Cimiano", "Asunci\u00f3n G\u00f3mez-P\u00e9rez", "Paul Buitelaar", "John McCrae"], "venue": null, "citeRegEx": "Gracia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gracia et al\\.", "year": 2012}, {"title": "The efficacy of human post-editing for language translation", "author": ["Green et al.2013] Spence Green", "Jeffrey Heer", "Christopher D Manning"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Green et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Green et al\\.", "year": 2013}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Recurrent continuous translation models. Seattle, October", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Opennmt: Open-source toolkit for neural machine translation. CoRR, abs/1701.02810", "author": ["Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush"], "venue": null, "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Stroudsburg, PA, USA.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit. AAMT", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Assessing post-editing efficiency in a realistic translation environment. InMachine Translation Summit XIV", "author": ["L\u00e4ubli et al.2013] Samuel L\u00e4ubli", "Mark Fishel", "Gary Massey", "Maureen Ehrensberger-Dow", "Martin Volk"], "venue": null, "citeRegEx": "L\u00e4ubli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "L\u00e4ubli et al\\.", "year": 2013}, {"title": "DBpedia - a large-scale, multilin", "author": ["Lehmann et al.2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Christian Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "Character-based neural machine translation. CoRR, abs/1511.04586", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proceedings of the International Workshop on Spoken Language Translation,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206", "author": ["Luong et al.2014] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Combining statistical and semantic approaches to the translation of ontologies and taxonomies", "author": ["McCrae et al.2011] John McCrae", "Mauricio Espinoza", "ElenaMontiel-Ponsoda", "GuadalupeAguado-de Cea", "Philipp Cimiano"], "venue": "In Fifth workshop on Syn-", "citeRegEx": "McCrae et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McCrae et al\\.", "year": 2011}, {"title": "Domain adaptation for ontology localization", "author": ["Mihael Arcan", "Kartik Asooja", "Jorge Gracia", "Paul Buitelaar", "Philipp Cimiano"], "venue": "Web Semantics,", "citeRegEx": "McCrae et al\\.,? \\Q2016\\E", "shortCiteRegEx": "McCrae et al\\.", "year": 2016}, {"title": "Using Wikipedia to Translate Domain-specific Terms in SMT", "author": ["Niehues", "Waibel2011] Jan Niehues", "Alex Waibel"], "venue": "In International Workshop on Spoken Language Translation,", "citeRegEx": "Niehues et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niehues et al\\.", "year": 2011}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Cross-lingual querying and comparison of linked financial and business data", "author": ["O\u2019Riain et al.2013] Se\u00e1n O\u2019Riain", "Barry Coughlan", "Paul Buitelaar", "Thierry Declerck", "Uli Krieger", "Susan Marie Thomas"], "venue": null, "citeRegEx": "O.Riain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "O.Riain et al\\.", "year": 2013}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "chrf: character n-gram f-score for automatic mt evaluation", "author": ["Maja Popovi\u0107"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Popovi\u0107.,? \\Q2015\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Domain specialization: a post-training domain adaptation for neural machine translation. CoRR, abs/1612.06141", "author": ["Josep Maria Crego", "Jean Senellart"], "venue": null, "citeRegEx": "Servan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Servan et al\\.", "year": 2016}, {"title": "Improving machine translation through linked data", "author": ["Georg Rehm", "Felix Sasaki"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Srivastava et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2017}, {"title": "Results of the WMT15 Metrics Shared Task", "author": ["Amir Kamran", "Philipp Koehn", "Ond\u0159ej Bojar"], "venue": "In Proceedings of the 10th Workshop on Statistical Machine Translation", "citeRegEx": "Stanojevi\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stanojevi\u0107 et al\\.", "year": 2015}, {"title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages", "author": ["Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "D\u00e1niel Varga"], "venue": null, "citeRegEx": "Steinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Extracting bilingual word pairs from wikipedia. In Collaboration: interoperability between people in the creation of language resources for less-resourced languages", "author": ["Tyers", "Pieanaar2008] Francis M. Tyers", "Jacques A. Pieanaar"], "venue": null, "citeRegEx": "Tyers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "ontologies, taxonomies or knowledge graphs, are represented in English only (Gracia et al., 2012).", "startOffset": 76, "endOffset": 97}, {"referenceID": 31, "context": "for cross-lingual business intelligence in the financial domain (O\u2019Riain et al., 2013; Arcan et al., 2013), providing information related to an ontology label, e.", "startOffset": 64, "endOffset": 106}, {"referenceID": 0, "context": "for cross-lingual business intelligence in the financial domain (O\u2019Riain et al., 2013; Arcan et al., 2013), providing information related to an ontology label, e.", "startOffset": 64, "endOffset": 106}, {"referenceID": 12, "context": "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; L\u00e4ubli et al., 2013; Green et al., 2013).", "startOffset": 258, "endOffset": 322}, {"referenceID": 22, "context": "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; L\u00e4ubli et al., 2013; Green et al., 2013).", "startOffset": 258, "endOffset": 322}, {"referenceID": 16, "context": "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; L\u00e4ubli et al., 2013; Green et al., 2013).", "startOffset": 258, "endOffset": 322}, {"referenceID": 3, "context": "Due to the large success of NMT in recent years (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014), we evaluate its the translation performance against the usage of SMT, comparing which method handles better the translation of domain-specific expressions in isolation, i.", "startOffset": 48, "endOffset": 127}, {"referenceID": 40, "context": "Due to the large success of NMT in recent years (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014), we evaluate its the translation performance against the usage of SMT, comparing which method handles better the translation of domain-specific expressions in isolation, i.", "startOffset": 48, "endOffset": 127}, {"referenceID": 8, "context": "EuroWordNet or IATE (Declerck et al., 2006; Cimiano et al., 2010).", "startOffset": 20, "endOffset": 65}, {"referenceID": 6, "context": "EuroWordNet or IATE (Declerck et al., 2006; Cimiano et al., 2010).", "startOffset": 20, "endOffset": 65}, {"referenceID": 13, "context": "BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009).", "startOffset": 90, "endOffset": 130}, {"referenceID": 11, "context": "BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009).", "startOffset": 90, "endOffset": 130}, {"referenceID": 3, "context": ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al.", "startOffset": 8, "endOffset": 510}, {"referenceID": 3, "context": ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al. (2011), where the structure of the ontology along with existing multilingual ontologies was used to annotate the labels with their semantic senses.", "startOffset": 8, "endOffset": 535}, {"referenceID": 3, "context": ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al. (2011), where the structure of the ontology along with existing multilingual ontologies was used to annotate the labels with their semantic senses. Furthermore, McCrae et al. (2016) show positive effects of different domain adaptation techniques, i.", "startOffset": 8, "endOffset": 710}, {"referenceID": 0, "context": "A different approach on ontology label disambiguation was shown in Arcan et al. (2015), where the authors identified relevant in-domain parallel sentences and used them to train an ontology-specific SMT system.", "startOffset": 67, "endOffset": 87}, {"referenceID": 7, "context": "Besides the interwiki link system, Erdmann et al. (2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia.", "startOffset": 35, "endOffset": 57}, {"referenceID": 7, "context": "Besides the interwiki link system, Erdmann et al. (2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, Niehues and Waibel (2011) and Arcan et al.", "startOffset": 35, "endOffset": 236}, {"referenceID": 0, "context": "To cast the problem of ambiguous Wikipedia titles, Niehues and Waibel (2011) and Arcan et al. (2014) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge.", "startOffset": 81, "endOffset": 101}, {"referenceID": 35, "context": "In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016).", "startOffset": 134, "endOffset": 203}, {"referenceID": 36, "context": "In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016).", "startOffset": 134, "endOffset": 203}, {"referenceID": 23, "context": "Nevertheless, without the help of subword segmentation, Luong et al. (2014) utilized the out-of-vocabulary issue by a post-processing step that replaced every unknown word with the usage of a dictionary.", "startOffset": 56, "endOffset": 76}, {"referenceID": 4, "context": "Differently to the post-processing step, Chatterjee et al. (2017) propose a mechanism that guides an existing NMT decoder with the ability to prioritize and adequately handle translation candidates provided by the external resource.", "startOffset": 41, "endOffset": 66}, {"referenceID": 4, "context": "Differently to the post-processing step, Chatterjee et al. (2017) propose a mechanism that guides an existing NMT decoder with the ability to prioritize and adequately handle translation candidates provided by the external resource. In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016). In addition to the fine-tuning method, Chu et al. (2017) tune the neural model with in- and out-domain data, whereby they use tags to annotate the domains within the used corpora.", "startOffset": 41, "endOffset": 495}, {"referenceID": 24, "context": "character based NMT (Costa-Juss\u00e0 and Fonollosa, 2016; Ling et al., 2015) or using subword units, e.", "startOffset": 20, "endOffset": 72}, {"referenceID": 14, "context": "BPE (Gage, 1994) is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.", "startOffset": 4, "endOffset": 16}, {"referenceID": 23, "context": "character based NMT (Costa-Juss\u00e0 and Fonollosa, 2016; Ling et al., 2015) or using subword units, e.g. Byte Pair Encoding (BPE). The latter one was successfully adapted for word segmentation specifically for the NMT scenario Sennrich et al. (2015). BPE (Gage, 1994) is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.", "startOffset": 54, "endOffset": 247}, {"referenceID": 30, "context": "To adapt the loglinear weights of the SMT system to the resource type and domain, we rerun MERT (Och, 2003) using the development set of the available resources.", "startOffset": 96, "endOffset": 107}, {"referenceID": 39, "context": "0 (Steinberger et al., 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).", "startOffset": 2, "endOffset": 28}, {"referenceID": 21, "context": ", 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).", "startOffset": 21, "endOffset": 34}, {"referenceID": 41, "context": ", 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).", "startOffset": 57, "endOffset": 74}, {"referenceID": 23, "context": "In our work we use the DBpedia (Lehmann et al., 2015) repository, since it provides a structured and preprocessed Wikipedia abstracts.", "startOffset": 31, "endOffset": 53}, {"referenceID": 17, "context": "The KenLM toolkit (Heafield, 2011) was used to build the 5-gram language model.", "startOffset": 18, "endOffset": 34}, {"referenceID": 19, "context": "OpenNMT OpenNMT (Klein et al., 2017) is a generic deep learning framework mainly specialized in sequence-to-sequence (seq2seq) models covering a variety of tasks such as machine translation, summarisation, image to text, and speech recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 32, "context": "For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi\u0107, 2015) metrics.", "startOffset": 46, "endOffset": 69}, {"referenceID": 33, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi\u0107, 2015) metrics.", "startOffset": 53, "endOffset": 68}, {"referenceID": 38, "context": "task (Stanojevi\u0107 et al., 2015), especially when translating from English into morphologically rich(er) languages.", "startOffset": 5, "endOffset": 30}, {"referenceID": 37, "context": "Similar observation was shown in Srivastava et al. (2017), where the authors also used Wikipedia entries with similar outcome.", "startOffset": 33, "endOffset": 58}], "year": 2017, "abstractText": "Our work presented in this paper focuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domainspecific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model.", "creator": "LaTeX with hyperref package"}}}