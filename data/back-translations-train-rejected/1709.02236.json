{"id": "1709.02236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses", "abstract": "The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object's affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject's fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.", "histories": [["v1", "Tue, 29 Aug 2017 16:59:19 GMT  (4584kb,D)", "http://arxiv.org/abs/1709.02236v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["andrea gigli", "arjan gijsberts", "valentina gregori", "matteo cognolato", "manfredo atzori", "barbara caputo"], "accepted": false, "id": "1709.02236"}, "pdf": {"name": "1709.02236.pdf", "metadata": {"source": "CRF", "title": "Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses", "authors": ["Andrea Gigli", "Arjan Gijsberts", "Valentina Gregori", "Matteo Cognolato", "Manfredo Atzori", "Barbara Caputo"], "emails": ["surname@dis.uniroma1.it", "name.surname@hevs.ch"], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "II. RELATED WORK", "text": "The difficulty of reliably measuring and interpreting sEMG has led to active research into the inclusion of other types of sensory modalities to control myoelectric prostheses, such as sonomyography, mechanomyography and force myography (see [5, 6] for detailed overviews). In addition to those that measure muscle activity, modalities that provide an informative context about the intended movement have also been combined with sEMG. Several studies have shown that the acceleration of the relevant arm provides useful information about arm orientation and dynamics that complement sEMG [2, 7].More recently, computer vision and vision information have also been considered to detect the intention. Theirar Xiv: 170 9.02 236v 1 [cs.C V] 29 Aug 201 7relevance has been demonstrated in early studies where innovative systems for controlling the perception of a transradial prosthesis have been proposed."}, {"heading": "III. GAZE INTEGRATION", "text": "The basic idea behind this work is to extract a representation of the object observed during a catch and use it as a tool to support a standard sEMG-based catch classifier. To do this, we have developed a method that automatically detects stable eye fixations, extracts relevant visual information related to these fixations, and then integrates this information into the motion classifier."}, {"heading": "A. Fixation detection", "text": "The first step of the algorithm is to find fixations in the fixation of eye tracking data. A fixation consists of a period (usually between 350ms and 450ms [14]) during which the gaze remains in a limited area of the visual field. As we are only interested in fixations that precede an understanding, we try to identify an increase in muscle activity by identifying the visual square (RMS) of the myoelectric signals in a sliding window with length indications. As can be seen in Figure 1, the average activity across all the electrodes referred to as AvgRmsEmg increases dramatically during the initial phase of the visual range until detection. We identify these increases in an online manner using Bollinger bands that calculate the number of standard deviations that is an actual value of a signal within a historical mean within a sliding length window."}, {"heading": "C. Multimodal integration", "text": "At the end of the feature extraction process, the visual cue can be used alone or in conjunction with the myoelectric cue to train a grip classifier. Among the possible methods for integrating multimodal cue, we opt for mid-level integration [17], also known as kernel-level integration. This method combines pairs (x, y) of multimodal samples of type x = {x1, \u00b7 \u00b7, xC} by calculating their similarity via a weighted sum cue-specific kernel functionmc (x, y) = C \u2211 i = 1 wiki (x i, yi) for wi > 0. (4) The weights wi of the kernel combination are free hyperparameters of the multi-cue kernel. Such similarities are used by a kernel machine classifier to create the predictive model."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "We collected a custom dataset in which we recorded sEMG and glances while the subjects performed a series of accesses to different objects. Below, we explain the dataset and how the data was used in our offline evaluation."}, {"heading": "A. Dataset", "text": "Five intact subjects (4M, 1F) participated in our study. We selected ten graffiti graffiti based on the relevant literature [18] and their perceived significance for everyday activities (ADL). Each of the graffiti was performed on three representative objects that could reasonably be manipulated with their respective understanding. In selecting these objects, we tried to use them as much as possible for multiple graffiti to enforce a variety of relationships: grasps can be used with multiple objects and objects that can be used with their respective understanding, avoiding the risk that the identity of an object alone will be sufficient to clearly predict an understanding. During the acquisition, we ensured that there was a minimum of five objects placed in front of the subject to promote realistic viewing behavior and increase visual clarity. Aside from multiple objects, the acquisition protocol was expanded in two other manners to promote variability in myoelectric signals."}, {"heading": "B. Classifier", "text": "Our classification setup was also inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20]. This learning method is a so-called kernel method, which means that it addresses nonlinear problems by using core functions that implicitly map the original input space into a high-dimensional feature space. It also means that it is easy to use the multicue kernel described in Section III of this classification. Based on reports in previous work [19], we decided to combine the marginal discrete wavelet transform (mDWT) representation for sEMG in a 200ms sliding window with the expedit-2 kernel functionk2 (x, y) = exp (\u2212 n), we opted for the marginal wavelet transform (mDWT) representation in a 200ms sliding window with the expedit-2 functionk2 kernel."}, {"heading": "V. RESULTS", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which"}, {"heading": "VI. CONCLUSIONS", "text": "This paper showed how the standard sEMG-based grass classification benefits from integrating the affordability of the manipulated objects. We proposed a method to automatically extract the affordability of the object from a first-person video recording of the scene and an estimate of the viewing position. It identifies relevant fixations of the gaze based on ocular and muscular activity, the objects observed during these fixations are segmented and their affordability is encoded into high-grade visual features extracted by a conventional neural network. Although we have only performed an offline evaluation of the method, the fixation detection protocol has been designed to follow an online execution paradigm, and the method has been evaluated based on data collected from intact subjects who perform several of the most common captures in day-to-life activities."}], "references": [{"title": "Myoelectric forearm prostheses: State of the art from a user-centered perspective", "author": ["B. Peerdeman", "D. Boere", "H. Witteveen", "R.H. in \u2018t Veld", "H. Hermens", "S. Stramigioli", "H. Rietman", "P. Veltink", "S. Misra"], "venue": "Journal of Rehabilitation Research and Development, vol. 48, no. 6, pp. 719\u2013738, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A multi-modal approach for hand motion classification using surface emg and accelerometers", "author": ["A. Fougner", "E. Scheme", "A.D.C. Chan", "K. Englehart", "\u00d8. Stavdahl"], "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2011, pp. 4247\u20134250.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive vision system for control of dexterous prosthetic hands: Experimental evaluation", "author": ["S. Do\u0161en", "C. Cipriani", "M. Kosti\u0107", "M. Controzzi", "M.C. Carrozza", "D.B. Popovi\u0107"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 7, no. 1, p. 42, 2010. [Online]. Available: http: //dx.doi.org/10.1186/1743-0003-7-42", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Transradial prosthesis: Artificial vision for control of prehension", "author": ["S. Do\u0161en", "D.B. Popovi\u0107"], "venue": "Artificial Organs, vol. 35, no. 1, pp. 37\u201348, 2011. [Online]. Available: http://dx.doi.org/10.1111/j.1525-1594.2010.01040.x", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-modal sensing techniques for interfacing hand prostheses: A review", "author": ["Y. Fang", "Nalinda", "D. Zhou", "H. Liu"], "venue": "IEEE Sensors Journal, vol. 15, no. 11, pp. 6065\u20136076, Nov 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Noninvasive control interfaces for intention detection in active movement-assistive devices", "author": ["J. Lobo-Prat", "P.N. Kooren", "A.H. Stienen", "J.L. Herder", "B.F. Koopman", "P.H. Veltink"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 11, no. 1, p. 168, 2014. [Online]. Available: http://dx.doi.org/10.1186/1743-0003-11-168", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting accelerometers to improve movement classification for prosthetics", "author": ["A. Gijsberts", "B. Caputo"], "venue": "2013 IEEE 13th International Conference on Rehabilitation Robotics (ICORR), June 2013, pp. 1\u20135.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling handassistive devices: utilizing electrooculography as a substitute for vision", "author": ["Y. Hao", "M. Controzzi", "C. Cipriani", "D.B. Popovi\u0107", "X. Yang", "W. Chen", "X. Zheng", "M.C. Carrozza"], "venue": "IEEE Robotics Automation Magazine, vol. 20, no. 1, pp. 40\u201352, March 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Stereovision and augmented reality for closedloop control of grasping in hand prostheses", "author": ["M. Markovic", "S. Do\u0161en", "C. Cipriani", "D. Popovi\u0107", "D. Farina"], "venue": "Journal of  Neural Engineering, vol. 11, no. 4, p. 046001, 2014. [Online]. Available: http://stacks.iop.org/1741-2552/11/i=4/a=046001", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sensor fusion and computer vision for contextaware control of a multi degree-of-freedom prosthesis", "author": ["M. Markovic", "S. Do\u0161en", "D. Popovi\u0107", "B. Graimann", "D. Farina"], "venue": "Journal of Neural Engineering, vol. 12, no. 6, p. 066022, 2015. [Online]. Available: http://stacks.iop.org/1741-2552/12/ i=6/a=066022", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An exploratory study on the use of convolutional neural networks for object grasp classification", "author": ["G. Ghazaei", "A. Alameer", "P. Degenaar", "G. Morgan", "K. Nazarpour"], "venue": "2nd IET International Conference on Intelligent Signal Processing 2015 (ISP), Dec 2015, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal decoding and congruent sensory information enhance reaching performance in subjects with cervical spinal cord injury", "author": ["E.A. Corbett", "N.A. Sachs", "K.P. K\u00f6rding", "E.J. Perreault"], "venue": "Frontiers in Neuroscience, vol. 8, p. 123, 2014. [Online]. Available: http://journal.frontiersin.org/ article/10.3389/fnins.2014.00123", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Demonstration of a semi-autonomous hybrid brain-machine interface using human intracranial eeg, eye tracking, and computer vision to control a robotic upper limb prosthetic", "author": ["D.P. McMullen", "G. Hotson", "K.D. Katyal", "B.A. Wester", "M.S. Fifer", "T.G. Mcgee", "A. Harris", "M.S. Johannes", "R.J. Vogelstein", "A.D. Ravitz", "W.S. Anderson", "N.V. Thakor", "N.E. Crone"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 22, no. 4, pp. 784\u2013796, July 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Eye\u2013hand coordination in object manipulation", "author": ["R.S. Johansson", "G. Westling", "A. B\u00e4ckstr\u00f6m", "J.R. Flanagan"], "venue": "Journal of Neuroscience, vol. 21, no. 17, pp. 6917\u20136932, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Active visual segmentation", "author": ["A.K. Mishra", "Y. Aloimonos", "L.-F. Cheong", "A.A. Kassim"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 639\u2013 653, April 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014. [Online]. Available: http://arxiv.org/abs/ 1409.1556", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative cue integration for medical image annotation", "author": ["T. Tommasi", "F. Orabona", "B. Caputo"], "venue": "Pattern Recognition Letters, vol. 29, no. 15, pp. 1996\u20132002, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "The grasp taxonomy of human grasp types", "author": ["T. Feix", "J. Romero", "H.B. Schmiedmayer", "A.M. Dollar", "D. Kragic"], "venue": "IEEE Transactions on Human-Machine Systems, vol. 46, no. 1, pp. 66\u201377, Feb 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Movement error rate for evaluation of machine learning methods for semg-based hand movement classification", "author": ["A. Gijsberts", "M. Atzori", "C. Castellini", "H. M\u00fcller", "B. Caputo"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 22, no. 4, pp. 735\u2013744, 7 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularized least squares classification", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "Advances in Learning Theory: Methods, Model and Applications, vol. 190. VIOS Press, 2003, pp. 131\u2013154.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Although advanced myoelectric prostheses have the potential to restore some of the lost functionality, their acceptance among amputees is very low [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "An interesting avenue is to reduce the dependency on surface electromyography (sEMG) by including other sources of contextual information, such as inertial sensors [2] or", "startOffset": 164, "endOffset": 167}, {"referenceID": 2, "context": "computer vision [3, 4].", "startOffset": 16, "endOffset": 22}, {"referenceID": 3, "context": "computer vision [3, 4].", "startOffset": 16, "endOffset": 22}, {"referenceID": 4, "context": "ses, such as sonomyography, mechanomyography, and force myography (for detailed overviews, see [5, 6]).", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "ses, such as sonomyography, mechanomyography, and force myography (for detailed overviews, see [5, 6]).", "startOffset": 95, "endOffset": 101}, {"referenceID": 1, "context": "on arm orientation and dynamics that is complementary to sEMG [2, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 6, "context": "on arm orientation and dynamics that is complementary to sEMG [2, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 2, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 27, "endOffset": 33}, {"referenceID": 3, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 27, "endOffset": 33}, {"referenceID": 7, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[11], who used deep learning to classify grasps based on the object\u2019s appearance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] use the subject\u2019s gaze to help to determine the target position of a reaching movement, while McMullen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] combine this with computer vision to initiate and automatically perform the reach-grasp-drop motion of the robot arm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "A fixation consists of a period of time (generally between 350ms to 450ms [14]) where the eye-gaze remains in a limited area of the visual field.", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The object image is fed into the VGG-16 CNN pre-trained on ImageNet [16] and the activation of the second-last fullyconnected layer is taken as the image visual feature.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Among the possible methods to integrate multimodal cues, we opt for mid-level integration [17], also known as integration at the kernel level.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "We selected ten grasps based on relevant literature [18] and on their perceived importance for Activities of Daily Living (ADL).", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Also our classification setup was inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Also our classification setup was inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20].", "startOffset": 114, "endOffset": 118}, {"referenceID": 18, "context": "Based on reports in previous work [19], we opted to combine the marginal Discrete Wavelet Transform (mDWT) representation for sEMG in a sliding window of 200ms with the exp-\u03c7 kernel function", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "A linear kernel is typically sufficient for the representation at high levels of a CNN, but we prefer an RBF kernel to ensure that the outputs of both kernels in the combination are in the range [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 0, "context": "The addition of visual cues consistently reduces the prediction error during the grasp (t \u2208 [0, 1]).", "startOffset": 92, "endOffset": 98}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object\u2019s affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject\u2019s fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.", "creator": "LaTeX with hyperref package"}}}