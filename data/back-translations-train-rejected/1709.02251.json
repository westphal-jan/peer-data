{"id": "1709.02251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction", "abstract": "Continuous dimensional emotion prediction is a challenging task where the fusion of various modalities usually achieves state-of-the-art performance such as early fusion or late fusion. In this paper, we propose a novel multi-modal fusion strategy named conditional attention fusion, which can dynamically pay attention to different modalities at each time step. Long-short term memory recurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model to capture long time dependencies. The weights assigned to different modalities are automatically decided by the current input features and recent history information rather than being fixed at any kinds of situation. Our experimental results on a benchmark dataset AVEC2015 show the effectiveness of our method which outperforms several common fusion strategies for valence prediction.", "histories": [["v1", "Mon, 4 Sep 2017 12:05:47 GMT  (6020kb,D)", "http://arxiv.org/abs/1709.02251v1", "Appeared at ACM Multimedia 2016"]], "COMMENTS": "Appeared at ACM Multimedia 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["shizhe chen", "qin jin"], "accepted": false, "id": "1709.02251"}, "pdf": {"name": "1709.02251.pdf", "metadata": {"source": "CRF", "title": "Multi-modal Conditional A\u0082ention Fusion for Dimensional Emotion Prediction", "authors": ["Shizhe Chen", "Qin Jin"], "emails": ["cszhe1@ruc.edu.cn", "qjin@ruc.edu.cn"], "sections": [{"heading": null, "text": "Keywords Continuous Dimensional Emotion Prediction; Multimodal Fusion; LSTM-RNN"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that the greater part of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to"}, {"heading": "2 MULTI-MODAL FEATURES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Audio Features", "text": "We use the OpenSMILE toolkit [18] to extract low-level features such as MFCCs, loudness, F0, ji er and shimmer. All features are extracted using 40ms frame window size without overlapping with the Ground Truth labels, as [19] shows that short-term features can reveal more detail, increasing performance for effective prediction using LSTMs. Low-level acoustic features are in 76 dimensions.ar Xiv: 170 9.02 251v 1 [cs.C V] 4S ep2 017"}, {"heading": "2.2 Visual Features", "text": "Two sets of visual characteristics are extracted from the facial expression: external characteristics and geometric characteristics [5]. External characteristics are calculated using local gabor binary points from ree Orthogonal Planes (LGBP-TOP) and compressed to 84 dimensions by PCA. Geometric characteristics in 316 dimensions are calculated from 49 face markers. Boxes in which no face is recognized are marked with zeros. We link external characteristics and geometric characteristics as visual characteristics."}, {"heading": "3 EMOTION PREDICTION MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Uni-Modality Prediction Model", "text": "Long-term memory (LSTM) architecture [20] is the most advanced model for sequence analysis and can exploit extensive dependencies in the data. In this paper we use the peephole LSTM version [21] proposed by Graves. The function of hidden cells and gates is broken down as follows: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict \u2212 1 + bi) ft = \u03c3 (Wxf xt + Whf ht \u2212 1 + Wcf ct \u2212 1 + bf) ct = ft \u00b7 ct \u2212 1 + it \u00b7 tanh (Wxcxt + Whcht \u2212 1 + bc) (1) ot = \u03c3 (Wxoxt + Whoht \u2212 1 + Wcoct \u2212 1 + bo) ht = ot \u00b7 tanh (ct), where i, f, o and c refer to the input gate, forget gate, output gate and cell state."}, {"heading": "3.2 Conditional Attention Fusion Model", "text": "For two reasons, let xat and xvt refer to the audio-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t"}, {"heading": "3.3 Model Learning", "text": "Intuitively, the acoustic characteristics are more reliable when the acoustic energy is higher, since the headset microphone can record the speech of both the speaker and other speakers in conversations. Higher energy can refer to a higher probability that the speech comes from the target. Likewise, the facial features are only reliable if faces are correctly recognized. Thus, adding such page information could be helpful in learning the a-ention weight.We transform the acoustic energy into a scale [0, 1], and we use \u00a4at to designate its value in the t-th time.For visual characteristics, we use \u00a4vt {0, 1} to indicate whether the subject's face is recognized, since the face recognition provided in the dataset has no recognition function. We therefore interpret the internal loss function for a sequence as follows: L-t = 1 2 (H-a-t) 2 (H-a) + 2 (H-a) t-\u03b2 (T-\u03b2 + 2), where (T-\u03b2) are usually set high."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "The AVEC2015 Dimensional Emotion dataset is a subset of the RECOLA dataset [22], a multimodal corpus of remote and collaborative interactions. There are 27 subjects in the dataset, divided equally into training, development and test sets. Audio, video and physiological data are collected for each participant for the first 5 minutes of interaction. Excitement and value are recorded at scale [-1, 1] for all 40ms [5]. Since transmission times on the test set are limited, we cross-validate the development set. We randomly select 5 subjects as a development set to optimize hyperparameters, and the remaining 4 speakers are used as a test set. We do the experiments 8 time."}, {"heading": "4.2 Experimental Setup", "text": "Annotation delay compensation [13] is applied because there is a delay between signal content and basic truth marking due to the perceptual processing of the annotators. We drop the first N-basic truth markers and the last N-input markers. N is optimized by the non-temporal regression model SVR on the training set. e missing predictions in the first N-frames are evaluated with zeros. Table 1: CCC performance of uni-modal characteristics of visual characteristics Visual characteristics arousal 0.787 0.432 Value 0.595 0.620Table 2: CCC performance of nuclear fusion functions to avoid loss of value is evaluated with zeros.Table 1: CCC performance of uni-modal characteristics Visual characteristics arousal 0.787 0.432 Value 0.595 0.620Table 2: CCC performance of nuclear fusion functions to avoid loss of value is evaluated with zeros.Finally, a predictive loop is applied to predict the signal content."}, {"heading": "4.3 Experimental Results", "text": "Table 1 shows the predictive power with uni-modal characteristics. Acoustic characteristics achieve the best performance based on excitation prediction and the visual characteristics are slightly better than acoustic characteristics based on valence prediction. The performance of erent fusion methods based on excitation prediction is shown in Figure 4 (a). Early fusion achieves the best average performance and our proposed fusion method performs the second best visual performance of all fusion strategies. However, there is no significant difference between early fusion prediction and acoustic uni modality when comparing Figure 4 (a) with Table 1 (Student t-test with p-value = 0.07). We know that there is a strong correlation between excitation and acoustic energy as shown in Figure 3, where we flatten the acoustic energy with Window 100 and Shi and scale it according to the mean and standard deviation between energy and excitation characteristics."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we propose a multimodal fusion strategy called Conditional a Ention Fusion for continuous dimensional emotion prediction based on LSTM-RNN. It can dynamically take into account the different modalities according to current modality characteristics and historical information, which increases the stability of the model. Experiments with the benchmark data set AVEC 2015 show that our proposed fusion approach significantly exceeds the other common fusion approaches such as early fusion, model-based fusion and late fusion for valence prediction. In the future, we will use more features from the existing modalities and apply strategies to express the correlation and independence of the different modality characteristics."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "is supported by the National Key Research and Development Plan under grant number 2016YFB1001202."}], "references": [{"title": "A\u0082ective computing: challenges", "author": ["Rosalind W. Picard"], "venue": "International Journal of Human-Computer Studies,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Computationally modeling human emotion", "author": ["Stacy Marsella", "Jonathan Gratch"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Lstm-modeling of continuous emotions in an audiovisual a\u0082ect recognition framework", "author": ["MartinW\u00f6llmer", "Moritz Kaiser", "Florian Eyben", "Bj\u00f6rn Schuller", "Gerhard Rigoll"], "venue": "Image and Vision Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Avec 2014: 3d dimensional a\u0082ect and depression recognition challenge", "author": ["Michel Valstar", "Bj\u00f6rn Schuller", "Kirsty Smith", "Timur Almaev", "Florian Eyben", "Jarek Krajewski", "Roddy Cowie", "Maja Pantic"], "venue": "In Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Av+ec 2015: \u008ce \u0080rst a\u0082ect recognition challenge bridging across audio, video, and physiological data", "author": ["Fabien Ringeval", "Bj\u00f6rn Schuller", "Michel Valstar", "Shashank Jaiswal", "Erik Marchi", "Denis Lalanne", "Roddy Cowie", "Maja Pantic"], "venue": "In International Workshop on Audio/visual Emotion Challenge,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Survey on speech emotion recognition: Features, classi\u0080cation schemes, and databases", "author": ["Moataz El Ayadi", "Mohamed S Kamel", "Fakhri Karray"], "venue": "Pa\u0088ern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Po\u008as"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Automatic facial expression analysis: a survey", "author": ["Beat Fasel", "Juergen Lue\u008ain"], "venue": "Pa\u0088ern recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Real-time automatic emotion recognition from body gestures", "author": ["Stefano Piana", "Alessandra Stagliano", "Francesca Odone", "Alessandro Verri", "Antonio Camurri"], "venue": "arXiv preprint arXiv:1402.5047,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Emotional state classi\u0080cation from EEG data using machine learning approach", "author": ["Xiao-Wei Wang", "Dan Nie", "Bao-Liang Lu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies", "author": ["Chung-HsienWu", "Jen-Chun Lin", "andWen-LiWei"], "venue": "APSIPA Transactions on Signal and Information Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Emotion recognition using acoustic and lexical features", "author": ["Viktor Rozgic", "Sankaranarayanan Ananthakrishnan", "Shirin Saleem", "Rohit Kumar", "Aravind Namandi Vembu", "Rohit Prasad"], "venue": "In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An investigation of annotation delay compensation and output-associative fusion for multimodal continuous emotion prediction", "author": ["Zhaocheng Huang", "Ting Dang", "Nicholas Cummins", "Brian Stasak", "Phu Le", "Vidhyasaharan Sethu", "Julien Epps"], "venue": "In \u008ae International Workshop on Audio/visual Emotion Challenge,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Multimodal a\u0082ective dimension prediction using deep bidirectional long shortterm memory recurrent neural networks", "author": ["Lang He", "Dongmei Jiang", "Le Yang", "Ercheng Pei", "Peng Wu", "Hichem Sahli"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Exploring inter-feature and inter-class relationships with deep neural networks for video classi\u0080cation", "author": ["Zuxuan Wu", "Yu-Gang Jiang", "Jun Wang", "Jian Pu", "Xiangyang Xue"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM \u201914,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Emotion recognition in the wild with feature fusion and multiple kernel learning", "author": ["JunKai Chen", "Zenghai Chen", "Zheru Chi", "Hong Fu"], "venue": "In Proceedings of the 16th International Conference on Multimodal Interaction,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Audio-visual emotion recognition with boosted coupled HMM", "author": ["Kun Lu", "Yunde Jia"], "venue": "In Proceedings of the 21st International Conference on Pa\u0088ern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Opensmile: the munich versatile and fast open-source audio feature extractor", "author": ["Florian Eyben", "Martin llmer", "Bj\u00f6rn Schuller"], "venue": "Acm Mm,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Multi-modal dimensional emotion recognition using recurrent neural networks", "author": ["Shizhe Chen", "Qin Jin"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "Eprint Arxiv,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Introducing the RECOLAmultimodal corpus of remote collaborative and a\u0082ective  interactions", "author": ["Fabien Ringeval", "Andreas Sonderegger", "J\u00fcrgen S. Sauer", "Denis Lalanne"], "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Long short term memory recurrent neural network based multimodal dimensional emotion recognition", "author": ["Linlin Chao", "Jianhua Tao", "Minghao Yang", "Ya Li", "Zhengqi Wen"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Understanding human emotions is a key component to improve human-computer interactions [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Dimensional emotion is one of the most popular computing models for emotion recognition [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 4, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 5, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 202, "endOffset": 205}, {"referenceID": 9, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "Fusion strategies for di\u0082erent modalities in previous works can be divided into 3 categories, namely feature-level (early) fusion, decision-level (late) fusion and model-level fusion [11].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "successfully improve performance [12].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "ities and trains a second level model such as RVM [13], BLSTM [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "ities and trains a second level model such as RVM [13], BLSTM [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "For example, for neural networks, modellevel fusion could be concatenation of di\u0082erent hidden layers from di\u0082erent modalities [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "For kernel classi\u0080ers, model-level fusion could be kernel fusion [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "As for Hidden Markov Model (HMM) classi\u0080ers, novel forms of feature interactions have been proposed [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "We use the AVEC2015 dimensional emotion dataset [5] to evaluate our methods.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "We utilize the OpenSMILE toolkit [18] to extract low-level features including MFCCs, loudness, F0, ji\u008aer and shimmer.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "All the features are extracted using 40ms frame window size without overlap to match with the groundtruth labels since it is demonstrated in [19] that short-time features can reveal more details and thus boost performance for a\u0082ective prediction using LSTMs.", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "Two sets of visual features are extracted from facial expression: appearance-based features and geometric-based features [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 19, "context": "Long short term memory (LSTM) architecture [20] is the state-ofthe-art model for sequence analysis and can exploit long range dependencies in the data.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "In this paper, we use the peephole LSTM version proposed by Graves [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "We transform the acoustic energy into scale [0, 1], and we use \u0434a t to denote its value at the t th timestep.", "startOffset": 44, "endOffset": 50}, {"referenceID": 21, "context": "\u008ce AVEC2015 dimensional emotion dataset is a subset of the RECOLA dataset [22], a multimodal corpus of remote and collaborative a\u0082ective interactions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "Arousal and valence are annotated in scale [-1, 1] for every 40ms [5].", "startOffset": 43, "endOffset": 50}, {"referenceID": 4, "context": "Arousal and valence are annotated in scale [-1, 1] for every 40ms [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "\u008ce concordance correlation coe\u0081cient (CCC) [5] works as the evaluation metric.", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "Annotation delay compensation [13] is applied because there exists a delay between signal content and groundtruth labels due to annotators\u2019 perceptual processing.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "\u008ce input features are normalized into the range [-1,1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 22, "context": "\u008ce size of mini-batch is 256 and truncated backpropagation through time (BPTT) [23] is applied.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "[19] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] use the same feature set as ours and Chao et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] use more features including CNNs for valence prediction.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Continuous dimensional emotion prediction is a challenging task where the fusion of various modalities usually achieves state-of-theart performance such as early fusion or late fusion. In this paper, we propose a novel multi-modal fusion strategy named conditional a\u008aention fusion, which can dynamically pay a\u008aention to di\u0082erent modalities at each time step. Long-short term memory recurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model to capture long time dependencies. \u008ce weights assigned to di\u0082erent modalities are automatically decided by the current input features and recent history information rather than being \u0080xed at any kinds of situation. Our experimental results on a benchmark dataset AVEC2015 show the e\u0082ectiveness of our method which outperforms several common fusion strategies for valence prediction.", "creator": "LaTeX with hyperref package"}}}