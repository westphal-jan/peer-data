{"id": "1709.02260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Embedded Binarized Neural Networks", "abstract": "We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing current binarized neural networks (BNNs) in the literature to perform feedforward inference efficiently on small embedded devices. We focus on minimizing the required memory footprint, given that these devices often have memory as small as tens of kilobytes (KB). Beyond minimizing the memory required to store weights, as in a BNN, we show that it is essential to minimize the memory used for temporaries which hold intermediate results between layers in feedforward inference. To accomplish this, eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network. All intermediate results from a layer are stored as binary values, as opposed to floating-points used in current BNN implementations, leading to a 32x reduction in required temporary space. We provide empirical evidence that our proposed eBNN approach allows efficient inference (10s of ms) on devices with severely limited memory (10s of KB). For example, eBNN achieves 95\\% accuracy on the MNIST dataset running on an Intel Curie with only 15 KB of usable memory with an inference runtime of under 50 ms per sample. To ease the development of applications in embedded contexts, we make our source code available that allows users to train and discover eBNN models for a learning task at hand, which fit within the memory constraint of the target device.", "histories": [["v1", "Wed, 6 Sep 2017 06:45:33 GMT  (1279kb,D)", "http://arxiv.org/abs/1709.02260v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["bradley mcdanel", "surat teerapittayanon", "h t kung"], "accepted": false, "id": "1709.02260"}, "pdf": {"name": "1709.02260.pdf", "metadata": {"source": "CRF", "title": "Embedded Binarized Neural Networks", "authors": ["Bradley McDanel", "Surat Teerapittayanon"], "emails": ["mcdanel@fas.harvard.edu", "steerapi@seas.harvard.edu", "kung@harvard.edu"], "sections": [{"heading": null, "text": "With the aim of enabling current binary neural networks (BNs) in the literature to efficiently draw conclusions about small embedded devices, we focus on minimizing the memory footprint required, as these devices often have memory of up to ten kilobytes (KB). To achieve this, eBNN reorders the calculation of inferences while maintaining the original BNN structure, using only a single temporary flow point for the entire neural network. All intermediate results from one layer are stored as binary values, as opposed to floating points used in current BNN implementations, resulting in a 32-fold reduction in the required temporary memory space."}, {"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Related Work and Background", "text": "In this section, we give a brief introduction to the components (so-called layers) used in deep neural networks (DNNs) and show how these layers are extended to binarized neural networks (BNNs) used in our approach."}, {"heading": "2.1 Deep Neural Network Layers", "text": "A DNN is a machine learning method that consists of several layers. DNNs have achieved enormous success in recent years with many different input modalities, because they can learn better feature representations, which leads to higher accuracy, because more layers are added to the network. We briefly describe the important layers and functions of the DNN feedback, which we use in this essay. \u2022 A fully networked layer has full connections to all neurons in the previous layer and can be considered as a matrix multiplication between the input and weights of the functional layers. For classification tasks, these layers are typically used at the end of a NN to map the output to a vector of length n, where n is the number of classes. \u2022 A revolutionary layer consists of a series of filters, which are typically learned during modeling via supervised backpropagation, which can identify discriminative patterns that can be found in the input sample of each filtering layer with several different layers of the target layers."}, {"heading": "2.2 Binarized Neural Networks", "text": "In 2015, Courbariaux et al. proposed BinaryConnect, a method for forming DNNs where all propagations (both forward and backward) use binary weights [3]. In 2016, they expanded this work with BinaryNet and formally introduced Binarized Neural Networks (BNNs) [2]. The second paper provides details on implementing how to efficiently perform binary matrix multiplication, instead of in fully connected and revolutionary layers, through the use of bit operations (xnor and popcount). In BNNNs, all weights of filters in one layer must be \u2212 1 or 1 (which is stored as 0 and 1) instead of a 32-bit floating point value. This representation results in much more space-efficient models compared to standard floating point DNNUs. A key to the success of BNNNUs is the binary activation feature, which allows all negative inputs \u2212 1 and positive inputs \u2212 1."}, {"heading": "3 Embedded Binarized Neural Networks", "text": "In this section we present embedded Binarized Neural Networks (eBNs), which reorganize the order of calculation into standard BNs by combining several layers into fused blocks. Each fused block uses binary rather than floating-point periods to keep the intermediate results between the layers. Binarization of time points is important because, as we show in Section 4, floating-point periods consume a large portion of the total memory of embedded devices, which in some cases makes it impossible to run BNNs on embedded devices at all. For example, the Intel Curie used in our evaluation has 15 KB of usable SRAM memory. The smallest possible 1-layer Convolutionary BNN (a single 3x3 Convolutionary Filter with 3 channels) would require only 22 B to keep the neural network parameters (4 flowpoint combination parameters from 1-layer Constitutional BNN to 2-KB, and 1-KB-to-28-to-to-to-KB-to-to-to-save-to-to-KB)."}, {"heading": "3.1 Fused Binary Blocks", "text": "In this paper, we envision the calculation of several NN layers within a block to create the necessary space for intermediate results. In this paper, we show three mixed binary blocks operating on a 28 x 28 cm input sample: a misused binary block, a misused binary block, and a missing binary block."}, {"heading": "3.2 Memory Cost for Feedforward Inference", "text": "In fact, it is a way in which people are able to put themselves in the world in which they live."}, {"heading": "4 Evaluation", "text": "In this context, it should be noted that this is not an isolated case, but a case which is an isolated case, in which case it is a case which is an isolated case, in which case it is a case which is a case which is a case which is a case which is a case which is a case which is a priori a case which is a case which is a case which is a case which is a case which is a case which is a case which is a case which is a case which is a case which is a priori a case."}, {"heading": "5 Implementation", "text": "Our implementation of eBNN feedforward inferences is written in C and has a corresponding Python version inChainer [11], which is used to train the BNN models. Each fused binary block has a chainer association with a modification that allows it to output the model parameters for that layer into a generated C file. Once the network is trained in Python, it is automatically converted into a standalone C header file with corresponding inference code. We validated the accuracy of the C implementation by comparing the output at each stage with the Python version. Our codebase is open source and is available here: https: / / gitlab.com / htkung / ddnn.In addition to the software, we have implemented a service model that allows users to train and discover eBNNs that offer the best prediction accuracy while inserting within a device-specific memory constraint."}, {"heading": "6 Conclusion", "text": "BNNs have huge potential for embedded devices, due to the 32x reduction in model weights that can allow deep networks to fit within the device. However, the temporary requirements to maintain intermediate results between layers represent a significant share of the memory needed to draw conclusions. Temporary operations are particularly significant for Convolutionary Networks; for BNN, the temporary overhead is greater than the memory size (15KB) of our experimental device even for a single filter single-layer Convolutionary Network. Our proposed eBNN achieves a 32x space reduction for temporary Network. the eBNN scheme is based on a recalculation of the calculation (including recalculation for overlaps if needed) without changing the structure of the original BNNs, thereby maintaining the same accuracy as the original network. the optimizations used by eBNN to minimize the overhead from our temporary devices are intended to mitigate the basic DNN usage as the other NNNNNNs are for automated DN10KB usage."}, {"heading": "Acknowledgment", "text": "This work is supported in part by donations from Intel Corporation and in part by the Naval Supply Systems Command Award under the Naval Postgraduate School Agreements N00244-15-0050 and N00244-16-1-0018."}, {"heading": "7 References", "text": "[1] R. Collobert, S. Bengio, and J. Marie \u0301 thoz. Torch: a modlar machinelearning software library. Technical report, Idiap, 2002. [2] M. Courbariaux and Y. Bengio. Binarynet: Training deep neural net-works with weights and activations confined to + 1 or-1. arXiv preprint arXiv: 1602.02830, 2016. [3] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weight during propagations. In Advances in Neural Information Processing Systems, pages 3123-3131, 2015. [4] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. arXiv preprint arXiv: 1603.05027, 2016. [5] S. Ioffe and C. Szegedy. Batch normalization: Access in deep residual networks."}], "references": [{"title": "Torch: a modular machine learning software library", "author": ["R. Collobert", "S. Bengio", "J. Mari\u00e9thoz"], "venue": "Technical report, Idiap,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. M\u00fcller", "S. Behnke"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A. Belikov"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "To this end, we leverage recent results on Binarized Neural Networks (BNNs), where 1-bit weights are used instead of 32-bit floating-point weights [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "\u2022 Batch Normalization is a normalization technique used to address the covariate shift problem found in DNNs as demonstrated in [5], and is used to improve the training time and the accuracy of the network.", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "proposed BinaryConnect, a method of training DNNs where all propagations (both forward and backward step) use binary weights [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "In 2016, they expanded on this work with BinaryNet and formally introduced Binarized Neural Networks (BNNs) [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "XNorNet provides a different network structure for BNNs where pooling occurs before binary activation [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "While the 32x memory reduction from floats to bits of the weights makes BNNs an obvious candidate for low-power embedded systems, current BNN implementations are for large GPUs written in one of several popular GPU frameworks (Theano, Torch) [10, 1].", "startOffset": 242, "endOffset": 249}, {"referenceID": 0, "context": "While the 32x memory reduction from floats to bits of the weights makes BNNs an obvious candidate for low-power embedded systems, current BNN implementations are for large GPUs written in one of several popular GPU frameworks (Theano, Torch) [10, 1].", "startOffset": 242, "endOffset": 249}, {"referenceID": 3, "context": "1 Fused Binary Blocks We adopt the concept of NN blocks used in several deep network architectures [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Specifically, several different NN structures of varying depths and two standard community datasets (MNIST, CIFAR10) [7, 6] are used to evaluated the proposed eBNN.", "startOffset": 117, "endOffset": 123}, {"referenceID": 5, "context": "Specifically, several different NN structures of varying depths and two standard community datasets (MNIST, CIFAR10) [7, 6] are used to evaluated the proposed eBNN.", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "For larger networks, with additional network layers, pooling has been shown to be an important step [9], and even in a two layer network, we see pooling improves performance.", "startOffset": 100, "endOffset": 103}, {"referenceID": 10, "context": "Chainer [11] that is utilized to train the BNN models.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing current binarized neural networks (BNNs) in the literature to perform feedforward inference efficiently on small embedded devices. We focus on minimizing the required memory footprint, given that these devices often have memory as small as tens of kilobytes (KB). Beyond minimizing the memory required to store weights, as in a BNN, we show that it is essential to minimize the memory used for temporaries which hold intermediate results between layers in feedforward inference. To accomplish this, eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network. All intermediate results from a layer are stored as binary values, as opposed to floating-points used in current BNN implementations, leading to a 32x reduction in required temporary space. We provide empirical evidence that our proposed eBNN approach allows efficient inference (10s of ms) on devices with severely limited memory (10s of KB). For example, eBNN achieves 95% accuracy on the MNIST dataset running on an Intel Curie with only 15 KB of usable memory with an inference runtime of under 50 ms per sample. To ease the development of applications in embedded contexts, we make our source code available that allows users to train and discover eBNN models for a learning task at hand, which fit within the memory constraint of the target device.", "creator": "LaTeX with hyperref package"}}}