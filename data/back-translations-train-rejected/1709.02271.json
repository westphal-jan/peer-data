{"id": "1709.02271", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Leveraging Discourse Information Effectively for Authorship Attribution", "abstract": "We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.", "histories": [["v1", "Thu, 7 Sep 2017 14:22:50 GMT  (787kb,D)", "http://arxiv.org/abs/1709.02271v1", "Accepted at IJCNLP 2017 as a conference paper"]], "COMMENTS": "Accepted at IJCNLP 2017 as a conference paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["su wang", "elisa ferracane", "raymond j mooney"], "accepted": false, "id": "1709.02271"}, "pdf": {"name": "1709.02271.pdf", "metadata": {"source": "CRF", "title": "Leveraging Discourse Information Effectively for Authorship Attribution\u2217", "authors": ["Su Wang", "Elisa Ferracane", "Raymond J. Mooney"], "emails": ["shrekwang@utexas.edu,", "elisa@ferracane.com", "mooney@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "This task is to enable people to change themselves and themselves, both in terms of the way they live, in terms of the way they live, and in terms of the way they live, and in terms of the way they live."}, {"heading": "2 Background", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "3 Models", "text": "Building on Shrestha et al. (2017)'s work, we employ their character bigram layer i (CNN2) 4, and3Shrestha et al. (2017) test two variants of CNN models: CNN1 / CNN2 for unigram / bigram character CNNs according to their nature. < Our preliminary experiments showed that the use of character n-gram orders is performed higher than 2, probably due to the increased number of features and overfitting. The CNN2-PV allows us to perform a comparison with F & H14 and F15, which also include relationship probabilities vectors.CNN2 is the base model with no discourse features."}, {"heading": "4 Experiments and Results", "text": "We start with the introduction of the data sets (Section 4.1), followed by the description of the featurisation methods (Section 4.2), the experiments (Section 4.3) and finally the reporting of the results (Section 4.4)."}, {"heading": "4.1 Datasets", "text": "The statistics for the three data sets used in the experiments are summarised in Table 4.novel-9. This data set was summarised by F & H14: a collection of 19 novels by British and American authors of the 19th century in the Gutenberg Project. Compared to F & H14, we use the same resampling method (F & H14, Section 4.2) to correct the imbalance in authors by skipping the texts of less representative authors. Novel-50. This data set covers novel-9 and includes the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total of 250 novels.IMDB62. IMDB62 consists of 62K film reviews by 62 users (1,000 each) from the Internet Movie data set, compiled by Seroussi et al. (2011). Unlike the new data sets, the reviews are considerably shorter, with an average of 349 words per text."}, {"heading": "4.2 Featurization", "text": "As described in section 2, in both the GR and RST variants, we first capture an entity grid from each input set. CNN2-PV. We capture the probabilities of entity role transitions (in GR) or discourse relationships (in RST) for the entries. Each entry corresponds to a probability distribution vector. CNN2-EN. We use two schemes to generate discourse function sequences from an entity grid. While we always read the grid column by column (by a prominent entity), we vary whether we track the entity over a number of sentences (n lines at a time) or over the entire document (an entire column to atime), which are designated as local and global reading. For the GR discourse functions, we process the entity over an entity at a time (Figure 3, left)."}, {"heading": "4.3 Experiments", "text": "All baselinedataset experiments are evaluated in a novel way. As a comparison to previous work (F15), we evaluate our models with a paired classification task with GR discourse characteristics. In their model, novels are simultaneously divided into 1000-word chunks, and the model is evaluated with accuracy. In this multi-level model, we will then further evaluate the more difficult multi-class task, i.e., all class predictions are performed simultaneously with GR and RST discourse characteristics and the more robust F1 evaluation."}, {"heading": "4.4 Results", "text": "The results of the baseline dataset experiments are shown in Table 5, 6 and Figure 4. In Table 5, Baseline refers to the stupid baseline model that always predicts the more strongly represented author of the pair. Both SVMs are of F15, and we report on their results. SVM (LexSyn) takes character and word bi / trigrams and POS tags. SVM (LexSyn-PV) additionally contains probability vectors similar to our CNN2PV. In this part of the experiment, while the CNNs have a large margin over SVM, discourses in CNN2-PV are added, which bring only low performance. Table 6 reports on the results of the multiclass classification task, the more difficult task. Here, probability vection functions (i.e., PV) are missing to contribute much."}, {"heading": "5 Analysis", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "6 Conclusion", "text": "We have conducted an in-depth study of techniques that (i) characterize discourse information and (ii) effectively integrate discourse features into the state-of-the-art CNN character bigram classifier for AA. Beyond confirming the general superiority of RST characteristics over GR characteristics in larger and more difficult datasets, we present discourse embedding technology that is not available for previously proposed discourse-enhanced models. The new technique enabled us to significantly shift the shell of the current performance cap. Admittedly, we are losing the valuable RST tree structure by using RST characteristics with entity grids. In future work, we intend to introduce more complex methods such as RecNN (as per Ji and Smith (2017) to retain more information from the RST trees while reducing the parameter size. Furthermore, we want to understand how discourse embedding contributes to AA tasks and to find shorter grains for reference texts."}], "references": [{"title": "Modeling local coherence: an entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Empirical Methods on Natural Language Processing.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "RST-style discourse parsing and its applications in discourse analysis", "author": ["Vanessa Wei Feng."], "venue": "Ph.D. thesis, University of Toronto.", "citeRegEx": "Feng.,? 2015", "shortCiteRegEx": "Feng.", "year": 2015}, {"title": "Patterns of local discourse coherence as a feature for authorship attribution", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Literary and Linguistic Computing 29(2):191\u2013198.", "citeRegEx": "Feng and Hirst.,? 2014", "shortCiteRegEx": "Feng and Hirst.", "year": 2014}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi."], "venue": "Computational linguistics 21(2):203\u2013225.", "citeRegEx": "Grosz et al\\.,? 1995", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "ACL (1). pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Neural discourse structure for text categorization", "author": ["Yangfeng Ji", "Noah Smith."], "venue": "ACL 2017 to appear.", "citeRegEx": "Ji and Smith.,? 2017", "shortCiteRegEx": "Ji and Smith.", "year": 2017}, {"title": "A convolutional neural network for modeling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "ACL 2014 1:655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Adam: a method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "CoRR: arXiv1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William Mann", "Sandra Thompson."], "venue": "Text 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "A neural local coherence model", "author": ["Dat Tien Nguyen", "Shafiq Joty."], "venue": "ACL 2017, to appear.", "citeRegEx": "Nguyen and Joty.,? 2017", "shortCiteRegEx": "Nguyen and Joty.", "year": 2017}, {"title": "Character-level and multi-channel convolutional neural networks for large-scale authorship attribution", "author": ["Sebastian Ruder", "Parsa Ghaffari", "John Breslin."], "venue": "CoRR, arXiv:1609.06686 .", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "Continuous n-gram representations for authorship attribution", "author": ["Yunita Sari", "Andreas Vlachos", "Mark Stevenson."], "venue": "Proceedings of EACL.", "citeRegEx": "Sari et al\\.,? 2017", "shortCiteRegEx": "Sari et al\\.", "year": 2017}, {"title": "Authorship attribution of micromessages", "author": ["Roy Schwartz", "Oren Tsur", "Ari Rappoport", "Moshe Koppel."], "venue": "EMNLP 2013 pages 1880\u20131891.", "citeRegEx": "Schwartz et al\\.,? 2013", "shortCiteRegEx": "Schwartz et al\\.", "year": 2013}, {"title": "Authorship attribution with Latent Dirichlet Allocation", "author": ["Yanir Seroussi", "Ingrid Zukerman", "Fabian Bohnert."], "venue": "Proceedings of CNLL.", "citeRegEx": "Seroussi et al\\.,? 2011", "shortCiteRegEx": "Seroussi et al\\.", "year": 2011}, {"title": "Convolutional neural networks for authorship attribution of short texts", "author": ["Prasha Shrestha", "Sebastian Sierra", "Fabio A Gonz\u00e1lez", "Paolo Rosso", "Manuel Montes-y G\u00f3mez", "Thamar Solorio."], "venue": "EACL 2017 page 669.", "citeRegEx": "Shrestha et al\\.,? 2017", "shortCiteRegEx": "Shrestha et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A survey of modern authorship attribution methods", "author": ["Efstathios Stamatatos."], "venue": "Journal of the Americal Society for Information Science and Technology pages 155\u2013159.", "citeRegEx": "Stamatatos.,? 2009", "shortCiteRegEx": "Stamatatos.", "year": 2009}, {"title": "Overview of the Author Identification Task at PAN 2015", "author": ["Efstathios Stamatatos", "Walter Daelemans", "Ben Verhoeven", "Patrick Juola", "Aurelio L\u00f3pez-L\u00f3pez", "Martin Potthast", "Benno Stein."], "venue": "Linda Cappellato and Nicola Ferro and Gareth Jones and Eric", "citeRegEx": "Stamatatos et al\\.,? 2015", "shortCiteRegEx": "Stamatatos et al\\.", "year": 2015}, {"title": "Functional centering: Grounding referential coherence in information structure", "author": ["Michael Strube", "Udo Hahn."], "venue": "Computational linguistics 25(3):309\u2013 344.", "citeRegEx": "Strube and Hahn.,? 1999", "shortCiteRegEx": "Strube and Hahn.", "year": 1999}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher Manning."], "venue": "ACL 2015 1:1556\u20131566.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton."], "venue": "Journal of Machine Learning Research 9.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "This task typically makes use of stylometric cues at the surface lexical and syntactic level (Stamatatos et al., 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.", "startOffset": 93, "endOffset": 118}, {"referenceID": 12, "context": "More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017).", "startOffset": 137, "endOffset": 180}, {"referenceID": 16, "context": "More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017).", "startOffset": 137, "endOffset": 180}, {"referenceID": 12, "context": "The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017).", "startOffset": 182, "endOffset": 221}, {"referenceID": 13, "context": "The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017).", "startOffset": 182, "endOffset": 221}, {"referenceID": 3, "context": ", 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": ", 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.", "startOffset": 18, "endOffset": 56}, {"referenceID": 0, "context": "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences.", "startOffset": 60, "endOffset": 87}, {"referenceID": 0, "context": "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988, RST).", "startOffset": 60, "endOffset": 512}, {"referenceID": 0, "context": "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988, RST). Their study uses a linear-kernel SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entitygrid with grammatical relations enhances the nondiscourse model by almost 1% in accuracy, and using RST relations provides an improvement of 3%. The study, however, works with only one small dataset and their models produce overall unremarkable performance (\u223c85%). Ji and Smith (2017) propose an advanced Recursive Neural Network (RecNN) architecture to work with RST in the more general area of text categorization and present impressive results.", "startOffset": 60, "endOffset": 1159}, {"referenceID": 0, "context": "Their work is based on the entity-grid model of Barzilay and Lapata (2008) (henceforth B&L).", "startOffset": 48, "endOffset": 75}, {"referenceID": 5, "context": "Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999).", "startOffset": 189, "endOffset": 232}, {"referenceID": 20, "context": "Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999).", "startOffset": 189, "endOffset": 232}, {"referenceID": 14, "context": "They report stateof-the-art performance on a corpus of Twitter data (Schwartz et al., 2013), and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other stylometric features (Schwartz et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 14, "context": ", 2013), and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other stylometric features (Schwartz et al., 2013); (ii) LSTM-2: an LSTM trained on bigrams (Tai et al.", "startOffset": 176, "endOffset": 199}, {"referenceID": 21, "context": ", 2013); (ii) LSTM-2: an LSTM trained on bigrams (Tai et al., 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 18, "context": ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 8, "context": ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014).", "startOffset": 143, "endOffset": 170}, {"referenceID": 12, "context": "Shrestha et al. (2017) propose a convolutional neural network formulation for AA tasks (detailed in Section 3).", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014). The authors show that the model CNN23 produces the best performance overall. Ruder et al. (2016) apply character n-gram CNNs to a wide range of datasets, providing strong empirical evidence that the architecture generalizes well.", "startOffset": 144, "endOffset": 269}, {"referenceID": 8, "context": ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014). The authors show that the model CNN23 produces the best performance overall. Ruder et al. (2016) apply character n-gram CNNs to a wide range of datasets, providing strong empirical evidence that the architecture generalizes well. Further, they find that including word n-grams in addition to character n-grams reduces performance, which is in agreement with Sari et al. (2017)\u2019s findings.", "startOffset": 144, "endOffset": 549}, {"referenceID": 16, "context": "Building on Shrestha et al. (2017)\u2019s work, we employ their character-bigram CNN (CNN2)4, and", "startOffset": 12, "endOffset": 35}, {"referenceID": 2, "context": "We then apply the max-over-time pooling (Collobert et al., 2011) to the feature maps from each filter, and concatenate the resulting vectors to obtain a single vector y, which then goes through the softmax layer to produce predictions.", "startOffset": 40, "endOffset": 64}, {"referenceID": 1, "context": "Stanford Core NLP (Clark and Manning, 2016).", "startOffset": 18, "endOffset": 43}, {"referenceID": 1, "context": "Stanford Core NLP (Clark and Manning, 2016). Using RST Parser from Ji and Eisenstein (2014). The sequence comes in two variants, depending on the featurization technique, see Section 4.", "startOffset": 19, "endOffset": 92}, {"referenceID": 15, "context": "IMDB62 consists of 62K movie reviews from 62 users (1,000 each) from the Internet Movie dataset, compiled by Seroussi et al. (2011). Unlike the novel datasets, the reviews are considerably shorter, with a mean of 349 words per text.", "startOffset": 109, "endOffset": 132}, {"referenceID": 9, "context": "001 using the Adam Optimizer (Kingma and Ba, 2014).", "startOffset": 29, "endOffset": 50}, {"referenceID": 17, "context": "75 (Srivastava et al., 2014), and run 50 epochs (batch size 32).", "startOffset": 3, "endOffset": 28}, {"referenceID": 12, "context": "stylometric features such as word n-grams and POS tags do not add additional performance improvements (Ruder et al., 2016; Sari et al., 2017).", "startOffset": 102, "endOffset": 141}, {"referenceID": 13, "context": "stylometric features such as word n-grams and POS tags do not add additional performance improvements (Ruder et al., 2016; Sari et al., 2017).", "startOffset": 102, "endOffset": 141}, {"referenceID": 7, "context": "In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji and Smith (2017), to retain more information from the RST trees while reducing the parameter size.", "startOffset": 84, "endOffset": 104}], "year": 2017, "abstractText": "We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-ofthe-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.", "creator": "LaTeX with hyperref package"}}}