{"id": "1709.02279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Cynical Selection of Language Model Training Data", "abstract": "The Moore-Lewis method of \"intelligent selection of language model training data\" is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea-- which we set out to preserve-- treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems.", "histories": [["v1", "Thu, 7 Sep 2017 14:30:50 GMT  (16kb)", "http://arxiv.org/abs/1709.02279v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amittai axelrod"], "accepted": false, "id": "1709.02279"}, "pdf": {"name": "1709.02279.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["amittai@b0rked.org"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.02 279v 1 [cs.C L] 7S ep2 01and the out-of-domain (or data pool) corpora against each other. This powerful idea - which we want to preserve - treats the two corporas the opposite ends of a single spectrum. (2) The selected sentences are not able to model the in-domain data, nor do they cover the in-domain data. They are simply popular with the in-domain model; this is necessary, but not sufficient. (3) There is no way to say what the optimal number of sentences to choose is, apart from the selection of different thresholds and the structure of the systems."}, {"heading": "1. Scenario", "text": "We have a number of translation tasks, each defined as \"one type of data that we want to be able to translate well.\" Perhaps it is the data of a particular customer or some type of data such as \"customer service logs\" or just a system for a linguistic arc (\"Mexican Spanish to American English\"). Machine translation (MT) tasks often have one of these two problems: (1) data on a web scale or too much data than can be readily used to train or operate a MT system. We want to know what data we can exclude from the training without sacrificing performance. (2) Not enough parallel data to train a MT system. We want to know what data we can add to improve performance and how much improvement we could expect. Work independently of amazon.com. 1We can add data from known parallel data, or by paying to have monolingual data translated."}, {"heading": "2. Context", "text": "This year is the highest in the history of the country."}, {"heading": "3. Inputs", "text": "In fact, we are in a position to go in search of a solution that will enable us to go in search of a solution that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to, that will enable us to put ourselves in the position we are in."}, {"heading": "4. Notation", "text": "\u2022 The lowercase w variable is for word marks. Each uppercase W is a set of word marks in a corpus. \u2022 The lowercase v variable is for word types (lexical / vocabulary elements). Each uppercase V is a set of word marks in a corpus. \u2022 The lowercase c variable is for the number of a single word type. Each uppercase C is the number of one word type in a corpus. \u2022 wn is the set of tokens in the n-th selected line. \u2022 vn is the number of word types in the n-th selected line. \u2022 cn (v) is the number of word type v in the n-selected line. \u2022 Wn and Vn are the total number of word marks or word types in the first n selected lines. \u2022 Cn (v) is the number of word types v in the first selected lines. \u2022 Word reps and vreps are the first in the selected lines."}, {"heading": "5. Math", "text": "\"We are always able to determine the probability distribution of LM (the maximum probability estimate of LM).\" (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is). (It. (It is). (It is). (It is). (It is. (It is). (It is). (It is. (It is). (It is). (It is. (It is). (It is). (It is). (It is). (it. (it). (It is). (It is). (It is)."}, {"heading": "7. Batch Mode = Fast Mode", "text": "It is the most efficient strategy for the case where we really want to select the absolute best sentences for manual processing (transcription, translation, etc.), because the external cost of using each sentence is relatively high. However, it is common to simply want a relevant subcorpus for the automatic formation of a language model, an MT system, or some other downstream system. Here, the cost of using each sentence is relatively low, and it only matters whether a particular sentence is selected (or not). It does not matter whether the sentence is selected in the thirtieth or thirtieth step, and certainly not if it is the number 235,442 versus the number 235,443. As such, it makes little sense to worry about whether the best sentence is selected (or not) in the optimal iteration. Each iteration will make a difference, much like any port in a storm.We activate batch mode to speed up the process along a bit."}, {"heading": "8. Wrapping Up", "text": "The cynical data selection code passes through proper business / legal channels for publication, and will be on github with a permissive license once it is completed (expected September 2017). In a pinch, this work contains enough detail to fully implement the algorithm. The method presented in the previous sections has the following characteristics: \u2022 It responds to the extent to which two companies differ; it retains the core concept of the cross-entropy difference of playing the two distributions against each other, but only for those parts that actually differ between the two distributions. \u2022 The cynical selection method selects sentences whose probability estimation can be improved the most. It happens that the greatest possible improvement ranges from zero - unseen - to a bad estimate (once seen)."}], "references": [{"title": "Data Selection for Statistical Machine Translation", "author": ["A. Axelrod"], "venue": "PhD thesis, University of Washington.", "citeRegEx": "Axelrod,? 2014", "shortCiteRegEx": "Axelrod", "year": 2014}, {"title": "Domain Adaptation Via Pseudo In-Domain Data Selection", "author": ["A. Axelrod", "X. He", "J. Gao"], "venue": "EMNLP (Empirical Methods in Natural Language Processing), pages 355\u2013362.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Class-Based N-gram Language Difference Models for Data Selection", "author": ["A. Axelrod", "Y. Vyas", "M. Martindale", "M. Carpuat"], "venue": "IWSLT (International Workshop on Spoken Language Translation).", "citeRegEx": "Axelrod et al\\.,? 2015", "shortCiteRegEx": "Axelrod et al\\.", "year": 2015}, {"title": "Submodularity for Data Selection in Statistical Machine Translation", "author": ["J. Bilmes", "K. Kirchhoff"], "venue": "EMNLP (Empirical Methods in Natural Language Processing).", "citeRegEx": "Bilmes and Kirchhoff,? 2014", "shortCiteRegEx": "Bilmes and Kirchhoff", "year": 2014}, {"title": "Comparing Corpora", "author": ["A. Kilgarriff"], "venue": "International Journal of Corpus Linguistics.", "citeRegEx": "Kilgarriff,? 2001", "shortCiteRegEx": "Kilgarriff", "year": 2001}, {"title": "Intelligent Selection of Language Model Training Data", "author": ["R.C. Moore", "W.D. Lewis"], "venue": "ACL (Association for Computational Linguistics).", "citeRegEx": "Moore and Lewis,? 2010", "shortCiteRegEx": "Moore and Lewis", "year": 2010}, {"title": "A Universal Prior for Integers and Estimation by Minimum Description Length", "author": ["J. Rissanen"], "venue": "The Annals of Statistics, 11(2):416\u2013431.", "citeRegEx": "Rissanen,? 1983", "shortCiteRegEx": "Rissanen", "year": 1983}, {"title": "Selecting Relevant Text Subsets from Web-Data for Building Topic Specific Language Models", "author": ["A. Sethy", "P.G. Georgiou", "S. Narayanan"], "venue": "NAACL (North American Association for Computational Linguistics).", "citeRegEx": "Sethy et al\\.,? 2006a", "shortCiteRegEx": "Sethy et al\\.", "year": 2006}, {"title": "Text Data Acquisition for DomainSpecific Language Models", "author": ["A. Sethy", "P.G. Georgiou", "S. Narayanan"], "venue": "EMNLP (Empirical Methods in Natural Language Processing), (July):382\u2013389.", "citeRegEx": "Sethy et al\\.,? 2006b", "shortCiteRegEx": "Sethy et al\\.", "year": 2006}, {"title": "Scalable Backoff Language Models", "author": ["K. Seymore", "R. Rosenfeld"], "venue": "ICLSP (International Conference on Spoken Language Processing).", "citeRegEx": "Seymore and Rosenfeld,? 1996", "shortCiteRegEx": "Seymore and Rosenfeld", "year": 1996}, {"title": "Growing an N-gram Language Model", "author": ["V. Siivola", "B.L. Pellom"], "venue": "INTERSPEECH.", "citeRegEx": "Siivola and Pellom,? 2005", "shortCiteRegEx": "Siivola and Pellom", "year": 2005}, {"title": "Entropy-Based Pruning of Backoff Language Models", "author": ["A. Stolcke"], "venue": "DARPA Broadcast News Transcription and Understanding Workshop.", "citeRegEx": "Stolcke,? 1998", "shortCiteRegEx": "Stolcke", "year": 1998}, {"title": "Submodular Subset Selection for Large-Scale Speech Training Data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "C. Bartels", "J. Bilmes"], "venue": "ICASSP (International Conference on Acoustics, Speech, and Signal Processing).", "citeRegEx": "Wei et al\\.,? 2014", "shortCiteRegEx": "Wei et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This is the traditional data selection scenario, and we improve upon the standard Moore-Lewis cross-entropy difference method (Moore and Lewis, 2010).", "startOffset": 126, "endOffset": 149}, {"referenceID": 9, "context": "Language Model Size Seymore and Rosenfeld (1996) decided that pruning a model is better than incrementally growing it:", "startOffset": 20, "endOffset": 49}, {"referenceID": 7, "context": "The relative entropy score of the sentence actually can be decomposed into the sum of the word scores (see derivation by Sethy et al. (2006b)), so there is no need to assume.", "startOffset": 121, "endOffset": 142}, {"referenceID": 6, "context": "They compute gains on the data coding length, along the lines of the Minimum Description Length principle which minimizes the size of the model plus the training data as encoded by the model (Rissanen, 1983).", "startOffset": 191, "endOffset": 207}, {"referenceID": 6, "context": "They compute gains on the data coding length, along the lines of the Minimum Description Length principle which minimizes the size of the model plus the training data as encoded by the model (Rissanen, 1983). Sethy et al. (2006b) also take the growing", "startOffset": 192, "endOffset": 230}, {"referenceID": 4, "context": "Corpus Similarity Kilgarriff (2001) posited:", "startOffset": 18, "endOffset": 36}, {"referenceID": 5, "context": "The standard approach for data selection is the same for both language modeling (Moore and Lewis, 2010) and machine translation (Axelrod et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 1, "context": "The standard approach for data selection is the same for both language modeling (Moore and Lewis, 2010) and machine translation (Axelrod et al., 2011).", "startOffset": 128, "endOffset": 150}, {"referenceID": 7, "context": "Sethy et al. (2006b) describe the general idea as \u201can incremental greedy selection scheme based on relative entropy, which selects a sentence if adding it to the already selected set of sentences reduces the relative entropy with respect to the in-domain data distribution\u201d.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "1 More complete explanations can be found in previous work: Axelrod et al. (2011) and Axelrod (2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "1 More complete explanations can be found in previous work: Axelrod et al. (2011) and Axelrod (2014).", "startOffset": 60, "endOffset": 101}, {"referenceID": 7, "context": "Sethy et al. (2006a) and (2006b) derived instead the greedy relative-entropy delta.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Sethy et al. (2006a) and (2006b) derived instead the greedy relative-entropy delta.", "startOffset": 0, "endOffset": 33}, {"referenceID": 2, "context": "We now focus on reducing the size of the vocabulary itself, with the following intuition (Axelrod et al., 2015): \u201cWhere the frequencies of words differ, the corpora differ.", "startOffset": 89, "endOffset": 111}, {"referenceID": 0, "context": "to a single token, again following Axelrod et al. (2015) and Axelrod (2014).", "startOffset": 35, "endOffset": 57}, {"referenceID": 0, "context": "to a single token, again following Axelrod et al. (2015) and Axelrod (2014). Category 1 words become \u2018dubious\u2019, category 2 are \u2018bad\u2019, and category 3 are \u2018meh\u2019.", "startOffset": 35, "endOffset": 76}, {"referenceID": 3, "context": "This is the key benefit of the submodular data selection methods of Bilmes and Kirchhoff (2014) and Wei et al.", "startOffset": 68, "endOffset": 96}, {"referenceID": 3, "context": "This is the key benefit of the submodular data selection methods of Bilmes and Kirchhoff (2014) and Wei et al. (2014). Our cynical method seems to satisfy the definition of submodularity However, we are able to avoid much of their heavy lifting by always using the same feature function (entropy gain) and not needing to specify a budget.", "startOffset": 68, "endOffset": 118}, {"referenceID": 12, "context": "9 from Wei et al. (2014): \u201c.", "startOffset": 7, "endOffset": 25}], "year": 2017, "abstractText": "The Moore-Lewis method of \u201cintelligent selection of language model training data\u201d is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea \u2013 which we set out to preserve \u2013 treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems. We present \u201ccynical selection of training data\u201d: a greedy, lazy, approximate, and generally efficient method of accomplishing the same goal. It has the following properties: (1) Is responsive to the extent to which two corpora differ. (2) Quickly reaches near-optimal vocabulary coverage. (3) Takes into account what has already been selected. (4) Does not involve defining any kind of domain, nor any kind of classifier. (5) Has real units. (6) Knows approximately when to stop. 1. Scenario We have a number of translation tasks, each defined as \u201ca flavor of data we want to be able to translate well\u201d. Perhaps it\u2019s a specific client\u2019s data, or a kind of data such as \u201ccustomer support chat logs\u201d, or just a system for a language arc (\u201cMexican Spanish to American English\u2019). Machine Translation (MT) tasks often have one of these two problems: (1) Web-scale data, or too much data than can be used readily to train or run an MT system. We want to know what data we can exclude from training without sacrificing performance. (2) Not enough parallel data to train an MT system. We want to know what data we can add to improve performance, and how much improvement we might expect. Work unaffiliated with amazon.com. 1 2 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA We can add data from known parallel data, or by paying to have monolingual data translated. We present a single method for handling both: (1) Given a too-large parallel corpus, we identify the best subset to use for training a system that is at least as good as the full system. This is the traditional data selection scenario, and we improve upon the standard Moore-Lewis cross-entropy difference method (Moore and Lewis, 2010). (2) Given a representative monolingual corpus for a translation task, and an optional parallel corpus in the language pair, identify the monolingual sentences that should be manually translated and added to the parallel corpus in order to improve translation for the specific task. Translating monolingual data is expensive, and training on bilingual data is expensive. By \u201cexpense\u201d we mean effort, computation, time, and dollars. As a rule, we want to spend as little as possible to get as much out of the available data as we can.", "creator": "LaTeX with hyperref package"}}}