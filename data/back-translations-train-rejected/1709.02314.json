{"id": "1709.02314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Representation Learning for Visual-Relational Knowledge Graphs", "abstract": "Much progress has been made towards the goal of developing ML systems that are able to recognize and interpret visual scenes. With this paper, we propose query answering in visual-relational knowledge graphs (KGs) as a novel and important reasoning problem. A visual-relational KG is a KG whose entities are associated with image data. We introduce \\textsc{ImageGraph}, a publicly available KG with 1330 relation types, 14,870 entities, and 829,931 images. Visual-relational KGs naturally lead to several novel query types treating images as first-class citizens. We approach the query answering problems by combining ideas from the areas of KG embedding learning and deep learning for computer vision. The resulting ML models can answer queries such as \\textit{\"How are these two unseen images related to each other?\"} We also explore a novel zero-shot learning scenario where an image of an entirely new entity is linked to entities of an existing visual-relational KG. An extensive set of experiments shows that the proposed deep neural networks are able to answer the visual-relational queries efficiently and accurately.", "histories": [["v1", "Thu, 7 Sep 2017 15:31:54 GMT  (2033kb,D)", "http://arxiv.org/abs/1709.02314v1", null], ["v2", "Mon, 11 Sep 2017 16:41:07 GMT  (2167kb,D)", "http://arxiv.org/abs/1709.02314v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniel o\\~noro-rubio", "mathias niepert", "alberto garc\\'ia-dur\\'an", "roberto gonz\\'alez", "roberto j l\\'opez-sastre"], "accepted": false, "id": "1709.02314"}, "pdf": {"name": "1709.02314.pdf", "metadata": {"source": "CRF", "title": "Representation Learning for Visual-Relational Knowledge Graphs", "authors": ["Daniel O\u00f1oro-Rubio", "Mathias Niepert", "Roberto Gonz\u00e1lez-S\u00e1nchez"], "emails": ["daniel.onoro@neclab.eu", "mathias.niepert@neclab.eu", "alberto.duran@neclab.eu", "roberto.gonzalez@neclab.eu", "robertoj.lopez@uah.es"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "2 Related Work", "text": "The answer to the questions in a visual-relational knowledge diagram is the main objective of this paper. Previous work on combining relative and visual data has focused on the recognition of objects [10, 12, 28] and the detection of scenes [8, 26, 29, 33] required for more complex visual reasoning. In recent years, there has been an increase in arguments about people, objects and attributes that establish relationships between objects [14, 22, 10, 16, 37]. The VisualGenome project is a knowledge base that integrates language and vision modalities. It offers a knowledge diagram based on WORDNET that provides annotations of objects, attributes and relationships for each image."}, {"heading": "3 IMAGEGRAPH: A Visual-Relational Knowledge Graph", "text": "IMAGEGRAPH is a visual-relational KG whose relationship structure is based on Albert ET's [3]. Specifically, it is based on FB15K, a subset of FREEBASE used as benchmark data in previous work to complete FB15K. FB15K does not include visual data, and we perform the following steps to enrich the units with image data. To obtain images related to each of the units of FB15K, we have implemented a web crawler capable of analyzing query results for image search engines, Bing Images, and Yahoo Image Search. To minimize the amount of noise related to polysemous entity labels, there are more than 100 FREEBASE units with the text brand \"Springfield.\" We extract all of his Wikipedia keys for each unit in FB15K."}, {"heading": "4 Deep Representation Learning for Visual-Relational Graphs", "text": "A knowledge diagram (Kg) K is given by a series of triples T, i.e. statements of the form (h, r, t), where h, t, E are the head or tail entities, and r, R are a relation type. Entities can be associated with additional data such as images, text, and numerical characteristics. Figure 1 represents a small fragment of a Kg with relationships between entities and images associated with the entities. Previous work did not include image data and therefore focused on the following two types of queries. First, the query type (h,? r, t) asks for the relationships between header and tail entities. Second, the query types (h, r,? t) and (? h, r, t) ask for entities that complete three times correctly. The latter query type is often referred to as completion of the knowledge base."}, {"heading": "4.1 Visual-Relational Query Answering", "text": "When units are associated with image data, several completely new query types are possible. Figure 1 lists the query types on which we focus in this essay. We refer to images used as seen during the training, and all other images as unseen. (1) In the face of a pair of invisible images for which we do not know their KG units, we determine the relationships between these underlying KG units and a relationship determines the relationships between the two underlying units. (4) In the face of an invisible image of a completely new unit that is not part of the KG, and an invisible image for which we do not know the underlying KG unit, the relationships between the two underlying units are determined. (3) In the face of an invisible image of a completely new unit that is not part of the KG, and a known KG unit determine the relationships between the two units. For each of these query types, the conditional relationship between the underlying units is never observed by the query type force, and the other types are not observed by the query force."}, {"heading": "4.2 Deep Representation Learning for Query Answering", "text": "We will first describe the state of the art in traditional KGs completion and translate the concepts for answering questions related to whether it is actually a real task that it is about. (Most KGs completion methods are learnable in a vector space that involves mapping high values to correct triples and low values. (eh, et) where there is a relationship, eh and et are d-dimensional vectors (focused on the head and tail), and where there is an embedding function that maps the raw input representation of entities to the embedding space. In the case of KGs without additional visual data, the raw representation of an entity is simply its unilateral completion."}, {"heading": "5 Experiments", "text": "We conduct a series of experiments to evaluate our proposed network architectures for visual-relational response to queries as defined before 4. First, we describe the setup that applies to all experiments. Second, we explain and report on the results we have achieved for the different types of queries presented in the previous sections."}, {"heading": "5.1 General Set-up", "text": "We used CAFFE, a deep learning framework [17] for designing, training and evaluating the proposed deep representation of learning models. The embedding function g is based on the VGG16 model introduced in [31]. In summary, we added the VGG16 to the ILSVRC2012 challenge dataset from IMAGENET (7) and removed the softmax layer from the original VGG16. We added a 256-dimensional layer after the last dense layer of VGG16. The output of this layer serves as an embedding of the input images. The reason for reducing the embedding of dimensionality from 4096 to 256 is to achieve an efficient and compact latent representation that is feasible for KGGs with billions of entities."}, {"heading": "5.2 Visual Relation Prediction", "text": "Faced with a pair of invisible images for which we do not know their entities, we want to determine the relationships between their underlying entities, which can be expressed with (imgh,? r, imgt). Figure 1 (a) illustrates this query type, which we call visual relation prediction. We use the deep architectures shown in Figure 2 with softmax activation and categorical cross-sectional losses. For each test, we randomly stamp an image for both the head and tail entity. We use the architecture with softmax activation and the categorical cross-sectional losses."}, {"heading": "5.3 Image Completion", "text": "Considering an invisible image, for which we do not know the underlying entity in the KG, and a relationship, we determine the images seen that conclude the query. If the header is given, we will return a ranking of the images for the entity, but we will perform a ranking of the images for the entity of the head. To this end, we have conducted experiments with each of the three compositional processes and replaced the softmax activation with two different activation / loss functions. First, we have used the models trained with softmax activation and categorical cross-entropy loss. Second, we have the models that were substituted with softmax activation and softmax activation to make the corresponding binary cross-entropy losses."}, {"heading": "5.4 Zero-Shot Visual Relation Prediction", "text": "The last set of experiments deals with the problem of zero-shot learning via visual relation prediction. For both types of queries, we get an invisible image of a completely new entity that is not part of the KG.The first type of query asks for relations between the given image and an invisible image for which we do not know the underlying KG.The second type of query asks for relations between the given image and an existing KGK entity. We believe that this is a reasonable approach to zero-shot learning, since an invisible entity or category is integrated into an existing KGK. Relations to existing visual concepts provide a characterization of the invisible entity. Note that this problem cannot be addressed by embedding methods, as entities must be part of the KGK in order to gain an embedding."}, {"heading": "6 Conclusion", "text": "Knowledge diagrams are at the core of many AI applications. In the past, research has focused either on methods of completing AI that only target the relationship structure, or on retrieving relationships between objects within the same image. We present a new visual-relational knowledge diagram in which the entities are enriched with visual data. We propose several novel types of queries and demonstrate the introduction of novel deep neural networks that are suitable for answering queries. We evaluate several composition functions used in the AI completion literature. We also propose a new perspective on zero-shot learning, where an invisible image of a completely new entity is based in an existing AI."}], "references": [{"title": "A neural knowledge language model", "author": ["S. Ahn", "H. Choi", "T. Parnamaa", "Y. Bengio"], "venue": "arXiv preprint arXiv:1608.00318,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Ba", "K. Swersky", "S. Fidler", "R. Salakhutdinov"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In SIGMOD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta. Neil"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D.A. Forsyth"], "venue": "In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Efficient and expressive knowledge base completion using subgraph feature extraction", "author": ["M. Gardner", "T.M. Mitchell"], "venue": "In EMNLP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Observing human-object interactions: Using spatial and functional compatibility for recognition", "author": ["A. Gupta", "A. Kembhavi", "L.S. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Guu", "J. Miller", "P. Liang"], "venue": "arXiv preprint arXiv:1506.01094,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Incorporating scene context and object layout into appearance modeling", "author": ["H. Izadinia", "F. Sadeghi", "A. Farhadi"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "In arXiv preprint arXiv:1602.07332,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Visual relationship detection with language priors", "author": ["C. Lu", "R. Krishna", "M. Bernstein", "L. Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Beyond categories: The visual memex model for reasoning about object relationships", "author": ["T. Malisiewicz", "A.A. Efros"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Holographic embeddings of knowledge graphs", "author": ["M. Nickel", "L. Rosasco", "T.A. Poggio"], "venue": "In Proceedings of the Thirtieth Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "A three-way model for collective learning on multirelational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Scene recognition and weakly supervised object localization with deformable part-based models", "author": ["M. Pandey", "S. Lazebnik"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["B. Romera-Paredes", "P. Torr"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Detecting avocados to zucchinis: what have we done, and where are we going", "author": ["O. Russakovsky", "J. Deng", "Z. Huang", "A.C. Berg", "L. Fei-Fei"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Latent pyramidal regions for recognizing scenes", "author": ["F. Sadeghi", "M.F. Tappen"], "venue": "In Proceedings of the 12th European Conference on Computer Vision - Volume Part V,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["I.V. Serban", "A. Garc\u00eda-Dur\u00e1n", "C. Gulcehre", "S. Ahn", "S. Chandar", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06807,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Complex embeddings for simple link prediction", "author": ["T. Trouillon", "J. Welbl", "S. Riedel", "\u00c9. Gaussier", "G. Bouchard"], "venue": "arXiv preprint arXiv:1606.06357,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "SUN database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Learning multi-relational semantics using neural-embedding models", "author": ["B. Yang", "W.-t. Yih", "X. He", "J. Gao", "L. Deng"], "venue": "arXiv preprint arXiv:1411.4072,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Modeling mutual context of object and human pose in human-object interaction activities", "author": ["B. Yao", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Zero-shot learning via semantic similarity embedding", "author": ["Z. Zhang", "V. Saligrama"], "venue": "In ICCV,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Reasoning about object affordances in a knowledge base representation", "author": ["Y. Zhu", "A. Fathi", "L. Fei-Fei"], "venue": "In European conference on computer vision,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].", "startOffset": 137, "endOffset": 140}, {"referenceID": 29, "context": "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "While IMAGENET [7] and the VISUALGENOME [19] datasets are based on KGs such as WordNet they are predominantly used as either a classification data set as in the case of IMAGENET or to facilitate scene understanding in a single image.", "startOffset": 15, "endOffset": 18}, {"referenceID": 18, "context": "While IMAGENET [7] and the VISUALGENOME [19] datasets are based on KGs such as WordNet they are predominantly used as either a classification data set as in the case of IMAGENET or to facilitate scene understanding in a single image.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 67, "endOffset": 82}, {"referenceID": 24, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 67, "endOffset": 82}, {"referenceID": 14, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 67, "endOffset": 82}, {"referenceID": 22, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 67, "endOffset": 82}, {"referenceID": 19, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 115, "endOffset": 123}, {"referenceID": 10, "context": "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].", "startOffset": 115, "endOffset": 123}, {"referenceID": 9, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 86, "endOffset": 98}, {"referenceID": 11, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 86, "endOffset": 98}, {"referenceID": 27, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 86, "endOffset": 98}, {"referenceID": 7, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 121, "endOffset": 136}, {"referenceID": 25, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 121, "endOffset": 136}, {"referenceID": 28, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 121, "endOffset": 136}, {"referenceID": 32, "context": "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.", "startOffset": 121, "endOffset": 136}, {"referenceID": 13, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 8, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 21, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 34, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 9, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 5, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 15, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 36, "context": "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].", "startOffset": 119, "endOffset": 149}, {"referenceID": 18, "context": "The VisualGenome project [19] is a knowledge base that integrates language and vision modalities.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "[21] propose a model to detect relationships between objects depicted in one image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose to use the VisualGenome dataset to recover images from simple text queries such as man riding a bike near a mountain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27, 36]) rely on an underlying embedding space, such as attributes, in order to recognize the unseen categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "[27, 36]) rely on an underlying embedding space, such as attributes, in order to recognize the unseen categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "[2, 21]), leveraging distributional word representations so as to capture a notion of taxonomy and similarity.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[2, 21]), leveraging distributional word representations so as to capture a notion of taxonomy and similarity.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "IMAGEGRAPH is a visual-relational KG whose relational structure is based on that of FREEBASE [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 22, "context": "More specifically, it is based on FB15K, a subset of FREEBASE, which has been used as a benchmark data set in previous work on KG completion [23].", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "Entities Relations Triples Images |E| |R| Train Valid Test Train Valid Test FB15k [5] 14,951 1,345 483,142 50,000 59,071 0 0 0 IMAGEGRAPH 14,870 1,330 460,406 47,533 56,071 411,306 201,832 216,793", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| \u00d7 d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector \u03c6r: \u2022 Difference (TRANSE[5]): fr(eh, et) = \u2212||eh + \u03c6r \u2212 et||2 where \u03c6r is a d-dimensional vector; \u2022 Multiplication (DISTMULT[34]): fr(eh, et) = (eh \u2217 et) \u00b7 \u03c6r where \u2217 is the element-wise product and \u03c6r a d-dimensional vector; \u2022 Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) \u00b7 \u03c6r where [a ? b]k = \u2211d\u22121 i=0 aib(i+k) mod d and \u03c6r a d-dimensional vector; and \u2022 Concatenation: fr(eh, et) = (eh et) \u00b7\u03c6r where is the concatenation operator and \u03c6r a 2d-dimensional vector.", "startOffset": 289, "endOffset": 292}, {"referenceID": 33, "context": "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| \u00d7 d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector \u03c6r: \u2022 Difference (TRANSE[5]): fr(eh, et) = \u2212||eh + \u03c6r \u2212 et||2 where \u03c6r is a d-dimensional vector; \u2022 Multiplication (DISTMULT[34]): fr(eh, et) = (eh \u2217 et) \u00b7 \u03c6r where \u2217 is the element-wise product and \u03c6r a d-dimensional vector; \u2022 Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) \u00b7 \u03c6r where [a ? b]k = \u2211d\u22121 i=0 aib(i+k) mod d and \u03c6r a d-dimensional vector; and \u2022 Concatenation: fr(eh, et) = (eh et) \u00b7\u03c6r where is the concatenation operator and \u03c6r a 2d-dimensional vector.", "startOffset": 389, "endOffset": 393}, {"referenceID": 23, "context": "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| \u00d7 d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector \u03c6r: \u2022 Difference (TRANSE[5]): fr(eh, et) = \u2212||eh + \u03c6r \u2212 et||2 where \u03c6r is a d-dimensional vector; \u2022 Multiplication (DISTMULT[34]): fr(eh, et) = (eh \u2217 et) \u00b7 \u03c6r where \u2217 is the element-wise product and \u03c6r a d-dimensional vector; \u2022 Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) \u00b7 \u03c6r where [a ? b]k = \u2211d\u22121 i=0 aib(i+k) mod d and \u03c6r a d-dimensional vector; and \u2022 Concatenation: fr(eh, et) = (eh et) \u00b7\u03c6r where is the concatenation operator and \u03c6r a 2d-dimensional vector.", "startOffset": 519, "endOffset": 523}, {"referenceID": 31, "context": "The training objective is often based on the logistic loss, which has been shown to be superior for most of the composition functions [32],", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "We used CAFFE, a deep learning framework [17] for designing, training, and evaluating the proposed deep representation learning models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "The embedding function g is based on the VGG16 model introduced in [31].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "In summary, we pre-trained the VGG16 on the ILSVRC2012 challenge data set derived from IMAGENET [7] and removed the softmax layer of the original VGG16.", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "We initialized the weights of the newly added layers with the Xavier method [13].", "startOffset": 76, "endOffset": 80}], "year": 2017, "abstractText": "Much progress has been made towards the goal of developing ML systems that are able to recognize and interpret visual scenes. With this paper, we propose query answering in visual-relational knowledge graphs (KGs) as a novel and important reasoning problem. A visual-relational KG is a KG whose entities are associated with image data. We introduce IMAGEGRAPH, a publicly available KG with 1330 relation types, 14,870 entities, and 829,931 images. Visual-relational KGs naturally lead to several novel query types treating images as first-class citizens. We approach the query answering problems by combining ideas from the areas of KG embedding learning and deep learning for computer vision. The resulting ML models can answer queries such as \u201cHow are these two unseen images related to each other?\" We also explore a novel zero-shot learning scenario where an image of an entirely new entity is linked to entities of an existing visual-relational KG. An extensive set of experiments shows that the proposed deep neural networks are able to answer the visual-relational queries efficiently and accurately.", "creator": "LaTeX with hyperref package"}}}