{"id": "1709.02448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Network Vector: Distributed Representations of Networks with Global Context", "abstract": "We propose a neural embedding algorithm called Network Vector, which learns distributed representations of nodes and the entire networks simultaneously. By embedding networks in a low-dimensional space, the algorithm allows us to compare networks in terms of structural similarity and to solve outstanding predictive problems. Unlike alternative approaches that focus on node level features, we learn a continuous global vector that captures each node's global context by maximizing the predictive likelihood of random walk paths in the network. Our algorithm is scalable to real world graphs with many nodes. We evaluate our algorithm on datasets from diverse domains, and compare it with state-of-the-art techniques in node classification, role discovery and concept analogy tasks. The empirical results show the effectiveness and the efficiency of our algorithm.", "histories": [["v1", "Thu, 7 Sep 2017 20:51:27 GMT  (760kb,D)", "http://arxiv.org/abs/1709.02448v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["hao wu", "kristina lerman"], "accepted": false, "id": "1709.02448"}, "pdf": {"name": "1709.02448.pdf", "metadata": {"source": "CRF", "title": "Network Vector: Distributed Representations of Networks with Global Context", "authors": ["Hao Wu", "Kristina Lerman"], "emails": ["hwu732@usc.edu", "lerman@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they are able"}, {"heading": "2 Related Work", "text": "Our algorithm is based on distributed representations of concepts [Hinton, 1986]. Distributed representations encode structural relationships between concepts and are typically learned through back-propagation by neural networks. Recent advances in the processing of natural language have successfully adopted distributed representation models and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013b] to model word sequences in sentences and documents. These approaches include words like these that words in similar contexts tend to have similar representations in latent space. By exchanging nodes in a document, current research [Perozzi et al., 2014; Cao et al., 2015; Cao et al., 2015; Cao et al., 2015; Grover et al., 2016] attempts to learn node representations in a similar way."}, {"heading": "3 Network Vector", "text": "Let us consider the problem of embedding nodes of a network and the entire network in a low-dimensional vector space. Let us call G = {V, E} a graph, and V is the set of vertices, and E = V \u00b7 V is the set of edges with weights W. The aim of our approach is to map the entire graph to a low-dimensional vector represented by vG and Rd, and to map each node i to a unique vector vi-Rd in the same vector space. Although the dimensionality of the network representation vG may theoretically differ from that of the node representation vi, we assume the same dimensionality d to simplify the calculation in real applications. Let us assume that M graphs are given (e.g. first-person networks of M persons of interest in a social network) and N different nodes in the corpus, then (M N +) x parameters are to be learned."}, {"heading": "3.1 A Neural Architecture", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.2 Learning with Negative Sampling", "text": "The global network vector vG, the node vectors vi, and the location-dependent context parameters ci are initialized and optimized using random values by maximizing the target in Equation (3). Stochastic gradient ascent is performed to update the set of parameters \u03b8 = {vG, vi, ci}: \u0430 \u03b8 = \u0432 \u03b8 = \u03b8 logP (vn | v1: n \u2212 1, G) = \u2202 \u03b8 [exp (v \u207b > vn) \u2211 Nvm = 1 exp (v > vm)] (6), where the learning rate is. Compression requires the normalization term and is proportional to the number of unique nodes N. The complexity of compression is expensive and impracticable in real applications. In our approach, we adopt negative sampling [Mikolov et al., 2013b] for optimization. Negative sampling represents a simplified version of noise distributions as a negative sample, [2012] Explicit and a negative sample."}, {"heading": "3.3 An Inverse Architecture", "text": "The architecture in Figure 1 uses the linear combination of the global network vector and the context node vector to predict the target node in a slider window. Another way to train the global network vector is to model the likelihood of observing a sampled node out of the slider window, due to the feature vector vG given by P (vt | G) = 1ZG exp [\u2212 E (G; vt)] (8), where ZG is the normalization term specific to the characteristic representation of G. However, the energy function E (G; vt) is: E (G; vt) = \u2212 v > Gvt (9) This architecture is a counterpart of the Distributed Bag-ofWords version of Paragraph Vector [Le and Mikolov, 2014]. However, this architecture ignores the sequence of the nodes in the slider window and performs poorly in practice when used alone. We \u2212 expand the framework by looking at the context, using a 201pgram as a context, not the context of the product."}, {"heading": "3.4 Complexity Analysis", "text": "The calculation of the network vector consists of two key parts: the sampling of node sequences with random paths and the optimization of vectors. For each node sequence of fixed length l, we assume a randomly selected root node. At each step, the walk visits a new node based on the transition probabilities P (vc | va, vb) in Equation (1). The transition probabilities P (vc | va, vb) can be pre-calculated and stored in memory, using space O (| E | 2 / | V |). The sampling of a new node in the aisle can be efficiently done in O (1) time using alias sampling [Walker, 1977]. The total time complexity is O (r | V | l) for the repetition of r-times of random walks of fixed length l by making each node as a root.The time complexity of the optimization with negative abtaity in the length is proportional to the length of the node (7)."}, {"heading": "3.5 The Property of Network Vector", "text": "The property of the global network vector vG in architecture (as shown in Figure 1) can be explained by looking at the target in Equation (3). vG is part of the input into the neural network and can be considered as a term that helps to represent the distribution of the target node vn. The relevant part v > Gvn is logarithmically related to the probability P (vn | v1: n \u2212 1; G). Therefore, the more often a particular vn is observed in the data, the greater the value of v > Gvn will be, and thus vG will be closer to vn in vector space. The training goal is to maximize the logarithm of the product of all probabilities P (vn | v1: n \u2212 1; G), and the value is related to v > Gvn, where v \u0445n is the expected vector that can be achieved by averaging all observed vn in the data."}, {"heading": "Karate Network", "text": "As an illustrative example, we apply Network Vector to the classic karate network [Zachary, 1977]. The nodes in the network represent members in a karate club, and the edges are social connections between members outside the club. In total, there are 34 nodes and 78 undirected edges. We use the inverse architecture to train the vectors. Figure 2 shows the results of the method in a two-dimensional space. We use green circles to denote nodes and orange circles to denote the entire graph. The size of a node is proportional to its degree in the graph. We see that the learned global network vector is close to these high-grade nodes, such as nodes 1 and 34, which serve as nodes of two splits in the club. The resulting global vector predominantly represents the spine nodes (e.g. hubs) in the network and compensates for the lack of global information in local neighborhoods."}, {"heading": "Legal Citation Networks", "text": "Faced with a citation network of documents, for example, scientific papers or legal opinions, we want to identify similar documents even though we want to identify similar networks. These could be groundbreaking work that serves to open up new fields of discourse in science and law, or basic work that encompasses disciplines but has less impact on discourse, such as \"methodology\" papers in science. For the purpose of the case study, we have collected a large digitized record of federal court rulings from CourtListener Project1. The most cited court decisions from the United States Supreme Court are selected and first-person networks of citations are constructed for these cases. Two distinct diagrams are observed: \"citations have a few giant hubs\" and \"citations are well connected.\" We list some examples in Table 1 where the titles of the cases with different citation patterns are colored red and blue. The first-person networks of the first six cases in Table 1 have only a few giant hubs. \""}, {"heading": "4 Experiments", "text": "Network Vector simultaneously learns to represent network nodes and the entire network. We evaluate both representations in terms of predictive tasks. First, we apply Network Vector to an environment where only local information about nodes, such as their immediate neighbors, is available. We learn to represent ego networks of some nodes using Network Vector and evaluate role discovery in social networks and concept analogy in the Encyclopedia. Second, when information about nodes is available throughout the network, we can learn to represent nodes using Network Vector, using the additional global vector for the network to help learn high-quality node representations. The resulting node representations are evaluated using a multi-level classification."}, {"heading": "4.1 Role Discovery", "text": "The interaction between an engineer and her team is different from that of a senior manager. In the Wikipedia network, each article cites different concepts that explain the meaning of the article. Some concepts can \"bridge\" the network by linking different categories of concepts. For example, the concept of bat belongs to the category of mammals, but since a bat resembles a bird, it refers to many similar articles about the category of birds."}, {"heading": "Datasets", "text": "We use the following records in the review: \u2022 Enron Email Network: It contains email interaction data from about 150 users, mostly Enron executives. There are about half a million emails that are communicated from 85,601 unique email addresses2. We have 362,467 links left after removing duplicates and self-links. Each of the email addresses belonging to Enron employees has one of 9 different positions: CEO, President, Vice2http: / / www.cs.cmu.edu / President, Director, Managing Director, Manager, Employee, In House Lawyer and Trader. We use the positions as roles. This categorization is fine-grained. To understand how the representation of functions can reflect the characteristics of different layers in the company, we also use coarse-grained labels Leader (aggregated CEO, President, Vice President), Manager (aggregated Director, Managing Director) and Employe.This includes."}, {"heading": "Methods for Comparison", "text": "For each node, we first create its ego network, which represents the induced subgraph of its immediate neighbors, and learn global vector representations for the set of ego networks by Network Vector. (3) In our experiments, we repeat 10 times the root node initialization in random walks and the length of each random walk is given as l = 80. For comparison, we evalu-3http: / / schools-wikipedia.org / ate the performance of Network Vector against the following network feature-based algorithms [Berlingerio et al, 2012]: \u2022 Degrees: number of node, average node degree, maximum \"in\" and \"out node degrees."}, {"heading": "Results", "text": "In view of the ego network of a node, we classify the ego networks of other nodes according to their distance from it in the vector space of the characteristic representations. Table 2 shows the average precision of the retrieved nodes with the same roles (class names) at the boundary k = 1, 5, 10. For simplicity, cosinal similarity is used to calculate the distance between two nodes."}, {"heading": "Method Hit@1 Hit@5 Hit@10", "text": "We observe that Network Vector performs slightly better than node2vec on Enron's email interaction network, while the improvement in performance on the Wikipedia network is over 150%. Compared to the combination of degrees, clustering coefficients and eigenvalues, the improvement of the two learning algorithms Network Vector and node2vec is outstanding, with performance growth exceeding 100% in all cases."}, {"heading": "4.2 Concept Analogy", "text": "Considering a Wikipedia article pair that describes two concepts (a, b), and an article describing another concept (c), the task aims to find a concept where a concept d is such that a to b is such as c to d. For example, Europe to euro is like the US to dollar. This analogy problem can be solved by finding the concept closest to vb \u2212 va + vc in vector space, where the distance is calculated with cosmic similarity. There are 1632 semantic tuples in the Wikipedia network that are associated with the semantic pairs in [Mikolov et al., 2013a]. We use them as a yardstick. Table 3 shows the accuracy of hitting the answer d within the truncated k = 1, 5, 10 positions in the ranking. From the results we can see that network recipient values are much better used as efficient cludes."}, {"heading": "4.3 Multi-label Classification", "text": "Multi-label classification is a challenging task where each node can have one or more labels. A classifier is trained to predict multiple possible labels for each test node. In our Network Vector algorithm, the global representation of the entire network serves as an additional context in addition to the local neighborhood when displaying learning nodes."}, {"heading": "Datasets", "text": "To understand whether global representation helps to improve the representation of nodes, we perform a multi-level classification using the same benchmarks and experimental procedures as [Grover and Leskovec, 2016] using the same datasets: \u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by BlogCatalog websites. The labels represent the interests of bloggers on a list of topic categories. There are 10,312 nodes, 333,983 edges in the network and 39 different labels for nodes. \u2022 Protein-protein interactions (PPI) [Breitkreutz et al., 2008; Grover and Leskovec, 2016]: This is a subgraph of the entire PPI network for homo sapiens. The labels of the node come from brand sets [Liberzon et al, 2011] and represent biological states."}, {"heading": "Methods for Comparison", "text": "We compare the node representations learned from Network Vector against the following characteristics Learning methods for node representation: \u2022 Spectral clustering [Tang and Liu, 2011]: This method learns the d-smallest eigenvectors of the normalized graph Laplacian matrix, and uses them as ddimensional feature representations for nodes. \u2022 DeepWalk [Perozzi et al., 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences generated by uniform random strolls from the source nodes on graph. \u2022 LINE [Tang et al., 2015]: This method learns ddimensional feature representations by sampling nodes at 1-hop and 2-hop distance from the source nodes VFS-like manner and reading node-like way of 2 and 2-dimensional representations."}, {"heading": "Results", "text": "The default parameter setting (p = 1, q = 1) used in DeepWalk and the optimal parameter setting of node2vec reported in [Grover and Leskovec, 2016] are used. It is obvious that the global representation of the entire network allows Network Vector to exploit the global structure of the networks to learn better node representations. Network Vector achieves a slight increase in performance, 1.0% over node2vec, and DeepWalk uses the same uniform random sequences. It is obvious that the global representation of the entire network Network Vector uses the global structure of the networks to learn better node representation. Network Vector achieves a slight increase in performance, 1.0% over node2vec, and a significant increase of 12.4% over DeepWalk on BlogCatalog."}, {"heading": "Parameter Sensitivity", "text": "To understand how Network Vector improves when displaying learning nodes with biased random walks in the structure q, we evaluate performance while varying the parameter settings of (p, q). We fix p = \u221e to prevent sampled nodes from revisiting sampled nodes in random walks at the previous step, and vary the value q in the range of 2 \u2212 4 to 24 to perform DFS-like sampling at different degrees. Figure 3 shows the comparison results for Network Vector and ndoe2vec in both macro F1 and micro F1 values. As we can see, Network Vector consistently outperforms node2vec in different parameter settings of q in all three datasets. However, we observe on BlogCatalog that Network Vector achieves relatively greater gains over node2vec when q is large that the random walks in the direction of FS sampling are similar."}, {"heading": "Effect of Training Data", "text": "To see the effect of the training data, we compare the performance while varying the proportion of marked data from 10% to 90%. Figure 4 shows the results on the PPI. The parameters (p, q) are set using optimal values (4, 1). As we can see, the performance of node2vec and Network Vector generally increases when using more marked data. Network Vector achieves the greatest gain over node2vec of 9.0% in the macro F1 score and 10.3% in 40% marked data. If only 10% marked data is used, Network Vector only leads to a gain of 1.5% in the macro F1 score and 7.1% in the micro F1 score. We have similar observations at BlogCatalog and Wikipedia datasets, and the results are not shown."}, {"heading": "5 Conclusion", "text": "We introduced Network Vector, an algorithm for learning distributed representations of nodes and networks at the same time. By embedding the network in a low-dimensional vector space, our algorithm allows for quantitative comparison of networks. It also allows comparison of individual network nodes, since each node can be represented by its ego network - a network that includes the node itself, its neighbors, and all connections between them. Unlike existing network embedding methods, which only learn representations of component nodes, Network Vector directly learns the representation of an entire network. Learning a representation of a network allows us to evaluate the similarity between two networks or two individual nodes, which allows us to answer questions that were difficult to answer with existing methods. For example, a node in a network, for example, a manager within an organization, we can identify other people who play a similar role within that organization."}], "references": [{"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Netsimile: a scalable approach to size-independent network similarity", "author": ["Michele Berlingerio", "Danai Koutra", "Tina Eliassi-Rad", "Christos Faloutsos"], "venue": "arXiv preprint arXiv:1209.2684,", "citeRegEx": "Berlingerio et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "The biogrid interaction database: 2008 update", "author": ["Breitkreutz et al", "2008] Bobby-Joe Breitkreutz", "Chris Stark", "Teresa Reguly", "Lorrie Boucher", "Ashton Breitkreutz", "Michael Livstone", "Rose Oughtred", "Daniel H Lackner", "J\u00fcrg B\u00e4hler", "Valerie Wood"], "venue": "Nucleic acids research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["Shaosheng Cao", "Wei Lu", "Qiongkai Xu"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891\u2013900. ACM,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["Chang et al", "2015] Shiyu Chang", "Wei Han", "Jiliang Tang", "Guo-Jun Qi", "Charu C Aggarwal", "Thomas S Huang"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Guided learning for role discovery (glrd): framework, algorithms, and applications", "author": ["Gilpin et al", "2013] Sean Gilpin", "Tina Eliassi-Rad", "Ian Davidson"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "node2vec: Scalable feature learning for networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864,", "citeRegEx": "Grover and Leskovec. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rolx: structural role extraction & mining in large graphs", "author": ["Henderson et al", "2012] Keith Henderson", "Brian Gallagher", "Tina Eliassi-Rad", "Hanghang Tong", "Sugato Basu", "Leman Akoglu", "Danai Koutra", "Christos Faloutsos", "Lei Li"], "venue": "In Proceedings of the 18th ACM SIGKDD international", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "volume 1", "author": ["Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society"], "venue": "page 12. Amherst, MA,", "citeRegEx": "Hinton. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "author": ["Quoc Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Jill P Mesirov", "author": ["Arthur Liberzon", "Aravind Subramanian", "Reid Pinchback", "Helga Thorvaldsd\u00f3ttir", "Pablo Tamayo"], "venue": "Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):1739\u20131740,", "citeRegEx": "Liberzon et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "URL: http://www", "author": ["Matt Mahoney. Large text compression benchmark"], "venue": "mattmahoney. net/text/text. html,", "citeRegEx": "Mahoney. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330,", "citeRegEx": "Marcus et al.. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al", "2010] Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 24th international conference on Machine learning", "author": ["Andriy Mnih", "Geoffrey Hinton. Three new graphical models for statistical language modelling"], "venue": "pages 641\u2013648. ACM,", "citeRegEx": "Mnih and Hinton. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning word embeddings efficiently with", "author": ["Mnih", "Kavukcuoglu", "2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "Mnih and Teh. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM,", "citeRegEx": "Perozzi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Lei Tang", "Huan Liu. Relational learning via latent social dimensions"], "venue": "pages 817\u2013826. ACM,", "citeRegEx": "Tang and Liu. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Data Mining and Knowledge Discovery", "author": ["Lei Tang", "Huan Liu. Leveraging social media networks for classification"], "venue": "23(3):447\u2013478,", "citeRegEx": "Tang and Liu. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Line: Largescale information network embedding", "author": ["Tang et al", "2015] Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Featurerich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova et al", "2003] Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "ACM Transactions on Mathematical Software (TOMS)", "author": ["Alastair J Walker. An efficient method for generating discrete random variables with general distributions"], "venue": "3(3):253\u2013256,", "citeRegEx": "Walker. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Daixin Wang", "Peng Cui", "Wenwu Zhu. Structural deep network embedding"], "venue": "pages 1225\u2013 1234. ACM,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Journal of anthropological research", "author": ["Wayne W Zachary. An information flow model for conflict", "fission in small groups"], "venue": "33(4):452\u2013473,", "citeRegEx": "Zachary. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "Social computing data repository at asu", "author": ["Zafarani", "Liu", "2009] Reza Zafarani", "Huan Liu"], "venue": null, "citeRegEx": "Zafarani et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zafarani et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "To characterize network structure, traditional approaches extract features such as node degrees, clustering coefficients, eigenvalues, the lengths of shortest paths and so on [Berlingerio et al., 2012; Henderson et al., 2012; Gilpin et al., 2013].", "startOffset": 175, "endOffset": 246}, {"referenceID": 19, "context": "Recent advances in distributed representation of nodes [Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016] in networks created an alternate framework for unsupervised feature learning of nodes in networks.", "startOffset": 55, "endOffset": 123}, {"referenceID": 6, "context": "Recent advances in distributed representation of nodes [Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016] in networks created an alternate framework for unsupervised feature learning of nodes in networks.", "startOffset": 55, "endOffset": 123}, {"referenceID": 9, "context": "Our approach is inspired by Paragraph Vector [Le and Mikolov, 2014] that learns distributed representations of texts of variable length such as sentences and documents [Le and Mikolov, 2014].", "startOffset": 45, "endOffset": 67}, {"referenceID": 9, "context": "Our approach is inspired by Paragraph Vector [Le and Mikolov, 2014] that learns distributed representations of texts of variable length such as sentences and documents [Le and Mikolov, 2014].", "startOffset": 168, "endOffset": 190}, {"referenceID": 19, "context": "Specifically, we sample sequences of nodes from a network using random walks, same as in [Perozzi et al., 2014; Grover and Leskovec, 2016].", "startOffset": 89, "endOffset": 138}, {"referenceID": 6, "context": "Specifically, we sample sequences of nodes from a network using random walks, same as in [Perozzi et al., 2014; Grover and Leskovec, 2016].", "startOffset": 89, "endOffset": 138}, {"referenceID": 6, "context": "We compare Network Vector with state-of-the-art feature learning algorithm node2vec [Grover and Leskovec, 2016], LINE [Tang et al.", "startOffset": 84, "endOffset": 111}, {"referenceID": 19, "context": ", 2015], DeepWalk [Perozzi et al., 2014] and featurebased baselines such as node degrees, clustering coefficients and eigenvalues.", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "Our algorithm builds its foundation on learning distributed representations of concepts [Hinton, 1986] .", "startOffset": 88, "endOffset": 102}, {"referenceID": 0, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 16, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 14, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 15, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 19, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 3, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 6, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 19, "context": "For example, DeepWalk [Perozzi et al., 2014] samples node sequences from a network using a stream of short first-order random walks, and model them just like word sequences in documents using neural embeddings.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "GrapRep [Cao et al., 2015] extends LINE to exploit structural information beyond second-order proximity.", "startOffset": 8, "endOffset": 26}, {"referenceID": 6, "context": "To offer a flexible node sampling scheme, node2vec [Grover and Leskovec, 2016] utilizes second-order random walks, and combines Depth-First Search (DFS) and Breadth-First Search (BFS) strategies to explore the local neighborhood structure.", "startOffset": 51, "endOffset": 78}, {"referenceID": 3, "context": "Although recent approach GrapRep [Cao et al., 2015] attempts to capture long distance relationship between two different nodes, it limits scope to a fixed number of hops.", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": "This is inspired by Paragraph Vector [Le and Mikolov, 2014], which learns a continuous vector to represent a piece of text with variable-length, such as sentences, paragraphs and documents.", "startOffset": 37, "endOffset": 59}, {"referenceID": 9, "context": ", sentences and documents [Le and Mikolov, 2014].", "startOffset": 26, "endOffset": 48}, {"referenceID": 9, "context": "The concept of \u201cwords\u201d in a document [Le and Mikolov, 2014] is", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "In order to characterize the local context of a node, without loss of generality, we sample node sequences from the given network with second-order random walks in [Grover and Leskovec, 2016], which offer a flexible notion of a node\u2019s local neighborhood by combining Depth-First Search (DFS) and Breadth-First Search (BFS) strategies.", "startOffset": 164, "endOffset": 191}, {"referenceID": 16, "context": "We extend the scalable version of Log-Bilinear model [Mnih and Hinton, 2007], called vector Log-Bilinear model (vLBL) [Mnih and Kavukcuoglu, 2013].", "startOffset": 53, "endOffset": 76}, {"referenceID": 15, "context": "In our approach, we adopt negative sampling [Mikolov et al., 2013b] for optimization.", "startOffset": 44, "endOffset": 67}, {"referenceID": 18, "context": "Negative sampling represents a simplified version of noise contrastive estimation [Mnih and Teh, 2012], and trains a logistic regression to distinguish between data samples of vn from \u201cnoise\u201d distribution.", "startOffset": 82, "endOffset": 102}, {"referenceID": 9, "context": "This architecture is a counterpart of the Distributed Bag-ofWords version of Paragraph Vector [Le and Mikolov, 2014].", "startOffset": 94, "endOffset": 116}, {"referenceID": 14, "context": "We extend the framework by simultaneously training network and node vectors using a Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] like model.", "startOffset": 93, "endOffset": 139}, {"referenceID": 15, "context": "We extend the framework by simultaneously training network and node vectors using a Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] like model.", "startOffset": 93, "endOffset": 139}, {"referenceID": 24, "context": "Sampling a new node in the walk can be efficiently done inO(1) time using alias sampling [Walker, 1977].", "startOffset": 89, "endOffset": 103}, {"referenceID": 26, "context": "As an illustrative example, we apply Network Vector to the classic Karate network [Zachary, 1977].", "startOffset": 82, "endOffset": 97}, {"referenceID": 1, "context": "org/ ate the performance of Network Vector against the following network feature-based algorithms [Berlingerio et al., 2012]:", "startOffset": 98, "endOffset": 124}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: This approach learns low-dimensional feature representations of nodes in a network by interpolating between BFS and DFS for sampling node sequences.", "startOffset": 11, "endOffset": 38}, {"referenceID": 19, "context": "It\u2019s interesting to note when p = 1 and q = 1, node2vec boils down to DeepWalk [Perozzi et al., 2014], which utilizes uniform random walks.", "startOffset": 79, "endOffset": 101}, {"referenceID": 14, "context": "For Wikipedia network, we follow the word analogy task defined in [Mikolov et al., 2013a].", "startOffset": 66, "endOffset": 89}, {"referenceID": 14, "context": "There are 1,632 semantic tuples in Wikipedia network matched for the semantic pairs in [Mikolov et al., 2013a].", "startOffset": 87, "endOffset": 110}, {"referenceID": 6, "context": "To understand whether the global representation helps learning better node representation, we perform multi-label classification with the same benchmarks and experimental procedure as [Grover and Leskovec, 2016] using the same datasets: \u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by bloggers on the BlogCatalog website.", "startOffset": 184, "endOffset": 211}, {"referenceID": 20, "context": "To understand whether the global representation helps learning better node representation, we perform multi-label classification with the same benchmarks and experimental procedure as [Grover and Leskovec, 2016] using the same datasets: \u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by bloggers on the BlogCatalog website.", "startOffset": 251, "endOffset": 295}, {"referenceID": 6, "context": "\u2022 Protein-Protein Interactions (PPI) [Breitkreutz et al., 2008; Grover and Leskovec, 2016]: This is a subgraph of the entire PPI network for Homo Sapiens.", "startOffset": 37, "endOffset": 90}, {"referenceID": 10, "context": "The node labels are obtained from hallmark gene sets [Liberzon et al., 2011] and represent biological states.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": "\u2022 Wikipedia Cooccurrences [Mahoney, 2009; Grover and Leskovec, 2016]: This is a network of words appearing in the first million bytes of the Wikipedia dump.", "startOffset": 26, "endOffset": 68}, {"referenceID": 6, "context": "\u2022 Wikipedia Cooccurrences [Mahoney, 2009; Grover and Leskovec, 2016]: This is a network of words appearing in the first million bytes of the Wikipedia dump.", "startOffset": 26, "endOffset": 68}, {"referenceID": 12, "context": "The Part-of-Speech (POS) tags [Marcus et al., 1993] inferred using the Stanford POS-Tagger [Toutanova et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "\u2022 Spectral clustering [Tang and Liu, 2011]: This method learns the d-smallest eigenvectors of the normalized graph Laplacian matrix, and utilize them as the ddimensional feature representations for nodes.", "startOffset": 22, "endOffset": 42}, {"referenceID": 19, "context": "\u2022 DeepWalk [Perozzi et al., 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 14, "context": ", 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences, that are generated by uniform random walks from the source nodes on graph.", "startOffset": 81, "endOffset": 127}, {"referenceID": 15, "context": ", 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences, that are generated by uniform random walks from the source nodes on graph.", "startOffset": 81, "endOffset": 127}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: We use the original node2vec algorithm with optimal parameter settings of (p, q) reported in [Grover and Leskovec, 2016].", "startOffset": 11, "endOffset": 38}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: We use the original node2vec algorithm with optimal parameter settings of (p, q) reported in [Grover and Leskovec, 2016].", "startOffset": 133, "endOffset": 160}, {"referenceID": 3, "context": "For fair comparison, we exclude recent approaches GraRep [Cao et al., 2015], HNE [Chang et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 25, "context": ", 2015] and SDNE [Wang et al., 2016].", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": "For fair comparison, we use the inverse architecture of Network Vector, which is Skip-gram [Mikolov et al., 2013a] like and similar to that of node2vec.", "startOffset": 91, "endOffset": 114}, {"referenceID": 6, "context": "The parameter settings for Network Vector are in favor of node2vec, and exactly the same as in [Grover and Leskovec, 2016].", "startOffset": 95, "endOffset": 122}, {"referenceID": 6, "context": "The default parameter setting (p = 1, q = 1) used in DeepWalk and the optimal parameter setting of node2vec reported in [Grover and Leskovec, 2016] are used.", "startOffset": 120, "endOffset": 147}], "year": 2017, "abstractText": "We propose a neural embedding algorithm called Network Vector, which learns distributed representations of nodes and the entire networks simultaneously. By embedding networks in a lowdimensional space, the algorithm allows us to compare networks in terms of structural similarity and to solve outstanding predictive problems. Unlike alternative approaches that focus on node level features, we learn a continuous global vector that captures each node\u2019s global context by maximizing the predictive likelihood of random walk paths in the network. Our algorithm is scalable to real world graphs with many nodes. We evaluate our algorithm on datasets from diverse domains, and compare it with state-of-the-art techniques in node classification, role discovery and concept analogy tasks. The empirical results show the effectiveness and the efficiency of our algorithm.", "creator": "LaTeX with hyperref package"}}}