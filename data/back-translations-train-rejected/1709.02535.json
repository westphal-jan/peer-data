{"id": "1709.02535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Mirror Descent Search and Acceleration", "abstract": "In recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learn- ing algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.", "histories": [["v1", "Fri, 8 Sep 2017 04:43:48 GMT  (692kb)", "http://arxiv.org/abs/1709.02535v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["megumi miyashita", "shiro yano", "toshiyuki kondo"], "accepted": false, "id": "1709.02535"}, "pdf": {"name": "1709.02535.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Megumi Miyashita", "Shiro Yano", "Toshiyuki Kondo"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 9.02 535v 1 [cs.L G] 8S ep2 01 In recent years, attention has been drawn to the relationship between black box optimization and amplification learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Gain learning, on the other hand, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose an amplification learning algorithm based on the mirror descent method, which is the general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold: first, an enhancement method for amplification learning can be applied and such a method is considered here. Second, the relationship between existing amplification learning algorithms is clarified. On this basis, we propose Mirror Descent Search and derivative methods. Experimental results show that the proposed learning method proceeds faster with the proposed method."}, {"heading": "1. Introduction", "text": "In recent years, attention has focused, as outlined in [1], on the relationship between black box optimization and gain learning. Black box optimization is a framework for the problem of finding the input x * X that optimizes output f (x): an \u2192 R that is represented by an unknown function. Since the objective function is unknown, we solve the problem of black box optimization without favor information. In contrast, gain learning is a framework for finding a policy to optimize the expected cumulative reward of trial and error. Building on this, the solution of the black box optimization problem can be used as a solution for gain learning. In this research, we propose a reinforcement learning algorithm based on the method of mirror pedigree (MD) [2]. MD is a general optimization algorithm that uses a Bregman. Research has been partially supported by JSP05 EuAKS 200K1JPJPH0319 and JPH16KEN127KJPH119."}, {"heading": "1.1. Related works", "text": "In this section, we describe previous research and its relationship to research in this paper.Relative Entropy Policy Search (REPS) [?] and its derivative method focuses on information loss during political search. This information loss is the relative entropy of data distribution generated from the distribution of observation data. The new policy and is set as the upper limit. This corresponds to the definition of the upper limit of Kullback Librar (KL) divergence between individual distributions. Consequently, we can use extension methods in the MD.In [1, 5] is a derivative method formalized by taking politics into account at the upper level.Although the equations for episodic REPS and the proposed method are similar, our method may of course have different distance metrics than the KL divergence.As a result, we can use extension methods in the MD.In [1, 5] the authors focus on the relationship between reinforcement learning and black box optimization."}, {"heading": "2. Methods: MDS, G-MDS, AMDS, G-AMDS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Derivation of proposed algorithm: MDS and G-MDS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1. MDS", "text": "A reinforcement learning algorithm is an algorithm that aims to achieve an optimal policy for maximizing reward (i.e., by minimizing costs). Let's look at the problem of minimizing the objective function J (\u03b8). Instead of dealing directly with political parameters, we look at the probability function p (\u03b8). Therefore, we look for the following domain: P = {p (\u03b8) = [p), and the objective function is the expectation of costs. J = M (empirical). jp (empirical) jp (empirical). (empirical) The decision variable is p (empirical), and the objective function is the expectation of costs J (empirical). J = M (empirical). (empirical) jp (empirical). (empirical). (empirical) empirically \"empirically\" empirically. \""}, {"heading": "2.2. G-MDS", "text": "For the experiment, we considered a case in which the Bregman divergence B\u03c6 in Eq. (6) is the KL divergence, that is, in B\u03c6 is \u03c6 (xt) = \u2211 N \u2212 p = 1 xt, j log (xt, j) (x RN, xt, j > 0). Then, it can be rewritten as follows: pt (\u03b8i) = exp (\u2212 \u03b5tgt, i) pt \u2212 1 (\u03b8i) \u2211 N j = 1 exp (\u2212 4) pt \u2212 1 (\u2212 4) pt \u2212 1 (\u2212 6) pt \u2212 1 (\u03b8j, j) (\u03b8j) (12) In this paper, we consider pt (\u03b8i) as the Gaussian distribution of the average \u00b5t \u2212 1 and the variance \u043f\u043e\u0441\u0442\u043e\u0441\u0442i \u2212 1 \u2212 1, i in which it is generated accordingly: pt (\u0445i) = N (\u0432)."}, {"heading": "2.3. Derivation of the proposed algorithm: AMDS and G-AMDS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1. AMDS", "text": "Next, the method of accelerated mirror descent (AMD) is applied to the proposed method. (AMD) is an accelerated method that generalizes the accelerated gradient of Nesterov so that it can be applied to MD. (Eq.) Eq. (6) with AMD also yields the following equations: pt (\u03b8) = \u03bbt \u2212 1p z \u2212 1 (\u03b8) + (1 \u2212 \u03bbt \u2212 1 (\u03b8), with\u03bbt \u2212 1 = rr + t (18) pz. (\u03b8) = discrete sample flow of AMD = discrete sample flow of Ptsr {< gt, p \u00b2 (\u0432) > + Biel (pz) | pz \u00b2 distribution of AMD = discrete sample flow of AMD). (19) px."}, {"heading": "2.3.2. G-AMDS", "text": "We derive the same procedure as G-MDS using the KL divergence: Let the Bregman divergence B\u03c6 of equation (19) be the KL distance and let R = B\u03c9 in equation (20) to \u03c9 (x) = area divergence ni = 1 (xi + B) log (xi + B) (x-RN, xt, j > 0). Accordingly, this method is called GAMDS. Furthermore, the result cannot be calculated analytically. In fact, it is known that an efficient and numerical calculation is possible. Finally, we approach the distributions px-MDS (\u03b8) and pz-MDS (\u03b8) with a Gaussian distribution."}, {"heading": "3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. 2DOF Via-point task", "text": "In order to evaluate the proposed method, we performed a 2DOF via point task. The agent is represented as a point at the x-y level, this agent learns to traverse the point (0,5,0,2) at 250 ms. Before learning this, an initial trajectory is generated from (0, 0) to (1, 1). The reward function is as follows: rt = 5000f 2 t + 0,5\u03b8 T\u03b8 (21) \u0445 r250ms = 100000000 ((0.5 \u2212 x250ms) 2 + (0,2 \u2212 y250ms) 2) (22) Here, DMP [12] is used for parameterizing the policy, and the agent seeks a policy for each x-axis and y-axis. The parameter settings are as follows: 1000 updates, 15 rollouts and 10 basic functions."}, {"heading": "3.2. Experimental Results", "text": "In this section, we describe the experimental results. We summarize the results for GMDS and G-AMDS in Figure 2. In the figure, the thin line represents a standard deviation of 1. Table 1 shows the average and the deviation in convergence. The deviation for each search noise should be 1.0.From above, we confirm that G-AMDS learns at a faster rate than G-MDS. Therefore, it is effective to apply the proposed extension for MD to enhanced learning."}, {"heading": "4. Conclusions", "text": "In this study, we proposed MDS. We explained the theoretical derivatives of MDS, G-MDS, AMDS and G-AMDS. According to the experimental results, learning with the proposed G-AMDS proceeded faster. Furthermore, due to the fact that AMD is a generalization of Nesterov's acceleration method, we expect acceleration to be effective for an objective function with a saddle point."}, {"heading": "Acknowledgment", "text": "The research was supported by JSPS KAKENHI (funding figures JP26120005, JP16H03219 and JP17K12737).Annex A. Relationship between G-MDS and PI2 algorithmWe show here that the algorithm PI2 is equivalent, replacing the symbols as follows: Pt \u2212 1, i = exp (\u2212 \u03b7tgt, i) \u2211 N j = 1 exp (\u2212 \u03b7tgt, j) (A.1) Mt \u2212 1, i = 1 (A.2) That is, equivalent (17) becomes equivalent to equivalent. (A.3): \u00b5 t = \u00b5 t \u2212 1 + N \u0445 i = 1 (Pt \u2212 1, iMt \u2212 1, ight \u2212 1, i) (A.3) The PI2 algorithm is equivalent to equivalent to equivalent to equivalent to equivalent to equivalent to equivalent to A.3."}], "references": [{"title": "Policy improvement methods: Between black-box optimization and episodic reinforcement learning", "author": ["F. Stulp", "O. Sigaud"], "venue": "34 pages ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Y", "author": ["J. Peters", "K. Mulling"], "venue": "Altun, Relative entropy policy search., in: AAAI, Atlanta", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical relative entropy policy search", "author": ["C. Daniel", "G. Neumann", "J.R. Peters"], "venue": "in: International Conference on Artificial Intelligence and Statistics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "ROCK*- Efficient black-box optimization for policy learning", "author": ["J. Hwangbo", "C. Gehring", "H. Sommer", "R. Siegwart", "J. Buchli"], "venue": "in: 2014 IEEE-RAS International Conference on Humanoid Robots, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "Journal of Machine Learning Research 11 (Nov) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning of motor skills in high dimensions: A path integral approach", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "in: Robotics and Automation (ICRA), 2010 IEEE International Conference on, IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary computation 9 (2) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient bregman projections onto the simplex", "author": ["W. Krichene", "S. Krichene", "A. Bayen"], "venue": "in: Decision and Control (CDC), 2015 IEEE 54th Annual Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerated mirror descent in continuous and discrete time", "author": ["W. Krichene", "A. Bayen", "P.L. Bartlett"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning attractor landscapes for learning motor primitives", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Tech. rep. ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Introduction In recent years, as stated in [1], attention has focused on the relationship between black box optimization and reinforcement learning.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Episode-based REPS [4] is a derivation method formalized by considering the upper-level policy.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.", "startOffset": 3, "endOffset": 9}, {"referenceID": 3, "context": "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.", "startOffset": 3, "endOffset": 9}, {"referenceID": 0, "context": "Specifically, [1] explains the history of black box optimization and reinforcement learning, and proposes PI.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 5, "context": "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "It is considered a black box optimization method, where PI is derived on the basis of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8], a black box optimization algorithm.", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "We can select the Bregman divergence as the KL divergence \u03c6 (xt) = \u2211N j=1 xt,j log (xt,j), but we can also use the Euclidean distances assumed on the simplex [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 8, "context": "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 8, "context": "AMDS Next, the accelerated mirror descent (AMD) method [11] is applied to the proposed method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.", "startOffset": 30, "endOffset": 36}, {"referenceID": 9, "context": "2\u2212 y250ms) 2) (22) Here, DMP [12] is used for the parameterization of the policy, and the agent is seeking a policy for each x-axis and y-axis.", "startOffset": 29, "endOffset": 33}], "year": 2017, "abstractText": "In recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learning algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.", "creator": "LaTeX with hyperref package"}}}