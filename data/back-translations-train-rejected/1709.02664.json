{"id": "1709.02664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Multi-level Feedback Web Links Selection Problem: Learning and Optimization", "abstract": "Selecting the right web links for a website is important because appropriate links not only can provide high attractiveness but can also increase the website's revenue. In this work, we first show that web links have an intrinsic \\emph{multi-level feedback structure}. For example, consider a $2$-level feedback web link: the $1$st level feedback provides the Click-Through Rate (CTR) and the $2$nd level feedback provides the potential revenue, which collectively produce the compound $2$-level revenue. We consider the context-free links selection problem of selecting links for a homepage so as to maximize the total compound $2$-level revenue while keeping the total $1$st level feedback above a preset threshold. We further generalize the problem to links with $n~(n\\ge2)$-level feedback structure. The key challenge is that the links' multi-level feedback structures are unobservable unless the links are selected on the homepage. To our best knowledge, we are the first to model the links selection problem as a constrained multi-armed bandit problem and design an effective links selection algorithm by learning the links' multi-level structure with provable \\emph{sub-linear} regret and violation bounds. We uncover the multi-level feedback structures of web links in two real-world datasets. We also conduct extensive experiments on the datasets to compare our proposed \\textbf{LExp} algorithm with two state-of-the-art context-free bandit algorithms and show that \\textbf{LExp} algorithm is the most effective in links selection while satisfying the constraint.", "histories": [["v1", "Fri, 8 Sep 2017 11:55:00 GMT  (1180kb,D)", "http://arxiv.org/abs/1709.02664v1", "8 pages (with full proof), 4 figures, ICDM 2017 technical report"]], "COMMENTS": "8 pages (with full proof), 4 figures, ICDM 2017 technical report", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kechao cai", "kun chen", "longbo huang", "john c s lui"], "accepted": false, "id": "1709.02664"}, "pdf": {"name": "1709.02664.pdf", "metadata": {"source": "CRF", "title": "Multi-level Feedback Web Links Selection Problem: Learning and Optimization", "authors": ["Kechao Cai", "Kun Chen", "Longbo Huang", "John C.S. Lui"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "II. MODEL", "text": "In this section, we first present the context-free web link selection problem with a 2-step feedback structure."}, {"heading": "III. ALGORITHM & ANALYSIS", "text": "In this section, we will first explain the design of our limited bandit algorithm LExp (which stands for \"(KL) exponential weights\") and present the algorithmic details. Then, we will present both remorse and injury and show that our algorithm has the attractive property of being sublinear in terms of both remorse and injury. 2If h = 0, the problem corresponds to the classic unlimited multiple bandit problem (MP-MAB) [8]. Algorithm 1 LExp: Init: 1 = 1, \u03bb1 = 0, h > 0, \u03b2 = (1 \u2212 L \u2212 \u03b3 / K) / (1 \u2212 xi) 1: for t = 1,.., T do 2: At = 5, It = qatip: 0. 3: if maxi \u00b2 K: 0, p: p = 1 \u00b2, p: x \u00b2."}, {"heading": "A. Constrained Bandit Algorithm", "text": "The unique challenge for our algorithmic design is to strike a balance between maximizing multilevel rewards (or minimizing regret) on the one hand and satisfying the threshold condition on the other. To meet this challenge, we integrate the theory of the Lagrange Method into the design of LExp. We consider minimizing a modified regret function that includes the injury with an adjustable penalty coefficient that increases regret when there is an injury that does not deviate from zero. Specifically, LExp introduces a sublinear limit for the lagrange function of Reg\u03c0 (T) and Vio\u03c0 (T) in the following structure."}, {"heading": "B. Regret and Violation Analysis", "text": "Theoretically, it is possible that we are not able to find a solution. Theoretically, we may not be able to find a solution. Theoretically, we may not be able. Theoretically, we may not be able to find a solution. Theoretically, we may not be able. Theoretically, we may not be able. Theoretically, we may not be able. Theoretically, we may not be able to find a solution. Theoretically, we may not be able. Theoretically, we may not be able. Theoretically, we may not be able to find a solution. Theoretically, we may not be able. Theoretically, we may not be able to find a solution. Theoretically, we may not be able. Theoretically, we may not be able. Theoretically, we may be able."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will first examine the multi-level feedback structures of the web links in two real datasets of the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to in this article as \"Ad Clicks\" and \"Coupon Purchase.\" We will then conduct a comparative study using LExp and two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.M [5], to demonstrate the effectiveness of LExp in selecting links."}, {"heading": "A. Multi-level Feedback Structure Discovery", "text": "The ad clicks are collected by the users of the site Avito.ru, where a user interested in an ad must first click before submitting a phone request for further inquiries. The data includes the logs of the visit stream and the telephone request stream of 71, 677, 831 ads. We first perform some data cleaning, and the ads that receive few phone requests (less than 100) count the number of phone requests that we receive. In particular, we filter the ads that have an abnormal number of views (greater than 2000), and the ads that receive few phone requests (less than 100). Finally, we receive 225 ads of ad clicks."}, {"heading": "B. Comparative Study on Links Selection", "text": "We simulate the multi-stage feedback structures in the second level compared to the first level. (...) We simulate the two-stage feedback structures in the second level. (...) We simulate the multi-stage feedback processes in the second level. (...) We simulate the multi-stage feedback processes in the second level. (...) We simulate the multi-stage feedback processes in the third level. (...) We simulate the multi-stage feedback processes in the third level. (...) We simulate the multi-stage feedback processes in the third level. (...) We simulate the multi-stage feedback processes in the third level. (...) We simulate the multi-stage feedback processes in the third level. (...) We simulate the multi-stage feedback processes in the third level."}, {"heading": "V. RELATED WORK", "text": "A common approach to selecting links is to do A / B tests [12], which split traffic for multiple links on two different websites, and then evaluate their rewards. However, our algorithm can be seen as a complementary approach to testing A / B, for example, our algorithm can select the number of links over a certain level and allow for a more efficient selection of links."}, {"heading": "VI. CONCLUSION", "text": "To the best of our knowledge, we are the first to model the link selection problem with multi-level feedback structures as a stochastically limited bandit problem. We propose and design an effective link selection algorithm LExp with verifiable sublinear regrets and injury limits. Furthermore, we demonstrate how to learn / reduce the multi-level reward structures of web links in two real data sets. We conduct extensive experiments to compare LExp with state-of-the-art context-free algorithms and show that LExp is superior in selecting web links with limited multi-level feedback by balancing both remorse and injury."}], "references": [{"title": "Seven rules of thumb for web site experimenters", "author": ["R. Kohavi", "A. Deng", "R. Longbotham", "Y. Xu"], "venue": "Proceedings of SIGKDD, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of active learning in collaborative filtering recommender systems", "author": ["M. Elahi", "F. Ricci", "N. Rubens"], "venue": "Computer Science Review, vol. 20, pp. 29\u201350, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Trackmeornot: Enabling flexible control on web tracking", "author": ["W. Meng", "B. Lee", "X. Xing", "W. Lee"], "venue": "Proceedings of WWW, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Combinatorial multi-armed bandit: General framework, results and applications", "author": ["W. Chen", "Y. Wang", "Y. Yuan"], "venue": "Proceedings of ICML, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for adversarial bandit problems with multiple plays", "author": ["T. Uchiya", "A. Nakamura", "M. Kudo"], "venue": "Proceedings of ACL\u201910, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "On the existence of fast approximation schemes, ser", "author": ["B. Korte", "R. Schrader"], "venue": "Reprint series. Inst. fu\u0308r O\u0308konometrie u. Operations- Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Dependent rounding and its applications to approximation algorithms", "author": ["R. Gandhi", "S. Khuller", "S. Parthasarathy", "A. Srinivasan"], "venue": "Journal of the ACM (JACM), vol. 53, no. 3, pp. 324\u2013360, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple playspart i: Iid rewards", "author": ["V. Anantharam", "P. Varaiya", "J. Walrand"], "venue": "IEEE Transactions on Automatic Control, vol. 32, no. 11, pp. 968\u2013976, 1987.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1987}, {"title": "Avito context ad clicks", "author": ["Kaggle"], "venue": "2015, https://www.kaggle.com/c/ avito-context-ad-clicks.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupon purchase prediction", "author": ["\u2014\u2014"], "venue": "2016, https://www.kaggle.com/c/ coupon-purchase-prediction.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic multi-armed-bandit problem with non-stationary rewards", "author": ["O. Besbes", "Y. Gur", "A. Zeevi"], "venue": "Proceedings of NIPS, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Trustworthy analysis of online a/b tests: Pitfalls, challenges and solutions", "author": ["A. Deng", "J. Lu", "J. Litz"], "venue": "Proceedings of WSDM, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R. Schapire"], "venue": "Proceedings of WWW, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative filtering with low regret", "author": ["G. Bresler", "D. Shah", "L.F. Voloch"], "venue": "Proceedings of ACM SIGMETRICS, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative filtering bandits", "author": ["S. Li", "A. Karatzoglou", "C. Gentile"], "venue": "Proceedings of SIGIR, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput., vol. 32, no. 1, Jan. 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "author": ["J. Komiyama", "J. Hondaand", "H. Nakagawa"], "venue": "ICML, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Gathering additional feedback on search results by multi-armed bandits with respect to production ranking", "author": ["A. Vorobev", "D. Lefortier", "G. Gusev", "P. Serdyukov"], "venue": "Proceedings of WWW, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Banditbased algorithms for budgeted learning", "author": ["K. Deng", "C. Bourke", "S. Scott", "J. Sunderman", "Y. Zheng"], "venue": "Proceedings of ICDM, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Budgeted multi-armed bandits with multiple plays", "author": ["Y. Xia", "T. Qin", "W. Ma", "N. Yu", "T.-Y. Liu"], "venue": "Proceedings of IJCAI, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives", "author": ["S. Agrawal", "N.R. Devanur", "L. Li", "N. Rangarajan"], "venue": "Proceedings of COLT, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Online decision making under stochastic constraints", "author": ["M. Mahdavi", "T. Yang", "R. Jin"], "venue": "NIPS workshop on Discrete Optimization in Machine Learning, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Both the total attractiveness and the total compound revenue of a homepage are important measures for investors to assess the value of a website [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": ", the users\u2019 preferences) is not always available due to visits from casual users, cold start [2] or cookie blocking [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": ", the users\u2019 preferences) is not always available due to visits from casual users, cold start [2] or cookie blocking [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "(iv) We show that LExp is more effective in links selection than two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "M [5], via extensive experiments on two real-world datasets (Sec.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "(1) Problem (1) is known to be NP-hard [6].", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "S \u2032 = { x \u2208 [0, 1] \u2223\u2223\u2211K i=1 xiAi \u2265 h, \u2211K", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Without loss of generality, we normalize Ai(t) \u2208 [0, 1] and Bi(t) \u2208 [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Without loss of generality, we normalize Ai(t) \u2208 [0, 1] and Bi(t) \u2208 [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": ", x t K) be the probabilistic selection vector of the K arms at time t, where xi \u2208 [0, 1] is the probability of selecting of the arm i at time t.", "startOffset": 83, "endOffset": 89}, {"referenceID": 6, "context": "At time t, a set of L \u2264 K arms It \u2208 K is selected via a dependent rounding procedure [7], which guarantees the probability that i \u2208 It is xi at time t (see Sec.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "2If h = 0, the problem is equivalent to the classic unconstrained multiple play multi-armed bandit problem (MP-MAB) [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Let x be an arbitrary probabilistic selection vector which satisfies xi \u2208 [0, 1], 1xt = L and aTx \u2265 h.", "startOffset": 74, "endOffset": 80}, {"referenceID": 8, "context": "In this section, we first examine the web links\u2019 multilevel feedback structures in two real-world datasets from the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to as \u201cAd-Clicks\u201d and \u201cCoupon-Purchase\u201d in this paper.", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "In this section, we first examine the web links\u2019 multilevel feedback structures in two real-world datasets from the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to as \u201cAd-Clicks\u201d and \u201cCoupon-Purchase\u201d in this paper.", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "Then we conduct a comparative study by applying LExp and two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "M [5], to show the effectiveness of LExp in links selection.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "We normalize the numbers of views of each ad to the interval [0, 1] using min-max scaling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "We then normalize all the browsed times to [0, 1] using min-max scaling and refer to the normalized browsed times as to the CTRs of the coupons.", "startOffset": 43, "endOffset": 49}, {"referenceID": 10, "context": "In particular, for each of the 225 ads in Ad-Clicks, we treat its CTR/1st level feedback as a Bernoulli random variable with the mean CTR taking from Ad-Clicks, and we vary the Phone Request Rate/2nd level reward over time in a similar fashion as a sinusoidal wave (similar to [11]): the 2nd level reward starts from a random value drawn uniformly from 0 to the mean Phone Request Rate taking from Ad-Clicks; then in each time slot, it increases or decreases at rate 10/T until reaching the mean or 0.", "startOffset": 277, "endOffset": 281}, {"referenceID": 11, "context": "One common approach to the links selection problem is to perform A/B testing [12], which splits the traffic for different sets of links on two different web pages, and then evaluate their rewards.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "[13] first formulated a contextual bandit problem aiming at selecting articles/links that maximize the total number of clicks based on the user-click feedback.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Recently, [14] and [15] incorporated the collaborative filtering method into contextual bandit algorithms using users\u2019 profiles to recommend web links.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Recently, [14] and [15] incorporated the collaborative filtering method into contextual bandit algorithms using users\u2019 profiles to recommend web links.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "However, contextual information is not always available due to cold start [2] or blocking of cookie tracking [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "However, contextual information is not always available due to cold start [2] or blocking of cookie tracking [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "[5] presented the EXP3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "M bandit algorithm that extends the single-played EXP3 algorithm [16] to multiple-played cases using exponential weights.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "[4] proposed an algorithm that selects multiple arms with the highest upper confidence bound (UCB) indices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] presented the multiple-play Thompson Sampling algorithm (MP-TS) for arms with binary rewards.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] proposed a bandit-based ranking algorithm for ranking search queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "Finally, our constrained bandit problem is related but different from the bandit model considered in [22] which tries to balance regret and violation.", "startOffset": 101, "endOffset": 105}], "year": 2017, "abstractText": "Selecting the right web links for a website is important because appropriate links not only can provide high attractiveness but can also increase the website\u2019s revenue. In this work, we first show that web links have an intrinsic multilevel feedback structure. For example, consider a 2-level feedback web link: the 1st level feedback provides the Click-Through Rate (CTR) and the 2nd level feedback provides the potential revenue, which collectively produce the compound 2-level revenue. We consider the context-free links selection problem of selecting links for a homepage so as to maximize the total compound 2-level revenue while keeping the total 1st level feedback above a preset threshold. We further generalize the problem to links with n (n\u22652)-level feedback structure. To our best knowledge, we are the first to model the links selection problem as a constrained multi-armed bandit problem and design an effective links selection algorithm by learning the links\u2019 multi-level structure with provable sub-linear regret and violation bounds. We uncover the multi-level feedback structures of web links in two real-world datasets. We also conduct extensive experiments on the datasets to compare our proposed LExp algorithm with two state-of-the-art context-free bandit algorithms and show that LExp algorithm is the most effective in links selection while satisfying the constraint.", "creator": "LaTeX with hyperref package"}}}