{"id": "1709.02738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Cycles in adversarial regularized learning", "abstract": "Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system's behavior is Poincar\\'e recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents' choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents' utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.", "histories": [["v1", "Fri, 8 Sep 2017 15:16:54 GMT  (1106kb,D)", "http://arxiv.org/abs/1709.02738v1", "22 pages, 4 figures"]], "COMMENTS": "22 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["panayotis mertikopoulos", "christos papadimitriou", "georgios piliouras"], "accepted": false, "id": "1709.02738"}, "pdf": {"name": "1709.02738.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["panayotis.mertikopoulos@imag.fr,", "christos@berkeley.edu,", "georgios@sutd.edu.sg."], "sections": [{"heading": "1. Introduction", "text": "Regularization is a fundamental and incisive method of optimization, its current zeitgeist and its entry into machine learning are further costs. By introducing a new component in the objective, regulatory techniques overcome ill-conditioning and overadaptation, and they result in algorithms that achieve regret and thrift without sacrificing efficiency. [2, 5, 8] In the context of online optimization, these characteristics are exemplified in the family of learning algorithms known as \"Follow the Regularized Leader.\" [41] FoReL represents an important archetype of adaptive behavior for several reasons: it provides optimal minimum requirements and regret guarantees (O \u2212 1 / 2) in an adversarial setting, it offers significant flexibility with respect to the geometry of the problem at hand, and it captures numerous other dynamics as special cases (hedges, multiplicative weights, gradients, etc.)."}, {"heading": "2. Definitions from game theory", "text": "We start with some basic definitions from game theory. A finite game in its normal form consists of a finite number of players N = {1,. \u2212 N, each with a finite number of actions (or strategies) Ai. Player i's preferences for an action over another action are determined by an associated payout function. \u2212 N, which assigns a reward to each player. \u2212 A, u, i) Players can also use mixed strategies, i.e. mixed probability distributions xi = (xixi)."}, {"heading": "3. No-regret learning via regularization", "text": "In this case, even if the game allows for a uniform Xi balance, it is not reasonable to assume that each player is able to pre-calculate his component of an equilibrium strategy - let alone assume that all players are completely rational, that there is a shared knowledge of the best possible strategy of the player, etc. With this in mind, we only assume that each player is trying to at least minimize his \"regret,\" i.e. the average payout difference between a player's mixed strategy at the time t 0 and the best possible strategy of the player in the form that the game evolves over time, the player's regret i along the sequence of the game x (t) is defined asRegi (t) = max."}, {"heading": "4. Recurrence in adversarial regularized learning", "text": "In this section, our goal is to take a closer look at the ramifications of the rapid regret system (FoReL) beyond convergence to the coarse correlated equilibrium group. In fact, as is generally known, this sentence is quite large and may contain quite unrationalizable strategies: for example, Viossat and Zapechelnyuk [46] have recently shown that a coarse correlated equilibrium mapping could only trace the positive probability of selection to strictly dominated strategies. Furthermore, the time limitation inherent in the definition of players leaves open the possibility of complex daily behaviors, e.g. periodicity, recurrence, boundary cycles or chaos. Motivated by these, we examine the long-term behavior of (FoReL) in the popular setting of zero-sum games (with or without inner equilibrium) and several extensions."}, {"heading": "5. Conclusions", "text": "Our results show that the behavior of regulated learning in adversarial environments is much more complicated than the strong no-regrets characteristics of FoReL suggest at first glance. Although the empirical frequency of the game under FoReL converges with the number of roughly correlated balances (possibly at higher speeds, depending on the game structure), the actual course of the game under FoReL is recurring and shows cycles in zero-sum games. We find this characteristic particularly interesting because it suggests that \"black box\" guarantees are not the be-all and end-all of learning in games: the theory of dynamic systems is brimming with complex phenomena and ideas that naturally occur when one examines the behavior of learning algorithms more closely."}, {"heading": "Appendix A. Examples of FoReL dynamics", "text": "Example A.1 (Multiplicative weights and replicator dynamics) Perhaps the most well-known example of a regularized selection map is the so-called Logit selection map, which was first examined in the context of McFadden's discrete selection theory [22] and then leads to the multiplicative weights (MW). Dynamics: 8y: i = vi (x), xi = i (yi), xi = i (yi) (MW). (MW) This selection model was first developed in the context of McFadden's discrete selection theory (yxi) and then leads to multiplicative weighting (x). (MW) As is generally known, the logit map above is achieved by the model (3,2) taking into account entropic regulation strategies (x)."}, {"heading": "Appendix B. Liouville\u2019s formula and Poincar\u00e9 recurrence", "text": "s formula can be applied to any system of autonomous differential equations with a continuously differentiable vector field in an open domain of S-k.The divergence of B at x-S is defined as the trace of the corresponding Jacobic volume at x, i.e., div (x) = [x0, t] is the image of A under map. Since divergence is a continuous function, we can be integral about measurable quantities A-S. Given such a set A, leave A (t) = (x0, t) the image of A under map at a given time. A (t) is measurable and is volume vol [A (t)] = [A (t) dx."}, {"heading": "Appendix C. Technical proofs", "text": "The first result that we demonstrate in this appendix is a central technical problem in relation to the development of the coupling function (< hi = > Qi) (4,1): Lemma C.1. Let pi's and let Gi (yi) = h \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\""}, {"heading": "Appendix D. Auxiliary results", "text": "In this appendix we provide two additional results that are used in the sequence of Theorem 4.2. The first shows that if the difference between two strategies becomes large, the strategy with the lower number of points is extinct: Let A be a finite set and let A be a regulator on X. (A) If the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the sequin the"}], "references": [{"title": "A note on strictly competitive games", "author": ["I. Adler", "C. Daskalakis", "C.H. Papadimitriou"], "venue": "in WINE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Singular Riemannian barrier methods and gradient-projection dynamical systems for constrained optimization, Optimization", "author": ["H. Attouch", "J. Bolte", "P. Redont", "M. Teboulle"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Poincare recurrence: old and new, in XIVth", "author": ["L. Barreira"], "venue": "International Congress on Mathematical Physics. World Scientific.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Lectures on modern convex optimization: analysis, algorithms, and engineering", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Zero-sum polymatrix games: A generalization of minmax", "author": ["Y. Cai", "O. Candogan", "C. Daskalakis", "C. Papadimitriou"], "venue": "Mathematics of Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "On minmax theorems for multiplayer games", "author": ["Y. Cai", "C. Daskalakis"], "venue": "in ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Lugoisi, Prediction, Learning, and Games", "author": ["G.N. Cesa-Bianchi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Near-optimal no-regret algorithms for zero-sum games, in Proceedings of the Twenty-second", "author": ["C. Daskalakis", "A. Deckelbaum", "A. Kim"], "venue": "Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "On a network generalization of the minmax theorem, in ICALP 2009", "author": ["C. Daskalakis", "C.H. Papadimitriou"], "venue": "Proceedings of the 2009 International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning in games: Robustness of fast convergence, in Advances in Neural Information", "author": ["D.J. Foster", "T. Lykouris", "K. Sridharan", "E. Tardos"], "venue": "Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Linear Programming and the Theory of Games - Chapter XII) in Koopmans, Activity Analysis of Production and Allocation", "author": ["D. Gale", "H. Kuhn", "A.W. Tucker"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1951}, {"title": "Generative adversarial nets, in Advances in neural information processing", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Introduction to online convex optimization, Foundations and Trends\u00ae", "author": ["E. Hazan"], "venue": "in Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Evolutionary Games and Population Dynamics", "author": ["J. Hofbauer", "K. Sigmund"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Kiwiel, Free-steering relaxation methods for problems with strictly convex costs and linear constraints", "author": ["C. K"], "venue": "Mathematics of Operations Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "A continuous-time approach to online optimization", "author": ["J. Kwon", "P. Mertikopoulos"], "venue": "Journal of Dynamics and Games,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "The projection dynamic and the geometry of population", "author": ["R. Lahkar", "W.H. Sandholm"], "venue": "games, Games and Economic Behavior,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Conditional logit analysis of qualitative choice behavior, in Frontiers in Econometrics, P. Zarembka, ed", "author": ["D.L. McFadden"], "venue": "CYCLES IN ADVERSARIAL REGULARIZED", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1974}, {"title": "The emergence of rational behavior in the presence of stochastic perturbations", "author": ["P. Mertikopoulos", "A.L. Moustakas"], "venue": "The Annals of Applied Probability,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Learning in games via reinforcement and regularization", "author": ["P. Mertikopoulos", "W.H. Sandholm"], "venue": "Mathematics of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Projected dynamical systems in the formulation, stability analysis, and computation of fixed demand traffic network equilibria", "author": ["A. Nagurney", "D. Zhang"], "venue": "Transportation Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos, ArXiv e-prints, (2017)", "author": ["G. Palaiopanos", "I. Panageas", "G. Piliouras"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "From nash equilibria to chain recurrent sets: Solution concepts and topology, in ITCS, 2016", "author": ["C. Papadimitriou", "G. Piliouras"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Persistent patterns: Multi-agent learning beyond equilibrium and utility, in AAMAS", "author": ["G. Piliouras", "C. Nieto-Granda", "H.I. Christensen", "J.S. Shamma"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Optimization despite chaos: Convex relaxations to complex limit sets via poincar\u00e9 recurrence", "author": ["G. Piliouras", "J.S. Shamma"], "venue": "Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Optimization, learning, and games with predictable sequences, in Advances in Neural Information", "author": ["S. Rakhlin", "K. Sridharan"], "venue": "Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Variational Analysis, vol. 317 of A Series of Comprehensive Studies", "author": ["R.T. Rockafellar", "R.J.B. Wets"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Intrinsic robustness of the price of anarchy", "author": ["T. Roughgarden"], "venue": "in Proc. of STOC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Optimal properties of stimulus-response learning models", "author": ["A. Rustichini"], "venue": "Games and Economic Behavior,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Population Games and Evolutionary Dynamics", "author": ["W.H. Sandholm"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Chaos in learning a simple two-person game", "author": ["Y. Sato", "E. Akiyama", "J.D. Farmer"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "Deep learning games, in Advances in Neural Information", "author": ["D. Schuurmans", "M.A. Zinkevich"], "venue": "Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Convex repeated games and Fenchel duality, in Advances in Neural Information", "author": ["S. Shalev-Shwartz", "Y. Singer"], "venue": "Processing Systems", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Exponential weight algorithm in continuous time", "author": ["S. Sorin"], "venue": "Mathematical Programming,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Fast convergence of regularized learning in games", "author": ["V. Syrgkanis", "A. Agarwal", "H. Luo", "R.E. Schapire"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Evolutionarily stable strategies with two types of player", "author": ["P.D. Taylor"], "venue": "Journal of Applied Probability,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1979}, {"title": "Evolutionary stable strategies and game dynamics", "author": ["P.D. Taylor", "L.B. Jonker"], "venue": "Mathematical Biosciences,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1978}, {"title": "Evolutionary Game Theory, MIT Press; Cambridge, MA: Cambridge University Press", "author": ["J.W. Weibull"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 4, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 7, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 34, "context": "In the context of online optimization, these features are exemplified in the family of learning algorithms known as \u201cFollow the Regularized Leader\u201d (FoReL) [41].", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 7, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 13, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 28, "context": "The second involves proving some useful property of the game\u2019s CCE: For instance, leveraging (\u03bb, \u03bc)-robustness [33] implies that the social welfare at a CCE lies within a small constant of the optimum social welfare; as another example, the product of the marginal distributions of CCE in zero-sum games is Nash.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "[9] and Rakhlin and Sridharan [31] developed classes of dynamics that enjoy a O(log t/t) regret minimization rate in two-player zerosum games.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[9] and Rakhlin and Sridharan [31] developed classes of dynamics that enjoy a O(log t/t) regret minimization rate in two-player zerosum games.", "startOffset": 30, "endOffset": 34}, {"referenceID": 36, "context": "[43] further analyzed a recency biased variant of FoReL in more general multi-player games and showed that it is possible to achieve an O(t\u22123/4) regret minimization rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The social welfare converges at a rate of O(t\u22121), a result which was extended to standard versions of FoReL dynamics in [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "Indeed, convergent, recurrent, and even chaotic [26] systems may exhibit equally strong regret minimization properties in general games, so the question remains: What does the long-run behavior of FoReL look like, really? This question becomes particularly interesting and important under perfect competition (such as zero-sum games and variants thereof).", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "Finally, zero-sum games have also been used quite recently as a model for deep learning optimization techniques in image generation and discrimination [14, 39].", "startOffset": 151, "endOffset": 159}, {"referenceID": 32, "context": "Finally, zero-sum games have also been used quite recently as a model for deep learning optimization techniques in image generation and discrimination [14, 39].", "startOffset": 151, "endOffset": 159}, {"referenceID": 0, "context": "Importantly, the observed cycling behavior is robust to the agents\u2019 choice of regularization mechanism (each agent could be using a different regularizer), and it applies to any positive affine transformation of zero-sum games (and hence all strictly competitive games [1]) even though these transformations lead to different trajectories of play.", "startOffset": 269, "endOffset": 272}, {"referenceID": 5, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 6, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 9, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 37, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 148, "endOffset": 160}, {"referenceID": 38, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 148, "endOffset": 160}, {"referenceID": 20, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 189, "endOffset": 201}, {"referenceID": 14, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 30, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 39, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 14, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 24, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 25, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 31, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 33, "context": "1A standard trick is to decrease step-sizes by a constant factor after a window of \u201cdoubling\u201d length [40].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 6, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 9, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 16, "context": "1 to Appendix C; we also refer to [20] for a similar regret bound for (FoReL) in the context of online convex optimization.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "3) represents a striking improvement over the \u0398(t\u22121/2) worst-case bound for FoReL in discrete time [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 23, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 25, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 31, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 2, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 15, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 20, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 33, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 0, "context": "for all xi, xi \u2208 Xi and for all t \u2208 [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "For example, this class of games contains all strictly competitive games [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 18, "context": "This choice model was first studied in the context of discrete choice theory by McFadden [22] and it leads to the multiplicative weights (MW) dynamics:8 \u1e8fi = vi(x), xi = \u039bi(yi).", "startOffset": 89, "endOffset": 93}, {"referenceID": 38, "context": "This equation describes the replicator dynamics of [45], the most widely studied model for evolution under natural selection in population biology and evolutionary game theory.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 20, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 35, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 1, "context": "For more details about (MWU), we refer the reader to [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 20, "context": "over all intervals for which the support of x(t) remains constant [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The dynamics (PD) were introduced in game theory by [12] as a geometric model of the evolution of play in population games; for a closely related approach, see also [21, 25] and references therein.", "startOffset": 165, "endOffset": 173}, {"referenceID": 21, "context": "The dynamics (PD) were introduced in game theory by [12] as a geometric model of the evolution of play in population games; for a closely related approach, see also [21, 25] and references therein.", "startOffset": 165, "endOffset": 173}, {"referenceID": 3, "context": "Poincar\u00e9 Recurrence: [4, 30] If a flow preserves volume and has only bounded orbits then for each open set there exist orbits that intersect the set infinitely often.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "The key step in proving this characterization is Farkas\u2019 lemma; the version we employ here is due to Gale, Kuhn and Tucker [13]):", "startOffset": 123, "endOffset": 127}], "year": 2017, "abstractText": "Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system\u2019s behavior is Poincar\u00e9 recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents\u2019 choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents\u2019 utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.", "creator": "pdfLaTeX"}}}