{"id": "1709.02753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Privacy Loss in Apple's Implementation of Differential Privacy on MacOS 10.12", "abstract": "In June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy.", "histories": [["v1", "Fri, 8 Sep 2017 16:00:14 GMT  (2569kb,D)", "http://arxiv.org/abs/1709.02753v1", null], ["v2", "Mon, 11 Sep 2017 05:18:29 GMT  (2758kb,D)", "http://arxiv.org/abs/1709.02753v2", null]], "reviews": [], "SUBJECTS": "cs.CR cs.CY cs.LG", "authors": ["jun tang", "aleksandra korolova", "xiaolong bai", "xueqiang wang", "xiaofeng wang"], "accepted": false, "id": "1709.02753"}, "pdf": {"name": "1709.02753.pdf", "metadata": {"source": "CRF", "title": "Privacy Loss in Apple\u2019s Implementation of Differential Privacy on MacOS 10.12", "authors": ["Jun Tang", "Aleksandra Korolova", "Xiaolong Bai", "Xueqiang Wang", "Xiaofeng Wang"], "emails": ["juntang@usc.edu", "korolova@usc.edu", "bxl12@mails.tsinghua.edu.cn", "xw48@indiana.edu", "xw7@indiana.edu"], "sections": [{"heading": null, "text": "In this paper, through a combination of experiments, static and dynamic code analysis of the macOS Sierra (version 10.12) implementation, we examine the choices Apple has made to manage the privacy budget. We discover and describe Apple's set-up for differentiated private data processing, including the entire data pipeline, the parameters used to differentiate private disruption to each piece of data, and the frequency with which such data is sent to Apple's servers. We find that although Apple's deployment ensures that the (differentiated) data loss for each date transmitted to its servers is 1 or 2, the overall data loss allowed by the system is significantly higher, up to 16 per day for the four originally announced applications of emojis, new words, deeplinks, and lookup hints [21]. Furthermore, Apple renews the privacy budget available every day, resulting in a potential loss of privacy of 16 times as many days as the loss of privacy of Tfeniva decides to collect private data differently."}, {"heading": "1 INTRODUCTION", "text": "Differential privacy [7] has been widely recognized as the leading statistical privacy definition by the academic community [6, 11]. Therefore, Apple's deployment as one of the first large-scale commercial deployment of differential privacy (preceded only by Google's RAPPOR [10]) is of considerable interest to privacy theorists and practitioners alike. Moreover, since Apple can be perceived as a competitor to privacy with other consumer companies, the understanding of the actual privacy provided by the use of differential privacy in its desktop and mobile operating systems is of equal interest to consumers and consumer advocacy groups [16]. However, Apple's public communications on the use of differential privacy are extremely limited: Neither its developer documents [1, 2, 21, 22, 24] nor interstitials that cause users to choose to collect differentiated private data (Figures 8 and 9) provide details of the technology, except to say which data types it can be applied to."}, {"heading": "1.1 The (Differential) Privacy Budget", "text": "One of the key distinctions between differential privacy (DP) and colloquial privacy terms is that the definition provides a way to quantify the privacy risk that arises when a differentiated private algorithm is used. Typically, the loss of privacy is referred to as privacy and measured quantitatively by the algorithm by how much the risk to individual privacy can increase due to that person's inclusion in the algorithm. The higher the value of the algorithm, the less privacy protection it will provide; in particular, the increase in privacy risks is proportional to Exp."}, {"heading": "1.2 Our Findings", "text": "We note that although the loss of privacy per date is strictly limited to data protection budgets normally used in the literature, the daily loss of privacy allowed by the implementation exceeds values normally considered acceptable by the theoretical community [12], and the loss of privacy per device as a whole can be unlimited (Section 4)."}, {"heading": "2 OVERVIEW", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 System Components", "text": "We begin by listing the components of the DP system on Mac OS that we have identified: \u2022 The differential privacy framework is located at / System / Library / PrivateFrameworks / DifferentialPrivacy.framework. The framework contains code that implements differential privacy, which we decompile with Hopper Disassembler. Specifically, it contains code that is responsible for privatization by date and for periodic functions that manage the privacy budget, updates the database for privatized data, and the creation of report files that are sent to Apple servers. \u2022 The com.apple.dprivacyd daemon located at / usr / libexec / dprivacyd. We will study it with codetracing with LLDB. \u2022 A database that contains at / private / var / db / DifferentialPrivacy, which contains several tables of privatized records and a table that is related to the available budget."}, {"heading": "2.2 System Organization and Data Pipeline", "text": "The dprivacy (com.apple.dprivacyd) daemon executes the system responsible for implementing differential privacy. As soon as a user opts for differentiated private data collection in MacOS security and privacy settings (Figure 8), the dprivacy daemon is activated and the database that supports relevant data storage and management is created in / var / db / DifferentialPrivacy. In addition, there is a message on the console: \"dprivacyd: Take Work Now.\" According to Apple's original announcement [1, 21, 23], the use of DP focuses on four applications: new words, emojis, deep links and references in Notes, with iCloud data added as an additional application in early 2017 [2], and other types of data collection such as health data that were introduced in mid-2017 [24]."}, {"heading": "2.3 Study Questions", "text": "In order to understand the loss of privacy at the introduction of differential privacy by Apple, we need to understand the following aspects of the system: (1) What privacy parameters are used to achieve privatization before entering the privatized date into the database? This allows us to understand the privacy per date.2To reliably launch a DP application on emojis, the user must invoke the Emoji keyboard by pressing \"ctrl-cmd space,\" then click on an emoji (or select an emoji with the arrow key and press \"Enter\").For new words, the user can type a misspelled word into the Notes app and then ignore the spelling suggestion by pressing \"esc.\" (2) How often are records selected for inclusion in a report? How many records can be included in a report? How often are the reports created and filed? This allows us to modify the rate of data loss (3) the database available to modify (4 Is it possible to modify the database)."}, {"heading": "3 SYSTEM\u2019S DETAILS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Database", "text": "The ZOBHRECORD tables (see Figure 1) and ZCMSRECORD tables in the database store the malfunctioning data, with the former dedicated to the privatized emoji records, and the latter - the privatized new word records. Each emoji entered by a user is privatized and stored in the ZOBHRECORD table. By contrast, only words that were not previously typed are privatized and stored in the ZCMSRECORD table. One noteworthy table is ZPRIVACYBUDGETRECORD, whose scheme and sample content is shown in Figure 2. The table contains 7 entries, one for each application (NewWords, Emoji, AppDeepLink, Search and Health) and two for auxiliary functions (standard and test budget)."}, {"heading": "3.2 Configuration Files", "text": "There are four keyboard configuration files for the DP daemon: com.apple.dprivacyd.EN = Keyboard = Keynames, Keyproperties, algorithmparameters, budgetproperties}.plist. See Figures 4, 5, 6 and 7 for snippets of the configuration files, Figure 3 for a schematic relationship between keys in them, and Tables 1 and 2 for a snippet of their values.3.2.1 Keyboards of its values of their values.2 Keynames of the Properties, also Properties 1 and 2 for a snippet of their values.2 Properties of its properties, e.apple.apple.keynants Properties of the Properties of the Properties of its Properties, com.apple.Newsrpnbsp _ 2 Properties of its property types, com.apple.2 Properties of the Properties of the Properties of their values of their values.3 - a new word in English."}, {"heading": "4 PRIVACY LOSS FINDINGS", "text": "We will now answer the questions raised in Section 2.3."}, {"heading": "4.1 Each Datum\u2019s Privatization", "text": "PrivacyParameter of the KeyName sets the privacy parameter that is used to privatize thatKeyName data before it is included in the database. For example, the privatization algorithm for an emoji in French, Russian or English is OneBitHistogram with PrivacyParameter 1. For a new word in English that uses the US keyboard or a new word in Russian, the privatization algorithm that is executed is CountMedianSketch with PrivacyParameter 2, etc. See Table 1 for the values of PrivacyParameters used for the different data types in Mac OS 10.12.3. It is difficult to understand and verify the correctness of privatization algorithms if you only have access to the binary code. We expect the algorithms to implement the ideas described in the patents [17-19]."}, {"heading": "4.2 Report Generation and Privacy Budget Management over Time", "text": "This year is the highest in the history of the country."}, {"heading": "5 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Report File and Database Maintenance", "text": "In addition to the periodic tasks of ReportGenerator and PrivacyBudgetMaintenance, whose actions have already been described in Section 4.2, the following three periodic tasks are responsible for managing databases and report files (Figure 10): \u2022 StorageCulling (every 24 hours): Deletes submitted records and records with the wrong version number from the database. \u2022 StorageMaintenance (every 12 hours): Deletes records to limit database size and deletes records added to the database more than two weeks before the current date. \u2022 ReportFilesMaintenance (every 24 hours): Removes report files older than one month6 from the hard disk."}, {"heading": "5.2 Ease of Altering System\u2019s Performance", "text": "We have observed several precautions taken by Apple to prevent misuse of the implementation: \u2022 The configuration files are difficult to change because such a change on Mac OS requires disabling Apple's system integrity protection, which is not trivial. \u2022 We have not found a way to change the configuration files on iOS. \u2022 Even if it succeeds in changing the configuration files, if a privacy parameter in a set6Concluded configuration file is based on observation and dynamic code analysis, as we could not find it in the framework code.to, a value higher than epsilonMax, a constant value equal to 2 encoded in the code of the framework (Figure 13), the Privacy parameter used is reset to 1 at runtime and the corresponding record priority is set to 99999, effectively ensuring that it is not included in the report files (Figure 13). In addition, the corresponding data set to 99999 will not be included in the report files, which can be set to 999, and the corresponding data set to 9999 will not be used in the report file."}, {"heading": "5.3 Configuration Differences between MacOS versions", "text": "We have observed that Apple has changed the configuration files from MacOS 10.12.1 to 10.12.3 (see Table 3). The main differences are that in MacOS 10.12.3: \u2022 Session amount for com.apple.keyboard.NewWords is increased from 1 to 2, resulting in a higher daily loss of privacy. \u2022 BudgetName com.apple.health and PropertiesName LocalWords are introduced, signaling new applications for DP. \u2022 SubmissionPriority is introduced, signaling new safeguards against abuse. \u2022 Health-related PropertiesName and Budget are introduced, signaling that health-related data will also be included in DP data collection."}, {"heading": "5.4 A Note on iOS", "text": "The iOS implementation follows the same principles as the MacOS implementation. The configuration files for iOS 10.1.1 that we received from a jailbroken phone were identical to those for MacOS 10.12.1. The differences we found relate to iOS reports that contain more metrics (in particular, we were unable to trigger DeepLink functionality on MacOS, while there are plenty of such records on iOS), and to deleting files from the phone faster than from the computer (7 vs 30 days)."}, {"heading": "6 CONCLUSIONS AND FUTUREWORK", "text": "We applaud Apple for its use of differentiated privacy models and for the many safeguards that have been put in place to make abuse more difficult, but we believe that there are several significant flaws in its use. (1) The loss of data that the system allows is nowhere explained and takes considerable efforts to protect privacy. Furthermore, the lack of transparency opens the door to deliberate or inadvertent misuse by Apple itself by unilaterally collecting private data based on the risk announced by the data producer. (2) The lack of transparency regarding privacy opens the door to intentional or accidental misuse of Apple itself by unilaterally altering the data."}, {"heading": "A APPENDIX A.1 Code Support for Findings", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}], "references": [{"title": "Comment: Differential privacy and data collection is still not clearly defined as optin on iOS", "author": ["Greg Barbosa"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Proving Differential Privacy in Hoare Logic", "author": ["Gilles Barthe", "Marco Gaboardi", "Emilio Jes\u00fas Gallego Arias", "Justin Hsu", "C\u00e9sar Kunz", "Pierre-Yves Strub"], "venue": "In Proceedings of the 2014 IEEE 27th Computer Security Foundations Symposium (CSF)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Proving Differential Privacy via Probabilistic Couplings", "author": ["Gilles Barthe", "Marco Gaboardi", "Benjamin Gr\u00e9goire", "Justin Hsu", "Pierre-Yves Strub"], "venue": "In Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "A firm foundation for private data analysis", "author": ["Cynthia Dwork"], "venue": "Commun. ACM 54,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference (TCC)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Privacy in Information-Rich Intelligent Infrastructure. ArXiv e-prints (June 2017)", "author": ["C. Dwork", "G.J. Pappas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science 9,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response", "author": ["\u00dalfar Erlingsson", "Vasyl Pihur", "Aleksandra Korolova"], "venue": "In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security (CCS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Differential privacy: An economic method for choosing epsilon", "author": ["Justin Hsu", "Marco Gaboardi", "Andreas Haeberlen", "Sanjeev Khanna", "Arjun Narayan", "Benjamin C Pierce", "Aaron Roth"], "venue": "IEEE Computer Security Foundations Symposium (CSF)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On Significance of the Least Significant Bits for Differential Privacy", "author": ["Ilya Mironov"], "venue": "In Proceedings of the 2012 ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Differential Privacy: A Primer for a Non-technical Audience (Preliminary Version)", "author": ["Kobbi Nissim", "Thomas Steinke", "Alexandra Wood", "Micah Altman", "Aaron Bembenek", "Mark Bun", "Marco Gaboardi", "David O\u2019Brien", "Salil Vadhan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "https: //www.google.com/patents/US9594741 US Patent 9,594,741", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudiger", "V.R. Sridhar", "D. Davidson"], "venue": "Learning new words. (March", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "https: //www.google.com/patents/US9645998 US Patent 9,645,998", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudiger", "V.R. Sridhar", "D. Davidson"], "venue": "Learning new words. (May", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Emoji frequency detection and deep link frequency", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudinger", "V.V. Prakash", "A. Legendre", "S. Duplinsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Differential Privacy: From Theory to Deployment", "author": ["Abhradeep Guha Thakurta"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}], "referenceMentions": [{"referenceID": 11, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 4, "context": "Differential privacy [7] has been widely recognized as the leading statistical data privacy definition by the academic community [6, 11].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "Differential privacy [7] has been widely recognized as the leading statistical data privacy definition by the academic community [6, 11].", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "Thus, as one of the first large-scale commercial deployments of differential privacy (preceded only by Google\u2019s RAPPOR [10]), Apple\u2019s deployment is of significant interest to privacy theoreticians and practitioners alike.", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 12, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 13, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 5, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 14, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 3, "context": "the theoretical computer scientists [6], it is of crucial importance in practical deployments, as the meaning of a privacy risk of exp(1) vs exp(50) is radically different.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Whenever multiple data are submitted with differential privacy, the overall differential privacy loss incurred by that individual is viewed as bounded by the sum of the privacy losses of each of the submissions, due to what is known as composition theorems [9, 15].", "startOffset": 257, "endOffset": 264}, {"referenceID": 10, "context": "Whenever multiple data are submitted with differential privacy, the overall differential privacy loss incurred by that individual is viewed as bounded by the sum of the privacy losses of each of the submissions, due to what is known as composition theorems [9, 15].", "startOffset": 257, "endOffset": 264}, {"referenceID": 5, "context": "In fact, the need to understand the total privacy loss of differential privacy deployments has prompted Dwork and Mulligan to propose an \u201cEpsilon Registry\" [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "We find that although the privacy loss per datum is strictly limited to privacy budgets typically used in the literature, the daily privacy loss permitted by the implementation exceeds values typically considered acceptable by the theoretical community [12], and the overall privacy loss per device may be unbounded (Section 4).", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 12, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 13, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "After t days, by composition theorems [9, 15], the privacy loss incurred will be 1 \u00b7 1 \u00b7 t = t .", "startOffset": 38, "endOffset": 45}, {"referenceID": 10, "context": "After t days, by composition theorems [9, 15], the privacy loss incurred will be 1 \u00b7 1 \u00b7 t = t .", "startOffset": 38, "endOffset": 45}], "year": 2017, "abstractText": "In June 2016, Apple made a bold announcement that it will deploy local differential privacy for some of their user data collection in order to ensure privacy of user data, even from Apple [21, 23]. The details of Apple\u2019s approach remained sparse. Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy. In this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple\u2019s set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple\u2019s servers. We find that although Apple\u2019s deployment ensures that the (differential) privacy loss per each datum submitted to its servers is 1 or 2, the overall privacy loss permitted by the system is significantly higher, as high as 16 per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints [21]. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications. We applaud Apple\u2019s deployment of differential privacy for its bold demonstration of feasibility of innovation while guaranteeing rigorous privacy. However, we argue that in order to claim the benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the daily, weekly, and device lifetime privacy loss permitted. ACM Reference Format: Jun Tang, Aleksandra Korolova, Xiaolong Bai, XueqiangWang, and Xiaofeng Wang. 2017. Privacy Loss in Apple\u2019s Implementation of Differential Privacy", "creator": "LaTeX with hyperref package"}}}