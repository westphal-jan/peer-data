{"id": "1709.02755", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Training RNNs as Fast as CNNs", "abstract": "Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of $h_t$ is blocked until the entire computation of $h_{t-1}$ finishes, which is a major bottleneck for parallel computing. In this work, we propose an alternative RNN implementation by deliberately simplifying the state computation and exposing more parallelism. The proposed recurrent unit operates as fast as a convolutional layer and 5-10x faster than cuDNN-optimized LSTM. We demonstrate the unit's effectiveness across a wide range of applications including classification, question answering, language modeling, translation and speech recognition. We open source our implementation in PyTorch and CNTK.", "histories": [["v1", "Fri, 8 Sep 2017 16:02:30 GMT  (129kb,D)", "http://arxiv.org/abs/1709.02755v1", null], ["v2", "Tue, 12 Sep 2017 20:13:56 GMT  (130kb,D)", "http://arxiv.org/abs/1709.02755v2", "address related work in the new version"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tao lei", "yu zhang"], "accepted": false, "id": "1709.02755"}, "pdf": {"name": "1709.02755.pdf", "metadata": {"source": "CRF", "title": "Training RNNs as Fast as CNNs", "authors": ["Tao Lei", "Yu Zhang"], "emails": ["tao@asapp.com", "yzhang87@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This often involves the use of larger and deeper networks aligned with extensive hyperparameter settings, but the growing model sizes and hyperparameters have greatly increased training time. For example, training a state-of-the-art translation or speech recognition system would take several days to complete (Vaswani et al., 2017; Wu et al., 2016b; Sak et al., 2014). Apparently, the calculation has become a major bottleneck in deep learning research.To counteract the dramatically increased calculation, parallelisations such as GPU accelerated training are mainly used for deep learning (Diamos et al., 2016; Goyal et al., 2017). While operations such as convolution and attention are well suited for multithreaded / GPU computing, parallelisations remain less suitable for parallelization."}, {"heading": "2 Method", "text": "In this section we present the Simple Recurrent Unit (SRU). We start with a basic implementation of a gated recurrent neural network and then present necessary modifications for acceleration. The modifications can be applied to other gated recurrent neural networks, but they are not limited to this particular instance."}, {"heading": "2.1 SRU implementation", "text": "Most powerful recursive neural networks, such as long-term short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) and the gated recurrent unit (GRUs) (Cho et al., 2014), use neural gates to control the flow of information and alleviate the problem of the disappearance (or explosion) of the gradient. Consider a typical implementation, ct = ft ct \u2212 1 + it x-ft ct \u2212 1 + (1 \u2212 ft) x-twhere ft and it is sigmoid gates that are referred to as oblivion gates and entry gate. x-t is the transformed input in step. We choose the coupled version, which is = 1 \u2212 ft here for simplicity reasons. The calculation of x-rt also varies in different RNN instances. We use the simplest version that performs a linear transformation via the input gate."}, {"heading": "2.2 Speeding-up the recurrence", "text": "Existing RNN implementations use the previous output state ht \u2212 1 in the recurrence calculation. For example, the forget vector would be calculated by ft = \u03c3 (Wfxt + Rfht \u2212 1 + bf). The inclusion of Rht \u2212 1 breaks independence and parallelization: Each dimension of the hidden state depends on the other, so the calculation of ht must wait until the entire ht \u2212 1 is available. We propose to drop the connection (between ht \u2212 1 and the neural gates of step t) completely. The associated equations of SRU are given below, x-t = Wxt (3) ft = \u03c3 (Wfxt + bf) (4) rt = \u03c3 (Wrxt + br) (5) ct = ft \u2212 1 + (1 \u2212 ft) x-t (6) ht = rt g (ct) + (1 \u2212 rt) xt (7) xt \u2212 b)."}, {"heading": "2.3 CUDA level optimization", "text": "A naive implementation of SRU into existing deep learning libraries can already achieve over 5x acceleration compared to the na\u00efve LSTM implementation, which is still suboptimal, since implementation based on DL libraries involves a lot of computing effort, such as data copies and kernel launch latencies. In contrast, we implement a version with CUDA optimizations in PyTorch.The optimization of SRU is similar, but much simpler than the cuDNN core functions to speed up their computation. To demonstrate the potential and compare with these implementations, we implement a version with CUDA optimizations in PyTorch.The optimization of SRU is similar, but much simpler than the cuDNN-LSTM functions (Appleyard et al., 2016). Our RNN formulation allows two optimizations that become NRs for the first time."}, {"heading": "3 Experiments", "text": "We evaluate SRU using a variety of benchmarks. These benchmarks are selected to cover a wide range of application scenarios and arithmetic difficulties. Specifically, we train models using text algorithm 1 minibatch version of the forward pass defined in Eq (3) to (7). Input: x [l, i, j], U [l, i, j \u2032], bf [j] and br [j]; initial state c0 [i, j].l = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 n, i = 1, \u00b7 \u00b7 \u00b7 \u00b7, k, j], U [l, j \u2032], bf [j \u2032] and br [j]; initial state c0 [i, j].l = 1, \u00b7 \u00b7 \u00b7, n, i = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, k \u00b7 d tensors. For i = 1, \u00b7 \u00b7 \u00b7 \u00b7, c; k = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, c \u00b7, c \u00b7, c, c, c, c, c, \u00b7, \u00b7, and \u2212."}, {"heading": "3.1 Classification", "text": "Dataset We use 6 classification data sets from (Kim, 2014) 2: Film ratings (MR) (Pang and Lee, 2005), Subjectivity data (SUBJ) (Pang and Lee, 2004), Customer ratings (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), Opinion polarity from MPQA data (Wiebe et al., 2005) and Stanford sentimental Treebank (SST) (Socher et al., 2013) 3. All of these data sets contain several thousand commented sentences. We use the word2vec embedding, which has been trained on 100 billion tokens from Google News, following (Kim, 2014). Word vectors are normalized to unit vectors and are fixed during the training. Setup We train RNN encoders and use the last hidden state to predict the class identifier for a given entry set."}, {"heading": "3.2 Question answering", "text": "Dataset We use the Stanford Question Answering Dataset (SQuAD) dataset (Rajpurkar et al., 2016) as a benchmark. It is one of the largest machine understanding datasets and consists of over 100,000 pairs of questions extracted from Wikipedia articles. We use the standard train and developer sets provided on the official website. We use the open source model of the document reader as described in (Chen et al., 2017) and compare the model variants that use LSTM (original setup) and SRU (our setup). We use the open source PyTorch re-implementation4 of the document reader model. Due to minor implementation differences, this version achieves 1% worse performance compared to those used in (Chen et al., 2017) when using the same training options. Following the authors \"suggestions, we use a lower learning rate (0.0001 instead of 0.0002 for the word miax and 0.5% RAD)."}, {"heading": "3.3 Language modeling", "text": "Data set We use the Penn Treebank Corpus (PTB) as a benchmark for speech modeling. The processed data is taken together with traction, development and test splits (Mikolov et al., 2010), which contains about 1 million tokens with a shortened vocabulary of 10k. According to standard practice, the training data is treated as a long sequence (split into a few chunks for mini-batch training), and therefore the models are trained using shortened back-propagation-through-time (BPTT). Setup Our training configuration largely follows previous work (Zaremba et al., 2014; Gal and Ghahramani, 2016; Zoph and Le, 2016). We use a batch size of 32 and truncated back propagation with 35 steps. The default probability is 0.75% for the input embedding and output softmax layer. The default probability and probability of waste is 2% for the Q175 with the probability of S-1."}, {"heading": "3.4 Machine translation", "text": "This year we have the opportunity to establish ourselves in the region, \"he said.\" We have the opportunity to establish ourselves in the region, \"he said.\" But we will not be able to establish ourselves. \"In the next few years we will be able to establish ourselves.\" In the next few years we will establish ourselves in the region: \"We will establish ourselves in the region, but we will establish ourselves in the region.\" In the next few years we will establish ourselves in the region, \"he said,\" and we will establish ourselves in the region. \""}, {"heading": "3.5 Speech recognition", "text": "I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know."}, {"heading": "4 Conclusion", "text": "This paper presents Simple Recurrent Unit (SRU), a recursive module that runs as fast as CNNs and can be easily scaled to over 10 levels. We perform a comprehensive evaluation of NLP and speech recognition tasks demonstrating the effectiveness of this recurrent unit."}, {"heading": "5 Acknowledgement", "text": "We would like to thank Alexander Rush and Yoon Kim for their help with machine translation experiments and Danqi Chen for their help with SQuAD experiments. We would also like to thank Adam Yala and Yoav Artzi for useful discussions and comments. A special thanks goes to Hugh Perkins for his support in setting up the experimental environment, Runqi Yang for answering questions about his code and the PyTorch community for enabling flexible implementation of neural modules."}, {"heading": "A Additional results and analyses", "text": "A.1 Speech recognition Baseline Table 6 compared different LSTM baseline models. For all models, we follow the configurations reported in the paper Sak et al. (2014) 7. We found that more than 5 layers will significantly increase the word error rate (WHO). We can observe that our best LSTM model (with the least parameters) has 5 layers and each layer contains 1024 memory cells. We will use it as a base model in Section 3.5.Effect of Highway Transform for SRU The dimensions of xt and ht must be equal in Eq. 2. If this is not the case (e.g. the first layer of the SRU), we can perform a linear projection Wlh through the highway connections to adjust the dimensions to layer l: h \u2032 t = rt g (ct) + (1 \u2212 rt) Wlhx.We can also use a quadratic matrix Wlh for each layer."}], "references": [{"title": "Quasi-recurrent neural networks", "author": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher"], "venue": "In ICLR,", "citeRegEx": "Bradbury et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bradbury et al\\.", "year": 2017}, {"title": "Reading Wikipedia to answer open-domain questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Chen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Persistent rnns: Stashing recurrent weights on-chip", "author": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Diamos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diamos et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "In Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Godfrey et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "Accurate, large minibatch sgd: Training imagenet in 1 hour", "author": ["Priya Goyal", "Piotr Doll\u00e1r", "Ross Girshick", "Pieter Noordhuis", "Lukasz Wesolowski", "Aapo Kyrola", "Andrew Tulloch", "Yangqing Jia", "Kaiming He"], "venue": "arXiv preprint arXiv:1706.02677,", "citeRegEx": "Goyal et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Goyal et al\\.", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In arXiv,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A prioritized grid long short-term memory rnn for speech recognition", "author": ["W. Hsu", "Y. Zhang", "J. Glass"], "venue": "In Proc. SLT,", "citeRegEx": "Hsu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu and Liu.,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher"], "venue": "arXiv preprint arXiv:1611.01462,", "citeRegEx": "Inan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Inan et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Scalable Minimum Bayes Risk Training of Deep Neural Network Acoustic Models Using Distributed Hessian-free Optimization", "author": ["Brian Kingsbury", "Tara Sainath", "Hagen Soltau"], "venue": "In INTERSPEECH,", "citeRegEx": "Kingsbury et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kingsbury et al\\.", "year": 2012}, {"title": "Opennmt: Opensource toolkit for neural machine translation", "author": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush"], "venue": "In Proceedings of ACL 2017, System Demonstrations,", "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Recurrent additive networks", "author": ["Kenton Lee", "Omer Levy", "Luke Zettlemoyer"], "venue": "arXiv preprint arXiv:1705.07393,", "citeRegEx": "Lee et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Deriving neural architectures from sequence and graph kernels", "author": ["Tao Lei", "Wengong Jin", "Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "Lei et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2017}, {"title": "The dcu-ictcas mt system at wmt 2014 on german-english translation task", "author": ["Liangyou Li", "Xiaofeng Wu", "Santiago Cortes Vaillo", "Jun Xie", "Andy Way", "Qun Liu"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume 1. Association for Computational Linguistics,", "citeRegEx": "Li and Roth.,? \\Q2002\\E", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "On the state of the art of evaluation in neural language models", "author": ["G\u00e1bor Melis", "Chris Dyer", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1707.05589,", "citeRegEx": "Melis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Melis et al\\.", "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 115\u2013124", "author": ["Bo Pang", "Lillian Lee"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Pang and Lee.,? \\Q2005\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "The rwth aachen german-english machine translation system for wmt 2014", "author": ["Stephan Peitz", "Joern Wuebker", "Markus Freitag", "Hermann Ney"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Peitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peitz et al\\.", "year": 2014}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannenmann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In Automatic Speech Recognition and Understanding Workshop,", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahremani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Povey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2016}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Press and Wolf.,? \\Q2017\\E", "shortCiteRegEx": "Press and Wolf.", "year": 2017}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks", "author": ["Tara N. Sainath", "Oriol Vinyals", "Andrew Senior", "Hasim Sak"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling", "author": ["Hasim Sak", "Andrew Senior", "Francoise Fran\u00e7oise"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "The ibm 2016 english conversational telephone speech recognition system", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J. Kuo"], "venue": "In https://arxiv.org/abs/1604.08242,", "citeRegEx": "Saon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saon et al\\.", "year": 2016}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["Frank Seide", "Gang Li", "Xie Chen", "Dong Yu"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Attention is all you need", "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "venue": "arXiv preprint arXiv:1706.03762,", "citeRegEx": "Vaswani et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2017}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "venue": "Language resources and evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Neural computation,", "citeRegEx": "Williams and Peng.,? \\Q1990\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1990}, {"title": "An empirical exploration of skip connections for sequential tagging", "author": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of COLING 2016,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "B. Guenter", "O. Kuchaiev", "F. Seide", "H. Wang", "J. Droppo", "Z. Huang", "Y. Zhang", "G. Zweig", "C. Rossbach", "J. Currey", "J. Gao", "A. May", "A. Stolcke", "M. Slaney"], "venue": "Technical Report MSR, Microsoft Research,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Speech recognition with predictionadaptation-correction recurrent neural networks", "author": ["Yu Zhang", "Dong Yu", "Michael L Seltzer", "Jasha Droppo"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML),", "citeRegEx": "Zilly et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2017}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V Le"], "venue": "arXiv preprint arXiv:1611.01578,", "citeRegEx": "Zoph and Le.,? \\Q2016\\E", "shortCiteRegEx": "Zoph and Le.", "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "For instance, training a state-of-the-art translation or speech recognition system would take several days to complete (Vaswani et al., 2017; Wu et al., 2016b; Sak et al., 2014).", "startOffset": 119, "endOffset": 177}, {"referenceID": 32, "context": "For instance, training a state-of-the-art translation or speech recognition system would take several days to complete (Vaswani et al., 2017; Wu et al., 2016b; Sak et al., 2014).", "startOffset": 119, "endOffset": 177}, {"referenceID": 3, "context": "To counter the dramatically increased computation, parallelization such as GPU-accelerated training has been predominately adopted to scale deep learning (Diamos et al., 2016; Goyal et al., 2017).", "startOffset": 154, "endOffset": 195}, {"referenceID": 6, "context": "To counter the dramatically increased computation, parallelization such as GPU-accelerated training has been predominately adopted to scale deep learning (Diamos et al., 2016; Goyal et al., 2017).", "startOffset": 154, "endOffset": 195}, {"referenceID": 9, "context": "Most top-performing recurrent neural networks such as long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRUs) (Cho et al.", "startOffset": 85, "endOffset": 119}, {"referenceID": 2, "context": "Most top-performing recurrent neural networks such as long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRUs) (Cho et al., 2014) make use of neural gates to control the information flow and alleviate the gradient vanishing (or explosion) problem.", "startOffset": 152, "endOffset": 170}, {"referenceID": 18, "context": "We use the simplest version that performs a linear transformation over the input vector x\u0303t = Wxt (Lei et al., 2017; Lee et al., 2017).", "startOffset": 98, "endOffset": 134}, {"referenceID": 17, "context": "We use the simplest version that performs a linear transformation over the input vector x\u0303t = Wxt (Lei et al., 2017; Lee et al., 2017).", "startOffset": 98, "endOffset": 134}, {"referenceID": 7, "context": "First, we add skip connections between recurrent layers since they are shown quite effective for training deep networks (He et al., 2016; Srivastava et al., 2015; Wu et al., 2016a).", "startOffset": 120, "endOffset": 180}, {"referenceID": 36, "context": "First, we add skip connections between recurrent layers since they are shown quite effective for training deep networks (He et al., 2016; Srivastava et al., 2015; Wu et al., 2016a).", "startOffset": 120, "endOffset": 180}, {"referenceID": 36, "context": "Specifically, we use highway connections (Srivastava et al., 2015) and the output state ht is computed as, ht = rt ht + (1\u2212 rt) xt (1) = rt g(ct) + (1\u2212 rt) xt (2) where rt is the output of a reset gate.", "startOffset": 41, "endOffset": 66}, {"referenceID": 4, "context": "Second, we implement variational dropout (Gal and Ghahramani, 2016) in addition to the standard dropout for RNN regularization.", "startOffset": 41, "endOffset": 67}, {"referenceID": 0, "context": "Our formulation is similar to the recently proposed Quasi-RNN (Bradbury et al., 2017).", "startOffset": 62, "endOffset": 85}, {"referenceID": 18, "context": "A theoretical analysis regarding the representational characteristics of (a broader class of) such recurrent architectures is presented in (Lei et al., 2017).", "startOffset": 139, "endOffset": 157}, {"referenceID": 14, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al.", "startOffset": 46, "endOffset": 57}, {"referenceID": 25, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 24, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 11, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al.", "startOffset": 170, "endOffset": 188}, {"referenceID": 20, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al.", "startOffset": 205, "endOffset": 224}, {"referenceID": 38, "context": "Dataset We use 6 classification datasets from (Kim, 2014)2: movie reviews (MR) (Pang and Lee, 2005), subjectivity data (SUBJ) (Pang and Lee, 2004), customer reviews (CR) (Hu and Liu, 2004), TREC questions (Li and Roth, 2002), opinion polarity from MPQA data (Wiebe et al., 2005) and Stanford sentiment treebank (SST) (Socher et al.", "startOffset": 258, "endOffset": 278}, {"referenceID": 35, "context": ", 2005) and Stanford sentiment treebank (SST) (Socher et al., 2013)3.", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "We use the word2vec embeddings trained on 100 billion tokens from Google News, following (Kim, 2014).", "startOffset": 89, "endOffset": 100}, {"referenceID": 14, "context": "In addition, we train the same CNN model of (Kim, 2014) under our setting as a reference.", "startOffset": 44, "endOffset": 55}, {"referenceID": 14, "context": "We use the same filter widths and number of filters as (Kim, 2014).", "startOffset": 55, "endOffset": 66}, {"referenceID": 14, "context": "Figure 2 plots the validation curves of our model, cuDNN LSTM and the wide CNNs of (Kim, 2014).", "startOffset": 83, "endOffset": 94}, {"referenceID": 14, "context": "Wide CNNs refer to the sentence convolutional model (Kim, 2014) using 3, 4, 5-gram features (i.", "startOffset": 52, "endOffset": 63}, {"referenceID": 1, "context": "Model # layers d Size Dev EM Dev F1 Time / epoch RNN Total (Chen et al., 2017) 3 128 4.", "startOffset": 59, "endOffset": 78}, {"referenceID": 30, "context": "Dataset We use Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) as our benchmark.", "startOffset": 59, "endOffset": 83}, {"referenceID": 1, "context": "Setup We train the Document Reader model as described in (Chen et al., 2017) and compare the model variants which use LSTM (original setup) and SRU (our setup).", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "Due to minor implementation differences, this version obtains 1% worse performance compared to the results reported in (Chen et al., 2017) when using the same training options.", "startOffset": 119, "endOffset": 138}, {"referenceID": 1, "context": "9% F1 score, being on par with the results in the original work (Chen et al., 2017).", "startOffset": 64, "endOffset": 83}, {"referenceID": 23, "context": "The processed data along with train, dev and test splits are taken from (Mikolov et al., 2010), which contains about 1 million tokens with a truncated vocabulary of 10k.", "startOffset": 72, "endOffset": 94}, {"referenceID": 43, "context": "Setup Our training configuration largely follows prior work (Zaremba et al., 2014; Gal and Ghahramani, 2016; Zoph and Le, 2016).", "startOffset": 60, "endOffset": 127}, {"referenceID": 4, "context": "Setup Our training configuration largely follows prior work (Zaremba et al., 2014; Gal and Ghahramani, 2016; Zoph and Le, 2016).", "startOffset": 60, "endOffset": 127}, {"referenceID": 46, "context": "Setup Our training configuration largely follows prior work (Zaremba et al., 2014; Gal and Ghahramani, 2016; Zoph and Le, 2016).", "startOffset": 60, "endOffset": 127}, {"referenceID": 43, "context": "Model # layers Size Dev Test Time / epoch RNN Total LSTM (Zaremba et al., 2014) 2 66m 82.", "startOffset": 57, "endOffset": 79}, {"referenceID": 29, "context": "4 LSTM (Press and Wolf, 2017) 2 51m 75.", "startOffset": 7, "endOffset": 29}, {"referenceID": 12, "context": "2 LSTM (Inan et al., 2016) 2 28m 72.", "startOffset": 7, "endOffset": 26}, {"referenceID": 45, "context": "0 RHN (Zilly et al., 2017) 10 23m 67.", "startOffset": 6, "endOffset": 26}, {"referenceID": 18, "context": "4 KNN (Lei et al., 2017) 4 20m - 63.", "startOffset": 6, "endOffset": 24}, {"referenceID": 46, "context": "8 NAS (Zoph and Le, 2016) - 25m - 64.", "startOffset": 6, "endOffset": 25}, {"referenceID": 46, "context": "0 NAS (Zoph and Le, 2016) - 54m - 62.", "startOffset": 6, "endOffset": 25}, {"referenceID": 43, "context": "Models in comparison are trained using similar regularization and learning strategy: variational dropout is used except for (Zaremba et al., 2014), (Press and Wolf, 2017) and cuDNN LSTM; input and output word embeddings are tied except for (Zaremba et al.", "startOffset": 124, "endOffset": 146}, {"referenceID": 29, "context": ", 2014), (Press and Wolf, 2017) and cuDNN LSTM; input and output word embeddings are tied except for (Zaremba et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 43, "context": ", 2014), (Press and Wolf, 2017) and cuDNN LSTM; input and output word embeddings are tied except for (Zaremba et al., 2014); SGD with learning rate decaying is used for all models.", "startOffset": 101, "endOffset": 123}, {"referenceID": 26, "context": "Following standard practice (Peitz et al., 2014; Li et al., 2014; Jean et al., 2015), the training corpus was pre-processed and about 4 million translation pairs are left after processing.", "startOffset": 28, "endOffset": 84}, {"referenceID": 19, "context": "Following standard practice (Peitz et al., 2014; Li et al., 2014; Jean et al., 2015), the training corpus was pre-processed and about 4 million translation pairs are left after processing.", "startOffset": 28, "endOffset": 84}, {"referenceID": 13, "context": "Following standard practice (Peitz et al., 2014; Li et al., 2014; Jean et al., 2015), the training corpus was pre-processed and about 4 million translation pairs are left after processing.", "startOffset": 28, "endOffset": 84}, {"referenceID": 16, "context": "Setup We use OpenNMT (Klein et al., 2017), an open-source machine translation system for our experiments.", "startOffset": 21, "endOffset": 41}, {"referenceID": 21, "context": "The system trains a seq2seq model using a recurrent encoder-decoder architecture with attention (Luong et al., 2015).", "startOffset": 96, "endOffset": 116}, {"referenceID": 16, "context": "We obtain better BLEU scores compared to the results presented in the report of OpenNMT system (Klein et al., 2017).", "startOffset": 95, "endOffset": 115}, {"referenceID": 22, "context": "As recently demonstrated by (Melis et al., 2017), the LSTM can achieve a perplexity of 58 via better regularization and hyper-parameter tuning.", "startOffset": 28, "endOffset": 48}, {"referenceID": 16, "context": "OpenNMT default setup # layers Size Test BLEU Time in RNNs (Klein et al., 2017) 2 - - 17.", "startOffset": 59, "endOffset": 79}, {"referenceID": 16, "context": "60 (Klein et al., 2017) + BPE 2 - - 19.", "startOffset": 3, "endOffset": 23}, {"referenceID": 5, "context": "5 Speech recognition Dataset We use Switchboard-1 corpus (Godfrey et al., 1992) for our experiments.", "startOffset": 57, "endOffset": 79}, {"referenceID": 27, "context": "Setup We use Kaldi (Povey et al., 2011) for feature extraction, decoding, and training of initial HMM-GMM models.", "startOffset": 19, "endOffset": 39}, {"referenceID": 42, "context": "For speech recognition task, we use Computational Network Toolkit (CNTK) (Yu et al., 2014) instead of PyTorch for neural network training.", "startOffset": 73, "endOffset": 90}, {"referenceID": 31, "context": "Following (Sainath et al., 2015), all weights are randomly initialized from the uniform distribution with range [\u22120.", "startOffset": 10, "endOffset": 32}, {"referenceID": 34, "context": "05], and all biases are initialized to 0 without generative or discriminative pretraining (Seide et al., 2011).", "startOffset": 90, "endOffset": 110}, {"referenceID": 39, "context": "All neural network models, unless explicitly stated otherwise, are trained with a cross-entropy (CE) criterion using truncated back-propagation-through-time (BPTT) (Williams and Peng, 1990) for optimization.", "startOffset": 164, "endOffset": 189}, {"referenceID": 44, "context": "9 is used for subsequent epochs (Zhang et al., 2015).", "startOffset": 32, "endOffset": 52}, {"referenceID": 8, "context": "L2 constraint regularization (Hinton et al., 2012) with weight 10\u22125 is applied.", "startOffset": 29, "endOffset": 50}, {"referenceID": 32, "context": "We also delayed the output of LSTM by 10 frames as suggested in (Sak et al., 2014) to add more context for LSTM.", "startOffset": 64, "endOffset": 82}, {"referenceID": 15, "context": "The ASR performance can be further improved by using bidirectional model and state-level Minimum Bayes Risk (sMBR) training (Kingsbury et al., 2012).", "startOffset": 124, "endOffset": 148}, {"referenceID": 44, "context": "To train the bidirectional model, the latency-controlled method described in (Zhang et al., 2015) was applied.", "startOffset": 77, "endOffset": 97}, {"referenceID": 15, "context": "To train the recurrent model with sMBR criterion (Kingsbury et al., 2012), we adopted the two-forward-pass method described in (Zhang et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 44, "context": ", 2012), we adopted the two-forward-pass method described in (Zhang et al., 2015), and processed 40 utterances simultaneously.", "startOffset": 61, "endOffset": 81}, {"referenceID": 33, "context": "5 Very Deep CNN + sMBR (Saon et al., 2016) 10 10.", "startOffset": 23, "endOffset": 42}, {"referenceID": 28, "context": "5 LSTM + LF-MMI (Povey et al., 2016) 3 10.", "startOffset": 16, "endOffset": 36}, {"referenceID": 28, "context": "3 Bi-LSTM + LF-MMI (Povey et al., 2016) 3 9.", "startOffset": 19, "endOffset": 39}, {"referenceID": 28, "context": "Note that LF-MMI for sequence training, i-vectors for speaker adaptation, and speaker perturbation for data augmentation have been applied in (Povey et al., 2016).", "startOffset": 142, "endOffset": 162}, {"referenceID": 10, "context": "Moreover, we believe different highway variants such as grid LSTM (Hsu et al., 2016) can also further boost our model.", "startOffset": 66, "endOffset": 84}], "year": 2017, "abstractText": "Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of ht is blocked until the entire computation of ht\u22121 finishes, which is a major bottleneck for parallel computing. In this work, we propose an alternative RNN implementation by deliberately simplifying the state computation and exposing more parallelism. The proposed recurrent unit operates as fast as a convolutional layer and 5-10x faster than cuDNN-optimized LSTM. We demonstrate the unit\u2019s effectiveness across a wide range of applications including classification, question answering, language modeling, translation and speech recognition. We open source our implementation in PyTorch and CNTK1.", "creator": "LaTeX with hyperref package"}}}