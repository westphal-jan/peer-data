{"id": "1709.02783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "A Statistical Comparison of Some Theories of NP Word Order", "abstract": "A frequent object of study in linguistic typology is the order of elements {demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to predict the relative frequencies of these orders across languages. Here we use Poisson regression to statistically compare some prominent accounts of this variation. We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).", "histories": [["v1", "Fri, 8 Sep 2017 17:12:16 GMT  (56kb,D)", "http://arxiv.org/abs/1709.02783v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["richard futrell", "roger levy", "matthew dryer"], "accepted": false, "id": "1709.02783"}, "pdf": {"name": "1709.02783.pdf", "metadata": {"source": "CRF", "title": "A Statistical Comparison of Some Theories of NP Word Order", "authors": ["Richard Futrell", "Roger Levy"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A common object of study in linguistic typology is the variation in the order of the elements within the noun Phrase (NP) in different languages. In particular, much work focuses on predicting the relative frequency in different languages of the orders of the elements {demonstrative, adjective, numerical, noun}. Table 1 shows the relative frequencies of different orders for these elements in different languages (provided each language has only one dominant order) according to the data given in Dryer (in prep). In this table D stands for demonstrative, N for numerical, A stands for adjective, and n stands for noun. Genera counts are the counts of linguistic genres that have a particular order; adapted frequencies are calculated using a methodology described in Dryer (in prep), which aims to minimize any overrepresentation of some orders that may result from regional effects."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Basics", "text": "s ability to predict the observed frequencies of orders, using the Poisson regression first used in Cysouw (2010). In the Poisson regression, we represent each language with a set of m binary weighted characteristics and say that the expected frequency of a language in a sample of k languages is given by the following values: \u03bb = eVV = wb + w1 \u00b7 f1 + w2 \u00b7 f2 +... + wm \u00b7 fm, (1) where fi is an indicator variable variable with a value of 0 if the ith characteristic \u2212 and 1 if the ith characteristic is +, and where the weights wb and w1,... wm are the ones that maximize the probability of the observed counting of languages. Weight wb is called a bias term if the ith characteristic \u2212 and probability model \u2212 predicts the frequency model \u2212 that a certain language can have a regression \u2212."}, {"heading": "2.2 Feature systems under comparison", "text": "Within this framework, we compare three systems of characteristics: (1) the system in Dryer (pre-stage), (2) the system in Cysouw (2010) and (3) the theory of Cinque (2005). Since Cinque's theory is not formulated in characteristics, we use two reductions of his theory to characteristics: that in Merlo (2015) and our own, that in Figure 5. Our characteristics of the Cinque theory show a close parallel to the characteristics given in Cysouw (2010)."}, {"heading": "2.3 Dependent variables", "text": "We use the Poisson regression to predict two quantities: First, we try to predict the adjusted frequency of each order as indicated in Dryer (in prep) and as shown in Table 1. (We round the adjusted frequencies to the next integer to meet the Poisson regression assumption that the dependent variable is a natural number.) Second, we try to predict the number of genera specified in the same work."}, {"heading": "2.4 Basis for model comparison", "text": "We compare models with the protocol probability, the protocol probability associated with the observed frequencies under the model. A model fits well with the data if it assigns a high probability to the data, so a high protocol probability indicates a good fit. If the protocol probability is close for different models, we can also compare them according to their degree of freedom, i.e. the number of free parameters in the model. In general, simpler models with fewer parameters are preferred to models with more parameters."}, {"heading": "2.5 Notes on Featurization of Cinque (2005)", "text": "This year is the highest in the history of the country."}, {"heading": "2.6 Comparison with Merlo (2015)", "text": "Merlo (2015) conducts a study with similar goals and uses featurizations more or less than what we discussed above. It has frequency classes with a na\u00efve-Bayen-estimator and a weight-averaged-one-dependence-estimator, rather than Poisson-regression, as suggested by Cysouw. As a summary of what this causes: Merlo (2015) first discredits the Integrated-Order-Frequency-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Numbers-Analyses-Numbers-Numbers-Numbers-Numbers-"}, {"heading": "2.7 Comparison with Cysouw (2010)", "text": "As explained in Section 2.5, our approach here is very similar to that of Cysouw (2010): We use the same statistical model class and the same theory comparison (Cinque / Cysouw / Dryer); the differences are as follows: \u2022 We use the newer data of Dryer (pre-stage) instead of the earlier data of Dryer (2006); \u2022 We use the characteristics of Dryer (pre-stage) instead of the earlier characteristics of Dryer (pre-stage); \u2022 We correct what we consider to be a trait error made by Cysouw in describing the Cinque theory."}, {"heading": "3 Results", "text": "The results do not give a clear reason for the choice between the dry model and the Cinque model, but both models perform better than the Cysouw model (2010). Whether the dry model performs better or not depends on whether we use the model to predict adapted frequencies or genres."}, {"heading": "3.1 Predicting Adjusted Frequencies", "text": "Table 3 shows the probabilities for models that predict an adjusted frequency (rounded to the nearest integer), and also shows the number of parameters (i.e.) in each model. The table shows that the Cinque model slightly outperforms the dryer (in prep) when adjusting the data. For a more detailed comparison of model performance, we compared model forecasts with observed adjusted dryer frequencies (in prep). Figure 2 shows model forecasts compared to adjusted frequency. We wanted to know how much each order contributed to the fit of the model, so in Figure 3 we show signed \u03c72 discrepancies between model forecasts and adjusted frequency. The discrepancy in Figure 2 measures how much the prediction error for each order of words contributes to the overall discrepancy between data and model fit; signed discrepancy in Figure 2 shows this discrepancy in the direction of the discrepancy between the predicted frequency (or underestimated) of the model fit."}, {"heading": "3.2 Predicting Genera Counts", "text": "When we use the different characteristic systems to predict the number of genera, we obtain the following data log probabilities, which are shown in Table 4: So when we predict genera, we best match the data by using the characteristics of dryers (in preparation), followed by the characteristics of Cinque (2005), followed by the characteristics of Cysouw (2010). We think that the Cinque model performs worse in predicting genera primarily because it does not predict NnAD orders, while the Dryer model gets this sequence exactly right. This can be seen in Figure 6, which shows model predictions, and in Figure 7, which shows signed discrepancies compared to genera. Figures 8 and 9 show the optimal characteristic weights for the Dryer models and Cinque models in predicting genera."}, {"heading": "4 Discussion", "text": "The results are clear evidence that the dryer model (in prep) and the Cinque model (2005) offer feature systems that have better predictive power than the Cysouw model (2010), but in our opinion there is no strong case for favouring the Cinque model over the dryer model or vice versa. Although, under one particular interpretation, the Cinque model may offer a slightly higher accuracy of fit with the data, this is only true for a feature and not for predicting Genera Countings. The discrepancy in results between adjusted frequency and genera Countings may be due to the particular distribution characteristics of the adjusted frequency as described above. Overall, the analysis suggests that the Dryer model and the Cinque model have approximately similar predictive power and do not differentiate between the current data."}, {"heading": "Acknowledgments", "text": "This work was supported by the NSF DDRI Scholarship # 1551543 to R.F."}], "references": [{"title": "Deriving Greenberg\u2019s Universal 20 and its exceptions", "author": ["G. Cinque"], "venue": null, "citeRegEx": "Cinque,? \\Q2005\\E", "shortCiteRegEx": "Cinque", "year": 2005}, {"title": "The Antisymmetry of Syntax", "author": ["R.S. noun. Kayne"], "venue": null, "citeRegEx": "Kayne,? \\Q1994\\E", "shortCiteRegEx": "Kayne", "year": 1994}, {"title": "Predicting word order universals", "author": ["P. MA. Merlo"], "venue": "Journal of Language", "citeRegEx": "Merlo,? \\Q2015\\E", "shortCiteRegEx": "Merlo", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep).", "startOffset": 40, "endOffset": 54}, {"referenceID": 0, "context": "We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep).", "startOffset": 40, "endOffset": 96}, {"referenceID": 0, "context": "We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).", "startOffset": 40, "endOffset": 200}, {"referenceID": 0, "context": "We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).", "startOffset": 40, "endOffset": 341}, {"referenceID": 0, "context": "We consider three proposals from the literature: those given in Dryer (in prep), Cysouw (2010) and Cinque (2005). The proposal in Dryer (in prep) is an update from the previous proposal of Dryer (2006).", "startOffset": 99, "endOffset": 113}, {"referenceID": 0, "context": "We consider three proposals from the literature: those given in Dryer (in prep), Cysouw (2010) and Cinque (2005). The proposal in Dryer (in prep) is an update from the previous proposal of Dryer (2006). The first two of these theories are featural in nature: they associate each order with a set of marked features, and claim that orders with more marked features will be less frequent.", "startOffset": 99, "endOffset": 202}, {"referenceID": 0, "context": "We consider three proposals from the literature: those given in Dryer (in prep), Cysouw (2010) and Cinque (2005). The proposal in Dryer (in prep) is an update from the previous proposal of Dryer (2006). The first two of these theories are featural in nature: they associate each order with a set of marked features, and claim that orders with more marked features will be less frequent. The last model, that of Cinque (2005), is derivational in nature: it gives a generative model for how certain word orders arise, where certain decisions in the generative process are considered marked.", "startOffset": 99, "endOffset": 425}, {"referenceID": 0, "context": "We consider three proposals from the literature: those given in Dryer (in prep), Cysouw (2010) and Cinque (2005). The proposal in Dryer (in prep) is an update from the previous proposal of Dryer (2006). The first two of these theories are featural in nature: they associate each order with a set of marked features, and claim that orders with more marked features will be less frequent. The last model, that of Cinque (2005), is derivational in nature: it gives a generative model for how certain word orders arise, where certain decisions in the generative process are considered marked. Orders that require more marked operations to be generated are claimed to be less frequent. We reduce the last model to a featural model, and then compare which model provides a feature system which can best predict the typological data when the features have different degrees of markedness. 2 Method 2.1 Basics We consider each proposal from the literature to define a feature system, and compare the ability of each feature system to predict the observed frequencies of orders. To do so, we use Poisson regression, as first used in Cysouw (2010). In Poisson regression we represent each language with a set of m binary-valued features, and say that the expected frequency \u03bb of a language in a sample of k languages is given by: \u03bb = e V = wb + w1 \u00b7 f1 + w2 \u00b7 f2 + .", "startOffset": 99, "endOffset": 1138}, {"referenceID": 0, "context": "2 Feature systems under comparison Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in Cysouw (2010), and (3) the theory of Cinque (2005). Cinque\u2019s theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in Merlo (2015) and our own, shown in Figure 5.", "startOffset": 182, "endOffset": 196}, {"referenceID": 0, "context": "2 Feature systems under comparison Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in Cysouw (2010), and (3) the theory of Cinque (2005). Cinque\u2019s theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in Merlo (2015) and our own, shown in Figure 5.", "startOffset": 182, "endOffset": 334}, {"referenceID": 0, "context": "2 Feature systems under comparison Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in Cysouw (2010), and (3) the theory of Cinque (2005). Cinque\u2019s theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in Merlo (2015) and our own, shown in Figure 5. Our featurization of Cinque\u2019s theory closely parallels the featurization given in Cysouw (2010). 2.", "startOffset": 182, "endOffset": 462}, {"referenceID": 0, "context": "2 Feature systems under comparison Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in Cysouw (2010), and (3) the theory of Cinque (2005). Cinque\u2019s theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in Merlo (2015) and our own, shown in Figure 5. Our featurization of Cinque\u2019s theory closely parallels the featurization given in Cysouw (2010). 2.3 Dependent variables We apply Poisson regression to predict two quantities. First, we try to predict the adjusted frequency of each order, as given in Dryer (in prep) and shown in Table 1. (We round the adjusted frequencies to the nearest integer in order to satisfy the Poisson regression assumption that the dependent variable is a natural number.) Second, we try to predict the counts of genera given in the same paper. 2.4 Basis for model comparison We compare models using log likelihood, the log probability assigned to the observed frequencies under the model. A model fits the data well when it assigns high probability to the data, so high log likelihood indicates a good fit. When log likelihood for different models is close, we can also compare them by their degrees of freedom, which is the number of free parameters in the model. In general, simpler models with fewer parameters are preferable over ones with more parameters. 2.5 Notes on Featurization of Cinque (2005) Special care is needed when reducing the theory of Cinque (2005) to features so that it can be compared with the other theories in a regression framework.", "startOffset": 182, "endOffset": 1450}, {"referenceID": 0, "context": "2 Feature systems under comparison Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in Cysouw (2010), and (3) the theory of Cinque (2005). Cinque\u2019s theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in Merlo (2015) and our own, shown in Figure 5. Our featurization of Cinque\u2019s theory closely parallels the featurization given in Cysouw (2010). 2.3 Dependent variables We apply Poisson regression to predict two quantities. First, we try to predict the adjusted frequency of each order, as given in Dryer (in prep) and shown in Table 1. (We round the adjusted frequencies to the nearest integer in order to satisfy the Poisson regression assumption that the dependent variable is a natural number.) Second, we try to predict the counts of genera given in the same paper. 2.4 Basis for model comparison We compare models using log likelihood, the log probability assigned to the observed frequencies under the model. A model fits the data well when it assigns high probability to the data, so high log likelihood indicates a good fit. When log likelihood for different models is close, we can also compare them by their degrees of freedom, which is the number of free parameters in the model. In general, simpler models with fewer parameters are preferable over ones with more parameters. 2.5 Notes on Featurization of Cinque (2005) Special care is needed when reducing the theory of Cinque (2005) to features so that it can be compared with the other theories in a regression framework.", "startOffset": 182, "endOffset": 1515}, {"referenceID": 0, "context": "Figure 1: The universal base structure of D,N,A,n under the theory of Cinque (2005). The theory of Cinque (2005) is not featural in nature, but rather derivational.", "startOffset": 70, "endOffset": 84}, {"referenceID": 0, "context": "Figure 1: The universal base structure of D,N,A,n under the theory of Cinque (2005). The theory of Cinque (2005) is not featural in nature, but rather derivational.", "startOffset": 70, "endOffset": 113}, {"referenceID": 0, "context": "Figure 1: The universal base structure of D,N,A,n under the theory of Cinque (2005). The theory of Cinque (2005) is not featural in nature, but rather derivational. In this model, orders are built up by a generative process that makes decisions in a certain order; whereas in featural models, orders are assigned scores based on features that have no intrinsic order. For example, the centerpiece of the theory of Cinque (2005) is the claim that the Merge order of D>N>A>n is universal, and that the Linear Correspondence Axiom (LCA) of Kayne (1994) holds, such that every word order must be generated from a base structure of the form seen in Figure 1, plus movement operations.", "startOffset": 70, "endOffset": 428}, {"referenceID": 0, "context": "Figure 1: The universal base structure of D,N,A,n under the theory of Cinque (2005). The theory of Cinque (2005) is not featural in nature, but rather derivational. In this model, orders are built up by a generative process that makes decisions in a certain order; whereas in featural models, orders are assigned scores based on features that have no intrinsic order. For example, the centerpiece of the theory of Cinque (2005) is the claim that the Merge order of D>N>A>n is universal, and that the Linear Correspondence Axiom (LCA) of Kayne (1994) holds, such that every word order must be generated from a base structure of the form seen in Figure 1, plus movement operations.", "startOffset": 70, "endOffset": 550}, {"referenceID": 0, "context": "Figure 1: The universal base structure of D,N,A,n under the theory of Cinque (2005). The theory of Cinque (2005) is not featural in nature, but rather derivational. In this model, orders are built up by a generative process that makes decisions in a certain order; whereas in featural models, orders are assigned scores based on features that have no intrinsic order. For example, the centerpiece of the theory of Cinque (2005) is the claim that the Merge order of D>N>A>n is universal, and that the Linear Correspondence Axiom (LCA) of Kayne (1994) holds, such that every word order must be generated from a base structure of the form seen in Figure 1, plus movement operations. Orders that are not derivable from this base structure are not generated at all in Cinque\u2019s theory. For such orders, the question of what if any movement operations apply never even arises under Cinque\u2019s theory, in principle. For example, suppose we think of the Cinque model in terms of features: then AlternativeMergeOrder is a feature that can be + or \u2212, and some movement operation is reflected in a feature that can be + or \u2212. For an order that violates the specified merge order, we would give it value + for AlternativeMergeOrder and \u2212 for the movement feature, but this is not a completely correct reflection of Cinque\u2019s derivational theory. The reason is that under the generative process, if a word order violates the required merge order, then the model never even decides whether to perform a movement operation or not: thus the value of the movement feature should not be \u2212 or +, but undefined. The theory of Cinque (2005) also involves theoretically-derived graded markedness values for operations in the generative process: for instance, total movement is claimed to be unmarked, \u201cpicture of who\u201d type movement is claimed to be especially marked, and the rest of the movement features are claimed to be marked.", "startOffset": 70, "endOffset": 1616}, {"referenceID": 0, "context": "Nevertheless we followed Cinque in assigning this word order a + value for movement (of NP) without pied piping. This decision is also justified by Cinque\u2019s comment (p. 320) that his system is \u201c[k]eeping to the idea that. . . postnominal orders are only a function of the raising of the NP (or of an XP containing the NP). . . \u201d. Additionally, there is an inconsistency in Cinque (2005) between (7bv), where this order is stated to involve partial movement, and (6k), where partial movement is not listed as a type of markedness for this order.", "startOffset": 25, "endOffset": 387}, {"referenceID": 0, "context": "Nevertheless we followed Cinque in assigning this word order a + value for movement (of NP) without pied piping. This decision is also justified by Cinque\u2019s comment (p. 320) that his system is \u201c[k]eeping to the idea that. . . postnominal orders are only a function of the raising of the NP (or of an XP containing the NP). . . \u201d. Additionally, there is an inconsistency in Cinque (2005) between (7bv), where this order is stated to involve partial movement, and (6k), where partial movement is not listed as a type of markedness for this order. Here we went with (7bv) and listed this order as involving partial_move=+; we believe that this treatment is the most globally consistent overall, on analogy with orders such as NnAD which Cinque treats as involving partial movement because there are multiple types of movement and the first (raising of NP around A) is only partial. \u2022 As with AnDN, there is inconsistency between (7bv) and the wordorder-specific description of AnND (6w): in the former, this order is stated to involve partial movement of NP, but in the latter, partial movement is not mentioned as a type of markedness. As with AnDN, we listed this order as involving partial_move=+. Regarding the first two cases, it should be emphasized that Cinque (2005) is far from totally clear about what does and does not count as partial (and thus marked) movement.", "startOffset": 25, "endOffset": 1272}, {"referenceID": 0, "context": "Nevertheless we followed Cinque in assigning this word order a + value for movement (of NP) without pied piping. This decision is also justified by Cinque\u2019s comment (p. 320) that his system is \u201c[k]eeping to the idea that. . . postnominal orders are only a function of the raising of the NP (or of an XP containing the NP). . . \u201d. Additionally, there is an inconsistency in Cinque (2005) between (7bv), where this order is stated to involve partial movement, and (6k), where partial movement is not listed as a type of markedness for this order. Here we went with (7bv) and listed this order as involving partial_move=+; we believe that this treatment is the most globally consistent overall, on analogy with orders such as NnAD which Cinque treats as involving partial movement because there are multiple types of movement and the first (raising of NP around A) is only partial. \u2022 As with AnDN, there is inconsistency between (7bv) and the wordorder-specific description of AnND (6w): in the former, this order is stated to involve partial movement of NP, but in the latter, partial movement is not mentioned as a type of markedness. As with AnDN, we listed this order as involving partial_move=+. Regarding the first two cases, it should be emphasized that Cinque (2005) is far from totally clear about what does and does not count as partial (and thus marked) movement. For example, NnAD is described as involving partial (and thus marked) raising of NP around A, followed by a second raising that gets the raised constituent all the way to the left edge. But although nNAD likewise starts with a partial raising of NP around A (and N) followed by a second raising that gets the raised constituent all the way to the left edge, it is not considered to involve partial movement. Additionally, there is one case of what we believe is a coding error by Cysouw (2010) in his implementation of Cinque\u2019s model (see his Appendix on page 284): \u2022 Cysouw encodes nNAD as involving NP movement with pied piping of the picture of who variety, but Cinque describes this order ((6t)) as involving whose picture pied piping instead, which seems correct to us.", "startOffset": 25, "endOffset": 1866}, {"referenceID": 0, "context": "Nevertheless we followed Cinque in assigning this word order a + value for movement (of NP) without pied piping. This decision is also justified by Cinque\u2019s comment (p. 320) that his system is \u201c[k]eeping to the idea that. . . postnominal orders are only a function of the raising of the NP (or of an XP containing the NP). . . \u201d. Additionally, there is an inconsistency in Cinque (2005) between (7bv), where this order is stated to involve partial movement, and (6k), where partial movement is not listed as a type of markedness for this order. Here we went with (7bv) and listed this order as involving partial_move=+; we believe that this treatment is the most globally consistent overall, on analogy with orders such as NnAD which Cinque treats as involving partial movement because there are multiple types of movement and the first (raising of NP around A) is only partial. \u2022 As with AnDN, there is inconsistency between (7bv) and the wordorder-specific description of AnND (6w): in the former, this order is stated to involve partial movement of NP, but in the latter, partial movement is not mentioned as a type of markedness. As with AnDN, we listed this order as involving partial_move=+. Regarding the first two cases, it should be emphasized that Cinque (2005) is far from totally clear about what does and does not count as partial (and thus marked) movement. For example, NnAD is described as involving partial (and thus marked) raising of NP around A, followed by a second raising that gets the raised constituent all the way to the left edge. But although nNAD likewise starts with a partial raising of NP around A (and N) followed by a second raising that gets the raised constituent all the way to the left edge, it is not considered to involve partial movement. Additionally, there is one case of what we believe is a coding error by Cysouw (2010) in his implementation of Cinque\u2019s model (see his Appendix on page 284): \u2022 Cysouw encodes nNAD as involving NP movement with pied piping of the picture of who variety, but Cinque describes this order ((6t)) as involving whose picture pied piping instead, which seems correct to us. 2.6 Comparison with Merlo (2015) Merlo (2015) conducts a study with similar aims to ours and uses featurizations more or less the same as what we\u2019ve discussed above.", "startOffset": 25, "endOffset": 2180}, {"referenceID": 0, "context": "Nevertheless we followed Cinque in assigning this word order a + value for movement (of NP) without pied piping. This decision is also justified by Cinque\u2019s comment (p. 320) that his system is \u201c[k]eeping to the idea that. . . postnominal orders are only a function of the raising of the NP (or of an XP containing the NP). . . \u201d. Additionally, there is an inconsistency in Cinque (2005) between (7bv), where this order is stated to involve partial movement, and (6k), where partial movement is not listed as a type of markedness for this order. Here we went with (7bv) and listed this order as involving partial_move=+; we believe that this treatment is the most globally consistent overall, on analogy with orders such as NnAD which Cinque treats as involving partial movement because there are multiple types of movement and the first (raising of NP around A) is only partial. \u2022 As with AnDN, there is inconsistency between (7bv) and the wordorder-specific description of AnND (6w): in the former, this order is stated to involve partial movement of NP, but in the latter, partial movement is not mentioned as a type of markedness. As with AnDN, we listed this order as involving partial_move=+. Regarding the first two cases, it should be emphasized that Cinque (2005) is far from totally clear about what does and does not count as partial (and thus marked) movement. For example, NnAD is described as involving partial (and thus marked) raising of NP around A, followed by a second raising that gets the raised constituent all the way to the left edge. But although nNAD likewise starts with a partial raising of NP around A (and N) followed by a second raising that gets the raised constituent all the way to the left edge, it is not considered to involve partial movement. Additionally, there is one case of what we believe is a coding error by Cysouw (2010) in his implementation of Cinque\u2019s model (see his Appendix on page 284): \u2022 Cysouw encodes nNAD as involving NP movement with pied piping of the picture of who variety, but Cinque describes this order ((6t)) as involving whose picture pied piping instead, which seems correct to us. 2.6 Comparison with Merlo (2015) Merlo (2015) conducts a study with similar aims to ours and uses featurizations more or less the same as what we\u2019ve discussed above.", "startOffset": 25, "endOffset": 2193}, {"referenceID": 1, "context": "As a summary of how this works: Merlo (2015) first discretizes the integervalued word order frequency counts (by language or by genus) into 2, 4, or 7 categories; then she learns a model that categorizes language classes by their features according to the classic Naive Bayes formula: P (class|features) \u221d P (features|class)P (class) P (features|class) = \u220f fi\u2208features P (fi|class), where fi is the value of the ith feature in the featurization scheme under consideration; our fi here are Merlo\u2019s ai in her Equations 1\u20134, p.", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Technically these \u201cfeatures\u201d are attribute-value pairs, such as Symmetry1=T for the Dryer model or Partial=whose-pp for the Cinque model. The WAODE model is a bit more complex than the Naive Bayes model but is fundamentally similar. These models are trained under two approaches: type-based, where training data consists of language types and their features and frequencies, and token-based, where the frequent language types are repeated multiple times in the training data and the language types with frequency zero appear with frequency zero. In the token-based approach, the model is not penalized for miscategorizing language types with frequency None, because these do not appear in the training data. Our work differs from Merlo\u2019s approach on two points: \u2022 In predicting typological data, we use Poisson regression, which is a discriminative log-linear predictor, rather than Naive Bayes and WAODE, which are generative models. \u2022 Our models predict integer-valued typological counts, whereas the models in Merlo (2015) predict unordered categorically-valued frequency classes.", "startOffset": 124, "endOffset": 1026}, {"referenceID": 0, "context": "Technically these \u201cfeatures\u201d are attribute-value pairs, such as Symmetry1=T for the Dryer model or Partial=whose-pp for the Cinque model. The WAODE model is a bit more complex than the Naive Bayes model but is fundamentally similar. These models are trained under two approaches: type-based, where training data consists of language types and their features and frequencies, and token-based, where the frequent language types are repeated multiple times in the training data and the language types with frequency zero appear with frequency zero. In the token-based approach, the model is not penalized for miscategorizing language types with frequency None, because these do not appear in the training data. Our work differs from Merlo\u2019s approach on two points: \u2022 In predicting typological data, we use Poisson regression, which is a discriminative log-linear predictor, rather than Naive Bayes and WAODE, which are generative models. \u2022 Our models predict integer-valued typological counts, whereas the models in Merlo (2015) predict unordered categorically-valued frequency classes. We favor Poisson regression (and more generally log-linear models) over the Naive Bayes/WAODE approach because it allows us to predict more fine-grained typological data and to model the strong intuition that the effects of features on typological frequencies should be monotonic. Merlo\u2019s (2015) models have the property that feature weights are not monotonic in their preferences for frequency classes.", "startOffset": 124, "endOffset": 1380}, {"referenceID": 1, "context": "55 Table 2: Table of feature weights (conditional probabilities under the Naive Bayes assumption) from Merlo (2015), Table 11.", "startOffset": 103, "endOffset": 116}, {"referenceID": 1, "context": "55 Table 2: Table of feature weights (conditional probabilities under the Naive Bayes assumption) from Merlo (2015), Table 11. model where the goal is to classify each language into the categories (e.g.) {Very Frequent, Frequent, Rare, None}, there is nothing to prevent a feature from getting weights that favor Very Frequent and None while disfavoring Frequent and Rare. Examples of this non-monotonicity in feature weights can be seen in Merlo\u2019s (2015) Table 11, our Table 2: the feature Harmony=Y favors a language to be either Very Frequent or None, while favoring Frequent and Rare less.", "startOffset": 103, "endOffset": 456}, {"referenceID": 1, "context": "55 Table 2: Table of feature weights (conditional probabilities under the Naive Bayes assumption) from Merlo (2015), Table 11. model where the goal is to classify each language into the categories (e.g.) {Very Frequent, Frequent, Rare, None}, there is nothing to prevent a feature from getting weights that favor Very Frequent and None while disfavoring Frequent and Rare. Examples of this non-monotonicity in feature weights can be seen in Merlo\u2019s (2015) Table 11, our Table 2: the feature Harmony=Y favors a language to be either Very Frequent or None, while favoring Frequent and Rare less. The monotonicity in weights means that the weights from this framework cannot be considered markedness values, which either penalize an order (make it less frequent) or do not. In addition to making the model weights less interpretable, this non-monotonicity means that the model has the flexibility to take advantage of artifacts of the discretization of word order frequencies into bins. 2.7 Comparison with Cysouw (2010) As stated in Section 2.", "startOffset": 103, "endOffset": 1018}, {"referenceID": 1, "context": "55 Table 2: Table of feature weights (conditional probabilities under the Naive Bayes assumption) from Merlo (2015), Table 11. model where the goal is to classify each language into the categories (e.g.) {Very Frequent, Frequent, Rare, None}, there is nothing to prevent a feature from getting weights that favor Very Frequent and None while disfavoring Frequent and Rare. Examples of this non-monotonicity in feature weights can be seen in Merlo\u2019s (2015) Table 11, our Table 2: the feature Harmony=Y favors a language to be either Very Frequent or None, while favoring Frequent and Rare less. The monotonicity in weights means that the weights from this framework cannot be considered markedness values, which either penalize an order (make it less frequent) or do not. In addition to making the model weights less interpretable, this non-monotonicity means that the model has the flexibility to take advantage of artifacts of the discretization of word order frequencies into bins. 2.7 Comparison with Cysouw (2010) As stated in Section 2.5, our approach here is very similar to that of Cysouw (2010): we use the same statistical model class and the same theory comparison (Cinque/Cysouw/Dryer).", "startOffset": 103, "endOffset": 1103}, {"referenceID": 0, "context": "5, our approach here is very similar to that of Cysouw (2010): we use the same statistical model class and the same theory comparison (Cinque/Cysouw/Dryer). The differences are as follows: \u2022 We use the more recent data of Dryer (in prep) rather than the earlier data of Dryer (2006); \u2022 We use the feature set of Dryer (in prep) rather than the earlier feature set of Dryer (in prep); \u2022 We correct what we believe is a featurization error made by Cysouw in featurizing Cinque\u2019s theory.", "startOffset": 135, "endOffset": 283}, {"referenceID": 0, "context": "2 5 Cinque (2005) (our features) -53.", "startOffset": 4, "endOffset": 18}, {"referenceID": 0, "context": "2 5 Cinque (2005) (our features) -53.0 8 Cinque (2005) (Merlo\u2019s features) -56.", "startOffset": 4, "endOffset": 55}, {"referenceID": 0, "context": "2 5 Cinque (2005) (our features) -53.0 8 Cinque (2005) (Merlo\u2019s features) -56.5 8 Table 3: Log likelihoods of adjusted frequency data under various models, and the degrees of freedom (d.f.) of those models. Cysouw (2010) model.", "startOffset": 4, "endOffset": 221}, {"referenceID": 0, "context": "So when predicting genera, we get the best fit to the data using the set of features from Dryer (in prep), followed by Cinque\u2019s (2005) features, followed by Cysouw\u2019s (2010) features.", "startOffset": 119, "endOffset": 135}, {"referenceID": 0, "context": "So when predicting genera, we get the best fit to the data using the set of features from Dryer (in prep), followed by Cinque\u2019s (2005) features, followed by Cysouw\u2019s (2010) features.", "startOffset": 119, "endOffset": 173}, {"referenceID": 0, "context": "So when predicting genera, we get the best fit to the data using the set of features from Dryer (in prep), followed by Cinque\u2019s (2005) features, followed by Cysouw\u2019s (2010) features. We think Cinque\u2019s model comes out worse when predicting genera primarily because it underpredicts NnAD orders, whereas the Dryer model gets that order exactly correct. This can be seen in Figure 6, which shows model predictions, and Figure 7, which shows signed \u03c72 discrepancies compared to genera counts. Figures 8 and 9 show the optimal feature weights for the Dryer and Cinque models, respectively, when predicting genera counts. 4 Discussion The results give clear evidence that the Dryer (in prep) and Cinque (2005) model provide feature systems that have better predictive power than the model of Cysouw (2010).", "startOffset": 119, "endOffset": 704}, {"referenceID": 0, "context": "So when predicting genera, we get the best fit to the data using the set of features from Dryer (in prep), followed by Cinque\u2019s (2005) features, followed by Cysouw\u2019s (2010) features. We think Cinque\u2019s model comes out worse when predicting genera primarily because it underpredicts NnAD orders, whereas the Dryer model gets that order exactly correct. This can be seen in Figure 6, which shows model predictions, and Figure 7, which shows signed \u03c72 discrepancies compared to genera counts. Figures 8 and 9 show the optimal feature weights for the Dryer and Cinque models, respectively, when predicting genera counts. 4 Discussion The results give clear evidence that the Dryer (in prep) and Cinque (2005) model provide feature systems that have better predictive power than the model of Cysouw (2010). But in our opinion they do not give strong reason", "startOffset": 119, "endOffset": 800}, {"referenceID": 0, "context": "Figure 5: Feature weights from the Cinque (2005) model when predicting adjusted frequency (first column).", "startOffset": 35, "endOffset": 49}], "year": 2017, "abstractText": "A frequent object of study in linguistic typology is the order of elements {demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to predict the relative frequencies of these orders across languages. Here we use Poisson regression to statistically compare some prominent accounts of this variation. We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).", "creator": "LaTeX with hyperref package"}}}