{"id": "1709.02800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams", "abstract": "Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the MOA libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with 8 state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements.", "histories": [["v1", "Fri, 8 Sep 2017 00:58:40 GMT  (1076kb,D)", "http://arxiv.org/abs/1709.02800v1", "33 Pages, Accepted for publication in The ACM Transactions on Knowledge Discovery from Data (TKDD) in August 2017"]], "COMMENTS": "33 Pages, Accepted for publication in The ACM Transactions on Knowledge Discovery from Data (TKDD) in August 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hamed r bonab", "fazli can"], "accepted": false, "id": "1709.02800"}, "pdf": {"name": "1709.02800.pdf", "metadata": {"source": "CRF", "title": "GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams", "authors": ["Hamed R. Bonab", "Fazli Can"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "1. INTRODUCTION", "text": "This year, we have reached the stage where we feel we are in a position to achieve the objectives I have mentioned, but we do not feel we are in a position to achieve them."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "In this section, we explain our assumptions and specifications for time-evolving data flows. We distinguish different types of concept drift based on literature. We discuss different approaches to adapting concept drift to evolving environments, focusing on ensemble methods as they are inherently better able to handle concept drift and have proven to be better than individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017]."}, {"heading": "2.1. Basic Concepts and Notations", "text": "The traditional classification problem aims to depict a vector of attributes, x, in a vector of class labels, y \u2032, i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i.e., i., i.e., i.e., i.e., i., i.e., i., i., i., i., i., i.e., i., i.e., i., i.e., i., i., i., i., i., i., i., i.e., i., i"}, {"heading": "2.2. Ensemble Classifiers for Evolving Online Environments", "text": "A recently published study on concept drift adaptation [Gama et al. 2014], presents a new taxonomy of adaptive classifiers using four existing modules of different learning methods in time-changing environments. They are memory management, change properties and loss estimates. In this study, we focus on model management strategies, as a learning object, on the current state of the art, ensemble methods and methods in chronological order. The remaining modules, other than learning objects, are outside the scope of this paper. Two recently published studies on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al.] show the importance of ensemble learning methods, especially on changing environments and current research projects."}, {"heading": "3. GOOWE: GEOMETRICALLY OPTIMUM AND ONLINE-WEIGHTED ENSEMBLE", "text": "It is about the question of whether it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way and a way in which it is about a way in which it is about a way and a way and a way it is about a way and a way and a way and a way it is about a way in which it is about a way and a way and a way in which it is about a way and a way and a way it is about a way and a way and a way in which it is about a way and a way and a way and a way it is about a way and a way and a way in which it is about a way and a way and a way and a way in which it is about a way"}, {"heading": "4. EXPERIMENTAL EVALUATION SETUP", "text": "In the following sections, we present our experimental evaluation for various simulation scenarios that have been performed to form our proposed ensemble. In summary, our experimental evaluation is presented as follows: We - first describe synthetic and evolving real-world data streams that are used in our experiments.We explain each one by the type of concept drift, the number of class names and the number of instances used. While there is a lack of trustworthy, evolving real-world data streams [Krawczyk et al. 2017], we try to include all possible known / unknown categories of concept drift in our experiments. We also specify our experimental ACM transactions based on data, Volume 9, No. 4, Article 39, Release Date: August 2017. Maintenance frameworks, comparison elements for strategies (WE totality, GOM weighting and GOM totality)."}, {"heading": "4.1. Datasets as Data Streams", "text": "This year it is more than ever before."}, {"heading": "4.2. Experimental Framework: Detailed Design", "text": "It is a question of the future of the world, and of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the future of the world, of the world, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the world, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of the future, of"}, {"heading": "5. EXPERIMENTAL ANALYSES: THE IMPACT OF WEIGHTING AND MODEL MANAGEMENT STRATEGIES OF GOOWE", "text": "In this section, we focus mainly on answering the question: Why should GOOWE work better in terms of accuracy of predictions, or in other words: when / where in the learning process GOOWE achieves its advantage? To answer this question, we need to examine the effects of the GOOWE weight system on voice aggregation and ensemble maintenance in evolving environments as two main features of GOOWE. These two features distinguish GOOWE from other block-based ensembles, and we show the superiority of GOOWE in comparison to other ensembles based on these two key features. We designed two scenarios to investigate the effects of GOOWE weight system on voice aggregation and ensemble maintenance. Detailed information on each of these scenarios is given below. The main idea in both analyses is that by isolating the examined characteristic, the effects can be investigated. We select a basic and comparatively good ensemble method and specify all settings for training and testing, with the exception of the ensemble being examined."}, {"heading": "5.1. Analysis of Vote Aggregation", "text": "For the evaluation of the weight strategy proposed for GOOWE based on vote capture, we use the AUE2 implementation from the MOA framework, with the exception of vote aggregation, as the basic ensemble method, called Base1. We implement GOOWE's weight system for the Base1 ensemble classifier. The only variant of this new ensemble, compared to the original AUE2 version, ACM-Transactions on Knowledge Discovery from Data, Vol. 4, Article 39, is our weight system for voting. In this way, we are able to examine the impact of each weight function in the vote aggregation on the accuracy of predictions."}, {"heading": "5.2. Analysis of Model Management Strategy", "text": "In order to investigate the superiority of our model management strategy, we use the implementation of AUE2 from the MOA framework as Base2, similar to the previous analysis. In this analysis, we use GOOWE and other baseline weights in the process of decision-making for add-drop components. We implement these baselines for the Base2 ensemble. Note that for aggregating the voices of components in this analysis, we use majority decisions to disadvantage all ensembles equally. We use DWM with \u03b2 = 0.5 and AWE as baselines for this analysis. DWM with \u03b2 = 0.2 yielded the exact same results as DWM with \u03b2 = 0.5. We construct and maintain the Base2 ensemble using these weighting algorithms for each data stream. Table V presents the resulting accuracies. In Table V, we observe a similar superiority of the GOOWE weight system for rapidly changing data streams, compared to slowly changing data streams."}, {"heading": "6. COMPARATIVE EVALUATION", "text": "In this section, we examine GOOWE as an ensemble algorithm, as described in Algorithm 1, and compare it with the 8 most modern ensemble methods. We measure the accuracy of class identification (in percent), the maximum memory usage (in megabytes), and the total processing time of all thousand instances (in CentiSecond) for each of the ensemble algorithms - averages for synthetic data sets and exact values for real-world data sets reported in Table VI, VII, and VIII, respectively. For each synthetic data set, a one-sided variance analysis (ANOVA) is performed using Scheffe multiple comparisons [Scheffe 1959], and the most powerful algorithms are underlined. It is not possible to perform the statistical test of real-world data sets because they have only one value. For each of them, we highlight the most accurate and resource-intensive algorithms."}, {"heading": "6.1. RCD Data Streams with Gradual/Abrupt Drift Patterns", "text": "In recent years, it has become clear that most of them are people who are unable to integrate themselves."}, {"heading": "6.2. LCD Data Streams with Miscellaneous Drift Patterns", "text": "Table VI for the LCD generators (the second 8 lines) shows that GOOWE is among the top tier algorithms in the Rotating Hyperplane, TREE-F and LED datasets, in termsACM transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Release Date: August 2017.of Accuracy. Similar to the gradually changing RBF datasets, the Rotating Hyperplane dataset has incremental drifts. TREE-F is the smallest and fastest changing dataset with recurring drift patterns. These characteristics show the superiority of GOOWE. Generally, we can say for the LCD datasets that GOOWE works better for the rapidly changing datasets, compared with the slow ones. For the LED datasets there is no significant difference between different algorithms, and most of them respond there being no clear concept drifts."}, {"heading": "6.3. Real-World Data Streams with Unknown Drift Patterns", "text": "Table VI, for real world records (the last 4 rows), shows the superiority of GOOWE over other algorithms in PokerHand and CovPokElec records, in terms of accuracy. For CoverType and Airlines records, although GOOWE is not the best performance algorithm, still the difference to the best performing algorithms are less than 1ACM transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Release Date: August 2017. in addition, Table VII and VIII for real world records (the last 4 rows) show reasonable resource usage of GOOWE, in terms of time and storage.Figure 10 shows classification accuracy and memory usage behaviors for the CovPokElec real world records, which we compare with the arrival of GOGOOSE."}, {"heading": "7. STATISTICAL ANALYSIS AND FURTHER DISCUSSION", "text": "In order to ensure the significant difference in averages for classification accuracy, processing time and memory usage, we have performed statistical tests. First, a one-way analysis of variance (ANOVA) test using Scheffe multiple comparisons [Scheffe 1959] were performed at the level of different algorithms for each dataset. The zero hypothesis for each dataset when viewed individually is: There is no significant difference between the algorithms - we performed the Scheffe test at the significance level of \u03b1 = 0.05. The most accurate, least consuming of processing time and least consuming of the storage algorithms are underlined for each row of Table VI, VII and VIII. We have the top-tier group of Effeffe comparison results for each synthetic dataset [Scheffe 1959]. As we have already mentioned, it is not possible to perform the Scheffe dataset test statistically for the only real world as they have one."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we offer a geometrically optimal and online weighted ensemble classification, called GOOWE, for non-stationary environments. The most important contribution of the proposed algorithm is to provide spatial modeling for the use of the linear smallest squares (LSQ) solution to dynamically optimize the components of an ensemble classifier for evolutionary environments. Our algorithm, which differs from the use of LSQ in batch mode, has a dynamically changing optimal weight allocation to component classifiers and continuous training. We use data chunks for training and a sliding instance window containing the latest available data for testing; such an approach offers more robust in our experiments. We use the euclimatic standard as a yardstick for proximity to LSQ and testing."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Manouchehr Takrimi of Bilkent University, Jon M. Patton of Miami University of OH and Alper Can for their valuable comments and references to this essay."}], "references": [{"title": "Learning from time-changing data with adaptive windowing", "author": ["Albert Bifet", "Ricard Gavald\u00e0."], "venue": "Proceedings of the Seventh SIAM International Conference on Data Mining, April 26-28, 2007, Minneapolis, Minnesota, USA. 443\u2013448.", "citeRegEx": "Bifet and Gavald\u00e0.,? 2007", "shortCiteRegEx": "Bifet and Gavald\u00e0.", "year": 2007}, {"title": "MOA: Massive Online Analysis", "author": ["Albert Bifet", "Geoff Holmes", "Richard Kirkby", "Bernhard Pfahringer."], "venue": "Journal of Machine Learning Research 11 (2010), 1601\u20131604.", "citeRegEx": "Bifet et al\\.,? 2010b", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "Leveraging bagging for evolving data streams", "author": ["Albert Bifet", "Geoffrey Holmes", "Bernhard Pfahringer."], "venue": "Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part I. 135\u2013150.", "citeRegEx": "Bifet et al\\.,? 2010a", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "New ensemble methods for evolving data streams", "author": ["Albert Bifet", "Geoffrey Holmes", "Bernhard Pfahringer", "Richard Kirkby", "Ricard Gavald\u00e0."], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009. 139\u2013148.", "citeRegEx": "Bifet et al\\.,? 2009", "shortCiteRegEx": "Bifet et al\\.", "year": 2009}, {"title": "A theoretical framework on the ideal number of classifiers for online ensembles in data streams", "author": ["Hamed R. Bonab", "Fazli Can."], "venue": "Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016. 2053\u20132056.", "citeRegEx": "Bonab and Can.,? 2016", "shortCiteRegEx": "Bonab and Can.", "year": 2016}, {"title": "Combining block-based and online methods in learning ensembles from concept drifting data streams", "author": ["Dariusz Brzezinski", "Jerzy Stefanowski."], "venue": "Information Sciences 265 (2014), 50\u201367.", "citeRegEx": "Brzezinski and Stefanowski.,? 2014a", "shortCiteRegEx": "Brzezinski and Stefanowski.", "year": 2014}, {"title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm", "author": ["Dariusz Brzezinski", "Jerzy Stefanowski."], "venue": "IEEE Transactions on Neural Networks and Learning Systems 25, 1 (2014), 81\u201394.", "citeRegEx": "Brzezinski and Stefanowski.,? 2014b", "shortCiteRegEx": "Brzezinski and Stefanowski.", "year": 2014}, {"title": "Weighted least square ensemble networks", "author": ["Lai-Wan Chan."], "venue": "International Joint Conference on Neural Networks (IJCNN 1999), Vol. 2. IEEE, 1393\u20131396.", "citeRegEx": "Chan.,? 1999", "shortCiteRegEx": "Chan.", "year": 1999}, {"title": "Practical Nonparametric Statistics", "author": ["W. Conover."], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Conover.,? 1999", "shortCiteRegEx": "Conover.", "year": 1999}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar."], "venue": "Journal of Machine Learning Research 7 (2006), 1\u201330.", "citeRegEx": "Dem\u0161ar.,? 2006", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri."], "venue": "Journal of Artificial Intelligence Research (JAIR) 2 (1995), 263\u2013286.", "citeRegEx": "Dietterich and Bakiri.,? 1995", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["Gregory Ditzler", "Robi Polikar."], "venue": "IEEE Transactions on Knowledge and Data Engineering 25, 10 (2013), 2283\u20132301.", "citeRegEx": "Ditzler and Polikar.,? 2013", "shortCiteRegEx": "Ditzler and Polikar.", "year": 2013}, {"title": "Mining high-speed data streams", "author": ["Pedro M. Domingos", "Geoff Hulten."], "venue": "Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Boston, MA, USA, August 20-23, 2000. 71\u201380.", "citeRegEx": "Domingos and Hulten.,? 2000", "shortCiteRegEx": "Domingos and Hulten.", "year": 2000}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["Ryan Elwell", "Robi Polikar."], "venue": "IEEE Transactions on Neural Networks and Learning Systems 22, 10 (2011), 1517\u20131531.", "citeRegEx": "Elwell and Polikar.,? 2011", "shortCiteRegEx": "Elwell and Polikar.", "year": 2011}, {"title": "An adaptive ensemble classifier for mining concept drifting data streams", "author": ["Dewan Md Farid", "Li Zhang", "Alamgir Hossain", "Chowdhury Mofizur Rahman", "Rebecca Strachan", "Graham Sexton", "Keshav Dahal."], "venue": "Expert Systems with Applications 40, 15 (2013), 5895\u20135906.", "citeRegEx": "Farid et al\\.,? 2013", "shortCiteRegEx": "Farid et al\\.", "year": 2013}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire."], "venue": "J. Comput. System Sci. 55, 1 (1997), 119\u2013139.", "citeRegEx": "Freund and Schapire.,? 1997", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Stochastic gradient boosting", "author": ["Jerome H Friedman."], "venue": "Computational Statistics & Data Analysis 38, 4 (2002), 367\u2013378.", "citeRegEx": "Friedman.,? 2002", "shortCiteRegEx": "Friedman.", "year": 2002}, {"title": "Knowledge discovery from data streams", "author": ["Joao Gama."], "venue": "CRC Press.", "citeRegEx": "Gama.,? 2010", "shortCiteRegEx": "Gama.", "year": 2010}, {"title": "On evaluating stream learning algorithms", "author": ["Jo\u00e3o Gama", "Raquel Sebasti\u00e3o", "Pedro Pereira Rodrigues."], "venue": "Machine Learning 90, 3 (2013), 317\u2013346.", "citeRegEx": "Gama et al\\.,? 2013", "shortCiteRegEx": "Gama et al\\.", "year": 2013}, {"title": "A survey on concept drift adaptation", "author": ["Jo\u00e3o Gama", "Indre Zliobaite", "Albert Bifet", "Mykola Pechenizkiy", "Abdelhamid Bouchachia."], "venue": "Comput. Surveys 46, 4 (2014), 44:1\u201344:37.", "citeRegEx": "Gama et al\\.,? 2014", "shortCiteRegEx": "Gama et al\\.", "year": 2014}, {"title": "On appropriate assumptions to mine data streams: Analysis and practice", "author": ["Jing Gao", "Wei Fan", "Jiawei Han."], "venue": "Seventh IEEE International Conference on Data Mining (ICDM 2007). IEEE, 143\u2013152.", "citeRegEx": "Gao et al\\.,? 2007", "shortCiteRegEx": "Gao et al\\.", "year": 2007}, {"title": "A survey on ensemble learning for data stream classification", "author": ["Heitor Murilo Gomes", "Jean Paul Barddal", "Fabr\u0131\u0301cio Enembreck", "Albert Bifet"], "venue": "Comput. Surveys", "citeRegEx": "Gomes et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2017}, {"title": "Classifying uncertain and evolving data streams with distributed extreme learning machine", "author": ["Dong-Hong Han", "Xin Zhang", "Guo-Ren Wang."], "venue": "Journal of Computer Science and Technology 30, 4 (2015), 874\u2013887.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Least squares data fitting with applications", "author": ["Per Christian Hansen", "V\u0131\u0301ctor Pereyra", "Godela Scherer"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2013}, {"title": "How large should ensembles of classifiers be", "author": ["Daniel Hern\u00e1ndez-Lobato", "Gonzalo Mart\u0131\u0301Nez-Mu\u00f1Oz", "Alberto Su\u00e1rez"], "venue": "Pattern Recognition 46,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2013}, {"title": "Mining time-changing data streams", "author": ["Geoff Hulten", "Laurie Spencer", "Pedro M. Domingos."], "venue": "Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. 97\u2013106.", "citeRegEx": "Hulten et al\\.,? 2001", "shortCiteRegEx": "Hulten et al\\.", "year": 2001}, {"title": "Dynamic weighted majority: A new ensemble method for tracking concept drift", "author": ["Jeremy Z Kolter", "Marcus A Maloof."], "venue": "Third IEEE International Conference on Data Mining (ICDM 2003). IEEE, 123\u2013130.", "citeRegEx": "Kolter and Maloof.,? 2003", "shortCiteRegEx": "Kolter and Maloof.", "year": 2003}, {"title": "Using additive expert ensembles to cope with concept drift", "author": ["Jeremy Z. Kolter", "Marcus A. Maloof."], "venue": "Proceedings of 22nd International Conference on Machine Learning, (ICML 2005), Bonn, Germany, August 7-11, 2005. 449\u2013456.", "citeRegEx": "Kolter and Maloof.,? 2005", "shortCiteRegEx": "Kolter and Maloof.", "year": 2005}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["J Zico Kolter", "Marcus A Maloof."], "venue": "Journal of Machine Learning Research 8 (2007), 2755\u20132790.", "citeRegEx": "Kolter and Maloof.,? 2007", "shortCiteRegEx": "Kolter and Maloof.", "year": 2007}, {"title": "Ensemble learning for data stream analysis: A survey", "author": ["Bartosz Krawczyk", "Leandro L Minku", "Jo\u00e3o Gama", "Jerzy Stefanowski", "Micha\u0142 Wo\u017aniak."], "venue": "Information Fusion 37 (2017), 132\u2013156.", "citeRegEx": "Krawczyk et al\\.,? 2017", "shortCiteRegEx": "Krawczyk et al\\.", "year": 2017}, {"title": "Classifier ensembles for changing environments", "author": ["Ludmila I. Kuncheva."], "venue": "Proceedings of the 5th International Workshop on Multiple Classifier Systems, (MCS 2004), Cagliari, Italy, June 9-11, 2004. 1\u201315.", "citeRegEx": "Kuncheva.,? 2004", "shortCiteRegEx": "Kuncheva.", "year": 2004}, {"title": "Classifier ensembles for detecting concept change in streaming data: Overview and perspectives", "author": ["Ludmila I Kuncheva."], "venue": "2nd Workshop SUEMA, Vol. 2008. 5\u201310.", "citeRegEx": "Kuncheva.,? 2008", "shortCiteRegEx": "Kuncheva.", "year": 2008}, {"title": "Limiting the number of trees in random forests", "author": ["Patrice Latinne", "Olivier Debeir", "Christine Decaestecker."], "venue": "International Workshop on Multiple Classifier Systems. Springer, 178\u2013187.", "citeRegEx": "Latinne et al\\.,? 2001", "shortCiteRegEx": "Latinne et al\\.", "year": 2001}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone."], "venue": "Machine Learning 2, 4 (1987), 285\u2013318.", "citeRegEx": "Littlestone.,? 1987", "shortCiteRegEx": "Littlestone.", "year": 1987}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth."], "venue": "Information and Computation 108, 2 (1994), 212\u2013261.", "citeRegEx": "Littlestone and Warmuth.,? 1994", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Classification and novel class detection in concept-drifting data streams under time constraints", "author": ["Mohammad M. Masud", "Jing Gao", "Latifur Khan", "Jiawei Han", "Bhavani M. Thuraisingham."], "venue": "IEEE Transactions on Knowledge and Data Engineering 23, 6 (2011), 859\u2013874.", "citeRegEx": "Masud et al\\.,? 2011", "shortCiteRegEx": "Masud et al\\.", "year": 2011}, {"title": "The impact of diversity on online ensemble eearning in the presence of concept drift", "author": ["Leandro L. Minku", "Allan P. White", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 22, 5 (2010), 730\u2013742.", "citeRegEx": "Minku et al\\.,? 2010", "shortCiteRegEx": "Minku et al\\.", "year": 2010}, {"title": "DDD: A new ensemble approach for dealing with concept drift", "author": ["Leandro L. Minku", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 24, 4 (2012), 619\u2013633.", "citeRegEx": "Minku and Yao.,? 2012", "shortCiteRegEx": "Minku and Yao.", "year": 2012}, {"title": "Evolving stream classification using change detection", "author": ["Ahmad Mustafa", "Ahsanul Haque", "Latifur Khan", "Michael Baron", "Bhavani Thuraisingham."], "venue": "International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2014). IEEE, 154\u2013162.", "citeRegEx": "Mustafa et al\\.,? 2014", "shortCiteRegEx": "Mustafa et al\\.", "year": 2014}, {"title": "ACE: Adaptive classifiers-ensemble system for concept-drifting environments", "author": ["Kyosuke Nishida", "Koichiro Yamauchi", "Takashi Omori."], "venue": "Multiple Classifier Systems. Springer, 176\u2013185.", "citeRegEx": "Nishida et al\\.,? 2005", "shortCiteRegEx": "Nishida et al\\.", "year": 2005}, {"title": "How many trees in a random forest", "author": ["Thais Mayumi Oshiro", "Pedro Santoro Perez", "Jos\u00e9 Augusto Baranauskas."], "venue": "International Workshop on Machine Learning and Data Mining in Pattern Recognition. Springer, 154\u2013168.", "citeRegEx": "Oshiro et al\\.,? 2012", "shortCiteRegEx": "Oshiro et al\\.", "year": 2012}, {"title": "Online Ensemble Learning", "author": ["Nikunj C. Oza."], "venue": "Ph.D. Dissertation. Computer Science Division, Univ. California, Berkeley, CA, USA.", "citeRegEx": "Oza.,? 2001", "shortCiteRegEx": "Oza.", "year": 2001}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["Nikunj C Oza", "Stuart Russell."], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. ACM, 359\u2013364.", "citeRegEx": "Oza and Russell.,? 2001", "shortCiteRegEx": "Oza and Russell.", "year": 2001}, {"title": "Complexity and the geometry of voting", "author": ["Donald G. Saari."], "venue": "Mathematical and Computer Modelling 48, 9-10 (2008), 1335\u20131356.", "citeRegEx": "Saari.,? 2008", "shortCiteRegEx": "Saari.", "year": 2008}, {"title": "The Analysis of Variance", "author": ["Henry Scheffe."], "venue": "John Wiley, New York.", "citeRegEx": "Scheffe.,? 1959", "shortCiteRegEx": "Scheffe.", "year": 1959}, {"title": "A streaming ensemble algorithm (SEA) for large-scale classification", "author": ["W. Nick Street", "YongSeog Kim."], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. 377\u2013382.", "citeRegEx": "Street and Kim.,? 2001", "shortCiteRegEx": "Street and Kim.", "year": 2001}, {"title": "Online ensemble learning of data streams with gradually evolved classes", "author": ["Yu Sun", "Ke Tang", "Leandro L Minku", "Shuo Wang", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 28, 6 (2016), 1532\u20131545.", "citeRegEx": "Sun et al\\.,? 2016", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Analysis of decision boundaries in linearly combined neural classifiers", "author": ["Kagan Tumer", "Joydeep Ghosh."], "venue": "Pattern Recognition 29, 2 (1996), 341\u2013348.", "citeRegEx": "Tumer and Ghosh.,? 1996", "shortCiteRegEx": "Tumer and Ghosh.", "year": 1996}, {"title": "Mining concept-drifting data streams using ensemble classifiers", "author": ["Haixun Wang", "Wei Fan", "Philip S. Yu", "Jiawei Han."], "venue": "Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24 - 27, 2003. 226\u2013235.", "citeRegEx": "Wang et al\\.,? 2003", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Resampling-based ensemble methods for online class imbalance learning", "author": ["Shuo Wang", "Leandro L Minku", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 27, 5 (2015), 1356\u20131368.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Cascading randomized weighted majority: a new online ensemble learning algorithm", "author": ["Mohammadzaman Zamani", "Hamid Beigy", "Amirreza Shaban."], "venue": "Intelligent Data Analysis 20, 4 (2016), 877\u2013889.", "citeRegEx": "Zamani et al\\.,? 2016", "shortCiteRegEx": "Zamani et al\\.", "year": 2016}, {"title": "Categorizing and mining concept drifting data streams", "author": ["Peng Zhang", "Xingquan Zhu", "Yong Shi."], "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008. 812\u2013820.", "citeRegEx": "Zhang et al\\.,? 2008", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Active learning from stream data using optimal weight classifier ensemble", "author": ["Xingquan Zhu", "Peng Zhang", "Xiaodong Lin", "Yong Shi."], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B 40, 6 (2010), 1607\u20131621.", "citeRegEx": "Zhu et al\\.,? 2010", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}, {"title": "How good is the electricity benchmark for evaluating concept drift adaptation", "author": ["Indre Zliobaite."], "venue": "CoRR abs/1301.3524 (2013).", "citeRegEx": "Zliobaite.,? 2013", "shortCiteRegEx": "Zliobaite.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "In such dynamically evolving and non-stationary environments, data distribution can change over time, this is referred to as concept drift [Gama et al. 2014].", "startOffset": 139, "endOffset": 157}, {"referenceID": 19, "context": "Real concept drift is referred to as change in the conditional distribution of the output, given the input features, while the distribution of the input may stay unchanged [Gama 2010; Gama et al. 2014].", "startOffset": 172, "endOffset": 201}, {"referenceID": 3, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 19, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 21, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 29, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 3, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 48, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 21, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 29, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 52, "context": "However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments [Zhu et al. 2010].", "startOffset": 139, "endOffset": 156}, {"referenceID": 3, "context": "For evaluating the performance of an algorithm in a time-evolving data stream domain, it is necessary to use tens of millions of examples [Bifet et al. 2009].", "startOffset": 138, "endOffset": 157}, {"referenceID": 29, "context": "There is a shortage in trusted evolving real-world publicly available datasets for testing stream classifiers [Krawczyk et al. 2017].", "startOffset": 110, "endOffset": 132}, {"referenceID": 3, "context": "We use the most popular real-world datasets, and for generating synthetic data streams, we use the MOA libraries [Bifet et al. 2009].", "startOffset": 113, "endOffset": 132}, {"referenceID": 3, "context": "For classification accuracy measurement, we use the Interleaved Test-Then-Train approach [Bifet et al. 2009].", "startOffset": 89, "endOffset": 108}, {"referenceID": 23, "context": "\u2014 Provide a spatial modeling for online ensembles and use the linear least squares (LSQ) solution [Hansen et al. 2013] for optimizing the weights of components of an ensemble classifier for evolving environments.", "startOffset": 98, "endOffset": 118}, {"referenceID": 19, "context": "Four patterns of real concept drift over time (revised from [Gama et al. 2014]).", "startOffset": 60, "endOffset": 78}, {"referenceID": 3, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 19, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 21, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 29, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 3, "context": "Classifiers are supposed to use limited memory and limited processing time per instance [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2004].", "startOffset": 88, "endOffset": 140}, {"referenceID": 19, "context": "Classifiers are supposed to use limited memory and limited processing time per instance [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2004].", "startOffset": 88, "endOffset": 140}, {"referenceID": 19, "context": "P (yt+1|xt+1) 6= P (yt|xt), while the distribution of the input vector itself, P (xt), may remain the same [Gama et al. 2014].", "startOffset": 107, "endOffset": 125}, {"referenceID": 51, "context": "[Zhang et al. 2008] categorized real concept drifts into two scenarios; Loose Concept Drift (LCD) where only a change in P (yt|xt) causes the concept drift, and Rigorous Concept Drift (RCD), where change in both P (yt|xt) and P (xt) cause the concept drift.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "The reader is referred to [Gama et al. 2014] for various settings of the problem.", "startOffset": 26, "endOffset": 44}, {"referenceID": 19, "context": "Since most of the real-world problems are complex mixtures of these concept drifts, we expect any classifier to react and adapt reasonably to different types of concept drifts and remain robust to outliers, predicting with acceptable resource requirements [Gama et al. 2014].", "startOffset": 256, "endOffset": 274}, {"referenceID": 19, "context": "A recently published survey on concept drift adaption [Gama et al. 2014], presents a new taxonomy of adaptive classifiers using four existing modules of various learning methods in time-evolving environments.", "startOffset": 54, "endOffset": 72}, {"referenceID": 21, "context": "Two more recently published surveys on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al. 2017] show the importance of ensemble learning methods, especially on changing environments, and present ongoing research challenges.", "startOffset": 82, "endOffset": 123}, {"referenceID": 29, "context": "Two more recently published surveys on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al. 2017] show the importance of ensemble learning methods, especially on changing environments, and present ongoing research challenges.", "startOffset": 82, "endOffset": 123}, {"referenceID": 21, "context": "cover existing data stream ensemble learning methods, propose a consistent taxonomy among them, and compare them based on some important aspects like vote aggregation, diversity measurement, and dynamic updates [Gomes et al. 2017].", "startOffset": 211, "endOffset": 230}, {"referenceID": 29, "context": "discuss more advanced topics such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations, and structured outputs with a focus on ensemble learning [Krawczyk et al. 2017].", "startOffset": 205, "endOffset": 227}, {"referenceID": 36, "context": "In addition, there are some works to measure and maintain the diversity of component classifiers [Minku et al. 2010; Minku and Yao 2012].", "startOffset": 97, "endOffset": 136}, {"referenceID": 48, "context": "WINNOW [Littlestone 1987] Passive \u00d7 \u00d7 \u00d7 WM [Littlestone and Warmuth 1994] Passive \u00d7 \u00d7 \u00d7 Hedge(\u03b2) [Freund and Schapire 1997] Passive \u00d7 \u00d7 \u00d7 SEA [Street and Kim 2001] Passive \u00d7 \u00d7 \u00d7 OzaBag/OzaBoost [Oza 2001; Oza and Russell 2001] Passive \u00d7 \u00d7 \u00d7 DWM [Kolter and Maloof 2003; 2007] Passive \u00d7 \u00d7 AWE [Wang et al. 2003] Passive \u00d7 \u00d7 ACE [Nishida et al.", "startOffset": 292, "endOffset": 310}, {"referenceID": 39, "context": "2003] Passive \u00d7 \u00d7 ACE [Nishida et al. 2005] Active \u00d7 \u00d7 LevBag [Bifet et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 2, "context": "2005] Active \u00d7 \u00d7 LevBag [Bifet et al. 2010a] Active \u00d7 Learn++.", "startOffset": 24, "endOffset": 44}, {"referenceID": 48, "context": "The Accuracy Weighted Ensemble (AWE) [Wang et al. 2003] alternatively suggests a general framework for mining changing data streams using weighted ensemble classifiers by re-evaluating ensemble components with incoming data chunks.", "startOffset": 37, "endOffset": 55}, {"referenceID": 3, "context": "In particular, ensembles built on large data chunks may react too slowly to sudden drifts occurring inside the chunk [Bifet et al. 2009; Brzezinski and Stefanowski 2014b].", "startOffset": 117, "endOffset": 170}, {"referenceID": 39, "context": "To overcome this problem, Adaptive Classifier Ensemble (ACE) [Nishida et al. 2005], proposed an algorithm which uses a hybrid of one online classifier and a collection of batch classifiers (a mixture of active and passive approaches) along with a drift detection mechanism.", "startOffset": 61, "endOffset": 82}, {"referenceID": 2, "context": "Bifet [Bifet et al. 2010a] introduced Leverage Bagging (LevBag) as an extended version of OzaBagging, using the first four strategies of Kuncheva.", "startOffset": 6, "endOffset": 26}, {"referenceID": 21, "context": "Determining the number of component classifiers for an ensemble, discussed briefly in [Gomes et al. 2017; Krawczyk et al. 2017], is an important problem since it has high impact on the prediction ability of an ensemble, and resource consumptions, in terms of time and memory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 29, "context": "Determining the number of component classifiers for an ensemble, discussed briefly in [Gomes et al. 2017; Krawczyk et al. 2017], is an important problem since it has high impact on the prediction ability of an ensemble, and resource consumptions, in terms of time and memory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 32, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 40, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 24, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 18, "context": "d) of the whole stream data is not true for evolving online environments [Gama et al. 2013].", "startOffset": 73, "endOffset": 91}, {"referenceID": 20, "context": "The possibilities of changes are; \u201cfeature changes\u201d, or evolving of p(x) with time stamp t, \u201cconditional change\u201d, or the changes of class label y assignment to feature vector x, and \u201cdual changes\u201d, which includes both [Gao et al. 2007].", "startOffset": 218, "endOffset": 235}, {"referenceID": 51, "context": "[Zhang et al. 2008] categorized these change into LCD and RCD scenarios.", "startOffset": 0, "endOffset": 19}, {"referenceID": 38, "context": "h [Mustafa et al. 2014].", "startOffset": 2, "endOffset": 23}, {"referenceID": 38, "context": "There is a recent study for dynamic determination of chunk size according to concept drift speed [Mustafa et al. 2014].", "startOffset": 97, "endOffset": 118}, {"referenceID": 20, "context": "[Gao et al. 2007] categorized stream classifiers into two groups: The first group updates the training distribution as soon as the labeled instance becomes available, and the second group receives labeled data in chunks and updates the model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Inspired from the geometry of voting [Saari 2008] and using the least squares problem (LSQ) [Hansen et al. 2013], we designed a geometrically optimum and onlineweighted ensemble method for evolving environments, called GOOWE.", "startOffset": 92, "endOffset": 112}, {"referenceID": 23, "context": "There are clear statistical, mathematical, and computational advantages of using the Euclidean norm [Hansen et al. 2013].", "startOffset": 100, "endOffset": 120}, {"referenceID": 23, "context": "The corresponding residual vector is r = o\u2212 Sw, where for each instance Ii, S \u2208 Rm\u00d7p is the matrix with relevance scores sij in each row, w is the vector of weights to be determined, and o is the vector of ideal-point [Hansen et al. 2013].", "startOffset": 218, "endOffset": 238}, {"referenceID": 23, "context": "In the sense of the least squares solution [Hansen et al. 2013], since it is probable that A is rank-deficient, we may not have a unique solution and we denote the minimizer by w\u2217.", "startOffset": 43, "endOffset": 63}, {"referenceID": 23, "context": "According to Theorem 9 of [Hansen et al. 2013], the normal equations for w\u2217 can be written as", "startOffset": 26, "endOffset": 46}, {"referenceID": 23, "context": "The QR factorization suggests less expensive solutions for both full rank and rankdeficient cases [Hansen et al. 2013].", "startOffset": 98, "endOffset": 118}, {"referenceID": 29, "context": "While there is a shortage in trusted evolving realworld streams [Krawczyk et al. 2017], we try to include all possible known/unknown categories of concept drift in our experiments.", "startOffset": 64, "endOffset": 86}, {"referenceID": 19, "context": "Some studies use real-world datasets with artificial concept drifts, called real-world data with forced/synthetic concept drift [Gama et al. 2014].", "startOffset": 128, "endOffset": 146}, {"referenceID": 3, "context": "Synthetic data has several benefits like being easy to reproduce, having a low cost of storage and transmission, but most importantly, it provides an advantage of knowing where exactly drift has happened [Bifet et al. 2009; Gama et al. 2014].", "startOffset": 204, "endOffset": 241}, {"referenceID": 19, "context": "Synthetic data has several benefits like being easy to reproduce, having a low cost of storage and transmission, but most importantly, it provides an advantage of knowing where exactly drift has happened [Bifet et al. 2009; Gama et al. 2014].", "startOffset": 204, "endOffset": 241}, {"referenceID": 3, "context": "A proposed algorithm should be capable of handling large data streams\u2014with potentially an infinite number of instances [Bifet et al. 2009].", "startOffset": 119, "endOffset": 138}, {"referenceID": 3, "context": "Similar to common approaches [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; 2014a; Street and Kim 2001], in order to cover all patterns of changes over time; sudden/abrupt, incremental, gradual, and reoccurring as concept drifts including blips or noise; we use synthetic data stream generators, implemented in the MOA framework.", "startOffset": 29, "endOffset": 110}, {"referenceID": 51, "context": "[Zhang et al. 2008], we have 8 Rigorous Concept Drifting (RCD) and 8 Loose Concept Drifting (LCD) synthetic datasets.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "[Bifet et al. 2009] specified Random RBF generator as the RCD data stream, and the rest of synthetic data stream generators as the LCD data stream.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "We generate abruptly changing data streams using the sigmoid join operator (c = a \u2295Wt0 b; t0: point of change, W : length of change) [Bifet et al. 2009].", "startOffset": 133, "endOffset": 152}, {"referenceID": 25, "context": "Concept drift is defined by changing the orientation and position of the hyperplane [Hulten et al. 2001].", "startOffset": 84, "endOffset": 104}, {"referenceID": 1, "context": "Drift is defined by abruptly changing the tree after a given number of examples [Bifet et al. 2010b].", "startOffset": 80, "endOffset": 100}, {"referenceID": 3, "context": "CovPokElec is obtained by merging all attributes, and assuming that each dataset corresponds to a different concept [Bifet et al. 2009].", "startOffset": 116, "endOffset": 135}, {"referenceID": 1, "context": "In this paper, we use the Massive Online Analysis (MOA)5 framework [Bifet et al. 2010b].", "startOffset": 67, "endOffset": 87}, {"referenceID": 48, "context": "In our experiments, according to the chunk size analysis of [Wang et al. 2003] and similar to the experimental evaluations of [Brzezinski and Stefanowski 2014b], the chunk size for block-based ensembles (namely DWM, NSE, AWE, AUE2, and GOOWE) is set to 500 instances.", "startOffset": 60, "endOffset": 78}, {"referenceID": 48, "context": "Although this length can be smaller for most of the ensembles, to perform an equivalent comparison, we choose this value based on the suggested minimum chunk length of AWE [Wang et al. 2003].", "startOffset": 172, "endOffset": 190}, {"referenceID": 3, "context": "By considering the main requirements of data stream environments [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Street and Kim 2001] in our experimental setup, we chose the interleaved Test-Then-Train procedure for measuring prediction accuracy values.", "startOffset": 65, "endOffset": 139}, {"referenceID": 3, "context": "We draw scatter diagrams of the algorithms on the arrival of new chunks of data streams, as in [Bifet et al. 2009; Elwell and Polikar 2011; Brzezinski and Stefanowski 2014b].", "startOffset": 95, "endOffset": 173}, {"referenceID": 23, "context": "The LSQ proved to react well in noisy situations [Hansen et al. 2013], as it did in our algorithm for data streams with concept drifts.", "startOffset": 49, "endOffset": 69}], "year": 2017, "abstractText": "Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the MOA libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with 8 state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements. CCS Concepts: rInformation systems \u2192 Data stream mining; rTheory of computation \u2192 Online learning theory;", "creator": "TeX"}}}