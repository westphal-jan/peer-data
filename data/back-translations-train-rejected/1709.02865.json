{"id": "1709.02865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Prosocial learning agents solve generalized Stag Hunts better than selfish ones", "abstract": "There is much interest in applying reinforcement learning methods to multi-agent systems. A popular way to do so is the method of reactive training -- ie. treating other agents as if they are a stationary part of the learner's environment. Dyads of such learners, if they converge, will converge to Nash equilibria of the game. However, there is an important game theoretic issue here: positive-sum games can have multiple equilibria which differ in their payoffs. We show that even in simple coordination games reactive reinforcement learning agents will often coordinate on equilibria with suboptimal payoffs for both agents. We also show that receiving utility from rewards other agents receive - ie. having prosocial preferences - leads agents to converging to better equilibria in a class of generalized Stag Hunt games. We show this analytically for matrix games and experimentally for more complex Markov versions. Importantly, this is true even if only one of the agents has social preferences. This implies that even if an agent designer only controls a single agent out of a dyad and only cares about their agent's payoff, it can still be better for the designer to make the agent prosocial rather than selfish.", "histories": [["v1", "Fri, 8 Sep 2017 21:52:58 GMT  (350kb,D)", "http://arxiv.org/abs/1709.02865v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["alexander peysakhovich", "adam lerer"], "accepted": false, "id": "1709.02865"}, "pdf": {"name": "1709.02865.pdf", "metadata": {"source": "CRF", "title": "Prosocial learning agents solve generalized Stag Hunts better than selfish ones", "authors": ["Alexander Peysakhovich", "Adam Lerer"], "emails": [], "sections": [{"heading": "Introduction", "text": "In fact, we are able to maneuver ourselves into a situation in which we see ourselves in a position, in which we are able to change the world, and in which we are able to change the world, \"he said in an interview with the New York Times."}, {"heading": "Risk Dominance, the Stag Hunt, and Equilibrium Selection", "text": "We start by illustrating the intuition behind our results with the simple 2-player matrix Stag Hunt. In the Stag Hunt, players simultaneously choose either a risky option (Hunt the Stag) or a safe option (Forage), shown in the payout matrix below, whereas Hunting yields a higher payout if the other person also hunts but yields a poor payout if the player shows that he can hunt alone (because the Stag Hunt can hunt them), shown in the payout matrix below (rows represent strategy decisions for player 1, columns represent strategy decisions for player 2, entries represent payouts for each player as a function of the strategies chosen. Player 2 Hunt ForagePlayer 1 Hunt (2) (\u2212 g, 1) Forage (1, \u2212 g) Leave A1, A2 are the players \"action spaces, and letRi (a1, a2) will receive the reward players from a pair of actions."}, {"heading": "By the Stag Hunt inequalities, d \u2212 c < b \u2212 c, and both are", "text": "The intuition for the theorem stems from the fact that social preferences change the (A, B) payout for player 1 into (1 \u2212 \u03b1) c + \u03b1b. If b > c (intuitive, if the risk of coordination largely falls on the person who chooses A), then this enlarges the pelvis of A. If b > d > c then for some \u03b1, this makes A a (weak) dominant strategy. If A is a dominant strategy for one of the players, then the unique equilibrium into which the dyad will converge is (A, A). The same logic can be extended to coordination games for 2 players N \u00d7 N, as long as each 2 \u00d7 2 subset of this game is a generalized stag hunt (we leave the proof in the appendix)."}, {"heading": "Experiments", "text": "Our theorem implies that if we equip agents with prosocial preferences, the dynamics should now more reliably select the yield-dominant equilibrium in 2 x 2 matrix games. Relative improvement depends on the learning rule applied, so our experiments will first examine the effects of prosociality on the RL agents trained in these simple games. Secondly, it is not clear whether the results from the simple 2 x 2 case will extend to Markov games, where strategy spaces are much more complex. Therefore, we will investigate the influence of social preferences on coordination in more complex games."}, {"heading": "Matrix Stag Hunt", "text": "We start with the Matrix Stag Hunt game discussed in the introduction. We vary the prosociality of the two players as well as the g payout. We train political gradient-based agents and add persistent experiments (5 percent chance of evenly random action selection in each period) to maintain exploration. To optimize, we use Adam (Kingma and Ba 2014) with a learning rate of.01 as well as standard random initialization. We train each agent pair for 2000 rounds and consider average behavior over the last 100 rounds. We train 100 replicas and evaluate them together. We see from Figure 2 (left) that both g and social preferences influence convergence to the prevailing payout equilibrium. There are situations in which we control both agents (e.g. when using self-play to find good strategies). In this case, we see that adding social preferences to both agents can contribute to finding good balances when we are using agents, but not affecting the dynamics of what we are doing in an important new question."}, {"heading": "Markov Games", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "Conclusion", "text": "We have shown that game theory properties of multi-agent systems can impede the ability of RL methods to converge Pareto optimal strategies, even if such strategies are balances. We have also shown a simple method that can help alleviate this problem in a particular class of games. However, our results show the importance of game theory for the practical development of AI.We focus on off-equilibrium payments as an important criterion for determining the basis of attractiveness of multiagent optimization. However, one important thing we have omitted is functional approximation. Much modern RL does not work in the state space of a problem, but approaches state space using neural networking techniques (Silver et al. 2016; Mnih et al. 2015).The architectures of these networks can influence the relative complexity and thus the relative probability of different political strategies, which in turn can influence the attractiveness of different systems."}, {"heading": "Appendix", "text": "Proof 2 (episode 1) Let's consider a N \u00b7 N payout matrix U for Agent 1; let's assume symmetry, i.e. U (2) = UT for simplicity's sake. We can arrange the strategies so that for all i < j, Uii \u2265 Ujj without loss of universality. Then, each sub-space of U is a deer hunt, if for all i < j, Uii > Uji \u2265 Ujj > Uij."}, {"heading": "If agent 1 has prosociality \u03b1, then its payoff matrix is U\u03b1 = (1\u2212 \u03b1)U + \u03b1UT i.e. a linear interpolation between U and", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "UT , so there exists \u03b1 < 1 for which", "text": "U\u03b1ii > Uij \u2265 U\u03b1jj > U\u03b1ji for all i < j. So for Agent 1 I weakly dominate j for all i < j. If Agent 1 plays 0, then 0 is a dominant strategy for Agent 2 (for each \u03b1)."}, {"heading": "Details of Markov Game Training", "text": "Our policy is modeled by a multi-layer Convolutionary Neural Network that directly calculates \u03c0. For a given board size, the model has dlog (2) e + 1 repeated layers, each consisting of a 2D folding with core size 3, followed by batch normalization and ReLU. The first layer has step 1, while the successive layers each have step 2, reducing width and height from k to dk / 2, while doubling the number of channels. We perform reinforcement learning using a policy gradient using REINFORCE (Williams 1992) and RMSProp for optimization. The model is updated episodically in batches of 64 episodes, with a discount rate of 0.99. Markov Hirschjagd and Harvest are trained for 200,000 episode lengths, with episode lengths taken from Exp (250), while the escalation is trained for 500,000 episodes longest (since the escalation extends only over the duration of the Hunt for the loss of coordination)."}], "references": [{"title": "D", "author": ["A.A. Arechar", "A. Dreber", "D. Fudenberg", "Rand"], "venue": "G.", "citeRegEx": "Arechar et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["J.W. Crandall", "M. Oudah", "F. IshowoOloko", "S. Abdallah", "J.-F. Bonnefon", "M. Cebrian", "A. Shariff", "Goodrich"], "venue": "A.; Rahwan, I.; et al.", "citeRegEx": "Crandall et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "J", "author": ["A. Das", "S. Kottur", "Moura"], "venue": "M.; Lee, S.; and Batra, D.", "citeRegEx": "Das et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "A", "author": ["I. Erev", "Roth"], "venue": "E.", "citeRegEx": "Erev and Roth 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Emergent language in a multi-modal, multi-step referential game. arXiv preprint arXiv:1705.10369", "author": ["Evtimova"], "venue": null, "citeRegEx": "Evtimova,? \\Q2017\\E", "shortCiteRegEx": "Evtimova", "year": 2017}, {"title": "Y", "author": ["Foerster, J.", "Assael"], "venue": "M.; de Freitas, N.; and Whiteson, S.", "citeRegEx": "Foerster et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Counterfactual multiagent policy gradients. arXiv preprint arXiv:1705.08926", "author": ["Foerster"], "venue": null, "citeRegEx": "Foerster,? \\Q2017\\E", "shortCiteRegEx": "Foerster", "year": 2017}, {"title": "D", "author": ["D. Fudenberg", "Levine"], "venue": "K.", "citeRegEx": "Fudenberg and Levine 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "and Peysakhovich", "author": ["D. Fudenberg"], "venue": "A.", "citeRegEx": "Fudenberg and Peysakhovich 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Tirole", "author": ["D. Fudenberg"], "venue": "J.", "citeRegEx": "Fudenberg and Tirole 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "J", "author": ["Harsanyi"], "venue": "C.; Selten, R.; et al.", "citeRegEx": "Harsanyi. Selten. and others 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "M", "author": ["O.P. Hauser", "D.G. Rand", "A. Peysakhovich", "Nowak"], "venue": "A.", "citeRegEx": "Hauser et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Titov", "author": ["S. Havrylov"], "venue": "I.", "citeRegEx": "Havrylov and Titov 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "and Silver", "author": ["J. Heinrich"], "venue": "D.", "citeRegEx": "Heinrich and Silver 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["Kandori, M.", "Mailath"], "venue": "J.; and Rob, R.", "citeRegEx": "Kandori. Mailath. and Rob 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "and Kudenko", "author": ["S. Kapetanakis"], "venue": "D.", "citeRegEx": "Kapetanakis and Kudenko 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["M. Kleiman-Weiner", "M. Ho", "J. Austerweil", "M.L. Littman", "Tenenbaum"], "venue": "B.", "citeRegEx": "Kleiman.Weiner et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Peysakhovich Lazaridou", "A. Baroni 2017] Lazaridou", "A. Peysakhovich", "M. Baroni"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Lazaridou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2017}, {"title": "J", "author": ["Leibo"], "venue": "Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.; and Graepel, T.", "citeRegEx": "Leibo et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "and Peysakhovich", "author": ["A. Lerer"], "venue": "A.", "citeRegEx": "Lerer and Peysakhovich 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Y", "author": ["M. Lewis", "D. Yarats", "Dauphin"], "venue": "N.; Parikh, D.; and Batra, D.", "citeRegEx": "Lewis et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-agent actor-critic for mixed cooperative-competitive environments", "author": ["Lowe"], "venue": "arXiv preprint arXiv:1706.02275", "citeRegEx": "Lowe,? \\Q2017\\E", "shortCiteRegEx": "Lowe", "year": 2017}, {"title": "D", "author": ["A. Mao", "L. Dworkin", "S. Suri", "Watts"], "venue": "J.", "citeRegEx": "Mao et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "G", "author": ["Matignon, L.", "Laurent"], "venue": "J.; and Le Fort-Piat, N.", "citeRegEx": "Matignon. Laurent. and Le Fort.Piat 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland"], "venue": "K.; Ostrovski, G.; et al.", "citeRegEx": "Mnih et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["Mnih, V.", "Badia"], "venue": "P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K.", "citeRegEx": "Mnih et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["Neumann"], "venue": "v.", "citeRegEx": "Neumann 1928", "shortCiteRegEx": null, "year": 1928}, {"title": "M", "author": ["Nowak"], "venue": "A.", "citeRegEx": "Nowak 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Peysakhovich", "author": ["A. Ouss"], "venue": "A.", "citeRegEx": "Ouss and Peysakhovich 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Theoretical advantages of lenient learners: An evolutionary game theoretic perspective", "author": ["Tuyls Panait", "L. Luke 2008] Panait", "K. Tuyls", "S. Luke"], "venue": "Journal of Machine Learning Research 9(Mar):423\u2013457", "citeRegEx": "Panait et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Panait et al\\.", "year": 2008}, {"title": "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv preprint arXiv:1703.10069", "author": ["Peng"], "venue": null, "citeRegEx": "Peng,? \\Q2017\\E", "shortCiteRegEx": "Peng", "year": 2017}, {"title": "J", "author": ["Perolat, J.", "Leibo"], "venue": "Z.; Zambaldi, V.; Beattie, C.; Tuyls, K.; and Graepel, T.", "citeRegEx": "Perolat et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "J", "author": ["D.G. Rand", "A. Peysakhovich", "G.T. KraftTodd", "G.E. Newman", "O. Wurzbacher", "M.A. Nowak", "Greene"], "venue": "D.", "citeRegEx": "Rand et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "1953", "author": ["Shapley", "L. S"], "venue": "Stochastic games. Proceedings of the national academy of sciences 39(10):1095\u2013", "citeRegEx": "Shapley 1953", "shortCiteRegEx": null, "year": 1100}, {"title": "N", "author": ["H. Shirado", "Christakis"], "venue": "A.", "citeRegEx": "Shirado and Christakis 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "C", "author": ["D. Silver", "A. Huang", "Maddison"], "venue": "J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al.", "citeRegEx": "Silver et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Value iteration networks", "author": ["Tamar"], "venue": null, "citeRegEx": "Tamar,? \\Q2016\\E", "shortCiteRegEx": "Tamar", "year": 2016}, {"title": "R", "author": ["J.B. Van Huyck", "R.C. Battalio", "Beil"], "venue": "O.", "citeRegEx": "Van Huyck. Battalio. and Beil 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "R", "author": ["Williams"], "venue": "J.", "citeRegEx": "Williams 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Tian", "author": ["Y. Wu"], "venue": "Y.", "citeRegEx": "Wu and Tian 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "There is much interest in applying reinforcement learning methods to multi-agent systems. A popular way to do so is the method of reactive training \u2013 ie. treating other agents as if they are a stationary part of the learner\u2019s environment. Dyads of such learners, if they converge, will converge to Nash equilibria of the game. However, there is an important game theoretic issue here: positive-sum games can have multiple equilibria which differ in their payoffs. We show that even in simple coordination games reactive reinforcement learning agents will often coordinate on equilibria with suboptimal payoffs for both agents. We also show that receiving utility from rewards other agents receive ie. having prosocial preferences leads agents to converging to better equilibria in a class of generalized Stag Hunt games. We show this analytically for matrix games and experimentally for more complex Markov versions. Importantly, this is true even if only one of the agents has social preferences. This implies that even if an agent designer only controls a single agent out of a dyad and only cares about their agent\u2019s payoff, it can still be better for the designer to make the agent prosocial rather than selfish.", "creator": "LaTeX with hyperref package"}}}