{"id": "1709.02878", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow", "abstract": "We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.", "histories": [["v1", "Fri, 8 Sep 2017 23:13:01 GMT  (35kb,D)", "http://arxiv.org/abs/1709.02878v1", "White paper, 7 pages"]], "COMMENTS": "White paper, 7 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["danijar hafner", "james davidson", "vincent vanhoucke"], "accepted": false, "id": "1709.02878"}, "pdf": {"name": "1709.02878.pdf", "metadata": {"source": "CRF", "title": "TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow", "authors": ["Danijar Hafner", "James Davidson", "Vincent Vanhoucke"], "emails": ["mail@danijar.com,", "jcdavidson@google.com,", "vanhoucke@google.com"], "sections": [{"heading": "1 Introduction", "text": "We are introducing a unified interface for reinforcement learning agents with accompanying infrastructure integration with TensorFlow [1], which enables new algorithms to be developed efficiently. Defining a standard algorithm interface for reinforcement learning enables us to reuse a common infrastructure between all algorithms and to change the algorithm as easily as the environment. We aim to accelerate future reinforcement learning research by making this project available to the public. The main objectives of this project are to make it easy to implement new algorithms and train them quickly. Many reinforcement learning methods spend most of their time with the environment, compared to the time required for calculating and applying gradient updates. There are two bottlenecks during the simulation of the algorithm and the environment: the advance of the neural network and the evolution of the environment. We address both limitations through work performed during an interest with Google Brain."}, {"heading": "2 Related Work", "text": "We present a unified agent interface along with and efficient TensorFlow framework for parallel amplification learning and an implementation of the recently introduced proximal gradient algorithm [10].With OpenAI Gym, Brockman et al. [2] introduced a standardized interface for environments that has since found wide acceptance. We extend this work by an interface that combines and assembles multiple gym environments. In addition, we introduce a common interface for learning algorithms that are implemented in TensorFlow. Our goal is that this makes both environments and algorithms easily interchangeable in a research project."}, {"heading": "3 Background", "text": "In this section, we summarize the background required to understand the proximal optimization optimization algorithm = use cases = updates (10), of which we offer an efficient implementation, which we call BatchPPO.We consider the default reinforcement learning setting defined as a \"partially observable Markov decision process\" POMDP = < S, from which we expect multiple episodes (A, P, R, O, s0 > that defines a stochastic sequence of states st-S, observations xt, actions at A, and rewards rt-R. Starting from the initial state s0, we subtract observations xt-O (st), actions at Z (at | x1: t) from politics, rewards rt-R (st, at) from the reward function, and rewards rt-R. Starting from the initial state s0, we subtract observations xt-O (r), actions at the Z, actions at the policy, Z (xrt) from the policy function, and actions rt-Z (xrt)."}, {"heading": "4 Parallelism", "text": "There are two time-consuming calculations in this process: neural network lead-up and environment amplification. Fortunately, we can parallelise both processes by using multiple environments at the same time. Previous implementations used asynchronous labor for this [8], but we can take full advantage of the system with a simpler architecture while saving communication overhead. Our implementation is fully defined within the TensorFlow diagram. First, we parallelise neural network lead-time by vectorizing agent calculation to generate a batch of actions from a batch of observations, similar to Facebook's ELF [12], where agents process multiple observations at each step. Using a batch size during inferences allows us to use the internal thread pool of the TensorFlow session, or hardware accelerators such as GPUs and TPUs."}, {"heading": "5 Implementation", "text": "We present an algorithm interface and infrastructure for efficient interaction with parallel environments. First, we describe the algorithm interface, followed by the batch interface and the simulation operation, which combines the two."}, {"heading": "5.1 Algorithm", "text": "A TensorFlow Agents algorithm defines the inference and learning calculation for a batch of agents. It implements the following functions that define the algorithm's calculation graph: \u2022 begin _ episodes (agent _ indices) is called with a list of agents that start a new episode in the current time step. \u2022 experience (observation, action, reward, done, next _ obs) lets the algorithm process the batch of transition tuples after environments have been kicked. \u2022 end _ episodes (agent _ indices) is called for each batch index that ends an episode in the current time step. \u2022 These operations also provide string sorters that contain TensorFlow summaries. Our infrastructure combines these summaries and writes them together."}, {"heading": "5.2 Environments", "text": "In order to simulate environments efficiently in parallel, we provide the agents.tools.wappers.ExternalProcess (constructor) environment wrapper. This environment wrapper constructs an OpenAI Gym environment within an external process. Calls to step () and reset () and attribute accesses are passed to the environment and await the result. This environment wrapper is compatible with all existing Python environments that correspond to the OpenAI Gym interface [2] as long as they are not dependent on a common global state. agents.tools.tools.tools.batchEnv (envs, blocking) class expands the OpenAI Gym interface to vectorized environments. It combines several OpenAI Gym environments, whereby step () accepts a number of actions and returns batch observations, rewards, completed flags and information objects to the environment."}, {"heading": "5.3 Simulation", "text": "The TensorFlow operating tools (envs, algo, log, reset) simulate a stack environment in diagrams and an algorithm. Optional log and reset tensors specify whether the algorithm should compute TensorFlow summaries in the current step and whether all environments should be reset so that all agent indexes start new episodes. This is used when the training protocol switches between training and evaluation phases to cancel the ongoing episodes. This operation provides the ability to perform a single TensorFlow session call within the training loop so that the training loop resembles the case of supervised learning."}, {"heading": "6 Experiments", "text": "To demonstrate the performance of our infrastructure and implementation, we train the BatchPPO algorithm for control tasks in the MuJoCo domain [13]. Our agent uses two neural networks, one to calculate an average action from the current observation and one to provide an estimate of the state value. We also experimented with recurring neural networks, but generally observed slower learning and similar final performance. The standard deviation of the action elements is learned as an independent parameter vector. Measures are then sampled from the Gaussian distribution parameterized by the predicted mean, and the protocol state deviation is used to define the diagonal elements of the covariance matrix. Therefore, the elements that make up an action vector are independent. In the evaluation, we use the mean action of policy rather than the distribution.We used the same hyperparameter configuration for the tasks considered."}, {"heading": "7 Discussion", "text": "We have introduced an infrastructure paradigm and an implementation that uses TensorFlow to amplify parallel learning algorithms, including BatchPPO, an efficient implementation of the proximal policy optimization algorithm. We are making the case for the design decisions that lead to an easy and expandable but powerful implementation. We hope that making our infrastructure available to the public can accelerate further research into amplification learning and provide a powerful framework for implementing new algorithms. In the future, custom environments could be implemented vectorized, possibly within TensorFlow, to use parallel hardware without imposing cross-process communication costs."}, {"heading": "8 Acknowledgements", "text": "We would like to thank Nicolas Heess and Josh Merel from DeepMind for their insightful discussions and the TensorFlow team and community for developing TensorFlow to make this project possible."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "and W", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang"], "venue": "Zaremba. Openai gym", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Mpi for python: Python bindings for mpi", "author": ["L. Dalcin"], "venue": "https://github.com/ mpi4py/mpi4py", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "International Conference on Machine Learning, pages 1329\u20131338", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit", "author": ["R.H. Hahnloser", "R. Sarpeshkar", "M.A. Mahowald", "R.J. Douglas", "H.S. Seung"], "venue": "Nature, 405(6789):947", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["N. Heess", "S. Sriram", "J. Lemmon", "J. Merel", "G. Wayne", "Y. Tassa", "T. Erez", "Z. Wang", "A. Eslami", "M. Riedmiller"], "venue": "Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning, pages 1928\u20131937", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Modular rl: Implementation of trpo and related algorithms", "author": ["J. Schulman"], "venue": "https://github.com/joshu/modular_rl", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Proximal policy optimization algorithms", "author": ["J. Schulman", "F. Wolski", "P. Dhariwal", "A. Radford", "O. Klimov"], "venue": "arXiv preprint arXiv:1707.06347", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "and contributors", "author": ["S. Sidor", "J. Schulman", "M. Plappert"], "venue": "Openai baselines: High-quality implementations of reinforcement learning algorithms. https: //github.com/openai/baselines", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Elf: An extensive", "author": ["Y. Tian", "Q. Gong", "W. Shang", "Y. Wu", "L. Zitnick"], "venue": "lightweight and flexible research platform for real-time strategy games. arXiv preprint arXiv:1707.01067", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Mujoco: A physics engine for modelbased control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "We introduce a unified interface for reinforcement learning agents with accompanying infrastructure integrating with TensorFlow [1] that allows to efficiently develop new algorithms.", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "To simulate multiple environments in parallel, we extend the OpenAI Gym interface [2] to batched environments that are stepped in parallel.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "We release the TensorFlow Agents project to the open source community together with BatchPPO, an optimized implementation of the recently introduced proximal policy gradient algorithm [10], a simple and powerful reinforcement learning baseline that showed impressive results in locomotion tasks [6].", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "We release the TensorFlow Agents project to the open source community together with BatchPPO, an optimized implementation of the recently introduced proximal policy gradient algorithm [10], a simple and powerful reinforcement learning baseline that showed impressive results in locomotion tasks [6].", "startOffset": 295, "endOffset": 298}, {"referenceID": 9, "context": "We present a unified agent interface together with and efficient TensorFlow framework for parallel reinforcement learning and an implementation of the recently introduced proximal policy gradient algorithm [10].", "startOffset": 206, "endOffset": 210}, {"referenceID": 1, "context": "[2] introduced an standardized interface for environments that has since then gained wide adoption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] released the Rllab framework that implements several reinforcement learning algorithms and provides a common infrastructure for training and evaluation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "OpenAI Baselines [11] aims to provide high-quality implementations of reinforcement learning algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "This implementation relies on Python for most of the algorithm logic which is accelerated using Mpi4py [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Schulman [9] provides an implementation of PPO written in Numpy and Keras.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "In this section we summarize the background required to understand the proximal optimization optimization algorithm [10], of which we provide an efficient implementation that we call BatchPPO.", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "The objective for training the neural network is given by Williams [14] as:", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "[10] also proposed an alternative objective that clips the importance sampling ratio, but we found the objective using the KL penalty to work very well in practice and did not explore this option.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Previous implementations use asynchronous workers for this [8], but we can fully utilize the system with a simpler architecture while saving communication overhead.", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "This is similar to Facebook\u2019s ELF [12], where agents also process multiple observations at each step.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "This environment wrapper is compatible with all existing Python environments that comply to the OpenAI Gym interface [2], as long as they do not rely on shared global state.", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "Our results are on par or better than published results using the PPO algorithm [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "To demonstrate the performance of our infrastructure and implementation, we train the BatchPPO algorithm on control tasks in the MuJoCo [13] domain.", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "Specifically, we collect batches of 25 episodes before each update, and perform 25 gradient steps each for the policy and value networks using Adam optimizers [7] with fixed learning rates of 10\u22124 and 10\u22123 respectively.", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "and 100 units with ReLU non-linearities [5].", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "[10] and Heess et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], we use streaming statistics of the observations and rewards to normalize them.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We observe reliably high performance that is on par or better than published results from existing PPO implementations [10, 6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 5, "context": "We observe reliably high performance that is on par or better than published results from existing PPO implementations [10, 6].", "startOffset": 119, "endOffset": 126}], "year": 2017, "abstractText": "We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.", "creator": "LaTeX with hyperref package"}}}