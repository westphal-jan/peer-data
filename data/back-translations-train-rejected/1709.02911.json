{"id": "1709.02911", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings", "abstract": "In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study, we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.", "histories": [["v1", "Sat, 9 Sep 2017 05:04:19 GMT  (3105kb,D)", "http://arxiv.org/abs/1709.02911v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vindula jayawardana", "dimuthu lakmal", "nisansa de silva", "amal shehan perera", "keet sugathadasa", "buddhi ayesha", "madhavi perera"], "accepted": false, "id": "1709.02911"}, "pdf": {"name": "1709.02911.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings", "authors": ["Vindula Jayawardana", "Dimuthu Lakmal", "Nisansa de Silva", "Amal Shehan Perera", "Keet Sugathadasa", "Buddhi Ayesha", "Madhavi Perera"], "emails": ["vindula.13@cse.mrt.ac.lk"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "The following sections describe the background to this study and other related studies."}, {"heading": "A. Ontologies", "text": "Ontologies are mainly used to organize information as a form of knowledge representation in many domains. As Thomas R. Gruber [2] defines, \"Ontologies are an explicit and formal specification of terms in the domain and the relationships between them.\" Ontologies have expanded from the field of artificial intelligence to domain-specific tasks such as: linguistics [4, 5, 9, 17, 18], law [16], medicine [10, 11, 13]. Ontologies are widely used in the semantic iteration of the World-Wide Web. Ontology can model either the world or part of it from the point of view of the said domain [5]. The basic units of an ontology are the individuals (entities). By grouping these individuals, who can be either concrete objects or abstract objects, the structures referred to as classes are formed. A class in an ontology is a representation of a concept, a category, or a type of a class, or a kind of a class's dependence on one particular hierarchy of another, however, through the hierarchy of another."}, {"heading": "B. Word Vector Embeddings", "text": "As first proposed by Tomas Mikolov et al. [19], Word embedding systems are a set of methods for modeling natural language and learning traits, where words from a domain are mapped to vectors to create a model with a distributed representation of words. However, due to the flexibility and ease of fitting, we chose word2vec1 [20], GloVe [21] and Latent Dirichlet Allocation (LDA) [22] as embedding methods for this study. Word2vec was used in many areas due to its ability to meet the challenge of preserving the semantic sensitivity of a given context. It was used in sentiment analysis [23-26] and text classification [27]. Gerhard Wohlgenannt et al. [28] s approach to mimicking a simple ontology by means of 229 and 229 harms [27]."}, {"heading": "C. Word Set Expansion", "text": "Word lists containing closely related word groups are essential for the machine understanding and processing of natural languages. Creating and maintaining such closely related word lists is a complex process that requires human input and is done manually in the absence of tools [5]. These word lists typically contain words that are considered homogeneous in terms of the abstraction level of the application. Thus, two words W1 and W2 could belong to a single word list in an application, but belong to different word lists in another application. This vague definition and use makes creating and maintaining these word lists a complex task. De Silva et al. [5] describe a supervised learning mechanism that uses word ontology to augment word lists containing closely related word groups. This study was an extension of their previous work [17], which was conducted to improve the refactoring process of the RelEx2Frame component of the OpenCog AGI framework by extending variables that are used in RelEx."}, {"heading": "D. Ontology Population", "text": "The ontology population is inherently a complex activity, and the ontology population has been addressed through the use of techniques such as rule-based and machine learning. SPRAT [32] combines aspects of traditional detection of named entities, ontology-based information extraction, and relationship extraction to identify patterns for extracting a variety of entity types and relationships between them and to redevelop them in concepts and instances of an ontology. Rene Witte et al. [15] have developed a GATE resource called OwlExporter that allows existing NLP analysis pipelines to be easily mapped to OWL ontologies, allowing linguists to create ontology population systems without requiring extensive knowledge of the ontology APIs."}, {"heading": "E. Semi Supervised Ontology Population", "text": "Although monitored methods of machine learning have shown promising results when it comes to extracting information, they accumulate more training costs as they require a huge amount of labeled training data. As a solution, semi-monitored methods of machine learning have been introduced that require a much smaller amount of labeled training data. Carlson [33] proposed a semi-monitored learning model to populate instances of a set of target categories and relationships of an ontology by providing labeled data and a set of constraints that connect classes and relationships of an ontology. Semi-monitored algorithms tend to show unacceptable results due to \"semantic drift\" and constraints that have been introduced to overcome the problem."}, {"heading": "III. METHODOLOGY", "text": "We will discuss the methodology used in this research study in the following sections. Each of the following sections describes one step of our process. An overview of the methodology we propose is shown in Figure 1."}, {"heading": "A. Ontology Creation", "text": "In creating the ontology, we focused on the area of consumer protection law and created a legal ontology based on Findlaw [36] as a reference. However, for the clarity of this essay, we extract a subontology from it and use it to explain the methodology to make the process simple and intuitive. In selecting a part of the ontology, we mainly focused on more complex relationships and taxonomic presences. An overview of the selected part of ontology is shown in Figure 2. After creating the subontology, we manually populated the ontology with seed authorities in collaboration of domain experts and knowledge engineers."}, {"heading": "B. Training word Embeddings", "text": "The method of word embedding used in this study was created using a word2vec model. We obtained a large legal text corpus from Findlaw [36] and created a word2vec model using the corpus.The reason for selecting the word2vec word embedding for this study is the success of other studies such as [16] and [31] in the legal field that use word2vec as the method of word embedding. The text corpus consisted of legal cases in 78 legal categories. In creating the legal text corpus, we used the Stanford CoreNLP to pre-process the text with tokenization and sentence splitting. Below are the important parameters that we specified during the training of the model. \u2022 Size (dimensionality): 200 \u2022 Context window size: 10 \u2022 Study model: CBOW \u2022 Min.: 5 \u2022 Training algorithm: hierarchical software max."}, {"heading": "C. Deriving Representative Class Vectors", "text": "Ontology classes are sets of homogeneous instance objects that can be converted into a vector space by word vector embedding. A method for deriving a representative vector for ontology classes whose instances are mapped to a vector space is presented in [16]. We took the same approach and started with deriving five candidate vectors, which are then used to develop a machine learning model that calculates a representative vector for each of the classes in the selected subontology shown in Fig. 2. In the following sections we will describe in detail how we used these derived class vectors in our proposed methodology."}, {"heading": "E. Candidate Model Building", "text": "The five models are summarised below as follows: \"There is only one model in which the question is whether there is a model in which there is a model in which there is a model.\" \"There is only one model in which there is a model.\" \"There are only two models.\" \"There are two models.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" There are them. \"\" There are them. \"There are them.\" \"There are them.\" \"There are them.\" \"There are them.\" There are them. \"\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them. \"There are them.\" There are them."}, {"heading": "F. Model Accuracy Measure", "text": "After creating the above models, we evaluated the accuracy of each model. Since each model produces a disordered set of suggested words, we sorted them according to the Neural Network, which was trained according to the methodology proposed in [31]. Upon completion of the sorting, we established a threshold for selecting the best candidates. Finally, we measured the accuracy of each model as follows: For this task, we engaged domain experts and knowledge engineers. For a given model, WMi denotes words of the model Mi and Wj, g in the context of class j: PrecisionMi, j = WMi, j = Wj, gWMi, j (8) RecallMi, j = WMi, j-Wj, gWj, g (9) Here, WMi denotes words of the model Mi and Wj, g denotes the set of words suggested by domain experts, which must be the gold standard for class j. Model accuracy and model memory of Mi was calculated by averaging the class values for and recalling them to accuracy."}, {"heading": "G. Ensemble Model", "text": "In the task of creating the ensemble model, we assigned each model a candidate weight based on the formula 1 as calculated in the previous step. Let Mi be a model from the obtained models, and let F1Mi be the formula 1 of the model Mi. Therefore, in the models the weight of the model Wi is calculated as shown in Equation 11, where p is the total number of models. Wi = F1i p \u2211 i = 1F1i (11) As described above, the calculation of the weight of each model results in the ensemble model as shown in Equation 12. Faced with an unmarked instance Y, the ensemble should be a p \u00b7 n matrix in which n is the number of classes in ontology and p the number of basic models. Each column of the matrix corresponds to a class in ontology and each row corresponds to a model."}, {"heading": "IV. RESULTS", "text": "When testing our ensemble model, we used a different instance corpus. In this corpus, we divided the models in the order of 70%, 20%, and 10% as a training set, validation set, or test set. The validation set was used to fine-tune the models. Finally, the test set was used to check the accuracy of the models. We report on our results in Table I below, where we compare the individual models: membership by distance model (M1), membership by dissimilar exclusion model (M2), quantity expansion model (M3), center grouping by model (M4), hierarchical cluster model (M5), and the ensemble model as a whole. In Figure 4, we compare the precision, memory, and F1 of each of the candidate models with the ensemble model. In defining the ensemble model, Equation 17 defines the calculated weights of each model in the order of magnitude M1 to M5."}, {"heading": "V. CONCLUSION AND FUTURE WORKS", "text": "Through this work, we demonstrated the use of word embedding on semi-monitored ontology population. We focused mainly on semi-monitored population, which essentially lies between the monitored population and the unmonitored population. The main motive behind the semi-monitored embedding of the process is to reduce the level of manual intervention in ontology populations while maintaining a considerable level of accuracy. As shown in the results, our ensemble model outperforms the five individual models in colonizing the selected legal ontology. The results in this study are mainly important in two respects, as mentioned below. Firstly, an important part of the ontology engineering cycle is the ability to keep a handmade ontology up to date. By using the semi-monitored ontology population, we can reduce the effort of manual intervention to keep ontology up to date. Secondly, there is a novelty in the methodology proposed in our study."}], "references": [{"title": "Formal ontology and information systems,", "author": ["N. Guarino"], "venue": "Proceedings of FOIS\u201998, Trento, Italy,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "A translation approach to portable ontology specifications,", "author": ["T.R. Gruber"], "venue": "Knowledge Acquisition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "N", "author": ["X.-Q. Yang"], "venue": "Sun, T.-L. Sun et al., \u201cThe application of latent semantic indexing and ontology in text classification,\u201d International Journal of Innovative Computing, Information and Control, vol. 5, no. 12, pp. 4491\u20134499", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Safs3 algorithm: Frequency statistic and semantic similarity based semantic classification use case,", "author": ["N. de Silva"], "venue": "Advances in ICT for Emerging Regions (ICTer),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "and M", "author": ["N. De Silva", "A. Perera"], "venue": "Maldeniya, \u201cSemisupervised algorithm for concept ontology based word set expansion,\u201d Advances in ICT for Emerging Regions (ICTer), 2013 International Conference on, pp. 125\u2013131", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "and K", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross"], "venue": "J. Miller, \u201cIntroduction to wordnet: An on-line lexical database,\u201d International journal of lexicography, vol. 3, no. 4, pp. 235\u2013244", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Nouns in wordnet: a lexical inheritance system,", "author": ["G.A. Miller"], "venue": "International journal of Lexicography,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Wiley Online Library", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "and N", "author": ["I. Wijesiri", "M. Gallage", "B. Gunathilaka", "M. Lakjeewa", "D.C. Wimalasuriya", "G. Dias", "R. Paranavithana"], "venue": "De Silva, \u201cBuilding a wordnet for sinhala,\u201d in 7th Global Wordnet Conference", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Y", "author": ["J. Huang", "F. Gutierrez", "H.J. Strachan", "D. Dou", "W. Huang", "B. Smith", "J.A. Blake", "K. Eilbeck", "D.A. Natale"], "venue": "Lin et al., \u201cOmnisearch: a semantic search system based on the ontology for microrna target (omit) for micrornatarget gene interaction data,\u201d Journal of biomedical semantics, vol. 7, no. 1, p. 1", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["J. Huang", "K. Eilbeck", "B. Smith", "J.A. Blake", "D. Dou", "W. Huang", "D.A. Natale", "A. Ruttenberg", "J. Huan"], "venue": "T. Zimmermann et al., \u201cThe development of non-coding rna ontology,\u201d International journal of data mining and bioinformatics, vol. 15, no. 3, pp. 214\u2013232", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Ontology-based information extraction: An introduction and a survey of  current approaches,", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Discovering inconsistencies in pubmed abstracts through ontology-based information extraction,", "author": ["N. de Silva", "D. Dou", "J. Huang"], "venue": "Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Ivo Serrab", "author": ["R.G. Carla Fariaa"], "venue": "\u201cA domain-independent process for automatic ontology population from text,\u201d Science of Computer Programming", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "N", "author": ["V. Jayawardana", "D. Lakmal"], "venue": "de Silva, A. S. Perera, K. Sugathadasa, and B. Ayesha, \u201cDeriving a representative vector for ontology classes with instance word vector embeddings,\u201d arXiv preprint arXiv:1706.02909", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Semap-mapping dependency relationships into semantic frame relationships,", "author": ["N. de Silva", "C. Fernando", "M. Maldeniya", "D. Wijeratne", "A. Perera", "B. Goertzel"], "venue": "ERU Research Symposium,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Subject specific stream classification preprocessing algorithm for twitter data stream,", "author": ["N. de Silva", "D. Maldeniya", "C. Wijeratne"], "venue": "arXiv preprint arXiv:1705.09995,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "and J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado"], "venue": "Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d Advances in neural information processing systems, pp. 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "and J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado"], "venue": "Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "and C", "author": ["J. Pennington", "R. Socher"], "venue": "D. Manning, \u201cGlove: Global vectors for word representation.\u201d in EMNLP, vol. 14", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "and C", "author": ["R. Das", "M. Zaheer"], "venue": "Dyer, \u201cGaussian lda for topic models with word embeddings.\u201d in ACL (1)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "and B", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu"], "venue": "Qin, \u201cLearning sentiment-specific word embedding for twitter sentiment classification,\u201d Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 1, pp. 1555\u20131565", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "and Z", "author": ["B. Xue", "C. Fu"], "venue": "Shaobin, \u201cStudy on sentiment computing and classification of sina weibo with word2vec,\u201d Big Data (BigData Congress), 2014 IEEE International Congress on. IEEE, pp. 358\u2013363", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and Y", "author": ["D. Zhang", "H. Xu", "Z. Su"], "venue": "Xu, \u201cChinese comments sentiment classification based on word2vec and svm perf,\u201d Expert Systems with Applications, vol. 42, no. 4, pp. 1857\u20131863", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentiment analysis of citations using word2vec,", "author": ["H. Liu"], "venue": "arXiv preprint arXiv:1704.00177,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "and Y", "author": ["J. Lilleberg", "Y. Zhu"], "venue": "Zhang, \u201cSupport vector machines and word2vec for text classification with se-  mantic features,\u201d in Cognitive Informatics & Cognitive Computing (ICCI* CC), 2015 IEEE 14th International Conference on. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Using word2vec to build a simple ontology learning system", "author": ["G. Wohlgenannt", "F. Minic"], "venue": "Available at: http: //ceur-ws.org/Vol-1690/paper37.pdf. Accessed:", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "node2vec: Scalable feature learning for networks,", "author": ["A. Grover", "J. Leskovec"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "N", "author": ["K. Sugathadasa", "B. Ayesha"], "venue": "de Silva, A. S. Perera, V. Jayawardana, D. Lakmal, and M. Perera, \u201cSynergistic union of word2vec and lexicon for domain specific semantic similarity,\u201d arXiv preprint arXiv:1706.01967", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Sprat: a tool for automatic semantic pattern-based ontology population,", "author": ["A.F. Diana Maynard", "W. Peters"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "E", "author": ["A. Carlson", "J. Betteridge", "R.C. Wang"], "venue": "R. Hruschka, Jr., and T. M. Mitchell, \u201cCoupled semi-supervised learning for information extraction,\u201d WSDM \u201910 Proceedings of the third ACM international conference on Web search and data mining, pp. 101\u2013110", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "E", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles"], "venue": "R. Hruschka, Jr., and T. M. Mitchell, \u201cToward an architecture for never-ending language learning,\u201d Proceeding AAAI\u201910 Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pp. 1306\u20131313", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "William W", "author": ["R.S. Zhilin Yang"], "venue": "Cohen, \u201cRevisiting semisupervised learning with graph embeddings,\u201d Proceedings of International Conference on Machine Learning", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Many of the research areas such as knowledge engineering and representation, information retrieval and extraction, and knowledge management and agent systems [1] have incorporated the use of ontologies to a greater extent.", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "Gruber [2], an ontology is a \u201cformal and explicit specification of a shared conceptualization\u201d.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 36, "endOffset": 42}, {"referenceID": 3, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 36, "endOffset": 42}, {"referenceID": 4, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 6, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 7, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 8, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 9, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 141, "endOffset": 149}, {"referenceID": 10, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 141, "endOffset": 149}, {"referenceID": 11, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 178, "endOffset": 186}, {"referenceID": 12, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 178, "endOffset": 186}, {"referenceID": 13, "context": "[14], ontology population looks for instantiating the constituent elements of an ontology, like properties and non-taxonomic relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "For this purpose, we built an iterative model based on the class representative vector for ontology classes [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "In another model, we used set expansion as described by [5], for the purpose of ontology population.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Gruber [2], \u2019ontologies are an explicit and formal specifications of the terms in the domain and the relations among them\u2019.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 4, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 8, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 15, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 16, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 14, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 10, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 12, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 4, "context": "An ontology may model either the world or a part of it as seen by the said area\u2019s viewpoint [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 17, "context": "[19], word embedding systems, are a set of natural language modeling and feature learning techniques, where words from a domain are mapped to vectors to create a model that has a distributed representation of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 22, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 23, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 24, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 25, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "[28]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [29]\u2019s usage of word2vec extension: node2vec [30], to overcome the problems in vectorization of an ontology, are two major works that have been carried out in relation to ontologies with the use of word2vec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [29]\u2019s usage of word2vec extension: node2vec [30], to overcome the problems in vectorization of an ontology, are two major works that have been carried out in relation to ontologies with the use of word2vec.", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "More recently there have been successful studies on using word2vec on the legal domain [16, 31].", "startOffset": 87, "endOffset": 95}, {"referenceID": 28, "context": "More recently there have been successful studies on using word2vec on the legal domain [16, 31].", "startOffset": 87, "endOffset": 95}, {"referenceID": 4, "context": "Creating and maintaining such closely related word lists is a complex process that requires human input and is carried out manually in the absence of tools [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "[5] describe a supervised learning mechanism which employs a word ontology to expand word lists containing closely related sets of words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "This study has been an extension of their previous work [17], which was done to enhance the refactoring process of the RelEx2Frame component of OpenCog AGI Framework, by expanding concept variables used in RelEx.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "SPRAT [32] combines aspects from traditional named entity recognition, ontology-based information extraction, and relation extraction, in order to identify patterns for the extraction of a variety of entity types and relations between them, and to re-engineer them into concepts and instances in an ontology.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "Carlson [33] proposed a semi-supervised learning model to populate instances of a set of target categories and relations of an ontology by providing seed labeled data and a set of constraints which couples classes and relationships of an ontology.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "Carlson [34] has expanded coupled semi-supervised learning [33] to never-ending language learning (NELL); an agent that runs forever to extract information from the web and populate them continuously into a knowledge base.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "Carlson [34] has expanded coupled semi-supervised learning [33] to never-ending language learning (NELL); an agent that runs forever to extract information from the web and populate them continuously into a knowledge base.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "Zhilin Yang [35] has presented a semi supervised learning methodology based on graph embeddings.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "The reason for selecting word2vec word embedding for this study is the success demonstrated by other studies such as [16] and [31] in the legal domain that uses word2vec as the word embedding method.", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "The reason for selecting word2vec word embedding for this study is the success demonstrated by other studies such as [16] and [31] in the legal domain that uses word2vec as the word embedding method.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "A methodology to derive a representative vector for ontology classes, whose instances were mapped to a vector space is presented in [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "3) Set Expansion Based Model (M3): For the purpose of set expansion based model, we selected the algorithm presented in [5], which was built on the earlier algorithm described in [17].", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "3) Set Expansion Based Model (M3): For the purpose of set expansion based model, we selected the algorithm presented in [5], which was built on the earlier algorithm described in [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 4, "context": "The rationale behind this selection is the fact that as per [5], WordNet [6] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The rationale behind this selection is the fact that as per [5], WordNet [6] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.", "startOffset": 73, "endOffset": 76}, {"referenceID": 28, "context": "As each model outputs an unordered set of suggested words, we sorted them using the Neural Network, trained according to the methodology proposed in [31].", "startOffset": 149, "endOffset": 153}], "year": 2017, "abstractText": "In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the filed of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models. keywords: Ontology, Ontology Population, Word Embeddings, word2vec", "creator": "LaTeX with hyperref package"}}}