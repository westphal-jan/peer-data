{"id": "1709.03339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning", "abstract": "Landing an unmanned aerial vehicle on a ground marker is an open problem despite the effort of the research community. Previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad. In this article, we propose a method based on deep reinforcement learning which only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the quadrotor on it. The proposed approach is based on a hierarchy of Deep Q-Networks (DQNs) which are used as high-level control policy for the navigation toward the marker. We implemented different technical solutions, such as the combination of vanilla and double DQNs trained using a form of prioritized buffer replay which separates experiences in multiple containers. Learning is achieved without any human supervision, giving to the agent an high-level feedback. The results show that the quadrotor can autonomously accomplish landing on a large variety of simulated environments and with relevant noise. In some conditions the DQN outperformed human pilots tested in the same environment.", "histories": [["v1", "Mon, 11 Sep 2017 11:39:47 GMT  (6173kb,D)", "http://arxiv.org/abs/1709.03339v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["riccardo polvara", "massimiliano patacchiola", "sanjay sharma", "jian wan", "rew manning", "robert sutton", "angelo cangelosi"], "accepted": false, "id": "1709.03339"}, "pdf": {"name": "1709.03339.pdf", "metadata": {"source": "CRF", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning", "authors": ["Riccardo Polvara", "Massimiliano Patacchiola", "Sanjay Sharma", "Jian Wan", "Andrew Manning", "Robert Sutton", "Angelo Cangelosi"], "emails": ["riccardo.polvara@plymouth.ac.uk"], "sections": [{"heading": null, "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the German Press Agency.\" We haven't managed to save the world, \"he said.\" We haven't managed to save the world. \"Indeed,\" We haven't managed to save the world. \""}, {"heading": "II. RELATED WORK", "text": "The idea behind this is that people live in the city in which they live, move into another world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "III. PROPOSED METHOD", "text": "In this section we describe the landing problem in terms of reinforcement learning and present the technical solutions used in our method."}, {"heading": "A. Problem definition and notation", "text": "The detection requires an exploration of the XY planes, on which the quadrotor must be moved horizontally to align the body frame with the marker. In the vertical descent phase, the vehicle must reduce the distance from the marker by vertical movements. In addition, the drone must move on the XY plane to keep the marker centered. A graphical representation of the two phases is shown in Figure 2. Formally, both the problems on Markov can be reduced, as well as the decision processes (MDPs). At any time, the agent t receives the state st, performs an action in the action space and receives a reward rt, which is given by a reward function R (st, at). The action brings the agent to a new state + 1 in accordance with the ecological transition model T (st, at)."}, {"heading": "B. Partitioned buffer replay", "text": "In MDPs with a sparse and delayed reward, it is difficult to get positive feedback. In these cases, the experiences gained in buffer repetition are extremely unbalanced. Neutral transitions are common, and for this reason they are highly likely to be sampled, whereas positive and negative experiences are unusual and difficult to try. Landing is a form of blind cliffwalk [19]. The drone must move vertically and horizontally to reach the ground. Positive reward is extremely sparse and delayed because it is achieved in a small part of the state. To deal with this type of problem, it has been proposed to divide the experience into two buckets, one with high priority and the other with low priority. [21] Our approach is to extend this method to K-buckets. Another form of prioritized buffer repetition has been proposed to increase the probability. The authors present a method in which important transitions are tried more frequently."}, {"heading": "C. Hierarchy of DQNs", "text": "Our method is based on the use of a hierarchy of DQNs representing sub-policies used for different phases of navigation. Similar to a finite state machine, global policy is divided into modules and each module is subject to a specific DQN or loop of rules. DQNs are able to autonomously understand when it is time to call the next state. The advantages of such a method are twofold: on the one hand, it is possible to reduce the complexity of the task by means of a divide-and-conquer approach; on the other, the use of a function approach device is limited to specific sandboxes that make its use in robotic applications safer. The landing problem can be described by three main steps: detection of boundary stones, downward manoeuvres, touch down. In Section III-A, we describe the first two phases. The use of a function approach device consists of reducing the performance of the motors in the last centimeters of the descent, reducing the UV and then deactivating the component in most of the components."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will present the methodology and results obtained in the training and testing of two types of DQNs. In section IV-A, the methodology and results obtained with the DQN, specializing in the detection phase of boundary stones, will be presented. In section IV-B, the second series of simulations related to the vertical sinking phase will be presented. In both training and testing, we used the same environment (Gazebo 7.7.x, ROS Kinetic) and drone (Parrot AR Drone 2). The drone used is a widely used commercial quadrotor with a weight of 420 grams, dimensions 53 x 52 cm and two cameras (frontal high resolution and a lower QVGA). The control command sent to the vehicle is represented by a continuous vector [\u2212 1, 1] which makes it possible to move the drone at a specific speed on the three axes. We do not need to point out that the physics of the engine have been simplified."}, {"heading": "A. First series of simulations", "text": "We looked at two networks with the same structure (Figure 3) and we trained them in two different situations. The first network was trained with a uniform asphalt texture (DQN-single), while the second network was trained with multiple textures (DQNmulti). The ability to generalize new invisible situations is very important and it should be seriously involved in the landing problem. Training the first network on a single texture is one way to quantify the effects of a limited dataset on the performance of the agent. In the DQN multi condition, the networks were trained using seven different sets of textures: asphalt, bricks, grass, soil (Figure 5). These networks should surpass those that were trained in the state with a single texture. To simplify the UAV movements that we only allowed translation on the xy plane."}, {"heading": "B. Second series of simulations", "text": "In the second series of simulations, we have reduced the number of textures used."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "In this work, we used DRL to realize a system for the autonomous landing of a quadrotor on a static pad. The main modules of the system are two DQNs that can control the drone in two delicate phases: boundary detection and vertical descent. Both DQNs were trained in different environments and with relevant noises. We showed that the system can perform superhuman performance in the marker detection phase and almost human performance in the vertical descent phase. Furthermore, we showed that we can train robust networks for navigation in large three-dimensional environments by training on multiple maps with random textures. Future work should focus mainly on bridging the reality gap. The reality gap are the obstacles that hinder the implementation of many robotic solutions in the real world, especially in DRL, where a large number of episodes is needed to maintain stable strategies."}, {"heading": "Acknowledgements", "text": "We thank NVIDIA Corporation for their support by donating the Tesla K40 GPU used for this research."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEE, 2016, pp. 1\u20138.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning with double q-learning.", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "in AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Continuous on-board monocular-vision-based elevation mapping applied to autonomous landing of micro aerial vehicles", "author": ["C. Forster", "M. Faessler", "F. Fontana", "M. Werlberger", "D. Scaramuzza"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 111\u2013118.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Landing on a moving target using an autonomous helicopter", "author": ["S. Saripalli", "G. Sukhatme"], "venue": "Field and service robotics. Springer, 2006, pp. 277\u2013286.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Autonomous landing of a helicopter uav with a ground-based multisensory fusion system", "author": ["D. Zhou", "Z. Zhong", "D. Zhang", "L. Shen", "C. Yan"], "venue": "Seventh International Conference on Machine Vision (ICMV 2014). International Society for Optics and Photonics, 2015, pp. 94 451R\u201394 451R.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards autonomous landing on a moving vessel through fiducial markers", "author": ["R. Polvara", "S. Sharma", "J. Wan", "A. Manning", "R. Sutton"], "venue": "IEEE European Conference on Mobile Robotics (ECMR). IEEE, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Localization framework for real-time uav autonomous landing: An on-ground deployed visual approach", "author": ["W. Kong", "T. Hu", "D. Zhang", "L. Shen", "J. Zhang"], "venue": "Sensors, vol. 17, no. 6, p. 1437, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Airborne vision-based navigation method for uav accuracy landing using infrared lamps", "author": ["Y. Gui", "P. Guo", "H. Zhang", "Z. Lei", "X. Zhou", "J. Du", "Q. Yu"], "venue": "Journal of Intelligent & Robotic Systems, vol. 72, no. 2, p. 197, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Ground stereo vision-based navigation for autonomous take-off and landing of uavs: a chan-vese model approach", "author": ["D. Tang", "T. Hu", "L. Shen", "D. Zhang", "W. Kong", "K.H. Low"], "venue": "International Journal of Advanced Robotic Systems, vol. 13, no. 2, p. 67, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A vision based onboard approach for landing and position control of an autonomous multirotor uav in gps-denied environments", "author": ["S. Lange", "N. Sunderhauf", "P. Protzel"], "venue": "Advanced Robotics, 2009. ICAR 2009. International Conference on. IEEE, 2009, pp. 1\u20136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Monocular vision-based real-time target recognition and tracking for autonomously landing an uav in a cluttered shipboard environment", "author": ["S. Lin", "M.A. Garratt", "A.J. Lambert"], "venue": "Autonomous Robots, vol. 41, no. 4, pp. 881\u2013901, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Vision-based autonomous quadrotor landing on a moving platform", "author": ["F. Davide", "Z. Alessio", "S. Alessandro", "D. Jeffrey", "D. Scaramuzza"], "venue": "IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). IEEE, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Autonomous landing of a vtol uav on a moving platform using image-based visual servoing", "author": ["D. Lee", "T. Ryan", "H.J. Kim"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 971\u2013976.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods", "author": ["M. Patacchiola", "A. Cangelosi"], "venue": "Pattern Recognition, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep sparse rectifier neural networks.", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in Aistats,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Autonomous reinforcement learning with experience replay", "author": ["P. Wawrzy\u0144Ski", "A.K. Tanwani"], "venue": "Neural Networks, vol. 41, pp. 156\u2013 167, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "arXiv preprint arXiv:1506.08941, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning, vol. 4, no. 2, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Cad2rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In this work we propose a completely different approach, based on recent breakthrough achieved with Deep Reinforcement Learning (DRL) [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Previous applications mainly focused on deterministic environments, such as the Atari game suite [1], and the Doom platform [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Previous applications mainly focused on deterministic environments, such as the Atari game suite [1], and the Doom platform [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Moreover, we used double DQN [3] to reduce overestimation problems which commonly arise when the agent moves in stochastic environments.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "In a recent work [4] the data from a downward-looking camera and an inertial measurement unit were combined in order to build a threedimensional reconstruction of the terrain.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "In [5] the authors", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "A ground-based multisensory fusion system has been proposed in [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "In [7] the authors combined data from a monocular camera, odometry, and inertial unit in order to estimate the position of the drone with respect to a fiducial", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "accomplished in [8] using a ground-based vision system.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "A system based on infra-red lights has been used in [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "A Chane-Vase based approach has been proposed in [10] for ground stereo-vision detection.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "A method for landing an autonomous UAV in GPS-denied environments using an onboard monocular camera has been proposed in [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "A modified version of the international landing patter, characterized by the letter \u201dH\u201d, has been used in [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "A recent work [13] used computer vision algorithm for detecting a moving target using only an onboard camera.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In [14] a vision-based visual", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In this work we use two Convolutional Neural Networks (CNNs) for function approximation following the approach presented in [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "image classification [15] and head pose estimation [16].", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "image classification [15] and head pose estimation [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "used the rectified linear unit [17].", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The dataset D is also called buffer replay and it is a way to randomize the samples breaking the correlation and reducing the variance [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "The vertical descend phase is a form of Blind Cliffwalk [19] where the agent has to take the right action in order to progress through a sequence of N states.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Another issue connected with the reward sparsity is a well known problem called overestimation [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "A solution to overestimation has been recently proposed and has been called double DQN [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 18, "context": "The landing problem is a form of Blind Cliffwalk [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "To deal with this kind of problems it has been proposed to divide the experiences in two buckets, one with high priority and the other with low priority [21].", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "in [19].", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "To stabilize the flight we introduced discrete movements, meaning that each action was repeated for 2 seconds and then stopped leading to a 1 meter shift similarly to the no-operation parameter used in [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 21, "context": "As optimizer we used the RMSProp algorithm [22]", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The weights were initialized using the Xavier initialization method [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "An example is domain randomization [24], a method for training models on simulated images that transfer to real images by randomizing rendering in the simulator.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Landing an unmanned aerial vehicle on a ground marker is an open problem despite the effort of the research community. Previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad. In this article, we propose a method based on deep reinforcement learning which only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the quadrotor on it. The proposed approach is based on a hierarchy of Deep Q-Networks (DQNs) which are used as high-level control policy for the navigation toward the marker. We implemented different technical solutions, such as the combination of vanilla and double DQNs trained using a form of prioritized buffer replay which separates experiences in multiple containers. Learning is achieved without any human supervision, giving to the agent an high-level feedback. The results show that the quadrotor can autonomously accomplish landing on a large variety of simulated environments and with relevant noise. In some conditions the DQN outperformed human pilots tested in the same environment.", "creator": "LaTeX with hyperref package"}}}