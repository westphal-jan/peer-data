{"id": "1709.03413", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Gigamachine: incremental machine learning on desktop computers", "abstract": "We present a concrete design for Solomonoff's incremental machine learning system suitable for desktop computers. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on a stochastic Context Free Grammar together with new update algorithms that use the same grammar as a guiding probability distribution for incremental machine learning. The updates include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. The issues of extending the a priori probability distribution and bootstrapping are discussed. We have implemented a good portion of the proposed algorithms. Experiments with toy problems show that the update algorithms work as expected.", "histories": [["v1", "Fri, 8 Sep 2017 17:39:26 GMT  (19kb)", "http://arxiv.org/abs/1709.03413v1", "This is the original submission for my AGI-2010 paper titled Stochastic Grammar Based Incremental Machine Learning Using Scheme which may be found onthis http URLand presented a partial but general solution to the transfer learning problem in AI. arXiv admin note: substantial text overlap witharXiv:1103.1003"]], "COMMENTS": "This is the original submission for my AGI-2010 paper titled Stochastic Grammar Based Incremental Machine Learning Using Scheme which may be found onthis http URLand presented a partial but general solution to the transfer learning problem in AI. arXiv admin note: substantial text overlap witharXiv:1103.1003", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["eray \\\"ozkural"], "accepted": false, "id": "1709.03413"}, "pdf": {"name": "1709.03413.pdf", "metadata": {"source": "CRF", "title": "Gigamachine: incremental machine learning on desktop computers", "authors": ["Eray \u00d6zkural"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 9.03 413v 1 [cs.A I] 8 Sep 2"}, {"heading": "Introduction", "text": "The field of Artificial General Intelligence (AGI) has received considerable attention from researchers over the last decade as computing capacity marches toward the human scale. Many interesting theoretical proposals have been made (Sol89; Hut02; Sch09) and practical general-purpose programs have been demonstrated (for example (Sch04; RC03)). We understand the requirements of such a system much better today than before, so we believe that now is the time to start constructing an AGI system or at least a prototype based on the solid theoretical foundation that exists today. We expect that during the painstaking work of writing such general-purpose programs, we will have to solve various theoretical problems and deal with practical details. It would be best to uncover these problems early on. Gigamachine is our first implementation of an AGI system in the O'Caml language with the goal of building such a \"Phase 1 machine,\" which Solomonoff calls the foundation of a fairly powerful solinmental learning system ()."}, {"heading": "Scheme as the reference machine", "text": "(Sol09) argues that the choice of a reference machine introduces a necessary bias into the learning system, and the search for the \"ultimate machine\" can be a distraction. In (Sol02) and other publications by Solomonoff, we see that a previously (seemingly) unimplemented reference machine called AZ is being introduced. AZ language is a functional programming language that assumes a prefix (Polish) notation for expressions. Solomonoff also suggests that primitives such as +, \u2212, sinful, cos, etc. For a specific application, one must choose a universal computer with as many suitable primitives as possible, because it would take a lot of time for the system to discover these primitives for itself, and the training sequence would have to be longer to accommodate the discovery of this primitivity."}, {"heading": "Adaptation to program generation", "text": "Although we do not believe that Scheme R5RS is the ultimate reference engine, it has provided a good platform for testing our ideas about incremental learning. We have implemented most of the R5RS syntax, with a few omissions. We have chosen to exclude the syntax for quasi-quotes and syntax transformation syntax, since the advanced macro syntax would complicate our program-based logic, and since it is an advanced property used only in more complex programs that we do not expect to generate in the gigamachine. Further simplifications were deemed necessary. In some parts of the syntax, the same semantics can be expressed in different ways, for example (quote (0 1) and \"(0 1) have the same semantics. In this case, we used only quotes. In the case of numerical literature, we do not generate standard radicals and use only standard libraries 10. All of the R5S libraries were added to the system-based system-based system (exception 6)."}, {"heading": "Program Search", "text": "In many AGI systems, a variant or extension of Levin Search (Lev73) is used to find solutions. Solomonoff's incremental machine learning uses Levin Search as the basic search algorithm for finding solutions (Sol89). In our system, we also use the Guiding probability mass function (pmf) for the search process. We first describe a generalized version of Levin Search and then build on it."}, {"heading": "Generalized Levin Search", "text": "In algorithm 1, we specify a generalized Levin search algorithm similar to the one described in (Sch04). Inputs are as follows: U is a universal computer and U (p, t) executes a program p up to a duration of t and returns its output. G is a grammar that defines the language of valid programs (and L (G) is their language) in U. P is an a priori pmf of U. TESTPROG is an algorithm that takes a candidate program x and a test program in the program coding of U. TrueVal is the value of \"true\" in the language of U. The constant t0 is the initial time limit and the constant tq is the time quantum that is the minimum time in which we execute a program. We start with a global time limit t equal to t0."}, {"heading": "Using a stochastic CFG in Levin Search", "text": "A stochastic CFG is a CFG augmented by a probability value for each production. For each non-terminal head, the probabilities of the production of this head must obviously add up to one. We can expand our Levin search procedure to work with a stochastic CFG that assigns probabilities to each sentence in the language. To do this, we need two things: first, a generational logic for individual sentences and second, a search strategy to list the sentences that meet the condition in line 3 of the algorithm. In the current system, we use the most left-handed derivative to generate a sentence, so intermediate steps are left-handed forms (EVERY 01, chapter 5). Calculation of the a priori probability of a sentence depends on the obvious fact that in a derivative S \u21d2 1 \u21d2 2 \u21d2... \u21d2 n, where the productions p1, p2,..., pn were applied to start the symbol S, the probability of the sentence is necessarily relevant to us (this is the probability that n is independent of the probabilities)."}, {"heading": "Depth-limited depth-first search", "text": "Depth-first search is a common search strategy when the search space is large and the depth is manageable, but the probability horizon can be calculated by t and tq as ph = tq / t, which ensures that we do not waste time generating any programs that we will not execute. Depth-first search is implemented by extending the leftmost nonterminal in a sentential form by truncating the sentential forms that are a priori probabilities smaller than the probability horizon, sorting the extended sentinal forms in order to reduce a priori probability, and then recursively expanding the list of sentences obtained in this way in a sental form. Recursion can be implemented via a stack or a simple recursion."}, {"heading": "Generation of literals", "text": "In (Sol09) the crack distribution P (n) = A2 \u2212 log \u0445 2 n is proposed for generating integer literals. An alternative is the zeta distribution with the pmf distribution given by Ps (k) = k \u2212 s / \u0445 (s), which is the Riemann Zeta function. We used the zeta distribution with s = 2 and used a precalculated table to generate up to a fixed integer (1024 in our current implementation).The zeta distribution has empirical support, so it was preferred.The upper limit is present to avoid too many programs with the same constants, for example, the expression syntax deals with larger integers (* 1024 200).A smaller or larger upper limit may be appropriate, this is a matter of experimenting.The string literals are generated as a result of characters in grammar, as the standard sequence rules seemed reasonable. We did not see the need to implement a special generation, but a special procedure can be associated with a ciphers, of course."}, {"heading": "Defining and referencing variables", "text": "A major problem in our implementation was the plethora of \"unbound reference errors.\" We have designed a simple solution that is easy to implement. We get a static environment while deriving a sentence that is passed to the right, possibly with modification, and the environment is also passed to non-terminal procedures. First, the environment includes the input parameters of the function to be searched for. When a variable is defined, a robotic variable name is generated in the form of varinteger, which scans the non-terminal integer from the zeta distribution. We use the same precalculated zeta distributions and the same upper limit as for integer character generation. This is not a drastic limitation, as it only limits temporary variables within the same scope. If a variable reference is generated, the environment is present and we choose between available variable names with uniform distribution. Nevertheless, this solution will not nest the definition as we respect the one that is available in a scan (3)."}, {"heading": "Stochastic CFG updates", "text": "We propose four update algorithms that work in parallel. (Sol09) mentions PPM (JC84). PPM can indeed be used to extrapolate a number of programs, but we do not think it is practicable for incremental machine learning. In fact, we adopted one of the most recent variants of the PAQ family of compression programs (Mah05) for this purpose (by first compressing the set of programs and then dynamically appending bits to the end of the stream during decompression), and we saw that the extrapolated programs were largely syntactically incorrect. We can develop a PPM variant that is useful for extrapolating programs, but that would not make our work any easier. The update algorithms we propose are more powerful."}, {"heading": "Modifying production probabilities", "text": "The simplest type of update is to modify the probabilities, because new solutions are added to the solution corpus. However, to do this, the search algorithm must provide the derivative that led to the solution (which we do), or the solution must be analyzed with the same grammar. Then, the probability for each productionA \u2192 \u03b2 in the solution corpus can easily be excluded from the update by the ratio of the frequency of productions A \u2192 \u03b2 in the solution corpus to the frequency of productions in the corpus with a head of A. Of course, the non-terminal procedures are excluded from the update, since they can be variant. However, we cannot simply write the probabilities thus calculated above the original probabilities, since there will be few solutions at first, and most probabilities will be zero. We use exponential smoothing to solve this problem."}, {"heading": "Re-using previous solutions", "text": "The simplest way we have found is to include all solutions in the library of the schema interpreter, we add new solutions as follows, the new solution among other previous solutions receives a probability of \u03b3 in the hope that this solution will soon be used again, and then the probabilities of the old productions of the previous solution are normalized, so that they result in a sum of 0.5. If it is impossible or difficult to add the solutions to the schema interpreter in the hope that this solution will soon be used again, and then the probabilities of the old productions of the previous solution are normalized, so that they result in a sum of 0.5. If it is impossible to add the solutions to the schema interpreter as in our case, then all solutions can be defined as blocks are defined at the beginning of the program."}, {"heading": "Learning programming idioms", "text": "Programmers not only learn concrete solutions to problems, but also learn abstract programs or program schemes. One way to formalize this is to learn sentential forms. If we can extract appropriate sentential forms, we can add them to grammar using the same algorithm that modifies the production probabilities. We have not yet implemented this update algorithm because we use a basic symbolic expression syntax that is used many times in Scheme grammar) until a certain level or limit in the derivative tree of a solver can be reached (which is the body of a Scheme function) or an expression (which is the basic symbolic expression syntax used many times in Scheme grammar)."}, {"heading": "Frequent sub-program mining", "text": "Common subroutines in the solution corpus, i.e. subroutines that occur with a frequency above a certain support threshold, can be added as alternative productions to the frequently occurring non-terminal expression in Scheme grammar. For example, if the solution corpus contains several (lambda (x y) (* x y)) subroutines, frequent subroutine mining would detect this and we can add it as an alternative expression to Scheme grammar. We have not yet detailed this update algorithm, but it appears reasonable and it can benefit from the well-developed field of data mining."}, {"heading": "Discussion", "text": "What does a programmer know? In order to encode useful information in the a priori probability distribution, we need to think about what a human programmer knows when he writes a program. The richness of the R5RS programming language requires that we solve some problems, such as avoiding unbound references, in order to make even the simplest program searches executable, so it is important that we encode as much a priori knowledge of programming as possible into the system. among other things, a programmer knows the following. a) The syntax of the programming language, sometimes imperfect. Our system knows the syntax perfectly and does not make syntactical errors. b) The semantics of the programming language are imperfect again. Our system knows little about writing semantically correct programs and often generates wrong programs. We need to add more semantic checks to improve this. Our system does not know the referential semantics of programmers, just how to execute the program. It can be more useful for understanding the human search problem by making the time perfect."}, {"heading": "Bootstrapping problems", "text": "The incremental machine learning capabilities of the Phase 1 machine in (Sol02) are used to calculate the conditional distributions required to start the Phase 2 machine. Our current implementation may find short programs, but despite future improvements in processing speed, it may have difficulty finding the type of programs needed without a huge training sequence that approaches these programs very accurately, and it has little chance of rewriting itself if the current implementation is not further developed."}, {"heading": "Experiments", "text": "In the meantime, we have developed a simple training sequence consisting of a series of problems. However, for each problem, we have a sequence of input and output pairs, and we use incremental surgical solutions (Sol02; Sol08). After each partial or complete solution, a stochastic CFG update is applied. In our first toy sequence, we find a solution for the first pair, and then the first three pairs, and so on, are sought."}, {"heading": "Conclusion", "text": "We have adapted the R5RS scheme as a universal reference calculator to our system. Stochastic CFG is used in sequential LSEARCH to calculate a priori probabilities and efficiently generate programs to avoid syntactically erroneous programs. We derive sentences with left-sided derivatives. We use a probability horizon to limit the depth of the deepest searches, and we also propose the use of best-first searches and memory-aware hybrid searches. We have specialized productions for number literals, variable ties and variable references; in particular, we avoid unbound variables in generated programs. We have proposed four update algorithms for incremental machine learning, two of which have been implemented. To the extent that the update algorithms work, their use has been demonstrated in a toy training sequence."}], "references": [{"title": "Scheme: An interpreter for extended lambda calculus", "author": ["Jr. Gerald Jay Sussman", "Guy Lewis Steele"], "venue": "Technical Report AI Lab Memo AIM-349,", "citeRegEx": "Sussman and Steele.,? \\Q1975\\E", "shortCiteRegEx": "Sussman and Steele.", "year": 1975}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["Marcus Hutter"], "venue": "International Journal of Foundations of Computer Science,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "Data compression using adaptive coding and partial string matching", "author": ["I.H. Witten J.G. Cleary"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Cleary.,? \\Q1984\\E", "shortCiteRegEx": "Cleary.", "year": 1984}, {"title": "Introduction to Automata Theory, Languages, and Computation", "author": ["Rajeev Motwani"], "venue": "Addison Wesley, second edition,", "citeRegEx": "Hopcroft and Motwani.,? \\Q2001\\E", "shortCiteRegEx": "Hopcroft and Motwani.", "year": 2001}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission,", "citeRegEx": "Levin.,? \\Q1973\\E", "shortCiteRegEx": "Levin.", "year": 1973}, {"title": "Adaptive weighing of context models for lossless data compression", "author": ["Matt Mahoney"], "venue": "Technical Report CS-2005-16, Florida Tech.,", "citeRegEx": "Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Mahoney.", "year": 2005}, {"title": "Tagging english text with a probabilistic model", "author": ["Bernard Merialdo"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo.,? \\Q1993\\E", "shortCiteRegEx": "Merialdo.", "year": 1993}, {"title": "Clustering by compression", "author": ["P.M.B. Vitanyi R. Cilibrasi"], "venue": "Technical report,", "citeRegEx": "Cilibrasi.,? \\Q2003\\E", "shortCiteRegEx": "Cilibrasi.", "year": 2003}, {"title": "Revised5 report on the algorithmic language scheme", "author": ["Jonathan Rees Richard Kelsey", "William Clinger"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "Kelsey and Clinger.,? \\Q1998\\E", "shortCiteRegEx": "Kelsey and Clinger.", "year": 1998}, {"title": "Optimal ordered problem solver", "author": ["Juergen Schmidhuber"], "venue": "Technical Report TR IDSIA-12-02,", "citeRegEx": "Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2002}, {"title": "Optimal ordered problem solver", "author": ["Juergen Schmidhuber"], "venue": "Machine Learning,", "citeRegEx": "Schmidhuber.,? \\Q2004\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2004}, {"title": "Ultimate cognition \u00e0 la G\u00f6del", "author": ["J. Schmidhuber"], "venue": "Cognitive Computation,", "citeRegEx": "Schmidhuber.,? \\Q2009\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2009}, {"title": "A system for incremental learning based on algorithmic probability", "author": ["Ray Solomonoff"], "venue": "In Proceedings of the Sixth Israeli Conference on Artificial Intelligence,", "citeRegEx": "Solomonoff.,? \\Q1989\\E", "shortCiteRegEx": "Solomonoff.", "year": 1989}, {"title": "Progress in incremental machine learning", "author": ["Ray Solomonoff"], "venue": "In NIPS Workshop on Universal Learning Algorithms and Optimal Search,", "citeRegEx": "Solomonoff.,? \\Q2002\\E", "shortCiteRegEx": "Solomonoff.", "year": 2002}, {"title": "Three kinds of probabilistic induction: Universal distributions and convergence theorems", "author": ["Ray Solomonoff"], "venue": "The Computer Journal,", "citeRegEx": "Solomonoff.,? \\Q2008\\E", "shortCiteRegEx": "Solomonoff.", "year": 2008}, {"title": "Algorithmic probability: Theory and applications", "author": ["Ray Solomonoff"], "venue": "Information Theory and Statistical Learning, Springer Science+Business Media,", "citeRegEx": "Solomonoff.,? \\Q2009\\E", "shortCiteRegEx": "Solomonoff.", "year": 2009}], "referenceMentions": [], "year": 2017, "abstractText": "We present a concrete design for Solomonoff\u2019s incremental machine learning system suitable for desktop computers. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on a stochastic Context Free Grammar together with new update algorithms that use the same grammar as a guiding probability distribution for incremental machine learning. The updates include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. The issues of extending the a priori probability distribution and bootstrapping are discussed. We have implemented a good portion of the proposed algorithms. Experiments with toy problems show that the update algorithms work as expected.", "creator": "LaTeX with hyperref package"}}}