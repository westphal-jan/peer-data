{"id": "1709.03450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "UI-Net: Interactive Artificial Neural Networks for Iterative Image Segmentation Based on a User Model", "abstract": "For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.", "histories": [["v1", "Mon, 11 Sep 2017 15:50:24 GMT  (766kb,D)", "http://arxiv.org/abs/1709.03450v1", "This work is submitted to the 2017 Eurographics Workshop on Visual Computing for Biology and Medicine"]], "COMMENTS": "This work is submitted to the 2017 Eurographics Workshop on Visual Computing for Biology and Medicine", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.NE", "authors": ["mario amrehn", "sven gaube", "mathias unberath", "frank schebesch", "tim horz", "maddalena strumia", "stefan steidl", "markus kowarschik", "reas maier"], "accepted": false, "id": "1709.03450"}, "pdf": {"name": "1709.03450.pdf", "metadata": {"source": "META", "title": "UI-Net: Interactive Artificial Neural Networks", "authors": ["Mario Amrehn", "Sven Gaube", "Mathias Unberath", "Frank Schebesch", "Tim Horz", "Maddalena Strumia", "Stefan Steidl", "Markus Kowarschik", "Andreas Maier"], "emails": [], "sections": [{"heading": null, "text": "CCS concepts \u2022 computer methods \u2022 learning through implicit feedback; neural networks; supervised learning by classification;"}, {"heading": "1. Introduction", "text": "In fact, it is in such a way that most people who are able to be able to be able to move, are able to be able to move, are able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to live, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to live to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be to be able to be able to be able to be able to be able to be able to be to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be to be to be able to be able to be to be able to be able to be to be to"}, {"heading": "1.1. User Input", "text": "According to [OS01], user interactions can be categorized depending on the user interface of the interactive segmentation system: (1) A menu-driven input scheme such as in [RPN15] limits the scope of action of the user by swapping his control of the segmentation result for more guidance through the system. (2) Setting parameter values directly requires the user to look into the algorithm. (3) The image input on Figure I-RD1, D2, D3 is the most intuitive case for the user. N = D1 \u00b7 D2 \u00b7 D3 is the number of elements in the image. This method imitates human behavior during knowledge transfer via a visual medium. A visual user input is used for the scope of this paper. This is the most sophisticated class of user simulation, but also the most intuitive interaction scheme for the human operator. According to Nickel, these [10] different user interactions can be included in one, and [10] multiple user interaction sets."}, {"heading": "1.2. State-of-the-art", "text": "Upconvolutionary network topologies such as FCN are a promising technique for solving elementary (dense) prediction problems on image data [LSD15, LBH15, WGCM16]. Classical Convolutionary Neural Networks (CNNs) usually attach fully connected layers or multi-layer layers to their contractual path. In contrast, FCNs consist exclusively of conventional and pooling layers. Missing layer types are replaced by unpooling / upsampling and deconventional layers in an expanding path. Shift-invariant filtering operations are therefore applied in every step of segment calculation and form hierarchies of acquired features. In this paper, we use the FCN topology to classify the pixel plane, commonly referred to as the U-network."}, {"heading": "2. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Interactive Network Architecture", "text": "Pictorial scribbles (seed dots, lines and shapes) are drawn by the user as an overlay mask M on the visualization of image I to the segment. Lines and complex shapes are represented as a series of seed dots. A seed point denotes a tuple si: = (pi, li), where pi [0, N) is a position in the image space and li \u00b2 {background, foreground} represents the label at this position in a binary segmentation system. Seed points are defined by the user to function as a representative subset S of the segmentation basic truth G: = {s0,..., sN \u2212 1}, S G. In each iteration, active user models add labeled scribbles to S, based on the difference between I and values li at the image coordinates pi, where tuples (pi, li) and S, as a seed mask MS, are designated."}, {"heading": "2.2. User Model", "text": "The user model simulates a human user during the training phase of the neural network by changing the input of the network for each epoch. User input is considered additive. For the initial interaction, St = 0b: = MGbe-MGbd, binary erosion and binary dilation on the voxel data MGc \u00a9 2017 The Author (s) Eurographics Proceedings c \u00a9 2017 The Eurographics Association. [RRW \u0445 09]. The superficially designated image elements after b-iterations of foreground erosion MGbe are combined with the background designated image elements after b-iterations of the foreground dilation MGbd. This method prevents the initial seeding near the true contour line and imitates the image elements Ult. s Uig rough estimation of the object to the segment, as shown in Fig."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data", "text": "The data set used in this paper consists of 27 fully annotated data sets, corresponding to reconstructions of DYNA CT scans of the pancreas of human patients with voxel resolutions from 0.463 mm3 to 0.683 mm3. The hepatic lesions are fully annotated by two medical experts and can be fully embedded in manually selected cubic volumes of 1003 voxels, which defines the fixed output size of the FCN. For the input volumes of interest (VOI), the dimensions of the output images must be enlarged to compensate for the reduction in the input image dimensions in each successive hidden layer due to the edge machining during individual folding operations. As the lesions are not at the limit of the abdominal image volume, these cubic volumes can be increased to VOIs of 2843 voxels with 92 voxels of the surrounding image dimensions."}, {"heading": "3.2. UI-Net Parameters", "text": "We opted for a network depth of 4, 32 initial filters of size 3 \u00d7 3, a batch size of 10 and 30 epochs for training without premature pausing. The learning rate of 10 \u2212 4 and impulse 0.9 are determined after training multiple FCNs on the same data set and varying parameters by evaluating their accuracy progress per epoch wr. t. smoothness, overall inclination and position of the minimum validation loss value. Data augmentation by elastic deformation [SSP03] is used to increase the amount of training and validation data by a factor of four as an additional regulator to counteract the risk of overmatch during training. A standard deviation \u03c3 = 4 of the Gaussian value in pixels and scaling factor \u03b1 = 68 to control the deformation intensity are chosen. The percentage of the active user model of new input data for the segment sample is set to a value of 5% of the user for each segment, whereas the percentage of the active user model of the sample is set to a more reasonable value for each segment, where the value of 1 is determined by the user for each segment is set to a value of 5% of the human sampling."}, {"heading": "3.3. UI-Net Seeds", "text": "Three experiments are performed using the UI network architecture: (1) The smaller b, the more information is used to generate input St = 0b to verify the segmentation quality etc. Rt. Additional domain knowledge inserted into the input layer. The smaller b, the more information is provided. No updated step of the user model is used here during the training, as the networks are trained with the same data in each epoch. We point to a network with the property St = i = St = i + 1 as being static during the training. (2) To deduce whether additional input data provided by the rules-based user model improves the segmentation quality, seeds are generated using an alternative system to (1)."}, {"heading": "4. Results and Discussion", "text": "As illustrated in Fig. 5 (a, b), the number of seed points given correlates with the overall segmentation quality (experiments (1, 2)).For evaluation with the actual user model, the interactive user input version of the UI network performs best, as illustrated in Fig. 5 (c) (experiment (3).The UI network trained with an interacting user model consistently performs better with each additional input from the user and continuously improves its segmentation outcomes. Therefore, training an FCN with a user model that responds to deficiencies in the current segmentation outcomes during the training can improve the overall segmentation outcomes. As illustrated in Fig. 5 (c, d), the UI network provides superior segmentation outcomes with the interactive and non-learn-based GrowCut approach."}, {"heading": "5. Conclusion and Outlook", "text": "The Auser model simulates plausible user interactions during the learning phase of the network. Unlike traditional FCNs, each classification generates new user input by an operator and incorporates it into the input of the network. Subsequently, the UI network can be trained with this information; the UI network learns to incorporate user information into the classification process; the proposed UI network architecture may be superior to fully automated approaches to highly accurate segmentation results, especially in medical applications where few records need to be processed and only a small database of fully commented images is available for training; the interactive user input version would require more training periods than a network of static user input for equivalent results if the test setup is not interactive, as shown in Figure 5 (a); the technique described to integrate user information into an FCN segmentation system may also be implemented alongside an interactive user input system."}], "references": [{"title": "Comparative evaluation of interactive segmentation approaches. Bildverarbeitung f\u00fcr die Medizin", "author": ["P. AMREHN M", "J. GLASBRENNER", "S. STEIDL", "K. MAIER A"], "venue": "(BVM) (2016),", "citeRegEx": "M. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "M. et al\\.", "year": 2016}, {"title": "Measures of the amount of ecologic association between species", "author": ["R. DICE L"], "venue": "Ecology (1945),", "citeRegEx": "L.,? \\Q1945\\E", "shortCiteRegEx": "L.", "year": 1945}, {"title": "Recognizing extrahepatic collateral vessels that supply hepatocellular carcinoma to avoid complications of transcatheter arterial chemoembolization", "author": ["H.-C. KIM", "W. CHUNG J", "W. LEE", "J. JAE H", "H. PARK J"], "venue": "Radiographics", "citeRegEx": "KIM et al\\.,? \\Q2005\\E", "shortCiteRegEx": "KIM et al\\.", "year": 2005}, {"title": "Transcatheter intraarterial therapies: rationale and overview", "author": ["J. LEWANDOWSKI R", "J.-F. GESCHWIND", "E. LIAPI", "R. SALEM"], "venue": "Radiology (2011),", "citeRegEx": "R. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "R. et al\\.", "year": 2011}, {"title": "Randomized controlled trial of transarterial lipiodol chemoembolization for unresectable hepatocellular carcinoma", "author": ["LO C.-M", "NGAN H", "TSO W.-K", "LIU C.-L", "LAM C.-M", "POON R.T.-P", "FAN S.-T", "WONG J"], "venue": "Hepatology", "citeRegEx": "C..M. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "C..M. et al\\.", "year": 2002}, {"title": "Fully convolutional networks for semantic segmentation. Computer Vision and Pattern Recognition", "author": ["J. LONG", "E. SHELHAMER", "T. DARRELL"], "venue": null, "citeRegEx": "LONG et al\\.,? \\Q2015\\E", "shortCiteRegEx": "LONG et al\\.", "year": 2015}, {"title": "Learning an interactive segmentation system", "author": ["H. NICKISCH", "C. ROTHER", "P. KOHLI", "C. RHEMANN"], "venue": "Computer Vision, Graphics and Image Processing (ICVGIP)", "citeRegEx": "NICKISCH et al\\.,? \\Q2010\\E", "shortCiteRegEx": "NICKISCH et al\\.", "year": 2010}, {"title": "Interaction in the segmentation of medical images: A survey", "author": ["D. OLABARRIAGA S", "M. SMEULDERS A. W"], "venue": "Medical Image Analysis (MIA)", "citeRegEx": "S. and W.,? \\Q2001\\E", "shortCiteRegEx": "S. and W.", "year": 2001}, {"title": "U-net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention (MICCAI", "author": ["O. RONNEBERGER", "P. FISCHER", "T. BROX"], "venue": null, "citeRegEx": "RONNEBERGER et al\\.,? \\Q2015\\E", "shortCiteRegEx": "RONNEBERGER et al\\.", "year": 2015}, {"title": "Image segmentation in twenty questions", "author": ["C. RUPPRECHT", "L. PETER", "N. NAVAB"], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "RUPPRECHT et al\\.,? \\Q2015\\E", "shortCiteRegEx": "RUPPRECHT et al\\.", "year": 2015}, {"title": "A perceptually motivated online benchmark for image matting", "author": ["C. RHEMANN", "C. ROTHER", "J. WANG", "M. GELAUTZ", "P. KOHLI", "P. ROTT"], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "RHEMANN et al\\.,? \\Q2009\\E", "shortCiteRegEx": "RHEMANN et al\\.", "year": 2009}, {"title": "Best practices for convolutional neural networks applied to visual document analysis. Document Analysis and Recognition", "author": ["Y. SIMARD P", "D. STEINKRAUS", "C. PLATT J"], "venue": null, "citeRegEx": "P. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "P. et al\\.", "year": 2003}, {"title": "GrowCut: Interactive multi-label ND image segmentation by cellular automata", "author": ["V. VEZHNEVETS", "V. KONOUCHINE"], "venue": "Computer Graphics and Applications (Graphicon)", "citeRegEx": "VEZHNEVETS and KONOUCHINE,? \\Q2005\\E", "shortCiteRegEx": "VEZHNEVETS and KONOUCHINE", "year": 2005}, {"title": "Deep learning computed tomography. Medical Image Computing and Computer-Assisted Intervention", "author": ["T. W\u00dcRFL", "C. GHESU F", "V. CHRISTLEIN", "K. MAIER A"], "venue": null, "citeRegEx": "W\u00dcRFL et al\\.,? \\Q2016\\E", "shortCiteRegEx": "W\u00dcRFL et al\\.", "year": 2016}, {"title": "How transferable are features in deep neural networks", "author": ["J. YOSINSKI", "J. CLUNE", "Y. BENGIO", "H. LIPSON"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "YOSINSKI et al\\.,? \\Q2014\\E", "shortCiteRegEx": "YOSINSKI et al\\.", "year": 2014}, {"title": "A benchmark for interactive image segmentation algorithms. Person-Oriented Vision (POV", "author": ["Y. ZHAO", "X. NIE", "Y. DUAN", "Y. HUANG", "S. LUO"], "venue": null, "citeRegEx": "ZHAO et al\\.,? \\Q2011\\E", "shortCiteRegEx": "ZHAO et al\\.", "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction. CCS Concepts \u2022Computing methodologies \u2192 Learning from implicit feedback; Neural networks; Supervised learning by classification;", "creator": "LaTeX with hyperref package"}}}