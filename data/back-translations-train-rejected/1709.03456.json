{"id": "1709.03456", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations", "abstract": "This paper introduces a novel activity dataset which exhibits real-life and diverse scenarios of complex, temporally-extended human activities and actions. The dataset presents a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. Furthermore, we also describe the methodology used to acquire annotations through crowdsourcing. Finally some activity recognition benchmarks are presented using current state-of-the-art techniques. We believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable for other common tasks in robotics/computer vision research such as object detection and human skeleton tracking.", "histories": [["v1", "Mon, 11 Sep 2017 16:01:17 GMT  (681kb,D)", "http://arxiv.org/abs/1709.03456v1", null], ["v2", "Thu, 21 Sep 2017 16:52:04 GMT  (682kb,D)", "http://arxiv.org/abs/1709.03456v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jawad tayyub", "majd hawasly", "david c hogg", "anthony g cohn"], "accepted": false, "id": "1709.03456"}, "pdf": {"name": "1709.03456.pdf", "metadata": {"source": "CRF", "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations", "authors": ["Jawad Tayyub", "Majd Hawasly", "David C. Hogg", "Anthony G. Cohn"], "emails": [], "sections": [{"heading": null, "text": "Keywords Aktivity Dataset, Crowdsourcing"}, {"heading": "1 Introduction", "text": "Artificial intelligence embedded in a human environment, such as a robot, is necessary to fully understand its environment so that it can take appropriate steps and make decisions to reap benefits. Recognizable / human activities are largely different in their complexity, length, and expressivity from short-term actions that only last a few seconds, such as selecting people who are in a restaurant. \"Most research in the field of human activity focuses on relatively short activities. Although there has been a steady shift toward a slightly longer activity, there is still a need for standardized data that represent longer and more complex activities."}, {"heading": "2 Related Datasets", "text": "This year it has come to be a reactionary, reactionary, reactionary and reactionary party."}, {"heading": "3 Dataset Description", "text": "The dataset presented in this essay can be accessed via [Dataset URL DOI]. The full size of the dataset prepared using sagej.clson-line is 222 GB. This is the entire dataset except for the point clouds data of the scenes available on request."}, {"heading": "3.1 Recording Setup", "text": "This dataset comprises 62 videos recorded with a high resolution of 1920x1080 pixels using a Microsoft Kinect 2 sensor, installed at an altitude of approximately 4.5 feet above the ground, with the aim of simulating one that a commonly used robot such as PR2, Tiago, Scitos G5, etc., would normally have when patrolling in an environment. Approximately 8-10 objects relevant to the activity were available to the subjects, and then five subjects were employed to perform the scenes, informed of the highest level activity they were to perform, and then provided with the objects needed to perform the task. 1-2 subjects were involved in each scene, and these subjects were carefully selected from outside the faculty, who had no experience in computer science, in order to capture the natural action of activities that are unbiased by any research knowledge."}, {"heading": "3.2 Dataset Content", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, dance, to dance, to dance, to dance, to dance,"}, {"heading": "4 Dataset Annotations", "text": "This task is expensive, time-consuming and can lead to distorted annotations for experts with expertise (Rye et al. 2010; Nguyen-Dinh et al. 2013). However, in order to reduce the cost and time needed to remove annotations, crowdsourcing platforms are becoming increasingly popular. A crowdsourcing platform offers a large pool of global workers who are able to perform a Human Intelligent Task (HIT), such as the annotation of a video, for a small financial incentive. For large sets of data and long videos, as is the case for our dataset, this is a suitable option to achieve annotations in a cost-effective and efficient manner."}, {"heading": "4.1 Interface Design", "text": "eSi rf\u00fc ide eeirrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4.2 Parameters", "text": "In addition to the design decisions that have been made regarding the interface used, Amazon Mechanical Turk allows the definition of several key parameters that affect the quality of the annotations made. In this section we briefly describe the important parameters and our selection of their values. An overview of the selected parameters is given in Table 1.It can be seen from the table that there are several parameters that require careful thought and experimentation to optimize the quality of the recorded annotations. We will elaborate some of our choices for the parameters further. \"Assignment Duration\" defines the maximum time allocated to the worker to complete the task. A pilot study was conducted with 6 volunteers to estimate the average maximum time needed (15 minutes) for a task to annotate a 1000 frame video. Given this, a maximum time of (f number of frames) / 1000 minutes after a linear relationship results. The \"reward amount\" is calculated with a maximum of 0.5 dollars for a task of 1000 frames."}, {"heading": "5 Conclusion", "text": "In this paper, we have presented an activity dataset of naturally occurring daily activities that could be observed by mobile robots. Access to the dataset is available at https: / / doi.org / 10.5518 / 249. We also presented the activity notes collected through crowdsourcing. We believe this dataset will be useful for robotics and computer vision research. The dataset presents new challenges for long-term autonomous robot systems to capture the activities they observe. In the future, we plan to expand the dataset to include object tracking and other activities where more people interact and are more complex. We also plan to record videos in a real environment such as a real restaurant or real office."}, {"heading": "Acknowledgements", "text": "We thank our colleagues in the School of Computing Robotics Lab, other schools of the university and in the STRANDS project consortium (http: / / strands-project.eu) for their contributions and financial support through the EU FP7 project 600623 (STRANDS).Notes1. http: / / homepages.inf.ed.ac.uk / rbf / CAVIAR / caviar.htm 2. https: / / www.mturk.com / mturk / welcome"}], "references": [{"title": "Actions as space-time shapes", "author": ["M Blank", "L Gorelick", "E Shechtman", "M Irani", "R Basri"], "venue": "Computer Vision,", "citeRegEx": "Blank et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blank et al\\.", "year": 2005}, {"title": "A (2013) A survey of video datasets for human action and activity recognition", "author": ["JM Chaquet", "EJ Carmona", "Fern\u00e1ndez-Caballero"], "venue": "Computer Vision and Image Understanding", "citeRegEx": "Chaquet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chaquet et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation for unsupervised activity analysis on an autonomous mobile robot", "author": ["P Duckworth", "M Al-Omari", "J Charles", "DC Hogg", "AG Cohn"], "venue": null, "citeRegEx": "Duckworth et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Duckworth et al\\.", "year": 2017}, {"title": "The STRANDS project: Long-term autonomy in everyday environments", "author": ["N Hawes", "C Burbridge", "F Jovan", "L Kunze", "B Lacerda", "L Mudrov\u00e1", "J Young", "J Wyatt", "D Hebesberger", "T K\u00f6rtner"], "venue": null, "citeRegEx": "Hawes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hawes et al\\.", "year": 2016}, {"title": "Learning human activities and object affordances from RGB-D videos", "author": ["HS Koppula", "R Gupta", "A Saxena"], "venue": "The International Journal of Robotics Research", "citeRegEx": "Koppula et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Koppula et al\\.", "year": 2013}, {"title": "Recognizing realistic actions from videos in the wild", "author": ["J Liu", "J Luo", "M Shah"], "venue": "Computer vision and pattern recognition,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Actions in context", "author": ["M Marsza\u0142ek", "I Laptev", "C Schmid"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition", "citeRegEx": "Marsza\u0142ek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marsza\u0142ek et al\\.", "year": 2009}, {"title": "Tagging human activities in video by crowdsourcing", "author": ["LV Nguyen-Dinh", "C Waldburger", "D Roggen", "G Tr\u00f6ster"], "venue": "Proceedings of the 3rd ACM conference on International conference on multimedia retrieval", "citeRegEx": "Nguyen.Dinh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen.Dinh et al\\.", "year": 2013}, {"title": "Collecting complex activity datasets in highly rich networked sensor environments", "author": ["D Roggen", "A Calatroni", "M Rossi", "T Holleczek", "K F\u00f6rster", "G Tr\u00f6ster", "P Lukowicz", "D Bannach", "G Pirkl", "A Ferscha"], "venue": "Networked Sensing Systems (INSS),", "citeRegEx": "Roggen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Roggen et al\\.", "year": 2010}, {"title": "Recognizing human actions: a local svm approach", "author": ["C Schuldt", "I Laptev", "B Caputo"], "venue": "Pattern Recognition,", "citeRegEx": "Schuldt et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schuldt et al\\.", "year": 2004}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K Soomro", "AR Zamir", "M Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unstructured human activity detection from RGBD images", "author": ["J Sung", "C Ponce", "B Selman", "A Saxena"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "Sung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2012}, {"title": "Convolutional pose machines", "author": ["SE Wei", "V Ramakrishna", "T Kanade", "Y Sheikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}, {"title": "Event-based analysis of video", "author": ["L Zelnik-Manor", "M Irani"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Zelnik.Manor and Irani,? \\Q2001\\E", "shortCiteRegEx": "Zelnik.Manor and Irani", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "A fully autonomous system capable of autonomously running for days or months amongst humans, for example the Strands project Hawes et al. (2016), needs to be equipped with the capability of recognising long-term activities that last hours or days using embedded sensors.", "startOffset": 125, "endOffset": 145}, {"referenceID": 3, "context": "A fully autonomous system capable of autonomously running for days or months amongst humans, for example the Strands project Hawes et al. (2016), needs to be equipped with the capability of recognising long-term activities that last hours or days using embedded sensors. However, popular activity datasets, e.g. OPPORTUNITY Activity Dataset Roggen et al. (2010), offer sensor data from a multitude of inertial and other sensors that are either body worn or installed in a perspective point, such as on the ceiling or corners of rooms, whose outputs are not normally available to an embedded robot in general environments.", "startOffset": 125, "endOffset": 362}, {"referenceID": 0, "context": "Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the \u2019activities of daily living\u2019 subcategory of the \u2019specific actions category\u2019.", "startOffset": 121, "endOffset": 143}, {"referenceID": 0, "context": "Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the \u2019activities of daily living\u2019 subcategory of the \u2019specific actions category\u2019. Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al.", "startOffset": 121, "endOffset": 331}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally.", "startOffset": 86, "endOffset": 106}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc.", "startOffset": 86, "endOffset": 333}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al.", "startOffset": 86, "endOffset": 622}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al.", "startOffset": 86, "endOffset": 641}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc.", "startOffset": 86, "endOffset": 689}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting.", "startOffset": 86, "endOffset": 1155}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al.", "startOffset": 86, "endOffset": 1514}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al. (2012) presented challenging and datasets consisting of longer videos than before and with complex activities such as stacking boxes, taking medicine etc.", "startOffset": 86, "endOffset": 1534}, {"referenceID": 12, "context": "\u2022 Skeletons: Two state-of-the-art skeleton trackers are used to generate skeleton tracks for all subjects in the recording Wei et al. (2016); Rafi et al.", "startOffset": 123, "endOffset": 141}, {"referenceID": 12, "context": "\u2022 Skeletons: Two state-of-the-art skeleton trackers are used to generate skeleton tracks for all subjects in the recording Wei et al. (2016); Rafi et al. (2016). These are provided in the skeletons files for each recording.", "startOffset": 123, "endOffset": 161}, {"referenceID": 8, "context": "The task is expensive, timeconsuming and, in case of having domain knowledgeable experts, can result in biased annotations (Roggen et al. 2010; Nguyen-Dinh et al. 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 7, "context": "The task is expensive, timeconsuming and, in case of having domain knowledgeable experts, can result in biased annotations (Roggen et al. 2010; Nguyen-Dinh et al. 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 7, "context": "2010; Nguyen-Dinh et al. 2013). In order to reduce cost and time taken to annotate activities, crowdsourcing platforms are increasingly gaining popularity. A crowdsourcing platform offer a large pool of world-wide workers that are able to perform a human intelligent task (HIT), such as annotation of a video, for a small financial incentive. For large datasets and long videos, as is the case for our dataset, this is a suitable option to attain annotations in a cost-effective and efficient manner. Furthermore, since many annotators are employed to annotate each single video, this helped ensure that a rich and varied perspective of the latent activities were reflected in the annotations. A single person usually tends to annotate videos sequentially one activity after the other in a flat temporal sequence. With multiple annotators, we increased the degree to which we obtained annotations at multiple level of temporal granualirty. Combining these annotations from different workers provides a richer annotation of the video. For our dataset, we make use of a popular crowdsourcing platform called Amazon Mechanical Turk(AMT) 2. There are however many challenges and design decisions to be taken when developing a system to elicit annotations using crowdsourcing such as deciding on the payment amount, making a clear interface for workers to perform the task, detection and removal of spam or non-diligent workers etc. Overcoming these challenges factors greatly affects the ability to obtain accurate annotations as shown by NguyenDinh et al. (2013). We will describe our design process next.", "startOffset": 6, "endOffset": 1560}], "year": 2017, "abstractText": "This paper introduces a novel activity dataset which exhibits real-life and diverse scenarios of complex, temporallyextended human activities and actions. The dataset presents a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. Furthermore, we also describe the methodology used to acquire annotations through crowdsourcing. Finally some activity recognition benchmarks are presented using current state-of-the-art techniques. We believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable for other common tasks in robotics/computer vision research such as object detection and human skeleton tracking.", "creator": "LaTeX with hyperref package"}}}