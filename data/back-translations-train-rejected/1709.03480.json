{"id": "1709.03480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Combining Strategic Learning and Tactical Search in Real-Time Strategy Games", "abstract": "A commonly used technique for managing AI complexity in real-time strategy (RTS) games is to use action and/or state abstractions. High-level abstractions can often lead to good strategic decision making, but tactical decision quality may suffer due to lost details. A competing method is to sample the search space which often leads to good tactical performance in simple scenarios, but poor high-level planning.", "histories": [["v1", "Mon, 11 Sep 2017 17:17:51 GMT  (169kb,D)", "http://arxiv.org/abs/1709.03480v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nicolas a barriga", "marius stanescu", "michael buro"], "accepted": false, "id": "1709.03480"}, "pdf": {"name": "1709.03480.pdf", "metadata": {"source": "CRF", "title": "Combining Strategic Learning and Tactical Search in Real-Time Strategy Games", "authors": ["Nicolas A. Barriga", "Marius Stanescu", "Michael Buro"], "emails": ["barriga@ualberta.ca", "astanesc@ualberta.ca", "mburo@ualberta.ca"], "sections": [{"heading": "1 Introduction", "text": "In recent years, numerous challenging research issues have attracted AI researchers to use real-time strategy games (RTS) as a test bed in several areas, such as case-based reasoning and planning (Ontan, o \"n et al. 2007), evolutionary calculation (Barriga, Stanescu and Buro 2014), machine learning (Synnaeve and Bessie, re 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversal search (Churchill and Buro 2011; Barriga, Stanescu and Buro 2015). Functional AI solutions to most RTS sub-problems exist, but the combination of these does not come close to the human level. To cope with large government spaces and branching factors in RTS games, recent work focuses on intelligent sampling of copyrights c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aia.org)."}, {"heading": "2 Related Work", "text": "In fact, it is the case that you will be able to put yourself at the top of society in the way that you put yourself at the top of the society in which it is situated."}, {"heading": "3 Algorithm Details", "text": "In fact, it is the case that most of them are able to abide by the rules which they have imposed on themselves. (...) Indeed, it is the case that most of them are able to abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they do not abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) (...) () () (...) () () () () () () () () () () () ()) () () () ()) () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () ()) () () () () () () ()) () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () (() () () () () (() () () (() () () (() () () () () () () ((() () ((() (() (() () () () (() () () (() ((() (() () () (() (()"}, {"heading": "4 Experiments and Results", "text": "All experiments were performed on machines running Fedora 25, with an Intel Core i7-7700K CPU, with 32 GB of RAM, and an NVIDIA GeForce GTX 1070 with 8 GB of RAM. The Java version used for \u00b5RTS was OpenJDK 1.8.0, Caffe git commit 365ac88 was compiled with g + 5.3.0, and pycaffe was executed with python 2.7.13. The doll search version we used for all subsequent experiments uses alpha beta search via a single selection point with four options, the four options being WorkerRush, LightRush, RangedRush, and HeavyRush, and were also used as baseline in the following experiments. Further details about these scripts can be found in (Stanescu et al. 2016). Two other current algorithms were also used as benchmarks, Na \u00a8 veMCTS, Ontan, Indio 2013, and Adversarial Structure, respectively, hierarchical in the hierarchical hierarchy of the hierarchy."}, {"heading": "4.1 State Evaluation Network", "text": "Training data for the rating network was generated by running games between a series of bots that used 5 different cards with 12 different starting positions each. Ties were discarded, and the remaining games were divided into 2190 training games and 262 test games. 12 states were randomly sampled from each game, for a total of 26,280 training samples and 3,144 test samples. Data is marked with a Boolean value indicating whether the first player won. All rating functions were trained on the same data sets. Network weights are initialized using the Xavier initialization (Glorot and Bengio 2010). We used Adaptive Instant Assessment (ADAM) (Kingma and Ba 2014) with default values of \u03b21 = 0.9, \u03b22 = 0.999, = 10 \u2212 8 and a basic learning rate of 10 \u2212 4. The batch size of the network was calculated at 256.The rating network achieves an accuracy of 95% of the classification of samples as wins or losses."}, {"heading": "4.2 Policy Network", "text": "We used the same procedure as in the previous subsection, but now we marked the samples with the result of a 10-second doll search using the rating network. The resulting political network has an accuracy of 73% for predicting the correct doll pull and an accuracy of 95% for predicting the two best moves. Table 3 shows that the political network comes very close to the doll search and defeats all scripts."}, {"heading": "4.3 Strategy and Tactics", "text": "We will do this by allocating a fraction of the allotted time to the strategic algorithm and the rest to the tactical algorithm, which will be called Na\u0131 \u00bc veMCTS in our experiments. We expect the political network to perform better in this scenario as it runs significantly faster than PuppetSearch while maintaining a similar performance. The best time interval between strategic and tactical algorithms was determined experimentally with 20% for Puppet Search and 80% for Na\u0131 \u00bc veMCTS. The political network uses a fixed time (about 3ms), and the remaining time is allocated to tactical search.Table 4 shows that both strategic algorithms benefit greatly from merging with a tactical algorithm."}, {"heading": "PS CNN - 59.2 89.2 72.5 73.6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PS CNN - 80 95 82.5 85.8", "text": "More importantly for the political network, which now scores 56.7% over its Puppet Search counterpart and has a 4.3% higher overall win rate, despite significantly worse results against WorkerRush and AHTN-P. These appear to be due to a strategic error on the part of the political network, which, if its cause can be identified and corrected, would lead to even higher performance."}, {"heading": "5 Conclusions and Future Work", "text": "We have expanded on previous research that has used CNNs to accurately assess RTS game states each time, applying small maps to larger map sizes that are normally used in commercial RTS games. Average accuracy of predicting wins over all seasons is higher than in smaller scenarios. This is probably the case because strategic decisions are more important than tactical decisions in larger maps, and strategic development is easier to quantify by the network. Although the network is several orders of magnitude slower than competing simpler rating features, its accuracy makes it more effective. If the Puppet Search uses high-level opposing search algorithms, its performance is better than when using simpler but faster functionality. We have also trained a political network to predict the outcome of the Puppet Search. The winning rate of the resulting network is similar to that of the original search, with some exceptions against specific opponents."}], "references": [{"title": "Building placement optimization in real-time strategy games", "author": ["N.A. Barriga", "M. Stanescu", "M. Buro"], "venue": "Workshop on Artificial Intelligence in Adversarial RealTime Games, AIIDE. Barriga, N. A.; Stanescu, M.; and Buro, M. 2015. Puppet", "citeRegEx": "Barriga et al\\.,? 2014", "shortCiteRegEx": "Barriga et al\\.", "year": 2014}, {"title": "Combining scripted behavior with game tree search for stronger, more robust game ai", "author": ["N.A. Barriga", "M. Stanescu", "M. Buro"], "venue": "Game AI Pro 3: Collected Wisdom of Game AI Professionals. CRC Press. chapter 14.", "citeRegEx": "Barriga et al\\.,? 2017a", "shortCiteRegEx": "Barriga et al\\.", "year": 2017}, {"title": "Game tree search based on non-deterministic action scripts in realtime strategy games", "author": ["N.A. Barriga", "M. Stanescu", "M. Buro"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games (TCIAIG).", "citeRegEx": "Barriga et al\\.,? 2017b", "shortCiteRegEx": "Barriga et al\\.", "year": 2017}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning, 41\u201348. ACM.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Build order optimization in StarCraft", "author": ["D. Churchill", "M. Buro"], "venue": "AI and Interactive Digital Entertainment Conference, AIIDE (AAAI), 14\u201319.", "citeRegEx": "Churchill and Buro,? 2011", "shortCiteRegEx": "Churchill and Buro", "year": 2011}, {"title": "Portfolio greedy search and simulation for large-scale combat in StarCraft", "author": ["D. Churchill", "M. Buro"], "venue": "IEEE Conference on Computational Intelligence in Games (CIG), 1\u20138. IEEE.", "citeRegEx": "Churchill and Buro,? 2013", "shortCiteRegEx": "Churchill and Buro", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "An empirical study of potential-based reward shaping and advice in complex, multi-agent systems", "author": ["S. Devlin", "D. Kudenko", "M. Grze\u015b"], "venue": "Advances in Complex Systems 14(02):251\u2013278.", "citeRegEx": "Devlin et al\\.,? 2011", "shortCiteRegEx": "Devlin et al\\.", "year": 2011}, {"title": "Stabilising experience replay for deep Multi-Agent reinforcement learning", "author": ["J. Foerster", "N. Nardelli", "G. Farquhar", "P.H.S. Torr", "P. Kohli", "S. Whiteson"], "venue": "Thirty-fourth International Conference on Machine Learning.", "citeRegEx": "Foerster et al\\.,? 2017", "shortCiteRegEx": "Foerster et al\\.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. v. Hasselt", "A. Guez", "D. Silver"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2094\u20132100. AAAI Press.", "citeRegEx": "Hasselt et al\\.,? 2016", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence 35(1):221\u2013231.", "citeRegEx": "Ji et al\\.,? 2013", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Adversarial hierarchicaltask network planning for complex real-time games", "author": ["S. Onta\u00f1\u00f3n", "M. Buro"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI), 1652\u20131658.", "citeRegEx": "Onta\u00f1\u00f3n and Buro,? 2015", "shortCiteRegEx": "Onta\u00f1\u00f3n and Buro", "year": 2015}, {"title": "Case-based planning and execution for real-time strategy games", "author": ["S. Onta\u00f1\u00f3n", "K. Mishra", "N. Sugandh", "A. Ram"], "venue": "ICCBR \u201907, 164\u2013178. Berlin, Heidelberg: Springer-Verlag.", "citeRegEx": "Onta\u00f1\u00f3n et al\\.,? 2007", "shortCiteRegEx": "Onta\u00f1\u00f3n et al\\.", "year": 2007}, {"title": "The combinatorial multi-armed bandit problem and its application to real-time strategy games", "author": ["S. Onta\u00f1\u00f3n"], "venue": "AIIDE.", "citeRegEx": "Onta\u00f1\u00f3n,? 2013", "shortCiteRegEx": "Onta\u00f1\u00f3n", "year": 2013}, {"title": "Informed monte carlo tree search for real-time strategy games", "author": ["S. Onta\u00f1\u00f3n"], "venue": "Computational Intelligence and Games (CIG), 2016 IEEE Conference on, 1\u20138. IEEE.", "citeRegEx": "Onta\u00f1\u00f3n,? 2016", "shortCiteRegEx": "Onta\u00f1\u00f3n", "year": 2016}, {"title": "Combinatorial multi-armed bandits for real-time strategy games", "author": ["S. Onta\u00f1\u00f3n"], "venue": "Journal of Artificial Intelligence Research 58:665\u2013702.", "citeRegEx": "Onta\u00f1\u00f3n,? 2017", "shortCiteRegEx": "Onta\u00f1\u00f3n", "year": 2017}, {"title": "Multiagent Bidirectionally-Coordinated nets for learning to play StarCraft combat games", "author": ["P. Peng", "Q. Yuan", "Y. Wen", "Y. Yang", "Z. Tang", "H. Long", "J. Wang"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484\u2013489", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806.", "citeRegEx": "Springenberg et al\\.,? 2014", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Hierarchical adversarial search applied to real-time strategy games", "author": ["M. Stanescu", "N.A. Barriga", "M. Buro"], "venue": "Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 66\u201372.", "citeRegEx": "Stanescu et al\\.,? 2014", "shortCiteRegEx": "Stanescu et al\\.", "year": 2014}, {"title": "Using Lanchester attrition laws for combat prediction in StarCraft", "author": ["M. Stanescu", "N.A. Barriga", "M. Buro"], "venue": "Eleventh Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 86\u2013", "citeRegEx": "Stanescu et al\\.,? 2015", "shortCiteRegEx": "Stanescu et al\\.", "year": 2015}, {"title": "Evaluating real-time strategy game states using convolutional neural networks", "author": ["M. Stanescu", "N.A. Barriga", "A. Hess", "M. Buro"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG).", "citeRegEx": "Stanescu et al\\.,? 2016", "shortCiteRegEx": "Stanescu et al\\.", "year": 2016}, {"title": "Learning multiagent communication with backpropagation", "author": ["S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I.; and Garnett, R., eds., Advances in Neural Information Processing Systems 29. Curran Associates, Inc. 2244\u20132252.", "citeRegEx": "Sukhbaatar et al\\.,? 2016", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "A Bayesian model for plan recognition in RTS games applied to StarCraft", "author": ["G. Synnaeve", "P. Bessi\u00e8re"], "venue": "AAAI., ed., Proceedings of the Seventh Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE 2011), Proceedings of AIIDE, 79\u201384.", "citeRegEx": "Synnaeve and Bessi\u00e8re,? 2011", "shortCiteRegEx": "Synnaeve and Bessi\u00e8re", "year": 2011}, {"title": "Multiscale Bayesian modeling for RTS games: An application to StarCraft AI", "author": ["G. Synnaeve", "P. Bessi\u00e8re"], "venue": "IEEE Transactions on Computational intelligence and AI in Games 8(4):338\u2013350.", "citeRegEx": "Synnaeve and Bessi\u00e8re,? 2016", "shortCiteRegEx": "Synnaeve and Bessi\u00e8re", "year": 2016}, {"title": "Game-tree search over high-level game states in RTS games", "author": ["A. Uriarte", "S. Onta\u00f1\u00f3n"], "venue": "Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE\u201914, 73\u201379. Usunier, N.; Synnaeve, G.; Lin, Z.; and Chintala, S. 2017.", "citeRegEx": "Uriarte and Onta\u00f1\u00f3n,? 2014", "shortCiteRegEx": "Uriarte and Onta\u00f1\u00f3n", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "In recent years, numerous challenging research problems have attracted AI researchers to using real-time strategy (RTS) games as test-bed in several areas, such as casebased reasoning and planning (Onta\u00f1\u00f3n et al. 2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessi\u00e8re 2011), deep learning (Usunier et al.", "startOffset": 197, "endOffset": 218}, {"referenceID": 27, "context": "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessi\u00e8re 2011), deep learning (Usunier et al.", "startOffset": 85, "endOffset": 113}, {"referenceID": 8, "context": "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessi\u00e8re 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).", "startOffset": 129, "endOffset": 190}, {"referenceID": 20, "context": "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessi\u00e8re 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).", "startOffset": 129, "endOffset": 190}, {"referenceID": 4, "context": "2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).", "startOffset": 43, "endOffset": 102}, {"referenceID": 5, "context": "shtml#mvm the search space (Churchill and Buro 2013; Onta\u00f1\u00f3n 2017; 2016; Onta\u00f1\u00f3n and Buro 2015) and state and action abstractions (Uriarte and Onta\u00f1\u00f3n 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).", "startOffset": 27, "endOffset": 95}, {"referenceID": 19, "context": "shtml#mvm the search space (Churchill and Buro 2013; Onta\u00f1\u00f3n 2017; 2016; Onta\u00f1\u00f3n and Buro 2015) and state and action abstractions (Uriarte and Onta\u00f1\u00f3n 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).", "startOffset": 27, "endOffset": 95}, {"referenceID": 15, "context": "shtml#mvm the search space (Churchill and Buro 2013; Onta\u00f1\u00f3n 2017; 2016; Onta\u00f1\u00f3n and Buro 2015) and state and action abstractions (Uriarte and Onta\u00f1\u00f3n 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).", "startOffset": 27, "endOffset": 95}, {"referenceID": 29, "context": "shtml#mvm the search space (Churchill and Buro 2013; Onta\u00f1\u00f3n 2017; 2016; Onta\u00f1\u00f3n and Buro 2015) and state and action abstractions (Uriarte and Onta\u00f1\u00f3n 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).", "startOffset": 130, "endOffset": 225}, {"referenceID": 25, "context": "We will base our network on previous work on CNNs for state evaluation (Stanescu et al. 2016), reformulating the earlier approach to handle larger maps.", "startOffset": 71, "endOffset": 93}, {"referenceID": 6, "context": "These include uni-dimensional streams in natural language processing (Collobert and Weston 2008), two-dimensional board games (Silver et al.", "startOffset": 69, "endOffset": 96}, {"referenceID": 21, "context": "These include uni-dimensional streams in natural language processing (Collobert and Weston 2008), two-dimensional board games (Silver et al. 2016), or threedimensional video analysis (Ji et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 11, "context": "2016), or threedimensional video analysis (Ji et al. 2013).", "startOffset": 42, "endOffset": 58}, {"referenceID": 14, "context": "mance in several Atari games, by using Q-learning, a well known reinforcement learning (RL) algorithm (Mnih et al. 2015).", "startOffset": 102, "endOffset": 120}, {"referenceID": 21, "context": "But the most remarkable accomplishment may be AlphaGo (Silver et al. 2016), a Go playing program that last year defeated Lee Sedol, one of the top human professionals, a feat that was thought to be at least a decade away.", "startOffset": 54, "endOffset": 74}, {"referenceID": 25, "context": "Additional encouraging results were achieved for the task of evaluating RTS game states (Stanescu et al. 2016).", "startOffset": 88, "endOffset": 110}, {"referenceID": 20, "context": "However, symmetric communication prevents handling heterogeneous agent types, limitation later removed by (Peng et al. 2017) which use a dedicated bidirection communication channel and recurrent neural networks.", "startOffset": 106, "endOffset": 124}, {"referenceID": 25, "context": "We build on previous work on RTS game state evaluation (Stanescu et al. 2016) applied to \u03bcRTS (see figure 1).", "startOffset": 55, "endOffset": 77}, {"referenceID": 22, "context": "To tackle this problem, we designed a fully convolutional network (FCN) which only consists of intermediate convolutional layers (Springenberg et al. 2014) and has the advantage of being an architecture that can fit a wide range of board sizes.", "startOffset": 129, "endOffset": 155}, {"referenceID": 25, "context": "More details about these scripts can be found in (Stanescu et al. 2016).", "startOffset": 49, "endOffset": 71}, {"referenceID": 17, "context": "Two other recent algorithms were also used as benchmarks, Na \u0308veMCTS (Onta\u00f1\u00f3n 2013) and Adversarial Hierarchical Task Networks (AHTNs) (Onta\u00f1\u00f3n and Buro 2015).", "startOffset": 69, "endOffset": 83}, {"referenceID": 15, "context": "Two other recent algorithms were also used as benchmarks, Na \u0308veMCTS (Onta\u00f1\u00f3n 2013) and Adversarial Hierarchical Task Networks (AHTNs) (Onta\u00f1\u00f3n and Buro 2015).", "startOffset": 135, "endOffset": 158}, {"referenceID": 9, "context": "The network\u2019s weights are initialized using Xavier initialization (Glorot and Bengio 2010).", "startOffset": 66, "endOffset": 90}, {"referenceID": 12, "context": "We used adaptive moment estimation (ADAM) (Kingma and Ba 2014) with default values of \u03b21 = 0.", "startOffset": 42, "endOffset": 62}, {"referenceID": 25, "context": "The network\u2019s accuracy is even higher than previous results in 8x8 maps (Stanescu et al. 2016).", "startOffset": 72, "endOffset": 94}, {"referenceID": 28, "context": "However, this has proved to be a difficult goal, as previous attempts by other researchers have had limited success on simple scenarios (Usunier et al. 2017; Synnaeve and Bessi\u00e8re 2016).", "startOffset": 136, "endOffset": 185}, {"referenceID": 20, "context": "Recent research avenues based on integrating concepts such as communication (Sukhbaatar, Szlam, and Fergus 2016), unit grouping and bidirectional recurrent neural networks (Peng et al. 2017) suggest that strong tactical networks might soon be available.", "startOffset": 172, "endOffset": 190}, {"referenceID": 3, "context": "There are several strategies available that could help overcome these issues, such as curriculum learning (Bengio et al. 2009), reward shaping (Devlin, Kudenko, and Grze\u015b 2011), or implementing double DQN learning (Hasselt, Guez, and Silver 2016).", "startOffset": 106, "endOffset": 126}], "year": 2017, "abstractText": "A commonly used technique for managing AI complexity in real-time strategy (RTS) games is to use action and/or state abstractions. High-level abstractions can often lead to goodions. High-level abstractions can often lead to good strategic decision making, but tactical decision quality may suffer due to lost details. A competing method is to sample the search space which often leads to good tactical performance in simple scenarios, but poor high-level planning. We propose to use a deep convolutional neural network (CNN) to select among a limited set of abstract action choices, and to utilize the remaining computation time for game tree search to improve low level tactics. The CNN is trained by supervised learning on game states labelled by Puppet Search, a strategic search algorithm that uses action abstractions. The network is then used to select a script \u2014 anions. The network is then used to select a script \u2014 an abstract action \u2014 to produce low level actions for all units.action \u2014 to produce low level actions for all units. Subsequently, the game tree search algorithm improves the tactical actions of a subset of units using a limited view of the game state only considering units close to opponent units. Experiments in the \u03bcRTS game show that the combined algorithm results in higher win-rates than either of its two independent components and other state-of-the-art \u03bcRTS agents. To the best of our knowledge, this is the first successful application of a convolutional network to play a full RTS game on standard game maps, as previous work has focused on subproblems, such as combat, or on very small maps.", "creator": "TeX"}}}