{"id": "1709.03582", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Art of singular vectors and universal adversarial perturbations", "abstract": "Vulnerability of state-of-the-art deep neural networks to adversarial attacks has been attracting a lot of attention recently. In this work we propose a new algorithm for constructing universal adversarial perturbations. Our approach is based on computing the so called $(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns and by using a batch of just $64$ images we can construct adversarial perturbations with relatively high fooling rate. We also investigate a correlation between the singular values of the Jacobian matrices and the fooling rate of a corresponding singular vector.", "histories": [["v1", "Mon, 11 Sep 2017 20:22:37 GMT  (1308kb,D)", "http://arxiv.org/abs/1709.03582v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["valentin khrulkov", "ivan oseledets"], "accepted": false, "id": "1709.03582"}, "pdf": {"name": "1709.03582.pdf", "metadata": {"source": "CRF", "title": "Art of singular vectors and universal adversarial perturbations", "authors": ["Valentin Khrulkov", "Ivan Oseledets"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It was recently shown (Moosavi-Dezfooli et al. 2016) that for many state-of-the-art deep neural networks (DNN) universal hostile disturbances exist - single vectors of a small standard that are added to the images in a dataset with high probability, leading to a misclassification of these images. In this thesis we present an algorithm for the construction of universal adverse disturbances as so-called (p, q) singular vectors of the jacobic matrices of characteristic maps of a DNN. The (p, q) singular vector is defined as simple = argmax."}, {"heading": "2 Problem statement", "text": "Suppose we have a standard feed DNN that uses a vector x = input q = input q (x) and prints a vector of probabilities p (x) for class markers. Our goal is to generate the parameters q \u2265 1 and L > 0 for as many x in a dataset as possible. The efficiency of the given universal universal adverse error is called a folly rate for the dataset X of size N and is called | {x \u00b2 X: argmax p (x) 6 = Argmax p (x + \u03b5)} | N. Let us call the outputs of the i-th hidden layer of the network by fi (x). Then we have it havefi (x + \u03b5) \u2212 fi (x) p: Pergmax p (x) pergmax p (x)."}, {"heading": "3 Generalized power method", "text": "Suppose that for some linear mapAwe the matrixby vector products of A > Q = A > Q = A > Q = A > A > J = A = A = A = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B B = B = B = B = B B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B ="}, {"heading": "4 Efficient implementation of matrix-by-vector products", "text": "Matrices involved in algorithm 2 for typical DNNs are too large to be explicitly constructed. However, let's assume that we can construct matrix-by-vector products using the automatic differentiation available in most deep learning packages, which are then evaluated in a fraction of a second. Let's say we get an operation degree [f] (x0) that calculates the gradient of a scalar function f (x) in relation to the vector variable x at point x0. Let's let fi (x) be a fixed layer of the DNN, so that x-Rn and fi-Rm, i.e. Ji (x), x-Rm-n, and for vectors v1-Rn, v2-Rm, we Ji (x) v1, J > i (x) v2 (v2-v2), v2-v2 (v2), vrifi-vrix (vrifi), and vrix (vrix) products."}, {"heading": "5 Experiments", "text": "In this section, we analyze various contrary disturbances constructed using algorithm 2. For testing, we used the ILSVRC 2012 validation dataset (Russakovsky et al. 2015) (50000 images). In our experiments, we chose p = \u221e, q = 10 and calculated the (p, q) - singular vectors for different layers of VGG-16 and VGG-19 (Simonyan and Zisserman 2014) and ResNet50 (He et al. 2016). q = 10 was chosen to smooth out the optimization problem and effectively serves as a replacement for q = \u221e, for which we discovered that the highest dumbing rates are achieved. Batch size in algorithm 2 was chosen 64, and we used the same 64 images to construct all the contrary disturbances."}, {"heading": "6 Fooling rate and singular values", "text": "7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "7 Analysis of perturbations", "text": "In this section, we analyze some of the erroneous images that we created in 2016 and 2016, in order to then put them into the world. () We have seen ourselves put into the world, in which we see ourselves able to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) (...) We have managed to change the world. (...) (...) We have managed to change the world. (...) (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...).).....). (...).)......................................................................................................................................................."}, {"heading": "Acknowledgements", "text": "This study was supported by the Ministry of Education and Science of the Russian Federation (grant 14,756.31.0001), the RFBR grant 16-31-60095-mol-a-dk, 16-31-00372-mola and the Skoltech NGP programme."}], "references": [{"title": "and Vijayaraghavan", "author": ["A. Bhaskara"], "venue": "A.", "citeRegEx": "Bhaskara and Vijayaraghavan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "D", "author": ["Boyd"], "venue": "W.", "citeRegEx": "Boyd 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "Parseval networks: Improving robustness to adversarial examples", "author": ["Cisse"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cisse,? \\Q2017\\E", "shortCiteRegEx": "Cisse", "year": 2017}, {"title": "Robustness of classifiers: from adversarial to random noise", "author": ["Moosavi-Dezfooli Fawzi", "A. Frossard 2016] Fawzi", "S.-M. Moosavi-Dezfooli", "P. Frossard"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fawzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2016}, {"title": "I", "author": ["Goodfellow"], "venue": "J.; Shlens, J.; and Szegedy, C.", "citeRegEx": "Goodfellow. Shlens. and Szegedy 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "He,? \\Q2016\\E", "shortCiteRegEx": "He", "year": 2016}, {"title": "Universal adversarial perturbations", "author": ["Moosavi-Dezfooli"], "venue": "arXiv preprint arXiv:1610.08401", "citeRegEx": "Moosavi.Dezfooli,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Fawzi Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "2016. cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768", "author": ["Papernot"], "venue": null, "citeRegEx": "Papernot,? \\Q2016\\E", "shortCiteRegEx": "Papernot", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky"], "venue": null, "citeRegEx": "Russakovsky,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky", "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["A. man"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "man,? \\Q2014\\E", "shortCiteRegEx": "man", "year": 2014}, {"title": "and Bau III", "author": ["L.N. Trefethen"], "venue": "D.", "citeRegEx": "Trefethen and Bau III 1997", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [], "year": 2017, "abstractText": "Vulnerability of state-of-the-art deep neural networks to adversarial attacks has been attracting a lot of attention recently. In this work we propose a new algorithm for constructing universal adversarial perturbations. Our approach is based on computing the so called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns and by using a batch of just 64 images we can construct adversarial perturbations with relatively high fooling rate. We also investigate a correlation between the singular values of the Jacobian matrices and the fooling rate of a corresponding singular vector.", "creator": "LaTeX with hyperref package"}}}