{"id": "1709.03879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Ultimate Intelligence Part III: Measures of Intelligence, Perception and Intelligent Agents", "abstract": "We propose that operator induction serves as an adequate model of perception. We explain how to reduce universal agent models to operator induction. We propose a universal measure of operator induction fitness, and show how it can be used in a reinforcement learning model and a homeostasis (self-preserving) agent based on the free energy principle. We show that the action of the homeostasis agent can be explained by the operator induction model.", "histories": [["v1", "Fri, 8 Sep 2017 17:45:30 GMT  (14kb)", "http://arxiv.org/abs/1709.03879v1", "Third installation of the Ultimate Intelligence series. Submitted to AGI-2017. arXiv admin note: text overlap witharXiv:1504.03303"]], "COMMENTS": "Third installation of the Ultimate Intelligence series. Submitted to AGI-2017. arXiv admin note: text overlap witharXiv:1504.03303", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eray \\\"ozkural"], "accepted": false, "id": "1709.03879"}, "pdf": {"name": "1709.03879.pdf", "metadata": {"source": "CRF", "title": "Ultimate Intelligence Part III: Measures of Intelligence, Perception and Intelligent Agents", "authors": ["Eray \u00d6zkural"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 9.03 879v 1 [cs.A I] 8 Sep 201 7 \"We need to know - we will know!\" - David Hilbert"}, {"heading": "1 Introduction", "text": "The Ultimate Intelligence Research Program is inspired by Seth Lloyd's work on the Ultimate Physical Limits of Calculation. [15] We are investigating the Ultimate Physical Limits and Conditions of Intelligence. This is the third installation in the series, in the first two parts new physical complexity yardsticks, priority values and limits of inductive inference were proposed [18,17]. We frame the question of Ultimate Limits of Intelligence in a General Physical Environment, for which we provide a general definition of an intelligent system and a physical performance criterion that turns out to be, as expected, a relationship between physical quantities and information, the latter of which we had conceptually reduced to physics with minimal machine volume complexity [18]."}, {"heading": "2 Notation and Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Universal Induction", "text": "Let us recall Solomonoff's universal distribution [21]. Let U be a universal computer that executes programs with a prefix-free encoding such as LISP; y = U (x) means that the output of program x to U is where x and y are bit strings. 1Any unspecified variable or function is presented as bit string1. A prefix-free code is a series of codes in which no code is a prefix of another. A computer file uses a prefix-free code ending with an EOF symbol, so the most reasonable programming languages are prefix-free. | x | denotes the length of a bit string x. f (\u00b7) refers to the function f rather than its application.The algorithmic probability that a bit string x, 0, 1} + is generated by a random program x is a random programming language."}, {"heading": "2.2 Operator induction", "text": "Operator induction is a general form of supervised machine learning in which we learn a stochastic map from n question and answer pairs D = {(qi, ai)}, which are sampled from a (calculable) stochastic source \u00b5. Operator induction can be solved by finding a series of operators Oj (\u00b7 \u00b7) in available time, each of which has a conditional probability density function (cpdf), so that the following fit quality maximized\u044b = \u2211 jn (3) is for a stochastic source \u00b5, each term in the summary being the contribution of a model. (4) Qi and ai are question and answer pairs in the input data set drawn from \u00b5, and O j is a compatible cpdf in Equation 4. We can use the found m operators to predict invisible data with a mixing model [24]. (PU qm + qm = j1 + jm = high probability)."}, {"heading": "2.3 Set induction", "text": "Set induction generalizes unattended machine learning, in which we learn a probability density function (pdf) from a set of n bit strings D = {d1, d2,..., dn} from a stochastic source \u00b5. We can then inductively conclude that new members are added to the sentence with: P (dn + 1) = PU (D-dn + 1) PU (D) (6) Set induction is clearly a limited case of operator induction, in which we set Qi's to zero string. Set induction is a universal form of clustering, and it perfectly models perception. When we apply set induction to a large set of 2D images of a room, this inevitably results in a 3D representation. When we apply it to physical sensor data, the physical theory - perfectly general, with infinite ranges - that explains the data, perception is merely a more consequential case, though not a set ministerial problem."}, {"heading": "2.4 Universal measures of intelligence", "text": "There is a lot of literature on the definition of an intelligence measure. Hutter has defined an intelligence order relationship in the context of his Universal Reinforcement Learning Model AIXI [8], which suggests that intelligence corresponds to the set of problems an agent can solve. Also noteworthy is the Universal Intelligence Measurement [10,11], which in turn is based on the AIXI model. Its universal intelligence measurement is based on the following philosophical definition, which was compiled from their review of the definitions of intelligence in AI literacy. Intelligence measures an agent's ability to achieve goals in a wide range of environments. It implies that intelligence requires an autonomous, targeted agent. Intelligence measurement of [10] is defined as equivalent (\u03c0) = E-2 \u2212 HU (\u00b5) V-2, whereby \u00b5 is a calculable reward limit in an environment where ZIP is acting and ZIP is expected to be the total Z\u00b5 V in the future \u00b5 (\u00b5 V)."}, {"heading": "2.5 The free energy principle.", "text": "In Asimov's story entitled \"The Last Question,\" the task of life is identified as overcoming the second law of thermodynamics, which is, however, futile. Variable free energy essentially measures predictive errors, and it was introduced by Feynmann to solve complex problems in quantum physics. In thermodynamic free energy, energies are negative log probabilities such as entropy. The free energy principle states that each system must minimize its free energy in order to maintain its order. An adaptive system that tends to minimize average surprise (entropy) will tend to survive longer. A biological organism can be modeled as an adaptive system that has an implicit probability model of the environment, and the variable free energy will therefore increase the chances of survival to minimize free energy. Divergence between the pdf of the environment and an arbitrary pdf that is minimized by its own model is minimized in tone."}, {"heading": "3 Perception as General Intelligence", "text": "Since we are mainly interested in stochastic problems in the physical world, we propose a simple informal definition of intelligence: definition 2. Intelligence measures the ability of a mechanism to solve predictive problems. Mechanism is, as usual, any physical machine, see [4] which suggests the same thing. Therefore, a general formulation of the Solomonoff induction, the operator induction, could also serve as a model of general intelligence [24]. Remember that the operator induction can deduce from any physically plausible cpdf, so its approximation can solve any classically monitored problem of machine learning. The only minor problem with Equation 7 might be that it seems to exclude classical AI systems that are not agents, such as expert systems, machine learning tools, knowledge representation systems, search and planning algorithms, etc., which are somewhat more naturally covered by our informal definition."}, {"heading": "3.1 Is operator induction adequate?", "text": "There are two strong objections to the operator induction that we know of. It is argued that in a dynamic environment, as in a physical environment, we must use an active reward problem so that we can take into account changes in the environment, as in space-time embedded agents [16], which also provide an agent-based intelligence metric. This objection can be answered by the simple solution that every decision of an active intelligent system can be considered a separate production problem. The second objection is that the basic solomonoff induction can only predict the next bit, but not the expected cumulative reward that can solve its extensions by explaining that we can reduce an agent model to a perception and action planning, as in OOOOPS-RL [20]. In OPS-RL, the perception module looks for the best world model that describes the history of sensory input and actions in general time using world OPS search programs and programs."}, {"heading": "4 Physical Quantification of Intelligence", "text": "Definition 1 corresponds to any type of reinforcement-learning or target-follower agent in the AI literature and can be adapted to solve other problems. Instead of an reinforcement-learning approach, the uncontrolled active inference-agent approach is proposed in [7], and the authors argue that they do not need to rely on the notion of reward, value or benefit. In particular, the authors claim that they could solve the mountain-car problem by formulating energy-saving perception. We therefore propose a perception-intelligence measure."}, {"heading": "4.1 Universal measure of perception fitness", "text": "Note that operator induction is considered insufficient to describe universal agents such as AIXI, as the basic sequence induction is not suitable for modeling optimization problems [8]. However, a modified Levin search procedure can solve such optimization problems, as in the search for an optimal control program [20]. In OOPS-RL, the perception module looks for the best world model that maximizes cumulative reward equally. In this essay, we consider the perception module of such a generic agent that must generate a world model using the world model of the perception model. We can use the intelligence measurement equation 7 in a physical theory of intelligence, but it contains terms such as usefulness that have no physical units (i.e., we would prefer a more reductive definition). Therefore, we try to obtain such a measure by using an inverted intelligence theory (an inverted universe)."}, {"heading": "4.2 Application to homeostasis agent", "text": "In a presentation to Friston's group in January 2015, we found that the minimization of S \u00b2 B is identical to the application of Solomonoff's entropy formulation, which merely incorporates the negative logarithm of algorithmic probability. [22] In the unsupervised agent context, the solution to this minimization problem could indicate that an optimal behavioral policy is constituted as internal dynamics that can be modeled as an unending program. [22] We could apply induction directly to the minimization of KL divergence, as could the solution of operator divergence. Theorem 1. Minimizing free energy is equivalent to solving the operator production problem, which can be modeled as a non-terminating program."}, {"heading": "4.3 Discussion", "text": "While we are either optimizing perceptual models or selecting an action that meets expectations, it may be possible to express the optimal policy of adaptive agents in a general optimization framework. A more in-depth analysis of the unattended agent will be presented in a later paper, and a more general, reductive definition of intelligence should be explored, which could ultimately contribute to the unification of AGI theory."}], "references": [{"title": "Can we measure the difficulty of an optimization problem", "author": ["T. Alpcan", "T. Everitt", "M. Hutter"], "venue": "IEEE Information Theory Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Principles of the self-organizing system", "author": ["W.R. Ashby"], "venue": "v. Foerster, H., Zopf, G.W. (eds.) Principles of Self-Organization: Transactions of the University of Illinois Symposium, pp. 255\u2013278. Pergamon, London", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1962}, {"title": "Principles of the self-organizing dynamic system", "author": ["W. Ashby"], "venue": "The Journal of General Psychology 37(2), 125\u2013128", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1947}, {"title": "Artificial General Intelligence: 4th International Conference, AGI", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "Proceedings, chap. Compression and Intelligence: Social Environments and Communication,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Active inference and learning", "author": ["K. Friston", "T. FitzGerald", "F. Rigoli", "P. Schwartenbeck", "J. ODoherty", "G. Pezzulo"], "venue": "Neuroscience and Biobehavioral Reviews", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "A free energy principle for the brain", "author": ["K. Friston", "J. Kilner", "L. Harrison"], "venue": "Journal of Physiology-Paris 100(13),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Reinforcement learning or active inference", "author": ["K.J. Friston", "J. Daunizeau", "S.J. Kiebel"], "venue": "PLOS ONE 4(7),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192down approach", "author": ["M. Hutter"], "venue": "Goertzel, B., Pennachin, C. (eds.) Artificial General Intelligence, pp. 227\u2013290. Cognitive Technologies, Springer, Berlin", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "A free energy principle for biological systems", "author": ["F. Karl"], "venue": "Entropy 14(11),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds Mach. 17(4), 391\u2013444", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "An approximation of the universal intelligence measure", "author": ["S. Legg", "J. Veness"], "venue": "Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, Lecture Notes in Computer Science, vol. 7070, pp. 236\u2013249. Springer Berlin Heidelberg", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Universal problems of full search", "author": ["L. Levin"], "venue": "Problems of Information Transmission 9(3), 256\u2013266", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1973}, {"title": "Some theorems on the algorithmic approach to probability theory and information theory", "author": ["L.A. Levin"], "venue": "CoRR abs/1009.5894", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["M. Li", "P.M. Vitanyi"], "venue": "Springer Publishing Company, Incorporated, 3 edn.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Ultimate physical limits to computation", "author": ["S. Lloyd"], "venue": "Nature406", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Space-time embedded intelligence", "author": ["L. Orseau", "M. Ring"], "venue": "Artificial General Intelligence, Lecture Notes in Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Ultimate Intelligence Part II: Physical Measure and Complexity of Intelligence", "author": ["E. \u00d6zkural"], "venue": "ArXiv e-prints", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Ultimate intelligence part I: physical completeness and objectivity of induction", "author": ["E. \u00d6zkural"], "venue": "Artificial General Intelligence - 8th International Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Delusion, survival, and intelligent agents", "author": ["M. Ring", "L. Orseau"], "venue": "Artificial General Intelligence, pp. 11\u201320. Springer Berlin Heidelberg", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal ordered problem solver", "author": ["J. Schmidhuber"], "venue": "Machine Learning 54, 211\u2013256", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A formal theory of inductive inference, part i", "author": ["R.J. Solomonoff"], "venue": "Information and Control 7(1), 1\u201322", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1964}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "IEEE Trans. on Information Theory IT-24(4), 422\u2013432", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Progress in incremental machine learning", "author": ["R.J. Solomonoff"], "venue": "Tech. Rep. IDSIA-1603, IDSIA, Lugano, Switzerland", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Three kinds of probabilistic induction: Universal distributions and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "The Computer Journal 51(5), 566\u2013570", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "The information bottleneck method", "author": ["N. Tishby", "F.C. Pereira", "W. Bialek"], "venue": "ArXiv Physics e-prints", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimum message length and kolmogorov complexity", "author": ["C.S. Wallace", "D.L. Dowe"], "venue": "The Computer Journal 42(4),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "A information measure for classification", "author": ["C.S. Wallace", "D.M. Boulton"], "venue": "Computer Journal 11(2), 185\u2013194", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1968}], "referenceMentions": [{"referenceID": 14, "context": "The ultimate intelligence research program is inspired by Seth Lloyd\u2019s work on the ultimate physical limits to computation [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "This is the third installation of the paper series, the first two parts proposed new physical complexity measures, priors and limits of inductive inference [18,17].", "startOffset": 156, "endOffset": 163}, {"referenceID": 16, "context": "This is the third installation of the paper series, the first two parts proposed new physical complexity measures, priors and limits of inductive inference [18,17].", "startOffset": 156, "endOffset": 163}, {"referenceID": 17, "context": "We frame the question of ultimate limits of intelligence in a general physical setting, for this we provide a general definition of an intelligent system and a physical performance criterion, which as anticipated turns out to be a relation of physical quantities and information, the latter of which we had conceptually reduced to physics with minimum machine volume complexity in [18].", "startOffset": 381, "endOffset": 385}, {"referenceID": 20, "context": "Let us recall Solomonoff\u2019s universal distribution [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "which conforms to Kolmogorov\u2019s axioms [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "We also give the basic definitions of Algorithmic Information Theory (AIT) [14], where the algorithmic entropy, or complexity of a bit string x \u2208 {0, 1} is HU (x) = min({|\u03c0| | U(\u03c0) = x}) H \u2217 U (x) = \u2212 log2 PU (x) (2) We use some variables in overloaded fashion in the paper, e.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "We can use the found m operators to predict unseen data with a mixture model [24]", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "The goodness of fit in this case strikes a balance between high a priori probability and reproduction of data like in minimum message length (MML) method [27,26], yet uses a universal mixture like in sequence induction.", "startOffset": 154, "endOffset": 161}, {"referenceID": 25, "context": "The goodness of fit in this case strikes a balance between high a priori probability and reproduction of data like in minimum message length (MML) method [27,26], yet uses a universal mixture like in sequence induction.", "startOffset": 154, "endOffset": 161}, {"referenceID": 22, "context": "The convergence theorem for operator induction was proven in [23] using Hutter\u2019s extension to arbitrary alphabet, and it bounds total error by HU (\u03bc) ln 2 similarly to sequence induction.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Hutter has defined an intelligence order relation in the context of his universal reinforcement learning (RL) model AIXI [8], which suggests that intelligence corresponds to the set of problems an agent can solve.", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "Also notable is the universal intelligence measure [10,11], which is again based on the AIXI model.", "startOffset": 51, "endOffset": 58}, {"referenceID": 10, "context": "Also notable is the universal intelligence measure [10,11], which is again based on the AIXI model.", "startOffset": 51, "endOffset": 58}, {"referenceID": 9, "context": "The intelligence measure of [10] is defined as", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "The divergence between the pdf of environment and an arbitrary pdf encoded by its own mechanism is minimized in Friston\u2019s model [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "It has been shown in detail that the free energy principle adequately models a self-preserving agent in a stochastic dynamical system [6,9], which we can interpret as an environment with computable pdf.", "startOffset": 134, "endOffset": 139}, {"referenceID": 8, "context": "It has been shown in detail that the free energy principle adequately models a self-preserving agent in a stochastic dynamical system [6,9], which we can interpret as an environment with computable pdf.", "startOffset": 134, "endOffset": 139}, {"referenceID": 8, "context": "Friston formalizes the self-preservation (homeostasis) problem as finding an internal dynamics that minimizes the uncertainty (Shannon entropy) of the external states, and shows a solution based on the principle of least action [9] wherein minimizing free energy is synonymous with minimizing the entropy of the external states (principle of least action), which subsequently corresponds to active inference.", "startOffset": 228, "endOffset": 231}, {"referenceID": 8, "context": "Minimization of free energy turns out to be equivalent to the information bottleneck principle of Tishby [9,25].", "startOffset": 105, "endOffset": 111}, {"referenceID": 24, "context": "Minimization of free energy turns out to be equivalent to the information bottleneck principle of Tishby [9,25].", "startOffset": 105, "endOffset": 111}, {"referenceID": 2, "context": "The information bottleneck method is equivalent to the pioneering work of Ashby, which is simple enough to state here [3,2]: SB = I(\u03bb;F )\u2212 I(S;\u03bb) (14)", "startOffset": 118, "endOffset": 123}, {"referenceID": 1, "context": "The information bottleneck method is equivalent to the pioneering work of Ashby, which is simple enough to state here [3,2]: SB = I(\u03bb;F )\u2212 I(S;\u03bb) (14)", "startOffset": 118, "endOffset": 123}, {"referenceID": 4, "context": "Please see [5] for a comprehensive application of the free energy principle to agents and learning.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Mechanism is any physical machine as usual, see [4] which suggests likewise.", "startOffset": 48, "endOffset": 51}, {"referenceID": 23, "context": "Therefore, a general formulation of Solomonoff induction, operator induction, might serve as a model of general intelligence, as well [24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "It is argued that in a dynamic environment, as in a physical environment, we must use an active agent model so that we can account for changes in the environment, as in the space-time embedded agent [16] which also provides an agent-based intelligence measure.", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "We counter this objection by stating that we can reduce an agent model to a perception and action-planning problem as in OOPS-RL [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "OOPS has a generalized Levin Search [12] which may be tweaked to solve either prediction or optimization problems.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Hutter has also observed that standard sequence induction does not readily address optimization problems [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 22, "context": "On the other hand, Levin Search with a proper universal probability density function (pdf) of programs can be modified to solve induction problems (sequence, set, operator, and sequence prediction with arbitrary loss), inversion problems (computer science problems in P and NP), and optimization problems [23].", "startOffset": 305, "endOffset": 309}, {"referenceID": 7, "context": "In that sense, AIXI implies yet another variation of Levin Search for solving a particular universal optimization problem, however, it also has the unique advantage that formal transformations between AIXI problem and many important problems including function minimization and strategic games have been shown [8].", "startOffset": 310, "endOffset": 313}, {"referenceID": 22, "context": "Nevertheless, the discussion in [23] is rather brief.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "Also see [1] for a discussion of universal optimization.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "The unsupervised, active inference agent approach is proposed instead of reinforcement learning approach in [7], and the authors argue that they did not need to invoke the notion of reward, value or utility.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "Note that operator induction is considered to be insufficient to describe universal agents such as AIXI, because basic sequence induction is inappropriate for modelling optimization problems [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 19, "context": "However, a modified Levin search procedure can solve such optimization problems as in finding an optimal control program [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 21, "context": "using Solomonoff\u2019s entropy formulation that takes the negative logarithm of algorithmic probability [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Note that this agent is conceptually related to the survival property of RL agents discussed in [19].", "startOffset": 96, "endOffset": 100}], "year": 2017, "abstractText": "We propose that operator induction serves as an adequate model of perception. We explain how to reduce universal agent models to operator induction. We propose a universal measure of operator induction fitness, and show how it can be used in a reinforcement learning model and a homeostasis (self-preserving) agent based on the free energy principle. We show that the action of the homeostasis agent can be explained by the operator induction model. \u201cWir m\u00fcssen wissen \u2013 wir werden wissen!\u201d", "creator": "LaTeX with hyperref package"}}}