{"id": "1709.05746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter: Domain Randomization and Adaptation with Modular Networks", "abstract": "A modular method is proposed to learn and transfer visuo-motor policies from simulation to the real world in an efficient manner by combining domain randomization and adaptation. The feasibility of the approach is demonstrated in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The learned visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm control accuracy.", "histories": [["v1", "Mon, 18 Sep 2017 02:27:02 GMT  (3504kb,D)", "http://arxiv.org/abs/1709.05746v1", "Under review for the IEEE International Conference on Robotics and Automation (ICRA) 2018. arXiv admin note: text overlap witharXiv:1610.06781"]], "COMMENTS": "Under review for the IEEE International Conference on Robotics and Automation (ICRA) 2018. arXiv admin note: text overlap witharXiv:1610.06781", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG cs.SY", "authors": ["fangyi zhang", "j\\\"urgen leitner", "michael milford", "peter corke"], "accepted": false, "id": "1709.05746"}, "pdf": {"name": "1709.05746.pdf", "metadata": {"source": "CRF", "title": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter: Domain Randomization and Adaptation with Modular Networks*", "authors": ["Fangyi Zhang", "J\u00fcrgen Leitner", "Michael Milford", "Peter Corke"], "emails": ["fangyi.zhang@hdr.qut.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTIONThe advent of large data sets and complex machine learning models, commonly referred to as deep learning, has created a trend in recent years away from handmade solutions to more data-driven solutions. Learning techniques have shown significant improvements in robustness and performance [1], especially in computer vision. Traditionally, robotic approaches have been based on handcrafted controllers that combine (heuristic) motion planners with the use of handcrafted functions to visually localize the goal. Recently, learning approaches have been introduced to address this problem [2] - [5], but a consistent problem most approaches face is the reliance on large amounts of data to train these models. For example, Google researchers have addressed this problem by developing an \"arm farm\" with 6 to 14 robots collecting data in parallel [3]. Generalization poses another challenge: many current training systems are different from those used in robotic models, when learned to be botic."}, {"heading": "II. RELATED WORK", "text": "In fact, most people are able to move to another world in which they are able to live, in which they want to live."}, {"heading": "III. METHODOLOGY", "text": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also for visual motor control [8]. In [8], auxiliary tasks such as estimating cube positions in training are used to improve performance in an end-to-end manner. In this paper, we propose a modular approach to using the task of object position estimation for more efficient learning and transfer of visual motor strategies."}, {"heading": "A. Modular Deep Networks", "text": "In order to make the learning and transmission of visual motor guidelines more efficient, we propose to divide a deep neural network into perception and control modules connected by a bottleneck layer (Fig. 2).The bottleneck forces the network to learn a low-dimensional representation, similar to auto-encoders [24].The difference is that we explicitly equate the bottleneck layer with the object position (x).With the bottleneck, the perception module learns how to estimate the object position x * on the basis of a raw pixel image I; the control module learns to determine the most appropriate articulation velocities v taking into account the object position and the articulation angle q (defined as a scene configuration).The values of x * and q are normalized to the interval [0]."}, {"heading": "B. Training Method", "text": "1) Perception: The perception module is trained on the basis of supervised learning - first on the basis of simulated images, then on a small number of real samples for the transfer of skills - with the square loss functionLp = 12m \u00b2 m \u00b2 j = 1 x \u00b2 yp (Ij) \u2212 x \u00b2 j \u00b2 2, (1) where yp (Ij) is the prediction of x \u00b2 j for Ij \u00b2 m. The physical meaning of x \u00b2 guarantees the convenience of collecting labelled training data. 2) Control: The control module is also trained on the basis of simulated data only: Lc = 12m \u00b2 j = 1 \u00b2 yc (sj) \u2212 vj \u00b2 2, (2) where yc (sj) is the prediction of vj for state sj. Here s =.3) Lc \u00b2 fine-tuning with weighted losses: In order to further improve hand-eye coordination, an end-to-to-to-control of the network is performed (weighted)."}, {"heading": "IV. BENCHMARK: ROBOTIC REACHING", "text": "To evaluate the feasibility of the proposed approach, we use a canonical target achievement task as a yardstick. The task is defined as controlling a robot arm so that its end-effect position x in the operating room moves to the position of a target x-x-Rm (object position introduced in Section IIIA). The common configuration of the robot is represented by its articulated angle q-Rn. The two spaces are linked by forward kinematics, i.e. x = K (q). The reach controller adjusts the robot configuration in speed mode to minimize the error between the current and target position of the robot, i.e., away from orientation, i.e., we are looking at a 7 DoF robot arm (i.e. q-R7 directs its end-effect position in 3D, i.e. x-R3 - ignores orientation."}, {"heading": "A. Task Setup", "text": "In the real-world task, the left arm (7 DoF) of a Baxter robot is used to reach a blue cuboid in a confusing jumble. All objects are placed arbitrarily in the operating area (50 x 60 cm), as shown in Fig. 3A. The blue cuboid has a side length of 6.5 cm. The robot observes the environment through a monocular camera in its right hand (Fig. 1A) and delivers RGB images with a resolution of 256 x 256. The left arm is controlled in speed mode. A range is considered successful if the Euclidean distance between the upper center of the target quadrant and the lower center of the suction pad (\"Top Center\" and \"Bottom Center\" in Fig. 3) is less than 4.6 cm (half the diagonal length of any side of the cuboid). In the task, the left arm is randomly initialized to a configuration with a normal distribution around the reference configuration in Fig. 3B."}, {"heading": "B. Network Architecture", "text": "In this thesis, we used a network with the architecture shown in Fig. 2. The perception module has an architecture adapted by VGG16 [22] for lower computing costs, but without losing too much power for this task. It consists of twelve convolutionary layers with 3 x 3 filters and seven maximum 2 x 2 pooling layers, followed by three fully connected layers. Simulated or real RGB images are clipped and scanned to 256 x 256 as inputs to the perception module. The control module consists of three fully connected layers with 400 and 300 units in the two hidden layers. Input into the control module is the scene configuration, whose outputs are the estimates for common velocities v (7 DoF). Networks with a first convolutionary layer initialized with weights from pre-tracted VGG16 [22] are observed to achieve faster and higher accuracy, especially when the training sets are relatively small."}, {"heading": "C. Datasets Collection", "text": "In this thesis, we mark the position of the target cuboid in the upper half of the image as the target position in the lower half of the image. The simulated data was collected using V-REP [26] (a robotic simulation platform) by domain randomization [7] in the following aspects: Number of distractor objects in clutter: random in [0, 9]; Shape of distractor objects in clutter: random in 9primitive shapes with different geometries (5 cubes, 2 cylinders); Pose of distractor objects in clutter: random in [0, 9] Shape of distractor objects in clutter: random in the shape of distractor objects in clutter; \u2022 Color of distractor objects in the RGB values: arbitrary in the common space, 2 cylinders; Pose of distractor objects: random position in the operating area and random orientation of the axis up; \u2022 Color of distractor objects in the RGB values: random WGB values."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "We evaluated the performance of the proposed approach under three aspects: perceptual accuracy, control performance and hand-eye coordination. In the real world, performance was evaluated by the following metrics: \u2022 Perception error: the Euclidean distance between the estimated and ground-truth object position; \u2022 Control error: the Euclidean distance between the target cuboid and the lower center of the end effector (\"upper center\" and \"lower center\" in Fig. 3A); \u2022 Success rate: the percentage of successful achievement among all attempts where a range is considered successful if the final euclidean distance between target and end effector is less than 4.6 cm, as defined in Section IV-A; \u2022 Time cost: the number of control steps used in an entire achievement process."}, {"heading": "A. Perception Accuracy", "text": "To investigate the influence of the numbers of simulated and real images on perception, we evaluated 15 different perception modules. They were trained with different image combinations: \u2022 the number of simulated images from 0 to 3000; \u2022 the number of real images from 0 to 279. As in Section III-B.1, all 15 perception modules were trained with real images, which were then completely new except that the first revolutionary layer with weights of upstream VGG16 was initiated. In the training, we used a minibatch size of 32 and a learning rate of 0.01. RmsProp was accepted, which was included in the training of control modules."}, {"heading": "B. Control Performance", "text": "To investigate how many trajectories are sufficient to form a control module, we evaluated three control modules with different control data sets that have different trajectories: 118, 333 and 2964. As introduced in Section IV-C, the trajectories in each data set in the simulation were collected (thus cheaply) for targets that are evenly distributed in the operating area. In the training, we used a mini-batch size of 64 and a decreasing learning rate of 0.01 to 0.01. Metrics of control errors and success rates were used in the evaluation, and their performance in the real world is shown in the figure."}, {"heading": "C. Hand-eye Coordination", "text": "To evaluate the feasibility of the approach, we compare three combined networks: \u2022 EE0: consisting of PP and CC, directly connected after separate training without end-to-end fine-tuning; \u2022 EE1: EE0 end-to-end fine-tuning; \u2022 EE0 end-to-end fine-tuning naive, using only the control loss Lc; \u2022 EE2: EE0 fine-tuning with the proposed approach without weighted fine-tuning. In the weighted end-to-end fine-tuning for EE2, \u03b2 = 0.9 (empirically determined), we used a learning rate of 0.01 and a mini-batch size of 8 and 64 for control and perception losses. 30225 image velocity pairs in the control data set (333 trajectories) for CC were used in the fine-tuning to obtain a better image position."}, {"heading": "VI. DISCUSSION", "text": "This work leads us to the following observations: a) Value of a modular structure and an end-to-end fine-tuning: The significant performance improvement between EE2 and EE0 after an end-to-end fine-tuning with weighted losses shows the feasibility of the modular approach. However, we benefit from the modular structure as well as domain randomization and customization, visuomotor strategies for a table-top task can be learned and simulated from simulation into the real world with only 33225 (including the 30225 ones for end-to-end fine-tuning) and 186 real samples that achieve a comparable performance [8] (achieving the level of multi-stage task) but with less training data. The modular approach can be used in a more general way, although we have explicitly equated the bottlenecks with the target position in this work."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we proposed a modular approach to learning and transferring visual-motor strategies from simulation to the real world through domain randomization and adaptation, demonstrating feasibility in the task of achieving a desktop object in chaos with a 7 DoF robotic arm in speed mode. By using weighted losses to fine-tune a combined network in an end-to-end manner, its performance was significantly improved (27%) and achieved a success rate of 93.3% with an average control error of 2.2 cm. The learned strategies are robust against novel distracting objects in chaos and even a moving target. The modular approach promises a more efficient transfer of visual-motor strategies from simulation to the real world."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "Australasian Conference on Robotics and Automation (ACRA), December 2015. [Online]. Available: https://arxiv.org/abs/1511.03791", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P.P. Sampedro", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER), 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual servoing from deep neural networks", "author": ["Q. Bateux", "E. Marchand", "J. Leitner", "F. Chaumette", "P. Corke"], "venue": "New Frontiers for Deep Learning in Robotics Workshop at Robotics: Science and Systems Conference (RSS), 2017. [Online]. Available: https: //arxiv.org/abs/1705.08940", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Leveraging deep reinforcement learning for reaching robotic tasks", "author": ["K. Katyal", "I.-J. Wang", "P. Burli"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Bridging between computer and robot vision through data augmentation: a case study on object recognition", "author": ["A. D\u2019Innocente", "F.M. Carlucci", "M. Colosi", "B. Caputo"], "venue": "International Conference on Computer Vision Systems (ICVS), 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task", "author": ["S. James", "A.J. Davison", "E. Johns"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "A similarity-based approach to skill transfer", "author": ["T. Fitzgerald", "A. Goel", "A. Thomaz"], "venue": "Women in Robotics Workshop at Robotics: Science and Systems Conference (RSS), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR), 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep-network solution towards modelless obstacle avoidance", "author": ["L. Tai", "S. Li", "M. Liu"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A robustness analysis of deep q networks", "author": ["A.W. Tow", "S. Shirazi", "J. Leitner", "N. S\u00fcnderhauf", "M. Milford", "B. Upcroft"], "venue": "Australasian Conference on Robotics and Automation (ACRA), 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1\u201340, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a visuomotor controller for real world robotic grasping using simulated depth images", "author": ["U. Viereck", "A. t. Pas", "K. Saenko", "R. Platt"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1633\u20131685, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Modular deep q networks for sim-to-real transfer of visuo-motor policies", "author": ["F. Zhang", "J. Leitner", "M. Milford", "P. Corke"], "venue": "Queensland University of Technology, Tech. Rep., 2017. [Online]. Available: https://arxiv.org/abs/1610.06781", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "science, vol. 313, no. 5786, pp. 504\u2013 507, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "A theoretical framework for back-propagation", "author": ["Y. LeCun"], "venue": "Proceedings of the 1988 Connectionist Models Summer School, D. Touretzky, G. Hinton, and T. Sejnowski, Eds. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988, pp. 21\u201328.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1988}, {"title": "V-rep: A versatile and scalable robot simulation framework", "author": ["E. Rohmer", "S.P. Singh", "M. Freese"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1321\u20131326.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Tuning modular networks with weighted losses for hand-eye coordination", "author": ["F. Zhang", "J. Leitner", "M. Milford", "P.I. Corke"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017. [Online]. Available: https://arxiv.org/abs/1705.05116", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Learning techniques have shown significant improvements in robustness and performance [1], particularly in the computer vision field.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Recently learning approaches to tackle this problem have been presented [2]\u2013[5], however a consistent issue faced by most approaches is the reliance on large amounts of data to train these models.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Recently learning approaches to tackle this problem have been presented [2]\u2013[5], however a consistent issue faced by most approaches is the reliance on large amounts of data to train these models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "For example, Google researchers addressed this problem by developing an \"arm farm\" with 6 to 14 robots collecting data in parallel [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 227, "endOffset": 231}, {"referenceID": 2, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 310, "endOffset": 313}, {"referenceID": 10, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 315, "endOffset": 319}, {"referenceID": 11, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 127, "endOffset": 131}, {"referenceID": 2, "context": "required for deep learning has been sped up by using many robots operating in parallel [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Another example of dataset collection for grasping is the approach to self-supervised grasp learning in the real world where force sensors were used to autonomously label samples [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "DeepMind showed that a deep reinforcement learning system is able to synthesize control actions for computer games directly from vision data [14].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "While this result is an important and exciting breakthrough it does not transfer directly to real robots with real cameras observing real scenes [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "Introducing a real camera observing the game screen was even worse [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "introduced a CNN-based policy representation architecture with an added guided policy search (GPS) to learn visuo-motor policies (from joint angles and camera images to joint torques) [16], which allows reduction in the number of real world training examples by providing an oracle (or expert\u2019s initial condition to start learning).", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "Recently it has been proposed to simulated depth images to learn and then transfer grasping skills to real-world robotic arms [17], yet no adaptation in the real-world has been performed.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "knowledge between different tasks (scenarios) [18], [19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "knowledge between different tasks (scenarios) [18], [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Progressive neural networks are leveraged to improve transfer and avoid catastrophic forgetting when learning complex sequences of tasks [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "Modular reinforcement learning approaches have shown skill transfer capabilities in simulation [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "similaritybased approach to skill transfer for robots [9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "2% in a \u201chook loop\u201d task, with 10 times less real-world images [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Domain randomization has also been successfully used to transfer deep neural networks from simulation to the real world [7], [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Domain randomization has also been successfully used to transfer deep neural networks from simulation to the real world [7], [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also visuomotor control [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also visuomotor control [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "In [8], auxiliary tasks such as cube pose estimation are used in training to improve performance in an end-to-end manner.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": ", extracting useful information from visual inputs, while the fully connected (FC) layers perform control [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "The bottleneck forces the network to learn a low-dimensional representation, not unlike Auto-encoders [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "The values of x\u2217 and q are normalized to the interval [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 21, "context": "The perception module architecture is customized from VGG16 [22] with its first convolutional layer initialized with weights from pre-trained VGG16.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "where L c is a pseudo-loss which reflects the loss of Lc in the bottleneck; \u03b2 \u2208 [0, 1] is a balancing weight.", "startOffset": 80, "endOffset": 86}, {"referenceID": 24, "context": "From the backpropagation algorithm [25], we can infer that \u03b4L = \u03b2\u03b4Lp + (1 \u2212 \u03b2)\u03b4LBN c , where \u03b4L is the gradients resulting from L; \u03b4Lp and \u03b4LBN c are the gradients resulting respectively from Lp and L c (equivalent to that resulting from Lc in the perception module).", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "The perception module has an architecture customized from VGG16 [22] for lower computational cost but without losing too much performance for this task.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Networks with a first convolutional layer initialized with weights from pretrained VGG16 [22] are observed to converge faster and", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Simulated images were collected from a V-REP simulator using domain randomization [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 25, "context": "The simulated data was collected using V-REP [26] (a robotic simulation platform) through domain randomization [7] in the following aspects:", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "The simulated data was collected using V-REP [26] (a robotic simulation platform) through domain randomization [7] in the following aspects:", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "\u2022 number of distractor objects in clutter: random in [0, 9]; \u2022 shape of distractor objects in clutter: random in 9 primitive shapes with different geometries (5 cuboids, 2 spheres, 2 cylinders); \u2022 pose of distractor objects: random position in the operation area and random orientation of the axis pointing upwards; \u2022 color of distractor objects: random RGB values; \u2022 left arm configuration: random in joint space, excluding the ones with self-collision; \u2022 color of the table, floor, robot body and target cuboid: random changes based on reference colors (\u00b110%); \u2022 camera pose: random changes of each right arm joint angle based on reference angles (\u00b11%);", "startOffset": 53, "endOffset": 59}, {"referenceID": 21, "context": "The training was from scratch, except that the first convolutional layer was initialized with weights from pre-trained VGG16 [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "RmsProp [27] was adopted, the same in the training of control modules (Section V-B) and end-to-end fine-tuning (Section V-C).", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "to the real world with just 33225 simulated (including the 30225 ones for end-to-end fine-tuning) and 186 real samples, achieving a comparable performance to [8] (the reaching stage of the multi-stage task) but with fewer training data.", "startOffset": 158, "endOffset": 161}, {"referenceID": 22, "context": "The feasibility of the modular approach for reinforcement learning (DQN) has been validated in a planar reaching task [23], [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "The feasibility of the modular approach for reinforcement learning (DQN) has been validated in a planar reaching task [23], [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "0 cm), much higher than expected from [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Nevertheless, the adaptation with just a few real images (as few as 93) is able to transfer a network from simulation to the real world, and needs fewer simulated images to reach a comparable accuracy to [7].", "startOffset": 204, "endOffset": 207}], "year": 2017, "abstractText": "A modular method is proposed to learn and transfer visuo-motor policies from simulation to the real world in an efficient manner by combining domain randomization and adaptation. The feasibility of the approach is demonstrated in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The learned visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm control accuracy.", "creator": "LaTeX with hyperref package"}}}