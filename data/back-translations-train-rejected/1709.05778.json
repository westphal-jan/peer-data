{"id": "1709.05778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model for Short Text Multi-class Classification Problems", "abstract": "The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions. However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence. In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems. Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-of-words enrichment applies only to text being classified.", "histories": [["v1", "Mon, 18 Sep 2017 05:00:34 GMT  (32kb,D)", "http://arxiv.org/abs/1709.05778v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bradford heap", "michael bain", "wayne wobcke", "alfred krzywicki", "susanne schmeidl"], "accepted": false, "id": "1709.05778"}, "pdf": {"name": "1709.05778.pdf", "metadata": {"source": "META", "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model for Short Text Multi-class Classification Problems", "authors": ["Bradford Heap", "Michael Bain", "Wayne Wobcke", "Alfred Krzywicki", "Susanne Schmeidl"], "emails": ["b.heap@unsw.edu.au", "m.bain@unsw.edu.au", "w.wobcke@unsw.edu.au", "alfredk@unsw.edu.au", "s.schmeidl@unsw.edu.au"], "sections": [{"heading": "Introduction", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to flourish, and where most of them are able to flourish in order to flourish."}, {"heading": "Limitations of the Bag-of-Words Model", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they are able to"}, {"heading": "Bag-of-Words Enrichment with Word Vectors", "text": "In this section, we describe our approach to enriching the dictionary bag vector by using information from contextual words found in an unattended word vector model, which allows the dictionary bag model to contain more non-zero elements that provide linear classifiers with more information for making predictions. This process of enriching the dictionary bag model involves a number of steps that are shown in Fig. 1 and described below: 1. Train or obtain an unattended word vector model W with dimensions d that model the vocabulary and language of the sentences in the labeled or unlabeled data and / or other domain-related texts. The word vector model must, for a given character t in the vocabulary, output a list of tokens related to related to related, ordered in order of similarity between the words and the text in the labeled or unlabeled text."}, {"heading": "1. Word Vector Training Algorithms", "text": "In this work, we train Word vector models using Mikolov et al.'s (2013a) Continuous Skip-gram model, since it fulfills our requirement that the Word vector model capture both syntactically related and jointly occurring terms. This training algorithm differs from previous Word embedding models, instead of using the previous and next n words to predict a missing word, only one word is used as input, and the task is to predict the words that occur in sentences around the given input word. This approach captures the relationships between the two simultaneously occurring words and syntactically related words. For example, the synonymous terms \"Daesh\" and \"ISIS\" will both produce related words of \"fighters,\" \"terror\" and \"loyalists,\" as they will appear in the Skip-gram training model in a very similar area of vector space. Furthermore, the term \"jointly emerging Trump\" will produce a common domain specific word in a common model."}, {"heading": "2. Constructing Nearest Neighbour Bag-of-Words Models using Word Vectors", "text": "Once trained, the distance between the individual tokens in a Word vector model is agnostic for the distribution of word frequencies. This means that two adjacent terms in the Word vector model have a similarity to the words they cooperated with during the construction of the model, regardless of the number of training examples for each term. This can result in rare terms having high-frequency terms as close neighbors. This close association between related terms allows us to find more common terms for a given rare term. By capturing these more common related terms, we can use their information in the model to help classify a short text. The construction of a next adjacent word bag model requires two hyperparameters: the rare word frequency threshold n and the number of adjacent words that are to be included from the Word vector model. Appropriate values for these parameters must be derived from Word evaluation values, e.g. Word Vector values can be derived from both of the training relationships (both)."}, {"heading": "3. Aggregation of the Nearest Neighbour", "text": "In a final step before classifying the short text, we combine the original sack-of-words vector with the sack-of-word vector constructed for each of the rare words in the original short text. This aggregation of the sack-of-words vectors has three advantages: 1) increasing the term frequency of each word that occurs in the original sack-of-words and then becomes the closest neighbor of a rare word; 2) increasing the non-zero elements in the enriched sack-of-word vector by including adjacent words; 3) using vocabulary words (which are not included in the labeled text) can contribute to classification by their closest neighbors found in the word-vector model (when the word vector is enriched on a larger corpus). This approach addresses the classic sack-of-word model of the batch-vector-vocabulary problems of: and selective words."}, {"heading": "Short Text Classification Models", "text": "In this paper, we focus on two widely used classification models, Multinomial Naive Bayes (MNB) and Support Vector Machines (SVM), which have been widely established as basic methods for text classification in many areas (Wang and Manning 2012). 1Haibatullah is the current leader of the Taliban (since May 2016) and Mullah Omar is a former ringleader. Multinomial Naive Bayes. These classifiers apply the Bayes rule to a collection of designated training examples to estimate the probability of a given text belonging to a particular class.Formally, the classifier is defined as: CNB = argmax c-C P (c). ti-T-P (ti | c), where P (c), the prior probability of belonging to a particular class, is applicable to a particular class."}, {"heading": "Neural Network Classifiers", "text": "We compare the linear classifiers with two recently developed neural network models for text classification, which use data from a prefabricated Word Vector as input into another neural network model.Paragraph Vectors. Paragraph Vectors (also called doc2vec) are an extension of Mikolov et al.'s (2013a) word2vec word embedding training algorithm. The Paragraph Vectors model is designed to overcome the problems of loss of word order information in dictionary models (Le and Mikolov 2014).The primary change is during training markers (\"Paragraph IDs\") are inserted into the same dense vector space as the words. After training, the classification of text is performed by calculating the mean vector of the words in a text to be classified, and then the text labeled with the text lettering, the Neural Vector on the Vector Classification is the closest vector to the nature ID that the recorder vector has demonstrated."}, {"heading": "Evaluation", "text": "In fact, it is that most people are able to determine for themselves what they want and what they want. (...) It is not that people are able to decide what they want. (...) It is not that they want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is as if they do not want it. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (It is. (...). (). (It is. (...). (). (It is.). (It is. (). (). (It is. (). (It is.). (It is. (...). (). (It is. (). (It is. (). (). (It is. (). (It is.). (It is. (). (It is. (). (It is.). (It is. (). (It is.). (). (It is. (). (It is.). (It is. (). (It is."}, {"heading": "Related Work", "text": "A number of approaches to transforming the bag-of-words model or modifying linear models to increase classification accuracy have been proposed in previous work, generally including: 1) reducing the dimensionality of models by removing stopwords (Mansuy and Hilderman 2006), containing terms (Porter 1980) and removing the rarest words (Yang and Pedersen 1997); 2) transforming word frequencies by TF-IDF and other measures to identify key terms in certain classes (Joachims 1997); 3) limiting classes to maximum word size by methods such as mutual information measurements (Dumais et al. 1998); 4) using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopedias (Strube and Ponzetto 2006)."}, {"heading": "Conclusion", "text": "In this paper, we developed a method for enriching word bag representations for short text classification. We use a word vector model that is trained on unmarked domain-related text to capture semantic and syntactic relationships between words. Using the word vector model, we locate words in vector space that rarely occur in a set of marked training examples, and then use the adjacent words to enrich the word bag vector to be classified. This enriched representation allows linear classifiers to use the class information of more words to improve classification forecasting. A major advantage of the approach is that it does not require any change in the formation of linear classification models. Combining the information in the unattended word vector model with a supervised linear model improves micro- and macro-memory across the baseline for difficult multi-class problems, although the potential for CNs should be further investigated."}, {"heading": "Acknowledgment", "text": "This work was supported by the Data to Decisions Cooperative Research Centre. We thank Josie Gardner for identifying the ICG DRC dataset."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, 160\u2013167.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Disaggregating Conflict by Actors, Time, and Location", "author": ["K. Donnay", "E. Gadjanova", "R. Bhavnani"], "venue": "Backer, D. amd Huth, P., and Wilkenfeld, J., eds., Peace and Conflict 2014. Boulder, CO: Paradigm. 44 \u2013 56.", "citeRegEx": "Donnay et al\\.,? 2014", "shortCiteRegEx": "Donnay et al\\.", "year": 2014}, {"title": "Inductive Learning Algorithms and Representations for Text Categorization", "author": ["S.T. Dumais", "J.C. Platt", "D. Hecherman", "M. Sahami"], "venue": "International Conference on Information and Knowledge Management, 148\u2013155.", "citeRegEx": "Dumais et al\\.,? 1998", "shortCiteRegEx": "Dumais et al\\.", "year": 1998}, {"title": "Overcoming the Brittleness Bottleneck using Wikipedia: Enhancing Text Categorization with Encyclopedic Knowledge", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "National Conference on Artificial Intelligence, 1301\u20131306.", "citeRegEx": "Gabrilovich and Markovitch,? 2006", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2006}, {"title": "Classification by Pairwise Coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics 26(2):457\u2013471.", "citeRegEx": "Hastie and Tibshirani,? 1998", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1998}, {"title": "A Joint Human/Machine Process for Coding Events and Conflict Drivers", "author": ["B. Heap", "A. Krzywicki", "S. Schmeidl", "W. Wobcke", "M. Bain"], "venue": "International Conference on Advanced Data Mining and Applications (in press).", "citeRegEx": "Heap et al\\.,? 2017", "shortCiteRegEx": "Heap et al\\.", "year": 2017}, {"title": "OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research", "author": ["W.R. Hersh", "C. Buckley", "T.J. Leone", "D.H. Hickam"], "venue": "SIGIR, 192\u2013 201.", "citeRegEx": "Hersh et al\\.,? 1994", "shortCiteRegEx": "Hersh et al\\.", "year": 1994}, {"title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization", "author": ["T. Joachims"], "venue": "International Conference on Machine Learning, 143\u2013151.", "citeRegEx": "Joachims,? 1997", "shortCiteRegEx": "Joachims", "year": 1997}, {"title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features", "author": ["T. Joachims"], "venue": "ECML-98, 137\u2013142.", "citeRegEx": "Joachims,? 1998", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Y. Kim"], "venue": "EMNLP, 1746\u20131751.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Combining Classifiers in Text Categorization", "author": ["L.S. Larkey", "W.B. Croft"], "venue": "SIGIR, 289\u2013297.", "citeRegEx": "Larkey and Croft,? 1996", "shortCiteRegEx": "Larkey and Croft", "year": 1996}, {"title": "An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation", "author": ["J.H. Lau", "T. Baldwin"], "venue": "Workshop on Representation Learning for NLP, 78\u201386.", "citeRegEx": "Lau and Baldwin,? 2016", "shortCiteRegEx": "Lau and Baldwin", "year": 2016}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "International Conference on Machine Learning, volume 32, 1188\u20131196.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "A semantic term weighting scheme for text categorization", "author": ["Q. Luo", "E. Chen", "H. Xiong"], "venue": "Expert Systems with Applications 38(10):12708\u201312716.", "citeRegEx": "Luo et al\\.,? 2011", "shortCiteRegEx": "Luo et al\\.", "year": 2011}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142\u2013 150.", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "The Use of WordNet in Information Retrieval", "author": ["R. Mandala", "T. Takenobu", "T. Hozumi"], "venue": "Workshop on Usage of WordNet in Natural Language Processing Systems, 31\u2013", "citeRegEx": "Mandala et al\\.,? 1998", "shortCiteRegEx": "Mandala et al\\.", "year": 1998}, {"title": "A Characterization of WordNet Features in Boolean Models For Text Classification", "author": ["T.N. Mansuy", "R.J. Hilderman"], "venue": "Australasian Data Mining Conference, 103\u2013 109.", "citeRegEx": "Mansuy and Hilderman,? 2006", "shortCiteRegEx": "Mansuy and Hilderman", "year": 2006}, {"title": "Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification", "author": ["D. Mavroeidis", "G. Tsatsaronis", "M. Vazirgiannis", "M. Theobald", "G. Weikum"], "venue": "PKDD, 181\u2013192.", "citeRegEx": "Mavroeidis et al\\.,? 2005", "shortCiteRegEx": "Mavroeidis et al\\.", "year": 2005}, {"title": "A Comparison of Event Models for Naive Bayes Text Classification", "author": ["A. McCallum", "K. Nigam"], "venue": "AAAI-98 Workshop on Learning for Text Categorization, volume 752, 41\u201348.", "citeRegEx": "McCallum and Nigam,? 1998", "shortCiteRegEx": "McCallum and Nigam", "year": 1998}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, volume 26, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program 14(3):130\u2013137.", "citeRegEx": "Porter,? 1980", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Tackling the Poor Assumptions of Naive Bayes Text Classifiers", "author": ["J.D. Rennie", "L. Shih", "J. Teevan", "D.R. Karger"], "venue": "International Conference on International Conference on Machine Learning, 616\u2013623.", "citeRegEx": "Rennie et al\\.,? 2003", "shortCiteRegEx": "Rennie et al\\.", "year": 2003}, {"title": "Text Classification Using WordNet Hypernyms", "author": ["S. Scott", "S. Matwin"], "venue": "Workshop on Usage of WordNet in Natural Language Processing Systems, 45\u201352.", "citeRegEx": "Scott and Matwin,? 1998", "shortCiteRegEx": "Scott and Matwin", "year": 1998}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys 34(1):1\u201347.", "citeRegEx": "Sebastiani,? 2002", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "WikiRelate! Computing Semantic Relatedness Using Wikipedia", "author": ["M. Strube", "S.P. Ponzetto"], "venue": "National Conference on Artificial intelligence, 1419\u20131424.", "citeRegEx": "Strube and Ponzetto,? 2006", "shortCiteRegEx": "Strube and Ponzetto", "year": 2006}, {"title": "Building Semantic Kernels for Text Classification using Wikipedia", "author": ["P. Wang", "C. Domeniconi"], "venue": "ACM SIGKDD, 713\u2013721.", "citeRegEx": "Wang and Domeniconi,? 2008", "shortCiteRegEx": "Wang and Domeniconi", "year": 2008}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, 90\u201394.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "A Comparative Study on Feature Selection in Text Categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "International Conference on Machine Learning, 412\u2013420.", "citeRegEx": "Yang and Pedersen,? 1997", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}, {"title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification", "author": ["Y. Zhang", "B.C. Wallace"], "venue": "CoRR abs/1510.03820.", "citeRegEx": "Zhang and Wallace,? 2015", "shortCiteRegEx": "Zhang and Wallace", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes (McCallum and Nigam 1998) and Support Vector Machines (Joachims 1998)).", "startOffset": 147, "endOffset": 172}, {"referenceID": 9, "context": "The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes (McCallum and Nigam 1998) and Support Vector Machines (Joachims 1998)).", "startOffset": 201, "endOffset": 216}, {"referenceID": 25, "context": "Linear classifiers have been widely studied for many text classification problems (Sebastiani 2002), such as document classification or sentiment analysis, due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions (Dumais et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 3, "context": "Linear classifiers have been widely studied for many text classification problems (Sebastiani 2002), such as document classification or sentiment analysis, due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions (Dumais et al. 1998).", "startOffset": 349, "endOffset": 369}, {"referenceID": 28, "context": ", fragments of text, single sentences or document titles) the bag-of-words representation contains extremely sparse data which reduces the accuracy of the linear classification models (Wang and Manning 2012).", "startOffset": 184, "endOffset": 207}, {"referenceID": 8, "context": "As there is limited data in the smaller classes, many supervised text classification methods are less accurate for these classes (Joachims 1997; Rennie et al. 2003).", "startOffset": 129, "endOffset": 164}, {"referenceID": 23, "context": "As there is limited data in the smaller classes, many supervised text classification methods are less accurate for these classes (Joachims 1997; Rennie et al. 2003).", "startOffset": 129, "endOffset": 164}, {"referenceID": 4, "context": "In addition, due to the specialized technical vocabulary used in the medical and social science fields, such classification problems are influenced by expert background knowledge not directly expressed in the text itself (Gabrilovich and Markovitch 2006).", "startOffset": 221, "endOffset": 254}, {"referenceID": 0, "context": "Word Vector models build on the ideas of capturing word co-occurrence information and assume that contextually related words will often occur with a similar set of surrounding words (Bengio et al. 2003).", "startOffset": 182, "endOffset": 202}, {"referenceID": 1, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often have semantic or syntatic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 133, "endOffset": 182}, {"referenceID": 20, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often have semantic or syntatic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 133, "endOffset": 182}, {"referenceID": 7, "context": "The second evaluation is performed on a subset of the benchmark OHSUMED dataset (Hersh et al. 1994) in which we consider the classification of medical articles which contain a title but no abstract.", "startOffset": 80, "endOffset": 99}, {"referenceID": 24, "context": "The bag-of-words vector MT contains the term frequency of each token from V that occurs in T , where the value of the vector is 0 for every token in V that does not occur in T (Scott and Matwin 1998; Wang and Domeniconi 2008).", "startOffset": 176, "endOffset": 225}, {"referenceID": 27, "context": "The bag-of-words vector MT contains the term frequency of each token from V that occurs in T , where the value of the vector is 0 for every token in V that does not occur in T (Scott and Matwin 1998; Wang and Domeniconi 2008).", "startOffset": 176, "endOffset": 225}, {"referenceID": 24, "context": "However, in problems with a large overlap between the terms used to describe two or more classes, accuracy can substantially decrease (Scott and Matwin 1998).", "startOffset": 134, "endOffset": 157}, {"referenceID": 0, "context": "Word Vector models are an extension of the ideas of word co-occurrences and build on the idea that contextually related words often occur with a similar set of surrounding words (Bengio et al. 2003).", "startOffset": 178, "endOffset": 198}, {"referenceID": 1, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often share semantic or syntactic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 135, "endOffset": 184}, {"referenceID": 20, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often share semantic or syntactic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 135, "endOffset": 184}, {"referenceID": 0, "context": "Word Vector models are constructed using neural networks (Bengio et al. 2003; Collobert and Weston 2008).", "startOffset": 57, "endOffset": 104}, {"referenceID": 1, "context": "Word Vector models are constructed using neural networks (Bengio et al. 2003; Collobert and Weston 2008).", "startOffset": 57, "endOffset": 104}, {"referenceID": 1, "context": ", Wikipedia (Collobert and Weston 2008) or Google News (Mikolov et al.", "startOffset": 12, "endOffset": 39}, {"referenceID": 20, "context": ", Wikipedia (Collobert and Weston 2008) or Google News (Mikolov et al. 2013a)).", "startOffset": 55, "endOffset": 77}, {"referenceID": 0, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 13, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 0, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014). Word similarity is achieved in the intermediate layers of the neural network as a by-product. Combining Word Vector models with other machine learning approaches has shown promising results. In particular, Maas et al. (2011) used Word Vector models to discover additional features for sentiment analysis using SVM.", "startOffset": 152, "endOffset": 419}, {"referenceID": 20, "context": "The technical details and empirical evaluations of this approach have been presented in previous works (Mikolov et al. 2013a; 2013b).", "startOffset": 103, "endOffset": 132}, {"referenceID": 20, "context": "In this work, we train Word Vector models using Mikolov et al.\u2019s (2013a) Continuous Skip-gram Model as it addresses our requirement that the Word Vector Model captures both syntactically related and common co-occurring terms.", "startOffset": 48, "endOffset": 73}, {"referenceID": 28, "context": "These two models have been widely established as baselines methods for text classification across many domains (Wang and Manning 2012).", "startOffset": 111, "endOffset": 134}, {"referenceID": 5, "context": "A text is then classified by every classification pair and the class which the text is classified into most often (Hastie and Tibshirani 1998).", "startOffset": 114, "endOffset": 142}, {"referenceID": 13, "context": "The Paragraph Vectors model is designed to overcome the problems of the loss of word ordering information in bag-of-words models (Le and Mikolov 2014).", "startOffset": 129, "endOffset": 150}, {"referenceID": 12, "context": "Recent analysis has shown that replicating the results of the original Paragraph Vectors paper has been difficult due to the nature of tuning the model\u2019s hyperparameters (Lau and Baldwin 2016).", "startOffset": 170, "endOffset": 192}, {"referenceID": 18, "context": "Paragraph Vectors (also called doc2vec) are an extension of Mikolov et al.\u2019s (2013a) word2vec word embedding training algorithm.", "startOffset": 60, "endOffset": 85}, {"referenceID": 10, "context": "Kim (2014) uses pretrained Word Vector models as input into a one layer Convolutional Neural Network (CNN).", "startOffset": 0, "endOffset": 11}, {"referenceID": 30, "context": "However, subsequent work has shown that tuning the large number of hyperparameters is difficult and often impractical due to the computational cost (Zhang and Wallace 2015).", "startOffset": 148, "endOffset": 172}, {"referenceID": 7, "context": "OHSUMED Dataset Our second evaluation considered the evaluation of medical academic article references contained in the OHSUMED dataset (Hersh et al. 1994).", "startOffset": 136, "endOffset": 155}, {"referenceID": 9, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 4, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 27, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 4, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008). We consider a subset of this dataset that contains references which only contain a title and no abstract. This particular problem setting has previously been considered by Gabrilovich and Markovitch (2006) who used Wikipedia to enrich an SVM classifier.", "startOffset": 75, "endOffset": 341}, {"referenceID": 6, "context": "Previous work has shown this dataset is very challenging for state of the art classification algorithms (Heap et al. 2017).", "startOffset": 104, "endOffset": 122}, {"referenceID": 6, "context": "This is reflective of the difficultly in using this dataset for fully automated machine coding (Heap et al. 2017).", "startOffset": 95, "endOffset": 113}, {"referenceID": 11, "context": "(Larkey and Croft 1996)).", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 22, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 144, "endOffset": 157}, {"referenceID": 29, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 196, "endOffset": 220}, {"referenceID": 8, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 332, "endOffset": 347}, {"referenceID": 3, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al. 1998); 4) Using dictionaries (Mavroeidis et al.", "startOffset": 449, "endOffset": 469}, {"referenceID": 18, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 17, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 24, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 26, "context": "2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 75, "endOffset": 127}, {"referenceID": 27, "context": "2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 75, "endOffset": 127}, {"referenceID": 17, "context": "However, the names of people and organisations are unlikely to occur in dictionaries (Luo, Chen, and Xiong 2011; Mansuy and Hilderman 2006) and may be used rarely in training text.", "startOffset": 85, "endOffset": 139}, {"referenceID": 3, "context": "These text transformation techniques can have a large influence over the accuracy of the classification models (Dumais et al. 1998; Yang and Pedersen 1997).", "startOffset": 111, "endOffset": 155}, {"referenceID": 29, "context": "These text transformation techniques can have a large influence over the accuracy of the classification models (Dumais et al. 1998; Yang and Pedersen 1997).", "startOffset": 111, "endOffset": 155}], "year": 2017, "abstractText": "The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions. However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence. In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems. Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-ofwords enrichment applies only to text being classified.", "creator": "TeX"}}}