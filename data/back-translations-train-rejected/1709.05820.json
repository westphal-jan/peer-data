{"id": "1709.05820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Toward a full-scale neural machine translation in production: the Booking.com use case", "abstract": "While some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are op- timization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results.", "histories": [["v1", "Mon, 18 Sep 2017 08:46:20 GMT  (55kb)", "http://arxiv.org/abs/1709.05820v1", "11 pages, 4 figures, presented at the Commercial MT Users and Translators Track"], ["v2", "Mon, 25 Sep 2017 10:26:44 GMT  (55kb)", "http://arxiv.org/abs/1709.05820v2", "11 pages, 4 figures, presented at MT Summit XVI, Commercial MT Users and Translators Track"]], "COMMENTS": "11 pages, 4 figures, presented at the Commercial MT Users and Translators Track", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pavel levin", "nishikant dhanuka", "talaat khalil", "fedor kovalev", "maxim khalilov"], "accepted": false, "id": "1709.05820"}, "pdf": {"name": "1709.05820.pdf", "metadata": {"source": "CRF", "title": "Toward a full-scale neural machine translation in production: the Booking.com use case", "authors": ["Pavel Levin", "Nishikant Dhanuka", "Fedor Kovalev", "Maxim Khalilov"], "emails": ["pavel.levin@booking.com", "nishikant.dhanuka@booking.com", "talaat.khalil@booking.com", "fedor.kovalev@booking.com", "maxim.khalilov@booking.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.05 820v 1 [cs.C L] 1Although some notable advances have been made in Neural Machine Translation (NMT), there have not been many reports of its development and evaluation in practice. This paper attempts to fill this gap by presenting some of our insights from building an NMT system for the internal travel sector in a large-scale e-commerce environment. The three main topics we cover are optimization and training (including various optimization strategies and body sizes), dealing with real-world content, and evaluating the results."}, {"heading": "1 Introduction", "text": "Booking.com is one of the largest online companies in the world, operating in 43 different languages, connecting millions of daily visitors to 1.4 million bookable accommodations, while offering both parties multilingual support and information at every step of the way. As the company grows rapidly and the need for higher quality translated content grows, machine translation (MT) is becoming an increasingly attractive option to automate this difficult task. Our experiments [9] consistently demonstrate the superiority of neural machine translation systems over more traditional statistical systems, even when pitted against well-established and tested general-purpose systems. Therefore, our recent focus is on adapting and improving our own in-house NMU systems to make them practical and effective for us. This work highlights some of the key insights on our journey and should be of interest to anyone trying to use a custom NMU system."}, {"heading": "2 Optimization and training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Model architecture", "text": "The core of our translation pipeline is based on OpenNMT [7], a framework written by Lua for training neural encoder decoder architectures. Normally, both the encoder and the decoder use recurring neural networks (RNNs), in our case typically long-term short-term memory (LSTM) units [5], each with 4 layers. We always use a (global) attention layer with input power to help the model learn faster by retaining a \"memory\" of past alignment decisions [10]. For European languages, we use \"case features\" (see Section 3.1) as additional input variables from the embedding space of \"cases\" [14]. The main word embedding is linked to the embedding of cases to form the inputs to the encoder. On each layer of the encoder, the RNNs are bidirectional [13] as well as the encoder encoding rates [4]."}, {"heading": "2.2 Optimization and model fitting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Single-GPU environment", "text": "In order to optimize the formation of our NMT system in single GPU environments, we evaluated various algorithms primarily based on their speed of convergence and translation quality. The data set used were English-German property descriptions with one million parallel sentences. We conducted experiments with four known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18]. Our SGD decay strategy is based on a combination of perplexity value and epoch number, which means that we reduce the current learning rate by a multiplication factor of 0.7 if the current epoch of validation perplexity does not decrease, and after each epoch after the 9th epoch. Our initial learning parameters for SGD, Adam, Adagrad and Adadelta are 1.0, 0.0002, 0.01 and 1.0, respectively."}, {"heading": "2.2.2 Multi-GPU environment", "text": "Next, we experimented with the use of multiple GPUs by using data parallelism technique that trains stacks in parallel on different GPUs. On a single GPU, our model takes 6h11m per epoch on average, and we usually see that it converges around the 15th epoch, which means that training a model to only 1 million sets takes about 4 days. 15 epochs on a corpus of size 10M could easily be translated to about 40 days1. In an attempt to accelerate our development cycle, we ran some experiments with synchronous and asynchronous SGD (with decay) on a cluster of 2, 4, 6 and 8 GPUs. The main difference between these two approaches is that in synchronous mode all gradients are accumulated and parameter updates synchronized, while in asynchronous mode each GPU calculates its own course and with the \"master copy\" of the parameters independently and synchronously."}, {"heading": "2.3 The importance of corpus size", "text": "To see how much benefit we get from an increased body size, we compared models trained on 1M, 2.5M, 5M, 7.5M and 10M sets. To make a fair comparison, we report the learning curves based on the number of iterations (training time) rather than the number of epochs. Figure 3 shows our results. Essentially, there were no big surprises. It seems that the model with clearer sentences with sufficient iteration has a higher BLEU score. Note how in the beginning smaller sets actually win, but with sufficient training time, the model begins to take full advantage of more data. The largest corpus size of 10M does not perform best at the end of 90M iterations, but as we will see in Section 4.3, this is not true and by human assessment 10M provides the best results that are simply not covered by the BLEU metric."}, {"heading": "3 Handling real-world content", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tokenization and case features", "text": "In our final models, we use byte-pair encoding (BPE) tokenization procedure [15]. BPE is a compression technique that has recently been adapted to find optimal tokens for composing sequences in sequence-to-sequence learning tasks. In theory, the technique should find a perfect compromise between using word-level translation (and dealing with units outside the vocabulary) and character-level translation (and dealing with much longer sequences of tokens).The procedure is very simple. We start with a set of tokens that represent the list of acceptable characters, and expand it at each step by adding a concatenation of two elements already included in the list, which is most common in our corpus.The number of iterations can be considered the only hyperparameter of the algorithm. We can either apply BPE to the source and the objectives separately, or we can apply it to the combined corpus (see table 2)."}, {"heading": "3.2 Handling named entities", "text": "Text in the travel domain contains a large number of entities. There is almost always a specific target involved, a property name, distances, times, etc. Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found that RNN encoder decoder architecture is not sufficient to achieve acceptable results, mainly due to mishandled named entities. This section outlines our approach to processing such entities, which drastically improves translation quality. As an example, mistranslated distances are one of the most common types of errors when NMT is naively applied to raw text, even with very large body sizes (over 10M parallel sets). Interestingly, NMT often converts correctly between kilometers and miles for frequently occurring distances (e.g. 5km, 10 miles); however, the number of distance-related errors in our validation type is not comparable to 12 hours (one more than 24 hours)."}, {"heading": "4 Quality evaluation", "text": "As opposed to simple classification or regression tasks, sequence learning problems are much more difficult to evaluate. The problem stems from the fact that there can be many possible solutions and it is difficult (and often impossible) to compare the output of the model with all valid \"true values.\" In order to automatically assess the quality of translations, the so-called BLEU score [12] is a useful heuristic that roughly measures the degree of word overlap between the model translation and a human translation. BLEU score is attractive because it is fully automatic for translated sentences and correspondingly predicted sentences. However, several problems have been identified with the use of the BLEU score alone. As a purely numerical metric, BLEU prefers translations that have more common words and n-grams with the reference translation, regardless of the sentence grammar. Furthermore, models that rephrase the sentence in a way that uses words other than the reference sentence while preserving its meaning.In this section, we will first describe how to use our linguistic models (section 4.1) and use our expertise (section 4.uistic models)."}, {"heading": "4.1 Human evaluation loop", "text": "Our main human assessment is based on the appropriateness / fluidity methodology2, which, as the name suggests, is based on two criteria: appropriateness and fluidity. Adequacy indicates the degree to which the meaning of the source sentence is maintained while fluently assessing how grammatically good (from the native speaker's perspective) the translated segment sounds. Each sentence is evaluated by two independent professional translators from English to German (native speakers of German). For the experiments in Section 4.3, we selected 200 randomly selected sentences and translators with at least one year of experience in professional translation of Booking.com content. In addition, we use human assessment bodies to assess the quality of entity management (as described in Section 3.2). For this task, each sentence containing a specific entity type is given a binary assessment of whether the entity is translated correctly or not."}, {"heading": "4.2 Business sensitivity analysis", "text": "One important flaw of the BLEU is that it does not say anything about the so-called \"business sensitive\" errors."}, {"heading": "4.3 BLEU score vs human-based metrics", "text": "While the BLEU score is very convenient to use because it can be calculated automatically, the most important metrics we really trust are based on a human basis (see Section 4.1). Here we look at how the BLEU values from our Anglo-German corpus size experiment of Section 2.3 are correlated with adequacy / flowability metrics. Results are shown in Figure 4. Training with the corpus size of 10M clearly delivers the best performance according to human assessment, but this is not reflected in the BLEU score. As we can see, the correlation between human metrics and the BLEU score is rather poor. In particular, if we had only looked at BLEU, we could easily have drawn the wrong conclusion about our experiment from Section 2.3."}, {"heading": "5 Conclusion", "text": "We presented our approach to developing a large-scale NMT system, focusing specifically on practical considerations; we presented the performance of various optimization strategies for model training in single- and multi-channel GPU environments; we noted that a combination of Adam and SGD works best on a single GPU as the learning rate decays; and asynchronous parallelization of SGD is a great strategy to dramatically accelerate training; we presented the benefits of BPE tokenization for machine translation; and we advocated the pre-processing of designated units for better translation; and finally, we presented our approach to dealing with critical translation errors through our business sensitivity framework, arguing that BLEU score alone can be a bad way to track the improvisation of the MT system; and in the future, we will continue to conduct optimization-related experiments, especially with a view to better strategies to exploit multiple-U issues that we will not be able to use in our massive linguistic areas."}, {"heading": "Acknowledgements", "text": "We would like to thank our language specialists for their valuable human feedback and Darina Kozlova for her important advice on human evaluation and for the patient coordination of all this work."}], "references": [{"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Merrienboer", "D. Bahdanau", "B. Yoshua"], "venue": "In Proceedings of SSST-8,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems. arXiv preprint arXiv:1610.05540", "author": ["J. Crego", "J. Kim", "G. Klein", "A. Rebollo", "K. Yang", "J. Senellart", "E. Akhanov", "P. Brunelle", "A. Coquard", "Y Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush"], "venue": "arXiv preprint arXiv:1701.02810", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "Technical report, http://hunch. net", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Machine translation at booking.com: Journey and lessons learned", "author": ["P. Levin", "N. Dhanuka", "M. Khalilov"], "venue": "In Proceedings of the 20th International Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Linguistic input features improve neural machine translation. arXiv preprint arXiv:1606.02892", "author": ["R. Sennrich", "B. Haddow"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1508.07909", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of machine learning research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["W. Xing", "Z. Lu", "Z. Tu", "H. Li", "D. Xiong", "M. Zhang"], "venue": "In Proceedings of AAAI 2017,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Our experiments [9] consistently show the superiority of neural machine translation (NMT) systems over the more traditional statistical ones, even when we benchmark them against the well-established and tested general purpose systems.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "In this part we show how in addition to the BLEU metric [12], the de facto standard for automatic MT scoring, we employ human evaluation of translation adequacy and fluency.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "1 Model architecture The core of our translation pipeline is based on OpenNMT [7], which is a Lua written framework for training encoder-decoder neural architectures.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Usually, both the encoder and the decoder recurrent neural networks (RNNs), in our case typically long short-termmemory (LSTM) units [5], each with 4 layers.", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "We always use (global) attention layer with input feeding to help the model learn faster by keeping a \u201cmemory\u201d of past alignment decisions [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "1) as additional input variables from the \u201ccases\u201d embedding space [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "At each layer of the encoder the RNNs are bi-directional [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Both the encoder and the decoder use residual connections between layers [4] as well as the dropout rate of 0.", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "3 [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 135, "endOffset": 138}, {"referenceID": 17, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "1 Tokenization and case features In our final models we use byte-pair encoding (BPE) tokenization procedure [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "Table 2: Comparison of the BLEU scores of identically trained models with different BPE configurations, as well as the baseline with a vocabulary of 50,000 most common words (see [9] for more details on the baseline model).", "startOffset": 179, "endOffset": 182}, {"referenceID": 13, "context": "Case features get their own embeddings which get combined with token embeddings during the translation [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 1, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 16, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 11, "context": "To assess the quality of translations automatically, a useful heuristic is the so-called BLEU score [12] which roughly measures the degree of word overlap between the model translation and a human translation.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "For the first layer of finding relevant sentences, we learn word and phrase embeddings by training word2vec [11] on our full (monolingual) corpora.", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "For the classification task we use a bag-of-ngrams linear model approach [8].", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "While some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results.", "creator": "LaTeX with hyperref package"}}}