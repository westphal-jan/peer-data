{"id": "1709.05861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "abstract": "This paper reports the analysis of audio and visual features in predicting the emotion dimensions under the seventh Audio/Visual Emotion Subchallenge (AVEC 2017). For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event. Then we trained fully connected neural network regression model on the dataset for all these different modalities. We applied multimodal fusion on the output models to get the Concordance correlation coefficient on Development set as well as Test set.", "histories": [["v1", "Mon, 18 Sep 2017 11:01:43 GMT  (313kb,D)", "http://arxiv.org/abs/1709.05861v1", "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385"], ["v2", "Tue, 24 Oct 2017 12:08:09 GMT  (313kb,D)", "http://arxiv.org/abs/1709.05861v2", "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385"]], "COMMENTS": "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["narotam singh", "nittin singh", "abhinav dhall indian institute of technology ropar)"], "accepted": false, "id": "1709.05861"}, "pdf": {"name": "1709.05861.pdf", "metadata": {"source": "CRF", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "authors": ["Narotam Singh", "Nittin Singh", "Abhinav Dhall"], "emails": ["*2015csb1065@iitrpr.ac.in,", "2015csb1067@iitrpr.ac.in,", "abhinav@iitrpr.ac.in"], "sections": [{"heading": null, "text": "Keywords - HOG, SIFT, GMM, Fisher, Neural Networks *. INTRODUCTIONEmotions are one of the most important and complex aspects of human consciousness. Researchers have tried to classify or measure human emotions, as this research can lead to a significant improvement in human-computer interaction. Many studies have defined emotions as discrete categories. In the discrete emotion theory, it is assumed that people have some basic emotions. These basic emotions include anger, disgust, fear, happiness, sadness and surprise [4]. While some other studies depict emotions using continuous dimensional models. Dimensional models of emotion attempt to depict emotions as a specific continuous region in 2-D or 3-D continuum. Harold Schlosberg in his study defines the three dimensions of emotion: pleasantness-unpleasantness, attention-reaction, level of activation [24]. Dimensional analysis of emotions indicates that each dimension is working independently from / with different regions of our brain and interrelates to each other."}, {"heading": "II. RELATED WORKS", "text": "Affective Computing Analysis has attracted many researchers since it was first introduced into modern computing technology by Rosalind Picard in 1995. [20] Since then, researchers have introduced new techniques for predicting human emotions. In terms of extracting traits using audio signals and human speech, traits were used so that LPC [18] was used. MFCC [22] traits were also used by some who achieved better results. Teams participating in the 2016 AVEC competition [21] also used physiological traits such as heart rate (HR), ECG signals, skin conductivity (SCR), skin conductivity (SCL), etc. Apart from visual traits such as HOG [17] and SIFT [13] traits, PHOG are also successful in human recognition [1]. HOG [17] and LBP (Local Binary Patterns) traits can also be used in combination, as in [29] where they complement each other."}, {"heading": "III. DATASET", "text": "This challenge is based on a novel database of human interactions recorded \"in the wild\": Automatic Sentiment Analysis in the Wild (SEWA2) [25] dataset. The dataset contains the video data, audio data and manual transcriptions of the language of the corresponding subjects for training, development and tests with emotional dimensions such as arousal, value and sympathy. Among the audio data are the audio files of the subjects, 23 LLDs from the feature set eGeMAPSv0.1a, 88 acoustic features from the feature set eGeMAPSv0.1a and the Bag-of-Audio-Word-Representation (BoAW) of the LDs for all subjects. Among the video data are the video files of the subjects, video features (pixel coordinates of viewpoints and copying): normalized Bag-of-video-word representation."}, {"heading": "IV. FEATURES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Audio Features", "text": "The BoAW representation (generated by openXBOW [16]) of 23 LLDs from eGeMAPSv0.1a [5] feature set extracted with openSmile [6] are provided in the Sub-Challenge dataset. We used BoAW with block size 6 seconds as it is used for the training process using neural network.B. Visual FeaturesBefore extracting any visual feature, we detected the human faces from the frames of the videos and applied the affine transformation using the facial landing coordinates provided in the dataset to get the vertical faces. Faces are detected using the Viola-Jones face detection [27] algorithm and then cropped. All the visual features discussed further are extracted from these affine-warped faces. Most visual features are trained according to normalization process, except in those cases where it increases the error. Images are scaled and converted to gray scale as per need."}, {"heading": "V. TRAINING", "text": "We used Neural Networks for this regression task instead of LibSVM because it was very time consuming for such a large dataset. The number of layers used in the network is more or less the same in all models. ReLU (rectified linear unit) activation function is used between layers, except for the last layer where Tanh function is used to get the final output. We observed that 4-5 layers were sufficient and increasing the layers more than this was mathematically expensive and did not improve the results."}, {"heading": "VI. RESULTS", "text": "For each emotion dimension, the models trained on the Train Set are used to predict the Development (Dev) Set. CCC is then calculated for each model. \u03c1 is the correlation coefficient between two variables x and y, \u03c32x, \u03c3 2 y is the variance of x and y, respectively, while \u00b5x, \u00b5y are the respective averages. Here, CCC is the concordance correlation coefficient calculated as follows: CCC = 2\u03c1\u03c3x\u03c3y\u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y) 2"}, {"heading": "VII. MULTIMODAL FUSION", "text": "Now we can perform multimodal fusion of these features to achieve final merged outcomes. Feature Arousal Valence LikingTest-Multimodal 0.361 0.437 -0.001"}, {"heading": "VIII. CONCLUSION", "text": "This report summarizes our two-year internship project, which turned out to be very informative as a beginner, and this was our first time working with such large data sets. Although the field of facial recognition is not in a very advanced state, the growth rate provides a fairly good future. Of our project, we can say that visual characteristics such as HOG, SIFT, etc. are able to capture the important facial information. We observed that the facial characteristics of the HOG and VGG facial characteristics performed quite well compared to the others. We expected better results from the SIFT-GMM characteristics, but the results obtained are not very encouraging. However, overall results have shown that these visual characteristics are quite reliable in most cases and can be used as a method for detecting emotions. Furthermore, it was observed that multimodal fusion yields even better results by giving appropriate weights to the respective characteristics. Furthermore, we observed that weights depend on the success of certain models to indicate that the model's ability to differentiate between specific characteristics."}, {"heading": "ACKNOWLEDGMENT", "text": "We are very grateful to our teacher and the entire AVEC 2017 organizing team for giving us the opportunity to participate in this informative event, which has helped us gain more knowledge and insight into the field of computer vision and machine learning, as well as some basic knowledge of deep learning."}], "references": [{"title": "Representing shape with a spatial pyramid kernel", "author": ["A.Z.A. BOSCH", "X. MUNOZ"], "venue": "Conference on Image and Video Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A temporally piece-wise fisher vector approach for depression analysis", "author": ["A. DHALL", "R. GOECKE"], "venue": "In 2015 International Conference on Affective Computing and Intelligent Interaction (ACII),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An argument for basic emotions", "author": ["P. EKMAN"], "venue": "Cognition and emotion,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1106}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing", "author": ["F. EYBEN", "K.R. SCHERER", "B.W. SCHULLER", "J. SUNDBERG", "E. ANDR", "C. BUSSO", "L.Y. DEVILLERS", "J. EPPS", "P. LAUKKA", "S.S. NARAYANAN", "K.P. TRUONG"], "venue": "IEEE Transactions on Affective Computing 7,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor", "author": ["F. EYBEN", "F. WENINGER", "F.G.B. S"], "venue": "In Proc. ACM Multimedia (MM) (Barcelona,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A visual bag of words method for interactive qualitative localization and mapping", "author": ["D. FILLIAT"], "venue": "International Conference on Robotics and Automation, Italy", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. HE", "D. JIANG", "L. YANG", "E. PEI", "P. WU", "H. SAHLI"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep learning driven hypergraph representation for image-based emotion recognition", "author": ["HUANG Y", "LU"], "venue": "In Proceedings of the International Conference on Multimodal Interaction (Tokyo, Japan,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Image classifcation with the fisher vector: Theory and practice", "author": ["JORGE SANCHEZ", "FLORENT PERRONNIN", "T.M.J. V"], "venue": "International Journal of Computer Vision", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deap : a database for emotion analysis using physiological signals", "author": ["KOELSTRA", "E.A. SANDER"], "venue": "IEEE Transactions on Affective Computing vol.3", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. LOWE"], "venue": "Int. J. Comput. Vision,Hingham,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Multi-modal fusion based on classication using rejection option and markov fusion networks", "author": ["M. GLODEK", "G.P.M. SCHELS", "F. SCHWENKER"], "venue": "In Proc. of ICPR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "LSTMmodeling of continuous emotions in an audiovisual aect recognition framework", "author": ["M. WOLLMER", "M. KAISER", "F.E.B. S", "G. RIGOLL"], "venue": "Image and Vision Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit", "author": ["B.W.S. MAXIMILIAN SCHMITT"], "venue": "arXiv preprint arXiv:1605.06778", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Histograms of oriented gradients for human detection", "author": ["B.T. NAVNEET DALAL"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Emotion recognition in speech using neural networks", "author": ["J. NICHOLSON", "K. TAKAHASHI", "R. NAKATSU"], "venue": "Neural Computing & Applications 9,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Deep face recognition", "author": ["O.M. PARKHI", "A. VEDALDI", "A. ZISSERMAN"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multimodal emotion recognition for avec 2016 challenge", "author": ["F. POVOLNY", "P. MATEJKA", "M. HRADIS", "A. POPKOV\u00c1", "L. OTRUSINA", "P. SMRZ", "I. WOOD", "C. ROBIN", "L. LAMEL"], "venue": "In Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge (New York, NY,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Avec 2017\u2013real-life depression, and affect recognition workshop and challenge", "author": ["F. RINGEVAL", "B. SCHULLER", "M. VALSTAR", "J. GRATCH", "R. COWIE", "S. SCHERER", "S. MOZGAI", "N. CUMMINS", "M. SCHMITT", "M. PANTIC"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Multimodal sentiment analysis in the wild: Ethical considerations on data collection, annotation, and exploitation", "author": ["B. SCHULLER", "GANASCIA", "J.-G", "L. DEVILLERS"], "venue": "In Actes du Workshop on Ethics In Corpus Collection,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. VEDALDI", "K. LENC"], "venue": "In Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Robust real-time object detection", "author": ["P. VIOLA", "M. JONES"], "venue": "International Journal of Computer Vision", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "An hog-lbp human detector with partial occlusion handling", "author": ["WANG T.H. X", "YAN"], "venue": "International Conference on Computer Vision", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "These basic emotions include anger, disgust, fear, happiness, sadness and surprise [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "Another dimension is liking which points the wanting of a particular experience [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "We have extracted the visual features in the form of HOG features, deep visual features and fisher vector representation of SIFT [13] features and the audio features are used as it is provided by the organizers of the challenge.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Regarding the extraction of features using audio signals and from human speech, feature sets such that LPC [18] have been used.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Teams which participated in AVEC 2016 competition [21] also used physiological features like the heart rate (HR), ECG signals, the skin conductance response (SCR), the skin conductance level (SCL), etc.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 15, "context": "HOG [17] and LBP (Local binary patterns) features can also be used in combination like in [29], where they complemented each other.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "HOG [17] and LBP (Local binary patterns) features can also be used in combination like in [29], where they complemented each other.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "LSTM (Long Short Term Memory) is a type of Recurrent neural Network proposed in 1997 which was used for affect recognition in [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "This network was also used by teams in the AVEC 2015 competition [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Talking of neural networks, deep neural networks like Deep Convolutional Neural Network in [10] used transductive learning approach for emotion recognition along with the hypergraphs.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Many such techniques exist, one of them being Markov fusion Network [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "This challenge is based on a novel database of humanhuman interactions recorded \u2018in-the-wild\u2019: Automatic Sentiment Analysis in the Wild (SEWA2) [25] data set.", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "The BoAW representation (generated by openXBOW [16]) of 23 LLDs from eGeMAPSv0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "1a [5] feature set extracted using openSmile [6] are provided in the sub challenge dataset.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "1a [5] feature set extracted using openSmile [6] are provided in the sub challenge dataset.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "Faces are detected using the Viola-Jones face detection [27] algorithm and then cropped.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "1) Histogram of Gradients (HOG) features: Faces are resized to the default window size of HOG [17] (64x128) so as to get a single feature from the frames.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "In the context of computer vision it has proved to be a very effective representation of an image [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "We first calculated the SIFT [13] descriptors for each frame and only the descriptors having highest response value were chosen (top 50 descriptors).", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "a) Fisher Vector Extraction: Following the fisher vector approach in [2], we took the previously calculated SIFT features of each frame and got the Fisher vector encodings [11] of these sift features based on the pre-calculated GMM model.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "a) Fisher Vector Extraction: Following the fisher vector approach in [2], we took the previously calculated SIFT features of each frame and got the Fisher vector encodings [11] of these sift features based on the pre-calculated GMM model.", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "3) Deep Visual Features: An output of a particular layer of pretrained models : VGG-Face [19] and ResNet-50-dag [8] are used as features.", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "3) Deep Visual Features: An output of a particular layer of pretrained models : VGG-Face [19] and ResNet-50-dag [8] are used as features.", "startOffset": 112, "endOffset": 115}, {"referenceID": 21, "context": "These models are deep convolutional neural networks obtained from MatConvNet toolbox [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Output of 35th layer of VGG-Face and output of 174th layer of ResNet50-dag [8] are used as features of the frames of each video clip.", "startOffset": 75, "endOffset": 78}, {"referenceID": 19, "context": "TABLE I: AVEC Baseline [23] : Concordance Correlation Coefficient Table for Development (Dev) Set and Test Set.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "This paper reports the analysis of audio and visual features in predicting the emotion dimensions under the seventh Audio/Visual Emotion Subchallenge (AVEC 2017). For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event. Then we trained fully connected neural network regression model on the dataset for all these different modalities. We applied multimodal fusion on the output models to get the Concordance correlation coefficient on Development set as well as Test set. Keywords\u2014HOG, SIFT, GMM, Fisher, Neural Networks.", "creator": "LaTeX with hyperref package"}}}