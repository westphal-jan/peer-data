{"id": "1709.05865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Depression Scale Recognition from Audio, Visual and Text Analysis", "abstract": "Depression is a major mental health disorder that is rapidly affecting lives worldwide. Depression not only impacts emotional but also physical and psychological state of the person. Its symptoms include lack of interest in daily activities, feeling low, anxiety, frustration, loss of weight and even feeling of self-hatred. This report describes work done by us for Audio Visual Emotion Challenge (AVEC) 2017 during our second year BTech summer internship. With the increase in demand to detect depression automatically with the help of machine learning algorithms, we present our multimodal feature extraction and decision level fusion approach for the same. Features are extracted by processing on the provided Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector approach were applied on the visual data; statistical descriptors on gaze, pose; low level audio features and head pose and text features were also extracted. Classification is done on fused as well as independent features using Support Vector Machine (SVM) and neural networks. The results obtained were able to cross the provided baseline on validation data set by 17% on audio features and 24.5% on video features.", "histories": [["v1", "Mon, 18 Sep 2017 11:26:01 GMT  (363kb,D)", "http://arxiv.org/abs/1709.05865v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["shubham dham", "anirudh sharma", "abhinav dhall"], "accepted": false, "id": "1709.05865"}, "pdf": {"name": "1709.05865.pdf", "metadata": {"source": "CRF", "title": "Depression Scale Recognition from Audio, Visual and Text Analysis", "authors": ["Shubham Dham", "Anirudh Sharma", "Abhinav Dhall"], "emails": ["2015csb1117@iitrpr.ac.in", "2015csb1007@iitrpr.ac.in", "abhinav@iitrpr.ac.in"], "sections": [{"heading": null, "text": "Keywords - AVEC 2017, SVM, Depression, Neural Network, RMSE, MAE, Fusion, Language Processing. I. INTRODUCTIONDepression is a common mood disorder that affects the life of the affected individual. It is a worldwide problem that affects countless lives. Depressed people are more prone to anxiety, sadness, loneliness, hopelessness and are often anxious and disinterested. They find it difficult to concentrate on their work, communicate with people and even become introverted [18]. Current techniques are based on clinical trials of the patient in person. These methods are subjective, are conducted in interviews and depend on reports from the patient."}, {"heading": "II. RELATED WORK", "text": "They have also brought papers with decent accuracy as presented in the published results. Le Yang et al. [23] during AVEC 2016 achieved quite plausible accuracy through Decision Tree classification and multimodal fusion of audiovisual and text features. Asim Jan et al. [7] in the AVEC 2014 competition used Motion History Histogram for detection of depression by extraction Local Binary Pattern (LBP) and Edge Orientation Histogram Features (EOF). Low Lu-Shih et al. [8] proposed description of depression detection by acoustic language analysis. The five acoustic characteristic categories used were prosodic, spectral, glottal and Teager energy operator (TEO) based features, with TEO observed to perform best."}, {"heading": "III. FEATURE EXTRACTION", "text": "This year, it has come to the point where there is only one person who is able to take care of a person who is able to take care of a person who is able to take care of a person and a person who is able to take care of a person who is able to take care of a person, a person who is able to take care of a person, a person who is able to take care of a person, a person who is able to envy a person."}, {"heading": "B. Audio Features", "text": "The audio data provided by AVEC consisted of an audio file of the entire interview with the participant, pre-extracted features using the COVAREP toolbox at 10 ms intervals over the entire recording (F0, NAQ, QOQ, H1H2, PSP, MDQ, Peak Slope, Rd, Rd conf, MCEP 0-24, HMPDM 1-24, HMPDD 1-12 and formants 1-3), a transcript file with speech times and values of the participant and the virtual interviewer, and a formant file. The specified audio file was edited to only hear the participant's voice. The audio file was then used to isolate the participant's voice using the speech time giveb in a transcription file. Two sets of features were calculated for the audio models. The first group consists of the following statistical characteristics as they are represented in a low-level table."}, {"heading": "C. Text Features", "text": "The total number of sentences spoken by the participant, the words spoken by the participant, the average number of words spoken in each sentence, the ratio of laughter to the total number of words spoken by the participant, the ratio of depression-related words to the total number of words spoken by the participant are the extracted text properties. The total number of sentences and words spoken has been normalized by the duration of the video. Referring to the essay mentioned [14], slow and less spoken words, longer pauses and short answers are manifestations of depression. For the negative / depression-related words, a dictionary of about 720 words has been created manually using online resources [15], [16]. Another seven features have been extracted using the Affective Norms for English Word Ratings (ANEW) [17]."}, {"heading": "IV. CLASSIFICATION", "text": "SVM and Neural Network were used as classification algorithms. SVM classifier was applied separately to all of the above features. Eight models were trained for each of the above features, with each model determining the values 0, 1, 2 or 3 as intensities of PHQ8 NoInterest, PHQ8 Depressive, PHQ8 Sleep, PHQ8 Tired, PHQ8 Appetite, PHQ8 Failure, PHQ8 Concentrating and PHQ8 Moving. Results of all eight models were added to obtain the final predicted PHQ8 value [22] of 24. Neural Network was only applied to the calculated Fisher vectors, because the dimension of other feature vectors for a video was very small and therefore the application of layers to such a small dimension did not make sense. Furthermore, the results were experimentally verified that the results were not good. Therefore, for Fisher vectors, similar to SVM, 8 classification networks were trained, the sum of which was the integrity of which resulted in a further Q8 result."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Support Vector Machine (SVM)", "text": "Initially, the standard parameters for the SVM classifier were used, but the training accuracy achieved with standard parameters was very low. Therefore, a K-fold cross-validation was applied to obtain the optimal values of cost and gamma for the classifier for each of the above features. Finally, the cross-validation performed was 5x, applied using the Python script. Cross-validation enabled the model to adjust the training data for a number of values of cost and gamma. Finally, those values of cost and gamma for which the accuracy was highest during development were selected. The best results along with SVM parameters obtained for each of the above-mentioned values, i.e. audio, text, head posture and fish vectors on the validation dataset are shown in Table 2. Results for other modalities are not listed because they are not even close to the baseline values, i.e. the best MSE validation results were set on the MHI only."}, {"heading": "B. Neural Networks", "text": "The number of dropouts and the dimensions of the layers were determined manually to minimize the RMSE and MAE of each network. In all networks, the \"Adam\" optimizer was used via \"SGD\" because it was faster and more efficient. The best RMS and MAE for both optimizers are shown in Table IV. With the results obtained on the characteristics of both approaches, SVM and neural networks, the decision-level fusion was applied to the results of different modalities; the results of audio and text were merged (Table 4) and Head and Fisher together (Table 5), because the dimensionality of audio text and Fisher-Head modalities were narrow. Since RMSE was nearly the same for each of them, the weights were decided experimentally as fusion."}, {"heading": "VI. CONCLUSION", "text": "The behavior of a depressed person shows relative changes in speech patterns, facial expressions, and head movements compared to a non-depressed person. In this work, we introduced tasks to detect depression by visual, audio, and text characteristics using SVM and neural networks as classifiers. GMM clustering and fishing vectors were calculated based on the relative distance of the facial regions. Face regions used to detect the relative distance of certain points are the ones that have the most to do with facial expressions such as smiling, laughing, and other visible emotions. Head posture, statistical descriptors of gazes, poses, and flashing speed were also calculated. Verbal responses of the person coded in the form of text (sentences, words, negative words) and audio (low level) hold information about the person's behavior. The extracted features were trained on SVM. Results from audio, fish vectors, and text characteristics, both individually and combined, as well as the basic data (7) validated."}, {"heading": "VII. ACKNOWLEDGEMENT", "text": "We would like to express our gratitude to our teachers and organizers of AVEC 2017 who gave us a unique opportunity to participate in this project (AVEC 2017), which helped us to develop a better understanding of computer vision, machine learning and even basic concepts of deep learning."}], "references": [{"title": "An ethological description of depression", "author": ["J. Pedersen"], "venue": "Acta psychiatrica scandinavica,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "The ethological ap- proach to the assessment of depressive disorders, The Jour- nal of nervous and mental disease", "author": ["L. Fossi", "C. Faravelli", "M. Paoli"], "venue": "vol. 172,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1984}, {"title": "Nonverbal cues for depression", "author": ["P. Waxer"], "venue": "Journal of Abnormal Psychology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1974}, {"title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge.", "author": ["Valstar", "Michel"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "AVEC 2017Real-life Depression, and Affect Recognition Workshop and Challenge.", "author": ["Ringeval", "Fabien"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Automatic depression scale prediction using facial expression dynamics and regression.", "author": ["Jan", "Asim"], "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Detection of clinical depression in adolescents\u2019 using acoustic speech analysis.", "author": ["L. Low"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Automatic detection of depression in speech using gaussian mixture modeling with factor analysis.", "author": ["Sturim", "Douglas"], "venue": "Twelfth Annual Conference of the International Speech Communication Association", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Head pose and movement analysis as an indicator of depression.", "author": ["Alghowinem", "Sharifa"], "venue": "Affective Computing and Intelligent Interaction (ACII),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks.", "author": ["Tzirakis", "Panagiotis"], "venue": "arXiv preprint arXiv:1704.08619", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Depression Assessment by Fusing High and Low Level Features from Audio, Video, and Text.", "author": ["Pampouchidou", "Anastasia"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards the Usage of Optical Flow Temporal Features for Facial Expression Classification.", "author": ["Ptucha", "Raymond", "Andreas Savakis"], "venue": "Advances in Visual Computing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Non-verbal communication in depression", "author": ["Ellgring", "Heiner"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A multimodal context-based approach for distress assessment.", "author": ["Ghosh", "Sayan", "Moitreya Chatterjee", "Louis-Philippe Morency"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Affective norms for English words (ANEW): Instruction manual and affective ratings. Technical report C-1, the center for research in psychophysiology", "author": ["Bradley", "Margaret M", "Peter J. Lang"], "venue": "University of Florida,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Depression and chronic medical illness.", "author": ["Katon", "Wayne", "Mark D. Sullivan"], "venue": "J Clin Psychiatry 51.Suppl", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Diagnosis of depression by behavioural signals: a multimodal approach.", "author": ["Cummins", "Nicholas"], "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Action and event recognition with fisher vectors on a compact feature set.", "author": ["Oneata", "Dan", "Jakob Verbeek", "Cordelia Schmid"], "venue": "Proceedings of the IEEE international conference on computer vision", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "A temporally piece-wise fisher vector approach for depression analysis.", "author": ["Dhall", "Abhinav", "Roland Goecke"], "venue": "Affective Computing and Intelligent Interaction (ACII), 2015 International Conference on. IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "The PHQ-8 as a measure of current depression in the general population.", "author": ["Kroenke", "Kurt"], "venue": "Journal of affective disorders", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Decision Tree Based Depression Classification from Audio Video and Language Information.", "author": ["Yang", "Le"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "They find it hard to concentrate on their work, communicate with people and even become introvert [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Studies have shown that depressed people tend to avoid eye contact, engage less in verbal communication, speak anxiously in short phrases and monotonously [2] [3] [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "Studies have shown that depressed people tend to avoid eye contact, engage less in verbal communication, speak anxiously in short phrases and monotonously [2] [3] [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Studies have shown that depressed people tend to avoid eye contact, engage less in verbal communication, speak anxiously in short phrases and monotonously [2] [3] [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 3, "context": "Taking motivation from AVEC 2016 [5], our team participated in AVEC 2017 [6], which has similar dataset as AVEC 2016.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Taking motivation from AVEC 2016 [5], our team participated in AVEC 2017 [6], which has similar dataset as AVEC 2016.", "startOffset": 73, "endOffset": 76}, {"referenceID": 20, "context": "[23] during AVEC 2016 achieved quite plausible accuracy through Decision Tree classification and multimodal fusion of audio-visual and text features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] in AVEC 2014 competition used Motion History Histogram for depression recognition by extracting Local Binary Pattern (LBP) and Edge Orientation Histogram features (EOF).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] proposed description of depression recognition using acoustic speech analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] also brought forward GMM clustering in classification on speech data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Depressed people show less nodding [3] , with more likeliness to position head down than non-depressed people [4], avoid eye contact [3].", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Depressed people show less nodding [3] , with more likeliness to position head down than non-depressed people [4], avoid eye contact [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Depressed people show less nodding [3] , with more likeliness to position head down than non-depressed people [4], avoid eye contact [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "[10] used head pose and movement features associated with the face to perform classification with SVM on depression recognition; and concluded that head movements of depressed people are different than that of normal person.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] in AVEC 2016 presented a deep learning based approach in assessing emotion as well depression state of the person.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] in AVEC 2016 extracted features from the recorded verbal communication recorded in the transcript file provided in the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] during AVEC 2013, presented multimodal approach by fusing audio and visual modalities with different techniques to detect depression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "1] [12].", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Then the fisher vectors [20] [21] were extracted for each video from the clusters produced using GMM.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "Then the fisher vectors [20] [21] were extracted for each video from the clusters produced using GMM.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "5) Other Features: For visual features, Motion History image that converts the motion into grayscale image, with the most recent movement shown by pixels with highest grayscale value and earliest by pixels with least grayscale value [13], was also computed on the facial 2D landmarks.", "startOffset": 233, "endOffset": 237}, {"referenceID": 12, "context": "Referring to the mentioned paper [14], slow and less amount of speech, longer speech pauses and brief answers are manifestations of depression.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "For the negative/depression related words, a dictionary of about 720 words was constructed manually using online resources [15], [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Another seven features were extracted using the Affective Norms for English Words ratings (ANEW) [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Results of all the eight models were added to obtain the final predicted PHQ8 score [22] out of 24.", "startOffset": 84, "endOffset": 88}], "year": 2017, "abstractText": "Depression is a major mental health disorder that is rapidly affecting lives worldwide. Depression not only impacts emotional but also physical and psychological state of the person. Its symptoms include lack of interest in daily activities, feeling low, anxiety, frustration, loss of weight and even feeling of selfhatred. This report describes work done by us for Audio Visual Emotion Challenge (AVEC) 2017 during our second year BTech summer internship. With the increase in demand to detect depression automatically with the help of machine learning algorithms, we present our multimodal feature extraction and decision level fusion approach for the same. Features are extracted by processing on the provided Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector approach were applied on the visual data; statistical descriptors on gaze, pose; low level audio features and head pose and text features were also extracted. Classification is done on fused as well as independent features using Support Vector Machine (SVM) and neural networks. The results obtained were able to cross the provided baseline on validation data set by 17% on audio features and 24.5% on video features. Keywords\u2014AVEC 2017, SVM, Depression, Neural network, RMSE, MAE, fusion, speech processing.", "creator": "LaTeX with hyperref package"}}}