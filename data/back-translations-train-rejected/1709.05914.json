{"id": "1709.05914", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Limitations of Cross-Lingual Learning from Image Search", "abstract": "Cross-lingual representation learning is an important step in making NLP scale to all the world's languages. Recent work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused on the translation of selected nouns only. In our work, we investigate whether the meaning of other parts-of-speech, in particular adjectives and verbs, can be learned in the same way. We also experiment with combining the representations learned from visual data with embeddings learned from textual data. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.", "histories": [["v1", "Mon, 18 Sep 2017 13:28:40 GMT  (8723kb,D)", "http://arxiv.org/abs/1709.05914v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mareike hartmann", "anders soegaard"], "accepted": false, "id": "1709.05914"}, "pdf": {"name": "1709.05914.pdf", "metadata": {"source": "CRF", "title": "Limitations of Cross-Lingual Learning from Image Search", "authors": ["Mareike Hartmann", "Anders S\u00f8gaard"], "emails": ["hartmann@di.ku.dk", "soegaard@di.ku.dk"], "sections": [{"heading": "1 Introduction", "text": "Typically, multilingual word representations are learned by word alignments, sentence alignments, or, less often, by aligned, comparable documents (Levy et al., 2017). Ammar et al. (2016) propose a method that learns multilingual word embeddings from monolingual corpora and dictionaries with translation pairs. However, for many languages, such resources are not available. Bergsma and Van Durme (2011) introduced an alternative idea, namely to learn bilingual word embeddings from image data collected via web image search. The idea behind their approach is to present words in a visual space and to find valid translations between words based on similarities between their visual representations. Representations of words in visual space are constructed by representing a word through a series of images associated with that word, i.e. the word is a semantic tag for the images in Set.Kiela et al. (2015) we achieve great performance by using the same set of images to represent the word by performing a task associated with that word."}, {"heading": "1.1 Contributions", "text": "We present a novel dataset for learning word embedding from images, which includes verbs as well as adjectives. In addition to looking at picture word representations by image groups, we are also experimenting with representations of Xiv: 170 9.05 914v 1 [cs.C L] 18 September 2017. However, our results suggest that none of the approaches relating to image data is directly applicable to learning cross-language representations of adjectives and verbs. In summary, our work is a negative outcome paper, which shows that a promising approach to learning bilingual dictionaries for languages with limited resources does not go beyond simple nouns."}, {"heading": "2 Data", "text": "Simlex-999 We use the Simlex-999 dataset of English word pairs (Hill et al., 2014) to compile the word lists for our experiments.The dataset contains words from three language ranges (nouns, adjectives and verbs) that comprise different levels of concreteness. We collect all the different words in the dataset and sample half of the words for each part of the language. The final word list comprises 557 English words (375 nouns, 116 verbs and 66 adjectives).We translate the word list into 5 languages (German, Danish, French, Russian, Italian) using the Yandex translation API1 and refine translations manually. 2image records With the Bing Image Search API3, we represent each word in a word list by a series of images, collecting the first 50 JPEG images returned by the search engine when retrieving the word. In this way, we compile image sets for 6 different languages, containing the image sets."}, {"heading": "3 Approach", "text": "To find the translation of a word, e.g. from English to German, we compare the images representing the English word with all images that 1https: / / translate.yandex.com / 2The Italian word list is compiled using the translated Simlex 999 data set (Leviant and Reichart, 2015). 3https: / / www.microsoft.com / cognitive-services / enus / bing-image-search-api 4We completely remove translation pairs where more than 10 images are translinguistically identical. Although identical images returned by the search engine are a strong indication that two words are translations of each other, we would like to eliminate the effect of possible linguistic indexing applied by the search engine internally."}, {"heading": "3.1 SIFT and Color Features", "text": "SIFT features (Lowe, 2004) are descriptors for points of interest in an image. By matching them with descriptor representations in a code book, images can be represented through a bag of code words from the code book (Sivic and Zisserman, 2003). We apply the optimal hyperparameters reported by Bergsma and Van Durme and generate a code book by grouping 20,000 code words from 430,000 randomly selected descriptors in the English image data. Color features are implemented as color histograms for the red, blue, and green channels, and as grayscale histograms."}, {"heading": "3.2 Convolutional Neural Network Feature Representations", "text": "According to Kiela et al. (2015), we calculate the properties of revolutionary neural networks (CNN) in an AlexNet (Krizhevsky et al., 2012) that was trained in advance of the ImageNet classification."}, {"heading": "3.3 Representations from Cross-Lingual Word-Embeddings", "text": "To investigate whether it is possible to learn cross-language representations of verbs and adjectives from image data, we also want to know how well text-based methods perform this task. Therefore, we extend our experiments to cross-language word embedding that is trained on text data. We use the multilingual word embedding provided by Ammar et al. (2016). [5] These word embedding results from first trained word embedding in a monolingual space. Subsequently, these representations are mapped into a common multilingual space, maximizing the similarity between representations that are translations. [5http: / / 128.2.220.95 / multilingual / data / Note] Since this cross-language presentation learning algorithm requires a dictionary in advance, it does not scale to the language scenario we are looking at here. Nevertheless, we include these embedding because it strengthens our negative result."}, {"heading": "3.4 Combined Representations", "text": "To obtain the combined representation of an image, we associate the CNN feature vector with the text-based representation of the associated image word. This representation is called COMBI.Since the CNN features have dimensionality 4096 and the word embeddings have dimensionality 40, we also calculate a version of the CNN features with reduced dimensionality (40 dimensions) using Main Component Analysis (VISPCA). We then create a combined representation by combining the 40-dimensional CNN feature vector with the 40-dimensional word embeddings, resulting in an 80-dimensional combined representation. This representation is called COMBIPCA."}, {"heading": "3.5 Similarity Computation", "text": "We implement several methods to determine similarities between the representations. Similarities between individual images Bergsma and Van Durme (2011) determine similarities between image sets based on similarities between all individual images. For each image in image set 1, the maximum similarity value is calculated for each image in image set 2. These maximum similarity values are then either averaged (AVGMAX) or taken to their maximum (MAXMAX). Similarities between aggregated representations In addition to the methods described above, Kiela et al. (2015) generate an aggregated representation for each image set and then calculate the similarity between image sets by calculating the similarity between the aggregated representations. Aggregated representations for image sets are translated into the set either by taking the component average (CNN-MEAN) or by the component maximum (CNN-MAX) of the images."}, {"heading": "3.6 Evaluation Metrics", "text": "The ranking performance is evaluated by calculating the Mean Reciprocal Rank (MRR) asMRR = 1M M \u2211 i = 11rank (ws, wt) M is the number of words to be translated and rank (ws, wt) is the position at which the correct translation wt is ranked for the source word ws. In addition to MRR, we also evaluate cross-linguistic representations using precision at k (P @ k). For the next adjacent approach, we can only calculate P @ 1."}, {"heading": "4 Experiments and Results", "text": "We perform experiments for 5 language pairs English-German, English-Danish, English-French, English-Russian and English-Italian. In the first experiment, we only look at the representations calculated from image data (SIFT + color and CNN characteristics) and compare the different methods for calculating similarity described in Section 3.5. For each English word, we classify all words in the corresponding target languages based on similarities between image sentences and evaluate the models \"ability to identify correct translations, i.e. to place the correct translation at a position near the top. We compare 4 settings that differ in the amount of translated English words. In the ALL setting, all English words in the wordlist are translated. NN, VB and ADJ refer to settings in which only nouns, verbs and adjectives are translated. In a second experiment, we apply the most effective similarity calculation method to visual representations (AVAGMX) and the visual textual settings combined with the GLOGR model and the representations."}, {"heading": "4.1 Results", "text": "We present the results for the comparison between different similarity calculation methods for purely visual representations in Tables 1 and 2. Figures 2 and 3 show the results for the performance of AVGMAX and LOGREGR on different representations. Comparison of similarity calculation methods on visual representations Table 1 shows results for images represented by CNN features averaged over all language pairs. Table 2 shows results for images represented by SIFT and color features averaged over all language pairs. First, we note that our results for the translation of nouns with CNN features are slightly lower than those reported in previous work (Kiela et al. (2015) P @ 1 = 0.567 for a data set containing 500 nouns, which may be due to a higher proportion of more abstract nouns in our data set. Second, our experiments confirm that the CNN features exceed the color and SIFT features of PCs by a large distance when it comes to the results of keels in line with the results of keels."}, {"heading": "4.2 Analysis", "text": "One possible explanation is that the images associated with verbs and adjectives are less suitable to represent the meaning of a concept than images associated with nouns.Kiela et al. (2015), however, assume that the lexicon induction via image similarity has a lower image quality than the images containing words that are more abstract. To approximate the degree of abstraction of a concept, they calculate the image scattering d for a word w than the average cosmic distance between all image pairs in the image set {ij,.} associated with the word w (w) = 2 n, they calculate the image scattering d for a word w for a word w than the average cosmic distance between all image pairs. (2015) we find that their model is worse on Dataset al. (2015) that their model is worse on Dataset al."}, {"heading": "4.3 Discussion", "text": "The analysis described in the previous section provides some insights into why learning translations of adjectives and verbs from image search engine search results works poorly. First, as soon as we move away from concrete nouns such as the bicycle, the images returned by the search engine are diverse, and the image sets collected are highly dispersed; the images often reveal concepts related to the words searched for, rather than the concepts behind the words themselves. While this problem appears to be inherent in the process of learning verbs and adjectives from image data representations in general, our analysis also reveals problems related to our specific approach, which is to rely on an image search engine to provide us with resources from which we can learn. In many cases, the search engine does not capture the intended meaning of the query word, which leads to search results that are completely independent of the images of the query translation of the query, which are not associated with the search engine itself, and it is not clear how to increase the search engine's specific and specific adjectives."}, {"heading": "5 Conclusion", "text": "We have shown that existing work on learning cross-lingual word representations from images obtained via web image search is not transferred to other parts of the language as nouns. Even with supervised learning from image characteristics, we are not able to achieve positive results for verbs and adjectives. In our analysis, we identify dependence on the image search engine as one of the main problems of the approach when applying it to adjectives and verbs. Therefore, we plan to switch to another resource in future work to collect image data."}], "references": [{"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A. Smith."], "venue": "arXiv preprint arXiv:1602.01925 .", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Learning Bilingual Lexicons using the Visual Similarity of Labeled Web Images", "author": ["Shane Bergsma", "Benjamin Van Durme."], "venue": "IJCAI. Barcelona, Spain, pages 1764\u20131769.", "citeRegEx": "Bergsma and Durme.,? 2011", "shortCiteRegEx": "Bergsma and Durme.", "year": 2011}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what I mean", "author": ["Felix Hill", "Anna Korhonen."], "venue": "EMNLP. Doha, Qatar, pages 255\u2013265.", "citeRegEx": "Hill and Korhonen.,? 2014", "shortCiteRegEx": "Hill and Korhonen.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1408.3456 .", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims."], "venue": "SIGKDD. ACM, New York, NY, USA, pages 133\u2013142.", "citeRegEx": "Joachims.,? 2002", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Douwe Kiela", "L\u00e9on Bottou."], "venue": "EMNLP. Doha, Qatar, pages 36\u201345.", "citeRegEx": "Kiela and Bottou.,? 2014", "shortCiteRegEx": "Kiela and Bottou.", "year": 2014}, {"title": "Improving multi-modal representations using image dispersion: Why less is sometimes more", "author": ["Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark."], "venue": "ACL. Association for Computational Linguistics, Baltimore, Maryland, pages 835\u2013841.", "citeRegEx": "Kiela et al\\.,? 2014", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "author": ["Douwe Kiela", "Ivan Vulic", "Stephen Clark."], "venue": "EMNLP. Lisbon, Portugal, pages 148\u2013158.", "citeRegEx": "Kiela et al\\.,? 2015", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "NIPS. Lake Tahoe, Nevada, United States, pages 1106\u20131114.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "NAACL. Denver, Colorado, USA, pages 153\u2013163.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics", "author": ["Ira Leviant", "Roi Reichart."], "venue": "arXiv preprint arXiv:1508.00106 .", "citeRegEx": "Leviant and Reichart.,? 2015", "shortCiteRegEx": "Leviant and Reichart.", "year": 2015}, {"title": "A strong baseline for learning cross-lingual word embeddings from sentence alignments", "author": ["Omer Levy", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "EACL.", "citeRegEx": "Levy et al\\.,? 2017", "shortCiteRegEx": "Levy et al\\.", "year": 2017}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["David G. Lowe."], "venue": "International Journal of Computer Vision 60(2):91\u2013110.", "citeRegEx": "Lowe.,? 2004", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Visually grounded meaning representations", "author": ["Carina Silberer", "Vittorio Ferrari", "Mirella Lapata."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence To appear.", "citeRegEx": "Silberer et al\\.,? 2016", "shortCiteRegEx": "Silberer et al\\.", "year": 2016}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "Andrew Zisserman."], "venue": "null. IEEE, page 1470.", "citeRegEx": "Sivic and Zisserman.,? 2003", "shortCiteRegEx": "Sivic and Zisserman.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "Typically, cross-lingual word representations are learned from word alignments, sentence alignments, or, more rarely, from aligned, comparable documents (Levy et al., 2017).", "startOffset": 153, "endOffset": 172}, {"referenceID": 0, "context": "Ammar et al. (2016) propose a method that learns multilingual word-embeddings from monolingual corpora and dictionaries containing translation pairs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).", "startOffset": 88, "endOffset": 204}, {"referenceID": 2, "context": "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).", "startOffset": 88, "endOffset": 204}, {"referenceID": 5, "context": "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).", "startOffset": 88, "endOffset": 204}, {"referenceID": 9, "context": "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).", "startOffset": 88, "endOffset": 204}, {"referenceID": 13, "context": "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).", "startOffset": 88, "endOffset": 204}, {"referenceID": 6, "context": "We introduce a novel dataset for learning crosslingual word embeddings from images, and we evaluate the approaches by Bergsma and Van Durme (2011) and Kiela et al. (2015) on this data set, which apart from nouns includes both adjectives and verbs.", "startOffset": 151, "endOffset": 171}, {"referenceID": 3, "context": "Simlex-999 We use the Simlex-999 data set of English word pairs (Hill et al., 2014) to compile the word lists for our experiments.", "startOffset": 64, "endOffset": 83}, {"referenceID": 10, "context": "com/ The Italian word list is compiled using the translated Simlex-999 data set (Leviant and Reichart, 2015).", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": "Following previous work, we obtain two different feature representations for each image, color and SIFT features as proposed by Bergsma and Van Durme (2011) and convolutional network features as proposed by Kiela et al. (2015).", "startOffset": 207, "endOffset": 227}, {"referenceID": 12, "context": "SIFT features (Lowe, 2004) are descriptors for points of interest in an image.", "startOffset": 14, "endOffset": 26}, {"referenceID": 14, "context": "with descriptor representations in a codebook, images can be represented by a bag-of-codewords from the codebook (Sivic and Zisserman, 2003).", "startOffset": 113, "endOffset": 140}, {"referenceID": 8, "context": "For each image, we extract its pre-softmax layer representation in an AlexNet (Krizhevsky et al., 2012) pre-trained on the ImageNet classification task.", "startOffset": 78, "endOffset": 103}, {"referenceID": 6, "context": "Following Kiela et al. (2015), we compute convolutional neural network (CNN) feature representations.", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "We use the pre-trained multilingual word-embeddings provided by Ammar et al. (2016).5 These wordembeddings are generated by first training wordembeddings in a monolingual space.", "startOffset": 64, "endOffset": 84}, {"referenceID": 6, "context": "Similarities Between Aggregated Representations In addition to the above described methods, Kiela et al. (2015) generate an aggregated representation for each image set and then compute the similarity between image sets by comput-", "startOffset": 92, "endOffset": 112}, {"referenceID": 6, "context": "First, we observe that our results for translating nouns using CNN features are slightly lower than those reported in previous work (Kiela et al. (2015) report P@1 = 0.", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": "In their analysis, Kiela et al. (2015) find that their model performs worse on datasets with a higher average image dispersion.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "Cross-lingual representation learning is an important step in making NLP scale to all the world\u2019s languages. Recent work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused on the translation of selected nouns only. In our work, we investigate whether the meaning of other parts-of-speech, in particular adjectives and verbs, can be learned in the same way. We also experiment with combining the representations learned from visual data with embeddings learned from textual data. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.", "creator": "LaTeX with hyperref package"}}}