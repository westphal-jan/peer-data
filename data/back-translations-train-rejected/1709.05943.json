{"id": "1709.05943", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Fast YOLO: A Fast You Only Look Once System for Real-time Embedded Object Detection in Video", "abstract": "Object detection is considered one of the most challenging problems in this field of computer vision, as it involves the combination of object classification and object localization within a scene. Recently, deep neural networks (DNNs) have been demonstrated to achieve superior object detection performance compared to other approaches, with YOLOv2 (an improved You Only Look Once model) being one of the state-of-the-art in DNN-based object detection methods in terms of both speed and accuracy. Although YOLOv2 can achieve real-time performance on a powerful GPU, it still remains very challenging for leveraging this approach for real-time object detection in video on embedded computing devices with limited computational power and limited memory. In this paper, we propose a new framework called Fast YOLO, a fast You Only Look Once framework which accelerates YOLOv2 to be able to perform object detection in video on embedded devices in a real-time manner. First, we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To further reduce power consumption on embedded devices while maintaining performance, a motion-adaptive inference method is introduced into the proposed Fast YOLO framework to reduce the frequency of deep inference with O-YOLOv2 based on temporal motion characteristics. Experimental results show that the proposed Fast YOLO framework can reduce the number of deep inferences by an average of 38.13%, and an average speedup of ~3.3X for objection detection in video compared to the original YOLOv2, leading Fast YOLO to run an average of ~18FPS on a Nvidia Jetson TX1 embedded system.", "histories": [["v1", "Mon, 18 Sep 2017 13:57:16 GMT  (2564kb)", "http://arxiv.org/abs/1709.05943v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["mohammad javad shafiee", "brendan chywl", "francis li", "alexander wong"], "accepted": false, "id": "1709.05943"}, "pdf": {"name": "1709.05943.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mohammad Javad Shafiee"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 9.05 943v 1 [cs.C V] 18 September 2017 Fast YOLO: A Fast You Only Look Once System for Real-time Embedded Object Detection in VideoObject detection is considered one of the most difficult problems in this field of computer visualization because it involves combining object classification and object localization within a scene. Recently, deep neural networks (DNNs) have been shown to achieve superior object detection performance in terms of both speed and accuracy compared to other approaches, with YOLOv2 (an improved You Only Look Oncemodel) being one of the most advanced methods of DNN-based object detection. Although YOLOv2 can achieve real-time performance on a powerful GPU, it still remains a major challenge for using this approach to real-time object detection in YOLO systems to take advantage of fast computing power and limited memory."}, {"heading": "1 Introduction", "text": "The goal of object detection is to locate different objects in a scene and map labels to the surrounding fields. [1, 2] The most common approach to tackling this problem is to designate existing systemicians to map labels to boxes in a scene. For example, a standard detection window approach can be used where a classifier determines the existence of an object and its associated labeling for all possible windows in the scene. However, this type of approach has significant limitations in terms of not only high computer-related complexity, but also high detection error rates. Recently, deep neural networks (DNNs) have demonstrated superior performance in a number of different applications [3, 4] with object detection being one of the key areas where DNNs have significantly outperformed existing approaches."}, {"heading": "2 Methodology", "text": "The proposed Fast YOLO framework is divided into two main components: i) optimized YOLOv2 architecture and ii) motion adaptive inference (see Figure 1). For each video image, a stack of images consisting of the video image with a reference image is passed into a 1 x 1 revolutionary layer. The result of the revolutionary layer is a motion probability map, which is then fed into a motion adaptive inference module to determine whether deep inference is required to calculate an updated class probability map. As mentioned in the introduction, the main goal is to introduce an object recognition framework in the video that can work faster on embedded devices while reducing resource consumption, which in turn significantly reduces power consumption."}, {"heading": "2.1 Optimized Network Architecture", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2.2 Motion-adaptive Inference", "text": "In order to further reduce the power consumption of the processor unit for the purpose of embedded object detection in the video, we take advantage of the fact that not all captured video images contain clear information and therefore deep inferences do not need to be made on all images. As such, we introduce a motion-adaptive inference approach to determine whether deep inferences are required for a particular video image. By making deep inferences using the O-YOLOv2 network introduced in the previous section, this motion-adaptive inference technique can help the frame reduce the need for computing resources, leading to a significant reduction in the power consumption of the system as well as an increase in processing speed. The motion-adaptive inference process can be seen in Figure 1. Each frame is stacked with the reference frame Iref to form a stack of images. A 1 \u00d7 1 convolutions layer is then applied to the stack of images to generate a motion card."}, {"heading": "3 Results & Discussion", "text": "The proposed Fast YOLO Framework is evaluated using two different strategies: First, we evaluate the modeling accuracy and performance of the optimized YOLOv2 (i.e. O-YOLOv2) network architecture with the original YOLOv2 network architecture on the Pascal VOC 2007 dataset [19] to demonstrate the effectiveness of the optimization process of the network architecture. Table 1 shows the architecture and performance comparisons between O-YOLOv2 and the original YOLOv2 architecture on the Pascal VOC dataset. It can be noted that the proposed Fast YOLO Framework, O-YOLOv2, and the original YOLOv2 network architecture is 2.8x smaller than the original YOLOv2 with only a 2% decrease in IOU, which would have little impact on the real video-based object recognition applications. Second, the proposed Fast YOLO Framework, O-YOLOv2, and the original YOLOv2 framework can be evaluated from an average Y56 seconds of the YOLO frame speed to the average X1."}, {"heading": "4 Conclusion", "text": "In this article, we introduced Fast YOLO, a new framework for real-time detection of embedded objects in video. Although YOLOv2 is considered a state-of-the-art framework with real-time inferences on powerful GPUs, it is not possible to use it on embedded devices in real-time. In this context, we are using the evolutionary Deep Intelligence Framework to create an optimized network architecture based on YOLOv2. The optimized network architecture is used within a motion-adaptable inference frame to speed up the detection process and reduce the energy consumption of the embedded device. Experimental results showed that the proposed Fast YOLO Framework can achieve an average runtime that is 3.3x faster than the original YOLOv2, can reduce the number of deep inferences by an average of 38.13%, and has a network architecture that is 2.8x more compact."}, {"heading": "Acknowledgments", "text": "The authors thank NSERC, the Canada Research Chairs Program, Nvidia and DarwinAI."}], "references": [{"title": "Rapid object detection using a boosted cascade of simple features Computer Vision and Pattern Recognition", "author": ["P. Viola", "M. Jones"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "An extended set of haar-like features for rapid object detection", "author": ["R. Lienhart", "J. Maydt"], "venue": "International Conference on Image Processing (ICIP)(2002)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks Advances in neural information processing systems (NIPS)(2012)", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition arXiv preprint arXiv:1409.1556(2014)", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation. Computer Vision and Pattern Recognition", "author": ["Girshick R", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Faster R- CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems", "author": ["Ren S", "K. He", "R. Girshick", "J. Sun"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "YOLO9000: better, faster, stronger. Computer Vision and Pattern Recognition", "author": ["J. Redmon", "A. Farhadi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J. Denker", "S. Solla", "R. Howard", "L. Jackel"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Training cnns with low-rank filters for efficient image classification", "author": ["Y. Ioannou", "D. Robertson", "J. Shotton", "R. Cipolla", "A. Criminisi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sparse convolutional neural networks. Computer Vision and Pattern Recognition", "author": ["B. Liu", "M. Wang", "H. Foroosh", "M. Tappen", "M. Pensky"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning structured sparsity in deep neural networks. Advances in neural information processing systems", "author": ["W. Wen", "C. Wu", "Y. Wang", "Y. Chen", "H. Li"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Deep learning with Darwin: Evolutionary synthesis of deep neural networks", "author": ["M.J. Shafiee", "A. Mishra", "Wong A"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic Cluster-driven Genetic Encoding. Advances in neural information processing systems workshop", "author": ["M.J. Shafiee", "Wong A"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution of deep neural networks", "author": ["M.J. Shafiee", "E. Barshan", "Wong A"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Multiple Object Tracking using K-Shortest Paths Optimization", "author": ["J. Berclaz", "F. Fleuret", "E. Turetken", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence(2011)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Object detection [1, 5] is one of the most challenging problems in this field of computer vision.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "Object detection [1, 5] is one of the most challenging problems in this field of computer vision.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "The most common approach [1, 2] to tackling this problem is to re-purpose existing trained classifiers to assign labels to bounding boxes in a scene.", "startOffset": 25, "endOffset": 31}, {"referenceID": 1, "context": "The most common approach [1, 2] to tackling this problem is to re-purpose existing trained classifiers to assign labels to bounding boxes in a scene.", "startOffset": 25, "endOffset": 31}, {"referenceID": 0, "context": "For example, a standard sliding window approach [1] can be used where a classifier determines the existence of an object and its associated label for all possible windows in the scene.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "Recently, deep neural networks (DNNs) have shown superior performance in a range of different applications [3, 4], with object detection being one of the key areas where DNNs have significantly outperformed existing approaches.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "Recently, deep neural networks (DNNs) have shown superior performance in a range of different applications [3, 4], with object detection being one of the key areas where DNNs have significantly outperformed existing approaches.", "startOffset": 107, "endOffset": 113}, {"referenceID": 4, "context": "For example, in the Region-CNN (R-CNN) [5] approach, a CNN architecture is used to generate bounding box proposals in an image instead of a sliding window approach, and thus a classifier only perform classification on bounding box proposals.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "More recently, a You Only Look Once (YOLO) object detection approach [6] was proposed that mitigated the computational complexity issues associated with R-CNN by formulating the object detection problem as a single regression problem, where bounding box coordinates and class probabilities are computed at the same time.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": ", 45 frames per second on a Nvidia Titan-X GPU), it was also shown that the localization error of YOLO is significantly higher than more recent R-CNN variants such as Faster R-CNN [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Motivated by the improvement of Faster R-CNN via the anchor proposal, Redmon and Farhadi [8] proposed an improved YOLO method (named YOLOv2) where anchor boxes are used to predict bounding boxes.", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "First, motivated by the promising results demonstrated by Shafiee et al [16, 17, 18], we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has significantly fewer parameters while maintaining strong detection performance.", "startOffset": 72, "endOffset": 84}, {"referenceID": 15, "context": "First, motivated by the promising results demonstrated by Shafiee et al [16, 17, 18], we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has significantly fewer parameters while maintaining strong detection performance.", "startOffset": 72, "endOffset": 84}, {"referenceID": 16, "context": "First, motivated by the promising results demonstrated by Shafiee et al [16, 17, 18], we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has significantly fewer parameters while maintaining strong detection performance.", "startOffset": 72, "endOffset": 84}, {"referenceID": 8, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 9, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 10, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 11, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 12, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 13, "context": "As such, rather than leveraging a hyper-parameter optimization approach to obtaining an optimal network architecture based on YOLOv2, we instead take advantage of network optimization strategies designed specifically for improving network efficiency [9, 10, 11, 12, 13, 14, 15].", "startOffset": 250, "endOffset": 277}, {"referenceID": 14, "context": "In particular, we leverage the evolutionary deep intelligence framework [16, 17, 18] to optimize the network architecture to synthesize a deep neural network that", "startOffset": 72, "endOffset": 84}, {"referenceID": 15, "context": "In particular, we leverage the evolutionary deep intelligence framework [16, 17, 18] to optimize the network architecture to synthesize a deep neural network that", "startOffset": 72, "endOffset": 84}, {"referenceID": 16, "context": "In particular, we leverage the evolutionary deep intelligence framework [16, 17, 18] to optimize the network architecture to synthesize a deep neural network that", "startOffset": 72, "endOffset": 84}, {"referenceID": 14, "context": "In the evolutionary deep intelligence framework [16, 17, 18], the architectural traits of a deep neural network is modeled via a probabilistic genetic encoding modeling strategy.", "startOffset": 48, "endOffset": 60}, {"referenceID": 15, "context": "In the evolutionary deep intelligence framework [16, 17, 18], the architectural traits of a deep neural network is modeled via a probabilistic genetic encoding modeling strategy.", "startOffset": 48, "endOffset": 60}, {"referenceID": 16, "context": "In the evolutionary deep intelligence framework [16, 17, 18], the architectural traits of a deep neural network is modeled via a probabilistic genetic encoding modeling strategy.", "startOffset": 48, "endOffset": 60}, {"referenceID": 17, "context": "Second, the proposed Fast YOLO framework, O-YOLOv2, and original YOLOv2 are evaluated in terms of average run-time on a Nvidia Jetson TX1 embedded system on a video from [20].", "startOffset": 170, "endOffset": 174}], "year": 2017, "abstractText": "Object detection is considered one of the most challenging problems in this field of computer vision, as it involves the combination of object classification and object localization within a scene. Recently, deep neural networks (DNNs) have been demonstrated to achieve superior object detection performance compared to other approaches, with YOLOv2 (an improved You Only Look Oncemodel) being one of the state-of-the-art in DNN-based object detection methods in terms of both speed and accuracy. Although YOLOv2 can achieve real-time performance on a powerful GPU, it still remains very challenging for leveraging this approach for real-time object detection in video on embedded computing devices with limited computational power and limited memory. In this paper, we propose a new framework called Fast YOLO, a fast You Only Look Once framework which accelerates YOLOv2 to be able to perform object detection in video on embedded devices in a realtime manner. First, we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has 2.8X fewer parameters with just a \u223c2% IOU drop. To further reduce power consumption on embedded devices while maintaining performance, a motion-adaptive inference method is introduced into the proposed Fast YOLO framework to reduce the frequency of deep inference with O-YOLOv2 based on temporal motion characteristics. Experimental results show that the proposed Fast YOLO framework can reduce the number of deep inferences by an average of 38.13%, and an average speedup of \u223c3.3X for objection detection in video compared to the original YOLOv2, leading Fast YOLO to run an average of \u223c18FPS on a Nvidia Jetson TX1 em-", "creator": "LaTeX with hyperref package"}}}