{"id": "1709.05976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Leveraging Distributional Semantics for Multi-Label Learning", "abstract": "We present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using Distributional Semantics). Our approach draws inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings for natural language processing tasks. Learning such embeddings can be reduced to a certain matrix factorization. Our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data. The framework can also be easily extended to incorporate auxiliary information such as label-label correlations; this is crucial especially when there are a lot of missing labels in the training data. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning.", "histories": [["v1", "Mon, 18 Sep 2017 14:34:16 GMT  (26kb)", "http://arxiv.org/abs/1709.05976v1", "9 Pages, 0 Figures"]], "COMMENTS": "9 Pages, 0 Figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["rahul wadbude", "vivek gupta", "piyush rai", "nagarajan natarajan", "harish karnick"], "accepted": false, "id": "1709.05976"}, "pdf": {"name": "1709.05976.pdf", "metadata": {"source": "CRF", "title": "Leveraging Distributional Semantics for Multi-Label Learning", "authors": ["Rahul Wadbude", "Vivek Gupta", "Piyush Rai", "Nagarajan Natarajan", "Harish Karnick"], "emails": ["warahul@iitk.ac.in", "t-vigu@microsoft.com", "piyush@iitk.ac.in", "t-nanata@microsoft.com", "hk@iitk.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.05 976v 1 [cs.L G] 18 Sep 20"}, {"heading": "Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Problem Formulation and Background", "text": "In the standardized multi-label learning formulation, the learning algorithm is given as a set of learning instances, both of which are defined on the basis of learning content. (...) In the real world, learning content is used in different learning systems that do not contain all relevant terms. (...) In the real world, learning content is used in different learning systems. (...) In the real world, learning content is not always considered irrelevant. (...) In addition, we may have access to terms in which the terms are relevant or irrelevant. (...) In the real world, learning content is not considered irrelevant. (...) In the real world, learning content is referred to by labels and vectors. (...) In addition, we may have access to label information designated by C-ZL +. (e.g. the number of times labels occur together, such as the Wikipedia corpus).The goal of multi-label learning is to learn a vector function (evaluated)."}, {"heading": "Embedding label vectors", "text": "We now derive the analog embedding technology for multi-label learning. However, a simple model is to treat each instance as a \"word\"; we define the \"context\" as the k-nearest neighbor of a given instance in space formed by the training label vectors, with cosinal similarity as a metric. We then arrive at a goal identical to (2) for the learning embedding z1, z2,.., zn for the instances x1, x2,., xn or: max z1, z2,..., znn. \"(1), znn,\" zn. (1), zn. (2), zn. (2), zn. (2), zn. (2), zn. (2), zn. (n.)."}, {"heading": "Using label correlations", "text": "In various practical use examples of natural language processing, common models are developed for embedding text documents and single words in a corpus (Dai, Olah and Le 2015) < < < < < < < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "SLEEC.", "text": "In fact, most of them are able to put themselves in another world, in another world, in another world, in another world, in another world."}, {"heading": "Learning and Knowledge Discovery in Databases 50\u201365.", "text": "[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111- 3119. [Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Proceedings of the 20th ACM, 1532-1543. [Prabhu and Varma 2014] Prabhu, Y., and Varma, M. 2014. Fastxml: A fast, precise and stable tree-classifier for word representation. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 263-272. ACM."}, {"heading": "Leveraging Distributional Semantics for Multi-Label Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A1. Joint Embedding and Regression", "text": "O t + 1 i = O t + 2 x \u2212 Z \u2212 Z \u2212 Z \u2212 Z = V xi (< zi, zj >) + n \u2212 Z \u2212 Z (V xi, zj >) + n \u2212 Z (V xi, zj >) (V xi, zj \u00b2) (V xi, zj \u00b2) (V < zi, zj \u00b2) (V, zj \u00b2)) (V xi, zj >) + n \u2212 Z (V xi, zj >) (V xi, zj \u00b2) (V xi, zj \u00b2) (V xi \u2212 Z) (V xi \u2212 Z) (Z xi \u2212 Z) (Z xi \u2212 Z) (Z xi, V \u2212 Z) (Z xi, V \u2212 Z) (Z) (Z) (Z) (Z) (Z) (Z) (Z) (Z) (Z) (V xi \u2212 V (Z) (Z) (V) (V \u2212 V) (Z) (Z) (V) (Z) (V \u2212 V (Z) (Z) (Z) (V) (Z) (Z) (V \u2212 Z) (Z) (V \u2212 Z) (Z) (V \u2212 Z) (Z) (Z) (V \u2212 Z) (Z) (Z) (V \u2212 Z) (Z) (V \u2212 Z) (Z) (Z) (V \u2212 Z) (V \u2212 Z (Z) (Z) (V \u2212 Z (Z) (V \u2212 V \u2212 Z (Z) (Z) (V \u2212 V (Z) (V \u2212 V (Z) (Z) (V \u2212 V (Z) (V \u2212 Z) (V (Z) (Z) (V \u2212 Z) (V (V \u2212 Z) (V (V \u2212 Z) (Z) (Z) (V \u2212 V (Z) (Z) (V \u2212 Z) (V (V \u2212 Z)) (V \u2212 V (Z) (V \u2212 V (Z)) (V (V \u2212 Z) (V (Z)) (V (V (V \u2212 Z)) (V (Z) (V (V (V \u2212 Z))) (V (V (V (V \u2212 Z))) (V (V (V (V (Z)))) (V (V"}, {"heading": "A2. SGNS Objective as Implicit SPPMI factorization", "text": "The SGNS (Mikolov et al. 2013) target is as follows: Oi = jj = jj = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j = Implemented j # Implemented # Implemented # # Implemented # Implemented # Implemented # Implemented # Implemented # Implemented # Implemented # # Implemented # # Implemented # # Implemented # # Implemented # # # Implemented # # Implemented # # # Implemented # # # Implemented # # Implemented # # # Implemented # # Implemented # # # Implemented # # Implemented # # # Implemented # # Implemented # # # Implemented # # # Implemented # # # Implemented # # # # Implemented # # # Implemented # # # Implemented #"}], "references": [{"title": "Dismec: Distributed sparse machines for extreme multi-label classification", "author": ["Babbar", "R. Sch\u00f6lkopf 2017] Babbar", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Babbar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Babbar et al\\.", "year": 2017}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Bhatia"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatia,? \\Q2015\\E", "shortCiteRegEx": "Bhatia", "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Eckstein,? \\Q2011\\E", "shortCiteRegEx": "Eckstein", "year": 2011}, {"title": "Document embedding with paragraph vectors", "author": ["Olah Dai", "A.M. Le 2015] Dai", "C. Olah", "Q.V. Le"], "venue": "arXiv preprint arXiv:1507.07998", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications", "author": ["Prabhu Jain", "H. Varma 2016] Jain", "Y. Prabhu", "M. Varma"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Tsoumakas Katakis", "I. Vlahavas 2008] Katakis", "G. Tsoumakas", "I. Vlahavas"], "venue": "ECML PKDD discovery challenge", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q.V. Mikolov 2014] Le", "T. andMikolov"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "O. Goldberg 2014] Levy", "Y. Goldberg"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis"], "venue": "Journal of machine learning research 5(Apr):361\u2013397", "citeRegEx": "Lewis,? \\Q2004\\E", "shortCiteRegEx": "Lewis", "year": 2004}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain. Machine Learning and Knowledge Discovery in Databases 50\u201365", "author": ["Loza Menc\u00eda", "E. F\u00fcrnkranz 2008] Loza Menc\u00eda", "J. F\u00fcrnkranz"], "venue": null, "citeRegEx": "Menc\u00eda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Menc\u00eda et al\\.", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Prabhu", "Y. Varma 2014] Prabhu", "M. Varma"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Prabhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prabhu et al\\.", "year": 2014}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Snoek"], "venue": "In Proceedings of the 14th ACM international conference on Multimedia,", "citeRegEx": "Snoek,? \\Q2006\\E", "shortCiteRegEx": "Snoek", "year": 2006}, {"title": "Effective and efficient multilabel classification in domains with large number of labels", "author": ["Katakis Tsoumakas", "G. Vlahavas 2008] Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data", "citeRegEx": "Tsoumakas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2008}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning 81(1):21\u201335", "author": ["Bengio Weston", "J. Usunier 2010] Weston", "S. Bengio", "N. Usunier"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification", "author": ["Yen"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yen,? \\Q2016\\E", "shortCiteRegEx": "Yen", "year": 2016}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yu,? \\Q2014\\E", "shortCiteRegEx": "Yu", "year": 2014}, {"title": "Deep extreme multi-label learning", "author": ["Zhang"], "venue": "arXiv preprint arXiv:1704.03718", "citeRegEx": "Zhang,? \\Q2017\\E", "shortCiteRegEx": "Zhang", "year": 2017}, {"title": "SGNS Objective as Implicit SPPMI factorization The SGNS", "author": ["\u03b7\u2207V Oi A"], "venue": "(Mikolov et al", "citeRegEx": "A2.,? \\Q2013\\E", "shortCiteRegEx": "A2.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using Distributional Semantics). Our approach draws inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings for natural language processing tasks. Learning such embeddings can be reduced to a certain matrix factorization. Our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data. The framework can also be easily extended to incorporate auxiliary information such as label-label correlations; this is crucial especially when there are a lot of missing labels in the training data. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning.", "creator": "LaTeX with hyperref package"}}}