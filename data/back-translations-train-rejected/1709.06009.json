{"id": "1709.06009", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "abstract": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.", "histories": [["v1", "Mon, 18 Sep 2017 15:32:57 GMT  (1143kb,D)", "http://arxiv.org/abs/1709.06009v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marlos c machado", "marc g bellemare", "erik talvitie", "joel veness", "matthew hausknecht", "michael bowling"], "accepted": false, "id": "1709.06009"}, "pdf": {"name": "1709.06009.pdf", "metadata": {"source": "CRF", "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "authors": ["Marlos C. Machado", "Marc G. Bellemare", "Erik Talvitie", "Joel Veness", "Matthew Hausknecht", "Michael Bowling"], "emails": ["machado@ualberta.ca", "bellemare@google.com", "erik.talvitie@fandm.edu", "aixi@google.com", "matthew.hausknecht@microsoft.com", "mbowling@ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "The mentioned hsci-eaJnlhsrdcnlhsAeaeaeetln rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf the rf the rf the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf the rf the rf the rf the rf the rf the rf\u00fc the rf the rf\u00fc the rf the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the r"}, {"heading": "2. Background", "text": "In this section we present the formalism behind reinforcement learning (Sutton & Barto, 1998) and how it is instantiated in the arcade learning environment, as well as the two most common representations of value functions used in reinforcement learning for Atari 2600 games: linear approximation and neural networks; as convention, we refer to scalar random variables by uppercase letters (e.g. St, Rt), vectors by bold lowercase letters (e.g. \u03b8, \u03c6), functions by non-bold lowercase letters (e.g. v, q), and sets with a calligraphic font (e.g. S, A)."}, {"heading": "2.1 Setting", "text": "We look at an agent who interacts with his environment in a sequential manner with the aim of maximizing cumulative reward. It is often assumed that the environment fulfills the Markov property and is modeled as a Markov decision process (MDP). An MDP is formally called a 4-tuple < S, p, r >. Starting from a state in which the agent initiates an action, he undertakes an action to which the environment reacts with a state in which the probability of a transition from a kernel p (s, a) to a point (St + 1 = s)."}, {"heading": "2.2 Control in the Arcade Learning Environment", "text": "The typical goal of enhanced learning (RL) algorithms is to learn a policy of approximate learning: S \u00b7 A \u2192 [0, 1], which maps each state to a probability distribution of actions. Ideally, after the policy is learned, the discounted cumulative sum of rewards is maximized. [2] Many RL algorithms achieve this by using an action-value function q\u03c0: S \u00b7 A \u2192 R, which maximizes the long-term value of action in a state s and then the following policies thereafter. [3] More precisely, q\u03c0 (s, a). = E [4], [5], i \u2212 1Rt + i, St = s, At = a], the expected discounted sum of rewards for some discount factors. [0, 1] where the expectation lies both over politics and over the probability category p. In the ALE, however, it is not feasible to learn an individual value for each state based on the large number of possible states."}, {"heading": "3. Divergent Evaluation Methodologies in the ALE", "text": "Since its introduction as a platform for assessing general competence in the field of artificial intelligence, ALE has received considerable attention. Hundreds of papers have used ALE as a test bed and used many different experimental protocols to evaluate agents. Unfortunately, these different assessment protocols are often not carefully differentiated, which makes direct comparisons difficult or misleading. In this section, we will discuss a number of methodological differences that have emerged in the literature. In the following sections, we will focus in particular on two particularly important methodological questions: 1) different metrics for summarizing agent performance and 2) different mechanisms for injecting stochasticity in the environment. The discussion about the divergence of assessment protocols and the need for their standardization first took place at the AAAI workshop on learning general competence in video games. One of the reasons why authors compare the results with different experimental protocols is the high computational effort required to evaluate algorithms in ALE - it may be difficult to compare existing approaches to ensure that the results are consistent."}, {"heading": "3.1 Methodological Differences", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4. Summarizing Learning Performance", "text": "A traditional goal in reinforcement learning is for agents to continually improve their performance when they receive more data (Wilson, 1985; Thrun & Mitchell, 1993; Ring, 1997; Singh et al., 2004; Hutter, 2005; Sutton et al., 2011). Measuring the extent to which this is the case for a particular agent can be a challenge, and this challenge is exacerbated in the arcade learning environment, where the agent is rated over 60 games. If one evaluates an agent in only a few problems, it is common practice to draw learning curves that provide a rich description of the agent's performance: how fast he learns, the highest performance he achieves, the stability of his solutions, whether they are likely to improve with further data, etc. While some results in ALE have been reported on learning curves (e.g. Mnih et al. 2016; Ostrovski et al. 2017; Schaul et al. 2016), it is difficult to present oneself effectively, let alone literally understand and compare learning curves related to this Mnih et al."}, {"heading": "4.1 Common Performance Measures", "text": "Here we discuss some common summary statistics of learning performance that were typically used in the Arcade Learning Environment to improve the learning experience.Evaluation after learning. In the first ALE benchmark results, Bellemare et al. (2013) trained agents for a fixed training period, then rated the policy learned with the average score in a series of evaluation episodes without learning. Of course, a number of subsequent studies used this evaluation protocol (e.g., Defazio and Graepel, 2013; Liang et al., 2016). One drawback of this approach is that it hides issues of sample efficiency, since agents are not evaluated throughout the training period. An agent can get a high score without continuously improving his performance. For example, an agent could spend his training time in a purely exploratory mode, gathering information, but performing poorly, and then switch to an exploitative mode during evaluation."}, {"heading": "4.2 Proposal: Performance During Training", "text": "The performance metric that we propose as the standard is simple and has been introduced before (e.g. Bellemare et al. 2012). At the end of the training (and ideally at other points), the average performance of the last k episodes is reported. This protocol does not use an explicit evaluation phase, which means that an agent must perform well during the learning process, which is more consistent with the goal of continuous learning and simplifies the experimental methodology at the same time. Unstable methods that exhibit spying and / or crashes of the learning curves are rated poorly compared to those that improve steadily and continuously, even if they perform well during most of the training. Another advantage is that this metric is well suited for analyzing the sample efficiency of an algorithm. While the performance of the agent at the end of the training is generally of great interest, it is easy to report the same statistics at different points during the training."}, {"heading": "5. Determinism and Stochasticity in the Arcade Learning Environment", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5.1 The Brute", "text": "Brute is an algorithm designed to exploit the features of the rigorous arcade learning environment. Although it was developed by some of t independently of the author of the article, it bears many similarities to the tractor tree method used by Kearns et al. (1999). Brute uses the trajectory of the agent ht = a, o1, 2, o2,..., ot as stat representa ion and assigns individual values to each state. Due to the determinism of ALE, a single sample from each state-stakeholder pair is sufficient for a perfect estimate of the agent's return to this point. Brute maintains a partial history tree containing ll visited stories. Each node associated with a story maintains an action-related transition function and a reward function. Brute estimates the value for each historical-action pair using dynamic bottom-up programming. The agent follows the best trajectory found so far, using random actions to search for better trajectories."}, {"heading": "5.1.1 Empirical Evaluation", "text": "We have evaluated the performance of Brute based on the five training games proposed by Bellemare et al. (2013). The average score obtained by Brute as well as DQN and Sarsa (\u03bb) + Blob-PROST is shown in Table 1. Agents interacted with the environment for 50 million frames and the numbers given are the average scores that agents achieved in the last 100 episodes played while learning. We discuss our experimental setup in Appendix B. Brute is coarse, but we see that it leads to competitive performance in a number of games. In fact, Bellemare et al. (2015) report using a different evaluation protocol that Brute has surpassed the best learning method of the time in 45 of 55 Atari 2600 games. However, as we will see, this performance depends crucially on the determinism of the environment. In the next section we will discuss how we have modified the ALE to introduce a form of fraturity that we call tackiness, and what we call small actions we will introduce;"}, {"heading": "5.2 Sticky Actions", "text": "This year, it has never been as good as it has been this year."}, {"heading": "5.2.1 Evaluating the Impact of Sticky Actions", "text": "We are now reviewing the performance of the Brute, DQN and Sarsa (\u03bb) + Blob-PROST under the adhesion protocol. The intuition is that the brute that exploits the assumption that the environment is deterministic should perform worse when stochasticity is introduced. We repeated the experiments from Section 5.1.1, but with \u03c2 = 0.25. Table 2 describes the performance of the algorithms both in the stochastic environment and in the deterministic environment. We can see that brute is the only algorithm that is substantially influenced by the adhesion actions. These results suggest that sticky actions enable us to evaluate empirically the robustness of a remedy against disturbances."}, {"heading": "5.3 Alternative Forms of Stochasticity", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "6. Benchmark Results in the Arcade Learning Environment", "text": "In this section we present new benchmark results for DQN and Sarsa (\u03bb) + Blob-PROST in 60 different Atari 2600 games. We hope that future work will adopt the experimental methodology described in this paper and be able to compare the results directly with this benchmark."}, {"heading": "6.1 Experimental Method", "text": "We evaluated DQN and Sarsa (\u03bb) + Blob-PROST in 60 different games of the Atari 2600. We report on the results with the \"adhesive effect\" option in the new version of the ALE (\u03c2 = 0.25) and evaluate the final performance during learning at 10, 50, 100 and 200 million images. We calculated the averages of each study based on the 100 last episodes up to the specified threshold, including the episode in which the total sum is exceeded. We report an average of 5 studies for DQN and an average of 24 studies for Sarsa (\u03bb) + Blob-PROST. To facilitate reproducibility, we have listed all relevant parameters used by Sarsa (\u03bb) + Blob-PROST and DQN in Appendix B. We encourage researchers to present their results in the same reproducible way at the ALE."}, {"heading": "6.2 Benchmark Results", "text": "We present extracts of the results obtained for Sarsa (\u03bb) + Blob-PROST and DQN in Tables 3 and 4. These tables report on the results achieved in the games we have used for training. These games were originally proposed by Bellemare et al. (2013) The complete results are available in Appendix C. As we report the performance of the algorithms at various points in time, these results give us insights into the learning progress made by each algorithm. Such analysis allows us to check how often the performance of an agent in each game reaches the best results before 200 million frames. In most games, these results give us insights into the learning progress made by each algorithm. + Blob-PROST's performance increases steadily for the entire learning period. In only 10% of games that achieve the results with 200 million frames, the values achieved with 100 million frames are lower than the values achieved with 100 million frames. This difference is statistically significant in terms of Games of Carnival: Wor 3 only."}, {"heading": "7. Open Problems and the Current State-of-the-Art in the ALE", "text": "It is also important to discuss the diversity of research issues for which the 6Welch community's t-test (p < 0.05; n = 5) has used the ALE as a test bed. In recent years, we have seen several successes in ALE, with new results being introduced at a rapid pace. We list five key areas of research the community has been working on with the ALE, and we use recent literature to argue that despite significant progress, these issues remain open. These lines of research are: \u2022 Representative Learning, \u2022 Exploration, \u2022 Transfer Learning, \u2022 Model Learning, and \u2022 Non-Political Learning."}, {"heading": "7.1 Representation Learning", "text": "In fact, it is the case that most of them will be able to adhere to the rules that they have applied in practice. (...) In fact, it is the case that they are able to prove themselves in practice. (...) In practice, it is the case that they are able to prove themselves in practice. (...) In practice, it is the case that they are able to prove themselves in practice. (...) In practice, it is the case that they are able to prove themselves in practice. (...) In practice, it is the case that they are able to prove themselves in practice. (...) In practice, it is as if they are able to remain in practice. (...) In practice, it is the case that they are able to prove themselves in practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice. (...) In practice, in practice, in practice, in practice. (...) In practice, in practice, in practice, in practice. (...) In practice, in practice."}, {"heading": "7.2 Planning and Model-Learning", "text": "Despite several successes of search algorithms in artificial intelligence (e.g., Campbell et al., 2002; Schaeffer et al., 2007; Silver et al., 2016), planning in the arcade learning environment remains rare compared to methods that learn politics or value functions (but see Bellemare et al., 2013b; Guo et al., 2014; Lipovetzky et al., 2015; Shleyfman et al., 2016; Jinnai and Fukunaga, 2017, for published planning results in the ALE). Developing heuristics that are generic enough to be successfully applied to dozens of different games is a difficult problem. The branching factor of the problem and the fact that the goals are sometimes thousands of steps ahead of the original state of the agent are also major levels of difficulty. Almost all planning successes in the ALE that are provided by the generative model, and so have an exact model of the environment."}, {"heading": "7.3 Exploration", "text": "Most exploration approaches focus on the tabular case and generally learn environmental models (e.g. Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl andLittman, 2008).The community is just beginning to explore exploration strategies in model-free environments when a convergence of functions is required (e.g. Bellemare et al., 2016b; Osband et al., 2016; Ostrovski et al., 2017; Machado et al., 2017; Vezhnevets et al., 2017 is the setting in which the ALE is located. Each state does not seem to be a viable strategy given the large number of possible states in a game (potentially 21024 different states, since the Atari has 2600 1024 bits of RAM memory).In several games such as Montezuma's Revenge and Private Eye (see Figure 6) it is difficult to get any feedback."}, {"heading": "7.4 Transfer Learning", "text": "Most of the work in ALE involves training agents in each game individually, but many Atari 2600 games have similar dynamics. We can expect the knowledge transfer to reduce the required number of samples required to learn to play similar games. Space Invaders and Demon Attack (Figure 7) are two similar games in which the agent is represented by a spaceship at the bottom of the screen and is expected to shoot incoming enemies. A more ambitious research question is how to harness general video game experiences by exchanging knowledge in games that are not directly analog. In this case, more abstract concepts could be learned, such as \"sometimes seeing new screens when the avatar goes to the edge of the current screen.\" There are attempts to apply transfer learning in ALE (Rusu et al., 2016; Parisotto et al., 2016). Such experiments are limited to a dozen games that are similar to and require learning them all at the same time, rather than a network approach that can be used in each case."}, {"heading": "7.4.1 Modes and Difficulties in the Arcade Learning Environment", "text": "Originally, many Atari 2600 games had a standard game mode and difficulty level, which could be changed by selecting buttons on the console. These mode and difficulty switches had different consequences, such as changing game dynamics or introducing new actions (see Figure 8). Until recently, ALE Agents allowed games to be played only in their standard mode and difficulty level. The latest version of ALE allows one to choose between all the different game modes and difficulties that are single-player games. We refer to each mode difficulty pair as a flavor. This new feature opens up research opportunities by introducing dozens of new environments that are very similar. As the underlying statuses of different tastes are likely to be highly related, we believe that negative transfers are less likely, providing an easier transfer option. The list of such games that ALE will originally support, and the number of tastes is available in Appendix D."}, {"heading": "7.5 Off-Policy Learning", "text": "In addition to proposing new algorithms that theoretically behave better (e.g. Maei and Sutton, 2010), attempts to reduce divergence in non-political learning currently consist of heuristics that attempt to relate observations, such as the use of an experiential replay buffer and the use of a target network in DQN (Mnih et al., 2015). Recent papers introduce changes in the updating rules of Q-Learning to reduce overestimation of value functions (van Hasselt et al., 2016), new operators that widen the gap in action of value function estimates (Bellemare et al., 2016a), and more robust, multi-level non-policy algorithms (Harutyunyan et al., 2016), characterized by a better understanding of policy functions and a better understanding of convergence functions."}, {"heading": "8. Conclusion", "text": "To encourage progress in this area, we introduced some methodological best practices and a new version of the Arcade Learning Environment that supports stochasticity and multiple game modes. We hope that such methodological practices, together with the new ALE, will allow a clear distinction between the different assessment protocols. We also provide benchmark results that follow these methodological best practices and can serve as a reference point for future work in ALE. In the last part of this paper, we evaluated strengthened learning algorithms that use linear and non-linear functional approximation, and we hope to have fostered the discussion of excessive efficiency by reporting the performance of algorithms at different moments in the learning period."}, {"heading": "Acknowledgements", "text": "The authors would like to thank David Silver and Tom Schaul for their thorough feedback on an earlier draft, as well as Re'mi Munos, Will Dabney, Mohammad Azar, Hector Geffner, Jean Harb and Pierre-Luc Bacon for useful discussions. We would also like to thank the various contributors to the Arcade Learning Environment GitHub Repository, in particular Nicolas Carion for implementing the mode and the difficult selection, and Ben Goodrich for providing a Python interface to ALE. Yitao Liang and Marlos C. Machado implemented the Blob-PROST features. This work was supported by grants from Alberta Innovates - Technology Futures (AITF), the Alberta Machine Intelligence Institute (Amii) and NSF funding IIS-1552533."}, {"heading": "Appendix A. The Brute", "text": "Although most of them exploit the features of the original arcade learning environment, we cannot integrate them directly into practice. Although they were developed independently of some of the authors, they share many similarities with the trajectory method used by Kearns et al. (1999). They are based on the following observations: the ALE is deterministic, episodic and guarantees a unique initial state, and most Atari games are about more than individual actions, i.e. most Atari games have important high-level goals, but individual actions have little impact. This algorithm results in competitive performance in a number of games.A.1 Determinism and startup configurations is a sequence of actions and observations ht = a1, a2, o2, o2, etc., with the reward rt in the observation ot.7 Histories describe sequential interactions between an agent and his environment."}, {"heading": "Appendix B. Experimental Setup", "text": "We used the same evaluation protocol and the same parameters in all the experiments discussed in this article. In the next section, we list the parameters used in defining the task in the Arcade Learning Environment. Later, we discuss the parameters used by the Brute, Sarsa (\u03bb) + Blob-PROST, and DQN.B.1 evaluation protocol and Arcade Learning Environment parameters. We report on our results, which aim to evaluate the robustness of the policies learned and learning algorithm. All the results we use for the Brute and for Sarsa (\u03bb) + Blob-PROST are averaged over 24 studies, and all the results we use for DQN are averaged over 5 studies. We rated DQN less times because its empirical validation is more expensive due to its requirement for special hardware (i.e., GPUs)."}, {"heading": "Appendix C. Complete Benchmark Results", "text": "We extend the results presented in section 6 (Tables 3 and 4) by reporting the performance of the algorithms in 60 games supported by ALE. We used the evaluation protocol described in Appendix B to generate the results below. Table 8 summarizes the performance of Sarsa (\u03bb) + Blob-PROST and Table 9 summarizes the performance of DQN. Games that were originally used in each method as training games are highlighted by the \u2020 symbol. Table 8 shows the list of games that we used to train Sarsa (\u03bb) + Blob-PROST longer than those in Table 9 because we report the training games used by Liang et al. (2016) that were the setting we originally replicated. Columns with an asterisk will be filled in later."}, {"heading": "Appendix D. Number of Game Modes and Difficulties in the Games", "text": "Supported by the Arcade Learning Environment"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the<lb>challenge of building AI agents with general competency across dozens of Atari 2600 games.<lb>It supports a variety of different problem settings and it has been receiving increasing<lb>attention from the scientific community, leading to some high-profile success stories such as<lb>the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at<lb>how the ALE is being used by the research community. We show how diverse the evaluation<lb>methodologies in the ALE have become with time, and highlight some key concerns when<lb>evaluating agents in the ALE. We use this discussion to present some methodological best<lb>practices and provide new benchmark results using these best practices. To further the<lb>progress in the field, we introduce a new version of the ALE that supports multiple game<lb>modes and provides a form of stochasticity we call sticky actions. We conclude this big<lb>picture look by revisiting challenges posed when the ALE was introduced, summarizing the<lb>state-of-the-art in various problems and highlighting problems that remain open.", "creator": "TeX"}}}