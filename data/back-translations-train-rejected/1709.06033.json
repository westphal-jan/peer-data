{"id": "1709.06033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Sequence to Sequence Learning for Event Prediction", "abstract": "This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events.", "histories": [["v1", "Mon, 18 Sep 2017 16:34:18 GMT  (67kb,D)", "http://arxiv.org/abs/1709.06033v1", "To appear in Proceedings of IJCNLP 2017"]], "COMMENTS": "To appear in Proceedings of IJCNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dai quoc nguyen", "dat quoc nguyen", "cuong xuan chu", "stefan thater", "manfred pinkal"], "accepted": false, "id": "1709.06033"}, "pdf": {"name": "1709.06033.pdf", "metadata": {"source": "CRF", "title": "Sequence to Sequence Learning for Event Prediction", "authors": ["Dai Quoc Nguyen", "Dat Quoc Nguyen", "Cuong Xuan Chu", "Stefan Thater", "Manfred Pinkal"], "emails": ["pinkal}@coli.uni-saarland.de", "dat.nguyen@mq.edu.au", "cxchu@mpi-inf.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it can only take a few days to get a result."}, {"heading": "2 Sequence to Sequence Learning", "text": "In view of a source sequence x1, x2,..., xm and a target sequence y1, y2,..., yn, sequence for sequence learning (SEQ2SEQ), it is necessary to estimate the conditional probability Pr (y1, y2,..., yn | x1, x2,..., xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016). Typically, SEQ2SEQ consists of an RNN encoder and an RNN decoder. The RNN encoder maps the source sequence into a vector representation c, which is then fed as input to the decoder to generate the target sequence. We use a bidirectional RNN architecture (BiRNN), which maps the sequence NNN architecture (Schuster and Palial, 1997)."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "WIKIHOW-based data sets: WIKIHOW is the largest collection of \"how-to\" tasks created by an online community, each task being represented by sub-tasks with detailed descriptions and illustrations, such as Figure 1. We have collected 168K articles (such as \"Bake-a-Cake\") that consist of 238K tasks (such as \"Making Vanilla Pound Cake\") and about 1.59 million sub-tasks (such as \"Gather your ingredients,\" \"Preheat the oven to 325 degrees\") that represent a variety of activities and events. Then we have created a corpus of about 1.34 million pairs of subsequent sub-tasks (i.e. source and target tendencies for the SEQ2SEQ model) for which we have the training sets of about 1.28 million pairs, each developing and testing sets of 26,800 pairs."}, {"heading": "3.2 Implementation details", "text": "The models are implemented in TensorFlow (Abadi et al., 2016) and trained with / without attention mechanism based on the training sets. Then, the trained models are used to generate a sentence describing a predicted event. We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences based on the target sentences corresponding to the source sentences. A SEQ2SEQ architecture with a single layer, adapted by Pichotta and Mooney (2016), is treated as a BASELINE model.We found word sizes of 30,000 and 5,000 most common words to be optimal for the WIKIHOW or DESCRIPT-based dataset. Words that do not occur in the vocabulary are mapped to a special Token UNK model. Word embeddings are trained with the pre-trained 300-dimensional word embedings of Word2Vec or DESCRIPT-based dataset. Words that do not occur in the vocabulary are mapped to a special Token UNK model. Word embeddings are mapped with the 300-dimensional word embeddings of Word2Vec or DESCRIPT-based dataset. Words that are mapped with Word2SEQ and DESCRIPT-based dataset."}, {"heading": "3.3 Evaluation using BLEU score", "text": "Table 1 presents our BLEU values with models based on WIKIHOW and DESCRIPT-based data on the respective test sets. There are significant differences in the observance of the WIKIHOW rates and the DESCRIPT rates. BLEU values between the two sets cannot be compared due to the much greater degree of variation in WIKIHOW. Values collected in Pichotta and Mooney (2016) via WIKIPEDIA are not comparable to our results for the same reason. Table 1 shows that 1-LAYER-BISEQ2SEQ achieves better results than the strong BASELINE. Specifically, 1-LAYER-BISEQ2SEQ improves the baseline by 0.3 + BLEU in both cases of ATT and NON-ATT, indicating the usefulness of using bidirectional data architectures."}, {"heading": "3.4 Evaluation based on paraphrase sets", "text": "BLEU values are difficult to interpret for the task: BLEU is a surface-based measurement variable as mentioned in (Qin and Specia, 2015), while event forecasting is essentially a semantic task. Table 2 shows output examples for the two-layered Bilstein SEQ2SEQ NON-ATT on the DESCRIPT-based dataset. Although the target and predicted sets have different surface shapes, they are perfect paraphrases of the same event type. To assess the semantic success of the prediction model, we use the gold paraphrase of event descriptions provided by the DESCRIPT corpus for 10 of its scenarios. We consider a subset of 682 pairs for which gold paraphrase information is available, and check whether a target event and its associated predicted event are paraphrases, i.e. they belong to the same gold paraphrase sets for which gold paraphrase information is predicted in Table 3.3. The accuracy results for STEU are not predicted in the same set of cases."}, {"heading": "4 Conclusions", "text": "We have created the new Open Domain and Closed Domain datasets based on WIKIHOW and DESCRIPT, which are available to the public at https: / / github.com / daiquocnguyen / EventPrediction. We have shown that more advanced SEQ2SEQ models with a bi-directional and multi-layered RNN architecture significantly outperform previous work. We have also introduced an alternative method of estimating event forecasts based on gold paraphrase sets that focuses on the semantic match between the target and predicted sets."}, {"heading": "Acknowledgments", "text": "This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) as part of CRC 1102 \"Information Density and Linguistic Coding.\" We thank Hannah Seitz for her kind help and support and anonymous reviewers for their helpful comments."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on", "author": ["jay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vasudevan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 3rd International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33\u201343.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, pages 179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proceedings of the 3rd International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, pages", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of the 4th International Conference on Learning Representations.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Using sentence-level lstm language models for script inference", "author": ["Karl Pichotta", "Raymond J. Mooney."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279\u2013289.", "citeRegEx": "Pichotta and Mooney.,? 2016", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2016}, {"title": "Truly exploring multiple references for machine translation evaluation", "author": ["Ying Qin", "Lucia Specia."], "venue": "Proceedings of the 18th Annual Conference of the European Association for Machine Translation, pages 113\u2013120.", "citeRegEx": "Qin and Specia.,? 2015", "shortCiteRegEx": "Qin and Specia.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal."], "venue": "IEEE Transactions on Signal Processing, pages 2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, pages 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems 28, pages 2773\u2013 2781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Descript: A crowdsourced corpus for the acquisition of highquality script knowledge", "author": ["Lilian D.A. Wanzare", "Alessandra Zarcone", "Stefan Thater", "Manfred Pinkal."], "venue": "Proceedings of the Tenth International Conference on Language Re-", "citeRegEx": "Wanzare et al\\.,? 2016", "shortCiteRegEx": "Wanzare et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.", "startOffset": 88, "endOffset": 173}, {"referenceID": 17, "context": "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.", "startOffset": 88, "endOffset": 173}, {"referenceID": 1, "context": "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.", "startOffset": 88, "endOffset": 173}, {"referenceID": 9, "context": "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.", "startOffset": 88, "endOffset": 173}, {"referenceID": 11, "context": "pata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al.", "startOffset": 32, "endOffset": 56}, {"referenceID": 8, "context": ", 2016) and multi-task learning (Luong et al., 2016).", "startOffset": 32, "endOffset": 52}, {"referenceID": 4, "context": "In general, SEQ2SEQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then", "startOffset": 88, "endOffset": 101}, {"referenceID": 7, "context": "SEQ2SEQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016).", "startOffset": 47, "endOffset": 67}, {"referenceID": 7, "context": "SEQ2SEQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use SEQ2SEQ for prediction of what comes next in a text.", "startOffset": 47, "endOffset": 98}, {"referenceID": 19, "context": "We also present another dataset based on the DESCRIPT corpus\u2014a crowdsourced corpus of event sequence descriptions (Wanzare et al., 2016).", "startOffset": 114, "endOffset": 136}, {"referenceID": 12, "context": "\u2022 Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.", "startOffset": 48, "endOffset": 71}, {"referenceID": 12, "context": "\u2022 Pichotta and Mooney (2016) use the BLEU score (Papineni et al.", "startOffset": 2, "endOffset": 29}, {"referenceID": 17, "context": ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).", "startOffset": 6, "endOffset": 113}, {"referenceID": 2, "context": ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).", "startOffset": 6, "endOffset": 113}, {"referenceID": 1, "context": ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).", "startOffset": 6, "endOffset": 113}, {"referenceID": 18, "context": ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).", "startOffset": 6, "endOffset": 113}, {"referenceID": 8, "context": ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).", "startOffset": 6, "endOffset": 113}, {"referenceID": 15, "context": "We use a bidirectional RNN (BiRNN) architecture (Schuster and Paliwal, 1997) for mapping the source sequence x1, x2, .", "startOffset": 48, "endOffset": 76}, {"referenceID": 17, "context": "When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder.", "startOffset": 35, "endOffset": 77}, {"referenceID": 2, "context": "When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder.", "startOffset": 35, "endOffset": 77}, {"referenceID": 1, "context": "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 155, "endOffset": 220}, {"referenceID": 9, "context": "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 155, "endOffset": 220}, {"referenceID": 18, "context": "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).", "startOffset": 155, "endOffset": 220}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016). Typically, SEQ2SEQ consists of a RNN encoder and a RNN decoder. The RNN encoder maps the source sequence into a vector representation c which is then fed as input to the decoder for generating the target sequence. We use a bidirectional RNN (BiRNN) architecture (Schuster and Paliwal, 1997) for mapping the source sequence x1, x2, ..., xm into the list of encoder states s1, s e 2, ..., s e m. The RNN decoder is able to work with or without attention mechanism. When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder. Then, at the timestep i (1 \u2264 i \u2264 n), the RNN decoder takes into account the hidden state si\u22121 and the previous input yi\u22121 to output the hidden state si and generate the target yi. Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). We adapt the attention mechanism proposed by Vinyals et al. (2015) to employ a concatenation of the hidden state si and the vector representation c to make predictions at the timestep i.", "startOffset": 8, "endOffset": 1191}, {"referenceID": 5, "context": "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al.", "startOffset": 112, "endOffset": 146}, {"referenceID": 2, "context": "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014).", "startOffset": 188, "endOffset": 206}, {"referenceID": 2, "context": "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016)", "startOffset": 189, "endOffset": 348}, {"referenceID": 2, "context": "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016)", "startOffset": 189, "endOffset": 379}, {"referenceID": 19, "context": "DESCRIPT-based dataset: The DESCRIPT corpus (Wanzare et al., 2016) is a crowdsourced corpus of event sequence descriptions on 40 different scenarios with approximately 100 event sequence descriptions per scenario.", "startOffset": 44, "endOffset": 66}, {"referenceID": 12, "context": "We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences against the target sentences corresponding to the source sentences.", "startOffset": 23, "endOffset": 46}, {"referenceID": 12, "context": "We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences against the target sentences corresponding to the source sentences. A SEQ2SEQ architecture using a single layer adapted by Pichotta and Mooney (2016) is treated as the BASELINE model.", "startOffset": 24, "endOffset": 233}, {"referenceID": 10, "context": "Word embeddings are initialized using the pre-trained 300-dimensional word embeddings provided by Word2Vec (Mikolov et al., 2013) and then updated during training.", "startOffset": 107, "endOffset": 129}, {"referenceID": 16, "context": "Dropout (Srivastava et al., 2014) is applied with probability of 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 6, "context": "The training objective is to minimize the crossentropy loss using the Adam optimizer (Kingma and Ba, 2015) and a mini-batch size of 64.", "startOffset": 85, "endOffset": 106}, {"referenceID": 13, "context": "The scores reported in Pichotta and Mooney (2016) on WIKIPEDIA are not comparable to our scores for the same reason.", "startOffset": 23, "endOffset": 50}, {"referenceID": 14, "context": "BLEU scores are difficult to interpret for the task: BLEU is a surface-based measure as mentioned in (Qin and Specia, 2015), while event prediction is essentially a semantic task.", "startOffset": 101, "endOffset": 123}, {"referenceID": 13, "context": "Our best model outperforms Pichotta and Mooney (2016)\u2019s BASELINE by 3.", "startOffset": 27, "endOffset": 54}], "year": 2017, "abstractText": "This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WIKIHOW and DESCRIPT respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events.", "creator": "LaTeX with hyperref package"}}}