{"id": "0712.3402", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2007", "title": "Graph kernels between point clouds", "abstract": "Point clouds are groups of points in two or three dimensions. Most core methods for learning point clouds have not yet addressed the specific geometric invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph cores for point clouds that allow using kernel methods for objects such as shapes, line drawings, or any three-dimensional point clouds. To design rich and numerically efficient cores with as few free parameters as possible, we use cores between covariance matrices and their factorizations on graphic models. We derive polynomial time dynamic programming curves and present applications for recognizing handwritten digits and Chinese characters from a few training examples.", "histories": [["v1", "Thu, 20 Dec 2007 13:06:50 GMT  (67kb)", "http://arxiv.org/abs/0712.3402v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francis r bach"], "accepted": true, "id": "0712.3402"}, "pdf": {"name": "0712.3402.pdf", "metadata": {"source": "CRF", "title": "Graph kernels between point clouds", "authors": ["Francis R. Bach"], "emails": ["francis.bach@mines.org"], "sections": [{"heading": null, "text": "ar X\niv :0\n71 2.\n34 02\nv1 [\ncs .L\nG ]\n2 0\nD ec\n2 00"}, {"heading": "1. Introduction", "text": "In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4]. They provide an elegant way of including known a priori information, by using directly the natural topological structure of objects. Using a priori knowledge through structured kernels have proved beneficial because it allows to reduce the number of training examples, and to re-use existing data representations that are already well developed by experts of those domains.\nIn this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8]. The natural geometrical structure of point clouds is hard to represent in a few real-valued features [9], in particular because of (a) the required local or global invariances by rotation, scaling, and/or translation, (b) the lack of pre-established registrations of the point clouds (i.e., points from one cloud are not matched to points from another cloud), and (c) the noise and occlusion that impose that only portions of two point clouds ought to be compared.\nFollowing one of the leading principles for designing kernels between structured data, we propose to look at all possible partial matches between two point clouds [10]. More precisely, we assume that each point cloud has a graph structure (most often a neighborhood graph), and we consider recently introduced graph kernels [11, 12]. Intuitively, these kernels consider all possible subgraphs and compare and count the matching subgraphs. However, the set of subgraphs (or even the set of paths) has exponential size and cannot be efficiently described recursively; so larger sets of substructures are commonly used, e.g., walks and tree-walks. As shown in Section 2, by choosing appropriate substructures and fully factorized local kernels, efficient dynamic programming implementations allow to sum over an exponential number of substructures in polynomial time. The kernel thus provides an efficient and elegant way of considering very large feature spaces (see, e.g., [10]).\nHowever, in the context of computer vision, substructures correspond to matched sets of points, and dealing with local invariances imposes to use a local kernel that cannot be readily expressed as a product of separate terms for each pair of points, and the usual dynamic programming approaches cannot then be applied. The main contribution of this paper is to design a local kernel that is not fully factorized but can be instead factorized according to the graph underlying the substructure. This is naturally done through graphical models and the design of positive kernels for covariance\n1\nmatrices that factorize on graphical models (Section 3). With this novel local kernel, we derive new polynomial time dynamic programming recursions in Section 4. In Section 5, we present simulations on handwritten character recognition."}, {"heading": "2. Graph kernels", "text": "In this section, we consider two labelled undirected graphs G = (V,E, a, x) and H = (W,F, b, y). Two types of labels are considered: attributes, which are denoted a(v) \u2208 A for vertex v \u2208 V and b(w) \u2208 A for vertex w \u2208 W and positions, which are denoted x(v) \u2208 X and y(w) \u2208 X . Our motivating examples are line drawings, where X = A = R2.\nThe definition of graph kernels between G and H relies on a set of substructures of the graphs. The most natural ones are paths, subtrees and more generally subgraphs; however, they do not lead to efficient enumerations, and recent work [11, 13] has focused on larger sets of substructures that we now present."}, {"heading": "2.1 Paths, walks, subtrees and tree-walks", "text": "Given the undirected graph G with vertex set V , a path is a sequence of distinct connected vertices, while a walk is a sequence of possibly non distinct connected vertices. For any positive integer \u03b2, we define \u03b2-walks as walks such that any \u03b2 + 1 successive vertices are distinct (1-walks are regular walks). Note that when the graph G is a tree (no cycles), then the set of 2-walks is equal to the set of paths (see examples in Figure 1). More generally, for any graph, \u03b2-walks of length \u03b2 + 1 are exactly paths of length \u03b2 + 1.\nA subtree is a subgraph of with no cycles. We can represent a subtree of G by a tree structure T over the vertex set {1, . . . , |T |}, where |T | is the number of nodes in T , and a sequence of distinct consistent labels I \u2208 V |T | (i.e., that are neighbors in G when neighbors in T ). In this paper, we consider only rooted subtrees, i.e., subtrees where a specific node is identified as the root.1\nThe notion of walk is extending the notion of path by allowing nodes to be equal. Similarly, we can extend the notion of subtrees to tree-walks, which can have nodes that are equal. More precisely, we define an \u03b1-ary tree-walk of depth \u03b3 of G as a (non complete) labelled \u03b1-ary tree of depth \u03b3 with nodes labelled by vertices in G, and such that the labels of neighbors in the tree-walk are neighbors in G. A tree-walk may be represented by a tree structure T over the vertex set {1, . . . , |T |} and a sequence of consistent but possibly non distinct labels I \u2208 V |T |. We can also define \u03b2-tree-walks, as tree-walks such that for each node in T , its label (i.e. an element of V ) and the ones of all its descendants up to the \u03b2-th generation are all distinct. With that definition, 1-tree-walks are regular tree-walks (see Figure 2). Note that if \u03b1 = 1, we get back \u03b2-walks and the graph kernels that we use are often referred to as random walk kernels [11]. From now on, we refer to the descendants up to the \u03b2-th generation as the \u03b2-descendants.\nWe let denote T\u03b1,\u03b3 the set of tree structures of depth less than \u03b3 and with at most \u03b1 children per node. For T \u2208 T\u03b1,\u03b3 , we define J\u03b2(T,G) the set of consistent labellings of T by vertices in V leading to \u03b2-tree-walks. With these definitions, a \u03b2-tree-walk of G is characterized by a tree structure T \u2208 T\u03b1,\u03b3 and a labelling I \u2208 J\u03b2(T,G).\n1. Moreover, all the trees that we consider are unordered trees (i.e., no order is considered among siblings).\n2"}, {"heading": "2.2 Graph kernels", "text": "Assuming a local kernel qT,I,J(G,H) between two tree-walks that share the same structure is given, following [11], we can define the tree-kernel as the sum over all matching tree-walks of G and H of the local kernel:\nkT\u03b1,\u03b2,\u03b3(G,H) = \u2211\nT\u2208T\u03b1,\u03b3\nf\u03bb,\u03bd(T ) \u2211\nI\u2208J\u03b2(T,G)\n\u2211\nJ\u2208J\u03b2(T,H)\nqT,I,J(G,H). (1)\nIf the kernel qT,I,J(G,H) has positive values and is equal to 1 if the two tree-walks are equal, it can be seen as a soft matching indicator, and then the kernel in Eq. (1) simply counts the softly matched tree-walks in the two graphs.\nWe add a nonnegative penalization f\u03bb,\u03bd(T ) depending only on the tree-structure. Besides the usual penalization of the number of nodes |T |, we also add a penalization of the number of leaf nodes \u2113(T ). More precisely, we use the penalization f\u03bb,\u03bd = \u03bb\n|T |\u03bd\u2113(T ). This penalization suggested by [13] is essential in our situation to avoid that trees with nodes of higher degrees dominate the sum.\nIf qT,I,J (G,H) is a positive kernel between G and H , then k T \u03b1,\u03b2,\u03b3(G,H) also defines a positive kernel. The kernel kT\u03b1,\u03b2,\u03b3(G,H) sums the local kernel qT,I,J(G,H) overall all tree-walks of G and H that share the same tree structure. The number of matching tree-walks is exponential in the depth \u03b3, thus, in order to deal with potentially deep trees, a recursive definition is needed. It requires a specific type of local kernels."}, {"heading": "2.3 Local kernel", "text": "We use a combination (product) of a kernel for attributes and a kernel for positions. For attributes, we use the following usual factorized form kA(a(I), b(J)) = \u220f|I|\np=1 kA(a(Ip), b(Jp), where kA is a positive definite kernel on A \u00d7 A. This allows the separate comparison of each matched pair of points and efficient dynamic programming recursions [11, 4]. However, for our local kernel on positions, we need a kernel that jointly depends on the whole vectors x(I) and y(J), and not only on pairs (x(Ip), y(Jp)).\nIn this paper, we focus on X = Rd and translation invariant local kernels, which implies that the local kernel for positions may only depend on differences x(i)\u2212x(i\u2032) and y(j)\u2212y(j\u2032) for (i, i\u2032) \u2208 I\u00d7I and (j, j\u2032) \u2208 J \u00d7 J . We further reduce these to the kernel matrices corresponding to a translation invariant kernel kX (x\u2212x\u2032). Depending on the application, kX may or may not be rotation invariant.\nThat is, we define full kernel matrices K \u2208 R|V |\u00d7|V | and L \u2208 R|W |\u00d7|W | for each graph, defined as K(v, v\u2032) = kX (x(v)\u2212 x(v\u2032)) (and similarly for L). For simplicity, we assume that these matrices are positive definite (i.e., invertible), which can be enforced by adding a multiple of the identity matrix. The local kernel will thus only depend on the submatrices KI = KI,I and LJ = LJ,J , which are positive definite matrices. Note that we use kernel matrices K and L to represent the geometry of each graph, and that we use a kernel on such kernel matrices.\nWe use the following kernel on positive matrices K and L, the (squared) Bhattacharyya kernel kB, defined as [14]:\nkB(K,L) = |K|1/2|L|1/2 \u2223 \u2223 K+L 2 \u2223 \u2223 \u22121 , (2)\n3\nwhich is a positive kernel with pointwise positive values and such that kB(K,K) = 1 (|K| denotes the determinant of K).\nBy taking the product of the attribute-based local kernel and the position-based local kernel, we get the following local kernel q0T,I,J (G,H) = kB(KI , LJ)kA(AI , BJ). However, this local kernel q0T,I,J(G,H) does not yet depend on the tree structure T and the recursion may be efficient only if qT can be computed recursively. The factorized term kA(AI , BJ) does not cause any problems; however, for the term kB(KI , LJ), we need an approximation based on T . As we show in Section 3, this can be obtained by a factorization according to the appropriate graphical model."}, {"heading": "3. Positive matrices and graphical models", "text": "The main idea underlying the factorization of the kernel is to consider symmetric positive matrices as covariance matrices and look at graphical models defined for Gaussian random vectors with those covariance matrices. In this section we assume that we have n random variables Z1, . . . , Zn with probability distribution p(z) = p(z1, . . . , zn). Given a kernel matrix K (in our case defined as Kij = e \u2212\u03b1\u2016xi\u2212xj\u2016 2\n, for positions x1, . . . , xn), we consider random variables Z1, . . . , Zn such that cov(Zi, Zj) = Kij . In this section, with this identification, we consider covariance matrices as kernel matrices, and vice-versa."}, {"heading": "3.1 Graphical models and junction trees", "text": "Graphical models provide a flexible and intuitive way of defining factorized probability distributions. Given any undirected graph Q with vertices in {1, . . . , n}, the distribution p(z) is said to factorize in Q if it can be written as a product of potentials over all cliques (completely connected subgraphs) of the graph Q. When the distribution is Gaussian with covariance matrix K \u2208 Rn\u00d7n, the distribution factorizes if and only if (K\u22121)ij = 0 for each (i, j) which is not an edge in Q [15, 16].\nIn this paper, we only consider decomposable graphical models, for which the graph Q is triangulated (i.e., there exists no chordless cycles of length larger than 4). In this case, the joint distribution is uniquely defined from its marginals p(zC) on the cliques C of the graph Q. Namely, if C(Q) is the set of maximal cliques of Q, we can build a tree of cliques, a junction tree, such that p(z) = \u220f\nC\u2208C(Q) p(zC)/ \u220f C,C\u2032\u2208C(Q),C\u223cC\u2032 p(zC\u2229C\u2032). (see Figure 3 for an example of graphical model\nand junction tree). The sets C \u2229C\u2032 are usually referred to as separators and we let denote S(Q) the set of such separators.\n4"}, {"heading": "3.2 Graphical models and projections", "text": "We let denote \u03a0Q(K) the covariance matrix that factorizes in Q which is closest to K for the Kullback-Leibler divergence between normal distributions with covariance matrices K and L. In this paper, we essentially replace K by \u03a0Q(K); i.e., we project all our covariance matrices onto a graphical model, which is a classical tool in probabilistic modelling [16]. We leave the study of the approximation properties of such a projection (in particular along the lines of [17]) to future work.\nPractically, since our kernel on kernel matrices involves determinant, we simply need to compute |\u03a0Q(K)| efficiently. For decomposable graphical models, \u03a0Q(K) can be obtained in closed form [15] and its determinant has the following simple expression:\nlog |\u03a0Q(K)| = \u2211 C\u2208C(Q) log |KC | \u2212 \u2211 S\u2208S(Q) log |KS |. (3)\nIf the junction tree is rooted (by choosing any clique as the root), then for each clique but the root, a unique parent clique pQ(C) is defined, and we have:\nlog |\u03a0Q(K)| = \u2211 C\u2208C(Q) log |KC| |KpQ(C)| = \u2211 C\u2208C(Q) log |KC|pQ(C)|, (4)\nwhere pQ(C) is the parent clique of Q (and \u2205 for the root clique) and the conditional covariance is defined, as usual, as KC|pQ(C) = KC,C \u2212KC,pQ(C)K \u22121 pQ(C),pQ(C) KpQ(C),C ."}, {"heading": "3.3 Graphical models and kernels", "text": "We now propose several ways of defining a kernel adapted to graphical models. All of them are based on replacing determinants |M | by |\u03a0Q(M)|, and their different decompositions in Eq. (3) and Eq. (4). Using Eq. (3), we obtain the first kernel:\nkQB,0(K,L) = \u220f C\u2208C(Q) kB(KC , LC) ( \u220f S\u2208S(Q) kB(KS , LS) )\u22121 . (5)\nHowever, this is not always a positive kernel for general covariance matrices:\nProposition 1 For any decomposable model Q, the kernel kQB,0 defined in Eq. (5) is a positive kernel on the set of covariance matrices K such that for all separators S \u2208 S(Q), KS,S = I. In particular, when all separators have cardinal one, this is a kernel on correlation matrices.\nIn order to remove the condition on separators, we consider the rooted junction tree representation in Eq. (4) and define another kernel as\nkQB (K,L) = \u220f C\u2208C(Q) k C|pQ(C) B (K,L). (6)\nFor the root, we define k R|\u2205 B (K,L) = kB(KR, LR) and the kernels k C|pQ(C) B (K,L) are defined as kernels between conditional Gaussian distributions of ZC given ZpQ(C). We use\nk C|pQ(C) B (K,L)=\n|KC|pQ(C)| 1/2|LC|pQ(C)| 1/2\n\u2223 \u2223 \u2223\n1 2KC|pQ(C)+ 1 2LC|pQ(C)+ 1 4 (KC,pQ(C)K \u22121 pQ(C) \u2212LC,pQ(C)L \u22121 pQ(C)\n)\u22972 \u2223 \u2223\n\u2223\n, (7)\nwhich corresponds to putting a prior with identity covariance matrix on variables Z\u03a0(Q) and considering the kernel between the resulting joint covariance matrices on variables indexed by (C, pQ(C)) (we use the notation M\u22972 = MM\u22a4). We now have a positive kernel on all covariance matrices:\nProposition 2 For any decomposable model Q, the kernel kQB (K,L) defined in Eq. (6) and Eq. (7) is a positive kernel on the set of covariance matrices.\n5\nNote that the kernel is not invariant by the choice of the particular root of the junction tree. However, in our setting, this is not an issue because we have a natural way of rooting the junction trees (i.e, following the rooted tree-walk).\nIn Section 4, we will use the notation k I1|I2,J1|J2 B (K,L) for |I1| = |I2| and |J1| = |J2| to denote the kernel between covariance matrices KI1\u222aI2 and LI1\u222aI2 adapted to the conditional distributions I1|I2 and J1|J2 (defined through Eq. (7))."}, {"heading": "3.4 Choice of graphical models", "text": "Given the rooted tree structure T of a \u03b2-tree-walk, we now need to define the graphical model Q\u03b2(T ) that we use to project our kernel matrices. We define Q\u03b2(T ) such that for all nodes in T , the node together with all its \u03b2-descendants form a clique, i.e., a node is connected to its \u03b2-descendants and all \u03b2-descendants are also mutually connected (see Figure 3 for example for \u03b2 = 1): the set of cliques are thus the set of families of depth \u03b2 (i.e., with \u03b2 + 1 generations). Thus our final kernel is:\nkT\u03b1,\u03b2,\u03b3(G,H) = \u2211\nT\u2208T\u03b1,\u03b3\nf\u03bb,\u03bd(T ) \u2211\nI\u2208J\u03b2(T,G)\n\u2211\nJ\u2208J\u03b2(T,H)\nk Q\u03b2(T ) B (KI , LJ)kA(AI , BJ). (8)\nThe main intuition behind this definition is to sum local similarities over all matching subgraphs. In order to obtain a tractable formulation, we simply needed to (a) extend the set of subgraphs (to tree-walks of depth \u03b3) and (b) factorize the local similarities along the graphs. Note that the graph Q\u03b2(T ) that we chose is the densest graph for which the following dynamic programming recursions may hold."}, {"heading": "4. Dynamic programming recursions", "text": "In order to derive dynamic programming recursions, we follow [13] and rely on the fact that \u03b1-ary \u03b2-tree-walks of G can essentially be defined through 1-tree-walks on the augmented graph of all subtrees of G of depth at most \u03b2 \u2212 1 and arity less than \u03b1.\nWe thus consider the set V\u03b1,\u03b2 of non complete rooted (unordered) subtrees of G = (V,E), of depths less than \u03b2 \u2212 1 and arity less than \u03b1. Given two different rooted unordered labelled trees, they are said equivalent if they share the same tree structure, and this is denoted \u223ct.\nOn this set V\u03b1,\u03b2 , we define a directed graph with edge set E\u03b1,\u03b2 as follows: R0 \u2208 V\u03b1,\u03b2 is connected to R1 \u2208 V\u03b1,\u03b2 if \u201cthe tree R1 extends the tree R0 one generation further\u201d, i.e., if and only if (a) the first \u03b2\u2212 2 generations of R1 are exactly equal to one of the complete subtree of R0 rooted at a child of the root of R0, and (b) the nodes of depth \u03b2 \u2212 1 of R1 are distinct from the nodes in R0. This defines a graph G\u03b1,\u03b2 = (V\u03b1,\u03b2 , E\u03b1,\u03b2) (see Figure 4). Similarly we define a graph H\u03b1,\u03b2 = (W\u03b1,\u03b2 , F\u03b1,\u03b2) for the graph H .\nFor a \u03b2-tree-walk, the root with its \u03b2 \u2212 1-descendants must have distinct vertices and thus correspond exactly to elements of V\u03b1,\u03b2 . We let k T \u03b1,\u03b2,\u03b3(G,H,R0, S0) denote the same kernel as defined in Eq. (8), but restricted to tree-walks that start respectively start with R0 and S0. Note that if R0 and S0 are not equivalent, then k T \u03b1,\u03b2,\u03b3(G,H,R0, S0) = 0\n6\nWe obtain the following recursion for all R0 \u2208 V\u03b1,\u03b2 and and S0 \u2208 W\u03b1,\u03b2 such that R0 \u223ct S0:\nkT\u03b1,\u03b2,\u03b3(G,H,R0, S0) =\n\u03b1 \u2211\np=0\n\u03bbp\u03bd(p\u22121)+ \u2211\nR1, . . . , Rp \u2282 NG\u03b1,\u03b2 (R0) R1, . . . , Rp disjoint\n\u2211\nS1, . . . , Sp \u2282 NH\u03b1,\u03b2 (S0) S1, . . . , Sp disjoint\nkA(Ar(R), Br(S)) k \u222api=1Ri|R0,\u222a p i=1Si|S0 B (K,L) \u220fp\ni=1 k Ri,Si B (K,L)\n(\np \u220f\ni=1\nkT\u03b1,\u03b2,\u03b3\u22121(G,H,Ri, Si)\n)\n.\nNote that if any of the trees Ri is not equivalent to Si, it does not contribute to the sum. The recursion is initialized with kT\u03b1,\u03b2,\u03b3(G,H,R0, S0) = \u03bb |R0|\u03bd\u2113(R0)kA(AR0 , BS0)kB(KR0 , LS0) and the final kernel is obtained as kT\u03b1,\u03b2,\u03b3(G,H) = \u2211 R0\u223ctS0 kT\u03b1,\u03b2,\u03b3(G,H,R0, S0).\nNote that we may reduce the computational load by considering a set of trees of smaller arity in the previous recursions; i.e., we consider V1,\u03b2 instead of V\u03b1,\u03b2 with tree-kernels of arity \u03b1 > 1."}, {"heading": "4.1 Computational complexity", "text": "The complexity of computing one kernel between two graphs is linear in \u03b3 (the order of the treewalks), and quadratic in the size of V\u03b1,\u03b2 and W\u03b1,\u03b2 . However, those sets have exponential size in \u03b2 and \u03b1 in general. And thus, we are limited to small values (typically \u03b1 6 3 and \u03b2 6 6) which are sufficient for good classification performance (see Section 5). For example, for the handwritten digits we use in simulations, the average number of nodes in the graphs are 18\u00b14 , while the average cardinal of V\u03b1,\u03b2 is 37\u00b1 13 (\u03b1 = 1, \u03b2 = 4), and 70\u00b1 44 (\u03b1 = 2, \u03b2 = 4)."}, {"heading": "5. Application to handwritten character recognition", "text": "We have tested our new kernels on the task of isolated handwritten character recognition, handwritten arabic numerals (MNIST dataset) and Chinese characters (ETL9B dataset). We selected the first 100 examples for the ten classes in the MNIST dataset, while for the ETL9B dataset, we selected the five hardest classes to discriminate among the 3,000 (by computing distances between class means) and then selected the first 50 examples per class. Our learning task it to classify those characters; we use a one-vs-one multiclass scheme with 2-norm support vector machines [10].\nWe consider characters as drawings in R2, which are sets of possibly intersecting contours. Those are naturally represented as undirected planar graphs. We have thinned and subsampled uniformly each character to reduce the sizes of the graphs (see two examples in Figure 5).\nThe kernel on positions is kX (x, y) = exp(\u2212\u03c4\u2016x \u2212 y\u20162) + \u03ba\u03b4(x, y), but could take into account different weights on horizontal and vertical directions. We add the positions from the center of the bounding box as features, to take into account the global positions, i.e., we use kA(x, y) = exp(\u2212\u03c5\u2016x\u2212 y\u20162). This is necessary because the problem of handwritten character recognition is not globally translation invariant.\n7\nIn this paper we have defined a family of kernels, corresponding to different values of the following free parameters (shown with their possible values):\n\u03b1 arity of tree-walks 1, 2 \u03b2 order of tree-walks 1, 2, 4, 6 \u03b3 depth of tree-walks 1, 2, 4, 8, 16, 24 \u03bb penalization on number of nodes 1 \u03bd penalization on number of leaf nodes .1, .01 \u03c4 bandwidth for kernel on positions .05, .01, .1 \u03ba ridge parameter .001 \u03c5 bandwidth for kernel on attributes .05, .01, .1\nThe first two sets of parameters (\u03b1, \u03b2, \u03b3, \u03bb, \u03bd) are parameters of the graph kernel, independent of the application, while the last set (\u03c4, \u03ba, \u03bd) are parameters of the kernels for attributes and positions. Note that with only a few important scale parameters (\u03c4 and \u03bd), we are able to characterize complex interactions between the vertices and edges of the graphs. In practice, this is very important, to avoid considering many distinct parameters for all sizes and topologies of subtrees.\nIn simulations, we performed two loops of 5-fold cross-validation: in the outer loop, we consider 5 different training folds with their corresponding testing folds. On each training fold, we consider all possible values of \u03b1 and \u03b2. For all of those values, we select all other parameters (including the regularization parameters of the SVM) by 5-fold cross-validation (the inner folds). Once the best parameters are found only by looking at the training fold, we train on the whole training fold, and test on the testing fold. We output the means and standard deviations of the testing errors for each testing fold. We compare the performance with the Gaussian-RBF kernel with bandwidth learned by cross-validation in the same way in Figure 6.\nThese results show that our new family of kernels that use the natural structure of line drawings are outperforming the \u201cblind\u201d Gaussian-RBF kernel (error rate of 5.4% instead of 9.3% for the MNIST digits and 25.6% instead of 48.4% for the ETL9B characters). Note that for arabic numerals, best performance is achieved for \u03b1 = 1 (walks instead of tree-walks), which is not surprising since most digits have a linear structure (graphs are chains). On the contrary, for Chinese characters, best performance is achieved for binary tree-walks."}, {"heading": "6. Conclusion", "text": "We have presented a new kernel for point clouds which is based on comparisons of local subsets of the point clouds. Those comparisons are made tractable by (a) considering subsets based on tree-walks and walks, and (b) using a specific factorized form for the local kernels between tree-walks, namely a factorization on a graphical model.\nMoreover, we have reported applications to handwritten character recognition where we showed that the kernels were able to capture the relevant information to allow good predictions from few training examples. We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].\n8"}], "references": [{"title": "Local alignment kernels for biological sequences. In Kernel Methods in Computational Biology", "author": ["J.-P. Vert", "H. Saigo", "T. Akutsu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J.-P. Vert", "O. Birkenes", "T. Matsui"], "venue": "In Proc. ICASSP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Image classification with segmentation graph kernels", "author": ["Z. Harchaoui", "F. Bach"], "venue": "In Proc. CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Offline Chinese handwriting recognition: A survey", "author": ["S.N. Srihari", "X. Yang", "G.R. Ball"], "venue": "Frontiers of Computer Science in China,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Feature kernel functions: Improving svms using high-level knowledge", "author": ["Q. Sun", "G. DeJong"], "venue": "In Proc. CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Computer Vision: A Modern Approach", "author": ["D.A. Forsyth", "J. Ponce"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J. Ramon", "T. G\u00e4rtner"], "venue": "In First International Workshop on Mining Graphs, Trees and Sequences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Kernels for graphs. In Kernel Methods in Computational Biology", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Graph kernels based on tree patterns for molecules", "author": ["P. Mah\u00e9", "J.-P. Vert"], "venue": "Technical Report CCSD- 00095488,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "A kernel between sets of vectors", "author": ["R.I. Kondor", "T. Jebara"], "venue": "In Proc. ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Graphical models", "author": ["M.I. Jordan"], "venue": "Statistical Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Graphical models and point pattern matching", "author": ["T.S. Caetano", "T.M. Caelli", "D. Schuurmans", "D.A.C. Barone"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Kernel on bag of paths for measuring similarity of shapes", "author": ["F. Suard", "A. Rakotomamonjy", "A. Benrshrair"], "venue": "In Proc. ESANN,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A structural alignment kernel for protein", "author": ["J. Qiu", "M. Hue", "A. Ben-Hur", "J.-P. Vert", "W.S. Noble"], "venue": "structures. Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 164, "endOffset": 170}, {"referenceID": 6, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 164, "endOffset": 170}, {"referenceID": 7, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "The natural geometrical structure of point clouds is hard to represent in a few real-valued features [9], in particular because of (a) the required local or global invariances by rotation, scaling, and/or translation, (b) the lack of pre-established registrations of the point clouds (i.", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Following one of the leading principles for designing kernels between structured data, we propose to look at all possible partial matches between two point clouds [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "More precisely, we assume that each point cloud has a graph structure (most often a neighborhood graph), and we consider recently introduced graph kernels [11, 12].", "startOffset": 155, "endOffset": 163}, {"referenceID": 11, "context": "More precisely, we assume that each point cloud has a graph structure (most often a neighborhood graph), and we consider recently introduced graph kernels [11, 12].", "startOffset": 155, "endOffset": 163}, {"referenceID": 9, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "The most natural ones are paths, subtrees and more generally subgraphs; however, they do not lead to efficient enumerations, and recent work [11, 13] has focused on larger sets of substructures that we now present.", "startOffset": 141, "endOffset": 149}, {"referenceID": 12, "context": "The most natural ones are paths, subtrees and more generally subgraphs; however, they do not lead to efficient enumerations, and recent work [11, 13] has focused on larger sets of substructures that we now present.", "startOffset": 141, "endOffset": 149}, {"referenceID": 10, "context": "Note that if \u03b1 = 1, we get back \u03b2-walks and the graph kernels that we use are often referred to as random walk kernels [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "2 Graph kernels Assuming a local kernel qT,I,J(G,H) between two tree-walks that share the same structure is given, following [11], we can define the tree-kernel as the sum over all matching tree-walks of G and H of the local kernel: k \u03b1,\u03b2,\u03b3(G,H) = \u2211", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "This penalization suggested by [13] is essential in our situation to avoid that trees with nodes of higher degrees dominate the sum.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "This allows the separate comparison of each matched pair of points and efficient dynamic programming recursions [11, 4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 3, "context": "This allows the separate comparison of each matched pair of points and efficient dynamic programming recursions [11, 4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 13, "context": "We use the following kernel on positive matrices K and L, the (squared) Bhattacharyya kernel kB, defined as [14]: kB(K,L) = |K||L| \u2223", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "When the distribution is Gaussian with covariance matrix K \u2208 R, the distribution factorizes if and only if (K)ij = 0 for each (i, j) which is not an edge in Q [15, 16].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "When the distribution is Gaussian with covariance matrix K \u2208 R, the distribution factorizes if and only if (K)ij = 0 for each (i, j) which is not an edge in Q [15, 16].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": ", we project all our covariance matrices onto a graphical model, which is a classical tool in probabilistic modelling [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "We leave the study of the approximation properties of such a projection (in particular along the lines of [17]) to future work.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "For decomposable graphical models, \u03a0Q(K) can be obtained in closed form [15] and its determinant has the following simple expression: log |\u03a0Q(K)| = \u2211", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "Dynamic programming recursions In order to derive dynamic programming recursions, we follow [13] and rely on the fact that \u03b1-ary \u03b2-tree-walks of G can essentially be defined through 1-tree-walks on the augmented graph of all subtrees of G of depth at most \u03b2 \u2212 1 and arity less than \u03b1.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Our learning task it to classify those characters; we use a one-vs-one multiclass scheme with 2-norm support vector machines [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 119, "endOffset": 126}, {"referenceID": 17, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 119, "endOffset": 126}, {"referenceID": 18, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 221, "endOffset": 225}], "year": 2008, "abstractText": "Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.", "creator": "LaTeX with hyperref package"}}}