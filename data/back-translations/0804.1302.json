{"id": "0804.1302", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2008", "title": "Bolasso: model consistent Lasso estimation through the bootstrap", "abstract": "We look at the problem of the least square linear regression with regularization by the l1 standard, a problem commonly referred to as lasso. In this paper, we present a detailed asymptotic analysis of the model consistency of the lasso. For various decays of the regularization parameter, we calculate asymptotic equivalents of the probability of correct model selection (i.e., variable selection).For a specific rate decay, we show that the lasso selects all variables to enter the model, with the probability of an exponentially high one, while selecting all other variables with a strictly positive probability.We show that this property implies that when we perform the lasso for multiple bootstrapped replications of a given sample, then cutting the carriers of the lasso bootstrap estimation leads to a consistent model selection. This novel variable selection algorithm is used to store data ression on the other method, called the inasymptomatic one.", "histories": [["v1", "Tue, 8 Apr 2008 15:40:03 GMT  (89kb)", "http://arxiv.org/abs/0804.1302v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["francis r bach"], "accepted": true, "id": "0804.1302"}, "pdf": {"name": "0804.1302.pdf", "metadata": {"source": "CRF", "title": "Bolasso: Model Consistent Lasso Estimation through the Bootstrap", "authors": ["Francis R. Bach"], "emails": ["francis.bach@mines.org"], "sections": [{"heading": null, "text": "ar X\niv :0\n80 4.\n13 02\nv1 [\ncs .L"}, {"heading": "1 Introduction", "text": "Regularization by the \u21131-norm has attracted a lot of interest in recent years in machine learning, statistics and signal processing. In the context of least-square linear regression, the problem is usually referred to as the Lasso (Tibshirani, 1994). Much of the early effort has been dedicated to algorithms to solve the optimization problem efficiently. In particular, the Lars algorithm of Efron et al. (2004) allows to find the entire regularization path (i.e., the set of solutions for all values of the regularization parameters) at the cost of a single matrix inversion.\nMoreover, a well-known justification of the regularization by the \u21131-norm is that it leads to sparse solutions, i.e., loading vectors with many zeros, and thus performs model selection. Recent works (Zhao & Yu, 2006; Yuan & Lin,\n1\n2007; Zou, 2006; Wainwright, 2006) have looked precisely at the model consistency of the Lasso, i.e., if we know that the data were generated from a sparse loading vector, does the Lasso actually recover it when the number of observed data points grows? In the case of a fixed number of covariates, the Lasso does recover the sparsity pattern if and only if a certain simple condition on the generating covariance matrices is verified (Yuan & Lin, 2007). In particular, in low correlation settings, the Lasso is indeed consistent. However, in presence of strong correlations, the Lasso cannot be consistent, shedding light on potential problems of such procedures for variable selection. Adaptive versions where data-dependent weights are added to the \u21131-norm then allow to keep the consistency in all situations (Zou, 2006).\nIn this paper, we first derive a detailed asymptotic analysis of sparsity pattern selection of the Lasso estimation procedure, that extends previous analysis (Zhao & Yu, 2006; Yuan & Lin, 2007; Zou, 2006), by focusing on a specific decay of the regularization parameter. We show that when the decay is proportional to n\u22121/2, where n is the number of observations, then the Lasso will select all the variables that should enter the model (the relevant variables) with probability tending to one exponentially fast with n, while it selects all other variables (the irrelevant variables) with strictly positive probability. If several datasets generated from the same distribution were available, then the latter property would suggest to consider the intersection of the supports of the Lasso estimates for each dataset: all relevant variables would always be selected for all datasets, while irrelevant variables would enter the models randomly, and intersecting the supports from sufficiently many different datasets would simply eliminate them. However, in practice, only one dataset is given; but resampling methods such as the bootstrap are exactly dedicated to mimic the availability of several datasets by resampling from the same unique dataset (Efron & Tibshirani, 1998). In this paper, we show that when using the bootstrap and intersecting the supports, we actually get a consistent model estimate, without the consistency condition required by the regular Lasso. We refer to this new procedure as the Bolasso (bootstrap-enhanced least absolute shrinkage operator). Finally, our Bolasso framework could be seen as a voting scheme applied to the supports of the bootstrap Lasso estimates; however, our procedure may rather be considered as a consensus combination scheme, as we keep the (largest) subset of variables on which all regressors agree in terms of variable selection, which is in our case provably consistent and also allows to get rid of a potential additional hyperparameter.\nThe paper is organized as follows: in Section 2, we present the asymptotic analysis of model selection for the Lasso; in Section 3, we describe the Bolasso algorithm as well as its proof of model consistency, while in Section 4, we illustrate our results on synthetic data, where the true sparse generating model is known, and data from the UCI machine learning repository. Sketches of proofs can be found in Appendix A.\n2\nNotations For a vector v \u2208 Rp, we let denote \u2016v\u20162 = (v \u22a4v)1/2 the \u21132-norm, \u2016v\u2016\u221e = maxi\u2208{1,...,p} |vi| the \u2113\u221e-norm and \u2016v\u20161 = \u2211p\ni=1 |vi| the \u21131-norm. For a \u2208 R, sign(a) denotes the sign of a, defined as sign(a) = 1 if a > 0, \u22121 if a < 0, and 0 if a = 0. For a vector v \u2208 Rp, sign(v) \u2208 Rp denotes the the vector of signs of elements of v.\nMoreover, given a vector v \u2208 Rp and a subset I of {1, . . . , p}, vI denotes the vector in RCard(I) of elements of v indexed by I. Similarly, for a matrix A \u2208 Rp\u00d7p, AI,J denotes the submatrix of A composed of elements of A whose rows are in I and columns are in J ."}, {"heading": "2 Asymptotic Analysis of Model Selection for", "text": "the Lasso\nIn this section, we describe existing and new asymptotic results regarding the model selection capabilities of the Lasso."}, {"heading": "2.1 Assumptions", "text": "We consider the problem of predicting a response Y \u2208 R from covariates X = (X1, . . . , Xp)\n\u22a4 \u2208 Rp. The only assumptions that we make on the joint distribution PXY of (X,Y ) are the following:\n(A1) The cumulant generating functions E exp(s\u2016X\u201622) and E exp(sY 2) are fi-\nnite for some s > 0.\n(A2) The joint matrix of second order moments Q = EXX\u22a4 \u2208 Rp\u00d7p is invertible.\n(A3) E(Y |X) = X\u22a4w and var(Y |X) = \u03c32 a.s. for some w \u2208 Rp and \u03c3 \u2208 R\u2217+.\nWe let denote J = {j,wj 6= 0} the sparsity pattern of w, s = sign(w) the sign pattern of w, and \u03b5 = Y \u2212 X\u22a4w the additive noise.1 Note that our assumption regarding cumulant generating functions is satisfied when X and \u03b5 have compact support, and also, when the densities of X and \u03b5 have light tails.\nWe consider independent and identically distributed (i.i.d.) data (xi, yi) \u2208 R\np \u00d7 R, i = 1, . . . , n, sampled from PXY ; the data are given in the form of matrices Y \u2208 Rn and X \u2208 Rn\u00d7p.\nNote that the i.i.d. assumption, together with (A1-3), are the simplest assumptions for studying the asymptotic behavior of the Lasso; and it is of course of interest to allow more general assumptions, in particular growing number of variables p, more general random variables, etc. (see, e.g., Meinshausen and Yu (2006)), which are outside the scope of this paper.\n1Throughout this paper, we use boldface fonts for population quantities.\n3"}, {"heading": "2.2 Lasso Estimation", "text": "We consider the square loss function 12n \u2211n i=1(yi\u2212w \u22a4xi) 2 = 12n\u2016Y \u2212Xw\u2016 2 2 and the regularization by the \u21131-norm defined as \u2016w\u20161 = \u2211p\ni=1 |wi|. That is, we look at the following Lasso optimization problem (Tibshirani, 1994):\nmin w\u2208Rp\n1 2n\u2016Y \u2212Xw\u2016 2 2 + \u00b5n\u2016w\u20161, (1)\nwhere \u00b5n > 0 is the regularization parameter. We denote w\u0302 any global minimum of Eq. (1)\u2014it may not be unique in general, but will with probability tending to one exponentially fast under assumption (A1)."}, {"heading": "2.3 Model Consistency - General Results", "text": "In this section, we detail the asymptotic behavior of the Lasso estimate w\u0302, both in terms of the difference in norm with the population value w (i.e., regular consistency) and of the sign pattern sign(w\u0302), for all asymptotic behaviors of the regularization parameter \u00b5n. Note that information about the sign pattern includes information about the support, i.e., the indices i for which w\u0302i is different from zero; moreover, when w\u0302 is consistent, consistency of the sign pattern is in fact equivalent to the consistency of the support.\nWe now consider five mutually exclusive possible situations which explain various portions of the regularization path (we assume (A1-3)); many of these results appear elsewhere (Yuan & Lin, 2007; Zhao & Yu, 2006; Fu & Knight, 2000; Zou, 2006; Bach, 2007) but some of the finer results presented below are new (see Section 2.4).\n1. If \u00b5n tends to infinity, then w\u0302 = 0 with probability tending to one.\n2. If \u00b5n tends to a finite strictly positive constant \u00b50, then w\u0302 converges in probability to the unique global minimum of 12 (w\u2212w)\n\u22a4Q(w\u2212w)+\u00b50\u2016w\u20161. Thus, the estimate w\u0302 never converges in probability to w, while the sign pattern tends to the one of the previous global minimum, which may or may not be the same as the one of w.2\n3. If \u00b5n tends to zero slower than n \u22121/2, then w\u0302 converges in probability to w\n(regular consistency) and the sign pattern converges to the sign pattern of the global minimum of 12v \u22a4Qv+v\u22a4 J sign(wJ)+\u2016vJc\u20161. This sign pattern is equal to the population sign vector s = sign(w) if and only if the following consistency condition is satisfied:\n\u2016QJcJQ \u22121 JJ sign(wJ)\u2016\u221e 6 1. (2)\nThus, if Eq. (2) is satisfied, the probability of correct sign estimation is tending to one, and to zero otherwise (Yuan & Lin, 2007).\n2Here and in the third regime, we do not take into account the pathological cases where the sign pattern of the limit in unstable, i.e., the limit is exactly at a hinge point of the regularization path.\n4\n4. If \u00b5n = \u00b50n \u22121/2 for \u00b50 \u2208 (0,\u221e), then the sign pattern of w\u0302 agrees on\nJ with the one of w with probability tending to one, while for all sign patterns consistent on J with the one of w, the probability of obtaining this pattern is tending to a limit in (0, 1) (in particular strictly positive); that is, all patterns consistent on J are possible with positive probability. See Section 2.4 for more details.\n5. If \u00b5n tends to zero faster than n \u22121/2, then w\u0302 is consistent (i.e., converges\nin probability to w) but the support of w\u0302 is equal to {1, . . . , p} with probability tending to one (the signs of variables in Jc may be negative or positive). That is, the \u21131-norm has no sparsifying effect.\nAmong the five previous regimes, the only ones with consistent estimates (in norm) and a sparsity-inducing effect are \u00b5n tending to zero and \u00b5nn\n1/2 tending to a limit \u00b50 \u2208 (0,\u221e] (i.e., potentially infinite). When \u00b50 = +\u221e, then we can only hope for model consistent estimates if the consistency condition in Eq. (2) is satisfied. This somewhat disappointing result for the Lasso has led to various improvements on the Lasso to ensure model consistency even when Eq. (2) is not satisfied (Yuan & Lin, 2007; Zou, 2006). Those are based on adaptive weights based on the non regularized least-square estimate. We propose in Section 3 an alternative way which is based on resampling.\nIn this paper, we now consider the specific case where \u00b5n = \u00b50n \u22121/2 for \u00b50 \u2208 (0,\u221e), where we derive new asymptotic results. Indeed, in this situation, we get the correct signs of the relevant variables (those in J) with probability tending to one, but we also get all possible sign patterns consistent with this, i.e., all other variables (those not in J) may be non zero with asymptotically strictly positive probability. However, if we were to repeat the Lasso estimation for many datasets obtained from the same distribution, we would obtain for each \u00b50, a set of active variables, all of which include J with probability tending to one, but potentially containing all other subsets. By intersecting those, we would get exactly J.\nHowever, this requires multiple copies of the samples, which are not usually available. Instead, we consider bootstrapped samples which exactly mimic the behavior of having multiple copies. See Section 3 for more details.\n2.4 Model Consistency with Exact Root-n Regularization\nDecay\nIn this section we present detailed new results regarding the pattern consistency for \u00b5n tending to zero exactly at rate n \u22121/2 (see proofs in Appendix A):\nProposition 1 Assume (A1-3) and \u00b5n = \u00b50n \u22121/2, \u00b50 > 0. Then for any sign pattern s \u2208 {\u22121, 0, 1}p such that sJ = sign(wJ), P(sign(w\u0302) = s) tends to a limit \u03c1(s, \u00b50) \u2208 (0, 1), and we have:\nP(sign(w\u0302) = s)\u2212 \u03c1(s, \u00b50) = O(n \u22121/2 logn).\n5\nProposition 2 Assume (A1-3) and \u00b5n = \u00b50n \u22121/2, \u00b50 > 0. Then, for any pattern s \u2208 {\u22121, 0, 1}p such that sJ 6= sign(wJ), there exist a constant A(\u00b50) > 0 such that\nlogP(sign(w\u0302) = s) 6 \u2212nA(\u00b50) +O(n \u22121/2).\nThe last two propositions state that we get all relevant variables with probability tending to one exponentially fast, while we get exactly get all other patterns with probability tending to a limit strictly between zero and one. Note that the results that we give in this paper are valid for finite n, i.e., we could derive actual bounds on probability of sign pattern selections with known constants that explictly depend on w, Q and PXY ."}, {"heading": "3 Bolasso: Bootstrapped Lasso", "text": "Given the n i.i.d. observations (xi, yi) \u2208 R d \u00d7R, i = 1, . . . , n, given by matrices X \u2208 Rn\u00d7p and Y \u2208 Rn, we consider m bootstrap replications of the n data points (Efron & Tibshirani, 1998); that is, for k = 1, . . . ,m, we consider a\nghost sample (xki , y k i ) \u2208 R\nd \u00d7 R, i = 1, . . . , n, given by matrices X k \u2208 Rn\u00d7p\nand Y k \u2208 Rn. The n pairs (xki , y k i ), i = 1, . . . , n, are sampled uniformly at random with replacement from the n original pairs in (X,Y ). The sampling of the nm pairs of observations is independent. In other words, we defined the distribution of the ghost sample (X \u2217 , Y \u2217 ) by sampling n points with replacement from (X,Y ), and, given (X,Y ), the m ghost samples are independently sampled i.i.d. from the distribution of (X \u2217 , Y \u2217 ).\nThe asymptotic analysis from Section 2 suggests to estimate the supports Jk = {j, w\u0302 k j 6= 0} of the Lasso estimates w\u0302\nk for the bootstrap samples, k = 1, . . . ,m, and to intersect them to define the Bolasso model estimate of the support: J =\n\u22c2m k=1 Jk. Once J is selected, we estimate w by the unregularized\nleast-square fit restricted to variables in J . The detailed algorithm is given in Algorithm 1. The algorithm has only one extra parameter (the number of bootstrap samples m). Following Proposition 3, log(m) should be chosen growing with n asymptotically slower than n. In simulations, we always use m = 128 (except in Figure 3, where we exactly study the influence of m).\nNote that in practice, the Bolasso estimate can be computed simultaneously for a large number of regularization parameters because of the efficiency of the Lars algorithm (which we use in simulations), that allows to find the entire regularization path for the Lasso at the (empirical) cost of a single matrix inversion (Efron et al., 2004). Thus computational complexity of the Bolasso is O(m(p3 + p2n)).\nThe following proposition (proved in Appendix A) shows that the previous algorithm leads to consistent model selection.\nProposition 3 Assume (A1-3) and \u00b5n = \u00b50n \u22121/2, \u00b50 > 0. Then the probability that the Bolasso does not exactly select the correct model, i.e., for all m > 0,\n6\nAlgorithm 1 Bolasso\nInput: data (X,Y ) \u2208 Rn\u00d7(p+1)\nnumber of bootstrap replicates m regularization parameter \u00b5\nfor k = 1 to m do Generate bootstrap samples (X k , Y k ) \u2208 Rn\u00d7(p+1)\nCompute Lasso estimate w\u0302k from (X k , Y k ) Compute support Jk = {j, w\u0302 k j 6= 0}\nend for Compute J =\n\u22c2m k=1 Jk\nCompute w\u0302J from (XJ , Y )\nP(J 6= J) has the following upper bound:\nP(J 6= J) 6 mA1e \u2212A2n +A3 log n n1/2 +A4 logm m ,\nwhere A1, A2, A3, A4 are strictly positive constants.\nTherefore, if log(m) tends to infinity slower than n when n tends to infinity, the Bolasso asymptotically selects with overwhelming probability the correct active variable, and by regular consistency of the restricted least-square estimate, the correct sign pattern as well. Note that the previous bound is true whether the condition in Eq. (2) is satisfied or not, but could be improved on if we suppose that Eq. (2) is satisfied. See Section 4.1 for a detailed comparison with the Lasso on synthetic examples."}, {"heading": "4 Simulations", "text": "In this section, we illustrate the consistency results obtained in this paper with a few simple simulations on synthetic examples similar to the ones used by Bach (2007) and some medium scale datasets from the UCI machine learning repository (Asuncion & Newman, 2007)."}, {"heading": "4.1 Synthetic examples", "text": "For a given dimension p, we sampled X \u2208 Rp from a normal distribution with zero mean and covariance matrix generated as follows: (a) sample a p \u00d7 p matrix G with independent standard normal distributions, (b) form Q = GG\u22a4, (c) scale Q to unit diagonal. We then selected the first Card(J) = r variables and sampled non zero loading vectors as follows: (a) sample each loading from independent standard normal distributions, (b) rescale those to unit magnitude, (c) rescale those by a scaling which is uniform at random between 13 and 1 (to ensure minj\u2208J |wj | > 1/3). Finally, we chose a constant noise level \u03c3 equal to 0.1 times (E(w\u22a4X)2)1/2, and the additive noise \u03b5 is normally distributed with zero mean and variance \u03c32. Note that the joint distribution on (X,Y )\n7\nthus defined satisfies with probability one (with respect to the sampling of the covariance matrix) assumptions (A1-3).\nIn Figure 1, we sampled two distributions PXY with p = 16 and r = 8 relevant variables, one for which the consistency condition in Eq. (2) is satisfied (left), one for which it was not satisfied (right). For a fixed number of sample n = 1000, we generated 256 replications and computed the empirical frequencies of selecting any given variable for the Lasso as the regularization parameter \u00b5 varies. Those plots show the various asymptotic regimes of the Lasso detailed in Section 2. In particular, on the right plot, although no \u00b5 leads to perfect selection (i.e., exactly variables with indices less than r = 8 are selected), there is a range where all relevant variables are always selected, while all others are selected with probability within (0, 1).\nIn Figure 2, we plot the results under the same conditions for the Bolasso (with a fixed number of bootstrap replications m = 128). We can see that in the Lasso-consistent case (left), the Bolasso widens the consistency region, while in the Lasso-inconsistent case (right), the Bolasso \u201ccreates\u201d a consistency region.\n8\nIn Figure 3, we selected the same two distributions and compared the probability of exactly selecting the correct support pattern, for the Lasso, and for the Bolasso with varying numbers of bootstrap replications (those probabilities are computed by averaging over 256 experiments with the same distribution). In Figure 3, we can see that in the Lasso-inconsistent case (right), the Bolasso indeed allows to fix the unability of the Lasso to find the correct pattern. Moreover, increasing m looks always beneficial; note that although it seems to contradict the asymptotic analysis in Section 3 (which imposes an upper bound for consistency), this is due to the fact that not selecting (at least) the relevant variables has very low probability and is not observed with only 256 replications.\nFinally, in Figure 4, we compare various variable selection procedures for linear regression, to the Bolasso, with two distributions where p = 64, r = 8 and varying n. For all the methods we consider, there is a natural way to select exactly r variables with no free parameters (for the Bolasso, we select the most stable pattern with r elements, i.e., the pattern which corresponds to most values of \u00b5). We can see that the Bolasso outperforms all other variable selection methods, even in settings where the number of samples becomes of the order of the number of variables, which requires additional theoretical analysis, subject of ongoing research. Note in particular that we compare with bagging of least-square regression (Breiman, 1996a) followed by a thresholding of the loading vector, which is another simple way of using bootstrap samples: the Bolasso provides a more efficient way to use the extra information, not for usual stabilization purposes (Breiman, 1996b), but directly for model selection. Note finally, that the bagging of Lasso estimates requires an additional parameter and is thus not tested."}, {"heading": "4.2 UCI datasets", "text": "The previous simulations have shown that the Bolasso is succesful at performing model selection in synthetic examples. We now apply it to several linear\n9\nregression problems and compare it to alternative methods for linear regression, namely, ridge regression, Lasso, bagging of Lasso estimates (Breiman, 1996a), and a soft version of the Bolasso (referred to as Bolasso-S), where instead of intersecting the supports for each bootstrap replications, we select those which are present in at least 90% of the bootstrap replications. In Table 1, we consider data randomly generated as in Section 4.1 (with p = 32, r = 8, n = 64), where the true model is known to be composed of a sparse loading vector, while in Table 2, we consider regression datasets from the UCI machine learning repository. For all of those, we perform 10 replications of 10-fold cross validation and for all methods (which all have one free regularization parameter), we select the best regularization parameter on the 100 folds and plot the mean square prediction error and its standard deviation.\nNote that when the generating model is actually sparse (Table 1), the Bolasso outperforms all other models, while in other cases (Table 2) the Bolasso is sometimes too strict in intersecting models, i.e., the softened version works better and is competitive with other methods. Studying the effects of this softened scheme (which is more similar to usual voting schemes), in particular in terms of the potential trade-off between good model selection and low prediction error, and under conditions where p is large, is the subject of ongoing work."}, {"heading": "5 Conclusion", "text": "We have presented a detailed analysis of variable selection properties of a boostrapped version of the Lasso. The model estimation procedure, referred to as the Bolasso, is provably consistent under general assumptions. This work brings to light that poor variable selection results of the Lasso may be easily enhanced thanks to a simple parameter-free resampling procedure. Our contribution also suggests that the use of bootstrap samples by L. Breiman in\n10\nBagging/Arcing/Random Forests (Breiman, 1998) may have been so far slightly overlooked and considered a minor feature, while using boostrap samples may actually be a key computational feature in such algorithms for good model selection performances, and eventually good prediction performances on real datasets.\nThe current work could be extended in various ways: first, we have focused on a fixed total number of variables, and allowing the numbers of variables to grow is important in theory and in practice (Meinshausen & Yu, 2006). Second, the same technique can be applied to similar settings than least-square regression with the \u21131-norm, namely regularization by block \u21131-norms (Bach, 2007) and other losses such as general convex classification losses. Finally, theoretical and practical connections could be made with other work on resampling methods and boosting (Bu\u0308hlmann, 2006)."}, {"heading": "A Proof of Model Consistency Results", "text": "In this appendix, we give sketches of proofs for the asymptotic results presented in Section 2 and Section 3. The proofs rely on the well-known property of the Lasso optimization problems, namely that if the sign pattern of the solution is\n11\nknown, then we can get the solution in closed form.\nA.1 Optimality Conditions\nWe let denote \u03b5 = Y \u2212Xw \u2208 Rn, Q = X \u22a4 X/n \u2208 Rp\u00d7p and q = X \u22a4 \u03b5/n \u2208 Rp. First, we can equivalently rewrite Eq. (1) as:\nmin w\u2208Rp\n1 2 (w \u2212 w) \u22a4Q(w \u2212 w)\u2212 q\u22a4(w \u2212 w) + \u00b5n\u2016w\u20161. (3)\nThe optimality conditions for Eq. (3) can be written in terms of the sign pattern s = s(w) = sign(w) and the sparsity pattern J = J(w) = {j, wj 6= 0} (Yuan & Lin, 2007):\n\u2016(QJcJQ \u22121 JJQJJ \u2212QJcJ)wJ + (QJcJQ \u22121 JJqJ \u2212 qJc)\n+\u00b5nQJcJQ \u22121 JJsJ \u2016\u221e 6 \u00b5n, (4)\nsign(Q\u22121JJQJJwJ +Q \u22121 JJqJ \u2212 \u00b5nQ \u22121 JJsJ ) = sJ . (5)\nIn this paper, we focus on regularization parameters \u00b5n of the form \u00b5n = \u00b50n\n\u22121/2. The main idea behind the results is to consider that (Q, q) are distributed according to their limiting distributions, obtained from the law of large numbers and the central limit theorem, i.e., Q converges to Q a.s. and n1/2q is asymptotically normally distributed with mean zero and covariance matrix \u03c32Q. When assuming this, Propositions 1 and 2 are straightforward. The main effort is to make sure that we can safely replace (Q, q) by their limiting distributions. The following lemmas give sufficient conditions for correct estimation of the signs of variables in J and for selecting a given pattern s (note that all constants could be expressed in terms of Q and w, details are omitted here):\nLemma 1 Assume (A2) and \u2016Q \u2212 Q\u20162 6 \u03bbmin(Q)/2. Then sign(w\u0302J) 6= sign(wJ) implies \u2016Q \u22121/2q\u20162 > C1 \u2212 \u00b5nC2, where C1, C2 > 0.\nLemma 2 Assume (A2) and let s \u2208 {\u22121, 0, 1}p such that sJ = sign(wJ). Let J = {j, sj 6= 0} \u2283 J. Assume\n\u2016Q\u2212Q\u20162 6 min {\u03b71, \u03bbmin(Q)/2} , (6)\n\u2016Q\u22121/2q\u20162 6 min{\u03b72, C1 \u2212 \u00b5nC4}, (7)\n\u2016QJcJQ \u22121 JJqJ \u2212 qJc \u2212 \u00b5nQJcJQ \u22121 JJsJ\u2016\u221e 6 \u00b5n\n\u2212C5\u03b71\u00b5n \u2212 C6\u03b71\u03b72, (8)\n\u2200i \u2208 J\\J, si [ Q\u22121JJ (qJ\u2212\u00b5nsJ ) ] i >\u00b5nC7\u03b71+C8\u03b71\u03b72, (9)\nwith C4, C5, C6, C7, C8 are positive constants. Then sign(w\u0302) = sign(w).\nThose two lemmas are interesting because they relate optimality of certain sign patterns to quantities from which we can derive concentration inequalities.\n12\nA.2 Concentration Inequalities\nThroughout the proofs, we need to provide upper bounds on the following quantities P(\u2016Q\u22121/2q\u20162 > \u03b1) and P(\u2016Q\u2212 Q\u20162 > \u03b7). We obtain, following standard arguments (Boucheron et al., 2004): if \u03b1 < C9 and \u03b7 < C10 (where C9, C10 > 0 are constants),\nP(\u2016Q\u22121/2q\u20162 > \u03b1) 6 4p exp ( \u2212 n\u03b1 2\n2pC9\n)\n.\nP(\u2016Q\u2212 Q\u20162 > \u03b7) 6 4p 2 exp\n(\n\u2212 n\u03b7 2\n2p2C10\n)\n.\nWe also consider multivariate Berry-Esseen inequalities (Bentkus, 2003); the probability P(n1/2q \u2208 C) can be estimated as P(t \u2208 C) where t is normal with mean zero and covariance matrix \u03c32Q. The error is then uniformly (for all convex sets C) upperbounded by:\n400p1/4n\u22121/2\u03bbmin(Q) \u22123/2 E|\u03b5|3\u2016X\u201632 = C11n \u22121/2.\nA.3 Proof of Proposition 1\nBy Lemma 2, for any given A, and n large enough, the probability that the sign is different from s is upperbounded by\nP\n(\n\u2016Q\u22121/2q\u20162> A(logn)1/2\nn1/2\n) + P ( \u2016Q\u2212Q\u20162> A(logn)1/2\nn1/2\n)\n+P {t /\u2208 C(s, \u00b50(1\u2212 \u03b1))} + 2C11n \u22121/2,\nwhere C(s, \u03b2) is the set of t such that (a) \u2016QJcJQ \u22121 JJ tJ\u2212tJc\u2212\u03b2QJcJQ \u22121 JJsJ\u2016\u221e 6\n\u03b2 and (b) for all i \u2208 J\\J, si [ Q\u22121JJ (tJ \u2212 \u03b2sJ) ] i > 0. Note that here \u03b1 = O((log n)n\u22121/2) tends to zero and that we have: P {t /\u2208 C(s, \u00b50(1 \u2212 \u03b1))} 6 P {t /\u2208 C(s, \u00b50)}+O(\u03b1). All terms (ifA is large enough) are thusO((log n)n\n\u22121/2). This shows that P(sign(w\u0302) = sign(w)) > \u03c1(s, \u00b50) + O((log n)n\n\u22121/2) where \u03c1(s, \u00b50) = P {t \u2208 C(s, \u00b50)} \u2208 (0, 1)\u2013the probability is strictly between 0 and 1 because the set and its complement have non empty interiors and the normal distribution has a positive definite covariance matrix \u03c32Q. The other inequality can be proved similarly. Note that the constant in O((log n)n\u22121/2) depends on \u00b50 but by carefully considering this dependence on \u00b50, we could make the inequality uniform in \u00b50 as long as \u00b50 tends to zero or infinity at most at a logarithmic speed (i.e., \u00b5n deviates from n\n\u22121/2 by at most a logarithmic factor). Also, it would be interesting to consider uniform bounds on portions of the regularization path.\nA.4 Proof of Proposition 2\nFrom Lemma 1, the probability of not selecting any of the variables in J is upperbounded by P(\u2016Q\u22121/2q\u20162 > C1 \u2212 \u00b5nC2) + P(\u2016Q \u2212 Q\u20162 > \u03bbmin(Q)/2), which is straightforwardly upper bounded (using Section A.2) by a term of the required form.\n13\nA.5 Proof of Proposition 3\nIn order to simplify the proof, we made the simplifying assumption that the random variables X and \u03b5 have compact supports. Extending the proofs to take into account the looser condition that \u2016X\u20162 and \u03b52 have non uniformly infinite cumulant generating functions (i.e., assumption (A1)) can be done with minor changes. The probability that\n\u22c2m k=1 Jk is different from J is upper bounded by\nthe sum of the following probabilities:\n(a) Selecting at least variables in J: the probability that for the k-th replication, one index in J is not selected, each of them which is upper bounded by P(\u2016Q\u22121/2q\u2217\u20162 > C1/2)+ P(\u2016Q\u2212Q \u2217\u20162 > \u03bbmin(Q)/2), where q \u2217 corresponds to the ghost sample; as common in theoretical analysis of the bootstrap, we relate q\u2217 to q as follows: P(\u2016Q\u22121/2q\u2217\u20162 > C1/2) 6 P(\u2016Q\n\u22121/2(q\u2217 \u2212 q)\u20162 > C1/4) + P(\u2016Q \u22121/2q\u20162 > C1/4) (and similarly for P(\u2016Q\u2212Q \u2217\u20162 > \u03bbmin(Q)/2)). Because we have assumed that X and \u03b5 have compact supports, the bootstrapped variables have also compact support and we can use concentration inequalities (given the original variables X, and also after expectation with respect toX). Thus the probability for one bootstrap replication is upperbounded by Be\u2212Cn where B and C are strictly positive constants. Thus the overall contribution of this part is less than mBe\u2212Cn.\n(b) Selecting at most variables in J: the probability that for all replications, the set J is not exactly selected (note that this is not tight at all since on top of the relevant variables which are selected with overwhelming probability, different additional variables may be selected for different replications and cancel out when intersecting).\nOur goal is thus to bound E { P(J\u2217 6= J|X)m } . By previous lemmas, we\nhave that P(J\u2217 6= J|X) is upper bounded by P ( \u2016Q\u22121/2q\u2217\u20162 > A(logn)1/2\nn1/2 |X\n)\n+\nP\n(\n\u2016Q\u2212Q\u2217\u20162 > A(logn)1/2\nn1/2 |X\n)\n+P(t\u2217 /\u2208 C(\u00b50)|X)+2C11n \u22121/2+O( logn\nn1/2 ), where\nnow, given X,Y , t\u2217 is normally distributed with mean n1/2q and covariance matrix 1n \u2211n i=1 \u03b5 2 ixix \u22a4 i .\nThe first two terms and the last two ones are uniformly O( log n n1/2\n) (if A is large enough). We then have to consider the remaining term. We have C(\u00b50) = {t\n\u2217 \u2208 R\np, \u2016QJcJQ \u22121 JJ t\u2217 J \u2212 t\u2217 Jc \u2212 \u00b50QJcJQ \u22121 JJ\nsJ\u2016\u221e 6 \u00b50}. By Hoeffding\u2019s inequality, we can replace the covariance matrix that depends on X and Y by \u03c32Q, at cost O(n\u22121/2). We thus have to bound P(n1/2q + y /\u2208 C(\u00b50)|q) for y normally distributed and C(\u00b50) a fixed compact set. Because the set is compact, there exist constants A,B > 0 such that, if \u2016n1/2q\u20162 6 \u03b1 for \u03b1 large enough, then P(n1/2q + y /\u2208 C(\u00b50)|q) 6 1 \u2212 Ae \u2212B\u03b12 . Thus, by truncation, we obtain a bound of the form: E { P(J\u2217 6= J|X)m } 6 (1\u2212Ae\u2212B\u03b1 2\n+F logn n1/2 )m +Ce\u2212B\u03b1 2 6\nexp(\u2212mAe\u2212B\u03b1 2 +mF logn n1/2 )+Ce\u2212B\u03b1 2 , where we have used Hoeffding\u2019s inequality to upper bound P(\u2016n1/2q\u20162 > \u03b1). By minimizing in closed form with respect to e\u2212B\u03b1 2 , i.e., with e\u2212B\u03b1 2\n= F logn An1/2 + log(mA/C)mA , we obtain the desired inequality.\n14"}], "references": [{"title": "Consistency of the group Lasso and multiple kernel learning (Tech", "author": ["F. Bach"], "venue": "report HAL-00164735). http://hal.archives-ouvertes.fr.", "citeRegEx": "Bach,? 2007", "shortCiteRegEx": "Bach", "year": 2007}, {"title": "On the dependence of the Berry\u2013Esseen bound on dimension", "author": ["V. Bentkus"], "venue": "Journal of Statistical Planning and Inference, 113, 385\u2013402.", "citeRegEx": "Bentkus,? 2003", "shortCiteRegEx": "Bentkus", "year": 2003}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 24, 123\u2013140.", "citeRegEx": "Breiman,? 1996a", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Heuristics of instability and stabilization in model selection", "author": ["L. Breiman"], "venue": "Ann. Stat., 24, 2350\u20132383.", "citeRegEx": "Breiman,? 1996b", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Arcing classifier", "author": ["L. Breiman"], "venue": "Ann. Stat., 26, 801\u2013849.", "citeRegEx": "Breiman,? 1998", "shortCiteRegEx": "Breiman", "year": 1998}, {"title": "Boosting for high-dimensional linear models", "author": ["P. B\u00fchlmann"], "venue": "Ann. Stat., 34, 559\u2013583.", "citeRegEx": "B\u00fchlmann,? 2006", "shortCiteRegEx": "B\u00fchlmann", "year": 2006}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1998\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1998}, {"title": "Asymptotics for Lasso-type estimators", "author": ["W. Fu", "K. Knight"], "venue": "Ann. Stat.,", "citeRegEx": "Fu and Knight,? \\Q2000\\E", "shortCiteRegEx": "Fu and Knight", "year": 2000}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data (Tech. report 720)", "author": ["N. Meinshausen", "B. Yu"], "venue": "Dpt. of Statistics, UC Berkeley", "citeRegEx": "Meinshausen and Yu,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and Yu", "year": 2006}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B, 58, 267\u2013288.", "citeRegEx": "Tibshirani,? 1994", "shortCiteRegEx": "Tibshirani", "year": 1994}, {"title": "Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1-constrained quadratic programming (Tech", "author": ["M.J. Wainwright"], "venue": "report 709). Dpt. of Statistics, UC Berkeley.", "citeRegEx": "Wainwright,? 2006", "shortCiteRegEx": "Wainwright", "year": 2006}, {"title": "On the non-negative garrotte estimator", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "Yuan and Lin,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2007}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mac. Learn. Res.,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Am. Stat. Ass., 101, 1418\u20131429.", "citeRegEx": "Zou,? 2006", "shortCiteRegEx": "Zou", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "In the context of least-square linear regression, the problem is usually referred to as the Lasso (Tibshirani, 1994).", "startOffset": 98, "endOffset": 116}, {"referenceID": 9, "context": "In the context of least-square linear regression, the problem is usually referred to as the Lasso (Tibshirani, 1994). Much of the early effort has been dedicated to algorithms to solve the optimization problem efficiently. In particular, the Lars algorithm of Efron et al. (2004) allows to find the entire regularization path (i.", "startOffset": 99, "endOffset": 280}, {"referenceID": 13, "context": "Adaptive versions where data-dependent weights are added to the l1-norm then allow to keep the consistency in all situations (Zou, 2006).", "startOffset": 125, "endOffset": 136}, {"referenceID": 13, "context": "In this paper, we first derive a detailed asymptotic analysis of sparsity pattern selection of the Lasso estimation procedure, that extends previous analysis (Zhao & Yu, 2006; Yuan & Lin, 2007; Zou, 2006), by focusing on a specific decay of the regularization parameter.", "startOffset": 158, "endOffset": 204}, {"referenceID": 8, "context": ", Meinshausen and Yu (2006)), which are outside the scope of this paper.", "startOffset": 2, "endOffset": 28}, {"referenceID": 9, "context": "That is, we look at the following Lasso optimization problem (Tibshirani, 1994):", "startOffset": 61, "endOffset": 79}, {"referenceID": 13, "context": "We now consider five mutually exclusive possible situations which explain various portions of the regularization path (we assume (A1-3)); many of these results appear elsewhere (Yuan & Lin, 2007; Zhao & Yu, 2006; Fu & Knight, 2000; Zou, 2006; Bach, 2007) but some of the finer results presented below are new (see Section 2.", "startOffset": 177, "endOffset": 254}, {"referenceID": 0, "context": "We now consider five mutually exclusive possible situations which explain various portions of the regularization path (we assume (A1-3)); many of these results appear elsewhere (Yuan & Lin, 2007; Zhao & Yu, 2006; Fu & Knight, 2000; Zou, 2006; Bach, 2007) but some of the finer results presented below are new (see Section 2.", "startOffset": 177, "endOffset": 254}, {"referenceID": 13, "context": "(2) is not satisfied (Yuan & Lin, 2007; Zou, 2006).", "startOffset": 21, "endOffset": 50}, {"referenceID": 0, "context": "In this section, we illustrate the consistency results obtained in this paper with a few simple simulations on synthetic examples similar to the ones used by Bach (2007) and some medium scale datasets from the UCI machine learning repository (Asuncion & Newman, 2007).", "startOffset": 158, "endOffset": 170}, {"referenceID": 2, "context": "Note in particular that we compare with bagging of least-square regression (Breiman, 1996a) followed by a thresholding of the loading vector, which is another simple way of using bootstrap samples: the Bolasso provides a more efficient way to use the extra information, not for usual stabilization purposes (Breiman, 1996b), but directly for model selection.", "startOffset": 75, "endOffset": 91}, {"referenceID": 3, "context": "Note in particular that we compare with bagging of least-square regression (Breiman, 1996a) followed by a thresholding of the loading vector, which is another simple way of using bootstrap samples: the Bolasso provides a more efficient way to use the extra information, not for usual stabilization purposes (Breiman, 1996b), but directly for model selection.", "startOffset": 307, "endOffset": 323}, {"referenceID": 2, "context": "regression problems and compare it to alternative methods for linear regression, namely, ridge regression, Lasso, bagging of Lasso estimates (Breiman, 1996a), and a soft version of the Bolasso (referred to as Bolasso-S), where instead of intersecting the supports for each bootstrap replications, we select those which are present in at least 90% of the bootstrap replications.", "startOffset": 141, "endOffset": 157}, {"referenceID": 4, "context": "Bagging/Arcing/Random Forests (Breiman, 1998) may have been so far slightly overlooked and considered a minor feature, while using boostrap samples may actually be a key computational feature in such algorithms for good model selection performances, and eventually good prediction performances on real datasets.", "startOffset": 30, "endOffset": 45}, {"referenceID": 0, "context": "Second, the same technique can be applied to similar settings than least-square regression with the l1-norm, namely regularization by block l1-norms (Bach, 2007) and other losses such as general convex classification losses.", "startOffset": 149, "endOffset": 161}, {"referenceID": 5, "context": "Finally, theoretical and practical connections could be made with other work on resampling methods and boosting (B\u00fchlmann, 2006).", "startOffset": 112, "endOffset": 128}, {"referenceID": 1, "context": "We also consider multivariate Berry-Esseen inequalities (Bentkus, 2003); the probability P(nq \u2208 C) can be estimated as P(t \u2208 C) where t is normal with mean zero and covariance matrix \u03c3Q.", "startOffset": 56, "endOffset": 71}], "year": 2008, "abstractText": "We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.", "creator": null}}}