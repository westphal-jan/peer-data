{"id": "0907.1814", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2009", "title": "Bayesian Query-Focused Summarization", "abstract": "We present BayeSum (for \"Bayesian Abstract\"), a model for sentence extraction in query-focused summary. BayeSum uses the common case where multiple documents are relevant to a single query. By using these documents as a corroboration for query terms, BayeSum does not suffer from a lack of information in short queries. We show that approximate conclusions in BayeSum are possible for large data sets and lead to a state-of-the-art summary system. In addition, we show how BayeSum can be understood as a legitimate query enhancement technique in language modeling for the IR frame.", "histories": [["v1", "Fri, 10 Jul 2009 13:24:55 GMT  (36kb)", "http://arxiv.org/abs/0907.1814v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["hal daum\\'e iii"], "accepted": true, "id": "0907.1814"}, "pdf": {"name": "0907.1814.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :0\n90 7.\n18 14\nv1 [\ncs .C\nL ]\n1 0\nJu l 2"}, {"heading": "1 Introduction", "text": "We describe BAYESUM, an algorithm for performing query-focused summarization in the common case that there are many relevant documents for a given query. Given a query and a collection of relevant documents, our algorithm functions by asking itself the following question: what is it about these relevant documents that differentiates them from the non-relevant documents? BAYESUM can be seen as providing a statistical formulation of this exact question.\nThe key requirement of BAYESUM is that multiple relevant documents are known for the query in question. This is not a severe limitation. In two well-studied problems, it is the de-facto standard. In standard multidocument summarization (with or without a query), we have access to known relevant documents for some user need. Similarly, in the case of a web-search application, an underlying IR engine will retrieve multiple (presumably)\nrelevant documents for a given query. For both of these tasks, BAYESUM performs well, even when the underlying retrieval model is noisy.\nThe idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998)."}, {"heading": "2 Bayesian Query-Focused Summarization", "text": "In this section, we describe our Bayesian queryfocused summarization model (BAYESUM). This task is very similar to the standard ad-hoc IR task, with the important distinction that we are comparing query models against sentence models, rather than against document models. The shortness of sentences means that one must do a good job of creating the query models.\nTo maintain generality, so that our model is applicable to any problem for which multiple relevant documents are known for a query, we formulate our model in terms of relevance judgments. For a collection of D documents and Q queries, we assume we have a D \u00d7 Q binary matrix r, where rdq = 1 if an only if document d is relevant to query q. In multidocument summarization, rdq will be 1 exactly when d is in the document set corresponding to query q; in search-engine sum-\nmarization, it will be 1 exactly when d is returned by the search engine for query q."}, {"heading": "2.1 Language Modeling for IR", "text": "BAYESUM is built on the concept of language models for information retrieval. The idea behind the language modeling techniques used in IR is to represent either queries or documents (or both) as probability distributions, and then use standard probabilistic techniques for comparing them. These probability distributions are almost always \u201cbag of words\u201d distributions that assign a probability to words from a fixed vocabulary V .\nOne approach is to build a probability distribution for a given document, pd(\u00b7), and to look at the probability of a query under that distribution: pd(q). Documents are ranked according to how likely they make the query (Ponte and Croft, 1998). Other researchers have built probability distributions over queries pq(\u00b7) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001). A third approach builds a probability distribution pq(\u00b7) for the query, a probability distribution pd(\u00b7) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002):\nKL (pq || pd) = \u2211\nw\u2208V\npq(w) log pq(w)\npd(w) (1)\nThe KL divergence between two probability distributions is zero when they are identical and otherwise strictly positive. It implicitly assumes that both distributions pq and pd have the same support: they assign non-zero probability to exactly the same subset of V; in order to account for this, the distributions pq and pd are smoothed against a background general English model. This final mode\u2014the KL model\u2014is the one on which BAYESUM is based."}, {"heading": "2.2 Bayesian Statistical Model", "text": "In the language of information retrieval, the queryfocused sentence extraction task boils down to estimating a good query model, pq(\u00b7). Once we have such a model, we could estimate sentence models for each sentence in a relevant document, and rank the sentences according to Eq (1).\nThe BAYESUM system is based on the following model: we hypothesize that a sentence appears in a document because it is relevant to some\nquery, because it provides background information about the document (but is not relevant to a known query) or simply because it contains useless, general English filler. Similarly, we model each word as appearing for one of those purposes. More specifically, our model assumes that each word can be assigned a discrete, exact source, such as \u201cthis word is relevant to query q1\u201d or \u201cthis word is general English.\u201d At the sentence level, however, sentences are assigned degrees: \u201cthis sentence is 60% about query q1, 30% background document information, and 10% general English.\u201d\nTo model this, we define a general English language model, pG(\u00b7) to capture the English filler. Furthermore, for each document dk, we define a background document language model, pdk(\u00b7); similarly, for each query qj , we define a query-specific language model pqj(\u00b7). Every word in a document dk is modeled as being generated from a mixture of pG, pdk and {pqj : query qj is relevant to document dk}. Supposing there are J total queries and K total documents, we say that the nth word from the sth sentence in document d, wdsn, has a corresponding hidden variable, zdsn that specifies exactly which of these distributions is used to generate that one word. In particular, zdsn is a vector of length 1 + J + K , where exactly one element is 1 and the rest are 0.\nAt the sentence level, we introduce a second layer of hidden variables. For the sth sentence in document d, we let \u03c0ds be a vector also of length 1 + J + K that represents our degree of belief that this sentence came from any of the models. The \u03c0dss lie in the J + K-dimensional simplex \u2206J+K = {\u03b8 = \u3008\u03b81, . . . , \u03b8J+K+1\u3009 : (\u2200i) \u03b8i \u2265 0, \u2211\ni \u03b8i = 1}. The interpretation of the \u03c0 variables is that if the \u201cgeneral English\u201d component of \u03c0 is 0.9, then 90% of the words in this sentence will be general English. The \u03c0 and z variables are constrained so that a sentence cannot be generated by a document language model other than its own document and cannot be generated by a query language model for a query to which it is not relevant.\nSince the \u03c0s are unknown, and it is unlikely that there is a \u201ctrue\u201d correct value, we place a corpuslevel prior on them. Since \u03c0 is a multinomial distribution over its corresponding zs, it is natural to use a Dirichlet distribution as a prior over \u03c0. A Dirichlet distribution is parameterized by a vector \u03b1 of equal length to the corresponding multinomial parameter, again with the positivity restric-\ntion, but no longer required to sum to one. It has continuous density over a variable \u03b81, . . . , \u03b8I given by: Dir(\u03b8 | \u03b1) = \u0393( P i \u03b1i) Q\ni \u0393(\u03b1i)\n\u220f i \u03b8 \u03b1i\u22121 i . The\nfirst term is a normalization term that ensures that \u222b\n\u2206I d\u03b8 Dir(\u03b8 | \u03b1) = 1."}, {"heading": "2.3 Generative Story", "text": "The generative story for our model defines a distribution over a corpus of queries, {qj}1:J , and documents, {dk}1:K , as follows:\n1. For each query j = 1 . . . J : Generate each word qjn in qj by pqj(qjn)\n2. For each document k = 1 . . . K and each sentence s in document k:\n(a) Select the current sentence degree \u03c0ks by Dir(\u03c0ks | \u03b1)rk(\u03c0ks) (b) For each word wksn in sentence s: \u2022 Select the word source zksn accord-\ning to Mult(z | \u03c0ks) \u2022 Generate the word wksn by \n\n pG(wksn) if zksn = 0 pdk(wksn) if zksn = k + 1 pqj(wksn) if zksn = j +K + 1\nWe used r to denote relevance judgments: rk(\u03c0) = 0 if any document component of \u03c0 except the one corresponding to k is non-zero, or if any query component of \u03c0 except those queries to which document k is deemed relevant is non-zero (this prevents a document using the \u201cwrong\u201d document or query components). We have further assumed that the z vector is laid out so that z0 corresponds to general English, zk+1 corresponds to document dk for 0 \u2264 j < J and that zj+K+1 corresponds to query qj for 0 \u2264 k < K."}, {"heading": "2.4 Graphical Model", "text": "The graphical model corresponding to this generative story is in Figure 1. This model depicts the four known parameters in square boxes (\u03b1, pQ, pD and pG) with the three observed random variables in shaded circles (the queries q, the relevance judgments r and the words w) and two unobserved random variables in empty circles (the word-level indicator variables z and the sentence level degrees \u03c0). The rounded plates denote replication: there are J queries and K documents, containing S sentences in a given document and N words in a given sentence. The joint probability over the observed random variables is given in Eq (2):\np (q1:J , r,d1:K) =\n[\n\u220f\nj\n\u220f\nn\npqj (qjn)\n]\n\u00d7 (2)\n[\n\u220f\nk\n\u220f\ns\n\u222b\n\u2206 d\u03c0ks p (\u03c0ks | \u03b1, r)\n\u220f\nn\n\u2211\nzksn\np (zksn | \u03c0ks) p (wksn | zksn)\n]\nThis expression computes the probability of the data by integrating out the unknown variables. In the case of the \u03c0 variables, this is accomplished by integrating over \u2206, the multinomial simplex, according to the prior distribution given by \u03b1. In the case of the z variables, this is accomplished by summing over all possible (discrete) values. The final word probability is conditioned on the z value by selecting the appropriate distribution from pG, pD and pQ. Computing this expression and finding optimal model parameters is intractable due to the coupling of the variables under the integral."}, {"heading": "3 Statistical Inference in BAYESUM", "text": "Bayesian inference problems often give rise to intractable integrals, and a large variety of techniques have been proposed to deal with this. The most popular are Markov Chain Monte Carlo (MCMC), the Laplace (or saddle-point) approximation and the variational approximation. A third, less common, but very effective technique, especially for dealing with mixture models, is expectation propagation (Minka, 2001). In this paper, we will focus on expectation propagation; experiments not reported here have shown variational\nEM to perform comparably but take roughly 50% longer to converge.\nExpectation propagation (EP) is an inference technique introduced by Minka (2001) as a generalization of both belief propagation and assumed density filtering. In his thesis, Minka showed that EP is very effective in mixture modeling problems, and later demonstrated its superiority to variational techniques in the Generative Aspect Model (Minka and Lafferty, 2003). The key idea is to compute an integral of a product of terms by iteratively applying a sequence of \u201cdeletion/inclusion\u201d steps. Given an integral of the form: \u222b\n\u2206 d\u03c0 p(\u03c0) \u220f\nn tn(\u03c0), EP approximates each term tn by a simpler term t\u0303n, giving Eq (3).\n\u222b\n\u2206 d\u03c0 q(\u03c0) q(\u03c0) = p(\u03c0)\n\u220f\nn\nt\u0303n(\u03c0) (3)\nIn each deletion/inclusion step, one of the approximate terms is deleted from q(\u00b7), leaving q\u2212n(\u00b7) = q(\u00b7)/t\u0303n(\u00b7). A new approximation for tn(\u00b7) is computed so that tn(\u00b7)q\u2212n(\u00b7) has the same integral, mean and variance as t\u0303n(\u00b7)q\u2212n(\u00b7). This new approximation, t\u0303n(\u00b7) is then included back into the full expression for q(\u00b7) and the process repeats. This algorithm always has a fixed point and there are methods for ensuring that the approximation remains in a location where the integral is well-defined. Unlike variational EM, the approximation given by EP is global, and often leads to much more reliable estimates of the true integral.\nIn the case of our model, we follow Minka and Lafferty (2003), who adapts latent Dirichlet allocation of Blei et al. (2003) to EP. Due to space constraints, we omit the inference algorithms and instead direct the interested reader to the description given by Minka and Lafferty (2003)."}, {"heading": "4 Search-Engine Experiments", "text": "The first experiments we run are for query-focused single document summarization, where relevant documents are returned from a search engine, and a short summary is desired of each document."}, {"heading": "4.1 Data", "text": "The data we use to train and test BAYESUM is drawn from the Text REtrieval Conference (TREC) competitions. This data set consists of queries, documents and relevance judgments, exactly as required by our model. The queries are\ntypically broken down into four fields of increasing length: the title (3-4 words), the summary (1 sentence), the narrative (2-4 sentences) and the concepts (a list of keywords). Obviously, one would expect that the longer the query, the better a model would be able to do, and this is borne out experimentally (Section 4.5).\nOf the TREC data, we have trained our model on 350 queries (queries numbered 51-350 and 401-450) and all corresponding relevant documents. This amounts to roughly 43k documents, 2.1m sentences and 65.8m words. The mean number of relevant documents per query is 137 and the median is 81 (the most prolific query has 968 relevant documents). On the other hand, each document is relevant to, on average, 1.11 queries (the median is 5.5 and the most generally relevant document is relevant to 20 different queries). In all cases, we apply stemming using the Porter stemmer; for all other models, we remove stop words.\nIn order to evaluate our model, we had seven human judges manually perform the queryfocused sentence extraction task. The judges were supplied with the full TREC query and a single document relevant to that query, and were asked to select up to four sentences from the document that best met the needs given by the query. Each judge annotated 25 queries with some overlap to allow for an evaluation of inter-annotator agreement, yielding annotations for a total of 166 unique query/document pairs. On the doubly annotated data, we computed the inter-annotator agreement using the kappa measure. The kappa value found was 0.58, which is low, but not abysmal (also, keep in mind that this is computed over only 25 of the 166 examples)."}, {"heading": "4.2 Evaluation Criteria", "text": "Since there are differing numbers of sentences selected per document by the human judges, one cannot compute precision and recall; instead, we opt for other standard IR performance measures. We consider three related criteria: mean average precision (MAP), mean reciprocal rank (MRR) and precision at 2 (P@2). MAP is computed by calculating precision at every sentence as ordered by the system up until all relevant sentences are selected and averaged. MRR is the reciprocal of the rank of the first relevant sentence. P@2 is the precision computed at the first point that two relevant sentences have been selected (in the rare case that\nhumans selected only one sentence, we use P@1)."}, {"heading": "4.3 Baseline Models", "text": "As baselines, we consider four strawman models and two state-of-the-art information retrieval models. The first strawman, RANDOM ranks sentences randomly. The second strawman, POSITION, ranks sentences according to their absolute position (in the context of non-query-focused summarization, this is an incredibly powerful baseline). The third and fourth models are based on the vector space interpretation of IR. The third model, JACCARD, uses standard Jaccard distance score (intersection over union) between each sentence and the query to rank sentences. The fourth, COSINE, uses TF-IDF weighted cosine similarity.\nThe two state-of-the-art IR models used as comparative systems are based on the language modeling framework described in Section 2.1. These systems compute a language model for each query and for each sentence in a document. Sentences are then ranked according to the KL divergence between the query model and the sentence model, smoothed against a general model estimated from the entire collection, as described in the case of document retrieval by Lavrenko et al. (2002). This is the first system we compare against, called KL.\nThe second true system, KL+REL is based on augmenting the KL system with blind relevance feedback (query expansion). Specifically, we first run each query against the document set returned by the relevance judgments and retrieve the top n sentences. We then expand the query by interpolating the original query model with a query model estimated on these sentences. This serves as a method of query expansion. We ran experiments ranging n in {5, 10, 25, 50, 100} and the interpolation parameter \u03bb in {0.2, 0.4, 0.6, 0.8} and used oracle selection (on MRR) to choose the values that performed best (the results are thus overly optimistic). These values were n = 25 and \u03bb = 0.4.\nOf all the systems compared, only BAYESUM and the KL+REL model use the relevance judgments; however, they both have access to exactly the same information. The other models only run on the subset of the data used for evaluation (the corpus language model for the KL system and the IDF values for the COSINE model are computed on the full data set). EP ran for 2.5 hours."}, {"heading": "4.4 Performance on all Query Fields", "text": "Our first evaluation compares results when all query fields are used (title, summary, description and concepts1). These results are shown in Table 1. As we can see from these results, the JACCARD system alone is not sufficient to beat the position-based baseline. The COSINE does beat the position baseline by a bit of a margin (5 points better in MAP, 9 points in MRR and 4 points in P@2), and is in turn beaten by the KL system (which is 7 points, 14 points and 4 points better in MAP, MRR and P@2, respectively). Blind relevance feedback (parameters of which were chosen by an oracle to maximize the P@2 metric) actually hurts MAP and MRR performance by 0.3 and 1.2, respectively, and increases P@2 by 1.5. Over the best performing baseline system (either KL or KL+REL), BAYESUM wins by a margin of 7.5 points in MAP, 6.7 for MRR and 4.4 for P@2."}, {"heading": "4.5 Varying Query Fields", "text": "Our next experimental comparison has to do with reducing the amount of information given in the query. In Table 2, we show the performance of the KL, KL-REL and BAYESUM systems, as we use different query fields. There are several things to notice in these results. First, the standard KL model without blind relevance feedback performs worse than the position-based model when only the 3-4 word title is available. Second, BAYESUM using only the title outperform the KL model with relevance feedback using all fields. In fact, one can apply BAYESUM without using any of the query fields; in this case, only the relevance judgments are available to make sense\n1A reviewer pointed out that concepts were later removed from TREC because they were \u201ctoo good.\u201d Section 4.5 considers the case without the concepts field.\nof what the query might be. Even in this circumstance, BAYESUM achieves a MAP of 39.4, an MRR of 64.7 and a P@2 of 30.4, still better across the board than KL-REL with all query fields. While initially this seems counterintuitive, it is actually not so unreasonable: there is significantly more information available in several hundred positive relevance judgments than in a few sentences. However, the simple blind relevance feedback mechanism so popular in IR is unable to adequately model this.\nWith the exception of the KL model without relevance feedback, adding the description on top of the title does not seem to make any difference for any of the models (and, in fact, occasionally hurts according to some metrics). Adding the summary improves performance in most cases, but not significantly. Adding concepts tends to improve results slightly more substantially than any other."}, {"heading": "4.6 Noisy Relevance Judgments", "text": "Our model hinges on the assumption that, for a given query, we have access to a collection of known relevant documents. In most real-world cases, this assumption is violated. Even in multidocument summarization as run in the DUC competitions, the assumption of access to a collection of documents all relevant to a user need is unrealistic. In the real world, we will have to deal with document collections that \u201caccidentally\u201d contain irrelevant documents. The experiments in this section show that BAYESUM is comparatively robust.\nFor this experiment, we use the IR engine that performed best in the TREC 1 evaluation: Inquery (Callan et al., 1992). We used the offi-\ncial TREC results of Inquery on the subset of the TREC corpus we consider. The Inquery Rprecision on this task is 0.39 using title only, and 0.51 using all fields. In order to obtain curves as the IR engine improves, we have linearly interpolated the Inquery rankings with the true relevance judgments. By tweaking the interpolation parameter, we obtain an IR engine with improving performance, but with a reasonable bias. We have run both BAYESUM and KL-Rel on the relevance judgments obtained by this method for six values of the interpolation parameter. The results are shown in Figure 2.\nAs we can observe from the figure, the solid lines (BAYESUM) are always above the dotted lines (KL-Rel). Considering the KL-Rel results alone, we can see that for a non-perfect IR engine, it makes little difference what query fields we use for the summarization task: they all obtain roughly equal scores. This is because the performance in KL-Rel is dominated by the performance of the IR engine. Looking only at the BAYESUM results, we can see a much stronger, and perhaps surprising difference. For an imperfect IR system, it is better to use only the title than to use the title, description and summary for the summarization component. We believe this is because the title is more on topic than the other fields, which contain terms like \u201cA relevant document should describe . . . .\u201d Never-\ntheless, BAYESUM has a more upward trend than KL-Rel, which indicates that improved IR will result in improved summarization for BAYESUM but not for KL-Rel."}, {"heading": "5 Multidocument Experiments", "text": "We present two results using BAYESUM in the multidocument summarization settings, based on the official results from the Multilingual Summarization Evaluation (MSE) and Document Understanding Conference (DUC) competitions in 2005."}, {"heading": "5.1 Performance at MSE 2005", "text": "We participated in the Multilingual Summarization Evaluation (MSE) workshop with a system based on BAYESUM. The task for this competition was generic (no query) multidocument summarization. Fortunately, not having a query is not a hindrance to our model. To account for the redundancy present in document collections, we applied a greedy selection technique that selects sentences central to the document cluster but far from previously selected sentences (Daume\u0301 III and Marcu, 2005a). In MSE, our system performed very well. According to the human \u201cpyramid\u201d evaluation, our system came first with a score of 0.529; the next best score was 0.489. In the automatic \u201cBasic Element\u201d evaluation, our system scored 0.0704 (with a 95% confidence interval of [0.0429, 0.1057]), which was the third best score on a site basis (out of 10 sites), and was not statistically significantly different from the best system, which scored 0.0981."}, {"heading": "5.2 Performance at DUC 2005", "text": "We also participated in the Document Understanding Conference (DUC) competition. The chosen task for DUC was query-focused multidocument summarization. We entered a nearly identical system to DUC as to MSE, with an additional rulebased sentence compression component (Daume\u0301 III and Marcu, 2005b). Human evaluators considered both responsiveness (how well did the summary answer the query) and linguistic quality. Our system achieved the highest responsiveness score in the competition. We scored more poorly on the linguistic quality evaluation, which (only 5 out of about 30 systems performed worse); this is likely due to the sentence compression we performed on top of BAYESUM. On the automatic Rouge-based evaluations, our system performed between third\nand sixth (depending on the Rouge parameters), but was never statistically significantly worse than the best performing systems."}, {"heading": "6 Discussion and Future Work", "text": "In this paper we have described a model for automatically generating a query-focused summary, when one has access to multiple relevance judgments. Our Bayesian Query-Focused Summarization model (BAYESUM) consistently outperforms contending, state of the art information retrieval models, even when it is forced to work with significantly less information (either in the complexity of the query terms or the quality of relevance judgments documents). When we applied our system as a stand-alone summarization model in the 2005 MSE and DUC tasks, we achieved among the highest scores in the evaluation metrics. The primary weakness of the model is that it currently only operates in a purely extractive setting.\nOne question that arises is: why does BAYESUM so strongly outperform KL-Rel, given that BAYESUM can be seen as Bayesian formalism for relevance feedback (query expansion)? Both models have access to exactly the same information: the queries and the true relevance judgments. This is especially interesting due to the fact that the two relevance feedback parameters for KLRel were chosen optimally in our experiments, yet BAYESUM consistently won out. One explanation for this performance win is that BAYESUM provides a separate weight for each word, for each query. This gives it significantly more flexibility. Doing something similar with ad-hoc query expansion techniques is difficult due to the enormous number of parameters; see, for instance, (Buckley and Salton, 1995).\nOne significant advantage of working in the Bayesian statistical framework is that it gives us a straightforward way to integrate other sources of knowledge into our model in a coherent manner. One could consider, for instance, to extend this model to the multi-document setting, where one would need to explicitly model redundancy across documents. Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002).\nOur model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005). However, our model has several advantages over this technique. First, our model has\nno tunable parameters: the random-walk method has many (graph connectivity, various thresholds, choice of similarity metrics, etc.). Moreover, since our model is properly Bayesian, it is straightforward to extend it to model other aspects of the problem, or to related problems. Doing so in a non ad-hoc manner in the random-walk model would be nearly impossible.\nAnother interesting avenue of future work is to relax the bag-of-words assumption. Recent work has shown, in related models, how this can be done for moving from bag-of-words models to bag-ofngram models (Wallach, 2006); more interesting than moving to ngrams would be to move to dependency parse trees, which could likely be accounted for in a similar fashion. One could also potentially relax the assumption that the relevance judgments are known, and attempt to integrate them out as well, essentially simultaneously performing IR and summarization.\nAcknowledgments. We thank Dave Blei and Tom Minka for discussions related to topic models, and to the\nanonymous reviewers, whose comments have been of great\nbenefit. This work was partially supported by the National\nScience Foundation, Grant IIS-0326276."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["David Blei", "Andrew Ng", "Michael Jordan."], "venue": "Journal of Machine Learning Research (JMLR), 3:993\u20131022, January.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Optimization of relevance feedback weights", "author": ["Chris Buckley", "Gerard Salton."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Buckley and Salton.,? 1995", "shortCiteRegEx": "Buckley and Salton.", "year": 1995}, {"title": "The INQUERY retrieval system", "author": ["Jamie Callan", "Bruce Croft", "Stephen Harding."], "venue": "Proceedings of the 3rd International Conference on Database and Expert Systems Applications.", "citeRegEx": "Callan et al\\.,? 1992", "shortCiteRegEx": "Callan et al\\.", "year": 1992}, {"title": "Bayesian multi-document summarization at MSE", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures.", "citeRegEx": "III and Marcu.,? 2005a", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Bayesian summarization at DUC and a suggestion for extrinsic evaluation", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Document Understanding Conference.", "citeRegEx": "III and Marcu.,? 2005b", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["John Lafferty", "ChengXiang Zhai."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Lafferty and Zhai.,? 2001", "shortCiteRegEx": "Lafferty and Zhai.", "year": 2001}, {"title": "Crosslingual relevance models", "author": ["Victor Lavrenko", "M. Choquette", "Bruce Croft."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Lavrenko et al\\.,? 2002", "shortCiteRegEx": "Lavrenko et al\\.", "year": 2002}, {"title": "Passage retrieval based on language models", "author": ["Xiaoyong Liu", "Bruce Croft."], "venue": "Processing of the Conference on Information and Knowledge Management (CIKM).", "citeRegEx": "Liu and Croft.,? 2002", "shortCiteRegEx": "Liu and Croft.", "year": 2002}, {"title": "Expectationpropagation for the generative aspect model", "author": ["Thomas Minka", "John Lafferty."], "venue": "Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Minka and Lafferty.,? 2003", "shortCiteRegEx": "Minka and Lafferty.", "year": 2003}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Thomas Minka."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.", "citeRegEx": "Minka.,? 2001", "shortCiteRegEx": "Minka.", "year": 2001}, {"title": "A translation model for sentence retrieval", "author": ["Vanessa Murdock", "Bruce Croft."], "venue": "Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 684\u2013", "citeRegEx": "Murdock and Croft.,? 2005", "shortCiteRegEx": "Murdock and Croft.", "year": 2005}, {"title": "Using random walks for questionfocused sentence retrieval", "author": ["Jahna Otterbacher", "Gunes Erkan", "Dragomir R. Radev."], "venue": "Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Lan-", "citeRegEx": "Otterbacher et al\\.,? 2005", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "A language modeling approach to information retrieval", "author": ["Jay M. Ponte", "Bruce Croft."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Ponte and Croft.,? 1998", "shortCiteRegEx": "Ponte and Croft.", "year": 1998}, {"title": "Topic modeling: beyond bagof-words", "author": ["Hanna Wallach."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Wallach.,? 2006", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Novelty and redundancy detection in adaptive filtering", "author": ["Yi Zhang", "Jamie Callan", "Thomas Minka."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Zhang et al\\.,? 2002", "shortCiteRegEx": "Zhang et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005).", "startOffset": 133, "endOffset": 179}, {"referenceID": 10, "context": "First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005).", "startOffset": 133, "endOffset": 179}, {"referenceID": 12, "context": "That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998).", "startOffset": 111, "endOffset": 134}, {"referenceID": 12, "context": "Documents are ranked according to how likely they make the query (Ponte and Croft, 1998).", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": "Other researchers have built probability distributions over queries pq(\u00b7) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001).", "startOffset": 158, "endOffset": 183}, {"referenceID": 6, "context": "A third approach builds a probability distribution pq(\u00b7) for the query, a probability distribution pd(\u00b7) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002):", "startOffset": 207, "endOffset": 230}, {"referenceID": 9, "context": "less common, but very effective technique, especially for dealing with mixture models, is expectation propagation (Minka, 2001).", "startOffset": 114, "endOffset": 127}, {"referenceID": 8, "context": "In his thesis, Minka showed that EP is very effective in mixture modeling problems, and later demonstrated its superiority to variational techniques in the Generative Aspect Model (Minka and Lafferty, 2003).", "startOffset": 180, "endOffset": 206}, {"referenceID": 8, "context": "Expectation propagation (EP) is an inference technique introduced by Minka (2001) as a generalization of both belief propagation and assumed density filtering.", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "In the case of our model, we follow Minka and Lafferty (2003), who adapts latent Dirichlet allo-", "startOffset": 36, "endOffset": 62}, {"referenceID": 0, "context": "cation of Blei et al. (2003) to EP.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "instead direct the interested reader to the description given by Minka and Lafferty (2003).", "startOffset": 65, "endOffset": 91}, {"referenceID": 6, "context": "Sentences are then ranked according to the KL divergence between the query model and the sentence model, smoothed against a general model estimated from the entire collection, as described in the case of document retrieval by Lavrenko et al. (2002). This is the first system we compare against, called KL.", "startOffset": 226, "endOffset": 249}, {"referenceID": 2, "context": "performed best in the TREC 1 evaluation: Inquery (Callan et al., 1992).", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "Doing something similar with ad-hoc query expansion techniques is difficult due to the enormous number of parameters; see, for instance, (Buckley and Salton, 1995).", "startOffset": 137, "endOffset": 163}, {"referenceID": 14, "context": "Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002).", "startOffset": 107, "endOffset": 127}, {"referenceID": 11, "context": "Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005).", "startOffset": 69, "endOffset": 95}, {"referenceID": 13, "context": "Recent work has shown, in related models, how this can be done for moving from bag-of-words models to bag-ofngram models (Wallach, 2006); more interesting than moving to ngrams would be to move to dependency parse trees, which could likely be accounted for in a similar fashion.", "startOffset": 121, "endOffset": 136}], "year": 2013, "abstractText": "We present BAYESUM (for \u201cBayesian summarization\u201d), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a stateof-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}