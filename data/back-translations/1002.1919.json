{"id": "1002.1919", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2010", "title": "Thai Rhetorical Structure Analysis", "abstract": "A Rhetorical Structure Tree (RS tree) is a representation of the discourse relationships between elementary discourse units (EDUs). An RS tree is very useful for many word processing tasks using relationships between EDUs such as text comprehension, summary, and question answers.The Thai language, with its unique linguistic properties, requires a unique RS tree construction technique. This paper proposes an approach to the Thai RS tree construction, which consists of three main steps: EDU segmentation, Thai RS tree construction, and Discourse Relationship (DR) identification. Two hidden Markov models derived from grammatical rules are used to segment EDUs, a cluster technique with its similarity measurement derived from Thai semantic rules is used to construct a Thai RS tree, and a decision tree whose rules extracted from the RS are used to evaluate the 90% of the RS tree identification technique proposed by the RS.", "histories": [["v1", "Tue, 9 Feb 2010 20:01:06 GMT  (826kb)", "http://arxiv.org/abs/1002.1919v1", "IEEE format, International Journal of Computer Science and Information Security, IJCSIS January 2010, ISSN 1947 5500,this http URL"], ["v2", "Tue, 16 Mar 2010 05:04:16 GMT  (331kb)", "http://arxiv.org/abs/1002.1919v2", "IEEE format, International Journal of Computer Science and Information Security, IJCSIS January 2010, ISSN 1947 5500,this http URL"]], "COMMENTS": "IEEE format, International Journal of Computer Science and Information Security, IJCSIS January 2010, ISSN 1947 5500,this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["somnuk sinthupoun", "ohm sornil"], "accepted": false, "id": "1002.1919"}, "pdf": {"name": "1002.1919.pdf", "metadata": {"source": "CRF", "title": "THAI RHETORICAL STRUCTURE ANALYSIS", "authors": ["Somnuk Sinthupoun"], "emails": [], "sections": [{"heading": null, "text": "Keywords- Thai Language, Element Discourse Unit, Rhetorical Structure Tree, Discourse Relation.\nI. INTRODUCTION\nA RS tree is a tree-like representation of discourse relations among elementary discourse units (EDUs) which can be defined as follows: RS tree = (Status, DR, Promotion, Left, Right) where Status is either nucleus or satellite EDU (nucleus expresses what is more essential to writer\u2019s purpose than satellite); DR is a Discourse Relation (DR); Promotion is a subset of EDUs; and Left, Right can be either NULL or recursively defined objects of type RS tree [4, 6].\nSome researchers consider an EDU to be a clause or clauselike [6] excerpt, while others consider them to be a sentence [10] in discourse parsing. A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].\nThere are many DRs. Some have a single nucleus such as elaboration and condition, while others have multiple nucleuses such as contrast [20]. A number of techniques for determining the DR between EDUs are proposed, such as using verb semantics [13] to build verb based events which represent EDUs, using cue phrase/discourse marker (e.g.,\n\u201cbecause\u201d, \u201chowever\u201d) [5], and using machine learning techniques [6].\nFor Thai RS tree construction, Sukvaree, et.al. [16] purposes a technique to construct a tree by using a spanning tree which make decision by discourse marker and focus of EDU, into two phases which consist of local and global EDU spanning tree. Using spanning tree, RS tree construction used the right adjacent rule to add right hand side to attach left hand side EDU.\nFor Thai DR recognition, Sukvaree, et.al. [16] purposes a technique to recognize a DR by using DR marker tag to recognize DR. If DR marker tag has many semantic DRs, the maximum probability value in chain of DR marker is used to identify the DR. There are four relations: cause-result, constion, contrast and elaboration. Wattanamethanont, et.al. [11] purposes a technique to recognize a DR by using Na\u00efve bayes classifier. The feature of the machine learning are DR marker, key phrase and word co-occurrence. There are three relation: elaboration, logical and sequence.\nThis article proposes a new approach for Thai RS Tree construction which consists of two major steps: EDU segmentation and Thai RS tree construction. Two hidden markov models taking into account syntactic properties of Thai language are employed to segment EDUs, and a clustering technique with its similarity measure derived from linguistic properties of Thai language is employed to construct a Thai RS tree. Once the tree is created, DR between EDUs is then determined by a decision tree.\nII. ISSUES IN THAI RS TREE\nThai language possesses unique characteristics syntactically and semantically. This makes techniques proposed for other languages not directly applicable to Thai language. A number of important issues are discussed in this section.\nA. Issues in Thai EDU Extraction\nNo Explicit EDU boundary Thai language has no punctuation masks (comma, semi-colon and blank) and no special symbols (full stop) to identify the start and the end of EDUs. Unlike English contains specific symbols (e.g. \u2019.\u2019, comma, semi-colon and blank) to specify the start and the end of EDUs, they can be used to separate a text into EDUs.\n95 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nTherefore, EDU identification becomes a problem for Thai RS tree construction.\nEDU1 EDU2 EDU3\nOmission Problem Given two EDUs, an absence of subject, object or conjunction in anaphoric EDU may happen. Such as anaphoric EDU omits the subject that refers back to the object of antecedent EDU. Accordingly, EDU segmentation is ambiguous. Thai : \u201c \u201d (A friend\u2019s going\nto borrow this book. Because she hasn\u2019t been able to buy it.)\nThere are three Possible : 1) [S( )V( )O( )]EDU1 [Because S(\u0424) V( )]EDU2 2) [S( )V( )O( )]EDU1 [Because(\u0424)S(\u0424)V( )]EDU2 3) [S( )V( )O(\u0424)]EDU1 [Because(\u0424)S( )V( )]EDU2\nB. Issues in Rhetorical Structure Tree Construction\nAfter EDUs are extracted correctly, relationships among EDUs need to be determined, and a number of issues need to be considered.\nAdjacent Marker Problem Given three EDUs and two markers indicating different relationship, as shown in Ex. 1, there are two possible for the RS Tree. First, relationship between EDU1 and EDU2 is determined by using discourse marker \u201c \u201d (but), next that between (EDU1, EDU2) and EDU3 is determined. On the other hand, the relationship between EDU2 and EDU3 is determined first by using discourse marker \u2018 \u2019 (if), next that between (EDU2, EDU3) and EDU1 is determined. Ex. 1. EDU1: \u0e01 \" (A court has order to separate marriage property.)\nOmission Problem The absence of Subject, Object or Preposition which is modifier nucleus of VP especially in anaphoric EDU of Thai language are often occur. Ex. 2. EDU1: \u0e01 \" (A court has order to separate marriage property.)\nEDU2: \u04241 \u0e01 \"\u0e01\u0e01 \u0e01 \u04242 (A court will cancel to separate marriage property.)\nIn Ex. 2, EDU2 omits subject \u201c \u201d (court) and object\n\u2018 \" \u2019 (marriage property). Therefore, word co-occurrence only is not enough to solve relationship between EDU1 and EDU2. This research use Absence rules to solve relationship between EDU1 and EDU2.\nImplicit Marker Problem The absences of discourse marker in Thai language are often occurred. In Ex. 3, \u201c \u201d (but) is a discourse marker which is omitted but relationship between EDU1 and EDU2 still have relationship similar to that EDU1 and EDU2 have the discourse marker.\nEx. 3. EDU1: \u0e01 \" (A court has order to separate marriage property.)\nEDU2: \u04241 % \" (But a wife or a husband disagrees to separate marriage property.)\nTherefore, only use cue phrase is not enough to solve\nrelationship between EDUs such as EDU2.\nC. Issues in DR identification\nAfter RS trees are constructed correctly, DRs among EDUs need to be identified, and a number of issues need to be considered.\nAdjacent Marker Problem Given three EDUs and two markers indicating different relationship, as shown in Ex. 1, there are two possible for the DR. First, relationship between EDU1 and EDU2 is \u201ccontrast\u201d relation which is determined by using discourse marker \u201c \u201d (but), next that between (EDU1, EDU2) and EDU3 is determined. On the other hand, the relationship between EDU2 and EDU3 is \u201ccondition\u201d relation which is determined first by using discourse marker \u2018 \u2019 (if), next that between (EDU2, EDU3) and EDU1 is determined.\nImplicit DR Marker Problem The absences of DR marker in Thai language are often occurred. In Ex. 3, \u201c \u201d (but) is a \u201ccontrast\u201d relation marker which is omitted but relationship between EDU1 and EDU2 still have \u201ccontrast\u201d relation similar to that EDU1 and EDU2 have the DR marker. Therefore, only use DR marker is not enough to solve DR between EDUs such as EDU2.\nDR Marker Ambiguity Problem one DR marker can have many semantics DR such as \u201c \u201d (when) can infer\n\u201ccondition\u201d or \u201ccause-result\u201d relation, \u201c \u201d (but) can infer \u201ccontrast\u201d or \u201celaboration\u201d relation in Ex. 4.\nThai : [w1w2\u2026wmwm+1wm+2\u2026wnwn+1wn+2\u2026wo] English : [w1 w2 \u2026 wm],[wm+1 wm+2 \u2026 wn];[wn+1 wn+2 \u2026 wo].\nwhere wi is a word in text.\nIdentify applicable sponsor/s here. (sponsors)\n96 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nEx. 4. EDU1: \u0e01 \" (A court has order to separate marriage property.)\nEDU2: % \" (Butcontrast relation a wife or a husband disagree,) EDU1: \u0e01 \" (A court has order to separate marriage property.) EDU2: & % \" ' ( ) (But onlyelaboration relation a wife and a husband agree,)\nIII. STRUCTURES OF THAI EDUS\nA Thai EDU consists of infrastructure and adjunct constituents. There are twelve possible arrangements of Thai EDU [17], as shown in Table 1. The structure of an EDU \u201cA teacher usually doesn\u2019t drink alcohol\u201d is shown in Fig. 2.\nTable 1: The twelve patterns of Thai EDUs.\nIV. EDU SEGMENTATION\nThis section describes the EDU segmentation process proposed in this research. To reduce the segmentation ambiguity caused form modifiers, omissions of words and discourse markers, the first noun phrases and verb phrases which can be constituents of EDUs are determined, based on the possible structures. These phrases are then used to identify boundaries of EDUs.\nA noun phrase (NP) is a noun or a pronoun and its expansion which may function as one of the four Thai EDU constituents, namely the subject (S), the object (O), the indirect object and the Nomen (N). The structure of a noun phrase consists of five constituents which are: head (H), intransitive modifier (Mi), adjunctive modifier (Ma), quantifier (Q), and determinative (D).\nA verb phrase (VP) is a verb and its expansion which may function as one of the three Thai EDU constituents, namely intransitive verb (Vi), transitive verb (Vt) and double transitive verb (Vtt). The structure of a verb phrase consists of four constituents which are: nucleus (Nuc), pre-nuclear auxiliary (Aux1), post-nuclear auxiliary (Aux2), and modifier (M).\nAn Arrangement of NP and VP constituent [17] is shown in Table 2. There are twenty five possible arrangements of noun phrase and ten possible arrangements of verb phrases.\nTo perform the phrase identification, word segmentation and part of speech (POS) tagging are performed using SWATH [15] which extracts words and classifies them into 44 types such as common noun (NCMN), active verb (VACT), personal pronoun (PPRS), definite determiner (DDAC), unit classifier (CNIT) and negate (NEG). A hidden markov model (HMM) [14] employs these POS tag categories to determine phrases. The model assumes that at time step t the system is in a state PC (t) which has a probability of emitting a particular visible state POS tag (t), the transition probabilities aij among\nEDU Examples Rules Vi \"* (I\u2019m hungry.) NPS-Vi-NPS\nS-Vi + - \u0e01 (It\u2019s rain.) Vi-S ') - ,-(Are you pain?) Vt-O \"*- (I\u2019m hungry.) NPO-NPS-VtNPO S-Vt-O -( - '\u0e01 (The car hit the boy.) O-S-Vt ./ -0 - . * 1\n(I\u2019ve already seen this photograph.)\nVtt-O-I - - (I haven\u2019t given the patient the medicine.)\nNPS-Vtt-NPONPI\nS-Vtt-O-I - - .\u0e01\u0e01* - . (Who gave you the sweet?) O-S-Vtt-I * )- - \u0e01 - ,- (Who would dare to ask you the secret?) NPO-NPS-VttNPI I-S-Vtt-O .-/2 - -) (Niece, I am going to give you this house.) NPI-NPS-VttNPO N /2 (Auntie) NPN-NPN\nN-N / \u0e01\u0e01 - (Whose pen is this?)"}, {"heading": "Noun Phrase Noun Phrase Verb Phrase", "text": "97 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nhidden states and bjk for the probability of the emission of a visible state:\n\u03b1\u03b9\u03d5 = \u03c0(\u03a0\u03a7\u03d5(\u03c4+1)|\u03a0\u03a7\u03b9(\u03c4)). (1)\n\u03b2\u03d5\u03ba = \u03c0(\u03c4\u03b1\u03b3\u03ba(\u03c4)|\u03a0\u03a7\u03d5(\u03c4)). (2)\nwhere PC (t) is Phrase Constituent at time step t, tag(t) is POS\ntag at time step t.\nThe transition probability for the hidden states where a particular sequence PCT = {PC (1), PC (1), \u2026, PC(T) } of T\nhidden states can be written as:\n| 1 (3) The hidden state and the corresponding visible state where the model generated the particular sequence of T visible POS tag\nstate tagT can write as:\n| | (4) The probability that model produces a sequence tagT of visible Pos tag states is\nmax , | | 1\n(5)\nThe expression, p(PC(t)|PC(t-1)) is the probability of PC(t) given the previous PC(t-1), and p(tag(t)|PC(t)) is the probability of POS tag(t) given the Phrase Constituent(t). This research use Baum-Welch [14] learning to determine model parameters, the transition probabilities aij and bjk, from an ensemble of training samples.\nGiven a sequence of visible state tagT, the decoding is to find the most probable sequence of hidden states. This research uses Virterbi [14] to calculates p(tagT) of visible POS states recursively because each term p(tag(t)|PC(t)) p(PC(t)|PC(t-1)) involves only tag(t), PC(t) and PC(t-1) by defining follows:\n0, 0 ! \" # # # $ % & 1, 0 ! # # # $ % & ' () * # )+,+- . /& 0#%&,1 (6) where bjkt represents the transition probability bjk selected by\nthe visible state emitted at time t. Thus the only nonzero contribution to the arg is for index k which matches the visible state tag(t).\nFigure 3 shows a phrase model of string \u201c \u04241 \u04242 \u04243 3 0 \u201d (A friend\u2019s going to borrow this book. Because she (\u04241) hasn\u2019t been able to buy it (\u04242). Therefore she (\u04243) must borrow it from me.) which POS tags of string is \u201c (A Friend-NCMN) (is going toXVMM) (borrow-VACT) (book- NCMN)\n(numerative-CNIT) (this-DDAC) (Because-CONJ) 4 (she(\u04241)- PPRS) (hasn\u2019t been-NEG) (able to-XVMM) (buy-VACT) (it(\u04242)) (Therefore: CONJ) 4 (she(\u04243): PPRS) 3 (must: XVMM) (borrow: VACT) (book: NCMM) 0 (me: PPRS)\u201d. The hidden state of a phrase model consists of H(NCMN- book (2/4), friend (1/4); PPRSme(1/4)), D (CNIT- numerative (1/2); DDAC- this(1/2)), Discourse-marker (CONJ- because(1/2), therefore (1/2) ), Aux1 (XVMM- is going to(1/4), must(1/4), able to(1/4); NEGhasn\u2019t been (1/4)) and Nuc (VACT- borrow (2/3), buy (1/3)). The hidden state of Thai EDU model consists of S (H- friend (1)), O (H- book (2/4); D- numerative, this (2/4)) I (H- me (1)), Discourse-marker (marker- because, therefore(1)) and Vt (Aux1- must, is going to, able to, hasn\u2019t been (4/7); Nucborrow, buy (3/7)).\n98 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nB. EDU Boundary Determination\nAfter we determine NP and VP, another HMM on EDU constituents (shown in Fig. 5.) is then created to determine the starting and the ending of boundaries of EDUs. This model can handle the subject and object absence problems, discussed earlier.\nwhere EDUC(t) is EDU Constituent at time step t, tag(t) is Phrase tag at time step t.\nThe expression, p(EDUC(t)|EDUC(t-1)) is the probability of EDUC(t) is EDU constituent at time t given the previous EDUC(t-1) and p(tag(t)|EDUC(t)) is the probability of Phrase tag(t) given the EDU Constituent(t).\nStart H Aux1 Nuc H D D END\nStart 1 0 0 0 0 0 0 0\nS 0 1[1/6*1] 0 0 0 0 0 0\nO 0 0 0 0 3*10-3 6*10-4 1*10-4 5*10-5\nI 0 0 0 0 0 0 0 0\nMarke r\n0 1[2/6*0] 0 0 0 0 0 0\nVt 0 1[3/6*0] 9*10-2 2*10-2 0 0 0 0\nEnd 0 0 0 0 0 0 0 0\nt = 0 1 2 3 4 5 6 7\nOutput Start S Vt Vt O O O End\nFig.6. The result from Viterbi taged on EDU segmentation model.\nC. EDU Constituent Function Determination\nAfter we determine EDU constituent by using EDU segmentation model, rule base in Table 1 and Viterbi algorithm are then used to determine the grouping of function in EDUs. Evaluation of EDU constituent function determination is process which insert EDU constituent function tag for using in RS tree step. For example a string \u201c - - - - - \u201d (A friend\u2019s going to borrow this book.), the result from Viterbi tagged on the EDU segmentation model is shown as: S, Vt, Vt, O, O, O\nRules in Table 1 are used to group constituents into appropriate functions.. For example, the rule based \u201cNPO-NPSVt-NPO\u201c is used with label \u201cS, Vt, Vt, O, O, O\u201d because this label consists of Vt. Therefore, the result from Evaluation of EDU constituent function determination is shown as: NPS \u2013 (V, V)t \u2013 (NP, NP, NP)O\nV. THAI RS TREE CONSTRUCTION\nIn this section, we describe our technique to construct an RS tree from a corpus based on rules derived from Thai linguistic characteristics. The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19]. Given a pair of EDUs, an author may write by using any or all of the three rules. The model calculates an EDUEDU similarity matrices based on these sets of rules. Finally, a hierarchical clustering employs these matrices to build a rhetorical structure tree.\nA. Linguistic Rules for EDU Relations Absence rules In Thai language, it has been observed that frequently in writing some constituents of EDUs may be absent while the meanings are still the same. In the example below, the NP (object) \u201c \u201d (dessert) is absence from the anaphoric EDU according to rule, i.e., rule \u0424\n(O, O). Cataphoric EDU (VtO) : \u0e01 & (Would you like to\nmake dessert?) Anaphoric EDU (Vt) : \u0e01 & (Yes, I like to make.)\nRepetition rules It has been observed that frequently the\nanaphoric EDU is related to its cataphoric EDU by repetition of NP (subject, object), preposition phrase (PP) where it functions as a modifier of a nucleus or a verb phrase (VP). In the following example, two EDUs relate by a repetition of NP (object) \u201c) \u201d (House), i.e., rule \u044f (O, O).\nCataphoric EDU (VttOI) : \u0e01 ) (I\u2019m going to sell\nhim a house.) Anaphoric EDU (VtO) : ) (Which house are\nyou going to sell?)\nAddition rules It has been observed that frequently the anaphoric EDU is related to its cataphoric EDU by addition of discourse marker, and possibly accompanied by Absence and\n99 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nRepetition rules. In the example below Discourse Marker \u201c \u201d (because) is added in front of the anaphoric EDU, i.e., rule \u0414 (Marker, Before).\nB. EDU Similarity Scoring\nA similarity score between two EDUs is calculated from contents of the EDUs, based on the three types of rules. A feature vector consists of Subject, Absence of Subject, Object, Absence of Object, Preposition, Absence of Preposition, Nucleus, Modifier Nucleus, Head, Absence of Head, Modifier Head, Absence of Modifier Head, Marker Before, and Marker After elements, each corresponding to a rule in the rule sets. The value of an element is dependent upon the type of rule, as follows:\nFeatures based on absence rules:\nA feature is generated from each absence rule with a value of: 89 \u0424 1, 2 #% ;& /& < $;& 1 |=1 =2|>. $ ;',& .9 %& & ?&% (8) 5$%&, < $;& 0\nwhere O1 is the order of cataphoric EDU, O2 is the order of anaphoric EDU, C1 is constituent of cataphoric EDU, C2 is constituent of anaphoric EDU.\nGiven the following example:\nEDU1: ( *) (/Subject) / \u0e01 ) (/Nucleus) , \u0e01 ) * (/Object) (The villager performs the family-industry.) EDU2: (/Before) (/Subject, Absence of subject) * (/Nucleus) ) \" ( \" (/Object) (and saves property of the nation.) EDU3: , \u0e01 ) * (/Subject) 3 /6 (/Nucleus) ) \" ( \" (/Object) (Therefore, the family-industry is a property of the nation.)\nwith the rule \u201cThe anaphoric EDU2 is related to its cataphoric EDU1 by absence of subject\u201c, i.e., \u0424 (S, S), with the absence of subject \u201c( *) \u201d (Villager).\n< $;&234 ,@AB+CD 1 |1 2|3 < $;&234F,GBHCIDC JK @AB+CD (9) Features based on repetition rules:\nA feature is generated from each repetition rule with a value of:\n89 \u044f 1, 2 #% ;& /&\n(10) < $;& 1 |=1 =2|>. $ ;',& .9 %& & ?&%M >. $ .9 & & # 0. !%>. $ .9 0. !% # %& & ?&%\nElse, values = 0\nwhere O1 is the order of cataphoric EDU, O2 is the order of\n100 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nanaphoric EDU, C1 is constituent of cataphoric EDU, C2 is constituent of anaphoric EDU.\nIn the example, the properties of EDU1 and EDU3 match with the rule \u201cThe anaphoric EDU is related to its cataphoric\nEDU by repetition of subject\u201c, i.e., \u044f (O, S), with the absence of object \u201c , \u0e01 ) *\u201d (Family-Industry).\nvalueEDU1, Object = valueEDU3, Subject\n1 | *N|N M N M N (11) Features based on addition rules:\nA feature is generated from each addition rule with a value of: If \u0414 (M, L) is true then values = 1 Else, value = 0 (12)\nWhere M is Marker of EDU, L is link.\nIn the example, the properties of EDU1 and EDU2 match with the rule \u201cThe anaphoric EDU is related to its cataphoric\nEDU by addition of discourse marker \u201c \u201d (AND) \u201c, i.e., \u0414 (Marker, Before), link EDU2 with in front of EDU2 (EDU1). If addition rule is true, value will be assigned 1. But if addition is false, value will be assigned 0."}, {"heading": "Similarity Calculation", "text": "Similarity between two EDUs (cataphoric and anaphoric)\ncan be calculated as:\n\u03a3\u03b9\u00b5\u03b9\u03bb\u03b1\u03c1\u03b9\u03c4\u03c8 O- PQ)+ RSTU VWXY *@U VWXY RSTU VWXY *R)IU VWXY (14) where Sk (k \\in {1, 2, 3}) is the normalized matrix whose values in matrix range from one (for the inter-EDU ranked best) down to zero (for the inter-EDU ranked worst), Matrix rule (MR) is a matrix from absence rule class, repetition rule class, and addition rule class, respectively; and Max\u2019s and Min\u2019s are taken over all occurring ranks for each rule [7]. The input score are normalized into range [0, 1] before computation takes standard combination algorithm [7]. Si can be calculated separately for each type of rule, as follows:"}, {"heading": "Absence and repetition rules:", "text": "\u039c\u03a1\u03b9\u03d5 = |P # ;!&234X S SZ[J\\)D M P # ;!&234YGISZ[J\\)D| (15) where i and j are the order of EDU and |i-j| < MD\nThese rules consist of two parts (Cataphoric and Anaphoric). If two parts of absence and repetition rules are true, then absence and repetition rules are true. But if one part of absence and repetition rules is false, then absence and repetition rules are false."}, {"heading": "Addition rule:", "text": "\u039c\u03a1\u03b9\u03d5 = |P # ;!&234 X S SZ[J\\)D ] P # ;!&234YGISZ[J\\)D| (16) Where i and j are the order of EDU and |i-j| < MD\nThis rule consists of two parts (Cataphoric and Anaphoric). If one part of addition rule are true, then addition rule are true. The similarity above is used with a disjunctive hypothesis, where MD is the maximum distance of appropriate disjunctive hypothesis. Essentially, the disjunctive hypothesis enumerates relations of EDUi over number of the Cartesian product {i, i + 1, \u2026, i + MD + 1} X { j, j + 1, \u2026, j + MD + 1}, i.e., all the pairs of EDUs that separated by an imaginary line drawn between EDUi and EDUj. The maximum distance 4 is the appropriate of disjunctive hypothesis.\nVI. THAI RS TREE CONSTRUCTION\nThe methods used in this part are a hierarchical clustering. Because results from hierarchical clustering can be represent relation between EDUs in binary tree.\nEach sample (an EDU in this case) begins in a cluster of its own and while there is more than one cluster left. The two closest clusters are combined into a new cluster and the distance between the newly formed cluster and each other cluster is calculated. The method of hierarchical clustering is shown in Table 4 and the result from hierarchical clustering is shown in Fig. 7.\nIn this section, we describe our technique to recognize Thai DRs from a corpus based on rules derived from Thai linguistic characteristics. The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19]. A Decision tree (C5.0) employs these features to recognize a DR.\nA. Linguistic Rules for DR Recognition\nA feature score of DRs is calculated from contents of the EDUs, based on the three types of rules. A feature score consists of Cataphoric score (Subject, Object, Preposition, Nucleus, Marker Before and Marker After) and Anaphoric score (Subject, Absence of Subject, Object, Absence of\n101 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nObject, Preposition, Absence of Preposition, Nucleus, Modifier Nucleus, Head, Absence of Head, Modifier Head, Absence of Modifier Head, Marker Before, and Marker After) elements, each corresponding to a rule in the rule sets. The value of an element is dependent upon the type of rule, as follows:\nFeatures based on absence rules:\nA feature is generated from each absence rule with a value of:\nIf \u00d8(C1,C2) is true then ValueCataphoric = 1 ValueAnaphoric = \u201cOmit\u201d =2 (17) ELSE, ValueCataphoric = 1 ValueAnaphoric = -1\nwhere C1 is constituent of cataphoric EDU, C2 is constituent\nof anaphoric EDU.\nGiven the following example:\nEDU1: ( *) (/Subject) / \u0e01 ) (/Nucleus) , \u0e01 ) * (/Object) (The villager performs the family-industry.) EDU2: (/Before) (/Subject, Absence of subject) * (/Nucleus) ) \" ( \" (/Object) (and saves property of the nation.) EDU3: , \u0e01 ) * (/Subject) 3 /6 (/Nucleus) ) \" ( \" (/Object) (Therefore, the family-industry is a property of the nation.)\nIn the example, the properties of EDU1 and EDU2 match\nwith the rule \u201cThe anaphoric EDU2 is related to its cataphoric EDU1 by absence of subject\u201c, i.e., \u0424 (S, S), with the absence of subject \u201c( *) \u201d (Villager).\nvalueEDU1, Subject = 1 valueEDU2, Absence of Subject = \"Omit\"=2 (18)\nFeatures based on repetition rules:\nA feature is generated from each repetition rule with a value of: If \u044f (C1,C2) is true then ValueCataphoric = 1 ValueAnaphoric = 1 Else, ValueCataphoric = 1 ValueAnaphoric = -1 (19)\nwhere C1 is constituent of cataphoric EDU, C2 is constituent of anaphoric EDU.\nIn the example, the properties of EDU1 and EDU3 match with the rule \u201cThe anaphoric EDU is related to its cataphoric\nEDU by repetition of subject\u201c, i.e., \u044f (O, S), with the absence of object \u201c , \u0e01 ) *\u201d (Family-Industry).\nvalueEDU1, Object = 1\nvalueEDU2, Subject = 1\n(20)\nFeatures based on addition rules:\nA feature is generated from each addition rule with a value\nof: If \u0414 (M, L) is true then valuesL = M Else, valueL = -1\n(21)\nwhere M is Marker of EDU, L is link.\nIn the example, the properties of EDU1 and EDU2 match with the rule \u201cThe anaphoric EDU is related to its cataphoric\nEDU by addition of discourse marker \u201c \u201d (AND) \u201c, i.e., \u0414 (Marker, Before), link EDU2 with in front of EDU2 (EDU1). If addition rule is true, value will be assigned 1. But if addition is false, value will be assigned 0."}, {"heading": "VIII. EXPERIMENTAL EVALUATION", "text": "A. Evaluation of Thai EDU Segmentation\nIn order to evaluate the effectiveness of the EDU segmentation process, a consensus of five linguists familiar with rhetorical structures of Thai texts is used to manually\nvalueEDU1, Before =\u201dAND\u201d = valueEDU2, Before (22)\n102 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nsegment EDUs of Thai family law which consists of 10,568 EDUs in total.\nThe EDU segmentation model is trained with 8,000 random EDUs, and the rest are used to measure performance.\nThe training proceeds till the estimated transition probability changes no more than a predetermined value of 0.02 or the accuracy achieves 98%.\nThe performances of both phrase and EDU segmentations are evaluated using recall (Eq. 23) and precision (Eq. 24) measures, which are widely used to measure the performance. Q&? $$ # ?. &? $_% >/ # / %&%; 567% #!& #9#&! ,_ ;%# '.!&$# >/ # / %&%; 567% #!& #9#&! ,_ $_% % (23) &?#%#. # ?. &? $_% >/ # / %&%; 567% #!& #9#&! ,_ ;%# '.!&$>. $ ;',& .9 >/ # / %&%; 567% #!& #9#&! ,_ ;%# aPP (24)\nThe results show that the proposed method achieves the recall values of 84.8% and 85.3%; and the precision values of 93.5% and 94.2% for phrase and EDU segmentations, respectively.\nB. Evaluation of EDU Constituent Function Determination\nIn order to evaluate the effectiveness of grouping EDU constituents and determining their functions in an EDU using\nthe proposed HMM model on NP and VP, the experiments employ three texts (Absence data with 84 EDUs, Repetition data with 117 EDUs and a subset of the Family law with 367\nEDUs). The Absence data contains EDUs mostly following absence rules while the Repetition data contains those mostly following repetition rules. The analysts created training and testing data sets by manually building EDU constituents and inserting appropriate tags into each text.\nThe results of the Viterbi algorithm are labels of EDU constituents, then rules based on NP and VP are applied to group EDU constituents and insert function tags.\nTable 5: Performance of determining EDU constituent functions\nRules Absence data Repetition data Family law data NPS-Vi-NPS NPS (100%) NPS (100%) NPS (100%) NPO-NPS-VtNPO NPS & NPO (100%) NPS &NPO (100%) NPS &NPO (100%) NPS-VttNPO-NPI NPS &NPO&NPI (100%) NPS &NPO&NPI (100%) NPS &NPO&NPI (100%) NPO-NPSVtt-NPI NPI-NPS-VttNPO NPS (100%), NPO&NPI (91.37%) NPS (100%), NPO&NPI (79.59%) NPS (100%), NPO&NPI (90.21%) N-N NPN (100%) NPN (100%) NPN (100%)\nTable 5 shows the results of grouping EDU constituents (Subject (S), Object (O), Indirect Object (I) and Nomen (N)) by using rules based on NPs assuming the position of verb phrases (Vi, Vt and Vtt) are known. In general, all rules except NPO-NPS-Vtt-NPI and NPI-NPS-Vtt-NPO perform well.\nTo resolve ambiguities with these two rules, a probability table of words in positions of NPI and NPO coming after position of Vtt (P(Vtt| NPI, NPO)) is used. The results of evaluating EDU constituents by using rules based on NP together with the probability table yield higher performance of NPO and NPI in absence data (92.24%), Repetition data (85.78%) and Family law (93.71%).\nC. Evaluation of Thai RS Tree Construction\nIn order to evaluate the effectiveness of the Thai RS Tree construction process, linguists manually built the rhetorical structure trees of three texts which are Absence, Repetition and Family Law data sets, with a total of 568 EDUs. The performances of the rhetorical structure were evaluated by using recall and precision measures, i.e., (Eq. 25) and (Eq. 26) [6], respectively.\nIn the Absence and Repetition data sets, though relations between EDUs follow mostly Absence rules and Repetition rules, respectively, many EDUs also follow other kinds of rules. For example,\nAnaphoric EDU (SvtO) : ), ,7 / 7- (S) \u0e01 (Vt)\n(\u044f O) (A Postman will select\nQ&? $$ #?. &? $_% # & $ .!&% #!& #9#&! ,_ QO>## & $ .!&% #!& #9#&! ,_ $_% % (25) &?#%#. #?. &? $_% # & $ .!&% #!& #9#&! ,_ QO>>. $ ;',& .9 # & $ .!&% #!& #9#&! ,_ QO> (26)\n103 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nletters) Cataphoric EDU ((S)VtO) : (\u0414) (\u0424 S) ) (Vt)\n(\u044f O) (And will receive and send letters)\nRecall and precision are calculated with respect to the ability of an algorithm to construct an RS tree structure similar to that created by the linguists.\nTable 6 shows example calculations of recall and precision of Thai RS Trees on a text created by the Minimum Variance\nand Unweighted Arithmetic Average algorithms in Fig. 8. Table 7 shows the results of evaluating Thai RS Tree construction on the three data sets. The performance on the Family law which combines many kinds of rules in its content is 94.90% recall and 95.21% precision. The results also show that Unweighted Arithmetic Average clustering algorithm gives the best performance for Thai RS Tree construction.\nA. Evaluation of Thai DR Recognition\nIn order to evaluate the effectiveness of the Thai DR recognition process, linguists manually built the DRs of Family Law data sets from RS tree, with a total of 1,248 EDUs. The DR model is trained with 828 random EDUs, and the rest are used to measure performance. The performances of the DR recognition were evaluated by using analysis module of Clementine version 12.0.\nTable 8 shows the results of evaluating ten Thai DRs recognition on the data sets. The performance on the Family law which combines many kinds of rules in its content, found marker and not found marker is 82.50%, 85.09% and 81.28%, respectively.\nTable 9 shows the results of comparison found marker and not found marker to recognize ten Thai DRs on the data sets. The performance on the Family law which found and not found marker in its content is 99.40% and 100%, respectively.\nIX. CONCLUSIONS\nThai rhetorical structure tree (RST) construction is an important task for many textual analysis applications such as automatic text summarization and question-answering. This article proposes a novel two-step technique to construct Thai RS tree combining machine learning techniques with linguistic properties of the language.\nFirst, phrases are determined and then are used to segment elementary discourse units (EDUs). The phrase segmentation model is a hidden markov model constructed from the possible arrangements of Thai phrases based on part-of-speech of words, and the EDU segmentation model is another hidden markov model constructed from the possible arrangements of Thai EDUs.\nAs a side effect, functions of the EDU constituents can be determined by the EDU segmentation model together with linguistic rules grouping related constituents into a large unit. Experiments show the EDU segmentation effectiveness of 85.30% and 94.2% in recall and precision, respectively.\nA hierarchical clustering algorithm with EDU similarity derived from semantic rules of the language is proposed to construct an RS tree. The technique is experimentally evaluated and the effectiveness is 94.90% and 95.21% in recall and precision, respectively.\nTable 7: The results from RS tree experiment.\nData N. Clustering Method Recall Precision\nAbsence 84 Neighbour Joining 87.23 89.13\nSingle Linkage 82.97 84.78\nUn weighted Arithmetic\nAverage 87.23 89.13\nMinimum Variance 89.40 91.30\nWeighted Arithmetic\nAverage 87.23 89.13\nRepetition 117 Neighbour Joining 89.70 91.04\nSingle Linkage 83.82 85.07\nUnweighted Arithmetic\nAverage 89.70 91.04\nMinimum Variance 77.94 79.10\nWeighted Arithmetic\nAverage 89.70 91.04\nFamily- 367 Neighbour Joining 85.98 86.26\nLaw Single Linkage 64.01 64.21\nUnweighted Arithmetic\nAverage 94.90 95.21\nMinimum Variance 63.37 63.57\nWeighted Arithmetic\nAverage 90.44 90.73\n104 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nA decision tree (C5.0) algorithm with DR features derived from semantic rules of the language is proposed to recognize a DR of EDUs in RS tree. The technique is experimentally evaluated by setting boosting 10 number of trials, pruning severity 75, maximum record per child branch 2 and winnow attributes and the effectiveness is 82.81%."}, {"heading": "AUTHORS PROFILE", "text": "Somnuk Sinthupoun is a teacher at the Department of Computer Science, Maejo University, Thailand. He holds a M.S. in Computer Science from National Institute of Development Administration (NIDA.), and a B. in Computer Science from Maejo University. His main research interests include artificial intelligence, information retrieval, data mining, and related areas.\nOhm Sornil is an Assistant Professor at the Department of Computer Science, National Institute of Development Administration, Thailand. He holds a Ph.D. in Computer Science from Virginia Polytechnic Institute and State University (Virginia Tech), an M.S. in Computer Science from Syracuse University, an M.B.A. in Finance from Mahidol University, and a B.Eng. in Electrical Engineering from Kasetsart University. His main research interests include computer and network security, artificial intelligence, information retrieval, data mining, and related areas.\n105 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"}], "references": [{"title": "Clauses in the Thai Language", "author": ["D. Chamnirokasant"], "venue": "Unpublished master\u2019s thesis, Chulalongkorn University, Thailand", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1969}, {"title": "editor", "author": ["D.K. Harman"], "venue": "\u201cThe second Text Retrieval conference (TREC- 2)\u201d, Gaithersburg, MD, USA, March 1994. U.S. Government Painting Office, Washington D.C", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A study of sentence groups in Thai essays", "author": ["D. Mahatdhanasin"], "venue": "Unpublished master\u2019s thesis, Chulalongkorn University, Thailand", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1980}, {"title": "Build Up Rhetorical Structure Trees", "author": ["D. Marcu"], "venue": "American Association for Artificial Intelligence", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "A decision-based approach to rhetorical parsing", "author": ["D. Marcu"], "venue": "the 37th Annual Meeting of the Association for Computational Linguistics, ACL, Maryland, pp. 365-372 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "The theory and Practice of Discourse Parsing and Summarization", "author": ["D. Marcu"], "venue": "The MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Combination of multiple search", "author": ["E.A. Fox", "J.A. Shaw"], "venue": "Harman [2], pages 243-249", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Thai Elementary Discourse Unit Segmentation by Discourse Segmentation Cues and Syntactic Information", "author": ["J. Charoensuk", "A. Kawtrakul"], "venue": "the Sixth Symposium on Natural Language Processing 2005 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards a delimitation of discursive segment for Natural Language Processing applications", "author": ["L. Alonso", "I. Castell\u00f3n"], "venue": "International Workshop on Semantics, Pragmatics and Rhetorics, San Sebasti\u00e1n 22-24 November", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A formal model of the structure of discourse", "author": ["L. Polanyi"], "venue": "Journal of Pragmatics, 12, 601-638", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1988}, {"title": "Discourse relation recognition by using Na\u00efve Bayesian classifier", "author": ["M. Wattanamethanont", "T. Sukvaree", "A. Kultrakul"], "venue": "The 9 National Computer Science and Engineering Conference ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Sentence Level Discourse Parsing using Syntactic and Lexical Information", "author": ["R. Soricut", "D. Marcu"], "venue": "Proceedings of the 2003 Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), May 27-June 1, Edmonton, Canada", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning FOL rules based on rich verb semantic representations to automatically label rhetorical relations (EACL)", "author": ["R. Subba", "S.N.K.B. Di Eugenio"], "venue": "In Workshop on learning Structured Information in Natural Language Applications ,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "An introduction to the application of the theory of probabilistic function of a Markov proceeds to automatic speech recognition", "author": ["S. Levinson", "R. Rabiner", "M. Sondhi"], "venue": "Bell System Technical Journal, 62:1035-1074", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1983}, {"title": "Building A Large Thai Text Corpus---Part-Of-Speech Tagged Corpus: ORCHID---", "author": ["T. Charoenporn", "V. Sornlertlamvanich", "H. Isahara"], "venue": "proceedings of the Natural Language Processing Pacific Rim Symposium", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1976}, {"title": "RST based Text Summarization with Ontology Driven in Agriculture Domain", "author": ["T. Sukvaree", "J. Charoensuk", "M. Wattanamethanont", "A. Kultrakul"], "venue": "Department of Computer Engineering, Kasetsart University, Bangkok, Thailand", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Inter-Sentence Relations in Modern Conversational Thai", "author": ["V. Panupong"], "venue": "The Siam Society, Bangkok", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1970}, {"title": "Referent Resolution for Zero Pronouns in Thai", "author": ["W. Aroonmanakun"], "venue": "Southeast Asian Linguistic Studies in Honour of Vichin Panupong. (Abramson, Arthur S., ed.). pp. 11-24. Chulalongkorn University Press, Bangkok. ISBN 974-636-995-4 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Zero Pronoun Resolution in Thai: A Centering Approach", "author": ["W. Aroonmanakun"], "venue": "Burnham, Denis, Interdisciplinary Approaches to Language Processing: The International Conference on Human and Machine Processing on Human and Machine Processing of Language and Speech. NECTEC: Bangkok, 127-147", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Rhetorical structure theory", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Toward a functional theory of text organization\u201d. Text, 8", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1988}], "referenceMentions": [{"referenceID": 3, "context": "A RS tree is a tree-like representation of discourse relations among elementary discourse units (EDUs) which can be defined as follows: RS tree = (Status, DR, Promotion, Left, Right) where Status is either nucleus or satellite EDU (nucleus expresses what is more essential to writer\u2019s purpose than satellite); DR is a Discourse Relation (DR); Promotion is a subset of EDUs; and Left, Right can be either NULL or recursively defined objects of type RS tree [4, 6].", "startOffset": 456, "endOffset": 462}, {"referenceID": 5, "context": "A RS tree is a tree-like representation of discourse relations among elementary discourse units (EDUs) which can be defined as follows: RS tree = (Status, DR, Promotion, Left, Right) where Status is either nucleus or satellite EDU (nucleus expresses what is more essential to writer\u2019s purpose than satellite); DR is a Discourse Relation (DR); Promotion is a subset of EDUs; and Left, Right can be either NULL or recursively defined objects of type RS tree [4, 6].", "startOffset": 456, "endOffset": 462}, {"referenceID": 5, "context": "Some researchers consider an EDU to be a clause or clauselike [6] excerpt, while others consider them to be a sentence [10] in discourse parsing.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Some researchers consider an EDU to be a clause or clauselike [6] excerpt, while others consider them to be a sentence [10] in discourse parsing.", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 105, "endOffset": 114}, {"referenceID": 7, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 105, "endOffset": 114}, {"referenceID": 8, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 105, "endOffset": 114}, {"referenceID": 5, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 134, "endOffset": 140}, {"referenceID": 8, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 168, "endOffset": 179}, {"referenceID": 9, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 168, "endOffset": 179}, {"referenceID": 11, "context": "A number of techniques are proposed to determine EDU boundaries for English such as using discourse cues [5, 8, 9], punctuation marks [6, 9], and syntactic information [6, 10, 12].", "startOffset": 168, "endOffset": 179}, {"referenceID": 19, "context": "Some have a single nucleus such as elaboration and condition, while others have multiple nucleuses such as contrast [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "A number of techniques for determining the DR between EDUs are proposed, such as using verb semantics [13] to build verb based events which represent EDUs, using cue phrase/discourse marker (e.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": ", \u201cbecause\u201d, \u201chowever\u201d) [5], and using machine learning techniques [6].", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": ", \u201cbecause\u201d, \u201chowever\u201d) [5], and using machine learning techniques [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 15, "context": "[16] purposes a technique to construct a tree by using a spanning tree which make decision by discourse marker and focus of EDU, into two phases which consist of local and global EDU spanning tree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] purposes a technique to recognize a DR by using DR marker tag to recognize DR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] purposes a technique to recognize a DR by using Na\u00efve bayes classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "There are twelve possible arrangements of Thai EDU [17], as shown in Table 1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "An Arrangement of NP and VP constituent [17] is shown in Table 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Phrase Identification To perform the phrase identification, word segmentation and part of speech (POS) tagging are performed using SWATH [15] which extracts words and classifies them into 44 types such as common noun (NCMN), active verb (VACT), personal pronoun (PPRS), definite determiner (DDAC), unit classifier (CNIT) and negate (NEG).", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "A hidden markov model (HMM) [14] employs these POS tag categories to determine phrases.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "This research use Baum-Welch [14] learning to determine model parameters, the transition probabilities aij and bjk, from an ensemble of training samples.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "This research uses Virterbi [14] to calculates p(tag) of visible POS states recursively because each term p(tag(t)|PC(t)) p(PC(t)|PC(t-1)) involves only tag(t), PC(t) and PC(t-1) by defining follows: 0, 0 ! \" # # # $ % & 1, 0 ! # # # $ % & ' () * # )+,+- .", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 2, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 16, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 17, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 18, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "where Sk (k \\in {1, 2, 3}) is the normalized matrix whose values in matrix range from one (for the inter-EDU ranked best) down to zero (for the inter-EDU ranked worst), Matrix rule (MR) is a matrix from absence rule class, repetition rule class, and addition rule class, respectively; and Max\u2019s and Min\u2019s are taken over all occurring ranks for each rule [7].", "startOffset": 354, "endOffset": 357}, {"referenceID": 0, "context": "The input score are normalized into range [0, 1] before computation takes standard combination algorithm [7].", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "The input score are normalized into range [0, 1] before computation takes standard combination algorithm [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 2, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 16, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 17, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 18, "context": "The rules are classified into three types which are Absence, Repetition and Addition [1, 3, 17, 18, 19].", "startOffset": 85, "endOffset": 103}, {"referenceID": 5, "context": "26) [6], respectively.", "startOffset": 4, "endOffset": 7}], "year": 2553, "abstractText": "A rhetorical structure tree (RS tree) is a representation of discourse relations among elementary discourse units (EDUs). A RS tree is very useful to many text processing tasks employing relationships among EDUs such as text understanding, summarization, and question-answering. Thai language with its unique linguistic characteristics requires a unique RS tree construction technique. This paper proposes an approach for Thai RS tree construction which consists of three major steps: EDU segmentation, Thai RS tree construction, and discourse relation (DR) identification. Two hidden markov models derived from grammatical rules are used to segment EDUs, a clustering technique with its similarity measure derived from Thai semantic rules is used to construct a Thai RS tree, and a decision tree whose features extracted from the rules is used to determine the DR between EDUs. The proposed technique is evaluated using three Thai corpora. The results show the Thai RS tree construction and the DR identification effectiveness of 94.90% and 82.81%, respectively. KeywordsThai Language, Element Discourse Unit, Rhetorical Structure Tree, Discourse Relation.", "creator": "PDFCreator Version 0.9.8"}}}