{"id": "1002.3195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2010", "title": "Efficiently Discovering Hammock Paths from Induced Similarity Networks", "abstract": "Similarity networks are important abstractions in many information management applications such as referral systems, corpora analyses and medical informatics. For example, by establishing similarity networks between films that are similarly rated by users, or between documents that contain common terms, and between clinical trials that include the same topics, we can aim to find the global structure of the connectivity underlying the data, and use the network as a basis to establish connections between seemingly different entities. In the above applications, composing similarities between objects of interest is used in random recommendations, storytelling, or clinical diagnosis. We present an algorithmic framework for traversing similar paths using the term \"hammock paths,\" which represent a generalization of traditional paths. Our framework is explorative in nature, so that it examines candidate objects in the face of incipient and incipient objects of interest to follow the path, and to estimate the potential for a desired path.", "histories": [["v1", "Wed, 17 Feb 2010 04:07:06 GMT  (2419kb,D)", "http://arxiv.org/abs/1002.3195v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["m shahriar hossain", "michael narayan", "naren ramakrishnan"], "accepted": false, "id": "1002.3195"}, "pdf": {"name": "1002.3195.pdf", "metadata": {"source": "CRF", "title": "Efficiently Discovering Hammock Paths from Induced Similarity Networks", "authors": ["M. Shahriar Hossain", "Michael Narayan", "Naren Ramakrishnan"], "emails": ["naren}@vt.edu"], "sections": [{"heading": "1. Introduction", "text": "There is significant current interest in modeling and understanding network structures, especially in online social communities, web graphs, and biological networks. We focus here on (unipartite) similarity networks induced from bipartite graphs, such as a movies \u00d7 people dataset. Two movies can be connected if they have been rated similarly by sufficient number of people; this is the basis for the popular itembased recommendation algorithms [4]. A similarity network thus exposes an indirect level of clustering, community formation, and organization that is not immediately apparent from the bipartite network.\nSimilarity networks are key abstractions in recommender systems but they also find uses in other information management applications such as collection analysis and medical informatics. We focus on how these networks can be used for exploratory discovery, i.e., to see how similarities can be composed to reach potentially distant entities. For instance, how is the movie \u2018Roman Holiday\u2019 (a romantic classic) connected to \u2018Terminator 3\u2019 (a Sci-Fi movie)? What are the in-between movies and waypoints that help link these two disparate movies? Are these waypoints characteristic to the domain or to the dataset? For recommender systems, these questions are especially germane to tasks such as serendipitous recommendation, gift buying, and characterizing the movie watching interests of its users.\nIn addition to recommender systems, we consider similarity networks induced from literature and semistructured sources of data. Here path finding has applications to literature-based discovery (similar to the ARROWSMITH program [12]) and clinical diagnosis. For instance, how is congestive heart failure related to kidney disease? The discovered waypoints correspond to possibilities by which different disease response pathways interact with each other.\nAdmittedly, these questions can be answered by first inducing the similarity network and running shortest path queries over it, but we desire to find paths without materializing the entire network. This would be wasteful because only a subnetwork is likely to be traversed in response to a query. Instead, we seek to\nar X\niv :1\n00 2.\n31 95\nv1 [\ncs .A\nI] 1\n7 Fe\nb 20\n10\npre-process the original bipartite graph into suitable data structures that can then be harnessed to find chains of connections.\nIn this paper, we present an algorithmic framework for traversing paths using the notion of \u2018hammock\u2019 paths which are generalization of traditional paths. Our framework is exploratory in nature so that, given starting and ending objects of interest, it explores candidate objects for path following, and heuristics to admissibly estimate the potential for paths to lead to a desired destination. Our key contributions are:\n1. We formulate the path finding problem over similarity networks in terms of hammocks and cliques, two intuitive-to-understand constructs for navigating similarity networks. Hammocks are ways to impose tighter requirements over individual links (leading to longer paths) and cliques are ways to coalesce multiple links (resulting in shorter paths).\n2. We present an algorithmic framework that finds hammock paths in both binary datasets (bipartite graphs) and vector-valued datasets (weighted bipartite graphs). Our framework uses a concept lattice representation as an in-memory data structure to organize the search for paths and introduces admissible heuristics that quickly prune out unpromising paths.\n3. We present compelling experimental results on three diverse datasets: the Netflix dataset of movie ratings, a significant portion of the PubMed corpus, and a database of clinical trials. Experimental results demonstrate the scalability and accuracy of our approach as well as its potential for unstructured knowledge discovery."}, {"heading": "2. Background", "text": "The formulations we study can be intuitively understood using Fig. 1. A simple path between movies can be induced through co-raters, as shown in Fig. 1 (top): \u2018Roman Holiday\u2019 was rated by user u5 who also rated \u2018Rear Window\u2019 which was also rated by u9, and so on. In practice, paths are induced between two movies only if a sufficient number of people have rated them and rated them similarly. This leads to a sequence of \u2018hammocks\u2019 as shown in Fig. 1 (middle). A final level of generalization is to organize the hammocks into groups so that we traverse cliques of movies (with a hammock between every pair of them), see Fig. 1 (bottom). By finding such hammocks and traversing them systematically, similarities can\n\u2018diffuse\u2019 to reach possibly distant entities. Note that the path length increases as the hammock requirement is strengthened and then decreases as the clique requirement is imposed. Work closest to ours can be found in the graph modeling, redescription mining, and lattice-based information retrieval literature. Graph modeling: The use of the word \u2018hammock\u2019 for induced similarity networks appears to have been first made in [8] although this work does not aim to find paths. The notion of kNC-plots was introduced in [6] where k denotes what we refer to as hammock width in this paper. Rather than finding local paths, this paper is focused on finding the global connectivity structure of the induced networks as the hammock width is increased. It is also restricted to binary spaces whereas we focus on vector valued spaces as well. Random intersection graphs [13] are a class of theoretical models proposed in the random graph community. These models randomly assign each vertex with a subset of a given set and posit edges if these subsets intersect. Under these modeling assumptions, connectivity and other properties of these graphs can be characterized [2]. Redescription mining: Redescriptions, a pattern class introduced in [11], induce subsets of data that share strong overlap. A hammock in our notation can be viewed as a redescription between the objects it connects. Kumar et al. [5] study the problem of \u2018storytelling\u2019 over a space of descriptors, which is the goal of finding a sequence (story) of redescriptions between two subsets by positing intermediate subsets that share sufficient overlap with their neighbors in the story. Although this is similar to our objective, the heuristic innovations in [5] are restricted to binary spaces as well and cannot find paths of the same expressiveness (hammocks organized into cliques) and with the same efficiencies as done here. Lattice based information retrieval: The use of concept lattices as organizing data structures for fast retrieval, query, and data mining is not new [9, 10]. Our work is different in both the theoretical framework by which the lattice concepts are used to build chains and in their uses/applications."}, {"heading": "3. Problem Formulation", "text": "We begin by formally defining the space over which similarity networks are induced.\nDefinition 1. A dataset D = (O,F ,V) is a set of objectsO, a set of features F , and a relation V \u2286 O\u00d7F .\nWe can thus think of each f \u2208 F as inducing a subset of O, which we call o(f). We further require that the objects induced by F form a covering of O, i.e. \u22c3 f\u2208F o(f) = O. In the applications studied here, the (O,F) are (movies, users), (documents, terms), and (clinical trials, keywords), respectively, with the obvious meaning of V in each case. In Fig. 2 we introduce a dataset which we will use as a running example. Here the objects are movies (O = {m1,m2,m3,m4,m5,m6,m7,m8}) and the features are users (F = {uA, uB, uC , uD, uE , uF , uG}).\nA note on notation: We use calligraphic O and F to represent the complete object and feature sets for some database. Lower case letters o and f are used to represent individual members of O and F , respectively. To represent subsets of O and F we use upper case letters O and F , resp.\nFor a feature set F \u2286 F , we overload notation and define the operator o(F ) : F \u2192 O as the set of objects which are associated with all of the features in F , i.e.,\no(F ) = \u22c2 f\u2208F o(f)\nWe also define a parallel operator f(O) as the set of features associated with the object set O. In Fig. 2, we have o({uA, uD, uE}) = {m1,m3,m5,m7} and f({m1,m3,m5,m7}) = {uA, uD, uE}.\nWe can now define the closure operator c : 2F \u2192 2F as c(F ) = f(o(F )). A feature set F is closed if and only if c(F ) = F . (A parallel closure operator exists on 2O, but is not necessary for our purposes.) Using our running example from Figure 2 we see that feature set {uA, uD, uE} is closed, whereas {uA, uD} is not, since c({uA, uD}) = {uA, uD, uE} 6= {uA, uD}.\nDefinition 2. For each feature f \u2208 F , we define a predicate pf (o) such that pf (o) is true if and only if o \u2208 o(f). The set of all such predicates is denoted by P . A descriptor is a boolean expression over P .\nA descriptor thus provides a mechanism to specify a set of objects which correspond to some Boolean expression on features. Again referring to Figure 2 we might define a descriptor uA(o) \u2228 uB(o), which would correspond to all the movies which were rated by user A, by user B, or by both; it induces the set {m1,m3,m5,m6,m7}. It is easy to see that we can create a descriptor for the objects in o(F ) by creating a conjunction over the features in F . For simplicity, we often speak of descriptor F for some feature set, which refers to this conjunction.\nWe employ the notion of redescriptions introduced by Ramakrishnan et al. [11] to capture similarities between descriptors. If for descriptors F and F \u2032, where F is tautologically distinct from F \u2032 (i.e. F \u2228\u00acF \u2032 is not a tautology), it is the case that o(F ) = o(F \u2032), we say that F and F \u2032 are redescriptions of each other, as they induce the same set of objects. In other words, a redescription provides (at least) two different ways to describe a given object set. We next introduce the concept of a redescription set:\nDefinition 3. A redescription set for a descriptor F , RF = (O\u2032, F \u2032) is a tuple where O\u2032 = o(F ) and F \u2032 = c(F ).\nFig. 2 shows many redescription sets organized alongside a concept lattice (to be defined soon). One redescription set is: C7 = ({m1,m3,m5,m7}, {uA, uD, uE}), which would correspond to both descriptors {uA, uD, uE} and {uA, uD}.\nWe say that a descriptor F is a relaxation of descriptor F \u2032, denoted by (F \u2264 F \u2032), if the feature set F = {f1, f2, ..., fm} \u2286 {f \u20321, f \u20322, ..., f \u2032n} = F \u2032. In Fig. 2, {uA, uB} \u2264 {uA, uB, uC}. The following is easy to verify:\nLemma 1. F \u2264 F \u2032 \u2192 o(F ) \u2287 o(F \u2032).\nFinally, we bring in the notion of a concept lattice, the lattice defined over the redescription sets using the operator \u2264 with a join of F and a meet of \u2205. Returning once again to our running example, Figure 2 shows a concept lattice with the lower nodes being relaxations of the higher nodes in the lattice. In this space we say that a descriptor F is a child of F \u2032 if and only if F \u2264 F \u2032 and \u2200F \u2032\u2032 : F \u2264 F \u2032\u2032 \u2264 F \u2032 \u2192 (F \u2032\u2032 = F ) \u2228 (F \u2032\u2032 = F \u2032). Thus in our running example C10 has two children: C4 and C7.\nDefinition 4. The Jaccard coefficient between two objects is defined as\nJ(o1, o2) = |f(o1) \u2229 f(o2)| |f(o1) \u222a f(o2)|\nThe Jaccard coefficient is thus a measure of how similar two objects are based upon how many features they share in proportion to their overall size.\nDefinition 5. The Soergel distance between two objects o1 and o2 is given by\nD(o1, o2) = |f(o1)|+ |f(o2)| \u2212 2|f(o1) \u2229 f(o2)| |f(o1)|+ |f(o2)| \u2212 |f(o1) \u2229 f(o2)|\nUnlike the Jaccard coefficient (which is a similarity measure), the Soergel distance is a true distance measure: it is exactly 0.0 when the objects o1 and o2 have exactly the same features, is symmetric, and obeys the triangle inequality. We also note that the Soergel distance is exactly 1.0 when the induced feature sets are disjoint.\nWe now define two types of paths\u2014clique paths and hammock paths\u2014which we use to constrain our path generation.\nDefinition 6. A hammock of width w is a tuple (o1, o2) \u2208 O \u00d7O where |f(o1) \u2229 f(o2)| = w.\nA hammock is thus a pair of objects which share at least w common features. Another way to think of a hammock is as a redescription set containing two objects ({o1} and {o2}), with a feature set which contains at least w features. Observe that a hammock is a basic unit of similarity modeling in recommender systems and many other applications: it posits similarity in one domain using relationships with another domain. We now define hammock paths composed of hammocks.\nDefinition 7. A hammock path with width w, from object o1 to object ot, denoted by Hw(o1, ot), is a series of objects \u3008o1, o2, ..., ot\u22121, ot\u3009 such that \u2200i : 1 \u2264 i \u2264 t\u2212 1 the pair (oi, oi+1) is a hammock of width at least w.\nWe now define a clique which extends the concept of a hammock to a set of objects rather than a single pair:\nDefinition 8. A k-clique, qk,w, is a set of k objects O = {o1, o2, . . . , ok} such that \u2200oi, oj \u2208 O : |f(oi) \u2229 f(oj)| \u2265 w, for some width w. The function \u03a5 maps a clique to the set of k objects contained in said clique.\nNote that a k-clique is defined over a collection of k(k\u22121)/2 hammocks of width at least w; the overlap threshold applied to every pair of objects of a clique ensures that there is at least as high an overlap between any two objects in the clique.\nDefinition 9. A k-clique path Qk,w(o1, ot) is a series of t\u2212 1 consecutive k-cliques Q = \u3008q1, q2, . . . , qt\u22121\u3009 such that o1 \u2208 \u03a5(q1), ot \u2208 \u03a5(qt\u22121), and \u2200qi \u2208 Q : \u03a5 (qi) \u2229\u03a5 (qi+1) 6= \u2205.\nThe problem we seek to address can now be given as a pair of constraints upon the clique and hammock paths between two objects, formally defined as follows:\nProblem 1. Given D = (O,F), start and end objects o1 \u2208 O and ot \u2208 O, we seek to find a chain/series of objects S1,t = \u3008o1, o2, . . . , ot\u3009 where S1,t is a hammock path Hw(o1, ot) for some width w, and further, there is some k-clique path Qk,w(o1, ot) = \u3008q1, q2, . . . , qt\u22121\u3009 where \u2200i : 1 \u2264 i \u2264 t\u2212 1, oi \u2208 \u03a5(qi)."}, {"heading": "4 Extension to Vector Spaces", "text": "To accommodate vector spaces, we generalize Definition 1 so that a database D = (O,F , V ) is now a tuple defined over a set of objects O, features F , and a function V : O \u00d7 F \u2192 R. This helps us go beyond simple binary associations to record the strength of the association (e.g., rating values, term weights) or other auxiliary continuous-valued data. (Our running example of Figure 2 contains movie ratings by users.) The weighted Soergel distance between two objects o1 and o2 is defined by\nD(o1, o2) = \u2211 f\u2208F |V (o1, f)\u2212 V (o2, f)|\u2211\nf\u2208F max(V (o1, f), V (o2, f))\nwhich reduces to the unweighted case if V (o, f) = 1 for any object o \u2208 o(f) and V (o, f) = 0 for any object o 6\u2208 o(f). Definitions 6, 7, 8, 9 get similarly generalized. In place of the hammock width constraint w, we use a weighted Soergel distance threshold \u03b8 that must be satisfied between the vectors that aim to form hammocks, hammock paths, cliques, or clique paths. In place of the notation Hw(o1, ot), we use H\u03b8(o1, ot), and so on. The new problem becomes:\nProblem 2. Given D = (O,F , V ), start and end objects o1 \u2208 O and ot \u2208 O, we seek to find a chain/series of objects S1,t = \u3008o1, o2, . . . , ot\u3009 where S1,t is a hammock path H\u03b8(o1, ot) for some distance \u03b8, and further, there is some k-clique path Qk,\u03b8(o1, ot) = \u3008q1, q2, . . . , qt\u22121\u3009 where \u2200i : 1 \u2264 i \u2264 t\u2212 1, oi \u2208 \u03a5(qi)."}, {"heading": "5. Algorithms", "text": "Our overall methodology is based on using the concept lattice to structure the search for paths. Recall that the two parameters influencing the quality of the path\u2014hammock width and clique size\u2014impose a duality. The hammock width is posed over feature sets but the clique size is over objects. We use the clique size to prune the concept lattice during construction (by incorporating it as a support constraint) and the hammock width to select candidates for dynamic construction of paths. There are three key computational stages: (i) construction of the concept lattice, (ii) generating promising candidates for path following, and (iii) evaluating candidates for potential to lead to destination. Of these, the first stage can be viewed as a startup cost that can be amortized over multiple path finding tasks.\nWe adopt the CHARM-L [14] algorithm of Zaki for constructing concept lattices and mining redescriptions. The second and third stages are organized as part of an A* search algorithm that begins with the starting object, uses the concept lattice to identify candidates satisfying the hammock and clique size requirements, and evaluates them heuristically for their promise in leading to the end object. In practice, we will place a limit on the branching factor (b) of the search, thus sacrificing completeness for efficiency. We showcase these steps in detail below, including the construction of admissible heuristics."}, {"heading": "5.1. Successor Generation", "text": "Successor generation is the task of, given an object, using the hammock and clique size requirements, to identify a set of possible successors for path following. Note that this does not use the end object in its computation. We study three techniques for successor generation:\n1. Cover Tree Nearest Neighbor,\n2. Nearest Neighbors Approximation (NNA), and\n3. k-Clique Near Neighbor (kCNN).\nThe first technique is targeted toward finding paths when the clique size requirement is 2 (top and middle paths of Fig. 1). That is, this technique is able to generate b singleton successors, where b is the branching factor of the search. The second two techniques concentrate on paths of any clique size, such as the bottom path of Fig. 1. Instead of generating singleton successors, the NNA and kCNN algorithms are able to generate successor-sets, where each of these sets constitutes a candidate k-clique with the given object."}, {"heading": "5.1.1 Cover Tree Nearest Neighbor", "text": "The cover tree [1] is a data structure for fast nearest neighbor operations in a space of objects organized alongside any distance metric (here, we use the Soergel distance [7] ). The space complexity is O(\u2016O\u2016), i.e., linear in the object size of the database. A nearest neighbor query requires logarithmic time in the object space O ( c12log (n) ) where c is the expansion constant associated with the featureset dimension of the dataset (see [1] for details)."}, {"heading": "5.1.2 Nearest Neighbors Approximation (NNA)", "text": "The second mechanism we use for successor generation is to approximate the nearest neighbors of an object using the concept lattice. We use the Jaccard coefficient between two objects as an indicator to inversely (and approximately) track the Soergel distance between the objects. In order to efficiently calculate an object\u2019s nearest neighbors, however, we cannot simply calculate the Jaccard coefficient between it and every other object. We harness the concept lattice to avoid wasteful comparisons. We first define a predicate \u03c4 on redescriptions and objects.\nAlgorithm 1 NNA(o) Input: An object o \u2208 O fringe \u2190 >(o) order by feature set size prospects \u2190 \u2205 while fringe 6= \u2205 do r \u2190 dequeue from fringe while prospects 6= \u2205 do o\u2032 \u2190 head prospects if J(o, o\u2032) > |Items(o)||Items(r)| then\nyield o\u2032\ndequeue prospects else\nbreak end if\nend while for all r\u2032 \u2208 ChildrenOf r do\nadd r\u2032 to fringe order by feature set size for all o\u2032 in o(r\u2032) do\nadd o\u2032 to prospects order by J(o, o\u2032) end for\nend for end while\nDefinition 10. For redescription RF = (O,F ) and object o, \u03c4(o,RF ) if and only if o \u2208 o(F ) and there is no F \u2032 such that F \u2264 F \u2032 and o \u2208 o(F \u2032).\nInformally, we can say \u03c4(o,RF ) if F is on the edge of redescriptions which contain the object o. In Figure 2, using object m1 the only feature set F for which \u03c4(m1, RF ) would evaluate to true is F = {uA, uD, uE}. Note that if no support threshold (clique constraint) is given, there will always be exactly one such feature set for each object. When using a support threshold greater than one, this is no longer true; so we now define the set >(o, 2F ) for object o.\nDefinition 11. For object o and all redescriptions 2F in a concept lattice containing o, >(o, 2F ) = {F : F \u2208 2F \u2227 \u03c4(o,RF )}.\nWe generally omit the 2F where it is clear from context. The set>(o) is then the set of all redescriptions which form the upper edge in the concept lattice where o appears. All the objects in Figure 2 have singleton sets for >, however, if we change the support threshold from one object to five objects, then the object m1 will have >(m1) = {{uA}, {uD, uE}}.\nA formal description of our NNA algorithm is shown in Algorithm 1.\nDefinition 12. NNAk(o) is the kth object returned by the NNA algorithm when run on input object o.\nTheorem 1. Given a database with object set O with size |O| = n containing object o \u2208 O, \u2200i, j \u2208 Z : 1 \u2264 i \u2264 j \u2264 n\u2192 J(o,NNAi(o)) \u2265 J(o,NNAj(o)).\n(Proof omitted due to space constraints.) In other words, NNA returns better approximate redescriptions of an object first. This is still, however, an approximation since it uses the Jacccard coefficient rather than the Soergel distance.\n5.1.3 k-Clique Near Neighbor (kCNN)\nThe basic idea of the kCNN approach is, in addition to finding a good set of successor nodes for a given object o, to be able to have sufficient number of them so that, combinatorially, they contribute a desired number of cliques. With a clique size constraint of k, it is not sufficient to merely pick the top k neighbors of the given object, since the successor generation function expects multiple clique candidates. (Note that, even if we picked the top k neighbors, we will still need to subject them to a check to verify that every pair satisfies the hammock width constraint.) Given that this function expects b clique candidates, minimum number m of candidate objects to identify can be cast as the solution to the inequalities:(\nm\u2212 1 k\n) < b and ( m k ) \u2265 b\nThe object list of each concept of the lattice is ordered in the number of features (e.g., see Fig. 2) and this aids in picking the top m candidate objects for the given object o. We pick up these m candidate objects for o from the object list of the concept containing the longest feature set and redescription set containing o. Note that, in practice, the object list of each concept is much larger than m and as a result kCNN does not need to traverse the lattice to obtain promising candidates. kCNN thus forms combinations of size k from these m objects to obtain a total of b k-cliques. Since m is calculated using the two inequalities, the total number of such combinations is equal to or slightly greater than b (but never less than b). Each clique is given an average distance score calculated from the distances of the objects of the clique and the current object o. This aids kCNN in returning a priority queue of exactly b candidate k-cliques."}, {"heading": "5.2. Evaluating Candidates", "text": "We now have a basket of candidates that are close to the current object and must determine which of these has potential to lead to the destination. We present two operational modes to rank candidates: (i) the normal mode and (ii) the mixed mode."}, {"heading": "5.2.1 Normal Mode", "text": "The normal mode is suitable for the general case where we have all the objects and features resident in the database. The primary criteria of optimality for the A* search procedure is the cumulative Soergel distance\nof the path. We use the straight line Soergel distance for the heuristic. It is easy to see that this can never oveestimate the cost of a path from any object o to the goal (and is hence admissible), because the Soergel distance maintains the triangle inequality."}, {"heading": "5.2.2 Mixed Mode", "text": "The mixed mode distance measure is effective for large datasets where only important information are stored but other information are removed from the system after recording some of their aggregated information, to save space and cost of establishment. With the mixed mode approach, for simplicity, we assume that all the information about items outside the concept lattice are absent but some of their aggregated information like number of features truncated are provided. Fig. 3 shows the distribution of common and uncommon features of objects o1 and o2 inside and outside a concept lattice. The mixed mode Soergel distance is given by:\nDmixed(o1, o2) = |U1L|+ |U2L|\n|U1L|+ |U2L|+ |CL|+ |TO| where the terms are defined in Fig. 3.\nTheorem 2. Dmixed(o1, o2) never overestimates the original Soergel distance D(o1, o2).\nProof: Omitted due to more general Theorem 3 later."}, {"heading": "5.2.3 Mixed Mode in Vector Spaces", "text": "However, the mixed mode becomes more complex in the vector space model than its set model. Figure 4 shows our formula for the mixed mode in the vector space. Similar to the unweighted one, we assume that all the information about features of objects outside the concept lattice are absent in the weighted mixed\nmode approach. But some of their aggregated information like minimum (minw) and maximum (maxw) weights are provided.\nConsider the set of features TO that do not appear in the lattice due to the support threshold of the concept lattice, minsup. Some features of TO can be common to both objects o1 and o2. |U1O| and |U2O| are the numbers of uncommon features in objects o1 and o2, which are thus outside the frequent concept lattice. Length |TO| is a known variable due to the recorded aggregated information, but |U1O| and |U2O| are unknown. This is why |U1O| and |U2O| do not appear in Dmixed(o1, o2). For Dmixed(o1, o2), we consider that all the features of TO (i.e., features outside the lattice) are common in both objects o1 and o2 and all these features have the same weight which is max(maxw(o1),maxw(o2)).\nTheorem 3. Dmixed(o1, o2) never overestimates the original Soergel distance D(o1, o2).\nProof: Let the numerator of D(o1, o2) and Dmixed(o1, o2) be \u03b71,2 and \u03b71,2mixed respectively. Similarly, let $1,2 and $1,2mixed be the denominators of D(o1, o2) and Dmixed(o1, o2). It is clear that \u03b7 1,2 mixed \u2264 \u03b71,2. Therefore, it suffices to show that $1,2mixed \u2265 $1,2. For ease of derivation, we define the following notation :\n\u03b1 = |U1L| \u00d7minw (o1) + |U2L| \u00d7minw (o2) \u03b2 = \u2211 f\u2208CL max(V (o1, f), V (o2, f))\n\u03b6 = \u2211\nf\u2208(TO\u2212U1O\u2212U2O)\nmax(V (o1, f), V (o2, f))\n\u03c7 = |U1| \u00d7minw (o1) + |U2| \u00d7minw (o2) \u03be = |U1O| \u00d7minw (o1) + |U2O| \u00d7minw (o2) % = |TO| \u00d7max (maxw (o1) ,maxw (o2))\nTherefore we have, $1,2 = \u03c7 + \u03b2 + \u03b6. Now, $1,2mixed = \u03b1 + \u03b2 + % = \u03c7 + \u03b2 + \u03b6 \u2212 \u03be + % \u2212 \u03b6 = $1,2 + %\u2212 \u03b6\u2212 \u03be = $1,2 +L, where L = %\u2212 \u03b6\u2212 \u03be. Since L \u2265 0 is always true, $1,2mixed \u2265 $1,2. Therefore, Dmixed(o1, o2) \u2264 D(o1, o2)."}, {"heading": "6. Experimental Results", "text": "We present our experimental results on four datasets (see Table 1) using a 64-bit Windows Vista Intel Core2 Quad CPU Q9450 @ 2.66GHz and 8 GB physical memory."}, {"heading": "6.1. Evaluating successor generation strategies", "text": "The goal of our first experiment was to assess the number of nodes explored by the A* search and the time taken as a function of the discovered path length, as a function of different successor generation\nstrategies. For this purpose, we defined a synthetic dataset involving 1000 randomly selected movies from the Netflix dataset and aimed to generate more than 50,000 similarity paths between randomly selected pairs, with a \u03b8 threshold of 0.95 and a branching factor bound (b) of 20. We introduced a time threshold of 120 seconds in the A* search and Fig. 5 depicts the results of only successful searches. Around 80% of these searches failed when using the the cover tree and NNA approaches, but the kCNN approach was able to either successfully generate paths or to declare that no path exists in less than 120 seconds. As Fig. 5 shows, the cover tree and NNA terminate early due to the applied time constraint but kCNN generated long paths of length 14 and 13 with k=2 and 7, respectively. The runtime trends shown in Fig. 5 (right) also mirror the number of nodes explored in Fig. 5 (left).\nThis result is not surprising, as the cover tree algorithm does not factor in the clique constraint, thus preventing it from taking advantage of the search space reduction that this constraint provides. NNA does take advantage of this constraint, however, and it generates a strict ordering on the Jaccard\u2019s coefficient over the cliques, whereas kCNN simply generates some b candidate cliques. In practice, the kCNN relaxation results in the discovery of candidate cliques more rapidly than the NNA algorithm, while still remaining accurate as in both cases a post processing step is necessary to determine if a given candidate does in fact meet the search threshold. Through the rest of this paper, we thus use the kCNN algorithm as it generally provides the best performance of the three algorithms."}, {"heading": "6.2. Netflix dataset", "text": "Viewing movies as objects and users as features, we construct the concept lattice for the Netflix dataset with a support constraint of 20%. The resulting lattice contains 5,884 concepts, 120 of whom were leaves and the rest had child concepts. We picked 50,000 pairs of movies and attempted to generate hammock paths between them.\nFigure 6 shows our experimental results with varying clique size and fixed distance threshold. From the\ngraph at the left, it is evident that the performance of our kCNN algorithm depends on the clique size since the graph shows that hammock paths with lower clique sizes are generated faster than those with higher clique sizes. The tendency of the run time basically follows the required number of nodes explored to generate the paths (the second graph). The third graph shows the corresponding trends of effective branching factor. It is evident that the higher the clique size the worse (or larger) the effective branching factor is. Effective branching factor is a measure to understand the size of the traversed search space compared with corresponding breadth first search (BFS). Therefore it is a measure of the efficacy of the heuristic in pruning out unwanted paths. Although the effective branching factor becomes worse with larger clique size, it generates smaller hammock paths. Thus, the lengths of the paths are also affected by the clique size requirement. The plot at the right depicts the lengths of the hammock paths as a function of clique size. It demonstrates that our algorithm has the capability to generate longer chains with smaller cliques. For example, Figure 7 gives hammock paths between the 1943 classic drama \u201cTitanic\u201d and the action movie \u201cDie Hard 2: Die Harder\u201d, for four different clique sizes (k=9, 12, 15 and 18)."}, {"heading": "6.3. PubMed Abstracts", "text": "In our PubMed case study, we view paper abstracts as objects and terms as features. We curated 161,693 publications related to several cytokines, transcription factors and feedback molecules and modeled only their titles and abstracts. There were around 133,252 unique stemmed terms in the dataset after the removal of stop words, numerals, DNA sequences, and special characters. Again, for randomly selected pairs of papers, we discovered over a million hammock paths.\nFigure 8(a) shows the trends of number of nodes explored, time to generate paths, and effective branching factor, as a function of path length. Although Figure 8(a) shows that the average number of nodes explored is large with higher threshold, it is not necessary that this behavior would remain the same for other datasets. The result can in fact be opposite in some datasets where number of paths generated are not lessened by reducing \u03b8, especially when each object has a large number of features.\nFigure 8(b) (left) shows that the use of Soergel distance heuristic saves significant object exploration over the vanilla BFS. The larger the clique size, the higher the percentage by which the heuristic reduces the number of node explorations. It shows that the use of the straight line Soergel distance as the heuristic saves more than 300% node exploration by the A* procedure over the BFS (h=0), for a clique size k=14. The saved amount is more than 100% even with the smallest clique size k=2. The average time to generate hammock path also reduces due to the saved node exploration. Figure 8(b) (middle) shows that the heuristic saves more than 800% runtime with clique size k=14. Even with a clique size k=2, the savings are near 200%. In the best case with k=14, the heuristic also improves the effective branching factor by 90% over\nthe BFS shown in the plot at the right. In the worst case with k=2, it offers around 4% improvement of the effective branching factor.\nDespite the use of truncated dataset, Figure 8(c) shows that the mixed mode gains due to the heuristic over the BFS have a similar trend to the normal mode of Figure 8(b). Therefore, the mixed mode offers a practical mechanism to provide the best possible gains from lossy datasets without time consuming remodeling of the vector space (e.g., [5] uses costly remodelling as a post processing step)."}, {"heading": "6.4. Clinical Trials", "text": "We curated more than 60 thousand clinical trials from clinicaltrials.gov and concentrated on the purpose and description of each trial. Here trials are objects and terms are features. The concept lattice we used had around a thousand unique concepts and was generated with minsup=10%. Due to space restrictions, we show qualitative rather than quantitative results here.\nTable 2 describes a significant chain of trials connecting two disparate clinical studies related to congestive heart failure and kidney transplantation in patients with cystinosis. (One accepted practice to assess the statistical significance of discovered chains is, for each step in the path, to assess the likelihood of overlap for the given descriptor sizes using the hypergeometric distribution and attribute a p-value after FDR corrections such the Benjamini-Hochberg procedure.) The chain starts with heart failure trials and goes through studies on atrial fibrillation (abnormal heart rhythm), cardiac ablation, colorectal cancer, advanced solid tumors, and eventually reaches clinical study on kidney transplantation in patients with cystinosis. Such connections between cardio-vascular disease and kidney failures are an intense topic of current research (e.g., see [3])."}, {"heading": "7. Discussion", "text": "We have presented an efficient algorithmic approach to discover hammock paths in similarity networks without inducing these networks in their entirety. The experimental results have demonstrated scalability, effectiveness of our heuristics, and ability to yield domain-specific insight. We posit that our approach can be a useful information exploration tool for understanding the structure of connectivities underlying boolean and vector-valued association datasets. Future work is geared toward incorporating additional distance measures and defining new compressed representations of datasets that can serve multiple uses, from concept modeling to distance estimation."}], "references": [{"title": "Cover Trees for Nearest Neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "ICML \u201906, pages 97\u2013104", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparsification\u2014A Technique for Speeding up Dynamic Graph Algorithms", "author": ["D. Eppstein", "Z. Galil", "G.F. Italiano", "A. Nissenzweig"], "venue": "J. ACM, 44(5):669\u2013696", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Outcomes of Simultaneous Heart-Kidney Transplant in the US: A Retrospective Analysis Using OPTN/UNOS Data", "author": ["J. Gill"], "venue": "Am J Transplant.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Evaluation of Item-Based Top-N Recommendation Algorithms", "author": ["G. Karypis"], "venue": "CIKM \u201901, pages 247\u2013254", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Algorithms for Storytelling", "author": ["D. Kumar", "N. Ramakrishnan", "R.F. Helm", "M. Potts"], "venue": "KDD \u201906, pages 604\u2013610", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Connectivity Structure of Bipartite Graphs via the KNC-plot", "author": ["R. Kumar", "A. Tomkins", "E. Vee"], "venue": "WSDM \u201908, pages 129\u2013138", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Introduction to Chemoinformatics", "author": ["A. Leach", "V. Gillet"], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Studying Recommendation Algorithms by Graph Analysis", "author": ["B.J. Mirza", "B.J. Keller", "N. Ramakrishnan"], "venue": "J. Intell. Inf. Syst., 20(2):131\u2013160", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A Lattice-Based Model for Recommender Systems", "author": ["S. Narayanaswamy", "R. Bhatnagar"], "venue": "ICTAI \u201908, pages 349\u2013356", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A Browser for Bibliographic Information Retrieval", "author": ["G.S. Pedersen"], "venue": "based on an Application of Lattice Theory. In SIGIR \u201993, pages 270\u2013279", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Turning CARTwheels: an Alternating Algorithm for Mining Redescriptions", "author": ["N. Ramakrishnan", "D. Kumar", "B. Mishra", "M. Potts", "R.F. Helm"], "venue": "KDD \u201904, pages 266\u2013275", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Using ARROWSMITH: A Computer-assisted Approach to Formulating and Assessing Scientific Hypotheses", "author": ["N.R. Smalheiser", "D.R. Swanson"], "venue": "Computer Methods and Programs in Biomedicine, 57(3):149\u2013153", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "The vertex degree distribution of random intersection graphs", "author": ["D. Stark"], "venue": "Random Struct. Algorithms, 24(3):249\u2013 258", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Reasoning About Sets Using Redescription Mining", "author": ["M.J. Zaki", "N. Ramakrishnan"], "venue": "KDD \u201905, pages 364\u2013373", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "Two movies can be connected if they have been rated similarly by sufficient number of people; this is the basis for the popular itembased recommendation algorithms [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 11, "context": "Here path finding has applications to literature-based discovery (similar to the ARROWSMITH program [12]) and clinical diagnosis.", "startOffset": 100, "endOffset": 104}, {"referenceID": 7, "context": "Graph modeling: The use of the word \u2018hammock\u2019 for induced similarity networks appears to have been first made in [8] although this work does not aim to find paths.", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "The notion of kNC-plots was introduced in [6] where k denotes what we refer to as hammock width in this paper.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "Random intersection graphs [13] are a class of theoretical models proposed in the random graph community.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Under these modeling assumptions, connectivity and other properties of these graphs can be characterized [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "Redescription mining: Redescriptions, a pattern class introduced in [11], induce subsets of data that share strong overlap.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "[5] study the problem of \u2018storytelling\u2019 over a space of descriptors, which is the goal of finding a sequence (story) of redescriptions between two subsets by positing intermediate subsets that share sufficient overlap with their neighbors in the story.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Although this is similar to our objective, the heuristic innovations in [5] are restricted to binary spaces as well and cannot find paths of the same expressiveness (hammocks organized into cliques) and with the same efficiencies as done here.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Lattice based information retrieval: The use of concept lattices as organizing data structures for fast retrieval, query, and data mining is not new [9, 10].", "startOffset": 149, "endOffset": 156}, {"referenceID": 9, "context": "Lattice based information retrieval: The use of concept lattices as organizing data structures for fast retrieval, query, and data mining is not new [9, 10].", "startOffset": 149, "endOffset": 156}, {"referenceID": 10, "context": "[11] to capture similarities between descriptors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We adopt the CHARM-L [14] algorithm of Zaki for constructing concept lattices and mining redescriptions.", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "The cover tree [1] is a data structure for fast nearest neighbor operations in a space of objects organized alongside any distance metric (here, we use the Soergel distance [7] ).", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The cover tree [1] is a data structure for fast nearest neighbor operations in a space of objects organized alongside any distance metric (here, we use the Soergel distance [7] ).", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "A nearest neighbor query requires logarithmic time in the object space O ( c12log (n) ) where c is the expansion constant associated with the featureset dimension of the dataset (see [1] for details).", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": ", [5] uses costly remodelling as a post processing step).", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": ", see [3]).", "startOffset": 6, "endOffset": 9}], "year": 2010, "abstractText": "Similarity networks are important abstractions in many information management applications such as recommender systems, corpora analysis, and medical informatics. For instance, in a recommender system, by inducing similarity networks between movies rated similarly by users, we can aim to find the global structure of connectivities underlying the data, and use the network to posit connections between given entities. We present an algorithmic framework to efficiently find paths in an induced similarity network without materializing the network in its entirety. Our framework introduces the notion of \u2018hammock\u2019 paths which are generalizations of traditional paths in bipartite graphs. Given starting and ending objects of interest, it explores candidate objects for path following, and heuristics to admissibly estimate the potential for paths to lead to a desired destination. We present three diverse applications, modeled after the Netflix dataset, a broad subset of the PubMed corpus, and a database of clinical trials. Experimental results demonstrate the potential of our approach for unstructured knowledge discovery in similarity networks.", "creator": "LaTeX with hyperref package"}}}