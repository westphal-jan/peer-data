{"id": "1004.5194", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2010", "title": "Clustering processes", "abstract": "We propose a very natural asymptotic notion of consistency and show that, under most general non-parametric assumptions, simple consistent algorithms exist: the concept of consistency is as follows: two samples should be placed in the same cluster if and only if they are generated by the same distribution; with this concept of consistency, clustering generalizes such classic statistical problems as homogeneity testing and process classification; we show that in the case of a known number of clusters, consistency can only be achieved on the sole assumption that the common distribution of the data is stationary ergodic (no parametric or Marcovian assumptions, no assumptions of independence, neither between nor within the samples); if the number of clusters is unknown, consistency can be achieved under appropriate assumptions about the mixing rates of the processes (again no parametric or independent assumptions).", "histories": [["v1", "Thu, 29 Apr 2010 06:38:47 GMT  (16kb)", "http://arxiv.org/abs/1004.5194v1", "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this article please see:arXiv:1005.0826v1"]], "COMMENTS": "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this article please see:arXiv:1005.0826v1", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["daniil ryabko"], "accepted": true, "id": "1004.5194"}, "pdf": {"name": "1004.5194.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 4.\n51 94\nv1 [\ncs .L\nG ]\n2 9\nA pr\nThe problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent."}, {"heading": "1 Introduction", "text": "Given a finite set of objects, the problem is to \u201ccluster\u201d similar objects together. This intuitively simple goal is notoriously hard to formalize. Most of the work on clustering is concerned with particular parametric data generating models, or particular algorithms, a given similarity measure, and (very often) a given number of clusters. It is clear that,\nas in almost learning problems, in clustering finding the right similarity measure is an integral part of the problem. However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009). What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons. Indeed, in this case finding k centres (points which minimize the cumulative distance from each point in the sample to one of the centres) seems to be a natural goal, but this problem is NPhard Mahajan et al. (2009).\nIn this work we concentrate on a subset of the clustering problem: clustering processes. That is, each data point is itself a sample generated by a certain discrete-time stochastic process. This version of the problem has numerous applications, such as clustering biological data, financial observations, or behavioural patterns, and as such it has gained a tremendous attention in the literature.\nThe main observation that we make in this work is that, in the case of clustering processes, one can benefit from the notion of ergodicity to define what appears to be a very natural notion of consistency. This notion of consistency is shown to be satisfied by simple algorithms that we present, which are polynomial in all arguments. This can be achieved without any modeling assumptions on the data (e.g. Hidden Markov, Gaussian, etc.), without assuming independence of any kind within or between the samples. The only assumption that we make is that the joint distribution of the data is stationary ergodic. The assumption of stationarity means, intuitively, that\nthe time index itself bares no information: it does not matter whether we have started recording observations at time 0 or at time 100. By virtue of the ergodic theorem, any stationary process can be represented as a mixture of stationary ergodic processes. In other words, a stationary process can be thought of as first selecting a stationary ergodic process (according to some prior distribution) and then observing its outcomes. Thus, the assumption that the data is stationary ergodic is both very natural and rather weak. At the same time, ergodicity means that, in asymptotic, the properties of the process can be learned from observation. This allows us to define the clustering problem as follows. N samples are given: x1 = (x11, . . . , x 1 n1), . . . ,xN = (x N 1 , . . . , x N nN ). Each sample is drawn by one out of k different stationary ergodic distributions. The samples are not assumed to be drawn independently; rather, it is assumed that the joint distribution of the samples is stationary ergodic. The target clustering is as follows: those and only those samples are put into the same cluster that were generated by the same distribution. The number k of target clusters can be either known or unknown (different consistency results can be obtained in these cases). A clustering algorithm is called asymptotically consistent if the probability that it outputs the target clustering converges to 1, as the lengths (n1, . . . , nN ) of the samples tend to infinity (a variant of this definition is to require the algorithm to stabilize on the correct answer with probability 1). Note the particular regime of asymptotic: not with respect to the number of samples N , but with respect to the length of the samples n1, . . . , nN . Similar formulations have appeared in the literature before. Perhaps the most close approach is mixture models Smyth (1997); Zhong & Ghosh (2003): it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability). Since the model of the data is specified quite well, one can use likelihoodbased distances (and then, for example, the k-means algorithm), or Bayesian inference, to cluster the data. Clearly, the main difference from our setting is in that we do not assume any known model of the data; not even between-sample independence is assumed. The problem of clustering in our formulation generalizes two classical problems of mathematical statistics. The first one is homogeneity testing, or the twosample problem. Two samples x1 = (x 1 1, . . . , x 1 n1) and x2 = (x 2 1, . . . , x 2 n2) are given, and it is required to test whether they were generated by the same distribution, or by different distributions. This corresponds to clustering just two data points (N = 2) with the number k of clusters unknown: either k = 1 or k = 2. The second problem is process classification, or the three-sample problem. Three samples x1,x2,x3 are given, it is known that two of them were generated by the same distribution, while the third one was generated by a different distribution. It is required to find out which two were generated by the same distribution. This corresponds to clustering three data points, with the number of clusters known: k = 2. The classical approach is of course to consider Gaussian i.i.d. data, but general non-parametric solutions exist not only for i.i.d. data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in Ryabko (2010b) it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense. However, a solution to the threesample problem, for (real-valued) stationary ergodic processes was given in Ryabko & Ryabko (2010). In this work we demonstrate that, if the number k of clusters is known, then there is an asymptotically consistent clustering algorithm, under the only assumption that the joint distribution of data is stationary ergodic. If k is unknown, then in this general case there is no consistent clustering algorithm (as follows from the mentioned result for the twosample problem). However, if an upper-bound \u03b1n on the \u03b1-mixing rates of the joint distribution of the processes is known, and \u03b1n \u2192 0, then there is a consistent clustering algorithm. Both algorithms are\nrather simple, and are based on the empirical estimates of the so-called distributional distance. For two processes \u03c11, \u03c12 a distributional distance d is defined as\n\u2211\u221e k=1 wk|\u03c11(Bk)\u2212\u03c12(Bk)|, where wk are pos-\nitive summable real weights, e.g. wk = 2 \u2212k, and Bk range over a countable field that generates the sigmaalgebra of the underlying probability space. For example, if we are talking about finite-alphabet processes with the binary alphabet A = {0, 1}, Bk would range over the set A\u2217 = \u222ak\u2208NA\nk; that is, over all tuples 0, 1, 00, 01, 10, 11, 000, 001, . . . (of course, we could just as well omit, say, 1 and 11); therefore, the distributional distance in this case is the weighted sum of differences of probabilities of all possible tuples. In this work we consider real-valued processes, so Bk have to range through a suitable sequence of intervals, all pairs of such intervals, triples, etc. (see the formal definitions below). This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument. Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these\nalgorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in Ryabko & Astola (2006) for the related problems of hypotheses testing, leading both to theoretical and practical results. As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in Cilibrasi & Vitanyi (2005), and (in a different way) in Bagnall et al. (2006). Combining our consistency framework with these compression-based methods is a promising direction for further research."}, {"heading": "2 Preliminaries", "text": "Let A be an alphabet, and denote A\u2217 the set of tuples \u222a\u221ei=1A\ni. In this work we consider the case A = R; extensions to the multidimensional case, as well as to more general spaces, are straightforward. Distributions, or (stochastic) processes, are measures on the space (A\u221e,FA\u221e), where FA\u221e is the Borel sigmaalgebra of A\u221e. When talking about joint distributions of N samples, we mean distributions on the space ((AN )\u221e,F(AN)\u221e).\nFor each k, l \u2208 N, let Bk,l be the partition of the set Ak into k-dimensional cubes with volume hkl = (1/l)k (the cubes start at 0). Moreover, define Bk = \u222al\u2208NB k,l and B = \u222a\u221ek=1B k. The set {B \u00d7 A\u221e : B \u2208 Bk,l, k, l \u2208 N} generates the Borel \u03c3-algebra on R\u221e = A\u221e. For a set B \u2208 B let |B| be the index k of the set Bk that B comes from: |B| = k : B \u2208 Bk. We use the abbreviation X1..k for X1, . . . , Xk. For a sequence x \u2208 An and a set B \u2208 B denote \u03bd(x, B) the frequency with which the sequence x falls in the set B.\n\u03bd(x, B) := {\n1 n\u2212|B|+1 \u2211n\u2212|B|+1 i=1 I{(Xi,...,Xi+|B|\u22121)\u2208B} if n \u2265 |B|,\n0 otherwise.\nA process \u03c1 is stationary if \u03c1(X1..|B| = B) = \u03c1(Xt..t+|B|\u22121 = B) for any B \u2208 A\n\u2217 and t \u2208 N. We further abbreviate \u03c1(B) := \u03c1(X1..|B| = B). A stationary process \u03c1 is called (stationary) ergodic if the frequency of occurrence of each word B in a sequence X1, X2, . . . generated by \u03c1 tends to its a priori (or limiting) probability a.s.: \u03c1(limn\u2192\u221e \u03bd(X1..n, B) = \u03c1(B)) = 1. Denote E the set of all stationary ergodic processes.\nDefinition 1 (distributional distance). The distributional distance is defined for a pair of processes \u03c11, \u03c12 as follows (e.g. Gray (1988))\nd(\u03c11, \u03c12) =\n\u221e \u2211\nm,l=1\nwmwl \u2211\nB\u2208Bm,l\n|\u03c11(B) \u2212 \u03c12(B)|,\nwhere wj = 2 \u2212j.\n(The weights in the definition are fixed for the sake of concreteness only; we could take any other summable sequence of positive weights instead.) In words, we are taking a sum over a series of partitions into cubes of decreasing volume (indexed by l) of all sets Ak, k \u2208 N, and count the differences in probabilities of all cubes in all these partitions. These differences in probabilities are weighted: smaller weights are given to larger k and finer partitions. It is easy to see that d is a metric. We refer to Gray (1988) for more information on this metric and its properties. The clustering algorithms presented below are based on empirical estimates of the distance d:\nd\u0302(X11..n1 , X 2 1..n2) =\n\u221e \u2211\nm,l=1\nwmwl \u2211\nB\u2208Bm,l\n|\u03bd(X11..n1 , B)\u2212 \u03bd(X 2 1..n2 , B)|,\n(1)\nwhere n1, n2 \u2208 N, \u03c1 \u2208 S, X i 1..ni \u2208 A ni . Although the expression (1) involves taking three infinite sums, it will be shown below that it can be easily calculated.\nLemma 1 (d\u0302 is consistent). Let \u03c11, \u03c12 \u2208 E and let two samples x1 = X 1 1..n1 and x2 = X 2 1..n2 be generated by a distribution \u03c1 such that the marginal distri-\nbution of X i1..n1 is \u03c1i, i = 1, 2, and the joint distribution \u03c1 is stationary ergodic. Then\nlim n1,n2\u2192\u221e\nd\u0302(X11..n1 , X 2 1..n2) = d(\u03c11, \u03c12) \u03c1\u2013a.s.\nProof. The idea of the proof is simple: for each set B \u2208 B, the frequency with which the sample x1 falls into B converges to the probability \u03c11(B), and analogously for the second sample. When the sample sizes grow, there will be more and more sets B \u2208 B whose frequencies have already converged to the probabilities, so that the cumulative weight of those sets whose frequencies have not converged yet, will tend to 0. For any \u03b5 > 0 we can find an index J such that \u2211\u221e\ni,j=J wiwj < \u03b5/3. Moreover, for each m, l\nwe can find such elements Bm,l1 , . . . , B m,l tm,l , for some tm,l \u2208 N, of the partition B m,l that \u03c1i(\u222a tm,l i=1B m,l i ) \u2265 1 \u2212 \u03b5/6Jwmwl. For each B m,l j , where m, l \u2264 J and j \u2264 tm,l, we have \u03bd((X 1 1 , . . . , X 1 n1), B m,l j ) \u2192 \u03c11(B m,l j ) a.s., so that\n|\u03bd((X11 , . . . , X 1 n1), B m,l j )\u2212 \u03c11(B m,l j )|\n< \u03c11(B m,l j )\u03b5/(6Jwj)\nfor all n1 \u2265 u, for some u \u2208 N; define U m,l j := u. Let U := maxm,l\u2264J,j\u2264tm,l U m,l j (U depends on the realization X11 , X 1 2 , . . . ). Define analogously V for the sequence (X21 , X 2 2 , . . . ). Thus for n1 > U and n2 > V we have\n|d\u0302(x1,x2) \u2212 d(\u03c11, \u03c12)| = \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u221e \u2211\nm,l=1\nwmwl \u2211\nB\u2208Bk,l\n( |\u03bd(x1, B) \u2212 \u03bd(x2, B)| \u2212 |\u03c11(B) \u2212 \u03c12(B)| )\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u221e \u2211\nm,l=1\nwmwl \u2211\nB\u2208Bk,l\nwi ( |\u03bd(x1, B)\u2212\u03c11(B)|+ |\u03bd(x2, B)\u2212\u03c12(B)| )\n\u2264\nJ \u2211\nm,l=1\nwmwl\ntk,l \u2211\ni=1\n(\n|\u03bd(x1, B m,l i ) \u2212 \u03c11(B m,l i )|\n+ |\u03bd(x2, B m,l i ) \u2212 \u03c12(B m,l i )| ) + 2\u03b5/3\n\u2264 J \u2211\nm,l=1\nwmwl\ntk,l \u2211\ni=1\n(\u03c11(B m,l\ni )\u03b5/(6Jwmwl)\n+ \u03c12(B m,l\ni )\u03b5/(6Jwmwl)) + 2\u03b5/3 \u2264 \u03b5,\nwhich proves the statement."}, {"heading": "3 Main results", "text": "The clustering problem can be defined as follows. We are given N samples x1, . . . ,xN , where each sample xi is a string of length ni of symbols from A: xi = X i 1..ni . Each sample is generated by one out of k different unknown stationary ergodic distributions \u03c11, . . . , \u03c1k \u2208 E . Thus, there is a partitioning I = {I1, . . . , Ik} of the set {1..N} into k disjoint subsets Ij , j = 1..k\n{1..N} = \u222akj=1Ij ,\nsuch that xj , 1 \u2264 j \u2264 N is generated by \u03c1j if and only if j \u2208 Ij . The partitioning I is called the target clustering and the sets Ii, 1 \u2264 i \u2264 k, are called the target clusters. Given samples x1, . . . ,xN and a target clustering I, let I(x) denote the cluster that contains x. A clustering function F takes a finite number of samples x1, . . . ,xN and an optional parameter k (the target number of clusters) and outputs a partition F (x1, . . . ,xN , (k)) = {T1, . . . , Tk} of the set {1..N}.\nDefinition 2 (asymptotic consistency). Let a finite number N of samples be given, and let the target clustering partition be I. Define n = min{n1, . . . , nN}. A clustering function F is strongly asymptotically consistent if F (x1, . . . ,xN , (k)) = I from some n on with probability 1. A clustering function is weakly asymptotically consistent if P (F (x1, . . . ,xN , (k)) = I) \u2192 1.\nNote that the consistency is asymptotic with respect to the minimal length of the sample, and not with respect to the number of samples."}, {"heading": "3.1 Known number of clusters", "text": "Algorithm 1 is a simple clustering algorithm, which, given the number k of clusters, will be shown to be consistent under most general assumptions. It works as follows. The point x1 is assigned to the first cluster. Next, find the point that is farthest away from x1 in the empirical distributional distance d\u0302, and assign this point to the second cluster. For each j = 3..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign\nit to the cluster j. Thus we have one point in each of the k clusters. Next simply assign each of the remaining points to the cluster that contains the closest points from those k already assigned. (One may notice that Algorithm 1 is one iteration of the k-means algorithm, with a specific initialization, and a specially designed distance.)\nAlgorithm 1 The case of known number of clusters k\nINPUT: The number of clusters k, samples x1, . . . , xN . Initialize: j := 1, c1 := 1, T1 := {xc1}. for j := 2 to k do cj := argmax{i = 1, . . . , N : min j\u22121 t=1 d\u0302(xi,xct)}\nTj := {xcj} end for for i = 1 to N do Put xi into the set Targmink\nj=1 d\u0302(xi,xcj )\nend for OUTPUT: the sets Tj, j = 1..k.\nProposition 1 (calculating d\u0302(x1,x2)). For two samples x1 = X 1 1..n1 and x2 = X 2 1..n2 the computational complexity (time and space) of calculating\nthe empirical distributional distance d\u0302(x1,x2) (1) is O(n2 log s\u22121min), where n = max(n1, n2) and\nsmin = min i=1..n1,j=1..n2,X1i 6=X 2 j\n|X1i \u2212X 2 j |.\nProof. First, observe that for fixed m and l, the sum\nTm,l := \u2211\nB\u2208Bm,l\n|\u03bd(X11..n1 , B)\u2212 \u03bd(X 2 1..n2 , B)| (2)\nhas not more than n1 + n2 \u2212 2m + 2 non-zero terms (assuming m \u2264 n1, n2; the other case is obvious). Indeed, for each i = 0, 1, in the sample xi there are ni \u2212 m + 1 tuples of size k: X i1..m, X i 2..m+1, . . . , X i n1\u2212m+1..n1 . Therefore, the complexity of calculating Tm,l is O(n1 + n2 \u2212 2m+ 2) = O(n). Furthermore, observe that for each m, for all l > log s\u22121min the term T m,l is constant. Therefore, it is enough to calculate Tm,1, . . . , Tm,log s \u22121 min , since for fixed m\n\u221e \u2211\nl=1\nwmwlT m,l = wmw\nlog s \u22121 min\nTm,log s \u22121 min +\nlog s \u22121 min\n\u2211\nl=1\nwmwlT m,l\n(that is, we double the weight of the last nonzero term). Thus, the complexity of calculating \u2211\u221e\nl=1 wmwlT m,l is O(n log s\u22121min). Finally, for all m > n we have Tm,l = 0. Since d\u0302(x1,x2) = \u2211\u221e\nm,l=1 wm, wlT m,l, the statement is proven.\nTheorem 1. Let N \u2208 N and suppose that the samples x1, . . . ,xN are generated in such a way that the joint distribution is stationary ergodic. If the correct number of clusters k is known, then Algorithm 1 is strongly asymptotically consistent. Algorithm 1 makes O(kN) calculations of d\u0302(\u00b7, \u00b7), so that its computational complexity is O(kNn2max log s \u22121 min), where nmax = max k i=1 ni and\nsmin = min u,v=1..N,u6=v,i=1..nu,j=1..nv ,Xui 6=X v j\n|Xui \u2212X v j |.\nObserve that the samples are not required to be generated independently. The only requirement on the distribution of samples is that the joint distribution is stationary ergodic. This is perhaps one of the mildest possible probabilistic assumptions.\nProof. By Lemma 1, d\u0302(xi,xj), i, j \u2208 {1..N} converges to 0 if and only if xi and xj are in the same cluster. Since there are only finitely many samples xi, there exists some \u03b4 > 0 such that, from some n on, we will have d\u0302(xi,xj) < \u03b4 if xi,xj belong to the same target cluster (I(xi) = I(xj)), and d\u0302(xi,xj) > \u03b4 otherwise (I(xi) 6= I(xj)). Therefore, from some n on, for every j \u2264 k we will have max{i = 1, . . . , N : minj\u22121t=1 d\u0302(xi,xct)} > \u03b4 and the sample xcj , where cj = argmax{i = 1, . . . , N : min j\u22121 t=1 d\u0302(xi,xct)}, will be selected from a target cluster that does not contain any xci , i < j. The consistency statement follows. Next, let us find how many pairwise distance estimates d\u0302(xi,xj) the algorithm has to make. On the first iteration of the loop, it has to calculate d\u0302(xi,xc1) for all i = 1..N . On the second iteration, it needs again d\u0302(xi,xc1) for all i = 1..N , which are already calculated, and also d\u0302(xi,xc2) for all i = 1..N , and so on: on jth iteration of the loop we need to calculate d(xi,xcj ), i = 1..N , which gives at most kN pairwise distance calculations in total. The statement about computational complexity follows from\nthis and Proposition 1: indeed, apart from the calculation of d\u0302, the rest of the computations is of order O(kN).\nComplexity\u2013precision trade\u2013off. The bound on the computational complexity of Algorithm 1, given in Theorem 1, is given for the case of precisely calculated distance estimates d\u0302(\u00b7, \u00b7). However, precise estimates are not needed if we only want to have an asymptotically consistent algorithm. Indeed, following the proof of Lemma 1, it is easy to check that if we replace in (1) the infinite sums with sums over any number of terms mn, ln that grows to infinity with n = min(n1, n2), and if we replace partitions B\nm,l by their (finite) subsets Bm,l,n which increase to Bm,l, then we still have a consistent estimate of d(\u00b7, \u00b7).\nDefinition 3 (d\u030c). Let mn, ln be some sequences of numbers, Bm,l,n \u2282 Bm,l for all m, l, n \u2208 N, and denote n := min{n1, n2}. Define\nd\u030c(X11..n1 , X 2 1..n2) :=\nmn \u2211\nm=1\nln \u2211\nl=1\nwmwl \u2211\nB\u2208Bm,l,n\n|\u03bd(X11..n1 , B)\u2212\u03bd(X 2 1..n2 , B)|.\n(3)\nLemma 2 (d\u030c is consistent). Assume the conditions of Lemma 1. Let ln and mn be any sequences of integers that go to infinity with n, and let, for each m, l \u2208 N, the sets Bm,l,n, n \u2208 N be an increasing sequence of subsets of Bm,l, such that \u222an\u2208NB m,l,n = Bm,l. Then\nlim n1,n2\u2192\u221e\nd\u030c(X11..n1 , X 2 1..n2) = d(\u03c11, \u03c12) \u03c1\u2013a.s..\nProof. It is enough to observe that\nlim n1,n2\u2192\u221e\nmn \u2211\nm=1\nln \u2211\nl=1\nwmwl \u2211\nB\u2208Bm,l,n\n|\u03c11(B) \u2212 \u03c12(B)|\n= d(\u03c11, \u03c12),\nand then follow the proof of Lemma 1.\nIf we use the estimate d\u030c(\u00b7, \u00b7) in Algorithm 1 (instead\nof d\u0302(\u00b7, \u00b7)), then we still get an asymptotically consistent clustering function. Thus the following statement holds true.\nProposition 2. Assume the conditions of Theorem 1. For all sequences mn, ln of numbers that increase to infinity with n, there is a strongly asymptotically consistent clustering algorithm, whose computational complexity is O(kNnmaxmnmax lnmax).\nOn the one hand, Proposition 2 can be thought of as an artifact of the asymptotic definition of consistency; on the other hand, in practice precise calculation of d\u0302(\u00b7, \u00b7) is hardly necessary. What we get from Proposition 2 is the possibility to select the appropriate trade\u2013off between the computational burden, and the precision of clustering before asymptotic. Note that the bound in Proposition 2 does not involve the sizes of the sets Bm,l,n; in particular, one can take Bm,l,n = Bm,l for all n. This is because, for every two samples X11..n and X 2 1..n, this sum has no more than 2n non-zero terms, whatever are m, l. However, in the following section, where we are after clustering with an unknown number of clusters k, and thus after controlled rates of convergence, the sizes of the sets Bm,l,n will appear in the bounds."}, {"heading": "3.2 Unknown number of clusters", "text": "So far we have shown that when the number of clusters is known in advance, consistent clustering is possible under the only assumption that the joint distribution of the samples is stationary ergodic. However, under this assumption, in general, consistent clustering with unknown number of clusters is impossible. Indeed, as was shown in Ryabko (2010b), when we have only two binary-valued samples, generated independently by two stationary ergodic distributions, it is impossible to decide whether they have been generated by the same or by different distributions, even in the sense of weak asymptotic consistency (this holds even if the distributions come from a smaller class: the set of all B-processes). Therefore, if the number of clusters is unknown, we have to settle for less, which means that we have to make stronger assumptions on the data. What we need is known rates of convergence of frequencies to their expectations. Such rates are provided by assumptions on the mixing rates of the distribution generating the data. Here we will show that under rather mild as-\nsumptions on the mixing rates (and, again, without any modeling assumptions or assumptions of independence), consistent clustering is possible when the number of clusters is unknown. In this section we assume that all the samples are [0, 1]-valued (that is, Xji \u2208 [0, 1]); extension to arbitrary bounded (multidimensional) ranges is straightforward. Next we introduce mixing coefficients, mainly following Bosq (1996) in formulations. Informally, mixing coefficients of a stochastic process measure how fast the process forgets about its past. Any one-way infinite stationary process X1, X2, . . . can be extended backwards to make a two-way infinite process . . . , X\u22121, X0, X1, . . . with the same distribution. In the definition below we assume such an extension. Define the \u03b1 mixing coefficients as\n\u03b1(n) = sup A\u2208\u03c3(...,X\u22121,X0),B\u2208\u03c3(Xn,Xn+1,... ))\n|P (A \u2229B)\u2212 P (A)P (B)|, (4)\nwhere \u03c3(..) stays for the sigma-algebra generated by random variables in brackets. These coefficients are non-increasing. A process is called strongly \u03b1-mixing if \u03b1(n) \u2192 0. Many important classes of processes satisfy the mixing conditions. For example, if a process is a stationary irreducible aperiodic Hidden Markov process, then it is \u03b1-mixing. If the underlying Markov chain is finite-state, then the coefficients decrease exponentially fast. Other probabilistic assumptions can be used to obtain bounds on the mixing coefficients, see e.g. Bradley (2005) and references therein. Algorithm 2 is very simple. Its inputs are: samples x1, . . . , xN ; the threshold level \u03b4 \u2208 (0, 1), the parameters m, l \u2208 N, Bm,l,n. The algorithm assigns to the same cluster all samples which are at most \u03b4-far from each other, as measured by d\u030c(\u00b7, \u00b7). The estimate d\u030c(\u00b7, \u00b7) can be calculated in the same way as\nd\u0302(\u00b7, \u00b7) (see Proposition 1 and its proof). We do not give a pseudo code implementation of this algorithm, since it\u2019s rather obvious. The idea is that the threshold level \u03b4 is selected according to the minimal length of a sample and the (known bounds on) mixing rates of the process \u03c1 generating the samples (see Theorem 2). The next theorem shows that, if the joint distribu-\ntion of the samples satisfies \u03b1(n) \u2264 \u03b1n \u2192 0, where \u03b1n are known, then one can select (based on \u03b1n only) the parameters of Algorithm 2 in such a way that it is weakly asymptotically consistent. Moreover, a bound on the probability of error before asymptotic is provided.\nTheorem 2 (Algorithm 2 is consistent, unknown k). Fix sequences \u03b1n \u2208 (0, 1), mn, ln, bn \u2208 N, and let Bm,l,n \u2282 Bm,l be an increasing sequence of finite sets, for each m, l \u2208 N. Set bn := maxl\u2264ln,m\u2264mn |B\nm,l,n|. Let also \u03b4n \u2208 (0, 1). Let N \u2208 N and suppose that the samples x1, . . . ,xN are generated in such a way that the (unknown) joint distribution \u03c1 is stationary ergodic, and satisfies \u03b1n(\u03c1) \u2264 \u03b1n, for all n \u2208 N. Then for every sequence qn \u2208 [0..n/2], Algorithm 2, with the above parameters, satisfies\n\u03c1(T 6= I) \u2264 2N(N +1)(mnlnbn\u03b3n(\u03b4n)+ \u03b3n(\u03b5\u03c1)) (5)\nwhere\n\u03b3(\u03b4) = (2e\u2212qn\u03b4 2/32 + 11(1 + 4/\u03b4)1/2qn\u03b1(n\u22122mn)/2qn),"}, {"heading": "T is the partition output by the algorithm, I is the target clustering, \u03b5\u03c1 is a constant that depends only", "text": "on \u03c1, and n = mini=1..N ni. In particular, if \u03b1n = o(1), then, selecting the parameters in such a way that \u03b4n = o(1), qn,mn, ln, bn = o(n), qn,mn, ln \u2192 \u221e, \u222ak\u2208NB\nm,l,k = Bm,l, bm,ln \u2192 \u221e, for all m, l \u2208 N, and, finally,\nmnlnbn(e \u2212qn\u03b4 2 n + \u03b4\u22121/2n qn\u03b1(n\u22122mn)/2qn) = o(1),\nas is always possible, Algorithm 2 is weakly asymptotically consistent (with the number of clusters k unknown). The computational complexity of Algorithm 2 is O(N2mnmax lnmaxbnmax), and is bounded by O(N2n2max log s \u22121), where nmax and log s \u22121 min are defined as in Theorem 1.\nProof. We use the following bound from Bosq (1996): for any zero-mean random process Y1, Y2, . . . , every n \u2208 N and every q \u2208 [1..n/2] we have\nP\n(\n|\nn \u2211\ni=1\nYi| > n\u03b5\n)\n\u2264 4 exp(\u2212q\u03b52/8) + 22(1 + 4/\u03b5)1/2q\u03b1(n/2q).\nFor every j = 1..N , every m < n, l \u2208 N, and B \u2208 Bm,l, define the processes Y j1 , Y j 2 , . . . , where\nY jt := I(Xjt ,...,X j t+m\u22121)\u2208B \u2212 \u03c1(Xj1..m \u2208 B).\nIt is easy to see that \u03b1-mixing coefficients for this process satisfy \u03b1(n) \u2264 \u03b1n\u22122m. Thus,\n\u03c1(|\u03bd(Xj1..nj , B)\u2212\u03c1(X j 1..m \u2208 B)| > \u03b5/2) \u2264 \u03b3n(\u03b5) (6)\nThen for every i, j \u2208 [1..N ] such that I(xi) = I(xj) (that is, xi and xj are in the same cluster) we have\n\u03c1(|\u03bd(X i1..ni , B)\u2212 \u03bd(X j 1..nj , B)| > \u03b5) \u2264 2\u03b3n(\u03b5).\nUsing the union bound, summing over m, l, and B, we obtain\n\u03c1(d\u030c(xi,xj) > \u03b5) \u2264 2mnlnbn\u03b3n(\u03b5). (7)\nNext, let i, j be such that I(xi) 6= I(xj). Then, for some mi,j , li,j \u2208 N there is Bi,j \u2208 B mi,j ,li,j such that |\u03c1(X i1..|Bi,j | \u2208 Bi,j) \u2212 \u03c1(X j 1..|Bi,j | \u2208 Bi,j)| > 2\u03c4i,j for some \u03c4i,j > 0. Then for every \u03b5 < \u03c4i,j/2 we have\n\u03c1(|\u03bd(X i1..ni , Bi,j)\u2212 \u03bd(X j 1..nj , Bi,j)| < \u03b5) \u2264\n\u03c1(|\u03bd(X i1..ni , Bi,j)\u2212 \u03c1(X i 1..|B| \u2208 Bi,j)| > \u03c4i,j)\n+ \u03c1(|\u03bd(Xj1..nj , Bi,j)\u2212 \u03c1(X j 1..|Bi,j | \u2208 Bi,j)| > \u03c4i,j)\n\u2264 2\u03b3n(\u03c4i,j). (8)\nMoreover, for \u03b5 < wmi,jwli,j \u03c4i,j/2\n\u03c1(d\u030c(xi,xj) > \u03b5) \u2264 2\u03b3n(wmi,jwli,j \u03c4i,j). (9)\nDefine \u03b5\u03c1 := mini,j=1..N :I(xi) 6=I(xj) wmi,jwli,j \u03c4i,j/2. Clearly, from this and (8), for every \u03b5 < 2\u03b5\u03c1 we obtain\n\u03c1(d\u030c(xi,xj) > \u03b5) \u2264 2\u03b3n(\u03b5\u03c1). (10)\nIf, for every pair i, j of samples, d\u030c(xi,xj) < \u03b4n if and only if I(xi) = I(xj), then Algorithm 2 gives a correct answer. Therefore, taking the bounds (7) and (10) together for each of the N(N + 1)/2 pairs of samples, we obtain (5). The complexity statement can be established analogously to that in Theorem 1.\nWhile Theorem 2 shows that \u03b1-mixing with a known bound on the coefficients is sufficient to achieve asymptotic consistency, the bound (5) on the probability of error includes as multiplicative terms all the parameters mn, ln and bn of the algorithm, which can make it large for practically useful choices of the parameters. The multiplicative factors are due to the fact that we take a bound on the divergence of each individual frequency of each cell of each partition from its expectation, and then take a union bound over all of these. To obtain a more realistic performance guarantee, we would like to have a bound on the divergence of all the frequencies of all cells of a given partition from their expectations. Such uniform divergence estimates are possible under stronger assumptions; namely, they can be established under some assumptions on \u03b2-mixing coefficients, which are defined as follows\n\u03b2(n) = E sup B\u2208\u03c3(Xn,... )) |P (B) \u2212 P (B|\u03c3(. . . , X0))|.\nThese coefficients satisfy 2\u03b1(n) \u2264 \u03b2(n) (see e.g. Bosq (1996)), so assumptions on the speed of decrease of \u03b2-coefficients are stronger. Using the uniform bounds given in Karandikara & Vidyasagar (2002), one can obtain a statement similarto that in Theorem 2, with \u03b1-mixing replaced by \u03b2-mixing, and without the multiplicative factor bn."}, {"heading": "4 Conclusion", "text": "We have proposed a framework for defining consistency of clustering algorithms, when the data comes as a set of samples drawn from stationary processes. The main advantage of this framework is its generality: no assumptions have to be made on the distribution of the data, beyond stationarity and ergodicity. The proposed notion of consistency is so simple and natural, that it may be suggested to be used as a basic sanity-check for all clustering algorithms that are used on sequence-like data. For example, it is easy to see that the k-means algorithm will be consistent with some initializations (e.g. with the one used in Algorithm 1) but not with others (e.g. not with the random one).\nWhile the algorithms that we presented to demonstrate the existence of consistent clustering methods are computationally efficient and easy to implement, the main value of the established results is theoretical. As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009). Another direction for future research concerns optimal bounds on the speed of convergence: while we show that such bounds can be obtained (of course, only in the case of known mixing rates), finding practical and tight bounds, for different notions of mixing rates, remains open.\nFinally, here we have only considered the setting in which the number N of samples is fixed, while the asymptotic is with respect to the lengths of the samples. For on-line clustering problems, it would be interesting to consider the formulation where both N and the lengths of the samples grow."}], "references": [{"title": "A bit level representation for time series data mining with shape based similarity", "author": ["A. Bagnall", "C. Ratanamahatana", "E. Keogh", "S. Lonardi", "G. Janacek"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Bagnall et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bagnall et al\\.", "year": 2006}, {"title": "Nonparametric Statistics for Stochastic Processes", "author": ["D. Bosq"], "venue": "Estimation and Prediction. Springer,", "citeRegEx": "Bosq,? \\Q1996\\E", "shortCiteRegEx": "Bosq", "year": 1996}, {"title": "Basic properties of strong mixing conditions. A survey and some open questions", "author": ["R.C. Bradley"], "venue": "Probability Surveys,", "citeRegEx": "Bradley,? \\Q2005\\E", "shortCiteRegEx": "Bradley", "year": 2005}, {"title": "Clustering by compression", "author": ["R. Cilibrasi", "P.M.B. Vitanyi"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Cilibrasi and Vitanyi,? \\Q2005\\E", "shortCiteRegEx": "Cilibrasi and Vitanyi", "year": 2005}, {"title": "Probability, Random Processes, and Ergodic Properties", "author": ["R. Gray"], "venue": null, "citeRegEx": "Gray,? \\Q1988\\E", "shortCiteRegEx": "Gray", "year": 1988}, {"title": "Rates of uniform convergence of empirical means with mixing processes", "author": ["R.L. Karandikara", "M. Vidyasagar"], "venue": "Stat.&Prob. Lett.,", "citeRegEx": "Karandikara and Vidyasagar,? \\Q2002\\E", "shortCiteRegEx": "Karandikara and Vidyasagar", "year": 2002}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "In NIPS :446\u2013453,", "citeRegEx": "Kleinberg,? \\Q2002\\E", "shortCiteRegEx": "Kleinberg", "year": 2002}, {"title": "Testing Statistical Hypotheses, 2nd ed", "author": ["E. Lehmann"], "venue": null, "citeRegEx": "Lehmann,? \\Q1986\\E", "shortCiteRegEx": "Lehmann", "year": 1986}, {"title": "The planar k-means problem is NP-hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "In WALCOM : 274\u2013285,", "citeRegEx": "Mahajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mahajan et al\\.", "year": 2009}, {"title": "Compression-based methods for nonparametric prediction and estimation of some characteristics of time series", "author": ["B. Ryabko"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Ryabko,? \\Q2009\\E", "shortCiteRegEx": "Ryabko", "year": 2009}, {"title": "Universal codes as a basis for time series testing", "author": ["B. Ryabko", "J. Astola"], "venue": "Stat. Methodology,", "citeRegEx": "Ryabko and Astola,? \\Q2006\\E", "shortCiteRegEx": "Ryabko and Astola", "year": 2006}, {"title": "Testing composite hypotheses about discrete-valued stationary processes", "author": ["D. Ryabko"], "venue": "In ITW : 291\u2013", "citeRegEx": "Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko", "year": 2010}, {"title": "Discrimination between B-processes is impossible", "author": ["D. Ryabko"], "venue": "J. Theor. Prob.,", "citeRegEx": "Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko", "year": 2010}, {"title": "Nonparametric statistical inference for ergodic processes", "author": ["D. Ryabko", "B. Ryabko"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Ryabko and Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko and Ryabko", "year": 2010}, {"title": "Clustering sequences with hidden markov models", "author": ["P. Smyth"], "venue": "In NIPS : 648\u2013654", "citeRegEx": "Smyth,? \\Q1997\\E", "shortCiteRegEx": "Smyth", "year": 1997}, {"title": "A uniqueness theorem for clustering", "author": ["R. Zadeh", "S. Ben-David"], "venue": "In UAI,", "citeRegEx": "Zadeh and Ben.David,? \\Q2009\\E", "shortCiteRegEx": "Zadeh and Ben.David", "year": 2009}, {"title": "A unified framework for model-based clustering", "author": ["S. Zhong", "J. Ghosh"], "venue": "JMLR, 4:1001\u20131037,", "citeRegEx": "Zhong and Ghosh,? \\Q2003\\E", "shortCiteRegEx": "Zhong and Ghosh", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009).", "startOffset": 106, "endOffset": 123}, {"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009). What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons.", "startOffset": 106, "endOffset": 149}, {"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009). What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons. Indeed, in this case finding k centres (points which minimize the cumulative distance from each point in the sample to one of the centres) seems to be a natural goal, but this problem is NPhard Mahajan et al. (2009).", "startOffset": 106, "endOffset": 583}, {"referenceID": 10, "context": "Perhaps the most close approach is mixture models Smyth (1997); Zhong & Ghosh (2003): it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability).", "startOffset": 50, "endOffset": 63}, {"referenceID": 10, "context": "Perhaps the most close approach is mixture models Smyth (1997); Zhong & Ghosh (2003): it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability).", "startOffset": 50, "endOffset": 85}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions.", "startOffset": 5, "endOffset": 20}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions.", "startOffset": 5, "endOffset": 62}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in Ryabko (2010b) it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense.", "startOffset": 5, "endOffset": 307}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in Ryabko (2010b) it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense. However, a solution to the threesample problem, for (real-valued) stationary ergodic processes was given in Ryabko & Ryabko (2010). In this work we demonstrate that, if the number k of clusters is known, then there is an asymptotically consistent clustering algorithm, under the only assumption that the joint distribution of data is stationary ergodic.", "startOffset": 5, "endOffset": 603}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a).", "startOffset": 109, "endOffset": 132}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated.", "startOffset": 109, "endOffset": 148}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument. Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these algorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in Ryabko & Astola (2006) for the related problems of hypotheses testing, leading both to theoretical and practical results.", "startOffset": 109, "endOffset": 1923}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument. Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these algorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in Ryabko & Astola (2006) for the related problems of hypotheses testing, leading both to theoretical and practical results. As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in Cilibrasi & Vitanyi (2005), and (in a different way) in Bagnall et al.", "startOffset": 109, "endOffset": 2165}, {"referenceID": 0, "context": "As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in Cilibrasi & Vitanyi (2005), and (in a different way) in Bagnall et al. (2006). Combining our consistency framework with these compression-based methods is a promising direction for further research.", "startOffset": 172, "endOffset": 194}, {"referenceID": 4, "context": "Gray (1988))", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "We refer to Gray (1988) for more information on this metric and its properties.", "startOffset": 12, "endOffset": 24}, {"referenceID": 8, "context": "Indeed, as was shown in Ryabko (2010b), when we have only two binary-valued samples, generated independently by two stationary ergodic distributions, it is impossible to decide whether they have been generated by the same or by different distributions, even in the sense of weak asymptotic consistency (this holds even if the distributions come from a smaller class: the set of all B-processes).", "startOffset": 24, "endOffset": 39}, {"referenceID": 1, "context": "Next we introduce mixing coefficients, mainly following Bosq (1996) in formulations.", "startOffset": 56, "endOffset": 68}, {"referenceID": 2, "context": "Bradley (2005) and references therein.", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "We use the following bound from Bosq (1996): for any zero-mean random process Y1, Y2, .", "startOffset": 32, "endOffset": 44}, {"referenceID": 1, "context": "Bosq (1996)), so assumptions on the speed of decrease of \u03b2-coefficients are stronger.", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "Bosq (1996)), so assumptions on the speed of decrease of \u03b2-coefficients are stronger. Using the uniform bounds given in Karandikara & Vidyasagar (2002), one can obtain a statement similarto that in Theorem 2, with \u03b1-mixing replaced by \u03b2-mixing, and without the multiplicative factor bn.", "startOffset": 0, "endOffset": 152}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009).", "startOffset": 219, "endOffset": 242}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009).", "startOffset": 219, "endOffset": 270}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009). Another direction for future research concerns optimal bounds on the speed of convergence: while we show that such bounds can be obtained (of course, only in the case of known mixing rates), finding practical and tight bounds, for different notions of mixing rates, remains open.", "startOffset": 219, "endOffset": 285}], "year": 2010, "abstractText": "The problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent.", "creator": "LaTeX with hyperref package"}}}