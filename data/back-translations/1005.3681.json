{"id": "1005.3681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2010", "title": "Learning Kernel-Based Halfspaces with the Zero-One Loss", "abstract": "We describe and analyze a new algorithm for agnostic learning of kernel-based semispaces in relation to the\\ emph {zero-one} loss function. Unlike most earlier formulations, which relied on convex loss functions as substitutes (e.g. hinge loss in SVM and log loss in logistic regression), we offer finite time / example guarantees in relation to the more natural zero-one loss function. The proposed algorithm can learn kernel-based semispaces in the worst case $\\ poly (\\ exp (L\\ log (L /\\ epsilon))) $, for $\\ emph {any} $distribution, where $L $is a Lipschitz constant (which can be considered the equivalent of the margin), and the learned classifier is at most $\\ epsilon $inferior to the optimal semispace. We also demonstrate a hardness result that shows that polynographic semispaces cannot be learned under a certain kernel assumption.", "histories": [["v1", "Thu, 20 May 2010 12:39:56 GMT  (25kb)", "https://arxiv.org/abs/1005.3681v1", "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010)"], ["v2", "Sun, 1 Aug 2010 08:31:29 GMT  (25kb)", "http://arxiv.org/abs/1005.3681v2", "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010). Compared to the previous arXiv version, this version contains some small corrections in the proof of Lemma 3 and in appendix A"]], "COMMENTS": "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "ohad shamir", "karthik sridharan"], "accepted": false, "id": "1005.3681"}, "pdf": {"name": "1005.3681.pdf", "metadata": {"source": "CRF", "title": "Learning Kernel-Based Halfspaces with the Zero-One Loss", "authors": ["Shai Shalev-Shwartz", "Karthik Sridharan"], "emails": ["shais@cs.huji.ac.il", "ohadsh@cs.huji.ac.il", "karthik@tti-c.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n36 81\nv2 [\ncs .L\nG ]\n1 A\nug 2\n01 0"}, {"heading": "1 Introduction", "text": "A highly important hypothesis class in machine learning theory and applications is that of halfspaces in a Reproducing Kernel Hilbert Space (RKHS). Choosing a halfspace based on empirical data is often performed using Support Vector Machines (SVMs) [26]. SVMs replace the more natural 0- 1 loss function with a convex surrogate \u2013 the hinge-loss. By doing so, we can rely on convex optimization tools. However, there are no guarantees on how well the hinge-loss approximates the 0- 1 loss function. There do exist some recent results on the asymptotic relationship between surrogate convex loss functions and the 0-1 loss function [28, 4], but these do not come with finite-sample or finite-time guarantees. In this paper, we tackle the task of learning kernel-based halfspaces with respect to the non-convex 0-1 loss function. Our goal is to derive learning algorithms and to analyze them in the finite-sample finite-time setting.\nFollowing the standard statistical learning framework, we assume that there is an unknown distribution, D, over the set of labeled examples, X \u00d7 {0, 1}, and our primary goal is to find a classifier, h : X \u2192 {0, 1}, with low generalization error,\nerrD(h) def = E (x,y)\u223cD [|h(x) \u2212 y|] . (1)\nThe learning algorithm is allowed to sample a training set of labeled examples, (x1, y1), . . . , (xm, ym), where each example is sampled i.i.d. from D, and it returns a classifier. Following the agnostic PAC learning framework [16], we say that an algorithm (\u01eb, \u03b4)-learns a concept class H of classifiers using m examples, if with probability of at least 1\u2212 \u03b4 over a random choice of m examples the algorithm returns a classifier h\u0302 that satisfies\nerrD(h\u0302) \u2264 inf h\u2208H errD(h) + \u01eb . (2)\nWe note that h\u0302 does not necessarily belong to H . Namely, we are concerned with improper learning, which is as useful as proper learning for the purpose of deriving good classifiers. A common learning paradigm is the Empirical Risk Minimization (ERM) rule, which returns a classifier that minimizes the average error over the training set,\nh\u0302 \u2208 argmin h\u2208H\n1\nm\nm \u2211\ni=1\n|h(xi)\u2212 yi| .\nThe class of (origin centered) halfspaces is defined as follows. Let X be a compact subset of a RKHS, which w.l.o.g. will be taken to be the unit ball around the origin. Let \u03c60\u22121 : R \u2192 R be the function \u03c60\u22121(a) = 1(a \u2265 0) = 12 (sgn(a) + 1). The class of halfspaces is the set of classifiers\nH\u03c60\u22121 def = {x 7\u2192 \u03c60\u22121(\u3008w,x\u3009) : w \u2208 X} .\nAlthough we represent the halfspace using w \u2208 X , which is a vector in the RKHS whose dimensionality can be infinite, in practice we only need a function that implements inner products in the RKHS (a.k.a. a kernel function), and one can define w as the coefficients of a linear combination of examples in our training set. To simplify the notation throughout the paper, we represent w simply as a vector in the RKHS.\nIt is well known that if the dimensionality of X is n, then the VC dimension of H\u03c60\u22121 equals n. This implies that the number of training examples required to obtain a guarantee of the form given in Equation (2) for the class of halfspaces scales at least linearly with the dimension n [26]. Since kernel-based learning algorithms allow X to be an infinite dimensional inner product space, we must use a different class in order to obtain a guarantee of the form given in Equation (2).\nOne way to define a slightly different concept class is to approximate the non-continuous function, \u03c60\u22121, with a Lipschitz continuous function, \u03c6 : R \u2192 [0, 1], which is often called a transfer function. For example, we can use a sigmoidal transfer function\n\u03c6sig(a) def =\n1\n1 + exp(\u22124La) , (3)\nwhich is a L-Lipschitz function. Other L-Lipschitz transfer functions are the erf function and the piece-wise linear function:\n\u03c6erf(a) def = 12 (\n1 + erf (\u221a \u03c0 L a )) , \u03c6pw(a) def = max { min { 1 2 + La , 1 } 0 }\n(4)\nAn illustration of these transfer functions is given in Figure 1. Analogously to the definition of H\u03c60\u22121 , for a general transfer function \u03c6 we define H\u03c6 to be the set of predictors x 7\u2192 \u03c6(\u3008w,x\u3009). Since now the range of \u03c6 is not {0, 1} but rather the entire interval [0, 1], we interpret \u03c6(\u3008w,x\u3009) as the probability to output the label 1. The definition of errD(h) remains1 as in Equation (1).\nThe advantage of using a Lipschitz transfer function can be seen via Rademacher generalization bounds [3]. In fact, a simple corollary of the contraction lemma implies the following:\nTheorem 1 Let \u01eb, \u03b4 \u2208 (0, 1) and let \u03c6 be an L-Lipschitz transfer function. Let m be an integer satisfying\nm \u2265 ( 2L+ 3 \u221a 2 ln(8/\u03b4)\n\u01eb\n)2\n.\nThen, for any distribution D over X \u00d7 {0, 1}, the ERM algorithm (\u01eb, \u03b4)-learns the concept class H\u03c6 using m examples.\n1 Note that in this case errD(h) can be interpreted as P(x,y)\u223cD,b\u223c\u03c6(\u3008w,x\u3009)[y 6= b].\nThe above theorem tells us that the sample complexity of learning H\u03c6 is \u2126\u0303(L 2/\u01eb2). Crucially, the sample complexity does not depend on the dimensionality of X , but only on the Lipschitz constant of the transfer function. This allows us to learn with kernels, when the dimensionality of X can even be infinite. A related analysis compares the error rate of a halfspace w to the number of margin mistakes w makes on the training set - see Section 4.1 for a comparison.\nFrom the computational complexity point of view, the result given in Theorem 1 is problematic, since the ERM algorithm should solve the non-convex optimization problem\nargmin w:\u2016w\u2016\u22641\n1\nm\nm \u2211\ni=1\n|\u03c6(\u3008w,xi\u3009)\u2212 yi| . (5)\nSolving this problem in polynomial time is hard under reasonable assumptions (see Section 3 in which we present a formal hardness result). Adapting a technique due to [6] we show in Appendix A that it is possible to find an \u01eb-accurate solution to Equation (5) (where the transfer function is \u03c6pw) in time poly ( exp ( L2\n\u01eb2 log( L \u01eb ) )) . The main contribution of this paper is the derivation and analysis of a\nmore simple learning algorithm that (\u01eb, \u03b4)-learns the class Hsig using time and sample complexity of at most poly ( exp (\nL log(L\u01eb ) ))\n. That is, the runtime of our algorithm is exponentially smaller than the runtime required to solve the ERM problem using the technique described in [6]. Moreover, the algorithm of [6] performs an exhaustive search over all (L/\u01eb)2 subsets of the m examples in the training set, and therefore its runtime is always order of mL 2/\u01eb2 . In contrast, our algorithm\u2019s runtime depends on a parameter B, which is bounded by exp(L) only under a worst-case assumption. Depending on the underlying distribution, B can be much smaller than the worst-case bound. In practice, we will cross-validate for B, and therefore the worst-case bound will often be pessimistic.\nThe rest of the paper is organized as follows. In Section 2 we describe our main results. Next, in Section 3 we provide a hardness result, showing that it is not likely that there exists an algorithm that learns Hsig or Hpw in time polynomial in L. We outline additional related work in Section 4. In particular, the relation between our approach and margin-based analysis is described in Section 4.1, and the relation to approaches utilizing a distributional assumption is discussed in Section 4.2. We wrap up with a discussion in Section 5."}, {"heading": "2 Main Results", "text": "In this section we present our main result. Recall that we would like to derive an algorithm which learns the class Hsig. However, the ERM optimization problem associated with Hsig is non-convex. The main idea behind our construction is to learn a larger hypothesis class, denoted HB, which approximately contains Hsig, and for which the ERM optimization problem becomes convex. The price we need to pay is that from the statistical point of view, it is more difficult to learn the class HB than the class Hsig, therefore the sample complexity increases.\nThe class HB we use is a class of linear predictors in some other RKHS. The kernel function that implements the inner product in the newly constructed RKHS is\nK(x,x\u2032) def =\n1\n1\u2212 \u03bd\u3008x,x\u2032\u3009 , (6)\nwhere \u03bd \u2208 (0, 1) is a parameter and \u3008x,x\u2032\u3009 is the inner product in the original RKHS. As mentioned previously, \u3008x,x\u2032\u3009 is usually implemented by some kernel function K \u2032(z, z\u2032), where z and z\u2032 are the pre-images of x and x\u2032 with respect to the feature mapping induced by K \u2032. Therefore, the kernel in Equation (6) is simply a composition with K \u2032, i.e. K(z, z\u2032) = 1/(1\u2212 \u03bdK \u2032(z, z\u2032)).\nTo simplify the presentation we will set \u03bd = 1/2, although in practice other choices might be more effective. It is easy to verify that K is a valid positive definite kernel function (see for example [21, 10]). Therefore, there exists some mapping \u03c8 : X \u2192 V, where V is an RKHS with \u3008\u03c8(x), \u03c8(x\u2032)\u3009 = K(x,x\u2032). The class HB is defined to be:\nHB def = {x 7\u2192 \u3008v, \u03c8(x)\u3009 : v \u2208 V, \u2016v\u20162 \u2264 B} . (7)\nThe main result we prove in this section is the following:\nTheorem 2 Let \u01eb, \u03b4 \u2208 (0, 1) and let L \u2265 3. Let B = 2L4 + exp ( 7L log ( 2L \u01eb ) + 3 ) and let m be a sample size that satisfies m \u2265 8B\u01eb2 ( 2 + 9 \u221a ln(8/\u03b4) )2\n. Then, for any distribution D, with probability of at least 1\u2212 \u03b4, any ERM predictor h\u0302 \u2208 HB with respect to HB satisfies\nerrD(h\u0302) \u2264 min h\u2208Hsig errD(hsig) + \u01eb .\nWe note that the bound on B is far from being the tightest possible in terms of constants and second-order terms. Also, the assumption of L \u2265 3 is rather arbitrary, and is meant to simplify the presentation of the bound.\nTo prove this theorem, we start with analyzing the time and sample complexity of learning HB. The sample complexity analysis follows directly from a Rademacher generalization bound [3]. In particular, the following theorem tells us that the sample complexity of learning HB with the ERM rule is order of B/\u01eb2 examples.\nTheorem 3 Let \u01eb, \u03b4 \u2208 (0, 1), let B \u2265 1, and let m be a sample size that satisfies\nm \u2265 2B \u01eb2 ( 2 + 9 \u221a ln(8/\u03b4) )2 .\nThen, for any distribution D, the ERM algorithm (\u01eb, \u03b4)-learns HB.\nProof Since K(x,x) \u2264 2, the Rademacher complexity of HB is bounded by \u221a\n2B/m (see also [14]). Additionally, using Cauchy-Schwartz inequality we have that the loss is bounded, |\u3008v, \u03c8(x)\u3009 \u2212 y| \u2264\u221a 2B + 1. The result now follows directly from [3, 14].\nNext, we show that the ERM problem with respect to HB can be solved in time poly(m). The ERM problem associated with HB is\nmin v:\u2016v\u20162\u2264B\n1\nm\nm \u2211\ni=1\n|\u3008v, \u03c8(xi)\u3009 \u2212 yi| .\nSince the objective function is defined only via inner products with \u03c8(xi), and the constraint on v is defined by the \u21132-norm, it follows by the Representer theorem [27] that there is an optimal solution v\u22c6 that can be written as v\u22c6 =\n\u2211m i=1 \u03b1i\u03c8(xi). Therefore, instead of optimizing over v, we\ncan optimize over the set of weights \u03b11, . . . , \u03b1m by solving the equivalent optimization problem\nmin \u03b11,...,\u03b1m\n1\nm\nm \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\nj=1\n\u03b1jK(xj ,xi)\u2212 yi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 s.t. m \u2211\ni,j=1\n\u03b1i\u03b1jK(xi,xj) \u2264 B .\nThis is a convex optimization problem in Rm and therefore can be solved in time poly(m) using standard optimization tools.2 We therefore obtain:\nCorollary 1 Let \u01eb, \u03b4 \u2208 (0, 1) and let B \u2265 1. Then, for any distribution D, it is possible to (\u01eb, \u03b4)learn HB in sample and time complexity of poly ( B \u01eb log(1/\u03b4) ) .\nIt is left to understand why the class HB approximately contains the class Hsig. Recall that for any transfer function, \u03c6, we define the class H\u03c6 to be all the predictors of the form x 7\u2192 \u03c6(\u3008w,x\u3009). The first step is to show that HB contains the union of H\u03c6 over all polynomial transfer functions that satisfy a certain boundedness condition on their coefficients.\nLemma 1 Let PB be the following set of polynomials (possibly with infinite degree)\nPB def =\n\n\n\np(a) =\n\u221e \u2211\nj=0\n\u03b2j a j :\n\u221e \u2211\nj=0\n\u03b22j 2 j \u2264 B\n\n\n\n. (8)\nThen, \u22c3\np\u2208PB Hp \u2282 HB .\nProof To simplify the proof, we first assume that X is simply the unit ball in Rn, for an arbitrarily large but finite n. Consider the mapping \u03c8 : X \u2192 RN defined as follows: for any x \u2208 X , we let \u03c8(x) be an infinite vector, indexed by k1 . . . , kj for all (k1, . . . , kj) \u2208 {1, . . . , n}j and j = 0 . . .\u221e, where\n2 In fact, using stochastic gradient descent, we can (\u01eb, \u03b4)-learn HB in time O(m 2), where m is as defined\nin Theorem 3 \u2014See for example [8, 22].\nthe entry at index k1 . . . , kj equals 2 \u2212j/2xk1 \u00b7xk2 \u00b7 \u00b7 \u00b7xkj . The inner-product between \u03c8(x) and \u03c8(x\u2032) for any x,x\u2032 \u2208 X can be calculated as follows,\n\u3008\u03c8(x), \u03c8(x\u2032)\u3009 = \u221e \u2211\nj=0\n\u2211\n(k1,...,kj)\u2208{1,...,n}j 2\u2212jxk1x \u2032 k1 \u00b7 \u00b7 \u00b7xkjx \u2032 kj =\n\u221e \u2211\nj=0\n2\u2212j(\u3008x,x\u2032\u3009)j = 1 1\u2212 12 \u3008x,x\u2032\u3009 .\nThis is exactly the kernel function defined in Equation (6) (recall that we set \u03bd = 1/2) and therefore \u03c8 maps to the RKHS defined by K. Consider any polynomial p(a) =\n\u2211\u221e j=0 \u03b2ja j in PB, and any\nw \u2208 X . Let vw be an element in RN explicitly defined as being equal to \u03b2j2j/2wk1 \u00b7 \u00b7 \u00b7wkj at index k1, . . . , kj (for all k1, . . . , kj \u2208 {1, . . . , n}j, j = 0 . . .\u221e). By definition of \u03c8 and vw, we have that\n\u3008vw, \u03c8(x)\u3009 = \u221e \u2211\nj=0\n\u2211\nk1,...,kj\n2\u2212j/2\u03b2j2 j/2wk1 \u00b7 \u00b7 \u00b7wkjxk1 \u00b7 \u00b7 \u00b7 \u00b7xkj =\n\u221e \u2211\nj=0\n\u03b2j(\u3008w,x\u3009)j = p(\u3008w,x\u3009) .\nIn addition,\n\u2016vw\u20162 = \u221e \u2211\nj=0\n\u2211\nk1,...,kj\n\u03b22j 2 jw2k1 \u00b7 \u00b7 \u00b7w 2 kj =\n\u221e \u2211\nj=0\n\u03b22j 2 j \u2211\nk1\nw2k1\n\u2211\nk2\nw2k2 \u00b7 \u00b7 \u00b7 \u2211\nkj\nw2kj = \u221e \u2211\nj=0\n\u03b22j 2 j ( \u2016w\u20162 )j \u2264 B.\nThus, the predictor x 7\u2192 \u3008vw, \u03c8(x)\u3009 belongs to HB and is the same as the predictor x 7\u2192 p(\u3008w,x\u3009). This proves that Hp \u2282 HB for all p \u2208 PB as required. Finally, if X is an infinite dimensional RKHS, the only technicality is that in order to represent x as a (possibly infinite) vector, we need to show that our RKHS has a countable basis. This holds since the inner product \u3008x,x\u2032\u3009 over X is continuous and bounded (see [1]).\nFinally, the following lemma states that with a sufficiently large B, there exists a polynomial in PB which approximately equals to \u03c6sig. This implies that HB approximately contains Hsig.\nLemma 2 Let \u03c6sig be as defined in Equation (3), where for simplicity we assume L \u2265 3. For any \u01eb > 0, let\nB = 2L4 + exp ( 7L log ( 2L \u01eb ) + 3 ) .\nThen there exists p \u2208 PB such that \u2200x,w \u2208 X , |p(\u3008w,x\u3009) \u2212 \u03c6sig(\u3008w,x\u3009)| \u2264 \u01eb .\nThe proof of the lemma is based on a Chebyshev approximation technique and is given in Appendix B. Since the proof is rather involved, we also present a similar lemma, whose proof is simpler, for the \u03c6erf transfer function (see Appendix C). It is interesting to note that \u03c6erf actually belongs to PB for a sufficiently large B, since it can be defined via its infinite-degree Taylor expansion. However, the bound for \u03c6erf depends on exp(L\n2), rather than exp(L) for the sigmoid transfer function \u03c6sig.\nFinally, Theorem 2 is obtained as follows: Combining Theorem 3 and Lemma 1 we get that with probability of at least 1\u2212 \u03b4,\nerrD(h\u0302) \u2264 min h\u2208HB errD(h) + \u01eb/2 \u2264 min p\u2208PB min h\u2208Hp errD(h) + \u01eb/2 . (9)\nFrom Lemma 2 we obtain that for any w \u2208 X , if h(x) = \u03c6sig(\u3008w,x\u3009) then there exists a polynomial p0 \u2208 PB such that if h\u2032(x) = p0(\u3008w,x\u3009) then errD(h\u2032) \u2264 errD(h) + \u01eb/2. Since it holds for all w, we get that\nmin p\u2208PB min h\u2208Hp errD(h) \u2264 min h\u2208Hsig errD(h) + \u01eb/2 .\nCombining this with Equation (9), Theorem 2 follows."}, {"heading": "3 Hardness", "text": "In this section we derive a hardness result for agnostic learning of Hsig or Hpw with respect to the zero-one loss. The hardness result relies on the hardness of standard (non-agnostic)3 PAC learning of intersection of halfspaces given in Klivans and Sherstov [17] (see also similar arguments in [12]). The hardness result is representation-independent \u2014it makes no restrictions on the learning algorithm and in particular also holds for improper learning algorithms. The hardness result is based on the following cryptographic assumption:\n3In the standard PAC model, we assume that some hypothesis in the class has errD(h) = 0, while in the agnostic PAC model, which we study in this paper, errD(h) might be strictly greater than zero for all h \u2208 H . Note that our definition of (\u01eb, \u03b4)-learning in this paper is in the agnostic model.\nAssumption 1 There is no polynomial time solution to the O\u0303(n1.5)-unique-Shortest-Vector-Problem.\nIn a nutshell, given a basis v1, . . . ,vn \u2208 Rn, the O\u0303(n1.5)-unique-Shortest-Vector-Problem consists of finding the shortest nonzero vector in {a1v1 + . . . + anvn : a1, . . . , an \u2208 Z}, even given the information that it is shorter by a factor of at least O\u0303(n1.5) than any other non-parallel vector. This problem is believed to be hard - there are no known sub-exponential algorithms, and it is known to be NP-hard if O\u0303(n1.5) is replaced by a small constant (see [17] for more details).\nWith this assumption, Klivans and Sherstov proved the following:\nTheorem 4 (Theorem 1.2 in Klivans and Sherstov [17]) Let X = {\u00b11}n, let H = {x 7\u2192 \u03c60,1(\u3008w,x\u3009 \u2212 \u03b8 \u2212 1/2) : \u03b8 \u2208 N,w \u2208 Nn, |\u03b8|+ \u2016w\u20161 \u2264 poly(n)} ,\nand let Hk = {x 7\u2192 (h1(x) \u2227 . . . \u2227 hk(x)) : \u2200i, hi \u2208 H}. Then, based on Assumption 1, Hk is not efficiently learnable in the standard PAC model for any k = n\u03c1 where \u03c1 > 0 is a constant.\nThe above theorem implies the following.\nLemma 3 Based on Assumption 1, there is no algorithm that runs in time poly(n, 1/\u01eb, 1/\u03b4) and (\u01eb, \u03b4)-learns the class H defined in Theorem 4.\nProof To prove the lemma we show that if there is a polynomial time algorithm that learns H in the agnostic model, then there exists a weak learning algorithm (with a polynomial edge) that learns Hk in the standard (non-agnostic) PAC model. In the standard PAC model, weak learning implies strong learning [20], hence the existence of a weak learning algorithm that learns Hk will contradict Theorem 4.\nIndeed, let D be any distribution such that there exists h\u22c6 \u2208 Hk with errD(h\u22c6) = 0. Let us rewrite h\u22c6 = h\u22c61 \u2227 . . .\u2227 h\u22c6k where for all i, h\u22c6i \u2208 H . To show that there exists a weak learner, we first show that there exists some h \u2208 H with errD(h) \u2264 1/2\u2212 1/2k2.\nSince for each x if h\u22c6(x) = 0 then there exists j s.t. h\u22c6j (x) = 0, we can use the union bound to get that\n1 = P[\u2203j : h\u22c6j (x) = 0|h\u22c6(x) = 0] \u2264 \u2211\nj\nP[h\u22c6j (x) = 0|h\u22c6(x) = 0] \u2264 kmax j P[h\u22c6j (x) = 0|h\u22c6(x) = 0] .\nSo, for j that maximizes P[h\u22c6j (x) = 0|h\u22c6(x) = 0] we get that P[h\u22c6j (x) = 0|h\u22c6(x) = 0] \u2265 1/k. Therefore,\nerrD(h \u22c6 j ) = P[h \u22c6 j (x) = 1 \u2227 h\u22c6(x) = 0] = P[h\u22c6(x) = 0] P[h\u22c6j (x) = 1|h\u22c6(x) = 0]\n= P[h\u22c6(x) = 0] (1\u2212 P[h\u22c6j (x) = 0|h\u22c6(x) = 0]) \u2264 P[h\u22c6(x) = 0] (1\u2212 1/k) .\nNow, if P[h\u22c6(x) = 0] \u2264 1/2 + 1/k2 then the above gives\nerrD(h \u22c6 j ) \u2264 (1/2 + 1/k2)(1\u2212 1/k) \u2264 1/2\u2212 1/2k2 ,\nwhere the inequality holds for any positive integer k. Otherwise, if P[h\u22c6(x) = 0] > 1/2+ 1/k2, then the constant predictor h(x) = 0 has errD(h) < 1/2\u2212 1/k2. In both cases we have shown that there exists a predictor in H with error of at most 1/2\u2212 1/2k2.\nFinally, if we can agnostically learn H in time poly(n, 1/\u01eb, 1/\u03b4), then we can find h\u2032 with errD(h\u2032) \u2264 minh\u2208H errD(h) + \u01eb \u2264 1/2 \u2212 1/2k2 + \u01eb in time poly(n, 1/\u01eb, 1/\u03b4) (recall that k = n\u03c1 for some \u03c1 > 0). This means that we can have a weak learner that runs in polynomial time, and this concludes our proof.\nLet h be a hypothesis in the class H defined in Theorem 4 and take any x \u2208 {\u00b11}n. Then, there exist an integer \u03b8 and a vector of integers w such that h(x) = \u03c60,1(\u3008w,x\u3009 \u2212 \u03b8 \u2212 1/2). But since \u3008w,x\u3009 \u2212 \u03b8 is also an integer, if we let L = 1 this means that h(x) = \u03c6pw(\u3008w,x\u3009 \u2212 \u03b8 \u2212 1/2) as well. Furthermore, letting x\u2032 \u2208 Rn+1 denote the concatenation of x with the constant 1 and letting w\u2032 \u2208 Rn+1 denote the concatenation of w with the scalar (\u2212\u03b8\u2212 1/2) we obtain that h(x) = \u03c6pw(\u3008w\u2032,x\u2032\u3009). Last, let us normalize w\u0303 = w\u2032/\u2016w\u2032\u2016, x\u0303 = x/\u2016x\u2032\u2016, and redefine L to be \u2016w\u2032\u2016 \u2016x\u2032\u2016, we get that h(x) = \u03c6pw(\u3008w\u0303, x\u0303\u3009). That is, we have shown that H is contained in a class of the form Hpw with a Lipschitz constant bounded by poly(n). Combining the above with Lemma 3 we obtain the following:\nCorollary 2 Let L be a Lipschitz constant and let Hpw be the class defined by the L-Lipschitz transfer function \u03c6pw. Then, based on Assumption 1, there is no algorithm that runs in time poly(L, 1/\u01eb, 1/\u03b4) and (\u01eb, \u03b4)-learns the class Hpw.\nA similar argument leads to the hardness of learning Hsig.\nTheorem 5 Let L be a Lipschitz constant and let Hsig be the class defined by the L-Lipschitz transfer function \u03c6sig. Then, based on Assumption 1, there is no algorithm that runs in time poly(L, 1/\u01eb, 1/\u03b4) and (\u01eb, \u03b4)-learns the class Hsig.\nProof Let h be a hypothesis in the class H defined in Theorem 4 and take any x \u2208 {\u00b11}n. Then, there exist an integer \u03b8 and a vector of integers w such that h(x) = \u03c60,1(\u3008w,x\u3009\u2212 \u03b8\u2212 1/2). However, since \u3008w,x\u3009 \u2212 \u03b8 is also an integer, we see that\n|\u03c60,1(\u3008w,x\u3009 \u2212 \u03b8 \u2212 1/2)\u2212 \u03c6sig(\u3008w,x\u3009 \u2212 \u03b8 \u2212 1/2)| \u2264 1\n1 + exp(2L) .\nThis means that for any \u01eb > 0, if we pick L = log(2/\u01eb\u22121)2 and define hsig(x) = \u03c6sig(\u3008w,x\u3009 \u2212 \u03b8\u2212 1/2), then |h(x) \u2212 hsig(x)| \u2264 \u01eb/2. Furthermore, letting x\u2032 \u2208 Rn+1 denote the concatenation of x with the constant 1 and letting w\u2032 \u2208 Rn+1 denote the concatenation of w with the scalar (\u2212\u03b8 \u2212 1/2) we obtain that hsig(x) = \u03c6sig(\u3008w\u2032,x\u2032\u3009). Last, let us normalize w\u0303 = w\u2032/\u2016w\u2032\u2016, x\u0303 = x/\u2016x\u2032\u2016, and redefine L to be\nL = \u2016w\u2032\u2016\u2016x\u2032\u2016 log(2/\u01eb\u2212 1)\n2 (10)\nso that hsig(x) = \u03c6sig(\u3008w\u0303, x\u0303\u3009). Thus we see that if there exists an algorithm that runs in time poly(L, 1/\u01eb, 1/\u03b4) and (\u01eb/2, \u03b4)-learns the class Hsig, then since for all h \u2208 H exists hsig \u2208 Hsig such that |hsig(x) \u2212 h(x)| \u2264 \u01eb/2, there also exists an algorithm that (\u01eb, \u03b4)-learns the concept class H defined in Theorem 4 in time polynomial in (L, 1/\u01eb, 1/\u03b4) (for L defined in Equation 10). But by definition of L in Equation 10 and the fact that \u2016w\u2032\u2016 and \u2016x\u2032\u2016 are of size poly(n), this means that there is an algorithm that runs in time polynomial in (n, 1/\u01eb, 1/\u03b4) and (\u01eb, \u03b4)-learns the class H , which contradicts Lemma 3."}, {"heading": "4 Related work", "text": "The problem of learning kernel-based halfspaces has been extensively studied before, mainly in the framework of SVM [26, 10, 21]. When the data is separable with a margin \u00b5, it is possible to learn a halfspaces in polynomial time. The learning problem becomes much more difficult when the data is not separable with margin.\nIn terms of hardness results, [6] derive hardness results for proper learning with sufficiently small margins. There are also strong hardness of approximation results for proper learning without margin (see for example [13] and the references therein). We emphasize that we allow improper learning, which is just as useful for the purpose of learning good classifiers, and thus these hardness results do not apply. Instead, the hardness result we derived in Section 3 hold for improper learning as well. As mentioned before, the main tool we rely on for deriving the hardness result is the representation independent hardness result for learning intersections of halfspaces given in [17].\nPractical algorithms such as SVM often replace the 0-1 error function with a convex surrogate, and then apply convex optimization tools. However, there are no guarantees on how well the surrogate function approximates the 0-1 error function. Recently, [28, 4] studied the asymptotic relationship between surrogate convex loss functions and the 0-1 error function. In contrast, in this paper we show that even with a finite sample, surrogate convex loss functions can be competitive with the 0-1 error function as long as we replace inner-products with the kernel K(x,x\u2032) = 1/(1\u2212 0.5\u3008x,x\u2032\u3009)."}, {"heading": "4.1 Margin analysis", "text": "Recall that we circumvented the dependence of the VC dimension of H\u03c60\u22121 on the dimensionality of X by replacing \u03c60\u22121 with a Lipschitz transfer function. Another common approach is to require that the learned classifier will be competitive with the margin error rate of the optimal halfspace. Formally, the \u00b5-margin error rate of a halfspace of the form hw(x) = 1(\u3008w,x\u3009 > 0) is defined as:\nerrD,\u00b5(w) = Pr[hw(x) 6= y \u2228 |\u3008w,x\u3009| \u2264 \u00b5] . (11)\nIntuitively, errD,\u00b5(w) is the error rate of hw had we \u00b5-shifted each point in the worst possible way. Margin based analysis restates the goal of the learner (as given in Equation (2)) and requires that the learner will find a classifier h that satisfies:\nerrD(h) \u2264 min w:\u2016w\u2016=1 errD,\u00b5(w) + \u01eb . (12)\nBounds of the above form are called margin-based bounds and are widely used in the statistical analysis of Support Vector Machines and AdaBoost. It was shown [3, 19] that m = \u0398(log(1/\u03b4)/(\u00b5 \u01eb)2) examples are sufficient (and necessary) to learn a classifier for which Equation (12) holds with probability of at least 1 \u2212 \u03b4. Note that as in the sample complexity bound we gave in Theorem 1, the margin based sample complexity bound also does not depend on the dimension.\nIn fact, the Lipschitz approach used in this paper and the margin-based approach are closely related. First, it is easy to verify that if we set L = 1/(2\u00b5), then for any w the hypothesis h(x) = \u03c6pw(\u3008w,x\u3009) satisfies errD(h) \u2264 errD,\u00b5(w). Therefore, an algorithm that (\u01eb, \u03b4)-learnsHpw also guarantees that Equation (12) holds. Second, it is also easy to verify that if we set L = 14\u00b5 log ( 2\u2212\u01eb \u01eb ) then for any w the hypothesis h(x) = \u03c6sig(\u3008w,x\u3009) satisfies errD(h) \u2264 errD,\u00b5(w) + \u01eb/2. Therefore, an algorithm that (\u01eb/2, \u03b4)-learns Hsig also guarantees that Equation (12) holds.\nAs a direct corollary of the above discussion we obtain that it is possible to learn a vector w that guarantees Equation (12) in time poly(exp(O\u0303(1/\u00b5))).\nA computational complexity analysis under margin assumptions was first carried out in [6] (see also the hierarchical worst-case analysis recently proposed in [5]). The technique used in [6] is based on the observation that in the noise-free case, an optimal halfspace can be expressed as a linear sum of at most 1/\u00b52 examples. Therefore, one can perform an exhaustive search over all sub-sequences of 1/\u00b52 examples, and choose the optimal halfspace. Note that this algorithm will always run in time m1/\u00b5 2 . Since the sample complexity bound requires that m will be order of 1/(\u00b5\u01eb)2, the runtime of the method described by [6] becomes poly(exp(O\u0303(1/\u00b52))). In comparison, our algorithm achieves a\nbetter runtime of poly(exp(O\u0303(1/\u00b5))). Moreover, while the algorithm of [6] performs an exhaustive\nsearch, our algorithm\u2019s runtime depends on the parameter B, which is poly(exp(O\u0303(1/\u00b5))) only under a worst-case assumption. Since in practice we will cross-validate for B, it is plausible that in many real-world scenarios the runtime of our algorithm will be much smaller."}, {"heading": "4.2 Distributional Assumptions", "text": "The idea of approximating the zero-one transfer function with a polynomial was first proposed by [15] who studied the problem of agnostically learning halfspaces without kernels in Rn under distributional assumption. In particular, they showed that if the distribution over X is uniform over the unit ball, then it is possible to agnostically learn H\u03c60\u22121 in time poly(n\n1/\u01eb4). This was further generalized by [7], who showed that similar bounds hold for product distributions.\nBeside distributional assumptions, these works are characterized by explicit dependence on the dimension of X , and therefore are not adequate for the kernel-based setting we consider in this paper, in which the dimensionality of X can even be infinite. More precisely, while [15] try to approximate the zero-one transfer function with a low-degree polynomial, we require instead that the coefficients of the polynomials are bounded. The principle that when learning in high dimensions \u201cthe size of the parameters is more important than their number\u201d was one of the main advantages in the analysis of the statistical properties of several learning algorithms (e.g. [2]).\nInterestingly, in [23] we show that the very same algorithm we use in this paper recover the same complexity bound of [15]."}, {"heading": "5 Discussion", "text": "In this paper we described and analyzed a new technique for agnostically learning kernel-based halfspaces with the zero-one loss function. The bound we derive has an exponential dependence on L, the Lipschitz coefficient of the transfer function. While we prove that (under a certain cryptographic assumption) no algorithm can have a polynomial dependence on L, the immediate open question is whether the dependence on L can be further improved.\nA perhaps surprising property of our analysis is that we propose a single algorithm, returning a single classifier, which is simultaneously competitive against all transfer functions p \u2208 PB . In particular, it learns with respect to the \u201coptimal\u201d transfer function, where by optimal we mean the one which attains the smallest error rate, E[|p(\u3008w,x\u3009) \u2212 y|], over the distribution D.\nOur algorithm boils down to linear regression with the absolute loss function and while composing a particular kernel function over our original RKHS. It is possible to show that solving the vanilla\nSVM, with the hinge-loss, and composing again our particular kernel over the desired kernel, can also give similar guarantees. It is therefore interesting to study if there is something special about the kernel we propose or maybe other kernel functions (e.g. the Gaussian kernel) can give similar guarantees.\nAnother possible direction is to consider other types of margin-based analysis or transfer functions. For example, in the statistical learning literature, there are several definitions of \u201cnoise\u201d conditions, some of them are related to margin, which lead to faster decrease of the error rate as a function of the number of examples (see for example [9, 25, 24]). Studying the computational complexity of learning under these conditions is left to future work."}, {"heading": "Acknowledgments", "text": "We would like to thank Adam Klivans for helping with the Hardness results. This work was partially supported by a Google Faculty Research Grant."}, {"heading": "A Solving the ERM problem given in Equation (5)", "text": "In this section we show how to approximately solve Equation (5) when the transfer function is \u03c6pw. The technique we use is similar to the covering technique described in [6].\nFor each i, let bi = 2(yi \u2212 1/2). It is easy to verify that the objective of Equation (5) can be rewritten as\n1\nm\nm \u2211\ni=1\nf(bi\u3008w,xi\u3009) where f(a) = min{1,max{0, 1/2\u2212 La}} . (13)\nLet g(a) = max{0, 1/2\u2212La}. Note that g is a convex function, g(a) \u2265 f(a) for every a, and equality holds whenever a \u2265 \u22121/2L.\nLet w\u22c6 be a minimizer of Equation (13) over the unit ball. We partition the set [m] into\nI1 = {i \u2208 [m] : g(bi\u3008w\u22c6,xi\u3009) = f(bi\u3008w\u22c6,xi\u3009)} , I2 = [m] \\ I1 .\nNow, let w\u0302 be a vector that satisfies\n\u2211 i\u2208I1 g(bi\u3008w\u0302,xi\u3009) \u2264 min w:\u2016w\u2016\u22641 \u2211 i\u2208I1 g(bi\u3008w,xi\u3009) + \u01ebm . (14)\nClearly, we have\nm \u2211\ni=1\nf(bi\u3008w\u0302,xi\u3009) \u2264 \u2211\ni\u2208I1 g(bi\u3008w\u0302,xi\u3009) +\n\u2211 i\u2208I2 f(bi\u3008w\u0302,xi\u3009)\n\u2264 \u2211\ni\u2208I1 g(bi\u3008w\u0302,xi\u3009) + |I2|\n\u2264 \u2211\ni\u2208I1 g(bi\u3008w\u22c6,xi\u3009) + \u01ebm+ |I2|\n= m \u2211\ni=1\nf(bi\u3008w\u22c6,xi\u3009) + \u01ebm .\nDividing the two sides of the above by m we obtain that w\u0302 is an \u01eb-accurate solution to Equation (13). Therefore, it suffices to show a method that finds a vector w\u0302 that satisfies Equation (14). To do so, we use a standard generalization bound (based on Rademacher complexity) as follows:\nLemma 4 Let us sample i1, . . . , ik i.i.d. according to the uniform distribution over I1. Let w\u0302 be a minimizer of \u2211k\nj=1 g(bij \u3008w,xij \u3009) over w in the unit ball. Then,\nE\n[\n1 |I1|\n\u2211 i\u2208I1 g(bi\u3008w\u0302,xi\u3009)\u2212 min w:\u2016w\u2016\u22641 1 |I1| \u2211 i\u2208I1 g(bi\u3008w,xi\u3009)\n]\n\u2264 2L/ \u221a k ,\nwhere expectation is over the choice of i1, . . . , ik.\nProof Simply note that g is L-Lipschitz and then apply a Rademacher generalization bound with the contraction lemma.\nThe above lemma immediately implies that if k \u2265 4L2/\u01eb2, then there exist i1, . . . , ik in I1 such that if w\u0302 \u2208 argmin\nw:\u2016w\u2016\u22641 \u2211k j=1 g(bij \u3008w,xij \u3009) then w\u0302 satisfies Equation (14) and therefore it is an \u01eb-accurate solution of Equation (13). The procedure will therefore perform an exhaustive search over all i1, . . . , ik in [m], for each such sequence the procedure will find w\u0302 \u2208 argminw:\u2016w\u2016\u22641 \u2211k\nj=1 g(bij \u3008w,xij \u3009) (in polynomial time). Finally, the procedure will output the w\u0302 that minimizes the objective of Equation (13). The total runtime of the procedure is therefore poly(mk). Plugging in the value of k = \u23084L2/\u01eb2\u2309 and the value of m according to the sample complexity bound given in Theorem 1 we obtain the total runtime of\npoly ( (L/\u01eb)L 2/\u01eb2 ) = poly ( exp ( L2 \u01eb2 log(L/\u01eb) )) ."}, {"heading": "B Proof of Lemma 2", "text": "In order to approximate \u03c6sig with a polynomial, we will use the technique of Chebyshev approximation (cf. [18]). One can write any continuous function on [\u22121,+1] as a Chebyshev expansion \u2211\u221e\nn=0 \u03b1nTn(\u00b7), where each Tn(\u00b7) is a particular n-th degree polynomial denoted as the n-th Chebyshev polynomial (of the first kind). These polynomials are defined as T0(x) = 1, T1(x) = x, and then recursively via Tn+1(x) = 2xTn(x) \u2212 Tn\u22121(x). For any n, Tn(\u00b7) is bounded in [\u22121,+1]. The coefficients in the Chebyshev expansion of \u03c6sig are equal to\n\u03b1n = 1 + 1(n > 0)\n\u03c0\n\u222b 1\nx=\u22121\n\u03c6sig(x)Tn(x)\u221a 1\u2212 x2 dx. (15)\nTruncating the series after some threshold n = N provides an N -th degree polynomial which approximates the original function.\nIn order to obtain a bound on B, we need to understand the behavior of the coefficients in the Chebyshev approximation. These are determined in turn by the behavior of \u03b1n as well as the coefficients of each Chebyshev polynomial Tn(\u00b7). The following two lemmas provide the necessary bounds.\nLemma 5 For any n > 1, |\u03b1n| in the Chebyshev expansion of \u03c6sig on [\u22121,+1] is upper bounded as follows:\n|\u03b1n| \u2264 1/2L+ 1/\u03c0\n(1 + \u03c0/4L)n .\nAlso, we have |\u03b10| \u2264 1, |\u03b11| \u2264 2.\nProof The coefficients \u03b1n, n = 1, . . . in the Chebyshev series are given explicitly by\n\u03b1n = 2\n\u03c0\n\u222b 1\nx=\u22121\n\u03c6sig(x)Tn(x)\u221a 1\u2212 x2 dx. (16)\nFor \u03b10, the same equality holds with 2/\u03c0 replaced by 1/\u03c0, so \u03b10 equals\n1\n\u03c0\n\u222b 1\nx=\u22121\n\u03c6sig(x)\u221a 1\u2212 x2 dx,\nwhich by definition of \u03c6sig(x), is at most (1/\u03c0) \u222b 1 x=\u22121 (\u221a 1\u2212 x2 )\u22121 dx = 1. As for \u03b11, it equals\n2\n\u03c0\n\u222b 1\nx=\u22121\n\u03c6sig(x)x\u221a 1\u2212 x2 dx,\nwhose absolute value is at most (2/\u03c0) \u222b 1 x=\u22121 (\u221a 1\u2212 x2 )\u22121 dx = 2.\nTo evaluate the integral in Equation (16) for general n and L, we will need to use some tools from complex analysis. The calculation follows [11], to which we refer the reader for justification of the steps and further details4.\nOn the complex plane, the integral in Equation (16) can be viewed as a line integral over [\u22121,+1]. Using properties of Chebyshev polynomials, this integral can be converted into a more general complex-valued integral over an arbitrary closed curve C on the complex plane which satisfies certain regularity conditions:\n\u03b1n = 1\n\u03c0i\n\u222b\nC\n\u03c6sig(z)dz\u221a z2 \u2212 1(z \u00b1 \u221a z2 \u2212 1)n dz, (17)\nwhere the sign in \u00b1 is chosen so that |z \u00b1 \u221a z2 \u2212 1| > 1. In particular, for any parameter \u03c1 > 1, the set of points z satisfying |z \u00b1 \u221a z2 \u2212 1| = \u03c1 form an ellipse, which grows larger with \u03c1 and with foci at z = \u00b11 and which grows larger with \u03c1. Since we are free to choose C, we choose it as this ellipse while letting \u03c1 \u2192 \u221e.\nTo understand what happens when \u03c1 \u2192 \u221e, we need to characterize the singularities of \u03c6sig(z), namely the points z where \u03c6sig(z) is not well defined. Recalling that \u03c6sig(z) = (1 + e\n\u22124Lz)\u22121, we see that the problematic points are i(\u03c0 + 2\u03c0k)/4L for any k = \u00b11,\u00b12, . . ., where the denominator in \u03c6sig(z) equals zero. Note that this forms a discrete set of isolated points - in other words, \u03c6sig is\n4We note that such calculations also appear in standard textbooks on the subject, but they are usually carried under asymptotic assumptions and disregarding coefficients which are important for our purposes.\na meromorphic function. The fact that \u03c6sig is \u2019well behaved\u2019 in this sense allows us to perform the analysis below.\nThe behavior of the function at its singularities is defined via the residue of the function at each singularity c, which equals limz\u2192c(z \u2212 c)\u03c6sig(z) assuming the limit exists (in that case, the singularity is called a simple pole, otherwise a higher order limit might be needed). In our case, the residue for the singularity at i\u03c0/4L equals\nlim z\u21920\nz\n1 + e\u2212i\u03c0\u22124Lz = lim z\u21920 z 1\u2212 e\u22124Lz = limz\u21920 1/4L e\u22124Lz = 1/4L,\nwhere we used l\u2019Ho\u0302pital\u2019s rule to calculate the limit. The same residue also apply to all the other singularities.\nFor points in the complex plane uniformly bounded away from these singularities, |\u03c6sig(z)| is bounded, and therefore it can be shown that the integral in Equation (17) will tend to zero as we let C become an arbitrarily large ellipse (not passing too close to any of the singularities) by taking \u03c1 \u2192 \u221e. However, as \u03c1 varies smoothly, the ellipse does cross over singularity points, and these contribute to the integral. For meromorphic functions, with a discrete set of isolated singularities, we can simply sum over all contributions, and it can be shown (see equation 10 in [11] and the subsequent discussion) that\n\u03b1n = \u22122 \u221e \u2211\nk=\u2212\u221e\nrk \u221a\nz2k \u2212 1 ( zk \u00b1 \u221a z2k \u2212 1 )n ,\nwhere zk is the singularity point i(\u03c0 + 2\u03c0k)/4L with corresponding residue rk. Substituting the results for our chosen function, we have\n\u03b1n =\n\u221e \u2211\nk=\u2212\u221e\n1/4L \u221a\n(i(\u03c0 + 2\u03c0k)/4L)2 \u2212 1 ( i(\u03c0 + 2\u03c0k)/4L\u00b1 \u221a (i(\u03c0 + 2\u03c0k)/4L)2 \u2212 1 )n .\nA routine simplification leads to the following5:\n\u03b1n = \u221e \u2211\nk=\u2212\u221e\n1/4L\nin+1 \u221a ((\u03c0 + 2\u03c0k)/4L) 2 + 1\n(\n(\u03c0 + 2\u03c0k)/4L\u00b1 \u221a ((\u03c0 + 2\u03c0k)/4L) 2 + 1\n)n .\nIt can be verified that \u00b1 should be chosen according to 1(k \u2265 0). Therefore,\n|\u03b1n| = \u221e \u2211\nk=\u2212\u221e\n1/4L \u221a\n((\u03c0 + 2\u03c0k)/4L) 2 + 1\n(\n|\u03c0 + 2\u03c0k|/4L+ \u221a ((\u03c0 + 2\u03c0k)/4L) 2 + 1\n)n\n\u2264 \u221e \u2211\nk=\u2212\u221e\n1/4L (|\u03c0 + 2\u03c0k|1/4L+ 1)n \u2264 1/4L (1 + \u03c0/4L)n + 2\n\u221e \u2211\nk=1\n1/4L\n(1 + \u03c0(1 + 2k)/4L) n\n\u2264 1/4L (1 + \u03c0/4L)n +\n\u222b \u221e\nk=0\n1/2L\n(1 + \u03c0(1 + 2k)/4L) n dk\nSolving the integral and simplifying gives us\n|\u03b1n| \u2264 1\n(1 + \u03c0/4L)n\n(\n1/4L+ 1 + \u03c0/4L\n\u03c0(n\u2212 1)\n)\n.\nSince n \u2265 2, the result in the lemma follows.\n5On first look, it might appear that \u03b1n takes imaginary values for even n, due to the i n+1 factor, despite \u03b1n being equal to a real-valued integral. However, it can be shown that \u03b1n = 0 for even n. This additional analysis can also be used to slightly tighten our final results in terms of constants in the exponent, but it was not included for simplicity.\nLemma 6 For any non-negative integer n and j = 0, 1, . . . , n, let tn,j be the coefficient of x j in Tn(x). Then tn,j = 0 for any j with a different parity than n, and for any j > 0,\n|tn,j | \u2264 en+j\u221a 2\u03c0\nProof The fact that tn,j = 0 for j, n with different parities, and |tn,0| \u2264 1 is standard. Using an explicit formula from the literature (see [18], pg. 24), as well as Stirling approximation, we have that\n|tn,j | = 2n\u2212(n\u2212j)\u22121 n\nn\u2212 n\u2212j2\n( n\u2212 n\u2212j2 n\u2212j 2 ) = 2jn n+ j ( n+j 2 ) ! ( n\u2212j 2 ) !j!\n\u2264 2 jn\nj!(n+ j)\n(\nn+ j\n2\n)j\n= n(n+ j)j (n+ j)j! \u2264 n(n+ j) j (n+ j) \u221a 2\u03c0j(j/e)j = nej (n+ j) \u221a 2\u03c0j\n(\n1 + n\nj\n)j\n\u2264 ne j\n(n+ j) \u221a 2\u03c0j en.\nfrom which the lemma follows.\nWe are now in a position to prove a bound on B. As discussed earlier, \u03c6sig(x) in the domain [\u22121,+1] equals the expansion \u2211\u221en=0 \u03b1nTx. The error resulting from truncating the Chebyshev expanding at index N , for any x \u2208 [\u22121,+1], equals\n\u2223 \u2223 \u2223 \u2223 \u2223 \u03c6sig(x) \u2212 N \u2211\nn=0\n\u03b1nTn(x)\n\u2223 \u2223 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223 \u2223 \u221e \u2211\nn=N+1\n\u03b1nTn(x)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 \u221e \u2211\nn=N+1\n|\u03b1n|,\nwhere in the last transition we used the fact that |Tn(x)| \u2264 1. Using Lemma 5 and assuming N > 0, this is at most\n\u221e \u2211\nn=N+1\n1/2L+ 1/\u03c0 (1 + \u03c0/4L)n = 2 + 4L/\u03c0 \u03c0(1 + \u03c0/4L)N .\nIn order to achieve an accuracy of less than \u01eb in the approximation, we need to equate this to \u01eb and solve for N , i.e.\nN =\n\u2308\nlog1+\u03c0/4L\n(\n2 + 4L/\u03c0\n\u03c0\u01eb\n)\u2309\n(18)\nThe series left after truncation is \u2211N n=0 \u03b1nTn(x), which we can write as \u2211N j=0 \u03b2jx j . Using Lemma 5 and Lemma 6, the absolute value of the coefficient \u03b2j for j > 1 can be upper bounded by\n\u2211\nn=j..N,n=j mod 2\n|an||tn,j | \u2264 \u2211\nn=j..N,n=j mod 2\n1/2L+ 1/\u03c0 (1 + \u03c0/4L)n en+j\u221a 2\u03c0\n= (1/2L+ 1/\u03c0)ej\u221a\n2\u03c0\n\u2211\nn=j..N,n=j mod 2\n(\ne\n1 + \u03c0/4L\n)n\n= (1/2L+ 1/\u03c0)ej\u221a\n2\u03c0\n(\ne\n1 + \u03c0/4L\n)j \u230aN\u2212j2 \u230b \u2211\nn=0\n(\ne\n1 + \u03c0/4L\n)2n\n\u2264 (1/2L+ 1/\u03c0)e j\n\u221a 2\u03c0\n(\ne\n1 + \u03c0/4L\n)j (e/(1 + \u03c0/4L))N\u2212j+2 \u2212 1\n(e/(1 + \u03c0/4L))2 \u2212 1 .\nSince we assume L \u2265 3, we have in particular e/(1 + \u03c0/4L) > 1, so we can upper bound the expression above by dropping the 1 in the numerator, to get\n1/2L+ 1/\u03c0\u221a 2\u03c0((e/(1 + \u03c0/4L))2 \u2212 1)\n(\ne\n1 + \u03c0/4L\n)N+2\nej .\nThe cases \u03b20, \u03b21 need to be treated separately, due to the different form of the bounds on \u03b10, \u03b11. Repeating a similar analysis (using the actual values of tn,1, tn,0 for any n), we get\n\u03b20 \u2264 1 + 1\n\u03c0 +\n2L\n\u03c02\n\u03b21 \u2264 2 + 3(1 + 2L/\u03c0)(4L+ \u03c0)\n2\u03c02 .\nNow that we got a bound on the \u03b2j , we can plug it into the bound on B, and get\nB =\nN \u2211\nj=0\n2j\u03b22j \u2264 \u03b220 + 2\u03b221 + N \u2211\nj=2\n(\n1/2L+ 1/\u03c0\u221a 2\u03c0((e/(1 + \u03c0/4L))2 \u2212 1)\n)2 ( e\n1 + \u03c0/4L\n)2N+4\n(2e2)j\n\u2264 \u03b220 + 2\u03b221 + ( 1/2L+ 1/\u03c0\u221a 2\u03c0((e/(1 + \u03c0/4L))2 \u2212 1) )2 ( e 1 + \u03c0/4L )2N+4 (2e2)N+1 e2 \u2212 1\n= \u03b220 + 2\u03b2 2 1 +\n2(1/2L+ 1/\u03c0)2e6\n(e2 \u2212 1)2\u03c0((e/(1 + \u03c0/4L))2 \u2212 1)2(1 + \u03c0/4L)4\n( \u221a 2e2\n1 + \u03c0/4L\n)2N\n.\nTo make the expression more readable, we use the (rather arbitrary) assumption that L \u2265 3. In that case, by some numerical calculations, it is not difficult to show that we can upper bound the above by\n2L4 + 0.15\n( \u221a 2e2\n1 + \u03c0/4L\n)2N\n\u2264 2L4 + 0.15(2e4)N .\nCombining this with Equation (18), we get that this is upper bounded by\n2L4 + 0.15(2e4)log1+\u03c0/4L( 2+4L/\u03c0 \u03c0\u01eb )+1,\nor at most\n2L4 + exp\n\n\nlog(2e4) log (\n2+4L/\u03c0 \u03c0\u01eb\n)\nlog(1 + \u03c0/4L) + 3\n\n .\nUsing the fact that log(1 + x) \u2265 x \u2212 x2 for x \u2265 0, and the assumption that L \u2265 3, we can bound the exponent by\nlog(2e4) log (\n2+4L/\u03c0 \u03c0\u01eb\n)\n\u03c0 4L\n( 1\u2212 \u03c08L ) + 3 \u2264 7 log(2L/\u01eb)L+ 3.\nSubstituting back, we get the result stated in Lemma 2."}, {"heading": "C The \u03c6erf(\u00b7) Function", "text": "In this section, we prove a result anaologous to Lemma 2, using the \u03c6erf(\u00b7) transfer function. In a certain sense, it is stronger, because we can show that \u03c6erf(\u00b7) actually belongs to PB for sufficiently large B. However, the resulting bound is worse than Lemma 2, as it depends on exp(L2) rather than exp(L). However, the proof is much simpler, which helps to illustrate the technique.\nThe relevant lemma is the following:\nLemma 7 Let \u03c6erf(\u00b7) be as defined in Equation (4), where for simplicity we assume L \u2265 3. For any \u01eb > 0, let\nB \u2264 1 4 + 2L2 ( 1 + 3\u03c0eL2e4\u03c0L 2 ) .\nThen \u03c6erf(\u00b7) \u2208 PB.\nProof By a standard fact, \u03c6erf(\u00b7) is equal to its infinite Taylor series expansion at any point, and this series equals\n\u03c6erf(a) = 1\n2 + 1\u221a \u03c0\n\u221e \u2211\nn=0\n(\u22121)n(\u221a\u03c0La)2n+1 n!(2n+ 1) .\nLuckily, this is an infinite degree polynomial, and it is only left to calculate for which values of B does it belong to PB. Plugging in the coefficients in the bound on B, we get that\nB \u2264 1 4 + 1 \u03c0\n\u221e \u2211\nn=0\n(2\u03c0L2)2n+1\n(n!)2(2n+ 1)2 \u2264 1 4 + 1 \u03c0\n\u221e \u2211\nn=0\n(2\u03c0L2)2n+1\n(n!)2\n= 1\n4 + 2L2\n(\n1 +\n\u221e \u2211\nn=1\n(2\u03c0L2)2n\n(n!)2\n)\n\u2264 1 4 + 2L2\n(\n1 +\n\u221e \u2211\nn=1\n(2\u03c0L2)2n\n(n/e)2n\n)\n= 1\n4 + 2L2\n(\n1 +\n\u221e \u2211\nn=1\n(\n2\u03c0eL2\nn\n)2n )\n.\nThinking of (2\u03c0eL2/n)2n as a continuous function of n, a simple derivative exercise shows that it is maximized for n = 2\u03c0L2, with value e4\u03c0L 2\n. Therefore, we can upper bound the series in the expression above as follows:\n\u221e \u2211\nn=1\n(\n2\u03c0eL2\nn\n)2n\n=\n\u230a2 \u221a 2\u03c0eL2\u230b \u2211\nn=1\n(\n2\u03c0eL2\nn\n)2n\n+\n\u221e \u2211\nn=\u23082 \u221a 2\u03c0eL2\u2309\n(\n2\u03c0eL2\nn\n)2n\n\u2264 2 \u221a 2\u03c0eL2e4\u03c0L 2 +\n\u221e \u2211\nn=\u23082 \u221a 2\u03c0eL2\u2309\n(\n1\n2\n)n\n\u2264 3\u03c0eL2e4\u03c0L2 .\nwhere the last transition is by the assumption that L \u2265 3. Substituting into the bound on B, we get the result stated in the lemma."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "<lb>We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces<lb>with respect to the zero-one loss function. Unlike most previous formulations which rely on<lb>surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression),<lb>we provide finite time/sample guarantees with respect to the more natural zero-one loss<lb>function. The proposed algorithm can learn kernel-based halfspaces in worst-case time<lb>poly(exp(L log(L/\u01eb))), for any distribution, where L is a Lipschitz constant (which can be<lb>thought of as the reciprocal of the margin), and the learned classifier is worse than the<lb>optimal halfspace by at most \u01eb. We also prove a hardness result, showing that under a<lb>certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time<lb>polynomial in L.<lb>", "creator": "LaTeX with hyperref package"}}}