{"id": "1007.3159", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2010", "title": "Logic-Based Decision Support for Strategic Environmental Assessment", "abstract": "Strategic Environmental Assessment is a method of introducing a systematic assessment of the environmental impacts of plans and programs, based on the so-called coaxial matrices that define the dependencies between planetary activities (infrastructure, facilities, resource extraction, buildings, etc.) and positive and negative environmental impacts, as well as dependencies between these impacts and environmental receptors. So far, this method has been implemented manually by environmental experts to assess the environmental impacts of a given plan or program, but it is never applied during the planning / program build process. A decision support system based on clear logical semantics would be an invaluable tool not only in assessing a single, pre-defined plan, but also during the planning process, to create an optimized, ecologically evaluated plan and study possible alternative scenarios. We propose two logic-based approaches to the problem, one based on constraint logic programming and one based on probabilistic loming, to discuss the benefits of their convenience in the future.", "histories": [["v1", "Mon, 19 Jul 2010 14:36:54 GMT  (97kb)", "http://arxiv.org/abs/1007.3159v1", "17 pages, 1 figure, 26th Int'l. Conference on Logic Programming (ICLP'10)"]], "COMMENTS": "17 pages, 1 figure, 26th Int'l. Conference on Logic Programming (ICLP'10)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marco gavanelli", "fabrizio riguzzi", "michela milano", "paolo cagnoli"], "accepted": false, "id": "1007.3159"}, "pdf": {"name": "1007.3159.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["PAOLO CAGNOLI"], "emails": ["(marco.gavanelli@unife.it)", "fabrizio.riguzzi)@unife.it)", "michela.milano@unibo.it)", "PCagnoli@arpa.emr.it)"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 7.\n31 59\nv1 [\nKEYWORDS: Strategic Environmental Assessment, Regional Planning, Constraint Logic Programming, Probabilistic Logic Programming, Causality"}, {"heading": "1 Introduction", "text": "Computational Sustainability (Gomes 2009) is a very recent, interdisciplinary research field that aims to apply techniques from computer science, information science, operations research, applied mathematics, and statistics to the problem of balancing environmental, economic, and societal needs for sustainable development.\nAmong the many possible applications of information technology to sustainable development, decision support systems represent a very important topic. Currently, environmental experts take decisions, perform evaluations and build plans manually, simply relying on experience, with little or no support from automated tools.\nWe believe the main reason why decision support systems are not widely applied in this field is twofold: first, despite significant advances in algorithmic research, the current state of decision support systems still faces severe difficulties or cannot cope at all with the highly complex structure of sustainability problems. Second, there is a lack of appropriate models for sustainability related applications. These models should be developed in tight collaboration between computer scientists and environmental scientists, economists and biologists that can provide not only models and data, but also feedback on system solutions.\nComputational Logic can play a very important role in the design and implementation of decision support systems in this setting. First it enables a very intuitive and expressive representation of reality, and second it provides a number of reasoning mechanisms that can be successfully applied to the many aspects of sustainability problems: logical inference, constraint reasoning and probabilistic reasoning. In addition, Computational Logic tools rely on a well-defined semantics, and one can reason on the program to give explanations of the obtained results (or failure).\nSustainable development encompasses three pillars: society, economy and the environment. In this paper, we focus particularly on the environment. We address the problem of defining a logic-based decision support system for Strategic Environmental Assessment (SEA), a legally enforced procedure aimed to introduce systematic evaluation of the environmental effects of plans and programs. It typically applies to development, waste, transport, energy and land use plans, both regional and local, within the European Union. In this paper, we consider as a case study the assessment of Emilia Romagna regional plans. SEA is based on the so-called coaxial matrices that quantify dependencies between activities (e.g. infrastructures and plants) contained in a plan and positive and negative environmental impacts (e.g., alteration of woods, water pollution), and dependencies between impacts and environmental receptors (e.g., quantity of CO2 in the atmosphere).\nWe propose two alternative logic-based approaches: one exploits Constraint Logic Programming on Real Numbers CLP(R), and models coaxial matrices as sets of linear equations and inequations; this is a simple, efficient model, that presumes the available information to be precise, and assumes that influences can be summed up. The second approach is based on Logic Programs with Annotated Disjunction (LPADs) where activities and impacts are combined using the laws of probability. We apply the two approaches on coaxial matrices referring to eleven types of plans that legally require the SEA. Experiments are performed on a real energy regional plan.\nThe structure of the paper is as follows: section 2 describes regional planning and Strategic Environmental Assessment along with coaxial matrices. Section 3 recalls the main concepts behind constraint logic programming and probabilistic logic programming along with its causal interpretation. Section 4 shows the implementation of the coaxial matrices in CLP(R) while section 5 describes the approach based on\nLPAD. Section 6 presents experimental results on a real energy plan. A discussion and a description of open issues conclude the paper."}, {"heading": "2 Strategic Environmental Assessment", "text": "Regional planning is the science of efficient placement of land use activities and infrastructures for the sustainable growth of a region. Our case study is the Emilia Romagna region of Italy, and we developed this work on real data provided by the environmental regional agency. Regional plans are classified into types; the SEA is legally required for eleven types of plans (namely Agriculture, Forest, Fishing, Energy, Industry, Transport, Waste, Water, Telecommunication, Tourism, Urban and Environmental plans), those addressed in this work. Each plan defines activities that should be carried out during the plan\u2019s implementation. Activities are roughly divided into six types:\n\u2022 infrastructures and plants; \u2022 buildings and land use transformations; \u2022 resource extraction; \u2022 modifications of hydraulic regime; \u2022 industrial transformations; \u2022 environmental management.\nBefore any implementation, these plans have to be environmentally assessed, under the Strategic Environmental Assessment Directive. SEA is a method for incorporating environmental considerations into policies, plans and programs that is prescribed by European Union policy.\nOne of the instruments used for assessing a regional plan in Emilia Romagna are the so-called coaxial matrices, that are a development of the network method (Sorensen and Moss 1973).\nOne matrix M defines the dependencies between the above-mentioned activities contained in a plan and positive and negative impacts (also called pressures) on the environment. Each element mij of the matrix M defines a qualitative dependency between the activity i and the negative or positive impact j. The dependency can be high, medium, low or null. Examples of negative impacts are energy, water and land consumption, variation of water flows, water and air pollution and so on. Examples of positive impacts are reduction of water/air pollution, reduction of greenhouse gas emission, reduction of noise, natural resource saving, creation of new ecosystems and so on.\nThe second matrix N defines how the impacts influence environmental receptors. Each element nij of the matrix N defines a qualitative dependency between the negative or positive impact i and an environmental receptor j. Again the dependency can be high, medium, low or null. Examples of environmental receptors are the quality of surface water and groundwater, quality of landscapes, energy availability, wildlife wellness and so on.\nThe matrices currently used in Emilia Romagna contain 93 activities, 29 negative impacts, 19 positive impacts and 23 receptors and refer to the above-mentioned 11 plans.\nThe coaxial matrices are currently used by environmental experts that manually evaluate a single, already defined, plan. A plan basically defines the so-called magnitude of each activity: magnitudes are real values that intuitively express \u201chow much\u201d of an activity is performed. The unit is different for each activity: for example, for activity Thermoelectric power plants the magnitude says how many MW of electric power will be produced by thermoelectric plants, while the magnitude of Oil/gas/steam pipelines gives the number of kilometers of pipes installed. A manual evaluation of alternatives and what-if queries are very difficult to consider. In addition, planning is now carried out without a rigorous consideration of environmental aspects contained in the coaxial matrices.\nIn this paper, we propose two logic-based approaches for the design of a decision support system that can be used to assess a single, already defined plan, to evaluate different scenarios during the planning phase or to optimize the definition of land use activities and infrastructures.\nIn both cases, we convert the qualitative values into real numbers in the interval [0, 1]. The environmental expert suggested the values to be 0.25 for low, 0.5 for medium, and 0.75 for high.\nThe first approach is based on Constraint Logic Programming on Real numbers (CLP(R)), that is extremely efficient when dealing with linear equations. On the other hand, this approach does not take into consideration the subjective and stochastic nature of the available data: each value in the matrices is simply used as a coefficient in a linear equation, so we assume that positive and negative impacts derived from planned activities can be summed. While in general, impacts can indeed be summed, in some cases a mere summation is not the most realistic relation and more sophisticated combinations should be considered.\nFor this reason, we also evaluate a Causal Probabilistic Logic Programming approach that is grounded on the well-established theory of probability and causality. The same coefficients are now interpreted as probabilities, that will be combined through probability laws to provide the likelihood of a given receptor being affected. The price to be paid is a higher computation time. A realistic decision support system should merge the two approaches and this is a subject of the current research activity."}, {"heading": "3 Background", "text": "We provide some preliminaries on the two logic-based techniques used in this paper."}, {"heading": "3.1 Constraint Logic Programming", "text": "Constraint Logic Programming (CLP) (Jaffar and Maher 1994) is a class of programming languages which extend classical Logic Programming. Variables can be assigned either terms (as in Prolog), or interpreted values, taken from a sort, that is a parameter of the specific CLP language. For example, we can have CLP(R) (Jaffar et al. 1992), on the sort of real values, or CLP(FD), in which variables range on finite domains. The sort also contains interpreted functions (that, in numerical\ndomains, can be the usual operations +, -, \u00d7, etc.) and predicates (e.g., <, 6=, \u2265, etc.), which are called constraints. The declarative semantics gives the intuitive interpretation of the specific sort to constraints and interpreted terms: e.g., 1.3+2 < 5 is true in CLP(R). The operational semantics resembles that of Prolog for atoms built on the usual predicates (i.e., those predicates defined by a set of clauses), but stores the interpreted ones, the constraints, to a special data structure, called the constraint store. The store is then interpreted and modified by an external machinery, called the constraint solver. The solver is able to check if the conjunction of constraints in the store is (un)satisfiable, and is also able to modify the store, possibly simplifying it to a refined state. Usually, the constraint solver does not perform complete propagation: if it returns false, then there is definitely no solution, but in some cases it may fail to detect infeasibility even if no solution exists.\nCLP(R) is the instance of CLP in which variables range on the reals. The available constraints are linear equalities and inequalities, and the solver is usually implemented through the simplex algorithm, which is very fast and is complete for linear (in)equalities (it always returns true or false). Also, the user can communicate an objective function to the solver: a linear term that should be minimized or maximized while satisfying all constraints.\nMany implementations of CLP(R) exist nowadays (De Koninck et al. 2006), and many Prolog flavours (Zhou et al. 1996; Hermenegildo et al. 2008) have their own CLP(R) library. We decided to adopt ECLiPSe, that features a library called Eplex (Shen and Schimpf 2005). This library interfaces ECLiPSe to an external mixed integer linear programming solver, which can be either a state-of-the-art commercial one (like CPLEX or Xpress-MP), or an open source solver. By default, Eplex hides most of the details of the solver, but nevertheless, when required, the user can trim various parameters to boost the performance, and also inspect the internals of the solver. This feature becomes very useful in practical applications, and will be used to provide additional valuable information to the user, as detailed in Section 4."}, {"heading": "3.2 Causal Probabilistic Logic Programming", "text": "In this section we first present Probabilistic Logic Programming and then we discuss how to model causation with it."}, {"heading": "3.2.1 Probabilistic Logic Programming", "text": "The integration of logic and probability has been widely studied in Logic Programming and various languages semantics have been proposed, such as Probabilistic Logic Programs (Dantsin 1991), Independent Choice Logic (Poole 1997), PRISM (Sato and Kameya 1997), pD (Fuhr 2000), CLP(BN) (Santos Costa et al. 2003) and ProbLog (De Raedt et al. 2007).\nLogic Programs with Annotated Disjunctions (LPADs) (Vennekens et al. 2004)\nare particularly suitable for reasoning about causes and effects (Vennekens et al. 2009). They extend logic programs by allowing clauses to be disjunctive and by annotating each atom in the head with a probability. A clause can be causally interpreted by\nsupposing that the truth of the body causes the truth of one of the atoms in the head non-deterministically chosen on the basis of the annotations.\nAn LPAD theory T consists of a finite set of annotated disjunctive clauses. These\nclauses have the following form\n(H1 : \u03b11) \u2228 (H2 : \u03b12) \u2228 . . . \u2228 (Hh : \u03b1h) : \u2212B1, B2, . . . Bb\nwhere the His are logical atoms, the Bis are logical literals and the \u03b1is are real numbers in the interval [0, 1] such that \u2211h i=1 \u03b1i \u2264 1. If \u2211h i=1 \u03b1i < 1, the head of the clause implicitly contains an extra atom null that does not appear in the body of any clause and whose annotation is 1\u2212 \u2211h\ni=1 \u03b1i. If C is the clause above, H(C, i)\nis Hi, \u03b1(C, i) is \u03b1i and body(C) is B1, B2, . . . Bb.\nThe semantics of a non-ground theory T is defined through its grounding g(T )\nand Vennekens et al. (2004) require that g(T ) is finite.\nAn atomic choice \u03c7 is a triple (C, \u03b8, i) where C \u2208 T , \u03b8 is a substitution that grounds C and i \u2208 {1, . . . , n} where n is the number of atoms in the head of C. (C, \u03b8, i) means that, for the ground clause C\u03b8, the head H(C, i) : \u03b1(C, i) was chosen. A selection \u03c3 is a set of atomic choices such that for each clause C\u03b8 in g(T ) there exists one and only one atomic choice (C, \u03b8, i) in \u03c3. We denote the set of all selections of a program T by ST .\nA selection \u03c3 identifies a normal logic program T\u03c3 = {(H(C, i) : \u2212body(C))\u03b8| (C, \u03b8, i) \u2208 \u03c3} that is called an instance of T . A probability distribution is defined over the space of instances by assuming independence among the choices made for each clause, thus the probability P\u03c3 of an instance T\u03c3 is given by P\u03c3 =\u220f (C,\u03b8,i)\u2208\u03c3 \u03b1(C, i).\nThe meaning of the instances of an LPAD is given by the well-founded semantics. For each instance T\u03c3, we require that its well-founded model WF (T\u03c3) is total, since we want to model uncertainty only by means of disjunctions.\nThe probability of a formula Q is given by the sum of the probabilities of the\ninstances in which the formula is true according to the well-founded semantics:\nP (Q) = \u2211\n\u03c3\u2208ST ,WF (T\u03c3)|=Q\nP\u03c3\nAn LPAD T can be translated into a Bayesian network \u03b2(T ) that has a Boolean random variable for each ground atom plus a random variable choiceC\u03b8 for each grounding C\u03b8 of each clause C of T whose values are the atoms in the head of C\u03b8 plus null.\nchoiceC\u03b8 assumes value H(C, i)\u03b8 with probability \u03b1(C, i) if the configuration of its parents makes the body true, while it assumes value null with probability 1 if the configuration makes the body false. The parents of ground atom A are all the choiceC\u03b8 variables such that A appears in the head of C\u03b8. A assumes value true with probability 1 if one of the parent choice variables assumes value A, otherwise it assumes value false with probability 1.\nVarious approaches have been proposed for computing the probability of queries from an LPAD. Riguzzi (2008) discusses an extension of SLG resolution, called SLGAD, that is able to compute the probability of queries by repeatedly branching on\ndisjunctive clauses. A different approach was taken by Meert et al. (2009), where an LPAD is first transformed into its equivalent Bayesian network and then inference is performed on the network using the variable elimination algorithm. Riguzzi (2007) presents the cplint system that first finds explanations (sets of atomic choices) for queries and then computes the probability by means of Binary Decision Diagrams, as proposed in (De Raedt et al. 2007) for the ProbLog language. cplint was used in the experiments in Section 6.2 because of its speed (Riguzzi 2009)."}, {"heading": "3.2.2 Causal Models", "text": "Determining when an event causes another event is very important in many domains, take for example science, medicine, pharmacology or economics. Causality has been widely debated by philosophers and statisticians: often it has been confused with correlation, while they are in fact distinct concepts, since two events may be correlated without one causing the other. Recently, Pearl (2000) helped to clarify the concept of causation by discussing how to represent causal information and how to perform inference from it. He illustrates two types of causal models: causal Bayesian networks and structural equations.\nCausal Bayesian networks differ from standard Bayesian networks because the edge from variable X to variable Y means that X is a cause for Y , while in standard Bayesian networks it simply means that there is a statistical dependence.\nPearl (2000) proposed an approach for computing the probability of effects of actions and suggested to use the notation P (y|do(x)) to indicate the effect of the action of setting the variable X to value x on the event of variable Y taking value y. P (y|do(x)) is different from the probability of y given x (P (y|x)) because we do not simply observe X = x but we intervene on the model by making sure X = x is true.\nThe technique proposed in (Pearl 2000) for computing P (y|do(x)) consists of removing the parents of X from a causal Bayesian network, setting X to x and computing P (y) in the obtained network.\nThis approach can be applied to probabilistic logic languages that can be translated to Bayesian networks, such as LPADs. In order to compute the probability of a ground atom Y of being true given an intervention that consists of making a ground atom X true from an LPAD T , we need to remove X from the head of all the clauses that contain it and add X as a fact to T . The probability of Y can then be computed from the resulting LPAD T \u2032 by using standard inference, i.e., by computing P (Y )."}, {"heading": "4 Coaxial Matrices in CLP(R)", "text": "The coaxial matrices can be simply interpreted as a linear programming model. Amongst the many ways to invoke a linear programming solver, we decided to use CLP(R); in this way the model is written as a knowledge base in a computational logic language, that could be easier to integrate with the probabilistic approach in Section 5.\nIn more detail, the environmental impacts caused by activity i (with magnitude\nai) can be estimated with the system of linear equations\n\u2200j \u2208 {1, . . . , Np} pj = m i jai.\nWhen considering a whole regional plan, we sum up the contributions of all the activities and obtain the estimate of the influence on each environmental impact:\n\u2200j \u2208 {1, . . . , Np} pj = Na\u2211\ni=1\nmijai. (1)\nIn the same way, given the vector of environmental impacts P = (p1, . . . , pNp), one can estimate the influence on the environmental receptor ri by means of the matrix N , that relates impacts with receptors:\n\u2200j \u2208 {1, . . . , Nr} rj =\nNp\u2211\ni=1\nnijpi. (2)\nThe system of equations (1-2) are imposed as constraints in a CLP(R) program; thanks to this formalisation, a number of queries of high interest both for the planner and for the evaluator of the environmental policy can be posed to the system as CLP(R) goals.\nThe final goal for the evaluator of the environmental policy is computing the environmental footprint of a devised plan. The plan is given as a set of values representing the magnitude of each of the activities. In other words, given the set of values A = (a1, . . . , aNa), we can compute the environmental footprint R = (r1, . . . , rNr ), simply by applying equations (1) and (2).\nAnother query studies the impact of a single unit (in a standardized format) of activity ai; for example, we are interested to know what the environmental footprint is of producing 1 MW of electric power through a thermoelectric plant. We instantiate the vector of activities to a unary vector with ai = 1 if i = therm and ai = 0 otherwise:\nA = (0, 0, . . . , 1, . . . , 0)\nIn this way, one can find out, by looking at the resulting vector R, which of the receptors are (positively or negatively) influenced by the devised activity. Also, one can get an estimate of those receptors that are more heavily influenced, and those that are only marginally influenced. This query can also be used by experts to calibrate the numbers in the coaxial matrices, by considering each activity singularly.\nAnother important query for the final user is asking which of the possible activities (always in normalized form) has a major impact on some given receptor ri. In fact, in CLP(R), one can maximize or minimize some objective function, so the model becomes\nmax(ri)\ns.t. (1)(2)\u2211 j aj = 1\n\u2200j, aj is integer\nFinally, if there are laws imposing limits on some receptors (limits for CO2 emis-\nsions, for example) one can very easily impose constraints on receptors (e.g., rCO2 \u2264 limitCO2), and find if an activity can either be performed at all, or if it requires some compensation (e.g., another activity that improves on the receptor, like reforestation for CO2), or if it can be done in association with other activities.\nIn cases where there are two or more alternative activities that cater for the same need, the regulations prescribe that alternatives should be studied, and compared. For example, the need for additional electrical power is satisfied by building a new plant; however one can choose the type of plant, depending on the environmental conditions. In an area with highly polluted air, a thermoelectric plant could raise the pollution over the law limit, so a different type of plant could be devised, like a solar power plant. On the other hand, a solar plant could be too expensive, and make other activities that are necessary in the area (e.g., building a school, a hospital, etc.) unaffordable. In this case, the planner can impose a constraint stating that there is a regional need for at least k MW of electrical power; he/she imposes\n\u2211\ni\u2208PowerPlants\nai \u2265 k\n(where PowerP lants is the set of indices in the vector A corresponding to those plants that provide electrical power) and then can optimize for one of the receptors, e.g., rCO2 , or some weighted sum of receptors of interest. Or, the planner may ask what the maximum power is that can be generated in the region without violating the law limits on the receptors\nmax \u2211\ni\u2208PowerPlants ai\ns.t. (1)(2)\n\u2200i \u2208 {1, . . . , Nr} ri \u2264 limiti\nIn this way, we find the maximum number of MW that can be produced, as well as the electrical power produced by each type of plant. Note that in this way the solver could find an assignment that imposes the execution of compensation activities, as hinted earlier. If there are not enough resources for compensation, we can impose that such activities must not be performed (e.g., by assigning value 0 to all these activities), or we can impose that, given a vector C with the cost of each activity, the total cost of the activities should not be higher than the allotted finances F :\nNa\u2211\ni=1\nciai \u2264 F (3)\nIn the same way, other types of resources, like time, person-months, energy, can be taken into consideration.\nWe are currently improving the model to take into account the fact that different activities can have different impacts on the environment depending on the type of zone they are placed. For example, if we build a power line within a natural park, its impact is definitely higher than building it near a city. An additional feature we are studying is the fact that, depending on the zone we are considering, different receptors might have different weights. For instance, the water quality is extremely\nimportant on a river delta, where the whole ecosystem relies on the river water, while it is less important in an industrialized area."}, {"heading": "4.1 Sensitivity Analysis", "text": "The simplex algorithm provides the optimal value of the objective function, the optimal assignment to the decision variables, and also other information that is of high interest for the decision maker. In particular, it provides the so-called reduced costs, and the dual solution. These indicators provide precious information on the sensitivity of the found solution to the parameters of the constraint model.\nThe dual solution is a set of values that correspond to the constraints. It can be thought of as the derivatives of the objective function with respect to the right hand side (RHS) of the constraints. This means that we immediately see, in the dual solution, which of the constraints are tight, i.e., which would change the value of the objective function if the RHS coefficient changes. For example, if we are optimizing the number fMW of MW of electric power and we have a constraint rCO2 \u2264 limitCO2 , the corresponding dual value dCO2 \u2202fMW\n\u2202limitCO2 in the optimal so-\nlution answers the question: \u201cHow much would the production of energy decrease in case the limit of CO2 lowers one point?\u201d This is important information, since regulations change, and tend to become more strict.\nThe same analysis can be performed on the problem of optimizing some (weighted sum of) receptors, given a total number of plants (or required MW). In this case, the dual value associated to a constraint represents how much the receptor will improve if that constraint is partially relaxed (if the RHS becomes less strict). For example, suppose we are optimizing the emissions of nitrogen oxides (NOx), and we have the constraint (3) stating a limit on the total cost of the activities, for example, in euro. After obtaining the optimal value, the planner could ask: \u201cSuppose now that we had more money: if I add one euro, how much would the emissions of NOx decrease?\u201d The answer is the dual value de of the constraint (3). This analysis is very attractive for the evaluator."}, {"heading": "5 A Causal Model for Coaxial Matrices", "text": "In this section we consider an interpretation of Coaxial Matrices that differs from the one in Section 4. Instead of associating a real number to each activity, impact and receptor, we associate a Boolean random variable to each of them and we consider the interaction levels expressed in the matrix as probabilistic causal dependencies. In this approach, we assume that an activity is either carried out or not, an impact is either present or not and a receptor is either achieved or not. In other words, we do not consider the magnitude or level of the variables under analysis. We used this approximation to get useful insights on the probabilistic modeling of the problem. In the future, we plan to consider more refined approximations with multivalued random variables or even continuous random variables.\nActivities, impacts and receptors are represented by LPAD atoms (propositions)\nand the effects of activities on impacts and of impacts on receptors are expressed by means of LPAD rules that represent the Coaxial Matrices.\nThe model thus contains rules that express the effect of the activities on the\nnegative impacts (where mij is an element of the M matrix):\nnegative impactj : m i j :- activityi.\nAlso, there are rules expressing the effect of the activities on the positive impacts:\npositive impact j : m i j :- activityi.\nFor example, the model contains the rule\n\u2019Dispersion of dangerous materials\u2019:0.75 :-\n\u2019External movements of dangerous materials\u2019.\nthat relates an activity and a negative impact, and the rule\n\u2019Creation of work opportunities\u2019:0.5 :-\n\u2019External movements of dangerous materials\u2019.\nthat relates an activity and a positive impact.\nNegative impacts reduce the probability of receptors, while positive impacts increase it. However adding a clause with a certain atom in the head can only increase the probability of the atom. To model the fact that negative impacts lower the probability of receptors, we use, for each receptor receptork, two auxiliary predicates receptor posk and receptor negk that collect the evidence in favor or against the achievement of the receptor.\nThe rules that express the negative effect of the negative impacts on the receptors\ntake the form:\nreceptor negk : n j k :- negative impactj.\nwhile the rules that express the positive effect of the positive impacts on the receptors take the form:\nreceptor posk : n j k :- positive impact j .\nwhere njk is an element of the N matrix. For example, the rule\n\u2019Human health/wellbeing_neg\u2019:0.25:-\n\u2019Dispersion of dangerous materials\u2019.\nexpresses a negative effect of a negative impact on a receptor, and the rule\n\u2019Human health/wellbeing_pos\u2019:0.75:-\u2019Creation of work opportunities\u2019.\nexpresses a positive effect of a positive impact on a receptor.\nFinally, the positive and negative evidence regarding the receptor are combined\nwith the following rules:\nreceptor : 0.1 :- \\+ receptor pos, receptor neg. receptor : 0.5 :- \\+ receptor pos, \\+ receptor neg. receptor : 0.5 :- receptor pos, receptor neg. receptor : 0.9 :- receptor pos, \\+ receptor neg.\nFor example, the model contains the rules\n\u2019Human health/wellbeing\u2019:0.1 :- \\+ \u2019Human health/wellbeing_pos\u2019,\n\u2019Human health/wellbeing_neg\u2019.\n\u2019Human health/wellbeing\u2019:0.5 :- \\+ \u2019Human health/wellbeing_pos\u2019,\n\\+ \u2019Human health/wellbeing_neg\u2019.\n\u2019Human health/wellbeing\u2019:0.5 :- \u2019Human health/wellbeing_pos\u2019,\n\u2019Human health/wellbeing_neg\u2019.\n\u2019Human health/wellbeing\u2019:0.9 :- \u2019Human health/wellbeing_pos\u2019,\n\\+ \u2019Human health/wellbeing_neg\u2019.\nthat collect positive and negative effects on the receptor \u201cHuman health/wellbeing\u201d.\nThese rules express the fact that \u201cHuman health and wellbeing\u201d is unlikely if there is no positive evidence on it and there is negative evidence on it (first rule). It is very likely if there is positive evidence on it and no negative evidence on it (last rule). In the other cases, the probability of \u201cHuman health and wellbeing\u201d is in between (second and third rule).\nAll the parameters were subjectively estimated and validated by the expert."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Experimental results of CLP(R)", "text": "The agency for the environment of the Emilia-Romagna region (Italy) kindly provided us with the coaxial matrices used for assessing eleven types of plans (that we translate into the CLP model) and the data of a regional energy plan: for each of the activities, we have a \u201cmagnitude\u201d value. Thanks to the CLP model described earlier, we are able to compute the corresponding values of impacts and receptors.\nInitially the results were counterintuitive: the considered plan concerned energy (aimed at raising the available electrical power in the region), while the receptor energy availability had a lower value than the previous year. These types of results may be partially due to the qualitative information contained in the matrices, but also highlight possible human mistakes in the data of the matrices. Indeed, a flaw was found (and fixed) in the matrices, showing how logic-based decision support can contribute to increase the reliability of the environmental assessment.\nOnce the human mistakes had been corrected, we reran the experiments. The new results were highly appreciated by the evaluator: the decision support system foresaw strong decrease of quality of air (mainly due to the boost on thermoelectric plants), and water availability (since thermoelectric plants need refrigeration).\nAs the plan had a large impact on some receptors, we tried to improve it from an environmental viewpoint: the magnitude of each of the activities was allowed to deviate up to 1% with respect to the original plan, and we optimized the quality of air receptor. We had an improvement of about 20.3% on this receptor, which shows that even by allowing small variations one can get significant improvements. On the other hand, we had a decrease of industrial indicators, such as the availability of productive resources or the availability of energy.\nWe also tried two dual goals. The first considers the given plan, keeps all activities constant except the building of (various types of) power plants, fixes the amount of produced energy, and tries to optimize on the quality of air. The second, instead, maximizes the electrical power supply without sacrificing any of the environmental receptors, i.e., none of the receptors could worsen with respect to the original plan. The first query gave a positive result: by producing electricity with environmental friendly power plants (wind-powered aerogenerators) we could produce the same amount of energy but have a 57% improvement on the quality of air.\nThe second, instead, had a negative result: we could not improve the produced electrical power without worsening at least one receptor. These seemingly contradictory results actually have an interesting explanation. The receptors taken into account by the environmental assessment range on all aspects influenced by a human activity, spanning, e.g., from value of cultural heritage to stability of riverbeds, from quality of underground water to visual impact on the landscape. Aerogenerators, recommended in the previous optimization, have a significant visual impact, so they are not implementable unless we relax the visual requirement.\nComputing time for this analysis was hardly measurable: all times were far less than a fraction of second on a modern PC. Thanks to such a fast computation, we could comment the results of the queries online with the experts of the regional agency, identify errors in the provided data, and try variations of the parameters."}, {"heading": "6.2 Causal Model", "text": "Given the causal model presented in Section 5, we can ask various what-if queries\n1. if these activities are performed, what is the probability of a certain impact\nof appearing?\n2. if these works are performed, what is the probability of a certain receptor\nbeing satisfied?\nQueries of type 2 are more interesting because they relate the works directly with their final effects of interest. However, they are also more complex to compute. Moreover, the queries above can be generalized to the case in which the activities are performed with a certain probability.\nWe can answer the queries above by following the approach described in 3.2.2: we add a fact for each activity that is carried out and we ask for the probability of the query from the modified program.\nWe report on a number of queries together with their execution times on Linux machines with an Intel Core 2 Duo E6550 (2333 MHz) processor and 4 GB of RAM.\nThe probability of the negative impact \u201cDispersion of Dangerous Materials\u201d performing the activities \u201cExternal movements of dangerous materials\u201d and \u201cInternal movements of dangerous materials\u201d is 0.937500. The CPU time was below 10\u22126 seconds.\nThe probability of the receptor \u201cHuman health/wellbeing\u201d given that we perform the activities \u201cExternal movements of dangerous materials\u201d and \u201cInternal movements of dangerous materials\u201d is 0.546915 and the query took 22.713 seconds.\nIf we perform the activity \u201cIndustrial processing and transformation\u201d the probability of the receptor \u201cHuman health/wellbeing\u201d is 0.474918, computed in 84.453 seconds. This query takes longer than the previous ones because the work \u201cIndustrial processing and transformation\u201d has an influence on many more impacts than \u201cExternal movements of dangerous materials\u201d and \u201cInternal movements of dangerous materials\u201d and all these influences must be combined to find the effect on \u201cHuman health and wellbeing\u201d. To give the reader an idea of the complexity of this query, there are 655,660 explanations, 12,847,036 atomic choices appear in the explanations and 42 random variables are involved.\nThe probability of receptor \u201cAtmosphere quality, microclimate\u201d given the action\n\u201cIndustrial processing and transformation\u201d is 0.360851. The CPU time was 0.02s.\nIf we add the activity \u201cOil and gas extraction plants\u201d the probability of the receptor \u201cAtmosphere quality, microclimate\u201d lowers to 0.326481, computed in 6.852s\nBy adding the activity \u201cFire extinguishing plants\u201d the probability of the receptor \u201cAtmosphere quality, microclimate\u201d rises to 0.454471, due to the positive effects of the last activity. The CPU time was 92.67 seconds.\nAs can be seen from the last three cases, increasing the number of activities increases the computation time, since we have to combine the effects of the different causes. The last query has 606,726 explanations, 10,973,022 atomic choices appear in the explanations and 36 random variables are involved."}, {"heading": "6.3 Comparison", "text": "As hinted earlier, we developed two models with the final aim to integrate the two. Before such an ambitious goal can be reached, we need to identify strengths and weaknesses of the two. In order to have a systematic comparison, we produced two tables (one for each approach) in which each cell contains the effect of a single activity on a single receptor. One table reports the results of the linear model and the other those of the causal model. The tables are thus of size Na\u00d7Nr = 93\u00d723 = 2139.\nIn the scatter plot of Figure 1 we draw the results of the causal model against those of the linear model: the linear model results are on the x-axis, while in the\ncausal model results are on the y-axis. As we can see, most of the point are clustered along a simple curve, which seems to indicate a close relationship between the two models that we are going to investigate in the near future. Moreover, whenever the linear value is positive, the probability of improving the receptor is greater than 0.5 and vice-versa, showing that the two models may disagree on the values, but they agree on the direction of the effect on the receptor.\nTo better investigate the results, we considered the points farthest from the curve that are highlighted in Figure 1. Since these are the points for which the linear and causal model differ the most, we asked the expert to evaluate the results for those points, to understand which model gave the best result. For the points shown as triangles in Figure 1, the expert was unable to state which answer is better. For the points with a diamond symbol, the CLP(R) approach gave a better result. In the point shown as a star, both approaches failed to give a correct result.\nFrom these results, we can say that often the effects can be summed up, although in some cases other combinations could be necessary. In future work, we plan to try other techniques, such as fuzzy logic, and to use probabilities to model the uncertainty of the parameters of the matrices."}, {"heading": "7 Conclusions", "text": "The environmental assessment is now becoming a systematic procedure imposed by the laws, and its importance is doomed to increase year after year. In this paper, we proposed how two technologies taken from computational logic can successfully address practical problems of the environmental assessment. The work was conducted using the real data used in previous years for the environmental assessment of Emilia Romagna plans.\nConstraint Logic Programming showed an efficient management of large models and provides useful sensitivity analysis that can be used by planners and evaluators to assess multiple alternative scenarios. The drawback is that contributions from different activities and different impacts are merely summed, while in some cases more sophisticated combinations are required. We plan to investigate the use of other CLP languages, like CLP(FD), that allow for more general constraints.\nOn the other hand, the probabilistic model takes into consideration the subjective and stochastic nature of the provided data, paying the cost of a higher computational effort. In addition, some activity contributions and impacts should indeed be summed, while others are conveniently merged through probability laws.\nWe believe that computational logics can have a big impact in this field. One future work will be trying to merge the two models into a single component: in CL and AI, formalisms have been proposed that take constraints and probabilities under a same umbrella, like the Valued CSP model (Schiex et al. 1995) or the semiring framework (Bistarelli et al. 1997)."}], "references": [{"title": "Semiring-based constraint satisfaction and optimization", "author": ["S. Bistarelli", "U. Montanari", "F. Rossi"], "venue": "Journal of the ACM 44, 2, 201\u2013236.", "citeRegEx": "Bistarelli et al\\.,? 1997", "shortCiteRegEx": "Bistarelli et al\\.", "year": 1997}, {"title": "Probabilistic logic programs and their semantics", "author": ["E. Dantsin"], "venue": "Proceedings of the 2nd Russian Conference on Logic Programming, A. Voronkov, Ed. Lecture Notes in Computer Science, vol. 592. Springer, Berlin / Heidelberg, 152\u2013164.", "citeRegEx": "Dantsin,? 1991", "shortCiteRegEx": "Dantsin", "year": 1991}, {"title": "INCLP(R) - interval-based nonlinear constraint logic programming over the reals", "author": ["L. De Koninck", "T. Schrijvers", "B. Demoen"], "venue": "WLP, M. Fink, H. Tompits, and S. Woltran, Eds. INFSYS Research Report, vol. 1843-06-02. Technische Universit\u00e4t Wien, Austria, 91\u2013100.", "citeRegEx": "Koninck et al\\.,? 2006", "shortCiteRegEx": "Koninck et al\\.", "year": 2006}, {"title": "ProbLog: A probabilistic Prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, M. M. Veloso, Ed. 2462\u20132467.", "citeRegEx": "Raedt et al\\.,? 2007", "shortCiteRegEx": "Raedt et al\\.", "year": 2007}, {"title": "Probabilistic Datalog: Implementing logical information retrieval for advanced applications", "author": ["N. Fuhr"], "venue": "Journal of the American Society for Information Science 51, 2, 95\u2013110.", "citeRegEx": "Fuhr,? 2000", "shortCiteRegEx": "Fuhr", "year": 2000}, {"title": "Challenges for constraint reasoning and optimization in computational sustainability", "author": ["C.P. Gomes"], "venue": "Int.l Conference on Principles and Practice of Constraint Programming, I. P. Gent, Ed. Lecture Notes in Computer Science, vol. 5732. Springer, Berlin/Heidelberg, 2\u20134.", "citeRegEx": "Gomes,? 2009", "shortCiteRegEx": "Gomes", "year": 2009}, {"title": "An overview of the Ciao multiparadigm language and program development environment and its design philosophy", "author": ["M.V. Hermenegildo", "F. Bueno", "M. Carro", "P. L\u00f3pez-Gar\u0107\u0131a", "J.F. Morales", "G. Puebla"], "venue": "Concurrency, Graphs and Models, P. Degano, R. De Nicola, and J. Meseguer, Eds. Lecture Notes in Computer Science, vol. 5065. Springer, Berlin/Heidelberg, 209\u2013237.", "citeRegEx": "Hermenegildo et al\\.,? 2008", "shortCiteRegEx": "Hermenegildo et al\\.", "year": 2008}, {"title": "Constraint logic programming: A survey", "author": ["J. Jaffar", "M.J. Maher"], "venue": "Journal of Logic Programming 19/20, 503\u2013581.", "citeRegEx": "Jaffar and Maher,? 1994", "shortCiteRegEx": "Jaffar and Maher", "year": 1994}, {"title": "The CLP(R) language and system", "author": ["J. Jaffar", "S. Michaylov", "P.J. Stuckey", "R.H.C. Yap"], "venue": "ACM Transactions on Programming Languages and Systems 14, 3, 339\u2013395.", "citeRegEx": "Jaffar et al\\.,? 1992", "shortCiteRegEx": "Jaffar et al\\.", "year": 1992}, {"title": "CP-Logic theory inference with contextual variable elimination and comparison to BDD based inference methods", "author": ["W. Meert", "J. Struyf", "H. Blockeel"], "venue": "Proceedings of the 19th International Conference on Inductive Logic Programming, L. De Raedt, Ed. KU Leuven, Leuven, Belgium.", "citeRegEx": "Meert et al\\.,? 2009", "shortCiteRegEx": "Meert et al\\.", "year": 2009}, {"title": "Causality", "author": ["J. Pearl"], "venue": "Cambridge University Press.", "citeRegEx": "Pearl,? 2000", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "The independent choice logic for modelling multiple agents under uncertainty", "author": ["D. Poole"], "venue": "Artificial Intelligence 94, 1-2, 7\u201356.", "citeRegEx": "Poole,? 1997", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "A top down interpreter for LPAD and CP-logic", "author": ["F. Riguzzi"], "venue": "AI*IA 2007: Artificial Intelligence and Human-Oriented Computing, R. Basili and M. T. Pazienza, Eds. Lecture Notes in Artificial Intelligence, vol. 4733. Springer-Verlag, Berlin/Heidelberg, 109\u2013120.", "citeRegEx": "Riguzzi,? 2007", "shortCiteRegEx": "Riguzzi", "year": 2007}, {"title": "Inference with logic programs with annotated disjunctions under the well founded semantics", "author": ["F. Riguzzi"], "venue": "International Conference on Logic Programming, M. G. de la Banda and E. Pontelli, Eds. Lecture Notes in Computer Science, vol. 5366. Springer, Berlin/Heidelberg, 667\u2013771.", "citeRegEx": "Riguzzi,? 2008", "shortCiteRegEx": "Riguzzi", "year": 2008}, {"title": "Extended semantics and inference for the Independent Choice Logic", "author": ["F. Riguzzi"], "venue": "Logic Journal of the IGPL 17, 6, 589\u2013629.", "citeRegEx": "Riguzzi,? 2009", "shortCiteRegEx": "Riguzzi", "year": 2009}, {"title": "PRISM: A language for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "International Joint Conference on Artificial Intelligence, M. E. Pollack, Ed. Morgan Kaufmann, 1330\u20131339.", "citeRegEx": "Sato and Kameya,? 1997", "shortCiteRegEx": "Sato and Kameya", "year": 1997}, {"title": "Valued constraint satisfaction problems: Hard and easy problems", "author": ["T. Schiex", "H. Fargier", "G. Verfaillie"], "venue": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI 95. Vol. 1. Morgan Kaufmann, 631\u2013639.", "citeRegEx": "Schiex et al\\.,? 1995", "shortCiteRegEx": "Schiex et al\\.", "year": 1995}, {"title": "Eplex: Harnessing mathematical programming solvers for constraint logic programming", "author": ["K. Shen", "J. Schimpf"], "venue": "Principles and Practice of Constraint Programming - CP 2005, P. van Beek, Ed. Lecture Notes in Computer Science, vol. 3709. SpringerVerlag, Berlin/Heidelberg, 622\u2013636.", "citeRegEx": "Shen and Schimpf,? 2005", "shortCiteRegEx": "Shen and Schimpf", "year": 2005}, {"title": "Procedures and programs to assist in the impact statement process", "author": ["J.C. Sorensen", "M.L. Moss"], "venue": "Tech. rep., Univ. of California, Berkely.", "citeRegEx": "Sorensen and Moss,? 1973", "shortCiteRegEx": "Sorensen and Moss", "year": 1973}, {"title": "CP-logic: A language of causal probabilistic events and its relation to logic programming", "author": ["J. Vennekens", "M. Denecker", "M. Bruynooghe"], "venue": "Theory and Practice of Logic Programming 9, 3, 245\u2013308.", "citeRegEx": "Vennekens et al\\.,? 2009", "shortCiteRegEx": "Vennekens et al\\.", "year": 2009}, {"title": "Logic programs with annotated disjunctions", "author": ["J. Vennekens", "S. Verbaeten", "M. Bruynooghe"], "venue": "International Conference on Logic Programming, B. Demoen and V. Lifschitz, Eds. Lecture Notes in Computer Science, vol. 3131. Springer, Heidelberg, Germany, 195\u2013209.", "citeRegEx": "Vennekens et al\\.,? 2004", "shortCiteRegEx": "Vennekens et al\\.", "year": 2004}, {"title": "BProlog: A high performance Prolog compiler", "author": ["Zhou", "N.-F.", "I. Nagasawa", "M. Umeda", "K. Katamine", "T. Hirota"], "venue": "IEA/AIE, T. Tanaka, S. Ohsuga, and M. Ali, Eds. Gordon and Breach Science Publishers, 790.", "citeRegEx": "Zhou et al\\.,? 1996", "shortCiteRegEx": "Zhou et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "Computational Sustainability (Gomes 2009) is a very recent, interdisciplinary research field that aims to apply techniques from computer science, information science, operations research, applied mathematics, and statistics to the problem of balancing environmental, economic, and societal needs for sustainable development.", "startOffset": 29, "endOffset": 41}, {"referenceID": 18, "context": "One of the instruments used for assessing a regional plan in Emilia Romagna are the so-called coaxial matrices, that are a development of the network method (Sorensen and Moss 1973).", "startOffset": 157, "endOffset": 181}, {"referenceID": 7, "context": "Constraint Logic Programming (CLP) (Jaffar and Maher 1994) is a class of programming languages which extend classical Logic Programming.", "startOffset": 35, "endOffset": 58}, {"referenceID": 8, "context": "For example, we can have CLP(R) (Jaffar et al. 1992), on the sort of real values, or CLP(FD), in which variables range on finite domains.", "startOffset": 32, "endOffset": 52}, {"referenceID": 21, "context": "2006), and many Prolog flavours (Zhou et al. 1996; Hermenegildo et al. 2008) have their own CLP(R) library.", "startOffset": 32, "endOffset": 76}, {"referenceID": 6, "context": "2006), and many Prolog flavours (Zhou et al. 1996; Hermenegildo et al. 2008) have their own CLP(R) library.", "startOffset": 32, "endOffset": 76}, {"referenceID": 17, "context": "We decided to adopt ECLPS, that features a library called Eplex (Shen and Schimpf 2005).", "startOffset": 64, "endOffset": 87}, {"referenceID": 1, "context": "The integration of logic and probability has been widely studied in Logic Programming and various languages semantics have been proposed, such as Probabilistic Logic Programs (Dantsin 1991), Independent Choice Logic (Poole 1997), PRISM (Sato and Kameya 1997), pD (Fuhr 2000), CLP(BN) (Santos Costa et al.", "startOffset": 175, "endOffset": 189}, {"referenceID": 11, "context": "The integration of logic and probability has been widely studied in Logic Programming and various languages semantics have been proposed, such as Probabilistic Logic Programs (Dantsin 1991), Independent Choice Logic (Poole 1997), PRISM (Sato and Kameya 1997), pD (Fuhr 2000), CLP(BN) (Santos Costa et al.", "startOffset": 216, "endOffset": 228}, {"referenceID": 15, "context": "The integration of logic and probability has been widely studied in Logic Programming and various languages semantics have been proposed, such as Probabilistic Logic Programs (Dantsin 1991), Independent Choice Logic (Poole 1997), PRISM (Sato and Kameya 1997), pD (Fuhr 2000), CLP(BN) (Santos Costa et al.", "startOffset": 236, "endOffset": 258}, {"referenceID": 4, "context": "The integration of logic and probability has been widely studied in Logic Programming and various languages semantics have been proposed, such as Probabilistic Logic Programs (Dantsin 1991), Independent Choice Logic (Poole 1997), PRISM (Sato and Kameya 1997), pD (Fuhr 2000), CLP(BN) (Santos Costa et al.", "startOffset": 263, "endOffset": 274}, {"referenceID": 20, "context": "Logic Programs with Annotated Disjunctions (LPADs) (Vennekens et al. 2004) are particularly suitable for reasoning about causes and effects (Vennekens et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 19, "context": "2004) are particularly suitable for reasoning about causes and effects (Vennekens et al. 2009).", "startOffset": 71, "endOffset": 94}, {"referenceID": 19, "context": "The semantics of a non-ground theory T is defined through its grounding g(T ) and Vennekens et al. (2004) require that g(T ) is finite.", "startOffset": 82, "endOffset": 106}, {"referenceID": 12, "context": "Riguzzi (2008) discusses an extension of SLG resolution, called SLGAD, that is able to compute the probability of queries by repeatedly branching on", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "2 because of its speed (Riguzzi 2009).", "startOffset": 23, "endOffset": 37}, {"referenceID": 8, "context": "A different approach was taken by Meert et al. (2009), where an LPAD is first transformed into its equivalent Bayesian network and then inference is performed on the network using the variable elimination algorithm.", "startOffset": 34, "endOffset": 54}, {"referenceID": 8, "context": "A different approach was taken by Meert et al. (2009), where an LPAD is first transformed into its equivalent Bayesian network and then inference is performed on the network using the variable elimination algorithm. Riguzzi (2007) presents the cplint system that first finds explanations (sets of atomic choices) for queries and then computes the probability by means of Binary Decision Diagrams, as proposed in (De Raedt et al.", "startOffset": 34, "endOffset": 231}, {"referenceID": 10, "context": "The technique proposed in (Pearl 2000) for computing P (y|do(x)) consists of removing the parents of X from a causal Bayesian network, setting X to x and computing P (y) in the obtained network.", "startOffset": 26, "endOffset": 38}, {"referenceID": 10, "context": "Recently, Pearl (2000) helped to clarify the concept of causation by discussing how to represent causal information and how to perform inference from it.", "startOffset": 10, "endOffset": 23}, {"referenceID": 10, "context": "Recently, Pearl (2000) helped to clarify the concept of causation by discussing how to represent causal information and how to perform inference from it. He illustrates two types of causal models: causal Bayesian networks and structural equations. Causal Bayesian networks differ from standard Bayesian networks because the edge from variable X to variable Y means that X is a cause for Y , while in standard Bayesian networks it simply means that there is a statistical dependence. Pearl (2000) proposed an approach for computing the probability of effects of actions and suggested to use the notation P (y|do(x)) to indicate the effect of the action of setting the variable X to value x on the event of variable Y taking value y.", "startOffset": 10, "endOffset": 496}, {"referenceID": 16, "context": "One future work will be trying to merge the two models into a single component: in CL and AI, formalisms have been proposed that take constraints and probabilities under a same umbrella, like the Valued CSP model (Schiex et al. 1995) or the semiring framework (Bistarelli et al.", "startOffset": 213, "endOffset": 233}, {"referenceID": 0, "context": "1995) or the semiring framework (Bistarelli et al. 1997).", "startOffset": 32, "endOffset": 56}], "year": 2010, "abstractText": "Strategic Environmental Assessment is a procedure aimed at introducing systematic assessment of the environmental effects of plans and programs. This procedure is based on the so-called coaxial matrices that define dependencies between plan activities (infrastructures, plants, resource extractions, buildings, etc.) and positive and negative environmental impacts, and dependencies between these impacts and environmental receptors. Up to now, this procedure is manually implemented by environmental experts for checking the environmental effects of a given plan or program, but it is never applied during the plan/program construction. A decision support system, based on a clear logic semantics, would be an invaluable tool not only in assessing a single, already defined plan, but also during the planning process in order to produce an optimized, environmentally assessed plan and to study possible alternative scenarios. We propose two logic-based approaches to the problem, one based on Constraint Logic Programming and one on Probabilistic Logic Programming that could be, in the future, conveniently merged to exploit the advantages of both. We test the proposed approaches on a real energy plan and we discuss their limitations and advantages.", "creator": "LaTeX with hyperref package"}}}