{"id": "1011.0097", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2010", "title": "Sparse Inverse Covariance Selection via Alternating Linearization Methods", "abstract": "Since the conditional dependencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data by solving a convex highest probability problem with a $\\ ell _ 1 $regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the specific structure of the problem; in particular, the partial problems solved in each iteration have closed solutions. In addition, our algorithm receives a $\\ epsilon $optimal solution in $O (1 /\\ epsilon) $itations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.", "histories": [["v1", "Sat, 30 Oct 2010 18:30:43 GMT  (185kb)", "http://arxiv.org/abs/1011.0097v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["katya scheinberg", "shiqian ma", "donald goldfarb"], "accepted": true, "id": "1011.0097"}, "pdf": {"name": "1011.0097.pdf", "metadata": {"source": "CRF", "title": "Sparse Inverse Covariance Selection via Alternating Linearization Methods", "authors": ["Katya Scheinberg"], "emails": ["katyas@lehigh.edu", "sm2756@columbia.edu", "goldfarb@columbia.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n01 1.\n00 97\nv1 [\ncs .L\nG ]\n3 0\nO ct"}, {"heading": "1 Introduction", "text": "In multivariate data analysis, graphical models such as Gaussian Markov Random Fields provide a way to discover meaningful interactions among variables. Let Y = {y(1), . . . , y(n)} be an n-dimensional random vector following an n-variate Gaussian distribution N (\u00b5,\u03a3), and let G = (V,E) be a Markov network representing the conditional independence structure of N (\u00b5,\u03a3). Specifically, the set of vertices V = {1, . . . , n} corresponds to the set of variables in Y , and the edge set E contains an edge (i, j) if and only if y(i) is conditionally dependent on y(j) given all remaining variables; i.e., the lack of an edge between i and j denotes the conditional independence of y(i) and y(j), which corresponds to a zero entry in the inverse covariance matrix \u03a3\u22121 ([1]). Thus learning the structure of this graphical model is equivalent to the problem of learning the zero-pattern of \u03a3\u22121. To estimate this sparse inverse covariance matrix, one can solve the following sparse inverse covariance selection (SICS) problem: maxX\u2208Sn ++ log det(X) \u2212 \u3008\u03a3\u0302, X\u3009 \u2212 \u03c1\u2016X\u20160, where Sn++ denotes the set of n \u00d7 n positive definite matrices, \u2016X\u20160 is the number of nonzeros in X , \u03a3\u0302 = 1\np\n\u2211p i=1(Yi \u2212 \u03b2\u0302)(Yi \u2212 \u03b2\u0302)\u22a4 is the sample covariance matrix, \u03b2\u0302 = 1p \u2211p\ni=1 Yi is the sample mean and Yi is the i-th random sample of Y . This problem is NP-hard in general due to the combinatorial nature of the cardinality term \u03c1\u2016X\u20160 ([2]). To get a numerically tractable problem, one can replace the cardinality term \u2016X\u20160 by \u2016X\u20161 := \u2211\ni,j |Xij |, the envelope of \u2016X\u20160 over the set {X \u2208 Rn\u00d7n : \u2016X\u2016\u221e \u2264 1} (see [3]). This results in the convex optimization problem (see e.g., [4, 5, 6, 7]):\nmin X\u2208Sn\n++\n\u2212 log det(X) + \u3008\u03a3\u0302, X\u3009+ \u03c1\u2016X\u20161. (1)\nNote that (1) can be rewritten as minX\u2208Sn ++ max\u2016U\u2016\u221e\u2264\u03c1\u2212 log detX + \u3008\u03a3\u0302 + U,X\u3009, where \u2016U\u2016\u221e is the largest absolute value of the entries of U . By exchanging the order of max and min, we obtain\nthe dual problem max\u2016U\u2016\u221e\u2264\u03c1minX\u2208Sn++ \u2212 log detX + \u3008\u03a3\u0302 + U,X\u3009, which is equivalent to\nmax W\u2208Sn\n++\n{log detW + n : \u2016W \u2212 \u03a3\u0302\u2016\u221e \u2264 \u03c1}. (2)\nBoth the primal and dual problems have strictly convex objectives; hence, their optimal solutions are unique. Given a dual solution W , X = W\u22121 is primal feasible resulting in the duality gap\ngap := \u3008\u03a3\u0302,W\u22121\u3009+ \u03c1\u2016W\u22121\u20161 \u2212 n. (3)\nThe primal and the dual SICS problems (1) and (2) are semidefinite programming problems and can be solved via interior point methods (IPMs) in polynomial time. However, the per-iteration computational cost and memory requirements of an IPM are prohibitively high for the SICS problem. Although an approximate IPM has recently been proposed for the SICS problem [8], most of the methods developed for it are first-order methods. Banerjee et al. [7] proposed a block coordinate descent (BCD) method to solve the dual problem (2). Their method updates one row and one column of W in each iteration by solving a convex quadratic programming problem by an IPM. The glasso method of Friedman et al. [5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9]. Sun et al. [10] proposed solving the primal problem (1) by using a BCD method. They formulate the subproblem as a min-max problem and solve it using a prox method proposed by Nemirovski [11]. The SINCO method proposed by Scheinberg and Rish [12] is a greedy CD method applied to the primal problem. All of these BCD and CD approaches lack iteration complexity bounds. They also have been shown to be inferior in practice to gradient based approaches. A projected gradient method for solving the dual problem (2) that is considered to be state-of-the-art for SICS was proposed by Duchi et al. [13]. However, there are no iteration complexity results for it either. Variants of Nesterov\u2019s method [14, 15] have been applied to solve the SICS problem. d\u2019Aspremont et al. [16] applied Nesterov\u2019s optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth \u21131 term, obtaining an iteration complexity bound of O(1/\u01eb) for an \u01eb-optimal solution, but the implementation in [16] was very slow and did not produce good results. Lu [17] solved the dual problem (2), which is a smooth problem, by Nesterov\u2019s algorithm, and improved the iteration complexity to O(1/ \u221a \u01eb). However, since the practical performance of this algorithm was not attractive, Lu gave a variant (VSM) of it that exhibited better performance. The iteration complexity of VSM is unknown. Yuan [18] proposed an alternating direction method based on an augmented Lagrangian framework (see the ADAL method (8) below). This method also lacks complexity results. The proximal point algorithm proposed by Wang et al. in [19] requires a reformulation of the problem that increases the size of the problem making it impractical for solving large-scale problems. Also, there is no iteration complexity bound for this algorithm. The IPM in [8] also requires such a reformulation.\nOur contribution. In this paper, we propose an alternating linearization method (ALM) for solving the primal SICS problem. An advantage of solving the primal problem is that the \u21131 penalty term in the objective function directly promotes sparsity in the optimal inverse covariance matrix.\nAlthough developed independently, our method is closely related to Yuan\u2019s method [18]. Both methods exploit the special form of the primal problem (1) by alternatingly minimizing one of the terms of the objective function plus an approximation to the other term. The main difference between the two methods is in the construction of these approximations. As we will show, our method has a theoretically justified interpretation and is based on an algorithmic framework with complexity bounds, while no complexity bound is available for Yuan\u2019s method. Also our method has an intuitive interpretation from a learning perspective. Extensive numerical test results on both synthetic data and real problems have shown that our ALM algorithm significantly outperforms other existing algorithms, such as the PSM algorithm proposed by Duchi et al. [13] and the VSM algorithm proposed by Lu [17]. Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].\nOrganization of the paper. In Section 2 we briefly review alternating linearization methods for minimizing the sum of two convex functions and establish convergence and iteration complexity results. We show how to use ALM to solve SICS problems and give intuition from a learning perspective in Section 3. Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17]."}, {"heading": "2 Alternating Linearization Methods", "text": "We consider here the alternating linearization method (ALM) for solving the following problem:\nmin F (x) \u2261 f(x) + g(x), (4) where f and g are both convex functions. An effective way to solve (4) is to \u201csplit\u201d f and g by introducing a new variable, i.e., to rewrite (4) as\nmin x,y\n{f(x) + g(y) : x\u2212 y = 0}, (5)\nand apply an alternating direction augmented Lagrangian method to it. Given a penalty parameter 1/\u00b5, at the k-th iteration, the augmented Lagrangian method minimizes the augmented Lagrangian function\nL(x, y;\u03bb) := f(x) + g(y)\u2212 \u3008\u03bb, x \u2212 y\u3009+ 1 2\u00b5 \u2016x\u2212 y\u201622,\nwith respect to x and y, i.e., it solves the subproblem\n(xk, yk) := argmin x,y L(x, y;\u03bbk), (6)\nand updates the Lagrange multiplier \u03bb via:\n\u03bbk+1 := \u03bbk \u2212 (xk \u2212 yk)/\u00b5. (7) Since minimizing L(x, y;\u03bb) with respect to x and y jointly is usually difficult, while doing so with respect to x and y alternatingly can often be done efficiently, the following alternating direction version of the augmented Lagrangian method (ADAL) is often advocated (see, e.g., [20, 21]):\n\n\n xk+1 := argminx L(x, yk;\u03bbk) yk+1 := argminy L(xk+1, y;\u03bbk) \u03bbk+1 := \u03bbk \u2212 (xk+1 \u2212 yk+1)/\u00b5.\n(8)\nIf we also update \u03bb after we solve the subproblem with respect to x, we get the following symmetric version of the ADAL method.\n\n \n  \nxk+1 := argminx L(x, yk;\u03bbky) \u03bbk+1x := \u03bb k y \u2212 (xk+1 \u2212 yk)/\u00b5 yk+1 := argminy L(xk+1, y;\u03bbk+1x ) \u03bbk+1y := \u03bb k+1 x \u2212 (xk+1 \u2212 yk+1)/\u00b5.\n(9)\nAlgorithm (9) has certain theoretical advantages when f and g are smooth. In this case, from the first-order optimality conditions for the two subproblems in (9), we have that:\n\u03bbk+1x = \u2207f(xk+1) and \u03bbk+1y = \u2212\u2207g(yk+1). (10) Substituting these relations into (9), we obtain the following equivalent algorithm for solving (4), which we refer to as the alternating linearization minimization (ALM) algorithm.\nAlgorithm 1 Alternating linearization method (ALM) for smooth problem\nInput: x0 = y0 for k = 0, 1, \u00b7 \u00b7 \u00b7 do 1. Solve xk+1 := argminxQg(x, yk) \u2261 f(x) + g(yk) + \u2329 \u2207g(yk), x\u2212 yk \u232a\n+ 12\u00b5\u2016x\u2212 yk\u201622; 2. Solve yk+1 := argminy Qf (xk+1, y) \u2261 f(xk+1) + \u2329 \u2207f(xk+1), y \u2212 xk+1 \u232a\n+ 12\u00b5\u2016y \u2212 xk+1\u201622 + g(y);\nend for\nAlgorithm 1 can be viewed in the following way: at each iteration we construct a quadratic approximation of the function g(x) at the current iterate yk and minimize the sum of this approximation and f(x). The approximation is based on linearizing g(x) (hence the name ALM) and adding a \u201cprox\u201d term 12\u00b5\u2016x\u2212 yk\u201622. When \u00b5 is small enough (\u00b5 \u2264 1/L(g), where L(g) is the Lipschitz constant for\n\u2207g) this quadratic function, g(yk)+ \u2329 \u2207g(yk), x \u2212 yk \u232a + 12\u00b5\u2016x\u2212yk\u201622 is an upper approximation to g(x), which means that the reduction in the value of F (x) achieved by minimizingQg(x, yk) in Step 1 is not smaller than the reduction achieved in the value of Qg(x, yk) itself. Similarly, in Step 2 we build an upper approximation to f(x) at xk+1, f(xk+1)+ \u2329 \u2207f(xk+1), y \u2212 xk+1 \u232a\n+ 12\u00b5\u2016y\u2212xk+1\u201622, and minimize the sum Qf (xk+1, y) of it and g(y).\nLet us now assume that f(x) is in the class C1,1 with Lipschitz constant L(f), while g(x) is simply convex. Then from the first-order optimality conditions for the second minimization in (9), we have \u2212\u03bbk+1y \u2208 \u2202g(yk+1), the subdifferential of g(y) at y = yk+1. Hence, replacing \u2207g(yk) in the definition of Qg(x, yk) by \u2212\u03bbk+1y in (9), we obtain the following modified version of (9).\nAlgorithm 2 Alternating linearization method with skipping step\nInput: x0 = y0 for k = 0, 1, \u00b7 \u00b7 \u00b7 do 1. Solve xk+1 := argminxQ(x, yk) \u2261 f(x) + g(yk)\u2212 \u2329 \u03bbk, x\u2212 yk \u232a\n+ 12\u00b5\u2016x\u2212 yk\u201622; 2. If F (xk+1) > Q(xk+1, yk) then xk+1 := yk. 3. Solve yk+1 := argminy Qf(xk+1, y); 4. \u03bbk+1 = \u2207f(xk+1)\u2212 (xk+1 \u2212 yk+1)/\u00b5.\nend for\nAlgorithm 2 is identical to the symmetric ADAL algorithm (9) as long as F (xk+1) \u2264 Q(xk+1, yk) at each iteration (and to Algorithm 1 if g(x) is in C1,1 and \u00b5 \u2264 1/max{L(f), L(g)}). If this condition fails, then the algorithm simply sets xk+1 \u2190 yk. Algorithm 2 has the following convergence property and iteration complexity bound. For a proof see the Appendix.\nTheorem 2.1. Assume \u2207f is Lipschitz continuous with constant L(f). For \u03b2/L(f) \u2264 \u00b5 \u2264 1/L(f) where 0 < \u03b2 \u2264 1, Algorithm 2 satisfies\nF (yk)\u2212 F (x\u2217) \u2264 \u2016x 0 \u2212 x\u2217\u20162\n2\u00b5(k + kn) , \u2200k, (11)\nwhere x\u2217 is an optimal solution of (4) and kn is the number of iterations until the k \u2212 th for which F (xk+1) \u2264 Q(xk+1, yk). Thus Algorithm 2 produces a sequence which converges to the optimal solution in function value, and the number of iterations needed is O(1/\u01eb) for an \u01eb-optimal solution.\nIf g(x) is also a smooth function in the class C1,1 with Lipschitz constantL(g) \u2264 1/\u00b5, then Theorem 2.1 also applies to Algorithm 1 since in this case kn = k (i.e., no \u201cskipping\u201d occurs). Note that the iteration complexity bound in Theorem 2.1 can be improved. Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O(1/ \u221a \u01eb), using only first-order information. His acceleration technique is based on using a linear combination of previous iterates to obtain a point where the approximation is built. This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al. [25] and many others. A similar technique can be adopted to derive a fast version of Algorithm 2 that has an improved complexity bound of O(1/ \u221a \u01eb), while keeping the computational effort in each iteration almost unchanged. However, we do not present this method here, since when applied to the SICS problem, it did not work as well as Algorithm 2."}, {"heading": "3 ALM for SICS", "text": "The SICS problem\nmin X\u2208Sn\n++\nF (X) \u2261 f(X) + g(X), (12)\nwhere f(X) = \u2212 log det(X) + \u3008\u03a3\u0302, X\u3009 and g(X) = \u03c1\u2016X\u20161, is of the same form as (4). However, in this case neither f(X) nor g(X) have Lipschitz continuous gradients. Moreover, f(X) is only defined for positive definite matrices while g(X) is defined everywhere. These properties of the objective function make the SICS problem especially challenging for optimization methods. Nevertheless, we can still apply (9) to solve the problem directly. Moreover, we can apply Algorithm 2 and obtain the complexity bound in Theorem 2.1 as follows.\nThe log det(X) term in f(X) implicitly requires that X \u2208 Sn++ and the gradient of f(X), which is given by \u2212X\u22121 + \u03a3\u0302, is not Lipschitz continuous in Sn++. Fortunately, as proved in Proposition 3.1 in [17], the optimal solution of (12) X\u2217 \u03b1I , where \u03b1 = 1\n\u2016\u03a3\u0302\u2016+n\u03c1 . Therefore, if we define\nC := {X \u2208 Sn : X \u03b12 I}, the SICS problem (12) can be formulated as: min X,Y {f(X) + g(Y ) : X \u2212 Y = 0, X \u2208 C, Y \u2208 C}. (13)\nWe can include constraints X \u2208 C in Step 1 and Y \u2208 C in Step 3 of Algorithm 2. Theorem 2.1 can then be applied as discussed in [25]. However, a difficulty now arises when performing the minimization in Y . Without the constraint Y \u2208 C, only a matrix shrinkage operation is needed, but with this additional constraint the problem becomes harder to solve. Minimization in X with or without the constraint X \u2208 C is accomplished by performing an SVD. Hence the constraint can be easily imposed.\nInstead of imposing constraint Y \u2208 C we can obtain feasible solutions by a line search on \u00b5. We know that the constraint X \u03b12 I is not tight at the solution. Hence if we start the algorithm with X \u03b1I and restrict the step size \u00b5 to be sufficiently small then the iterates of the method will remain in C. Note however, that the bound on the Lipschitz constant of the gradient of f(X) is 1/\u03b12 and hence can be very large. It is not practical to restrict \u00b5 in the algorithm to be smaller than \u03b12, since \u00b5 determines the step size at each iteration. Hence, for a practical approach we can only claim that the theoretical convergence rate bound holds in only a small neighborhood of the optimal solution. We now present a practical version of our algorithm applied to the SICS problem.\nAlgorithm 3 Alternating linearization method (ALM) for SICS\nInput: X0 = Y 0, \u00b50. for k = 0, 1, \u00b7 \u00b7 \u00b7 do\n0. Pick \u00b5k+1 \u2264 \u00b5k. 1. Solve Xk+1 := argminX\u2208C f(X) + g(Y k)\u2212 \u3008\u039bk, X \u2212 Y k\u3009+ 12\u00b5k+1 \u2016X \u2212 Y\nk\u20162F ; 2. If g(Xk+1) > g(Y k)\u2212 \u3008\u039bk, Xk+1 \u2212 Y k\u3009+ 12\u00b5k+1 \u2016X\nk+1 \u2212 Y k\u20162F , then Xk+1 := Y k. 3. Solve Y k+1 := argminY f(Xk+1) + \u3008\u2207f(Xk+1), Y \u2212Xk+1\u3009 + 12\u00b5k+1 \u2016Y \u2212X\nk+1\u20162F + g(Y ); 4. \u039bk+1 = \u2207f(Xk+1)\u2212 (Xk+1 \u2212 Y k+1)/\u00b5k+1.\nend for\nWe now show how to solve the two optimization problems in Algorithm 3. The first-order optimality conditions for Step 1 in Algorithm 3, ignoring the constraint X \u2208 C are:\n\u2207f(X)\u2212 \u039bk + (X \u2212 Y k)/\u00b5k+1 = 0. (14) Consider V Diag(d)V \u22a4 - the spectral decomposition of Y k + \u00b5k+1(\u039bk \u2212 \u03a3\u0302) and let\n\u03b3i =\n(\ndi + \u221a d2i + 4\u00b5k+1\n)\n/2, i = 1, . . . , n. (15)\nSince \u2207f(X) = \u2212X\u22121 + \u03a3\u0302, it is easy to verify that Xk+1 := V Diag(\u03b3)V \u22a4 satisfies (14). When the constraint X \u2208 C is imposed, the optimal solution changes to Xk+1 := V Diag(\u03b3)V \u22a4 with \u03b3i = max { \u03b1/2, ( di + \u221a d2i + 4\u00b5k+1 ) /2 } , i = 1, . . . , n. We observe that solving (14) requires approximately the same effort (O(n3)) as is required to compute \u2207f(Xk+1). Moreover, from the solution to (14), \u2207f(Xk+1) is obtained with only a negligible amount of additional effort, since (Xk+1)\u22121 := V Diag(\u03b3)\u22121V \u22a4.\nThe first-order optimality conditions for Step 2 in Algorithm 3 are:\n0 \u2208 \u2207f(Xk+1) + (Y \u2212Xk+1)/\u00b5k+1 + \u2202g(Y ). (16) Since g(Y ) = \u03c1\u2016Y \u20161, it is well known that the solution to (16) is given by\nY k+1 = shrink(Xk+1 \u2212 \u00b5k+1(\u03a3\u0302\u2212 (Xk+1)\u22121), \u00b5k+1\u03c1),\nwhere the \u201cshrinkage operator\u201d shrink(Z, \u03c1) updates each element Zij of the matrix Z by the formula shrink(Z, \u03c1)ij = sgn(Zij) \u00b7max{|Zij | \u2212 \u03c1, 0}. The O(n3) complexity of Step 1, which requires a spectral decomposition, dominates the O(n2) complexity of Step 2 which requires a simple shrinkage. There is no closed-form solution for the subproblem corresponding to Y when the constraint Y \u2208 C is imposed. Hence, we neither impose this constraint explicitly nor do so by a line search on \u00b5k, since in practice this degrades the performance of the algorithm substantially. Thus, the resulting iterates Y k may not be positive definite, while the iterates Xk remain so. Eventually due to the convergence of Y k and Xk, the Y k iterates become positive definite and the constraint Y \u2208 C is satisfied. Let us now remark on the learning based intuition behind Algorithm 3. We recall that \u2212\u039bk \u2208 \u2202g(Y k). The two steps of the algorithm can be written as\nXk+1 := arg min X\u2208C {f(X) + 1 2\u00b5k+1 \u2016X \u2212 (Y k + \u00b5k+1\u039bk)\u20162F } (17)\nand\nY k+1 := argmin Y {g(Y ) + 1 2\u00b5k+1 \u2016Y \u2212 (Xk+1 \u2212 \u00b5k+1(\u03a3\u0302\u2212 (Xk+1)\u22121))\u20162F }. (18)\nThe SICS problem is trying to optimize two conflicting objectives: on the one hand it tries to find a covariance matrix X\u22121 that best fits the observed data, i.e., is as close to \u03a3\u0302 as possible, and on the other hand it tries to obtain a sparse matrix X . The proposed algorithm address these two objectives in an alternating manner. Given an initial \u201cguess\u201d of the sparse matrix Y k we update this guess by a subgradient descent step of length \u00b5k+1: Y k + \u00b5k+1\u039bk. Recall that \u2212\u039bk \u2208 \u2202g(Y k). Then problem (17) seeks a solution X that optimizes the first objective (best fit of the data) while adding a regularization term which imposes a Gaussian prior on X whose mean is the current guess for the sparse matrix: Y k+\u00b5k+1\u039bk. The solution to (17) gives us a guess for the inverse covariance Xk+1. We again update it by taking a gradient descent step: Xk+1\u2212\u00b5k+1(\u03a3\u0302\u2212 (Xk+1)\u22121). Then problem (18) seeks a sparse solution Y while also imposing a Gaussian prior on Y whose mean is the guess for the inverse covariance matrix Xk+1 \u2212 \u00b5k+1(\u03a3\u0302 \u2212 (Xk+1)\u22121). Hence the sequence of Xk\u2019s is a sequence of positive definite inverse covariance matrices that converge to a sparse matrix, while the sequence of Y k\u2019s is a sequence of sparse matrices that converges to a positive definite inverse covariance matrix.\nAn important question is how to pick \u00b5k+1. Theory tells us that if we pick a small enough value, then we can obtain the complexity bounds. However, in practice this value is too small. We discuss the simple strategy that we use in the next section."}, {"heading": "4 Numerical Experiments", "text": "In this section, we present numerical results on both synthetic and real data to demonstrate the efficiency of our SICS ALM algorithm. Our codes for ALM were written in MATLAB. All numerical experiments were run in MATLAB 7.3.0 on a Dell Precision 670 workstation with an Intel Xeon(TM) 3.4GHZ CPU and 6GB of RAM.\nSince \u2212\u039bk \u2208 \u2202g(Y k), \u2016\u039bk\u2016\u221e \u2264 \u03c1; hence \u03a3\u0302\u2212 \u039bk is a feasible solution to the dual problem (2) as long as it is positive definite. Thus the duality gap at the k-th iteration is given by:\nDgap := \u2212 log det(Xk) + \u3008\u03a3\u0302, Xk\u3009+ \u03c1\u2016Xk\u20161 \u2212 log det(\u03a3\u0302\u2212 \u039bk)\u2212 n. (19) We define the relative duality gap as: Rel.gap := Dgap/(1+ |pobj|+ |dobj|), where pobj and dobj are respectively the objective function values of the primal problem (12) at point Xk, and the dual problem (2) at \u03a3\u0302 \u2212 \u039bk. Defining dk(\u03c6(x)) \u2261 max{1, \u03c6(xk), \u03c6(xk\u22121)}, we measure the relative changes of objective function value F (X) and the iterates X and Y as follows:\nFrel := |F (Xk)\u2212 F (Xk\u22121)|\ndk(|F (X)|) , Xrel := \u2016Xk \u2212Xk\u22121\u2016F dk(\u2016X\u2016F ) , Y rel := \u2016Y k \u2212 Y k\u22121\u2016F d(\u2016Y \u2016F ) .\nWe terminate ALM when either\n(i) Dgap \u2264 \u01ebgap or (ii) max{Frel,Xrel, Y rel} \u2264 \u01ebrel. (20)\nNote that in (19), computing log det(Xk) is easy since the spectral decomposition of Xk is already available (see (14) and (15)), but computing log det(\u03a3\u0302 \u2212 \u039bk) requires another expensive spectral decomposition. Thus, in practice, we only check (20)(i) every Ngap iterations. We check (20)(ii) at every iteration since this is inexpensive.\nA continuation strategy for updating \u00b5 is also crucial to ALM. In our experiments, we adopted the following update rule. After every N\u00b5 iterations, we set \u00b5 := max{\u00b5 \u00b7\u03b7\u00b5, \u00b5\u0304}; i.e., we simply reduce \u00b5 by a constant factor \u03b7\u00b5 every N\u00b5 iterations until a desired lower bound on \u00b5 is achieved.\nWe compare ALM (i.e., Algorithm 3 with the above stopping criteria and \u00b5 updates), with the projected subgradient method (PSM) proposed by Duchi et al. in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems. The per-iteration complexity of all three algorithms is roughly the same; hence a comparison of the number of iterations is meaningful. The parameters used in PSM and VSM are set at their default values. We used the following parameter values in ALM: \u01ebgap = 10\u22123, \u01ebrel = 10\u22128, Ngap = 20, N\u00b5 = 20, \u00b5\u0304 = max{\u00b50\u03b78\u00b5, 10\u22126}, \u03b7\u00b5 = 1/3, where \u00b50 is the initial \u00b5 which is set according to \u03c1; specifically, in our experiments, \u00b50 = 100/\u03c1, if \u03c1 < 0.5, \u00b50 = \u03c1 if 0.5 \u2264 \u03c1 \u2264 10, and \u00b50 = \u03c1/100 if \u03c1 > 10."}, {"heading": "4.1 Experiments on synthetic data", "text": "We randomly created test problems using a procedure proposed by Scheinberg and Rish in [12]. Similar procedures were used by Wang et al. in [19] and Li and Toh in [8]. For a given dimension n, we first created a sparse matrix U \u2208 Rn\u00d7n with nonzero entries equal to -1 or 1 with equal probability. Then we computed S := (U \u2217 U\u22a4)\u22121 as the true covariance matrix. Hence, S\u22121 was sparse. We then drew p = 5n iid vectors, Y1, . . . , Yp, from the Gaussian distribution N (0, S) by using the mvnrnd function in MATLAB, and computed a sample covariance matrix \u03a3\u0302 := 1\np \u2211p i=1 YiY \u22a4 i .\nWe compared ALM with PSM [13] and VSM [17] on these randomly created data with different \u03c1. The PSM code was terminated using its default stopping criteria, which included (20)(i) with \u01ebgap = 10\n\u22123. VSM was also terminated when Dgap \u2264 10\u22123. Since PSM and VSM solve the dual problem (2), the duality gap which is given by (3) is available without any additional spectral decompositions. The results are shown in Table 1. All CPU times reported are in seconds.\nFrom Table 1 we see that on these randomly created SICS problems, ALM outperforms PSM and VSM in both accuracy and CPU time with the performance gap increasing as \u03c1 increases. For example, for \u03c1 = 1.0 and n = 2000, ALM achieves Dgap = 9.58e \u2212 4 in about 1 hour and 15 minutes, while PSM and VSM need about 3 hours and 25 minutes and 10 hours and 23 minutes, respectively, to achieve similar accuracy.\n1The MATLAB can be downloaded from http://www.cs.ubc.ca/\u223cschmidtm/Software/PQN.html 2The MATLAB code can be downloaded from http://www.math.sfu.ca/\u223czhaosong"}, {"heading": "4.2 Experiments on real data", "text": "We tested ALM on real data from gene expression networks using the five data sets from [8] provided to us by Kim-Chuan Toh: (1) Lymph node status; (2) Estrogen receptor; (3) Arabidopsis thaliana; (4) Leukemia; (5) Hereditary breast cancer. See [8] and references therein for the descriptions of these data sets. Table 2 presents our test results. As suggested in [8], we set \u03c1 = 0.5. From Table 2 we see that ALM is much faster and provided more accurate solutions than PSM and VSM."}, {"heading": "4.3 Solution Sparsity", "text": "In this section, we compare the sparsity patterns of the solutions produced by ALM, PSM and VSM. For ALM, the sparsity of the solution is given by the sparsity of Y . Since PSM and VSM solve the dual problem, the primal solution X , obtained by inverting the dual solution W , is never sparse due to floating point errors. Thus it is not fair to measure the sparsity of X or a truncated version of X . Instead, we measure the sparsity of solutions produced by PSM and VSM by appealing to complementary slackness. Specifically, the (i, j)-th element of the inverse covariance matrix is deemed to be nonzero if and only if |Wij \u2212 \u03a3\u0302ij | = \u03c1. We give results for a random problem (n = 500) and the first real data set in Table 3. For each value of \u03c1, the first three rows show the number of nonzeros in the solution and the last three rows show the number of entries that are nonzero in the solution produced by one of the methods but are zero in the solution produced by the other method. The sparsity of the ground truth inverse covariance matrix of the synthetic data is 6.76%. From Table 3 we can see that when \u03c1 is relatively large (\u03c1 \u2265 0.5), all three algorithms\nproduce solutions with exactly the same sparsity patterns. Only when \u03c1 is very small, are there slight differences. We note that the ROC curves depicting the trade-off between the number of true positive elements recovered versus the number of false positive elements as a function of the regularization parameter \u03c1 are also almost identical for the three methods."}, {"heading": "Acknowledgements", "text": "We would like to thank Professor Kim-Chuan Toh for providing the data set used in Section 4.2. The research reported here was supported in part by NSF Grants DMS 06-06712 and DMS 10-16571, ONR Grant N00014-08-1-1118 and DOE Grant DE-FG02-08ER25856."}, {"heading": "5 Appendix", "text": "We show in the following that the iteration complexity of Algorithm 2 is O(1/\u01eb) to get an \u01eb-optimal solution. First, we need the following definitions and a lemma which is a generalization of Lemma 2.3 in [24]. Let \u03a8 : Rn \u2192 R and \u03a6 : Rn \u2192 R be convex functions and define\nQ\u03c8(u, v) := \u03c6(u) + \u03c8(v) + \u3008\u03b3\u03c8(v), u \u2212 v\u3009+ 1\n2\u00b5 \u2016u\u2212 v\u201622,\nwhere \u03b3\u03c8(v) is any subgradient in the subdifferential \u2202\u03c8(v) of \u03c8(v) at the point v, and\np\u03c8(v) := argmin u Q\u03c8(u, v). (21)\nLemma 5.1. Let \u03a6(\u00b7) = \u03c6(\u00b7) + \u03c8(\u00b7). For any v, if \u03a6(p\u03c8(v)) \u2264 Q\u03c8(p\u03c8(v), v), (22)\nthen for any u,\n2\u00b5(\u03a6(u)\u2212 \u03a6(p\u03c8(v))) \u2265 \u2016p\u03c8(v)\u2212 u\u20162 \u2212 \u2016v \u2212 u\u20162. (23)\nProof. From (22), we have\n\u03a6(u)\u2212 \u03a6(p\u03c8(v)) \u2265 \u03a6(u)\u2212Q\u03c8(p\u03c8(v), v) = \u03a6(u)\u2212 (\n\u03c6(p\u03c8(v)) + \u03c8(v) + \u3008\u03b3\u03c8(v), p\u03c8(v)\u2212 v\u3009+ 12\u00b5\u2016p\u03c8(v)\u2212 v\u201622 ) . (24)\nNow since \u03c6 and \u03c8 are convex we have\n\u03c6(u) \u2265 \u03c6(p\u03c8(v)) + \u3008u \u2212 p\u03c8(v), \u03b3\u03c6(p\u03c8(v))\u3009, (25)\n\u03c8(u) \u2265 \u03c8(v) + \u3008u\u2212 v, \u03b3\u03c8(v)\u3009, (26) where \u03b3\u03c6(\u00b7) is a subgradient of \u03c6(\u00b7) and \u03b3\u03c6(p\u03c8(v)) satisfies the first-order optimality conditions for (21), i.e.,\n\u03b3\u03c6(p\u03c8(v)) + \u03b3\u03c8(v) + 1\n\u00b5 (p\u03c8(v)\u2212 v) = 0. (27)\nSumming (25) and (26) yields\n\u03a6(u) \u2265 \u03c6(p\u03c8(v)) + \u3008u \u2212 p\u03c8(v), \u03b3\u03c6(p\u03c8(v))\u3009+ \u03c8(v) + \u3008u\u2212 v, \u03b3\u03c8(v)\u3009. (28) Therefore, from (24), (27) and (28) it follows that\n\u03a6(u)\u2212 \u03a6(p\u03c8(v)) \u2265 \u3008\u03b3\u03c8(v) + \u03b3\u03c6(p\u03c8(v)), u \u2212 p\u03c8(v)\u3009 \u2212 1\n2\u00b5 \u2016p\u03c8(v)\u2212 v\u201622\n= \u3008\u2212 1 \u00b5 (p\u03c8(v) \u2212 v), u\u2212 p\u03c8(v)\u3009 \u2212 1 2\u00b5 \u2016p\u03c8(v)\u2212 v\u201622\n= 1\n2\u00b5\n( \u2016p\u03c8(v)\u2212 u\u20162 \u2212 \u2016u\u2212 v\u20162 ) .\nProof of Theorem 2.1. Let I be the set of all iteration indices until k \u2212 1-st for which no skipping occurs and let Ic be its complement. Let I = {ni}, i = 0, . . . , kn \u2212 1. It follows that for all n \u2208 Ic xn+1 = yn.\nFor n \u2208 I we can apply Lemma 5.1 to obtain the following inequalities. In (23), by letting \u03c8 = f , \u03c6 = g, u = x\u2217 and u = xn+1, we get p\u03c8(v) = yn+1, \u03a6 = F and\n2\u00b5(F (x\u2217)\u2212 F (yn+1)) \u2265 \u2016yn+1 \u2212 x\u2217\u20162 \u2212 \u2016xn+1 \u2212 x\u2217\u20162. (29) Similarly, by letting \u03c8 = g, \u03c6 = f , u = x\u2217 and v = yn in (23) we get pg(v) = xn+1, \u03a6 = F and\n2\u00b5(F (x\u2217)\u2212 F (xn+1)) \u2265 \u2016xn+1 \u2212 x\u2217\u20162 \u2212 \u2016yn \u2212 x\u2217\u20162. (30)\nTaking the summation of (29) and (30) we get\n2\u00b5(2F (x\u2217)\u2212 F (xn+1)\u2212 F (yn+1)) \u2265 \u2016yn+1 \u2212 x\u2217\u20162 \u2212 \u2016yn \u2212 x\u2217\u20162. (31)\nFor n \u2208 Ic, (29) holds, and we get 2\u00b5(F (x\u2217)\u2212 F (yn+1)) \u2265 \u2016yn+1 \u2212 x\u2217\u20162 \u2212 \u2016yn \u2212 x\u2217\u20162, (32)\ndue to the fact that xn+1 = yn in this case.\nSumming (31) and (32) over n = 0, 1, . . . , k \u2212 1 we get\n2\u00b5((2|I|+ |Ic|)F (x\u2217)\u2212 \u2211\nn\u2208I\nF (xn+1)\u2212 k\u22121 \u2211\nn=0\nF (yn+1)) (33)\n\u2265 k\u22121 \u2211\nn=0\n( \u2016yn+1 \u2212 x\u2217\u20162 \u2212 \u2016yn \u2212 x\u2217\u20162 )\n=\u2016yk \u2212 x\u2217\u20162 \u2212 \u2016y0 \u2212 x\u2217\u20162\n\u2265\u2212 \u2016x0 \u2212 x\u2217\u20162.\nFor any n, since Lemma 5.1 holds for any u, letting u = xn+1 instead of x\u2217 we get from (29) that\n2\u00b5(F (xn+1)\u2212 F (yn+1)) \u2265 \u2016yn+1 \u2212 xn+1\u20162 \u2265 0, (34) or, equivalently,\n2\u00b5(F (xn)\u2212 F (yn)) \u2265 \u2016yn \u2212 xn\u20162 \u2265 0. (35)\nSimilarly, for n \u2208 I by letting u = yn instead of x\u2217 we get from (30) that 2\u00b5(F (yn)\u2212 F (xn+1)) \u2265 \u2016xn+1 \u2212 yn\u20162 \u2265 0. (36)\nOn the other hand, for n \u2208 Ic, (36) also holds because xn+1 = yn, and hence holds for all n. Adding (34) and (36) we obtain\n2\u00b5(F (yn)\u2212 F (yn+1)) \u2265 0. (37) and adding (35) and (36) we obtain\n2\u00b5(F (xn)\u2212 F (xn+1)) \u2265 0. (38) (37) and (38) show that the sequences F (yn) and F (xn) are non-increasing. Thus we have,\nk\u22121 \u2211\nn=0\nF (yn+1) \u2265 kF (yk) and \u2211\nn\u2208I\nF (xn+1) \u2265 knF (xk). (39)\nCombining (33) and (39) yields\n2\u00b5 ( (k + kn)F (x \u2217)\u2212 knF (xk)\u2212 kF (yk) ) \u2265 \u2212\u2016x0 \u2212 x\u2217\u20162. (40)\nFrom (35) we know that F (xk) \u2265 F (yk). Thus (40) implies that\n2\u00b5(k + kn) ( F (yk)\u2212 F (x\u2217) ) \u2264 \u2016x0 \u2212 x\u2217\u20162, which gives us the desired result (11).\nAlso, for any given \u01eb > 0, as long as k \u2265 L(f)\u2016x 0\u2212x\u2217\u20162\n2\u03b2\u01eb , we have from (11) that F (y k)\u2212F (x\u2217) \u2264 \u01eb;\ni.e., the number of iterations needed is O(1/\u01eb) for an \u01eb-optimal solution."}], "references": [{"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Convex Analysis and Minimization Algorithms II: Advanced Theory and Bundle Methods", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Model selection and estimation in the Gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "High-dimensional graphical model selection using l1regularized logistic regression", "author": ["M. Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian for binary data", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "An inexact interior point method for l1-regularized sparse covariance selection", "author": ["L. Li", "K.-C. Toh"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Mining brain region connectivity for alzheimer\u2019s disease study via sparse inverse covariance estimation", "author": ["L. Sun", "R. Patel", "J. Liu", "K. Chen", "T. Wu", "J. Li", "E. Reiman", "J. Ye"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Sinco - a greedy coordinate ascent method for sparse inverse covariance selection problem", "author": ["K. Scheinberg", "I. Rish"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Projected subgradient methods for learning sparse Gaussian", "author": ["J. Duchi", "S. Gould", "D. Koller"], "venue": "Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Smooth minimization for non-smooth functions", "author": ["Y.E. Nesterov"], "venue": "Math. Program. Ser. A,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Introductory lectures on convex optimization", "author": ["Y.E. Nesterov"], "venue": "87:xviii+236,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "First-order methods for sparse covariance selection", "author": ["A. D\u2019Aspremont", "O. Banerjee", "L. El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and its Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Z. Lu"], "venue": "SIAM J. Optim.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Alternating direction methods for sparse covariance selection", "author": ["X. Yuan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Solving log-determinant optimization problems by a Newton-CG primal proximal point algorithm", "author": ["C. Wang", "D. Sun", "K.-C. Toh"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems", "author": ["M. Fortin", "R. Glowinski"], "venue": "North-Holland Pub. Co.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1983}, {"title": "Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics", "author": ["R. Glowinski", "P. Le Tallec"], "venue": "SIAM, Philadelphia, Pennsylvania,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O(1/k)", "author": ["Y.E. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1983}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "SIAM J. Optim.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["D. Goldfarb", "S. Ma", "K. Scheinberg"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": ", the lack of an edge between i and j denotes the conditional independence of y and y, which corresponds to a zero entry in the inverse covariance matrix \u03a3 ([1]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "This problem is NP-hard in general due to the combinatorial nature of the cardinality term \u03c1\u2016X\u20160 ([2]).", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "To get a numerically tractable problem, one can replace the cardinality term \u2016X\u20160 by \u2016X\u20161 := \u2211 i,j |Xij |, the envelope of \u2016X\u20160 over the set {X \u2208 R : \u2016X\u2016\u221e \u2264 1} (see [3]).", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 5, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 6, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 7, "context": "Although an approximate IPM has recently been proposed for the SICS problem [8], most of the methods developed for it are first-order methods.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "[7] proposed a block coordinate descent (BCD) method to solve the dual problem (2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "[10] proposed solving the primal problem (1) by using a BCD method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "They formulate the subproblem as a min-max problem and solve it using a prox method proposed by Nemirovski [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The SINCO method proposed by Scheinberg and Rish [12] is a greedy CD method applied to the primal problem.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Variants of Nesterov\u2019s method [14, 15] have been applied to solve the SICS problem.", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "Variants of Nesterov\u2019s method [14, 15] have been applied to solve the SICS problem.", "startOffset": 30, "endOffset": 38}, {"referenceID": 15, "context": "[16] applied Nesterov\u2019s optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth l1 term, obtaining an iteration complexity bound of O(1/\u01eb) for an \u01eb-optimal solution, but the implementation in [16] was very slow and did not produce good results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] applied Nesterov\u2019s optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth l1 term, obtaining an iteration complexity bound of O(1/\u01eb) for an \u01eb-optimal solution, but the implementation in [16] was very slow and did not produce good results.", "startOffset": 225, "endOffset": 229}, {"referenceID": 16, "context": "Lu [17] solved the dual problem (2), which is a smooth problem, by Nesterov\u2019s algorithm, and improved the iteration complexity to O(1/ \u221a \u01eb).", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Yuan [18] proposed an alternating direction method based on an augmented Lagrangian framework (see the ADAL method (8) below).", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "in [19] requires a reformulation of the problem that increases the size of the problem making it impractical for solving large-scale problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The IPM in [8] also requires such a reformulation.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "Although developed independently, our method is closely related to Yuan\u2019s method [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "[13] and the VSM algorithm proposed by Lu [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[13] and the VSM algorithm proposed by Lu [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 12, "context": "Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": ", [20, 21]):", "startOffset": 2, "endOffset": 10}, {"referenceID": 20, "context": ", [20, 21]):", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O(1/ \u221a \u01eb), using only first-order information.", "startOffset": 9, "endOffset": 17}, {"referenceID": 21, "context": "Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O(1/ \u221a \u01eb), using only first-order information.", "startOffset": 9, "endOffset": 17}, {"referenceID": 22, "context": "This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[25] and many others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 in [17], the optimal solution of (12) X \u03b1I , where \u03b1 = 1 \u2016\u03a3\u0302\u2016+n\u03c1 .", "startOffset": 5, "endOffset": 9}, {"referenceID": 24, "context": "1 can then be applied as discussed in [25].", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "We randomly created test problems using a procedure proposed by Scheinberg and Rish in [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "in [19] and Li and Toh in [8].", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "in [19] and Li and Toh in [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "We compared ALM with PSM [13] and VSM [17] on these randomly created data with different \u03c1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "We compared ALM with PSM [13] and VSM [17] on these randomly created data with different \u03c1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "We tested ALM on real data from gene expression networks using the five data sets from [8] provided to us by Kim-Chuan Toh: (1) Lymph node status; (2) Estrogen receptor; (3) Arabidopsis thaliana; (4) Leukemia; (5) Hereditary breast cancer.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "See [8] and references therein for the descriptions of these data sets.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "As suggested in [8], we set \u03c1 = 0.", "startOffset": 16, "endOffset": 19}], "year": 2010, "abstractText": "Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an l1-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem\u2019s special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an \u01eb-optimal solution in O(1/\u01eb) iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.", "creator": "LaTeX with hyperref package"}}}