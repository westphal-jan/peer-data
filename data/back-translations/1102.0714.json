{"id": "1102.0714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2011", "title": "An architecture for the evaluation of intelligent systems", "abstract": "One of the most important areas of research in Artificial Intelligence is the coding of agents (programs) that are able to learn independently in any situation, which means that agents must be useful for purposes other than those for which they were created, such as playing chess. In this way, we are trying to get closer to the original goal of Artificial Intelligence. One of the problems in deciding whether an agent is really intelligent or not is the measurement of his intelligence, as there is currently no way to measure it reliably. The purpose of this project is to create an interpreter that allows the execution of multiple environments, including those that are randomly generated, so that an agent (a person or program) can interact with them. Once the interaction between the agent and the environment is over, the interpreter measures the intelligence of the agent according to the actions, states and rewards that the agent underwent during the test within the environment.", "histories": [["v1", "Thu, 3 Feb 2011 15:58:18 GMT  (1129kb)", "http://arxiv.org/abs/1102.0714v1", "112 pages. In Spanish. Final Project Thesis"]], "COMMENTS": "112 pages. In Spanish. Final Project Thesis", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["javier insa-cabrera", "jose hernandez-orallo"], "accepted": false, "id": "1102.0714"}, "pdf": {"name": "1102.0714.pdf", "metadata": {"source": "CRF", "title": "Una arquitectura para la evaluacio\u0301n de sistemas inteligentes", "authors": ["PROYECTO FINAL DE CARRERA"], "emails": [], "sections": [{"heading": "UNIVERSIDAD POLIT\u00c9CNICA DE VALENCIA", "text": ""}, {"heading": "Escuela T\u00e9cnica Superior de Ingenier\u00eda Inform\u00e1tica", "text": "Una arquitectura para la evaluaci\u00f3n\nde sistemas inteligentes\nPROYECTO FINAL DE CARRERA\nRealizado por:\nJavier Insa Cabrera\nDirigido por:"}, {"heading": "Jos\u00e9 Hern\u00e1ndez Orallo", "text": "Valencia, 01 de octubre de 2010\nP\u00e1gina I\nResumen\nUno de los principales \u00e1mbitos de investigaci\u00f3n en Inteligencia Artificial es la codificaci\u00f3n de agentes (programas) que sean capaces de aprender por s\u00ed solos en cualquier situaci\u00f3n y que no sirvan \u00fanicamente para el fin que fueron creados, como por ejemplo para jugar al ajedrez. De este modo pretendemos acercarnos m\u00e1s a lo que realmente es la llamada Inteligencia Artificial.\nUno de los problemas para saber si realmente un agente es inteligente o no es la medici\u00f3n de su inteligencia, ya que de momento no existe forma alguna de medirla de forma fiable.\nEl objetivo de este proyecto es la creaci\u00f3n de un int\u00e9rprete que permita ejecutar diferentes entornos, incluso generados aleatoriamente, para que un agente (una persona o un programa) pueda interactuar en \u00e9stos. Una vez que la interacci\u00f3n entre el agente y el entorno ha terminado, el int\u00e9rprete medir\u00e1 la inteligencia del agente en funci\u00f3n de los diferentes estados por los que ha pasado el entorno y las acciones realizadas en cada estado por el agente durante la prueba.\nDe este modo se conseguir\u00e1 medir la inteligencia de los agentes en cualquier entorno posible, as\u00ed como hacer comparaciones entre los distintos agentes evaluando cual de todos ellos es el m\u00e1s inteligente.\nPara poder realizar las pruebas, el int\u00e9rprete debe ser capaz de generar aleatoriamente distintos entornos que realmente sean \u00fatiles para la medici\u00f3n de la inteligencia de los agentes, ya que no cualquier entorno generado aleatoriamente ser\u00e1 \u00fatil para tal prop\u00f3sito.\nP\u00e1gina II\nResum\nUn dels principals \u00e0mbits d\u2019investigaci\u00f3 en Intel\u00b7lig\u00e8ncia Artificial \u00e9s la codificaci\u00f3 d'agents (programes) que siguen capa\u00e7os d'aprendre per si mateixos en qualsevol situaci\u00f3 i que no servisquen \u00fanicament per la finalitat que van ser creats, com per exemple per jugar als escacs. D'aquesta manera pretenem apropar-nos m\u00e9s al que realment \u00e9s l'anomenada Intel\u00b7lig\u00e8ncia Artificial.\nUn dels problemes per saber si realment un agent \u00e9s intel\u00b7ligent o no \u00e9s el mesurament de la seua intel\u00b7lig\u00e8ncia, com que de moment no hi ha cap manera de mesurar-la de forma fiable.\nL'objectiu d'aquest projecte \u00e9s la creaci\u00f3 d'un int\u00e8rpret que permeta executar diferents entorns, fins i tot generats aleat\u00f2riament, perqu\u00e8 un agent (una persona o un programa) puga interactuar dins d\u2019aquests. Una vegada que la interacci\u00f3 entre l'agent i l'entorn ha acabat, l'int\u00e8rpret mesurar\u00e0 la intel\u00b7lig\u00e8ncia de l'agent en funci\u00f3 dels diferents estats pels quals ha passat l'entorn i les accions realitzades en cada estat per l'agent durant la prova.\nD'aquesta manera s'aconseguir\u00e0 mesurar la intel\u00b7lig\u00e8ncia dels agents en qualsevol entorn possible, aix\u00ed com fer comparacions entre els diferents agents avaluant quin de tots ells \u00e9s el m\u00e9s intel\u00b7ligent.\nPer poder realitzar les proves, l'int\u00e8rpret ha de ser capa\u00e7 de generar aleat\u00f2riament diferents entorns que realment siguen \u00fatils per al mesurament de la intel\u00b7lig\u00e8ncia dels agents, at\u00e9s que no qualsevol entorn generat aleat\u00f2riament ser\u00e0 \u00fatil per al prop\u00f2sit esmentat.\nP\u00e1gina III\nAbstract\nOne of the main research areas in Artificial Intelligence is the coding of agents (programs) which are able to learn by themselves in any situation. This means that agents must be useful for purposes other than those they were created for, as, for example, playing chess. In this way we try to get closer to the pristine goal of Artificial Intelligence.\nOne of the problems to decide whether an agent is really intelligent or not is the measurement of its intelligence, since there is currently no way to measure it in a reliable way.\nThe purpose of this project is to create an interpreter that allows for the execution of several environments, including those which are generated randomly, so that an agent (a person or a program) can interact with them. Once the interaction between the agent and the environment is over, the interpreter will measure the intelligence of the agent according to the actions, states and rewards the agent has undergone inside the environment during the test.\nAs a result we will be able to measure agents\u2019 intelligence in any possible environment, and to make comparisons between several agents, in order to determine which of them is the most intelligent.\nIn order to perform the tests, the interpreter must be able to randomly generate environments that are really useful to measure agents\u2019 intelligence, since not any randomly generated environment will serve that purpose.\nP\u00e1gina IV\nP\u00e1gina V\n\u00cdndice\n1. Introducci\u00f3n ............................................................................................................................ 1\n1.1. Contexto ........................................................................................................................... 1 1.2. Motivaci\u00f3n ....................................................................................................................... 1 1.3. Objetivos .......................................................................................................................... 3 2. Precedentes ............................................................................................................................. 5 3. Marco conceptual ................................................................................................................... 9\n3.1. Requisitos ........................................................................................................................ 9 3.2. Entornos y agentes ....................................................................................................... 10 3.3. Medici\u00f3n ......................................................................................................................... 14 3.4. Distribuci\u00f3n universal y su aplicaci\u00f3n a entornos ................................................ 16 3.5. Entornos discriminativos: Sensibilidad a las recompensas ................................ 19 3.6. Un entorno no sesgado y balanceado ....................................................................... 21\n3.6.1. Clases de entornos Turing-completas .............................................................. 21 3.6.2. Acciones, observaciones y el espacio ............................................................... 22 3.6.3. Definici\u00f3n de la clase de entorno ...................................................................... 24 3.6.4. Propiedades ........................................................................................................... 28 3.6.5. Codificaci\u00f3n y generaci\u00f3n del entorno ............................................................. 28\n4. Implementaci\u00f3n aproximada del marco conceptual .................................................... 31\n4.1. Diagrama de clases ....................................................................................................... 31 4.2. Principales diferencias entre el marco conceptual y la implementaci\u00f3n realizada ..................................................................................................................................... 32\n4.2.1. Observaciones ....................................................................................................... 32 4.2.2. Objetos y agentes .................................................................................................. 33 4.2.3. Comportamiento de los agentes ........................................................................ 33 4.2.4. Interacciones entre agentes ............................................................................... 33 4.3. Descripci\u00f3n de las clases del sistema ....................................................................... 33\n4.3.1. Clases para representar los agentes y objetos ............................................... 33\n4.3.1.1. Agente ............................................................................................................. 33 4.3.1.2. Agente Evaluable (Subclase de Agente) ................................................... 34 4.3.1.3. Agentes Good & Evil (Subclase de Agente) .............................................. 35 4.3.1.4. Agente Humano (Subclase de Agente Evaluable) ................................... 35 4.3.1.5. Agente Aleatorio (Subclase de Agente Evaluable) ................................. 36 4.3.1.6. Agente Observador (Subclase de Agente Evaluable) ............................. 36 4.3.1.7. Objeto .............................................................................................................. 37 4.3.2. Clases para representar el espacio ................................................................... 38\n4.3.2.1. Celda ................................................................................................................ 38 4.3.2.2. Conexi\u00f3n ......................................................................................................... 38 4.3.2.3. Espacio ............................................................................................................ 39 4.3.3. Clases para representar los entornos .............................................................. 39\n4.3.3.1. Entorno ........................................................................................................... 39 4.3.3.2. Entorno_L (Subclase de Entorno) .............................................................. 40\nP\u00e1gina VI\n4.3.3.3. Realizaci\u00f3n de una sesi\u00f3n de evaluaci\u00f3n ................................................. 41\n4.3.3.3.1. Preparaci\u00f3n del entorno ........................................................................ 41 4.3.3.3.2. Descripci\u00f3n del espacio .......................................................................... 42 4.3.3.3.3. Comienzo de la sesi\u00f3n ............................................................................. 42 4.3.3.3.4. Bucle principal.......................................................................................... 43 4.3.3.3.5. Finalizaci\u00f3n de la sesi\u00f3n ......................................................................... 44\n4.3.4. Otras clases auxiliares ......................................................................................... 44\n4.3.4.1. Interacci\u00f3n ..................................................................................................... 44 4.3.4.2. Observaci\u00f3n ................................................................................................... 44\n4.4. Codificaci\u00f3n y descripci\u00f3n de espacios .................................................................... 45\n4.4.1. Codificaci\u00f3n del espacio ...................................................................................... 45 4.4.2. Descripci\u00f3n manual del espacio ........................................................................ 46 4.4.3. Descripci\u00f3n aleatoria del espacio ..................................................................... 46 4.5. Interfaz ........................................................................................................................... 47 5. Experimentos ........................................................................................................................ 51\n5.1. Entornos definidos manualmente............................................................................. 51 5.2. Entornos generados autom\u00e1ticamente .................................................................... 62\n5.2.1. Espacios generados autom\u00e1ticamente (Conectados) .................................... 62 5.2.2. Espacios generados autom\u00e1ticamente (Fuertemente conectados) ........... 72 5.3. Entorno sesgado ........................................................................................................... 82 5.4. Evaluaci\u00f3n social .......................................................................................................... 86 5.5. Varios movimientos de los agentes generadores .................................................. 90 6. Conclusiones y trabajo futuro .......................................................................................... 103 Ap\u00e9ndices ..................................................................................................................................... 107\nA1. Extractos representativos del C\u00f3digo Fuente ....................................................... 107\nA1.1. Bucle principal .................................................................................................... 107 A1.2. Generaci\u00f3n aleatoria de espacios ................................................................... 109 A2. Referencias .................................................................................................................. 109\nP\u00e1gina VII\nP\u00e1gina VIII\nP\u00e1gina 1\n1. Introducci\u00f3n\n1.1. Contexto\nEste proyecto de fin de carrera surge a ra\u00edz de una Acci\u00f3n Complementaria dentro del Programa Explora-Ingenio financiada por el Ministerio de Ciencia e Innovaci\u00f3n con t\u00edtulo \u201cAnytime Universal Intelligence (ANYNT)\u201d (http://users.dsic.upv.es/proy/anynt/), cuyo investigador principal es Jos\u00e9 Hern\u00e1ndez Orallo. El objetivo del programa Explora-Ingenio es explorar l\u00edneas de investigaci\u00f3n de alto riesgo e impacto, posiblemente heterodoxas, cuya viabilidad se intenta determinar. Seg\u00fan el equipo que evalu\u00f3 el proyecto, \u00e9ste \u201ces una propuesta con la suficiente osad\u00eda intelectual y riesgo como para ser considerada en este Programa explora\u201d.\nEn el contexto de este proyecto de investigaci\u00f3n, en 2009 se ofertaron en la ETSINF diferentes proyectos de fin de carrera relacionados con la implementaci\u00f3n de prototipos y estructuras b\u00e1sicas de los tests de inteligencia que se propon\u00eda desarrollar. A finales del 2009, empec\u00e9 a trabajar en el tema con Jos\u00e9 Hern\u00e1ndez Orallo. El propio car\u00e1cter del proyecto ANYNT implicaba que el proyecto de fin de carrera que me propon\u00eda ten\u00eda un car\u00e1cter marcadamente investigador y que, frecuentemente, habr\u00eda que tomar decisiones de implementaci\u00f3n y de evaluaci\u00f3n a medida que avanzara la implementaci\u00f3n o se obtuviera alg\u00fan resultado investigador. En definitiva, un proyecto bastante cerrado en su contexto (el del propio proyecto de investigaci\u00f3n) pero inicialmente bastante abierto en objetivos, y que naturalmente ha de ser entendido en un contexto investigador, y no tanto desde la perspectiva de un buen acabado de un producto final.\nDurante la primera mitad de 2010, y compaginando los estudios, el proyecto fin de carrera fue avanzando, siguiendo fundamentalmente las publicaciones [Hernandez-Orallo 2010a] [Hernandez-Orallo 2010b] y [Hernandez-Orallo & Dowe 2010]. En junio de 2010 me present\u00e9 a una convocatoria de contratado dentro del mismo proyecto de investigaci\u00f3n y fui seleccionado, principalmente por el premio extraordinario al expediente acad\u00e9mico. Desde mi incorporaci\u00f3n como contratado, el proyecto de fin de carrera constituye un primer paso a partir del cual seguir trabajando en una segunda fase m\u00e1s ambiciosa.\n1.2. Motivaci\u00f3n\nUna de las cuestiones m\u00e1s debatidas en el campo de la inteligencia artificial, la ciencia cognitiva, la psicometr\u00eda, la biolog\u00eda y la antropolog\u00eda es la definici\u00f3n de inteligencia y su medici\u00f3n.\nLa medici\u00f3n de la inteligencia humana es de suma relevancia para muchas \u00e1reas de aplicaci\u00f3n. Consecuentemente, los tests psicom\u00e9tricos son comunes para (i) ayudar a ni\u00f1os y alumnos a aprender de manera eficiente (educaci\u00f3n), (ii) evaluar al personal (selecci\u00f3n y reclutamiento) y tambi\u00e9n (iii) en el tratamiento de diferentes enfermedades y problemas de aprendizaje (terapias de aprendizaje).\nP\u00e1gina 2\nDe una manera bastante similar, la medici\u00f3n de la inteligencia en m\u00e1quinas es requerida en \u00e1reas tales como (i) crear agentes, robots, y otros tipos de \u201csistemas inteligentes\u201d que adquieran conocimientos r\u00e1pidamente (adquisici\u00f3n de conocimiento), (ii) para seleccionar/evaluar las habilidades de \u201csistemas inteligentes\u201d (acreditaci\u00f3n/certificaci\u00f3n) y tambi\u00e9n (iii) cuando se trata de corregir o mejorar las capacidades de estos sistemas (dise\u00f1o o correcci\u00f3n de sistemas inteligentes).\nAunque la terminolog\u00eda es diferente, las similitudes en ambas \u00e1reas son claras. Adicionalmente, una tercera \u00e1rea af\u00edn es la psicolog\u00eda o cognici\u00f3n comparativa para varios tipos de animales, donde existe un debate sobre c\u00f3mo evaluar las habilidades cognitivas en los animales superiores (en especial los grandes simios y cet\u00e1ceos).\nEl caso de la medici\u00f3n de la inteligencia en m\u00e1quinas es especialmente relevante\ndebido a varias cuestiones:\n\u2022 Una ciencia o tecnolog\u00eda no puede avanzar si no disponemos de t\u00e9cnicas de medici\u00f3n adecuadas para evaluar sus progresos. Es dif\u00edcil desarrollar la\naeron\u00e1utica incluso a trav\u00e9s de la observaci\u00f3n de aves en pleno vuelo si no tenemos las mediciones de peso, altura o de aerodin\u00e1mica. La inteligencia artificial tiene una referencia (el homo sapiens) pero sufre una falta de medidas y t\u00e9cnicas de medici\u00f3n para sus artefactos.\n\u2022 La ubicuidad de los robots, ayudantes, mayordomos y otros tipos de agentes aut\u00f3nomos que realizan tareas en lugar de los seres humanos, hace que sea dif\u00edcil\ntratar apropiadamente con ellos en un nuevo espacio de colaboraci\u00f3n donde la interacci\u00f3n virtual sustituye a la interacci\u00f3n f\u00edsica, tales como plataformas de colaboraci\u00f3n, redes sociales, arquitecturas orientadas a servicios, e-burocracia, Web 2.0, etc. En muchas aplicaciones necesitamos diferenciar entre est\u00fapidos robots capacitados para realizar una \u00fanica tarea y robots m\u00e1s generales e inteligentes.\no Por un lado, algunas aplicaciones requieren que se les permita a los robots participar en proyectos o concederles permisos como interlocutores para la\ngesti\u00f3n, negociaciones o acuerdos. Por lo tanto, se espera que los robots sean eficaces para llevar a cabo las tareas que se les ha delegado, para las cuales tienen que ser inteligentes (desean evitarse los robots no inteligentes).\no Por otro lado, en algunos casos, queremos evitar que robots de cualquier tipo puedan realizar ciertas tareas (p. ej. robots maliciosos podr\u00edan crear\nmillones de cuentas en un servicio de correo gratuito). Con este fin, son extensamente utilizados los CAPTCHAs [von Ahn et al 2002] [von Ahn et al 2008], pero mecanismos m\u00e1s precisos y fiables ser\u00e1n necesarios en un futuro cercano.\nDemos un ejemplo. Imaginemos un entorno colaborativo, donde varios agentes (algunos de ellos humanos y otros m\u00e1quinas), tienen que construir un sitio web especializado con informaci\u00f3n sobre un tema espec\u00edfico (p. ej. una entrada de la Wikipedia, un esbozo de un proyecto europeo, un evento reciente, una biblioteca de m\u00fasica, un proyecto geogr\u00e1fico, etc.). El equipo tendr\u00e1 que decidir las normas de interacci\u00f3n (c\u00f3mo la informaci\u00f3n es aceptada, integrada y publicada). Para llegar a formar\nP\u00e1gina 3\nparte del equipo, cada componente necesitar\u00e1 certificar que \u00e9l/ella tiene algunas habilidades b\u00e1sicas de razonamiento/aprendizaje. Si un agente inepto (o persona) trata de unirse al club requeriremos alg\u00fan tipo de evaluaci\u00f3n para decidir si debemos dejarle entrar.\n\u00bfExisten mecanismos como estos disponibles hoy en d\u00eda? La respuesta es no. A parte de los CAPTCHAs, los cuales solamente miden habilidades muy espec\u00edficas, no disponemos de ning\u00fan tipo de test donde podamos evaluar r\u00e1pida y fiablemente la inteligencia de un sistema inteligente (un agente).\nEl prop\u00f3sito del proyecto de investigaci\u00f3n es explorar las posibilidades de construir\ntests que sean:\n\u2022 Universales: Pueden ser aplicados a humanos, robots, animales, comunidades o incluso h\u00edbridos. Para hacer frente a esta propiedad el test debe derivar de\nprincipios no antropom\u00f3rficos universales fundados en ciencias de la computaci\u00f3n y en la teor\u00eda de la informaci\u00f3n. Este objetivo, por s\u00ed mismo, es muy desafiante ya que los tests actuales se basan en suposiciones antropom\u00f3rficas.\n\u2022 Anytime: Se pueden adaptar a la velocidad del examinado, y pueden adaptarse din\u00e1micamente sus preguntas, interacciones o temas al examinado, a fin de\nevaluar su inteligencia m\u00e1s r\u00e1pido. El test debe ser lo suficientemente flexible para evaluar muy r\u00e1pidamente (con poca fiabilidad) para algunas aplicaciones (p. ej. aplicaciones donde los CAPTCHAs son usados hoy en d\u00eda) o proporcionar estimaciones de alta fiabilidad si se proporciona m\u00e1s tiempo (p. ej. aplicaciones de las cuales se derivan serias consecuencias debido a la intromisi\u00f3n de un agente est\u00fapido).\nSiguiendo las ideas de la primera definici\u00f3n de inteligencia y tests basados en la teor\u00eda de informaci\u00f3n algor\u00edtmica [Dowe & Hajek 1997] [Hernandez-Orallo 2000a] [Legg & Hutter 2007], nos enfrentamos al reto de construir el primer test de inteligencia universal, formal, pero al mismo tiempo pr\u00e1ctico. La cuesti\u00f3n principal es la noci\u00f3n de test \u201canytime\u201d, el cual permitir\u00e1 una convergencia r\u00e1pida del test al nivel de inteligencia del sujeto y una evaluaci\u00f3n progresivamente mejor cuanto m\u00e1s tiempo le proporcionemos. Si tenemos \u00e9xito, la ciencia ser\u00e1 capaz de medir la inteligencia de animales superiores (p. ej. simios) humanos y m\u00e1quinas de una manera universal y pr\u00e1ctica.\nDentro del proyecto de investigaci\u00f3n anterior existen diferentes tareas de implementaci\u00f3n a las cuales este proyecto de fin de carrera se circunscribe, tal y como se especifica en los objetivos que veremos a continuaci\u00f3n.\n1.3. Objetivos\nPodemos distinguir un objetivo general, a partir del cual detallamos una serie de\nobjetivos espec\u00edficos.\nP\u00e1gina 4\nObjetivo general\nDesarrollar un entorno de aplicaci\u00f3n de tests de inteligencia a diferentes agentes (m\u00e1quinas o personas) usando la teor\u00eda de medici\u00f3n desarrollada en [Hernandez-Orallo & Dowe 2010] y la clase de entornos desarrollada en [Hern\u00e1ndez-Orallo 2010b]. B\u00e1sicamente esto significa la construcci\u00f3n de un sistema en donde distintos tipos de agentes sean capaces de interactuar con entornos autogenerados en donde, en base a observaciones facilitadas por el sistema y acciones respondidas por los agentes, pueda ser medida la inteligencia de estos agentes.\nObjetivos espec\u00edficos\n\u2022 Int\u00e9rprete de entornos: La construcci\u00f3n de un sistema que permita la interacci\u00f3n entre unos agentes en un entorno y, posteriormente, la medici\u00f3n de la inteligencia\ndemostrada por el agente.\n\u2022 Codificaci\u00f3n manual de los entornos: Permitir la construcci\u00f3n de entornos utilizando un mecanismo manual que permita su definici\u00f3n interna y su\ncomportamiento.\n\u2022 Generaci\u00f3n autom\u00e1tica de los entornos siguiendo alguna distribuci\u00f3n: El sistema deber\u00e1 permitir la construcci\u00f3n de entornos autom\u00e1ticamente utilizando una\ndistribuci\u00f3n para decidir su representaci\u00f3n y su posterior construcci\u00f3n en base a dicha representaci\u00f3n.\n\u2022 Entorno gr\u00e1fico de evaluaci\u00f3n de los entornos y que proporcione los resultados: Se debe facilitar al usuario una interfaz con la que interactuar con el entorno,\nofreci\u00e9ndole lo necesario para la realizaci\u00f3n de los tests y que le muestre posteriormente los resultados obtenidos.\n\u2022 Realizaci\u00f3n de pruebas y experimentos con entornos y agentes muy sencillos, y con entornos y agentes aleatorios: Pruebas para comprobar que el sistema\nfunciona correctamente y experimentos para estudiar los resultados que distintos agentes obtienen al interactuar en algunos entornos codificados manualmente y en otros entornos generados autom\u00e1ticamente.\nP\u00e1gina 5"}, {"heading": "2. Precedentes", "text": "Existen dos tipos de tests que son de inter\u00e9s en esta \u00e1rea: tests psicom\u00e9tricos y tests\nde inteligencia en m\u00e1quinas.\nLos tests psicom\u00e9tricos [Martinez-Arias et al 2006] tienen una larga historia [Spearman 1904], son efectivos, f\u00e1ciles de administrar, r\u00e1pidos y muy estables cuando son usados en el mismo individuo a trav\u00e9s del tiempo. De hecho, han proporcionado una de las definiciones de inteligencia m\u00e1s pr\u00e1cticas: \u201cla inteligencia es lo que se mide por tests de inteligencia\u201d. Sin embargo, los tests psicom\u00e9tricos son antropom\u00f3rficos: no pueden evaluar la inteligencia de sistemas diferentes del Homo sapiens.\nNuevos enfoques en la psicometr\u00eda como el Item Response Theory (IRT) permiten la selecci\u00f3n de items bas\u00e1ndose en sus caracter\u00edsticas de demanda cognitivas, proporcionando resultados para entender lo que se est\u00e1 midiendo y adaptando el test al nivel del individuo que se est\u00e1 examinando. Los items generados por la teor\u00eda cognitiva y analizados por IRT son una herramienta prometedora, pero estos modelos no son implementados completamente en las pruebas [Embretson & Mc Collam 2000].\nA pesar de que estos y otros esfuerzos han intentado establecer \u201ca priori\u201d c\u00f3mo debe ser un test de inteligencia (p. ej. [Embretson 1998]), y se han encontrado adaptaciones para distintos tipos de sujetos, en general necesitamos diferentes versiones de los tests psicom\u00e9tricos para evaluar a ni\u00f1os de diferentes edades, ya que los tests psicom\u00e9tricos para el Homo sapiens adulto se basan en unos conocimientos y habilidades que los ni\u00f1os no han adquirido todav\u00eda.\nLo mismo sucede con otros animales. Psic\u00f3logos comparativos y otros cient\u00edficos en el \u00e1rea de la cognici\u00f3n comparativa normalmente dise\u00f1an tests espec\u00edficos para especies diferentes. Se puede ver un ejemplo de estos tests especializados para ni\u00f1os y simios en [Herrmann et al 2007]. Adicionalmente, se ha demostrado que los tests psicom\u00e9tricos no funcionan para m\u00e1quinas en su etapa de progreso actual de la inteligencia artificial [Sanghi & Dowe 2003], ya que pueden ser enga\u00f1ados por programas de ordenador muy simples. Pero el principal inconveniente de los tests psicom\u00e9tricos para evaluar otros sujetos diferentes a los humanos es que no existe una definici\u00f3n matem\u00e1tica tras ellos. Por lo tanto es dif\u00edcil dise\u00f1ar un test que funcione en cualquier sujeto inteligente (m\u00e1quina, humano, animal, \u2026), y no solo en el tipo espec\u00edfico de sujeto en el que experimentalmente hemos evaluado que funciona.\nLos tests de inteligencia para m\u00e1quinas han sido propuestos desde que Alan Turing [Turing 1950] introdujo el juego de imitaci\u00f3n en 1950, actualmente conocido como el test de Turing. En este test, un sistema es considerado inteligente si es capaz de imitar a un humano (p. ej. ser indistinguible de un humano) durante un periodo de tiempo y sujeto a un di\u00e1logo (tele-texto) con uno o m\u00e1s jueces. Aunque todav\u00eda se acepta como referencia para comprobar si finalmente la inteligencia artificial se acerca a la inteligencia de los humanos, ha generado debate a lo largo del tiempo. Por supuesto, tambi\u00e9n se han sugerido varias variantes y alternativas internas. El test de Turing e ideas relacionadas\nP\u00e1gina 6\npresentan varios problemas como test de inteligencia para m\u00e1quinas: el test de Turing es antropom\u00f3rfico (mide la humanidad, no la inteligencia), no es gradual (no proporciona una puntuaci\u00f3n), no es pr\u00e1ctico (cada vez es m\u00e1s f\u00e1cil de enga\u00f1ar y requiere mucho tiempo para obtener evaluaciones fiables) y requiere de un juez humano.\nUna reciente e ingenua aproximaci\u00f3n a los tests de inteligencia para m\u00e1quinas son los llamados CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) [von Ahn et al 2002] [von Ahn et al 2008]. Los CAPTCHAs son cualquier tipo de preguntas simples que puedan ser f\u00e1cilmente resueltas por un humano pero no por las tecnolog\u00edas de inteligencia artificial actuales. Los CAPTCHAs t\u00edpicos son los problemas de reconocimiento de caracteres donde las letras aparecen distorsionadas. Estas distorsiones hacen que para las m\u00e1quinas (robots) sea dif\u00edcil reconocer las letras. A continuaci\u00f3n podemos ver un ejemplo de CAPTCHA procedente de Gmail:\nEl objetivo inmediato de un CAPTCHA es separar a los humanos y a las m\u00e1quinas. El objetivo final es prevenir que robots u otro tipo de agentes o programas sean capaces de crear miles de cuentas (u otras tareas que solo los humanos deber\u00edan hacer). N\u00f3tese que los robots podr\u00edan bloquear o da\u00f1ar muchos servicios de Internet que utilizamos diariamente si esta u otras t\u00e9cnicas de control no existieran.\nEl problema de los CAPTCHAs es que se est\u00e1n haciendo cada vez m\u00e1s y m\u00e1s dif\u00edciles para los humanos, ya que los robots se est\u00e1n especializando y mejorando para poder leerlos. Siempre que una nueva t\u00e9cnica CAPTCHA es desarrollada, aparecen nuevos robots que tienen posibilidades de pasar el test. Esto fuerza a los desarrolladores de CAPTCHAs a que vuelvan a cambiarlos, y as\u00ed sucesivamente. La raz\u00f3n es que son espec\u00edficos y se basan en algunas tareas particulares. Aunque los CAPTCHAs funcionan\nP\u00e1gina 7\nrazonablemente bien en la actualidad, dentro de 10 \u00f3 20 a\u00f1os, se tendr\u00e1n que hacer las cosas tan dif\u00edciles y generales, que los humanos necesitar\u00e1n m\u00e1s tiempo y varios intentos para pasarlos. De hecho, ya est\u00e1 sucediendo, y perdemos cada vez m\u00e1s y m\u00e1s tiempo en los CAPTCHAs cada d\u00eda.\nA parte de los tests, ciertas aproximaciones m\u00e1s te\u00f3ricas y formales a la inteligencia de las m\u00e1quinas han sido llevadas a cabo por cient\u00edficos prominentes en el siglo 20 como A.M. Turing, R.J Solomonoff, E.M. Gold, C.S. Wallace, J.J. Rissanen, M. Blum, G.J. Chaitin y otros. El hito es el desarrollo de la teor\u00eda de informaci\u00f3n algor\u00edtmica (tambi\u00e9n conocida como Complejidad Kolmogorov) (ver [Li & Vitanyi 2008] para una referencia m\u00e1s completa), su relaci\u00f3n con el aprendizaje (inferencia inductiva y predicci\u00f3n) [Solomonoff 1964] [Wallace & Boulton 1968] [Solomonoff 1986] [Wallace & Dowe 1999] [Wallace 2005] y finalmente su relaci\u00f3n con la inteligencia.\nEstas ideas llevaron a algunos miembros del equipo del proyecto de investigaci\u00f3n ANYNT a introducir varias definiciones formales a la inteligencia, a saber, las obras [Dowe & Hajek 1997, 1998] [Hernandez-Orallo & Minaya-Collado 1998] [Hernandez-Orallo 2000a] [Hernandez-Orallo 2000b]. Todos estos tests y definiciones son matem\u00e1ticas, no antropom\u00f3rficas (p. ej. universales), significativos e intuitivos. En este sentido no toman al Homo sapiens como referencia ni juez (como en el test de Turing), no han evolucionado a trav\u00e9s de prueba y error a trav\u00e9s de la experimentaci\u00f3n en tests sobre sujetos durante un siglo (como generalmente se da en psicometr\u00eda) sino que son construidos sobre nociones fundamentales y matem\u00e1ticas en la teor\u00eda de computaci\u00f3n. Y algunos de ellos lograron que fueran pr\u00e1cticos (un test de inteligencia factible) a coste de ser parcial (en el sentido de que miden rasgos necesarios de la inteligencia, pero no todos ellos). Veamos a continuaci\u00f3n estas aproximaciones en mayor detalle:\nPor un lado, Dowe & Hajek [Dowe & Hajek 1997, 1998] sugirieron la introducci\u00f3n de problemas de inferencia de inducci\u00f3n en los tests de Turing para, entre otras cosas, hacer frente a la paradoja de Searle \u201cLa habitaci\u00f3n China\u201d, y tambi\u00e9n porque una habilidad de inferencia inductiva es un requisito necesario (aunque posiblemente \u201cno suficiente\u201d) para la inteligencia.\nPor otro lado, al mismo tiempo y de manera similar, y tambi\u00e9n de forma independiente, en [Hernandez-Orallo & Minaya-Collado 1998] [Hernandez-Orallo 2000a] la inteligencia se defini\u00f3 como la habilidad de comprender, dando una definici\u00f3n formal de la noci\u00f3n de comprensi\u00f3n como la identificaci\u00f3n de un patr\u00f3n \u2018predominante\u2019 de una evidencia dada, derivada de la teor\u00eda de inducci\u00f3n de Solomonoff, la complejidad de Kolmogorov y la b\u00fasqueda \u00f3ptima de Levin. La definici\u00f3n es el resultado de un test, llamado C-test [Hernandez-Orallo & Minaya-Collado 1998] formado por series de complejidad creciente computacionalmente obtenidas. Las secuencias fueron formateadas y presentadas de una manera bastante similar a las pruebas psicom\u00e9tricas, y como resultado, el test fue administrado a humanos. No obstante, el objetivo principal fue que el test pudiese llegar a ser administrado a otros tipos de sistemas inteligentes.\nEn [Hernandez-Orallo 2000b] se esboza una factorizaci\u00f3n (y por lo tanto extensi\u00f3n) de estos tests de inferencia inductiva para explorar qu\u00e9 otras habilidades podr\u00edan conformar un completo (y por lo tanto suficiente) test. Con el fin de aplicar el test para sistemas\nP\u00e1gina 8\ninteligentes inferiores, (todav\u00eda) incapaces de entender el lenguaje natural, el prop\u00f3sito de una extensi\u00f3n din\u00e1mica del C-test en [Hernandez-Orallo 2000a] se expres\u00f3 as\u00ed: \u201cLa presentaci\u00f3n del test deber\u00eda cambiar ligeramente. Los ejercicios deber\u00e1n proporcionarse uno por uno y, despu\u00e9s de cada respuesta, se le deber\u00e1 proporcionar la respuesta correcta al sujeto (deber\u00e1n usarse recompensas y castigos)\u201d.\nAlgunos trabajos recientes realizados por Legg y Hutter (p. ej. [Legg & Hutter 2007]) han seguido los pasos anteriores y, fuertemente influenciados por la teor\u00eda de Hutter de los agentes \u00f3ptimos AIXI [Hutter 2005], han dado otra definici\u00f3n de inteligencia de m\u00e1quina, denominada \u201cInteligencia Universal\u201d, tambi\u00e9n surgida de la complejidad de Kolmogorov y la inducci\u00f3n de Solomonoff.\nLa idea b\u00e1sica es evaluar la inteligencia de un agente \u03c0 en varios entornos \u00b5, elegidos utilizando una distribuci\u00f3n universal (derivadas de la complejidad de Kolmogorov, p. ej. p(\u00b5)= 2-K(\u00b5)), utilizando recompensas que se acumulan durante la interacci\u00f3n con el entorno. Por tanto la inteligencia se define como la competencia de un agente en distintos entornos, donde los entornos simples tienen una mayor probabilidad que los entornos complejos.\nLa comparaci\u00f3n con los trabajos de Hern\u00e1ndez-Orallo (pero tambi\u00e9n el test de\ncompresi\u00f3n de Dowe & Hajek) se resumen en [Legg & Hutter 2007] con la siguiente tabla:\nAgente universal Test universal\nEntorno pasivo Inducci\u00f3n de Solomonoff C-test\nEntorno activo AIXI Inteligencia Universal\nDe hecho, la definici\u00f3n basada en el C-test puede ser considerada una instancia del trabajo de Legg y Hutter ya que el agente no tiene permitido hacer una acci\u00f3n hasta que ha visto un n\u00famero de observaciones (la secuencia de inferencia inductiva). Una de las contribuciones m\u00e1s relevantes en el trabajo de Legs y Hutter es que su definici\u00f3n de Inteligencia Universal permite evaluar formalmente el funcionamiento te\u00f3rico de algunos agentes: un agente aleatorio, un agente especializado, \u2026 o un agente super-inteligente, como AIXI [Hutter 2005] que seg\u00fan se afirma, si alguna vez se construye, sacar\u00eda la mejor puntuaci\u00f3n en el test de Inteligencia Universal. En pocas palabras, el trabajo de Legg y Hutter es otro paso que ayuda a dar forma a las cosas que se deben abordar en un futuro pr\u00f3ximo a fin de alcanzar finalmente una teor\u00eda de la inteligencia de m\u00e1quina.\nSin embargo, existen cinco problemas que hemos identificado en su definici\u00f3n que evitan que se ponga en pr\u00e1ctica. Primero, tenemos una suma infinita de todos los entornos. Segundo, tambi\u00e9n tenemos una suma infinita de todas las posibles recompensas (la vida de los agentes en cada entornos es infinita). Tercero, K() no es computable. Cuatro, y m\u00e1s importante, no se tiene en cuenta el tiempo. Y, quinto, hay cierta confusi\u00f3n entre la inducci\u00f3n y la predicci\u00f3n.\nP\u00e1gina 9"}, {"heading": "3. Marco conceptual", "text": "3.1. Requisitos\nEn el proyecto de investigaci\u00f3n se propone una modificaci\u00f3n y extensi\u00f3n de las definiciones y tests previos a fin de construir un primer test y definici\u00f3n general, formal y factible. La definici\u00f3n y el test se basa en desarrollos previos en tests basados en la complejidad de Kolmogorov [Dowe & Hajek 1997, 1998] [Hernandez-Orallo & MinayaCollado 1998] [Hernandez-Orallo 2000a, 2000b] [Legg & Hutter 2007]. La principal idea de todos ellos es que se usa la Distribuci\u00f3n Universal para la generaci\u00f3n de preguntas y entornos, y por lo tanto cualquier sesgo particular hacia un individuo espec\u00edfico, especies o culturas se evita. Esto hace al test universal para cualquier posible tipo de sujeto. Pero aparte de esta condici\u00f3n \u201cUniversal\u201d nos centramos en algunos requisitos pr\u00e1cticos adicionales:\n\u2022 Debe permitir medir cualquier tipo de sistema inteligente (biol\u00f3gico o computacional) que exista actualmente o pueda ser construido en el futuro\n(sistemas anytime).\n\u2022 El test debe adaptarse r\u00e1pidamente al nivel de inteligencia y escala de tiempo del sistema. Debe permitir evaluar tanto sistemas ineptos como brillantes (cualquier\nnivel de inteligencia) as\u00ed como a sistemas muy lentos y muy r\u00e1pidos (escala any time).\n\u2022 La calidad de la evaluaci\u00f3n depender\u00e1 del tiempo que dejemos al test. Esto significa que el test puede ser interrumpido en cualquier momento, produciendo\nuna aproximaci\u00f3n a la puntuaci\u00f3n de la inteligencia. Cuanto m\u00e1s tiempo dejemos para realizar el test, mejores evaluaciones (test anytime).\nDebido a estos requisitos llamaremos a los tests \u201canytime universal intelligence tests\u201d. Si tenemos \u00e9xito, la ciencia ser\u00e1 capaz de medir la inteligencia de animales superiores (p. ej. simios), humanos, m\u00e1quinas, h\u00edbridos o comunidades de humanos y m\u00e1quinas e incluso seres extraterrestres, de un modo absoluto.\nLa principal dificultad para hacerlo factible es que cuanto m\u00e1s general tratamos que sea el test, menos cosas del agente debemos asumir. Esto significa que es posible que necesitemos m\u00e1s tiempo para evaluar la inteligencia, ya que no podemos confiar en un conocimiento com\u00fan ni un lenguaje com\u00fan para explicar las instrucciones ni dar nada por sentado. El problema es similar para evaluar ni\u00f1os o animales, donde todo debe ser muy simb\u00f3lico y simple, y los sujetos deben ser dirigidos hacia el objetivo a trav\u00e9s de recompensas y castigos. Esta es la raz\u00f3n por la que estos tests son m\u00e1s interactivos que los tests psicom\u00e9tricos tradicionales para los Homo sapiens adultos.\nDe hecho, existe una tesis com\u00fan en todas las pruebas de inteligencia (tanto de la psicometr\u00eda, la cognici\u00f3n comparativa o la inteligencia artificial): el tiempo que se requiere para evaluar la inteligencia de un sujeto depende (1) del conocimiento o caracter\u00edsticas que se esperan y conocen sobre el sujeto (misma especie, misma cultura, mismo lenguaje,\nP\u00e1gina 10\netc.) y (2) en la adaptabilidad del examinador. Mientras que (1) se lleva al extremo en psicometr\u00eda humana, donde los tests son simplemente formularios con preguntas explicadas en lenguaje natural, (2) se lleva al extremo en el Test de Turing o cualquier tipo de entrevista de evaluaci\u00f3n, donde los examinadores son humanos los cuales din\u00e1micamente adaptan sus preguntas dependiendo de las respuestas del sujeto.\nConsecuentemente, si queremos evaluar diferentes tipos de sujetos sin depender de ninguna suposici\u00f3n acerca de su naturaleza o conocimiento, y si queremos conseguir una valoraci\u00f3n fiable en un reducido (pr\u00e1ctico) periodo de tiempo, debemos necesariamente dise\u00f1ar tests interactivos y adaptables.\n3.2. Entornos y agentes\nAunque muy diferentes, existen tres dimensiones donde cualquier planteamiento de medici\u00f3n de inteligencia tiene que tomar decisiones. En todos los ajustes de evaluaci\u00f3n de inteligencia, encontramos (1) un sujeto o agente a examinar, (2) un conjunto de problemas, tareas o entornos y (3) un protocolo para la aplicaci\u00f3n de medida y la derivaci\u00f3n de uno o m\u00e1s resultados de rendimiento. Dependiendo de los supuestos o limitaciones impuestas a cada uno de estos temas, tenemos, como resultado, un marco de evaluaci\u00f3n diferente. La literatura de la inteligencia artificial se ha centrado en (1) y (2), especialmente en el \u00e1rea de las arquitecturas cognitivas, donde impl\u00edcitamente o expl\u00edcitamente un conjunto de restricciones o requisitos se establece en los agentes y entornos.\nPor ejemplo, SOAR [Laird et al 1987][Laird 2008], una de las m\u00e1s exitosas y conocidas arquitecturas cognitivas en la inteligencia artificial, impl\u00edcitamente asume algunas caracter\u00edsticas sobre los agentes y entornos. Algunos de estos supuestos se convierten en requisitos expl\u00edcitos en [Laird & Wray 2010], donde la relaci\u00f3n entre los requisitos de la inteligencia y los requisitos en la arquitectura cognitiva son dilucidados. M\u00e1s precisamente, este trabajo describe ocho caracter\u00edsticas de los entornos, tareas y agentes los cuales son considerados importantes para el nivel de inteligencia de los humanos, de los cuales se derivan doce requisitos para las arquitecturas, p. ej. para construir agentes inteligentes. De todas formas, las caracter\u00edsticas de los entornos, tareas y agentes son una buena cuenta de los requisitos en los agentes y entornos, los cuales han ido apareciendo\nP\u00e1gina 11\nen la literatura en los \u00faltimos quince a\u00f1os. Las caracter\u00edsticas pueden ser resumidas del siguiente modo: los entornos deben ser din\u00e1micos, reactivos y complejos, conteniendo diversos objetos interactivos, y tambi\u00e9n algunos otros agentes que afecten al rendimiento. Las tareas deben ser complejas, diversas y novedosas. Debe haber regularidades en m\u00faltiples escalas de tiempo. La percepci\u00f3n de los agentes es limitada, y el entorno puede ser parcialmente observable. Los agentes deben ser capaces de responder r\u00e1pidamente a la din\u00e1mica del entorno pero los recursos computacionales de los agentes deben considerarse limitados. Finalmente, la existencia de un agente es a largo plazo y continua, por lo que debe equilibrar las tareas o recompensas inmediatas con m\u00e1s objetivos a largo plazo. Podemos ver que ninguno de los enfoques de medici\u00f3n vistos en la subsecci\u00f3n anterior sigue estos requisitos.\nLa noci\u00f3n de agente es hoy en d\u00eda la corriente principal de la inteligencia artificial y, aparte de los requisitos previos (u otros), no merece una mayor aclaraci\u00f3n. La distinci\u00f3n entre objetivo, tarea y entorno es una cuesti\u00f3n m\u00e1s compleja, ya que depende de la intenci\u00f3n y la voluntad del agente. En la psicometr\u00eda para ni\u00f1os y la cognici\u00f3n comparativa de animales, los tests de inteligencia no pueden asumir que seamos capaces de programar algunos objetivos o explicar expl\u00edcitamente una tarea al examinado. Consecuentemente, las pruebas, se realizan generalmente utilizando recompensas, un enfoque acondicionado para que los sujetos se centren indirectamente en la tarea. Curiosamente, este es el mismo criterio adoptado en el aprendizaje por refuerzo [Sutton & Barto 1998]. A pesar de que el aprendizaje por refuerzo generalmente se ve como una formalizaci\u00f3n de este ajuste en el contexto de la inteligencia artificial y el aprendizaje de m\u00e1quinas, tambi\u00e9n se puede ver con una perspectiva m\u00e1s amplia dado a que el aprendizaje por refuerzo es el estudio de c\u00f3mo los animales y los sistemas artificiales optimizan su comportamiento condicionado por recompensas y castigos dados.\nLa idea m\u00e1s general de este ajuste es la interacci\u00f3n entre un agente y un entorno a trav\u00e9s de acciones, recompensas y observaciones, el cual es tambi\u00e9n similar a la configuraci\u00f3n que t\u00edpicamente se usa en control o teor\u00eda de sistemas, y puede ser esbozado como se ve en la Figura 1:\nobservation\nreward\nenvironment agent\naction\nFigura 1.Interacci\u00f3n entre un agente y un entorno [Legg & Hutter 2007].\nLas acciones son limitadas por un conjunto finito de s\u00edmbolos A, (p. ej. {izquierda, derecha, arriba, abajo}), las recompensas se recogen de un subconjunto R de n\u00fameros racionales entre 0 y 1, y las observaciones son tambi\u00e9n limitadas por un conjunto finito O de posibilidades (p. ej. una rejilla de celdas binarias de n\u00d7m y los objetos situados en ella, o un conjunto de diodos emisores de luz, LEDs). Utilizaremos ai, ri y oi para\nP\u00e1gina 12\n(respectivamente) denotar acciones, recompensas y observaciones en la interacci\u00f3n o ciclo i (o estado). Las recompensas y las observaciones son las salidas del entorno. El par <ri, oi> es tambi\u00e9n conocido como una percepci\u00f3n. El orden de los eventos es siempre: recompensa, observaci\u00f3n y acci\u00f3n. Una secuencia de estos eventos es entonces una cadena como r1o1a1r2o2a2. Tanto el agente como el entorno se definen como una medida probabil\u00edstica. Por ejemplo, dado un agente, denotado como \u03c0, el termino \u03c0(ak | r1o1a1r2o2a2 \u2026 rkok) denota la probabilidad de que el agente \u03c0 ejecute la acci\u00f3n ak despu\u00e9s de la secuencia de eventos r1o1a1r2o2a2 \u2026 rkok. De forma similar, un entorno \u00b5 tambi\u00e9n es una medida probabil\u00edstica la cual asigna probabilidades para cada posible par de observaciones y recompensas. Como \u00b5(rkok | r1o1a1r2o2a2 \u2026 rk-1ok-1ak-1), por ejemplo, denota la probabilidad en el entorno \u00b5 de proporcionar rkok despu\u00e9s de la secuencia de eventos r1o1a1r2o2a2 \u2026 rk-1ok-1ak-1. N\u00f3tese que si para todos los ciclos existe una \u00fanica acci\u00f3n/percepci\u00f3n con probabilidad 1 (y 0 para el resto), entonces tenemos un agente/entorno determinista. Si combinamos la medici\u00f3n probabil\u00edstica para el agente y el entorno tenemos una medida probabil\u00edstica para el historial de interacci\u00f3n o secuencia. Los historiales de interacci\u00f3n ser\u00e1n deterministas (respectivamente, computables) si tanto el agente y el entorno son deterministas (respectivamente, computables). Denotaremos con ai \u00b5,\u03c0 , ri \u00b5,\u03c0 y oi \u00b5,\u03c0 , la acci\u00f3n, la recompensa y la observaci\u00f3n en la interacci\u00f3n o ciclo i para el entorno \u00b5 y el agente \u03c0.\nEJEMPLO 1.\nConsidera el escenario de un test donde un chimpanc\u00e9 (el agente) puede presionar uno de tres posibles botones (A = {B1, B2, B3}), las recompensas son solo la entrega (o no) de una banana (R = {0, 1}) y la observaci\u00f3n son tres celdas donde una pelota debe estar exactamente dentro de una de ellas (O = {C1, C2, C3}). Un ejemplo de un posible entorno es:\n\u00b5(rkok | r1o1a1r2o2a2 ... rk\u20131ok\u20131ak\u20131) = 1 si ((ak\u20131 = B1 y ok\u20131 = C1) o (ak\u20131 = B2 y ok\u20131 = C2) o (ak\u20131 = B3 y ok\u20131 = C3)) y (rk = +1)\n\u00b5(rkok | r1o1a1r2o2a2 ... rk\u20131ok\u20131ak\u20131) = 1 si \u00ac((ak\u20131 = B1 y ok\u20131 = C1) o (ak\u20131 = B2 y ok\u20131 = C2) o (ak\u20131 = B3 y ok\u20131 = C3)) y (rk = 0)\n\u00b5(rkok | r1o1a1r2o2a2 ... rk\u20131ok\u20131ak\u20131) = 0 en cualquier otro caso.\nLa observaci\u00f3n ok en ambos casos es generada aleatoriamente con una distribuci\u00f3n uniforme entre las tres posibilidades de O. La primera recompensa es 1 (empezamos el juego d\u00e1ndole una banana al chimpanc\u00e9).\nDe acuerdo con la definici\u00f3n del entorno anterior, un chimpanc\u00e9 que siempre seleccione el bot\u00f3n correspondiente a la celda donde se encuentre la pelota puntuar\u00e1 una recompensa de +1 (una banana) en cada ciclo. Por ejemplo, si el entorno muestra C2 y el chimpanc\u00e9 presiona B2, entonces se le recompensar\u00e1 con una banana.\nAunque el ejemplo anterior es muy sencillo, la configuraci\u00f3n general que se muestra en la Figura 1, utilizando entornos, agentes, acciones, observaciones y recompensas es lo suficientemente poderoso como para representar los requisitos establecidos en [Laird &\nP\u00e1gina 13\nWray 2010], as\u00ed como cualquier medida de la inteligencia establecida vista en la subsecci\u00f3n anterior (excepto, quiz\u00e1s, muchos tipos de tests psicom\u00e9tricos como CAPTCHAs o el Test de Turing, debido a la ausencia de recompensas).\nBajo esta configuraci\u00f3n primero tenemos que pensar en un conjunto de entornos tales que sean los suficientemente complejos como para cumplir los requisitos anteriores. O, en otras palabras, no podemos asumir ninguna restricci\u00f3n en los entornos y observaciones si queremos que la configuraci\u00f3n sea lo suficientemente general para una medici\u00f3n de inteligencia. En particular, en el aprendizaje por refuerzo, muchas t\u00e9cnicas asumen que el entorno es un Markov Decision Process (MDP). En otros casos, sin embargo, se asume que los entornos son completamente observables, p. ej. que existe una funci\u00f3n entre las observaciones y los estados. No podemos asumir esto, ya que muchos problemas del mundo real no son completamente observables. Este es especialmente el caso en contextos sociales donde otros individuos pueden tener vistas diferentes (y parciales) de la misma situaci\u00f3n.\nEn [Legg 2008], se desarrolla una taxonom\u00eda de entornos, mientras que se distingue entre muchos tipos de entornos. Por ejemplo, entornos pasivos son aquellos en los cuales las acciones de los agentes solamente pueden afectar a las recompensas pero no a las observaciones. Una sub-categor\u00eda especial es la secuencia de predicci\u00f3n como se utiliza en los tests psicom\u00e9tricos cl\u00e1sicos, y tambi\u00e9n los problemas de clasificaci\u00f3n t\u00edpicos en el aprendizaje autom\u00e1tico. Algunos otros tipos de entornos son los Markov Decision Processes (MDP) de orden n, donde la siguiente observaci\u00f3n solo puede depender de las \u00faltimas n observaciones y las \u00faltimas n acciones. Se puede ver que los MDPs de orden n pueden reducirse a MDPs de primer orden (o simplemente MDPs). En este caso, es natural hablar de \u201cestados\u201d, como muchos juegos de mesa y algunos laberintos, ya que la pr\u00f3xima recompensa y observaci\u00f3n solo dependen de la observaci\u00f3n y acci\u00f3n anterior. Un tipo especial de MDPs son los MDPs erg\u00f3dicos, los cuales se caracterizan por poderse alcanzar cualquier posible observaci\u00f3n (en uno o m\u00e1s ciclos) a partir de cualquier estado. Esto significa que en cualquier estado, las acciones del agente pueden hacerle recuperarse de una mala decisi\u00f3n anterior. Creemos que esto es una gran limitaci\u00f3n y, como ha sido anteriormente mencionado y defendido por muchos (p. ej. [Laird & Wray 2010]), necesitamos ser tan generales como sea posible en la clase de entornos que necesitamos considerar.\nTrataremos de considerar el concepto m\u00e1s general (o clase) de entorno y el concepto m\u00e1s general de agente, a fin de permitir una prueba universal, en la forma en la que da cuenta de una variedad de contextos y aplicable a cualquier tipo de agente. Por el momento, \u00fanicamente asumiremos que los entornos son infinitos (p. ej. ninguna secuencia de acciones los hace parar) y que est\u00e1n basados en un modelo, p. ej., tenemos una descripci\u00f3n, programa o modelo tras de ellos. Tambi\u00e9n asumiremos que este modelo es computable.\nAparte de las caracter\u00edsticas de los entornos y agentes, cuando abordemos la cuesti\u00f3n de la evaluaci\u00f3n del agente, necesitamos estudiar sin restricci\u00f3n alguna la distribuci\u00f3n de las recompensas que se necesita, y, muy especialmente, c\u00f3mo se agregan las recompensas. En el aprendizaje por refuerzo, se han definido muchas agregaciones de funciones o ganancias. Por ejemplo, la forma m\u00e1s com\u00fan de evaluar el rendimiento de un\nP\u00e1gina 14\nagente \u03c0 en un entorno \u00b5 es calcular el valor esperado de la suma de todas las recompensas, p.ej.:\nDEFINICI\u00d3N 1. RECOMPENSA ACUMULADA ESPERADA\n( )\u2211\u221e== 1 ,: i irEV \u03c0\u00b5\u03c0\u00b5\nEsta no es la \u00fanica opci\u00f3n, ya que se puede plantear a qu\u00e9 se le da m\u00e1s relevancia, si a las recompensas inmediatas o a las de largo plazo, con el fin de recompensar las pol\u00edticas codiciosas o de exploraci\u00f3n. Esto se relaciona con la vida del agente (el n\u00famero de interacciones permitidas) y tambi\u00e9n si existe un l\u00edmite en las recompensas [Hutter 2006]. Todo esto se volver\u00e1 a examinar a continuaci\u00f3n.\n3.3. Medici\u00f3n\nSiguiendo la justificaci\u00f3n de [Hernandez-Orallo & Dowe 2010], se imponen una serie de restricciones sobre las recompensas y los entornos, con el objetivo de garantizar una serie de propiedades de convergencia y de balance frente a agentes aleatorios.\nLa primera idea es utilizar recompensas sim\u00e9tricas, las cuales pueden estar en un\nrango entre -1 y 1, p. ej.:\nDEFINICI\u00d3N 2. RECOMPENSAS SIM\u00c9TRICAS\n11: \u2264\u2264\u2212\u2200 iri\nN\u00f3tese que esto no impide que la recompensa acumulada en un cierto punto sea mayor que 1 o menor que -1. Por lo que, si hacemos muchas acciones, podemos tener una recompensa acumulada mayor que 1. Observando implementaciones f\u00edsicas, las recompensas negativas no tienen por qu\u00e9 estar asociadas con castigos, lo cual se considera poco \u00e9tico para individuos biol\u00f3gicos. Por ejemplo, si estamos evaluando un simio, las recompensas desde -1 a -1/3 podr\u00edan implicar no dar nada, desde -1/3 a 1/3 dar una pieza de fruta, y desde 1/3 a 1 dos piezas. O una recompensa negativa puede implicar eliminar una fruta adjudicada previamente.\nSi nos fijamos en las recompensas sim\u00e9tricas, tambi\u00e9n esperamos que los entornos sean sim\u00e9tricos, o m\u00e1s precisamente, que sean balanceados en como proporcionan recompensas. Esto puede verse del siguiente modo: en un test fiable, queremos que muchos (si no todos) los entornos ofrezcan una recompensa de 0 para agentes aleatorios. La siguiente definici\u00f3n lo formaliza.\nP\u00e1gina 15\nDEFINICI\u00d3N 3. ENTORNO BALANCEADO\nUn entorno \u00b5 est\u00e1 balanceado si y solo si 1) 11: \u2264\u2264\u2212\u2200 iri 2) Dado un agente aleatorio \u03c0, se mantiene la siguiente igualdad:\n( ) 0 1 , == \u2211 \u221e =i i rEV r \u03c0\u00b5\u03c0\u00b5\nEsto excluye tanto a entornos hostiles como a entornos ben\u00e9volos, p. ej. entornos donde realizando acciones aleatorias se conseguir\u00eda m\u00e1s recompensas negativas (respectivamente positivas) que positivas (respectivamente negativas). En muchos casos no es dif\u00edcil de probar que un entorno particular est\u00e1 balanceado. Para entornos complejos, la restricci\u00f3n previa se puede comprobar experimentalmente. Otra aproximaci\u00f3n es proporcionar una m\u00e1quina de referencia que \u00fanicamente genere entornos balanceados.\nN\u00f3tese que las modificaciones previas en las recompensas ahora nos permiten usar\nuna media en lugar de una recompensa acumulada, es decir:\nDEFINICI\u00d3N 4. RECOMPENSA MEDIA\ni\ni\ni n\nnV nv j\nj\n)( )(\n\u03c0 \u00b5\u03c0 \u00b5 =\nY podemos calcular el valor esperado (aunque el l\u00edmite no pueda existir) de la media previa, denotada por )( \u03c0\u00b5 jvE , para un valor grande arbitrario de n. Veamos esto con un ejemplo:\nEJEMPLO 2.\nConsidera una modificaci\u00f3n de las caracter\u00edsticas del test visto en el ejemplo 1. Un robot (el agente) puede presionar uno de tres posibles botones (A = {B1, B2, B3}), las recompensas son la no entrega de ninguna banana, una banana o dos bananas (R = {-1, 0, 1}) y la observaci\u00f3n son tres celdas donde una pelota blanca y otra negra deben estar dentro de una (pero distinta) celda, es decir (O = {0WB, 0BW, W0B, B0W, WB0, 0BW}), donde W denota que la celda tiene una pelota blanca, B denota que la celda tiene una pelota negra y 0 denota que est\u00e1 vac\u00eda. Un ejemplo de un posible entorno es:\n\u00b5(rkok | r1o1a1 ... rk\u20131ok\u20131ak\u20131) = 1 si (ak\u20131 = B1 y ok\u20131 = Wxx) o (ak\u20131 = B2 y ok\u20131 = xWx)\no (ak\u20131 = B3 y ok\u20131 = xxW) y (rk = +1)\n\u00b5( rkok | r1o1a1 ... rk\u20131ok\u20131ak\u20131) = 1 si (ak\u20131 = B1 y ok\u20131 = Bxx) o (ak\u20131 = B2 y ok\u20131 = xBx) o (ak\u20131 = B3 y ok\u20131 = xxB) y (rk = \u22121) \u00b5( rkok | r1o1a1 ... rk\u20131ok\u20131ak\u20131) = 1 si \u00ac((ak\u20131 = B1 y ok\u20131 = Wxx)\nP\u00e1gina 16\no (ak\u20131 = B2 y ok\u20131 = xWx)\no (ak\u20131 = B3 y ok\u20131 = xxW)) y\n\u00ac((ak\u20131 = B1 y ok\u20131 = Bxx) o (ak\u20131 = B2 y ok\u20131 = xBx)\no (ak\u20131 = B3 y ok\u20131 = xxB) y (rk = 0)\n\u00b5( rkok | r1o1a1 ... rk\u20131ok\u20131ak\u20131) = 0 en cualquier otro caso La observaci\u00f3n ok, se genera de entre las cuatro observaciones {0WB, 0BW, W0B, WB0} de modo uniformemente aleatorio. La primera recompensa r1 es 0.\nUn primer robot (\u03c01) tiene el comportamiento de siempre presionar el bot\u00f3n B1, p. ej. \u03c01(B1| X) para toda secuencia de X. Consecuentemente, el rendimiento de \u03c01 en este entorno es:\n2\n10 lim\n2\n1 lim\n2\n1 )( 1\n, 1\n1 =+=  \n\n\n  \n =\n\u221e\u2192\u221e\u2192\n=\n\u221e\u2192 \u2211 i n i i n i\nn k k\nn nn\nn\nn\nr EvE\nii\ni\ni j\n\u03c0\u00b5 \u03c0 \u00b5\nUn segundo robot (\u03c02) tiene el comportamiento de siempre presionar el bot\u00f3n B2, p.ej. \u03c02(B2| X) para toda secuencia de X. Consecuentemente, el rendimiento de \u03c02 en este entorno es:\n4\n10 limlim\n2\n1 lim\n4\n1 )( 1\n, 2\n2 \u2212=+\u2212+=  \n\n\n  \n =\n\u221e\u2192\u221e\u2192\u221e\u2192\n=\n\u221e\u2192 \u2211 i n i i n i i n i\nn k k\nn nn\nn\nn\nn\nn\nr EvE\niii\ni\ni j\n\u03c0\u00b5 \u03c0 \u00b5\nUn tercer robot (\u03c03) tiene el comportamiento de siempre presionar el bot\u00f3n B3, p. ej. \u03c03(B3| X) para toda secuencia de X. Consecuentemente, el rendimiento de \u03c03 en este entorno es:\n4\n10 limlim\n2\n1 lim\n4\n1 )( 1\n, 3\n3 \u2212=+\u2212+=  \n\n\n  \n =\n\u221e\u2192\u221e\u2192\u221e\u2192\n=\n\u221e\u2192 \u2211 i n i i n i i n i\nn k k\nn nn\nn\nn\nn\nn\nr EvE\niii\ni\ni j\n\u03c0\u00b5 \u03c0 \u00b5\nUn cuarto robot (\u03c04) tiene un comportamiento aleatorio. Por lo tanto el rendimiento de \u03c04 es:\n0lim 4\n1 lim\n4\n1 lim\n2\n1\n3\n1 3...)( 1\n, 4\n4 =      \u2212+\u2212+\u22c5==  \n\n\n  \n =\n\u221e\u2192\u221e\u2192\u221e\u2192\n=\n\u221e\u2192 \u2211 i i n i i n i i n i\nn k k\nn n\nn\nn\nn\nn\nn\nn\nr EvE\niii\ni\ni j\n\u03c0\u00b5 \u03c0 \u00b5\nConsecuentemente, el agente \u03c01 es mejor que el aleatorio (\u03c04) en este entorno, y \u03c02 y \u03c03 son peores. Y, finalmente, ya que la recompensa global esperada de un agente aleatorio es 0, este entorno est\u00e1 balanceado.\n3.4. Distribuci\u00f3n universal y su aplicaci\u00f3n a entornos\nEn esta secci\u00f3n daremos una breve introducci\u00f3n al \u00e1rea de la Teor\u00eda de Informaci\u00f3n Algor\u00edtmica y a las nociones de la complejidad de Kolmogorov, las distribuciones\nP\u00e1gina 17\nuniversales, la complejidad Kt de Levin, y su relaci\u00f3n con las nociones de dificultad, comprensi\u00f3n, aleatoriedad, el principio MML, la predicci\u00f3n y la inferencia inductiva. Despu\u00e9s, estudiaremos las aproximaciones que han aparecido utilizando estas nociones formales para dar definiciones matem\u00e1ticas a la inteligencia o desarrollar tests de inteligencia a partir de ellos, empezando por una comprensi\u00f3n mejorada de los tests de Turing, el C-test y la definici\u00f3n de Legg y Hutter de la Inteligencia Universal.\nLa Teor\u00eda de Informaci\u00f3n Algor\u00edtmica es un campo en la inform\u00e1tica que se relaciona adecuadamente con las nociones de la computaci\u00f3n y la informaci\u00f3n. La idea clave es la noci\u00f3n de la Complejidad de Kolmogorov de un objeto, el cual es definido como la longitud del programa m\u00e1s corto p el cual genera una cadena dada x sobre una m\u00e1quina U. Formalmente,\nDEFINICI\u00d3N 5. COMPLEJIDAD DE KOLMOGOROV\n)(min:)( )( such that plxK xpUp U = =\nDonde l(p) se refiere a la longitud en bits de p y U(p) se refiere al resultado de ejecutar p en U. Por ejemplo, si U es el lenguaje de programaci\u00f3n Lisp y x = 1010101010101010, entonces KList(x) es la longitud en bits del programa m\u00e1s corto en Lisp que genera la cadena x. La relevancia en la elecci\u00f3n de U depende mayoritariamente en el tama\u00f1o de x. Dado que cualquier m\u00e1quina universal puede emular a otra, se mantiene que para cualquier par de m\u00e1quinas U y V, existe una constante c(U,V), la cual solo depende de U y de V y no depende de x, y as\u00ed para todo x, |KU(x) \u2212 KV(x)| \u2264 c(U,V). El valor de c(U,V) es relativamente peque\u00f1o para x suficientemente largas.\nDe la definici\u00f3n previa, podemos definir la probabilidad universal para la m\u00e1quina U\ncomo sigue:\nDEFINICI\u00d3N 6. DISTRIBUCI\u00d3N UNIVERSAL\n)( 2)( xK\nU Uxp\n\u2212=\nque le da mayor probabilidad a objetos cuya descripci\u00f3n m\u00e1s corta es peque\u00f1a y menor probabilidad a objetos cuya descripci\u00f3n m\u00e1s corta sea larga. Cuando U es universal, la distribuci\u00f3n es similar (hasta una diferencia constante) a la distribuci\u00f3n universal para cualquier otra m\u00e1quina universal diferente, dado que una puede emular a la otra. Teniendo en cuenta los programas como hip\u00f3tesis en el lenguaje hipot\u00e9tico definido por la m\u00e1quina, esto allana el camino a la teor\u00eda matem\u00e1tica de la inferencia inductiva, la cual fue desarrollada por Solomonoff [Solomonoff 1964], formalizando la navaja de Occam de manera adecuada para la predicci\u00f3n, al afirmar que la predicci\u00f3n que maximiza la probabilidad universal finalmente descubrir\u00e1 cualquier regularidad en los datos, que se relaciona con la noci\u00f3n de la Longitud M\u00ednima del Mensaje (MML) para la inferencia inductiva [Wallace & Boulton 1968] [Wallace & Dowe 1999] [Wallace 2005], y tambi\u00e9n se relaciona con la noci\u00f3n de la compresi\u00f3n de datos.\nLas nociones de predicci\u00f3n e inducci\u00f3n son muy parecidas pero no id\u00e9nticas, debido a que la predicci\u00f3n se puede obtener por una combinaci\u00f3n (p. ej. Bayesiana) de varios modelos plausibles, mientras que la inducci\u00f3n normalmente se centra en descubrir el\nP\u00e1gina 18\nmodelo m\u00e1s plausible y por lo general implica una explicaci\u00f3n de las observaciones. Sin embargo, estas nociones se usan frecuentemente como sin\u00f3nimos. De hecho, el papel seminal de Solomonoff [Solomonoff 1964] se refiere a \u201cla teor\u00eda de la inferencia inductiva\u201d cuando realmente se refiere a \u201cla teor\u00eda de la predicci\u00f3n\u201d. Adem\u00e1s, tambi\u00e9n hay importantes diferencias entre la compresi\u00f3n en una parte y la compresi\u00f3n en dos partes (inducci\u00f3n MML). En el primer caso, el modelo no distingue entre patrones y excepciones mientras que la segunda expl\u00edcitamente separa las regularidades (patr\u00f3n principal) de las excepciones. V\u00e9ase [Wallace 2005] (sec. 10.1) y [Dowe 2008] (parte de sec. 0.3.1 refiri\u00e9ndose a Solomonoff) para m\u00e1s detalles de esto.\nUno de los principales problemas de la Teor\u00eda de Informaci\u00f3n Algor\u00edtmica es que la Complejidad Kolmogorov es incomputable. Una soluci\u00f3n popular al problema de la computabilidad de K() para cadenas finitas es utilizar una versi\u00f3n de tiempo limitado o ponderada de la complejidad de Kolmogorov (y por lo tanto de la distribuci\u00f3n universal de la que se deriva). Una elecci\u00f3n popular es la complejidad Kt de Levin [Levin 1973] [Li & Vitanyi 2008]:\nDEFINICI\u00d3N 7. COMPLEJIDAD KT DE LEVIN\n{ }),,(log)(min:)( )( such that xpUtimeplxKt xpUp U += =\nDonde l(p) denota la longitud en bits de p, U(p) denota el resultado de ejecutar p en U\ny time(U,p,x) denota el tiempo que U utiliza ejecutando p para producir x.\nFinalmente, a pesar de la incomputabilidad de K y la complejidad computacional de sus aproximaciones, han habido algunos esfuerzos en utilizar la Teor\u00eda de Informaci\u00f3n Algor\u00edtmica para dise\u00f1ar una b\u00fasqueda \u00f3ptima o estrategias de aprendizaje. La b\u00fasqueda de Levin (o universal) [Levin 1973] es un algoritmo de b\u00fasqueda iterativo para resolver problemas de inversi\u00f3n basados en Kt, el cual ha inspirado a otras estrategias de agente general como AIXI de Hutter, un agente que es capaz de adaptarse \u00f3ptimamente en algunos entornos [Hutter 2007], para los que existe una aproximaci\u00f3n de trabajo [Veness et al 2009].\nEn [Hernandez-Orallo & Dowe 2010] se introduce una variante de la complejidad anterior que permite asegurar que las interacciones terminan en un tiempo corto, haciendo por tanto factible la medici\u00f3n usando la distribuci\u00f3n que se deriva de ella.\nLa aproximaci\u00f3n considera un tiempo m\u00e1ximo para cada salida. Primero definimos \u2206ctime(U, p, i) como el tiempo necesario para imprimir el par <ri,oi> tras la acci\u00f3n ai\u22121, es decir el tiempo de ciclo de respuesta. A partir de aqu\u00ed podemos establecer el l\u00edmite superior para el tiempo de c\u00f3mputo m\u00e1ximo que el entorno puede consumir para generar la recompensa y la observaci\u00f3n despu\u00e9s de la acci\u00f3n del agente.\nDEFINICI\u00d3N 8. COMPLEJIDAD KT PONDERANDO LOS PASOS DE LA INTERACCI\u00d3N\n            \u2206+= \u2264= )),,((maxlog)(min:),( )( such that max ipUctimeplnxKt nixpUp U\nP\u00e1gina 19\nlo que significa la suma de la longitud del entorno m\u00e1s el logaritmo del m\u00e1ximo tiempo de respuesta de este entorno con la m\u00e1quina U. N\u00f3tese que este l\u00edmite superior puede usarse en la implementaci\u00f3n de entornos, especialmente para hacer su generaci\u00f3n computable. Para hacer esto, ya que son infinitos, definimos su complejidad para un n\u00famero l\u00edmite de ciclos n, haciendo su definici\u00f3n computable. Este l\u00edmite n no es solo necesario para la computaci\u00f3n; tambi\u00e9n es pr\u00e1ctico en algunos otros casos donde la computabilidad no es un problema pero no existe un m\u00e1ximo. Por ejemplo, considera un entorno cuya salida i-\u00e9sima dependa del c\u00e1lculo de si el n\u00famero i es primo o no. En este caso, el m\u00e1ximo \u2206ctime no est\u00e1 delimitado y, por tanto el KtmaxU de esta secuencia ser\u00eda infinito. Por lo tanto, el entorno ser\u00eda descartado si no establecemos un l\u00edmite de n.\nLa funci\u00f3n de complejidad previa asegura que el tiempo de respuesta en cualquier interacci\u00f3n con un entorno est\u00e1 delimitado, pero a\u00fan conservamos la navaja de Occam en la probabilidad derivada.\n3.5. Entornos discriminativos: Sensibilidad a las recompensas\nAdem\u00e1s, muchos entornos (tanto simples como complejos) ser\u00e1n completamente in\u00fatiles para evaluar la inteligencia, como entornos que dejan de interactuar, o entornos con recompensas constantes, o entornos que son muy similares a otros entornos usados anteriormente, etc. Incluyendo algunos, o la mayor\u00eda, de ellos en la muestra de entornos es una p\u00e9rdida de recursos de testeo; si pudi\u00e9ramos hacer una muestra m\u00e1s precisa ser\u00edamos capaces de hacer una evaluaci\u00f3n m\u00e1s eficiente. La cuesti\u00f3n es determinar un criterio no arbitrario para excluir algunos entornos. Por ejemplo, la definici\u00f3n de Legg y Hutter [Legg & Hutter 2007] fuerza a que los entornos interact\u00faen infinitamente, y puesto que la descripci\u00f3n debe ser finita, debe existir alg\u00fan patr\u00f3n, que puede ser eventualmente aprendido (o no) por el examinado. Pero esto incluye obviamente entornos que \u201csiempre producen la misma observaci\u00f3n y recompensa\u201d. De hecho, no son solo posibles sino altamente probables en muchas m\u00e1quinas de referencia. Otro caso patol\u00f3gico es un entorno cuyas \u201cobservaciones y recompensas producidas sean aleatorias\u201d, pero esto tiene una alta complejidad, si suponemos entornos deterministas. En ambos casos el comportamiento de cualquier agente en estos entornos casi ser\u00eda el mismo. En otras palabras, no tendr\u00edan poder discriminativo. As\u00ed que estos entornos ser\u00edan in\u00fatiles para discriminar entre agentes.\nEn un entorno interactivo, un requisito claro para que un entorno sea discriminativo es que lo que haga el agente debe tener consecuencias en las recompensas. Sin ninguna restricci\u00f3n, algunos (la mayor\u00eda) entornos simples ser\u00edan completamente insensibles a las acciones de los agentes. Como se ha mencionado antes, en [Legg 2008], se ha desarrollado una taxonom\u00eda de entornos, y se presenta el concepto de MDPs erg\u00f3dicos en los que siempre se puede volver a un estado anterior. Los MDP erg\u00f3dicos son una restricci\u00f3n muy importante, mientras que muchos entornos reales no nos dan una \u201csegunda oportunidad\u201d. Si las \u201csegundas oportunidades\u201d estuvieran siempre disponibles, el comportamiento de los agentes tender\u00eda a ser m\u00e1s impetuoso y menos reflexivo. Adem\u00e1s, parece m\u00e1s f\u00e1cil aprender y tener \u00e9xito en esta clase de entornos que en una clase general.\nP\u00e1gina 20\nEn lugar de eso, vamos a restringir los entornos para que sean sensibles a las acciones de los agentes. Esto significa que una acci\u00f3n equivocada (p. ej. Ir por una puerta equivocada) podr\u00eda llevar al agente a una parte del entorno desde donde nunca podr\u00e1 volver (no-erg\u00f3dico), pero al menos las acciones hechas por el agente pueden modificar las recompensas en este sub-entorno. M\u00e1s precisamente, queremos que un agente sea capaz de influenciar en las recompensas en cualquier punto del sub-entorno. Esto no implica ergodicidad pero si al menos sensibilidad a las recompensas. Esto significa que no podemos alcanzar un punto en donde las recompensas se dan independientemente de lo que hagamos (un callej\u00f3n sin salida). Esto se puede formalizar de esta manera:\nDEFINICI\u00d3N 9. ENTORNO SENSIBLE A LAS RECOMPENSAS Dado un entorno determinista \u00b5, decimos que es n-acciones sensible a las recompensas si para toda secuencia de acciones a1a2\u2026a3 de longitud k existe un entorno positivo m\u2264n tal que existen dos secuencias de acciones b1b2\u2026bm y c1c2\u2026cm cuya suma de recompensas que se obtiene con la secuencia de acciones a1a2\u2026akb1b2\u2026bm es diferente a la suma de recompensas de la secuencia a1a2\u2026akc1c2\u2026cm.\nN\u00f3tese que la definici\u00f3n anterior no significa que cualquier acci\u00f3n tiene un impacto en las recompensas (inmediata o posteriormente), pero s\u00ed que en cualquier punto siempre hay al menos dos secuencias de acciones diferentes que puede llevar al agente a obtener recompensas acumuladas diferentes para n interacciones. Esto significa que estos entornos pueden tener un agente pegado por un tiempo (en un \u201cagujero\u201d) si no se realizan las acciones buenas, pero existe un camino para salir de ah\u00ed o al menos para encontrar recompensas diferentes dentro del agujero. En otras palabras, no existen puntos cielo/infierno ni tienen un comportamiento \u201cobservador\u201d, as\u00ed que en cualquier punto el agente puede esforzarse para incrementar sus recompensas (o impedir que decrezcan).\nSeg\u00fan las definiciones anteriores, muchos juegos de mesa que conocemos no son entornos sensibles a las recompensas. Por ejemplo, existen posiciones en el juego de las damas donde inevitablemente cualquier movimiento conduce a una distinta forma de perder/ganar (asumiendo cierta habilidad por parte del oponente o usando la soluci\u00f3n perfecta dada por [Schaeffer 2007]). Pero tambi\u00e9n n\u00f3tese que no es muy dif\u00edcil modificar la puntuaci\u00f3n para hacerla completamente sensible a las recompensas asign\u00e1ndole puntos a la puntuaci\u00f3n que dependan de los movimientos y la posici\u00f3n antes de perder (p. ej. perder en 45 movimientos es mejor que perder en 25 movimientos).\nAhora vamos a dar a una definici\u00f3n m\u00e1s refinada de la Inteligencia Universal utilizando los entornos sensibles a las recompensas, las recompensas sim\u00e9tricas, el entorno balanceado y la media de las recompensas:\nP\u00e1gina 21\nDEFINICI\u00d3N 10. INTELIGENCIA UNIVERSAL (CONJUNTO FINITO DE ENTORNOS BALANCEADOS Y SENSIBLE A LAS RECOMPENSAS, N\u00daMERO FINITO DE INTERACCIONES, COMPLEJIDAD KTMAX) CON PUNTUACI\u00d3N AJUSTADA.\n\u03d2IV(\u03c0, U, m, ni) := \u2211 \u2208\u22c5 S ii nV nm j\u00b5 \u03c0 \u00b5 )( 1 donde S es un subconjunto finito de m entornos balanceados siendo ambos nr-acciones sensible a las recompensas (con nr =ni) extra\u00eddo con ),(max 2)( iU nKt U tp \u00b5\u00b5 \u2212=\n3.6. Un entorno no sesgado y balanceado\nEn esta secci\u00f3n presentamos una clase de entornos apropiada para la evaluaci\u00f3n\nsiguiendo las propiedades de la secci\u00f3n anterior."}, {"heading": "3.6.1. Clases de entornos Turing-completas", "text": "Definir cualquier clase de entorno y construir un generador de entornos a partir de ella es f\u00e1cil. De hecho, hay muchos entornos y generadores de juegos, los cuales construyen diferentes campos de juegos siguiendo algunas metareglas b\u00e1sicas (p. ej. [Pell 1994]). Una cuesti\u00f3n diferente es si queremos seleccionar una clase de entorno que sea universal (p. ej. Turing-completo) e imparcial (no particularmente f\u00e1cil para algunos tipos de agentes y dif\u00edciles para otros). El problema es especialmente inc\u00f3modo ya que no solo estamos hablando sobre un lenguaje que sea capaz de expresar estos entornos, sino un lenguaje que pueda usarse para autom\u00e1ticamente generar dichos entornos. Pi\u00e9nsese, por ejemplo, que podemos generar aleatoriamente programas en cualquier lenguaje de programaci\u00f3n, pero la probabilidad de que alguno de estos programas tenga alg\u00fan sentido es rid\u00edcula.\nEl tipo y expresividad de entornos es un problema t\u00edpico en inteligencia artificial. Por ejemplo, [Weyns, Parunak & Michel 2005] es un estudio de entornos usados en sistemas multiagente. En este estudio, podemos ver muchas formas espec\u00edficas en las cuales las acciones y las observaciones pueden expresarse.\nRadicalmente opuesto es ir a lo m\u00e1s general posible y utilizar las propias m\u00e1quinas de Turing. Esta aproximaci\u00f3n es demasiado general en muchos sentidos: haciendo un filtro de tal manera que cada computaci\u00f3n entre estados sea finita es un problema dif\u00edcil (m\u00e1s precisamente, indecidible). Pero si restringimos la clase de entorno a una clase de entorno no universal (o m\u00e1s dr\u00e1sticamente a clases de entornos finitas), no solo podemos tener una gran parcialidad sino tambi\u00e9n podemos tener algunas otras propiedades indeseables.\nAparte de las m\u00e1quinas de Turing (o variantes) hay muchos otros modelos de computaci\u00f3n Turing-completos. De hecho, en [Hernandez-Orallo & Minaya-Collado 1998] y [Hernandez-Orallo 2000a], para medir la habilidad inductiva secuencial, usamos un tipo de m\u00e1quina de registro.\nP\u00e1gina 22\nPor ejemplo, los algoritmos de Markov [Markov 1960] son formalismos Turingcompletos que pueden f\u00e1cilmente usarse para representar entornos (v\u00e9ase una definici\u00f3n de estos algoritmos y su uso para codificar entornos y agentes en [Hernandez-Orallo 2010b]).\nEl problema de utilizar un lenguaje universal como un algoritmo de Markov es que muchos conjuntos de producci\u00f3n no construyen un entorno v\u00e1lido. Pueden producir una secuencia inv\u00e1lida de recompensas, observaciones y acciones, o secuencia alguna, o secuencias muy peque\u00f1as (por lo que el entorno deja de interactuar muy pronto) o no terminan. Adicionalmente, considerando solo los entornos sint\u00e1cticamente v\u00e1lidos y terminantes, hay otras restricciones impuestas por el marco [Hernandez-Orallo & Dowe 2010]: deben ser entornos sensibles a las recompensas y deben estar balanceados con respecto a los agentes aleatorios. Y finalmente, tambi\u00e9n hay una certeza de que de este modo es muy improbable generar un entorno que sea social, p. ej. que pueda contener otros agentes con los que interactuar. Como se ha tratado frecuentemente (v\u00e9ase p. ej. [Hernandez-Orallo & Dowe 2010]) la inteligencia de los animales (y consecuentemente de los humanos) debe haber evolucionado en entornos reales llenos de objetos y otros animales.\nNuestra aproximaci\u00f3n ser\u00e1 diferente. En lugar de generar entornos de m\u00e1quinas universales incontroladas, definiremos un conjunto controlado, y entonces incluiremos dentro el comportamiento universal."}, {"heading": "3.6.2. Acciones, observaciones y el espacio", "text": "Aparte del comportamiento de un entorno, que puede variar desde ser muy simple a muy complejo, debemos primero clarificar la interfaz. \u00bfCu\u00e1ntas acciones se van a permitir? \u00bfCu\u00e1ntas observaciones diferentes? Est\u00e1 claro que el m\u00ednimo n\u00famero de acciones tienen que ser dos, pero no parece decidirse a priori ning\u00fan l\u00edmite superior. Lo mismo ocurre con las observaciones.\nSi echamos un vistazo a algunos tests de cognici\u00f3n para humanos y animales nohumanos, las acciones pueden ser muy variadas. Para los humanos, puede ser escribiendo una palabra o n\u00famero siguiendo una secuencia. Para animales, t\u00edpicamente es m\u00e1s reducido, con un n\u00famero de botones a presionar peque\u00f1o. Las observaciones pueden tener una variedad mucho mayor. Por ejemplo, una secuencia de cuadrados y s\u00edmbolos, o una secuencia de n\u00fameros o palabras, o incluso im\u00e1genes, son t\u00edpicas en los tests de IQ. Incluso en los tests para animales, podemos tener observaciones din\u00e1micas (rotaciones, movimientos, sonidos\u2026). T\u00edpicamente, incluso para entornos discretos, el espacio de observaciones se hace grande por simple combinatoria.\nAntes de meternos en detalles con la interfaz, tenemos que pensar en entornos que puedan contener agentes. Esto no solo sucede en la vida real, sino tambi\u00e9n es un requisito para la evoluci\u00f3n y, por lo tanto, para la inteligencia como la conocemos. La existencia de varios agentes que pueden interactuar requiere de un espacio. El espacio no es necesariamente un espacio f\u00edsico o virtual, sino un conjunto de reglas comunes que\nP\u00e1gina 23\ngobiernen lo que los agentes pueden percibir y hacer. En el mundo real, este conjunto de reglas comunes es la f\u00edsica.\nLo bueno de pensar en espacios es que un espacio implica la reducci\u00f3n de las percepciones y acciones posibles. Si definimos un espacio com\u00fan, tenemos muchas elecciones sobre observaciones y acciones ya adoptadas. Una primera (y com\u00fan) idea para un espacio es una rejilla 2D. Esta es la elecci\u00f3n t\u00edpica en laberintos y videojuegos 2D. A partir de una rejilla 2D, la observaci\u00f3n es una imagen de la rejilla con los objetos y los agentes dentro. Las acciones t\u00edpicas son los movimientos: izquierda, derecha, arriba y abajo. Alternativamente, por supuesto, podemos usar un espacio 3D, ya que nuestro mundo es 3D.\nEl problema de una rejilla 2D o 3D es que est\u00e1 claramente pensada a favor de los humanos y muchos otros animales que tienen habilidades para orientarse en este tipo de espacios. Otros tipos de animales o personas con discapacidades (p. ej. personas ciegas) podr\u00edan tener algunas dificultades en este tipo de espacios.\nFigura 1: Dos espacios. Una rejilla toroidal y un anillo de dos niveles.\nFigura 2: Interfaces posibles para un espacio adimensional. Izquierda: Se muestran todas las celdas.\nDerecha: Solo se muestran las celdas accesibles.\nEn vez de esto proponemos un tipo de espacio m\u00e1s general. Una rejilla 2D es un grafo con una topolog\u00eda muy especial, donde hay conceptos como la direcci\u00f3n, adyacencia, etc. Una generalizaci\u00f3n es un grafo donde las celdas est\u00e1n libremente conectadas a otras celdas sin ning\u00fan patr\u00f3n particularmente predefinido. La Figura 1 muestra dos espacios. N\u00f3tese que la proximidad actual de dos celdas (en t\u00e9rminos de flechas entre ellas) generalmente no corresponde con su proximidad en una representaci\u00f3n 2D (incluso en estos espacios relativamente regulares).\nP\u00e1gina 24\nLas observaciones y acciones pueden verse gr\u00e1ficamente, como se puede ver en la parte m\u00e1s a la izquierda de la Figura 2 la cual ilustra dos interfaces posibles. N\u00f3tese que las celdas no est\u00e1n distribuidas por su proximidad actual, sino simplemente en una l\u00ednea horizontal. La celda con el borde m\u00e1s grueso es la celda actual, y las celdas a las que apuntan las flechas son las celdas donde el agente puede moverse. Los agentes y objetos son representados por s\u00edmbolos diferentes. Dependiendo del agente (p. ej. un humano adulto o un chimpanc\u00e9), esto puede convertirse en un tipo de interfaz t\u00e1ctil (donde las celdas pueden presionarse) o en uno m\u00e1s robusto (con botones). Para agentes artificiales, realmente da lo mismo.\nN\u00f3tese que todas las conexiones entre las celdas no son expl\u00edcitas (no vemos el grafo completo, solo la parte que est\u00e1 conectada a la celda actual). Podemos restringir adem\u00e1s la observaci\u00f3n a solo la celda actual y las celdas donde llevan las acciones (derecha de la Figura 2). Esto ser\u00e1 problem\u00e1tico para algunos agentes (p. ej. ni\u00f1os peque\u00f1os y algunos animales) para los cuales lo que no ves no existe. Al final, hay muchas opciones para representar la interfaz y tenemos que ser muy cuidadosos para evitar sesgos."}, {"heading": "3.6.3. Definici\u00f3n de la clase de entorno", "text": "Despu\u00e9s de la discusi\u00f3n previa, estamos listos para hacer varias elecciones importantes y dar la definici\u00f3n de la clase de entorno. Primero debemos definir el espacio y los objetos, y entonces las observaciones, acciones y recompensas.\nCon na = |A| \u2265 2 representamos el n\u00famero de acciones, con nc \u2265 2 el n\u00famero de celdas, y con no el n\u00famero de objetos/agentes (sin incluir el agente que ser\u00e1 evaluado y dos objetos especiales conocidos como Good y Evil)."}, {"heading": "Espacio", "text": "El espacio se define como un grafo dirigido etiquetado de nc nodos (o v\u00e9rtices), donde cada nodo representa una celda. Los nodos se numeran, empezando desde 1, por lo que las celdas se referencian como C1, C2,\u2026, Cnc. Desde cada celda salen na flechas (o aristas), cada uno de ellos denota Ci \u2192\u03b1 Cj, significando que la acci\u00f3n \u03b1 \u0404 A va desde Ci hasta Cj. Al menos dos flechas han de dirigirse a diferentes celdas. Al menos una de las flechas debe llevar a la misma celda.\nUn camino desde Ci hasta Cm es una secuencia de flechas Ci \u2192 Cj, Cj \u2192 Ck,\u2026, Cl \u2192 Cm. El grafo debe ser completamente conectado, p. ej., para cada par de celdas Ci, Cj existe un camino de Ci a Cj y viceversa.\nDada la definici\u00f3n previa, la topolog\u00eda del espacio puede ser muy variada. Puede incluir una rejilla t\u00edpica, y tambi\u00e9n topolog\u00edas mucho m\u00e1s complejas. En general, el n\u00famero de acciones na es un factor que influencia mucho m\u00e1s a la topolog\u00eda que el n\u00famero de celdas nc.\nP\u00e1gina 25"}, {"heading": "Objetos", "text": "Las celdas pueden contener objetos de un conjunto predefinido de objetos \u2126, donde n\u03c9 = |\u2126|. Los objetos pueden realizar acciones siguiendo las reglas del espacio, pero aparte de estas reglas, pueden tener cualquier comportamiento (determinista o no). Los objetos pueden ser reactivos a sus observaciones. Los objetos realizan una y solo una acci\u00f3n en cada interacci\u00f3n del entorno (excepto los agentes especiales Good y Evil, que pueden realizar varias acciones de golpe).\nAparte del agente evaluable \u03c0, como ya hemos mencionado, existen dos objetos especiales, llamados Good y Evil, representados por \u2295 y \u2296 respectivamente, los cuales pueden ser vistos por el agente evaluable \u03c0. Sin embargo, son indistinguibles para el resto de objetos (incluy\u00e9ndose a s\u00ed mismos), por lo que para ellos en sus observaciones se representan con el mismo s\u00edmbolo \u2299.\nGood y Evil deben tener el mismo comportamiento. Esto no quiere decir que realicen los mismos movimientos, sino que tengan la misma l\u00f3gica o programa tras de ellos. N\u00f3tese que Good y Evil se ven mutuamente de la misma forma (e igualmente el resto de objetos excepto \u03c0).\nLos objetos pueden compartir una misma celda, excepto Good y Evil, que no pueden estar en la misma celda. Si su comportamiento les conduce a la misma celda, entonces uno (elegido aleatoriamente con la misma probabilidad) realiza el movimiento y el otro se queda en su celda original. Los objetos \u2295 y \u2296 pueden realizar varias acciones en una \u00fanica interacci\u00f3n, p. ej. pueden realizar cualquier secuencia de acciones (no-vac\u00eda). Una raz\u00f3n para esto es evitar que Good sea seguido por el agente de una manera f\u00e1cil y \u00f3ptima para conseguir recompensas positivas en la mayor\u00eda de los entornos. Los objetos son colocados aleatoriamente en las celdas en la inicializaci\u00f3n del entorno.\nAunque Good y Evil tengan el mismo comportamiento, la celda inicial la cual es asignada (aleatoriamente) a cada uno de ellos deber\u00eda determinar una situaci\u00f3n donde su comportamiento es finalmente muy asim\u00e9trico desde el punto de vista del agente \u03c0. Por ejemplo, considera el siguiente ejemplo:\nFigura 3: Un espacio anillo donde el estado inicial puede ser cr\u00edtico.\nP\u00e1gina 26\nEjemplo Imagina el espacio visto en la Figura 3 y considera el comportamiento de \u2295 y \u2296 de forma que realizan la acci\u00f3n r si y solo si el agente \u03c0 comparte una celda con cualquiera de ellos. De lo contrario, realizan la acci\u00f3n l.\nDesde el estado representado en la Figura 3, est\u00e1 claro que la situaci\u00f3n relativa de los tres objetos solo puede ser como se ha visto (\u03c0, \u2296, \u2295) o cuando el agente comparte una celda con \u2296 seguido por \u2295 a la derecha. As\u00ed, en este entorno, es imposible para \u03c0 compartir celda con \u2295, mientras que si que es posible con \u2296, aunque \u2295 y \u2296 tienen el mismo comportamiento. El estado inicial es cr\u00edtico.\nSiguiendo el ejemplo anterior, podemos definir una \u201ccl\u00e1usula c\u00edclica\u201d que funciona como sigue. Dado un entorno con na acciones, y nc celdas, calculamos un n\u00famero aleatorio n entre 1 y nc ^ (na) (uniformemente), y entonces despu\u00e9s de n interacciones, las posiciones de \u2295 y \u2296 son intercambiadas. Entonces, calculamos nuevamente otro n\u00famero aleatorio (de la misma forma) y cambiamos de nuevo las posiciones. Y as\u00ed sucesivamente. La raz\u00f3n de ser al azar es para evitar que el ciclo coincida con ning\u00fan ciclo o patr\u00f3n que est\u00e9 presente en el comportamiento de los agentes. El objetivo de esta cl\u00e1usula es evitar la relevancia del estado inicial.\nFinalmente, las primeras interacciones con el entorno pueden tener lo que llamamos \u201cbasura inicial\u201d [Hernandez-Orallo 2009]. Considera, p. ej., un comportamiento para \u2295 y \u2296 que sea \u201cempezar haciendo a1a2a0a1a1a1a0a2a2a0a1a1a0a0 y entonces hacer a0a1 siempre\u201d. La primera parte de su comportamiento es completamente aleatoria y completamente no-discriminativa. Solo cuando se llega al patr\u00f3n (la segunda parte del comportamiento), tiene sentido empezar a evaluar el comportamiento del agente. Consecuentemente, sugerimos dejar a un agente jugar durante n interacciones para superar la mayor parte de la basura inicial (si existe) y entonces empezar la evaluaci\u00f3n. El valor para n puede calcularse igual que el valor usado anteriormente para la \u201ccl\u00e1usula c\u00edclica\u201d."}, {"heading": "Observaciones y acciones", "text": "Una observaci\u00f3n es una secuencia de contenido de celdas. Cada elemento en la secuencia muestra la presencia o ausencia de cada objeto, incluyendo al agente evaluable. Adicionalmente, cada celda que es alcanzable por una acci\u00f3n incluye la informaci\u00f3n de la acci\u00f3n que lleva a la celda.\nEn particular, el contenido de cada celda es una secuencia de objetos, donde \u03c0 debe aparecer antes de \u2295 y \u2296, y del resto de objetos siguiendo sus \u00edndices. Despu\u00e9s siguen las acciones posibles, tambi\u00e9n ordenadas por su \u00edndice, y denotadas por Ai en lugar de \u03b1i. Cada secuencia contenida en la celda est\u00e1 separada por el s\u00edmbolo \u2018:\u2019.\nPor ejemplo, si tenemos un entorno con na = 2, nc = 4 y no = 2 entonces la siguiente secuencia \u03c0\u03c92A1 : \u2296 : \u2295\u03c91A2 : es una posible observaci\u00f3n vista por el agente evaluable \u03c0. El significado de esta secuencia es que en la celda 1 tenemos al agente evaluable y al objeto \u03c92, en la celda 2 tenemos a Evil, en la celda 3 tenemos a Good y al objeto \u03c91 y la celda 4 est\u00e1 vac\u00eda. Adicionalmente, vemos que podemos quedarnos en la celda 1 con la acci\u00f3n \u03b11 y podemos ir a la celda 3 con la acci\u00f3n \u03b12. La misma observaci\u00f3n ser\u00eda vista como \u03c0\u03c92A1 : \u2299 : \u2299\u03c91A2 : por el resto de objetos (incluyendo a Good y Evil).\nP\u00e1gina 27"}, {"heading": "Recompensas", "text": "Trabajaremos con la noci\u00f3n de \u201crastro\u201d y la noci\u00f3n de \u201crecompensa de celda\u201d que denotamos como r(Ci). Inicialmente, r(Ci) = 0 para todo i. Las recompensas de las celdas se actualizan con los movimientos de \u2295 y \u2296. En cada interacci\u00f3n, ponemos ri\u2295 a la recompensa de la celda donde est\u00e9 \u2295 y -ri\u2296 a la recompensa de la celda donde est\u00e9 \u2296. En cada interacci\u00f3n, todas las dem\u00e1s recompensas de las celdas se dividen entre 2. Por lo que, intuitivamente podemos ver que \u2295 va dejando un rastro positivo y \u2296 va dejando un rastro negativo. El agente \u03c0 se come la recompensa que encuentra en la celda que ocupa, actualizando la recompensa acumulada p = p + r(Ci). Con se come se refiere a que justo despu\u00e9s de obtener la recompensa, la recompensa de la celda se establece a 0.\nLos objetos \u2295 y \u2296 dan una recompensa de 0 si comparten una celda con \u03c0. Los valores de ri\u2295 y -ri\u2296 que dejan \u2295 y \u2296 en el resto de las ocasiones son tambi\u00e9n parte del comportamiento de \u2295 y \u2296 (lo cual es lo mismo, pero esto no significa que ri\u2295 = -ri\u2296, para todo i). Solo una restricci\u00f3n es impuesta en c\u00f3mo estos valores pueden ser generados, \u2200i : 0 < ri\u2295 \u2264 \u00bd y 0 < ri\u2296 \u2264 \u00bd. Finalmente, n\u00f3tese que las recompensas y los rastros no son parte de las observaciones, por lo que no se pueden (directamente) observar por un objeto (incluyendo a \u03c0).\nA fin de mantener las recompensas acumuladas entre -1 y 1 y prevenir que \u03c0 se duerma en los laureles, al final de la evaluaci\u00f3n, dividimos la recompensa acumulada por el n\u00famero final de interacciones, con un ajuste, como se ve en [Hernandez-Orallo 2010a].\nUsaremos el t\u00e9rmino \u039b para esta clase de entorno."}, {"heading": "Ejemplos", "text": "Dada la configuraci\u00f3n de la clase de entorno dada arriba, veamos algunos ejemplos\ndentro de \u039b. Empecemos con el entorno m\u00e1s simple dentro de la clase.\nEjemplo Considera na = 2 = |{\u03b11, \u03b12}|, nc = 2 y no = 0. El espacio est\u00e1 necesariamente compuesto de dos celdas y dos flechas, \u03b11 que va a la misma celda y \u03b12 que se dirige a la otra celda. El comportamiento m\u00e1s simple para \u2295 y \u2296 es ejecutar siempre \u03b11 o \u03b12, y teniendo ri\u2295 = ri\u2296 = \u00bd para siempre. Inicialmente, Good y Evil son situados aleatoriamente en las dos celdas (sin poder situarse en la misma celda). Ya que el entorno es sim\u00e9trico, considera que \u2295 est\u00e1 en la celda 1 y \u2296 est\u00e1 en la celda 2. En el caso de que el comportamiento sea \u03b11 (recuerda que es el mismo para ambos \u2295 y \u2296), entonces ambos Good y Evil se quedar\u00e1n para siempre en su celda. La recompensa ser\u00e1 entonces \u00bd si el agente empieza en la posici\u00f3n donde est\u00e1 \u2296 y se mueve al lugar donde est\u00e1 \u2295, entonces recibir\u00e1 recompensas de 0 ya que se mantiene donde est\u00e1 \u2295. Si el agente empieza donde est\u00e1 \u2295 entonces el movimiento a la celda donde est\u00e1 \u2296 recibir\u00e1 -\u00bd y pronto intentar\u00e1 volver donde est\u00e1 \u2295, recibiendo \u00bd. Aunque la media converger\u00e1 a 0, es conveniente mantenerse donde est\u00e9 \u2295. En el caso de que el comportamiento de \u2295 y \u2296 sea \u03b12, entonces ambos Good y Evil intercambiar\u00e1n siempre las celdas. Consecuentemente, la mejor estrategia para el agente ser\u00e1 moverse a la celda donde est\u00e9 \u2295, obteniendo \u00bd de media.\nP\u00e1gina 28\nOtros entornos m\u00e1s complejos pueden verse en [Hernandez-Orallo 2010b]. De hecho, con un poco de paciencia, cualquier tipo de juego o tarea que conocemos puede ser simulado utilizando esta clase de entorno, como vemos con el tres en raya en [Hernandez-Orallo 2010b]."}, {"heading": "3.6.4. Propiedades", "text": "En esta secci\u00f3n, analizamos si la clase de entorno anterior es sensible a las recompensas (el agente puede realizar acciones de forma que pueden afectar a las recompensas), y tambi\u00e9n es balanceado (un agente aleatorio tendr\u00e1 una recompensa acumulada esperada igual a 0). Para la definici\u00f3n formal de estas propiedades, v\u00e9ase [Hernandez-Orallo & Dowe 2010]. Para las pruebas de las siguientes proposiciones v\u00e9ase [Hernandez-Orallo 2010b].\nProposici\u00f3n 0.1 \u039b es sensible a las recompensas.\nProposici\u00f3n 0.2 \u039b est\u00e1 balanceado.\nLas proposiciones previas muestran que los entornos siguen los requisitos de un test\nanytime [Hernandez-Orallo & Dowe 2010]."}, {"heading": "3.6.5. Codificaci\u00f3n y generaci\u00f3n del entorno", "text": "Finalmente, para construir un test, necesitamos generar los entornos autom\u00e1ticamente y calcular su complejidad. Para generar un entorno, necesitamos generar el espacio (constantes y topolog\u00eda) y el comportamiento de todos los objetos (excepto \u03c0). La primera idea puede ser usar una gram\u00e1tica generativa, como de costumbre. Sin embargo, podemos elegir una gram\u00e1tica generativa que no sea universal pero que solo genere espacios v\u00e1lidos. Esto es un error. Con una gram\u00e1tica generativa nouniversal podr\u00eda ocurrir que la complejidad de un espacio con 100 celdas id\u00e9nticas pueda ser 100 veces la complejidad de un espacio con una celda. Pero esto va completamente en contra de la complejidad de Kolmogorov y el ordenamiento de los entornos que estamos buscando. En lugar de esto, usaremos gram\u00e1ticas generativas universales, en concreto algoritmos de Markov."}, {"heading": "Codificando y generando espacios", "text": "Primero codificamos el n\u00famero de acciones na utilizando cualquier codificaci\u00f3n est\u00e1ndar para los n\u00famero naturales (p. ej. la funci\u00f3n log* en [Rissanen 1983] [Wallace 2005]). El grafo del espacio est\u00e1 definido por un algoritmo de Markov sin restricciones en su definici\u00f3n, pero con la siguiente post-condici\u00f3n. El espacio generado tiene que definirse por una cadena como sigue (utilizamos una notaci\u00f3n en lenguaje regular)\nP\u00e1gina 29\n[{+|-}a1 + ][{+|-}a2 + ]\u2026[{+|-}ana + ] para cada celda. Esto significa que enumeramos todas las celdas, y la informaci\u00f3n en cada celda se compone de las flechas salientes (m\u00e1s precisamente a qu\u00e9 celdas llega por el n\u00famero de veces que la acci\u00f3n aparece). Utilizamos un \u00edndice toroidal donde, p. ej. 1 \u2013 2 = nc \u2013 1, por lo que podemos usar referencias positivas o negativas (este es el significado de +|-). Cuando nos referimos a la misma celda se omite la acci\u00f3n.\nEl espacio de la Figura 3 se codifica con +r+r+r+r+r y un algoritmo de Markov que lo\ngenera es:\n1. S \u2192 +r 2. \u2192 SSSSS\u2126 3. \u2126 \u2192\u2219\nCon la descripci\u00f3n y definiciones anteriores no es dif\u00edcil ver como codificar (inequ\u00edvocamente) cualquier entorno v\u00e1lido. Sin embargo, si queremos generar entornos usando algoritmos de Markov, la cosa es mucho m\u00e1s dif\u00edcil, ya que una generaci\u00f3n aleatoria de las reglas de Markov puede generar cadenas que no representen ning\u00fan entorno. Aunque las optimizaciones pueden existir, ya que no queremos perder generalidad y manejar la medici\u00f3n de la complejidad, aqu\u00ed proponemos dejar al algoritmo ejecutarse un n\u00famero limitado de iteraciones y entonces pasarle un postprocesamiento (eliminar + repetidos, s\u00edmbolos inv\u00e1lidos, etc.) y entonces comprobar si la cadena resultante es un entorno v\u00e1lido (sint\u00e1ctica y sem\u00e1nticamente).\nPara el c\u00e1lculo de la complejidad, cualquier aproximaci\u00f3n de la longitud del algoritmo de Markov (p. ej. el n\u00famero de s\u00edmbolos del algoritmo entero) ser\u00eda v\u00e1lido como una aproximaci\u00f3n a su complejidad."}, {"heading": "Codificando y generando objetos", "text": "Primero codificamos el n\u00famero de objetos n\u03c9, utilizando cualquier codificaci\u00f3n para los n\u00fameros naturales como se menciona arriba. Despu\u00e9s tenemos que codificar su celda inicial, codificando tambi\u00e9n no + 3 n\u00fameros naturales delimitados por nc (no necesariamente diferentes, consecuentemente log nc cada uno). En el caso de que \u2295 y \u2296 est\u00e9n en la misma celda, una nueva celda es generada aleatoriamente y asignada a \u2296.\nEl comportamiento (que debe ser universal) es generado por otro algoritmo de Markov como sigue. La cadena de entrada del algoritmo es la observaci\u00f3n, codificada como hemos visto en secciones anteriores. Solo con la observaci\u00f3n actual los objetos no tendr\u00edan acceso a la memoria y estar\u00edan muy limitados, por lo que dos s\u00edmbolos especiales \u2206 y \u2207 significan poner la cadena actual en memoria o recuperar la \u00faltima cadena insertada a la memoria respectivamente. Una vez el algoritmo de Markov es ejecutado, todos los s\u00edmbolos que no est\u00e1n en el conjunto de acciones A son eliminados de la cadena generada. Entonces, para todos los objetos (excepto para \u2295 y \u2296), la acci\u00f3n m\u00e1s a la derecha en la cadena es la acci\u00f3n a realizar. Para \u2295 y \u2296 se utiliza toda la cadena.\nP\u00e1gina 30\nVeamos un ejemplo del comportamiento para \u2295 y \u2296 (recordando que son iguales)\npara el espacio visto en la Figura 3 y sin considerar otros objetos:\n1. \u03c0[\u2299]L \u2192\u2219 rrr 2. \u2192\u2219 l\nLa notaci\u00f3n [] representa una parte opcional. Consecuentemente, el algoritmo previo significa realizar siempre la acci\u00f3n l a menos que \u03c0 se encuentre inmediatamente a la izquierda. En este caso, realizamos la acci\u00f3n r tres veces. Por ejemplo, si la observaci\u00f3n en la Figura 3 para \u2296 se representa por : \u03c0L : \u2299 : R\u2299 :, entonces la aplicaci\u00f3n al algoritmo previo en esta observaci\u00f3n proporciona como resultado : rrr\u2299 : R\u2299 : y despu\u00e9s del postprocesamiento (todos los s\u00edmbolos que no est\u00e1n en el conjunto de acciones A son eliminados) tendremos rrr.\nPara el c\u00e1lculo de la complejidad, alguna aproximaci\u00f3n de la longitud del algoritmo (p.\nej. el n\u00famero de s\u00edmbolos del algoritmo entero) ser\u00eda v\u00e1lido como su complejidad.\nEs importante remarcar que la complejidad de un conjunto de objetos no es la suma de las complejidades de los objetos. Una opci\u00f3n es generar un \u00fanico algoritmo de Markov para todos los objetos, pero en este caso llegar\u00edamos a la soluci\u00f3n general (pero impracticable) de la que hemos hablado al principio. Alternativamente, sugerimos utilizar un bit extra en cada regla y entonces tenemos una forma de referirnos a otras reglas en otros objetos. Consecuentemente, si 100 objetos comparten muchas reglas, la complejidad de todo el conjunto ser\u00e1 mucho m\u00e1s peque\u00f1a que la suma de las partes.\nP\u00e1gina 31"}, {"heading": "4. Implementaci\u00f3n aproximada del marco", "text": "conceptual\nYa que el marco conceptual es una idea general de c\u00f3mo debe construirse el sistema, para comenzar a construirlo hemos decidido implementar una aproximaci\u00f3n de este marco conceptual anteriormente descrito. Hemos implementado las caracter\u00edsticas y propiedades m\u00e1s generales definidas en el apartado anterior y se ha optado por realizar una simplificaci\u00f3n sobre otros aspectos m\u00e1s concretos de implementaci\u00f3n sin por ello descuidar las propiedades que debe tener el sistema. La intenci\u00f3n es poder empezar a evaluar el marco conceptual con este prototipo, para tener una primera estimaci\u00f3n de su viabilidad.\nPor una parte hemos mantenido la estructura principal: manteniendo a los agentes, entornos y espacios y las interacciones entre ellos. Por otro lado hemos simplificado otros aspectos como la construcci\u00f3n de las observaciones, la codificaci\u00f3n y generaci\u00f3n de los espacios, el comportamiento de los agentes y algunos aspectos de las interacciones entre los agentes.\nEn esta secci\u00f3n se describe c\u00f3mo se ha estructurado el programa. En concreto veremos un diagrama de clases, donde queda representado a grandes rasgos la estructura interna del sistema, las principales diferencias entre el marco conceptual antes descrito y la implementaci\u00f3n realizada, una descripci\u00f3n de las clases m\u00e1s importantes del sistema, como se han codificado y descrito los espacios y la interfaz de usuario construida para el entorno \u039b.\n4.1. Diagrama de clases\nPara facilitar el entendimiento del diagrama de clases se han suprimido las funciones\nde las clases, ya que, de lo contrario, resultaba complicado entender su estructura.\nP\u00e1gina 32\nEn la siguiente imagen podemos ver el diagrama de clases simplificado del sistema.\n4.2. Principales diferencias entre el marco conceptual y la implementaci\u00f3n realizada"}, {"heading": "4.2.1. Observaciones", "text": "Seg\u00fan el marco conceptual visto anteriormente, a los agentes se les proporcionan las observaciones como una tira de caracteres tal y como podemos ver en el apartado 3.6.3. Sin embargo, para esta primera implementaci\u00f3n, hemos optado por realizar una copia de la estructura del espacio que lo representa dentro del programa y facilitarles a todos los agentes esta misma copia. De este modo se les facilita a todos los agentes la observaci\u00f3n del entorno d\u00e1ndoles a todos una misma visi\u00f3n del entorno.\nP\u00e1gina 33"}, {"heading": "4.2.2. Objetos y agentes", "text": "Hemos realizado una distinci\u00f3n mayor entre los objetos y los agentes. En esta implementaci\u00f3n hemos optado por dividir estos dos tipos de modo que los objetos sean objetos inanimados mientras que los agentes se podr\u00e1n mover a trav\u00e9s del espacio."}, {"heading": "4.2.3. Comportamiento de los agentes", "text": "De momento no hemos codificado ning\u00fan lenguaje de especificaci\u00f3n para la generaci\u00f3n del comportamiento de los agentes. A los agentes se les indicar\u00e1 de forma manual cual ser\u00e1 su comportamiento, siendo \u00e9ste una decisi\u00f3n aleatoria, un patr\u00f3n de movimientos a seguir, etc."}, {"heading": "4.2.4. Interacciones entre agentes", "text": "Por un lado los agentes generadores de recompensas (Good y Evil) siempre dejan caer la misma recompensa pasen por la celda que pasen y compartan o no celda con cualquier otro agente. Sin embargo hemos optado por dividir las recompensas que los agentes recogen de las celdas entre todos los agentes que se encuentren en ese momento en la celda, de modo que si en una celda que contiene una recompensa de +1 se encuentran 4 agentes dentro de la celda, cada agente recibir\u00e1 +0.25 de recompensa.\n4.3. Descripci\u00f3n de las clases del sistema\nEn las siguientes subsecciones veremos la descripci\u00f3n de las clases implementadas en\nel sistema."}, {"heading": "4.3.1. Clases para representar los agentes y objetos", "text": "4.3.1.1. Agente\nPara representar a las entidades que interact\u00faan dentro del sistema se ha desarrollado la clase Agent. Estas entidades representan tanto a las personas que interaccionan con el sistema, a programas o m\u00e1quinas que pueden interactuar autom\u00e1ticamente con \u00e9ste, como a entidades que forman parte del sistema y, por lo tanto, son controladas directamente por \u00e9ste.\nAl iniciar la sesi\u00f3n cada agente se sit\u00faa aleatoriamente en una celda y, en cada interacci\u00f3n con el sistema, \u00e9ste se puede mover a las celdas adyacentes o puede decidir realizar una acci\u00f3n que le mantiene en la misma celda. De este modo los agentes tienen la libertad de moverse libremente a trav\u00e9s del espacio donde realizan la sesi\u00f3n.\nP\u00e1gina 34\nDebido a que existen varios tipos de agentes que pueden interactuar dentro del sistema, la clase Agent se ha especializado en varias clases hijas encargadas de proporcionar una funcionalidad especializada para cada tipo de agente.\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nEnvironment Entorno en el que est\u00e1 subscrito el agente\nName Nombre que identifica al agente\nActions Conjunto de acciones que puede realizar\nLocation Celda en la que se encuentra\nNObservationsSaved N\u00famero de observaciones que es capaz de recordar\nInteractionHistory Historial de interacciones realizadas con el entorno\nFunciones Descripci\u00f3n\nSetLocation(NewCell) Coloca al agente en la nueva celda\nAct() Almacena y devuelve la acci\u00f3n realizada y almacena\nel tiempo tardado en realizarla\nReward(Reward) Almacena la recompensa y termina la interacci\u00f3n\nactual\nObservation(Observation) Comienza una nueva interacci\u00f3n y almacena la\nobservaci\u00f3n\nAction() Devuelve la acci\u00f3n que realiza durante la interacci\u00f3n\ncon el entorno\n4.3.1.2. Agente Evaluable (Subclase de Agente)\nPara representar a los agentes que se desea que el sistema eval\u00fae se ha creado la clase EvaluableAgent. Los agentes que se instancien bajo esta clase son los que el sistema evaluar\u00e1.\nA continuaci\u00f3n podemos ver una tabla con las funciones de la clase.\nFunciones Descripci\u00f3n\nGetTotalReward() Calcula y devuelve la recompensa global del agente\nP\u00e1gina 35\n4.3.1.3. Agentes Good & Evil (Subclase de Agente)\nEn todo entorno existen dos agentes llamados Good y Evil. Estos agentes son los encargados de proporcionar las recompensas en el espacio. Al comenzar cada interacci\u00f3n, estos agentes dejan caer recompensas en la celda en la que se encuentren en ese momento, de forma que van dejando un rastro de recompensas en el espacio. Estas recompensas son valores num\u00e9ricos entre -1 y 1 y ser\u00e1n el objetivo de los agentes evaluables, los cuales tratar\u00e1n de conseguir la mayor recompensa posible durante la sesi\u00f3n. As\u00ed pues, si la instancia de la clase representa al agente Good, \u00e9ste deja caer recompensas positivas, mientras que el agente Evil deja caer recompensas negativas. Al finalizar cada interacci\u00f3n las recompensas que se encuentren por todo el entorno se dividen entre 2, disminuyendo as\u00ed progresivamente su valor.\nPor norma general estos agentes tratan de moverse a una celda adyacente. Como caso excepcional, ambos agentes no pueden encontrarse simult\u00e1neamente en la misma celda, por lo que si se diera la situaci\u00f3n de que ambos agentes tratasen de moverse a la misma celda al mismo tiempo, uno de ellos no podr\u00e1 realizar su \u00faltima acci\u00f3n, qued\u00e1ndose as\u00ed en la celda en la que se encontraba en la interacci\u00f3n anterior. De este modo se consigue que ambos agentes no se encuentren nunca en la misma celda.\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nRandomMovements Indica si el agente se mover\u00e1 de forma aleatoria o,\npor el contrario, seguir\u00e1 un patr\u00f3n de movimientos\nMovements Patr\u00f3n que contiene la secuencia de movimientos\nque realizar\u00e1 indefinidamente el agente\nFunciones Descripci\u00f3n\nAction() Devuelve la acci\u00f3n a realizar tanto si el agente act\u00faa\nde forma aleatoria o siguiendo un patr\u00f3n\n4.3.1.4. Agente Humano (Subclase de Agente Evaluable)\nDentro de los agentes evaluables se encuentra HumanAgent. Esta clase permite al usuario interactuar con el sistema. En cada interacci\u00f3n, el agente puede elegir de entre las acciones posibles, qu\u00e9 acci\u00f3n realizar, teniendo tambi\u00e9n la opci\u00f3n de permanecer en la misma celda y, por lo tanto, no moverse en esa interacci\u00f3n.\nP\u00e1gina 36\nA continuaci\u00f3n podemos ver una tabla con las funciones de la clase.\nFunciones Descripci\u00f3n\nAction() Espera a recibir la acci\u00f3n del usuario y la devuelve\n4.3.1.5. Agente Aleatorio (Subclase de Agente Evaluable)\nDentro de los agentes evaluables tambi\u00e9n se encuentra RandomAgent. Este agente es controlado por el sistema y representa a un agente que realiza acciones dentro del espacio de forma completamente aleatoria, eligiendo en cada interacci\u00f3n una acci\u00f3n cualquiera de entre todas las posibles dentro del rango de acciones permitidas.\nPara controlar el rango de tiempo que tardar\u00e1 un agente aleatorio en realizar una acci\u00f3n, se le especifica el tiempo m\u00ednimo y m\u00e1ximo del que dispondr\u00e1 para realizar dicha acci\u00f3n. Este tiempo ser\u00e1 \u201cvirtual\u201d, de modo que el agente realmente no tardar\u00e1 este tiempo en realizar cada acci\u00f3n.\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nMinimumTime Indica el tiempo m\u00ednimo que tardar\u00e1 en realizar un\nmovimiento\nMaximumTime Indica el tiempo m\u00e1ximo que tardar\u00e1 en realizar un\nmovimiento\nFunciones Descripci\u00f3n\nAction() Devuelve la acci\u00f3n a realizar elegida aleatoriamente\n4.3.1.6. Agente Observador (Subclase de Agente Evaluable)\nAl igual que el agente aleatorio, el ObserverAgent est\u00e1 controlado por el sistema. A diferencia del agente aleatorio, este agente puede mirar las celdas que tiene a su alrededor para decidir a cu\u00e1l de ellas moverse. Para ello comprueba si est\u00e1n los agentes Good o Evil en alguna de estas celdas, en el caso de encontrar al agente Good se mover\u00e1 a la celda donde \u00e9ste se encuentre, de lo contrario, se mover\u00e1 a cualquier otra celda adyacente siempre y cuando no se encuentre el agente Evil en esa celda.\nP\u00e1gina 37\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nMinimumTime Indica el tiempo m\u00ednimo que tardar\u00e1 en realizar un\nmovimiento\nMaximumTime Indica el tiempo m\u00e1ximo que tardar\u00e1 en realizar un\nmovimiento\nGoodAgent Referencia al agente Good dentro del entorno actual\nEvilAgent Referencia al agente Evil dentro del entorno actual\nFunciones Descripci\u00f3n\nAction() Devuelve la acci\u00f3n a realizar mirando las celdas\nadyacentes\n4.3.1.7. Objeto\nLos objetos (Objeto) representan entidades inanimadas, las cuales, al igual que los agentes, se encuentran en todo momento en una celda, con la salvedad de que estos objetos no pueden moverse de la celda en la que se encuentren, permaneciendo as\u00ed en la misma celda durante el transcurso de la sesi\u00f3n.\nEstos objetos sirven para enriquecer la definici\u00f3n del entorno, de modo que, por ejemplo, los agentes pueden recordar haber pasado por una celda al ver que en \u00e9sta se encuentra un objeto que ya vio anteriormente. As\u00ed el agente tiene m\u00e1s informaci\u00f3n de la zona del espacio en la que se encuentra y podr\u00e1 decidir mejor que acci\u00f3n realizar.\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nName Nombre que identifica al objeto\nLocation Celda en la que se encuentra\nFunciones Descripci\u00f3n\nSetLocation(NewCell) Coloca al objeto en la nueva celda\nP\u00e1gina 38"}, {"heading": "4.3.2. Clases para representar el espacio", "text": "4.3.2.1. Celda\nUna celda (Cell) representa una ubicaci\u00f3n donde se pueden situar elementos durante el transcurso de las sesiones. Dentro de cada celda se pueden situar tanto objetos como agentes. Estos agentes pueden ir cambiando de celda durante cada interacci\u00f3n, mientras que los objetos permanecen inm\u00f3viles en su celda durante el transcurro de la sesi\u00f3n. Adem\u00e1s, cada celda contiene la recompensa que deja caer los agentes Good y Evil si alguno de estos pasa en alg\u00fan momento por \u00e9sta, e ir\u00e1 disminuyendo su valor dividi\u00e9ndose entre 2 en cada iteraci\u00f3n.\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nNumber N\u00famero que identifica la celda del conjunto de\nceldas\nObjects Lista de objetos que se encuentran situados en la\ncelda\nAgents Lista de agentes que se encuentran situados en la\ncelda\nReward Recompensa que queda en la celda\nFunciones Descripci\u00f3n\nClone() Crea una copia de la celda actual\nGetActions() Devuelve el conjunto de acciones que se pueden\nrealizar desde la celda\nMove(Action) Devuelve la celda a la que se accede si se realizase la\nacci\u00f3n Action\n4.3.2.2. Conexi\u00f3n\nAl igual que en la teor\u00eda de grafos, donde para pasar de un v\u00e9rtice a otro contiguo se debe atravesar una arista, para moverse de una celda a otra contigua se debe pasar a trav\u00e9s de una conexi\u00f3n (Connection). Cada conexi\u00f3n representa la acci\u00f3n que se debe gastar desde la celda que la contiene para poder llegar a la celda contigua. De modo que al encontrarse en una celda donde exista una conexi\u00f3n representando a una acci\u00f3n, al utilizar esta acci\u00f3n se llegar\u00e1 a la celda con la que conecte esta conexi\u00f3n.\nP\u00e1gina 39\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nAction Acci\u00f3n que el agente debe realizar para utilizar la\nconexi\u00f3n\nFunciones Descripci\u00f3n\nGetStartCell() Devuelve la celda que contiene a la conexi\u00f3n\nGetEndCell() Devuelve la celda a la que se llega tras atravesar la\nconexi\u00f3n\n4.3.2.3. Espacio\nToda sesi\u00f3n se realiza dentro de un espacio (Space), donde los agentes podr\u00e1n moverse libremente. Este espacio se define como un grafo dirigido y, por lo tanto, hereda las caracter\u00edsticas de estos grafos, conteniendo as\u00ed una lista de v\u00e9rtices que representan a las celdas contenidas en el espacio, y una lista de aristas las cuales representan las conexiones entre las celdas.\nA continuaci\u00f3n podemos ver una tabla con las funciones de la clase.\nFunciones Descripci\u00f3n\nGetNumberOfCells() Devuelve el n\u00famero de celdas que existen en el\nespacio\nLocateObject(Object, Number)\nColoca un objeto en la celda con n\u00famero Number\nLocateAgent(Agent, Number)\nMueve un agente a la celda con n\u00famero Number"}, {"heading": "4.3.3. Clases para representar los entornos", "text": "4.3.3.1. Entorno\nEn los entornos (Environment) se engloban todos los elementos necesarios para poder realizar las sesiones. Esta clase tiene asociados unos agentes y sus acciones y recompensas, determina el agente que se evaluar\u00e1, qu\u00e9 acciones podr\u00e1n realizar los agentes, etc. y se encarga de realizar la secuencia de acciones e interacciones\nP\u00e1gina 40\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nActions Acciones permitidas\nMinReward M\u00ednima recompensa que se podr\u00e1 devolver a un\nagente\nMaxReward M\u00e1xima recompensa que se podr\u00e1 devolver a un\nagente\nAgents Lista de agentes que interact\u00faan\nObjects 1 Lista de objetos definidos en el espacio\nFunciones Descripci\u00f3n\nSetEvaluableAgent(Agent) Indica qu\u00e9 agente va a ser evaluado\nAddAgent(Agent) A\u00f1ade un agente a la lista de agentes\nInteract (Number of Interactions) Realiza una sesi\u00f3n durante el n\u00famero de interacciones definido\nInteractWithTime(Time) Realiza una sesi\u00f3n durante el tiempo definido\n4.3.3.2. Entorno_L (Subclase de Entorno)\nUna de las principales clases del sistema es la clase Environment_L que describe un tipo de entorno adecuado para realizar tests de inteligencia como se describe en el capitulo 3 (secci\u00f3n 3.5). En esta clase se puede definir, adem\u00e1s de lo que permite su clase padre, c\u00f3mo ser\u00e1 el espacio donde interactuar\u00e1n los agentes. Adem\u00e1s, integra autom\u00e1ticamente a los agentes Good y Evil dentro del entorno.\nPara conseguir que la evaluaci\u00f3n sea no sesgada para un agente aleatorio (su esperanza sea 0) existe la posibilidad de relocalizar la posici\u00f3n de los agentes Good y Evil cada vez que transcurra cierto n\u00famero de interacciones.\n1 Esta funcionalidad no se desarrolla en este PFC.\nP\u00e1gina 41\nA continuaci\u00f3n podemos ver una tabla con los atributos y funciones de la clase.\nAtributos Descripci\u00f3n\nSpace Espacio donde se realiza la sesi\u00f3n\nSpaceDescription Descripci\u00f3n textual del espacio\nGoodAgent Agente Good presente en la sesi\u00f3n\nEvilAgent Agente Evil presente en la sesi\u00f3n\nFunciones Descripci\u00f3n\nInteract (Number of Interactions) Realiza la sesi\u00f3n durante el n\u00famero de interacciones definido\nInteract (Number of Interactions, Number of Interactions to relocate) Realiza la sesi\u00f3n durante el n\u00famero de interacciones definido y relocaliza a los agentes Good y Evil cada n\u00famero de interacciones para relocalizar\nInteractWithTime(Time) Realiza la sesi\u00f3n durante el tiempo definido\nInteractWithTime(Time, Number of Interactions to relocate) Realiza la sesi\u00f3n durante el tiempo definido y relocaliza a los agentes Good y Evil cada n\u00famero de interacciones para relocalizar\nGenerateSpace Genera aleatoriamente el espacio donde se realiza la\nsesi\u00f3n\nInterpretSpace (Description)\nInterpreta y construye el espacio a partir de su descripci\u00f3n\n4.3.3.3. Realizaci\u00f3n de una sesi\u00f3n de evaluaci\u00f3n\n4.3.3.3.1. Preparaci\u00f3n del entorno\nAl iniciar una sesi\u00f3n, se debe especificar en el entorno cu\u00e1l ser\u00e1 el agente que se desea\nevaluar y c\u00f3mo ser\u00e1 el espacio en el que realizar\u00e1 la sesi\u00f3n.\nAl construir el entorno se crean autom\u00e1ticamente los agentes Good y Evil y se a\u00f1aden\na la lista de agentes que el entorno va a manejar.\nAntes de comenzar la interacci\u00f3n con los agentes, \u00e9stos deben suscribirse al entorno para que puedan interactuar con \u00e9ste. Sin embargo, solamente uno podr\u00e1 ser el agente a evaluar, debiendo indicarlo antes de comenzar la sesi\u00f3n.\nP\u00e1gina 42\n4.3.3.3.2. Descripci\u00f3n del espacio\nPara describir c\u00f3mo ser\u00e1 el espacio donde se realizar\u00e1 la sesi\u00f3n, existen dos m\u00e9todos\npara definir la distribuci\u00f3n de las celdas y las conexiones entre estas:\n\u2022 Dejar que el entorno genere aleatoriamente el espacio obteniendo as\u00ed una descripci\u00f3n del entorno, para posteriormente interpretarlo y construirlo.\no Para ello se ha dise\u00f1ado un mecanismo en donde, para cada celda, se van generando las acciones que se pueden realizar y con qu\u00e9 celda se conecta a\ntrav\u00e9s de dicha acci\u00f3n. De este modo conseguimos un espacio en donde todas las celdas se comunican con las dem\u00e1s celdas a trav\u00e9s de las acciones.\n\u2022 Proporcionar manualmente la descripci\u00f3n de c\u00f3mo debe ser el espacio y construirlo en funci\u00f3n de \u00e9sta.\no Otro mecanismo para definir el espacio es la introducci\u00f3n manual de la representaci\u00f3n del espacio. La representaci\u00f3n usada para construir el\nespacio utilizando este mecanismo es la misma que los espacios creados a trav\u00e9s del mecanismo anterior.\nNo todos los espacios generados son v\u00e1lidos para las sesiones que se desean realizar. Por ejemplo, un espacio en el que no se pueda acceder de ninguna forma a una celda no ser\u00e1 v\u00e1lido para la evaluaci\u00f3n de la sesi\u00f3n. Para que un espacio sea v\u00e1lido, todas las celdas deber\u00e1n estar conectadas entre s\u00ed, sin dejar ninguna celda aislada del resto. Es decir, como se vio en la secci\u00f3n 3.5 el espacio debe ser un grafo completamente conectado.\nEn esta implementaci\u00f3n se le facilita al usuario unas funciones en donde podr\u00e1 decidir si desea generar autom\u00e1ticamente un espacio conectado, fuertemente conectado o si, por el contrario, desea crearlo siguiendo la descripci\u00f3n que \u00e9l proporciona quedando bajo su responsabilidad que el espacio est\u00e9 o no balanceado. En el caso de que decida generar autom\u00e1ticamente el espacio, el sistema se encargar\u00e1 de generar un espacio y comprobar\u00e1 que sea conectado (o fuertemente conectado, dependiendo de c\u00f3mo quiere el usuario que sea el espacio), en caso de que no lo sea volver\u00e1 a generar autom\u00e1ticamente otro nuevo espacio hasta generar uno que cumpla con la conectividad requerida por el usuario.\n4.3.3.3.3. Comienzo de la sesi\u00f3n\nUna vez que se inicia la sesi\u00f3n, el sistema prepara el entorno para poder comenzar.\n\u2022 En primer lugar se sit\u00faa cada agente en una celda elegida aleatoriamente. \u2022 Hay que tener en cuenta que los agentes Good y Evil no pueden encontrarse en\nning\u00fan momento en la misma celda, por lo que, en el caso de que al colocarlos inicialmente se encuentren en la misma celda, se deben recolocar de nuevo hasta que acaben en celdas distintas.\nP\u00e1gina 43\n\u2022 Finalmente se les asocia una recompensa a todas las celdas de 0, indicando as\u00ed que todas las celdas tienen inicialmente una recompensa nula.\n4.3.3.3.4. Bucle principal\nTras tener preparado el entorno, el sistema comienza las interacciones con los\nagentes, siguiendo estos pasos:\n\u2022 Si en un principio se quer\u00eda que los agentes Good y Evil se resituasen transcurridos un cierto n\u00famero de interacciones y ha llegado el momento, se procede a su\nrelocalizaci\u00f3n.\n\u2022 Inicialmente los agentes Good y Evil dejan caer sus respectivas recompensas en la celda en la que se encuentran, dejando caer el agente Good una recompensa\npositiva y el agente Evil una recompensa negativa.\n\u2022 Posteriormente se crea una copia del espacio tal y como se encuentra en ese preciso instante, el cual ser\u00e1 el que se les mande a los agentes como la\nobservaci\u00f3n del espacio.\n\u2022 A continuaci\u00f3n y para cada agente: o Se entrega la recompensa de la \u00faltima interacci\u00f3n que realizaron. En caso de\nser la primera interacci\u00f3n se le da al agente una recompensa inicial de 0.\no Se manda la observaci\u00f3n actual del entorno. o Cada agente proporciona la acci\u00f3n que desea realizar en la iteraci\u00f3n actual.\n\u2022 Una vez que todos los agentes han interactuado en la iteraci\u00f3n actual, se actualiza el estado del entorno realizando simult\u00e1neamente las acciones de todos los\nagentes.\n\u2022 Ya que los agentes Good y Evil no deben poder encontrarse en la misma celda, se comprueba si tras realizar el \u00faltimo movimiento ambos se encuentran\nsimult\u00e1neamente en la misma celda. Si ambos se encontrasen en la misma se procede de la siguiente manera para resolver el conflicto:\no Si el agente Good no se movi\u00f3 en esta interacci\u00f3n, se devuelve al agente Evil a la celda en la que se encontraba. o An\u00e1logamente a la situaci\u00f3n anterior, si el agente Evil no se movi\u00f3 se devuelve al agente Good a la celda en la que se encontraba. o Si ambos agentes se movieron durante la interacci\u00f3n, se decide aleatoriamente cual de los dos agentes no se mueve en esta interacci\u00f3n y es\ndevuelto a la celda en la que se encontraba.\n\u2022 Una vez que todos los agentes han sido colocados, se les almacena, para su posterior entrega en la siguiente iteraci\u00f3n, la recompensa de la celda a la que se\nmovieron. Dada la situaci\u00f3n de que varios agentes acaben en la misma celda, la recompensa que est\u00e9 en \u00e9sta se divide entre el n\u00famero de agentes que se encuentren en dicha celda.\n\u2022 Como \u00faltimo paso del bucle se actualizan las recompensas de todas las celdas, dividiendo todas entre 2 y colocando una recompensa de 0 en las celdas donde se\nencuentre alg\u00fan agente.\nP\u00e1gina 44\nEn la secci\u00f3n A1.1 podemos ver el c\u00f3digo fuente del bucle principal escrito en JAVA.\n4.3.3.3.5. Finalizaci\u00f3n de la sesi\u00f3n\n\u2022 Finalmente, cuando se ha completado el n\u00famero de interacciones o el tiempo disponible para realizar la sesi\u00f3n, el sistema la da por terminada y muestra el\nresultado que ha obtenido el agente que se est\u00e1 evaluando, obteniendo as\u00ed la medida de rendimiento o inteligencia que ha mostrado el agente durante el transcurso de la sesi\u00f3n."}, {"heading": "4.3.4. Otras clases auxiliares", "text": "4.3.4.1. Interacci\u00f3n\nTodos los agentes guardan un historial de las interacciones (Interaction) realizadas con el entorno durante la sesi\u00f3n en donde pueden consultar todo lo que realizaron durante \u00e9sta.\nPara poder guardar la observaci\u00f3n de cada interacci\u00f3n se ha tenido que duplicar la informaci\u00f3n contenida en el espacio, ya que, de lo contrario, al guardar directamente la informaci\u00f3n del espacio y realizar modificaciones, la informaci\u00f3n guardada ser\u00eda modificada.\nUn inconveniente de tener que duplicar la informaci\u00f3n contenida en el espacio, es que esta informaci\u00f3n es la que m\u00e1s espacio ocupa (computacionalmente hablando), por lo que se ha tenido que dar la posibilidad de dejar un m\u00e1ximo de las \u00faltimas interacciones a almacenar, de modo que el agente podr\u00e1 \u201crecordar\u201d hasta un m\u00e1ximo de observaciones pasadas (Atributo NObservationsSaved en Agent).\nA continuaci\u00f3n podemos ver una tabla con los atributos de la clase.\nAtributos Descripci\u00f3n\nObservation Observaci\u00f3n proporcionada por el entorno\nAction Acci\u00f3n realizada por el agente\nReward Recompensa devuelta por el entorno\nElapsedTime Tiempo tardado en realizar la acci\u00f3n\n4.3.4.2. Observaci\u00f3n\nPara permitir distintos tipos de representaciones de las observaciones se ha creado una estructura de clases en donde se almacenar\u00e1 la observaci\u00f3n. Existen dos opciones\nP\u00e1gina 45\npara representar las observaciones: las \u201cObservaciones Simples\u201d (Simple Observation) que representan a las observaciones como una tira de caracteres, y las \u201cObservaciones L\u201d (Observation L) en donde se almacena la estructura del espacio con los objetos y agentes presentes en \u00e9l para representar las observaciones.\n4.4. Codificaci\u00f3n y descripci\u00f3n de espacios\nExisten dos formas de construir los espacios: proporcionando manualmente una\ndescripci\u00f3n del espacio o dejando que el entorno lo genere aleatoriamente.\nEn ambos casos es necesaria una descripci\u00f3n de c\u00f3mo ser\u00e1 el espacio para que posteriormente el entorno lo interprete y lo genere utilizando un grafo dirigido para representarlo.\nA continuaci\u00f3n veremos c\u00f3mo se codifican estos espacios y los distintos\nprocedimientos para su construcci\u00f3n."}, {"heading": "4.4.1. Codificaci\u00f3n del espacio", "text": "Para codificar los espacios se ha optado por crear una descripci\u00f3n textual de \u00e9stos donde se indicar\u00e1: cu\u00e1ntas celdas contiene el espacio y las conexiones existentes entre estas celdas.\nEn primer lugar se divide el espacio por partes en funci\u00f3n del n\u00famero de celdas que tenga, de modo que si tiene 5 celdas, la descripci\u00f3n del espacio se dividir\u00e1 en 5 partes separadas por barras verticales \u201c|\u201d.\nUna vez ya dividido el espacio por celdas, se procede a codificar qu\u00e9 acciones se pueden hacer desde cada una de las celdas y a qu\u00e9 celda se llegar\u00e1 a trav\u00e9s de dicha acci\u00f3n. Para ello inicialmente se indican, de forma num\u00e9rica, todas las acciones que se pueden realizar durante la sesi\u00f3n y, tras cada acci\u00f3n, a qu\u00e9 celda se mover\u00e1 una vez realizada la acci\u00f3n. Para poder indicar a qu\u00e9 celda se mover\u00e1 se ha optado por representar el movimiento como un desplazamiento de un n\u00famero de celdas a partir de la celda actual, indic\u00e1ndolo como una sucesi\u00f3n de \u201c+\u201d o \u201c-\u201d, que representar\u00e1n un movimiento hacia delante o hacia atr\u00e1s respectivamente. En caso de que una acci\u00f3n no tenga ning\u00fan desplazamiento significar\u00e1 que desde esa celda el uso de esa acci\u00f3n no har\u00e1 cambiar al agente de celda. Si al intentar realizar un desplazamiento hacia delante se llegase a una celda fuera del n\u00famero de celdas se seguir\u00e1 a partir de la primera celda y, an\u00e1logamente, al intentar realizarlo hacia atr\u00e1s se seguir\u00e1 a partir de la \u00faltima celda. Es decir se sigue un razonamiento circular.\nPara entender mejor la codificaci\u00f3n de los espacios vamos a ver un ejemplo:\nComo descripci\u00f3n del espacio se tiene la siguiente tira de caracteres:\n1+2++3|1+23-|1+23|1+2--3-\nP\u00e1gina 46\nComo podemos ver en la descripci\u00f3n, el espacio se divide en 4 celdas y se pueden realizar 3 acciones. Desde la primera celda se llega hasta la segunda celda tras realizar la acci\u00f3n 1, la acci\u00f3n 2 nos lleva a la tercera celda y la acci\u00f3n 3 deja al agente en la misma celda. Si vemos la descripci\u00f3n de la cuarta celda podemos ver como la acci\u00f3n 1 nos mueve a la primera celda, con la acci\u00f3n 2 llegamos a la segunda celda y la acci\u00f3n 3 nos devuelve a la tercera celda.\nEn la siguiente imagen podemos ver una representaci\u00f3n gr\u00e1fica del espacio descrito."}, {"heading": "4.4.2. Descripci\u00f3n manual del espacio", "text": "Una forma de construir el espacio es proporcion\u00e1ndole directamente al entorno, utilizando la codificaci\u00f3n antes descrita, un tira de caracteres describiendo c\u00f3mo debe ser el espacio que se va a utilizar. Una vez obtenida la descripci\u00f3n del espacio, el entorno lo generar\u00e1 autom\u00e1ticamente construyendo el grafo dirigido que lo representa, creando las celdas y conexiones necesarias para representar adecuadamente el espacio descrito."}, {"heading": "4.4.3. Descripci\u00f3n aleatoria del espacio", "text": "La segunda forma de construir el espacio es dejando que el entorno se encargue autom\u00e1ticamente de generar de forma aleatoria la descripci\u00f3n del espacio que se utilizar\u00e1 para la sesi\u00f3n para que, posteriormente, sea interpretado y generado de igual modo que en la forma manual.\nP\u00e1gina 47\nPara generar aleatoriamente el espacio se ha decidido utilizar el siguiente m\u00e9todo:\n\u2022 Inicialmente se decide cu\u00e1l es el n\u00famero de celdas de las que se compone el espacio a generar. Para ello se ha utilizado una distribuci\u00f3n universal como la\ndescrita en la secci\u00f3n 3.3, pero para enteros. En esta distribuci\u00f3n se considera que el n\u00famero con mayor probabilidad de salir ser\u00e1 el n\u00famero m\u00e1s peque\u00f1o desde el que iniciamos la distribuci\u00f3n, con un 50% de probabilidad, en caso de no elegirse este n\u00famero se pasar\u00e1 a considerar el siguiente n\u00famero con la misma probabilidad (50%) y as\u00ed sucesivamente hasta que se decida el n\u00famero de casillas de las que se dispondr\u00e1. El n\u00famero m\u00ednimo de celdas que tendr\u00e1 el espacio es 2.\n\u2022 Al igual que para elegir el n\u00famero de celdas, se utiliza una distribuci\u00f3n universal para decidir el n\u00famero de acciones disponibles, utilizando como cota m\u00e1xima el\nn\u00famero de celdas que tendr\u00e1 el espacio, ya que, si hubieran m\u00e1s acciones que celdas, dos acciones de la misma celda siempre acabar\u00e1n haciendo lo mismo.\n\u2022 Una vez que disponemos del n\u00famero de celdas y de acciones que tendr\u00e1 el espacio, generamos aleatoriamente (con una distribuci\u00f3n uniforme), para cada\ncelda y cada acci\u00f3n, con qu\u00e9 celda se conectar\u00e1 (con cada una de las acciones) cada una de las celdas, pudi\u00e9ndose incluso decidir que una acci\u00f3n no conectar\u00e1 la celda con ninguna otra y asumi\u00e9ndose en este caso que la acci\u00f3n lo mantiene en la celda. Para ello se elige: el s\u00edmbolo indicando si la conexi\u00f3n se har\u00e1 con una celda anterior o posterior y un n\u00famero aleatorio entre 0 y el n\u00famero de celdas del espacio para saber cu\u00e1ntas celdas se desplazar\u00e1 el agente.\nEn la secci\u00f3n A1.2 podemos ver el c\u00f3digo fuente de la generaci\u00f3n aleatoria de espacios\nescrito en JAVA.\n4.5. Interfaz\nA continuaci\u00f3n vamos a explicar en detalle c\u00f3mo funciona la interfaz de usuario del\nprograma.\nNada m\u00e1s empezar podemos elegir en el men\u00fa inicial cu\u00e1l ser\u00e1 el agente evaluable que utilizaremos para realizar la sesi\u00f3n (humano o aleatorio) y c\u00f3mo queremos que se construya el espacio (de forma manual o aleatoria).\nUna vez decidida la configuraci\u00f3n que utilizaremos, podemos empezar la sesi\u00f3n.\nAl comenzar la sesi\u00f3n, el entorno se prepara para empezar, preparando el espacio y colocando aleatoriamente a los agentes que interactuar\u00e1n durante el transcurso de la sesi\u00f3n.\nP\u00e1gina 48\nEn la siguiente imagen podemos ver c\u00f3mo queda el entorno tras indicarle que el agente evaluable ser\u00e1 un agente humano, y que deseamos utilizar un espacio definido por nosotros. Para explicar la interfaz hemos habilitado la visibilidad de las recompensas en cada celda, sin embargo para realizar una sesi\u00f3n real esta informaci\u00f3n permanecer\u00e1 oculta, por lo que los agentes no podr\u00e1n utilizarla.\nComo podemos ver, la celda con el borde m\u00e1s grueso indica d\u00f3nde se encuentra situado el agente que el sistema est\u00e1 evaluando. A su vez, dentro de cada celda podemos ver una representaci\u00f3n gr\u00e1fica de los agentes y objetos que est\u00e1n situados dentro de \u00e9stas. Entre los agentes vemos representados: al agente que se est\u00e1 evaluando como \u03c0, al agente Good como \u2295 y al agente Evil como \u2296.\nA continuaci\u00f3n y en cada celda podemos ver de forma textual: la recompensa que tiene la celda, los agentes que est\u00e1n situados en \u00e9sta y la lista de acciones que se puede realizar y con qu\u00e9 celda conecta la acci\u00f3n.\nFinalmente podemos ver en tiempo real la recompensa global que est\u00e1 teniendo el\nagente durante el transcurso de la sesi\u00f3n.\nPuesto que el agente a evaluar es humano, el entorno nos permite seleccionar qu\u00e9 movimientos queremos realizar. Para ello podemos elegir utilizando los botones situados en la secci\u00f3n Movements la celda a la que queremos movernos.\nP\u00e1gina 49\nEn la imagen siguiente podemos ver c\u00f3mo reacciona el entorno tras realizar el\nmovimiento B (correspondiente a la acci\u00f3n 1).\nComo podemos ver, el agente evaluable ha recogido la recompensa 1 que estaba situada en la celda 2, mientras que los agentes Good y Evil se han movido a las celdas 1 y 4 respectivamente, dejando caer en sus respectivas celdas sus recompensas. Si nos fijamos en la celda 3 podemos ver como el agente Evil ha dejado tras de s\u00ed su rastro de recompensas, que como se ha explicado anteriormente va dividi\u00e9ndose entre 2 en cada iteraci\u00f3n.\nSi hubi\u00e9ramos elegido al agente Aleatorio como agente a evaluar la interfaz de usuario hubiera cambiado ligeramente. Puesto que un agente Aleatorio no puede ser controlado por el usuario, el interfaz no nos permite la interacci\u00f3n con el entorno deshabilit\u00e1ndonos las acciones que puede realizar el agente. En la siguiente imagen podemos ver el mismo entorno habiendo seleccionado como agente a evaluar al agente Aleatorio.\nP\u00e1gina 50\nP\u00e1gina 51"}, {"heading": "5. Experimentos", "text": "En esta secci\u00f3n veremos los resultados de algunos experimentos realizados utilizando la arquitectura dise\u00f1ada. En ellas veremos c\u00f3mo se cumplen las caracter\u00edsticas de los entornos balanceados, dando para agentes aleatorios resultados cercanos a 0. Tambi\u00e9n veremos los resultados de los mismos tipos de entornos para un agente llamado Observador, el cual dispone de informaci\u00f3n adicional que le ayuda a conseguir mejores resultados.\nA continuaci\u00f3n veremos los resultados de los experimentos realizados utilizando espacios previamente dise\u00f1ados y generados aleatoriamente, los cuales contienen diversidad de celdas y acciones. Los resultados de los experimentos se muestran como la media de varias sesiones siguiendo un amplio rango de iteraciones. Tambi\u00e9n veremos los resultados de experimentos en donde no se garantizan entornos balanceados y en donde se eval\u00faan a varios agentes en una misma sesi\u00f3n.\nTodos los experimentos se han realizado tanto relocalizando a los agentes generadores de recompensas (Good y Evil) como sin relocalizarlos. Para ver la distinci\u00f3n podemos ver, en las tablas donde se resumen los experimentos, la marca \u201c(Sin cambio)\u201d en aquellos experimentos en donde no se relocalizan a los agentes generadores y ninguna marca para aquellos en donde s\u00ed se les relocaliza.\n5.1. Entornos definidos manualmente\nEn este apartado veremos los resultados de experimentos realizados en entornos completamente balanceados, con los agentes Good y Evil siguiendo el mismo comportamiento y con espacios desde 2 hasta 10 celdas para los cuales los espacios estar\u00e1n siempre completamente conectados, es decir, se podr\u00e1 llegar siempre desde cualquier celda a cualquier otra celda realizando las acciones oportunas. Para cada espacio veremos su codificaci\u00f3n y una visi\u00f3n gr\u00e1fica donde podremos ver como se conecta el espacio.\nEn todos los espacios mostrados a continuaci\u00f3n, todas las celdas tienen siempre una acci\u00f3n por defecto que conecta a una celda consigo misma a trav\u00e9s de la acci\u00f3n 0. Para no repetir continuamente esta informaci\u00f3n en las im\u00e1genes y en la codificaci\u00f3n de los espacios esta informaci\u00f3n se obvia, siendo el sistema el encargado de construir esta acci\u00f3n para todas las celdas.\nP\u00e1gina 52"}, {"heading": "Experimentos con un espacio de 2 celdas", "text": "Espacio definido manualmente\nCodificaci\u00f3n del espacio: 1+|1-\nN\u00famero de celdas: 2\nN\u00famero de acciones: 1\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nMedia de resultados tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.04 0.5 0 0.5\n10 -0.02 0.5 0.03 0.5\n20 -0.015 0.5 -0.055 0.5\n50 0.0260 0.5 0.01 0.5\n100 -0.028 0.5 -0.002 0.5\n200 -0.008 0.5 -0.012 0.5\n500 0.0114 0.5 -0.009 0.5\n1000 0.0137 0.5 -0.0017 0.5\n2000 -0.0002 0.5 -0.00525 0.5\n1000000 0.0000453 0.5 0.0000849 0.5\nComo podemos observar, los resultados obtenidos por los agentes aleatorios van acerc\u00e1ndose a resultados cercanos a 0, pudiendo ser estos resultados tanto positivos como negativos, a medida que las sesiones tienen m\u00e1s iteraciones los resultados obtenidos para un agente aleatorio se acercan cada vez m\u00e1s a 0. Esto se debe a que en sesiones con pocas iteraciones no es posible medir fiablemente la inteligencia de un agente, ya que en cualquier momento podr\u00eda disponer de golpes de suerte (o de mala suerte). Al aumentar el n\u00famero de iteraciones necesarias para la sesi\u00f3n, el agente aleatorio obtiene cifras mucho m\u00e1s cercanas a 0, lo cual reafirma que la inteligencia demostrada por un agente aleatorio es la esperada para un entorno balanceado.\nP\u00e1gina 53\nEn lo referente al agente observador podemos ver como en este espacio siempre obtiene una recompensa de 0.5. Esto es debido a que ambas celdas siempre est\u00e1n ocupadas por alg\u00fan agente distinto al que se est\u00e1 evaluando, los cuales son o Good o Evil, y por lo tanto siempre deber\u00e1 compartir la recompensa obtenida en cualquier celda. Debido a que las sesiones se han realizado en un espacio tan peque\u00f1o, el agente observador siempre puede ver al agente Good, y por lo tanto siempre ir\u00e1 a por su recompensa. Sin embargo, como ya se ha dicho antes, est\u00e1 recompensa se dividir\u00e1 entre los dos agentes que ocupan la celda obteniendo siempre recompensas de 0.5. Por lo tanto, en este espacio tan peque\u00f1o, el agente observador siempre obtiene la mayor recompensa posible dando como resultado una recompensa media de 0.5.\nP\u00e1gina 54"}, {"heading": "Experimentos con un espacio de 4 celdas", "text": "Espacio definido manualmente\nCodificaci\u00f3n del espacio: 1+2++3|1+23-|1+23|1+2--3-\nN\u00famero de celdas: 4\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nMedia de resultados tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.01375 0.61 0.1 0.5\n10 0.03484375 0.695 -0.011875 0.685\n20 -0.06 0.66625 -0.05875 0.6525\n50 -0.045953125 0.717 0.020953125 0.686\n100 -0.0232026367 0.689125 0.0274492188 0.6895\n200 -0.0216215820 0.6755625 0.0034672851 0.67675\n500 -0.0226217712 0.6805734375 -0.0022934555 0.69005\n1000 -0.012627417 0.6735982422 -0.0053155426 0.6942\n2000 -0.0032242306 0.6795856934 -0.0125768757 0.68945\n1000000 -0.0001506778 0.6871037385 0.0014431725 0.7003017\nLos resultados obtenidos por el agente aleatorio siguen el mismo patr\u00f3n que en los\nexperimentos anteriores.\nP\u00e1gina 55\nUna vez que disponemos de un espacio con un mayor n\u00famero de celdas, las recompensas que se encuentran en \u00e9stas no siempre deber\u00e1n compartirse entre varios agentes, por lo tanto la esperanza media de recompensas ser\u00e1 superior a 0.5. Si realizamos los mismos experimentos sin recolocar a los agentes Good y Evil las recompensas obtenidas por el agente observador siempre resultan ligeramente superiores a cuando se realizaba el cambio de posiciones de Good y Evil. Este es debido a que el agente observador no debe volver a buscar al agente Good tras cada relocalizaci\u00f3n y, por lo tanto, no perder\u00e1 la recompensa que va dejando al tratar de encontrarle.\nP\u00e1gina 56"}, {"heading": "Experimentos con un espacio de 6 celdas", "text": "Espacio definido manualmente\nCodificaci\u00f3n del espacio: 1--2+|1++2--|1++2-|12---|1-2+|12---\nN\u00famero de celdas: 6\nN\u00famero de acciones: 2\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nMedia de resultados tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.03 0.5 0.155625 0.405"}, {"heading": "10 0.1425390625 0.54125 0.01984375 0.60125", "text": "20 0.0127319336 0.765 -0.026171875 0.698125\n50 -0.0056818848 0.7214375 -0.0224951172 0.7095\n100 0.0046063232 0.6898037109 0.0245116272 0.71296875\n200 -0.0140901871 0.6739960926 0.0214790373 0.7415\n500 -0.0093691259 0.6934712868 -0.0014536697 0.734\n1000 0.0033478214 0.6812413848 -0.0005849602 0.737475\n2000 -0.0000222531 0.6858013640 -0.0103504164 0.7333\n1000000 -0.0021152493 0.6837011002 -0.0020848308 0.7384596\nP\u00e1gina 57\nComo podemos ver, el agente aleatorio sigue obteniendo resultados cercanos a 0. Aunque en algunos experimentos ha tenido m\u00e1s suerte que en otros y por lo tanto su recompensa media difiere bastante del resto de experimentos. Podemos ver esto en los experimentos para el agente aleatorio con 10 iteraciones y con 5 iteraciones sin cambio para Good y Evil. Estos resultados excepcionales normalmente solo se dan en experimentos realizados en sesiones con pocas iteraciones, en donde la media de las recompensas obtenidas aun no es del todo fiable.\nAl igual que en los experimentos realizados en el espacio de 4 celdas, el agente observador sigue manteniendo resultados superiores a 0.5. Sin embargo podemos ver que existe una mayor diferencia entre los resultados obtenidos en los entornos donde no se cambia a Good ni a Evil y aquellos en los que si se les cambia. Esta diferencia va aumentando debido a que al agente observador le cuesta m\u00e1s encontrar nuevamente a Good en espacios cada vez m\u00e1s grandes, por lo que pierde cada vez m\u00e1s recompensas positivas tratando de encontrarle. Por otro lado podemos ver como las recompensas del observador son algo superiores que en el espacio anterior, ya que en espacios cada vez mayores los agentes Good y Evil tendr\u00e1n una menor probabilidad de encontrarse y el agente Good no deber\u00e1 renunciar a su movimiento, por lo que habr\u00e1n menos ocasiones en donde su recompensa la comparta con el observador. Por lo tanto, el observador compartir\u00e1 menos recompensas conforme el espacio contenga m\u00e1s celdas.\nP\u00e1gina 58"}, {"heading": "Experimentos con un espacio de 8 celdas", "text": "Espacio definido manualmente\nCodificaci\u00f3n del espacio: 1+2+++3|1+2-3---|1++2-3|1---2++3+|1+++2--3-|1--2+3|1-\n2+3+++|1-2---3\nN\u00famero de celdas: 8\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 59\nMedia de resultados tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0675 0.6525 0.0325 0.67\n10 0.026328125 0.795 0.0076171875 0.725\n20 0.0646356201 0.73625 -0.0195324707 0.67125\n50 0.0005088654 0.731375 -0.0046048894 0.77675\n100 0.0064107894 0.7168063924 0.0373921991 0.768\n200 -0.0098070030 0.7353776855 0.0008353519 0.7845\n500 -0.0026782425 0.7257369799 0.0045596895 0.7877\n1000 -0.0076095967 0.7322875084 -0.0029735875 0.79015\n2000 0.0052535156 0.7306702732 0.0043954454 0.790825\n1000000 0.0012538494 0.7362988065 0.0005897395 0.79489045\nSiguiendo la din\u00e1mica de los experimentos anteriores, el agente aleatorio sigue\nmanteniendo resultados en torno a 0.\nEn este caso, los resultados del agente observador son a\u00fan superiores que en los experimentos del espacio con 6 celdas. Como se ha visto antes esto se debe a que existen menos probabilidades de que los agentes Good y Evil se encuentren y por lo tanto la recompensa de Good se comparte con menor frecuencia. Sin embargo existe un motivo a\u00f1adido por lo que los resultados del observador han aumentado tanto con respecto a la diferencia entre los experimentos del espacio con 4 celdas y el de 6, en donde los resultados difieren en 0.4 mientras que los resultados entre estos experimentos y los experimentos del espacio anterior difieren en un intervalo entre 0.5 y 0.6. Esto es porque en estos experimentos Good y Evil disponen de m\u00e1s acciones para realizar desde pr\u00e1cticamente todas las celdas y, por lo tanto, decidir\u00e1n con menor frecuencia permanecer en su celda, aumentando la media de recompensas que recoger\u00e1 el observador.\nP\u00e1gina 60"}, {"heading": "Experimentos con un espacio de 10 celdas", "text": "Espacio definido manualmente\nCodificaci\u00f3n del espacio: 123+++++|1-23+++++|1-2+3+++++|12+3+++++|123+++++\n|1+2++3|1+23|1++++2-----3----|12-3|1--2-3\nN\u00famero de celdas: 10\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 61\nMedia de resultados tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.049375 0.3575 0.0175 0.31\n10 0.056484375 0.47609375 0.0416992188 0.535\n20 -0.0252062988 0.6625 0.0191992188 0.7175\n50 -0.0045199013 0.72475 -0.0269153652 0.673484375\n100 0.0023949995 0.6618558960 -0.0203063105 0.73071875\n200 -0.011540667 0.6591534714 -0.0131583569 0.75575\n500 -0.0026265456 0.6586628568 -0.0004145666 0.76438125\n1000 0.0022877179 0.6650044655 -0.0145762173 0.7564539063\n2000 -0.0041933919 0.6580340589 -0.0041297061 0.7626625\n1000000 -0.0028186475 0.6693211141 -0.0017103821 0.7645574133\nComo cabr\u00eda esperar al agente aleatorio sigue manteniendo los mismos resultados.\nAl agente observador le cuesta m\u00e1s encontrar al Good en este espacio con esta\ntopolog\u00eda, por lo que la diferencia de los resultados entre relocalizar o no a Good y a Evil\naumenta en estos experimentos. Cabr\u00eda esperar que los resultados medios del\nobservador sin relocalizar a Good y a Evil fuera superior que en los experimentos con el\nespacio de 8 celdas, ya que contiene m\u00e1s celdas y el mismo n\u00famero de acciones. Si nos\nfijamos m\u00e1s en la topolog\u00eda del espacio generalmente solo se pueden realizar una o dos\nacciones desde cada celda, sin contar quedarse en la misma celda, cuando en los\nexperimentos anteriores desde las celdas se pod\u00edan realizar dos o tres acciones. Ya que el\nagente Good tendr\u00e1 una mayor probabilidad de mantenerse en la celda en la que se\nencuentre sus recompensas ser\u00e1n m\u00e1s veces compartidas y, por lo tanto, la media de\nrecompensas no aumenta como era de esperar.\nP\u00e1gina 62\n5.2. Entornos generados autom\u00e1ticamente\nA continuaci\u00f3n mostramos los resultados a los experimentos realizados generando\naleatoriamente los espacios tal y como se describe en el apartado 4.4.3."}, {"heading": "5.2.1. Espacios generados autom\u00e1ticamente (Conectados)", "text": "Veamos los resultados obtenidos utilizando una media de 100 sesiones con los\ndistintos agentes e iteraciones utilizando distintos espacios en cada sesi\u00f3n teniendo como\n\u00fanica restricci\u00f3n que ninguna celda quede desconectada del resto.\n4-celdas 3-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 4\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.04675 0.522 -0.008 0.4825\n10 0.02975 0.543625 0.0120546875 0.5098125\n20 0.0395100098 0.565125 -0.0101875 0.56775\n50 0.0152491213 0.506125 0.0058412355 0.5735\n100 0.0092045895 0.5229386719 0.0242653084 0.5480625\n200 -0.003388797 0.4989532227 -0.0251157016 0.5288375\n500 -0.0006818127 0.478607851 -0.0309036888 0.537445\n1000 0.0044138150 0.4782791846 0.0101541339 0.56831375\n2000 0.0012512472 0.4670196489 0.0160087209 0.556584375\n100000 0.0018664954 0.4964514401 -0.0035218350 0.56775625\nP\u00e1gina 63\n4-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 4\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.036375 0.59 0.022125 0.577\n10 -0.031546875 0.5985 -0.013078125 0.6525\n20 0.0068369140 0.6365 -0.0067470703 0.6276875\n50 0.0070780273 0.6219117188 -0.0065885833 0.6309\n100 -1.7125701829 0.6148359130 0.0050069703 0.651575\n200 -0.0031231798 0.6204983398 -0.0150923710 0.6380875\n500 0.0021759797 0.6267056823 -0.0115480778 0.655715\n1000 0.0037967063 0.5886907487 -0.0030248689 0.636795\n2000 -0.0025212755 0.6180830476 0.0154158361 0.645344375\n100000 0.0019821357 0.6394119529 0.0072174910 0.643137375\nP\u00e1gina 64\n6-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 6\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.071625 0.553375 -0.061 0.57875\n10 0.0483203125 0.6700625 0.0099902343 0.656375\n20 0.0061313476 0.6654373798 0.0136739501 0.68552929687\n50 0.0057017860 0.6950187499 0.0234200533 0.7143734375\n100 0.0201091887 0.6770288970 -0.0002505793 0.722665625\n200 0.0015357381 0.6742627235 -0.0030017522 0.72453125\n500 0.0070556115 0.6666639230 0.0029403885 0.71003\n1000 0.0056254281 0.6698362898 0.0136878712 0.7185325\n2000 0.0079251038 0.6946516539 -0.0032442967 0.72596\n100000 0.0040647799 0.6860570746 -0.0038440883 0.731519625\nP\u00e1gina 65\n6-celdas 6-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 6\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0075625 0.707125 -0.0470625 0.689\n10 -0.0184033203 0.7268125 0.0123828125 0.746375\n20 -0.0197189598 0.74665625 -0.0074642486 0.75175\n50 -0.0037345133 0.757561615 0.0013374073 0.769125\n100 0.0076907712 0.7611456665 0.0012145443 0.77585\n200 0.0033358568 0.7593348992 0.0064964131 0.7769125\n500 0.0026132700 0.756693748 0.0007169638 0.771575\n1000 -0.0000698110 0.7598566768 0.0006846164 0.775176875\n2000 0.0010336988 0.759557398 -0.0044459509 0.7708675\n100000 0.0019426404 0.7601891151 0.0018648231 0.778565075\nP\u00e1gina 66\n8-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.024375 0.5385625 0.0236875 0.4915\n10 -0.007171875 0.65865625 -0.0072167968 0.629703125\n20 0.0188020324 0.71584375 0.0037644996 0.67070898437\n50 -0.0033710265 0.6881226531 -0.0117281041 0.7314\n100 -0.0022428926 0.7237871719 0.0012601332 0.75131875\n200 0.0173139745 0.7016593765 0.0022134205 0.75735419921\n500 0.0028826708 0.7079315228 -0.0077612121 0.751844375\n1000 0.0023655002 0.7005680074 0.0001749424 0.767868125\n2000 0.0018537326 0.7140603729 0.0026119929 0.7673409375\n100000 0.0043064303 0.7171128281 0.0082247295 0.7661230625\nP\u00e1gina 67\n8-celdas 6-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.042125 0.6885 0.006 0.681\n10 0.010375 0.7228125 -0.026921875 0.7365625\n20 -0.0080527896 0.7798125 -0.0188741035 0.7798125\n50 -0.0015797378 0.7846015625 -0.0023470562 0.8034875\n100 -0.0002917564 0.7820518463 -0.0029158655 0.80525\n200 0.0007346433 0.7814969837 -0.0053570862 0.810796875\n500 -0.0023997683 0.7827005168 -0.0012930265 0.809630625\n1000 -0.0000345703 0.7824668042 0.0009552746 0.810276875\n2000 0.0012363421 0.7830145245 -0.0005975208 0.8101525\n100000 0.0002894769 0.7804884726 0.0009075608 0.8102544\nP\u00e1gina 68\n8-celdas 8-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 8\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.064875 0.757 -0.01025 0.74925\n10 -0.0170507812 0.795125 0.0113867187 0.79775\n20 0.0048254966 0.8055625 0.0104834518 0.815375\n50 -0.0065295809 0.8153328063 0.0034930459 0.820025\n100 -0.0016721380 0.8144338834 -0.0001513715 0.8249\n200 -0.0039921943 0.8135470728 -0.0044161980 0.831725\n500 0.0005881382 0.8154931645 -0.0014925117 0.83276\n1000 -0.0056278378 0.8151737539 0.0004738523 0.8343375\n2000 -0.0030229432 0.8151252478 -0.0035334687 0.83393125\n100000 -0.0004472968 0.8144933822 -0.0022796966 0.833287725\nP\u00e1gina 69\n10-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.003375 0.503 0.0348125 0.496\n10 -0.0067871093 0.6367578125 0.0029042968 0.618125\n20 0.0029400844 0.662703125 0.0060056285 0.6786015625\n50 -0.0126927395 0.720691467 -0.0003916556 0.74152036132\n100 0.0010064249 0.7064358273 -0.0025521861 0.777025\n200 0.0021113307 0.7126902210 -0.0017698301 0.76821494140\n500 0.0081824189 0.7088160847 -0.0064782360 0.781974375\n1000 0.0023236015 0.7163777811 0.0137967548 0.78701601562\n2000 0.0054380301 0.7104929906 0.0007256349 0.78096863281\n100000 0.0047555243 0.7138575059 0.0045655560 0.78658997968\nP\u00e1gina 70\n10-celdas 7-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 7\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0025 0.66275 0.007375 0.62725"}, {"heading": "10 0.0162539062 0.7975 0.0067558593 0.77346875", "text": "20 -0.0139494881 0.80575 -0.0144310302 0.818671875\n50 0.0058377407 0.811679782 -0.0038752322 0.83715\n100 -0.0123828109 0.8106688241 0.0032449395 0.833878125\n200 -0.0005640175 0.8066227070 0.0021693778 0.8405\n500 -0.0002181396 0.8103837011 0.0013080553 0.8436875\n1000 -0.0018019882 0.8113200001 0.0003544006 0.845283125\n2000 -0.0005624005 0.8105365493 0.0004085095 0.8450575\n100000 0.0000173130 0.8120781481 0.0000560432 0.84557391875\nP\u00e1gina 71\n10-celdas 10-acciones\nEspacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 10\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0100625 0.7665 -0.019375 0.7645\n10 0.00984375 0.8095625 0.0020917968 0.8275\n20 -0.003994873 0.84775 0.0191303443 0.84225\n50 -0.0153001013 0.8492577506 -0.0099471920 0.86035\n100 0.0017704384 0.8515060465 0.0041125094 0.8641625\n200 0.0055527598 0.8467617110 0.0028669731 0.86915\n500 -0.0025842754 0.8505609456 0.0037606309 0.868675\n1000 -0.0016398764 0.8476845503 0.0003439481 0.8681925\n2000 0.0003998080 0.8493090214 0.0002639389 0.86781\n100000 0.0004557696 0.8486287340 0.0004261841 0.8679295\nComo hemos podido observar el agente aleatorio siempre obtiene resultados cercanos\na 0, cumpli\u00e9ndose las propiedades de un entorno balanceado.\nA medida que va creciendo el n\u00famero de celdas y de acciones, al igual que con los\nespacios definidos, el agente Good tiene menor probabilidad de encontrarse con el\nagente Evil y m\u00e1s posibilidades de cambiar de celda, lo cual explica los crecientes\nresultados a medida que va aumentando el n\u00famero de celdas y de acciones. Como cabr\u00eda\nesperar el observador sigue obteniendo mejores resultados cuando no se relocalizan ni a\nGood ni a Evil.\nP\u00e1gina 72"}, {"heading": "5.2.2. Espacios generados autom\u00e1ticamente (Fuertemente", "text": "conectados)\nVeamos los resultados obtenidos utilizando una media de 100 sesiones con los\ndistintos agentes e iteraciones utilizando distintos espacios en cada sesi\u00f3n teniendo como\nrestricciones que ninguna celda quede desconectada del resto de celdas y que siempre\nexista la posibilidad de llegar de una celda a cualquier otra del espacio.\n4-celdas 3-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 4\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0559375 0.52925 -0.054 0.5615"}, {"heading": "10 0.0311015625 0.616875 0.0290976562 0.63703125", "text": "20 -0.0162795429 0.669 -0.0282729492 0.650625\n50 -0.0041670043 0.6596765625 0.0190241519 0.672775\n100 0.0059552291 0.6535291015 0.0021379867 0.666625\n200 0.0028428422 0.6621362792 -0.0039155030 0.67644375\n500 0.0090564506 0.6549043872 -0.0027577401 0.67529\n1000 -0.0040643431 0.6541745177 -0.0019210290 0.67767125\n2000 -0.0016869102 0.6556849958 0.0041417218 0.678718125\n100000 -0.0000455535 0.6540757565 -0.0007266093 0.6762191\nP\u00e1gina 73\n4-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 4\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0060625 0.604 0.0163125 0.585125\n10 -0.0125957031 0.65425 0.0022734375 0.658375\n20 -0.0071548156 0.675875 0.0328095550 0.6716875\n50 0.0172693517 0.6737371093 0.0148806640 0.68635\n100 0.0063840385 0.6824394531 0.0041909473 0.691725\n200 0.0030967557 0.6686276851 -0.0033190012 0.68488125\n500 -0.0017187205 0.6787061772 -0.0023710905 0.6845\n1000 0.0040511488 0.6716999725 0.0028339158 0.69140375\n2000 0.0026429969 0.6824245528 0.0033481580 0.6919275\n100000 0.0036169436 0.6779132341 0.0034483574 0.68647725625\nP\u00e1gina 74\n6-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 6\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.008375 0.6135 0.0236875 0.5605\n10 0.0096523437 0.65303125 -0.0134296875 0.6638125\n20 0.0170052347 0.7070625 0.0073968658 0.7104375\n50 0.0015991142 0.7195690429 -0.0071953084 0.726825\n100 0.0009080716 0.7096931018 0.0102078705 0.74024375\n200 0.0037127370 0.7114983885 0.0098327245 0.74724335937\n500 0.0037571275 0.7191618014 0.0066783974 0.74959375\n1000 0.0067209183 0.7128960171 0.0073153420 0.75042359375\n2000 0.0091325779 0.7167080586 0.0072821669 0.7497528125\n100000 0.0032163755 0.7157017915 0.0034025599 0.7522874875\nP\u00e1gina 75\n6-celdas 6-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 6\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.03225 0.68525 0.047625 0.715\n10 -0.011875 0.724375 -0.0522226562 0.75675\n20 -0.0195453338 0.76275 0.0059307861 0.7605625\n50 0.0023880575 0.7547959472 0.0118458852 0.76375\n100 -0.0015145474 0.7639124877 0.0021453768 0.7742625\n200 0.0006489422 0.7598449461 -0.0046083085 0.7756625\n500 0.0034337616 0.7645893438 0.0034215969 0.77374\n1000 0.0060917931 0.7606200810 0.0028078536 0.7797375\n2000 0.0023283563 0.7632689170 0.0015290917 0.77973375\n100000 0.0006137602 0.7573598717 0.0014342578 0.779636525\nP\u00e1gina 76\n8-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0224375 0.613375 -0.0093125 0.5381875"}, {"heading": "10 0.0230917968 0.676484375 -0.0049335937 0.64228125", "text": ""}, {"heading": "20 -0.0042501735 0.7115859375 0.0056370620 0.72321875", "text": "50 0.0279502706 0.7356921047 0.0033554223 0.758615625\n100 0.0124253130 0.7113351461 0.0156291859 0.772246875\n200 0.0118206168 0.7252318740 0.0092197083 0.7791625\n500 0.0070662716 0.7219434424 0.0083700529 0.77841421875\n1000 0.0101475394 0.7241803343 0.0089241963 0.78193796875\n2000 0.0088895304 0.7209748786 0.0047236072 0.778891875\n100000 0.0073447912 0.7218125279 0.0089199839 0.77512321093\nP\u00e1gina 77\n8-celdas 6-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.001375 0.7145 -0.034375 0.706625\n10 0.0178945312 0.74125 0.0070058593 0.7331875\n20 -0.0105775337 0.7750625 -0.0031084747 0.765125\n50 0.001415135 0.7886808349 -0.006634225 0.796225\n100 0.0019076936 0.7850293668 -0.0033597725 0.804325\n200 -0.0056681451 0.7861249763 -0.0043902148 0.806825\n500 0.0023970364 0.7844784053 -0.002102987 0.81118\n1000 0.0001708415 0.7815085989 0.0003116397 0.80873875\n2000 -0.0007306984 0.7794557296 0.0006464564 0.811493125\n100000 0.0002585017 0.7821838415 0.0004136819 0.81182739375\nP\u00e1gina 78\n8-celdas 8-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 8\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0059375 0.7355 0.0151875 0.78625\n10 -0.0131113281 0.78775 0.0077460937 0.7980625\n20 -0.0044685211 0.807625 -0.0247678604 0.818625\n50 0.0025694720 0.8138312362 -0.0081238818 0.83015\n100 -0.0094017859 0.8167993530 0.0052363706 0.83363125\n200 0.0004332004 0.8149542020 0.0084704842 0.8295125\n500 -0.0018826736 0.8152654870 -0.0030245552 0.8297325\n1000 -0.0021321778 0.8156104487 -0.0006740240 0.831035\n2000 0.0016597958 0.8157987536 0.0003517454 0.83301125\n100000 -0.0017017071 0.8133519223 -0.0008519155 0.83378475\nP\u00e1gina 79\n10-celdas 4-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 4\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.00775 0.545875 -0.003375 0.529125"}, {"heading": "10 -0.0133144531 0.6095546875 0.0135009765 0.62215625", "text": "20 0.0223903026 0.7179296875 0.0149272508 0.681453125\n50 0.0086390515 0.7318503353 -0.0141471834 0.7633\n100 0.0130662086 0.7185673772 0.0061895334 0.77265546875\n200 0.0018630807 0.7143527745 0.0076909615 0.77997851562\n500 -0.0006402785 0.7220285703 0.0063592927 0.790005625\n1000 0.0026376552 0.7135880569 -0.0015747931 0.79191449218\n2000 0.0047512678 0.7179938130 0.0014539369 0.789928125\n100000 0.0031840408 0.7188911904 0.0025455900 0.797118975\nP\u00e1gina 80\n10-celdas 7-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 7\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.0315625 0.722 -0.056125 0.70175\n10 0.0343896484 0.769375 0.0120039062 0.78725\n20 -0.0026242761 0.79628125 0.0078400764 0.80175\n50 0.0017691624 0.8128048820 -0.0073491173 0.831412\n100 -0.0023990369 0.8117279079 0.0019531691 0.83767\n200 0.0000650018 0.8123028149 0.0005083199 0.84008828125\n500 0.0005450867 0.8121905251 0.0008084398 0.8453025\n1000 -0.0019673875 0.8136747984 0.0000549403 0.8451721875\n2000 0.0007466091 0.8102686322 0.0004461757 0.845355625\n100000 -0.0000180668 0.8137150793 -0.0001898676 0.84364703125\nP\u00e1gina 81\n10-celdas 10-acciones\nEspacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 10\nN\u00famero de acciones: 10\nGood y Evil: Comportamiento aleatorio\nMedia de resultados tras 100 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0226875 0.7615 -0.0031875 0.775\n10 0.0153847656 0.816875 -0.0310585937 0.8255\n20 -0.0223733577 0.849765625 0.0195264005 0.8361875\n50 -0.0142524479 0.8452241943 -0.0022178101 0.86205\n100 -0.0032213712 0.8499759917 -0.0010602513 0.8656125\n200 0.0033599156 0.8477624884 0.0016957615 0.8649625\n500 -0.0024712512 0.8475952018 0.0012036416 0.867875\n1000 -0.0002777197 0.8477101014 0.0013118449 0.8696825\n2000 0.0005061001 0.8487137980 0.0018370838 0.8685675\n100000 -0.0001684560 0.8475726295 -0.0002814336 0.8673165875\nEl agente aleatorio sigue la misma l\u00ednea que el resto de espacios vistos hasta el\nmomento, por lo cual los espacios generados fuertemente conectados siguen cumpliendo\nlas propiedades de un entorno balanceado.\nCon respecto al agente observador, sigue exactamente la misma din\u00e1mica que en los\nexperimentos realizados anteriormente con los espacios conectados generados. Si nos\nfijamos en estos resultados y los obtenidos con los espacios generados conectados\npodemos ver como son pr\u00e1cticamente iguales. Esto se debe a que al permitir que se\ngeneren tanta cantidad de acciones, los espacios que se crean raramente tienen un\npeque\u00f1o conjunto de estados en donde el agente evaluable pueda quedarse encerrado\ndurante toda la sesi\u00f3n. De hecho, haciendo pruebas con esta distribuci\u00f3n del espacio (10\nceldas y 10 acciones) teniendo como restricci\u00f3n que los espacios generados sean\nconectados, \u00fanicamente 3 de los 2000 espacios que se han generado no son tambi\u00e9n\nfuertemente conectados, lo que explica la gran similitud entre los resultados obtenidos.\nP\u00e1gina 82\n5.3. Entorno sesgado\nA continuaci\u00f3n veremos los resultados a experimentos realizados en el espacio\ndefinido de 4 celdas utilizando comportamientos distintos para el agente Good y para Evil.\nEntorno sesgado 1\nEntorno sesgado\nN\u00famero de celdas: 4\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 83\nMedia del resultado tras 10 sesiones realizadas donde Good se mantiene siempre en la\nmisma celda.\nAgente\nIteraciones Aleatorio\nAleatorio\n(Sin cambio)\n5 0.0275 -0.0925\n10 -0.0853125 -0.0834570312\n20 -0.0733984375 -0.1013476943\n50 -0.0744572753 -0.0545395507\n100 -0.0721672363 -0.080703125\n200 -0.0798338012 -0.1081768798\n500 -0.0763609828 -0.0759708496\n1000 -0.0705885308 -0.0916902229\n2000 -0.0833871496 -0.0610014770\n1000000 -0.0864727326 -0.0988842842\nEn estos experimentos el agente aleatorio siempre compartir\u00e1 las recompensas\npositivas con el agente Good y seguir\u00e1 recogiendo las recompensas negativas como\nanteriormente hac\u00eda. Esto hace que la media de recompensas recogidas siempre sea\nnegativa.\nP\u00e1gina 84\nEntorno sesgado 2\nEntorno sesgado\nN\u00famero de celdas: 4\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nMedia del resultado tras 10 sesiones realizadas donde Good siempre trata de cambiar\nde celda.\nAgente\nIteraciones Aleatorio\nAleatorio\n(Sin cambio)\n5 0.01875 0.21125\n10 0.095625 0.21125\n20 0.072265625 0.0569726562\n50 -0.0179550781 0.0548925781\n100 0.0143828125 0.0466845703\n200 0.0472050781 0.0500693359\n500 0.0506410400 0.0425306884\n1000 0.0511209838 0.0387437622\n2000 0.0498806686 0.0543033935\n1000000 0.0523993359 0.0546207446\nP\u00e1gina 85\nEn este caso el agente Good siempre tratar\u00e1 de cambiar de celda, a menos que se encuentre con Evil en cuyo caso uno de los dos tendr\u00e1 que esperar en su celda, por lo que generalmente el agente no compartir\u00e1 la recompensa con el agente Good.\nComo podemos ver en ambos entornos existe un sesgo con respecto a los resultados obtenidos por el agente aleatorio, lo cual significa que no se cumplen las propiedades de un entorno balanceado. Esto es debido a que no se han introducido elementos complementarios en el entorno (el agente Good y el agente Evil no se comportan exactamente igual), siendo predominante en cada uno de los experimentos uno de los elementos.\nP\u00e1gina 86\n5.4. Evaluaci\u00f3n social\nEn este caso vamos a medir a varios agentes evalu\u00e1ndose simult\u00e1neamente en la misma sesi\u00f3n, por lo que podremos ver como se ven modificados los resultados de los distintos agentes al existir otros compitiendo por las recompensas durante el transcurso de las sesiones. En estos experimentos introduciremos a los agentes aleatorio y observador simult\u00e1neamente en el entorno.\nEn la tabla vemos los resultados por parejas, donde cada resultado de Aleatorio y de\nObservador ser\u00e1 el resultado para cada uno al evaluarlos sobre las mismas sesiones.\nEspacio definido manualmente - 8 celdas\nEvaluaci\u00f3n social - Espacio definido manualmente\nN\u00famero de celdas: 8\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 87\nMedia del resultado tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0925 0.5316666666 0.0316666666 0.365\n10 -0.0190104166 0.61 -0.0347135416 0.61541666666\n20 -0.0845389811 0.7216666666 -0.1060742187 0.61833333333\n50 -0.0875097198 0.7161197915 -0.0821009115 0.71566666666\n100 -0.0736087347 0.7053124389 -0.0480256779 0.74825\n200 -0.0784876931 0.6839524720 -0.0745241745 0.741\n500 -0.0781876082 0.6871417968 -0.0746585093 0.73788333333\n1000 -0.0810844671 0.6835628865 -0.0829523893 0.73548333333\n2000 -0.0724479806 0.6936914750 -0.0688205723 0.74119895833\n1000000 -0.0713102075 0.6929307734 -0.0793472724 0.74182583333\nP\u00e1gina 88"}, {"heading": "Espacio generado autom\u00e1ticamente (Conectado)", "text": "8-celdas 6-acciones\nEvaluaci\u00f3n social - Espacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0634583333 0.6508333333 -0.0559583333 0.62533333333\n10 -0.0872135416 0.6813958333 -0.0926223958 0.688625\n20 -0.1037266680 0.7412083333 -0.0984965496 0.7297916666\n50 -0.0785706488 0.7436425781 -0.0929490238 0.743475\n100 -0.0998179061 0.7346942586 -0.1027326131 0.7559\n200 -0.0921733936 0.7361930467 -0.0953013076 0.7541885416\n500 -0.0939403367 0.7300403093 -0.0977293596 0.754136666\n1000 -0.0931912955 0.7300650927 -0.0982186178 0.75964125\n2000 -0.0928543908 0.7290798694 -0.0985127970 0.75536802083\n100000 -0.0942372888 0.7236268737 -0.0982732850 0.74866333541\nP\u00e1gina 89"}, {"heading": "Espacio generado autom\u00e1ticamente (Fuertemente", "text": "conectado)\n8-celdas 6-acciones\nEvaluaci\u00f3n social - Espacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0839166666 0.657 -0.0596875 0.62591666666\n10 -0.0926793619 0.7100416666 -0.0718736979 0.6895\n20 -0.0907177225 0.7303958333 -0.0816079489 0.7171875\n50 -0.0822846768 0.7236609375 -0.0895003887 0.74218333333\n100 -0.0889639014 0.7329951766 -0.1074970967 0.7507125\n200 -0.0911916412 0.7366449244 -0.0916739162 0.75441666666\n500 -0.0947957406 0.7306393021 -0.0982189874 0.75883\n1000 -0.0932831643 0.7326538204 -0.0987799716 0.75638041666\n2000 -0.0927858311 0.7291214223 -0.0969757873 0.7553403125\n100000 -0.0938490195 0.7351404194 -0.0975634640 0.758923725\nComo podemos observar la introducci\u00f3n de un agente observador obliga al agente\naleatorio a recoger mayoritariamente recompensas negativas y a compartir las positivas\ncon el observador, por lo que sus resultados son siempre negativos.\nSin embargo, el agente observador tambi\u00e9n resulta perjudicado al incluir al agente\naleatorio, ya que de vez en cuando tambi\u00e9n debe compartir las recompensas positivas\ncon el agente aleatorio, lo cual se traduce en una disminuci\u00f3n de los resultados con\nrespecto a los mismos experimentos evaluaci\u00f3n social.\nP\u00e1gina 90\n5.5. Varios movimientos de los agentes generadores\nHasta el momento hemos realizado los experimentos teniendo en cuenta que los\nagentes generadores Good y Evil solo pod\u00edan moverse de una casilla a otra adyacente, sin\nembargo estos agentes deber\u00edan poder moverse varias celdas simult\u00e1neamente. En estos\nexperimentos hemos dado a los agentes Good y Evil la oportunidad de moverse\nrealizando 2, 3 y 4 acciones simult\u00e1neas a trav\u00e9s del espacio."}, {"heading": "2 Movimientos", "text": "Espacio definido manualmente - 8 celdas\n2 Movimientos \u2013 Espacio definido manualmente\nN\u00famero de celdas: 8\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 91\nMedia del resultado tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.04 0.405 -0.05 0.42625\n10 0.03796875 0.3821875 0.0153515625 0.412578125\n20 -0.0363085937 0.4236915588 -0.0047903442 0.51425292968\n50 0.0097206726 0.5009875142 -0.0005978928 0.55971380615\n100 -0.0066461486 0.4774788761 -0.0101414661 0.51140614509\n200 -0.0080702943 0.4481070556 0.0126746131 0.49184478485\n500 -0.0166377544 0.4458106359 0.0032025186 0.50211970045\n1000 -0.0026152616 0.4622730091 -0.0045706541 0.49065660272\n2000 0.0006304102 0.4678949345 -0.0016159894 0.48975005375\n1000000 -0.0007433638 0.4675208032 -0.0006717888 0.47645791772\nP\u00e1gina 92"}, {"heading": "2 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Conectado)", "text": "8-celdas 6-acciones\n2 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0119375 0.5341875 -0.0072 0.55075\n10 0.0064091796 0.5379550781 -0.0058457031 0.53960351562\n20 0.0082336196 0.5430779724 -0.0035347251 0.56109193801\n50 0.006022677 0.5298084959 -0.0000125740 0.55853964548\n100 -0.0081134941 0.5518187971 -0.0095100618 0.55345392186\n200 -0.0064818422 0.5524241160 -0.0036136333 0.54766000853\n500 -0.0043393588 0.5505275479 -0.0027577104 0.55116168168\n1000 0.0007476604 0.5445017800 0.0016774965 0.54745065584\n2000 -0.0002261323 0.5371404278 0.0006449761 0.54885647237\n100000 0.0000726315 0.5445200320 -0.0006092810 0.54809953458\nP\u00e1gina 93"}, {"heading": "2 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Fuertemente", "text": "conectado)\n8-celdas 6-acciones\n2 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.006875 0.538125 -0.0324375 0.572062\n10 0.0100273437 0.5299609375 0.0036308593 0.5477578125\n20 0.0226711483 0.5366146240 -0.002918045 0.54955162048\n50 0.0077164861 0.5502106540 -0.0006324388 0.54632609325\n100 -0.0046460433 0.5414063711 -0.0064156002 0.53228034219\n200 0.0003139742 0.5421837545 0.0018089704 0.55251558018\n500 -0.0000909382 0.5393919641 0.0001765426 0.55325090242\n1000 0.000387935 0.5465902676 0.0000794154 0.54417609374\n2000 -0.0005180299 0.5382306384 -0.0020435247 0.55555706237\n100000 -0.0003742777 0.5446807327 -0.0004220987 0.55247444172\nP\u00e1gina 94\n3 Movimientos\nEspacio definido manualmente - 8 celdas\n3 Movimientos \u2013 Espacio definido manualmente\nN\u00famero de celdas: 8\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 95\nMedia del resultado tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.015 0.3075 0.0675 0.525\n10 0.0778125 0.495 0.0000195312 0.4809765625\n20 -0.0155657958 0.3947705078 -0.0081640625 0.48546264648\n50 -0.0277063903 0.4672634277 -0.0041567382 0.466694458\n100 -0.0123266344 0.4584087065 -0.008754115 0.47171520423\n200 0.0026176177 0.4822813549 -0.0004651916 0.44741162168\n500 -0.0020008018 0.4646291565 0.0111973494 0.45438940498\n1000 0.0027786851 0.4469535351 -0.0055641521 0.45812849095\n2000 -0.0051659124 0.4550553326 0.0015448156 0.45724515918\n1000000 0.0013313368 0.4527268877 0.0021513511 0.45961500528\nP\u00e1gina 96"}, {"heading": "3 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Conectado)", "text": "8-celdas 6-acciones\n3 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.0219375 0.49325 0.0156875 0.4818125\n10 0.008234375 0.5291503906 0.0188515625 0.52532421875\n20 -0.0003603782 0.5175086669 0.0001126976 0.51278277587\n50 0.0021808862 0.5166270456 -0.0115737323 0.51555128552\n100 0.001067267 0.5179938519 -0.0027140024 0.52435035397\n200 0.0020297547 0.5180077101 -0.0039365435 0.51644639428\n500 0.0021116354 0.5238462684 0.0019011115 0.51839674259\n1000 0.0041763178 0.5146958811 -0.0013714009 0.5152991525\n2000 0.0002451868 0.5107994995 0.0006154981 0.52274517534\n100000 0.0002321834 0.5141149108 0.0007630815 0.5186971441\nP\u00e1gina 97"}, {"heading": "3 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Fuertemente", "text": "conectado)\n8-celdas 6-acciones\n3 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.002375 0.5063125 -0.025625 0.4895\n10 0.0151914062 0.5172207031 -0.0110566406 0.54356054687\n20 -0.0289168739 0.5175667724 0.003539978 0.53750369262\n50 0.004336184 0.5057879984 -0.0066616579 0.50712485746\n100 0.0048225043 0.5112355104 0.0027196029 0.51155243528\n200 0.0025973139 0.5132000347 -0.0041791193 0.51492348993\n500 0.0011507622 0.5113789919 0.0018582105 0.51214954175\n1000 -0.0010128764 0.5093485237 0.0011275919 0.52257547003\n2000 0.0025834348 0.5034636958 0.0011110905 0.51276348561\n100000 0.0005249084 0.5076676257 -0.0001681908 0.51862807874\nP\u00e1gina 98"}, {"heading": "4 Movimientos", "text": "Espacio definido manualmente - 8 celdas\n4 Movimientos \u2013 Espacio definido manualmente\nN\u00famero de celdas: 8\nN\u00famero de acciones: 3\nGood y Evil: Comportamiento aleatorio\nVisi\u00f3n gr\u00e1fica\nP\u00e1gina 99\nMedia del resultado tras 10 sesiones realizadas.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.03625 0.295 -0.02 0.4325"}, {"heading": "10 0.04359375 0.40609375 -0.0296875 0.39453125", "text": "20 -0.0076364135 0.3544236755 0.0228564453 0.43929321289\n50 0.0297216796 0.4356188964 0.0276926269 0.45490655517\n100 0.0031295781 0.4159604034 0.0263404192 0.41855424499\n200 -0.0018015873 0.4172509078 0.0131416435 0.41539114761\n500 -0.0047641256 0.4048449179 0.000372763 0.41228777224\n1000 0.0091366043 0.4071086136 0.0001288413 0.40876417505\n2000 0.0003983884 0.4061799454 -0.0062200826 0.40967137190\n1000000 0.0000415605 0.4042661129 0.0000581977 0.40725332197\nP\u00e1gina 100"}, {"heading": "4 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Conectado)", "text": "8-celdas 6-acciones\n4 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 -0.019125 0.437375 0.0034375 0.4781875\n10 -0.0234580078 0.5111425781 -0.0145878906 0.49786328125\n20 0.0228353652 0.5105960083 -0.0145878906 0.49482608032\n50 -0.0199328547 0.4969350477 -0.0145878906 0.51526614642\n100 -0.0081563807 0.5143437625 -0.0029857476 0.49809998298\n200 0.0014267878 0.5008350189 0.0020672845 0.50130601562\n500 -0.0018580099 0.5021560366 -0.000277568 0.50506489835\n1000 -0.0005966052 0.5052771370 0.0009364216 0.51094270662\n2000 -0.0000055866 0.5043260169 -0.0014542887 0.51026966205\n100000 -0.0004186722 0.5009204262 -0.0005286776 0.50773962979\nP\u00e1gina 101"}, {"heading": "4 Movimientos", "text": ""}, {"heading": "Espacio generado autom\u00e1ticamente (Fuertemente", "text": "conectado)\n8-celdas 6-acciones\n4 Movimientos \u2013 Espacio generado autom\u00e1ticamente (Fuertemente conectado)\nN\u00famero de celdas: 8\nN\u00famero de acciones: 6\nGood y Evil: Comportamiento aleatorio\nMedia del resultado evaluando 100 sesiones realizadas con espacios generados al azar.\nAgente\nIteraciones Aleatorio Observador\nAleatorio\n(Sin cambio)\nObservador\n(Sin cambio)\n5 0.003125 0.49075 0.0433125 0.4849375\n10 0.0253671875 0.515 -0.0003730468 0.4478125\n20 0.0045796928 0.502413928 0.0094997062 0.51073068237\n50 0.0153383504 0.5048348373 -0.0027522481 0.49465015575\n100 -0.0094204826 0.496507526 -0.0097997535 0.50584293028\n200 -0.0042613635 0.4981908917 0.0006317677 0.50560896818\n500 -0.0029455798 0.5028718157 -0.0016293615 0.50808030833\n1000 -0.0006098342 0.5102287993 -0.002906116 0.49683524799\n2000 0.0008494687 0.5019137961 -0.0001656485 0.51116060912\n100000 -0.0004016026 0.5041956411 -0.0002865026 0.50274058414\nComo podemos observar el agente aleatorio no ve modificados sus resultados, ya que\nel cambio que se ha realizado ha sido en ambos agentes generadores (Good y Evil) y, por\nlo tanto, se siguen manteniendo las propiedades de un entorno balanceado.\nP\u00e1gina 102\nAl agente observador s\u00ed que le afecta este cambio, ya que ahora no le resulta tan f\u00e1cil\nseguir a Good a trav\u00e9s del espacio. Mientras que antes ten\u00eda que seguir continuamente a\nGood durante toda la sesi\u00f3n ahora tiene que realizar movimientos aleatorios hasta\nencontrarle y seguramente le volver\u00e1 a perder de vista al poco tiempo, por lo que no le\nresulta tan f\u00e1cil conseguir tan buenos resultados en este tipo de entornos. Tambi\u00e9n\npodemos ver como conforme aumenta el n\u00famero de movimientos que pueden realizar\nGood y Evil al observador le resulta m\u00e1s y m\u00e1s dif\u00edcil conseguir buenos resultados. Esto se\ndebe a que existen m\u00e1s posibilidades de que Good se aleje lo suficiente del observador\ncomo para que \u00e9ste no pueda seguirle en vez de mantenerse a una distancia lo\nsuficientemente cercana como para que contin\u00fae sigui\u00e9ndole.\nP\u00e1gina 103"}, {"heading": "6. Conclusiones y trabajo futuro", "text": "Conclusiones\nPodemos analizar los resultados de este proyecto de fin de carrera conforme a dos perspectivas diferentes: el cumplimiento de los objetivos marcados y el conocimiento que se ha adquirido a partir del desarrollo del proyecto y de los experimentos.\n1. Tras la realizaci\u00f3n del proyecto se han cumplido todos los objetivos propuestos en el apartado 1.3.\n\u2022 Tras la construcci\u00f3n de la arquitectura del sistema, \u00e9ste est\u00e1 preparado para realizar los tests propuestos a distintos tipos de agentes: Los experimentos\nrealizados demuestran que el sistema y su arquitectura son lo suficientemente potentes como para ser utilizado para evaluar agentes. El hecho de que el resultado medio obtenido por agentes aleatorios, en los entornos tanto definidos como autogenerados, sea cercano a 0 demuestra que la arquitectura construida est\u00e1 preparada para realizar tests en entornos balanceados y, por lo tanto, est\u00e1 listo para la evaluaci\u00f3n de la inteligencia en agentes. \u2022 El sistema permite la construcci\u00f3n de los entornos de forma manual: El sistema es capaz de construir y preparar un entorno desde cero a partir de la\ndescripci\u00f3n del entorno que el usuario desee construir.\n\u2022 El sistema permite la construcci\u00f3n de los entornos de forma autom\u00e1tica y aleatoria: El sistema es capaz de construir y preparar un entorno desde cero\nautogenerando la descripci\u00f3n que tendr\u00e1 el entorno y posteriormente construy\u00e9ndolo a partir de \u00e9ste. La autogeneraci\u00f3n puede incluir la comprobaci\u00f3n de ciertas propiedades que el entorno debe cumplir para ser un entorno v\u00e1lido. \u2022 El sistema cuenta con una interfaz de usuario que permite la evaluaci\u00f3n para agentes humanos: Se ha construido una interfaz que permite que los agentes\nhumanos sean evaluados dentro del sistema, as\u00ed como mostrar el resultado obtenido por este al finalizar la prueba. Adem\u00e1s, la interfaz tambi\u00e9n permite que el usuario observe el desarrollo de pruebas en todo tipo de agentes. \u2022 Las pruebas y experimentos realizados han sido de ayuda para evaluar el sistema construido: Tras realizar las pruebas y el estudio de los experimentos\nhemos podido ver que el sistema cumple con las propiedades inicialmente deseadas y hemos podido observar la inteligencia de varios tipos de agentes.\n2. Con la generaci\u00f3n de entornos sencillos y generadores de recompensas aleatorios hemos podido ver la diferencia entre agentes aleatorios y agentes con cierta\nestrategia. Tambi\u00e9n hemos visto su interrelaci\u00f3n cuando varios agentes compiten por las mismas recompensas. \u2022 Tras lo visto durante los experimentos realizados hemos podido observar como cambios en el comportamiento de los agentes que tratamos de evaluar\nsuponen cambios significativos en su inteligencia. Tal y como hemos podido ver\nP\u00e1gina 104\nun comportamiento aleatorio de los agentes no supone inteligencia alguna, ya que no trata de resolver el problema, \u00fanicamente se mueve aleatoriamente sin importarle los resultados a obtener. Por otro lado el simple hecho de ver las casillas adyacentes y, por lo tanto, poder decidir moverse a la mejor celda posible demuestra un aumento significativo de inteligencia como hemos podido observar en los experimentos. De lo que podemos deducir que si a\u00f1adimos m\u00e1s comportamiento inteligente a los agentes resultar\u00e1 en una mejora en su evaluaci\u00f3n. \u2022 La interrelaci\u00f3n de varios agentes en el mismo entorno entorpece la evaluaci\u00f3n del agente a evaluar en lo que a recompensas se refiere, ya que hace disminuir\nsu recompensa media conforme se encuentren simult\u00e1neamente m\u00e1s agentes en el entorno. Seg\u00fan lo visto, la compartici\u00f3n de las recompensas entre todos los agentes no resulta adecuado, por lo que se deber\u00e1 estudiar otra forma de introducir nuevos agentes sin que, por ello, entorpezca la evaluaci\u00f3n del agente que se est\u00e9 evaluando.\nTrabajo futuro\nA la vista de todo lo anterior, pensamos que la arquitectura supone una base de evaluaci\u00f3n sobre la cual iremos refinando y completando la variedad y complejidad de entornos y de agentes, as\u00ed como el dise\u00f1o de experimentos.\nEn concreto, el trabajo futuro se centrar\u00e1 en:\n\u2022 Generar los espacios siguiendo una distribuci\u00f3n universal: De momento los espacios se generan siguiendo una distribuci\u00f3n uniforme. Los espacios deber\u00e1n\ngenerarse siguiendo una distribuci\u00f3n universal, para acercarse m\u00e1s a la noci\u00f3n intuitiva de complejidad.\n\u2022 Generar objetos: La generaci\u00f3n de los entornos deber\u00e1 permitir la generaci\u00f3n autom\u00e1tica de los objetos. \u2022 Generar otros agentes y su comportamiento: El entorno deber\u00e1 poder generar autom\u00e1ticamente nuevos agentes y su comportamiento utilizando para ello el\nlenguaje de especificaci\u00f3n descrito en la secci\u00f3n 3.5.5.\n\u2022 Generar entornos autom\u00e1ticamente siguiendo alguna distribuci\u00f3n: Se deber\u00e1n poder generar y construir autom\u00e1ticamente todos los elementos de los entornos\n(espacios, objetos y agentes).\n\u2022 Construir la observaci\u00f3n del entorno siguiendo el lenguaje de especificaci\u00f3n descrito en la secci\u00f3n 3.5.3: De momento a los agentes se les entrega la\nobservaci\u00f3n como una copia del espacio utilizado durante la prueba, se deber\u00e1 construir una tira de caracteres que represente a las observaciones para entreg\u00e1rselas a los agentes.\n\u2022 Modificar la interfaz para que se ajuste a las interfaces mostradas en el apartado 3.5.2: Se deber\u00e1 completar el desarrollo de la interfaz y tambi\u00e9n se insertar\u00e1n\nnuevas funcionalidades.\n\u2022 Construir los tests a partir de sesiones: Se deber\u00e1n crear los tests a partir de las sesiones ya construidas.\nP\u00e1gina 105\n\u2022 Autoajustar la complejidad del entorno para cada sesi\u00f3n en funci\u00f3n de los resultados obtenidos por el agente que se est\u00e1 evaluando: Para la evaluaci\u00f3n\ncorrecta de los agentes se deber\u00e1 poder autoajustar la complejidad de los entornos generados para cada sesi\u00f3n, siguiendo el algoritmo en [Hernandez-Orallo & Dowe 2010].\n\u2022 Evaluar personas y animales. \u2022 Evaluar sistemas de IA, como las variantes de AIXI [Veness et al 2009] o agentes\ncon t\u00e9cnicas de Q-learning u otros.\nP\u00e1gina 106\nP\u00e1gina 107\nAp\u00e9ndices\nDebido a que el c\u00f3digo fuente de este proyecto debe ser portable y ejecutable en\ncualquier m\u00e1quina, preferiblemente como Applet Web, la aplicaci\u00f3n ha sido desarrollada en JAVA utilizando el entorno Eclipse. Los comentarios se han realizado en ingl\u00e9s, por si en el futuro parte del proyecto es utilizado o extendido por otros equipos investigadores."}, {"heading": "A1. Extractos representativos del C\u00f3digo Fuente", "text": "En este ap\u00e9ndice incluimos dos partes relevantes del c\u00f3digo fuente; la que se encarga del bucle principal de la interacci\u00f3n entre entorno y agente; y la generaci\u00f3n aleatoria de entornos.\nA1.1. Bucle principal // Loop to interact between the Environment and the Agent for (int actualInteraction = 0; actualInteraction <\nnumberOfInteractions; actualInteraction++) { if (numberOfInteractionsToRelocate != 0 && actualInteraction != 0 && actualInteraction % numberOfInteractionsToRelocate == 0)\nthis.RelocateGoodAndEvilAgents(); // Set the rewards to the cells where are the Good and Evil Agents Cell goodCell = this.goodAgent.GetLocation(); Cell evilCell = this.evilAgent.GetLocation(); goodCell.SetReward(this.maxReward); evilCell.SetReward(this.minReward); for (int actualAgent = 0; actualAgent < this.agents.size();\nactualAgent++) { Agent agent = this.agents.get(actualAgent); // Sends the last Reward to the agent double reward = interactions[actualAgent].GetReward(); this.SendReward(agent, reward); } Observation observation = this.TakeObservation(); for (int actualAgent = 0; actualAgent < this.agents.size(); actualAgent++) { Agent agent = this.agents.get(actualAgent); // Shows the actual Observation to the agent interactions[actualAgent].SetObservation(observation); this.SendObservation(agent, observation); } for (int actualAgent = 0; actualAgent < this.agents.size(); actualAgent++) { Agent agent = this.agents.get(actualAgent);\nP\u00e1gina 108\n// Asks to the Agent for the next action int[] action = this.GetAction(agent); long elapsedTime = agent.GetElapsedTime(); interactions[actualAgent].SetAction(action); interactions[actualAgent].SetElapsedTime(elapsedTime); } // Makes the actions made by the Agents for (int actualAgent = 0; actualAgent < this.agents.size();\nactualAgent++) { Agent agent = this.agents.get(actualAgent); int[] action = interactions[actualAgent].GetAction(); this.MakeAction(agent, action); } // If Good and Evil Agents are in the same cell one of them doesn't move if (this.goodAgent.GetLocation() == this.evilAgent.GetLocation()) // If the Good Agent didn't move the Evil Agent doesn't move if (this.goodAgent.GetLocation() == goodCell) this.LocateAgent(this.evilAgent, evilCell.GetNumber()); // If the Evil Agent didn't move the Good Agent doesn't move else if (this.evilAgent.GetLocation() == evilCell) this.LocateAgent(this.goodAgent, goodCell.GetNumber()); // If both Agents had move one of them doesn't move else if (new Random().nextInt(2) == 0) this.LocateAgent(this.goodAgent, goodCell.GetNumber()); else this.LocateAgent(this.evilAgent, evilCell.GetNumber()); // Set rewards to the Agents for (int actualAgent = 0; actualAgent < this.agents.size(); actualAgent++) { Agent agent = this.agents.get(actualAgent); // Evaluates the Agent action in the Environment double obtainedReward = this.EvaluateAction(agent, interactions[actualAgent]); interactions[actualAgent].SetReward(obtainedReward); } // Update the rewards of all cells by dividing by 2 for (int index = 1; index <= this.space.GetNumberOfCells(); index++) { Cell cell = this.space.GetCell(index); cell.SetReward(cell.GetReward() / 2); } // Update the reward to 0 of all cells where there is an Agent for (Agent agent:this.agents) { Cell cell = agent.GetLocation(); cell.SetReward(0.0); } }\nP\u00e1gina 109\nA1.2. Generaci\u00f3n aleatoria de espacios Random random = new Random(); Distribution distribution = new UniversalDistribution(); // Select the number of cells of the space int numCells = distribution.GetValue(Environment_L.MIN_CELLS); // Select the maximum actions available for the selected number of\ncells int maxActions = Math.min(numCells, Environment_L.MAX_ACTIONS); // Select the number of actions of the space int numActions = distribution.GetValue(Environment_L.MIN_ACTIONS, maxActions); // Constructs the space with the number of cells and actions selected String description = \"\"; // Construct each cell for (int cell = 0; cell < numCells; cell++) { if (cell != 0) description += \"|\"; // For each cell, generate each action for (int action = 1; action < numActions; action++) { int movements = random.nextInt(numCells); char movement = random.nextInt(2) == 0 ? '+' : '-'; description += action; for (int index = 0; index < movements; index++) description += movement; } } return description;"}, {"heading": "A2. Referencias", "text": "[Dowe & Hajek 1997] D L Dowe, A R Hajek (1997), \"A computational extension to the\nTuring Test\", Technical Report #97/322, Dept Computer Science, Monash University, Melbourne, 9pp, 1997. http://www.csse.monash.edu.au/publications/1997/tr-cs97322-abs.html.\n[Dowe & Hajek 1998] D L Dowe and A R Hajek (1998). A non-behavioural, computational\nextension to the Turing Test, pp101-106, Proceedings of the International Conference on Computational Intelligence & Multimedia Applications (ICCIMA'98), Gippsland, Australia, February 1998.\n[Dowe 2008] Dowe, D.L. \u201cForeword re C. S. Wallace\u201d, Computer Journal, 51, 5, pp. 523 \u2013\n560, September, 2008, Christopher Stewart WALLACE (1933-2004) memorial special issue.\n[Embretson & Mc Collam 2000] Embretson, S.E. y McCollam, K.M.S. (2000). Psychometric\napproaches to understanding and measuring intelligence. En R.J. Sternberg (Ed.). Handbook of intelligence (pp. 423-444). Cambridge, UK: Cambridge University Press.\nP\u00e1gina 110\n[Embretson 1998] Embretson, S. E. (1998). A cognitive design system approach to\ngenerating valid tests: Application to abstract reasoning. Psychological Methods, 3, 300-396.\n[Hernandez-Orallo & Dowe 2010] J. Hern\u00e1ndez-Orallo, D.L. Dowe \u201cMeasuring Universal\nIntelligence: Towards an Anytime Intelligence Test\u201d Artificial Intelligence Journal, Elsevier, doi:10.1016/j.artint.2010.09.006, 2010.\n[Hernandez-Orallo & Minaya-Collado 1998] Hern\u00e1ndez-Orallo, Jos\u00e9; Minaya-Collado, N.: A\nFormal Definition of Intelligence Based on an Intensional Variant of Kolmogorov Complexity, Proceedings of the International Symposium of Engineering of Intelligent Systems (EIS'98) ICSC Press 1998, pp. 146-163.\n[Hernandez-Orallo 2000a] Jos\u00e9 Hern\u00e1ndez-Orallo: Beyond the Turing Test. Journal of\nLogic, Language and Information 9(4): 447-466 (2000).\n[Hernandez-Orallo 2000b] Hern\u00e1ndez-Orallo, Jos\u00e9 On The Computational Measurement\nof Intelligence Factors, appeared in A. Meystel Performance Metrics for Intelligent Systems Workshop, National Institute of Standards and Technology, Gaithersburg, MD, USA, August 14-16, 2000, pp. 1-8, section XXI.\n[Hernandez-Orallo 2009] J. Hern\u00e1ndez-Orallo. On discriminative environments,\nrandomness, two-part compression and mml. TR, available at http://users.dsic.upv.es/proy/anynt/, 2009.\n[Hernandez-Orallo 2010a] J. Hern\u00e1ndez-Orallo, On evaluating agent performance in a\nfixed period of time, in: M. Hutter, E. Baum, E. Kitzelmann (Eds.), Artificial General Intelligence, 3rd International Conference AGI, Proceedings, \u201cAdvances in Intelligent Systems Research\" series, Atlantis Press, 2010, pp. 25-30.\n[Hernandez-Orallo 2010b] J. Hern\u00e1ndez-Orallo, A (hopefully) non-biased universal\nenvironment class for measuring intelligence of biological and artificial systems, in: M. Hutter, E. Baum, E. Kitzelmann (Eds.), Artificial General Intelligence, 3rd International Conference AGI, Proceedings, \u201cAdvances in Intelligent Systems Research\" series, Atlantis Press, 2010, pp. 182-183.\n[Herrmann et al 2007] Herrmann, E., Call, J., Hern\u00e1ndez-Lloreda, M.V., Hare, B.,\nTomasello, M. \u201cHumans Have Evolved Specialized Skills of Social Cognition: The Cultural Intelligence Hypothesis\u201d, Science, 7 September 2007, Vol. 317. no. 5843, pp. 1360 - 1366, DOI: 10.1126/science.1146282.\n[Hutter 2005] Hutter, M. \u201cUniversal Artificial Intelligence: Sequential Decisions based on\nAlgorithmic Probability\u201d, Springer, 2005.\n[Hutter 2006] Marcus Hutter, \u201cGeneral Discounting Versus Average Reward\u201d, ALT, Jose L.\nBalcazar and Philip M. Long and Frank Stephan, 244-258, Springer, Lecture Notes in Computer Science, 4264, 2006, http://dblp.unitrier.de/db/conf/alt/alt2006.html#Hutter06.\nP\u00e1gina 111\n[Hutter 2007] Hutter, M. \u201cUniversal algorithmic intelligence: A mathematical top\u2192down\napproach\u201d. In Artificial General Intelligence, pages 227\u2013290. Springer, Berlin, 2007.\n[Laird & Wray 2010] Laird, J.E. and Wray III, R.E., \u201cCognitive Architecture Requirements\nfor Achieving AGI\u201d, Advances in Intelligent Systems Research\" (ISSN 1951-6851), \"Artificial General Intelligence\", Third Conference on Artificial General Intelligence, AGI 2010, Lugano, Switzerland, March 5-8, 2010, editors, Eric Baum, Marcus Hutter and Emanuel Kitzelmann, ISBN: 978-90-78677-36-9.\n[Laird et al 1987] Laird, Rosenbloom, Newell, John and Paul, Allen (1987). \"Soar: An\nArchitecture for General Intelligence\". Artificial Intelligence, 33: 1-64.\n[Laird 2008] Laird, J.E. \u201cExtending the Soar cognitive architecture\u201d, Artificial General\nIntelligence 2008: Proceedings of the First AGI Conference, 2008.\n[Legg & Hutter 2007] Legg, S.; Hutter, M. Universal Intelligence: A Definition of Machine\nIntelligence Shane Legg and Marcus Hutter. In Minds and Machines, pages 391-444, volume 17, number 4, November 2007. http://www.vetta.org/documents/UniversalIntelligence.pdf.\n[Legg 2008] Shane Legg. Department of Informatics, University of Lugano, June 2008.\nhttp://www.vetta.org/documents/Machine_Super_Intelligence.pdf.\n[Levin 1973] Levin, L.A. \"Universal search problems\" Problems Inform. Transmission,\n9:265-266, 1973.\n[Li & Vitanyi 2008] Li, Ming; Vit\u00e1nyi, Paul \"An Introduction to Kolmogorov Complexity and\nits Applications\" 3rd Edition. Springer-Verlag, New York, 2008.\n[Markov 1960] A. A. Markov. The theory of algorithms. American Mathematical Society\nTranslations, series 2, 15:1-14, 1960.\n[Martinez-Arias et al 2006] Mart\u00ednez-Arias, M.Rosario; Hern\u00e1ndez-Lloreda, M.Victoria;\nHern\u00e1ndez-Lloreda, M.Jos\u00e9. \u201cPsicometr\u00eda\u201d, Alianza Editorial 2006.\n[Pell 1994] B. Pell. A strategic metagame player for general chesslike games. In AAAI,\npages 1378-1385, 1994.\n[Rissanen 1983] J. Rissanen. A universal prior for integers and estimation by minimum\ndescription length. Annals of Statistics, 11(2):416-431, 1983.\n[Sanghi & Dowe 2003] Sanghi, P. and D.L. Dowe (2003). A computer program capable of\npassing I.Q. tests, Proc. 4th International Conference on Cognitive Science (and 7th Australasian Society for Cognitive Science Conference), ICCS ASCS 2003 (http://www.cogsci.unsw.edu.au), Univ. of NSW, Sydney, Australia, 13-17 July 2003, Vol. 2, pp. 570-575.\n[Schaeffer 2007] Schaeffer, J. and Burch, N. and Bjornsson, Y. and Kishimoto, A. and\nMuller, M. and Lake, R. and Lu, P. and Sutphen, S., \u201cCheckers is solved\u201d, Science, 317, 5844, 1518, 2007, AAAS.\nP\u00e1gina 112\n[Solomonoff 1964] Solomonoff, R.J. \"A formal theory of inductive inference\" inform.\nContr. vol. 7, pp. 1-22, Mar. 1964; also, pp. 224-254, June 1964.\n[Solomonoff 1986] Solomonoff, R.J. \"The Application of Algorithmic Probability to\nProblems in Artificial Intelligence\" in L.N.Karnal and J.F. Lemmer (eds.) Uncertainty in Artificial Intelligence (L.N. Karnal and J.F. Lemmer, eds.), Elsevier Science, pp. 473-491, 1986.\n[Spearman 1904] Spearman, C. \u201c \u2018General Intelligence\u2019 objectively determined and\nmeasured\u201d American Journal of Psychology 15, 201-293, 1904.\n[Sutton & Barto 1998] Sutton, Richard S.; Andrew G. Barto (1998). Reinforcement\nLearning: An Introduction. MIT Press.\n[Turing 1950] Turing, Alan \u201cComputing Machinery and Intelligence\u201d Reprinted in \u201cMinds\nand Machines\u201d, edited by Alan Ross Anderson, Englewood Cliffs, N.J., Prentice Hall 1964.\n[Veness et al 2009] Joel Veness, Kee Siong Ng, Marcus Hutter, and David Silver. A Monte\nCarlo AIXI Approximation. CoRR, abs/0909.0801, 2009. informal publication (http://arxiv.org/abs/0909.0801).\n[von Ahn et al 2002] von Ahn, L.; Blum, M.; Langford, J. \u201cTelling Humans and Computers\nApart (Automatically) or How Lazy Cryptographers do AI\u201d Communications of the ACM, 2002. www.captcha.net.\n[von Ahn et al 2008] von Ahn, L.; Maurer, B.; McMillen, C.; Blum, M. \"reCAPTCHA:\nHuman-Based Character Recognition via Web Security Measures\" Science, Vol. 321, September 12th, 2008.\n[Wallace & Boulton 1968] Wallace, C.S. and Boulton, D. M. \"An Information Measure for\nClassification\" Computer Journal, 11, 2, pp. 185-195, 1968.\n[Wallace & Dowe 1999] C. S. Wallace and D. L. Dowe \"Minimum Message Length and\nKolmogorov Complexity\", Computer Journal, 1999, vol 42, no. 4,, pp. 270-283.\n[Wallace 2005] C. S. Wallace, Statistical and Inductive Inference by Minimum Message\nLength, Information Science and Statistics\", 432, Springer Verlag, May, 2005.\n[Weyns, Parunak & Michel 2005] D. Weyns, H.V.D. Parunak, F. Michel, T. Holvoet, and J.\nFerber. Environments for multi-agent systems, state-of-the-art and research challenges. In Environments for multi-agent systems, volume 3374 of LNCS, 2005."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": null, "creator": "PDFCreator Version 0.9.7"}}}