{"id": "1103.1013", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2011", "title": "A Feature Selection Method for Multivariate Performance Measures", "abstract": "The proposed method presents an exponential size optimization problem for both attribute groups and label configurations for a given dataset. To solve this problem, a two-layer algorithm is proposed for the intersection level: The outer layer performs the generation of group characteristics, while the inner layer learns the label configuration for multivariate performance metrics. Extensive experiments on large-area and high-dimensional real-world datasets show that the proposed method can be significantly better at selecting a small subset of characteristics than $l _ 1 $-SVM and SVM-RFE and achieves significantly improved performance over SVM $^ {perf} $in terms of $F _ 1 $score. It also learns a sparse but effective decision rule for multivariate performance metrics.", "histories": [["v1", "Sat, 5 Mar 2011 07:10:41 GMT  (73kb)", "https://arxiv.org/abs/1103.1013v1", null], ["v2", "Sat, 4 May 2013 14:48:06 GMT  (97kb)", "http://arxiv.org/abs/1103.1013v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi mao", "ivor w tsang"], "accepted": false, "id": "1103.1013"}, "pdf": {"name": "1103.1013.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["IVOR W. TSANG"], "emails": ["QMAO1@ntu.edu.sg.", "IvorTsang@ntu.edu.sg."], "sections": [{"heading": null, "text": "ar X\niv :1\n10 3.\n10 13\nv2 [\ncs .L\nG ]\n4 M\nay 2"}, {"heading": "1. INTRODUCTION", "text": "Machine learning methods have been widely applied to a variety of learning tasks (e.g. classification, ranking, structure prediction, etc) arising in computer vision, text mining, natural language processing and bioinformatics applications. Depending on applications, specific performance measures are required to evaluate the success of a learning algorithm. For instance, the error rate is a sound judgment for evaluating the classification performance of a learning method on datasets with balanced positive and negative examples. On the contrary, in text classification where positive examples are usually very few, one can simply assign all testing examples with the negative class (the major class), this trivial solution can easily achieve very low error rate due to the extreme imbalance of the data. However, the goal of text classification is to correctly detect positive examples. Hence, the error rate is considered as a poor criterion for the problems with highly skewed class distributions [11]. To address this issue, F1-score and Precision/Recall Breakeven Point (PRBEP) are employed as the evaluation criteria for text classification. Besides this, in information retrieval, search engine systems are required to return the top k documents (images) with the highest precision because most users only scan the first few of them presented by the system, so precision/recall at k are preferred choices.\nInstead of optimizing the error rate, Support Vector Machine for multivariate performance measures (SVMperf ) [11] was proposed to directly optimize the losses based on a variety of multivariate performance measures. A smoothing version of SVMperf [37] was proposed to accelerate the convergence of the optimization problem specially designed\nQi Mao and Ivor W. Tsang are with School of Computer Engineering, Nanyang Technological University, Singapore 639798, e-mail {QMAO1,IvorTsang}@ntu.edu.sg.\n1\nfor PRBEP and area under the Receiver Operating Characteristic curve (AUC). Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28]. Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.\nFor some real applications, such as image and document retrievals, a set of sparse yet discriminative features is a necessity for rapid prediction on massive databases. However, the learned weight vector of the aforementioned methods is usually non-sparse. In addition, there are many noisy or non-informative features in text documents and images. Even though the task-specific performance measures can be optimized directly, learning with these noisy or non-informative features may still hurt both prediction performance and efficiency. To alleviate these issues, one can resort to embedded feature selection methods [15], which can be categorized into the following two major directions.\nOne way is to consider the sparsity of a decision weight vector w by replacing l2-norm \u2016w\u20162 regularization in the structural risk functional (e.g. SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23]. A thorough study to compare several recently developed l1regularized algorithms has been conducted in [33]. According to this study, coordinate descent method using one-dimensional Newton direction (CDN) achieves the state-ofthe-art performance by solving l1-regularized models on large-scale and high-dimensional datasets. To achieve a sparser solution, the Approximation of the zeRO norm Minimization (AROM) was proposed [30] to optimize l0 models. Its resultant problem is non-convex, so it easily suffers from local optima. However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].\nAnother way is to sort the weights of a SVM classifier and remove the smallest weights iteratively, which is known as SVM with Recursive Feature Elimination (SVM-RFE) [9]. However, as discussed in [32], such nested \u201cmonotonic\u201d feature selection scheme leads to suboptimal performance. Non-monotonic feature selection (NMMKL) [32] has been proposed to solve this problem, but each feature corresponding to one kernel makes NMMKL infeasible for high-dimensional problems. Recently, Tan et al. [26] proposed Feature Generating Machine (FGM), which shows great scalability to non-monotonic feature selection on large-scale and very high-dimensional datasets.\nThe aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only. To fulfill the needs of different applications, it is imperative to have a feature selection method designed for optimizing task-specific performance measures.\nTo this end, we first propose a generalized sparse regularizer for feature selection. After that, a unified feature selection framework is presented for general loss functions based on the proposed regularizer. Particularly, in this paper, optimizing multivariate performance measures is studied in this framework. To our knowledge, this is the first work to optimize multivariate performance measures for feature selection. Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data. To tackle this challenge, we propose a two-layer cutting plane algorithm, including group feature generation (see Section 5.1) and group\nfeature selection (see Section 5.2), to solve this problem effectively and efficiently. Specifically, Multiple Kernel Learning (MKL) trained in the primal by cutting plane algorithm is proposed to deal with exponential size of constraints induced by multivariate losses.\nThis paper is an extension of our preliminary work [19]. The main contributions of this paper are listed as follows.\n\u2022 The implementation details and the convergence proof of the proposed two-layer cutting plane algorithm and MKL algorithm trained in the primal are presented. \u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details. By comparing with these methods, the advantages of our proposed methods are summarized as follows: (1) The tradeoff parameter C in l1 SVM [33] is too sensitive to be tuned properly\nsince it controls both margin loss and the sparsity of w. However, our method alleviates this problem by introducing an additional parameter B to control the sparsity of w. This separation makes parameter tuning for our methods much easier than those of SKM [3] and l1 SVM. (2) NMMKL [32] uses the similar parameter separation strategy, but it is intractable for this method to handle high-dimensional datasets, let alone optimize multivariate losses. The proposed method can readily optimize multivariate losses for high-dimensional problems. (3) FGM [26] is a special case of the propose framework when optimizing square hinge loss with indicator variables in integer domain. The proposed framework is formulated in the real domain for general loss functions. In particular, we provide a natural extension of FGM for multivariate losses. (4) The proposed framework can be interpreted by l0-norm constraint, so it can be considered as one of l0 methods. This gives another interpretation of the additional parameter B. \u2022 Recall that Multiple-Instance Learning via Embedded instance Selection (MILES) [6], which transforms multiple instance learning (MIL) into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features, achieves state-of-the-art performance for multiple instance learning problems. Under our unified feature selection framework, we extend MILES and study MIL for multivariate performance measure. To our best knowledge, this is seldom studied in MIL scenarios, but it is important for the real world applications of MIL tasks. \u2022 Extensive experiments on several challenging and very high-dimensional real world datasets show that the proposed method yields better performance than the stateof-the-art feature selection methods, and outperforms SVMperf using all features in terms of multivariate performance measures. The experimental results on the multiple instance dataset show that our proposed method achieves promising results.\nThe rest of the paper is organized as follows: We briefly review SVMperf in Section 2. We then introduce the proposed generalized sparse regularizer in Section 3. In particular, we study the feature selection framework for multivariate performance measures, its algorithm and its application to multiple instance learning in Section 4, 5 and 7, respectively. Section 6 gives the analysis of connections to a variety of feature selection methods. The extensive empirical results are shown in Section 8. Finally, conclusive remarks are presented in the last section.\nIn the sequel, A 0 means that the matrix A is symmetric and positive semidefinite (psd). We denote the transpose of a vector/matrix by the superscript T and lp norm of a vector v by ||v||p. Binary operator \u2299 represents the elementwise product between two vectors/matrices."}, {"heading": "2. SVM FOR MULTIVARIATE PERFORMANCE MEASURE", "text": "Given a training sample of input-output pairs (xi, yi) \u2208 X \u00d7 Y for i = 1, . . . , n drawn from some fixed but unknown probability distribution with X \u2286 Rm and Y \u2208 {\u22121,+1}. The learning problem is treated as a multivariate prediction problem by defining the hypotheses h : X \u2192 Y that map a tuple x \u2208 X of n feature vectors x = (x1, . . . , xn) to a tuple y \u2208 Y of n labels y = (y1, . . . , yn) where X = X \u00d7 . . . ,X and Y \u2286 {\u22121,+1}n. The linear discriminative function of SVMperf is defined as\n(1) hw(x) = argmax y\u2032\u2208Y f(x, y\u2032) = argmax y\u2032\u2208Y\nn\u2211\ni=1\ny\u2032iw T xi,\nwhere w = [w1, . . . , wm]T is the weight vector. To learn the hypothesis (1) from training data, large margin method is employed to obtain the good generalization performance by enforcing the constraints that the decision value of the ground truth labels y should be larger than any possible labels y\u2032 \u2208 Y\\{y}, i.e., f(x, y\u2032) \u2265 f(x, y\u2032) + \u2206(y, y\u2032), where \u2206(y, y\u2032) is some type of multivariate loss functions (several instantiated losses are presented in Section 5.4). Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,\nmin w,\u03be\u22650\n1\n2 \u2016w\u201622 + C\u03be(2)\ns.t. \u2200y\u2032 \u2208 Y\\y : wT n\u2211\ni=1\n(yi \u2212 y\u2032i)xi \u2265 \u2206(y, y\u2032)\u2212 \u03be,\nwhere C is a regularization parameter that trades off the empirical risk and the model complexity.\nThe optimization problem (2) is convex, but there is the exponential size of constraints. Fortunately, this problem can be solved in polynomial time by adopting the sparse approximation algorithm of structural SVMs. As shown in [11], optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure."}, {"heading": "3. GENERALIZED SPARSE REGULARIZER", "text": "In this paper, we focus on minimizing the regularized empirical loss functional as\n(3) min w \u2126(w) + C\u2113(w),\nwhere \u2126(.) is a regularization function and \u2113(.) is any loss function, including multivariate performance measure losses.\nSince l2-norm regularization is used in (2), the learned weight vector w is non-sparse, and so the linear discriminant function in (1) would involve many features for the prediction. As discussed in Section 1, selecting a small set of discriminative features is crucial to many real applications. In order to enforce the sparsity on w, we propose a new sparse\nregularizer\n\u2126(w) = min d\u2208D\n1\n2\nm\u2211\nj=1\n|wj |p dj ,\nwhere d is in the real domain of D = {d|\u2211mj=1 dj = B, 0 \u2264 dj \u2264 1, \u2200j = 1, . . . ,m}, p > 0 and B > 0 are two parameters. The optimal solution of the new proposed regularizer should satisfy wj = 0 if dj = 0 since |wj |p = 0 with p > 0 induces wj = 0, otherwise the objective value approaches to infinite. The l1-norm constraint \u2211m j=1 dj = B and 0 \u2264 dj \u2264 1 will force some dj to be zero, so the correspondingwj is zero, \u2200j = 1, . . . ,m. Hence, the parameter B is interpreted as a budget to control the sparsity of w.\nThis regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2]. However, it is different from l1 when B 6= 1. To explain the difference, we consider the problem (2) under the general framework (3). In the separable case, parameter C does not affect the optimum solution since the error \u03be = 0. If l1 norm is applied to replace l2 in Problem (2), the sparsity of w will be fixed once optimal solution is reached. Hence, parameter B in D now can be considered as the only factor to enforce sparsity on w. However, in the non-separable case where errors are allowed, parameter C will also influence the sparsity of w, but B is expected to enforce the sparsity of w more explicitly when C becomes larger. This argument will be empirically justified in Section 8.1.\nThe learning algorithm with the proposed generalized sparse regularizer is formulated as\nmin d\u2208D min w\n1\n2\nm\u2211\nj=1\n|wj |p dj + C\u2113(w).(4)\nThis formulation is more general for feature selection.\nLemma 1. If p \u2265 2, Problem (4) is jointly convex with respect to w and d; otherwise, it is not jointly convex.\nProof. We only need to prove that, if p \u2265 2, g(wj , dj) = |wj| p\ndj where dj > 0 is jointly con-\nvex with respect to wj and dj . The convexity of g in its domain is established when the fol-\nlowing holds: \u22072g =\n  2|wj | p d3 j\n\u2212 p|wj | p\u22121\nd2 j\n\u2212 p|wj| p\u22121\nd2 j\np(p\u22121)|wj | p\u22122\ndj\n  0 \u21d4 [ 2|wj |2 \u2212p|wj |dj\n\u2212p|wj|dj p(p\u2212 1)d2j\n]\n0, which is equivalent to vT\u22072gv \u2265 0 for any nonzero vector v. WLOG, we assume v = [1 a]T where a is any real number, then this condition is reduced to: 2|wj |2 \u2212 2ap|wj|dj + a2p(p\u2212 1)d2j \u2265 0 \u21d4 2 ( |wj | \u2212 apdj2 )2 \u2265 a 2d2jp(2\u2212p) 2 . This condition always holds when p \u2265 2, which completes the proof.\nIn what follows, we focus on the convex formulation with p = 2. In Section 6, we will discuss the relationships with a variety of the state-of-the-art feature selection methods."}, {"heading": "4. FEATURE SELECTION FOR MULTIVARIATE PERFORMANCE MEASURES", "text": "To optimize the multivariate loss functions and learn a sparse feature representation simultaneously, we propose to solve the following jointly convex problem over d and (w, \u03be)\nin the case of p = 2,\nmin d\u2208D min w,\u03be\u22650\n1\n2\nm \u2211\nj=1\n|wj | 2\ndj + C\u03be(5)\ns.t. \u2200y\u2032 \u2208 Y\\y : wT 1\nn\nn \u2211\ni=1\n(yi \u2212 y \u2032 i)xi \u2265 \u2206(y, y \u2032)\u2212 \u03be.\nThe partial dual with respect to (w, \u03be) is obtained by Lagrangian function L(w, \u03be, \u03b1, \u03c4) with dual variables\u03b1 \u2265 0 and \u03c4 \u2265 0 as follows: 12 \u2211m j=1 |wj | 2 dj +C\u03be\u2212\u03c4\u03be\u2212\u2211y\u2032\u2208Y\\y \u03b1y\u2032(wT 1n \u2211n i=1(yi\u2212 y\u2032i)xi\u2212\u2206(y, y\u2032)+\u03be). As the gradients of Lagrangian function with respect to (w, \u03be) vanish at the optimal points, we obtain the KKT conditions: wj = dj \u2211 y\u2032\u2208Y\\y \u03b1y\u2032 1 n \u2211n i=1(yi \u2212\ny\u2032i)xj,i and \u2211\ny\u2032\u2208Y\\y \u03b1y\u2032 \u2264 C. By substituting KKT conditions back to L(w, \u03be, \u03b1, \u03c4), we obtain the dual problem as\nmin d\u2208D max \u03b1\u2208A \u22121 2\n\u2211\ny\u2032\n\u2211\ny\u2032\u2032\n\u03b1y\u2032\u03b1y\u2032\u2032Q d y\u2032,y\u2032\u2032 +\n\u2211\ny\u2032\n\u03b1y\u2032by\u2032 ,(6)\nwhere \u2206(y, y) = 0, \u2206(y, y\u2032) > 0 if y 6= y\u2032,\nQdy\u2032,y\u2032\u2032 =\nm\u2211\nj=1\ndj\n( \u2211\ny\u2032\u2208Y\\y\n\u03b1y\u2032 1\nn\nn\u2211\ni=1\n(yi \u2212 y\u2032i)xj,i )2\n=\nm\u2211\nj=1\n( \u2211\ny\u2032\u2208Y\\y\n\u03b1y\u2032 1\nn\nn\u2211\ni=1\n(yi \u2212 y\u2032i)xj,i \u221a dj\n)2\n= \u3008ay\u2032 , ay\u2032\u2032\u3009, ay\u2032 = 1n \u2211n i=1(yi \u2212 y\u2032i)(xi \u2299 \u221a d), by\u2032 = 1n\u2206(y, y\n\u2032), and A = {\u03b1|\u2211y\u2032 \u03b1y\u2032 \u2264 C,\u03b1 \u2265 0}. Problem (6) is a challenging problem because of the exponential size of \u03b1 and highdimensional vector d for high-dimensional problems."}, {"heading": "5. TWO-LAYER CUTTING PLANE ALGORITHM", "text": "In this section, we propose a two-layer cutting plane algorithm to solve Problem (6) efficiently and effectively. The two layers, namely group feature generation and group feature selection, will be described in Section 5.1 and 5.2, respectively. The two-layer cutting plane algorithm will be presented in Section 5.3 and 5.4.\n5.1. Group Feature Generation. By denoting S(\u03b1, d) = \u2212 12 \u2211 y\u2032 \u2211 y\u2032\u2032 \u03b1y\u2032\u03b1y\u2032\u2032Q d y\u2032,y\u2032\u2032 +\u2211\ny\u2032 \u03b1y\u2032by\u2032 , Problem (6) turns out to be\nmin d\u2208D max \u03b1\u2208A S(\u03b1, d).\nSince domains D and A are nonempty, the function S(\u03b1\u2217,d) is closed and convex for all d \u2208 D given any \u03b1\u2217 \u2208 A, and the function S(\u03b1,d\u2217) is closed and concave for all \u03b1 \u2208 A given any d\u2217 \u2208 D, the saddle-point property: mind\u2208D max\u03b1\u2208A S(\u03b1, d) = max\u03b1\u2208A mind\u2208D S(\u03b1, d) holds [4].\nWe further denote Fd(\u03b1) = \u2212S(\u03b1, d), and then the equivalent optimization problems are obtained as\nmin \u03b1\u2208A max d\u2208D Fd(\u03b1) or min \u03b1\u2208A,\u03b3 \u03b3 : \u03b3 \u2265 Fd(\u03b1), \u2200d \u2208 D.(7)\nCutting plane algorithm [14] could be used here to solve this problem. Since maxd\u2208D Fd(\u03b1) \u2265 Fdt(\u03b1), \u2200dt \u2208 D, the lower bound approximation of (30) can be obtained bymaxd\u2208D Fd(\u03b1) \u2265 maxt=1,...,T Fdt(\u03b1). Then we minimize Problem (30) over the set {dt}Tt=1 by,\nmin \u03b1\u2208A max t=1,...,T Fdt(\u03b1) or min \u03b1\u2208A,\u03b3 \u03b3 :\u03b3\u2265Fdt(\u03b1), \u2200t=1,. . .,T.(8)\nAs from [22], such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst-case analysis. Specifically, for a fixed \u03b1t, the worstcase analysis can be done by solving,\n(9) dt = argmax d\u2208D Fd(\u03b1t),\nwhich is referred to as the group generation procedure. Even though Problem (8) and (9) cannot be solved directly due to the exponential size of \u03b1, we will show that they are readily solved in Section 5.2 and Section 5.4, respectively.\n5.2. Group Feature Selection. By introducing dual variables \u00b5 = [\u00b51, \u00b52, . . . , \u00b5T ]T \u2265 0, we can transform (8) to an MKL problem as follows,\n(10) max \u03b1\u2208A min \u00b5\u2208MT\n\u2212 1\n2\n\u2211\ny\u2032\n\u2211\ny\u2032\u2032\n\u03b1y\u2032\u03b1y\u2032\u2032\n(\nT \u2211\nt=1\n\u00b5tQ dt\ny\u2032,y\u2032\u2032\n)\n+ \u2211\ny\u2032\n\u03b1y\u2032by\u2032 ,\nwhere MT = { \u2211T\nt=1 \u00b5t = 1, \u00b5t \u2265 0, \u2200t = 1, . . . , T }. However, due to the exponential size of \u03b1, the complexity of Problem (29) remains. In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more. The following proposition shows that we can indirectly solve Problem (29) in the primal form.\nProposition 1. The primal form of Problem (29) is\nmin w1,...,wT ,\u03be\u22650\n1\n2\n( T\u2211\nt=1\n\u2016wt\u20162 )2 + C\u03be(11)\ns.t. \u03be \u2265 by\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009, \u2200y\u2032 \u2208 Y\\y.\nAccording to KKT conditions, the solution of (29) is\nwt = \u00b5t \u2211\ny\u2032\n\u03b1y\u2032aty\u2032(12)\nwhere \u00b5t is a dual value of the tth constraint of (8).\nThe detailed proof of Proposition 1 is given in the supplementary material. Here, we define the regularization term as \u2126(w) = 12 (\u2211T t=1 \u2016wt\u20162 )2 with w = [w1, . . . ,wT ]T and\nthe empirical risk function as (13) Remp(w) = max ( 0, max\ny\u2032\u2208Y\\y by\u2032 \u2212\nT\u2211\nt=1\n\u3008wt, aty\u2032\u3009 ) ,\nwhich is a convex but non-smooth function w.r.t w. Then we can apply the bundle method [27] to solve this primal problem. Problem (29) is transformed as\nmin w\nJ (w) = \u2126(w) + CRemp(w).\nSince Remp(w) is a convex function, its subgradient exists everywhere in its domain [10]. Suppose wk is a point where Remp(w) is finite, we can formulate the lower bound according to the definition of subgradient,\nRemp(w) \u2265 Remp(wk) + \u3008w \u2212 wk, pk\u3009 = \u3008w, pk\u3009+Remp(wk)\u2212 \u3008wk, pk\u3009\nwhere subgradient pk \u2208 \u2202wRemp(wk) is at wk. In order to obtain pk, we need to solve the following inference problem\n(14) yk = arg max y\u2032\u2208Y\\y\nby\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009\nwhich is a problem of integer programming. We delay the discussion of this problem to Section 5.4. After that, we can obtain the subgraident pkt = \u2212atyk , so that Remp(w k) = byk \u2212 \u2211T t=1\u3008wt, atyk\u3009 = byk + \u3008w k, pk\u3009.\nGiven the subgradient sequence p1, p2, . . . , pK , the tighter lower bound for Remp(w) can be reformulated as follows,\nRemp(w) \u2265 RKemp(w) = max ( 0, max\n1\u2264k\u2264K \u3008w, pk\u3009+ qk\n) ,\nwhere qk = Remp(wk)\u2212\u3008wk, pk\u3009 = byk . Following the bundle method [27], the criterion for selecting the next point wK+1 is to solve the following problem,\nmin w1,...,wT ,\u03be\u22650\n1\n2\n( T\u2211\nt=1\n\u2016wt\u20162 )2 + C\u03be(15)\ns.t. \u03be \u2265 \u3008w, pk\u3009+ qk, \u2200k = 1, . . . ,K. The following Corollary shows that Problem (15) can be easily solved by QCQP solvers, and the number of variables is independent of the number of examples.\nCorollary 1. In terms of Proposition 1, the dual form of Problem (15) is\nmax \u03b1\u2208AK max \u03b8\n\u2212\u03b8 + K\u2211\nk=1\n\u03b1kq k(16)\ns.t. 1\n2 \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\n\u03b1kpkt \u2225\u2225\u2225\u2225\u2225 2\n2\n\u2264 \u03b8, \u2200t = 1, . . . , T,\nwhere AK = { \u2211K\nk=1 \u03b1k \u2264 C,\u03b1k \u2265 0, \u2200k = 1, . . . ,K}, and which is a QCQP problem with T + 1 constraints and K + 1 variables.\nThe proof of Corollary 1 follows the same derivation of Proposition 1 with pkt = \u2212atyk , qk = byk and the size of \u03b1k as K . Consequently, the primal variables are recovered by wt = \u2212\u00b5t \u2211 k \u03b1kp k t .\nLet JK(w) = \u2126(w)+CRKemp(w), the \u01eb-optimal condition in Algorithm 1 is min0\u2264k\u2264K J (wK)\u2212 JK(wK) \u2264 \u01eb. The convergence proof in [27] does not apply in this case as the Fenchel dual of \u2126(w) fails to satisfy the strong convexity assumption if K > 1. As K = 1, Algorithm 1 is exactly the bundle method [27]. When K \u2265 2, we can adapt the proof of Theorem 5 in [13] for the following convergence results.\nAlgorithm 1 Group feature selection\n1: Input: x = (x1, . . . , xn), y = (y1, . . . , yn), an initial group set W , \u01eb, C 2: Y = \u2205, k = 0 3: repeat 4: k = k + 1 5: Finding the most violated y\u2032 6: Compute pk and qk 7: Y = Y \u222a {y\u2032} 8: Solving Problem (16) over W and Y 9: until \u01eb-optimal\nTheorem 1. For any 0 < C, 0 < \u01eb \u2264 4R2C and any training example (x1, y1), . . . , (xn, yn), Algorithm 1 converges to the desired precision \u01eb after at most,\n\u2308 log2 ( \u2206\n4R2C\n)\u2309 + \u2308 16R2C\n\u01eb\n\u2309\niterations. R2 = maxdt,y\u2032 \u2016 1n \u2211n i=1(yi \u2212 y\u2032i)(xi \u2299 \u221a\ndt)\u20162, \u2206 = maxy\u2032 \u2206(y\u2032, y) and \u2308.\u2309 is the integer ceiling function.\nProof. We adapt the proof of Theorem 5 in [13], and sketch the necessary changes corresponding to Problem (29). For a given set WT , the dual objective of (8) can be reformulated as\nmax \u03b1\u2208A min d\u2208WT\n\u0398d(\u03b1) = \u2212 1\n2\n\u2211\ny\u2032\n\u2211\ny\u2032\u2032\n\u03b1y\u2032\u03b1y\u2032\u2032Q d y\u2032,y\u2032\u2032 +\n\u2211\ny\u2032\n\u03b1y\u2032by\u2032 .\nSince there are the T constrained quadratic problems, we consider each d \u2208 WT at one time as max\u03b1\u2208A \u0398d(\u03b1), where Qd is positive semi-definite, and derivative \u2202\u0398d(\u03b1) = b\u2212 Qd\u03b1. The Lemma 2 in [13] states that a line search starting at \u03b1 along an ascent direction \u03b7 with maximum step-size C > 0 improves the objective by at least max0\u2264\u03b2\u2264C { \u0398d(\u03b1+ \u03b2\u03b7) \u2212 \u0398d(\u03b1) } \u2265 12 min { C, \u2202\u0398d(\u03b1) T \u03b7 \u03b7T Qd\u03b7 } \u2202\u0398d(\u03b1) T \u03b7. If we consider subgradient descent method, the line search along the subgradient of objective is \u2202\u0398d\u2217(\u03b1) where d \u2217 = mind\u2208WT \u0398d(\u03b1). Therefore, the maximum improvement is\nmax 0\u2264\u03b2\u2264C\n{\u0398d\u2217(\u03b1+ \u03b2\u03b7) \u2212\u0398d\u2217(\u03b1)}\n\u2265 1 2 min\n{ C, \u2202\u0398d\u2217(\u03b1) T \u03b7\n\u03b7TQd \u2217 \u03b7\n} \u2202\u0398d\u2217(\u03b1) T \u03b7\n\u2265 1 2 min d\u2208WT\n{ C, \u2202\u0398d(\u03b1) T \u03b7\n\u03b7TQd\u03b7\n} \u2202\u0398d(\u03b1) T \u03b7.(17)\nWe can see that it is a special case of [13] if T = 1. According to Theorem 5 in [13], for a newly added constraint y\u0302 and some \u03b3d > 0, we can obtain \u2202\u0398d(\u03b1)T \u03b7 = \u03b3d by setting the ascent direction \u03b7y\u0302 = 1 for the newly added y\u0302 and \u03b7y = \u2212 1C\u03b1y for the others. Here, we set \u03b3 = mind\u2208WT \u03b3d so as to be the lower bound of \u2202\u0398d(\u03b1)\nT \u03b7, \u2200d \u2208 WT . In addition, the upper bound for \u03b7TQd\u03b7 \u2264 4R2, \u2200d \u2208 WT can also be obtained by the fact that \u03b7TQd\u03b7 = Qdy\u0302,y\u0302 \u2212 2C \u2211 y\u2032 \u03b1y\u2032Q d y\u2032,y\u0302 + 1 C2 \u2211 y\u2032 \u2211 y\u2032\u2032 \u03b1y\u2032\u03b1y\u2032\u2032Q d y\u2032,y\u2032\u2032 \u2264 R2 + 2CCR2 +\n1 C2 C2R2 = 4R2, \u2200d \u2208 WT . By substituting them back to (17), the similar result shows\nthe increase of the objective is at least\nmin\n{ C\u03b3\n2 , \u03b32 8R2\n} .\nMoreover, the initial optimality gap is at most C\u2206. Following the remaining derivation in [13], the overall bound results are obtained.\nRemark 1: Problem (15) is similar to Support Kernel Machine (SKM) [3] in which the multiple Gaussian kernels are built on random subsets of features, with varying widths. However, our method can automatically choose the most violated subset of features as a group instead of a subset of random features. Such random features lead to a local optimum; while our method could guarantee the \u01eb-optimality stated in Theorem 1. However, due to the extra cost of computing nonlinear kernel, the current model are only implemented for linear kernel with learned subsets of features.\nRemark 2: The original Problem (30) could be easily formulated as a QCQP problem with exponential size of variables \u03b1 needed to be optimized and huge number of base kernels in the quadratic term. Unfortunately, the standard MKL methods cannot handle Problem (30) even for a small dataset, let alone the standard QCQP solver. However, Corollary 1 makes it practical to solve a sequence of small QCQP problems directly using standard off-line QCQP solvers, such as Mosek. Note that state-of-the-art MKL solvers can also be used to solve the small QCQP problems, but they are not preferred because their solutions are less accurate than that of standard QCQP solvers, which can solve Problem (16) more accurately in this case.\n5.3. The Proposed Algorithm. Algorithm 1 can obtain the \u01eb-optimal solution for the original dual problem (8). By denoting Gd(\u03b1) = 12 || \u2211K k=1 \u03b1kp k||22 \u2212 \u2211K k=1 \u03b1kq k, the group feature generation layer can directly use the \u01eb-optimal solution of the objective Gd(\u03b1) to approximate the original objective Fd(\u03b1). The two-layer cutting plane algorithm is presented in Algorithm 2. From the description of Algorithm 2, it is clear to see that\nAlgorithm 2 The Two-Layer Cutting Plane Algorithm\n1: Input: x = (x1, . . . , xn), y = (y1, . . . , yn), \u01eb, C 2: W = \u2205, t = 0 3: repeat 4: t = t+ 1 5: Finding the most violated dt 6: W = W \u222a {dt} 7: Call group feature selection(x, y, W , \u01eb, C) 8: until \u01eb-optimal\ngroups are dynamically generated and augmented into active set W for group selection. In terms of the convergence proof of FGM in [26] and Theorem 1, we can obtain the following theorem to illustrate the approximation with an \u01eb-optimal solution to the original problem.\nTheorem 2. After Algorithm 2 stops in a finite number of steps, the difference between optimal solution (d\u2217, \u03b1\u2217) of Problem (29) and the solution (d, \u03b1) of Algorithm 2 is Fd(\u03b1)\u2212 Fd\u2217(\u03b1\u2217) \u2264 \u01eb.\nThe detailed proof of Theorem 2 is given in the supplementary material.\n5.4. Finding the Most Violated y\u2032 and d. Algorithm 1 and Algorithm2 need to find the most violated y\u2032 and d, respectively. In this subsection, we discuss how to obtain these quantities efficiently. Algorithm 1 needs to calculate the subgradient of the empirical risk function RKemp(w). Since R K emp(w) is a pointwise supremum function, the subgradient should be in the convex hull of the gradient of the decomposed functions with the largest objective. Here, we just take one of these subgradients by solving\nyk = arg max y\u2032\u2208Y\\y\n\u2206(y\u2032, y)\u2212 n\u2211\ni=1\n(yi \u2212 y\u2032i)vi,(18)\nwhere vi = \u2211T t=1 w T t (xi \u2299 \u221a dt). After obtaining yk, it is easy to compute pkt = \u2212 1 n \u2211n i=1(yi \u2212 yki )(xi \u2299 \u221a dt) and qk = 1 n \u2211n i=1 \u2206(y\nk, y). For finding the most violated y\u2032, it depends on how to define the loss \u2206(y, y\u2032) in Problem (18). One of the instances is the Hamming loss which can be decomposed and computed independently, i.e., \u2206(y, y\u2032) = \u2211n i=1 \u03b4(yi, y \u2032 i), where \u03b4 is an indicator function with \u03b4(yi, y \u2032 i) = 0 if yi = y \u2032 i, otherwise 1. However, there are some multivariate performance measures which could not be solved independently. Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms. In this paper, we only use several multivariate performance measures based on contingency table as the showcases and their finding yk could be solved in time complexity O(n2) [11]. Given the true labels y and predicted labels y\u2032, the contingency tables is defined as follows\ny=1 y=-1 y\u2019=1 a b y\u2019=-1 c d\nF1-score: The F\u03b2-score is a weighted harmonic average of Precision and Recall. Ac-\ncording to the contingency table, we can obtain F\u03b2 = (1+\u03b22)a\n(1+\u03b22)a+b+\u03b22c . The most common choice is \u03b2 = 1. The corresponding balanced F1 measure loss can be written as \u2206F1(a, b, c, d) = 100(1\u2212 F1). Then, Algorithm 2 in [11] can be directly applied.\nPrecision/Recall@k: In search engine systems, most users scan only the first few links that are presented. In this situation, Prec@k and Rec@k measure the precision and recall of a classifier that predicts exactly k documents, i.e., Prec@k = a\na+b and Rec@k = a a+c ,\nsubject to a + b = k. The corresponding loss could be defined as \u2206Prec@k = 100(1 \u2212 Prec@k) and \u2206Rec@k = 100(1 \u2212 Rec@k). And the procedure of finding most violated y is similar to F-score, while the only difference is keeping constraint a + b = k and removing a+ b 6= k.\nPrecision/Recall Break-Even Point (PRBEP): The Precision/Recall Break-Even Point requires that the precision and its recall are equal. According to above definition, we can see PRBEP only adds a constraint a + b = a + c, or b = c. The corresponding loss is defined as \u2206PRBEP = 100(1 \u2212 PRBEP ). Finding the most violated y should enforce the constraint b = c.\nAfter t iterations in Algorithm 2, we transform \u03b1 in Problem (9) from the exponential size to a small size \u03b1t. Now, finding the most violated d becomes\ndt =argmax d\u2208D Gd(\u03b1t)(19)\n=argmax d\u2208D\n1\n2\n\u2225\u2225\u2225\u2225 K\u2211\nk=1\n\u03b1tkp k \u2225\u2225\u2225\u2225 2\n2\n\u2212 K\u2211\nk=1\n\u03b1tkq k\n=argmax d\u2208D\n1\n2\n\u2225\u2225\u2225\u2225 1\nn\nK\u2211\nk=1\n\u03b1tk\nn\u2211\ni=1\n(yi \u2212 yki )(xi \u2299 \u221a d) \u2225\u2225\u2225\u2225 2\n=argmax d\u2208D\n1\n2n2\nm\u2211\nj=1\nc2jdj\nwhere cj = \u2211K k=1 \u03b1 t k \u2211n i=1(yi \u2212 yki )xi,j . With the budget constraint \u2211m i=1 di = B in D, (19) can be solved by first sorting c2j \u2019s in the descent order and then setting the first B numbers corresponding to dtj to 1 and the rest to 0. This takes only O(m logm) operations."}, {"heading": "6. RELATIONS TO EXISTING METHODS", "text": "In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26]. It can be easily adapted to the general framework (4).\n6.1. Connections to SKM and l1 SVM. Let D1 = {d| \u2211m\nj=1 dj = 1, dj \u2265 0, \u2200j = 1, . . . ,m} be in the real domain. We observe that D = D1 when B = 1. According to [24], we transform Problem (5) in the special case of B = 1 to the following equivalent optimization problem,\nmin w,\u03be\u22650\n1\n2\n( m\u2211\nj=1\n|wj | )2 + C\u03be(20)\ns.t. \u2200y\u2032 \u2208 Y\\y : wT 1 n\nn\u2211\ni=1\n(yi \u2212 y\u2032i)xi \u2265 \u2206(y, y\u2032)\u2212 \u03be.\nSKM [3] attempts to obtain the sparsity of w by penalizing the square of a weighted block l1-norm ( \u2211k j=1 \u03b3j ||wj ||2)2 where k is the number of groups and wj is the weight vector\nfor the features in the jth group. The regularizer ( \u2211m\nj=1 |wj |)2 used in (20) is the square of the l1 norm (||w||1)2, which is a special case of SKM when k = m and \u03b3j = 1, i.e., each group contains only one feature. Minimizing the square of the l1-norm is very similar to l1-norm SVM [33] by setting \u2126(w) = ||w||1 with the non-negative (convex) loss function.\nRegardless of l1-norm or the square of l1-norm, the parameter C is too sensitive to be tuned properly since it controls both margin loss and the sparsity of w. However, our method alleviates this problem by two parameters C and B which control margin loss and sparsity of w, respectively. This separation makes parameter tuning of our method easier than those of SKM and l1 SVM.\n6.2. Connection to NMMKL. Instead of directly solving Problem (20), we formulate a more general problem (5) by introducing an additional budget parameter B, which directly controls the sparsity of w. The advantage is to make parameter tuning easily done since C is not sensitive to the sparsity of w. This strategy is also used in NMMKL [32], but one feature corresponding to one base kernel makes NMMKL intractable for highdimensional problems. The multivariate loss is even hard to be optimized by NMMKL since there are exponential dual variables in the dual form of NMMKL from the exponential number of constraints. However, our method can readily optimize multivariate loss on high-dimensional data.\n6.3. Connection to FGM. According to the work [40], we can reformulate Problem (20) as an equivalent optimization problem\nmin d\u2208D1 min w,\u03be\u22650\n1\n2\nm\u2211\nj=1\ndj |wj |2 + C\u03be(21)\ns.t.\u2200y\u2032 \u2208 Y\\y : 1 n\nm\u2211\nj=1\ndjwj\nn\u2211\ni=1\n(yi \u2212 y\u2032i)xj,i \u2265 \u2206(y, y\u2032)\u2212 \u03be.\nAfter the substitutions of vj = \u221a djwj , \u2200j = 1, . . . ,m and the general case of D, we can obtain the following problem\nmin d\u2208D min v,\u03be\u22650\n1\n2 \u2016v\u201622 + C\u03be(22)\ns.t.\u2200y\u2032 \u2208 Y\\y : vT 1 n\nn\u2211\ni=1\n(yi \u2212 y\u2032i)(xi \u2299 \u221a d) \u2265 \u2206\u0303(y, y\u2032)\u2212 \u03be,\nwhere v = [v1, . . . , vm]T . After deriving Lagrangian dual problem of (22), we observe that it is same as Problem (6). Problem (19) always finds the most violated d in the integer domain {0, 1}m, so the solutions of the following problem solved by the proposed twolayer cutting plane algorithm is the same as the solutions of Problem (6)\nmin d\u2208D2 min v,\u03be\u22650\n1\n2 \u2016v\u201622 + C\u03be(23)\ns.t.\u2200y\u2032 \u2208 Y\\y : vT 1 n\nn\u2211\ni=1\n(yi \u2212 y\u2032i)(xi \u2299 d) \u2265 \u2206\u0303(y, y\u2032)\u2212 \u03be,\nwhere the integer domain D2 = {d| \u2211m\nj=1 dj \u2264 B,d \u2208 {0, 1}m}. This formula can be equally derived as the extension of FGM for multivariate performance measures by defining the new hypotheses\n(24) h\u0303v(x) = argmax y\u2032\u2208Y\nn\u2211\ni=1\ny\u2032i(v \u2299 d)T xi,\nwhere h\u0303v : X \u2192 Y and d \u2208 D2. It is not trivial to perform the extension of FGM to optimize multivariate loss because original FGM method [26] cannot directly apply to solve the exponential number of constraints. And our domain of d is in real domain D which is more general than the integer domainD2 used in FGM and the proposed extension (23), even though the final solutions of (5) and (23) are the same.\n6.4. Connection to l0 SVM. The following Lemma indicates that the proposed formula can be interpreted by l0-norm constraint.\nLemma 2. (23) is equivalent to the following problem\nminw\u0303,\u03be\u22650 1\n2 \u2016w\u0303\u201622 + C\u03be(25)\ns.t. \u2200y\u2032 \u2208 Y\\y : w\u0303T 1 n\nn\u2211\ni=1\n(yi \u2212 y\u2032i)xi \u2265 \u2206\u0303(y, y\u2032)\u2212 \u03be,\n\u2016w\u0303\u20160 \u2264 B.\nProof. Note, at the optimality of (22), WLOG, suppose dj = 0, the corresponding vj must be 0. Thus, \u2016v\u20160 \u2264 \u2016d\u20160. Let w\u0303 = v\u2299d, we have \u2016w\u0303\u20160 = \u2016v\u2299d\u20160 \u2264 min{\u2016v\u20160, \u2016d\u20160} \u2264 \u2016d\u20160 = \u2211m j=1 dj \u2264 B. Moreover, \u2016w\u0303\u201622 = \u2016v \u2299 d\u201622 = \u2016v\u201622 at the optimality. Therefore, the optimal solution of (22) is a feasible solution of (25). On the other hand, for the optimal w\u0303 in (25), let v = w\u0303 and di = \u03b4(w\u0303i) where \u03b4(t) = 1 if t 6= 0; otherwise, 0. So, the optimal solution of (25) is a feasible solution of (22).\nThis gives another interpretation of parameter B from the perspective of l0-norm. Since l0-norm ||w\u0303||0 represents the number of non-zero entries of w\u0303, so B in our method can be considered as the parameter which directly controls the sparsity of w."}, {"heading": "7. MULTIPLE INSTANCE LEARNING FOR MULTIVARIATE PERFORMANCE MEASURES", "text": "We have already illustrated the proposed framework by optimizing multivariate performance measures for feature selection in Section 4. In this section, we extend this approach to solve multiple instance learning problems which have been employed to solve a variety of learning problems, e.g., drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature. However, it is crucial to optimize the task specific performance measures, e.g., F score is widely considered as the most important evaluation criterion for a learning method in image retrieval.\nMulti-instance learning was formally introduced in the context of drug activity prediction [7]. In this learning scenario, a bag is represented by a set of instances where each instance is represented by a feature vector. The classification label is only assigned to each bag instead of the instances in this bag. We name a bag as a positive bag if there is at least one positive instance in this bag, otherwise it is called negative bag. The learning problem is to decide whether the given unlabeled bag is positive or not. By defining a similarity measure between a bag and an instance, Multiple-Instance Learning via Embedded instance Selection (MILES) [6] successfully transforms multiple instance learning into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features.\nBefore discussing the transformation in MILES, we first give the notations of multiple instance learning problem. Following the notations in [6], we denote ith positive bags as B+i = {x+i,j} n + i j=1 which consists of n + i instances x + i,j , j = 1, . . . , n + i . Similarly, the ith negative bags is denoted as B\u2212i = {x\u2212i,j} n\u2212 i\nj=1. All instances belongs to the same feature space X . The number of positive bags and negative bags are \u2113+ and \u2113\u2212, respectively. The instances in all bags are rearranged as {x1, . . . ,xn} where n = \u2211\u2113 +\ni=1 n + i + \u2211\u2113\u2212 i=1 n \u2212 i .\nBy considering each instance in the training bags as a candidate for target concepts, the embedded feature space is represented as\n(26) x\u0302i = [s(x1,Bi), . . . , s(xn,Bi)]T \u2208 Rn, where the similarity measure between the bag Bi and the instance xk is defined as the most-likely-cause estimator\n(27) s(xk,Bi) = max j exp\n( \u2212||xi,j \u2212 x\nk||2 2\u03c32\n) .\nIt follows the intuition that the similarity between a concept and a bag is determined by the concept and the closest instance in this bag. The corresponding labels are constructed as follows: y\u0302i = 1 if Bi is a positive bag, otherwise y\u0302i = \u22121. For a given \u2113+ positive bags and \u2113\u2212 negative bags, we form a new classification representation of the multiple instance learning problem as {x\u0302i, y\u0302i}\u2113 ++\u2113\u2212 i=1 . For each instance x k, the new feature representation corresponds to the values of the kth feature variable s(xk, \u00b7) is [s(xk,B+1 ), . . . , s(x k,B+ \u2113+ ), s(xk,B\u22121 ), . . . , s(x k,B\u2212 \u2113\u2212 )]\nwhere the feature induced by xk provides the useful information for separating the positive and negative bags. The linear discriminant function\n(28) y\u0302 = sign(\u3008w, x\u0302\u3009+ b) wherew and b are the model parameters. The embedding induces a possible high-dimensional space when the number of instances in the training set is large. Since some instances may not be responsible for the label of the bags or might be similar to each other, many features are redundant or irrelevant, so MILES employs L1-SVM to select a subset of mapped features that is most relevant to the classification problem. However, L1-SVM cannot fulfill to obtain a high performance over the task-specific measures because it only focuses on optimizing zero-one loss function. Our proposed Algorithm 2 is a natural alternative feature selection method for multi-variate performance measures. The proposed algorithm for multiple instance learning to optimize multivariate measures is shown in Algorithm 3.\nAlgorithm 3 Learning a bag classifier\n1: Input: positive bags {B+i }\u2113 + i=1, negative bags {B\u2212i }\u2113 \u2212\ni=1, C, and \u01eb 2: Construct the embedding representation of training data\n{(x\u0302i, y\u0302i)}, \u2200i = 1, . . . , \u2113+ + \u2113\u2212\n3: x = [x\u03021, . . . , x\u0302\u2113++\u2113\u2212 ] and y = [y\u03021, . . . , y\u0302\u2113++\u2113\u2212 ] 4: call Algorithm 2 with arguments (x,y,C,\u01eb) 5: Output: parameters w\nAccording to Algorithm 3, we do not need the model parameter b since the structural SVM is irrelevant to the relative offset b, i.e., y\u0302 = argmaxy\u2208{\u22121,+1} y\u3008w\u0302, x\u0302\u3009."}, {"heading": "8. EXPERIMENTS", "text": "In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F11, which is l1 regularized SVM for optimizing F1 score\n1http://users.cecs.anu.edu.au/\u02dcchteo/BMRM.html\nby bundle method [27]. SVM-RFE and FGM use Liblinear software 2 as the QP solver for their SVM subproblems. For l1-SVM, we also use Liblinear software, which implements the state-of-the-art l1-SVM algorithm [33]. In addition to the comparison for 0-1 loss, we also perform experiments on image data for F1 measure. Furthermore, several specific measures on the contingency table are investigated on Text datasets by comparing with SVMperf [11]. All the datasets shown in Table 1 are of high dimensions.\nFor convenience, we name our proposed two-layer cutting plane algorithm FS\u2206multi, where \u2206 represents different type of multivariate performance measures. We implemented Algorithm 2 in MATLAB for all the multivariate performance measures listed above, using Mosek as the QCQP solver for Problem (16) which yields a worse-case complexity of O(KT 2). Removing inactive constraints from the working set [13] in the inner layer is employed for speedup the QCQP problem. Since the values of both K and T are much smaller than the number of examples n and its dimensionality m, the QCQP is very efficient as well as more accurate for large-scale and high-dimensional datasets. Furthermore, the codes simultaneously solve the primal and its dual form. So the optimal \u00b5 and \u03b1 can be obtained after solving Problem (16).\nFor a test pattern x, the discriminant function can be obtained by f(x) = \u3008w \u2299 d\u0303, x\u3009 where w = \u2211n i=1 \u03b2ixi, \u03b2i = 1 n \u2211K k=1 \u03b1k(yi \u2212 yki ), and d\u0303 = \u2211T t=1 \u00b5t \u221a dt. This leads to the faster prediction since only a few of the selected features are involved. After computing pk, the matrices of Problem (16) can be incrementally updated, so it can be done totally in O(TK2).\n8.1. Parameter Sensitivity Analysis. Before comparing FS\u2206multi with other methods, we first conduct empirical studies for the parameter sensitivity analysis on News20.binary. The goal is to examine the relationships among parameters C and B, performance measures and the number of selected features with the range of C in [0.1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].\nFigure 1(a-b) show the testing accuracy and F1 scores as well as the number of selected features by varying C and B. We observe that the results are very sensitive to C when B is very small. This indicates that the l1 model, which is equivalent to the proposed method in the case of B = 1, is vulnerable to the choice of C. On the other hand, the results are rather insensitive to C when B is large. Hence, the proposed method is less sensitive to C than l1 model. We also observe that the proposed method prefers a large C value for better performances. Figure 1(c-d) demonstrate the corresponding relationships among parameters B, C and the number of selected features of Figure 1(a-b). We observe that B and the number of selected features always exhibits a linear trend with a constant slope. Moreover, the slope remains the same when C \u2265 10, but a small C will increase the slope. This means that, compared with B, parameter C has less influence on the sparsity of w,\n2http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/\nand the learned feature selection model becomes stabilized when C \u2265 10. These empirical results are consistent to the discussions of parameter B in Section 3.\nSince large C needs more iterations to converge according to Theorem 1, the compromise is to set C not too large and let B dominate the selection of features. According to these observations, we can safely fix C and study the results by varying B to compare with other methods in the following experiments.\n8.2. Time Complexity Analysis. We empirically study the time complexity of FSF1multi by comparing with other methods. Two datasets News20.binary and Image (Desert) are used for illustration. The detailed setting are shown in Section 8.3 and Section 8.4, respectively. Figure 2 gives the training time over five different methods. On News20.binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter \u03bb \u2208 [10\u22127, 102] due to the extremely high dimensionality. We observe that the proposed methods are slower than l1-SVM, but much faster than SVM-RFE and l1-bmrm-F1. In addition, on Image dataset, when the termination condition with the relative difference between the objective and its convex linear lower bound lower than 0.1 is set, l1-bmrm-F1 also cannot converge\nafter the maximum iteration, which is consistent with the discussion in Appendix C of [27] that bundle method with l1 regularizer cannot guarantee the convergence. This leads to the similar number of selected features (e.g., 98 in Figure 2(b)) even though \u03bb is decreasing gradually.\nThese observations implies that our proposed two-layer cutting plane method needs less time for training with guaranteed convergence than bundle method. Moreover, our method can work on large scale and high dimensional data for optimizing user-specified measure, but bundle method cannot. As aforementioned, l1-bmrm-F1 is much slower on the high dimensional datasets in our experiments, so we can only report its results in Section 8.4.\n8.3. Feature Selection for Accuracy. Since [11] has proven that SVM\u2206multi with Hamming loss, namely\u2206Err(y, y\u2032) = 2(b+c), is the same as SVM. In this subsection, we evaluate the accuracy performances of FS\u2206multi for Hamming loss function, namely FS hamming multi as well as other state-of-the-art feature selection methods. We compare these methods on two binary datasets, News20.binary 3 and URL1 in Table 1. Both datasets are used in [26], and they are already split into training and testing sets.\nWe test FGM and SVM-RFE in the grid CFGM = [0.001, 0.01, 0.1, 1, 5, 10]and choose CFGM = 5 which gives good performance for both FGM and SVM-RFE. This is the same as [26]. For FShammingmulti , we do the experiments by fixing CFGMmulti as 0.1 \u00d7 n for URL1 and 1.0 \u00d7 n for New20.binary. The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1. The elimination scheme of features for SVM-RFE method can be referred to [26]. For l1-SVM, we report the results of different C values so as to obtain different number of selected features.\nFigure 3 reports testing accuracy on different datasets. The testing accuracy is comparable among different methods, but both FShammingmulti and FGM can obtain better prediction performances than SVM-RFE in a small number (less than 20) of selected features on both News20.binary and URL1. These results show that the proposed method with Hamming loss can work well on feature selection tasks especially when choosing only a few features. FShammingmulti also performs better than l1-SVM on News20.binary in most range of selected features. This is possibly because l1 models are more sensitive to noisy or redundant features on News20.binary dataset.\n3http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets\nFigure 4 shows that our method with the small B will select smaller number of features than the large B. We also observed that most of features selected by the small B also appeared in the subset of features using the large B. This phenomenon can be obviously observed on News20.binary. This leads to the conclusion that FShammingmulti can select the important features in the given datasets due to the insensitivity of parameter B. However, we notice that not all the features in the selected subset of features with smaller B fall into that of subset of features with the large B, so our method is non-monotonic feature selection. This argument is consistent with the test accuracy in Figure 3. News20.binary seems to be monotonic datasets from Figure 4, since FShammingmulti , FGM and SVM-RFE demonstrate similar performance. However, URL1 is more likely to be non-monotonic, as our method and FGM can do better than SVM-RFE. All the facts imply that the proposed method is comparable with FGM and SVM-RFE. And it also demonstrates the nonmonotonic property for feature selection.\n8.4. Feature Selection for Image Retrieval. In this subsection, we demonstrate the specific multivariate performance measures are important to select features for real applications. In particular, we evaluate F1 measure (commonly used performance measure) for\nthe task of image retrieval. Due to the success of transforming multiple instance learning into a feature selection problem by embedded instance selection, we use the same strategy in Algorithm 4.1 of [6] to construct a dense and high-dimensional dataset on a preprocessed image data 4. This dataset is used in [38] for multi-instance learning. It contains five categories and 2, 000 images. Each image is represented as a bag of nine instances generated by the SBN method [20]. Each image bag is represented by a collection of nine\n4http://lamda.nju.edu.cn/data MIMLimage.ashx\n15-dimensional feature vectors. After that, following [6], the natural scene image retrieval problem turns out to be a feature selection task to select relevant embedded instances for prediction. The Image dataset are split randomly with the proportion of 60% for training and 40% for testing (Table 1). Since F1-score is used for performance metric, we perform FS\u2206multi for F1-score, namely FS F1 multi as well as other state-of-the-art feature selection methods. As mentioned above, FGM and FShammingmulti have similar performances, we will not report the results of FGM here. FShammingmulti and FS \u2206 multi use the fixed C = 10 \u00d7 n. For other methods, we use the previous settings. The testing F1 values of all methods on each category are reported in Figure 5.\nFrom Figure 5, we observe that FSF1multi and FS hamming multi achieve significantly improved performance over l1-SVM in term of F1-score especially when choosing less than 100 features. Moreover, SVM-RFE also outperforms l1-SVM on three categories out of five. This verifies that \u21131 penalty does not perform as well as \u21130 methods like FS F1 multi and FS hamming multi on dense and high-dimensional datasets. It is possibly because \u21131-norm penalty is very sensitive to dense and noisy features. We also observe that FSF1multi performs better than FShammingmulti and SVM-RFE on four over five categories. l1-bmrm-F1 performs competitively but it is unstable and time-consuming as shown in Section 8.2. All these facts imply that directly optimizing F1 measure is useful to boost F1 performance measure, and our proposed FSF1multi is efficient and effective.\n8.5. Multivariate Performance Measures for Document Retrieval. In this subsection, we focus on feature selection for different multivariate performance measures on imbalanced text data shown in Table 1. For multiclass classification problems, one vs. rest strategy is used. The comparing model is SVMperf 5. Following [11], we use the same notation SVM\u2206multi for different multivariate performance measures. The command used for training SVMperf can work for different measures by -l option 6. In our experiments, we search the Cperf in the same range [2\u22126, . . . , 26] as in [11]. We choose the one which\n5www.cs.cornell.edu/People/tj/svm light/svm perf.html 6 svm perf learn -c Cperf -w 3 \u2013b 0 train file train model\ndemonstrates the best performance of SVM\u2206multi to each multivariate performance measure for comparison. FS\u2206multi and FS hamming multi fix CFGMmulti = 0.1 \u00d7 n for News20 except 5.0\u00d7n for Sector. For Rec@k, we use k as twice the number of positive examples, namely Rec@2p. The evaluation for this measure uses the same strategy to label twice the number of positive examples as positive in the test datasets, and then calculate Rec@2p.\nTable 2 shows the macro-average of the performance over all classes in a collection in which both FS\u2206multi and FS hamming multi at B = 250 are listed. The improvement of FS \u2206 multi over FShammingmulti and SVM \u2206 multi with respect to different B values are reported in Figure 6. From Table 2, FS\u2206multi is consistently better than FS hamming multi on all multivariate performance measures and two multiclass datasets. Similar results can be obtained comparing with SVM\u2206multi, while the only exception is the measure Rec@2p on News20 where SVM\u2206multi is a little better than FS \u2206 multi. The largest gains are observed for F1 score on all two text classification tasks. This implies that a small number of features selected by FS\u2206multi is enough to obtain comparable or even better performances for different measures than SVM\u2206multi using all features.\nFrom Figure 6, FS\u2206multi consistently performs better than FS hamming multi for all of the multivariate performance measures from the figures in the left-hand side. Moreover, the figures in the right-hand side show that the small number of features are good for F1 measures,\nbut poor for other measures. As the number of features increases, Rec@2p and PRBEP can approach to the results of SVM\u2206multi and all curves become flat. The performance of PRBEP and Rec@2p is relatively stable when sufficient features are selected, but our method can choose very few features for fast prediction. For F1 measure, our method is consistently better than SVM\u2206multi, and the results show significant improvement over all range of B. This improvement may be due to the reduction of noisy or non-informative features. Furthermore, FS\u2206multi can achieve better performance measures than FS hamming multi .\nWe also compared different feature selection algorithms such as SVM-RFE and l1-SVM on Sector and News20 in the same setting as the previous sections. The results in terms of F1 measure are reported in Figure 7. We clearly observe that FS\u2206multi outperforms l1-SVM on both datasets, and comparable or even better than SVM-RFE. For a small number of features, FS\u2206multi can still demonstrate very good F1 measure."}, {"heading": "9. CONCLUSION", "text": "In this paper, we propose a generalized sparse regularizer for feature selection, and the unified feature selection framework for general loss functions. We particularly study in details for multivariate losses. To solve the resultant optimization problem, a two-layer cutting plane algorithm was proposed. The convergence property of the proposed algorithm is studied. Moreover, connections to a variety of state-of-the-art feature selection methods are discussed in details. A variety of analyses by comparing with the various feature selection methods show that the proposed method is superior to others. Experimental results show that the proposed method is comparable with FGM and SVM-RFE and better than l1 models on feature selection task, and outperforms SVM for multivariate performance measures on full set of features."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by Singapore A*star under Grant SERC 112 280 4005\nAppendices"}, {"heading": "A. PROOF OF PROPOSITION 1", "text": "Since the loss term \u2206(y\u2032, y\u2032) = 0 for all y\u2032 \u2208 Y , we can equivalently transform Problem\nmin w1,...,wT ,\u03be\u22650\n1\n2\n( T\u2211\nt=1\n\u2016wt\u20162 )2 + C\u03be\ns.t. \u03be \u2265 by\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009, \u2200y\u2032 \u2208 Y\\y,\ninto the following optimization problem\nmin w1,...,wT ,\u03be\u22650\n1\n2\n( T\u2211\nt=1\n\u2016wt\u20162 )2 + C\u03be\ns.t. \u03be \u2265 by\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009, \u2200y\u2032 \u2208 Y.\nBy introducing a new variable u \u2208 R and moving out summation operator from objective to be a constraint, we can obtain the equivalent optimization problem as\nmin w,\u03be\u22650\n1 2 u2 + C\u03be\ns.t. \u03be \u2265 by\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009, \u2200y\u2032 \u2208 Y\nT\u2211\nt=1\n\u2016wt\u2016 \u2264 u.\nWe can further simplify above problem by introducing another variables \u03c1 \u2208 Rm such that \u2016wt\u2016 \u2264 \u03c1t, \u2200t = 1, . . . , T , to be\nmin w,u,\u03c1,\u03be\u22650\n1 2 u2 + C\u03be\ns.t. \u03be \u2265 by\u2032 \u2212 T\u2211\nt=1\n\u3008wt, aty\u2032\u3009, \u2200y\u2032 \u2208 Y\nT\u2211\nt=1\n\u03c1t \u2264 u\n||wt|| \u2264 \u03c1t, \u2200t = 1, . . . , T.\nWe know that for each t, \u2016wt\u2016 \u2264 \u03c1t is a second-order cone constraint. Following the recipe of [5], the self-dual cone \u2016vt\u20162 \u2264 \u03b7t, \u2200t = 1, . . . , T can be introduced to form the\nLagrangian function as follows\nL(w, \u03be, u, \u03c1;\u03b1, \u03c4, \u03b3, v, \u03b7)\n= 1\n2 u2 + C\u03be \u2212\n\u2211\ny\u2032\n\u03b1y\u2032 ( \u03be \u2212 by\u2032 + T\u2211\nt=1\n\u3008wt, aty\u2032\u3009 ) \u2212 \u03c4\u03be\n+\u03b3\n( T\u2211\nt=1\n\u03c1t \u2212 u ) \u2212 T\u2211\nt=1\n(\u3008vt,wt\u3009+ \u03b7t\u03c1t),\nwith dual variables \u03b1t \u2208 R+, \u03c4 \u2208 R+, \u03b3 \u2208 R+. The derivatives of the Lagrangian with respect to the primal variables have to vanish which leads to the following KKT conditions:\nvt = \u2212 \u2211\ny\u2032\n\u03b1y\u2032a t y\u2032 , \u2200t = 1, . . . , T\nC \u2212 \u2211\ny\u2032\n\u03b1y\u2032 \u2212 \u03c4 = 0\nu = \u03b3\n\u03b3 = \u03b7t, \u2200t = 1, . . . , T\nBy substituting all the primal variables with dual variables by above KKT conditions, we can obtain the following dual problem,\nmax \u03b1,\u03b3\n\u22121 2 \u03b32 +\n\u2211\ny\u2032\n\u03b1y\u2032by\u2032\ns.t. \u2225\u2225\u2225 \u2211\ny\u2032\n\u03b1y\u2032a t y\u2032 \u2225\u2225\u2225 \u2264 \u03b3, \u2200t = 1, . . . , T\n\u2211\ny\u2032\n\u03b1y\u2032 \u2264 C, \u03b1y\u2032 \u2265 0, \u2200y\u2032 \u2208 Y\nBy setting \u03b8 = 12\u03b3 2 and A = {\u2211y\u2032 \u03b1y\u2032 \u2264 C,\u03b1y\u2032 \u2265 0, \u2200y\u2032 \u2208 Y}, we can reformulate above problem as\nmax \u03b8,\u03b1\u2208A\n\u2212\u03b8 + \u2211\ny\u2032\n\u03b1y\u2032by\u2032\ns.t. 1\n2 \u03b1TQt\u03b1 \u2264 \u03b8, \u2200t = 1, . . . , T\nwhere Qty\u2032,y\u2032\u2032 = \u3008aty\u2032 , aty\u2032\u2032\u3009. According to the property of self-dual cone [3], we can obtain the primal solution from its dual as wt = \u2212\u00b5tvt = \u00b5t \u2211 y\u2032 \u03b1y\u2032a t y\u2032 where \u00b5j is the dual\nvariable of the jth quadratic constraint such that \u2211m\nj=1 \u00b5j = 1, \u00b5j \u2208 R+, \u2200j = 1, . . . ,m. By constructing Lagrangian with dual variables \u00b5 with respect to \u03b8, we can recover Problem\n(29) max \u03b1\u2208A min \u00b5\u2208MT \u22121 2\n\u2211\ny\u2032\n\u2211\ny\u2032\u2032\n\u03b1y\u2032\u03b1y\u2032\u2032\n( T\u2211\nt=1\n\u00b5tQ dt y\u2032,y\u2032\u2032\n) + \u2211\ny\u2032\n\u03b1y\u2032by\u2032 ,\nwhere MT = { \u2211T t=1 \u00b5t = 1, \u00b5t \u2265 0, \u2200t = 1, . . . , T }. This completes the proof."}, {"heading": "B. PROOF OF THEOREM 2", "text": "Given the Problem\nmin \u03b1\u2208A max d\u2208D Fd(\u03b1) or min \u03b1\u2208A,\u03b3 \u03b3 : \u03b3 \u2265 Fd(\u03b1), \u2200d \u2208 D,(30)\nwe have the equivalent optimization problem as\nmax \u03b1\u2208A,\u03b3\n\u2212\u03b3\ns.t. \u03b3 \u2265 Fd(\u03b1), \u2200d \u2208 D.\nThe outer layer of Algorithm 2 can generate a sequence of configurations ofd as {d1, . . . ,dk} after k iterations. In the kth iteration, the most violated constraint dk+1 is found in terms of \u03b1k, so thatFdk+1(\u03b1k) = maxd\u2208D Fd(\u03b1) according to Problem dt = argmaxd\u2208D Fd(\u03b1t). Hence, we can construct two sequences {\u03b3\nk } and {\u03b3k} such that\n\u03b3 k = max 1\u2264t\u2264k\nFdt(\u03b1t)(31)\n\u03b3k = min 1\u2264t\u2264k Fdt+1(\u03b1t) = min 1\u2264t\u2264k max d\u2208D Fd(\u03b1t)(32)\nSuppose that we can solve min\u03b1\u2208A max1\u2264t\u2264k Fdt(\u03b1) exactly. Due to the equivalence to Problem (29), it means that we can obtain the exact solution of the problem (29). Based on this assumption, equation (31) can be further reformed as\n\u03b3 k = max 1\u2264t\u2264k Fdt(\u03b1t) = min \u03b1\u2208A max 1\u2264t\u2264k Fdt(\u03b1t).(33)\nThis turns out to be the same problem of FGM [26]. For self-completeness, we give the theorem as follows,\nTheorem 3 ([26]). Let (\u03b1\u2217, \u03b3\u2217) be the globally optimal solution pair of Problem (30), sequences {\u03b3\nk } and {\u03b3k} have the following property\n(34) \u03b3 k \u2264 \u03b3k \u2264 \u03b3k.\nAs k increases, {\u03b3 k } is monotonically increasing and {\u03b3k} is monotonically decreasing.\nBased on above theorem, global optimal solution can be obtained after a finite number of iterations. However, the assumption of the accurate solution for (29) usually has no formal guarantee. We have already proven in Theorem 1 that the inner problem of Algorithm 2 can reach the desired precision \u01eb after a finite number of iterations by Algorithm 1. Therefore, according to Algorithm 2, we can construct the following sequence\n\u03b3\u2032 k = max 1\u2264t\u2264k Fdt(\u03b1t) \u2264 min \u03b1\u2208A max 1\u2264t\u2264k Fdt(\u03b1t) + \u01eb.(35)\nBy combining inequalities (34) and (35), we obtain the following inequalities\n(36) \u03b3\u2032 k \u2212 \u01eb \u2264 \u03b3 k \u2264 \u03b3k \u2264 \u03b3k.\nAfter a finite number of iterations, the global optimal solution is \u03b3\u2217 = \u03b3 k = \u03b3k = \u03b3k. Hence, the solution of the Algorithm 2 may be not less than the lower bound \u03b3\u2032 k\nby \u01eb. It is complete for Theorem 2."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning, 4:1\u2013106,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Analysis and Nonlinear Optimization", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, Cambridge, UK.,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "MILES: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "TPAMI, 28:1931\u20131947,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Solving the multiple instance problem with axisparallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-Perez"], "venue": "Artificial Intelligence, 89:31\u201371,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "A feature selection newton method for support vector machine classification", "author": ["G.M. Fung", "O.L. Mangasarian"], "venue": "Computational Optimization and Applications, 28:185\u2013202,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyou", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning, 46:389\u2013422,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J.B. Hiriart-Urruty", "C. Lemarechal"], "venue": "Springer-Verlag,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "SIGKDD,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C.J. Yu"], "venue": "Machine Learning, 77:27\u2013 59,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The cutting plane algorithm for solving convex programs", "author": ["J.E. Kelley"], "venue": "Journal of the Society for Industrial and Applied Mathematics, 8(4):703\u2013712,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1960}, {"title": "Embedded methods", "author": ["T.N. Lal", "O. Chapelle", "J. Weston", "A. Elisseeff"], "venue": "I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh, editors, Feature Extraction: Foundations and Applications, Studies in Fuzziness and Soft Computing, number 207, pages 137\u2013165. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Direct optimization of ranking measures", "author": ["Q.V. Le", "A. Smola"], "venue": "JMLR, 1:1\u201348,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A risk ratio comparison of l0 and l1 penalized regressions", "author": ["D. Lin", "D.P. Foster", "L.H. Ungar"], "venue": "Technical report, University of Pennsylvania,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse logistic regression with lp penalty for biomarker identification", "author": ["Z. Liu", "F. Jiang", "G. Tian", "S. Wang", "F. Sato", "S.J. Meltzer", "M. Tan"], "venue": "Statistical Applications in Genetics and Molecular Biology, 6(1),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimizing performance measures for feature selection", "author": ["Q. Mao", "I.W. Tsang"], "venue": "ICDM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimizing f-measure with support vector machines", "author": ["D.R. Musicant", "V. Kumar", "A. Ozgur"], "venue": "Proceedings of the 16th International Florida Artificial Intelligence Research Society Conference,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Cutting-set methods for robust convex optimization with pessimizing oracles", "author": ["A. Mutapcic", "S. Boyd"], "venue": "Optimization Methods & Software, 24(3):381406,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature selection, l1 vs", "author": ["A.Y. Ng"], "venue": "l2 regularization, and rotational invariance. In ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F.R. Bach", "Y. Grandvalet", "S. Canu"], "venue": "JMLR, 3:1439\u20131461,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Scholk\u00f6pf"], "venue": "JMLR, 7,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Bundle methods for regularized risk minimization", "author": ["C.H. Teo", "S.V.N. Vishwanathan", "A. Smola", "Quoc V. Le"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altum"], "venue": "JMLR, 6:1453\u20131484,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank by optimizing ndcg measure", "author": ["H. Valizadengan", "R. Jin", "R. Zhang", "J. Mao"], "venue": "NIPS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Use of the zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Scholk\u00f6pf"], "venue": "JMLR, 3:1439\u20131461,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Z. Xu", "R. Jin", "I. King", "M.R. Lyu"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-monotonic feature selection", "author": ["Z. Xu", "R. Jin", "J. Ye", "Michael R. Lyu", "I. King"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "A comparison of optimization methods and software for large-scale l1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "JMLR, 11:3183\u20133234,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "SIGIR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Content-based image retrieval using multiple-instance learning", "author": ["Q. Zhang", "S.A. Goldman", "W. Yu", "J. Fritts"], "venue": "ICML,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["T. Zhang"], "venue": "JMLR, 11:1081\u20131107, Mar", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S.V.N. Vishwanathan"], "venue": "UAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "1-norm support vector machine", "author": ["J. Zhu", "S. Rossett", "T. Hastie", "R. Tibshirani"], "venue": "NIPS,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiclass multiple kernel learning", "author": ["A. Zien", "C.S. Ong"], "venue": "ICML,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "Hence, the error rate is considered as a poor criterion for the problems with highly skewed class distributions [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Instead of optimizing the error rate, Support Vector Machine for multivariate performance measures (SVM ) [11] was proposed to directly optimize the losses based on a variety of multivariate performance measures.", "startOffset": 106, "endOffset": 110}, {"referenceID": 36, "context": "A smoothing version of SVM [37] was proposed to accelerate the convergence of the optimization problem specially designed", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 12, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 27, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 20, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "To alleviate these issues, one can resort to embedded feature selection methods [15], which can be categorized into the following two major directions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 7, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 22, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 32, "context": "A thorough study to compare several recently developed l1regularized algorithms has been conducted in [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "To achieve a sparser solution, the Approximation of the zeRO norm Minimization (AROM) was proposed [30] to optimize l0 models.", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 35, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 17, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 239, "endOffset": 243}, {"referenceID": 8, "context": "Another way is to sort the weights of a SVM classifier and remove the smallest weights iteratively, which is known as SVM with Recursive Feature Elimination (SVM-RFE) [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 31, "context": "However, as discussed in [32], such nested \u201cmonotonic\u201d feature selection scheme leads to suboptimal performance.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "Non-monotonic feature selection (NMMKL) [32] has been proposed to solve this problem, but each feature corresponding to one kernel makes NMMKL infeasible for high-dimensional problems.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "[26] proposed Feature Generating Machine (FGM), which shows great scalability to non-monotonic feature selection on large-scale and very high-dimensional datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 29, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 31, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 25, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 10, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 91, "endOffset": 99}, {"referenceID": 12, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 91, "endOffset": 99}, {"referenceID": 25, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "This paper is an extension of our preliminary work [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 91, "endOffset": 94}, {"referenceID": 31, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 141, "endOffset": 145}, {"referenceID": 32, "context": "By comparing with these methods, the advantages of our proposed methods are summarized as follows: (1) The tradeoff parameter C in l1 SVM [33] is too sensitive to be tuned properly since it controls both margin loss and the sparsity of w.", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "This separation makes parameter tuning for our methods much easier than those of SKM [3] and l1 SVM.", "startOffset": 85, "endOffset": 88}, {"referenceID": 31, "context": "(2) NMMKL [32] uses the similar parameter separation strategy, but it is intractable for this method to handle high-dimensional datasets, let alone optimize multivariate losses.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "(3) FGM [26] is a special case of the propose framework when optimizing square hinge loss with indicator variables in integer domain.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "\u2022 Recall that Multiple-Instance Learning via Embedded instance Selection (MILES) [6], which transforms multiple instance learning (MIL) into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features, achieves state-of-the-art performance for multiple instance learning problems.", "startOffset": 81, "endOffset": 84}, {"referenceID": 27, "context": "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,", "startOffset": 16, "endOffset": 24}, {"referenceID": 10, "context": "As shown in [11], optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "Since domains D and A are nonempty, the function S(\u03b1,d) is closed and convex for all d \u2208 D given any \u03b1 \u2208 A, and the function S(\u03b1,d) is closed and concave for all \u03b1 \u2208 A given any d \u2208 D, the saddle-point property: mind\u2208D max\u03b1\u2208A S(\u03b1, d) = max\u03b1\u2208A mind\u2208D S(\u03b1, d) holds [4].", "startOffset": 264, "endOffset": 267}, {"referenceID": 13, "context": "Cutting plane algorithm [14] could be used here to solve this problem.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "As from [22], such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst-case analysis.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 23, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 30, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 26, "context": "Then we can apply the bundle method [27] to solve this primal problem.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Since Remp(w) is a convex function, its subgradient exists everywhere in its domain [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "Following the bundle method [27], the criterion for selecting the next point w is to solve the following problem,", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "The convergence proof in [27] does not apply in this case as the Fenchel dual of \u03a9(w) fails to satisfy the strong convexity assumption if K > 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "As K = 1, Algorithm 1 is exactly the bundle method [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "When K \u2265 2, we can adapt the proof of Theorem 5 in [13] for the following convergence results.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "We adapt the proof of Theorem 5 in [13], and sketch the necessary changes corresponding to Problem (29).", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "The Lemma 2 in [13] states that a line search starting at \u03b1 along an ascent direction \u03b7 with maximum step-size C > 0 improves the objective by at least max0\u2264\u03b2\u2264C { \u0398d(\u03b1+ \u03b2\u03b7) \u2212 \u0398d(\u03b1) } \u2265 12 min { C, \u2202\u0398d(\u03b1) T \u03b7 \u03b7T Qd\u03b7 } \u2202\u0398d(\u03b1) T \u03b7.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "We can see that it is a special case of [13] if T = 1.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "According to Theorem 5 in [13], for a newly added constraint \u0177 and some \u03b3d > 0, we can obtain \u2202\u0398d(\u03b1) \u03b7 = \u03b3d by setting the ascent direction \u03b7\u0177 = 1 for the newly added \u0177 and \u03b7y = \u2212 1 C\u03b1y for the others.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "Following the remaining derivation in [13], the overall bound results are obtained.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Remark 1: Problem (15) is similar to Support Kernel Machine (SKM) [3] in which the multiple Gaussian kernels are built on random subsets of features, with varying widths.", "startOffset": 66, "endOffset": 69}, {"referenceID": 25, "context": "In terms of the convergence proof of FGM in [26] and Theorem 1, we can obtain the following theorem to illustrate the approximation with an \u01eb-optimal solution to the original problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 33, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 26, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 10, "context": "In this paper, we only use several multivariate performance measures based on contingency table as the showcases and their finding y could be solved in time complexity O(n) [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "Then, Algorithm 2 in [11] can be directly applied.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 200, "endOffset": 203}, {"referenceID": 31, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 211, "endOffset": 215}, {"referenceID": 32, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 224, "endOffset": 228}, {"referenceID": 29, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 237, "endOffset": 241}, {"referenceID": 25, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 250, "endOffset": 254}, {"referenceID": 23, "context": "According to [24], we transform Problem (5) in the special case of B = 1 to the following equivalent optimization problem,", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "SKM [3] attempts to obtain the sparsity of w by penalizing the square of a weighted block l1-norm ( \u2211k j=1 \u03b3j ||wj ||2) where k is the number of groups and wj is the weight vector for the features in the jth group.", "startOffset": 4, "endOffset": 7}, {"referenceID": 32, "context": "Minimizing the square of the l1-norm is very similar to l1-norm SVM [33] by setting \u03a9(w) = ||w||1 with the non-negative (convex) loss function.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "This strategy is also used in NMMKL [32], but one feature corresponding to one base kernel makes NMMKL intractable for highdimensional problems.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "According to the work [40], we can reformulate Problem (20) as an equivalent optimization problem", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "It is not trivial to perform the extension of FGM to optimize multivariate loss because original FGM method [26] cannot directly apply to solve the exponential number of constraints.", "startOffset": 108, "endOffset": 112}, {"referenceID": 6, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 27, "endOffset": 30}, {"referenceID": 34, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Multi-instance learning was formally introduced in the context of drug activity prediction [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "By defining a similarity measure between a bag and an instance, Multiple-Instance Learning via Embedded instance Selection (MILES) [6] successfully transforms multiple instance learning into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features.", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "Following the notations in [6], we denote ith positive bags as Bi = {xi,j} n + i j=1 which consists of n + i instances x + i,j , j = 1, .", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.", "startOffset": 172, "endOffset": 175}, {"referenceID": 25, "context": "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "by bundle method [27].", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "For l1-SVM, we also use Liblinear software, which implements the state-of-the-art l1-SVM algorithm [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Furthermore, several specific measures on the contingency table are investigated on Text datasets by comparing with SVM [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "Removing inactive constraints from the working set [13] in the inner layer is employed for speedup the QCQP problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 4, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 9, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 9, "context": "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter \u03bb \u2208 [10, 10] due to the extremely high dimensionality.", "startOffset": 175, "endOffset": 183}, {"referenceID": 9, "context": "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter \u03bb \u2208 [10, 10] due to the extremely high dimensionality.", "startOffset": 175, "endOffset": 183}, {"referenceID": 26, "context": "after the maximum iteration, which is consistent with the discussion in Appendix C of [27] that bundle method with l1 regularizer cannot guarantee the convergence.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Since [11] has proven that SVMmulti with Hamming loss, namely\u2206Err(y, y) = 2(b+c), is the same as SVM.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "Both datasets are used in [26], and they are already split into training and testing sets.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "This is the same as [26].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 4, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 9, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 1, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 4, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 9, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 19, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 29, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 39, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 25, "context": "The elimination scheme of features for SVM-RFE method can be referred to [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "1 of [6] to construct a dense and high-dimensional dataset on a preprocessed image data .", "startOffset": 5, "endOffset": 8}, {"referenceID": 37, "context": "This dataset is used in [38] for multi-instance learning.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "Each image is represented as a bag of nine instances generated by the SBN method [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "After that, following [6], the natural scene image retrieval problem turns out to be a feature selection task to select relevant embedded instances for prediction.", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "Following [11], we use the same notation SVMmulti for different multivariate performance measures.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": ", 2] as in [11].", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Following the recipe of [5], the self-dual cone \u2016vt\u20162 \u2264 \u03b7t, \u2200t = 1, .", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "According to the property of self-dual cone [3], we can obtain the primal solution from its dual as wt = \u2212\u03bctvt = \u03bct \u2211 y \u03b1y\u2032a t y where \u03bcj is the dual variable of the j quadratic constraint such that \u2211m j=1 \u03bcj = 1, \u03bcj \u2208 R+, \u2200j = 1, .", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "This turns out to be the same problem of FGM [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "For self-completeness, we give the theorem as follows, Theorem 3 ([26]).", "startOffset": 66, "endOffset": 70}], "year": 2013, "abstractText": "Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a twolayer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-ofthe-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms l1-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM in terms of F1-score.", "creator": "LaTeX with hyperref package"}}}