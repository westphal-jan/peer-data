{"id": "1103.4601", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2011", "title": "Doubly Robust Policy Evaluation and Learning", "abstract": "We study decision-making in environments where reward is only partially observed, but can be modeled as a function of an action and an observed context. Known as contextual bandits, this setting includes a variety of applications, including health policy and Internet advertising. A central task is to evaluate a new policy against historical data that consists of contexts, actions, and rewards received. The central challenge is that past data typically does not accurately reflect the extent of action taken by a new policy. Previous approaches are based on either reward models or past policy models, the former suffering from a high bias, while the latter are highly diverse. We leverage the strength and overcome the weaknesses of the two approaches by adapting doubly robust assessment techniques to the problems of policy assessment and optimization. We demonstrate that this approach delivers unbiased (and often lower) valuations when we either achieve a good model of the rewards of existing policies or have a better empirical model of past practices.", "histories": [["v1", "Wed, 23 Mar 2011 19:37:45 GMT  (52kb)", "https://arxiv.org/abs/1103.4601v1", "8 pages and 6 figures"], ["v2", "Fri, 6 May 2011 02:38:18 GMT  (58kb)", "http://arxiv.org/abs/1103.4601v2", "Published at ICML 2011, 8 pages, 6 figures"]], "COMMENTS": "8 pages and 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.AP stat.ML", "authors": ["miroslav dud\u00edk", "john langford", "lihong li"], "accepted": true, "id": "1103.4601"}, "pdf": {"name": "1103.4601.pdf", "metadata": {"source": "CRF", "title": "Doubly Robust Policy Evaluation and Learning", "authors": ["Miroslav Dud\u0131\u0301k", "John Langford", "Lihong Li"], "emails": ["MDUDIK@YAHOO-INC.COM", "JL@YAHOO-INC.COM", "LIHONG@YAHOO-INC.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 3.\n46 01\nv2 [\ncs .L\nG ]\n6 M\nay 2\nIn this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice."}, {"heading": "1. Introduction", "text": "We study decision making in environments where we receive feedback only for chosen actions. For example, in Internet advertising, we find only whether a user clicked\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\non some of the presented ads, but receive no information about the ads that were not presented. In health care, we only find out success rates for patients who received the treatments, but not for the alternatives. Both of these problems are instances of contextual bandits (Auer et al., 2002; Langford & Zhang, 2008). The context refers to additional information about the user or patient. Here, we focus on the offline version: we assume access to historic data, but no ability to gather new data (Langford et al., 2008; Strehl et al., 2011).\nTwo kinds of approaches address offline learning in contextual bandits. The first, which we call the direct method (DM), estimates the reward function from given data and uses this estimate in place of actual reward to evaluate the policy value on a set of contexts. The second kind, called inverse propensity score (IPS) (Horvitz & Thompson, 1952), uses importance weighting to correct for the incorrect proportions of actions in the historic data. The first approach requires an accurate model of rewards, whereas the second approach requires an accurate model of the past policy. In general, it might be difficult to accurately model rewards, so the first assumption can be too restrictive. On the other hand, it is usually possible to model the past policy quite well. However, the second kind of approach often suffers from large variance especially when the past policy differs significantly from the policy being evaluated.\nIn this paper, we propose to use the technique of doubly robust (DR) estimation to overcome problems with the two existing approaches. Doubly robust (or doubly protected) estimation (Cassel et al., 1976; Robins et al., 1994; Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) is a statistical approach for estimation from incomplete data with an important property: if either one of the two estimators (in DM and IPS) is correct, then the estimation is unbiased. This method thus increases the chances of drawing reliable inference.\nFor example, when conducting a survey, seemingly ancillary questions such as age, sex, and family income may be asked. Since not everyone contacted responds to the sur-\nvey, these values along with census statistics may be used to form an estimator of the probability of a response conditioned on age, sex, and family income. Using importance weighting inverse to these estimated probabilities, one estimator of overall opinions can be formed. An alternative estimator can be formed by directly regressing to predict the survey outcome given any available sources of information. Doubly robust estimation unifies these two techniques, so that unbiasedness is guaranteed if either the probability estimate is accurate or the regressed predictor is accurate.\nWe apply the doubly robust technique to policy value estimation in a contextual bandit setting. The core technique is analyzed in terms of bias in Section 3 and variance in Section 4. Unlike previous theoretical analyses, we do not assume that either the reward model or the past policy model are correct. Instead, we show how the deviations of the two models from the truth impact bias and variance of the doubly robust estimator. To our knowledge, this style of analysis is novel and may provide insights into doubly robust estimation beyond the specific setting studied here. In Section 5, we apply this method to both policy evaluation and optimization, finding that this approach substantially sharpens existing techniques."}, {"heading": "1.1. Prior Work", "text": "Doubly robust estimation is widely used in statistical inference (see, e.g., Kang & Schafer (2007) and the references therein). More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)). Our analysis is non-asymptotic and makes no such assumptions.\nSeveral other papers in machine learning have used ideas related to the basic technique discussed here, although not with the same language. For benign bandits, Hazan & Kale (2009) construct algorithms which use reward estimators in order to achieve a worst-case regret that depends on the variance of the bandit rather than time. Similarly, the Offset Tree algorithm (Beygelzimer & Langford, 2009) can be thought of as using a crude reward estimate for the \u201coffset\u201d. In both cases, the algorithms and estimators described here are substantially more sophisticated."}, {"heading": "2. Problem Definition and Approach", "text": "Let X be an input space and A = {1, . . . , k} a finite action space. A contextual bandit problem is specified by a distri-\nbution D over pairs (x,~r) where x \u2208 X is the context and ~r \u2208 [0, 1]A is a vector of rewards. The input data has been generated using some unknown policy (possibly adaptive and randomized) as follows:\n\u2022 The world draws a new example (x,~r) \u223c D. Only x is revealed. \u2022 The policy chooses an action a \u223c p(a | x, h), where h is the history of previous observations (that is, the concatenation of all preceding contexts, actions and observed rewards). \u2022 Reward ra is revealed. It should be emphasized that other rewards ra\u2032 with a\u2032 6= a are not observed.\nNote that neither the distribution D nor the policy p is known. Given a data set S = {(x, h, a, ra)} collected as above, we are interested in two tasks: policy evaluation and policy optimization. In policy evaluation, we are interested in estimating the value of a stationary policy \u03c0, defined as:\nV \u03c0 = E(x,~r)\u223cD[r\u03c0(x)] .\nOn the other hand, the goal of policy optimization is to find an optimal policy with maximum value: \u03c0\u2217 = argmax\u03c0 V\n\u03c0. In the theoretical sections of the paper, we treat the problem of policy evaluation. It is expected that better evaluation generally leads to better optimization (Strehl et al., 2011). In the experimental section, we study how our policy evaluation approach can be used for policy optimization in a classification setting."}, {"heading": "2.1. Existing Approaches", "text": "The key challenge in estimating policy value, given the data as described in the previous section, is the fact that we only have partial information about the reward, hence we cannot directly simulate our proposed policy on the data set S. There are two common solutions for overcoming this limitation. The first, called direct method (DM), forms an estimate \u02c6\u033aa(x) of the expected reward conditioned on the context and action. The policy value is then estimated by\nV\u0302 \u03c0DM = 1\n|S|\n\u2211\nx\u2208S\n\u02c6\u033a\u03c0(x)(x) .\nClearly, if \u02c6\u033aa(x) is a good approximation of the true expected reward, defined as \u033aa(x) = E(x,~r)\u223cD[ra | x], then the DM estimate is close to V \u03c0. Also, if \u02c6\u033a is unbiased, V\u0302 \u03c0DM is an unbiased estimate of V\n\u03c0. A problem with this method is that the estimate \u02c6\u033a is formed without the knowledge of \u03c0 and hence might focus on approximating \u033a mainly in the areas that are irrelevant for V \u03c0 and not sufficiently in the areas that are important for V \u03c0; see Beygelzimer & Langford (2009) for a more refined analysis.\nThe second approach, called inverse propensity score (IPS), is typically less prone to problems with bias. Instead of\napproximating the reward, IPS forms an approximation p\u0302(a | x, h) of p(a | x, h), and uses this estimate to correct for the shift in action proportions between the old, datacollection policy and the new policy:\nV\u0302 \u03c0IPS = 1\n|S|\n\u2211\n(x,h,a,ra)\u2208S\nraI(\u03c0(x) = a)\np\u0302(a | x, h)\nwhere I(\u00b7) is an indicator function evaluating to one if its argument is true and zero otherwise. If p\u0302(a | x, h) \u2248 p(a | x, h) then the IPS estimate above will be, approximately, an unbiased estimate of V \u03c0. Since we typically have a good (or even accurate) understanding of the datacollection policy, it is often easier to obtain a good estimate p\u0302, and thus IPS estimator is in practice less susceptible to problems with bias compared with the direct method. However, IPS typically has a much larger variance, due to the range of the random variable increasing. The issue becomes more severe when p(a | x, h) gets smaller. Our approach alleviates the large variance problem of IPS by taking advantage of the estimate \u02c6\u033a used by the direct method."}, {"heading": "2.2. Doubly Robust Estimator", "text": "Doubly robust estimators take advantage of both the estimate of the expected reward \u02c6\u033aa(x) and the estimate of action probabilities p\u0302(a | x, h). Here, we use a DR estimator of the form first suggested by Cassel et al. (1976) for regression, but previously not studied for policy learning:\nV\u0302 \u03c0DR = 1\n|S|\n\u2211\n(x,h,a,ra)\u2208S\n[\n(ra \u2212 \u02c6\u033aa(x))I(\u03c0(x) = a)\np\u0302(a | x, h)\n+ \u02c6\u033a\u03c0(x)(x) ] . (1)\nInformally, the estimator uses \u02c6\u033a as a baseline and if there is data available, a correction is applied. We will see that our estimator is accurate if at least one of the estimators, \u02c6\u033a and p\u0302, is accurate, hence the name doubly robust.\nIn practice, it is rare to have an accurate estimation of either \u033a or p. Thus, a basic question is: How does this estimator perform as the estimates \u02c6\u033a and p\u0302 deviate from the truth? The following two sections are dedicated to bias and variance analysis, respectively, of the DR estimator."}, {"heading": "3. Bias Analysis", "text": "Let \u2206 denote the additive deviation of \u02c6\u033a from \u033a, and \u03b4 a multiplicative deviation of p\u0302 from p:\n\u2206(a, x) = \u02c6\u033aa(x)\u2212 \u033aa(x),\n\u03b4(a, x, h) = 1\u2212 p(a | x, h)/p\u0302(a | x, h) .\nWe express the expected value of V\u0302 \u03c0DR using \u03b4(\u00b7, \u00b7, \u00b7) and \u2206(\u00b7, \u00b7). To remove clutter, we introduce shorthands \u033aa for\n\u033aa(x), \u02c6\u033aa for \u02c6\u033aa(x), I for I(\u03c0(x) = a), p for p(\u03c0(x) | x, h), p\u0302 for p\u0302(\u03c0(x) | x, h), \u2206 for \u2206(\u03c0(x), x)), and \u03b4 for \u03b4(\u03c0(x), x, h). In our analysis, we assume that the estimates p\u0302 and \u02c6\u033a are fixed independently of S (e.g., by splitting the original data set into S and a separate portion for estimating p\u0302 and \u02c6\u033a). To evaluate E[V\u0302 \u03c0DR], it suffices to focus on a single term in Eq. (1), conditioning on h:\nE(x,~r)\u223cD,a\u223cp(\u00b7|x,h)\n[\n(ra \u2212 \u02c6\u033aa)I\np\u0302 + \u02c6\u033a\u03c0(x)\n]\n= Ex,~r,a|h\n[\n(ra \u2212 \u033aa \u2212\u2206)I\np\u0302 + \u033a\u03c0(x) +\u2206\n]\n= Ex,a|h\n[\n(\u033aa \u2212 \u033aa)I\np\u0302 +\u2206\n( 1\u2212 I/p\u0302 )\n]\n+Ex[\u033a\u03c0(x)]\n= Ex|h [ \u2206 ( 1\u2212 p/p\u0302 )] + V \u03c0 = Ex|h[\u2206\u03b4] + V \u03c0 .\n(2)\nEven though x is independent of h, the conditioning on h remains in the last line, because \u03b4, p and p\u0302 are functions of h. Summing across all terms in Eq. (1), we obtain the following theorem:\nTheorem 1 Let \u2206 and \u03b4 be defined as above. Then, the bias of the doubly robust estimator is\n|ES [V\u0302 \u03c0 DR]\u2212 V \u03c0| =\n1\n|S|\n\u2223 \u2223 \u2223 ES [ \u2211\n(x,h)\u2208S\n\u2206\u03b4 ]\u2223 \u2223 \u2223 ."}, {"heading": "If the past policy and the past policy estimate are stationary", "text": "(i.e., independent of h), the expression simplifies to\n|E[V\u0302 \u03c0DR]\u2212 V \u03c0| = |Ex[\u2206\u03b4]| .\nIn contrast (for simplicity we assume stationarity):\n|E[V\u0302 \u03c0DM]\u2212 V \u03c0| = |Ex[\u2206]|\n|E[V\u0302 \u03c0IPS]\u2212 V \u03c0| = |Ex[\u033a\u03c0(x)\u03b4]| ,\nwhere the second equality is based on the observation that IPS is a special case of DR for \u02c6\u033aa(x) \u2261 0.\nIn general, neither of the estimators dominates the others. However, if either \u2206 \u2248 0, or \u03b4 \u2248 0, the expected value of the doubly robust estimator will be close to the true value, whereas DM requires \u2206 \u2248 0 and IPS requires \u03b4 \u2248 0. Also, if \u2206 \u2248 0 and \u03b4 \u226a 1, DR will still outperform DM, and similarly for IPS with roles of \u2206 and \u03b4 reversed. Thus, DR can effectively take advantage of both sources of information for better estimation."}, {"heading": "4. Variance Analysis", "text": "In the previous section, we argued that the expected value of V\u0302 \u03c0DR compares favorably with IPS and DM. In this section, we look at the variance of DR. Since large deviation\nbounds have a primary dependence on variance, a lower variance implies a faster convergence rate. We treat only the case with stationary past policy, and hence drop the dependence on h throughout.\nAs in the previous section, it suffices to analyze the second moment (and then variance) of a single term of Eq. (1). We use a similar decomposition as in Eq. (2). To simplify derivation we use the notation \u03b5 = (ra\u2212\u033aa)I/p\u0302. Note that, conditioned on x and a, the expectation of \u03b5 is zero. Hence, we can write the second moment as\nEx,~r,a\n[(\n(ra \u2212 \u02c6\u033aa)I\np\u0302 + \u02c6\u033a\u03c0(x)\n)2]\n= Ex,~r,a[\u03b5 2] +Ex[\u033a 2 \u03c0(x)] + 2Ex,a\n[ \u033a\u03c0(x)\u2206 ( 1\u2212 I/p\u0302 )]\n+Ex,a [ \u22062 ( 1\u2212 I/p\u0302 )2]\n= Ex,~r,a[\u03b5 2] +Ex[\u033a 2 \u03c0(x)] + 2Ex\n[ \u033a\u03c0(x)\u2206\u03b4 ]\n+Ex [ \u22062 ( 1\u2212 2p/p\u0302+ p/p\u03022 )]\n= Ex,~r,a[\u03b5 2] +Ex[\u033a 2 \u03c0(x)] + 2Ex\n[ \u033a\u03c0(x)\u2206\u03b4 ]\n+Ex [ \u22062 ( 1\u2212 2p/p\u0302+ p2/p\u03022 + p(1\u2212 p)/p\u03022 )]\n= Ex,~r,a[\u03b5 2] +Ex\n[( \u033a\u03c0(x) +\u2206\u03b4 )2]\n+Ex [ \u22062 \u00b7 p(1\u2212 p)/p\u03022 ]\n= Ex,~r,a[\u03b5 2] +Ex\n[( \u033a\u03c0(x) +\u2206\u03b4 )2]\n+Ex\n[\n1\u2212 p\np \u00b7\u22062(1\u2212 \u03b4)2\n]\n.\nSumming across all terms in Eq. (1) and combining with Theorem 1, we obtain the variance:\nTheorem 2 Let \u2206, \u03b4 and \u03b5 be defined as above. If the past policy and the policy estimate are stationary, then the variance of the doubly robust estimator is\nVar [ V\u0302 \u03c0DR ]\n= 1\n|S|\n(\nEx,~r,a[\u03b5 2] +Varx\n[ \u033a\u03c0(x) +\u2206\u03b4 ]\n+Ex\n[\n1\u2212 p\np \u00b7\u22062(1 \u2212 \u03b4)2\n])\n.\nThus, the variance can be decomposed into three terms. The first accounts for randomness in rewards. The second term is the variance of the estimator due to the randomness in x. And the last term can be viewed as the importance weighting penalty. A similar expression can be derived for the IPS estimator:\nVar [ V\u0302 \u03c0IPS ]\n= 1\n|S|\n(\nEx,~r,a[\u03b5 2] +Varx\n[ \u033a\u03c0(x) \u2212 \u033a\u03c0(x)\u03b4 ]\n+Ex\n[\n1\u2212 p\np \u00b7 \u033a2\u03c0(x)(1 \u2212 \u03b4) 2\n])\n.\nThe first term is identical, the second term will be of similar magnitude as the corresponding term of the DR estimator, provided that \u03b4 \u2248 0. However, the third term can be much larger for IPS if p(\u03c0(x) | x) \u226a 1 and |\u2206| is smaller than \u033a\u03c0(x). In contrast, for the direct method, we obtain\nVar [ V\u0302 \u03c0DM ]\n= 1\n|S| Varx\n[ \u033a\u03c0(x) +\u2206 ] .\nThus, the variance of the direct method does not have terms depending either on the past policy or the randomness in the rewards. This fact usually suffices to ensure that it is significantly lower than the variance of DR or IPS. However, as we mention in the previous section, the bias of the direct method is typically much larger, leading to larger errors in estimating policy value."}, {"heading": "5. Experiments", "text": "This section provides empirical evidence for the effectiveness of the DR estimator compared to IPS and DM. We consider two classes of problems: multiclass classification with bandit feedback in public benchmark datasets and estimation of average user visits to an Internet portal."}, {"heading": "5.1. Multiclass Classification with Bandit Feedback", "text": "We begin with a description of how to turn a k-class classification task into a k-armed contextual bandit problem. This transformation allows us to compare IPS and DR using public datasets for both policy evaluation and learning."}, {"heading": "5.1.1. DATA SETUP", "text": "In a classification task, we assume data are drawn IID from a fixed distribution: (x, c) \u223c D, where x \u2208 X is the feature vector and c \u2208 {1, 2, . . . , k} is the class label. A typical goal is to find a classifier \u03c0 : X 7\u2192 {1, 2, . . . , k} minimizing the classification error: e(\u03c0) = E(x,c)\u223cD [I(\u03c0(x) 6= c)] .\nAlternatively, we may turn the data point (x, c) into a costsensitive classification example (x, l1, l2, . . . , lk), where la = I(a 6= c) is the loss for predicting a. Then, a classifier \u03c0 may be interpreted as an action-selection policy, and its classification error is exactly the policy\u2019s expected loss.1\nTo construct a partially labeled dataset, exactly one loss component for each example is observed, following the approach of Beygelzimer & Langford (2009). Specifically, given any (x, l1, l2, . . . , lk), we randomly select a label a \u223c UNIF(1, 2, . . . , k), and then only reveal the component la. The final data are thus in the form of (x, a, la),\n1When considering classification problems, it is more natural to talk about minimizing classification errors. This loss minimization problem is symmetric to the reward maximization problem defined in Section 2.\nwhich is the form of data defined in Section 2. Furthermore, p(a | x) \u2261 1/k and is assumed to be known.\nTable 1 summarizes the benchmark problems adopted from the UCI repository (Asuncion & Newman, 2007)."}, {"heading": "5.1.2. POLICY EVALUATION", "text": "Here, we investigate whether the DR technique indeed gives more accurate estimates of the policy value (or classification error in our context). For each dataset:\n1. We randomly split data into training and test sets of (roughly) the same size; 2. On the training set with fully revealed losses, we run a direct loss minimization (DLM) algorithm of McAllester et al. (2011) to obtain a classifier (see Appendix A for details). This classifier constitutes the policy \u03c0 which we evaluate on test data; 3. We compute the classification error on fully observed test data. This error is treated as the ground truth for comparing various estimates; 4. Finally, we apply the transformation in Section 5.1.1 to the test data to obtain a partially labeled set, from which DM, IPS, and DR estimates are computed.\nBoth DM and DR require estimating the expected conditional loss denoted as l(x, a) for given (x, a). We use a linear loss model: l\u0302(x, a) = wa \u00b7 x, parameterized by k weight vectors {wa}a\u2208{1,...,k}, and use least-squares ridge regression to fit wa based on the training set. Step 4 is repeated 500 times, and the resulting bias and rmse (root mean squared error) are reported in Fig. 1.\nAs predicted by analysis, both IPS and DR are unbiased, since the probability estimate 1/k is accurate. In contrast, the linear loss model fails to capture the classification error accurately, and as a result, DM suffers a much larger bias.\nWhile IPS and DR estimators are unbiased, it is apparent from the rmse plot that the DR estimator enjoys a lower variance. As we shall see next, such an effect is substantial when it comes to policy optimization."}, {"heading": "5.1.3. POLICY OPTIMIZATION", "text": "We now consider policy optimization (classifier learning). Since DM is significantly worse on all datasets, as indicated in Fig. 1, we focus on the comparison between IPS and DR.\nHere, we apply the data transformation in Section 5.1.1 to\nthe training data, and then learn a classifier based on the loss estimated by IPS and DR, respectively. Specifically, for each dataset, we repeat the following steps 30 times:\n1. We randomly split data into training (70%) and test (30%) sets; 2. We apply the transformation in Section 5.1.1 to the training data to obtain a partially labeled set; 3. We then use the IPS and DR estimators to impute unrevealed losses in the training data; 4. Two cost-sensitive multiclass classification algorithms are used to learn a classifier from the losses completed by either IPS or DR: the first is DLM (McAllester et al., 2011), the other is the Filter Tree reduction of Beygelzimer et al. (2008) applied to a decision tree (see Appendix B for more details);\n5. Finally, we evaluate the learned classifiers on the test data to obtain classification error.\nAgain, we use least-squares ridge regression to build a linear loss estimator: l\u0302(x, a) = wa \u00b7 x. However, since the training data is partially labeled, wa is fitted only using training data (x, a\u2032, la\u2032) for which a = a\u2032.\nAverage classification errors (obtained in Step 5 above) of the 30 runs are plotted in Fig. 2. Clearly, for policy optimization, the advantage of the DR is even greater than for policy evaluation. In all datasets, DR provides substantially more reliable loss estimates than IPS, and results in significantly improved classifiers.\nFig. 2 also includes classification error of the Offset Tree reduction, which is designed specifically for policy optimization with partially labeled data.2 While the IPS versions of DLM and Filter Tree are rather weak, the DR versions are competitive with Offset Tree in all datasets, and in some cases significantly outperform Offset Tree.\nFinally, we note DR provided similar improvements to two very different algorithms, one based on gradient descent, the other based on tree induction. It suggests the generality of DR when combined with different algorithmic choices."}, {"heading": "5.2. Estimating Average User Visits", "text": "The next problem we consider is estimating the average number of user visits to a popular Internet portal. Real user visits to the website were recorded for about 4 mil-\n2We used decision trees as the base learner in Offset Trees. The numbers reported here are not identical to those by Beygelzimer & Langford (2009) probably because the filter-tree structures in our implementation were different.\nlion bcookies3 randomly selected from all bcookies during March 2010. Each bcookie is associated with a sparse binary feature vector of size around 5000. These features describe browsing behavior as well as other information (such as age, gender, and geographical location) of the bcookie. We chose a fixed time window in March 2010 and calculated the number of visits by each selected bcookie during this window. To summarize, the dataset contains N = 3854689 data: D = {(bi, xi, vi)}i=1,...,N , where bi is the i-th (unique) bcookie, xi is the corresponding binary feature vector, and vi is the number of visits.\nIf we can sample from D uniformly at random, the sample mean of vi will be an unbiased estimate of the true average number of user visits, which is 23.8 in this problem. However, in various situations, it may be difficult or impossible to ensure a uniform sampling scheme due to practical constraints, thus the sample mean may not reflect the true quantity of interest. This is known as covariate shift, a special case of our problem formulated in Section 2 with k = 2 arms. Formally, the partially labeled data consists of tuples (xi, ai, ri), where ai \u2208 {0, 1} indicates whether bcookie bi is sampled, ri = aivi is the observed number of visits, and pi is the probability that ai = 1. The goal here is to evaluate the value of a constant policy: \u03c0(x) \u2261 1.\nTo define the sampling probabilities pi, we adopted a similar approach as in Gretton et al. (2008). In particular, we obtained the first principal component (denoted x\u0304) of all features {xi}, and projected all data onto x\u0304. Let N be a univariate normal distribution with mean m+ (m\u0304\u2212m)/3\n3A bcookie is unique string that identifies a user. Strictly speaking, one user may correspond to multiple bcookies, but it suffices to equate a bcookie with a user for our purposes here.\nand standard deviation (m\u0304 \u2212m)/4, where m and m\u0304 were the minimum and mean of the projected values. Then, pi = min{N (xi \u00b7 x\u0304), 1} was the sampling probability of the i-th bcookie, bi.\nTo control data size, we randomly subsampled a fraction f \u2208 {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05} from the entire dataset D. For each bcookie bi in this subsample, set ai = 1 with probability pi, and ai = 0 otherwise. We then calculated the IPS and DR estimates on this subsample. The whole process was repeated 100 times.\nThe DR estimator required building a reward model \u02c6\u033a(x), which, given feature x, predicted the average number of visits. Again, least-squares ridge regression was used to fit a linear model \u02c6\u033a(x) = w \u00b7 x from sampled data.\nFig. 3 summarizes the estimation error of the two methods with increasing data size. For both IPS and DR, the estimation error goes down with more data. In terms of rmse,\nthe DR estimator is consistently better than IPS, especially when dataset size is smaller. The DR estimator often reduces the rmse by a fraction between 10% and 20%, and on average by 13.6%. By comparing to the bias and std metrics, it is clear that DR\u2019s gain of accuracy came from a lower variance, which accelerated convergence of the estimator to the true value. These results confirm our analysis that DR tends to reduce variance provided that a reasonable reward estimator is available."}, {"heading": "6. Conclusions", "text": "Doubly robust policy estimation is an effective technique which virtually always improves on the widely used inverse propensity score method. Our analysis shows that doubly robust methods tend to give more reliable and accurate estimates. The theory is corroborated by experiments on both benchmark data and a large-scale, real-world problem.\nIn the future, we expect the DR technique to become common practice in improving contextual bandit algorithms. As an example, it is interesting to develop a variant of Offset Tree that can take advantage of better reward models, rather than a crude, constant reward estimate (Beygelzimer & Langford, 2009)."}, {"heading": "Acknowledgements", "text": "We thank Deepak Agarwal for first bringing the doubly robust technique to our attention."}, {"heading": "A. Direct Loss Minimization", "text": "Given cost-sensitive multiclass classification data {(x, l1, . . . , lk)}, we perform approximate gradient descent on the policy loss (or classification error). In the experiments of Section 5.1, policy \u03c0 is specified by k weight vectors \u03b81, . . . , \u03b8k. Given x \u2208 X , the policy predicts as follows: \u03c0(x) = argmaxa\u2208{1,...,k}{x \u00b7 \u03b8a}.\nTo optimize \u03b8a, we adapt the \u201ctowards-better\u201d version of the direct loss minimization method of McAllester et al. (2011) as follows: given any data (x, l1, . . . , lk) and the current weights \u03b8a, the weights are adjusted by \u03b8a1 \u2190 \u03b8a1 + \u03b7x, \u03b8a2 \u2190 \u03b8a2 \u2212 \u03b7x where a1 = argmaxa {x \u00b7 \u03b8a \u2212 \u01ebla}, a2 = argmaxa {x \u00b7 \u03b8a}, \u03b7 \u2208 (0, 1) is a decaying learning rate, and \u01eb > 0 is an input parameter.\nFor computational reasons, we actually performed batched updates rather than incremental updatess. We found that the learning rate \u03b7 = t\u22120.3/2, where t is the batched iteration, worked well across all datasets. The parameter \u01eb was fixed to 0.1 for all datasets. Updates continued until the weights converged.\nFurthermore, since the policy loss is not convex in the weight vectors, we repeated the algorithm 20 times with randomly perturbed starting weights and then returned the best run\u2019s weight according to the learned policy\u2019s loss in the training data. We also tried using a holdout validation set for choosing the best weights out of the 20 candidates, but did not observe benefits from doing so."}, {"heading": "B. Filter Tree", "text": "The Filter Tree (Beygelzimer et al., 2008) is a reduction from cost-sensitive classification to binary classification. Its input is of the same form as for Direct Loss Minimization, but its output is a binary-tree based predictor where each node of the Filter Tree uses a binary classifier\u2014in this case the J48 decision tree implemented in Weka 3.6.4 (Hall et al., 2009). Thus, there are 2-class decision trees in the nodes, with the nodes arranged as per a Filter Tree. Training in a Filter Tree proceeds bottom-up, with each trained node filtering the examples observed by its parent until the entire tree is trained.\nTesting proceeds root-to-leaf, implying that the test time computation is logarithmic in the number of classes. We did not test the all-pairs Filter Tree, which has test time computation linear in the class count similar to DLM."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The offset tree for learning with partial labels", "author": ["A. Beygelzimer", "J. Langford"], "venue": "In KDD, pp", "citeRegEx": "Beygelzimer and Langford,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer and Langford", "year": 2009}, {"title": "Multiclass classification with filter-trees", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": "Unpublished technical report: http:// www.stat.berkeley.edu/\u223cpradeepr/paperz/filter-tree.pdf,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2008}, {"title": "Some results on generalized difference estimation and generalized regression estimation for finite populations", "author": ["C.M. Cassel", "C.E. S\u00e4rndal", "J.H. Wretman"], "venue": null, "citeRegEx": "Cassel et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Cassel et al\\.", "year": 1976}, {"title": "Evaluating online ad campaigns in a pipeline: causal models at scale", "author": ["D. Chan", "R. Ge", "O. Gershony", "T. Hesterberg", "D. Lambert"], "venue": "In KDD,", "citeRegEx": "Chan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2010}, {"title": "Dataset shift in machine learning. In Covariate Shift and Local Learning by Distribution Matching, pp. 131\u2013160", "author": ["A. Gretton", "A.J. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Better algorithms for benign bandits", "author": ["E. Hazan", "S. Kale"], "venue": "In SODA, pp", "citeRegEx": "Hazan and Kale,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2009}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Horvitz and Thompson,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson", "year": 1952}, {"title": "Demystifying double robustness: a comparison of alternative strategies for estimating a population mean from incomplete data", "author": ["J.D.Y. Kang", "J.L. Schafer"], "venue": "Statist. Sci.,", "citeRegEx": "Kang and Schafer,? \\Q2007\\E", "shortCiteRegEx": "Kang and Schafer", "year": 2007}, {"title": "More bang for their bucks: assessing new features for online advertisers", "author": ["D. Lambert", "D. Pregibon"], "venue": "In ADKDD,", "citeRegEx": "Lambert and Pregibon,? \\Q2007\\E", "shortCiteRegEx": "Lambert and Pregibon", "year": 2007}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS, pp", "citeRegEx": "Langford and Zhang,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2008}, {"title": "Exploration scavenging", "author": ["J. Langford", "A.L. Strehl", "J. Wortman"], "venue": "In ICML, pp", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Stratification and weighting via the propensity score in estimation of causal treatment effects: A comparative study", "author": ["J.K. Lunceford", "M. Davidian"], "venue": "Statistics in Medicine,", "citeRegEx": "Lunceford and Davidian,? \\Q2004\\E", "shortCiteRegEx": "Lunceford and Davidian", "year": 2004}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In NIPS, pp", "citeRegEx": "McAllester et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2011}, {"title": "Semiparametric efficiency in multivariate regression models with missing data", "author": ["J. Robins", "A. Rotnitzky"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Robins and Rotnitzky,? \\Q1995\\E", "shortCiteRegEx": "Robins and Rotnitzky", "year": 1995}, {"title": "Estimation of regression coefficients when some regressors are not always observed", "author": ["J.M. Robins", "A. Rotnitzky", "L.P. Zhao"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Robins et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Robins et al\\.", "year": 1994}, {"title": "Learning from logged implicit exploration data", "author": ["A. Strehl", "J. Langford", "L. Li", "S. Kakade"], "venue": "In NIPS, pp", "citeRegEx": "Strehl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Both of these problems are instances of contextual bandits (Auer et al., 2002; Langford & Zhang, 2008).", "startOffset": 59, "endOffset": 102}, {"referenceID": 12, "context": "Here, we focus on the offline version: we assume access to historic data, but no ability to gather new data (Langford et al., 2008; Strehl et al., 2011).", "startOffset": 108, "endOffset": 152}, {"referenceID": 17, "context": "Here, we focus on the offline version: we assume access to historic data, but no ability to gather new data (Langford et al., 2008; Strehl et al., 2011).", "startOffset": 108, "endOffset": 152}, {"referenceID": 3, "context": "Doubly robust (or doubly protected) estimation (Cassel et al., 1976; Robins et al., 1994; Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) is a statistical approach for estimation from incomplete data with an important property: if either one of the two estimators (in DM and IPS) is correct, then the estimation is unbiased.", "startOffset": 47, "endOffset": 165}, {"referenceID": 16, "context": "Doubly robust (or doubly protected) estimation (Cassel et al., 1976; Robins et al., 1994; Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) is a statistical approach for estimation from incomplete data with an important property: if either one of the two estimators (in DM and IPS) is correct, then the estimation is unbiased.", "startOffset": 47, "endOffset": 165}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010).", "startOffset": 119, "endOffset": 164}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 440}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 469}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 496}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)). Our analysis is non-asymptotic and makes no such assumptions. Several other papers in machine learning have used ideas related to the basic technique discussed here, although not with the same language. For benign bandits, Hazan & Kale (2009) construct algorithms which use reward estimators in order to achieve a worst-case regret that depends on the variance of the bandit rather than time.", "startOffset": 146, "endOffset": 741}, {"referenceID": 17, "context": "It is expected that better evaluation generally leads to better optimization (Strehl et al., 2011).", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "Here, we use a DR estimator of the form first suggested by Cassel et al. (1976) for regression, but previously not studied for policy learning:", "startOffset": 59, "endOffset": 80}, {"referenceID": 14, "context": "On the training set with fully revealed losses, we run a direct loss minimization (DLM) algorithm of McAllester et al. (2011) to obtain a classifier (see Appendix A for details).", "startOffset": 101, "endOffset": 126}, {"referenceID": 14, "context": "Two cost-sensitive multiclass classification algorithms are used to learn a classifier from the losses completed by either IPS or DR: the first is DLM (McAllester et al., 2011), the other is the Filter Tree reduction of Beygelzimer et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 2, "context": ", 2011), the other is the Filter Tree reduction of Beygelzimer et al. (2008) applied to a decision tree (see Appendix B for more details);", "startOffset": 51, "endOffset": 77}, {"referenceID": 5, "context": "To define the sampling probabilities pi, we adopted a similar approach as in Gretton et al. (2008). In particular, we obtained the first principal component (denoted x\u0304) of all features {xi}, and projected all data onto x\u0304.", "startOffset": 77, "endOffset": 99}, {"referenceID": 14, "context": "To optimize \u03b8a, we adapt the \u201ctowards-better\u201d version of the direct loss minimization method of McAllester et al. (2011) as follows: given any data (x, l1, .", "startOffset": 96, "endOffset": 121}], "year": 2011, "abstractText": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. In this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice.", "creator": "gnuplot 4.2 patchlevel 2 "}}}