{"id": "1105.4318", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2011", "title": "Correction of Noisy Sentences using a Monolingual Corpus", "abstract": "The correction of loud natural language texts is an important and well-studied problem in the processing of natural language. It has a number of applications in areas such as statistical machine translation, second language learning and natural language generation. In this thesis we will consider some statistical techniques for text correction. We define the error classes that occur frequently in the text and describe algorithms for their correction. The data comes from a poorly trained machine translation system. The algorithms use only one language model in the target language to correct the sentences. We use phrase correction methods in both algorithms. The phrases are replaced and combined to give us the final corrected sentence. We also present the methods to model different types of errors, in addition to the results of the functioning of the algorithms on the test sentence. We show that one of the approaches does not achieve the desired goal, while the other succeeds well.", "histories": [["v1", "Sun, 22 May 2011 09:01:38 GMT  (99kb,D)", "http://arxiv.org/abs/1105.4318v1", "67 pages, 2 figures, 4 tables, 2 algorithms"]], "COMMENTS": "67 pages, 2 figures, 4 tables, 2 algorithms", "reviews": [], "SUBJECTS": "cs.DL cs.AI", "authors": ["diptesh chatterhee"], "accepted": false, "id": "1105.4318"}, "pdf": {"name": "1105.4318.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Arnab Sengupta", "Rahul Sarkar"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n43 18\nv1 [\ncs .D\nL ]\n2 2\nM ay\n2 01\n1\nCorrection of Noisy Sentences using a\nMonolingual Corpus\nThesis submitted in partial fulfillment\nof the requirements for the degree of\nMaster of Technology\nin\nComputer Science and Engineering\nby\nDiptesh Chatterjee\nRoll No. 06CS3031\nUnder the supervision of\nDr. Sudeshna Sarkar\nDepartment of Computer Science and Engineering\nIndian Institute of Technology, Kharagpur\nWest Bengal, India, 721302"}, {"heading": "CERTIFICATE", "text": "This is to certify that the thesis entitled \u201cCorrection of Noisy\nSentences using a Monolingual Corpus\u201d is being submitted by\nDiptesh Chatterjee (Roll No. 06CS3031) in partial fulfillment of the\nrequirement for the award of the degree of Master of Technology in\nComputer Science and Engineering at Indian Institute of Technology,\nKharagpur, India.\nIt is faithful record of work carried out by him at the Department of\nComputer Science and Engineering, IIT Kharagpur,under my\nguidance and supervision."}, {"heading": "Prof. Sudeshna Sarkar", "text": ""}, {"heading": "Department of Computer Science and Engineering", "text": "IIT Kharagpur"}, {"heading": "ACKNOWLEDGEMENT", "text": "I would like to express my sincere gratitude to Prof. Sudeshna Sarkar for\nbeing my advisor and for providing me the opportunity to conduct research\nin the field of my interest. I am indebted to her continual support and guid-\nance throughout my project work in the Department of Computer Science\nand Engineering at IIT Kharagpur.\nI am really grateful to my friends Arnab Sengupta, Rahul Sarkar, Koushik\nHembram and Ashish Jhunjhunwala, who encouraged me at every stage of\nmy project and for being there for me every time the chips were down. I also\nthank Mr. Sanjay Chatterjee for providing som data to test my algorithms\non.\nI would also like to avail this opportunity to express my sincere gratitude to\nmy parents and family members who have been an endless source of encour-\nagement for me.\nLast, but definitely not the least, I would like to thank the developers of C++, Python, Matlab and LATEX for providing me with all the tools required for a successful completion of the project.\nDiptesh Chatterjee"}, {"heading": "DECLARATION", "text": "I certify that,\n1. The work contained in the thesis is original and has been done by myself\nunder the general supervision of my supervisor.\n2. The work has not been submitted to any other Institute for any degree\nor diploma.\n3. I have followed the guidelines provided by the Institute in writing the\nthesis.\n4. I have conformed to the norms and guidelines given in the Ethical Code\nof Conduct of the Institute.\n5. Whenever I have used materials (data, theoretical analysis and text)\nfrom other sources, I have given due credit to them by citing them in\nthe text of the thesis and giving their details in the reference.\n6. Whenever I have quoted written materials from other sources, I have\nput them under quotation marks and given due credit to the sources\nby citing them and giving required details in the reference.\nDiptesh Chatterjee"}, {"heading": "Contents", "text": ""}, {"heading": "List of Figures iii", "text": ""}, {"heading": "List of Tables iv", "text": ""}, {"heading": "List of Algorithms v", "text": ""}, {"heading": "1 Introduction 1", "text": "1.1 Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Decoding in Machine Translation . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Previous Work and Problem Definition 7", "text": "2.1 Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 The Problem Statement . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.1 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.3.2 BLEU Score . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "3 Fixed Length Phrase-based Correction Model 19", "text": "3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2 Experiments and Results . . . . . . . . . . . . . . . . . . . . . 21\n3.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"}, {"heading": "4 The Dynamic Programming based method 24", "text": "4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\ni\n4.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.3 Computing the Phrase Substitutes . . . . . . . . . . . . . . . 29\n4.3.1 Levenshtein distance between individual words . . . . . 31\n4.3.2 Synset Distance in Wordnet . . . . . . . . . . . . . . . 33\n4.3.3 Word order based metrics . . . . . . . . . . . . . . . . 35\n4.4 A Representative Example . . . . . . . . . . . . . . . . . . . . 36\n4.5 Experiments and Results . . . . . . . . . . . . . . . . . . . . . 40\n4.6 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44"}, {"heading": "5 Conclusion 50", "text": "Bibliography 56"}, {"heading": "List of Figures", "text": "1.1 Noisy Channel Model of Statistical Machine Translation . . . 4\n4.1 An illustration of how the Dynamic Programming algorithm\nproceeds for N = 5 . . . . . . . . . . . . . . . . . . . . . . . . 28\niii"}, {"heading": "List of Tables", "text": "3.1 Results of the fixed length phrase-based correction algorithm . 22\n4.1 The Distance functions used for experimentation . . . . . . . . 41\n4.2 Summary of Results over all the used distance functions . . . 42\n4.3 Results of experiments with twice translated sentences taken\nfrom the FIRE corpus . . . . . . . . . . . . . . . . . . . . . . 43\niv"}, {"heading": "List of Algorithms", "text": "1 Algorithm involving fixed length phrases to compute best sub-\nstitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2 Dynamic Programming based correction algorithm . . . . . . . 27\nv"}, {"heading": "Abstract", "text": "Correction of Noisy Natural Language Text is an important and well studied\nproblem in Natural Language Processing. It has a number of applications\nin domains like Statistical Machine Translation, Second Language Learning\nand Natural Language Generation. In this work, we consider some statistical\ntechniques for Text Correction. We define the classes of errors commonly\nfound in text and describe algorithms to correct them. The data has been\ntaken from a poorly trained Machine Translation system. The algorithms use\nonly a language model in the target language in order to correct the sentences.\nWe use phrase based correction methods in both the algorithms. The phrases\nare replaced and combined to give us the final corrected sentence. We also\npresent the methods to model different kinds of errors, in addition to results\nof the working of the algorithms on the test set. We show that one of the\napproaches fail to achieve the desired goal, whereas the other succeeds well.\nIn the end, we analyze the possible reasons for such a trend in performance.\nChapter 1"}, {"heading": "Introduction", "text": "One of the major challenges in Natural Language Processing(NLP) is generation of correct Natural Language sentences. The degree of correctness is measured by how correct the sentence is syntactically and semantically. The branch of NLP dealing with this task is known as Natural Language Generation. However, Natural Language is like an infinite labyrinth of ambiguities, traveling through which does seem to be a rather monumental task. So far, we have no satisfactory computational model of the human brain - one that can explain how humans truly learn, how knowledge is stored and how is it that we can retrieve information extremely fast [1]. The algorithms and models used in Natural Language Processing are nothing but collection of mere approximations which perform well on a very select set of data. So, it is quite obvious that such imperfect models will not be able to generate sentences having the quality of actual human-composed sentences.\nThe main goal of Natural Language Processing is to allow the human user to interact with the computer using Natural Language. Such imperfections in sentence structure as mentioned previously totally defeat the purpose of NLP. However, due to the absence of actual models of the human brain, at this moment we can only try to improve the quality of the language generated somehow. Such Natural Language Generation is performed by a number of applications like Machine Translation, Question Answering, Dialog Systems,\n1\nSummarization systems etc. Finding a way to improve the quality of sentences generated by these applications can lead to a better user experience. Also, a large volume of data is generated everyday by non-native speakers of a language. If we have at our disposal methods to improve the quality of noisy sentences, these methods can be used as part of a standalone system in order to purify the data generated by the non-native speakers. That is the major motivation behind such corrective mechanisms.\nLanguage post-processing is performed as a part of a number of NLP tasks like Machine Translation, Question Answering etc. However, most of the work done is reliant upon the availability of resources in that particular language, which is not always the case. Here, we look into the problem of doing these corrections using only statistical techniques and using minimal language-specific resources. These methods, if successful, would be a great step towards solving the problem of noise-removal in resource-sparse languages."}, {"heading": "1.1 Language Modeling", "text": "Mathematically, a Language Model [2] is expressed as a probability distribution as follows: P (Wi|Wi\u22121Wi\u22122 . . .Wi\u2212n+1) where every Wj is a word in that language. Language models are computed over a corpus of a particular language.It is usually the case that the larger and more diverse the corpus, the language model computed is better. The definition above specifies a n-gram language model, where n is known as the order of the model. A language model is in fact, a Markov chain, because the probability that a word occurs is dependent on its preceding words. These probability values are used to score contiguous sequences of words. Given a sequence of M words S = s1, s2, . . . , sM , the score assigned to it by the language model is:\nPLM(S) = \u220f j P (sj|sj\u22121, sj\u22122, . . . , sj\u2212n+1)\n2\nAn important concept in language modeling is that of backoff [3]. Due to the limitations of the corpus used to compute the language model, it is possible that it might not capture all possible word n-grams. So, when a new n-gram is encountered, whose probability is not found in the language model, we compute the score using a backoff procedure, which is simply an approximation of the probability in a relaxed setting. For example, one way to backoff might be to try and compute the probability of the word given a smaller history, in effect, retracing to the (n\u2212 1)-gram and so on. For example: P (W |W1,W2, ....Wn) \u223c P (W |W1,W2, ....Wn\u22121) (if P (W |W1,W2, ....Wn) is not present)\nAnother method used to compute scores for unknown n-grams is known as smoothing [4]. In this method, we give the unknown n-grams a bit of the probability mass after stripping it off from the known n-grams. There are many techniques available for smoothing. The most popular ones are KneserNey discounting, Witten-Bell discounting, and Good-Turing discounting.\nLanguage model helps capture the fluency of a document, that is, it evaluates how closely a document resembles a piece of Natural Language text. One important thing to note here is that language models do not have anything to do with the semantics of the language. It does not explicitly tell us how correct the sentence is semantically or syntactically, only tells us how closely the sequence of words in the text conforms to the sequence of words in an actual piece of text in that language."}, {"heading": "1.2 Decoding in Machine Translation", "text": "Statistical Machine Translation (SMT) is an active field of research in NLP. The fundamental principle used to model SMT is the noisy channel model [5]. Let e be a sentence in the source language. Our aim is to compute the sentence f in the target language which is the best translation for e. That is, we wish to select the f which maximizes the probability P (e|f). The Noisy Channel Model assumes that the sentence e and f are essentially the same.\n3\nThe sentence f has been passed through a noisy channel, which has introduced some impurities in the sentence f and transformed it into e as shown in Figure 1.1.\nWe model the phenomenon using Baye\u2019s rule as follows:\nP (f |e) = argmaxf P (f)P (e|f)\nP (e) = argmaxfP (f)P (e|f) (1.1)\nThe denominator term P (e) is removed because it is constant. As we can see from Equation 1.1, the right hand side has two terms. They are defined as follows:\n1. P (f) is known as the Language Model\n2. P (e|f) is known as the Translation Model\nThe translation model is computed using a parallel corpus [6]. A parallel corpus contains aligned pairs of sentences in two languages. The sentences in one language are the translations of the sentences in the other language. A single monolingual corpus is used to compute the language model. The SMT system can be trained on this parallel corpus using techniques such as the IBM models [5] and Minimum Error Rate Training [7].\nThe next task at hand is,given the translation and language models, compute the translation of a sentence in the source language. Usually, SMT systems output the k-best list of translations, that is, the best k translations according to the translation and the language models. This process is known as\n4\ndecoding [5]. In phrase-based translation models [8], the decoding process involves the following steps:\n1. Break up the input sentences into a sequence of phrases. Each phrase\nconsists of a contiguous block of words.\n2. Find the best replacement for each of the phrases according to the\nphrase table of the translation model.\n3. Reorder the replaced phrases appropriately using the reordering model\nwhich is a part of the translation model.\nDifferent algorithms have been proposed for decoding. Some of the frequently used decoders are as follows:\n1. Greedy Decoder [9]: It starts with the gloss for words and improves the\nprobability by taking the currently cheapest path. However, it might get stuck in a local minima. To avoid this pitfall, it sometimes uses a 2-step look ahead\n2. Beam Search based decoder [10]: Such a decoder starts translating\nfrom left to right and maintains a list of partial hypotheses. When new hypotheses are added to the space, it prunes out the weakest hypotheses in order to accommodate the fixed beam size. But the major problem of such a decoding algorithm is the enormous size of the search space, which can be exponential in the number of hypotheses.\n3. Word Graph based Decoding [11]: Such a decoder uses a search graph\nover the phrases and uses hypothesis recombination to output the kbest list of top translations.\n4. Finite State Transducer Based Decoding: Such decoders model the\ntranslation model as a FST with input symbols being the phrases in the source language and output symbols being phrases in target language. The output of the transducer is taken as the translation of the sentence input.\n5\n5. String to Tree model [12]: This is also known as parsing based decod-\ning. It uses dynamic programming, similar in nature to chart parsing. The hypothesis space can be efficiently modeled as a forest structure. A recent work in this area is the use of Synchronous Context Free Grammars (SCFG) in parsing. The hierarchical phrase-based translation model uses such parsing methodology. It also uses a technique called Cube Pruning [13] to prune the extremely large hypothesis space. This method has a polynomial time complexity and produces extremely good results.\n6\nChapter 2"}, {"heading": "Previous Work and Problem", "text": ""}, {"heading": "Definition", "text": ""}, {"heading": "2.1 Literature Review", "text": "John Truscott (2007) [14] showed via a series of linguistic experiments that error correction increases a learner\u2019s ability to write accurately. The paper refutes previous research stating that the benefits, if any, are very little. He classified the corrective mechanisms in a number of groups and analyzed the factors that probably biased the previous research exdeavours. Hence, correction of noisy text is a problem that is of high practical significance, and, unsurprisingly, has received a lot of attention in the recent years. Out of these, correction of spelling errors is the problem that has probably been the most well studied. The most common approach to solve this problem is maintaining a dictionary of words and computing the replacement candidate by comparing the word to be replaced to each word in the dictionary. But, this would involve a lot of computation. Several methods were proposed to alleviate this problem. For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods. In these methods, the words in the dictionary are divided into classes according to some word features. The input word is compared to words in classes that have similar features only.\n7\nAnother approach to solving word errors in text is finite state automata based approach. Oflazer (1996) [18] suggested a method where all words in a dictionary are treated as a regular language over an alphabet of letters. All words are represented by a finite state machine atuomaton. For each misspelt word, an exhaustive traversal of the dictionary automaton is initiated using a variant of the Wagner-Fisher algorithm [19] to control the traversal of the dictionary. This method carefully traversals in the dictionary such that inspection of most dictionary states is avoided. Schultz and Mihov (2002) [20] present a variant of this approach where the dictionary is also represented as a finite state automaton. In this technique, a finite word acceptor is constructed foe each word. The acceptor accepts all words that are at a Levenshtein distance [21] k from the input word. The dictionary automaton and the Levenshtein automaton are then traversed in parallel to obtain the corrections for each word. Hasan et. al(2008) [22] propose an approach where they assume that the dictionary is represented as a deterministic finite state automaton. However, they completely avoid computing Levenshtein distance by the use of a Levenshtein transducer. The approach can adopt several constraints on which characters can substitute other characters. These constraints are computed from a phonetic and spatial confusion matrix of characters. Some more recent work include the work in Context Sensitive Spelling Correction, which tries to detect incorrect usage of valid words in a certain context. Using a predefined confusion set is a common approach in this task, for example Golding and Roth (1996) [23] and Mangu and Brill (1997) [24]. Opposite to non-word spelling correction, in this direction, only contextual information was taken into account for modeling by assuming all spelling similarities are equal.\nIn the context of Machine Translation, automatic postprocessing has been studied quite widely. But, such methods often target specific phenomena, like correcting English determiners (Knight and Chander, 1994) [25], merging German compounds (Stymne, 2009) [26], or applying word substitution (Elming, 2006) [27]. Stymne et. al (2010) [28] also proposed another method\n8\nin using the Swedish grammer checker called Granska to evaluate errors and postprocess Statistical Machine Translation output.\nReordering of Machine Translation output is another problem that has been studied quite extensively. Xia and McCord (2004) [29] describe a method for French, where reordering rules that operate on context-free productions are acquired automatically. Niessen and Ney (2004) [30] describe an approach for translation from German to English that combines verbs with associated particles, and also reorders questions. Collins et al. (2005) [31] also describe an approach for German for reordering of German clauses, which have quite different orders from English clauses. Wang et al. (2007) [32] describe a method of Chinese syntactic reordering for SMT systems. Once again, it is a rule based system which specifies reordering rules for different types of phrases in the language and also requires a lot of language specific resource. In fact, this factor is common for all the above mentioned methods. None of them is language independent.\nGamon et. al (2009) [33] present a system for identifying and correcting English as Second Language (ESL) errors. In this work, they identify the common errors made by native Chinese and Japanese but second language English speakers. They have a number of error specific modules and they are all run in parallel in order to detect errors and correct them. Apart from the standard training data, they have also used the web as a potential source of information. The web is used to offer corrections to text typed in by the user and enable the user to identify whether the text actually matches the user\u2019s intent. But the problem with this system is that it is quite time consuming and a person who knows very little of the language cannot possibly benefit from this approach. Also, it was designed specifically for errors made by native Chinese/Japansese speakers.\nSchierle et al. (2007) [34] take spelling correction one step further and use it for text cleaning. They combine the edit distance approach with pointwise mutual information giving the neighbourhood co-occurrence informa-\n9\ntion. They use domain specific replacement strategies and abbreviation lists to increase the correction precision compared to edit distance alone. But the problem with their method is the requirement of a large domain specific corpus, which is readily available for news domain, but not always available."}, {"heading": "2.2 The Problem Statement", "text": "As mentioned previously, correction of noisy Natural Language sentences has a number of applications. In this work, we try to do the same using a statistical framework. The advantage of using a statistical framework is the fact that we do not need too much of language-specific resources. As a result, we can use the same algorithm in order to solve the problem for different languages.\nThe problem we wish to solve is unsupervised correction of noisy Natural Language sentence. To this end, we are given a large monolingual corpus in the language which we are working with. That monolingual corpus can be used as an indicator to the manner in which sentences are constructed in that language. The exact formalism may vary, but we have considered a language model formalism to improve the fluency of the sentences. An important thing to note here is that sentences are characterized by two basic characteristics [35]:\n1. Fluency : Given by the perplexity of a set of sentences according to a\nlanguage model\n2. Faithfulness : Given by one or more metric like the BLEU score [36]\nIn this work, we do not concentrate on the BLEU score. Our aim is only to improve the fluency of the sentences using a language model constructed from the monolingual corpus. Given a noisy sentence and a monolingual corpus, we wish to output the sentence that has the best score according to the language model constructed from the corpus.However, an important thing that we take note of here is the fact that the semantics of the sentence\n10\nshould be kept unaltered as far as possible. For that, we follow a phrase replacement approach. We segment the sentence into a sequence of contiguous phrases and find the best replacements for the phrases that would result in the best score according to the language model. The problem can be mapped to a decoding problem. As previously mentioned, a decoder takes in a phrase translation table, a distortion probability distribution and a language model and a input sentence and after breaking up the sentence into phrases, computes the best substitution for it. Since we are working on a monolingual setting, our phrase translation table is derived from the language model itself. We do not use the distortion probability distribution. Our algorithm works like a decoder with only the language model.\nGiven a sentence, finding an optimal segmentation which is in accordance with the cognitive process of human beings is a difficult task. As previously mentioned, this task is an integral part of our problem definition. Several theories have been proposed for this task. However, we do not wish to create a formalism which would be dependent on any kind of external languagespecific resource. As a result, we do not follow any rigorous linguistic method of phrase segmentation. We define a phrase as a contiguous sequence of words in a sentence and proceed to replace individual phrases in accordance with this definition. The next step is combination of the replaced phrases so that the final sentence has the highest possible score according to the language model. We aim to achieve an output sentence which has a more natural word order than the original sentence, but we also aim to keep the meaning preserved as much as possible.\nIn order to find replacement candidates for phrases, we need to model the errors. Some of the commonly observed errors in written English are [37]:\n1. Singular/Plural Form\n2. Verb Tense\n3. Word choice\n11\n4. Subject/Verb Agreement\n5. Preposition\n6. Spelling errors\n7. Word form\n8. Redundancy\n9. Missing word\n10. SVO/SOV word orders for non-native speakers of language\nWe can broadly classify these errors into the following categories:\n1. Reordering based errors\n2. Substitution based errors\n3. Spelling errors\n4. Missing words\nThe phrase replacement selection function should be such that it is able to accommodate all the above mentioned errors, and also be able to preserve the semantics of the original sentence. As a result, it is necessary to select a phrase that is not very different from the original phrase in terms of content. Also, all the above mentioned errors have to be modeled using a number of distance functions. The final distance function should be a combination of components for modeling all the above errors. Each component of the distance function computes a similarity score between the phrase to be replaced and its possible replacement in accordance with the type of error which that component is designed to model. The final score is the combination of all the individual scores.\nSince noisy data can come from various sources, the errors that occur in such data are varied. For example, in chat data, we can see a number of\n12\nabbreviated forms or non-standard language being used. In blogs, we see a number of out of dictionary words being used. Such data more often than not do not adhere to the standard syntax. Machine translation systems is another way of obtaining noisy data. A major problem with such data is the incorrect usage of synonyms resulting from a lack of good bitext. Also, if the machine translation system does not have a good enough reordering model, we can see a number of reordering based problems that can arise if the source and target languages have different word orders. So, the distance function being used is dependent upon the application and should be tuned accordingly."}, {"heading": "2.3 Evaluation Metrics", "text": "The main purpose of this work is to improve the correctness of a sentence given a Language Model. Since a Language Model helps measure the fluency of a piece of text, the main quantity being measured here is the fluency. Although fluency is not a sufficient metric to measure how good a piece of text is. This is because it is possible to string together a set of meaningful chunks in order to form a sentence. Such a sentence will get a good score according to the Language Model, but will not count as a good sentence in the language. As a result, we should also measure the faithfulness of the text. Faithfulness can be quantified by how close a given piece of text is in comparison to a human written piece of text. This is done by comparing the machine-generated text to a piece of standard reference human-written text.\nIn order to quantitatively measure the correctness of sentences, we need numerical methods in order to judge the fluency and faithfulness of the sentences generated. Fluency is measured by the Perplexity score according to a language model and the faithfulness is measured by the BLEU score.\n13"}, {"heading": "2.3.1 Perplexity", "text": "Perplexity helps to determine how probable a given sentence is according to a Language Model. In other words, it decides how the phrases will be ordered by a fluent speaker of the language. The ordering of phrases is the key factor here, as perplexity does not take into consideration any kind of syntactic or semantic rules that might govern the formation of a correct sentence in any language. Perplexity is purely an Information Theoretic measure.\nSay, we are given a language model LM which defines a probability distribution PLM . Let us say that LM is an N -gram language model. Say we have a sentence S = S1, S2, . . . , Sl of l words. We define the average logprob per word as follows:\nAverage Logprob (LP ) = \u22121 l \u2211l i=N logPLM(Si|Si\u2212N+1Si\u2212N . . . Si\u22121)\nThe average perplexity per word is defined as :\nPerplexity (PP ) = 2LP\nWe can see from the definition of Perplexity that it essentially is the size of the set of words from which the next word is chosen given that we observe the history of words spoken. This is highly dependent on the domain of discourse, with formal texts usually having a lower value of perplexity. As can also be seen from the definition, the Perplexity value is improved if the Language Model assigns greater probability mass to words which are actually used in correct order. So, if we assume that we have a good enough Language Model, we can say that a sentence has good fluency if it has a lower value of perplexity.\n14"}, {"heading": "2.3.2 BLEU Score", "text": "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine\u2019s output and that of a human: \u201dthe closer a machine translation is to a professional human translation, the better it is\u201d. BLEU was one of the first metrics to achieve a high correlation with human judgments of quality, and remains one of the most popular.\nScores are calculated for individual translated segments - generally sentences - by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation\u2019s overall quality. Intelligibility or grammatical correctness are not taken into account. BLEU is designed to approximate human judgment at a corpus level, and performs badly if used to evaluate the quality of individual sentences. BLEUs output is always a number between 0 and 1. This value indicates how similar the candidate and reference texts are, with values closer to 1 representing more similar texts.\nBLEU uses a modified form of precision to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text. This is illustrated in the following example 1:\nExample of Machine Translation output with high precision Candidate: the the the the the the the Reference 1: the cat is on the mat Reference 2: there is a cat on the mat Of the seven words in the candidate translation, all of them appear in the reference translations. Thus the candidate text is given a unigram precision of:\n1Example Taken from Wikipedia\n15\nP = m wt = 7 7 = 1\nwhere m is number of words from the candidate that are found in the reference, and wt is the total number of words in the candidate. This is a perfect score, despite the fact that the candidate translation above retains little of the content of either of the references.\nThe modification that BLEU makes is fairly straightforward. For each word in the candidate translation, the algorithm takes its maximum total count, mmax, in any of the reference translations. In the example above, the word \u201dthe\u201d appears twice in reference 1, and once in reference 2. Thus mmax = 2.\nFor the candidate translation, the count mw of each word is clipped to a maximum of mmax for that word. In this case, \u201dthe\u201d has mw = 7 and mmax = 2, thus mw is clipped to 2. mw is then summed over all words in the candidate. This sum is then divided by the total number of words in the candidate translation. In the above example, the modified unigram precision score would be: P = 2 7\nThe above method is used to calculate scores for a range of n-gram lengths. The length which has the \u201dhighest correlation with monolingual human judgments\u201d was found to be four. The unigram scores are found to account for the adequacy of the translation, how much information is retained. The longer n-gram scores account for the fluency of the translation, or to what extent it reads like \u201dgood English\u201d.\nThe modification made to precision does not solve the problem of short translations, which can produce very high precision scores, even using modified precision. An example of a candidate translation for the same references as above might be: the cat\n16\nIn this example, the modified unigram precision would be, P = 1 2 + 1 2 = 2 2\nas the word \u2019the\u2019 and the word \u2019cat\u2019 appear once each in the candidate, and the total number of words is two. The modified bigram precision would be 1 1 as the bigram, \u201dthe cat\u201d appears once in the candidate. It has been pointed out that precision is usually twinned with recall to overcome this problem , as the unigram recall of this example would be 2 6 or 2 7 . The problem being that as there are multiple reference translations, a bad translation could easily have an inflated recall, such as a translation which consisted of all the words in each of the references.\nIn order to produce a score for the whole corpus the modified precision scores for the segments are combined, using the geometric mean multiplied by a brevity penalty to prevent very short candidates from receiving too high a score. Let r be the total length of the reference corpus, and c the total length of the translation corpus. If c \u2264 r, the brevity penalty applies, defined to be e(1\u2212 r c ). (In the case of multiple reference sentences, r is taken to be the sum of the lengths of the sentences whose lengths are closest to the lengths of the candidate sentences."}, {"heading": "2.4 System Overview", "text": "The entire experimental procedure can be divided into the following components:\n1. Obtaining noisy natural language sentences.\n2. Constructing the language model.\n3. Obtaining the best substitution for an input noisy sentence.\nOut of the number of sources mentioned previously from which noisy data might be made available, we have used poor machine translation for our experiments. In order to obtain the noisy data, we used a Statistical Machine\n17\nTranslation (SMT) system trained on a very small amount of data. The required language model for the translation system was derived using this small amount of data. This data was fed to GIZA++ [38] to learn the translation model. The language model was created using the SRILM language modeling toolkit [39]. The translation model and the language model were then fed to MOSES [40] decoder which translated the candidate source language sentences into target language sentences. These translated sentences are the noisy sentences that we attempt to correct in the next step.\nNext, we use a large target language monolingual corpus of the in order to construct the language model for correction. Once again, SRILM is used to compute the language model in ARPA language model format. This language model along with the noisy sentences are then passed on to the fluency improvement algorithm in order to correct the noisy sentences.\n18\nChapter 3"}, {"heading": "Fixed Length Phrase-based", "text": ""}, {"heading": "Correction Model", "text": "Finding an optimal phrase segmentation is the most integral part of our task. In the first approach, we try to use fixed length phrases for this purpose. The replacement mechanism we follow is local to the phrases themselves, and the replacements made to a phrase do not in any way affect the replacements made to any other phrase."}, {"heading": "3.1 Algorithm", "text": "The first approach is the Fixed Length Phrase-based approach. In this approach, we divide the sentence into a set of phrases of fixed length and compute the best substitute for each phrase using overlapping n-grams. The algorithm then performs a set of local replacements on each phrase. Let us consider an example to see how this algorithm works:\nLet the phrase be P = p1p2 . . . p7 Let the language model have order = 4 Initially we have, starting index equal to 1. So first, consider the subphrase p1p2p3p4. Let the set of replacements for this phrase be:\n19\n1. r1 = x1x2x3\n2. r2 = y1y2y3y4\nFirst, replace p1 . . . p4 with r1. We obtain P \u2032 = x1x2x3p5p6p7. Next, make a recursive call to the function with P \u2032 as the phrase to be replaced and start index equal to 2. Carry out these recursive calls until the entire phrase P is replaced and compute the score. Next, perform a backtracking up the recursion stack and replace with the other available choices. When the control comes back to the top level function call, replace p1 . . . p4 with r2 and follow the same recursive algorithm. The sequence of recursions that give the best score for the replaced phrase is taken to be the replacement for the phrase P . This process is repeated for all phrases.\nIn order to find the best replacement for a phrase, we use a very simple\nAlgorithm 1 Algorithm involving fixed length phrases to compute best substitution Require: Language model LM Require: n = Order of LM Require: L =Size of phrases Require: Phrase P = w1w2 . . . wL Require: global current best = ScoreLM(P ) Require: global best sub = P\nrecursive function COMPUTE SUB(P, start index, Score) : if start index+ n > L then {The end of phrase has been reached} if LMScore > current best then current best = Score best sub = P\nend if else kbest =FIND K BEST(wstart index, . . . wstart index+n\u22121) for all x \u2208 kbest do P \u2032 =Substitute wstart index . . . wstart index+n\u22121 with x COMPUTE SUB(P \u2032, start index+ 1, Score+ ScoreLM(P\n\u2032)) end for\nend if\n20\nheuristic. We choose a phrase Q to be a possible replacement for a phrase P if the two phrases share at least 2 common words. The algorithm is formalized in Algorithm 1. The function FIND K BEST() takes in a phrase as argument and returns all phrases in the language model which has at least 2 words in common with the given phrase. The algorithm uses a global variable called cur best which stores the best substitute recorded till that instant. The steps followed for each phrase P are as follows:\n1. Initialize cur best as the phrase P and Score as the score of P according\nto language model.\n2. Call the function COMPUTE SUB(P ,0,Score).\n3. Store the string returned in step 2 as the replacement for the phrase\nP .\nAfter all the phrases have been substituted, the algorithm reconstructs the final sentence by simply concatenating the phrase substitutions. After it does so, it computes the score of the entire sentence according to the language model. If the score is greater than the score of the original sentence, it outputs the resulting sentence, otherwise outputs the original sentence.\nAs can be seen, this algorithm works locally on phrases and does not consider the relation between two phrases based on their location in the candidate sentence. However, within phrases, it follows a continuous substitution policy to determine the best substitution. The substitutions within one phrase are overlapping. An important feature of this algorithm is the fact that it does not bring about any loss in the quality of sentences. Due to the fact that we compare the best substitution with the original sentence, the resulting sentences are at least as good as the original ones."}, {"heading": "3.2 Experiments and Results", "text": "For our experiments, we used English as the target language. As previously mentioned, due to lack of data from other domain, we are using data from\n21\npoor machine translation systems in order to check our algorithms. For testing the performance of this approach, we used the Europarl [41] Corpus. We used 1000 sentences from the Europarl English-German bitext in order to train the machine translation system. 100 English sentences comprised the test set. In order to build the language model, we used the entire English part of the Europarl corpus. For perplexity computation, we used the perplexity computation module of SRILM.\nFor experimentation, we used a 4-gram Language Model, i.e., n = 4. The length of phrase size was chosen to be 7. The results obtained from evaluating over the 100 sentences in the test set are summarized in table 3.1.\nWe can see that this algorithm does not do well. In fact, we see no change in perplexity or BLEU score, signifying that it has, in fact, not made any change to the set of sentences. Therefore, this algorithm does not bring about any change to the quality of sentences in the test set. An analysis of the sentences used as the test set reveals the same fact. The output sentences were exactly the same as the input sentences."}, {"heading": "3.3 Analysis", "text": "An analysis of the steps of this algorithm reveals the fact that this algorithm probably performs too many substitutions, thereby actually attempting to increase the amount of noise in the sentences. Also, human beings do not usually attempt to make changes in sentences in this fashion. Corrections are made on phrases. One phrase is taken as a single unit, and some changes\n22\nare made in that phrase so that the resultant phrase resembles the actual phrase in some way, but the errors of the original phrase are eliminated. Also, this algorithm fails to deal with inter-phrasal correlation, for which a conditional probability among phrases should have been used. All these factors contribute to the failure of the algorithm.\nAnother factor that does not support the use of this algorithm is the time complexity. for each phrase, the algorithm takes time which is exponential in the number of replacement candidates. In order to understand the time complexity of the algorithm, let us model the phrase replacement procedure as a graph. Every possible phrase is a node and there exists an edge from one node vi to another node vj if there exists a possible substitution which can transform vi into vj. This algorithm tries to explore all possible sequence of substitutions to transform a sentence. Hence, in the graph model, it tries to explore all possible paths from a starting node. Since a graph can have exponential number of such paths, this algorithm has an exponential time complexity which renders it useless for almost all practical purposes.\n23\nChapter 4"}, {"heading": "The Dynamic Programming", "text": "based method"}, {"heading": "4.1 Motivation", "text": "The failure of the fixed length phrase-based approach led us to conclude that this approach was only trying to increase the amount of error already present in the sentences by trying to make too many substitutions. So we hypothesize that an approach that takes in a phrase as one unit and makes corrects a phrase in one step after identifying the errors might be a good way to proceed. Also, the previous approach did not have any function that explicitly models all the previously mentioned errors. As a result, in this approach, we also modify the function used to select the replacements fro the phrases. Another problem with the previous approach is its exponential time complexity and the fact that it consumes much memory due to its recursive nature. We need to find a more efficient way of selecting the best phrase segmentation of a sentence. We also want to allow variable length phrases. All this can be accomplished by a simple recursive formulation of the problem as follows: Suppose, we have a sentence S = s1s2 . . . sn Suppose, we wish to find the best replacements for the phrase Sij = si . . . sj Suppose, | denotes the concatenation operator. Then, we can write, Sij = Sil|S(l+1)j for any i \u2264 l \u2264 j.\n24\nHence, the task of computing Sij reduces to computing the best l in accordance with the above equation. Here, we define \u201dbest\u201d as the l for which the replacements have highest score according to language model. Iteratively carrying out this operation would give us the replacements for S1n = S.\nWe can easily see that this can be modeled as a bottom up dynamic programming problem. If we have replacement candidates for Sil and S(l+1)j, we can combine them using the above equation in order to get Sij. This formulation is, in fact, identical to the formulation of the famed Cocke-Younger-Kasami (CYK) algorithm [42][43][44] for parsing of context-free grammars. Once we have the initial set of replacements for each Sij computed using the distance function mentioned subsequently, we can use this formulation to combine them all in order to find the best substitute for the entire sentence. An important point to note with variable phrase lengths is the fact that we need to explore all possible phrasal decompositions, whose number is exponential in the length of the sentence. However, this formulation allows us to compute the best decomposition in time polynomial in the length of the sentence. Thus, this formulation gives us a more efficient way of solving the problem."}, {"heading": "4.2 Algorithm", "text": "Suppose we have a sentence S which we wish to correct. Say S has N words S1, . . . , SN . So, S = S1S2 . . . SN\nThis sentence may be divided into an O(N2) number of contiguous phrases. Each phrase may begin at any Si and end at any Sj satisfying the constraint 1 \u2264 i \u2264 j \u2264 N . We denote a phrase beginning at Si and ending at Sj as Pij. Let the Language Model be denoted by LM . The algorithm initially computes a list of substitutes for each Pij from LM . For this purpose, a function FIND BEST SUB() is called. It takes in a phrase P and returns a list of k best replacement phrases for P taken from LM . For now, we treat this function as a black box. This function is discussed in detail in the next\n25\nsection.\nLet the list of top-k substitutes for Pij be denoted by SUBij. SUBij constitutes the initial list of substitutes taken from the Language Model. Each entry of SUBij is a 2-tuple of the form (phrase, score) where the score of the phrase is the logprob value according to LM . We combine these lists using a Dynamic Programming algorithm. We fill a table REP . It is a N \u00d7N table. The entry REPij consists of the top k substitutes for Pij taken from SUBij and other entries of the table SUB. REPij is computed as follows:\n1. REPij = SUBij\n2. \u2200l, i \u2264 l \u2264 j \u2212 1, we have k2 candidate substitutes for Pij because there are k candidates present in each of SUBil and SUB(l+1)j. For\neach of these k2 candidates, Compute the score of the phrase sii|s(l+1)j according to LM , where sik \u2208 SUBik and s(k+1)j \u2208 SUB(k+1)j. Here | denotes the string concatenation operation.\n3. For each of the k2 phrases constructed in Step 2, add each (phrase,\nscore) pair to REPij.\n4. Retain the top k entries in REPij based on the scores.\nThe entries of REP1N give us the final optimum substitution for the sentence S. The algorithm is formally stated in Algorithm 2.\nThe algorithm fills up a N \u00d7 N table. First, it fills up the entries corresponding to phrases of length 1, then phrases of length 2 and so on. Let us consider a small example to see how the algorithm works:\nSay, we are trying to fill up REPij. Initialize REPij = SUBij. Say, we take a division at some l. Let, REPil = {P1, P2, . . . , Pk} and REP(l+1)j = {Q1, Q2, . . . , Qk}. The strings we add toREPij are {P1Q1, P1Q2, . . . , P1Qk, P2Q1, . . . , P2Qk, . . . , PkQ1, . . . , PkQk}.\n26\nAlgorithm 2 Dynamic Programming based correction algorithm sentence S = S1S2 . . . SN phrase Pij = Si . . . Sj. We have S = P1N SUB = N \u00d7N array storing the initial replacements. SUBij has replacements for Pij REP = N \u00d7 N array storing the final replacements. REPij has replacements for Pij \u2200i, j1 \u2264 i \u2264 j \u2264 N , compute SUBij = FIND BEST SUB(Pij) for i = 1 to N do REPii = SUBii //Fill up the table for all strings of length one\nend for //Length of substrings being replaced for l = 2 to N do\n//Starting position of the string being replaced for i = 1 to N \u2212 l + 1 do\n//Iterate over all possible break points for j = i to i+ l \u2212 1 do\n//Initialize REPi(i+l\u22121) to already computed replacements REPi(i+l\u22121) = SUBi(i+l\u22121) for all (p,s) \u2208 SUBij do\nfor all (p\u2019,s\u2019) \u2208 SUB(j+1)(i+l\u22121) do q = p|p\u2032, where | is the concatenation operation r=score(q) REPi(i+l\u22121) = REPi(i+l\u22121) \u222a (q, r)\nend for end for\nend for REPi(i+l\u22121) = Top k (q, r) pairs having best scores from REPi(i+l\u22121)\nend for end for\n27\nThen, we retain the top k best strings from the set REPij according to the scores assigned to the strings by the language model.\nFigure 4.2 gives us an illustration of the way the algorithm works. The shaded area of the table is the one which is actually filled during the course of the algorithm. The arrows depict the direction in which the table is filled.\nThe algorithm runs in polynomial time once the function FIND BEST SUB() has been computed. The Complexity of the remaining part of the algorithm stems from the 5 nested for loops. The three outermost loops run in O(N) time each and the two innermost loops run in O(k) time each, giving this loop a time complexity of O(N3k2). So, the overall time complexity of this algorithm is O(N3k2). In addition, the function FIND BEST SUB() is called for O(N2) times.\nA naive implementation of the function FIND BEST SUB() would be to perform a complete linear scan of the entire list, match the target phrase with each phrase in the list and select the best matches. This would be very inefficient, because of the extremely large number of phrases in the language\n28\nmodel. Hence, we need a more efficient method of implementing this function. This implementation issue has been handled in the next section."}, {"heading": "4.3 Computing the Phrase Substitutes", "text": "The phrasal substitutes have been computed using the FIND BEST SUB() function. This function takes in a single argument (the phrase p) and returns a list of 2-tuples of the form (q,s) where q is a candidate replacement for p and q has a score of s according to the language model being used. As mentioned in the previous section, a naive implementation of this function using linear search and a distance function would result in very high time complexity. So, an efficient implementation is essential.\nThis problem can be modeled as an Information Retrieval task. We can treat each phrase in the language model as a document and the replacement candidate as the query. Our aim is to find out which documents best match the query and also have high scores. In order to compute the k-best list, we follow a 2-step process:\n1. From all the phrases in the language model, compute the top T best\nphrases that match the replacement candidate.\n2. From the T phrases computed in the previous step, compute the k\nphrases having the best scores.\nAs in every Information Retrieval system, we need to first index the documents. Each phrase in the language model is treated as an individual document and assigned a docid. Next, an inverted index is constructed based on the words present in the phrases. The inverted index is similar to the index used in standard Information Retrieval systems. Corresponding to every word, there is a postings list of docid in which that word occurs. A dictionary of words is also maintained. In order to facilitate implementation, the words in the dictionary are stored in a prefix-tree (trie), which significantly\n29\nlowers the search complexity for the dictionary.\nDuring the retrieval phase, we use a distance function between words in order to match docid to the query. Let this distance function be D(w1, w2). The retrieval process can be formally expressed as follows:\nSay, the query q contains wq words q1, q2, . . . , qwq .\nLet us say, by applying D(qi, wj) for all wj in the dictionary, we get the set Sq = s1, s2, . . . , sx to be the set of words that are similar to the words in wq.\nLet the postings list for each si in Sq be represented by li.\nThen the list of documents which satisfy the query is given by Outq = \u222axi=1li. Thus, Outq is the list of phrases that have words which are in some way similar to the query replacement candidate phrase q.\nIn order to carry out the intersection in the previous step efficiently, the docids in each postings list is maintained in sorted order. Such representations in the form of a linked list give us an union time which is linear in the size of the two lists.\nOnce we get the list of phrases which have words similar to the words in the replacement candidate phrase, we can do a simple linear scan to find out the best T phrases from this list. This is because, in practice, we have |Outq| << M . We use a phrase level distance function computing the T - best list. A number of facts were taken into consideration while defining this distance function. The distance function is a linear combination of different components catering to different possible sources of error. They are:\n1. Orthographic Errors, like typographical errors etc.\n2. Word errors, for example, using words out of context or at unsuitable\n30\npositions.\n3. Reordering based errors, most commonly seen in the case of Machine\nTranslation from Free Word Order languages to Fixed Word Order languages.\nThe different components of the distance function are as follows:\n1. Levenshtein distance [21] between individual words (f1), which models\nthe orthographic errors.\n2. Synset Distance in Wordnet (f2), which models the incorrect use of\nsynonyms in sentences.\n3. Word order based metrics (f3), which models the errors created by\ndifferent word order in different languages.\nDistance function (fd) can be represented as a linear combination of the above metrics:\nfd = \u22114 i=1 \u03b1ifi\nEach of these metrics is explained in detail in the following subsections."}, {"heading": "4.3.1 Levenshtein distance between individual words", "text": "Levenshtein Distance is a commonly used metric in tools like spell checkers. It is commonly known as the Edit Distance algorithm. It is used to find out the distance between two words. Levenshtein distance measures the minimum number of changes required in order to transform one word into another. For this, it permits the following operations:\n1. Insertion: It refers to inserting a new character to transform the word.\nFor example, cat\u2192cart. In this, r is inserted between a and t in the original word.\n31\n2. Deletion: This operation is essentially the reverse of insertion, where\nwe delete an already present character in the original word to form a new word. For example, cart\u2192cat. In this, the r between a and t is deleted in the original word.\n3. Substitution: This operation involves the replacement of one charac-\nter in the original word with another character. For example, cart\u2192cast. Here, the r in the source word is replaced by s. This operation can be seen as a combination of insertion and deletion. The previous example can be expressed as an insertion followed by a deletion, like, cart\u2192carst\u2192cast, or as a deletion followed by an insertion, like cart\u2192cat\u2192cast. However, we use this as a separate metric in order to lower the edit distance value.\nThe Levenshtein distance measure is used in the algorithm at two stages:\n1. Firstly, this metric is used in order to fetch the phrases which seem\nrelevant to the replacement candidate. This is used as a part of the distance function D(w1, w2). Say, for some qi \u2208 wq, and some wj in the dictionary, we have D(qi, wj) < Dt (Dt is a predetermined threshold). Then we include wj in Sq.\n2. Secondly, this metric is used in the computation of the k-best list of\nreplacements as part of functions f1 and f4. The details are as follows:\n(a) As part of f1:\nSay, we are given a phrase P = p1, p2, . . . pn and a possible replacement for that phrase R = r1, r2, . . . rm. We compute the Levenshtein distance between each (pi, rj) pair. We normalize this value by the maximum of the lengths of pi and rj. Now we define the score for the phrase R as the minimum normalized edit distance for a word in P with words of R summed over all the words in P . Formally speaking,\n32\nf1 = \u2211 p\u2208P maxr\u2208R LevenshteinDistance(p,r)\nmax(len(pi),len(rj))\n(b) As part of f4:\nIn the function f3, the Levenshtein distance metric is used in order to preprocess the phrases before computing the actual word order based distance metrics. The preprocessing phase actually performs an alignment between the words in the replacement candidate and the target phrase. As in the previous case, let us consider P to be the phrase to be replaced and R to be a candidate replacement phrase. A word p \u2208 P is said to be aligned to a word r \u2208 R iff the Levenshtein distance between p and r is less than some threshold. In this phase, we perform a one-to-one alignment, with ties broken arbitrarily. That is, if two words in P get aligned to a single word in R, we choose only one of the words of P to be aligned and ignore the other one."}, {"heading": "4.3.2 Synset Distance in Wordnet", "text": "Almost all languages have an extremely rich vocabulary. As a result, it becomes almost impossible for a single person to remember all words in a language. So, it is quite often the case that words are used in an incorrect fashion. For example, quite often it is the case that a word is used in a sentence while one of it\u2019s synonyms or a word closely related to it has to be used in its stead. Let us consider the following sentence:"}, {"heading": "He was a one game surprise.", "text": "However, a more correct and semantically apt sentence would be:"}, {"heading": "He was a one game wonder.", "text": "Such inaccuracies can be taken care of only when the words in question are replaced by more suitable words. That is where the synset based word\n33\nmatching part of the distance function helps us compute better replacements for a word in a phrase.\nIn order to compute this metric, we use the Wordnet [45]. WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between these synonym sets. The purpose is twofold: to produce a combination of dictionary and thesaurus that is more intuitively usable, and to support automatic text analysis and artificial intelligence applications. The database and software tools have been released under a BSD style license and can be downloaded and used freely. The database can also be browsed online.\nEach synset in the wordnet consists of words having similar semantics, or in short, synonyms. The wordnet also supports a number of other relationships between synsets. In fact, we can view it as a graph where the vertices are the synsets and the edges are the relationships among the synsets. Some of the relationships supported by the Wordnet are:\n1. Hyponymy\n2. Hypernymy\n3. Meronymy\n4. Holonymy\nIn this work, only the synonyms have been used. As before, let us consider the replacement candidate to be P and a possible replacement to be R. For each word in P , we check if there is any word in R which belongs to the same synset. We define a Boolean function g(p,R) which takes the value of 1 if there is any r \u2208 R belonging to the same synset as p in the Wordnet, and 0 otherwise. The function f2 is defined as the sum of the function g(p,R) for all words p \u2208 P and normalized by the length of P . Therefore, we have,\n34\nf2 = \u2211 p\u2208P g(p,R)\nlen(P )"}, {"heading": "4.3.3 Word order based metrics", "text": "Word order becomes an important factor, especially in translation tasks. For example, in a language like Bengali or Hindi, the order of words is SubjectObject-Verb, whereas in English it is Subject-Verb-Object. So, a common problem in translation systems is the lack of a good reordering model. Also, when non-native speakers or new learners of a language try to write sentences in the language, word order errors are some of the more commonly observed errors. Hence, this is a problem worthy of consideration. And that is why this factor in the distance function is extremely important. If this factor is not kept, phrases might be chosen which have an incorrect word order but higher overall score. This factor takes care of the fact that correct word order is preserved when selecting replacement phrases.\nIn this work, two types of functions have been used for word order preservation:\n1. Rigid Word Order: As mentioned in the subsection on Levenshtein\ndistance, first a word level alignment is performed between the phrases P and R. Next, it is checked if the words in R aligned to P are in the same order as the corresponding words in P . The phrase R is considered for replacement if and only if the aligned words are in the same order in both P and R. Otherwise, the phrase R is discarded.\n2. Flexible Word Order: The rigid word order metric does not always\nwork. In fact, a certain degree of flexibility is required so that word order related problems can be taken care of. But it must be ensured that the flexibility is not absolute, otherwise, it might lead to the introduction of more noise in the sentences. In this case too, we consider the previous model where a phrase R is a possible replacement candidate\n35\nfor a phrase P . Using the word based Levenshtein distance algorithm, first an alignment is computed. Each word pair in the phrases is tagged. Next, either one of these two metrics is computed:\n(a) Length of Longest Common Subsequence: The length of the Longest\nCommon Subsequence (LCS) is computed between the aligned words in R and P . This value is normalized by the number of aligned words and that gives us the value of f3.\n(b) Number of Inversion Pairs: Given an array A = [A1, A2, . . . , An],\nA[i] and A[j] is said to be an inversion pair iff i > j but A[i] < A[j]. In this method too, the words are aligned and the number of inversion pairs in R are computed. The inverse of the number of inversion pairs gives the value of f3.\nLet us consider an example: Say, R = r1, r2, r3, r4 And,P = p1, p2, p3, p4, p5 Say, after the alignment phase, we have p1 aligns to r2, p2 aligns to r4 and p4 aligns to r3. So, the number of aligned pairs is 3. Let us tag the pairs with numbers and ignore the rest of the words for the time being. So, the phrases become: P = 1, 2, 3 R = 1, 3, 2 If we consider the LCS metric, the value will be f3 = 2 3 , as 2,3 is a possible Longest Common Subsequence of P and R. If we consider the Inversion metric, the value will be f3 = 1 3 . 3,2 is an inversion pair in R."}, {"heading": "4.4 A Representative Example", "text": "Let us consider the following sentence:\n36\nthe europe extreme right in is characterized by its and its use of immigration as differences the issue of.\nWe can see that there are a number of errors in this sentence. For example:\n1. The phrase europe extreme right should become european extreme right\nor extreme right of europe.\n2. The word in should not be there after right.\n3. The phrase its and its use. There should either be a word after the\nfirst its, or the phrase should be its use.\n4. The phrase differences the issue of makes no sense. This phrase should\nbe reordered or replaced by something else.\nLet us now see how the algorithm works on this sentence.\nFirstly, it calls on the function FIND BEST SUB() in order to compute the best replacements for all possible phrases. We have used the 5 best replacements in this experiment. A snapshot of the replacement set of some of the more significant phrases is given below along with the scores. Each replacement is written as a (phrase,score) pair.\nSUB(europe) = {(europe, 5.9865), (european, 5.9012), (european union, 4.5213), (the european life, 2.3242), (european governments, 2.2312)} SUB(extreme) = {(extreme, 1.2121), (extremist, 0.9821), (extremist wings, 0.5632), (the islamic extremists, 0.2180), (fundamentalists and extremists, 0.1134)} SUB(europe extreme) = {(european, 5.1212), (european extreme, 4.9212), (extremist threat, 4.2911),(threat to europe, 3.2931), (terrorism in europe, 3.1023)} SUB(europe extreme right) = {(european extreme right, 4.2313), (extreme europe right, 4.0981),(european right wing, 3.5673),(european liberal\n37\npolitics, 3.2342),(the european union, 3.2312)} SUB(extreme right in) = {(extreme right is, 3.4562), (extreme right, 3.2576), (rightist extremists are, 2.7853), (in the extremist propaganda, 1.0123), (right to say, 0.4323)} SUB(its and its use) = {(its use, 5.2142), (that it uses ,4,2123), (its diplomacy and its, 4.0123), (that it had used, 3.1463), (they had been used, 2.6432)} SUB(as differences the) = {(make good the differences, 2.1231),(as the differences, 1.4654), (differences of the, 1.2435), (different from the usual, 0.3461), (different issues in the, 0.2423)} SUB(differences the issue of ) = {(issue of the differences, 2.3242), (address the issues of differences, 2.1982), (address the matter of, 1.5692), (the differences among the, 1.2342), (the problems of different, 0.8734)}\nThe length of the sentence is N = 19. So, the algorithm creates a 19 \u00d7 19 matrix and fills up its diagonal and the upper triangle. First it fills up the diagonal for all the unigrams in the sentence, then all the bigrams and so on. In order to see how this algorithm actually works, let us consider the phrase the europe extreme right in. This phrase can be broken down in the following ways:\n1. the + europe extreme right in\n2. the europe + extreme right in\n3. the europe extreme + right in\n4. the europe extreme right + in\nAfter the lower rows of the table is filled up, the cell corresponding to the phrase the europe and extreme right in looks as follows: REP (the europe) = {(the europe, 12.2341), (the european, 12.1914), (the the european union, 10.1231), (not in the european union, 9.1231), (the european governments, 6.2114)} REP (extreme right in) = {(extreme right is, 3.4562), (extreme right, 3.2576),\n38\n(rightist extremists are, 2.7853), (in the extremist propaganda, 1.0123), (right to say, 0.4323)}\nCombining these two, we get 25 candidates for replacing the phrase the europe extreme right, like:\n1. (the europe extreme right is, 16.0012)\n2. (the europe extreme right, 15.8212)\n3. (the europe right extremists are, 15.1231)\n4. (the europe in the extremist propaganda, 14.2111)\n5. (the europe right to say, 12.9015)\n6. (the european extreme right is, 16.2918)\n7. (the european extreme right, 15.9210)\n8. (the european rightist extremists are, 15.0012)\n9. (the european in the extremist propaganda, 13.5681)\n10. (the european right to say, 13.0001)\n11. (the the european union extreme right is, 13.6221)\n12. (the the european union extreme right, 13.5512)\n13. (the the european union rightist extremists are, 12.9899)\n14. (the the european union in the extremist propaganda, 11.5101)\n15. (the the european union right to say, 10. 7014)\n16. (not in the european union extreme right is, 12.8213)\n17. (not in the european union extreme right, 12.5612)\n18. (not in the european union rightist extremists are, 12.5235)\n39\n19. (not in the european union in the extremist propaganda, 11.2322)\n20. (not in the european union right to say, 9.4534)\n21. (the european governments extreme right is, 10.9219)\n22. (the european governments extreme right, 10.6212)\n23. (the european governments rightist extremists are, 9.0028)\n24. (the european governments in the extreme propaganda, 7.5422)\n25. (the european governments right to say, 6.8921)\nThe 5 entries marked in bold in the above list mark the best 5 selections for the candidate phrase the europe extreme right. The same process is used to fill up the table REP , the best candidate replacement chosen for the entire sentence turns out to be:\nThe European extreme right is characterized by its use of immigration as the issue of differences.\nAs we can see, a number of changes have been made in the original sentence, like:\n1. europe extreme right \u2192 european extreme right\n2. its and its use \u2192 its use\n3. differences the issue of \u2192 the issue of differences\nQuite clearly, the final sentence is more correct than the original sentence."}, {"heading": "4.5 Experiments and Results", "text": "The variable part of the correction system described previously was the distance function used in order to find out suitable replacement candidates for\n40\nthe phrases. We conducted a number of experiments using several combinations of these distance functions. The used combinations are tabulated in Table 4.1.\nExperiments were performed using the Europarl Corpus. The languages chosen were German and English. Since we did not have sufficient amount of noisy data at our disposal, so we used a poorly tuned Statistical Machine Translation system in order to obtain the incorrect sentences. The source language was German and the target language was English. The correction was done on the English sentences. In fact, to make the task more difficult, a number of errors were manually incorporated into the incorrect English sentences to further increase the amount of noise present in the sentences.\nThe system is the same as explained in Section 2.4. In the implementation of the distance functions, there is one parameter that needs to be fixed. It is in the implementation of the Levenshtein Distance function, where a value of Dt = 3 has been used. This value ensures that the substitute word chosen is not too different, nor too similar to the word in question. We have also provided equal weight to all the components of the distance function, that is, \u22001 \u2264 i \u2264 4, \u03b1i = 0.25. Table 4.2 sums up the values of Perplexity and BLEU score for the different choices of the distance function. In order to analyze the performance of our algorithm on Machine Translation, we peformed another set of experiments. For this purpose, we did not use a sep-\n41\narate Machine Translation system, but used the Google Translator1 instead. The corpus chosen was the FIRE 22008 ad-hoc retrieval corpus. First, we selected 100 sentences from the English corpus. We translated those into Hindi using the Google Translator system and translated back the Hindi sentences into English using Google Translator again. Since English-Hindi translation in Google translator does not give us very good results, so a lot of noise was incorporated into the sentence. Let us take a look at some of the sentences in order to understand the amount of noise introduced:\n1. Original:Specifically CII has appreciated the special initiatives for\nagriculture, gems and jewellery, handlooms, leather and footwear.\nFinal:CII agriculture particularly gems and jewelry, handicraft, leather and shoes for the special initiative is appreciated.\n2. Original:Indian Chamber of Commerce president Anup Singh said,\nThe special package for agriculture and schemes like Vishesh Krishi Upaj Yojna will boost exports of fruits, vegetables and their valueadded products.\nFinal:Indian Chamber of Commerce president, said Anoop Singh, Vishesh agricultural planning and agricultural schemes like the special packages for fruits, vegetables and their value-added products will boost exports.\n1http://translate.google.com/ 2FIRE (Forum for Information REtrieval) is a conference conducted by ISI Kolkata.\nhttp://www.isical.ac.in/\u223cclia/\n42\nPerplexity BLEU Before Correction 27.521 0.557 After Correction 16.121 0.569\nAs is obvious, the final sentences contain a lot of errors, the most common being global clause reordering. Unfortunately, our system is not built to handle this problem. We used our noise correction algorithm with the Distance Function C on these sentences. We used the entire FIRE 2008 ad-hoc English corpus for computing our language model. The results are summarized in Table 4.3. Let us examine some of the output sentences from this experiment:\n1. CII agriculture particularly gems and jewelry handicraft leather and\nshoes for the special initiative is appreciated. \u2192 CII in agricultural particularly gems and jewelry handicraft leather and shoes special initiative has been appreciated.\n43\n2. Indian Chamber of Commerce president said Anoop Singh Vishesh agri-\ncultural planning and agricultural schemes like the special packages for fruits vegetables and their value added products will boost exports. \u2192 Indian Chamber of Commerce president Anoop Singh said Vishesh agricultural planning and agricultural schemes like the special packages for fruits vegetables and their value added products will boost revenue from exports.\n3. GMG to fly three times a week southeastern port city Chittagong and\nKolkata between plans. \u2192 GMG to fly three times a week between southeastern port city Chittagong and Kolkata.\n4. GMG to other Indian cities Mumbai and Delhi link is expected to be\nallowed to fly. \u2192 GMG to other Indian cities like Mumbai and Delhi link is expected to be permitted to fly.\n5. The government recently Thailand, which it hopes will eventually cover\nall countries in the region with a free trade treaty signed. \u2192 The Thailand government now hopes will eventually cover all countries in the region with a signed free trade treaty.\nAs we can see from the above examples, the global reordering problem has not been solved. But the algorithm has indeed made local changes to clauses and phrases and the output sentences are indeed better than the input sentences in terms of fluency. The semantics have not always been preserved, but that is not what we have aimed to do anyway."}, {"heading": "4.6 Analysis", "text": "The Dynamic Programming based correction model succeeds in improving the fluency of sentences quite significantly. This algorithm is quite similar to the standard decoding algorithms, where given a sentence, one is supposed to find a sequence of phrasal segmentations and using the phrase table, find a set of substitutes which would maximize the a-posteriori probability of the\n44\nsentence being generated. In this algorithm, since we do not have any explicit phrase table, we implement it by searching for nearly matching phrases from the Language Model itself. The distance function helps figure out what is a good match. This algorithm does not use a global distortion model like is done in most Statistical Machine Translation systems. The rationale behind this is the fact that normally errors in writing text do not involve long-distance reordering errors, rather most of the wrongly written words occur within a small window. Our test set was so designed that it contained sentences having all the types of errors mentioned in Section 2.2. Some types of errors were obtained right from the translation output, while some were introduced manually.\nA quick glance at Table 4.2 shows that the order of words in the chosen replacement phrases is extremely important, as it significantly lowers the average perplexity of the set of sentences. Although the Distance Function A lowers the perplexity of the test set of sentences, indicating that correction of fluency has indeed taken place, the lower perplexities in case of Distance Functions B,C and D imply that total freedom in case of word order may not be the way to go. In fact, this is quite intuitive from the fact that English is not a Free Word Order language.\nA look at the BLEU scores shows that they have not significantly changed in order to draw a conclusion about the algorithm\u2019s ability to alter the faithfulness of the sentences. However, a comparison between BLEU scores and Perplexity value shows that the BLEU score also follows the same trend as the perplexity values. Also, the Distance Functions C and D have resulted in almost comparable values of perplexity and BLEU, showing that using either of them would give us a good enough result.\nNow let us look at some sentences that were corrected by this algorithm and try to analyze the errors that each distance function corrects:\n1. The parties dominant of the center left and center right have faced\nprospect this in the policy of the ostrich applying.\n45\nDistance Function A: The parties dominant of the center left and center right have faced this prospect in the applying principle of the ostrich. Distance Function B: The dominant parties of the center left and right have faced this prospect in the policy of the ostrich applying. Distance function C: The dominant parties of the center left and right have faced this prospect in the policy of the ostrich applying.\n2. This is precsely the aim pursued largely by research teams in economics,\nsociology, psychology and pol science in the United States. Distance Function A: That precisely is the aim largely pursued by teams of research in economics, sociology, physiology and political science in the United States. Distance Function B: This is precisely the aim pursued largely by teams of research in economics, sociology, psychology and political science in the United States. Distance Function C: This is precisely the aim pursued largely by teams of research in economics, sociology, psychology and political science in the United States.\n3. This does not mean that it should be eliminated heterogeneity and cre-\nate racially homogeneous communities. Distance Function A: This does not mean that it should eliminate heterogeneity and create homogeneous communities racially. Distance Function B:This does not mean that should eliminate heterogeneity and create racially homogeneous communities. Distance Function C:This does not mean that should eliminate heterogeneity and create racially homogeneous communities.\n4. Other protest against action affirmative and maintains that a policy\nthat about race does not care is sufficient. Distance Function A:others protest against an affirmative action and maintains that a policy that about race does care is sufficient. Distance Function B:Others protest against affirmative action and\n46\nmaintains that a policy that does not about race care is sufficient. Distance Function C:Others protest against affirmative action and maintains that a policy that does not about race care is sufficient.\n5. Obviously, minorities have made progressed towards more integration\nand economic success. Distance Function A:Obviously minorities have made progress towards more economic success and integration. Distance Function B:Obviously, minorities have made progress towards more integration and economic success. Distance Function C:Obviously, minorities have made progress towards more integration and economic success.\n6. It will not be the case, as is clear the racial history of America.\nDistance Function A:It will not be the case, as is clear in the racial history of America. Distance Function C:It will not be the case, as is clear in the racial history of America. Distance Function C:It will not be the case, as is clear in the racial history of America.\nAs is apparent from the above examples, the Distance Functions B and C are almost identical in nature, and the changes performed by these functions is basically a superset of the changes performed by the Distance Function A. Distance Function A is able to correct substitution based errors, spelling errors and missing word errors, but may fail to correct reordering based errors. In fact, due to the unconstrained nature of the replacement phrases chosen, it is likely to introduce errors which were not already present. For example, in example 3, it makes changes racially homogeneous communities to homogeneous communities racially. The adjective racially is supposed to qualify the word homogeneous, but the output of the algorithm with Distance Function A makes a modification to this, rendering the sentence incorrect. Such errors might creep in due to the lack of constraints on ordering in this function.\n47\nAll three distance functions have the Levenshtein Distance and Synset Distance based metrics in common. As a result, they are able to correct the remaining categories of errors. For example,\n\u2022 Substitution based errors, like in example 4, other protest\u2192others protest\n\u2022 Spelling errors, like in example 2, precsely\u2192precisely\n\u2022 Missing word errors, like in example 6, clear the racial history\u2192clear in the racial history\nThe downside of this method is that it is highly dependent on the corpus being used, and is an extremely domain-dependent algorithm. The words used in the sentences that are being corrected must bear a high degree of correlation to the words used in the corpus. Otherwise, just in order to increase the likelihood of a sentence according to the corpus, some phrases might be substituted by completely unrelated phrases. Also, how this technique will perform on an extremely free word order language like Sanskrit is unknown, largely because of the absence of a suitable corpus in such a language and our lack of comprehensibility of the language. Thirdly, the corpus used must be large enough for a good enough correction. A small corpus will either miss out on possible sources of error introduce new errors just to make the sentence more likely according to the corpus.\nThis algorithm can find application in a number of areas. For example, we can use this for correction of sentences output by machine translation systems. If we have a small bitext, but a large enough monolingual corpus in the target language, we can use this algorithm with the monolingual corpus for postprocessing of the translation outputs. This algorithm can also be used for domain specific correction of machine translation outputs. Suppose we have a bitext that is generic (not adhering to a particular domain), but a domain specific corpus in the target language. Once the machine translation system generates the outputs, we can filter out the domain specific sentences and use this algorithm on the monolingual corpus to correct those sentences\n48\nthat belong to the particular domain. This method can also be used for second language learning, by correcting the sentences generated by non-native speakers of a particular language.\n49\nChapter 5"}, {"heading": "Conclusion", "text": "In this work, we present two algorithms to correct noisy sentences. One performs reasonably well, whereas the other fails completely. The fixed length phrase based algorithm tries to make too many corrections to the original text and in the process actually increases the number of errors in the text. This problem is rectified by the Dynamic Programming based approach. This approach actually formulates the problem as a decoding task and uses a model similar to phrase based decoding models in order to make corrections to the text. This model, as seen in Chapter 4, is able to correct a number of errors and increase the fluency of the sentences according to the language model used. We have also suggested a number of applications of this algorithm, like correcting machine translation output and domain specific correction of machine translation output. And as an example, we have explored the machine translation output correction application.\nThe primary contribution of the thesis is a method that is language independent and relatively simple. The number of methods explored previously by researchers has the disadvantage of being either language dependent or highly computation intensive. The Dynamic Programming based method is neither very much computation intensive, nor is it at all language dependent. Besides, it has quite low time and space complexities. The thing that distinguishes it from other approaches tried previously is the likeness of the\n50\ntask to decoding. As far as our knowledge goes, nobody has tried to apply decoding algorithms to correct errors in sentences previously. Another major contribution of this work is the varying set of distance functions. The functions cover almost all types of errors that can possibly occur and can be computationally modeled feasibly.\nMuch can be done to improve the performance of this algorithm and to build up on the work done. Firstly, we have applied this on a domain specific corpus. If a corpus can be constructed which incorporates data from all domain, the results might be interesting to investigate. Whether this algorithm can work with data from different domains taken together remains to be seen. Another direction that may be followed by future researchers is investigating how this algorithm works on different classes of languages and whether the type of language has any bearing upon the distance function being used. For example, it is likely that for a highly free word order language like Sanskrit, the Distance Function A might be the best choice because it does not constrict the word ordering by any means. A third approach may be to augment the set of Distance Functions in order to cover more errors. How the algorithm performs under these conditions remains to be seen.\n51"}], "references": [{"title": "Computational models of the brain: from structure to function", "author": ["M. Breakspear", "V. Jirsa", "G. Deco"], "venue": "Neuroimage, 3(52)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Foundations of Sttistical Natural Language Processing", "author": ["Christopher D. Manning", "Hinrich Schutze"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "The mathematic of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Statistical phrasebased translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT-NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Greedy decoding for statistical machine translation in almost linear time", "author": ["Ulrich Germann"], "venue": "In HLT-NAACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["Philipp Koehn"], "venue": "In AMTA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Generation of word graphs in statistical machine translation", "author": ["N. Ueffing", "F.J. Och", "H. Ney"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A decoder for syntax-based statistical mt", "author": ["Kenji Yamada", "Kevin Knight"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Hierarchical phrase based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Techniques for automatically correcting words in text", "author": ["Karen Kukich"], "venue": "ACM Comput. Surv.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Finding approximate matches in large lexicons", "author": ["Justin Zobel", "Philip Dart"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Hierarchically coded lexicon with variants", "author": ["Fran\u00e7ois de Bertrand de Beuvron", "Philippe Trigano"], "venue": "IJPRAI, 9(1):145\u2013165,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction", "author": ["Kemal Oflazer"], "venue": "Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "The string-to-string correction problem", "author": ["Robert A. Wagner", "Michael J. Fischer"], "venue": "J. ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1974}, {"title": "Fast string correction with levenshtein", "author": ["Klaus U. Schulz", "Stoyan Mihov"], "venue": "automata. IJDAR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["Vladimir I. Levenshtein"], "venue": "Technical Report", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1966}, {"title": "Language independent text correction using finite state automata", "author": ["Ahmed Hassan", "Sara Noeman", "Hany Hassan"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Applying winnow to contextsensitive spelling correction", "author": ["Andrew R. Golding", "Dan Roth"], "venue": "CoRR, cmp-lg/9607024,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1996}, {"title": "Automatic rule acquisition for spelling correction", "author": ["Lidia Mangu", "Eric Brill"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Proceedings of the Twelfth National Conference on Artificial Intelligense", "author": ["K. Knight", "I. Chander"], "venue": "pages 779\u2013784", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "A comparison of merging strategies for translation of german compounds", "author": ["Sara Stymne"], "venue": "In EACL (Student Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Using a grammar checker for evaluation and postprocessing of statistical machine translation", "author": ["Sara Stymne", "Lars Ahrenberg"], "venue": "In LREC\u201910,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Improving a statistical mt system with automatically learned rewrite patterns", "author": ["Fei Xia", "Michael McCord"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Statistical machine translation with scarce resources using morpho-syntactic information", "author": ["Sonja Niessen", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Clause restructuring for statistical machine translation", "author": ["Michael Collins", "Philipp Koehn", "Ivona Kucerova"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Chinese syntactic reordering for statistical machine translation", "author": ["Chao Wang", "Michael Collins", "Philipp Koehn"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Using statistical techniques and web search to correct esl errors", "author": ["Michael Gamon", "Claudia Leacock", "Chris Brockett", "William B. Dolan", "Jianfeng Gao", "Dmitriy Belenko", "Alexandre Klementiev"], "venue": "CALICO Journal,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "From spelling correction to text cleaning - using context information", "author": ["Martin Schierle", "Sascha Schulz", "Markus Ackermann"], "venue": "In GfKl,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Speech and Language Processing (2nd Edition) (Prentice Hall Series in Artificial Intelligence)", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Error analysis of the written english essays of secondary school students in malaysia: A case", "author": ["Saadiyah Darus", "Kaladevi Subramaniam"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Srilm \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "volume 2, pages 901\u2013904, Denver", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Programming languages and their compilers: Preliminary notes", "author": ["John Cocke"], "venue": "Courant Institute of Mathematical Sciences,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1969}, {"title": "Recognition and parsing of context-free languages in time n", "author": ["Daniel H. Younger"], "venue": "Information and Control,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1967}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "So far, we have no satisfactory computational model of the human brain - one that can explain how humans truly learn, how knowledge is stored and how is it that we can retrieve information extremely fast [1].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "Mathematically, a Language Model [2] is expressed as a probability distribution as follows: P (Wi|Wi\u22121Wi\u22122 .", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Another method used to compute scores for unknown n-grams is known as smoothing [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "The fundamental principle used to model SMT is the noisy channel model [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "The SMT system can be trained on this parallel corpus using techniques such as the IBM models [5] and Minimum Error Rate Training [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "The SMT system can be trained on this parallel corpus using techniques such as the IBM models [5] and Minimum Error Rate Training [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "decoding [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "In phrase-based translation models [8], the decoding process involves the following steps:", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Greedy Decoder [9]: It starts with the gloss for words and improves the probability by taking the currently cheapest path.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Beam Search based decoder [10]: Such a decoder starts translating from left to right and maintains a list of partial hypotheses.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Word Graph based Decoding [11]: Such a decoder uses a search graph over the phrases and uses hypothesis recombination to output the kbest list of top translations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "String to Tree model [12]: This is also known as parsing based decoding.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "It also uses a technique called Cube Pruning [13] to prune the extremely large hypothesis space.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "Oflazer (1996) [18] suggested a method where all words in a dictionary are treated as a regular language over an alphabet of letters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "For each misspelt word, an exhaustive traversal of the dictionary automaton is initiated using a variant of the Wagner-Fisher algorithm [19] to control the traversal of the dictionary.", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "Schultz and Mihov (2002) [20] present a variant of this approach where the dictionary is also represented as a finite state automaton.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "The acceptor accepts all words that are at a Levenshtein distance [21] k from the input word.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "al(2008) [22] propose an approach where they assume that the dictionary is represented as a deterministic finite state automaton.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Using a predefined confusion set is a common approach in this task, for example Golding and Roth (1996) [23] and Mangu and Brill (1997) [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "Using a predefined confusion set is a common approach in this task, for example Golding and Roth (1996) [23] and Mangu and Brill (1997) [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "But, such methods often target specific phenomena, like correcting English determiners (Knight and Chander, 1994) [25], merging German compounds (Stymne, 2009) [26], or applying word substitution (Elming, 2006) [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "But, such methods often target specific phenomena, like correcting English determiners (Knight and Chander, 1994) [25], merging German compounds (Stymne, 2009) [26], or applying word substitution (Elming, 2006) [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "al (2010) [28] also proposed another method", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Xia and McCord (2004) [29] describe a method for French, where reordering rules that operate on context-free productions are acquired automatically.", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "Niessen and Ney (2004) [30] describe an approach for translation from German to English that combines verbs with associated particles, and also reorders questions.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "(2005) [31] also describe an approach for German for reordering of German clauses, which have quite different orders from English clauses.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "(2007) [32] describe a method of Chinese syntactic reordering for SMT systems.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "al (2009) [33] present a system for identifying and correcting English as Second Language (ESL) errors.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "(2007) [34] take spelling correction one step further and use it for text cleaning.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "An important thing to note here is that sentences are characterized by two basic characteristics [35]:", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "Faithfulness : Given by one or more metric like the BLEU score [36]", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Some of the commonly observed errors in written English are [37]:", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "This data was fed to GIZA++ [38] to learn the translation model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "The language model was created using the SRILM language modeling toolkit [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 35, "context": "The translation model and the language model were then fed to MOSES [40] decoder which translated the candidate source language sentences into target language sentences.", "startOffset": 68, "endOffset": 72}, {"referenceID": 36, "context": "This formulation is, in fact, identical to the formulation of the famed Cocke-Younger-Kasami (CYK) algorithm [42][43][44] for parsing of context-free grammars.", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": "This formulation is, in fact, identical to the formulation of the famed Cocke-Younger-Kasami (CYK) algorithm [42][43][44] for parsing of context-free grammars.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Levenshtein distance [21] between individual words (f1), which models the orthographic errors.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "In order to compute this metric, we use the Wordnet [45].", "startOffset": 52, "endOffset": 56}], "year": 2016, "abstractText": "Correction of Noisy Natural Language Text is an important and well studied problem in Natural Language Processing. It has a number of applications in domains like Statistical Machine Translation, Second Language Learning and Natural Language Generation. In this work, we consider some statistical techniques for Text Correction. We define the classes of errors commonly found in text and describe algorithms to correct them. The data has been taken from a poorly trained Machine Translation system. The algorithms use only a language model in the target language in order to correct the sentences. We use phrase based correction methods in both the algorithms. The phrases are replaced and combined to give us the final corrected sentence. We also present the methods to model different kinds of errors, in addition to results of the working of the algorithms on the test set. We show that one of the approaches fail to achieve the desired goal, whereas the other succeeds well. In the end, we analyze the possible reasons for such a trend in performance. Chapter", "creator": "LaTeX with hyperref package"}}}