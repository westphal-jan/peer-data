{"id": "1105.4618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2011", "title": "Bounding the Fat Shattering Dimension of a Composition Function Class Built Using a Continuous Logic Connective", "abstract": "We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class consisting of subsets of a domain, and a function class consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis dimension (VC) and its generalization, the fat shattering dimension of the scale e, are explained and some examples of their calculations are given with evidence. We then explain Sauer's Lemma, which incorporates the VC dimension and is used to prove the equivalence of a concept class that is non-distributed and has a finite VC dimension.", "histories": [["v1", "Mon, 23 May 2011 20:04:16 GMT  (20kb)", "http://arxiv.org/abs/1105.4618v1", "Winter 2011 Honours research project done under the supervision of Dr. Vladimir Pestov at the University of Ottawa; 35 pages"]], "COMMENTS": "Winter 2011 Honours research project done under the supervision of Dr. Vladimir Pestov at the University of Ottawa; 35 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hubert haoyang duan"], "accepted": false, "id": "1105.4618"}, "pdf": {"name": "1105.4618.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Haoyang Duan"], "emails": [], "sections": [{"heading": null, "text": "We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale \u01eb, are explained and a few examples of their calculations are given with proofs. We then explain Sauer\u2019s Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distributionfree PAC learnable and it having finite VC dimension.\nAs the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale \u01eb of this new function class in terms of the Fat Shattering dimensions of the collection\u2019s classes.\nWe conclude this report by providing a few open questions and future research topics involving the PAC learning model.\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Brief Overview of Analysis and Measure Theory 3", "text": ""}, {"heading": "3 The Probably Approximately Correct Learning Model 7", "text": ""}, {"heading": "4 The Vapnik-Chervonenkis Dimension 11", "text": "4.1 Sauer\u2019s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 Characterization of concept class distribution-free PAC learning . . . 14"}, {"heading": "5 The Fat Shattering Dimension 15", "text": "5.1 Sufficient condition for function class distribution-free PAC learning . 17"}, {"heading": "6 The Fat Shattering Dimension of a Composition Function Class 19", "text": "6.1 Construction in the context of concept classes . . . . . . . . . . . . . 19 6.2 Construction of new function class with continuous logic connective . 20 6.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 6.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"}, {"heading": "7 Open Questions 29", "text": ""}, {"heading": "8 Conclusion 31", "text": "References 32"}, {"heading": "1 Introduction", "text": "In the area of statistical learning theory, the Probably Approximately Correct (PAC) learning model formalizes the notion of learning by using sample data points to produce valid hypotheses through algorithms. For instance, the following illustrates one learning problem which can be formalized in the PAC model. Given that there is a disease which affects certain people and out of 100 people in a hospital, 12 of them are sick with this disease. Is there a way to predict whether any given person in the hospital has the disease or not?\nThis report covers the PAC learning model applied to learning a collection of subsets C, called a concept class, of a domain X and more generally, a collection of functions F , called a function class, from X to the unit interval [0, 1]. The report involves mostly concepts from analysis and some concepts from probability theory, but only the completion of the first two years of undergraduate studies in mathematics are assumed from the readers.\nReport outline\nFirst, we give two definitions of PAC learning, one for a concept class C and the other for a function class F , and explore two combinatorial parameters, the VapnikChervonenkis (VC) dimension and the Fat Shattering dimension of scale \u01eb, for C and F , respectively. Then, we explain Sauer\u2019s Lemma, a theorem which involves the VC dimension of C and is used to prove that the finiteness of this dimension is a sufficient condition for C to be learnable.\nFinally, as the main new result of our research, given function classes F1, . . . ,Fk and a \u201ccontinuous logic connective\u201d (that is, a continuous function u : [0, 1]k \u2192 [0, 1]), we consider the construction of a new composition function class u(F1, . . . ,Fk), consisting of functions u(f1, . . . , fk) defined by u(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)) for fi \u2208 Fi. We then bound the Fat Shattering dimension of scale \u01eb of this class in terms of a sum of the Fat Shattering dimensions of scale \u03b4(\u01eb, k) of F1, . . . ,Fk, where \u03b4(\u01eb, k) only depends on \u01eb and k. There is a previously known analogous estimate for a composition of concept classes built using a usual connective of classical logic [18]. We deduce our new bound using results from Mendelson-Vershynin and Talagrand.\nBefore jumping into the PAC learning model, we provide some basic terminology and results from analysis and measure theory. From now on, any propositions or examples given with proofs, unless mentioned otherwise, are done by us and are independent of any sources."}, {"heading": "2 Brief Overview of Analysis and Measure Theory", "text": "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.\nProbability space\nDefinition 2.1. Let X be a set. A \u03c3-algebra S is a non-empty collection of subsets of X such that the following are satisfied:\n1. If A \u2208 S, then X \\ A \u2208 S\n2. If Ai \u2208 S for i \u2208 N, then \u22c3 i\u2208N Ai \u2208 S\nIf S is a \u03c3-algebra, then the pair (X,S) is called a measurable space.\nDefinition 2.2. Suppose (X,S) and (Y, T ) are two measurable spaces. A function f : X \u2192 Y is called measurable if f\u22121(T ) \u2208 S for all T \u2208 T .\nDefinition 2.3. Given a measurable space (X,S), a function \u00b5 : S \u2192 R+ = {r \u2208 R : r \u2265 0} is a measure if the following hold:\n1. \u00b5(\u2205) = 0\n2. If Ai \u2208 S for all i \u2208 N and Ai \u2229Aj = \u2205 whenever i 6= j, then\n\u00b5\n(\n\u22c3 i\u2208N Ai\n)\n= \u2211\ni\u2208N \u00b5(Ai)\nThe triple (X,S, \u00b5) is called a measure space. If in addition, \u00b5 satisfies \u00b5(X) = 1, then \u00b5 is a probability measure and (X,S, \u00b5) is called a probability space.\nGiven a probability space (X,S, \u00b5), one can measure the difference between two subsets A,B \u2208 S of X by looking at their symmetric difference A \u25b3 B, which is indeed in S:\n\u00b5(A\u25b3B) = \u00b5((A \u222aB) \\ (A \u2229 B)) = \u00b5(((X \\ A) \u2229 B) \u222a (A \u2229 (X \\B))).\nMore generally, given two measurable functions f, g : X \u2192 [0, 1], one can look at the expected value of their absolute difference by integrating with respect to \u00b5:\n\u222b\nX\n|f(x)\u2212 g(x)| d\u00b5(x).\nThis report does not go into any details involving the Lebesgue integral but does assume that integration of measurable functions to the real numbers, which is a measure space, makes sense and is linear and order-preserving:\n\u222b\nX\n(rf(x) + r\u2032g(x)) d\u00b5(x) = r\n\u222b\nX\nf(x) d\u00b5(x) + r\u2032 \u222b\nX\ng(x) d\u00b5(x)\nand \u222b\nX\nf(x) d\u00b5(x) \u2264 \u222b\nX\ng(x) d\u00b5(x),\nif f(x) \u2264 g(x) for all x \u2208 X . Validating hypotheses in the PAC learning model uses the idea of measuring the symmetric difference of two subsets of a probability space (X,S, \u00b5) and calculating the expected value of the difference of f, g : X \u2192 [0, 1]. The structure of metric spaces arises naturally from these two notions.\nMetric spaces\nDefinition 2.4. Let M be a nonempty set. A function d : M \u00d7M \u2192 R+ is a metric if the following hold for all m1, m2, m3 \u2208 M :\n1. d(m1, m2) = 0 if and only if m1 = m2\n2. d(m1, m2) = d(m2, m1)\n3. d(m1, m2) \u2264 d(m1, m3) + d(m3, m2)\nIn this case, the pair (M, d) is called a metric space.\nDefinition 2.5. Given a metric space (M, d), a metric sub-space of M (which is a metric space in its own right) is a nonempty subset M \u2032 \u2286 M equipped with the distance d|M\u2032 , the restriction of d to M \u2032.\nThe structure of a metric space exists in every vector space equipped with a norm.\nDefinition 2.6. Suppose V is a vector space over R. A function \u03c1 : V \u2192 R+ is a norm on V if for all v1, v2 \u2208 V and for all r \u2208 R,\n1. \u03c1(rv1) = |r|\u03c1(v1)\n2. \u03c1(v1 + v2) \u2264 \u03c1(v1) + \u03c1(v2)\n3. \u03c1(v1) = 0 if and only if v1 = 0\nIf \u03c1 is a norm on V , then (V, \u03c1) is called a normed vector space.\nProposition 2.7. Based on Definition 2.6, the function d : V \u00d7 V \u2192 R+ defined by d(u, v) = \u03c1(u\u2212 v) is a metric on V , and d is called the metric induced by the norm \u03c1 on V .\nThe following subsection provides a few examples of metric spaces which will be encountered in this report.\nExamples of metric spaces\nThe real numbers (R, \u03c1), with the absolute value norm \u03c1(r) = |r| for r \u2208 R, is a normed vector space so R can be equipped with a metric structure.\nExample 2.8. The set R with distance d defined by d(r1, r2) = |r1\u2212r2| for r1, r2 \u2208 R is a metric space.\nThe unit interval [0, 1] is a subset of R, so it is a metric sub-space of (R, d), and this space will be used quite often in this report.\nGiven a probability space (X,S, \u00b5), the set V of all bounded measurable functions from X to R is a vector space, with point-wise addition and scalar multiplication. The function \u03c1 : V \u2192 R+ defined by\n\u03c1(f) =\n\u221a\n( \u222b\nX\n(f(x))2d\u00b5(x)\n)\nis a norm on V if any two functions f, g : X \u2192 R which agree on a subset of X with full measure, \u00b5({x \u2208 X : f(x) = g(x)}) = 1, are identified.1 The norm \u03c1 is called the L2(\u00b5) norm on V and we normally write ||f ||2 = \u03c1(f) for f \u2208 V . As a result, V can be turned into a metric space.\nExample 2.9. Following the notations in the paragraph above, V is a metric space with distance d defined by\nd(f, g) = ||f \u2212 g||2 = \u221a ( \u222b\nX\n(f(x)\u2212 g(x))2d\u00b5(x) ) .\nWrite [0, 1]X for the set of all measurable functions from a probability space (X,S, \u00b5) to [0, 1]. Then, it is a metric sub-space of V with distance induced by the L2(\u00b5) norm on V , restricted of course to [0, 1]\nX . Given metric spaces (M1, d1), . . . , (Mk, dk), their product M1 \u00d7 . . . \u00d7Mk always\nhas a metric structure.\n1This identification can be done using an equivalence relation, so this report will not go into any details here.\nExample 2.10. If (M1, d1), . . . , (Mk, dk) are metric spaces, then their product M1 \u00d7 . . .\u00d7Mk is a metric space with distance d2 defined by\nd2((m1, . . . , mk), (m \u2032 1, . . . , m \u2032 k)) =\n\u221a\n((d1(m1, m \u2032 1)) 2 + . . .+ (dk(mk, m \u2032 k)) 2).\nThe distance d2 is normally referred to as the L2 product distance on M1\u00d7 . . .\u00d7Mk.\nFrom Examples 2.8 and 2.10, the set [0, 1]k, which denotes the set-theoretic product [0, 1]\u00d7 . . .\u00d7 [0, 1] is then a metric space with distance d2 defined by\nd2((r1, . . . , rk), (r \u2032 1, . . . , r \u2032 k)) =\n\u221a\n(|r1 \u2212 r\u20321|2 + . . .+ |rk \u2212 r\u2032k|2).\nAlso, following Examples 2.9 and 2.10, if F1, . . . ,Fk are sets of measurable functions from a probability space (X,S, \u00b5) to the unit interval, then Fi \u2286 [0, 1]X for each i = 1, . . . , k. Therefore, the product F1 \u00d7 . . . \u00d7 Fk is a metric space with distance defined by\nd2((f1, . . . , fk), (f \u2032 1, . . . , f \u2032 k)) =\n\u221a\n((||f1 \u2212 f \u20321||2)2 + . . .+ (||fk \u2212 f \u2032k||2)2)."}, {"heading": "3 The Probably Approximately Correct Learning", "text": "Model\nLet (X,S) be a measurable space. A concept class C of X is a subset of S and an element A \u2208 C (a measurable subset of X) is called a concept. A function class F is a collection of measurable functions from X to the unit interval [0, 1]. Unless stated otherwise, from this section onwards, the following notations will be used:\n1. X = (X,S): a measurable space\n2. \u00b5: a probability measure S \u2192 R+\n3. C: a concept class and F : a function class\n4. [0, 1]X : the set of all measurable functions f : X \u2192 [0, 1], instead of the customary notation of all functions from X to [0, 1].\nThis section provides the definitions of learning C and F in the Probably Approximately Correct (PAC) learning model, introduced in 1984 by Valiant.\nConcept class PAC learning involves producing a valid hypothesis for every concept A \u2208 C by first drawing random points, forming a training sample, from X labeled with whether these points are contained in A. In other words, a labeled sample of m points x1, . . . , xm \u2208 X for A consists of these points and the evaluations \u03c7A(x1), . . . , \u03c7A(xm) of the indicator function \u03c7A : X \u2192 {0, 1}, where\n\u03c7A(x) = 1 if and only if x \u2208 A.\nOn the other hand, an unlabeled sample of points does not include these evaluations. The set of all labeled samples of m points can then be identified with (X \u00d7{0, 1})m, and producing a hypothesis for A with a labeled sample is exactly the process of associating the sample to a concept H \u2208 C (i.e. this process is a function from the set of all labeled samples to the concept class).\nHere is the precise definition of a concept class being learnable.\nDefinition 3.1 ([16]). A concept class C is distribution-free Probably Approximately Correct learnable if there exists an algorithm2 L : \u222am\u2208N(X \u00d7 {0, 1})m \u2192 C with the following property: for every \u01eb > 0, for every \u03b4 > 0, there exists a M \u2208 N such that for every A \u2208 C, for every probability measure \u00b5, for every m \u2265 M , for any x1, . . . , xm \u2208 X, we have \u00b5(Hm \u25b3 A) < \u01eb with confidence at least 1 \u2212 \u03b4, where Hm = L((x1, \u03c7A(x1)), . . . , (xm, \u03c7A(xm))).\nConfidence of at least 1\u2212\u03b4 in the definition above, keeping to the same notations, simply means that the (product) measure of the set of allm-tuples (x1, . . . , xm) \u2208 Xm,\n2In this report, a learning algorithm is simply defined to be a function.\nwhere \u00b5(Hm \u25b3 A) < \u01eb for Hm = L((x1, \u03c7A(x1)), . . . , (xm, \u03c7A(xm))), is at least 1 \u2212 \u03b4. In other words, an equivalent statement to C is distribution-free PAC learnable is that for every \u01eb, \u03b4 > 0, there exists M \u2208 N such that for every A \u2208 C, probability measure \u00b5, and m \u2265 M ,\n\u00b5m({(x1, . . . , xm) \u2208 Xm : \u00b5(Hm \u25b3 A) \u2265 \u01eb}) \u2264 \u03b4, 3\nfor Hm = L((x1, \u03c7A(x1)), . . . , (xm, \u03c7A(xm))). A concept class C is distribution-free learnable in the PAC learning model if a hypothesis H can always be constructed from an algorithm L for every concept A \u2208 C, using any labeled sample for A, such that the measure of their symmetric difference H \u25b3 A is arbitrarily small with respect to every probability measure and with arbitrarily high confidence, as long as the sample size is large enough.\nEvery concept A \u2208 C is a subset of X so A can be associated to its indicator function \u03c7A : X \u2192 {0, 1}. Even more generally, \u03c7A is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {\u03c7A : X \u2192 [0, 1] : A \u2208 C}, so it is natural to generalize Definition 3.1 for any function class F .\nDefinition 3.1 involves the symmetric difference of two concepts and its generalization to measurable functions f, g : X \u2192 [0, 1] is the expected value of their absolute difference E\u00b5(f, g), as seen in the previous section:\nE\u00b5(f, g) =\n\u222b\nX\n|f(x)\u2212 g(x)| d\u00b5(x).\nA simple exercise can show that if f, g \u2208 [0, 1]X take values in {0, 1}, so they are indicator functions of two concepts A,B \u2286 X , then E\u00b5(f, g) coincide with the measure of their symmetric difference: E\u00b5(f, g) = \u00b5(A\u25b3 B), where f = \u03c7A and g = \u03c7B.\nWith the generalization of the symmetric difference, distribution-free PAC learning for any function class can be defined. In the context of function class learning, a labeled sample of m points x1, . . . , xm \u2208 X for a function f \u2208 F consists of these points and the evaluations f(x1), . . . , f(xm). Then, the set of all labeled samples of m points can be identified with (X \u00d7 [0, 1])m, and producing a hypothesis is the process of associating a labeled sample to a function H \u2208 F (just as in concept class learning).\nDefinition 3.2 ([18]). A function class F is distribution-free Probably Approximately Correct learnable if there exists an algorithm L : \u222am\u2208N(X \u00d7 [0, 1])m \u2192 F with the following property: for every \u01eb > 0, for every \u03b4 > 0, there exists a M \u2208 N such that for every f \u2208 F , for every probability measure \u00b5, for every m \u2265 M , for any x1, . . . , xm \u2208 X, we have E\u00b5(Hm, f) < \u01eb with confidence at least 1 \u2212 \u03b4, where Hm = L((x1, f(x1)), . . . , (xm, f(xm))).\n3The symbol \u00b5m denotes the product measure on Xm; the reader can refer to [6] for the details.\nBoth definitions of PAC learning contain the \u01eb and \u03b4 parameters. The error parameter \u01eb is used because the hypothesis is not required to have zero error - only an arbitrarily small error. The risk parameter \u03b4 exists because there is no guarantee that any collection of sufficiently large training points leads to a valid hypothesis; the learning algorithm is only expected to produce a valid hypothesis with the sample points with confidence at least 1\u2212 \u03b4. Hence, the name \u201cProbably (\u03b4) Approximately (\u01eb) Correct\u201d is used [8].\nThe following example illustrates that the set of all axis-aligned rectangles in R2 is distribution-free PAC learnable. Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].\nExample 3.3. In X = R2, the concept class C = {[a, b] \u00d7 [c, d] : a, b, c, d \u2208 R} is distribution-free PAC learnable.\nProof. Let \u01eb, \u03b4 > 0. Given a concept A and any sample of m training points x1, . . . , xm \u2208 X , define the hypothesis concept Hm to be the intersection of all rectangles containing only training points xi such that \u03c7A(xi) = 1. In other words, Hm is the smallest rectangle that contains only the sample points in A.\nLet \u00b5 be any probability measure, and in fact, Hm \u25b3 A = A \\Hm, which can be broken down into four sections T1, . . . , T4. If we can conclude that\n\u00b5\n(\n4 \u22c3\ni=1\nTi\n)\n< \u01eb,\nwith confidence at least 1\u2212 \u03b4, then the proof is complete. Consider the top section T1 and define T\u03031 to be the rectangle along the top parts of A whose measure is exactly \u01eb/4. The event T\u03031 \u2286 T1, which is equivalent to \u00b5(T1) \u2265 \u01eb/4, holds exactly when no points in the sample x1, . . . , xm fall in T\u03031, and the probability of this event (which is the measure of all such m-tuples of (x1, . . . , xm) \u2208 Xm where xi /\u2208 T\u03031 for all i = 1, . . . , m) is\n( 1\u2212 \u01eb 4 )m .\nSimilarly, the same holds for the other three sections T2, . . . , T4. Therefore, the probability that there exists at least one Ti such that \u00b5(Ti) \u2265 \u01eb/4, where i \u2208 {1, . . . , 4}, is at most\n4 ( 1\u2212 \u01eb 4 )m .\nHence, as long as we pick m large enough that 4(1 \u2212 \u01eb/4)m \u2264 \u03b4, with confidence (probability) at least 1\u2212 \u03b4, \u00b5(Ti) < \u01eb/4 for every i = 1, . . . , 4 and thus,\n\u00b5(Hm \u25b3 A) = \u00b5 ( 4 \u22c3\ni=1\nTi\n)\n\u2264 \u00b5(T1) + . . .+ \u00b5(T4) < 4 ( \u01eb\n4\n)\n= \u01eb.\nPlease note that this argument, though very intuitive, actually requires the classical Glivenko-Cantelli theorem.\nIn summary, as long as m \u2265 (4/\u01eb) ln(4/\u03b4), with confidence at least 1\u2212 \u03b4, \u00b5(Hm\u25b3 A) < \u01eb. We note that this estimate of the sample size only depends on \u01eb and \u03b4, so C is indeed distribution-free PAC learnable.\nIn the next section, a fundamental theorem which characterizes concept class distribution-free PAC learning will be stated, and two more concept classes, one learnable and the other not,4 will be given. However, in order to state this theorem, the notion of shattering, which is essential in learning theory, must be introduced.\n4They are direct results of the theorem."}, {"heading": "4 The Vapnik-Chervonenkis Dimension", "text": "The Vapnik-Chervonenkis dimension is a combinatorial parameter which is defined using the notion of shattering, developed first in 1971 by Vapnik and Chervonenkis.\nDefinition 4.1 ([17]). Given any set X and a collection A of subsets of X, the collection A shatters a subset S \u2286 X if for every B \u2286 S, there exists A \u2208 A such that\nA \u2229 S = B.\nThere is an equivalent condition, which is sometimes easier to work with, to shattering, expressed in terms of characteristic functions of subsets of X .\nProposition 4.2. The collection A shatters a subset S = {x1, . . . , xn} \u2286 X if and only if for every e = (e1, . . . , en) \u2208 {0, 1}n, there exists A \u2208 A such that\n\u03c7A(xi) = ei,\nfor all i = 1, . . . , n.\nProof. Trivial.\nDefinition 4.3 ([17]). The Vapnik-Chervonenkis (VC) dimension of the collection A, denoted by VC(A), is defined to be the cardinality of the largest finite subset S \u2286 X shattered by A. If A shatters arbitrarily large finite subsets of X, then the VC dimension of A is defined to be \u221e.\nThe VC dimension is defined for every collection A of subsets of any set X , so in particular, X = (X,S) can be a measurable space and A = C can be a concept class.\nThe following are a few examples of how to calculate VC dimensions in the context of X = Rn. In order to prove the VC dimension of a concept class C is d, we must provide a subset S \u2286 X with cardinality d which is shattered by C and prove that no subset with cardinality d+ 1 can be shattered by C.\nExample 4.4. If X = R, then the powerset of X has infinite VC dimension. More generally, for every infinite set X, VC(P(X)) = \u221e.\nExample 4.5. In the space X = R, let C = {[a, b] : a, b \u2208 R, a < b} be the collection of all closed intervals. Then, VC(C) = 2.\nProof. Consider the subset S = {1, 2} \u2286 R; C shatters S because\n[a, b] \u2229 S =\n\n  \n   \u2205 if a > 2 or b < 1 {1} if a \u2264 1, b < 2 {2} if a > 1, b \u2265 2 {1, 2} if a \u2264 1, b \u2265 2.\nOn the other hand, given any subset S = {x, y, z} \u2286 R with three distinct points, and assume the order to be x < y < z. Then, there are no closed interval in C containing x and z but not y.\nExample 4.6. Consider the space X = Rn. A hyperplane H~a,b is defined by a nonzero vector ~a = (a1, . . . , an) \u2208 Rn and a scalar b \u2208 R:\nH~a,b = {~x = (x1, . . . , xn) \u2208 Rn : ~x \u00b7 ~a = b} = {~x = (x1, . . . , xn) \u2208 Rn : x1a1 + . . .+ xnan = b}.\nWrite C as the set of all hyperplanes: C = {H~a,b : ~a \u2208 Rn \\ {~0}, b \u2208 R}. Then VC(C) = n. Proof. Consider the subset S = {~e1, . . . , ~en} \u2286 Rn, where ~ei is the vector with 1 on the i-th component and 0 everywhere else. Suppose B \u2286 S and there are two cases to consider:\n1. If B = \u2205, then let ~a = (1, 1, . . . , 1) \u2208 Rn and the hyperplane H~a,\u22121 = {~x = (x1, . . . , xn) \u2208 Rn : x1 + . . .+ xn = \u22121} is disjoint from S.\n2. If B 6= \u2205, then set ~a = (a1, . . . , an) \u2208 Rn \\ {~0}, where ai = \u03c7B(~ei). Then the hyperplane H~a,1 = {~x = (x1, . . . , xn) \u2208 Rn : x1a1 + . . .+ xnan = 1} satisfies\nH~a,1 \u2229 S = B.\nMoreover, no subset S = {~x1, . . . , ~xn, ~xn+1} \u2286 Rn with cardinality n + 1 can be shattered by C. At best, there exists a unique hyperplane H~a,b containing n of these points, say {~x1, . . . , ~xn}, so if ~xn+1 \u2208 H~a,b, then there are no hyperplanes that include ~x1, . . . , ~xn, but not ~xn+1. Otherwise, if ~xn+1 /\u2208 H~a,b, then there are no hyperplanes that include ~x1, . . . , ~xn, ~xn+1.\nThe first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.6, is a new result.\nA very important concept related to shattering is the growth of all the possible subsets A \u2229 S, for A \u2208 C, as S \u2286 X increases in size. It is clear that this growth is always exponential if C has infinite VC dimension; Sauer\u2019s Lemma explains the growth when VC(C) < \u221e."}, {"heading": "4.1 Sauer\u2019s Lemma", "text": "Given a concept class C of X , another way to express that C shatters a subset S \u2286 X , with cardinality n, is to consider the set of all A\u2229S, where A \u2208 C. Following Chapter 4 of [18], C shatters S if and only if\n|{A \u2229 S : A \u2208 C}| = 2n.\nMore generally, for any subset S \u2286 X , define\n\u03c0(S; C) = |{A \u2229 S : A \u2208 C}|\nand \u03c0(n; C) = max\n|S|=n \u03c0(S; C).\nThen, the VC dimension of C can now be expressed in terms of the growth of \u03c0(n; C) as n gets large.\nProposition 4.7. Given a concept class C, the following conditions are equivalent:\n1. VC(C) \u2265 n;\n2. C shatters some subset S \u2286 X with cardinality n;\n3. \u03c0(n; C) = 2n.\nMoreover, the class C has infinite VC dimension if and only if \u03c0(n; C) = 2n for all n \u2208 N. Conversely, C has finite VC dimension, say VC(C) \u2264 d, if and only if \u03c0(n; C) < 2n for all n > d.\nProof. The proof follows from the fact that C shatters S if and only if \u03c0(S; C) = 2n.\nThe extremely interesting fact, as seen in the next theorem, is that if C has finite VC dimension d, then \u03c0(n; C) is bounded by a polynomial in n of degree d, for n \u2265 d. This result, called Sauer\u2019s Lemma, was first proven in 1972 by Sauer. In other words, as n gets large, \u03c0(n; C) is either always an exponential function with base 2 or eventually bounded by a polynomial function of a fixed degree.\nTheorem 4.8 (Sauer\u2019s Lemma [12]). Suppose a concept class C has finite VC dimension d. Then\n\u03c0(n; C) \u2264 (en\nd\n)d\n,\nfor all n \u2265 d \u2265 1.\nOf course, everything in this subsection, including Sauer\u2019s Lemma, is true for any collection of subsets of any set but in the context of statistical learning theory, Sauer\u2019s Lemma is particularly useful because it is used to prove the equivalence of a concept class having finite VC dimension and the class being distribution-free PAC learnable."}, {"heading": "4.2 Characterization of concept class distribution-free PAC", "text": "learning\nThe following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis\u2019 paper [17] in 1971 and the 1989 paper [5] by Blumer et al..\nTheorem 4.9 ([17] and [5]). Let C be a concept class of a measurable space (X,S). The following are equivalent:\n1. C is distribution-free Probably Approximately Correct learnable.\n2. VC(C) < \u221e.\nBoth directions of the proof require expressing the number of sample training points required for learning in terms of the VC dimension of C; Sauer\u2019s Lemma is used to provide a sufficient number of points required for learning in the direction 2) \u21d2 1).\nUsing Theorem 4.9, one can more easily determine whether a given concept class is distribution-free PAC learnable.\nExample 4.10. Let X be any infinite set. Then the powerset P(X) is not distributionfree PAC learnable.\nExample 4.11. The set of all hyperplanes C = {H~a,b : ~a \u2208 Rn \\ {~0}, b \u2208 R}, as defined in Example 4.6, is distribution-free PAC learnable.\nBoth examples come directly from the calculations of their concept classes\u2019 VC dimensions in Examples 4.4 and 4.6 and from Theorem 4.9.\nEvery concept class C can be viewed as a function class FC = {\u03c7A : X \u2192 [0, 1] : A \u2208 C}, as seen in Section 3, so a natural question is whether the notion of shattering can be generalized. Indeed, the next section introduces the Fat Shattering dimension of scale \u01eb, which is a generalization of the VC dimension."}, {"heading": "5 The Fat Shattering Dimension", "text": "Let \u01eb > 0 from this section onwards. A combinatorial parameter which generalizes the Vapnik-Chervonenkis dimension is the Fat Shattering dimension of scale \u01eb, defined first by Kearns and Schapire in 1994.\nThis dimension, assigned to function classes, involves the notion of \u01eb-shattering, but similar to the notion of (regular) shattering, it can be defined for any collection of functions f : X \u2192 [0, 1], where X is any set, but for sake of this report, the following sections (still) assume X = (X,S) is a measurable space and the collection of functions is a function class F . Definition 5.1 ([7]). Let F be a function class. Given a subset S = {x1, . . . , xn} \u2286 X, the class F \u01eb-shatters S, with witness c = (c1, . . . , cn) \u2208 [0, 1]n, if for every e \u2208 {0, 1}n, there exists f \u2208 F such that\nf(xi) \u2265 ci + \u01eb for ei = 1, and f(xi) \u2264 ci \u2212 \u01eb for ei = 0.\nDefinition 5.2 ([7]). The Fat Shattering dimension of scale \u01eb > 0 of F , denoted by fat\u01eb(F), is defined to be the cardinality of the largest finite subset of X that can be \u01eb-shattered by F . If F can \u01eb-shatter arbitrarily large finite subsets, then the Fat Shattering dimension of scale \u01eb of F is defined to be \u221e.\nWhen the function class F consists of only functions taking values in {0, 1}, then the Fat Shattering dimension of any scale \u01eb \u2264 1/2 of F agrees with the VC dimension of the corresponding collection of subsets of X , induced by the (indicator) functions in F . Proposition 5.3. Suppose a function class F consists of only binary functions f : X \u2192 {0, 1}. For every f \u2208 F , there exists a unique subset Af \u2286 X such that \u03c7Af = f . Moreover, write C = {Af : f \u2208 F} and VC(C) = fat\u01eb(F) for all \u01eb \u2264 0.5. Proof. The first statement, of the existence of a unique subset Af \u2286 X for every binary function f , is clear. Let \u01eb \u2264 0.5. To show that VC(C) = fat\u01eb(F), it suffices to prove that C shatters S = {x1, . . . , xn} if and only if F \u01eb-shatters S.\nThe equivalent condition to shattering as seen in Proposition 4.2 will be used. Suppose C shatters S and define c = (0.5, 0.5, . . . , 0.5) \u2208 [0, 1]n. For every e \u2208 {0, 1}n, there exists Af \u2208 C, where f \u2208 F , such that\n\u03c7Af (xi) = ei,\nfor all i = 1, . . . , n and thus,\nf(xi) = \u03c7Af (xi) = ei \u2265 0.5 + \u01eb for ei = 1\nand f(xi) = \u03c7Af (xi) = ei \u2264 0.5\u2212 \u01eb for ei = 0.\nConversely, suppose F \u01eb-shatters S, with witness c = (c1, . . . , cn) \u2208 [0, 1]n. Let e \u2208 {0, 1}n and there exists f \u2208 F such that\nf(xi) \u2265 ci + \u01eb for ei = 1, and f(xi) \u2264 ci \u2212 \u01eb for ei = 0,\nbut f is binary and \u01eb is strictly positive, so f(xi) \u2265 ci+ \u01eb implies f(xi) = 1 for ei = 1 and f(xi) \u2264 ci \u2212 \u01eb implies f(xi) = 0 for ei = 0. As a result, consider Af \u2208 C and\n\u03c7Af (xi) = f(xi) = ei\nfor all i = 1, . . . , n. Therefore, VC(C) = fat\u01eb(F).\nHere is an example of a commonly used function class which we proved, independent of any sources, to have infinite Fat Shattering dimension of scale \u01eb.\nExample 5.4. Let X = R+ and let F be the set of all continuous functions f : X \u2192 [0, 1]. Then fat\u01eb(F) = \u221e for all 0 < \u01eb \u2264 0.5.\nProof. Suppose 0 < \u01eb \u2264 0.5, and consider a collection of continuous [0, 1]-valued functions defined as follows. Given e \u2208 {0, 1}N, a countable binary sequence, define fe : X \u2192 [0, 1] by\nfe(x) =\n{\n1 if ei = 1\n0 if ei = 0,\nif x = i \u2208 N. Otherwise, for x \u2208 [m,m+ 1], with m \u2208 N,\nfe(x) =\n\n \n  \u2212(x\u2212m) + 1 if em = 1, em+1 = 0 (x\u2212m) if em = 0, em+1 = 1 em if em = em+1.\nFor each e \u2208 {0, 1}N, fe is continuous because it is defined as a step function of lines which agree on the overlaps. Write F = {fe : e \u2208 {0, 1}N} and F \u2286 F . To show that fat\u01eb(F) = \u221e, it suffices to prove that fat\u01eb(F ) = \u221e. Consider the subset S = {1, . . . , n} \u2286 X for any n \u2208 N, and the collection F \u01eb-shatters S with witness c = (0.5, 0.5, . . . , 0.5) \u2208 [0, 1]n: for each e \u2208 {0, 1}n, it can be extended to a countable binary sequence e\u0303, where e\u0303i = ei for all i = 1, . . . , n and e\u0303i = 0 otherwise. Then, it is clear that\nfe\u0303(xi) = 1 \u2265 ci + \u01eb for e\u0303i = 1, and f(xi) = 0 \u2264 ci \u2212 \u01eb for e\u0303i = 0,\nwith xi = i \u2208 S for i = 1, . . . , n.\nWith the generalization from a concept class to a function class, a natural question is whether the finiteness of the Fat Shattering dimension of all scales \u01eb for a function\nclass F is equivalent to F being distribution-free PAC learnable. This question is addressed in the following subsection."}, {"heading": "5.1 Sufficient condition for function class distribution-free", "text": "PAC learning\nOne direction of Theorem 4.9 can be generalized and stated in terms of the Fat Shattering dimension of scale \u01eb of a function class.\nTheorem 5.5 ([1] and [18]). Let F be a function class. If fat\u01eb(F) < \u221e for all \u01eb > 0, then F is distribution-free PAC learnable.\nHowever, the converse to Theorem 5.5 is false. There exists a distribution-free PAC learnable function class with infinite Fat Shattering dimension of some scale \u01eb.\nIn fact, for every concept class C with cardinality \u21350 or 2\u21350 , there is an associated function class FC defined as follows. Set up a bijection b : C \u2192 [0, 1/3] or to [0, 1/3]\u2229 Q, depending on the cardinality of C, and for every A \u2208 C, define a function fA : X \u2192 [0, 1] by fA(x) = \u03c7A(x) + (\u22121)\u03c7A(x)b(A). Now, write FC = {fA : A \u2208 C}. Note that FC can be thought of the collection of all indicator functions of A \u2208 C, except that each \u201cindicator\u201d function fA has two unique identifying points b(A) and 1\u2212 b(A), instead of simply 0 and 1. The following proposition provides many counterexamples to Theorem 5.5, which are much simpler than the one found in [18].\nThe construction of the function class FC and the proposition below are developed from an idea of Example 2.10 in [11].\nProposition 5.6. Let C be a concept class. The associated function class FC = {fA : A \u2208 C}, defined in the previous paragraph, is always distribution-free PAC learnable; this class has infinite Fat Shattering dimension of all scales \u01eb < 1/6 if C has infinite VC dimension.\nProof. The function class FC is distribution-free PAC learnable because every function fA \u2208 FC can be uniquely identified with just one point x0 \u2208 X in any labeled sample: fA(x0) \u2208 {b(A), 1\u2212 b(A)} uniquely determines A and thus, fA.\nFurthermore, suppose C has infinite VC dimension. Let n \u2208 N be arbitrary and because VC(C) = \u221e, there exists S = {x1, . . . , xn} such that C shatters S. Suppose \u01eb < 1/6 and we claim that FC \u01eb-shatters S with witness c = (0.5, . . . , 0.5) \u2208 [0, 1]n. Indeed, let e \u2208 {0, 1}n and there exists A \u2208 C such that\n\u03c7A(xi) = ei,\nfor all i = 1, . . . , n, by Proposition 4.2. As a result,\nfA(xi) = 1\u2212 b(A) \u2265 0.5 + \u01eb for ei = 1\nand fA(xi) = b(A) \u2264 0.5\u2212 \u01eb for ei = 0.\nConsequently, FC has infinite Fat Shattering dimension of all scales \u01eb < 1/6.\nThe next section explains the main result of our research: bounding the Fat Shattering dimension of scale \u01eb of a composition function class which is built with a continuous logic connective."}, {"heading": "6 The Fat Shattering Dimension of a Composition", "text": "Function Class\nThe goals of this section are to construct a new function class from old ones by means of a continuous logic connective and to bound the Fat Shattering dimension of scale \u01eb of the new function class in terms of the dimensions of the old ones. The following subsection provides this construction, which can be found in Chapter 4 of [18], in the context of concept classes using a connective of classical logic."}, {"heading": "6.1 Construction in the context of concept classes", "text": "Let C1, C2, . . . , Ck be concept classes, where k \u2265 2, and let u : {0, 1}k \u2192 {0, 1} be any function, commonly known as a connective of classical logic. A new collection of subsets of X arises from C1, . . . , Ck as follows.\nAs mentioned earlier in this report, every element A \u2208 Ci can be identified as a binary function f : X \u2192 {0, 1}, namely its characteristic function f = \u03c7A, and vice versa. Now, for any k functions f1, . . . , fk : X \u2192 {0, 1}, where fi \u2208 Ci with i = 1, . . . , k, consider a new function u(f1, . . . , fk) : X \u2192 {0, 1} defined by\nu(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)).\nThe set of all possible u(f1, . . . , fk), denoted by u(C1, . . . , Ck), is given by\nu(C1, . . . , Ck) = {u(f1, . . . , fk) : fi \u2208 Ci}.\nFor instance, when k = 2, we can consider the \u201cExclusive Or\u201d connective \u2295 : {0, 1}2 \u2192 {0, 1} defined by\np\u2295 q = (p \u2227 \u00acq) \u2228 (\u00acp \u2227 q),\nwhich corresponds to the symmetric difference operation. Then, our new concept class constructed from C1 and C2 is\n{A1 \u25b3 A2 : A1 \u2208 C1, A2 \u2208 C2}.\nThe next theorem states that if C1, C2, . . . , Ck all have finite VC dimension to start with, then regardless of u, the new collection u(C1, . . . , Ck) always has finite VC dimension.\nTheorem 6.1 ([18]). Let k \u2265 2. Suppose C1, . . . , Ck are concept classes, each viewed as a collection of binary functions, and u : {0, 1}k \u2192 {0, 1} is any function. If the VC\ndimension of Ci is finite for all i = 1, . . . , k. Then there exists a constant \u03b1 = \u03b1k5, which depends only on k, such that\nVC(u(C1, . . . , Ck)) < d\u03b1k,\nwhere d = k\nmax i=1\nVC(Ci).\nThe proof of this theorem can be found in [18] and uses Sauer\u2019s Lemma to bound the VC dimension of u(C1, . . . , Ck). The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale \u01eb, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1]k \u2192 [0, 1]."}, {"heading": "6.2 Construction of new function class with continuous logic", "text": "connective\nIn first-order logic, there are only two truth-values 0 or 1, so a connective is a function {0, 1}k \u2192 {0, 1} in the classical sense. However, in continuous logic, truth-values can be found anywhere in the unit interval [0, 1]. Therefore, we should consider a function u : [0, 1]k \u2192 [0, 1], which will transform function classes, and require that u be a continuous logic connective. In other words, u should be continuous from the (product) metric space [0, 1]k to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.\nThe following provides the definition of a uniformly continuous function u from any metric space to another, but we must first qualify u with a modulus of uniform continuity.\nDefinition 6.2 (See e.g. [19]). A modulus of uniform continuity is any function \u03b4 : (0, 1] \u2192 (0, 1].\nDefinition 6.3 (See e.g. [19]). Let (M1, d1) and (M2, d2) be two metric spaces. A function u : M1 \u2192 M2 is uniformly continuous if there exists (a modulus of uniform continuity) \u03b4 : (0, 1] \u2192 (0, 1] such that for all \u01eb \u2208 (0, 1] and m1, m2 \u2208 M1, if d1(m1, m2) < \u03b4(\u01eb), then d2(u(m1), u(m2)) < \u01eb.\nSuch a \u03b4 is called a modulus of uniform continuity for u.\nIn particular, u : [0, 1]k \u2192 [0, 1], where [0, 1]k is equipped with the L2 product distance d2, is uniformly continuous with modulus of uniform continuity \u03b4 if for every\n5More specifically, \u03b1 = \u03b1k is the smallest integer such that\nk < \u03b1\nlog(e\u03b1) .\n\u01eb \u2208 (0, 1] and for every (r1, . . . , rk), (r\u20321, . . . , r\u2032k) \u2208 [0, 1]k,\nd2((r1, . . . , rk), (r \u2032 1, . . . , r \u2032 k)) < \u03b4(\u01eb) \u21d2 |u(r1, . . . , rk)\u2212 u(r\u20321, . . . , r\u2032k)| < \u01eb.\nGiven function classes F1, . . . ,Fk and a uniformly continuous function u : [0, 1]k \u2192 [0, 1], consider the new function class u(F1, . . . ,Fk) defined by\nu(F1, . . . ,Fk) = {u(f1, . . . , fk) : fi \u2208 Fi},\nwhere u(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x)) for all x \u2208 X , just as in Section 6.1 for concept classes, with fi \u2208 Fi and i = 1, . . . , k. Our main result states that the Fat Shattering dimension of scale \u01eb of u(F1, . . . ,Fk) is bounded by a sum of the Fat Shattering dimensions of scale \u03b4(\u01eb, k) of F1, . . . ,Fk, where \u03b4(\u01eb, k) is a function of the modulus of uniform continuity \u03b4(\u01eb) for u and k. It is a known result, seen in Chapter 5 of [18], that this new class u(F1, . . . ,Fk) has finite Fat Shattering dimension of all scales \u01eb > 0 (and thus, it is distribution-free PAC learnable) if each of F1, . . . ,Fk has finite Fat Shattering dimension of all scales, but no bounds were known."}, {"heading": "6.3 Main Result", "text": "Fix k \u2265 2 and the following theorem is our main new result.\nTheorem 6.4. Let \u01eb > 0, F1, . . . ,Fk be function classes of X, and u : [0, 1]k \u2192 [0, 1] be a uniformly continuous function with modulus of continuity \u03b4(\u01eb). Then\nfat\u01eb(u(F1, . . . ,Fk)) \u2264 (\nK log(4c\u2032k \u221a k/(\u03b4(\u01eb/(2c\u2032))\u01eb))\nK \u2032 log(2)\n)\nn \u2211\ni=1\nfat c \u03b4(\u01eb/(2c \u2032))\u01eb k \u221a k\n(Fi),\nwhere c, c\u2032, K,K \u2032 are some absolute constants.\nExtracting the actual values of these absolute constants is not easy, and we hope to find them in future research. For this reason, comparing the bound in Theorem 6.4 with the existing estimate for the VC dimension of a composition concept class is difficult; however, in statistical learning theory, estimates for function class learning are generally much worse than estimates for concept class learning.\nIn order to prove Theorem 6.4, for clarity, we first introduce an auxiliary function \u03c6 : F1 \u00d7 . . . \u00d7 Fk \u2192 [0, 1]X , which is uniformly continuous from the metric space F1\u00d7 . . .\u00d7Fk with the L2 product distance d\u03032 to the metric space [0, 1]X with distance induced by the L2(\u00b5) norm, and prove the following lemma.\nLemma 6.5. Let \u01eb > 0, F1, . . . ,Fk be function classes of X, and \u03c6 : F1\u00d7 . . .\u00d7Fk \u2192 [0, 1]X be uniformly continuous with some modulus of continuity \u03b4(\u01eb, k), a function\nof \u01eb and k. Then\nfatc\u2032\u01eb(\u03c6(F1 \u00d7 . . .\u00d7 Fk)) \u2264 (\nK log(2 \u221a k/\u03b4(\u01eb, k))\nK \u2032 log(2)\n)\nk \u2211\ni=1\nfat c \u03b4(\u01eb,k)\u221a\nk\n(Fi),\nwhere c, c\u2032, K,K \u2032 are some absolute constants and the symbol \u03c6(F1\u00d7 . . .\u00d7Fk) simply represents the image of \u03c6.\nThen, we will relate the two uniformly continuous functions u and \u03c6.\nLemma 6.6. Let \u01eb > 0. If u : [0, 1]k \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then the function \u03c6 : F1 \u00d7 . . .\u00d7 Fk \u2192 [0, 1]X defined by\n\u03c6(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis also uniformly continuous with modulus of continuity \u03b4(\u01eb/2)\u01eb 2k , and in fact, \u03c6(F1 \u00d7 . . .\u00d7Fk) = u(F1, . . . ,Fk)."}, {"heading": "6.4 Proofs", "text": "In order to prove Lemma 6.5, we first introduce the concept of an \u01eb-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale \u01eb by using results from Mendelson and Vershynin [9] and Talagrand [15].\nDefinition 6.7. Let \u01eb > 0 and suppose (M, d) is a metric space. The \u01eb-covering number, denoted by N(M, \u01eb, d), of M is the minimal number N such that there exists elements m1, m2, . . . , mN \u2208 M with the property that for all m \u2208 M , there exists i \u2208 {1, 2, . . . , N} for which\nd(m,mi) < \u01eb.\nThe set {m1, m2, . . . , mN} is called a (minimal) \u01eb-net of M . The following proposition relates the \u01eb-covering number of a product of metric spaces, with the L2 product distance d 2, M1 \u00d7 . . .\u00d7Mk to the \u01eb\u221ak -covering number of each space Mi.\nProposition 6.8. Let \u01eb > 0 and suppose (M1, d1), . . . , (Mk, dk) are metric spaces, each with finite \u01eb\u221a\nk -covering numbers, Ni = N(Mi, \u01eb\u221a k , di) for i = 1, . . . , k. Then\nN(M1 \u00d7 . . .\u00d7Mk, \u01eb, d2) \u2264 k \u220f\ni=1\nNi.\nProof. Let Ci = {ai1, . . . , aiNi} be a minimal \u01eb\u221ak -net for Mi with respect to distance di, where i = 1, . . . , k and suppose (a 1, . . . , ak) \u2208 M1 \u00d7 . . . \u00d7 Mk. Then, for each\ni = 1, . . . , k, there exists aiji \u2208 Ci, where 1 \u2264 ji \u2264 Ni such that di(ai, aiji) < \u01eb\u221ak . Hence,\nd2((a1, . . . , ak), (a1j1, . . . , a k jk )) =\n\u221a\n(\n(d1(a1, a1j1)) 2 + . . .+ (dk(ak, akjk))\n2 )\n<\n\u221a \u221a \u221a \u221a ( (\n\u01eb\u221a k\n)2\n+ . . .+\n(\n\u01eb\u221a k\n)2 )\n= \u01eb,\nwhere each (a1j1 , . . . , a k jk ) \u2208 C1 \u00d7 . . . \u00d7 Ck, which has cardinality \u03a0ki=1Ni. Therefore, N(M1 \u00d7 . . .\u00d7Mk, \u01eb, d2) \u2264 \u03a0ki=1Ni.\nAlso, if u : M1 \u2192 M2 is any uniformly continuous function with a modulus of uniform continuity \u03b4(\u01eb) from any metric space to another, then the image of a minimal \u03b4(\u01eb)-net of M1 under u becomes an \u01eb-net for u(M1).\nProposition 6.9. Let \u01eb > 0 and suppose (M1, d1) and (M2, d2) are two metric spaces. If a function u : M1 \u2192 M2 is uniformly continuous with a modulus of continuity \u03b4(\u01eb), then N(u(M1), \u01eb, d2) \u2264 N(M1, \u03b4(\u01eb), d1), where u(M1) denotes the image of u.\nProof. Suppose N = N(M1, \u03b4(\u01eb), d1) is the \u03b4(\u01eb)-covering number for M1 and let {m1, . . . , mN} be a \u03b4(\u01eb)-net for M1. Hence for every u(m) \u2208 u(M1), where m \u2208 M1, there exists i \u2208 {1, . . . , N} such that\nd1(m,mi) < \u03b4(\u01eb),\nwhich implies d2(u(m), u(mi)) < \u01eb as u is uniformly continuous. As a result, the set\n{u(m1), . . . , u(mN)}\nis an \u01eb-net for u(M1), so\nN(u(M1), \u01eb, d2) \u2264 N(M1, \u03b4(\u01eb), d1).\nIn particular, we can view F1, . . . ,Fk as metric spaces, all with distances induced by the L2(\u00b5) norm and suppose \u03c6 : F1 \u00d7 . . .\u00d7 Fk \u2192 [0, 1]X is uniformly continuous with modulus of continuity \u03b4(\u01eb, k). Then, by Proposition 6.8, if F1, . . . ,Fk all have finite \u03b4(\u01eb,k)\u221a\nk -covering numbers, the metric space F1 \u00d7 . . . \u00d7 Fk, with the L2 product\nmetric d\u03032, also has a finite \u03b4(\u01eb, k)-covering number: if we write N(Fi, \u03b4(\u01eb,k)\u221ak , L2(\u00b5)) as\nthe \u03b4(\u01eb,k)\u221a k -covering number for Fi, then,\nN(F1 \u00d7 . . .\u00d7 Fk, \u03b4(\u01eb, k), d\u03032) \u2264 k \u220f\ni=1\nN(Fi, \u03b4(\u01eb, k)\u221a\nk , L2(\u00b5)).\nNow, by Proposition 6.9,\nN(\u03c6(F1 \u00d7 . . .\u00d7 Fk), \u01eb, L2(\u00b5)) \u2264 N(F1 \u00d7 . . .\u00d7 Fk, \u03b4(\u01eb, k), d\u03032)\n\u2264 k \u220f\ni=1\nN(Fi, \u03b4(\u01eb, k)\u221a\nk , L2(\u00b5)).\nIn other words, the \u01eb-covering number for \u03c6(F1\u00d7. . .\u00d7Fk) is bounded by a product of the \u03b4(\u01eb,k)\u221a\nk -covering numbers of each Fi. To prove Lemma 6.5, we now state the main\ntheorem of a paper written by Mendelson and Vershynin, which relates the \u01eb-covering number of a function class to its Fat Shattering dimension of scale \u01eb.\nTheorem 6.10 ([9]). Let \u01eb > 0 and let F be a function class. Then for every probability measure \u00b5,\nN(F , \u01eb, L2(\u00b5)) \u2264 ( 2\n\u01eb\n)Kfatc\u01eb(F)\nfor absolute constants c,K.\nAnd Talagrand provides the converse.\nTheorem 6.11 ([15]). Following the notations of Theorem 6.10, there exists a probability measure \u00b5 such that\nN(F , \u01eb, L2(\u00b5)) \u2265 2K \u2032fatc\u2032\u01eb(F),\nfor absolute constants c\u2032, K \u2032.\nProof of Lemma 6.5. By Propositions 6.8 and 6.9,\nN(\u03c6(F1 \u00d7 . . .\u00d7 Fk), \u01eb, L2(\u00b5)) \u2264 k \u220f\ni=1\nN(Fi, \u03b4(\u01eb, k)\u221a\nk , L2(\u00b5)),\nso\nlog(N(\u03c6(F1 \u00d7 . . .\u00d7 Fk), \u01eb, L2(\u00b5))) \u2264 k \u2211\ni=1\nlog(N(Fi, \u03b4(\u01eb, k)\u221a\nk , L2(\u00b5))).\nBy Theorem 6.10,\nlogN(Fi, \u03b4(\u01eb, k)\u221a\nk , L2(\u00b5)) \u2264 Kfatc \u03b4(\u01eb,k)\u221a k\n(Fi) log(2 \u221a k/\u03b4(\u01eb, k)),\nfor any probability measure \u00b5 where c,K are absolute constants. Moreover, by Theorem 6.11 for some probability measure \u00b5 and absolute constants c\u2032, K \u2032,\nlog(N(\u03c6(F1 \u00d7 . . .\u00d7 Fk), \u01eb, L2(\u00b5))) \u2265 K \u2032fatc\u2032\u01eb(\u03c6(F1 \u00d7 . . .\u00d7 Fk)) log(2)\nand altogether,\nfatc\u2032\u01eb(\u03c6(F1 \u00d7 . . .\u00d7Fk)) \u2264 \u2211k i=1Kfatc \u03b4(\u01eb,k)\u221a k\n(Fi) log(2 \u221a k/\u03b4(\u01eb, k))\nK \u2032 log(2)\n=\n( K log(2 \u221a k/\u03b4(\u01eb, k))\nK \u2032 log(2)\n)\nk \u2211\ni=1\nfat c \u03b4(\u01eb,k)\u221a\nk\n(Fi).\nNow, all that is left is to prove Lemma 6.6.\nProof of Lemma 6.6. Suppose u : [0, 1]k \u2192 [0, 1] is uniformly continuous with a modulus of continuity \u03b4(\u01eb), where [0, 1]k is a metric space with the L2 product distance d2. We claim that the function \u03c6 : F1 \u00d7 . . .\u00d7Fk \u2192 [0, 1]X defined by\n\u03c6(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis uniformly continuous with modulus of continuity \u03b4(\u01eb/2)\u01eb 2k . Let \u01eb > 0 and\n(f1, . . . , fk), (f \u2032 1, . . . , f \u2032 k) \u2208 F1 \u00d7 . . .\u00d7Fk.\nSuppose\nd\u03032((f1, . . . , fk), (f \u2032 1, . . . , f \u2032 k)) =\n\u221a\n((||f1 \u2212 f \u20321||2)2 + . . .+ (||fk \u2212 f \u2032k||2)2)\n< \u03b4(\u01eb/2)\u01eb\n2k =\n\u221a\n\u03b4(\u01eb/2)2(\u01eb/2)2\nk2 .\nHence, for each i = 1, . . . , k,\n||fi \u2212 f \u2032i ||2 = \u221a ( \u222b\nX\n(fi(x)\u2212 f \u2032i(x))2 d\u00b5(x) ) <\n\u221a\n\u03b4(\u01eb/2)2(\u01eb/2)2\nk2 .\nWrite Ai = {x \u2208 X : |fi(x) \u2212 f \u2032i(x)| \u2265 \u221a \u03b4(\u01eb/2)2 k } and we must have that \u00b5(Ai) <\n(\u01eb/2)2\nk , for each i = 1, . . . , k. Otherwise,\n\u222b\nX\n(fi(x)\u2212 f \u2032i(x))2 d\u00b5(x) = \u222b\nAi\n(fi(x)\u2212 f \u2032i(x))2 d\u00b5(x) + \u222b\nX\\Ai (fi(x)\u2212 f \u2032i(x))2 d\u00b5(x)\n\u2265 \u222b\nAi\n( \u221a\n\u03b4(\u01eb/2)2\nk\n)2\nd\u00b5(x) +\n\u222b\nX\\Ai (fi(x)\u2212 f \u2032i(x))2 d\u00b5(x)\n= \u00b5(Ai)\n( \u221a\n\u03b4(\u01eb/2)2\nk\n)2\n+\n\u222b\nX\\Ai (fi(x)\u2212 f \u2032i(x))2 d\u00b5(x)\n\u2265 (\u01eb/2) 2\nk\n\u03b4(\u01eb/2)2\nk +\n\u222b\nX\\Ai (fi(x)\u2212 f \u2032i(x))2 d\u00b5(x)\n\u2265 \u03b4(\u01eb/2) 2(\u01eb/2)2\nk2 ,\nwhich is a contradiction. Now, write A = A1 \u222a . . . \u222a Ak and we have that X \\ A = {x \u2208 X : |fi(x) \u2212 f \u2032i(x)| < \u221a \u03b4(\u01eb/2)2 k , for all i = 1, . . . , k}. Suppose x \u2208 X \\ A and then\nd2((f1(x), . . . , fk(x)), (f \u2032 1(x), . . . , f \u2032 k(x))) =\n\u221a\n|f1(x)\u2212 f \u20321(x)|2 + . . .+ |fk(x)\u2212 f \u2032k(x)|2\n<\n\u221a\n(\n\u03b4(\u01eb/2)2\nk + . . .+\n\u03b4(\u01eb/2)2\nk\n)\n< \u03b4(\u01eb/2).\nConsequently, by the uniform continuity of u, for all x \u2208 X \\ A,\n|u(f1(x), . . . , fk(x))\u2212 u(f \u20321(x), . . . , f \u2032k(x))| < \u01eb/2.\nFinally,\n||\u03c6(f1, . . . , fk)\u2212 \u03c6(f \u20321, . . . , f \u2032k)||2 = \u221a ( \u222b\nX\n(u(f1(x), . . . , fk(x))\u2212 u(f \u20321(x), . . . , f \u2032k(x)))2 d\u00b5(x) )\n\u2264 \u221a ( \u222b\nX\\A (u(f1(x), . . . , fk(x))\u2212 u(f \u20321(x), . . . , f \u2032k(x)))2 d\u00b5(x)\n)\n+\n\u221a\n( \u222b\nA\n(u(f1(x), . . . , fk(x))\u2212 u(f \u20321(x), . . . , f \u2032k(x)))2 d\u00b5(x) )\n<\n\u221a\n( \u222b\nX\\A (\u01eb/2)2 d\u00b5(x)\n)\n+\n\u221a\n( \u222b\nA\n1 d\u00b5(x)\n)\n\u2264 (\u01eb/2) + (\u01eb/2) = \u01eb,\nas \u00b5(A) \u2264 \u2211ki=1 \u00b5(Ai) \u2264 k ( (\u01eb/2)2 k ) = (\u01eb/2)2.\nNow we will prove our main theorem.\nProof of Theorem 6.4. By Lemma 6.6, if u : [0, 1]k \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then \u03c6 : F1 \u00d7 . . .\u00d7 Fk \u2192 [0, 1]X defined by\n\u03c6(f1, . . . , fk)(x) = u(f1(x), . . . , fk(x))\nis also uniformly continuous with modulus of continuity \u03b4(\u01eb/2)\u01eb 2k . Then, apply Lemma 6.5 with \u03b4(\u01eb, k) = \u03b4(\u01eb/2)\u01eb 2k\nand with a simple change of variables c\u2032\u01eb\u2032 \u2192 \u01eb, Theorem 6.4 follows directly.\nAltogether, we can summarize the maps in this section in the following two diagrams (where i is the diagonal map):\nX i // Xk f1\u00d7...\u00d7fk // [0, 1]k u // [0, 1] ,\nwhile\nF1 \u00d7 . . .\u00d7 Fk \u03c6 // [0, 1]X .\nThis result is potentially useful because it allows us to construct new function classes using common continuous logic connectives and bound their Fat Shattering dimensions of scale \u01eb. For instance, the function u : [0, 1]2 \u2192 [0, 1] defined by u(r1, r2) = r1 \u00b7 r2 (multiplication) is uniformly continuous with a modulus of continuity \u03b4(\u01eb) = \u01eb\n2 . Indeed, let \u01eb > 0 and consider (r1, r2), (r \u2032 1, r \u2032 2) \u2208 [0, 1]2. Suppose\nd2((r1, r2), (r \u2032 1, r \u2032 2)) < \u03b4(\u01eb) = \u01eb 2 , so\n|r1 \u2212 r\u20321| < \u221a |r1 \u2212 r\u20321|2 + |r2 \u2212 r\u20322|2 < \u01eb\n2\nand similarly, |r2 \u2212 r\u20322| < \u01eb2 . Then,\n|u(r1, r2)\u2212 u(r\u20321, r\u20322)| = |r1r2 \u2212 r\u20321r\u20322| = |r1r2 \u2212 r1r\u20322 + r1r\u20322 \u2212 r\u20321r\u20322| = |r1(r2 \u2212 r\u20322) + r\u20322(r1 \u2212 r\u20321)| \u2264 |r1(r2 \u2212 r\u20322)|+ |r\u20322(r1 \u2212 r\u20321)| \u2264 |r2 \u2212 r\u20322|+ |r1 \u2212 r\u20321| < \u01eb\n2 +\n\u01eb 2 = \u01eb.\nAs a result, if F1 and F2 are two function classes with finite Fat Shattering dimensions of some scale \u01eb, then the function class u(F1,F2) = F1F2 = {f1 \u00b7 f2 : f1 \u2208 F1, f2 \u2208 F2}, defined by point-wise multiplication, also has finite Fat Shattering dimension of scale \u01eb, up to some constant factor and Theorem 6.4 provides a precise bound.\nWe have made an interesting connection, which has not been explored much in the past, between continuous logic and PAC learning, and we plan to investigate this connection even further. For instance, the relationship of compositions of function classes and continuous logic may be interesting to study because compositions of uniformly continuous functions are again uniformly continuous. Furthermore, we can try to add some topological structures to concept classes to see how PAC learning can be affected. The next section provides a couple of other possible future research topics."}, {"heading": "7 Open Questions", "text": "The definitions of distribution-free PAC learning, for both concept and function classes, in Section 3, made no assumptions about probability measures, as a learning algorithm has to produce a valid hypothesis for any probability measure \u00b5. If we fix a probability measure \u00b5 and ask whether a concept class, or a function class, is PAC learnable, then we are working in the context of fixed distribution PAC learning.\nDefinition 7.1 ([18]). Let \u00b5 be a probability measure. A function class F is Probably Approximately Correct learnable under \u00b5 if there exists an algorithm L : \u222am\u2208N(X \u00d7 [0, 1])m \u2192 F with the following property: for every \u01eb > 0, for every \u03b4 > 0, there exists a M \u2208 N such that for every f \u2208 F , for every m \u2265 M , for any x1, . . . , xm \u2208 X, we have E\u00b5(Hm, f) < \u01eb with confidence at least 1\u2212 \u03b4, where\nE\u00b5(Hm, f) =\n\u222b\nX\n|f(x)\u2212 g(x)| d\u00b5(x)\nand Hm = L((x1, f(x1)), . . . , (xm, f(xm))).\nWhen a function class F consists of only binary functions, i.e. F = C is a concept class, there is a theorem, proved by Benedek and Itai in 1991, which gives a characterization of fixed distribution PAC learnability.\nTheorem 7.2 ([4]). Fix a probability measure \u00b5 and consider a concept class C. The following are equivalent:\n1. C is Probably Approximately Correct learnable under \u00b5.\n2. (Finite Metric Entropy condition) The \u01eb-covering number of C when viewed as a metric space with distance d = \u00b5( \u25b3 ) is finite for every \u01eb > 0.\nHowever, there is no characterization for fixed distribution PAC learnability of a general function class. Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure \u00b5 if and only if the class has no witness of irregularity, a property that involves shattering [13],[14]. Every GC function class is PAC learnable under \u00b5 [11], but the property of having no witness of irregularity is strictly stronger than PAC learnability. We would like to propose the following conjecture for a possible characterization.\nConjecture 7.3. Fix a probability measure \u00b5 and consider a function class F . Let \u01eb > 0. The following are equivalent:\n1. The function class F is PAC learnable under \u00b5 to accuracy \u01eb.6 6Being PAC learnable to accuracy \u01eb means Definition 7.1 is satisfied, but only for this particular\n\u01eb.\n2. There exists M,N and \u03b3 > 0 such that for all functions f \u2208 F , with probability at least \u03b3, the set {g \u2208 F : g|x\u0304N = f|x\u0304N } has an \u01eb-covering number, with respect to the distance d = E\u00b5( , ), of at most M , where x\u0304N denotes a sample of N points.\nA very interesting research topic is to study this conjecture and either prove or disprove it. Also, by Proposition 5.6, the finiteness of the Fat Shattering dimension of all scales \u01eb > 0 does not characterize function class PAC learning in the distributionfree case; consequently, another topic of research would be to come up with a new combinatorial parameter for a function class, related to the notion of shattering, which would characterize learning. This new parameter would have to solve the problem of unique identifications of functions, a problem that does not occur with concept classes.\nYet another possible research topic is to generalize the definitions of PAC learning and introduce observation noise, both in the fixed distribution and distribution-free cases. The paper [3] written by Bartlett et al. proves that the finiteness of the Fat Shattering dimension of all scales of a function class F is equivalent to F being distribution-free learnable under certain noise distributions. It would be interesting to generalize this result and/or apply it in the fixed distribution setting."}, {"heading": "8 Conclusion", "text": "This report introduces the definitions of Probably Approximately Correct learning for concept and function classes and defines the Vapnik-Chervonenkis dimension for concept classes and the Fat Shattering dimension of scale \u01eb > 0 for function classes. Finiteness of the VC dimension characterizes concept class distribution-free PAC learning; however, the finiteness of the Fat Shattering dimension of all scales \u01eb is still only sufficient for function class learning, and not necessary.\nGiven function classes F1, . . . ,Fk, one can construct a new class u(F1, . . . ,Fk) using a continuous function u : [0, 1]k \u2192 [0, 1], a continuous logic connective. The main new result of this report shows that the Fat Shattering dimension of scale \u01eb of u(F1, . . . ,Fk) is bounded by a sum of the Fat Shattering dimensions of scale \u03b4(\u01eb, k) of classes F1, . . . ,Fk, up to some absolute constants. This result can be useful because it allows us to construct new function classes, which may be very natural objects, and bound their Fat Shattering dimensions."}], "references": [{"title": "Scale-Sensitive Dimensions", "author": ["N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "Uniform Convergence, and Learnability. Journal of the ACM 44.4 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Math\u00e9matiques: Topologie et Analyse", "author": ["G. Auliac", "J.Y. Caby"], "venue": "3rd Ed. Belgium: EdiScience", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Fat-Shattering and the Learnability of Real-Valued Functions", "author": ["P.L. Bartlett", "P.M. Long", "R.C. Williamson"], "venue": "Journal of Computer and System Sciences 52.3 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Learnability with respect to Fixed Distributions", "author": ["G.M. Benedek", "A. Itai"], "venue": "Theoretical Computer Science 86.2 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Learnability and the Vapnik-Chervonenkis Dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth"], "venue": "Journal of the ACM 36.4 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1989}, {"title": "Measure Theory", "author": ["J.L. Doob"], "venue": "New York: Springer-Verlag", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Efficient Distribution-free Learning of Probabilistic Concepts", "author": ["M.J. Kearns", "R. Schapire"], "venue": "Journal of Computer System Sciences 48.3 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "An Introduction to Computational Learning Theory", "author": ["M.J. Kearns", "U.V. Vazirani"], "venue": "Cambridge, Massachusetts: The MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Entropy and the Combinatorial Dimension", "author": ["S. Mendelson", "R. Vershynin"], "venue": "Inventiones Mathematicae 152 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Indexability", "author": ["V. Pestov"], "venue": "Concentration, and VC Theory. An invited paper, Proc. of the 3rd International Conf. on Similarity Search and Applications ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A Note on Sample Complexity of Learning Binary Output Neural Networks Under Fixed Input Distributions", "author": ["V. Pestov"], "venue": "Proc. 2010 Eleventh Brazilian Symposium on Neural Networks, IEEE Computer Society, Los Alamitos-Washington-Tokyo ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "On the Densities of Families of Sets", "author": ["N. Sauer"], "venue": "J. Combinatorial Theory 13 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1972}, {"title": "The Glivenko-Cantelli Problem", "author": ["M. Talagrand"], "venue": "Annals of Probability 15 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "The Glivenko-Cantelli Problem", "author": ["M. Talagrand"], "venue": "Ten Years Later. J. Theoret. Probab. 9 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Vapnik-Chervonenkis Type Conditions and Uniform Donsker Classes of Functions", "author": ["M. Talagrand"], "venue": "Annals of Probability 31.3 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "A Theory of the Learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM 27.11 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1984}, {"title": "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Prob. and its Appl. 16.2 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1971}, {"title": "A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems", "author": ["M. Vidyasagar"], "venue": "London: Springer-Verlag London Limited", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Model Theory for Metric Structures", "author": ["I.B. Yaacov", "A. Berenstein", "C.W. Henson", "A. Usvyatsov"], "venue": "London Math Society Lecture Note Series 350 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Is there a way to predict whether any given person in the hospital has the disease or not? This report covers the PAC learning model applied to learning a collection of subsets C, called a concept class, of a domain X and more generally, a collection of functions F , called a function class, from X to the unit interval [0, 1].", "startOffset": 321, "endOffset": 327}, {"referenceID": 0, "context": ",Fk and a \u201ccontinuous logic connective\u201d (that is, a continuous function u : [0, 1] \u2192 [0, 1]), we consider the construction of a new composition function class u(F1, .", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": ",Fk and a \u201ccontinuous logic connective\u201d (that is, a continuous function u : [0, 1] \u2192 [0, 1]), we consider the construction of a new composition function class u(F1, .", "startOffset": 85, "endOffset": 91}, {"referenceID": 17, "context": "There is a previously known analogous estimate for a composition of concept classes built using a usual connective of classical logic [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "This section lists some definitions and results in measure theory and analysis, found in standard textbooks, such as [6], [18], and [2], which are used in this report.", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "More generally, given two measurable functions f, g : X \u2192 [0, 1], one can look at the expected value of their absolute difference by integrating with respect to \u03bc:", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Validating hypotheses in the PAC learning model uses the idea of measuring the symmetric difference of two subsets of a probability space (X,S, \u03bc) and calculating the expected value of the difference of f, g : X \u2192 [0, 1].", "startOffset": 214, "endOffset": 220}, {"referenceID": 0, "context": "The unit interval [0, 1] is a subset of R, so it is a metric sub-space of (R, d), and this space will be used quite often in this report.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "Write [0, 1] for the set of all measurable functions from a probability space (X,S, \u03bc) to [0, 1].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Write [0, 1] for the set of all measurable functions from a probability space (X,S, \u03bc) to [0, 1].", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "Then, it is a metric sub-space of V with distance induced by the L2(\u03bc) norm on V , restricted of course to [0, 1] X .", "startOffset": 107, "endOffset": 113}, {"referenceID": 0, "context": "10, the set [0, 1], which denotes the set-theoretic product [0, 1]\u00d7 .", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "10, the set [0, 1], which denotes the set-theoretic product [0, 1]\u00d7 .", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "\u00d7 [0, 1] is then a metric space with distance d defined by d((r1, .", "startOffset": 2, "endOffset": 8}, {"referenceID": 0, "context": ",Fk are sets of measurable functions from a probability space (X,S, \u03bc) to the unit interval, then Fi \u2286 [0, 1] for each i = 1, .", "startOffset": 103, "endOffset": 109}, {"referenceID": 0, "context": "A function class F is a collection of measurable functions from X to the unit interval [0, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "[0, 1] : the set of all measurable functions f : X \u2192 [0, 1], instead of the customary notation of all functions from X to [0, 1].", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] : the set of all measurable functions f : X \u2192 [0, 1], instead of the customary notation of all functions from X to [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "[0, 1] : the set of all measurable functions f : X \u2192 [0, 1], instead of the customary notation of all functions from X to [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 15, "context": "1 ([16]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Even more generally, \u03c7A is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {\u03c7A : X \u2192 [0, 1] : A \u2208 C}, so it is natural to generalize Definition 3.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "Even more generally, \u03c7A is a function from X to [0, 1]; in other words, every concept class C can be identified as a function class FC = {\u03c7A : X \u2192 [0, 1] : A \u2208 C}, so it is natural to generalize Definition 3.", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": "1 involves the symmetric difference of two concepts and its generalization to measurable functions f, g : X \u2192 [0, 1] is the expected value of their absolute difference E\u03bc(f, g), as seen in the previous section:", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "A simple exercise can show that if f, g \u2208 [0, 1] take values in {0, 1}, so they are indicator functions of two concepts A,B \u2286 X , then E\u03bc(f, g) coincide with the measure of their symmetric difference: E\u03bc(f, g) = \u03bc(A\u25b3 B), where f = \u03c7A and g = \u03c7B.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "Then, the set of all labeled samples of m points can be identified with (X \u00d7 [0, 1]), and producing a hypothesis is the process of associating a labeled sample to a function H \u2208 F (just as in concept class learning).", "startOffset": 77, "endOffset": 83}, {"referenceID": 17, "context": "2 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "A function class F is distribution-free Probably Approximately Correct learnable if there exists an algorithm L : \u222am\u2208N(X \u00d7 [0, 1]) \u2192 F with the following property: for every \u01eb > 0, for every \u03b4 > 0, there exists a M \u2208 N such that for every f \u2208 F , for every probability measure \u03bc, for every m \u2265 M , for any x1, .", "startOffset": 123, "endOffset": 129}, {"referenceID": 5, "context": "The symbol \u03bc denotes the product measure on X; the reader can refer to [6] for the details.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "Hence, the name \u201cProbably (\u03b4) Approximately (\u01eb) Correct\u201d is used [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Both the statement and its proof can be found in Chapter 3 of [18] and Chapter 1 of [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "1 ([17]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "3 ([17]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "The first example is trivial and the second is fairly well-known, seen in [8] and [10], but we believe the third, Example 4.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "Following Chapter 4 of [18], C shatters S if and only if |{A \u2229 S : A \u2208 C}| = 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "8 (Sauer\u2019s Lemma [12]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "2 Characterization of concept class distribution-free PAC learning The following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis\u2019 paper [17] in 1971 and the 1989 paper [5] by Blumer et al.", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "2 Characterization of concept class distribution-free PAC learning The following is one of the main theorems concerning PAC learning, whose proof results from Vapnik and Chervonenkis\u2019 paper [17] in 1971 and the 1989 paper [5] by Blumer et al.", "startOffset": 222, "endOffset": 225}, {"referenceID": 16, "context": "9 ([17] and [5]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "9 ([17] and [5]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Every concept class C can be viewed as a function class FC = {\u03c7A : X \u2192 [0, 1] : A \u2208 C}, as seen in Section 3, so a natural question is whether the notion of shattering can be generalized.", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "This dimension, assigned to function classes, involves the notion of \u01eb-shattering, but similar to the notion of (regular) shattering, it can be defined for any collection of functions f : X \u2192 [0, 1], where X is any set, but for sake of this report, the following sections (still) assume X = (X,S) is a measurable space and the collection of functions is a function class F .", "startOffset": 192, "endOffset": 198}, {"referenceID": 6, "context": "1 ([7]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": ", cn) \u2208 [0, 1], if for every e \u2208 {0, 1}n, there exists f \u2208 F such that f(xi) \u2265 ci + \u01eb for ei = 1, and f(xi) \u2264 ci \u2212 \u01eb for ei = 0.", "startOffset": 8, "endOffset": 14}, {"referenceID": 6, "context": "2 ([7]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "5) \u2208 [0, 1].", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": ", cn) \u2208 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "Let X = R and let F be the set of all continuous functions f : X \u2192 [0, 1].", "startOffset": 67, "endOffset": 73}, {"referenceID": 0, "context": "5, and consider a collection of continuous [0, 1]-valued functions defined as follows.", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "Given e \u2208 {0, 1}N, a countable binary sequence, define fe : X \u2192 [0, 1] by fe(x) = {", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "5) \u2208 [0, 1]: for each e \u2208 {0, 1}n, it can be extended to a countable binary sequence \u1ebd, where \u1ebdi = ei for all i = 1, .", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "5 ([1] and [18]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "5 ([1] and [18]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "Set up a bijection b : C \u2192 [0, 1/3] or to [0, 1/3]\u2229 Q, depending on the cardinality of C, and for every A \u2208 C, define a function fA : X \u2192 [0, 1] by fA(x) = \u03c7A(x) + (\u22121)Ab(A).", "startOffset": 138, "endOffset": 144}, {"referenceID": 17, "context": "5, which are much simpler than the one found in [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "10 in [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "5) \u2208 [0, 1].", "startOffset": 5, "endOffset": 11}, {"referenceID": 17, "context": "The following subsection provides this construction, which can be found in Chapter 4 of [18], in the context of concept classes using a connective of classical logic.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "1 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The proof of this theorem can be found in [18] and uses Sauer\u2019s Lemma to bound the VC dimension of u(C1, .", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale \u01eb, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1] \u2192 [0, 1].", "startOffset": 266, "endOffset": 272}, {"referenceID": 0, "context": "The main objective of our project was to generalize this theorem for function classes, in terms of the Fat Shattering dimension of scale \u01eb, but the connective of classical logic u would have to be replaced by a continuous logic connective, a continuous function u : [0, 1] \u2192 [0, 1].", "startOffset": 275, "endOffset": 281}, {"referenceID": 0, "context": "However, in continuous logic, truth-values can be found anywhere in the unit interval [0, 1].", "startOffset": 86, "endOffset": 92}, {"referenceID": 0, "context": "Therefore, we should consider a function u : [0, 1] \u2192 [0, 1], which will transform function classes, and require that u be a continuous logic connective.", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "Therefore, we should consider a function u : [0, 1] \u2192 [0, 1], which will transform function classes, and require that u be a continuous logic connective.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "In other words, u should be continuous from the (product) metric space [0, 1] to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.", "startOffset": 71, "endOffset": 77}, {"referenceID": 18, "context": "In other words, u should be continuous from the (product) metric space [0, 1] to the unit interval [19]; in fact, because u is continuous from a compact metric space to a metric space, it is automatically uniformly continuous.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "[19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In particular, u : [0, 1] \u2192 [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity \u03b4 if for every More specifically, \u03b1 = \u03b1k is the smallest integer such that", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "In particular, u : [0, 1] \u2192 [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity \u03b4 if for every More specifically, \u03b1 = \u03b1k is the smallest integer such that", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "In particular, u : [0, 1] \u2192 [0, 1], where [0, 1] is equipped with the L2 product distance d, is uniformly continuous with modulus of uniform continuity \u03b4 if for every More specifically, \u03b1 = \u03b1k is the smallest integer such that", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": ", r\u2032 k) \u2208 [0, 1], d((r1, .", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": ",Fk and a uniformly continuous function u : [0, 1] \u2192 [0, 1], consider the new function class u(F1, .", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": ",Fk and a uniformly continuous function u : [0, 1] \u2192 [0, 1], consider the new function class u(F1, .", "startOffset": 53, "endOffset": 59}, {"referenceID": 17, "context": "It is a known result, seen in Chapter 5 of [18], that this new class u(F1, .", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": ",Fk be function classes of X, and u : [0, 1] \u2192 [0, 1] be a uniformly continuous function with modulus of continuity \u03b4(\u01eb).", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": ",Fk be function classes of X, and u : [0, 1] \u2192 [0, 1] be a uniformly continuous function with modulus of continuity \u03b4(\u01eb).", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "\u00d7 Fk \u2192 [0, 1] , which is uniformly continuous from the metric space F1\u00d7 .", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "\u00d7Fk with the L2 product distance d\u0303 to the metric space [0, 1] with distance induced by the L2(\u03bc) norm, and prove the following lemma.", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "\u00d7Fk \u2192 [0, 1] be uniformly continuous with some modulus of continuity \u03b4(\u01eb, k), a function", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "If u : [0, 1] \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then the function \u03c6 : F1 \u00d7 .", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "If u : [0, 1] \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then the function \u03c6 : F1 \u00d7 .", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "\u00d7 Fk \u2192 [0, 1] defined by \u03c6(f1, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 8, "context": "5, we first introduce the concept of an \u01eb-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale \u01eb by using results from Mendelson and Vershynin [9] and Talagrand [15].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "5, we first introduce the concept of an \u01eb-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale \u01eb by using results from Mendelson and Vershynin [9] and Talagrand [15].", "startOffset": 227, "endOffset": 230}, {"referenceID": 14, "context": "5, we first introduce the concept of an \u01eb-covering number for any metric space, based on [9], and relate this number for a function class to its Fat Shattering dimension of scale \u01eb by using results from Mendelson and Vershynin [9] and Talagrand [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 0, "context": "\u00d7 Fk \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb, k).", "startOffset": 7, "endOffset": 13}, {"referenceID": 8, "context": "10 ([9]).", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "11 ([15]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Suppose u : [0, 1] \u2192 [0, 1] is uniformly continuous with a modulus of continuity \u03b4(\u01eb), where [0, 1] is a metric space with the L2 product distance d.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Suppose u : [0, 1] \u2192 [0, 1] is uniformly continuous with a modulus of continuity \u03b4(\u01eb), where [0, 1] is a metric space with the L2 product distance d.", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "Suppose u : [0, 1] \u2192 [0, 1] is uniformly continuous with a modulus of continuity \u03b4(\u01eb), where [0, 1] is a metric space with the L2 product distance d.", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": "\u00d7Fk \u2192 [0, 1] defined by \u03c6(f1, .", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "6, if u : [0, 1] \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then \u03c6 : F1 \u00d7 .", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "6, if u : [0, 1] \u2192 [0, 1] is uniformly continuous with modulus of continuity \u03b4(\u01eb), then \u03c6 : F1 \u00d7 .", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "\u00d7 Fk \u2192 [0, 1] defined by \u03c6(f1, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "\u00d7fk // [0, 1] u // [0, 1] ,", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "\u00d7fk // [0, 1] u // [0, 1] ,", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "\u00d7 Fk \u03c6 // [0, 1] .", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "For instance, the function u : [0, 1] \u2192 [0, 1] defined by u(r1, r2) = r1 \u00b7 r2 (multiplication) is uniformly continuous with a modulus of continuity \u03b4(\u01eb) = \u01eb 2 .", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "For instance, the function u : [0, 1] \u2192 [0, 1] defined by u(r1, r2) = r1 \u00b7 r2 (multiplication) is uniformly continuous with a modulus of continuity \u03b4(\u01eb) = \u01eb 2 .", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "Indeed, let \u01eb > 0 and consider (r1, r2), (r \u2032 1, r \u2032 2) \u2208 [0, 1].", "startOffset": 58, "endOffset": 64}, {"referenceID": 17, "context": "1 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "A function class F is Probably Approximately Correct learnable under \u03bc if there exists an algorithm L : \u222am\u2208N(X \u00d7 [0, 1]) \u2192 F with the following property: for every \u01eb > 0, for every \u03b4 > 0, there exists a M \u2208 N such that for every f \u2208 F , for every m \u2265 M , for any x1, .", "startOffset": 113, "endOffset": 119}, {"referenceID": 3, "context": "2 ([4]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure \u03bc if and only if the class has no witness of irregularity, a property that involves shattering [13],[14].", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "Talagrand had proved that a function class is a GlivenkoCantelli (GC) function class with regard to a single measure \u03bc if and only if the class has no witness of irregularity, a property that involves shattering [13],[14].", "startOffset": 217, "endOffset": 221}, {"referenceID": 10, "context": "Every GC function class is PAC learnable under \u03bc [11], but the property of having no witness of irregularity is strictly stronger than PAC learnability.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "The paper [3] written by Bartlett et al.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": ",Fk) using a continuous function u : [0, 1] \u2192 [0, 1], a continuous logic connective.", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": ",Fk) using a continuous function u : [0, 1] \u2192 [0, 1], a continuous logic connective.", "startOffset": 46, "endOffset": 52}], "year": 2011, "abstractText": "We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale \u01eb, are explained and a few examples of their calculations are given with proofs. We then explain Sauer\u2019s Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distributionfree PAC learnable and it having finite VC dimension. As the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale \u01eb of this new function class in terms of the Fat Shattering dimensions of the collection\u2019s classes. We conclude this report by providing a few open questions and future research topics involving the PAC learning model.", "creator": "LaTeX with hyperref package"}}}