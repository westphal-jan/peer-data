{"id": "1106.0672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "Policy Recognition in the Abstract Hidden Markov Model", "abstract": "In this paper, we present a method for detecting the behavior of an agent in dynamic, noisy, uncertain areas and across multiple levels of abstraction. We refer to this problem as online planning detection under uncertainty and generally consider it to be a probabilistic conclusion to the stochastic process that represents the execution of the agent's plan. Our contributions in this paper are twofold. In terms of probabilistic inference, we present the Abstract Hidden Markov Model (AHMM), a novel type of stochastic processes that exhibit their dynamic structure of the Bajian network (DBN) and analyze the characteristics of this network. We then describe an application of the Rao-Blackwellised Particle Filter to the AHMM, which allows us to construct an efficient, hybrid inference method for this model. In terms of planning detection, we propose a novel planning detection framework that can be used on the AHMM as an execution-based model of the Black Hardened Modelling System.", "histories": [["v1", "Fri, 3 Jun 2011 14:54:32 GMT  (384kb)", "http://arxiv.org/abs/1106.0672v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["h h bui", "s venkatesh", "g west"], "accepted": false, "id": "1106.0672"}, "pdf": {"name": "1106.0672.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hung H. Bui", "Svetha Venkatesh"], "emails": [], "sections": [{"heading": "Journal of Arti ial Intelligen e Resear h 17 (2002) 451-499 Submitted 12/00; published 12/02", "text": "Poli y Re ognition in the Abstra t Hidden Markov ModelHung H. Bui buihh s. urtin.edu.auSvetha Venkatesh svetha s. urtin.edu.auGeo West geoff s. urtin.edu.auDepartment of Computer S ien eCurtin University of Te hnologyPO Box U1987, Perth, WA 6001, AustraliaAbstra tIn this paper, we present a method for re ognising an agent's behaviour in dynami ,noisy, un ertain domains, and a ross multiple levels of abstra tion. We term this problemon-line plan re ognition under un ertainty and view it generally as probabilisti inferen e onthe sto hasti pro ess representing the exe ution of the agent's plan. Our ontributions inthis paper are twofold. In terms of probabilisti inferen e, we introdu e the Abstra t HiddenMarkov Model (AHMM), a novel type of sto hasti pro esses, provide its dynami Bayesiannetwork (DBN) stru ture and analyse the properties of this network. We then des ribean appli ation of the Rao-Bla kwellised Parti le Filter to the AHMM whi h allows us to onstru t an e\u00c6 ient, hybrid inferen e method for this model. In terms of plan re ognition,we propose a novel plan re ognition framework based on the AHMM as the plan exe utionmodel. The Rao-Bla kwellised hybrid inferen e for AHMM an take advantage of theindependen e properties inherent in a model of plan exe ution, leading to an algorithm foronline probabilisti plan re ognition that s ales well with the number of levels in the planhierar hy. This illustrates that while sto hasti models for plan exe ution an be omplex,they exhibit spe ial stru tures whi h, if exploited, an lead to e\u00c6 ient plan re ognitionalgorithms. We demonstrate the usefulness of the AHMM framework via a behaviourre ognition system in a omplex spatial environment using distributed video surveillan edata.1. Introdu tionPlan re ognition is the problem of inferring an a tor's plan by wat hing the a tor's a tionsand their e e ts. Often, the a tor's behaviour follows a hierar hi al plan stru ture. Thus,in plan re ognition, the observer needs to infer about the a tor's plans and sub-plans atdi erent levels of abstra tion in its plan hierar hy. The problem is ompli ated by the twosour es of un ertainty inherent in the a tor's planning pro ess: (1) the sto hasti aspe t ofplan re nement (a plan an be non-deterministi ally re ned into di erent sub-plans), and(2) the sto hasti out omes of a tions (the same a tion an non-deterministi ally result indi erent out omes). Furthermore, the observer has to deal with a third sour e of un ertaintyarising from the noise and ina ura y in its own observation about the a tor's plan. Inaddition, we would like our observer to be able to perform the plan re ognition task \\on-line\" while the observations about the a tor's plan are streaming in. We refer to this generalproblem as on-line plan re ognition under un ertainty. 2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nBui, Venkatesh & WestThe seminal work in plan re ognition (Kautz & Allen, 1986) onsiders a plan hierar hy,but does not deal with the un ertainty aspe ts of the problem. As a result, the approa h an only postulate a set of possible plans for the a tor, but is unable to determine whi hplan is more probable. Sin e then, the important role of un ertainty reasoning in planre ognition has been re ognised (Charniak & Goldman, 1993; Bauer, 1994; van Beek, 1996),and Bayesian probability has been argued as the appropriate model (Charniak & Goldman,1993; van Beek, 1996). The dynami , \\on-line\" aspe t of plan re ognition has only beenre ently onsidered (Pynadath & Wellman, 1995, 2000; Goldman, Geib, & Miller, 1999;Huber, Durfee, & Wellman, 1994; Albre ht, Zukerman, & Ni holson, 1998). All of thisre ent work shares the view that online plan re ognition is largely a problem of probabilisti inferen e in a sto hasti pro ess that models the exe ution of the a tor's plan. While thisview o ers a general and oherent framework for modelling di erent sour es of un ertainty,the sto hasti pro ess that we need to deal with an be ome quite omplex, espe ially if we onsider a large plan hierar hy. Thus, the main issue here is the omputational omplexityfor dealing with this type of sto hasti pro esses, and whether the omplexity is s alable tomore omplex plan hierar hies.1.1 Aim and Signi an eIn this paper, we demonstrate that the type of plan re ognition problems des ribed aboves ales reasonably well with respe t to the number of levels of abstra tion in the plan hi-erar hy. This is in ontrast to the ommon-sense analysis that more levels in the planhierar hy would introdu e more variables to the sto hasti pro ess, whi h in turn, resultsin exponential omplexity w.r.t the number of levels in the hierar hy.In order to a hieve this, we rst assume a general sto hasti model of plan exe utionthat an model the three sour es of un ertainty involved. The model for planning witha hierar hy of abstra tion under un ertainty has been developed re ently by the abstra tprobabilisti planning ommunity (Sutton, Pre up, & Singh, 1999; Parr & Russell, 1997;Forestier & Varaiya, 1978; Hauskre ht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Dean& Lin, 1995). To our advantage, we adopt their basi model, known as the abstra t Markovpoli ies (AMP) 1 as our model for plan exe ution. The AMP is an extension of a poli yin Markov De ision Pro esses (MDP) that enables an abstra t poli y to invoke other morere ned poli ies and so on down the poli y hierar hy. Thus, the AMP is similar to a ontin-gent plan that pres ribes whi h sub-plan should be invoked at ea h appli able state of theworld to a hieve its intended goal, ex ept that it an represent both the un ertainty in theplan re nement and in the out omes of a tions. Sin e an AMP an be des ribed simply interms of a state spa e and a Markov poli y that sele ts among a set of other AMP's, usingthe AMP as the model for plan exe ution also helps us fo us on the stru ture of the poli yhierar hy.The exe ution of an AMP leads to a spe ial sto hasti pro ess whi h we alled theAbstra t Markov Model (AMM). The noisy observation about the environment state (e.g.,the e e ts of a tion) an then be modelled by making the state \\hidden\", similar to thehidden state in the Hidden Markov Models (Rabiner, 1989). The result is an interesting andnovel sto hasti pro ess whi h we term the Abstra t Hidden Markov Model. Intuitively, the1. Also known as options, poli ies of Abstra t Markov De ision Pro esses, or supervisor's poli ies.452\nPoli y re ognition in the Abstra t Hidden Markov ModelAHMM models how an AMP auses the adoption of other poli ies and a tions at di erentlevels of abstra tion, whi h in turn generate a sequen e of states and observations. In theplan re ognition task, an observer is given an AHMM orresponding to the a tor's planhierar hy, and is asked to infer about the urrent poli y being exe uted by the a tor at alllevels of the hierar hy, taking into a ount the sequen e of observations urrently available.This amounts to reversing the dire tion of ausality in the AHMM, i.e. to determine a setof poli ies that an explain the sequen e of observations at hand. We shall refer to thisproblem as poli y re ognition.Viewing the AHMM as a type of dynami Bayesian network (Dean & Kanazawa, 1989;Ni holson & Brady, 1992), it is known that the omplexity of this kind of inferen ing in theDBN depends on the size of the representation of the so- alled belief state, the onditionaljoint distribution of the variables in the DBN at time t given the observation sequen e upto t (Boyen & Koller, 1998). Thus we an ask the following question: how does the poli yhierar hy a e t the size of the belief state representation of the orresponding AHMM?Generally, for a poli y hierar hy with K levels, the belief state would have at leastK variables and thus the size of their joint distribution would be O(exp(K)). However,the AHMM has a spe i network stru ture that exhibits ertain onditional independen eproperties among its variables whi h an be exploited for e\u00c6 ien y. We rst identify theseuseful independen e properties in the AHMM and show that there is a ompa t representa-tion of the spe ial belief state in the ase where the state sequen e an be orre tly observed(full observability assumption) and the starting and ending time of ea h poli y is known.Consequently, poli y re ognition in this ase an be performed very e\u00c6 iently by updatingthe AHMM ompa t belief state. This partial result, although too restri ted to be useful byitself, leads to an important observation about the general belief state: although it annotbe represented ompa tly, it an be approximated e\u00c6 iently by a olle tion of ompa t spe- ial belief states. This makes the inferen e problem in the AHMM parti ularly amenableto a te hnique alled Rao-Bla kwellisation (Casella & Robert, 1996) whi h allows us to onstru t hybrid inferen e methods that ombine both exa t inferen e and approximatesampling-based inferen e for greater e\u00c6 ien y. The appli ation of Rao-Bla kwellisation tothe AHMM stru ture redu es the sampling spa e that we need to approximate to a spa ewith xed dimension that does not depend on K, ensuring that the hybrid inferen e algo-rithm s ales well w.r.t K.The ontributions of the paper are thus twofold. In terms of sto hasti pro esses anddynami Bayesian networks, we introdu e the AHMM, a novel type of sto hasti pro esses,provide its DBN stru ture and analyse the properties of this network. We present an appli- ation of the Rao-Bla kwellised Parti le Filter to the AHMM whi h results in an e\u00c6 ienthybrid inferen e method for this sto hasti model. In terms of plan re ognition, we proposea novel plan re ognition framework based on probabilisti inferen e using the AHMM as theplan exe ution model. The omplexity of the inferen e problem is addressed by applyinga range of re ently developed te hniques in probabilisti reasoning to the plan re ognitionproblem. Our work illustrates that while the sto hasti models for plan exe ution an be omplex, they exhibit ertain spe ial stru tures that an be exploited to onstru t e\u00c6 ientplan re ognition algorithms. 453\nBui, Venkatesh & West1.2 Stru ture of the PaperThe main body of the paper is organised as follows. Se tion 2 introdu es the ba kgroundmaterial in dynami Bayesian networks and probabilisti inferen e. Se tion 3 formally de- nes the abstra t Markov poli y and the poli y hierar hy. Se tion 4 presents the AHMM,its DBN representation and onditional independen e properties. The algorithms for pol-i y re ognition are dis ussed in Se tion 5, rst for the spe ial tra table ase and then forthe general ase. Se tion 6 presents our experimental results with the AHMM framework,in luding a real-time system for re ognising people behaviour in a omplex spatial environ-ment using distributed video surveillan e data. Se tion 7 provides a omparative review ofrelated work in probabilisti plan re ognition. Finally, we on lude and dis uss dire tionsfor further resear h in Se tion 8.2. Ba kground in Probabilisti Inferen eThe aim of this se tion is to familiarise readers with some on epts in probabilisti inferen ethat will be used later on in the paper. In subse tions 2.1 and 2.2, we dis uss BayesianNetworks (BN) and Dynami Bayesian Networks (DBN) in general. In subse tion 2.3,we dis uss the Sequential Importan e Sampling (SIS) algorithm, a general approximatesampling-based inferen e method for dynami models. Subse tions 2.4 and 2.5 introdu eRao-Bla kwellisation, a te hnique for improving sampling-basedmethods by utilising ertainspe ial stru tures of the dynami model. Later on, Rao-Bla kwellisation will be used as ourkey omputational te hnique for performing poli y re ognition.2.1 Bayesian NetworksThe Bayesian network (BN) (Pearl, 1988; Jensen, 1996; Castillo, Gutierrez, & Hadi, 1997)(also known as probabilisti network or belief network) is a well-established framework fordealing with un ertainty. It provides a graphi al and ompa t representation of the jointprobability distribution of a set of domain variables X1; : : : Xn in the form of a dire teda y li graph (DAG) whose nodes orrespond to the domain variables. For ea h nodeXi, the links from the parent nodes Pa(Xi) are parameterised by the onditional prob-ability of that node given the parents Pr(Xi jPa(Xi)). The network stru ture togetherwith the parameters en ode a fa torisation of the joint probability distribution (JPD)Pr(X1; : : : Xn) = Qni=1 Pr(Xi jPai). Given a Bayesian network, onditional independen estatements of the form X ? Y jZ (X is independent of Y given Z, where X;Y;Z are vari-ables or sets of variables) an be asserted if X is d-separated from Y by Z in the networkstru ture, where d-separation is a graph separation on ept for DAGs (Pearl, 1988). Thenetwork stru ture of a BN thus aptures ertain onditional independen e properties amongthe domain variables whi h an be exploited for e\u00c6 ient inferen e.The main inferen e task on a Bayesian network is to al ulate the onditional probabilityof a set of variables given the values of another set of variables (the eviden e). There aretwo types of omputation te hniques for doing this. Exa t inferen e algorithms (Lauritzen& Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990; D'Ambrosio, 1993) omputethe exa t value of the onditional probability required based on analyti al transformationthat exploits the onditional independen e relationships of the variables in the network.454\nPoli y re ognition in the Abstra t Hidden Markov ModelApproximative inferen e algorithms (Pearl, 1987; York, 1992; Henrion, 1988; Fung & Chang,1989; Sha hter & Peot, 1989) ompute only an approximation of the required probability,usually obtained either through \\forward\" sampling (Henrion, 1988; Fung & Chang, 1989;Sha hter & Peot, 1989) (a varian e of Bayesian Importan e Sampling (Geweke, 1989)), orthrough Gibbs (Monte-Carlo Markov-Chain) sampling (Pearl, 1987; York, 1992). Thesealgorithms have the advantages of simple implementation, an be applied to all types ofnetwork, and an trade o the a ura y in the estimates for omputation resour es. It isknown that exa t inferen e in BN is NP-hard with respe t to the network size (Cooper,1990), while approximate inferen e, although s ales well with the network size, is NP-hardwith respe t to the hard-bound a ura y of the estimates (Dagum & Luby, 1993). In thelight of these theoreti al results, approximate inferen e an be useful in large networks whenexa t omputation is intra table, but a ertain degree of error in the probability estimate an be tolerated by the appli ation.2.2 Dynami Bayesian NetworksTo model the temporal dynami s of the environment, the Dynami Bayesian Network(DBN) (Dean & Kanazawa, 1989; Ni holson & Brady, 1992; Dagum, Galper, & Horvitz,1992) is a spe ial Bayesian network ar hite ture for representing the evolution of the do-main variables over time. A DBN onsists of a sequen e of time-sli es where ea h time-sli e ontains a set of variables representing the state of the environment at the urrent time.A time-sli e is in itself a Bayesian network, with the same network stru ture repli ated atea h time-sli e. The temporal dynami s of the environment is en oded via the network linksfrom one time-sli e to the next. In addition, ea h time-sli e an ontain observation nodeswhi h model the (possibly noisy) observation about the urrent state of the environment.Given a DBN and a sequen e of observations, we might want to draw predi tionsabout the future state variables (predi ting), or about the unobserved variables in thepast (smoothing) (Kjaerul , 1992). This problem an be solved using an inferen e algo-rithm for Bayesian networks des ribed above. However, if we want to revise the predi tionas the observations arrive over time, reapplying the inferen e algorithm ea h time the ob-servation sequen e hanges ould be ostly, espe ially as the sequen e grows. To avoid this,we need to keep the joint distribution of all the variables in the urrent time-sli e, giventhe observation sequen e up to date. This probability distribution is termed the belief state(also known as the ltering distribution) and plays an important role in inferen ing in theDBN. All existing inferen e s hemes for the DBN involve maintaining and updating thebelief state (i.e., ltering). When a new observation is re eived, the urrent belief state isrolled over one time-sli e ahead following the evolution model, then onditioned on the newobservation to obtain the updated belief state.An obvious problem with this approa h is the size of the belief state that we need tomaintain. It has been noted that while the intera tion of the variables in the DBN islo alised, the variables in the belief state an be highly onne ted (Boyen & Koller, 1998).This is be ause the marginalisation of the past time-sli es usually destroys the onditionalindependen e of the urrent time-sli e. When the size of the belief state is large, exa tinferen e methods like (Kj rul , 1995) is intra table, and it be omes ne essary to maintainonly an approximation of the a tual belief state, either in the form of an approximate455\nBui, Venkatesh & Westdistribution that an be represented ompa tly (Boyen & Koller, 1998), or in the form ofa set of weighted samples as in the Sequential Monte-Carlo Sampling methods (Dou et,Godsill, & Andrieu, 2000b; Kanazawa, Koller, & Russell, 1995; Liu & Chen, 1998).The most simple ase of the DBN where, in ea h time-sli e, there is only a single statevariable and an observation node, is the well-known Hidden Markov Model (HMM) (Ra-biner, 1989). Filtering in this simple stru ture an be solved using dynami program-ming in the dis rete HMM (Rabiner, 1989), or Kalman ltering in the linear Gaussianmodel (Kalman, 1960). More re ently, extensions of the HMM with multiple hidden in-tera ting hains su h as the Coupled Hidden Markov Models (CHMM) and the Fa torialHidden Markov Models (FHMM) have been proposed (Brand, 1997; Ghahramani & Jordan,1997; Jordan, Ghahramani, & Saul, 1997). In these models, the size of the belief state isexponential in the number of hidden hains. Therefore, the inferen e and parameter estima-tion problems be ome intra table if the number of hidden hains is large. For this reason,approximate te hniques are required. CHMM (Brand, 1997) employs a deterministi ap-proximation that approximates full dynami programming by keeping only a xed numberof \\heads\" with highest probabilities. The \\heads\" are thus hosen deterministi ally ratherthan randomly as in sampling-based methods. FHMM (Ghahramani & Jordan, 1997; Jor-dan et al., 1997) uses variational approximation (Jordan, Ghahramani, Jaakkola, & Saul,1999) whi h approximates the full FHMM stru ture by a sparsi ed tra table stru ture. Thisidea is similar to the stru tured approximation method in (Boyen & Koller, 1998).Our AHMM an be viewed as a type of Coupled/Fa torial HMM sin e the AHMMalso onsists of a number of intera ting hains. However the type of intera tion in ourAHMM is di erent from the other types of intera tion that have been onsidered (Brand,1997; Jordan et al., 1997; Ghahramani & Jordan, 1997). This is be ause the main fo usof the AHMM is the dynami s of temporal abstra tion among the hains, rather than the orrelation between them at the same time interval. In addition, ea h node in the AHMMhas a spe i meaning (poli y, state, or poli y termination status), and the links have a lear ausal interpretation based on the poli y sele tion and persisten e model. This is in ontrast to the Coupled/Fa torial HMM where the nodes and links usually do not haveany lear semanti / ausal interpretation. The advantage is that prior knowledge about thetemporal de omposition of an abstra t pro ess an be in orporated in the AHMM morenaturally.2.3 Sequential Importan e Sampling (SIS)Sequential Importan e Sampling (SIS) (Dou et et al., 2000b; Liu & Chen, 1998), alsoknown as Parti le Filter (PF), is a general Monte-Carlo approximation s heme for dynami sto hasti models. In prin iple, the SIS method is the same as the so- alled BayesianImportan e Sampling (BIS) estimator in the stati ase (Geweke, 1989). Suppose that wewant to estimate the quantity f = R f(x)p(x)dx, i.e., the mean of f(x) where x is a randomvariable with density p. Note that if f is taken as the identity fun tion of an event Athen f is simply Pr(A). Let q(x) be an arbitrary2 density fun tion, termed the importan edistribution. Usually, the importan e distribution q is hosen so that is it easy to obtain2. For the weight to be properly de ned, the support of q has to be a subset of the support of p.456\nPoli y re ognition in the Abstra t Hidden Markov Modelrandom samples from it. The expe tation under estimation an then be rewritten as: f = R [f(x)p(x)=q(x)\u2104q(x)dxR [p(x)=q(x)\u2104q(x)dx = Eq f(x)p(x)=q(x)Eq p(x)=q(x)From this expression, the BIS estimator w.r.t q an be obtained: f f\u0302BIS = 1N PNi=1 f(x(i)) _w(x(i))1N PNi=1 _w(x(i)) = NXi=1 f(x(i)) ~w(x(i))where fx(i)g are the N i.i.d samples taken from q(x), _w(x) = p(x)=q(x) and ~w is thenormalised weight ~w(x(i)) = _w(x(i))=Pi _w(x(i)). Note that the normalised weight an be omputed from any weight fun tion w(x) / _w(x), i.e., the weight fun tion need only be omputed up to a normalising onstant fa tor.In the dynami ase, we want to estimate f = R~xt f(~xt)p(~xtj~ot) where ~xt = (x0; : : : ; xt)and ~ot = (o0; : : : ; ot) are two sequen es of random variables; ot represents the observationavailable to us at time t. Often, (~xt) is a Markov sequen e and ot is the observation of xtas in a HMM. In a DBN, xt orresponds to the set of state variables and ot orresponds tothe set of observations at time-sli e t. The SIS method presented here however applies tothe most general ase where (~xt) an be non-Markov, and ot not only depends on xt.We now an introdu e the importan e distribution q(~xtj~ot) to obtain the estimator: f f\u0302SIS = NXi=1 f(~x(i)t ) ~w(~x(i)t ) (1)To ensure that we an obtain sample from q(~xtj~ot) \\online\", i.e., to sample a new valuext for the sequen e ~xt when the urrent observation ot arrives, q must be restri ted to theform: q(~xtj~ot) = q(~xt 1j~ot 1)q(xtj~xt 1; ~ot)With this restri tion on q, we an use the weight fun tion w(~xt) = p(~xt; ~ot)=q(~xtj~ot) so thatthe weight an also be updated \\online\" using:w(~xt) = w(~xt 1)p(xt; otj~xt 1; ~ot 1)=q(xtj~xt 1; ~ot) (2)Let wt = w(~xt)=w(~xt 1) be the weight updating fa tor at time t, and qt = q(xtj~xt 1; ~ot)be the sampling distribution used at time t. From (2) we havewtqt = p(xt; otj~xt 1; ~ot 1) (3)whi h means that p(xt; otj~xt 1; ~ot 1) is fa torised into two parts: wt and qt. By hoos-ing di erent fa torisations, we obtain di erent forms for qt and thus di erent importantdistributions q. For example, when (~xt; ~ot) is a HMM, qt an be hosen as p(xtjxt 1)with wt = p(otjxt) as in the likelihood weighting (LW) method, or qt an be hosen asp(xtjxt 1; ot) with wt = p(otjxt 1) as in the likelihood weighting with eviden e rever-sal (LW-ER) (Kanazawa et al., 1995). In general, the \\forward\" qt an be hosen asp(xtj~xt 1; ~ot 1) with the orresponding weight wt = p(otj~xt; ~ot 1). The \\optimal\" qt, in457\nBui, Venkatesh & Westthe sense dis ussed in (Dou et et al., 2000b), is hosen as qt = p(xtj~xt 1; ~ot) with theasso iating wt = p(otj~xt 1; ~ot 1).The general SIS approximation s heme is thus as follows. At time t 1, we maintain Nsample sequen es f~x(i)t 1g and the N orresponding weight values fw(i)g. When the urrentobservation ot arrives, ea h sequen e ~x(i)t 1 is lengthened by a new value x(i)t sampled fromthe distribution q(xtj~x(i)t 1; ~ot). The weight value for ~x(i)t is then updated using (2). On ethe new samples and the new weights are obtained, the expe tation of any fun tional f anbe estimated using (1). This pro edure an be furthered enhan ed with a re-sampling stepand a Markov- hain sampling step (see Dou et et al. (2000b), Dou et, de Freitas, Murphy,and Russell (2000a)). We do not des ribe these important improvements of the SIS here.32.4 Rao-Bla kwellisationRao-Bla kwellisation is a general te hnique for improving the a ura y of sampling methodsby analyti ally marginalising some variables and only sampling the remainder (Casella &Robert, 1996). In its simplest form, onsider the problem of estimating the expe tationE f(x), where x is a joint produ t of two variables r; z. Using dire t Monte-Carlo sam-pling, we obtain the estimator: f\u0302 = 1N PN1 f(r(i); z(i)). Alternatively, a Rao-Bla kwellisedestimator an be derived by sampling only the variable r, with the other variable z beingintegrated out analyti ally:E f(r; z) = Er h(r) f\u0302RB = 1N NX1 h(r(i))where h(r) = Ez[f(r; z)jr\u2104. For our onvenien e, r will be referred to as the Rao-Bla kwellisingvariable.The Rao-Bla kwellised estimator f\u0302RB is generally more a urate than f\u0302 for the samenumber of samples N . This is a dire t onsequen e of the Rao-Bla kwell theorem whi hgives the relationship between un onditional and onditional varian e:VARX = VAR[E[XjY \u2104\u2104 + E[VAR[XjY \u2104\u2104When applying to the problem of estimating E f(r; z), we have:VAR f(r; z) = VAR[E[f(r; z)jr\u2104\u2104 + E[VAR[f(r; z)jr\u2104\u2104and thus VAR f(r; z) VAR[E[f(r; z)jr\u2104\u2104 = VARh(r). This suggests that for dire t Monte-Carlo sampling, the error of RB-sampling (sample only r and marginalise z) is alwayssmaller than the error of sampling both r and z for the same number of samples, ex ept inthe degenerated ase. For Bayesian Importan e Sampling, using the varian e onvergen eresult from (Geweke, 1989), one an also easily prove that as the number of samples tend toin nity, the RB-BIS would generally do better than BIS for the same number of samples.3. Note that these improvements an be used orthogonal to the Rao-Bla kwellisation pro edure dis ussedsubsequently. Our implementation of the poli y re ognition algorithm in the later se tions does in ludea re-sampling step, whi h is ru ial for keeping the error of SIS over time under ontrol.458\nPoli y re ognition in the Abstra t Hidden Markov Model2.5 SIS with Rao-Bla kwellisation (RB-SIS)Sin e SIS is a form of BIS, Rao-Bla kwellisation an also be used to improve its perfor-man e (Liu & Chen, 1998; Dou et et al., 2000b). Let us onsider again the problem ofestimating the expe tation f = R f(~xt)p(~xtj~ot), where ea h variable xt is the joint produ tof two variables (zt; rt). We shall restri t ourselves to the ase where ~xt is Markov andot is an observation of xt, i.e., when (~xt; ~ot) an be represented by a DBN. In addition,we only onsider f that depends only on the urrent variable xt, i.e., f is an expe tationover the ltering distribution p(xtj~ot). For example, if A is a \\future\" event, i.e., an eventthat depends on fxt0 jt0 tg, we an estimate p(Aj~ot) by letting f(xt) = p(Ajxt) so that f = Rxt p(Ajxt)p(xtj~ot) = p(Aj~ot).Applying Rao-Bla kwellisation to this setting, we an let h(~rt) = Rzt f(zt; rt)p(ztj~rt; ~ot),so that f = h = R~rt h(~rt)p(~rtj~ot). Thus, if we use SIS to estimate h, we also obtain anestimator for f : f f\u0302RBSIS = h\u0302SIS = NXi=1 h(~r(i)t ) ~w(~r(i)t ) (4)The bene t of doing this is the in rease in the a ura y of the estimator, as we nowonly need to sample the variables ~rt. The down side is that for ea h sample ~rt, we needto ompute h(~rt) using some exa t inferen e method. Furthermore, the SIS pro edure toestimate h might require some additional omplexity sin e the sequen e ~rt is generally non-Markov, and ot no longer depends only on rt. Overall, in omparison with the normal SISestimator f\u0302SIS (Eq. 1), for the same number of samples N , f\u0302RBSIS is more a urate but isalso more omputationally demanding to ompute.To see more learly what is involved in implementing the RB-SIS method, let us lookat the Rao-Bla kwellised belief state, i.e., the belief state of the dynami pro ess when theRao-Bla kwellising variables an be observed: Rt = p(zt; rt; otj~rt 1; ~ot 1) and its posteriorRt+ = p(ztj~rt; ~ot). All the entities needed in the RB-SIS pro edure an be omputed fromthese two distributions. Indeed, the fun tional h an be rewritten in terms of Rt+ as:h(~rt) = Zzt f(zt; rt)p(ztj~rt; ~ot) = Zzt f(zt; rt)Rt+(zt) (5)In addition, while performing SIS to estimate h, from Eq. (3), the weight wt and thesampling distribution qt an be omputed from Rt:wtqt = p(rt; otj~rt 1; ~ot 1) = Rt(rt; ot) = ZztRt(zt; rt; ot) (6)Thus, omputing the RB belief state Rt and its posterior Rt+ is an essential step inthe RB-SIS method. Sin e we have to maintain an RB belief state for ea h sample ofthe RB variables ~rt, it is ru ial that this an be done e\u00c6 iently using an exa t inferen emethod. If xt is omposed of many variables, as in the ase of a DBN, our hoi e of theRao-Bla kwellising variables should be so that the Rao-Bla kwellised belief state an bemaintained in a tra table way. Hen e, Rao-Bla kwellisation is espe ially useful when theset of variables in a DBN an be split into two parts su h that onditioning on the rst partmakes the stru ture of the se ond part tra table and amenable to exa t inferen e.459\nBui, Venkatesh & West BeginFor t = 0; 1; : : :For ea h sample i = 1; : : : ; NSample r(i)t from R(i)t (rtjot)Update weight w(i) = w(i)R(i)t (ot)Compute the posterior RB bel state R(i)t+ = R(i)t (ztjr(i)t ; ot)Compute the new RB belief state R(i)t+1 from R(i)t+Compute h(i) from R(i)t+Compute the estimator f\u0302RBSIS =PNi=1 h(i) ~w(i)End Figure 1: RB-SIS for general DBNThe general RB-SIS algorithm is given in Fig. 1. For illustrating purpose, we assumethat the \\optimal\" qt and the orresponding wt are being used (qt = Rt(rtjot) and wt =Rt(ot)). At ea h time point, we need to maintain N samples ~r(i)t , i = 1; : : : ; N . For ea hsample, in addition to the sample weight w(i), we also need to store a representation of theRB belief state orresponding to that sample sequen e: R(i)t = p(rt; zt; otj~r(i)t 1; ~ot 1) andR(i)t+ = p(ztj~r(i)t ; ~ot).A number of appli ations of the RB-SIS method (also known as the Rao-Bla kwellisedParti le Filter (RBPF)) have been dis ussed in the literature. A general framework forusing RB-SIS to do inferen e on DBNs has been presented by Dou et et al. (2000a), Murphy(2000), Murphy and Russell (2001). However, these authors have mainly fo used on the ase where the sequen e of the Rao-Bla kwellising variables (~rt) is Markov (for example,when the RB variables are the root nodes at ea h time sli e). This assumption simpli esthe sampling step in the RB pro edure sin e obtaining the sample for the RB variableat time t + 1 is straightforward. In our previous work (Bui, Venkatesh, & West, 2000),we introdu ed a hybrid-inferen e method for the AHMM in the spe ial ase of the state-spa e de omposition poli y hierar hy, whi h is essentially an RB-SIS method. Note thatwhen applied to AHMMs, the sequen e of Rao-Bla kwellising variables that we use does notsatisfy the Markov property. In this ase, are must be taken to design an e\u00c6 ient samplingstep, espe ially when the sampling distribution for the next RB variable does not have atra table form. The use of non-Markov RB variables also appears in other spe ial modelssu h as the Bayesian missing data model (Liu & Chen, 1998), and the partially observedGaussian state spa e model (Andrieu & Dou et, 2000) where the RB belief state an bemaintained by a Kalman lter.Sin e we have to make the Rao-Bla kwellised belief state tra table, the ontext vari-ables in the framework of ontext-spe i independen e (Boutilier, Friedman, Goldszmidt,& Koller, 1996) an be used onveniently as Rao-Bla kwellising variables (Murphy, 2000).Indeed, sin e the ontext variable a ts as a mixing gate for di erent Bayesian network stru -tures, onditioning on these variables would simplify the stru ture of the remaining vari-460\nPoli y re ognition in the Abstra t Hidden Markov Modelables. Be ause of this property of the ontext variables, Boutilier et al. (1996) have suggestedto use them as the ut-set variables in the ut-set onditioning inferen e method (Pearl,1988). The ut-set variables play a similar role to the Rao-Bla kwellising variables in whi hthey help to simplify the stru ture of the remaining network. In Rao-Bla kwellised sam-pling, instead of summing over all the possible values of the ut-set variables whi h an beintra table, only a number of representative sampled values are used.The idea of ombining both exa t and approximate inferen e in RB sampling is alsosimilar to the hybrid inferen e s heme des ribed by Dawid, Kj rul , and Lauritzen (1995),however it's un lear if RB sampling an be des ribed using their model of ommuni atingbelief universe. Also, Dawid et al. use hybrid inferen e mainly to do inferen e on networkswith a mixture of ontinuous and dis rete variables, as opposed to RB whose goal is toimprove the sampling performan e.3. Abstra t Markov Poli iesIn this se tion, we formally introdu e the AMP on ept as originating from the literatureof abstra t probabilisti planning with MDPs (Sutton et al., 1999; Parr & Russell, 1997;Forestier & Varaiya, 1978; Hauskre ht et al., 1998; Dean & Lin, 1995). The main motivationin abstra t probabilisti planning is to s ale up MDP-based planning to problems with largestate spa e. It has been noted that a hierar hi al organisation of poli ies an help redu ethe omplexity of MDP-based planning, similar to the role played by the plan hierar hyin lassi al planning (Sa erdoti, 1974). In omparison with a lassi al plan hierar hy, apoli y hierar hy an model di erent sour es of un ertainty in the planning pro ess su h assto hasti a tions, un ertain a tion out omes, and sto hasti environment dynami s.While the work in planning is on erned with nding the optimal poli y given somereward fun tion, our work fo uses on poli y re ognition whi h is the inverse problem, i.e., toinfer the agent's poli ies from wat hing the e e ts of the agent's a tions. The two problemshowever share a ommon element whi h is the model of a sto hasti plan hierar hy. In poli yre ognition, although it is possible to derive some information about the reward fun tionby observing the agent's behaviour, we hoose not to do this, thus omitting from our modelthe reward fun tion and also the optimality notion. This leaves the model open to tra kingarbitrary agent's behaviours, regardless of whether they are optimal or not.3.1 The General Model3.1.1 A tions and Poli iesIn an MDP, the world is modelled as a set of possible states S, termed the state spa e. Atea h state s, an agent has a set of a tions A available, where ea h a tion a, if employed, will ause the world to evolve to the next state s0 via a transition probability a(s; s0). An agent'splan of a tions is modelled as a poli y that pres ribes how the agent would hoose its a tionat ea h state. For a poli y , this is modelled by a sele tion fun tion : S A ! [0; 1\u2104where at ea h state s, (s; a) is the probability that the agent will hoose the a tion a. Itis easy to see that, given a xed poli y , the resulting state sequen e is a Markov hainwith transition probabilities Pr(s0 j s) = Pa (s; a) a(s; s0). Thus, a poli y an also beviewed as a Markov hain through the state spa e.461\nBui, Venkatesh & West3.1.2 Lo al Poli iesIn the original MDP, behaviours are modelled at only two levels: the primitive a tionlevel, and the plan level (poli y). We would like to onsider poli ies that sele t othermore re ned poli ies and so on, down a number of abstra tion levels. The idea is to formintermediate-level abstra t poli ies as poli ies de ned over a lo al region of the state spa e,having a ertain terminating ondition, and an be invoked and exe uted just like primitivea tions (Forestier & Varaiya, 1978; Sutton et al., 1999).De nition 1 (Lo al poli y). A lo al poli y is a tuple = hS;D; ; i where: S is the set of appli able states. D is the set of destination states. : D ! (0; 1\u2104 is the stopping probabilities su hthat (d) = 1;8 d 2 D n S. : S A ! [0; 1\u2104 is the sele tion fun tion. Given the urrent state s, (s; a) is theprobability that the a tion a is sele ted by the poli y at state s.The set S models the lo al region over whi h the poli y is appli able. S will be alled theset of appli able states, sin e the poli y an start from any state in S. We shall assume herethat S is dis rete, and thus shall not be on erned with the te hni al details in generalisingthe AHMM formulation to the ontinuous state spa e ase. The stopping ondition of thepoli y is modelled by a set of possible destination states D and a set of positive stoppingprobabilities (d); d 2 D where (d) is the probability that the poli y will terminate whenthe urrent state is d. It is possible to allow the poli y to stop at some state outside ofS, however, for all d 2 D n S we enfor e the ondition that (d) = 1, i.e., d is a terminaldestination state. Sometimes, we might only want to onsider poli ies with deterministi stopping ondition. In that ase, every destination is a terminal destination: 8d 2 D, (d) = 1. Thus, for a deterministi ally terminating poli y, we an ignore the redundantparameter , and need only spe ify the set of destinations D.Given a starting state s 2 S, a lo al poli y as de ned above generates a Markov se-quen e of states a ording to its transition model. Ea h time a destination state d 2 D isrea hed, the pro ess stops with probability (d). Sin e the pro ess starts from within S,but terminates only in one of the states in D, the destination states play the role of thepossible exits out of the lo al region S of the state spa e.When we want to make lear whi h poli y is urrently being referred to, we shall usethe subs ripted notations S , D , , to denote the elements of the poli y .Fig. 2 illustrates how a lo al poli y an be visualised. Fig. 2(a) shows the set ofappli able states S, the set of destinations D, and a hain starting within S and terminatingin D. The Bayesian network in Fig. 2(b) provides the detailed view of the hain from startto nish. The Bayesian network in Fig. 2( ) is the abstra t view of the hain where we areonly interested in its starting and stopping states.3.1.3 Abstra t Poli iesThe lo al poli y as de ned above sele ts among the set of primitive a tions. Similarly, butmore generally, we an de ne higher level poli ies that sele t among a set of other poli ies.462\nPoli y re ognition in the Abstra t Hidden Markov Model \u03c0\ns d 0 1 2 T\n(b)\ns\nS\nD\nd s d\n\u03c0\n(a) (c)Figure 2: Visualisation of a poli yDe nition 2 (Abstra t Poli y). Let be a set of abstra t poli ies. An abstra tpoli y over the poli ies in is a tuple hS ;D ; ; i where: S [ 2 S is the set of appli able states. D [ 2 D is the set of destination states. : D ! (0; 1\u2104 is the set of stoppingprobabilities. : S ! [0; 1\u2104 is the sele tion fun tion where (s; ) is the probability that sele ts the poli y at the state s.Note the re ursiveness in de nition 2 that allows an abstra t poli y to sele t among a setof other abstra t poli ies. At the base level, primitive a tions are viewed as abstra t poli iesthemselves. Sin e primitive a tions always stop after one time-step, Da Sa and (d) =18d 2 Da (Sutton et al., 1999). The idea that poli ies with suitable stopping ondition an be viewed just as primitive a tions is rst made expli it in (Sutton, 1995), whi halso introdu es the model for representing the stopping probabilities. Their subsequentwork (Sutton et al., 1999) introdu es the abstra t poli y on ept under the name options.The exe ution of an abstra t poli y is as follows. Starting from some state s, sele ts a poli y 2 a ording to the distribution (s; :). The sele ted poli y is thenexe uted until it is terminated in some state d 2 D . If d is also a destination state of (d 2 D ), the poli y stops with probability (d). If still ontinues, a new poli y 0 2 is sele ted by at d, whi h will be exe uted until its termination and so on (Fig. 3).Some remarks about the representation of an abstra t poli y are needed here. Lets 2 [ 2 S , we denote the subset of poli ies in whi h are appli able at s by (s) =f 2 j s 2 S g. For an abstra t poli y to be well-de ned, we have to make sure thatat ea h state s, only sele ts among the poli ies that are appli able at s. Thus, thesele tion fun tion has to be su h that (s; ) > 0 only if 2 (s). This helps to keepthe spe i ation of the sele tion fun tion to a manageable size, even when the set of allpoli ies to be hosen from an be large. In addition, the spe i ation of the sele tionfun tion and the stopping probabilities an make use of fa tored representations (Boutilier,Dearden, & Goldszmidt, 2000) in the ase where the state spa e is the omposite of a setof relatively independent variables. This ensures that we still have a ompa t spe i ation463\nBui, Venkatesh & West \u2019 s S S\u2019 D d\u2019\n\u03c0 \u03c0\ndFigure 3: A hain generated by an abstra t poli yof the probabilities onditioned on the state variable, even though the state spa e an beof high dimension.3.1.4 Poli y Hierar hyUsing abstra t poli ies as the building blo ks, we an onstru t a hierar hy of abstra tpoli ies as follows:De nition 3 (Poli y hierar hy). A poli y hierar hy is a sequen e H = ( 0; 1; : : : ; K)where K is the number of levels in the hierar hy, 0 is a set of primitive a tions, and fork = 1; : : : ;K, k is a set of abstra t poli ies over the poli ies in k 1.When a top-level poli y K is exe uted, it invokes a sequen e of level-(K-1) poli ies, ea hof whi h invokes a sequen e of level-(K-2) poli ies and so on. A level-1 poli y will invokea sequen e of primitive a tions whi h leads to a sequen e of states. Thus, the exe utionof K generates an overall state sequen e (s0; s1; : : : ; st; : : :) that terminates in one of thedestination states in D K . When K = 1 this sequen e is simply a Markov hain (withsuitable stopping onditions). However, for K 2, it will generally be non-Markovian,despite the fa t that all the poli ies are Markov, i.e., they sele t the lower level poli iesbased solely on the urrent state (Sutton et al., 1999). This is be ause knowing the urrentstate st alone does not provide information about the urrent intermediate-level poli ies,whi h an a e t the sele tion of the next state st+1. Intuitively, this means that an agent'sbehaviour to a hieve a given goal is usually non-Markovian, sin e its hoi e of a tionsdepends not only on the urrent state, but also on the urrent intermediate intentions ofthe agent.We term the dynami al pro ess in exe uting a top-level abstra t poli y K the Abstra tMarkov Model (AMM). When the states are only partially observable, the observation anbe modelled by the usual observation model Pr(ot j st) = !(st; ot). The resulting pro ess istermed the Abstra t Hidden Markov Model (AHMM) sin e the states are hidden as in theHidden Markov Model (Rabiner, 1989).The idea of having a higher level poli y ontrolling the lower level ones in an MDP an be tra ed ba k to the work by Forestier and Varaiya (1978), who investigated a twolayer stru ture similar to our 2-level poli y hierar hy with deterministi stopping ondition.Forestier and Varaiya showed that that the sub-pro ess, obtained by sub-sampling the state464\nPoli y re ognition in the Abstra t Hidden Markov Model (a)\n(b)Figure 4: The environment and its partitionsequen e at the time when the level-1 poli y terminates, is also Markov, thus the poli iesat level 1 simply play the role of an \\extended\" a tion. In our framework, given a poli yhierar hy, one an onsider a \\lifted\" model where only the poli ies from level k up and theobservations at the time points when a poli y at level k ends are onsidered. The level-kpoli ies an then be onsidered as primitive a tions, and the lifted model an be treatedlike a normal model.3.2 State-Spa e Region-Based De ompositionIn some ases, the state spa e or some of its dimensions already exhibit a natural hierar hi alstru ture. For example, in the spatial domain, the set of ground positions an be dividedinto small lo al spa es su h as rooms, orridors, et . A set of these lo al spa es an begrouped together to form a larger spa e at the higher level ( oors, buildings, et ). Anintuitive and often-used method for onstru ting the poli y hierar hy in this ase is viathe so- alled region-based de omposition of the state spa e (Dean & Lin, 1995; Hauskre htet al., 1998). Here, the state spa e S is su essively partitioned into a sequen e of partitionsPK ;PK 1; :::P1 orresponding to theK levels of abstra tion, where PK = fSg is the oarsestpartition, and P1 is the nest. For ea h region Ri of Pi, the periphery of Ri, Per(Ri) isde ned as the set of states not in Ri, but onne ted to some state in Ri. Let Peri be theset of all peripheral states at level i: Peri = [Ri2PiPer(Ri). Fig. 4(b) shows an examplewhere the state spa e representing a building is partitioned into 4 regions orresponding tothe 4 rooms. The peripheral states for a region is shown in Fig 4(a), and Fig 4(b) shows allsu h peripheral states.To onstru t the poli y hierar hy, we rst de ne for ea h region R1 2 P1 a set of abstra tpoli ies appli able on R1, and having Per(R1) as the destination states. For example, forea h room in Fig 4, we an de ne a set of poli ies that model the agent's di erent behaviourswhile it is inside the room, e.g., getting out through a parti ular door. These poli ies anbe initiated from inside the room, and terminate when the agent steps out of the room(not ne essarily through the target door sin e the poli y might fail to a hieve its intended465\nBui, Venkatesh & Westtarget). Note that sin e Per(R1) \\ R1 = ;, all the poli ies de ned in this manner havedeterministi stopping onditions.Let the set of all poli ies de ned be 1. At the higher level P2, for ea h region R2,we an de ne a set of poli ies that model the agent's behaviours inside that region withappli able state spa e R2, destination set Per(R2), and the onstraint that these poli iesmust use the poli ies previously de ned at level-1 to a hieve their goals. An example is apoli y to navigate between the room-doors to get from one building gate to another. Letthe set of all poli ies de ned at this level be 2. Continuing doing this at the higher levels,we obtain the poli y hierar hy H = ( 0; 1; 2; : : : ; K). A poli y hierar hy onstru tedthrough State-spa e Region-based De omposition is termed an SRD poli y hierar hy.An SRD poli y hierar hy has the property that the set of appli able states of all thepoli ies at a given abstra tion level forms a partition of the state spa e. Thus, from the statesequen e (s0; : : : ; st; : : :) resulting from the exe ution of the top level poli y, we an inferthe exa t starting and terminating times of all intermediate-level poli ies. For example, atlevel k, the starting/stopping times of the poli ies in this level are the time indi es t's atwhi h the state sequen e rosses over a region boundary: st 1 2 Rk and st 62 Rk for someregion Rk of the partition Pk. Later in se tion 5.1, we will show that this property helps tosimplify some of the omplexity of the poli y re ognition problem.3.3 A Poli y Hierar hy ExampleAs an example, onsider the task to monitor and predi t the movement of an agent througha building shown in Fig. 5(a). Ea h room is represented by a 5 5 grid, and two adja entrooms are onne ted via a door in the enter of their ommon edge. The four entran es tothe building are labeled north (N), west (W), south (S) and east (E). In addition, the doorin the enter of the building (C) a ts like an entran e between the building's north wingand south wing. At ea h state ( ell), the agent an move in 4 possible dire tions ex eptwhen it is blo ked by a wall.The poli y hierar hy to model the agent's behaviour in this environment an be on-stru ted based on region-based de omposition at three levels of abstra tion. Firstly, a regionhierar hy is onstru ted. The partition of the environment onsists of the 8 rooms at level 1,the two wings (north and south) at level 2, and the entire building at level 3. The behavioursof the agent at level 1 (within ea h room) is represented by a set of level 1 poli ies. Forexample, in ea h room, we use 4 level-1 poli ies to model the agent's behaviours of exitingthe room via the 4 di erent doors. These are essentially four Markov hains within the roomwhi h terminate outside of the room. One way to represent these poli ies is to spe ify whi hmovement a tion the agent should take given the urrent position and the urrent heading.At the higher level, the agent's behaviours within ea h wing are spe i ed. For example,we use 3 level-2 poli ies in ea h wing to model the agent's behaviours of exiting the wingvia the 3 wing exits. These poli ies are built on top of the set of level-1 poli ies alreadyde ned. They spe ify whi h level-1 poli ies the agent should take to leave the wing at theintended exit. Finally, at the top level, the agent's behaviours within the entire building an be spe i ed. For example, we use 4 top-level poli ies to model the agent's behavioursof leaving the building via the four building exits N, W, S, E. A sample of these poli iesand their parameters is given in Fig. 5(b). 466\nPoli y re ognition in the Abstra t Hidden Markov Model\n0.9 0.1\nGo to C (level 2 policy)\nUp Down Right 0.1 0.1 0.5\nLevel 3 (current state: W, destination: E)\n0.3\nLevel 1 Policy. (Destination is on the right)\n0.8\n0.2\nGo to right\u2212door (level 1 policy)\nGo to back\u2212door (level 1 policy)\nLevel 2 Policy (current state: W, destination : C)\nPrior for top\u2212level policy\nBack\u2212door\nRight\u2212door\n(a) The environment\n(b) Parameters of the AHMM\nW\nGo to S (level 2 policy)\nW\nN: 0.25, S: 0.25, E: 0.25, W: 0.25\nN\nS\nE\nC\nW\nFigure 5: An example poli y hierar hy3.4 AMM as a Plan Exe ution ModelUp to now, we have presented the AMM as a formal plan exe ution model to be used laterin the plan re ognition pro ess. In this subse tion, we dis uss the expressiveness of theAMM as a formal plan spe i ation language, and also the suitability of using the AMMto en ode plans in the ontext of plan re ognition. Note that the dis ussion here fo uses onthe representational aspe t of the AMM alone. A dis ussion of the omputational aspe tsof the AMM/AHMM in omparison with other works in probabilisti plan re ognition willbe presented in Se tion 7.The AMM is parti ularly well-suited for representing goal-dire ted behaviours at dif-ferent levels of abstra tion. Ea h poli y in the AMM an be viewed as a plan trying toa hieve a parti ular goal. However, unlike a lassi al plan, a poli y spe i es the ourse ofa tions at all appli able states, and is more similar to a ontingent plan. The ending of apoli y ould either means that the goal has been a hieved, or the attempt to a hieve thegoal using the urrent poli y has failed. This interpretation of the persisten e of a poli y ts into the persisten e model of intentions (Cohen & Levesque, 1990): when an intentionends, there is no guarantee that the intended goal has been a hieved. Thus, on eptually,there are two types of destination states: one orresponds to the intended goal states, andthe other orresponds to unintended failure states resulting from the sto hasti nature inthe exe ution of the plan. Due to its generality, the AMM does not need to distinguish be-tween these two types; both the su essful termination states and the unsu essful ones aretreated the same as possible destination states, albeit with di erent rea hing probabilities.44. One would expe t that an agent would more likely to rea h the intended destination state rather arandom failure state. 467\nBui, Venkatesh & WestUsing the AMM as a model of plan exe ution thus allows us to blur the di eren ebetween planning and re-planning. At the same time, it moves from the re ognition ofa lassi al plan towards the re ognition of the agent's intention. Most of the existingframework for probabilisti plan re ognition does not expli itly represent the urrent state,and thus, the relationship between states and the adoption and termination of urrent plansis ignored (Goldman et al., 1999).5 Thus, it would be impossible to tell if the urrent planhas failed and the new plan is an attempt to re over from this failure, or the urrent planhas su eeded and the new plan is part of a new higher level goal.A more expressive language for des ribing abstra t probabilisti plan is the Hierar hi alAbstra t Ma hines (HAM) proposed in (Parr & Russell, 1997; Parr, 1998). In a HAM, theabstra t poli y is repla ed by a sto hasti nite automaton, whi h an all other ma hinesat the lower level. Our abstra t poli ies an be written down as ma hines of this type. Su ha ma hine would hoose one of the ma hines orrespond to the poli ies at the lower leveland then go ba k to the start state after the alled ma hines have terminated. The HAMframework allows for ma hines with arbitrary nite number of ma hine states and transitionprobabilities,6 thus an readily represent more omplex plans su h as on atenation ofpoli ies, alternative poli y paths, et . It is possible to represent ea h ma hine in HAMas a poli y in our AMM, however with the ost of augmenting the state spa e to in ludethe ma hine states of all the ma hines in the urrent all sta k. Thus, the size of theAMM's new state spa e would be exponential with respe t to the number of nested levelsin the HAM's all sta k. While this shows in theory the expressiveness of HAM and ourpoli y hierar hy is the same, performing poli y re ognition on the HAM-equivalent poli yhierar hy is probably unwise sin e the state spa e be omes exponentially large after the onversion. A better idea would be to represent the internal state of ea h ma hine as avariable in a DBN and perform inferen e on this DBN stru ture dire tly.The AMM is also losely related to a model for probabilisti plan re ognition alled theProbabilisti State-Dependent Grammar (PSDG), independently proposed in (Pynadath,1999; Pynadath & Wellman, 2000). The PSDG an be des ribed as the Probabilisti Context Free Grammar (PCFG) (Jelinek, La erty, & Mer er, 1992), augmented with astate spa e, and a state transition probability table for ea h terminal symbol of the PCFG.In addition, the probability of ea h produ tion rule is made state dependent. As a result,the terminal symbol now a ts like primitive a tions and the non-terminal symbol hooses itsexpansion depending on the urrent state. Interestingly, the PSDG is dire tly related to theHAM language des ribed above, similar to the way produ tion-rule grammars are relatedto nite automata. Given a PSDG, we an onvert it to an equivalent HAM by onstru tinga ma hine for ea h non-terminating symbol, and modelling the produ tion rules for ea hnon-terminating symbol by the automaton.Our poli y hierar hy is equivalent to a spe ial lass of PSDG where only produ tionrules of the form X ! Y X and X ! ; are allowed. The former rule models the adoptionof a lower level poli y Y by a higher level poli y X, while the latter models the terminationof a poli y X. The PSDG model onsidered in (Pynadath, 1999; Pynadath & Wellman,2000) allows for more general rules of the form X ! Y1 : : : YmX, i.e., the re ursion symbol5. with the ex eptions of (Goldman et al., 1999; Pynadath & Wellman, 2000) whi h will be dis ussed indetail in Se tion 7.6. with the onstraint that there is no re ursion in the alling sta k to keep the sta k nite.468\nPoli y re ognition in the Abstra t Hidden Markov Modelmust be lo ated at the end of the expansion. Thus in a PSDG, a poli y might be expandedinto a sequen e of poli ies at the lower level whi h will be exe uted one after another before ontrol is returned to the higher level poli y. The impli it assumption here is that when apoli y in the sequen e terminates, it always does so at a state where the next poli y in thesequen e is appli able. Given this assumption, in the language of the AHMM we an de nea ompound poli y k as a poli y that simply and orderly exe utes a sequen e of poli ies atthe lower level k 1(1) ; : : : ; k 1(m) , independent of the urrent state. A PSDG is then equivalentto an AHMM if ompound poli ies of this form are allowed.Sin e the AMM losely follows the models used in abstra t probabilisti planning, it anbe used to model and re ognise the behaviours of any autonomous agent whose de isionmaking pro ess is equivalent to an abstra t MDP. It is also useful as a formal languagefor spe ifying ontingent plans whose exe ution an then be monitored using the poli yre ognition algorithm. The language is also ri h enough to spe ify a range of useful humanbehaviours, espe ially in domains where there is a natural hierar hi al de omposition ofthe state spa e. Se tion 6 presents an appli ation of the AHMM framework to the problemof re ognising people behaviours in a omplex spatial environment. Here, ea h poli y ofthe AHMM represents the evolution of possible traje tories of people movement while theperson performs a ertain task in the environment su h as heading towards a door, usingthe omputer at a ertain lo ation, et . The poli ies at di erent levels would represent theevolution of traje tories at di erent levels of abstra tion. Due to the existing hierar hy inthe domain, the poli ies an be onstru ted using the region-based de omposition of thestate spa e. The environment is populated with multiple ameras divided into di erentzones that an provide the urrent lo ation of the tra king target, albeit a noisy one. Thenoisy observations an be readily handled by the observation model in the AHMM. Thepoli y re ognition algorithm an then be applied to infer the person's urrent poli y atdi erent levels in the hierar hy.One main restri tion of the urrent AHMM model is that we onsider only one top-level poli y at a time, thus are unable to model the inter-leaving of on urrent plans.Another more subtle restri tion is the assumption that a high level poli y sele ts the lowerlevel poli ies depending only on the urrent state. If the state spa e is interpreted as thestates of the external environment, this assumption implies that the a tor either has fullobservation about the urrent state, or at least re nes its intentions based on the a tor'sobservation about the urrent state only (and not the entire observation history). Note thatthese restri tions of the AHMM also apply in the ase of the PSDG model.4. Dynami Bayesian Network RepresentationIn this se tion, we des ribe the Dynami Bayesian Network (DBN) representation of theAHMM. The network serves two purposes: (1) as the tool to derive the probabilisti in-dependen e property of this sto hasti model, and (2) as the omputational framework forthe poli y re ognition algorithms in Se tion 5.4.1 Network Constru tionAt time t, let st represent the urrent state, kt represent the urrent poli y at level k(k = 0; : : : ;K), ekt represent the ending status of kt , i.e., a boolean variable indi ating469\nBui, Venkatesh & West \u03c0\ne e\ns\n\u03c0\ne e\ns\n=F\n=F\n(a) (b) (c)\nk\nk k-1\n\u03c0\ne e\ns\n=T\nk\nk k-1\nk\nk-1\nk\nFigure 6: Sub-network for poli y terminationwhether the poli y kt terminates at the urrent time. These variables would make up the urrent time-sli e of the full DBN. For our onvenien e, the notation allt refers to the set ofall the urrent poli ies f Kt ; : : : ; 0t g. Before presenting the full network, we rst des ribethe two sub-stru tures that model how poli ies are terminated and sele ted. The full DBN an then be easily onstru ted from these sub-stru tures.4.1.1 Poli y TerminationFrom the de nition of abstra t poli ies, a level-k poli y kt terminates only if the lower levelpoli y k 1t terminates, and if so, kt terminates with probability kt (st). In the Bayesiannetwork representation, the terminating status ekt therefore has three parent nodes: kt , st,and ek 1t (Fig. 6(a)).The parent variable ek 1t however plays a spe ial role. If ek 1t = T , meaning the lowerlevel poli y terminates at the urrent time, Pr(ekt = T j kt ; st) = kt (st) whi h gives the onditional probability of ekt given the other two parent variables (Fig. 6(b)). However, ifek 1t = F , kt should not terminate and so ekt = F . Therefore, given that ek 1t = F , ekt isdeterministi ally determined and is independent of the other two parent variables kt andst. Using the notion of ontext-spe i independen e (CSI) (Boutilier et al., 1996), we anthen safely remove the links from the other two parents to ekt in the ontext that ek 1t isfalse (Fig. 6( )).At the bottom level, sin e the primitive a tion always terminates immediately, e0t = Tfor all t. Sin e we are modelling the exe ution of a single top-level poli y K , we an assumethat the top-level poli y does not terminate and remains un hanged: eKt = F and Kt = Kfor all t. Also, note that elt = T ) ekt = T for all k l, and elt = F ) ekt = F for all k l.Thus, at ea h time t, there exists 0 lt < K su h that ekt = T for all k lt, and ekt = Ffor all k > lt. The variable lt is termed the highest level of termination at time t. Knowingthe value of lt is equivalent to knowing the terminating status of all the urrent poli ies.4.1.2 Poli y Sele tionThe urrent poli y kt in general is dependent on the higher level poli y k+1t , the previousstate st 1, the previous poli y at the same level kt 1 and its ending status ekt 1. In theBayesian network, kt thus has these four variables as its parents (Fig. 7(a)). This depen-470\nPoli y re ognition in the Abstra t Hidden Markov Model \u03c0\u03c0\ns\n\u03c0\n\u03c0\u03c0\ns\n\u03c0\n\u03c0\u03c0\ns\n\u03c0\n(a) (b) (c)\ne e e = F = T prev\nk+1 k\nprev k\nprev k\nk+1 k\nk+1\nkk prev prev k\nprevprev\nk\nprev k prev\nFigure 7: Sub-network for poli y sele tionden y an be further broken down into two ases, depending on the value of the parentnode ekt 1.If the previous poli y has not terminated (ekt 1 = F ), the urrent poli y is the same asthe previous one: kt = kt 1, and the variable kt is thus independent of k+1t and st 1.Therefore, in the ontext ekt 1 = F , the two links from k+1t and st 1 to the urrent poli y an be removed, and the two nodes kt and kt 1 an be merged together (Fig. 7(b)).If the previous poli y has terminated (ekt 1 = T ), the urrent poli y is sele ted by thehigher level poli y with probability Pr( kt j k+1t ; st 1) = k+1t (st 1; kt ). In this ontext, ktis independent of kt 1 and the orresponding link in the Bayesian network an be removed(Fig. 7( )).4.1.3 The Full DBNThe full dynami Bayesian network an be onstru ted for all the poli y, ending status, andstate variables by putting the sub-networks for poli y termination and sele tion together(Fig. 8). At the top level, sin e eKt = F , we an remove the ending status nodes and mergeall the Kt into a single node K . At the base level, sin e e0t = T , we an remove the endingstatus nodes and also the links from 0t to 0t+1. To model the observation of the hiddenstates, an observation layer an be atta hed to the state layer as shown in Fig. 8.Suppose that we are given a ontext where ea h of the variable ekt is known. We anthen modify the full DBN using the orresponding link removal and node merging rules.The result is a more intuitive tree-shaped network in Fig. 9, where all the poli y nodes orresponding to the same poli y for its entire duration are grouped into one. The grouping an be done sin e knowing the value of ea h ekt is equivalent to knowing the exa t durationof ea h poli y in the hierar hy. One would expe t that performing probabilisti inferen eon this stru ture is more simple than that of the full DBN in Fig. 8. In parti ular, if thestate sequen e is known, the remainder of the network in Fig. 9 be omes singly- onne ted,i.e., a dire ted graph with no undire ted y les, allowing inferen e to be performed with omplexity linear to the size of the network (Pearl, 1988). The poli y re ognition algorithmsthat follow later exploit extensively this parti ular tra table ase of the AHMM.471\nBui, Venkatesh & West\ne\ne\nAction\nK\n2 2 1 0 1\nLevel K\nStop status\nPolicy Policy Stop status State Observation \u03c0\n\u03c0\n\u03c0 \u03c0\nPoli y re ognition in the Abstra t Hidden Markov Model4.2 Conditional Independen e in the Current Time-Sli eThe above dis ussion identi es a tra table ase for the AHMM, but it requires the knowledgeof the entire history of the state and the poli y ending status variables. In this subse tion,we fo us on the onditional independen e property of the nodes in the urrent time-sli e:st; 0t ; : : : ; Kt . Sin e these nodes will make up the belief state of any future inferen ealgorithm for our AHMM, any independen e properties among these variables, if exploited, an provide a more ompa t representation of the belief state and redu e the inferen e omplexity.Due to the way poli ies are invoked in the AMM, we an make an intuitive remarkthat the higher level poli ies an only in uen e what happens at the lower level through the urrent level. More pre isely, for a level k poli y kt , if we know its starting state, the ourseof its exe ution is fully determined, where being determined here means without in uen efrom what is happening at the higher levels. Furthermore, if we also know how long thepoli y has been exe uted, or equivalently its starting time, the urrent state of its exe utionis also determined. Thus, the higher level poli ies an only in uen e the urrent state ofexe ution of kt either through its starting state or starting time. In other words, if we know kt together with its starting time and starting state, then the urrent higher level poli iesare ompletely independent of the urrent lower level poli ies and the urrent state. Thetheorem 1 below formally states this in a pre ise form. Note that the ondition obtainedis the stri test: if one of the three onditional variables is unknown, there are examples ofAMMs in whi h the higher level poli ies an in uen e the lower level ones.Theorem 1. Let kt and bkt be two random variables representing the starting time and thestarting state, respe tively, of the urrent level-k poli y kt : kt = maxft0 < t j ekt0 = Tg andbkt = s kt . Let >kt = f k+1t ; : : : ; Kt g denote the set of urrent poli ies from level k + 1 upto K, and <kt = fst; 0t ; : : : ; k 1t g denote the set of urrent poli ies from level k 1 downto 0 together with the urrent state. We have: >kt ? <kt j kt ; bkt ; kt (7)Proof. We sket h here an intuitive proof of this theorem through the use of the Bayesiannetwork manipulation rules for ontext-spe i independen e whi h have been dis ussed in4.1.1 and 4.1.2. An alternative proof that does not use CSI an be found in (Bui et al.,2000).We rst note that the theorem is not obvious by looking at the full DBN in Fig. 8.Therefore, we shall pro eed by modifying the network stru ture in the ontext that weknow kt .At time kt , all the poli ies at level k and below must terminate: el kt = T for all l k.Thus we an remove all the links from these poli ies to the new poli ies at time kt + 1.On the other hand, from time kt + 1 until the urrent time t, all the poli ies at level kand above must not terminate: elt0 = F for all l k, kt +1 t0 < t. Thus we an group allthe poli ies at level l k between time kt +1 and t into one node representing the urrentpoli y at level l.These two network manipulation steps result in a network with the stru ture shown inFig. 10. On e the modi ed network stru ture is obtained, we an observe that >kt and473\nBui, Venkatesh & West\nk\n... ... ......\nk t k-1 t\nt k+1\nk t\nt t\nTime\nState\n\u03c0 \u03c0\n\u03c0\nt\nb \u03c4 sFigure 10: Network stru ture after being onditioned on kt <kt are d-separated by kt and bkt in the new stru ture. Thus >kt and <kt are independentgiven kt , bkt and kt .5. Poli y Re ognitionIn this se tion we begin to address the problem of poli y re ognition in the framework ofthe AHMM. We assume that a poli y hierar hy is given and is modelled by an AHMM,however the top level poli y and the details of its exe ution are unknown. The problem isthen to determine the top level poli y and other urrent poli ies at the lower levels giventhe urrent sequen e of observations. In more on rete terms, we are interested in the onditional probability: Pr( Kt ; : : : ; 0t j ~ot 1)and espe ially, the marginals: Pr( kt j ~ot 1); for all levels kComputing these probabilities gives us the information about the urrent poli ies at alllevels of abstra tion, from the urrent a tion (k = 0), to the top-level poli y (k = K),taking into a ount all the observations that we have up to date.In typi al monitoring situations, these probabilities need to be omputed \\online\", asea h new observation be omes available. To do this, it is required to update the beliefstate ( ltering distribution) of the AHMM at ea h time point t. This problem is generallyintra table unless the belief state has an e\u00c6 ient representation that a ords a losed formupdate pro edure. In our ase, the belief state is a joint distribution of K + 3 dis retevariables: Pr( Kt ; : : : ; 0t ; st; lt j ~ot). Without any further stru ture imposed on the beliefstate, the omplexity for updating it is exponential in K.To ope with this omplexity, one generally has to resort to some form of approximationto trade o a ura y for omputational resour es. On the other hand, the analysis of the474\nPoli y re ognition in the Abstra t Hidden Markov ModelAHMM network in the previous se tion suggests that the problem of inferen e in the AHMM an be tra table in the spe ial ase when the history of the state and terminating statusvariables is known. Motivated by this property of the AHMM, our main aim in this se tionis to derive a hybrid inferen e s heme that ombines both approximation and tra tableexa t inferen e for e\u00c6 ien y. We rst treat the spe ial ase of poli y re ognition wherethe belief state of the AHMM has a tra table stru ture in 5.1. We then present a hybridinferen e s heme for the general ase using the Rao-Bla kwellised Sequential Importan eSampling (RB-SIS) method in 5.2.5.1 Poli y Re ognition: the Tra table CaseHere, we address the poli y re ognition problem under two assumptions: (1) the statesequen e an be observed with ertainty, and (2) the exa t time when ea h poli y startsand ends is known. More pre isely, our observation at time t in ludes the state history~st = (s0; : : : ; st) and the poli y termination history ~lt = (l0; : : : ; lt). The belief state thatwe need to ompute in this ase is Bt = Pr( allt ; st; lt j ~st 1; ~lt 1) and its posterior afterabsorbing the observation at time t: Bt+ = Pr( allt j ~st; ~lt).The rst assumption means that the observer always knows the true urrent state and isoften referred to as \\full observability\". When the states are fully observable, we an ignorethe observation layer fotg in the AHMM and thus only have to deal with the AMM instead.The se ond assumption means that the observer is fully aware when the urrent poli yends and a new poli y begins. If the poli y hierar hy is onstru ted from the region-basedde omposition of the state spa e (subse tion 3.2), the termination status an be inferreddire tly from the state sequen e. Thus for SRD poli y hierar hies, only the full observability ondition is needed sin e the se ond assumption is subsumed by the rst and an be leftout. Ex ept for SRD poli y hierar hies, these two assumptions are usually too restri tivefor the poli y re ognition algorithm presented here to be useful by itself. However, thealgorithm for this spe ial ase will form the exa t step in the hybrid algorithm presented insubse tion 5.2 for the general ase.5.1.1 Representation of the belief stateWe rst look at the onditional joint distribution Pr( allt ; st j ~st 1; ~lt 1). From the termina-tion history ~lt 1, we an derive pre isely the starting time of the urrent level-k poli y: kt = maxf0g [ ft0 < tj ekt0 = Tg = maxf0g [ ft0 < tj lt0 kgOn the other hand, knowing the starting time together with the state history also givesus the starting state bkt . Thus, both the starting time and the starting state of kt an bederived from ~st 1 and ~lt 1. From Theorem 1, we obtain for all level k: >kt ? <kt j kt ; ~st 1; ~lt 1In other words, given ~st 1 and ~lt 1, the onditional joint distribution of f Kt ; : : : ; 0t ; stg an be represented by a Bayesian network with a simple hain stru ture. We denote this hain network by Ct Pr( allt ; st j ~st 1; ~lt 1) and term it the belief hain for the role it playsin the representation of the belief state (Fig. 11(a)). If a hain is drawn so that all links475\nBui, Venkatesh & West root K k+1 k\nk-1\n1 0 0\n1\nk-1\nk\nk+1\nK\ne\ne\ne\ne\ne K\nk+1 k k-1 1\ntBC t\n(a) (b)\ns\n\u03c0 \u03c0\n\u03c0\n\u03c0\n\u03c0 \u03c0\ns\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\nFigure 11: Representation of the belief statepoint away from the level-k node, we say that the hain has root at level k. The root of the hain an be moved from k to another level k0 simply by reversing the links lying betweenk and k0 using the standard link-reversal operation for Bayesian networks (Sha hter, 1986).Ea h node in the belief hain also has a manageable size. In prin iple, the domain of kt is k, the set of all poli ies at level k, and the domain of st is S, the set of all possiblestates. When K is large, we basi ally want to model a larger state spa e, and the setof poli ies to over this state spa e is also large. The sizes of these domains would mostlikely grow exponential w.r.t. K. However, given a parti ular state, the number of poli iesappli able at that state would remain relatively onstant and independent of K. For ea hpoli y kt , we know its starting state bkt , whi h implies that kt 2 k(bkt ), the set of alllevel-k poli ies appli able at bkt . Thus k(bkt ) an be used as the \\lo al\" domain for ktto avoid the exponential dependen y on K. Similarly, the domain for st an be taken asthe set of neighbouring states of st 1 (rea hable from st 1 by performing one primitivea tion). For a given state, we term the maximum number of relevant obje ts (appli ablepoli ies/a tions, neighbouring states) at a single level the degree of onne tivity N of thedomain being modelled. The size of the onditional probability table for ea h link of thebelief hain is then O(N 2), and the overall size of the belief hain is O(KN 2).We now an onstru t the belief state Bt from Ct. Sin e the urrent terminating statusis solely determined by the urrent poli ies and the urrent state, the belief state Bt an befa torised into:Pr( allt ; st; lt j ~st 1; ~lt 1) = Pr(lt j allt ; st) Pr( allt ; st j ~st 1; ~lt 1) = Pr(lt j allt ; st)CtNote that the variable lt is equivalent to the set of variables feKt ; : : : ; e1t g. Thus, the fullbelief state Bt an be realised by adding to Ct the links from the urrent poli ies and the urrent state to the terminating status variables ekt (Fig. 11(b)). The size of the belief state476\nPoli y re ognition in the Abstra t Hidden Markov Model\n0\n1\nk-1\nk\nk+1\nK\ne\ne\ne\ne\ne K\nk+1 k k-1 1\nk-1\nk\nk+1\nK\ne\ne\ne K\nk+1 k\n(a) (b)\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\nFigure 12: Belief state updating: from Bt to Bt+would still be O(KN 2). If the state is a omposite of many orthogonal variables, a fa toredrepresentation an be used so that the size of the belief state representation does not dependexponentially on the dimensionality of the state spa e. We dis uss fa tored representationsfurther under subse tion 5.2.2.5.1.2 Updating the belief stateSin e the belief state Bt an be represented by a simple belief network in Fig. 11(b), we anexpe t that a general exa t inferen e method for updating the belief state su h as (Kj rul ,1995) will work e\u00c6 iently. However, this general method works with undire ted networkrepresentation of the belief state distribution whi h an be in onvenient for us later on whenwe want to sample from su h a distribution. Here, we des ribe an algorithm that updatesthe belief state in the losed form given by the dire ted network in Fig. 11(b).Assuming that we have a omplete spe i ation of the belief state Bt, i.e., all the pa-rameters for its Bayesian network representation, we need to ompute the parameters forthe new network Bt+1. This is done in two steps, as in the standard \\roll-over\" of the beliefstate of a DBN: (1) absorbing the new eviden e st, lt and (2) proje ting the belief stateinto the next time step.The rst step orresponds to the instantiation of the variables st, e1t ; : : : ; eKt in theBayesian network Bt to obtain Bt+ whi h is the onditional joint distribution of Kt ; : : : ; 0t .By he king the onditional independen e relationships in Fig. 11(b), it is easy to see thatBt+ again has a simple hain network stru ture. Thus, on eptually, the problem here isto update the parameters of the hain Ct so as to absorb the given eviden e to form a new hain Bt+. This an be done by a number of link-reversal steps as follows.To instantiate st, we rst move the root of the hain Ct to st. The variable st then hasno parents and an be instantiated and deleted from the network (Fig. 12(a)).To instantiate lt whi h is equivalent to the value assignment (eKt = F; : : : ; elt+1t = F; eltt =T; : : : e1t = T ), starting from k = 1, we iteratively reverse the links from k 1t to kt andfrom kt to ekt (Fig. 12(b)). In algebrai forms, the rst link reversal operation orresponds477\nBui, Venkatesh & Westto omputing the following probabilities:Pr( kt j st; e1t ; : : : ; ek 1t ) = X k 1t Pr( kt j k 1t ) Pr( k 1t j st; e1t ; : : : ; ek 1t ) (8)Pr( k 1t j kt ; st; e1t ; : : : ; ek 1t ) / Pr( kt j k 1t ) Pr( k 1t j st; e1t ; : : : ; ek 1t ) (9)and the se ond link reversal orresponds to:Pr( kt j st; e1t ; : : : ; ekt ) / Pr(ekt j kt ; st; ek 1t ) Pr( kt j st; e1t ; : : : ; ek 1t ) (10)E e tively, the k-th link reversal step positions the root of the hain Ct at kt and absorbsthe eviden e ekt . By repeating this link reversal operations with k = 1; : : : ; lt+1, we obtaina new hain for Bt+ whi h has root at level lt+1. Note that there is no need to in orporatethe instantiations ekt = F for k > lt + 1 sin e they are the dire t onsequen es of theinstantiation elt+1t = F . The parameters of the hain Bt+ are given below. The upwardlinks remain the same as those of Ct, while the marginal at level lt + 1 and the downwardlinks are obtained as the results of the link reversal operations above:Pr( k+1t j kt ; st; lt) = Pr( k+1t j kt ); k lt + 1Pr( kt j st; lt) = Pr( kt j st; e1t ; : : : ; ekt ); k = lt + 1Pr( k 1t j kt ; st; lt) = Pr( k 1t j kt ; st; e1t ; : : : ; ek 1t ); k ltIn the se ond step, we ontinue to ompute Ct+1 from Bt+. Sin e all the poli ies atlevels higher than lt do not terminate, >ltt+1 = >ltt , and we an retain this upper sub- hainfrom Bt+ to Ct+1. In the lower part, for k lt, a new poli y kt+1 is reated by the poli y k+1t+1 at the state st, and thus a new sub- hain an be formed among the variables <ltt+1 withparameters Pr( kt+1 j k+1t+1 ; st) = k+1t+1 (st; kt+1). Note that the domain of the newly- reatednode kt+1 is k(st). The new hain Ct+1 is then the ombination of these two sub- hains,whi h will be a hain with root at level lt + 1 (see Fig. 13). On e we have the hain Ct+1,the new belief state Bt+1 an be obtained by simply adding the terminating status variablesfekt+1g to Ct+1.This ompletes the pro edure for updating the belief state from Bt to Bt+1, thus allowingus to ompute the belief state Bt at ea h time step. Although the belief state is the jointdistribution of all the urrent variables, due to its simple stru ture, the marginal distributionof a single variable an be omputed easily. For example, if we are only interested in the urrent level-k poli y kt , the marginal probability Pr( kt j ~st 1; ~lt 1) is simply the marginalat the level-k node in the hain Ct, and an be readily obtained from the hain parameters.The omplexity of the belief state updating pro edure at time t is proportional to ltsin e it only needs to modify the bottom lt levels of the belief state. On the other hand, theprobability that the urrent poli y at level l terminates an be assumed to be exponentiallysmall w.r.t. l. Thus, the average updating omplexity at ea h time-step is O(Pl l=exp(l))whi h is onstant-bounded, and thus does not depend on the number of levels in the poli yhierar hy. In terms of the number of poli ies and states, the updating omplexity is linearto the size of a poli y node in the belief hain, thus is linear to the degree of onne tivityof the domain. 478\nPoli y re ognition in the Abstra t Hidden Markov Model\nB\nC\nt+\nt+1\nK l 1\n0\nl 0 t+1 t+1\nt+1t\nt\nt\nt\nt\nt\nl\n+1\n\u03c0 \u03c0\n\u03c0\u03c0\n\u03c0 s s \u03c0\u03c0\nFigure 13: Belief state updating: from Bt+ to Ct+15.2 Poli y Re ognition: The General CaseWe now return to the general ase of poli y re ognition, i.e., without the two assumptions ofthe previous subse tion. This makes the inferen e tasks in the AHMM mu h more di\u00c6 ult.Sin e neither the starting times nor the starting states of the urrent poli ies are knownwith ertainty, theorem 1 annot be used. Thus, the set of urrent poli ies no longer formsa hain stru ture as it did in Ct sin e the onditional independen e properties of the urrenttime-sli e no longer hold. We therefore annot hope to represent the belief state by a simplestru ture as we did previously. An exa t method for updating the belief state will thus haveto operate on a stru ture with size exponential in K, and is bound to be intra table whenK is large.To ope with this omplexity, an approximation s heme su h as sequential importan esampling (SIS) (Dou et et al., 2000b; Liu & Chen, 1998; Kanazawa et al., 1995) an beemployed. In our previous work (Bui, Venkatesh, & West, 1999), we have applied an SISmethod known as the likelihoodweighting with eviden e reversal (LW-ER) (Kanazawa et al.,1995) to an AHMM-like network stru ture. However the SIS method needs to sample in theprodu t spa e of all the layers of the AHMM and thus be omes less a urate and ine\u00c6 ientwith large K. The key to get around this ine\u00c6 ien y is to utilise the spe ial stru ture ofthe AHMM, parti ularly, its spe ial tra table ase, to keep the set of variables that need tobe sampled to a minimum.The improvement of the SIS method to a hieve this is has been presented in subse -tion 2.5 in the name of the Rao-Bla kwellised SIS (RB-SIS) method. Rao-Bla kwellisationspe i ally allows the marginalisation of some variables analyti ally and only samples theremaining variables. As a result, this redu es the averaged error, measured as the varian eof the estimator (Casella & Robert, 1996). 479\nBui, Venkatesh & WestIn order to apply RB-SIS to the AHMM, the main problem is to identify whi h vari-ables should be used as the Rao-Bla kwellising variables and should still be sampled, withthe remaining variables being marginalised analyti ally. The key to hoosing the Rao-Bla kwellising variables, as we have shown in 2.5, is so that if those variables an be observed,the Rao-Bla kwellised belief state be omes tra table. In subse tion 5.1, we have demon-strated that if the state history ~st and the terminating status history ~lt an be observedthen the belief state has a simple network stru ture and an be updated with onstant av-erage omplexity. Thus, (st; lt) an be used onveniently as the Rao-Bla kwellising variablert. Note that the variables ~lt are the ontext variables whi h help to simplify the networkstru ture of the AHMM, while the state variables ~st help to make the remaining networksingly- onne ted so that exa t inferen e an operate e\u00c6 iently (see subse tion 4.1.3).5.2.1 RB-SIS for AHMMWe now dis uss the spe i appli ation of RB-SIS to the problem of belief state updatingand poli y re ognition in the AHMM. Our main obje tive is to use RB-SIS to estimate the onditional probability of the poli y urrently being exe uted at level-k given the urrentsequen e of observations Pr( kt+1 j ~ot).Mapping the RB-SIS general framework in subse tion 2.5 to the AHMM stru ture, theset of all urrent variables xt is now the set of urrent poli ies, terminating status nodes,and the urrent state: xt = ( allt ; st; lt). The probability under estimation Pr( kt+1 j ~ot) anbe viewed as an expe tation by letting f( allt ; st; lt) = Pr( kt+1j allt ; st; lt) so that: f = X allt ;st;lt Pr( kt+1j allt ; st; lt) Pr( allt ; st; ltj~ot) = Pr( kt+1 j ~ot)Using RB-SIS to estimate this expe tation, we shall split xt into two sets of variables:the set of RB variables rt = (st; lt), and the set of remaining variables zt = allt whi h is theset of all the urrent poli ies. The fun tional h, whi h depends only on the RB variablesand is obtained from f by integrating out the remaining variables (Eq. (5)), now has theform: h(~rt) = h(~st; ~lt) =X allt Pr( kt+1 j allt ; st; lt) Pr( allt j ~st; ~lt; ~ot) = Pr( kt+1 j ~st; ~lt) (11)whi h is the marginal Ct+1( kt+1) from the belief hain at time t+ 1.The RB belief state, whi h is the belief state of the AHMM when the RB variables areknown, be omes:Rt = Pr( allt ; st; lt; ot j ~st 1; ~lt 1; ~ot 1) = Pr( allt ; st; lt; ot j ~st 1; ~lt 1) (12)and is identi al to the spe ial belief state Bt dis ussed in subse tion 5.1, ex ept a minormodi ation to atta h the observation variable ot.From (11) and (12), both the h fun tion and the RB belief state an be omputedvery e\u00c6 iently using the exa t inferen e te hniques des ribed in 5.1. Thus RB-SIS an beimplemented e\u00c6 iently with minimal overhead in exa t inferen e.The main RB-SIS algorithm for the AHMM is given in Fig. 14. Note that we only needto sample the RB variables ~st and ~lt. For ea h sample i, in addition to the weights w(i),480\nPoli y re ognition in the Abstra t Hidden Markov Model BeginFor t = 0; 1; : : :For ea h sample i = 1; : : : ; NSample s(i)t ; l(i)t from B(i)t (st; ltj ot)Update weight w(i) = w(i)B(i)t (ot)Compute the posterior RB bel state B(i)t+ = B(i)t ( allt js(i)t ; l(i)t ; ot)Compute the belief hain C(i)t+1 from B(i)t+Compute the new belief state B(i)t+1 from C(i)t+1Compute h(i) = C(i)t+1( kt+1)Compute the estimator Pr( kt+1 j ~ot) f\u0302RBSIS =PNi=1 h(i) ~w(i)End Figure 14: RB-SIS for poli y re ognition\n0\n1\nk\u22121\nk\nk+1\nK\ne\ne\ne\ne K\nk+1 k k\u22121 1 e\nB t\n= F\n= T highest level\nof termination lt\ner\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0\ns\noFigure 15: Sampling the Rao-Bla kwellising variables in AHMMwe also maintain a parametri representation of the Rao-Bla kwellised belief state B(i)t , andthe value of the h fun tion for that sample h(i). The weights of the samples, together withthe values of the h fun tion an then be ombined to yield an approximation for f .Some details on how we an obtain the new samples at ea h time step are worth notinghere. Sin e we are using the optimal sampling distribution qt = Bt(st; ltj ot) to sample the481\nBui, Venkatesh & WestRB variables st and lt, we need to perform the eviden e reversal step.7 This an be doneby positioning the root of the belief hain Ct at st and reverse the link from st to ot. Thisgives us the network stru ture for Bert = Bt(st; lt; allt j ot) whi h is exa tly the same as Bt(see Fig. 15), ex ept that the eviden e ot has been absorbed into the marginal distributionof st. The weight wt = Bt(ot) an also be obtained as a by-produ t of this eviden e reversalstep. In order to sample st and lt from Bert without the need to ompute the marginaldistribution for these two variables, we an use forward sampling to sample every variableof Bert , starting from the root node st and pro eeding upward. Sin e lt by de nition is thehighest level of poli y termination, the sampling an stop at the rst level k where ekt = F .We an then assign lt the value k 1. Any unne essary samples for the poli y nodes alongthe way are dis arded. On e we have the new samples for st and lt, the updating of theRB belief state from Bt to Bt+1 is identi al to the belief state updating pro edure des ribedin 5.1. The h fun tion an then be obtained by omputing the orresponding marginal ofthe new belief hain Ct+1.At ea h time step, the omplexity of maintaining a sample (sampling the new RBvariables and updating the RB belief state) is again O(lt), and thus, on average, boundedby a onstant. The overall omplexity of maintaining every sample is thus O(N) on average.If a predi tion is needed, for ea h sample, we have to ompute h by manipulating the hainCt+1 with the omplexity O(K). Thus the omplexity at the time step when a predi tionneeds to be made is O(NK).In omparison with the use of an SIS method su h as LW-ER, the RB-SIS has thesame order of omputational omplexity (the SIS also has omplexity O(NK)). However,while the SIS method needs to sample every layers of the AHMM, the RB-SIS method onlyneeds to sample two sequen es of variables ~st, ~lt, and avoids having to sample the K poli ysequen es f~ kt g. After Rao-Bla kwellisation, the dimension of the sample spa e be omesmu h smaller, and more importantly, does not grow with K. As a result, the a ura y ofthe approximation by the RB-SIS method does not depend on the height of the hierar hyK. In ontrast, due to the problems of sampling in high dimensional spa e, the a ura y ofSIS methods tends to degrade, espe ially when K is large.5.2.2 Performing Eviden e Reversal with a Fa tored State Spa eIn many ases, the state spa e S is the Cartesian produ t of many state variables repre-senting relatively independent properties of a state: st = (s1t ; s2t ; : : : ; sMt ). Sin e the overallstate spa e is very large, spe ifying an a tion by the usual transition probability matrix isproblemati . It is advantageous in this ase to represent the state information in a fa toredform, i.e., representing ea h state variable smt in a separate node rather than lumping theminto a single node st. It has been shown that using fa tored representations, we an spe ifythe transition probability of ea h a tion in a ompa t form sin e an a tion is likely to a e tonly a small number of state variables and the spe i ation of the e e ts of a tions hasmany regularities (Boutilier et al., 2000).7. The term eviden e reversal is used in this paper to refer to a general pro edure in whi h the link tothe observation node is reversed prior to sampling (Kanazawa et al., 1995), thus allowing us to samplea ording to the optimal sampling distribution qt.482\nPoli y re ognition in the Abstra t Hidden Markov ModelThe representation of the belief hain Ct and also the RB belief state Bt an take dire tadvantage of this fa tored representation of a tions. Indeed, the hain parameter Ct(stj 0t )of the link from 0t to st is pre isely the transition probability for the a tion 0t at theprevious state st 1 (note that st 1 is known due to Rao-Bla kwellisation). This onditionaldistribution an be extra ted from the ompa t fa tored representation of 0t in the generalform of a Bayesian network of the variables fs1t ; s2t ; : : : ; sMt g. For our onvenien e, let usdenote this Bayesian network by F(:j 0t ). This network is usually sparse enough so that ex-a t inferen e an operate e\u00c6 iently. For example, in the spe ial ase where fs1t ; s2t ; : : : ; sMt gare independent given 0t and st 1, F will be fa tored ompletely into the produ t of Mmarginals of smt .Although fa tored representations an be used as part of the RB belief state, are mustbe taken when performing eviden e reversal, i.e. to reverse the link from the state variableto the observation node. In the pro edure for eviden e reversal dis ussed previously (seeFig. 15), we rst position the root of Ct at the node st, thus need to ompute and representthe distribution Pr(st). In the fa tored state spa e ase, this be omes a joint distributionof all the state variables fs1t ; s2t ; : : : ; sMt g. Without onditioning on the urrent a tion 0t ,the fa tored representation of the state variables fsmt g annot be utilised, thus resulting in omplexity exponential in M .The key to get around this di\u00c6 ulty is to always keep the spe i ation of the distributionof the urrent state onditioned on the urrent a tion, not vi e versa. Thus, when omputingBert = Bt(:jot), we rst position the root of the hain Ct at 0t , and then reverse the eviden efrom ot to both 0t and st. In algebrai form, we use the following fa torisation of the jointdistribution of the urrent a tion and state given the urrent observation:Pr( 0t ; stjot) = Pr(stj 0t ; ot) Pr( 0t jot) (13)Fig. 16 illustrates this eviden e reversal pro edure. In the model depi ted here, F anbe an arbitrary Bayesian network. The observation model an be spe i ed by atta hing theobservation nodes fo1t ; o2t ; : : :g to the state variables. The overall network representing thedistribution Pr(st; ot j 0t ) will be denoted by Fobs(:j 0t ).We rst look at the rst term in the RHS of (13). Let Fer(:j 0t ; ot) represent thedistribution Pr(st j 0t ; ot). Note that Fer an be obtained by onditioning Fobs(:j 0t ) on theobservation ot. This an be a hieved by applying an exa t inferen e method su h as the lustering algorithm (Lauritzen & Spiegelhalter, 1988) on the network Fobs(:j 0t ).For the se ond term in the RHS of (13), we note that:Pr(ot j 0t ) =Xst Pr(st; ot j 0t ) =Xst Fobs(st; otj 0t )This integration an be readily obtained as a by-produ t when performing the above lus-tering algorithm on Fobs(:j 0t ). On e Pr(ot j 0t ) is known, we an ompute Pr( 0t j ot) by:Pr( 0t j ot) / Pr(ot j 0t ) Pr( 0t )This shows that the belief state after eviden e reversal Bert = Bt(:j ot) still has a simplestru ture that exploits the independen e relationships between the state variables fsmt ggiven the urrent a tion 0t . Sampling the RB variables from this stru ture an pro eed as483\nBui, Venkatesh & West\n1\n2\n1\n2\nt\nt\nt\nt\nt 3 t\n4\n3 t\n0\n1\n2\n1\n2\nt\nt\nt\nt\nt 3 t\n4\n3 t\n0\n. ..\nF\nBelief state before evidence reversal Belief state after evidence reversal\n. ..\nF F\n. . . . ..\ns\ns\no o\no o\ns\n\u03c0\ner\ns\ns\no o\no o\ns\n\u03c0\nobs\nFigure 16: Eviden e reversal with fa tored state spa efollows: Pr( 0t j ot) is rst used to sample 0t ; Fer(stj 0t ; ot) is then used to sample st. On ewe have obtained the sample for 0t and st, we an pro eed to sample the remaining nodesin the network Bert to obtain a sample for lt as usual. Finally, we note that the weightwt = Pr(ot) an also be omputed e\u00c6 iently by:Pr(ot) =X 0t Pr(ot j 0t ) Pr( 0t )In this eviden e reversal pro edure, for ea h value of 0t , we need to perform exa tinferen e on the stru ture of Fobs(st; otj 0t ). Thus the omplexity of this pro edure heavilydepends on the omplexity of the network stru ture of F . However, as we have noted,due to the nature of the fa tored representation, F usually has a sparse stru ture so thatexa t inferen e an be performed e\u00c6 iently. For example, in the spe ial ase where Fis ompletely fa tored into the produ t of M independent state variables whi h are thenindependently observed, the omplexity be omes linear w.r.t. M .6. Experimental ResultsIn this se tion, we present our experimental results with the poli y re ognition algorithm. Insubse tion 6.1, we demonstrate the e e tiveness of the Rao-Bla kwellised sampling methodfor poli y re ognition by omparing the performan e of our Rao-Bla kwellised pro edureagainst likelihood weighting sampling in a syntheti tra king task. In subse tion 6.2, wepresent an appli ation of the AHMM framework to the problem of tra king human be-haviours in a omplex spatial environment using distributed video surveillan e data.484\nPoli y re ognition in the Abstra t Hidden Markov Model Rm 0\nRm 1\nRm 2\nRm 3\nRm 7\nRm 4\nRm 5Rm 6\nN\nS\nW\n25\n36 50 62\n7\n16\nE\nNorth Wing\nSouth WingFigure 17: The environment and a sample traje tory\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50 55 60\nP ro\nba bi\nlit y\nTime\nDestination probabilities\nwest south east north\nFigure 18: Probabilities of top-level destinations over time6.1 E e tiveness of Rao-Bla kwellisationTo demonstrate the e e tiveness of the Rao-Bla kwellised inferen e method for AHMM, weagain onsider the syntheti tra king task in whi h it is required to monitor and predi tthe movement of an agent through the building environment previously dis ussed in sub-se tion 3.3. The stru ture of the AHMM used is the same as the one shown in Fig. 5. Theparameters of the poli ies are hosen manually, and then used to simulate the movement ofthe agent in the building. To simulate the observation noise, we assume that the observa-tion of the agent's true position an be anywhere among its 8 neighbouring ells with theprobabilities given by a prede ned observation model.485\nBui, Venkatesh & West\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n10 20 30 40 50\nst d.\nd ev\nia tio\nn\nSample size (in 1000)\nSIS 0.26/sqrt(x)\nRB-SIS 0.055/sqrt(x)\n(a) Sample size and average error\n0 0.5\n1 1.5\n2 2.5\n3 3.5\n4 4.5\n5 5.5\n6 6.5\n7\n10 20 30 40 50\nC P\nU T\nim e\n(in s\nec on\nd)\nSample size (in 1000)\nSIS 0.035*x RB-SIS 0.08*x\n(b) Sample size and CPU time\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6\nst d.\nd ev\nia tio\nn\nCPU time (in second)\nSIS RB-SIS\n( ) CPU time and average errorFigure 19: Performan e pro les of SIS vs. RB-SIS 486\nPoli y re ognition in the Abstra t Hidden Markov Model\n0\n0.0005\n0.001\n0.0015\n0.002\n0.0025\n0.003\n0 5 10 15 20 25 30 35 40 45 50\nE ffi\nci en\ncy c\noe ffi\nci en\nt\nSample size (in 1000)\nSIS 0.00179375\nRB-SIS 0.000235286\nFigure 20: E\u00c6 ien y oe\u00c6 ients of SIS and RB-SISWe implement the RB-SIS method (with re-sampling) and use the poli y hierar hyspe i ation and the simulated observation sequen e as input to the algorithm. In a typi alrun, the algorithm an return the probability of the main building exit, the next wing exit,and the next room-door that the agent is urrently heading to. An example tra k is shownin Fig. 17. As the observations about the tra k arrive over time, the predi tion probabilitydistribution of whi h main building exit the tra k is heading to is shown in Fig. 18.To illustrate the advantage of RB-SIS, we also implement an SIS method without Rao-Bla kwellisation (LW with ER and re-sampling (Kanazawa et al., 1995)) and ompare theperforman e of the two algorithms. We run the two algorithms using di erent sample pop-ulation sizes to obtain their performan e pro les. For a given sample size N , the standarddeviation ( (N)) over 50 runs in the estimated probabilities of the top-level poli ies is usedas the measure of expe ted error in the probability estimates. We also re ord the averagetime taken in ea h update iteration.Fig. 19(a) plots the standard deviation of the two algorithms for di erent sample sizes.The behaviour of the error follows losely the theoreti al urve (N) = =pN , or 2(N) = 2=N , with SIS 0:26 and RB SIS 0:055. As expe ted, for the same number of samples,the RB-SIS algorithm delivers mu h better a ura y.Fig. 19(b) plots the average CPU time (T ) taken in ea h iteration versus the samplesize. As expe ted, T (N) is linear to N , with the RB-SIS taking about twi e longer due tothe overhead in updating the RB belief state while pro essing ea h sample.Fig. 19( ) plots the a tual CPU time taken versus the expe ted error for the two algo-rithms. It shows that for the same CPU time spent, the RB-SIS method still signi antlyredu es the error in the probability estimates.Note that for ea h algorithm, the quantity = 2(N)T (N) is approximately onstantsin e the dependen y on N an els one another out. Thus, this onstant an be used as ane\u00c6 ien y oe\u00c6 ient to measure the performan e of the sampling algorithm independent ofthe number of samples. For example, if an algorithm has a twi e smaller oe\u00c6 ient, it andeliver the same a ura y with half CPU time, or half the varian e for the same CPU time.Fig. 20 plots the e\u00c6 ien y oe\u00c6 ients for both SIS and RB-SIS, with SIS 0:0018 and487\nBui, Venkatesh & West RB SIS 0:000235. This indi ates a performan e gain of almost an order of magnitude(8 folds) for RB-SIS.6.2 Appli ation to Tra king Human BehavioursUsing the poli y re ognition algorithm, we have implemented a real-time surveillan e systemthat tra ks the behaviour of people in a omplex indoor environment using surveillan e videodata. The environment onsists of a orridor, the Vision lab and two o\u00c6 es (see Fig. 21).People enter/exit the s ene via the left or the right entran e of the orridor. The systemhas six stati ameras with overlapping eld of views whi h over most of the ground planein the s ene.The entire environment is divided into a grid of ells, and the urrent ell position ofthe tra ked obje t a ts like the urrent state in our AHMM. The ameras are alibrated sothat they an return the urrent position of the tra ked obje t on the ground, however thereturned oordinates are unreliable as the ameras have to deal with noisy video frames ando lusion of obje ts in the s ene. For more information on how low-level tra king is donewith multiple ameras, readers are referred to (Nguyen, Venkatesh, West, & Bui, 2002).We assume that the observation of a state an only be in the area surrounding it, thus theobservation model is a matrix spe ifying the observation likelihood for ea h ell within aneighbourhood of the urrent state.The poli y hierar hy for behaviours in this environment is onstru ted as follows. First,we onstru t the region hierar hy with three levels. At the bottom level, we identify 7regions of spe ial interest: the orridor, the two o\u00c6 es, the areas surrounding the Linuxserver, NT server, printer, and the remaining free spa e in the Vision lab (Fig. 21). At thehigher level, all regions in the Vision lab are grouped together. The top level onsists of theentire environment. The poli y hierar hy representing people's behaviors has three levels orresponding to the three levels of the region hierar hy (see Fig. 23). At the bottom level,we are interested in the behaviours that take pla e within ea h of the 7 regions of interest.For example, near the Linux server, the person might be using the Linux ma hine, or simplypassing through that region, leading to two di erent poli ies. Similar poli ies are de nedfor the NT server region, the printer region, and the two small o\u00c6 es. In the orridorand inside the Vision lab (region 1 and 5), we onstru t di erent poli ies orresponding tothe di erent destinations that the person is heading to. Region 5 also has a spe ial poli yrepresenting the \\walk-around\" behaviour. At the middle level, three poli ies are de nedfor the orridor and o\u00c6 e spa e representing a person's plan of exiting this spa e by theleft/right entran e or by the door of the Vision lab. We de ne only one poli y for the Visionlab to represent the typi al behaviour of a lab user (e.g., go to Linux server, followed bygo to printer).8 Finally, for the top level region (the whole environment), we de ne twopoli ies representing a person's leaving the s ene via the left/right entran e.Fig. 21 and 22 show two on urrent traje tories of two di erent people in this environ-ment. Some sample video frames aptured by the di erent ameras in the system are shownin Fig. 24.With the AHMM model de ned above, and a sequen e of observations returned bythe ameras, we rst determine the performan e pro les of RB-SIS and SIS in this real8. If we onsider di erent groups of lab users, ea h group might give rise to a di erent poli y at this level.488\nPoli y re ognition in the Abstra t Hidden Markov Model\n489\nBui, Venkatesh & West\nMiddle level Bottom level Top level 3 policies\n2 policies 2 policies NT region 4 policies 2 policies\n1 policy\n2 policies The environment\nCorridor & offices Vision lab\n5 policies Office 1 2 policies 2 policies Office 2Corridor Linux region Printer region Empty spaceFigure 23: The region and poli y hierar hy490\nPoli y re ognition in the Abstra t Hidden Markov Model\n(a) (b)Figure 24: (a) Person 1 enters the s ene and (b) Person 2 enters the s ene.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6\nst d.\nd ev\nia tio\nn\nCPU time (in second)\nRB-SIS SIS\n(a) Error vs. CPU time 00.05 0.1\n0.15\n0.2\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6\nE ffi\nci en\ncy c\noe ffi\nci en\nt\nCPU time (in second)\nSIS 0.06 RB-SIS 0.011\n(b) E\u00c6 ien y oe\u00c6 ientsFigure 25: Performan e of RB-SIS and SIS with real tra king data 491\nBui, Venkatesh & West\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n0 50 100 150 200 250 300 350 400 P\nro ba\nbi lit\ny\nTime\np(left_e) p(right_ e)\nFigure 26: The probabilities that person 1 is leaving the s ene via the entran es (top levelpoli ies)environment. The two algorithms behave in a similar way as in the previous experimentwith simulated data. Fig. 25 shows the error urve against the CPU time for the twoalgorithms. The e\u00c6 ien y o-e\u00c6 ient for RB-SIS in this ase is RB SIS 0:011, and forSIS is SIS 0:06. This shows that the RB-SIS still performs about 5 times better thanSIS in this domain.In the surveillan e system, the low level tra king module returns the observations atthe rate of approximately two per se ond. The observation is then passed to the RB-SISalgorithm whi h produ es the probability estimate of the urrent poli y at di erent levelsin the hierar hy. At the moment, our surveillan e system an run in real time using twoAMD 1G ma hines. Examples of the output returned by the system for the two traje toriesin Fig. 21 and Fig. 22 are given below.Fig. 26 shows the probabilities that person 1 is exiting the environment by the leftor right entran e (denoted by pleft e and pright e respe tively). At the beginning, pleft ein reases when person 1 is heading to the left entran e (see the traje tory in Fig. 21). Then,pleft e is approximately onstant from time sli e 50 when person 1 is inside the Vision lab.This is be ause only one middle level poli y is de ned for the Vision lab and his movementinside the lab is independent of his nal exit/entran e. At time sli e 310, pleft e de reaseswhen person 1 is leaving the lab, turning right, and entering o\u00c6 e 2. Then, it in reases andapproa hes 1 when he is leaving o\u00c6 e 2, turning left, and going towards the left entran e.In ontrast, pright e falls qui kly to zero during this time.We now look at the results of querying of the bottom level poli ies. Fig. 27 shows thedistribution of the possible destinations of person 2 from time sli e 180 to time sli e 260,when he is in region 5 (see the traje tory in Fig 22). The probabilities obtained show thatthe system is able to orre tly dete t the \\walk-around\" behaviour.The nal result (Fig. 28) shows the inferred behaviours of person 1 when he is at theLinux server region. Initially, the probabilities for \\using Linux server\" and for \\passingthrough\" are the same. As the person stays in the same position for an extended period oftime, the system is able to identify the orre t behaviour of person 1 as \\using the Linuxserver\". 492\nPoli y re ognition in the Abstra t Hidden Markov Model\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n180 190 200 210 220 230 240 250 260\nP ro\nba bi\nlit y\nTime\np(v_Linux) p(v_printer)\np(v_NT) p(w_ around)\nFigure 27: Behaviours of person 2 inside the Vision lab\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n50 100 150 200 250 300\nP ro\nba bi\nlit y\nTime\np(u_Linux) p(pass)\nFigure 28: Behaviour of person 1 inside the Linux server region\n493\nBui, Venkatesh & West7. Related Work in Probabilisti Plan Re ognitionThe ase for using probabilisti inferen e for plan re ognition has been argued onvin inglyby Charniak and Goldman (1993). However, the plan re ognition Bayesian network usedby Charniak and Goldman is a stati network. Thus their approa h would run into prob-lems when they have to pro ess on-line a stream of eviden e about the plan. More re entapproa hes (Pynadath & Wellman, 1995, 2000; Goldman et al., 1999; Huber et al., 1994;Albre ht et al., 1998) have used dynami sto hasti models for plan re ognition and thusare more suitable for doing on-line plan re ognition under un ertainty.Among these, the most losely related model to the AHMM is the Probabilisti State-Dependent Grammar (PSDG) (Pynadath, 1999; Pynadath & Wellman, 2000). A ompari-son of the representational aspe t of the two models has been dis ussed under subse tion 3.4.In terms of algorithms for plan re ognition, Pynadath and Wellman only o er an exa tmethod to deal with the ase where the states are fully observable. When the states arepartially observable, a brute-for e approa h is suggested whi h amounts to summing over allpossible states. We note that even for the fully observable ase, the belief state that we needto deal with an still be large sin e the poli y starting/ending times are unknown.9 Sin ean exa t method is used by Pynadath and Wellman, the omplexity for maintaining thebelief state would most likely be exponential to the number of levels in the PSDG expansionhierar hy (i.e., the height of our poli y hierar hy). On the other hand, our RB-SIS poli yre ognition algorithm an handle partially observable states and the Rao-Bla kwellisationpro edure ensures that the sampling algorithm s ales well with the number of levels in thepoli y hierar hy. Furthermore, as we have noted in subse tion 3.4, if we onsider ompoundpoli ies, the PSDG an be onverted to an AHMM. In our framework, a ompound poli y k = k 1(1) ; : : : ; k 1(m) an be represented just as a normal poli y, with a slight modi ationto let the variable ek take on values between 1 and m+ 1, where the value m+ 1 indi atesthat the ompound poli y has terminated. The poli y re ognition algorithm an then bemodi ed to also work with this model.Similar to our AHMM and the PSDG, the re ent work by Goldman et al. (1999) alsomakes use of a detailed model of the plan exe ution pro ess. Using the ri h language ofprobabilisti Horn abdu tion, they are able to model more sophisti ated plan stru turessu h as interleaved/ on urrent plans, partially-ordered plans. However the work servesmainly as a representational framework, and provides no analysis on the omplexity of planre ognition in this setting.Other work in probabilisti plan re ognition up to date has employed mu h oarsermodels for plan exe ution. Most have ignored the important in uen e of the state of theworld to the agent's planning de ision (Goldman et al., 1999). To the best of our knowledge,none of the work up to date has addressed the problem of partial and noisy observationof the state. Most, ex ept the PSDG, do not look at the observation of the out omes ofa tions, and assume that the a tion an be observed dire tly and a urately. We note thatthis kind of simplifying assumptions is needed in previous work so that the omputational omplexity of performing probabilisti plan re ognition remains manageable. In ontrary,our work here illustrates that although the plan re ognition dynami sto hasti model an9. Of ourse, if an SRD poli y hierar hy is onsidered then full observability alone is enough.494\nPoli y re ognition in the Abstra t Hidden Markov Modelbe omplex, they exhibit spe ial types of onditional independen e whi h, if exploited, anlead to e\u00c6 ient plan re ognition algorithms.8. Con lusion and Future WorkIn summary, we have presented an approa h for on-line plan re ognition under un ertaintyusing the AHMM as the model for the exe ution of a sto hasti plan hierar hy and its noisyobservation. The AHMM is a novel type of sto hasti pro esses, apable of representinga ri h lass of plans and the asso iating un ertainty in the planning and plan observationpro ess. We rst analyse the AHMM stru ture and its onditional independen e proper-ties. This leads to the proposed hybrid Rao-Bla kwellised Sequential Importan e Sampling(RB-SIS) algorithm for performing belief state updating ( ltering) for the AHMM whi hexploits the stru ture of the AHMM for greater e\u00c6 ien y and s alability. We show that the omplexity of RB-SIS when applied to the AHMM only depends linearly on the number oflevels K in the poli y hierar hy, while the sampling error does not depend on K.In terms of plan re ognition, these results show that while the sto hasti pro ess forrepresenting the exe ution of a plan hierar hy an be omplex, they exhibit ertain ondi-tional independen e properties that are inherent in the dynami s of the planning and a tingpro ess. These independen e properties, if exploited, an help to redu e the omplexity ofperforming inferen e on the plan exe ution sto hasti model, leading to feasible and s alablealgorithms for on-line plan re ognition in noisy and un ertain domains. The s alability ofthe algorithm for poli y re ognition provides the possibility to onsider more omplex planhierar hies and more detailed models of the plan exe ution pro ess. The key to a hieve thise\u00c6 ien y, as we have shown in the paper, is a ombination of re ently developed te hniquesin probabilisti inferen e: ompa t representations for Bayesian networks ( ontext-sensitiveindependen e, fa tored representations), and hybrid DBN inferen e whi h an take advan-tage of these ompa t representations (Rao-Bla kwellisation).Several future resear h dire tions are possible. To further investigate the AHMM, wewould like to onsider the problem of learning the parameters of an AHMM from a databaseof observation sequen es, e.g., to learn the plan exe ution model by observing multipleepisodes of an agent exe uting the same plan. The stru ture of the AHMM suggests thatwe an try to learn the model of ea h abstra t poli y separately. Indeed, if we an observethe exe ution of ea h abstra t poli y separately, the learning problem is redu ed to HMMparameter re-estimation for level-1 poli ies, and simple frequen y ounting for higher-levelpoli ies. If the observation sequen e is a long episode with no lear ut temporal boundarybetween the poli ies, the problem be omes a type of parameter estimation for DBN withhidden variables, and te hniques for dealing with hidden variables su h as EM (Dempster,Laird, & Rubin, 1977) an be applied.Extensions an be made to the AHMM to make the model more expressive and suitablefor representing more omplex agents' plans. For example, a more expressive plan exe utionmodel su h as the HAM model (Parr, 1998) an be onsidered so that state-independentsequen es of poli ies an be represented. The urrent model an also be enri hed to onsidera set of top-level poli ies whi h an be interleaved during their exe ution. We expe t thatthese new models would exhibit ontext-spe i independen e properties similar to the495\nBui, Venkatesh & WestAHMM, and Rao-Bla kwellised sampling methods for poli y re ognition in these models an be derived.A knowledgementWe would like to thank the anonymous reviewers for their insightful omments whi h havehelped improve both the presentation and the ontents of this paper. Many thanks to NamNguyen for his implementation of the distributed tra king system used in this paper.Referen esAlbre ht, D. W., Zukerman, I., & Ni holson, A. E. (1998). Bayesian models for keyholeplan re ognition in an adventure game. User Modelling and User-adapted Intera tion,8 (1{2), 5{47.Andrieu, C., & Dou et, A. (2000). Parti le ltering for partially observed Gaussian statespa e models. Te h. rep. CUED-F-INFENG/TR. 393, Signal Pro essing Group, Uni-versity of Cambridge, Cambridge, UK.Bauer, M. (1994). Integrating probabilisti reasoning into plan re ognition. In Pro eedingsof the Eleventh European Conferen e on Arti ial Intelligen e.Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Sto hasti dynami programmingwith fa tored representations. Arti ial Intelligen e, 121, 49{107.Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-spe i inde-penden e in Bayesian networks. In Pro eedings of the Twelveth Annual Conferen eon Un ertainty in Arti ial Intelligen e.Boyen, X., & Koller, D. (1998). Tra table inferen e for omplex sto hasti pro esses. InPro eedings of the Fourteenth Annual Conferen e on Un ertainty in Arti ial Intelli-gen e.Brand, M. (1997). Coupled hidden Markov models for modeling intera ting pro esses. Te h.rep. 405, MIT Media Lab.Bui, H. H., Venkatesh, S., & West, G. (1999). Layered dynami Bayesian networks forspatio-temporal modelling. Intelligent Data Analysis, 3 (5), 339{361.Bui, H. H., Venkatesh, S., &West, G. (2000). On the re ognition of abstra t Markov poli ies.In Pro eedings of the National Conferen e on Arti ial Intelligen e (AAAI-2000), pp.524{530.Casella, G., & Robert, C. P. (1996). Rao-Bla kwellisation of sampling s hemes. Biometrika,83, 81{94.Castillo, E., Gutierrez, J. M., & Hadi, A. S. (1997). Expert systems and probabilisti networkmodels. Springer.Charniak, E., & Goldman, R. (1993). A Bayesian model of plan re ognition. Arti ialIntelligen e, 64, 53{79.Cohen, P. R., & Levesque, H. J. (1990). Intention is hoi e with ommitment. Arti ialIntelligen e, 42, 213{261. 496\nPoli y re ognition in the Abstra t Hidden Markov ModelCooper, G. F. (1990). The omputational omplexity of probabilisti inferen e using Baysianbelief networks. Arti ial Intelligen e, 42, 393{405.Dagum, P., & Luby, M. (1993). Approximating probabilisti inferen e in Bayesian beliefnetworks is NP-hard. Arti ial Intelligen e, 60, 141{153.Dagum, P., Galper, A., & Horvitz, E. (1992). Dynami network models for fore asting. InPro eedings of the Eighth Annual Conferen e on Un ertainty in Arti ial Intelligen e,pp. 41{48.D'Ambrosio, B. (1993). In remental probabilisti inferen e. In Pro eedings of the NinthAnnual Conferen e on Un ertainty in Arti ial Intelligen e, pp. 301{308.Dawid, A. P., Kj rul , U., & Lauritzen, S. (1995). Hybrid propagation in jun tion trees. InZadeh, L. A. (Ed.), Advan es in Intelligent Computing, Le ture Notes in ComputerS ien e, pp. 87{97.Dean, T., & Kanazawa, K. (1989). A model for reasoning about persisten e and ausation.Computational Intelligen e, 5 (3), 142{150.Dean, T., & Lin, S.-H. (1995). De omposition te hniques for planning in sto hasti do-mains. In Pro eedings of the Fourteenth International Joint Conferen e on Arti ialIntelligen e (IJCAI-95).Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from in omplete datavia the EM algorithm. Journal of the Royal Statisti al So iety B, 39, 1{38.Dou et, A., de Freitas, N., Murphy, K., & Russell, S. (2000a). Rao-Bla kwellised parti- le ltering for dynami Bayesian networks. In Pro eedings of the Sixteenth AnnualConferen e on Un ertainty in Arti ial Intelligen e.Dou et, A., Godsill, S., & Andrieu, C. (2000b). On sequential Monte Carlo sampling meth-ods for Bayesian ltering. Statisti s and Computing, 10 (3), 197{208.Forestier, J.-P., & Varaiya, P. (1978). Multilayer ontrol of large Markov hains. IEEETransa tions on Automati Control, 23 (2), 298{305.Fung, R., & Chang, K. C. (1989). Weighting and integrating eviden e for sto hasti sim-ulation in bayesian networks. In Pro eedings of the Fifth Conferen e on Un ertaintyin Arti ial Intelligen e.Geweke, J. (1989). Bayesian inferen e in e onometri models using Monte Carlo integration.E onometri a, 57 (6), 1317{1339.Ghahramani, Z., & Jordan, M. I. (1997). Fa torial hidden Markov models. Ma hine Learn-ing, 29, 245{273.Goldman, R., Geib, C., & Miller, C. (1999). A new model of plan re ognition. In Pro eedingsof the Fifteenth Annual Conferen e on Un ertainty in Arti ial Intelligen e.Hauskre ht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierar hi alsolution of Markov de ision pro esses using ma ro-a tions. In Pro eedings of theFourteenth Annual Conferen e on Un ertainty in Arti ial Intelligen e.Henrion, M. (1988). Propagating un ertainty in Bayesian networks by probabilisti logi sampling. In Lemmer, J., & Kanal, L. (Eds.), Un ertainty in Arti ial Intelligen e 2,Amsterdam. North-Holland. 497\nBui, Venkatesh & WestHuber, M. J., Durfee, E. H., & Wellman, M. P. (1994). The automated mapping of plansfor plan re ognition. In Pro eedings of the Tenth Annual Conferen e on Un ertaintyin Arti ial Intelligen e.Jelinek, F., La erty, J. D., & Mer er, R. L. (1992). Basi methods of probabilisti on-text free grammar. In Lafa e, P., & Mori, R. D. (Eds.), Re ent Advan es in Spee hRe ognition and Understanding, pp. 345{360. Springer-Verlag.Jensen, F. (1996). An Introdu tion to Bayesian Networks. Springer.Jensen, F., Lauritzen, S., & Olesen, K. (1990). Bayesian updating in re ursive graphi almodels by lo al omputations. Computational Statisti s Quarterly, 4, 269{282.Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introdu tion tovariational methods for graphi al models. Ma hine learning, 37 (2), 183{233.Jordan, M. I., Ghahramani, Z., & Saul, L. K. (1997). Hidden Markov de ision trees. InMozer, M. C., Jordan, M. I., & Pets he, T. (Eds.), Advan es in Neural InformationPro essing Systems 9, Cambridge, MA. MIT Press.Kalman, R. E. (1960). A new approa h to linear ltering and predi tion problems. Transa -tions of the Ameri an So iety of Me hani al Engineering, Series D, Journal of Basi Engineering, 82, 35{45.Kanazawa, K., Koller, D., & Russell, S. (1995). Sto hasti simulation algorithms for dy-nami probabilisti networks. In Pro eedings of the Eleventh Annual Conferen e onUn ertainty in Arti ial Intelligen e, pp. 346{351.Kautz, H., & Allen, J. F. (1986). Generalized plan re ognition. In Pro eedings of the FifthNational Conferen e on Arti ial Intelligen e, pp. 32{38.Kjaerul , U. (1992). A omputational s heme for reasoning in dynami probabilisti net-works. In Pro eedings of the Eighth Annual Conferen e on Un ertainty in Arti ialIntelligen e, pp. 121{129.Kj rul , U. (1995). dHugin: A omputational system for dynami time-sli ed Bayesiannetworks. International Journal of Fore asting, 11, 89{111.Lauritzen, S., & Spiegelhalter, D. (1988). Lo al omputations with probabilities on graphi alstru tures and their appli ation to expert systems. Journal of the Royal Statisti alSo iety B, 50, 157{224.Liu, J. S., & Chen, R. (1998). Sequential Monte Carlo methods for dynami systems.Journal of the Ameri an Statisti al Asso iation, 93, 1032{1044.Murphy, K., & Russell, S. (2001). Rao-bla kwellised parti le ltering for dynami Bayesiannetworks. In Dou et, A., de Freitas, N., & Gordon, N. J. (Eds.), Sequential MonteCarlo Methods in Pra ti e. Springer-Verlag.Murphy, K. P. (2000). Bayesian map learning in dynami environments. In Advan es inNeural Information Pro essing Systems 12, pp. 1015{1021. MIT Press.Nguyen, N. T., Venkatesh, S., West, G., & Bui, H. H. (2002). Coordination of multiple ameras to tra k multiple people. In Pro eedings of the Asian Conferen e on ComputerVision (ACCV-2002), pp. 302{307. 498\nPoli y re ognition in the Abstra t Hidden Markov ModelNi holson, A. E., & Brady, J. M. (1992). The data asso iation problem when monitoringrobot vehi les using dynami belief networks. In Pro eedings of the Tenth EuropeanConferen e on Arti ial Intelligen e, pp. 689{693.Parr, R. (1998). Hierar hi al ontrol and learning for Markov De ision Pro esses. Ph.D.thesis, University of California, Berkeley.Parr, R., & Russell, S. (1997). Reinfor ement learning with hierar hies of ma hines. InAdvan es in Neural Information Pro essing Sytems (NIPS-97).Pearl, J. (1988). Probabilisti Reasoning in Intelligent Systems: Networks of Plausible In-feren e. Morgan Kaufmann, San Mateo, CA.Pearl, J. (1987). Evidential reasoning using sto hasti simulation of ausal models. Arti ialIntelligen e, 32, 245{257.Pynadath, D. V. (1999). Probabilisti grammars for plan re ognition. Ph.D. thesis, Com-puter S ien e and Engineering, University of Mi higan.Pynadath, D. V., & Wellman, M. P. (1995). A ounting for ontext in plan re ognition, withappli ation to tra\u00c6 monitoring. In Pro eedings of the Eleventh Annual Conferen eon Un ertainty in Arti ial Intelligen e.Pynadath, D. V., & Wellman, M. P. (2000). Probabilisti state-dependent grammars forplan re ognition. In Pro eedings of the Sixteenth Annual Conferen e on Un ertaintyin Arti ial Intelligen e.Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and sele ted appli ations inspee h re ognition. Pro eedings of the IEEE, 77 (2), 257{286.Sa erdoti, E. (1974). Planning in a hierar hy of abstra tion spa es. Arti ial Intelligen e,5, 115{135.Sha hter, R. (1986). Evaluating in uen e diagrams. Operations Resear h, 34, 871{882.Sha hter, R. D., & Peot, M. A. (1989). Simulation approa hes to general probabilisti inferen e on belief networks. In Pro eedings of the Fifth Conferen e on Un ertaintyin Arti ial Intelligen e.Sutton, R. S. (1995). Td models: Modelling the world at a mixture of time s ales. InPro eedings of the Internation Conferen e on Ma hine Learning (ICML-95).Sutton, R. S., Pre up, D., & Singh, S. (1999). Between MDP and semi-MDPs: A frameworkfor temporal abstra tion in reinfor ement learning. Arti ial Intelligen e, 112, 181{211.van Beek, P. (1996). An investigation of probabilisti interpretations of heuristi s in planre ognition. In Pro eedings of the Fifth International Conferen e on User Modeling,pp. 113{120.York, J. (1992). Use of Gibbs sampler in expert systems. Arti ial Intelligen e, 56, 115{130. 499"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": null, "creator": "dvipsk 5.86 p1.5d Copyright 1996-2001 ASCII Corp.(www-ptex@ascii.co.jp)"}}}