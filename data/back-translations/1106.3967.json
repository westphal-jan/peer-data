{"id": "1106.3967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2011", "title": "Intelligent Self-Repairable Web Wrappers", "abstract": "Systems and procedures designed to extract this data from web sources already exist, and various approaches and techniques have been studied in recent years. On the one hand, reliable solutions should provide robust web data mining algorithms that could automatically face possible malfunctions or failures; on the other, there is a lack in literature of solutions for maintaining these systems. Procedures that extract web data can be closely linked to the structure of the data source itself, so malfunctions or the acquisition of corrupt data could be caused, for example, by structural changes to data sources brought in by their owners. Nowadays, data integrity and maintenance checks are largely managed manually to ensure that these systems function correctly and reliably. In this essay, we propose a novel approach to create procedures that are able to extract data from web sources - the so-called web wrappers - that can counter possible malfunctions that can be caused by modifications to the structure of the data source itself and itself.", "histories": [["v1", "Mon, 20 Jun 2011 17:02:40 GMT  (848kb)", "http://arxiv.org/abs/1106.3967v1", "12 pages, 4 figures; Proceedings of the 12th International Conference of the Italian Association for Artificial Intelligence, 2011"]], "COMMENTS": "12 pages, 4 figures; Proceedings of the 12th International Conference of the Italian Association for Artificial Intelligence, 2011", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["emilio ferrara", "robert baumgartner"], "accepted": false, "id": "1106.3967"}, "pdf": {"name": "1106.3967.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Robert Baumgartner"], "emails": ["emilio.ferrara@unime.it", "robert.baumgartner@lixto.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n39 67\nv1 [\ncs .A\nI] 2\n0 Ju\nn 20\nKeywords: Web data extraction, wrappers, automatic adaptation"}, {"heading": "1 Introduction", "text": "The actual panorama of distribution of information through the Web depicts a clear situation: there is an incredible amount of data delivered under the form of Web data sources and a corresponding need of capability of mining this information in a reliable and efficient way. Mining information from Web sources is a task which can obviously be useful in several different area of the knowledge. Moreover, this topic interests both the academia and the enterprises. For example, consider the following scenarios: i) a research group which needs to acquire a dataset of information delivered through online services, say for example an online database publishing, day by day, information about the mapping of some genes; ii) a company for which it is essential, for marketing and product placement, to monitor the trends of pricing of services offered by its competitors, provided through the Web. Both the two actors need to extract, possibly, a huge amount of data during an extend period of time (e.g., months), at regular intervals (say, each day). One important aspect in both the cases is the reliability and\nthe quality of data extracted. It is utterly important that acquired information is correct, because the research group can not accept corrupted data and the comparison with competitors would fail in case of bad product data.\nThese two examples highlight common requirements in the panorama of Web data mining, and depict different related problems. Although in literature some techniques to design systems for the extraction of data from Web sources have been presented, there is a lack of work in the area of their maintenance. An ample number of questions and problems related to the possibility of automatizing the process of maintenance are still uncovered. This work tries to focus on some aspects related to the maintenance of these systems. We first introduce the theoretical background required to create intelligent procedures of Web data extraction. Then, we explain how to face malfunctioning likely to happen during the extraction process, for example caused by modifications in the structure of the data source. The second point in particular is the main focus of this work. Let us contextualize this problem: essentially there exist two different approaches to extract information from Web sources. The first one relies on machine learning platforms [5]; a system analyzes, possibly, huge amount of positive and negative examples during a training period, and, then, it infers some set of rules that makes it able to perform its tasks in the same domain or Web site. Different approaches rely on logic-based algorithms which analyze the structure of the data source and induct some procedures to extract required information exploiting structural characteristics of the Web source to identify and find required data. The second approach utilizes the knowledge a human can bring in about a particular site or domain. The wrapper is generated in a way that the human creates the rules and navigation paths together with the system in a supervised and interactive fashion. Still, the system can assist the wrapper designer and offer possibilities that make the wrapper execution as robust as possible, even in case of structural changes. From now, in this work we assume that the platform we are going to describe and improve adopts the latter philosophy.\nOrganization of the paper We describe related work in Section 2. In Section 3, the algorithmic background is introduced, describing an efficient tree matching technique. Section 4 covers the design of robust and adaptable procedures of Web data extraction, henceforth called intelligent self-repairable Web wrappers. Then, in Section 5 we describe the adaptation process during wrapper execution. We explain how these procedures can automatically, in an autonomous way, face malfunctioning, trying to adapt themselves to the modifications that possibly caused problems. A prototype has been implemented on top of a state-of-the-art extraction platform, the Lixto Visual Developer. Performance of this system are shown in Section 6, by means of precision and recall scores. Section 7 concludes summarizing our main achievements and depicting some future work."}, {"heading": "2 Background and Related Work", "text": "We split related literature in three main topics: i) Web data extraction systems; ii) maintenance and related problems; iii) tree matching algorithms.\nWeb data extraction systems The work related to systems of Web information extraction is manifold but well depicted by several surveys. Laender et al. [13] provided the first rigorous taxonomical classification of Web data extraction systems. Kushmerick [11] classified several finite-state approaches to generate wrappers, such as the wrapper induction, natural language processing approaches and hidden Markov models. Sarawagi [17] provided the most comprehensive survey on the information extraction panorama. This work covers different existing techniques explaining several approaches. In the last years, first Baumgartner et al. [1] and later Ferrara et al. [8] provided two different surveys on the discipline of Web data extraction. The first is mainly addressed to practitioners, the latter focuses on application fields of this discipline.\nMaintenance and related problems Although some interesting work, we can identify a general lack of solutions provided in the area of the Web wrapper maintenance. Kushmerick [12,10] for first introduced the concept of wrapper maintenance as the process of verifying the correct functioning of the data extraction procedures and manually, automatically or in a semi-automatic way, intervene in case of malfunctioning. Lerman and Minton [14], instead, faced both the problems of verifying the correctness of data extracted by a wrapper and eventually try to repair it. Their approach is a mix of machine learning techniques. Another approach based on machine learning has been provided by Chidlovskii [4]; he described a system which can automatically classify Web pages in order to extract information from those pages which can be handled adopting both conventional extraction rules and ensemble methods of machine learning, such as the content features analysis. Meng et al. [15] developed the SG-WRAM (Schema-Guided WRApper Maintenance) slightly modifying the perspective of Web wrappers generation, observing that changes in Web pages, even substantial, always preserve syntactic features (i.e., syntactic characteristics of data items like data patterns, string lengths, etc.), hyperlinks and annotations (e.g., descriptive information representing the semantic meaning of a piece of information in its context). Finally, another heuristic approach has been presented by Raposo et al. [16]; they adopted a collected sample of positive labeled examples during the normal execution of the wrappers, to be exploited in case of malfunctioning, in order to re-induct the broken wrapper ensuring a good accuracy of the process.\nTree Matching In general, the process of comparing the structure of two trees is a well-known classic problem. The possibility of transforming a tree into another one, through a sequence of (possibly different) operations, is another well-known algorithmic challenge, namely the tree editing problem. The minimum number of elementary transformations, such as adding/removing nodes, relabeling nodes or moving nodes, represents the distance between two trees. This value can be\nused to represent the measure of dissimilarity between two trees. The tree edit distance problem is a well-known NP-hard problem [3]. Several approximate solutions have been advanced during the years; the most appropriate algorithm to face the problem of matching up similar trees, has been suggested by Selkow [18]. This technique relies on the concept of finding isomorphic elements present in both the two compared trees, implementing a light-weight recursive top-down resolution during which the algorithm evaluates the position of nodes to measure the degree of isomorphism between them, analyzing and comparing their subtrees. Different versions of this algorithm exist; each of them presents some optimizations. Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees. An interesting evaluation of the simple tree matching and its weighted version, presented by Kim et al. [9], has been performed exploiting these two algorithms to extract information from HTML Web pages. These optimized algorithms underly the design of our self-repairable Web wrappers."}, {"heading": "3 The Tree Matching Algorithm", "text": "This work relies on some assumptions: i) Web pages are represented by using DOM trees, as the HTML standard imposes3; ii) it is possible to identify elements within a DOM tree by using the XPath language4; iii) the logics of XPath underly the functioning of Web wrappers (this is further explained in following sections and in [1,2]). Given these milestones, the main idea of our approach is to compare two trees, one representing the original Web page and another representing the page after that some modifications occurred. This is practical in order to automatize the adaptive process of automatic repairing of our wrappers. To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7]. Let d(n) be the degree of a node n (i.e., the number of first-level children); let T (i) be the i-th sub-tree of the tree rooted at node T ; let t(n) be the number of total siblings of a node n including itself. The Weighted Tree Matching here described (see Algorithm 1) optimizes the simple tree matching, for our specific domain."}, {"heading": "4 Web Wrappers", "text": "In supervised and interactive wrapper generation, the application designer is in charge of deciding how to characterize Web objects that are used for traversing the Web and for extracting information. It is one of the most important aspects of a wrapper to be resilient against changes (both changes over time and variations of similarly structured pages), and parts of the robustness of a data extractor depend on how the application designer configures it. However, it is crucial that the wrapper generation system assists the wrapper designer and suggests how\n3 http://www.w3.org/TR/DOM-Level-2-HTML/html.html 4 http://www.w3.org/TR/xpath/\nAlgorithm 1 WeightedTreeMatching(T \u2032 , T \u2032\u2032 )\n1: if T \u2032 has the same label of T \u2032\u2032 then 2: m\u2190 d(T \u2032 ) 3: n\u2190 d(T \u2032\u2032\n) 4: for i = 0 to m do 5: M [i][0]\u2190 0; 6: for j = 0 to n do 7: M [0][j] \u2190 0; 8: for all i such that 1 \u2264 i \u2264 m do 9: for all j such that 1 \u2264 j \u2264 n do 10: M [i][j] \u2190 Max(M [i][j \u2212 1], M [i \u2212 1][j], M [i \u2212 1][j \u2212 1] + W [i][j]) where\nW [i][j] = WeightedTreeMatching(T \u2032 (i\u2212 1), T \u2032\u2032\n(j \u2212 1)) 11: if m > 0 AND n > 0 then 12: return M[m][n] * 1 / Max(t(T \u2032 ), t(T \u2032\u2032\n)) 13: else 14: return M[m][n] + 1 / Max(t(T \u2032 ), t(T \u2032\u2032\n)) 15: else 16: return 0\nto make the identification of Web objects and trails through Web sites as stable as possible."}, {"heading": "4.1 Robust XPath Generation and Fall-back Strategies", "text": "In Lixto Visual Developer (VD) [2], a number of mechanisms are offered to create a resilient wrapper. During recording, one task is to generate a robust XPath or regular expression, interactively and supported by the system. During wrapper generation, in many cases only one labeled example object is available, especially in automatically recorded deep Web navigation sequences. In such cases, efficient heuristics in XPath generation and fallback strategies during replay, are required. Typical heuristics during recording for reliably identifying such single Web objects include:\n\u2013 Generalization of a chosen XPath by using form properties, element properties, textual properties and formatting properties. During replay, these ingredients are used as input for an algorithm that checks in which constellation to best apply this property information to satisfy the integrity constraints imposed on a rule (e.g., as result a single instance is required). \u2013 DOM Structural Generalization \u2013 starting from the full path, several generalized paths are created, using only characteristic elements and characteristic element sequences. A number of stable anchor points are identified and stored, from which relative paths to this object are created. Typical stable anchor points are identified automatically and include, e.g., the outermost table structure and the main content area (being chosen upon factors such as the longest content).\n\u2013 Positional information is considered if the structurally generalized paths identify more than one element. In this case, during execution, variations of the XPath generated with this \u201cindex heuristics\u201d are applied on the active Web page, removing indexes until the integrity constraints of the current rule are satisfied. \u2013 Attributes and properties of elements are taken into account, in particular of the element of choice, but we also consider ancestor attributes if the element attributes are not sufficient. \u2013 Attributes that make an element unique are preferred, i.e., similar elements are checked for distinguishing criteria. \u2013 Attribute Values are considered, if attribute names are not sufficient. Attribute Value Fragments are considered, if attribute values are not sufficient (using regular expressions). \u2013 The ID attributes are used as far as possible. If an ID is unique and meaningful for characterizing an element it is considered in the fallback strategies with a high weight. \u2013 Textual information and label information is used, only if explicitly turned on (since this might fail in case of a language switch).\nThe output of the heuristic step is a \u201cbest XPath\u201d shown to the wrapper designer, and a set of XPath expressions and priorities regarding when to use which fallback strategy, stored in the configuration. Figure 1 illustrates which information is stored by the system during recording. In this case, a drop down was selected by the application designer, and the system decided that the \u201cid\u201d attribute is the most reliable one and is chosen as best XPath. If this evaluation fails, the system will apply heuristics based on the (in this example, three) stored fallback XPaths, which mainly exploit form and index properties. In case one of the heuristics generates results that do not invalidate the defined integrity constraints, these Web objects are considered as result.\nDuring generation of rules (e.g., \u201cextract\u201d) and actions (e.g., \u201cclick\u201d), the wrapper designer imposes constraints on the results to be obtained, such as:\n\u2013 Cardinality Constraints: restrictions on the number of results, e.g., exactly one element or at least one element must be matched. \u2013 Data Type Constraints: restrictions on the data type of a result, e.g., a result must be of type integer or match a particular regular expression.\nConstraints can be defined individually per rule and action, or defined globally by using a schema on the output data model."}, {"heading": "4.2 Configuring Adaptable Wrappers", "text": "The procedures described in the previous section do not adapt the wrapper, but address situations in which the initially chosen XPath does no longer match and simply try different ones based on this one. In the configuration of wrapper adaptation, we go one step beyond: on the one hand we exploit tree and string\nsimilarity techniques to find the most similar Web object(s) on the new page, and on the other hand, in case the adaptation is triggered, the wrapper is changed on the fly using the new configuration created by the adaptation algorithms.\nAs before, integrity constraints can be imposed on extraction and navigation rules. Moreover, the application designer can choose whether to use wrapper adaptation on a particular rule in case the constraints are violated during runtime. When adaptation is chosen, alternatively to using XPath-based means to identify Web objects we store the actual result subtree. In case of HTML leaf elements, which are usually the elements under consideration for navigation actions, we instead store the tree rooted at the n-th ancestor of the element, and the additional fact where the result element is located within this tree. In this way, tree matching can also be exploited for HTML leaf elements.\nWrapper designers can choose between various similarity measures: this includes in particular the Simple Tree Matching algorithm [18] and the Weighted Tree Matching algorithm described in Section 3. In future, further algorithms will extend the capabilities of the tool, e.g., a bigram-based tree matching that is capable to deal with node permutations in a more favorable fashion. In addition to the similarity function, one can choose certain parameters, e.g., whether to use the HTML element name as node label or instead to use spelling attributes such as class and id attributes. Figure 2 illustrates the configuration of wrapper adaptation in Visual Developer."}, {"heading": "5 Automatic Wrapper Adaptation", "text": ""}, {"heading": "5.1 Self-repairing rules", "text": "Figure 3 describes the adaptation process. The wrapper adaptation process is triggered upon violation of defined constraints. In case in the initial wrapper an element is detected with an XPath, the adaptation procedure substitutes this by storing the subtree of a matched element. In case the wrapper definition already stores the example tree, and the similarity computation returns results that violate the defined constraints, the threshold is lowered or raised until a perfect match is generated.\nDuring runtime, the stored tree is compared to the elements on the new page, and the best fitting element(s) are considered as extraction results. During configuration, wrapper designers can choose an algorithm (such as the Weighted Tree Matching), and a similarity threshold. The similarity threshold can be constant, or defined to be within an interval of acceptable thresholds. During execution, various thresholds within the allowed range are considered, and the one generating the best fit with respect to the defined constraints is chosen.\nAs a next step, the stored tree is refined and generalized so that it maximizes the matching value for both the original subtree and the new trees, reflecting the changes of a Web page over time. This generalization process generates a simple\ntree grammar, a \u201ctree template\u201d that is allowed to use occurrence indicators (one or more element, at least one element, etc.) and optional depth levels. In further runs, the tree template is compared against the sub trees of an active Web page during execution. First, the algorithm checks which trees on the new page satisfy the tree template. In case the results are within the defined integrity constraints, no further action is taken. In case the results are not satisfying, the system searches for most similar trees based on the defined distance metrics; in this case, the wrapper is auto-adapted, the tree template is further refined and the threshold or threshold interval is automatically re-adjusted. At the very end of the process, the corrected wrapper is stored in the wrapper repository and committed to a versioning system to keep track of all changes."}, {"heading": "5.2 Wrapper Re-Induction", "text": "In practice, single adaptation steps of rules and actions are embedded into the whole execution process of a wrapper and the adapted wrapper is stored in the repository after all adaptation steps have been concluded. The need for adapting a particular rule influences the further execution steps.\nUsually, wrapper generation in VD is a hierarchical top-down process \u2013 e.g., first, a \u201chotel record\u201d is characterized, and inside the hotel record, entities such as \u201crating\u201d and \u201croom types\u201d. To define a rule to match such entities, the wrapper designer visually selects an example and together with system suggestions generalizes the rule configuration until the desired instances are matched. To support the automatic adaptation process during runtime, as described above, the wrapper designer further specifies what it means that extraction failed. In general, this means wrong or missing data, and with integrity constraints one can give indications how correct results look like. The upper half of Figure 4 summarizes the wrapper generation.\nDuring wrapper creation, the application designer provides a number of configuration settings to this process. This includes:\n\u2013 Threshold Values. \u2013 Priorities/Order of Adaptation Algorithms used. \u2013 Flags of the chosen algorithm (e.g., using HTML element name as node label,\nusing id/class attributes as node labels, etc.). \u2013 Triggers for bottom-up, top-down and process flow adaptation bubbling. \u2013 Whether stored tree-grams and XPath statements are updated based on\nadaptation results to be additionally used as inputs in future adaptation procedures (reflecting and addressing regular slight changes of a Web page over time).\nTriggers in Adaptation Settings can be used to force adaptation of further fragments of the wrapper as depicted in the lower half of Figure 4.\n\u2013 Top-down: forcing adaptation of all/some descendant rules (e.g., adapt the \u201cprice\u201d rule as well to identify prices within a record if the \u201crecord\u201d rule was adapted). \u2013 Bottom-up: forcing adaptation of a parent rule in case adaptation of a particular rule was not successful. Experimental evaluation pointed out that in such cases it is often the problem that the parent rule already provides wrong or missing results (even if matched by the integrity constraints) and has to be adapted first. \u2013 Process flow: it might happen that particular rule matches can no longer detected because the wrapper evaluates on the wrong page. Hence, there is the need to use variations in the deep web navigation actions. In particular, a simple approach explored at this time is to use a switch window or back step action to check if the previous window or another tab/popup provides the required information."}, {"heading": "6 Performances Measurement", "text": "For our initial performance evaluation we tested the robustness of our Wrappers against real world use-cases. Actual areas of interest for Web data extraction problems include social networks, retail market and Web communities. We defined a total of 7 scenarios and designed 10 adaptive wrappers each. Results, by means of precision, recall and F1-score, are as shown in Table 1. Column thresh. represents the fixed threshold value; tp, fp and fn summarize true and false positive, and false negative, respectively. Performance obtained by using simple and weighted tree matching are good; these algorithms are definitely viable solutions to our initial purpose and provide high degree of reliability (F-Measure > 90%)."}, {"heading": "7 Conclusions and Future Work", "text": "In literature, several implementations of systems to extract data from Web sources have been presented, but there is a lack of solutions about their maintenance. This paper tries to address this problem, describing adaptive techniques to make Web data extraction systems, based on wrappers, self-maintainable, adopting algorithms optimized to this purpose. So, enhanced Web wrappers become able to recognize structural modifications of Web sources and to adapt their functioning accordingly. Characteristics of our self-repairable solution are discussed in details, providing first experimental results to evaluate its robustness. More experimentation has to come in the next future.\nMoreover, as for future work, additional algorithms would be included in order to improve the capabilities of the adaptation feature; in particular, a viable idea could be to generalize a bigram-based tree matching algorithm capable of dealing with node permutations in a more efficient way with respect to Simple Tree Matching based algorithms adopted as to date. Similarly, the Jaro-Winkler\ndistance could be adapted to our tree matching problem in order to better reflect missing or added node levels, so as improving performance of our adaptation process. Finally, the tree-grammar could be extended to classify different topologies of templates (those frequently adopted by Web pages), in order to define several standard protocols of automatic adaptation, to be adopted in specific contexts."}], "references": [{"title": "Web data extraction system", "author": ["R. Baumgartner", "W. Gatterbauer", "G. Gottlob"], "venue": "Encyclopedia of Database Systems pp. 3465\u20133471", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable web data extraction for online market intelligence", "author": ["R. Baumgartner", "G. Gottlob", "M. Herzog"], "venue": "Proceedings of the VLDB Endowment 2(2), 1512\u20131523", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on tree edit distance and related problems", "author": ["P. Bille"], "venue": "Theoretical computer science 337(1-3), 217\u2013239", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic repairing of web wrappers by combining redundant views", "author": ["B. Chidlovskii"], "venue": "Proceedings of the 14th International Conference on Tools with Artificial Intelligence. pp. 399\u2013406. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A machine learning approach to web mining", "author": ["F. Esposito", "D. Malerba", "L. Di Pace", "P. Leo"], "venue": "AI* IA 99: Advances in Artificial Intelligence pp. 190\u2013201", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic wrapper adaptation by tree edit distance matching", "author": ["E. Ferrara", "R. Baumgartner"], "venue": "Combinations of Intelligent Methods and Applications pp. 41\u201354", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of automatically adaptable web wrappers", "author": ["E. Ferrara", "R. Baumgartner"], "venue": "Proceedings of the 3rd International Conference on Agents and Artificial Intelligence. pp. 211\u2013217", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Web data extraction, application and techniques: A survey", "author": ["E. Ferrara", "G. Fiumara", "R. Baumgartner"], "venue": "Technical Report", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Web information extraction by HTML tree edit distance matching", "author": ["Y. Kim", "J. Park", "T. Kim", "J. Choi"], "venue": "Proceedings of the International Conference on Convergence Information Technology. pp. 2455\u20132460. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Wrapper verification", "author": ["N. Kushmerick"], "venue": "World Wide Web 3(2), 79\u201394", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Finite-state approaches toWeb information extraction", "author": ["N. Kushmerick"], "venue": "Extraction in the Web Era pp. 77\u201391", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression testing for wrapper maintenance", "author": ["N Kushmerick"], "venue": "Proceedings of the National Conference on Artificial Intelligence. pp. 74\u2013284", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "A brief survey of web data extraction tools", "author": ["A. Laender", "B. Ribeiro-Neto", "A. da Silva", "J. Teixeira"], "venue": "ACM Sigmod Record 31(2), 84\u201393", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Wrapper maintenance: A machine learning approach", "author": ["K. Lerman", "S. Minton", "C. Knoblock"], "venue": "Journal of Artificial Intelligence Research 18(1), 149\u2013181", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Schema-guided wrapper maintenance for web-data extraction", "author": ["X. Meng", "D. Hu", "C. Li"], "venue": "Proceedings of the 5th ACM international workshop on Web information and data management. pp. 1\u20138. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatically generating labeled examples for web wrapper maintenance", "author": ["J. Raposo", "A. Pan", "M. Alvarez", "J. Hidalgo"], "venue": "Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence. pp. 250\u2013256", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Information extraction", "author": ["S. Sarawagi"], "venue": "Foundations and Trends in Databases 1(3), 261\u2013377", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "The tree-to-tree editing problem", "author": ["S. Selkow"], "venue": "Information processing letters 6(6), 184\u2013186", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1977}, {"title": "Identifying syntactic differences between two programs", "author": ["W. Yang"], "venue": "Software: Practice and Experience 21(7), 739\u2013755", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}], "referenceMentions": [{"referenceID": 4, "context": "The first one relies on machine learning platforms [5]; a system analyzes, possibly, huge amount of positive and negative examples during a training period, and, then, it infers some set of rules that makes it able to perform its tasks in the same domain or Web site.", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "[13] provided the first rigorous taxonomical classification of Web data extraction systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Kushmerick [11] classified several finite-state approaches to generate wrappers, such as the wrapper induction, natural language processing approaches and hidden Markov models.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "Sarawagi [17] provided the most comprehensive survey on the information extraction panorama.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "[1] and later Ferrara et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] provided two different surveys on the discipline of Web data extraction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Kushmerick [12,10] for first introduced the concept of wrapper maintenance as the process of verifying the correct functioning of the data extraction procedures and manually, automatically or in a semi-automatic way, intervene in case of malfunctioning.", "startOffset": 11, "endOffset": 18}, {"referenceID": 9, "context": "Kushmerick [12,10] for first introduced the concept of wrapper maintenance as the process of verifying the correct functioning of the data extraction procedures and manually, automatically or in a semi-automatic way, intervene in case of malfunctioning.", "startOffset": 11, "endOffset": 18}, {"referenceID": 13, "context": "Lerman and Minton [14], instead, faced both the problems of verifying the correctness of data extracted by a wrapper and eventually try to repair it.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "Another approach based on machine learning has been provided by Chidlovskii [4]; he described a system which can automatically classify Web pages in order to extract information from those pages which can be handled adopting both conventional extraction rules and ensemble methods of machine learning, such as the content features analysis.", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "[15] developed the SG-WRAM (Schema-Guided WRApper Maintenance) slightly modifying the perspective of Web wrappers generation, observing that changes in Web pages, even substantial, always preserve syntactic features (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16]; they adopted a collected sample of positive labeled examples during the normal execution of the wrappers, to be exploited in case of malfunctioning, in order to re-induct the broken wrapper ensuring a good accuracy of the process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The tree edit distance problem is a well-known NP-hard problem [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 17, "context": "Several approximate solutions have been advanced during the years; the most appropriate algorithm to face the problem of matching up similar trees, has been suggested by Selkow [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 24, "endOffset": 29}, {"referenceID": 6, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 24, "endOffset": 29}, {"referenceID": 18, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "[9], has been performed exploiting these two algorithms to extract information from HTML Web pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "This work relies on some assumptions: i) Web pages are represented by using DOM trees, as the HTML standard imposes; ii) it is possible to identify elements within a DOM tree by using the XPath language; iii) the logics of XPath underly the functioning of Web wrappers (this is further explained in following sections and in [1,2]).", "startOffset": 325, "endOffset": 330}, {"referenceID": 1, "context": "This work relies on some assumptions: i) Web pages are represented by using DOM trees, as the HTML standard imposes; ii) it is possible to identify elements within a DOM tree by using the XPath language; iii) the logics of XPath underly the functioning of Web wrappers (this is further explained in following sections and in [1,2]).", "startOffset": 325, "endOffset": 330}, {"referenceID": 17, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 116, "endOffset": 121}, {"referenceID": 6, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 116, "endOffset": 121}, {"referenceID": 1, "context": "In Lixto Visual Developer (VD) [2], a number of mechanisms are offered to create a resilient wrapper.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "Wrapper designers can choose between various similarity measures: this includes in particular the Simple Tree Matching algorithm [18] and the Weighted Tree Matching algorithm described in Section 3.", "startOffset": 129, "endOffset": 133}], "year": 2011, "abstractText": "The amount of information available on the Web grows at an incredible high rate. Systems and procedures devised to extract these data from Web sources already exist, and different approaches and techniques have been investigated during the last years. On the one hand, reliable solutions should provide robust algorithms of Web data mining which could automatically face possible malfunctioning or failures. On the other, in literature there is a lack of solutions about the maintenance of these systems. Procedures that extract Web data may be strictly interconnected with the structure of the data source itself; thus, malfunctioning or acquisition of corrupted data could be caused, for example, by structural modifications of data sources brought by their owners. Nowadays, verification of data integrity and maintenance are mostly manually managed, in order to ensure that these systems work correctly and reliably. In this paper we propose a novel approach to create procedures able to extract data from Web sources \u2013 the so called Web wrappers \u2013 which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves.", "creator": "LaTeX with hyperref package"}}}