{"id": "1107.0051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "On Prediction Using Variable Order Markov Models", "abstract": "This paper deals with algorithms for predicting discrete sequences via a finite alphabet using Markov models of variable order. The class of such algorithms is large and includes basically all lossless compression algorithms. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real sequences from three domains: proteins, English text and music. Comparison is made in terms of prediction quality based on average log loss. We also compare classification algorithms based on these prediction tasks with respect to a number of large protein classification tasks. Our results suggest that a \"decomposer\" CTW (a variant of the CTW algorithm) and PPM exceed all other algorithms in prediction tasks.", "histories": [["v1", "Thu, 30 Jun 2011 20:43:01 GMT  (411kb)", "http://arxiv.org/abs/1107.0051v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["r begleiter", "r el-yaniv", "g yona"], "accepted": false, "id": "1107.0051"}, "pdf": {"name": "1107.0051.pdf", "metadata": {"source": "CRF", "title": "On Prediction Using Variable Order Markov Models", "authors": ["Ron Begleiter", "Ran El-Yaniv"], "emails": ["ronbeg@cs.technion.ac.il", "rani@cs.technion.ac.il", "golan@cs.cornell.edu"], "sections": [{"heading": "1. Introduction", "text": "Learning of sequential data continues to be a fundamental task and a challenge in pattern recognition and machine learning. Applications involving sequential data may require prediction of new events, generation of new sequences, or decision making such as classification of sequences or sub-sequences. The classic application of discrete sequence prediction algorithms is lossless compression (Bell et al., 1990), but there are numerous other applications involving sequential data, which can be directly solved based on effective prediction of discrete sequences. Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Schu\u0308tze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classification (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).1\n1. Besides such applications, which directly concern sequences, there are other, less apparent applications, such as image and texture analysis and synthesis that can be solved based on discrete sequence prediction algorithms; see, e.g., (Bar-Joseph et al., 2001).\nc\u00a92004 AI Access Foundation. All rights reserved.\nThe literature on prediction algorithms for discrete sequences is rather extensive and offers various approaches to analyze and predict sequences over finite alphabets. Perhaps the most commonly used techniques are based on Hidden Markov Models (HMMs) (Rabiner, 1989). HMMs provide flexible structures that can model complex sources of sequential data. However, dealing with HMMs typically requires considerable understanding of and insight into the problem domain in order to restrict possible model architectures. Also, due to their flexibility, successful training of HMMs usually requires very large training samples (see hardness results of Abe & Warmuth, 1992, for learning HMMs).\nIn this paper we focus on general-purpose prediction algorithms, based on learning Variable order Markov Models (VMMs) over a finite alphabet \u03a3. Such algorithms attempt to learn probabilistic finite state automata, which can model sequential data of considerable complexity. In contrast to N -gram Markov models, which attempt to estimate conditional distributions of the form P (\u03c3|s), with s \u2208 \u03a3N and \u03c3 \u2208 \u03a3, VMM algorithms learn such conditional distributions where context lengths |s| vary in response to the available statistics in the training data. Thus, VMMs provide the means for capturing both large and small order Markov dependencies based on the observed data. Although in general less expressive than HMMs, VMM algorithms have been used to solve many applications with notable success. The simpler nature of VMM methods also makes them amenable for analysis, and some VMM algorithms that we discuss below enjoy tight theoretical performance guarantees, which in general are not possible in learning using HMMs.\nThere is an intimate relation between prediction of discrete sequences and lossless compression algorithms, where, in principle, any lossless compression algorithm can be used for prediction and vice versa (see, e.g., Feder & Merhav, 1994). Therefore, there exist an abundance of options when choosing a prediction algorithm. This multitude of possibilities poses a problem for practitioners seeking a prediction algorithm for the problem at hand.\nOur goal in this paper is to consider a number of VMM methods and compare their performance with respect to a variety of practical prediction tasks. We aim to provide useful insights into the choice of a good general-purpose prediction algorithm. To this end we selected six prominent VMM algorithms and considered sequential data from three different domains: molecular biology, text and music. We focus on a setting in which a learner has the start of some sequence (e.g., a musical piece) and the goal is to generate a model capable of predicting the rest of the sequence. We also consider a scenario where the learner has a set of training samples from some domain and the goal is to predict, as accurately as possible, new sequences from the same domain. We measure performance using log-loss (see below). This loss function, which is tightly related to compression, measures the quality of probabilistic predictions, which are required in many applications.\nIn addition to these prediction experiments we examine the six VMM algorithms with respect to a number of large protein classification problems. Protein classification is one of the most important problems in computational molecular biology. Such classification is useful for functional and structural categorization of proteins, and may direct experiments to further explore and study the functionality of proteins in the cell and their interactions with other molecules.\nIn the prediction experiments, two of the VMM algorithms we consider consistently achieve the best performance. These are the \u2018decomposed context tree weighting (CTW)\u2019 and \u2018prediction by partial match (PPM)\u2019 algorithms. CTW and PPM are well known in the\nlossless compression arena as outstanding players. Our results thus provide further evidence of the superiority of these algorithms, with respect to new domains and two different prediction settings. The results of our protein classification experiments are rather surprising as both of these two excellent predictors are inferior to an algorithm, which is obtained by simple modifications of the prediction component of the well-known Lempel-Ziv-78 compression algorithm (Ziv & Lempel, 1978; Langdon, 1983). This rather new algorithm, recently proposed by Nisenson et al. (2003), is a consistent winner in the all the protein classification experiments and achieves surprisingly good results that may be of independent interest in protein analysis.\nThis paper is organized as follows. We begin in Section 2 with some preliminary definitions. In Section 3, we present six VMM prediction algorithms. In Section 4, we present the experimental setup. In Sections 5-6, we discuss the experimental results. Part of the experimental related details are discussed in Appendices A-C. In Section 7, we discuss the related work. In Section 8, we conclude by introducing a number of open problems raised by this research. Note that the source code of all the algorithms we consider is available at http://www.cs.technion.ac.il/~ronbeg/vmm."}, {"heading": "2. Preliminaries", "text": "Let \u03a3 be a finite alphabet. A learner is given a training sequence qn1 = q1q2 \u00b7 \u00b7 \u00b7 qn, where qi \u2208 \u03a3 and qiqi+1 is the concatenation of qi and qi+1. Based on q n 1 , the goal is to learn a model P\u0302 that provides a probability assignment for any future outcome given some past. Specifically, for any \u201ccontext\u201d s \u2208 \u03a3\u2217 and symbol \u03c3 \u2208 \u03a3 the learner should generate a conditional probability distribution P\u0302 (\u03c3|s).\nPrediction performance is measured via the average log-loss \u2113(P\u0302 , xT1 ) of P\u0302 (\u00b7|\u00b7), with respect to a test sequence xT1 = x1 \u00b7 \u00b7 \u00b7 xT ,\n\u2113(P\u0302 , xT1 ) = \u2212 1\nT\nT \u2211\ni=1\nlog P\u0302 (xi|x1 \u00b7 \u00b7 \u00b7 xi\u22121), (1)\nwhere the logarithm is taken to base 2. Clearly, the average log-loss is directly related to the likelihood P\u0302 (xT1 ) = \u220fT i=1 P\u0302 (xi|x1 \u00b7 \u00b7 \u00b7 xi\u22121) and minimizing the average log-loss is completely equivalent to maximizing a probability assignment for the entire test sequence. Note that the converse is also true. A consistent probability assignment P\u0302 (xT1 ), for the entire sequence, which satisfies P\u0302 (xt\u221211 ) = \u2211 xt\u2208\u03a3 P\u0302 (x1 \u00b7 \u00b7 \u00b7 xt\u22121xt), for all t = 1, . . . , T , induces conditional probability assignments,\nP\u0302 (xt|x t\u22121 1 ) = P\u0302 (x t 1)/P\u0302 (x t\u22121 1 ), t = 1, . . . , T.\nTherefore, in the rest of the paper we interchangeably consider P\u0302 as a conditional distribution or as a complete (consistent) distribution of the test sequence xT1 .\nThe log-loss has various meanings. Perhaps the most important one is found in its equivalence to lossless compression. The quantity \u2212 log P\u0302 (xi|x1 \u00b7 \u00b7 \u00b7 xi\u22121), which is also called the \u2018self-information\u2019, is the ideal compression or \u201ccode length\u201d of xi, in bits per symbol, with respect to the conditional distribution P\u0302 (X|x1 \u00b7 \u00b7 \u00b7 xi\u22121), and can be implemented online (with arbitrarily small redundancy) using arithmetic encoding (Rissanen & Langdon, 1979).\nThus, the average log-loss also measures the average compression rate of the test sequence, when using the predictions generated by P\u0302 . In other words, a small average log-loss over the xT1 sequence implies a good compression of this sequence. 2\nWithin a probabilistic setting, the log-loss has the following meaning. Assume that the training and test sequences were emitted from some unknown probabilistic source P . Let the test sequence be given by the value of the sequence of random variables XT1 = X1 \u00b7 \u00b7 \u00b7XT . A well known fact is that the distribution P uniquely minimizes the mean log-loss; that is,3\nP = argmin P\u0302\n{\n\u2212EP{log P\u0302 (X T 1 )}\n}\n.\nDue to the equivalence of the log-loss and compression, as discussed above, the mean of the log-loss of P (under the true distribution P ) achieves the best possible compression, or the entropy HT (P ) = \u2212E log P (X T 1 ). However, the true distribution P is unknown and the learner generates the proxy P\u0302 using the training sequence. The extra loss (due to the use of P\u0302 , instead of P ) beyond the entropy, is called the redundancy and is given by\nDT (P ||P\u0302 ) = EP\n{\n\u2212 log P\u0302 (XT1 )\u2212 (\u2212 logP (X T 1 ))\n}\n. (2)\nIt is easy to see that DT (P ||P\u0302 ) is the (T th order) Kullback-Leibler (KL) divergence (see Cover & Thomas, 1991, Sec. 2.3). The normalized redundancy DT (P ||P\u0302 )/T (of a sequence of length T ) gives the extra bits per symbol (over the entropy rate) when compressing a sequence using P\u0302 .\nThis probabilistic setting motivates a desirable goal, when devising a general-purpose prediction algorithm: minimize the redundancy uniformly, with respect to all possible distributions. A prediction algorithm for which we can bound the redundancy uniformly, with respect to all distributions in some given class, is often called universal with respect to that class. A lower bound on the redundancy of any universal prediction (and compression) algorithm is \u2126(K( log T2T )), where K is (roughly) the number of parameters of the model encoding the distribution P\u0302 (Rissanen, 1984).4 Some of the algorithms we discuss below are universal with respect to some classes of sources. For example, when an upper bound on the Markov order is known, the ctw algorithm (see below) is universal (with respect to ergodic and stationary sources), and in fact, has bounded redundancy, which is close to the Rissannen lower bound."}, {"heading": "3. VMM Prediction Algorithms", "text": "In order to assess the significance of our results it is important to understand the sometimes subtle differences between the different algorithms tested in this study and their variations. In this section we describe in detail each one of these six algorithms. We have adhered to a unified notation schema in an effort to make the commonalities and differences between these algorithms clear.\n2. The converse is also true: any compression algorithm can be translated into probability assignment P\u0302 ; see, e.g., (Rissanen, 1984). 3. Note that very specific loss functions satisfy this property. (See Miller et al., 1993). 4. A related lower bound in terms of channel capacity is given by Merhav and Feder (1995).\nMost algorithms for learning VMMs include three components: counting, smoothing (probabilities of unobserved events) and variable-length modeling. Specifically, all such algorithms base their probability estimates on counts of the number of occurrences of symbols \u03c3 appearing after contexts s in the training sequence. These counts provide the basis for generating the predictor P\u0302 . The smoothing component defines how to handle unobserved events (with zero value counts). The existence of such events is also called the \u201czero frequency problem\u201d. Not handling the zero frequency problem is harmful because the log-loss of an unobserved but possible event, which is assigned a zero probability by P\u0302 , is infinite. The algorithms we consider handle such events with various techniques.\nFinally, variable length modeling can be done in many ways. Some of the algorithms discussed here construct a single model and some construct several models and average them. The models themselves can be bounded by a pre-determined constant bound, which means that the algorithm does not consider contexts that are longer than the bound. Alternatively, models may not be bounded a-priori, in which case the maximal context size is data-driven.\nThere are a great many VMM prediction algorithms. In fact, any lossless compression algorithm can be used for prediction. For the present study we selected six VMM prediction algorithms, described below. We attempted to include algorithms that are considered to be top performers in lossless compression. We, therefore, included the \u2018context tree weighting (CTW)\u2019 and \u2018prediction by partial match (PPM)\u2019 algorithms. The \u2018probabilistic suffix tree (PST)\u2019 algorithm is well known in the machine learning community. It was successfully used in a variety of applications and is hence included in our set. To gain some perspective we also included the well known lz78 (prediction) algorithm that forms the basis of many commercial applications for compression. We also included a recent prediction algorithm from Nisenson et al. (2003) that is a modification of the lz78 prediction algorithm. The algorithms we selected are quite different in terms of their implementations of the three components described above. In this sense, they represent different approaches for VMM prediction.5"}, {"heading": "3.1 Lempel-Ziv 78 (LZ78)", "text": "The lz78 algorithm is among the most popular lossless compression algorithms (Ziv & Lempel, 1978). It is used as the basis of the Unix compress utility and other popular archiving utilities for PCs. It also has performance guarantees within several analysis models. This algorithm (together with the lz77 compresion method) attracted enormous attention and inspired the area of lossless compression and sequence prediction.\nThe prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983). The presentation of this algorithm is simplified after the well-known lz78 compression algorithm, which works as follows, is understood. Given a sequence qn1 \u2208 \u03a3\nn, lz78 incrementally parses qn1 into non-overlapping adjacent \u2018phrases\u2019, which are collected into a phrase \u2018dictionary\u2019. The algorithm starts with a dictionary containing the empty phrase \u01eb. At each step the algorithm parses a new phrase, which is the shortest phrase that is not yet in the dictionary. Clearly, the newly parsed phrase s\u2032 extends an existing\n5. We did not include in the present work the prediction algorithm that can be derived from the more recent bzip compression algorithm (see http://www.digistar.com/bzip2), which is based on the successful Burrows-Wheeler Transform (Burrows & Wheeler, 1994; Manzini, 2001).\ndictionary phrase by one symbol; that is, s\u2032 = s\u03c3, where s is already in the dictionary (s can be the empty phrase). For compression, the algorithm encodes the index of s\u2032 (among all parsed phrases) followed by a fixed code for \u03c3. Note that coding issues will not concern us in this paper. Also observe that lz78 compresses sequences without explicit probabilistic estimates. Here is an example of this lz78 parsing: if q111 = abracadabra, then the parsed phrases are a|b|r|ac|ad|ab|ra. Observe that the empty sequence \u01eb is always in the dictionary and is omitted in our discussions.\nAn lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983). We describe separately the learning and prediction phases.6 For simplicity we first discuss the binary case where \u03a3 = {0, 1}, but the algorithm can be naturally extended to alphabets of any size (and in the experiments discussed below we do use the multi-alphabet algorithm). In the learning phase the algorithm constructs from the training sequence qn1 a binary tree (trie) that records the parsed phrases (as discussed above). In the tree we also maintain counters that hold statistics of qn1 . The initial tree contains a root and two (left and right) leaves. The left child of a node corresponds to a parsing of \u20180\u2019 and the right child corresponds to a parsing of \u20181\u2019. Each node maintains a counter. The counter in a leaf is always set to 1. The counter in an internal node is always maintained so that it equals the sum of its left and right child counters. Given a newly parsed phrase s\u2032, we start at the root and traverse the tree according to s\u2032 (clearly the tree contains a corresponding path, which ends at a leaf). When reaching a leaf, the tree is expanded by making this leaf an internal node and adding two leaf-sons to this new internal node. The counters along the path to the root are updated accordingly.\nTo compute the estimate P\u0302 (\u03c3|s) we start from the root and traverse the tree according to s. If we reach a leaf before \u201cconsuming\u201d s we continue this traversal from the root, etc. Upon completion of this traversal (at some internal node, or a leaf) the prediction for \u03c3 = \u20180\u2019 is the \u20180\u2019 (left) counter divided by the sum of \u20180\u2019 and \u20181\u2019 counters at that node, etc.\nFor larger alphabets, the algorithm is naturally extended such that the phrases are stored in a multi-way tree and each internal node has exactly k = |\u03a3| children. In addition, each node has k counters, one for each possible symbol. In Figure 1 we depict the resulting tree for the training sequence q111 = abracadabra and calculate the probability P\u0302 (b|ab), assuming \u03a3 = {a, b, c, d, r}.\nSeveral performance guarantees were proven for the lz78 compression (and prediction) algorithm. Within a probabilistic setting (see Section 2), when the unknown source is stationary and ergodic Markov of finite order, the redundancy is bounded above by (1/ ln n) where n is the length of the training sequence (Savari, 1997). Thus, the lz78 algorithm is a universal prediction algorithm with respect to the large class of stationary and ergodic Markov sources of finite order."}, {"heading": "3.2 Prediction by Partial Match (PPM)", "text": "The Prediction by Partial Match (ppm) algorithm (Cleary & Witten, 1984) is considered to be one of the best lossless compression algorithms.7 The algorithm requires an upper bound\n6. These \u201cphases\u201d can be combined and operated together online. 7. Specifically, the ppm-ii variant of ppm currently achieves the best compression rates over the standard\nCalgary Corpus benchmark (Shkarin, 2002).\nD on the maximal Markov order of the VMM it constructs. ppm handles the zero frequency problem using two mechanisms called escape and exclusion. There are several ppm variants distinguished by the implementation of the escape mechanism. In all variants the escape mechanism works as follows. For each context s of length k \u2264 D, we allocate a probability mass P\u0302k(escape|s) for all symbols that did not appear after the context s (in the training sequence). The remaining mass 1 \u2212 P\u0302k(escape|s) is distributed among all other symbols that have non-zero counts for this context. The particular mass allocation for \u2018escape\u2019 and the particular mass distribution P\u0302k(\u03c3|s), over these other symbols \u03c3, determine the ppm\nvariant. The mechanism of all ppm variants satisfies the following (recursive) relation,\nP\u0302 (\u03c3|snn\u2212D+1) =\n\n\n\nP\u0302D(\u03c3|s n n\u2212D+1),\nif sn n\u2212D+1\u03c3 apeared in the training sequence;\nP\u0302D(escape|s n n\u2212D+1) \u00b7 P\u0302 (\u03c3|s n n\u2212D+2), otherwise.\n(3)\nFor the empty context \u01eb, ppm takes P\u0302 (\u03c3|\u01eb) = 1/|\u03a3|.8\nThe exclusion mechanism is used to enhance the escape estimation. It is based on the observation that if a symbol \u03c3 appears after the context s of length k \u2264 D, there is no need to consider \u03c3 as part of the alphabet when calculating P\u0302k(\u00b7|s\n\u2032) for all s\u2032 suffix of s (see Equation (3)). Therefore, the estimates P\u0302k(\u00b7|s) are potentially more accurate since they are based on a smaller (observed) alphabet.\nThe particular ppm variant we consider here is called \u2018Method C\u2019 (ppm-c) and is defined as follows. For each sequence s and symbol \u03c3 let N(s\u03c3) denote the number of occurrences of s\u03c3 in the training sequence. Let \u03a3s be the set of symbols appearing after the context s (in the training sequence); that is, \u03a3s = {\u03c3 : N(s\u03c3) > 0}. For ppm-c we thus have\nP\u0302k(\u03c3|s) = N(s\u03c3)\n|\u03a3s|+ \u2211\n\u03c3\u2032\u2208\u03a3s\nN(s\u03c3\u2032) , if \u03c3 \u2208 \u03a3s; (4)\nP\u0302k(escape|s) = |\u03a3s|\n|\u03a3s|+ \u2211\n\u03c3\u2032\u2208\u03a3s\nN(s\u03c3\u2032) , (5)\nwhere we assume that |s| = k. This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997). As noted by Witten and Bell (1991) there is no principled justification for any of the various ppm escape mechanisms.\nOne implementation of ppm-c is based on a trie data structure. In the learning phase the algorithm constructs a trie T from the training sequence qn1 . Similar to the lz78 trie, each node in T is associated with a symbol and has a counter. In contrast to the unbounded lz78 trie, the maximal depth of T is D+1. The algorithm starts with a root node corresponding to the empty sequence \u01eb and incrementally parses the training sequence, one symbol at a time. Each parsed symbol qi and its D-sized context, x i\u22121 i\u2212D, define a potential path in T , which is constructed, if it does not yet exist. Note that after parsing the first D symbols, each newly constructed path is of length D+ 1. The counters along this path are incremented. Therefore, the counter of any node, with a corresponding path s\u03c3 (where \u03c3 is the symbol associated with the node) is N(s\u03c3).\nUpon completion, the resulting trie induces the probability P\u0302 (\u03c3|s) for each symbol \u03c3 and context s with |s| \u2264 D. To compute P\u0302 (\u03c3|s) we start from the root and traverse the tree according to the longest suffix of s, denoted s\u2032, such that s\u2032\u03c3 corresponds to a complete path from the root to a leaf. We then use the counters N(s\u2032\u03c3\u2032) to compute \u03a3s\u2032 and the estimates as given in Equations (3), (4) and (5).\nThe above implementation (via a trie) is natural for the learning phase and it is easy to see that the time complexity for learning qn1 is O(n) and the space required for the\n8. It also makes sense to use the frequency count of symbols over the training sequence.\nworst case trie is O(D \u00b7 n). However, a straightforward computation of P\u0302 (\u03c3|s) using the trie incurs O(|s|2) time.9 In Figure 2 we depict the resulting tree for the training sequence q111 = abracadabra and calculate the probabilities P\u0302 (b|ar) and P\u0302 (d|ra), assuming \u03a3 = {a,b,c,d,r}."}, {"heading": "3.3 The Context Tree Weighting Method (CTW)", "text": "The Context Tree Weighting Method (ctw) algorithm (Willems et al., 1995) is a strong lossless compression algorithm that is based on a clever idea for combining exponentially many VMMs of bounded order.10 In this section we consider the original ctw algorithm for binary alphabets. In the next section we discuss extensions for larger alphabets.\nIf the unknown source of the given sample sequences is a \u201ctree source\u201d of bounded order, then the compression redundancy of ctw approaches the Rissanen lower bound at rate 1/n.11 A D-bounded tree source is a pair (M, \u0398M ) where M is set of sequences of\n9. After the construction of the trie it is possible to generate a corresponding suffix tree that allows a rapid computation of P\u0302 (\u03c3|s) in O(|s|) time. It is possible to generate this suffix tree in time proportional to the sum of all paths in the trie. 10. The paper presenting this idea (Willems et al., 1995) received the 1996 Paper Award of the IEEE Information Theory Society. 11. Specifically, for \u03a3 = {0, 1}, Rissanen\u2019s lower bound on the redundancy when compressing xn1 is \u2126(|M |( log n\n2n )). The guaranteed ctw redundancy is O(|M |( log n 2n ) + 1 n |M | log |M |), where M is the suffix\nset of the unknown source.\nlength \u2264 D. This set must be a complete and proper suffix set, where \u2018complete\u2019 means that for any sequence x of length \u2265 D, there exists s \u2208 M , which is a suffix of x. \u2018Proper\u2019 means that no sequence in M has a strict suffix in M . The set \u0398M contains one probability distribution over \u03a3 for each sequence in M . A proper and complete suffix set is called a model. Given some initial sequence (\u201cstate\u201d) s \u2208 M , the tree source can generate a random sequence x by continually drawing the next symbol from the distribution associated with the unique suffix of x.\nAny full binary tree of height \u2264 D corresponds to one possible D-bounded model M . Any choice of appropriate probability distributions \u0398M defines, together with M , a unique tree source. Clearly, all prunings of the perfect binary tree12 of height D that are full binary trees correspond to the collection of all D-bounded models.\nFor each modelM , ctw estimates a set of probability distributions, \u0398M = {P\u0302M (\u00b7|s)}s\u2208M , such that (M,\u0398M ) is a tree source. Each distribution P\u0302M (\u00b7|s) is a smoothed version of a maximum likelihood estimate based on the training sequence. The core idea of the ctw algorithm is to generate a predictor that is a mixture of all these tree sources (corresponding to all these D-bounded models). Let MD be the collection of all D-bounded models. For any sequence xT1 the ctw estimated probability for x T 1 is given by\nP\u0302ctw(x T 1 ) =\n\u2211\nM\u2208MD\nw(M) \u00b7 P\u0302M (x T 1 ); (6)\nP\u0302M (x T 1 ) =\nT \u220f\ni=1\nP\u0302M (xi|suffixM (x i\u22121 i\u2212D)), (7)\nwhere suffixM (x) is the (unique) suffix of x in M and w(M) is an appropriate probability weight function. Clearly, the probability P\u0302M (x T 1 ), given in (7), is a product of independent events, by the definition of a tree source. Also note that we assume that the sequence xT1 has an \u201chistorical\u201d context defining the conditional probabilities P\u0302 (xi|suffixM (x i\u22121 i\u2212D)) of the first symbols. For example, we can assume that the sequence xT1 is a continuation of the training sequence. Alternatively, we can ignore (up to) the first D symbols of xT1 . A more sophisticated solution is proposed by Willems (1998); see also Section 7.\nNote that there is a huge (exponential in D) number of elements in the mixture (6), corresponding to all bounded models. To describe the ctw prediction algorithm we need to describe (i) how to (efficiently) estimate all the distributions P\u0302M (\u03c3|s); (ii) how to select the weights w(M); and (iii) how to efficiently compute the ctw mixture (6).\nFor each model M , ctw computes the estimate P\u0302M (\u00b7|s) using the Krichevsky-Trofimov (KT) estimator for memoryless binary sources (Krichevsky & Trofimov, 1981). The KTestimator enjoys an optimal proven redundancy bound of 2+log n2n . This estimator, which is very similar to Laplace\u2019s law of succession, is also very easy to calculate. Given a training sequence q 6= \u01eb, the KT-estimator, based on q, is equivalent to\nP\u0302kt(0|q) = N0(q) +\n1 2\nN0(q) +N1(q) + 1 ;\nP\u0302kt(1|q) = 1\u2212 P\u0302kt(0|q),\n12. In a perfect binary tree all internal nodes have two sons and all leaves are at the same level.\nwhere N0(q) = N(0) and N1(q) = N(1). That is, P\u0302kt(0|q) is the KT-estimated probability for \u20180\u2019, based on a frequency count in q.13\nFor any sequence s define subs(q) to be the unique (non-contiguous) sub-sequence of q consisting of the symbols following the occurrences of s in q. Namely, it is the concatenation of all symbols \u03c3 (in the order of their occurrence in q) such that s\u03c3 is a substring of q. For example, if s = 101 and q = 101011010, then sub101(q) = 010. Using this definition we can calculate the KT-estimated probability of a test sequence xT1 according to a context s, based on the training sequence q. Letting qs = subs(q) we have,\nP\u0302 skt(x T 1 |q) =\nT \u220f\ni=1\nP\u0302kt(xi|qs).\nFor the empty sequence \u01eb we have P\u0302 skt(\u01eb|q) = 1. One of the main technical ideas in the ctw algorithm is an efficient recursive computation of Equation (6). This is achieved through the following recursive definition. For any s such that |s| \u2264 D, define\nP\u0302 sctw(x T 1 ) =\n{\n1 2 P\u0302 s kt(x T 1 |q) + 1 2 P\u0302 0s ctw(x T 1 )P\u0302 1s ctw(x T 1 ), if |s| < D; P\u0302 skt(x T 1 |q), otherwise (|s| = D),\n(8)\nwhere q is, as usual, the training sequence. As shown by Willems et al. (1995, Lemma 2), for any training sequence q and test sequence x, P\u0302 \u01ebctw(x) is precisely the ctw mixture as given in Equation (6), where the probability weights w(M) are14\nw(M) = 2\u2212CD(M);\nCD(M) = | {s \u2208 M} | \u2212 1 + |{s \u2208 M : |s| < D}|.\nIn the learning phase, the ctw algorithm processes a training sequence qn1 and constructs a binary context tree T . An outgoing edge to a left son in T is labelled by \u20180\u2019 and an outgoing edge to a right son, by \u20181\u2019. Each node s \u2208 T is associated with the sequence corresponding to the path from this node to the root. This sequence (also called a \u2018context\u2019) is also denoted by s. Each node maintains two counters N0 and N1 that count the number of \u20180\u2019s (resp. \u20181\u2019s) appearing after the contexts s in the training sequence. We start with a perfect binary tree of height D and set the counters in each node to zero. We process the training sequence qn1 by considering all n \u2212 D contexts of size D and for each context we update the counters of the nodes along the path defined by this context. Upon completion we can estimate the probability of a test sequence xT1 using the relation (8) by computing P\u0302 \u01ebctw(x T 1 ). This probability estimate for the sequence x T 1 induces the conditional distributions P\u0302ctw(xi|x i\u22121 i\u2212D+1), as noted in Section 2. For example, in Figure 3 we depict the ctw tree with D = 2 constructed according to the training sequence q91 = 101011010.\n13. As shown by Tjalkens et al. (1997), the above simple form of the KT estimator is equivalent to the original form, given in terms of a Dirichlet distribution by Krichevsky and Trofimov (1981). The (original) KT-\nestimated probability of a sequence containing N0 zeros and N1 ones is \u222b 1\n0\n(1\u2212\u03b8)N0\u03b8N1\n\u03c0 \u221a \u03b8(1\u2212\u03b8) d\u03b8.\n14. There is a natural coding for D-bounded models such that the length of the code in bits for a model M is exactly CD(M).\nThis straightforward implementation requires O(|\u03a3|D) space (and time) for learning and O(T \u00b7 |\u03a3|D) for predicting xT1 . Clearly, such time and space complexities are hopeless even for moderate D values. We presented the algorithm this way for simplicity. However, as already mentioned by Willems et al. (1995), it is possible to obtain linear time complexities for both training and prediction. The idea is to only construct tree paths that occur in the training sequence. The counter values corresponding to all nodes along unexplored paths remain zero and, therefore, the estimated ctw probabilities for entire unvisited subtrees can be easily computed (using closed-form formulas). The resulting time (and space) complexity for training is O(Dn) and for prediction, O(TD). More details on this efficient implementation are nicely explained by Sadakane et al. (2000) (see also Tjalkens & Willems, 1997).\n0+3+1 = 1 8 ; because 10 is a leaf, according to Equation (8)\nP\u0302kt(0|10) = P\u0302ctw(0|10)."}, {"heading": "3.4 CTW for Multi-Alphabets", "text": "The above ctw algorithm for a binary alphabet can be extended in a natural manner to handle sequences over larger alphabets. One must (i) extend the KT-estimator for larger alphabets; and (ii) extend the recurrence relation (8). While these extensions can be easily obtained, it is reported that the resulting algorithm preforms poorly (see Volf, 2002, chap. 4). As noted by Volf, the reason is that the extended KT-estimator does not provide efficient smoothing for large alphabets. Therefore, the problem of extending the ctw algorithm for large alphabets is challenging.\nSeveral ctw extensions for larger alphabets have been proposed. Here we consider two. The first is a naive application of the standard binary ctw algorithm over a binary representation of the sequence. The binary representation is naturally obtained when the size of the alphabet is a power of 2. Suppose that k = log2 |\u03c3| is an integer. In this case we generate a binary sequence by concatenating binary words of size k, one for each alphabet symbol. If log2 |\u03c3| is not an integer we take k = \u2308log2 |\u03c3|\u2309. We denote the resulting\nalgorithm by bi-ctw. A more sophisticated binary decomposition of ctw was considered by Tjalkens et al. (1997). There eight binary machines were simultaneously constructed, one for each of the eight binary digits of the ascii representation of text. This decomposition does not achieve the best possible compression rates over the Calgary Corpus.\nThe second method we consider is Volf\u2019s \u2018decomposed ctw\u2019, denoted here by de-ctw (Volf, 2002). The de-ctw uses a tree-based hierarchical decomposition of the multi-valued prediction problem into binary problems. Each of the binary problems is solved via a slight variation of the binary ctw algorithm.\nLet \u03a3 be an alphabet with size k = |\u03a3|. Consider a full binary tree T\u03a3 with k leaves. Each leaf is uniquely associated with a symbol in \u03a3. Each internal node v of T\u03a3 defines the binary problem of predicting whether the next symbol is a leaf on v\u2019s left subtree or a leaf on v\u2019s right subtree. For example, for \u03a3 = {a,b,c,d,r}, Figure 4 depicts a tree T\u03a3 such that the root corresponds to the problem of predicting whether the next symbol is a or one of b,c,d and r. The idea is to learn a binary predictor, based on the ctw algorithm, for each internal node.\nIn a simplistic implementation of this idea, we construct a binary ctw for each internal node v \u2208 T\u03a3. We project the training sequence over the \u201crelevant\u201d symbols (i.e., corresponding to the subtree rooted by v) and translate the symbols on v\u2019s left (resp., right) sub-tree to 0s (resp., 1s). After training we predict the next symbol \u03c3 by assigning each symbol a probability that is the product of binary predictions along the path from the root of T\u03a3 to the leaf labeled by \u03c3.\nUnfortunately, this simple approach may result in poor performance and a slight modification is required.15 For each internal node v \u2208 T\u03a3, let ctwv be the associated binary predictor designed to handle the alphabet \u03a3v \u2286 \u03a3. Instead of translating the projected sequence (projected over \u03a3v) to a binary sequence, as in the simple approach, we construct ctwv based on a |\u03a3v|-ary context tree. Thus, ctwv still generates predictions over a binary alphabet but expands a suffix tree using the (not necessarily binary) \u03a3v alphabet. To generate binary predictions ctwv utilizes, in each node of the context tree, two counters for computing binary KT-estimations (as in the standard ctw algorithm). For example, in Figure 4(b) we depict ctw3 whose binary problem is defined by T\u03a3 of Figure 4(a). Another modification suggested by Volf, is to use a variant on the KT-estimator. Volf shows that the estimator Pe(0|q) = N0(q)+ 1 2\nN0(q)+N1(q)+1/8 achieves better results in practice. We used this\nestimator for the de-ctw implementation.16\nA major open issue when applying the de-ctw algorithm is the construction of an effective decomposition tree T\u03a3. Following (Volf, 2002, Chapter 5), we employ the heuristic now detailed. Given a training sequence qn1 , the algorithm takes T\u03a3 to be Hoffman\u2019s codetree of qn1 (see Cover & Thomas, 1991, chapter 5.6) based on the frequency counts of the symbols in qn1 .\n15. Consider the following example. Let \u03a3 = {a,b,z} and consider a decomposition T\u03a3 with two internal nodes where the binary problem of the root discriminates between {a,b} and {z} (and the other internal node corresponds to distinguishing between a and b). Using the simplistic approach, if we observe the training sequence azaz \u00b7 \u00b7 \u00b7 azazb, we will assign very high probability to z given the context zb, which is not necessarily supported by the training data. 16. We also used this estimator in the protein classification experiment.\nThe time and space complexity of the de-ctw algorithm, based on the efficient implementation of the binary ctw, are O(|\u03a3|Dn) for training and and O(|\u03a3|DT ) for prediction."}, {"heading": "3.5 Probabilistic Suffix Trees (PST)", "text": "The Probabilistic Suffix Tree (pst) prediction algorithm (Ron et al., 1996) attempts to construct the single \u201cbest\u201d D-bounded VMM according to the training sequence. It is assumed that an upper bound D on the Markov order of the \u201ctrue source\u201d is known to the learner.\nA pst over \u03a3 is a non empty rooted tree, where the degree of each node varies between zero (for leaves) and |\u03a3|. Each edge in the tree is associated with a unique symbol in \u03a3. These edge labels define a unique sequence s for each path from a node v to the root. The sequence s labels the node v. Any such pst tree induces a \u201csuffix set\u201d S consisting of the\nlabels of all the nodes. The goal of the pst learning algorithm is to identify a good suffix set S for a pst tree and to assign a probability distribution P\u0302 (\u03c3|s) over \u03a3, for each s \u2208 S. Note that a pst tree may not be a tree source, as defined in Section 3.3. The reason is that the set S is not necessarily proper. We describe the learning phase of the pst algorithm, which can be viewed as a three stage process.\n1. First, the algorithm extracts from the training sequence qn1 a set of candidate contexts and forms a candidate \u201csuffix set\u201d S\u0302. Each contiguous sub-sequence s of qn1 of length \u2264 D, such that the frequency of s in qn1 is larger than a user threshold will be in S\u0302. Note that this construction guarantees that S\u0302 can be accommodated in a (pst) tree (if a context s appears in S\u0302, then all suffixes of s must also be in S\u0302). For each candidate s we associate the conditional (estimated) probability P\u0302 (\u03c3|s), based on a direct maximum likelihood estimate (i.e., a frequency count).\n2. In the second stage, each s in the candidate set S\u0302 is examined using a two-condition test. If the context s passes the test, s and all its suffixes are included in the final pst tree. The test has two conditions that must hold simultaneously:\n(i) The context s is \u201cmeaningful\u201d for some symbol \u03c3; that is, P\u0302 (\u03c3|s) is larger than a user threshold.\n(ii) The context s contributes additional information in predicting \u03c3 relative to its \u201cparent\u201d; if s = \u03c3k\u03c3k\u22121 \u00b7 \u00b7 \u00b7 \u03c31, then its \u201cparent\u201d is its longest suffix s \u2032 =\n\u03c3k\u22121 \u00b7 \u00b7 \u00b7 \u03c31. We require that the ratio P\u0302 (\u03c3|s)\nP\u0302 (\u03c3|s\u2032) be larger than a user threshold r or\nsmaller than 1/r.\nNote that if a context s passes the test there is no need to examine any of its suffixes (which are all added to the tree as well).\n3. In the final stage, the probability distributions associated with the tree nodes (contexts) are smoothed. If P\u0302 (\u03c3|s) = 0, then it is assigned a minimum probability (a user-defined constant) and the resulting conditional distribution is re-normalized.\nAltogether the pst learning algorithm has five user parameters, which should be selected based on the learning problem at hand. The pst learning algorithm has a PAC style performance guarantee ensuring that with high probability the redundancy approaches zero at rate 1/n1/c for some positive integer c.17 An example of the pst tree constructed for the abracadabra training sequence is given in Figure 5.\nThe pst learning phase has a time complexity of O(Dn2) and a space complexity of O(Dn). The time complexity for calculating P\u0302 (\u03c3|s) is O(D)."}, {"heading": "3.6 LZ-MS: An Improved Lempel-Ziv Algorithm", "text": "There are plenty of variations on the classic lz78 compression algorithm (see Bell et al., 1990, for numerous examples). Here we consider a recent variant of the lz78 prediction\n17. Specifically, the result is that, with high probability (\u2265 1\u2212 \u03b4), the Kullback-Leibler divergence between the pst estimated probability distribution and the true underlying (unknown) VMM distribution is not larger than \u03b5 after observing a training sequence whose length is polynomial in 1\n\u03b5 , 1 \u03b4 , D, |\u03a3| and the\nnumber of states in the unknown VMM model.\nalgorithm due to Nisenson et al. (2003). The algorithm has two parameters M and S and, therefore, its acronym here is lz-ms.\nA major advantage of the lz78 algorithm is its speed. This speed is made possible by compromising on the systematic counts of all sub-sequences. For example, in Figure 1, the sub-sequence br is not parsed and, therefore, is not part of the tree. However, this subsequence is significant for calculating P\u0302 (a|br). While for a very long training sequence this compromise will not affect the prediction quality significantly, for short training sequences the lz78 algorithm yields sparse and noisy statistics. Another disadvantage of the lz78 algorithm is its loss of context when calculating P\u0302 (\u03c3|s). For example, in Figure 1, when taking \u03c3 = b and s = raa, the lz78 prediction algorithm will compute P\u0302 (b|raa) by traversing the tree along the raa path (starting from the root) and will end on a leaf. Therefore, P\u0302 (b|raa) = P\u0302 (b|\u01eb) = 5/33. Nevertheless, we could use the suffix a of raa to get the more accurate value P\u0302 (b|a) = 5/17. These are two major deficiencies of lz78. The lz-ms algorithm attempts to overcome both these disadvantages by introducing two corresponding heuristics. This algorithm improves the lz78 predictions by extracting more phrases during learning and by ensuring a minimal context for the next phrase, whenever possible.\nThe first modification is termed input shifting. It is controlled by the S parameter and used to extract more phrases from the training sequence. The training sequence qn1 = q1q2 \u00b7 \u00b7 \u00b7 qn is parsed S+1 times, where in the ith parsing the algorithm learns the sequence qiqi+1 \u00b7 \u00b7 \u00b7 qn using the standard lz78 learning procedure. The newly extracted phrases and counts are combined with the previous ones (using the same tree). Note that by taking\nS = 0 we leave the lz78 learning algorithm intact. For example, the second row of Table 1 illustrates the effect of input shifting with S = 1, which clearly enriches the set of parsed phrases.\nThe second modification is termed back-shift parsing and is controlled by the M parameter. Back-shift parsing attempts to guarantee a minimal context of M symbols when calculating probabilities. In the learning phase the algorithm back-shifts M symbols after each phrase extraction. To prevent the case where more symbols are back-shifted than parsed (which can happen when M > 1), the algorithm requires that back shifting remain within the last parsed phrase. For example, on the third row of Table 1 we see the resulting phrase set extracted from the training sequence using M = 1. Observe that after extracting the first phrase (a) lz-ms back-shifts one symbol, therefore, the next parsed symbol is (again) \u2018a\u2019. This implies that the next extracted phrase is ab. Since lz-ms back-shifts after each phrase extraction we conclude with the resulting extracted phrase set.\nThe back-shift parsing mechanism may improve the calculation of conditional probabilities, which is also modified as follows. Instead of returning to the root after traversing to a leaf, the last M -traversed symbols are first traced down from the root to some node v, and then the new traversal begins from v (if v exists). Take, for example, the calculation of P\u0302 (b|raa) using the tree in Figure 1. If we take M = 0, then we end with P\u0302 (b|raa) = P\u0302 (b|\u01eb) = 5/33; on the other hand, if we take M = 1, we end with P\u0302 (b|raa) = P\u0302 (b|a) = 5/17.\nBoth \u2018input shifting\u2019 and \u2018back-shift parsing\u2019 tend to enhance the statistics we extract from the training sequence. In addition, back-shift parsing tends to enhance the utilization of the extracted statistics. Table 1 shows the phrases parsed from q111 = abracadabra by lz-ms for several values of M and S. The phrases appear in the order of their parsing. Observe that each of these heuristics can introduce a slight bias into the extracted statistics.\nIt is interesting to compare the relative advantage of input shifting and back-shift parsing. In Appendix C we provide such a comparison using a prediction simulation over the Calgary Corpus. The results indicate that back-shift on its own can provide more power to lz78 than input shifting parsing alone. Nevertheless, the utilization of both mechanisms is always better than using either alone.\nThe time complexity of the lz-ms learning algorithm is at mostMS times the complexity of lz78. Prediction using lz-ms can take at most M times the prediction using lz78."}, {"heading": "4. Experimental Setup", "text": "We implemented and tested the six VMM algorithms described in Section 3 using discrete sequences from three domains: text, music and proteins (see more details below). The source code of Java implementations for the six algorithms is available at http://www. cs.technion.ac.il/~ronbeg/vmm. We considered the following prediction setting. The learner is given the first half of a sequence (e.g., a song) and is required to predict the rest. We denote the first half by qn1 = q1 \u00b7 \u00b7 \u00b7 qn and the second, by x n 1 = x1 \u00b7 \u00b7 \u00b7 xn. q n 1 is called the training sequence and xn1 is called the test sequence. During this learning stage the learner constructs a predictor, which is given in terms of a conditional distribution P\u0302 (\u03c3|s). The predictor is then fixed and tested over xn1 . The quality of the predictor is measured via the average log-loss of P\u0302 (\u00b7|\u00b7) with respect to xn1 as given in Equation (1).\n18 For the protein dataset we also considered a different, more natural setting in which the learner is provided with a number of training sequences emanating from some unknown source, representing some domain. The learner is then required to provide predictions for sequences from the same domain.\nDuring training, the learner should utilize the training sequence for learning a probabilistic model, as well as for selecting the best possible hyper-parameters. Each algorithm selects its hyper-parameters from a cross product combination of \u201cfeasible\u201d values, using a cross-validation (CV) scheme over the training sequence. In our experiments we used a five-fold CV. The training sequence qn1 was segmented into five (roughly) equal sized contiguous non-overlapping segments and each fold used one such segment for testing and a concatenation of rest of the segments for training. Before starting the experiments we selected, for each algorithm, a set of \u201cfeasible\u201d values for its vector of hyper-parameters. Each possible vector (from this set) was tested using five-fold CV scheme (applied over the training sequence!) yielding five loss estimates for this vector. The parameter vector with the best median19 (over the five estimates) was selected for training the algorithm over the entire training sequence. See Appendix A for more details on the exact choices of hyperparameters for each of the algorithms. Note that our setting is different from the standard setup used when testing online lossless compression schemes, where the compressor starts processing the test sequence without any prior knowledge. Also, it is usually the case that compression algorithms do not optimize their hyper-parameters (e.g., for text compression ppm-c is usually applied with a fixed D = 5).\nThe sequences we consider belong to three different domains: English text, music pieces and proteins.\n18. Cover and King (1978) considered a very similar prediction game in their well-known work on the estimation of the entropy of English text. 19. We used the median rather than average since the median of a small set of numbers is considerably more robust against outliers; see, e.g., http://standards.nctm.org/document/eexamples/chap6/6.6/\n\u2022 For the English text we chose the well-known \u2018Calgary Corpus\u2019 (Bell et al., 1990), which is traditionally used for benchmarking lossless compression algorithms.20\n\u2022 The music set was assembled from MIDI files of music pieces.21 The musical benchmark was compiled using a variety of well-known pieces of different styles. The styles we included are: classical, jazz and rock/pop. All the pieces we consider are polyphonic (played with several instruments simultaneously). Each MIDI file for a polyphonic piece consists of several channels (usually one channel per instrument). We considered each channel as an individual sequence; see Appendix B for more details.\n\u2022 The protein set includes proteins (amino-acid sequences) from the well-known Structural Classification of Proteins (SCOP) database (Murzin et al., 1995). Here we used all sequences in release 1.63 of SCOP, after eliminating redundancy (i.e., only one copy was retained of sequences that are 100% identical). This set is hierarchically classified into classes according to biological considerations. We relied on this classification for testing classifiers constructed from the VMM algorithms over a number of large classification tasks (see Section 6).\nTable 2 summarizes some statistics of the three benchmark tests. The three datasets can be obtained at http://www.cs.technion.ac.il/~ronbeg/vmm."}, {"heading": "5. Prediction Results", "text": "Here we present the prediction results of the six VMM algorithms for the three domains. The performance is measured via the log-loss as discussed above. Recall that the average log-loss of a predictor (over some sequence) is a slightly optimistic estimate of the compression rate (bits/symbol) that could be achieved by the algorithm over that sequence.\nTable 3 summarizes the results for the textual domain (Calgary Corpus). We see that de-ctw is the overall winner with an average loss of 3.02. The runner-up is ppm-c with an average loss of 3.03. However, there is no statistically significant difference between the\n20. The Calgary Corpus is available at ftp://ftp.cpsc.ucalgary.ca/pub/projects/text.compression.corpus. Note that this corpus contains, in addition to text, also a few binary files and some source code of computer programs, written in a number of programming languages. 21. MIDI (= Musical Instrument Digital Interface) is a standard protocol and language for electronic musical instruments. A standard MIDI file includes instructions on which notes to play and how long and loud to play them.\ntwo, as indicated by standard error of the mean (SEM) values. The worst algorithm is the lz78 with an average loss of 3.76. We see that in most cases de-ctw and ppm-c share the first and second best results, lz-ms is a runner-up in five cases, and lz78 wins one case and is a runner-up in one case. pst is the winner in a single case.\nIt is interesting to compare the loss results of Table 3 to known compression results of (some of) the algorithms for the same corpus, which are 2.14 bps for de-ctw and 2.48 bps for ppm-c (see Volf, 2002; Cleary & Teahan, 1997, respectively). While these results are dramatically better than the average log-loss we obtain for these algorithms, note that the setting is considerably different and in the compression applications the algorithms are continually learning the sequence. A closer inspection of this disparity between the compression results and our training/test results reveals that the difference is in fact not that large. Specifically, the large average log-loss is mainly due to one of the Calgary Corpus files, namely the obj1 file for which the log-loss of ppm-c is 6.75, while the compression results of Cleary and Teahan (1997) are 3.76 bps. Without obj1 file the average log-loss obtained by ppm-c on the Calgary Corpus is 2.81. Moreover, the 2.48 result is obtained on a smaller corpus that does not include the four files paper3 -6. Without these files the average log-loss of ppm-c is 2.65. This small difference may indicate that the involved sequences are not governed by a stationary distribution. Notice also that the compression results indicate that de-ctw is significantly better than ppm-c. However, in our setting these algorithms perform very similarly.\nNext we present the results for the music MIDI files, which are summarized in Table 4. The table has four sections corresponding to classical pieces, jazz, 14 improvisations of the same jazz piece and rock/pop. Individual average losses for these sections are specified as well. de-ctw is the overall winner and ppm-c is the overall runner-up. Both these algorithms are significantly better than the rest. The worst algorithm is, again, lz78.\nSome observations can be drawn about the \u201ccomplexity\u201d of the various pieces by considering the results of the best predictors with respect to the individual sequences. Taking the de-ctw average losses, in the classical music domain we see that the Mozart pieces are the easiest to predict (with an average loss of at most 0.93). The least predictable pieces are Debussy\u2019s Children Corner 6 (1.86) and Beethoven\u2019s Symphony 6(1.78 loss). The most predictable piece overall is the rock piece \u201cYou really got me\u201d by the Kinks. The least predictable rock piece is \u201cSunshine of your love\u201d by Cream. The least predictable piece overall is the jazz piece \u201cSatin doll 2\u201d by Duke Ellington. The most predictable jazz piece is \u201cThe girl from Ipanema\u201d by Jobim. Among the improvisations of \u201cAll of me\u201d, there is considerable variability starting with 0.5 loss (All of me 12) and ending with 1.65 loss (All of me 8). We emphasize that such observations may have a qualitative value due to the great variability among different arrangements of the same musical piece (particularly jazz tunes involving improvisation). The readers are encouraged to listen to these pieces and judge the \u201ccomplexity\u201d of these pieces for themselves. MIDI files of all the pieces are available at http://www.cs.technion.ac.il/~ronbeg/vmm.\nThe prediction results for the protein corpus appear in Table 5. Here ppm-c is the overall winner and de-ctw is the runner-up. The difference between them and the other\nalgorithms is not very large, with the exception of the pst algorithm, which suffered a significantly larger average loss.22\nThe striking observation, however, is that none of the algorithms could beat a trivial prediction based on a (zero-order) \u201cbackground\u201d distribution of amino acid frequencies as measured over a large database of proteins. The entropy of this background distribution is 4.19. Moreover, the entropy of a yet more trivial predictor based on the uniform distribution over an alphabet of size 20 is 4.32 \u2248 log2(20). Thus, not only could the algorithms not outperform these two trivial predictors, they generated predictions that are sometimes considerably worse.\nThese results indicate that the first half of a protein sequence does not contain predictive information about its second half. This is consistent with the general perception that protein chains are not repetitive 23. Rather, usually they are composed of several different elements called domains, each one with own specific function and unique source distribution (Rose,\n22. The failure of pst in this setting could be a result of an inadequate set of possible values for its hyperparameters; see Appendix A for details on the choices made. 23. This is not always true. It is not unusual to observe similar patterns (though not identical) within the same protein. However, this phenomenon is observed for a relatively small number of proteins.\n1979; Lesk & Rose, 1981; Holm & Sander, 1994). These unique distributions are usually more statistically different from each other (in terms of the KL-divergence) than from the background or uniform distribution. Therefore, a model that is trained on the first half of a protein is expected to perform worse than the trivial predictor - which explains the slight increase in the average code length. A similar finding was observed by Bejerano and Yona (2001) where models that were trained over specific protein families coded unrelated non-member proteins using an average code length that was higher than the entropy of the background distribution. This is also supported by other papers that suggest that protein sequences are incompressible (e.g. Nevill-Manning & Witten, 1999)."}, {"heading": "6. Protein Classification", "text": "For protein sequences, a more natural setup than the sequence prediction setting is the classification setting. Biologists classify proteins into relational sets such as \u2018families\u2019, \u2018superfamilies\u2019, \u2018fold families\u2019 and \u2018classes\u2019; these terms were coined by Dayhoff (1976) and Levitt and Chothia (1976). Being able to classify a new protein in its proper group can help to elucidate relationships between novel genes and existing proteins and characterize unknown genes.\nPrediction algorithms can be used for classification using a winner-takes-all (WTA) approach. Consider a training set of sequences belonging to k classes C1, . . . , Ck. For each class Ci we train a sequence predictor P\u0302i. Given a test sequence x we classify it as class c = argmaxi P\u0302i(x).\nWe tested the WTA classifiers derived from the above VMM algorithms over several large protein classification problems. These problems consider the same protein sequences mentioned in the above prediction experiment. For classification we used the partition into subsets given in the SCOP database. Specifically, the SCOP database (Murzin et al., 1995)\nclassifies proteins into a hierarchy of \u2018families\u2019, \u2018superfamilies\u2019, \u2018folds\u2019 and \u2018classes\u2019 where each \u2018class\u2019 is composed of several \u2018folds\u2019, each \u2018fold\u2019 is composed of several \u2018superfamilies\u2019 and each \u2018superfamily\u2019 is composed of several \u2018families\u2019. This classification was done manually based on both sequence and structural similarities, resulting in seven major \u2018classes\u2019 (at the top level), 765 \u2018folds\u2019,24 1231 \u2018superfamilies\u2019 and 2163 \u2018families\u2019 (release 1.63).\nThe seven major SCOP classes are quite distinct and can be easily distinguished (Kumarevel et al., 2000; Jin et al., 2003). Here we focused on the next level in this hierarchy, the \u2018fold\u2019 level. Our task was to classify proteins, that belong to the same \u2018class\u2019, into their unique \u2018fold\u2019. Since there are seven \u2018classes\u2019 in the SCOP database, we have seven independent classification problems. The number of \u2018folds\u2019 (i.e., machine learning classes) in each \u2018class\u2019 appear in Table 6. For example, the protein \u2018class\u2019 D contains 111 \u2018folds\u2019. The classification problem corresponding to D is, therefore, a multi-class problem with 111 (machine learning) classes. Consider one (of the seven) classification problems. For each \u2018fold\u2019, a statistical model was trained from a training subset of sequences that belong to that \u2018fold\u2019. Thus, the number of models we learn is the number of \u2018folds\u2019 (111 in the D \u2019Class\u2019). The resulting classifier is a WTA combination of all these models. The test set for this classifier contains all the proteins from the same SCOP \u2018class\u2019 that do not appear in any training set.\nOf the 765 \u2018folds\u2019, we considered only \u2018folds\u2019 with at least 10 members. VMM models were trained (one for each \u2018fold\u2019) and tested. The test results are grouped and averaged per SCOP \u2018class\u2019. The number of \u2018folds\u2019 in these classes varies between 12-111 (with an average of 58 \u2019folds\u2019).\nAs before, the experiment was repeated five times using five-fold CV. We measured the performance using the average over the zero-one loss (zero loss for a correct classification). Within each fold we fine-tuned the hyper-parameters of the algorithm using the same \u201cinternal\u201d five-fold cross validation methodology as described in Section 4.\nIn this setup we used a slightly different pst variant (denoted as pst\u2217) based on (Bejerano & Yona, 2001).25 The pst\u2217 variant is basically a standard pst with the following two modifications: (i) A context is chosen to be a candidate according to the number of times it appears in different \u2018fold\u2019 sequences; (ii) A context s is added to the tree if some symbol appears after s at least some predefined number of times.\nWe present the classification-experiment results in Table 6. The consistent winner here is the lz-ms algorithm, which came first in all of the classification problems, differing substantially from the runner-ups, de-ctw and ppm-c.\nIn general, all the VMM algorithms\u2019 classifications had a good average success rate (1\u2212 error) that ranges between 76% (for pst\u2217) to 86% (for lz-ms). The latter result is quite surprising, considering that the class definition is based on structural properties that are not easily recognizable from the sequence. In fact, existing methods of sequence comparison usually fail to identify relationships between sequences that belong to different superfamilies\n24. Note the difference between the SCOP \u2019classes\u2019 and the term class we use in classification problems; similarly, note the difference between the SCOP \u2018folds\u2019 (that are subsets of the SCOP \u2018classes\u2019) from the folds we use in cross-validation. Throughout our discussion, the biological \u2018classes\u2019 and \u2018folds\u2019 are cited. 25. The results of the original pst algorithm were poor (probably due to an inadequate set of values for the hyper-parameters). Recall that the pst\u2217 (and all other algorithms) hyper-parameter values were chosen before starting the experiments, following the description given in Section 4.\nwithin the same fold. The success rate of the lz-ms algorithm in this task suggests that the algorithm might be very effective in other protein classification tasks.\nThe success of the algorithms in the classification setup contrasts with their poor protein prediction rates (given in Table 5). To investigate this phenomenon, we conducted another protein prediction experiment based on the classification setup. In each fold in this experiment (among the five-CV), a predictor P\u0302i was trained using the concatenation of all training sequences in class Ci and was tested against each individual sequence in the test of class Ci. The results of this prediction experiment are given in Table 7. Here we can see that all of the algorithms yield significantly smaller losses than the loss of the trivial protein \u201cbackground\u201d distribution. Here again, ppm and de-ctw are favorites with significant statistical difference between them."}, {"heading": "7. Related work", "text": "In this section we briefly discuss some results that are related to the present work. We restrict ourselves to comparative studies that include algorithms we consider here and to discussions of some recent extensions of these algorithms. We also discuss some results concerning compression and prediction of proteins.\nThe ppm idea and its overwhelming success in benchmark tests has attracted considerable attention and numerous ppm variants have been proposed and studied. ppm-d (Howard, 1993) is very similar to to ppm-c, with a slight modification of the escape mechanism; see Section 3.2. The ppm\u2217 variant eliminates the need to specify a maximal order (Cleary & Teahan, 1997). For predicting the next symbol\u2019s probability, ppm\u2217 uses \u201cdeterministic\u201d contexts, contexts that give probability one to some symbols. If no such context exists, then ppm\u2217 use the longest possible context (i.e., the original ppm strategy). One of the most successful ppm variants over the Calgary Corpus is the ppm-z variant of Bloom\n(1998). The ppm-z variant introduces two improvements to the ppm algorithm: (i) estimating the best model order, for each encoded symbol; (ii) adding a dynamic escape estimation for different contexts. To date, the most successful ppm variant is the \u2018complicated PPM with information-inheritance\u2019 (ppm-ii) of Shkarin (2002), which achieves a compression rate of 2.041. Currently, this is the lowest published rate over the Calgary Corpus. The ppm-ii algorithm relies on an improved escape mechanism similar to the one used by ppm-z together with a different counting heuristic that aims to reduce the statistical sparseness that occurs in large Markovian contexts.\nTraditionally, the comparison between ppm variants (and other lossless compression algorithms) is over the Calgary Corpus. Other corpora for lossless compression were proposed and are available. Two examples are the Canterbury Corpus and the \u2018Large Canterbury Corpus\u2019 (Arnold & Bell, 1997) and the Silesia Compression Corpus (Deorowicz, 2003), which contains significantly larger files than both the Calgary and Canterbury corpora.26\nBunton (1997) provides a (Calgary Corpus) comparison between ppm-c, ppm-d and ppm\u2217 (with its \u2018C\u2019 and \u2018D\u2019 versions). The average compression rates achieved by these algorithm are: 2.417, 2.448, 2.339 and 2.374, respectively, with a fixed order bound D = 9 for ppm-c and ppm-d. According to Bloom27, his latest version of ppm-z achieves an average rate of 2.086. Bunton also proposes various improvements for the ppm schemes. Her best improved ppm achieves a 2.177 rate.28\nThe general-purpose ppm scheme turns out to be rather flexible and allows adaptations to particular domains. For example, Nevill-Manning and Witten (1999) achieve a slightly better rate than the trivial (log2(20)) one for proteins, using a specialized variant of ppm-d\n26. These corpora are available at http://corpus.canterbury.ac.nz/ and http://sun.iinf.polsl. gliwice.pl/~sdeor/corpus.htm, respectively. 27. See http://www.cbloom.com/src/ppmz.html 28. At the time (1996) this variant was probably the most successful ppm version.\nthat uses prior amino-acids frequencies to enhance the estimation. By combining domain knowledge into ppm using preprocessing and ppm that predicts over dual-alphabets, Drinic and Kirovski (2002) develop a specialized algorithm for compressing computer executable files (.exe). A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001).\nContext Tree Weighting (ctw), with its proven redundancy bound, has also attracted much attention. However, unlike the ppm approach, there are not as many empirical studies of this algorithm. A possible reason is that the original binary ctw (and straightforward extensions to larger alphabets) did not achieve the best lossless compression results (Tjalkens et al., 1997). The work of Volf, which extends the binary ctw to general alphabets, resulting in the de-ctw algorithm (see Section 3.4), showed that ctw can achieve excellent compression rates. Since then, the ctw scheme has been extended in several directions. Perhaps the most important extension is a modified ctw scheme that does not require the maximal order hyper-parameter (D). The redundancy of this modified algorithm is bounded in a similar manner as the original version. This more recent ctw variant is thus universal with respect to the class of stationary and ergodic Markov sources of finite order.\nSpecialized ctw variants were proposed for various domains. For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA.\nWe are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster. Finally, Kermorvant and Dupont (2002) propose to improve the smoothing mechanism used in the original pst algorithm. Experiments with a protein problem indicate that the proposed smoothing method can help.\nAs mentioned earlier, there are many extensions of the lz78 algorithm. See (Nelson & Gailly, 1996) for more details. It is interesting to note that Volf considered an \u201censemble\u201d consisting of both de-ctw and ppm. He shows that the resulting algorithm performs nearly as well as the ppm-z on the Calgary Corpus while beating the ppm-z on the Canterbury Corpus.\nPrevious studies on protein classification used a variety of methods, including generative models such as HMMs (Sonnhammer et al., 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al., 2004). Jaakkola et al. (1999) describe a model that combines the SVM discriminative model with a generative model (the HMM-based Fischer kernel). These methods can be quite effective for protein classification at the family level but they are often expensive to train and require some manual intervention to obtain optimal results (e.g., special input\npre-processing). Other studies used unsupervised learning techniques, such as clustering (Yona et al., 1999; Tatusov et al., 1997) and self-organizing maps (Agrafiotis, 1997). While these methods can be applied successfully to broader data sets, they are not as accurate as the supervised-learning algorithms mentioned above.\nOf all models, the HMM model is perhaps the most popular and successful model for protein classification to-date (Park et al., 1998). The specific class of HMMs that are used to model protein families (bio-HMM) has a special architecture that resembles the one used in speech-recognition. These models are first-order Markov models. The methods tested in this paper belong to the general class of HMMs. However, unlike the first-order bio-HMMs, the family of models tested here is capable of modeling higher order sources. Moreover, in many cases these models obtain comparable performance to bio-HMMs (see the work of Bejerano & Yona, 2001) and are easier to train than bio-HMMs, which makes them an appealing alternative.\nIt should be noted that the classification task that we attempted here has a different setting than the typical setting in other studies on protein classification. In most studies, the classification focuses on the family level and a model is trained for each family. Here we focused on the structural fold-family level. This is a more difficult task and it has been the subject of strong interest in fold prediction tasks (CASP, 2002). Predicting the structural fold of a protein sequence is the first substantial step toward predicting the threedimensional structure of a protein, which is considered one of the most difficult problems in computational biology.\nThe success rate of the VMM algorithms, and specifically the lz-ms algorithm, is especially interesting since the sequences that make up a fold-family belong to different protein families with different properties. These families manifest different variations over the same structural fold; however, detecting these similarities has been proved to be a difficult problem (Yona & Levitt, 2002), and existing methods that are based on pure sequence information achieve only moderate success in this task.\nAlthough we have not tested bio-HMMs, we suspect that such a model would be only moderately successful, since a single model architecture is incapable of describing the diverge set of protein families that make up a fold-family. In contrast, the VMM models tested here are capable of transcending the boundaries between different protein families since they do not limit themselves to a specific architecture. These models have very few hyper-parameters and they can be quite easily trained from the data without further pre-processing. Other studies that addressed a somewhat similar problem to ours used sequence comparison algorithms (McGuffin et al., 2001) or SVMs and Neural Networks (Ding & Dubchak, 2001) reporting a maximal accuracy of 56%. It is hard to compare our results to the results reported in these studies since they used different data sets and a different assessment methodology. However, the remarkably high success rate of the lz-ms model in this task suggests that this model can be very effective for structural-fold prediction."}, {"heading": "8. Concluding Remarks and Open Questions", "text": "In this paper we studied the empirical performance of a number of prominent prediction algorithms. We focused on prediction settings that are more closely related to those required\nby machine learning practitioners dealing with discrete sequences. Previous results related to these algorithms usually focused on a standard compression setting.\nWhile it should not be expected that one algorithm will consistently outperform others on all tasks, our rather extensive empirical evaluations over the three domains indicate that there are prominent algorithms, which consistently tend to generate more accurate predictions than the other algorithms we examine. These algorithms are the \u2018prediction by partial match\u2019 (ppm-c) and \u2018decomposed context tree weighting\u2019 (de-ctw). For classification of proteins we observe that there is also a consistent winner. However, somewhat surprisingly, the best predictor under the log-loss is not the best classifier. On the contrary, the consistently best protein classifier is based on the mediocre lz-ms predictor! This algorithm is a simple modification of the well-known Lempel-Ziv-78 (lz78) prediction algorithm, which can capture VMMs with large contexts. The surprisingly good classification accuracy achieved by this algorithm may be of independent interest to protein analysis research and clearly deserves further investigation.\nThis relative success of lz-ms is related to the winner-takes-all classification scheme we use. A classifier based on this approach can suffer from a degraded performance if only one or a few of the models generated (for some of the classes) are wrongly fitted to other classes (in which case the erroneous model is the winner). Clearly, the superior performance of lz-ms is related to its robustness in this regard and a closer inspection of the behavior of the algorithms may be revealing. A possible explanation of this robustness could be related to the unbounded memory length of the lz-ms model. In most of the other VMM models the memory length is a hyper-parameter. However, as discussed in Section 7, some of the other algorithms such as ppm can be extended to work without a known bound on the context length.\nAn important observation, seen in these results, is that one cannot rely on log-loss (compression) performance when selecting a VMM algorithm for classification (using a WTA approach). Since the classification of sequences has not been studied as extensively as compression and prediction, we believe that much can be gained by focusing on specialized VMM algorithms for classification.\nWe hope that our contribution will assist practitioners in selecting suitable prediction algorithms for prediction and classification tasks and we also provide the code of all the algorithms we considered. Finally, we conclude by a number of open questions and directions for future research.\nOne of the most successful general-purpose predictors we examined is the de-ctw algorithm, with its alphabet decomposition mechanism, as suggested by Volf (2002). As it turns out, this mechanism is crucial for the success of the the ctw scheme. However, currently there are no redundancy bounds (or other type of performance guarantees) for this decomposition. It would be interesting to prove such a bound, which will also motivate an optimality criterion for the decomposition-tree (see Section 3.4). Note that the heuristic proposed by Volf for generating the decomposition-tree relies on Huffman\u2019s coding, but we are not familiar with a compelling reason for the success of this approach.\nSimilarly, the ppm scheme (and the ppm-c variant we examined) do not have any known bounds on its log-loss redundancy. Clearly, such bounds could significantly contribute to understanding this algorithm (and its variants) and may help in designing stronger versions of this scheme.\nOne of the main factors that influence the success of the ppm algorithm is the details of its escape mechanism, including the allocation of probability mass for the \u2018escape\u2019 event. This probability allocation is, in essence, a solution to the zero frequency problem (also called \u201cmissing mass\u201d problem). So far, there are no formal justifications for the performance of the more successful escape implementations. It would be interesting to see if some of the recent results on the missing mass problem, such as those presented by Orlitsky et al. (2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations.\nWe can view the ctw algorithm as an ensemble of numerous VMMs. However, the weights of the various VMM models are fixed. Various ensemble methods in machine learning such as boosting and online expert advice algorithms attempt to optimize the weights of the various ensemble members. Could we achieve better performance guarantees and better empirical performance using adaptive weight learning approaches? Volf (2002) has some results (on combining lz78 and de-ctw) indicating that this is a promising direction. Other possibilities would be to employ portfolio selection algorithms to dynamically change the weights of several models, along the lines suggested by Kalai et al. (1999)."}, {"heading": "9. Acknowledgments", "text": "We are very grateful to Gil Bejerano, Charles Bloom and Paul Volf who provided valuable advice and assistance. We would also like to thank Bob Carpenter for his PPMC compression java implementation, Moti Nisenson for his LZms java implementation and Adiel Ben Shalom for his MIDI assistance. Finally, we thank the anonymous referees for their good comments. The work of Ran El-Yaniv was partially supported by the Technion V.P.R. fund for the promotion of sponsored research and the partial support of the PASCAL network of excellence."}, {"heading": "Appendix A. Algorithm Hyper-Parameters Selection", "text": "In our experimental protocol each algorithm optimizes its hyper-parameters during the training phase. The best found set of parameters is then used for training (see Section 4 for details). This appendix specifies the possible hyper-parameter values that were considered for each algorithm for both the prediction and classification experiments. These values were selected (based on commonsense) before the start of any experiment. We also provide here, for each algorithm, the average values that were actually selected.\nWe present these hyper-parameter values in two tables: Table 8 for the prediction setup, and Table 9, for the protein classification setup. We used two different hyper-parameter sets for these two setups due to memory complexity issues. Specifically, in the classification setup, as described in Section 6, the length of training sequences is significantly larger than the length of training sequences in the prediction experiment of Section 4. For example, in the protein classification, class \u2018C\u2019 consists of 4303 sequences from 78 different \u2018folds\u2019 and the average size of a sequence in class \u2018C\u2019 is 265. Therefore, the size of the training sequence for this class is 912, 236. In comparison, the maximal training sequence length in the text prediction experiment is 7853962 = 392, 698. Also, recall that in each classification\nexperiment, the number of constructed models is the number of \u2018folds\u2019 (e.g., 78 in class \u2018C\u2019), which also increases the memory overhead.\nThe requirement to accommodate long training sequences (in the classification experiment) directly influences the space used by most of the algorithms. The lengths of training sequences described above made it impossible for us to complete the protein classification experiment using the set of feasible parameters as described in Table 8. Therefore, for classification we reduced the set of possible hyper-parmeter and, as can be seen, each parameter set in Table 9 is a subset of the corresponding set in Table 8. Finally, note that this table refers to a slight variant of the standard pst, denoted by pst\u2217, which was used in the protein classification experiment (see Section 6).\nIn Tables 8 and 9 we also present the average (overall runs) of the hyper-parameter values selected by the cross-validated optimization. For example, in Table 8 it is interesting to see that different values were selected for the lz-ms algorithm, for different datasets. For protein sequence predictions lz-ms is optimized with values that make it almost identical to the lz78 algorithm (i.e. M = 0 and S = 0), while on the music predictions these parameters are optimized with significantly larger values (M = 6.12 and S = 16.11). Observe that the average order selected for ppm-c on \u2018text\u2019 data is 9.25. When originally introduced, ppm algorithms were applied with a hyper-parameter D = 4 (constrained by the memory available on computers at the time). Since then, it has often been used with a somewhat larger value, typically 5 or 6 and it was observed that performance begins to deteriorate when D is increased beyond this. For example, see Cleary and Teahan (1997, Figure 2) for a drawing of compression vs. D for book1. We observe very similar behavior when considering the English text files alone (not shown in our table). For example, the value selected the hyper-parameter D in most text files (including book1 ) is 5.\nThe pst hyper-parameters presented in Tables 8 and 9 corresponds to the pst algorithm description in Section 3.5. Specifically, the Pmin (alternatively, hits) hyper-parameter is the threshold used for filtering \u201csuffix set\u201d candidates, \u03b1 and \u03b3 define the first condition of the pst second stage, r (alternatively, Nmin) defines the second condition of pst\u2019s second stage and D defines the maximal order of the pst \u201csuffix set\u201d."}, {"heading": "Appendix B. Music Representation", "text": "Our music data consists of polyphonic music pieces given as MIDI files. A polyphonic music piece is usually represented in a MIDI file as a collection of channels. Each channel is typically associated with a single musical instrument and includes instructions (called \u201cevents\u201d) regarding which notes to play and how long and loud to play them. We treat each channel as an individual sequence. For example, if a piece is played by five instruments, encoded in five MIDI channels, we generated five individual sequences, one for each channel. Note that each entry in Table 4 is an average of all channels of the corresponding piece.\nWe now describe how we represent each MIDI channel. Each note in a channel is represented as a triple: pitch:volume:duration. Both pitch and volume have, according to the MIDI protocol, 128 possible values. Duration is measured in milliseconds. We also include a special note, which we call a \u2018silence note\u2019 and allow for negative durations of this silence note. Such negative assignments are in fact negative time intervals, which make it possible to represent chords or other simultaneous melodic lines within the same channel.\nThe sequences corresponding to a channel are the ascii representation of the sequence of pitch:volume:duration. For example, the note 102:83:4022 is represented as \u201c102:83:4022:\u201d.29 Clearly, this representation is over an alphabet \u03a3 = {0, 1, . . . , 9, :,\u2212}, of size is 12. It is important to note that our representation preserves much of the information encoded in the original MIDI representation and, in reality, allows one to almost exactly reconstruct the original tunes."}, {"heading": "Appendix C. On the LZ-MS Hyper-Parameters", "text": "This appendix provides an indication on the effectiveness of the M and S hyper-parameters of the lz-ms algorithm. The experimental setup is identical to the prediction setup described in Section 4. Table 10 shows the log-loss results over the Calgary Corpus of applications of\n29. The first ten notes of Chopin\u2019s Etude 12 Op. 10 are: \u201c83:120:240: 128:0:-240: 79:120:240: 128:0:-240: 77:120:240: 128:0:-240: 74:120:240: 128:0:-240: 71:120:240:\n128:0:-180:\u201d.\nthe lz-ms algorithm: in the second column only the M parameter is enabled (and S = 0); and the third column provides the results when only the S parameter is enabled (and M = 0). The losses in the last column correspond to applications where both M and S are enabled (this column is identical to the lz-ms results in Table 3). It is evident that the best choice of M alone is consistently better than the best choice of S alone. However, the best combination of both M and S is consistently better than any of these parameters alone."}], "references": [{"title": "On the computational complexity of approximating probability distributions by probabilistic automata", "author": ["N. Abe", "M. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Abe and Warmuth,? \\Q1992\\E", "shortCiteRegEx": "Abe and Warmuth", "year": 1992}, {"title": "Text compression by context tree weighting", "author": ["J. Aberg", "Y. Shtarkov"], "venue": "In Proceedings Data Compression Conference (DCC),", "citeRegEx": "Aberg and Shtarkov,? \\Q1997\\E", "shortCiteRegEx": "Aberg and Shtarkov", "year": 1997}, {"title": "A new method for analyzing protein sequence relationships based on sammon maps", "author": ["D. Agrafiotis"], "venue": "Protein Science, 6, 287\u2013293.", "citeRegEx": "Agrafiotis,? 1997", "shortCiteRegEx": "Agrafiotis", "year": 1997}, {"title": "A corpus for the evaluation of lossless compression algorithms", "author": ["R. Arnold", "T. Bell"], "venue": "In Designs, Codes and Cryptography,", "citeRegEx": "Arnold and Bell,? \\Q1997\\E", "shortCiteRegEx": "Arnold and Bell", "year": 1997}, {"title": "Texture mixing and texture movie synthesis using statistical learning", "author": ["Z. Bar-Joseph", "R. El-Yaniv", "D. Lischinski", "M. Werman"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "Bar.Joseph et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bar.Joseph et al\\.", "year": 2001}, {"title": "Variations on probabilistic suffix trees: Statistical modeling and the prediction of protein", "author": ["G. Bejerano", "G. Yona"], "venue": "families.. Bioinformatics,", "citeRegEx": "Bejerano and Yona,? \\Q2001\\E", "shortCiteRegEx": "Bejerano and Yona", "year": 2001}, {"title": "Solving problems of context modeling", "author": ["C. Bloom"], "venue": "http://www.cbloom.com/papers/index.html.", "citeRegEx": "Bloom,? 1998", "shortCiteRegEx": "Bloom", "year": 1998}, {"title": "Semantically motivated improvements for PPM variants", "author": ["S. Bunton"], "venue": "The Computer Journal, 40 (2/3), 76\u201392.", "citeRegEx": "Bunton,? 1997", "shortCiteRegEx": "Bunton", "year": 1997}, {"title": "A block-sorting lossless data compression algorithm", "author": ["M. Burrows", "D.J. Wheeler"], "venue": "Tech. rep. 124,", "citeRegEx": "Burrows and Wheeler,? \\Q1994\\E", "shortCiteRegEx": "Burrows and Wheeler", "year": 1994}, {"title": "Protein structure prediction center", "author": ["CASP"], "venue": "http://predictioncenter.llnl.gov/.", "citeRegEx": "CASP,? 2002", "shortCiteRegEx": "CASP", "year": 2002}, {"title": "Compressing XML with multiplexed hierarchical PPM models", "author": ["J. Cheney"], "venue": "Data Compression Conference, pp. 163\u2013172.", "citeRegEx": "Cheney,? 2001", "shortCiteRegEx": "Cheney", "year": 2001}, {"title": "Predicting daily behavior via wearable sensors", "author": ["B. Clarkson", "A. Pentland"], "venue": "Tech. rep. Vismod TR#540,", "citeRegEx": "Clarkson and Pentland,? \\Q2001\\E", "shortCiteRegEx": "Clarkson and Pentland", "year": 2001}, {"title": "Unbounded length contexts for PPM", "author": ["J. Cleary", "W. Teahan"], "venue": "Computer Journal,", "citeRegEx": "Cleary and Teahan,? \\Q1997\\E", "shortCiteRegEx": "Cleary and Teahan", "year": 1997}, {"title": "Data compression using adaptive coding and partial string matching", "author": ["J. Cleary", "I. Witten"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Cleary and Witten,? \\Q1984\\E", "shortCiteRegEx": "Cleary and Witten", "year": 1984}, {"title": "A convergent gambling estimate of the entropy of English", "author": ["T. Cover", "R. King"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and King,? \\Q1978\\E", "shortCiteRegEx": "Cover and King", "year": 1978}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q1991\\E", "shortCiteRegEx": "Cover and Thomas", "year": 1991}, {"title": "The origin and evolution of protein superfamilies", "author": ["M. Dayhoff"], "venue": "Federation Proceedings, pp. 2132\u20132138.", "citeRegEx": "Dayhoff,? 1976", "shortCiteRegEx": "Dayhoff", "year": 1976}, {"title": "Universal lossless data compression algorithms", "author": ["S. Deorowicz"], "venue": "Ph.D. thesis, Silesian University of Technology.", "citeRegEx": "Deorowicz,? 2003", "shortCiteRegEx": "Deorowicz", "year": 2003}, {"title": "Multi-class protein fold recognition using support vector machines and neural", "author": ["C. Ding", "I. Dubchak"], "venue": "networks. Bioinformatics,", "citeRegEx": "Ding and Dubchak,? \\Q2001\\E", "shortCiteRegEx": "Ding and Dubchak", "year": 2001}, {"title": "PPMexe: PPM for compressing software", "author": ["M. Drinic", "D. Kirovski"], "venue": "In Data Compression Conference,", "citeRegEx": "Drinic and Kirovski,? \\Q2002\\E", "shortCiteRegEx": "Drinic and Kirovski", "year": 2002}, {"title": "Using machine-learning methods for musical style modeling", "author": ["S. Dubnov", "G. Assayag", "O. Lartillot", "G. Bejerano"], "venue": "IEEE Computer,", "citeRegEx": "Dubnov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dubnov et al\\.", "year": 2003}, {"title": "Relations between entropy and error probability", "author": ["M. Feder", "N. Merhav"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Feder and Merhav,? \\Q1994\\E", "shortCiteRegEx": "Feder and Merhav", "year": 1994}, {"title": "Parser for protein folding", "author": ["L. Holm", "C. Sander"], "venue": "units. Proteins,", "citeRegEx": "Holm and Sander,? \\Q1994\\E", "shortCiteRegEx": "Holm and Sander", "year": 1994}, {"title": "The design and analysis of efficient lossless data compression systems", "author": ["P. Howard"], "venue": "Ph.D. thesis, Brown University.", "citeRegEx": "Howard,? 1993", "shortCiteRegEx": "Howard", "year": 1993}, {"title": "Using the Fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Prediction of protein structural classes by a new measure of information discrepancy", "author": ["L. Jin", "W. Fang", "H. Tang"], "venue": "Computational Biology and Chemistry,", "citeRegEx": "Jin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2003}, {"title": "On-line algorithms for combining language models", "author": ["A. Kalai", "S. Chen", "A. Blum", "R. Rosenfeld"], "venue": "In Proceedings of the International Conference on Accoustics,", "citeRegEx": "Kalai et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 1999}, {"title": "Improved smoothing for probabilistic suffix trees seen as variable order Markov chains", "author": ["C. Kermorvant", "P. Dupont"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "Kermorvant and Dupont,? \\Q2002\\E", "shortCiteRegEx": "Kermorvant and Dupont", "year": 2002}, {"title": "The performance of universal encoding", "author": ["R. Krichevsky", "V. Trofimov"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Krichevsky and Trofimov,? \\Q1981\\E", "shortCiteRegEx": "Krichevsky and Trofimov", "year": 1981}, {"title": "Structural class prediction: An application of residue distribution along the sequence", "author": ["T. Kumarevel", "M. Gromiha", "M. Ponnuswamy"], "venue": "Biophys Chem.,", "citeRegEx": "Kumarevel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kumarevel et al\\.", "year": 2000}, {"title": "A note on the Ziv-Lempel model for compressing individual sequences", "author": ["G. Langdon"], "venue": "IEEE Transactions on Information Theory, 29, 284\u2013 287.", "citeRegEx": "Langdon,? 1983", "shortCiteRegEx": "Langdon", "year": 1983}, {"title": "Folding units in globular proteins", "author": ["A. Lesk", "G. Rose"], "venue": "In Proceedings of the National Academy of Sciences,", "citeRegEx": "Lesk and Rose,? \\Q1981\\E", "shortCiteRegEx": "Lesk and Rose", "year": 1981}, {"title": "Mismatch string kernels for discriminative protein", "author": ["C. Leslie", "E. Eskin", "A. Cohen", "J. Weston", "W. Noble"], "venue": "classification. Bioinformatics,", "citeRegEx": "Leslie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2004}, {"title": "Structural patterns in globular proteins", "author": ["M. Levitt", "C. Chothia"], "venue": null, "citeRegEx": "Levitt and Chothia,? \\Q1976\\E", "shortCiteRegEx": "Levitt and Chothia", "year": 1976}, {"title": "An analysis of the burrows-wheeler transform", "author": ["G. Manzini"], "venue": "Journal of the ACM, 48 (3), 407\u2013430.", "citeRegEx": "Manzini,? 2001", "shortCiteRegEx": "Manzini", "year": 2001}, {"title": "Concentration inequalities for the missing mass and for histogram rule error", "author": ["D. McAllester", "L. Ortiz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "McAllester and Ortiz,? \\Q2003\\E", "shortCiteRegEx": "McAllester and Ortiz", "year": 2003}, {"title": "Maximum entropy Markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "In Proc. 17th International Conf. on Machine Learning,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "What are the baselines for protein fold recognition", "author": ["L. McGuffin", "K. Bryson", "D. Jones"], "venue": null, "citeRegEx": "McGuffin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGuffin et al\\.", "year": 2001}, {"title": "A strong version of the redundancy-capacity theorem of universal coding", "author": ["N. Merhav", "M. Feder"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Merhav and Feder,? \\Q1995\\E", "shortCiteRegEx": "Merhav and Feder", "year": 1995}, {"title": "On loss functions which minimize to conditional expected values and posterior probabilities", "author": ["J. Miller", "R. Goodman", "P. Smyth"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Implementing the PPM data compression scheme", "author": ["A. Moffat"], "venue": "IEEE Transactions on Communications, 38 (11), 1917\u20131921.", "citeRegEx": "Moffat,? 1990", "shortCiteRegEx": "Moffat", "year": 1990}, {"title": "SCOP: A structural classification of proteins database for the investigation of sequences and structures", "author": ["A. Murzin", "S. Brenner", "T. Hubbard", "C. Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Murzin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Murzin et al\\.", "year": 1995}, {"title": "The Data Compression Book (2nd ed.). MIS:Press", "author": ["M. Nelson", "J. Gailly"], "venue": null, "citeRegEx": "Nelson and Gailly,? \\Q1996\\E", "shortCiteRegEx": "Nelson and Gailly", "year": 1996}, {"title": "Protein is incompressible", "author": ["C. Nevill-Manning", "I. Witten"], "venue": "In Data Compression Conference,", "citeRegEx": "Nevill.Manning and Witten,? \\Q1999\\E", "shortCiteRegEx": "Nevill.Manning and Witten", "year": 1999}, {"title": "Towards behaviometric security systems: Learning to identify a typist", "author": ["M. Nisenson", "I. Yariv", "R. El-Yaniv", "R. Meir"], "venue": "In The 7th European Conference on Principles and Practice of Knowledge Discovery in Databases", "citeRegEx": "Nisenson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nisenson et al\\.", "year": 2003}, {"title": "Always Good Turing: Asymptotically optimal probability estimation", "author": ["A. Orlitsky", "N. Santhanam", "J. Zhang"], "venue": "Science,", "citeRegEx": "Orlitsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Orlitsky et al\\.", "year": 2003}, {"title": "Playing with virtual musicians: The continuator in practice", "author": ["F. Pachet"], "venue": "IEEE MultiMedia, 9 (3), 77\u201382.", "citeRegEx": "Pachet,? 2002", "shortCiteRegEx": "Pachet", "year": 2002}, {"title": "Sequence comparisons using multiple sequences detect three times as many remote homologues as pairwise methods", "author": ["J. Park", "K. Karplus", "C. Barrett", "R. Hughey", "D. Haussler", "T. Hubbard", "C. Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Park et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Park et al\\.", "year": 1998}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, 77 (3), 257\u2013286.", "citeRegEx": "Rabiner,? 1989", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Fundamentals of Speech Recognition", "author": ["L. Rabiner", "B. Juang"], "venue": null, "citeRegEx": "Rabiner and Juang,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1993}, {"title": "A universal data compression system", "author": ["J. Rissanen"], "venue": "IEEE Transactions on Information Theory, 29 (5), 656\u2013664.", "citeRegEx": "Rissanen,? 1983", "shortCiteRegEx": "Rissanen", "year": 1983}, {"title": "Universal coding, information, prediction, and estimation", "author": ["J. Rissanen"], "venue": "IEEE Transactions on Information Theory, 30 (4), 629\u2013636.", "citeRegEx": "Rissanen,? 1984", "shortCiteRegEx": "Rissanen", "year": 1984}, {"title": "The power of amnesia: Learning probabilistic automata with variable memory length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Ron et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1996}, {"title": "Hierarchic organization of domains in globular proteins", "author": ["G. Rose"], "venue": "Journal of Molecular Biology, 134, 447\u2013470.", "citeRegEx": "Rose,? 1979", "shortCiteRegEx": "Rose", "year": 1979}, {"title": "Implementing the context tree weighting method for text compression", "author": ["K. Sadakane", "T. Okazaki", "H. Imai"], "venue": "In Data Compression Conference,", "citeRegEx": "Sadakane et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sadakane et al\\.", "year": 2000}, {"title": "Redundancy of the Lempel-Ziv incremental parsing rule", "author": ["S. Savari"], "venue": "IEEE Transactions on Information Theory, 43, 9\u201321.", "citeRegEx": "Savari,? 1997", "shortCiteRegEx": "Savari", "year": 1997}, {"title": "Part-of-speech tagging using a variable memory Markov model", "author": ["H. Sch\u00fctze", "Y. Singer"], "venue": "In Proceedings of the 32nd Conference on Association for Computational Linguistics,", "citeRegEx": "Sch\u00fctze and Singer,? \\Q1994\\E", "shortCiteRegEx": "Sch\u00fctze and Singer", "year": 1994}, {"title": "PPM: One step to practicality", "author": ["D. Shkarin"], "venue": "Data Compression Conference, pp. 202\u2013212.", "citeRegEx": "Shkarin,? 2002", "shortCiteRegEx": "Shkarin", "year": 2002}, {"title": "Adaptive mixtures of probabilistic transducers", "author": ["Y. Singer"], "venue": "Neural Computation, 9 (8), 1711\u2013 1733.", "citeRegEx": "Singer,? 1997", "shortCiteRegEx": "Singer", "year": 1997}, {"title": "Dynamical encoding of cursive handwriting", "author": ["Y. Singer", "N. Tishby"], "venue": "Biological Cybernetics,", "citeRegEx": "Singer and Tishby,? \\Q1994\\E", "shortCiteRegEx": "Singer and Tishby", "year": 1994}, {"title": "Pfam: A comprehensive database of protein domain families based on seed", "author": ["E. Sonnhammer", "S. Eddy", "R. Durbin"], "venue": "alignments. Proteins,", "citeRegEx": "Sonnhammer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sonnhammer et al\\.", "year": 1997}, {"title": "A genomic perspective on protein families", "author": ["R. Tatusov", "V. Eugene", "J. David"], "venue": null, "citeRegEx": "Tatusov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tatusov et al\\.", "year": 1997}, {"title": "Using compression based language models for text categorization", "author": ["W. Teahan", "D. Harper"], "venue": "In Workshop on Language Modeling and Information Retrieval,", "citeRegEx": "Teahan and Harper,? \\Q2001\\E", "shortCiteRegEx": "Teahan and Harper", "year": 2001}, {"title": "Using compression for source based classification of text", "author": ["N. Thaper"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Thaper,? 2001", "shortCiteRegEx": "Thaper", "year": 2001}, {"title": "A context-tree weighting method for text generating sources", "author": ["T. Tjalkens", "P. Volf", "F. Willems"], "venue": "In Data Compression Conference,", "citeRegEx": "Tjalkens et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tjalkens et al\\.", "year": 1997}, {"title": "Implementing the context-tree weighting method: Arithmetic coding", "author": ["T. Tjalkens", "F. Willems"], "venue": "In International Conference on Combinatorics, Information Theory and Statistics,", "citeRegEx": "Tjalkens and Willems,? \\Q1997\\E", "shortCiteRegEx": "Tjalkens and Willems", "year": 1997}, {"title": "Weighting Techniques in Data Compression Theory and Algorithms", "author": ["P. Volf"], "venue": "Ph.D. thesis, Technische Universiteit Eindhoven.", "citeRegEx": "Volf,? 2002", "shortCiteRegEx": "Volf", "year": 2002}, {"title": "The context-tree weighting method: Extensions", "author": ["F. Willems"], "venue": "IEEE Transactions on Information Theory, 44 (2), 792\u2013798.", "citeRegEx": "Willems,? 1998", "shortCiteRegEx": "Willems", "year": 1998}, {"title": "The context-tree weighting method: Basic properties", "author": ["F. Willems", "Y. Shtarkov", "T. Tjalkens"], "venue": "IEEE Transactions on Information", "citeRegEx": "Willems et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1995}, {"title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression", "author": ["I. Witten", "T. Bell"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Witten and Bell,? \\Q1991\\E", "shortCiteRegEx": "Witten and Bell", "year": 1991}, {"title": "Text mining: A new frontier for lossless compression", "author": ["I. Witten", "Z. Bray", "M. Mahoui", "B. Teahan"], "venue": "In Proceedings of the Conference on Data Compression,", "citeRegEx": "Witten et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Within the twilight zone: A sensitive profile-profile comparison tool based on information theory", "author": ["G. Yona", "M. Levitt"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Yona and Levitt,? \\Q2002\\E", "shortCiteRegEx": "Yona and Levitt", "year": 2002}, {"title": "Protomap: Automatic classification of protein sequences, a hierarchy of protein families, and local maps of the protein", "author": ["G. Yona", "N. Linial", "M. Linial"], "venue": "space. Proteins,", "citeRegEx": "Yona et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Yona et al\\.", "year": 1999}, {"title": "Compression of individual sequences via variable-rate coding", "author": ["J. Ziv", "A. Lempel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ziv and Lempel,? \\Q1978\\E", "shortCiteRegEx": "Ziv and Lempel", "year": 1978}], "referenceMentions": [{"referenceID": 36, "context": "Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Sch\u00fctze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classification (Pachet, 2002; Dubnov et al.", "startOffset": 195, "endOffset": 218}, {"referenceID": 46, "context": ", 2000) music generation and classification (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al.", "startOffset": 44, "endOffset": 79}, {"referenceID": 20, "context": ", 2000) music generation and classification (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al.", "startOffset": 44, "endOffset": 79}, {"referenceID": 44, "context": ", 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).", "startOffset": 92, "endOffset": 142}, {"referenceID": 4, "context": ", (Bar-Joseph et al., 2001).", "startOffset": 2, "endOffset": 27}, {"referenceID": 48, "context": "Perhaps the most commonly used techniques are based on Hidden Markov Models (HMMs) (Rabiner, 1989).", "startOffset": 83, "endOffset": 98}, {"referenceID": 30, "context": "The results of our protein classification experiments are rather surprising as both of these two excellent predictors are inferior to an algorithm, which is obtained by simple modifications of the prediction component of the well-known Lempel-Ziv-78 compression algorithm (Ziv & Lempel, 1978; Langdon, 1983).", "startOffset": 272, "endOffset": 307}, {"referenceID": 30, "context": "The results of our protein classification experiments are rather surprising as both of these two excellent predictors are inferior to an algorithm, which is obtained by simple modifications of the prediction component of the well-known Lempel-Ziv-78 compression algorithm (Ziv & Lempel, 1978; Langdon, 1983). This rather new algorithm, recently proposed by Nisenson et al. (2003), is a consistent winner in the all the protein classification experiments and achieves surprisingly good results that may be of independent interest in protein analysis.", "startOffset": 293, "endOffset": 380}, {"referenceID": 51, "context": "A lower bound on the redundancy of any universal prediction (and compression) algorithm is \u03a9(K( log T 2T )), where K is (roughly) the number of parameters of the model encoding the distribution P\u0302 (Rissanen, 1984).", "startOffset": 197, "endOffset": 213}, {"referenceID": 51, "context": ", (Rissanen, 1984).", "startOffset": 2, "endOffset": 18}, {"referenceID": 38, "context": "A related lower bound in terms of channel capacity is given by Merhav and Feder (1995).", "startOffset": 63, "endOffset": 87}, {"referenceID": 44, "context": "We also included a recent prediction algorithm from Nisenson et al. (2003) that is a modification of the lz78 prediction algorithm.", "startOffset": 52, "endOffset": 75}, {"referenceID": 30, "context": "The prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983).", "startOffset": 66, "endOffset": 81}, {"referenceID": 30, "context": "The prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983). The presentation of this algorithm is simplified after the well-known lz78 compression algorithm, which works as follows, is understood.", "startOffset": 66, "endOffset": 101}, {"referenceID": 34, "context": "com/bzip2), which is based on the successful Burrows-Wheeler Transform (Burrows & Wheeler, 1994; Manzini, 2001).", "startOffset": 71, "endOffset": 111}, {"referenceID": 55, "context": "Within a probabilistic setting (see Section 2), when the unknown source is stationary and ergodic Markov of finite order, the redundancy is bounded above by (1/ ln n) where n is the length of the training sequence (Savari, 1997).", "startOffset": 214, "endOffset": 228}, {"referenceID": 30, "context": "An lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983).", "startOffset": 51, "endOffset": 66}, {"referenceID": 30, "context": "An lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983). We describe separately the learning and prediction phases.", "startOffset": 51, "endOffset": 86}, {"referenceID": 57, "context": "Specifically, the ppm-ii variant of ppm currently achieves the best compression rates over the standard Calgary Corpus benchmark (Shkarin, 2002).", "startOffset": 129, "endOffset": 144}, {"referenceID": 7, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997).", "startOffset": 143, "endOffset": 157}, {"referenceID": 39, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997).", "startOffset": 47, "endOffset": 61}, {"referenceID": 7, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997). As noted by Witten and Bell (1991) there is no principled justification for any of the various ppm escape mechanisms.", "startOffset": 144, "endOffset": 194}, {"referenceID": 68, "context": "The Context Tree Weighting Method (ctw) algorithm (Willems et al., 1995) is a strong lossless compression algorithm that is based on a clever idea for combining exponentially many VMMs of bounded order.", "startOffset": 50, "endOffset": 72}, {"referenceID": 68, "context": "The paper presenting this idea (Willems et al., 1995) received the 1996 Paper Award of the IEEE Information Theory Society.", "startOffset": 31, "endOffset": 53}, {"referenceID": 67, "context": "A more sophisticated solution is proposed by Willems (1998); see also Section 7.", "startOffset": 45, "endOffset": 60}, {"referenceID": 63, "context": "As shown by Tjalkens et al. (1997), the above simple form of the KT estimator is equivalent to the original form, given in terms of a Dirichlet distribution by Krichevsky and Trofimov (1981).", "startOffset": 12, "endOffset": 35}, {"referenceID": 28, "context": "(1997), the above simple form of the KT estimator is equivalent to the original form, given in terms of a Dirichlet distribution by Krichevsky and Trofimov (1981). The (original) KT-", "startOffset": 132, "endOffset": 163}, {"referenceID": 66, "context": "However, as already mentioned by Willems et al. (1995), it is possible to obtain linear time complexities for both training and prediction.", "startOffset": 33, "endOffset": 55}, {"referenceID": 54, "context": "More details on this efficient implementation are nicely explained by Sadakane et al. (2000) (see also Tjalkens & Willems, 1997).", "startOffset": 70, "endOffset": 93}, {"referenceID": 66, "context": "The second method we consider is Volf\u2019s \u2018decomposed ctw\u2019, denoted here by de-ctw (Volf, 2002).", "startOffset": 81, "endOffset": 93}, {"referenceID": 64, "context": "A more sophisticated binary decomposition of ctw was considered by Tjalkens et al. (1997). There eight binary machines were simultaneously constructed, one for each of the eight binary digits of the ascii representation of text.", "startOffset": 67, "endOffset": 90}, {"referenceID": 52, "context": "The Probabilistic Suffix Tree (pst) prediction algorithm (Ron et al., 1996) attempts to construct the single \u201cbest\u201d D-bounded VMM according to the training sequence.", "startOffset": 57, "endOffset": 75}, {"referenceID": 44, "context": "algorithm due to Nisenson et al. (2003). The algorithm has two parameters M and S and, therefore, its acronym here is lz-ms.", "startOffset": 17, "endOffset": 40}, {"referenceID": 14, "context": "Cover and King (1978) considered a very similar prediction game in their well-known work on the estimation of the entropy of English text.", "startOffset": 0, "endOffset": 22}, {"referenceID": 41, "context": "\u2022 The protein set includes proteins (amino-acid sequences) from the well-known Structural Classification of Proteins (SCOP) database (Murzin et al., 1995).", "startOffset": 133, "endOffset": 154}, {"referenceID": 12, "context": "75, while the compression results of Cleary and Teahan (1997) are 3.", "startOffset": 37, "endOffset": 62}, {"referenceID": 5, "context": "A similar finding was observed by Bejerano and Yona (2001) where models that were trained over specific protein families coded unrelated non-member proteins using an average code length that was higher than the entropy of the background distribution.", "startOffset": 34, "endOffset": 59}, {"referenceID": 41, "context": "Specifically, the SCOP database (Murzin et al., 1995)", "startOffset": 32, "endOffset": 53}, {"referenceID": 16, "context": "Biologists classify proteins into relational sets such as \u2018families\u2019, \u2018superfamilies\u2019, \u2018fold families\u2019 and \u2018classes\u2019; these terms were coined by Dayhoff (1976) and Levitt and Chothia (1976).", "startOffset": 145, "endOffset": 160}, {"referenceID": 16, "context": "Biologists classify proteins into relational sets such as \u2018families\u2019, \u2018superfamilies\u2019, \u2018fold families\u2019 and \u2018classes\u2019; these terms were coined by Dayhoff (1976) and Levitt and Chothia (1976). Being able to classify a new protein in its proper group can help to elucidate relationships between novel genes and existing proteins and characterize unknown genes.", "startOffset": 145, "endOffset": 190}, {"referenceID": 29, "context": "The seven major SCOP classes are quite distinct and can be easily distinguished (Kumarevel et al., 2000; Jin et al., 2003).", "startOffset": 80, "endOffset": 122}, {"referenceID": 25, "context": "The seven major SCOP classes are quite distinct and can be easily distinguished (Kumarevel et al., 2000; Jin et al., 2003).", "startOffset": 80, "endOffset": 122}, {"referenceID": 23, "context": "ppm-d (Howard, 1993) is very similar to to ppm-c, with a slight modification of the escape mechanism; see Section 3.", "startOffset": 6, "endOffset": 20}, {"referenceID": 17, "context": "Two examples are the Canterbury Corpus and the \u2018Large Canterbury Corpus\u2019 (Arnold & Bell, 1997) and the Silesia Compression Corpus (Deorowicz, 2003), which contains significantly larger files than both the Calgary and Canterbury corpora.", "startOffset": 130, "endOffset": 147}, {"referenceID": 53, "context": "To date, the most successful ppm variant is the \u2018complicated PPM with information-inheritance\u2019 (ppm-ii) of Shkarin (2002), which achieves a compression rate of 2.", "startOffset": 107, "endOffset": 122}, {"referenceID": 6, "context": "26 Bunton (1997) provides a (Calgary Corpus) comparison between ppm-c, ppm-d and ppm\u2217 (with its \u2018C\u2019 and \u2018D\u2019 versions).", "startOffset": 3, "endOffset": 17}, {"referenceID": 6, "context": "According to Bloom27, his latest version of ppm-z achieves an average rate of 2.086. Bunton also proposes various improvements for the ppm schemes. Her best improved ppm achieves a 2.177 rate.28 The general-purpose ppm scheme turns out to be rather flexible and allows adaptations to particular domains. For example, Nevill-Manning and Witten (1999) achieve a slightly better rate than the trivial (log2(20)) one for proteins, using a specialized variant of ppm-d", "startOffset": 13, "endOffset": 350}, {"referenceID": 6, "context": "cbloom.com/src/ppmz.html 28. At the time (1996) this variant was probably the most successful ppm version.", "startOffset": 1, "endOffset": 48}, {"referenceID": 64, "context": "A possible reason is that the original binary ctw (and straightforward extensions to larger alphabets) did not achieve the best lossless compression results (Tjalkens et al., 1997).", "startOffset": 157, "endOffset": 180}, {"referenceID": 60, "context": "Previous studies on protein classification used a variety of methods, including generative models such as HMMs (Sonnhammer et al., 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al.", "startOffset": 111, "endOffset": 136}, {"referenceID": 32, "context": ", 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al., 2004).", "startOffset": 97, "endOffset": 140}, {"referenceID": 16, "context": "By combining domain knowledge into ppm using preprocessing and ppm that predicts over dual-alphabets, Drinic and Kirovski (2002) develop a specialized algorithm for compressing computer executable files (.", "startOffset": 102, "endOffset": 129}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al.", "startOffset": 56, "endOffset": 70}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001).", "startOffset": 56, "endOffset": 153}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001).", "startOffset": 56, "endOffset": 220}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001). Context Tree Weighting (ctw), with its proven redundancy bound, has also attracted much attention.", "startOffset": 56, "endOffset": 246}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model.", "startOffset": 10, "endOffset": 36}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences.", "startOffset": 10, "endOffset": 231}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea.", "startOffset": 10, "endOffset": 534}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster.", "startOffset": 10, "endOffset": 663}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster. Finally, Kermorvant and Dupont (2002) propose to improve the smoothing mechanism used in the original pst algorithm.", "startOffset": 10, "endOffset": 1017}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster. Finally, Kermorvant and Dupont (2002) propose to improve the smoothing mechanism used in the original pst algorithm. Experiments with a protein problem indicate that the proposed smoothing method can help. As mentioned earlier, there are many extensions of the lz78 algorithm. See (Nelson & Gailly, 1996) for more details. It is interesting to note that Volf considered an \u201censemble\u201d consisting of both de-ctw and ppm. He shows that the resulting algorithm performs nearly as well as the ppm-z on the Calgary Corpus while beating the ppm-z on the Canterbury Corpus. Previous studies on protein classification used a variety of methods, including generative models such as HMMs (Sonnhammer et al., 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al., 2004). Jaakkola et al. (1999) describe a model that combines the SVM discriminative model with a generative model (the HMM-based Fischer kernel).", "startOffset": 10, "endOffset": 1839}, {"referenceID": 72, "context": "Other studies used unsupervised learning techniques, such as clustering (Yona et al., 1999; Tatusov et al., 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 72, "endOffset": 113}, {"referenceID": 61, "context": "Other studies used unsupervised learning techniques, such as clustering (Yona et al., 1999; Tatusov et al., 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 72, "endOffset": 113}, {"referenceID": 2, "context": ", 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 33, "endOffset": 51}, {"referenceID": 47, "context": "Of all models, the HMM model is perhaps the most popular and successful model for protein classification to-date (Park et al., 1998).", "startOffset": 113, "endOffset": 132}, {"referenceID": 9, "context": "This is a more difficult task and it has been the subject of strong interest in fold prediction tasks (CASP, 2002).", "startOffset": 102, "endOffset": 114}, {"referenceID": 37, "context": "Other studies that addressed a somewhat similar problem to ours used sequence comparison algorithms (McGuffin et al., 2001) or SVMs and Neural Networks (Ding & Dubchak, 2001) reporting a maximal accuracy of 56%.", "startOffset": 100, "endOffset": 123}, {"referenceID": 66, "context": "One of the most successful general-purpose predictors we examined is the de-ctw algorithm, with its alphabet decomposition mechanism, as suggested by Volf (2002). As it turns out, this mechanism is crucial for the success of the the ctw scheme.", "startOffset": 150, "endOffset": 162}, {"referenceID": 43, "context": "It would be interesting to see if some of the recent results on the missing mass problem, such as those presented by Orlitsky et al. (2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations.", "startOffset": 117, "endOffset": 140}, {"referenceID": 34, "context": "(2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations.", "startOffset": 11, "endOffset": 39}, {"referenceID": 34, "context": "(2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations. We can view the ctw algorithm as an ensemble of numerous VMMs. However, the weights of the various VMM models are fixed. Various ensemble methods in machine learning such as boosting and online expert advice algorithms attempt to optimize the weights of the various ensemble members. Could we achieve better performance guarantees and better empirical performance using adaptive weight learning approaches? Volf (2002) has some results (on combining lz78 and de-ctw) indicating that this is a promising direction.", "startOffset": 11, "endOffset": 544}, {"referenceID": 26, "context": "Other possibilities would be to employ portfolio selection algorithms to dynamically change the weights of several models, along the lines suggested by Kalai et al. (1999).", "startOffset": 152, "endOffset": 172}], "year": 2011, "abstractText": "This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a \u201cdecomposed\u201d CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}