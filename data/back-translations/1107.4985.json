{"id": "1107.4985", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2011", "title": "Variational Gaussian Process Dynamical Systems", "abstract": "High-dimensional time series are endemic in machine learning applications such as robotics (sensor data), computer biology (gene expression data), vision (video sequences), and graphics (motion capture data).Practical nonlinear probabilistic approaches to these data are required.In this paper, we present the variational Gaussian process dynamic system.Our work builds on newer variation approximations of latent variable models of Gaussian processes to enable nonlinear dimensionality reduction while simultaneously learning a dynamic history in latent space. It also enables the automatic determination of the appropriate dimensionality of latent space. We demonstrate the model using a human dataset for motion capture and a series of high-resolution video sequences.", "histories": [["v1", "Mon, 25 Jul 2011 15:54:05 GMT  (7958kb,AD)", "http://arxiv.org/abs/1107.4985v1", "16 pages, 19 figures"]], "COMMENTS": "16 pages, 19 figures", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.CV math.PR", "authors": ["andreas c damianou", "michalis k titsias", "neil d lawrence"], "accepted": true, "id": "1107.4985"}, "pdf": {"name": "1107.4985.pdf", "metadata": {"source": "CRF", "title": "Variational Gaussian Process Dynamical Systems", "authors": ["Andreas Damianou", "Michalis K. Titsias"], "emails": ["andreas.damianou@sheffield.ac.uk", "mtitsias@cs.man.ac.uk", "neil@dcs.shef.ac.uk"], "sections": [{"heading": null, "text": "data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences."}, {"heading": "1 Introduction", "text": "Nonlinear probabilistic modeling of high dimensional time series data is a key challenge for the machine learning community. A standard approach is to simultaneously apply a nonlinear dimensionality reduction to the data whilst governing the latent space with a nonlinear temporal prior. The key difficulty for such approaches is that analytic marginalization of the latent space is typically intractable. Markov chain Monte Carlo approaches can also be problematic as latent trajectories are strongly correlated making efficient sampling a challenge. One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5]. Ko and Fox [6] further extend these models for fully Bayesian filtering in a robotics setting. We refer to this class of dynamical models based on the GP-LVM as Gaussian process dynamical systems (GPDS). However, the use of a MAP approximation for training these models presents key problems. Firstly, since the latent variables are not marginalised, the parameters of the dynamical prior cannot be optimized without the risk of overfitting. Further, the dimensionality of the latent space cannot be determined by the model: adding further dimensions always increases the likelihood of the data. In this paper we build on recent developments in variational approximations for Gaussian processes [7, 8] to introduce a variational Gaussian process dynamical system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous lower bound on the marginal likelihood. As well as providing a principled approach to handling uncertainty in the latent space, this allows both the parameters of the latent dynamical process and the dimensionality of the latent space to be determined. The approximation enables the application of our model to time series containing millions of dimensions and thousands of time points. We illustrate this by modeling human motion capture data and high dimensional video sequences.\nar X\niv :1\n10 7.\n49 85\nv1 [\nst at\n.M L\n] 2"}, {"heading": "2 The Model", "text": "Assume a multivariate times series dataset {yn, tn}Nn=1, where yn \u2208 RD is a data vector observed at time tn \u2208 R+. We are especially interested in cases where each yn is a high dimensional vector and, therefore, we assume that there exists a low dimensional manifold that governs the generation of the data. Specifically, a temporal latent function x(t) \u2208 RQ (with Q D), governs an intermediate hidden layer when generating the data, and the dth feature from the data vector yn is then produced from xn = x(tn) according to\nynd = fd(xn) + nd , nd \u223c N (0, \u03b2\u22121), (1)\nwhere fd(x) is a latent mapping from the low dimensional space to dth dimension of the observation space and \u03b2 is the inverse variance of the white Gaussian noise. We do not want to make strong assumptions about the functional form of the latent functions (x, f).1 Instead we would like to infer them in a fully Bayesian non-parametric fashion using Gaussian processes [9]. Therefore, we assume that x is a multivariate Gaussian process indexed by time t and f is a different multivariate Gaussian process indexed by x, and we write\nxq(t) \u223c GP(0, kx(ti, tj)), q = 1, . . . , Q, (2) fd(x) \u223c GP(0, kf (xi,xj)), d = 1, . . . , D. (3)\nHere, the individual components of the latent function x are taken to be independent sample paths drawn from a Gaussian process with covariance function kx(ti, tj). Similarly, the components of f are independent draws from a Gaussian process with covariance function kf (xi,xj). These covariance functions, parametrized by parameters \u03b8x and \u03b8f respectively, play very distinct roles in the model. More precisely, kx determines the properties of each temporal latent function xq(t). For instance, the use of an Ornstein-Uhlbeck covariance function yields a Gauss-Markov process for xq(t), while the squared-exponential kernel gives rise to very smooth and non-Markovian processes. In our experiments, we will focus on the squared exponential covariance function (RBF), the Matern 3/2 which is only once differentiable, and a periodic covariance function [9, 10] which can be used when data exhibit strong periodicity. These kernel functions take the form:\nkx(rbf) (ti , tj ) = \u03c3 2 rbfe \u2212 (\nti\u2212tj) 2\n(2l2t ) , kx(mat) (ti, tj) = \u03c3 2 mat\n( 1 +\n\u221a 3|ti \u2212 tj | lt\n) e \u2212 \u221a 3|ti\u2212tj | lt ,\nkx(per) (ti , tj ) = \u03c3 2 pere \u2212 12 sin2( 2\u03c0T (ti\u2212tj)) lt . (4)\nThe covariance function kf determines the properties of the latent mapping f that maps each low dimensional variable xn to the observed vector yn. We wish this mapping to be a non-linear but smooth, and thus a suitable choice is the squared exponential covariance function\nkf (xi,xj) = \u03c3 2 arde\n\u2212 12 \u2211Q q=1 wq(xi,q\u2212xj ,q) 2\n, (5)\nwhich assumes a different scale wq for each latent dimension. This, as in the variational Bayesian formulation of the GP-LVM [8], enables an automatic relevance determination procedure (ARD), i.e. it allows Bayesian training to \u201cswitch off\u201d unnecessary dimensions by driving the values of the corresponding scales to zero.\nThe matrix Y \u2208 RN\u00d7D will collectively denote all observed data so that its nth row corresponds to the data point yn. Similarly, the matrix F \u2208 RN\u00d7D will denote the mapping latent variables, i.e. fnd = fd(xn), associated with observations Y from (1). Ana usly, X \u2208 RN\u00d7Q will store all low dimensional latent variables xnq = xq(tn). Further, we will refer to columns of these matrices by the vectors yd, fd,xq \u2208 RN . Given the latent variables we assume independence over the data features, and given time we assume independence over latent dimensions to give\np(Y, F,X|t) = p(Y |F )p(F |X)p(X|t) = D\u220f d=1 p(yd|fd)p(fd|X ) Q\u220f q=1 p(xq|t), (6)\n1To simplify our notation, we often write x instead of x(t) and f instead of f(x). Later we also use a similar convention for the kernel functions by often writing them as kf and kx.\nwhere t \u2208 RN and p(yd|fd) is a Gaussian likelihood function term defined from (1). Further, p(fd|X ) is a marginal GP prior such that\np(fd|X ) = N (fd|0,KNN ), (7)\nwhere KNN = kf (X,X) is the covariance matrix defined by the kernel function kf and similarly p(xq|t) is the marginal GP prior associated with the temporal function xq(t),\np(xq|t) = N (xq|0,Kt) , (8)\nwhere Kt = kx(t, t) is the covariance matrix obtained by evaluating the kernel function kx on the observed times t.\nBayesian inference using the above model poses a huge computational challenge as, for instance, marginalization of the variables X , that appear non-linearly inside the kernel matrix KNN , is troublesome. Practical approaches that have been considered until now (e.g. [5, 3]) marginalise out only F and seek a MAP solution for X . In the next section we describe how efficient variational approximations can be applied to marginalize X by extending the framework of [8]."}, {"heading": "2.1 Variational Bayesian training", "text": "The key difficulty with the Bayesian approach is propagating the prior density p(X|t) through the nonlinear mapping. This mapping gives the expressive power to the model, but simultaneously renders the associated marginal likelihood,\np(Y |t) = \u222b p(Y |F )p(F |X)p(X|t)dXdF, (9)\nintractable. We now invoke the variational Bayesian methodology to approximate the integral. Following a standard procedure [11], we introduce a variational distribution q(\u0398) and compute the Jensen\u2019s lower bound Fv on the logarithm of (9),\nFv(q,\u03b8) = \u222b q(\u0398) log\np(Y |F )p(F |X)p(X |t) q(\u0398) dXdF, (10)\nwhere \u03b8 denotes the model\u2019s parameters. However, the above form of the lower bound is problematic becauce X (in the GP term p(F |X)) appears non-linearly inside the kernel matrix KNN making the integration over X difficult. As shown in [8], this intractability is removed by applying the \u201cdata augmentation\u201d principle. More precisely, we augment the joint probability model in (6) by includingM extra samples of the GP latent mapping f , known as inducing points, so that um \u2208 RD is such a sample. The inducing points are evaluated at a set of pseudo-inputs X\u0303 \u2208 RM\u00d7Q. The augmented joint probability density takes the form\np(Y, F, U,X, X\u0303|t) = D\u220f d=1 p(yd|fd)p(fd|ud,X )p(ud|X\u0303)p(X|t), (11)\nwhere p(ud|X\u0303) is a zero-mean Gaussian with a covariance matrix KMM constructed using the same function as for the GP prior (7). By dropping X\u0303 from our expressions, we write the augmented GP prior analytically (see [9]) as\np(fd|ud, X) = N ( fd|KNMK\u22121MMud,KNN \u2212KNMK \u22121 MMKMN ) . (12)\nA key result in [8] is that a tractable lower bound (computed analogously to (10)) can be obtained through the variational density\nq(\u0398) = q(F,U,X) = q(F |U,X)q(U)q(X) = D\u220f d=1 p(fd|ud, X)q(ud)q(X), (13)\nwhere q(X) = \u220fQ q=1N (xq|\u00b5q, Sq) and q(ud) is an arbitrary variational distribution. Titsias and Lawrence [8] assume full independence for q(X) and the variational covariances are diagonal matrices. Here, in contrast, the\nposterior over the latent variables will have strong correlations, so Sq is taken to be a N \u00d7 N full covariance matrix. Optimization of the variational lower bound provides an approximation to the true posterior p(X|Y ) by q(X). In the augmented probability model, the \u201cdifficult\u201d term p(F |X) appearing in (10) is now replaced with (12) and, eventually, it cancels out with the first factor of the variational distribution (13) so that F can be marginalised out analytically. Given the above and after breaking the logarithm in (10), we obtain the final form of the lower bound (see supplementary material for more details)\nFv(q,\u03b8) = F\u0302v \u2212 KL(q(X) \u2016 p(X|t)), (14)\nwith F\u0302v = \u222b q(X) log p(Y |F )p(F |X) dXdF . Both terms in (14) are now tractable. Note that the first of the above terms involves the data while the second one only involves the prior. All the information regarding data point correlations is captured in the KL term and the connection with the observations comes through the variational distribution. Therefore, the first term in (14) has the same analytical solution as the one derived in [8]. (14) can be maximized by using gradient-based methods2. However, when not factorizing q(X) across data points yields O(N2) variational parameters to optimize. This issue is addressed in the next section."}, {"heading": "2.2 Reparametrization and Optimization", "text": "The optimization involves the model parameters \u03b8 = (\u03b2,\u03b8f ,\u03b8t), the variational parameters {\u00b5q, Sq}Qq=1 from q(X) and the inducing points3 X\u0303 .\nOptimization of the variational parameters appears challenging, due to their large number and the correlations between them. However, by reparametrizing our O ( N2 )\nvariational parameters according to the framework described in [12] we can obtain a set of O(N) less correlated variational parameters. Specifically, we first take the derivatives of the variational bound (14) w.r.t. Sq and \u00b5q and set them to zero, to find the stationary points,\nSq = ( K\u22121t + \u039bq )\u22121 and \u00b5q = Kt\u00b5\u0304q, (15)\nwhere \u039bq = \u22122\u03d1F\u0302v (q,\u03b8)\u03d1Sq is a N \u00d7 N diagonal, positive matrix and \u00b5\u0304q = \u03d1F\u0302v \u03d1\u00b5q is a N\u2212dimensional vector. The above stationary conditions tell us that, since Sq depends on a diagonal matrix \u039bq , we can reparametrize it using only the N\u2212dimensional diagonal of that matrix, denoted by \u03bbq . Then, we can optimise the 2(Q \u00d7 N) parameters (\u03bbq , \u00b5\u0304q) and obtain the original parameters using (15)."}, {"heading": "2.3 Learning from Multiple Sequences", "text": "Our objective is to model multivariate time series. A given data set may consist of a group of independent observed sequences, each with a different length (e.g. in human motion capture data several walks from a subject). Let, for example, the dataset be a group of S independent sequences ( Y (1), ..., Y (S) ) . We would like our model to capture the underlying commonality of these data. We handle this by allowing a different temporal latent function for each of the independent sequences, so that X(s) is the set of latent variables corresponding to the sequence s. These sets are a priori assumed to be independent since they correspond to separate sequences, i.e. p ( X(1), X(2), ..., X(S) ) = \u220fS s=1 p(X\n(s)), where we dropped the conditioning on time for simplicity. This factorisation leads to a block-diagonal structure for the time covariance matrix Kt, where each block corresponds to one sequenece. In this setting, each block of observations Y (s) is generated from its corresponding X(s) according to Y (s) = F (s) + , where the latent function which governs this mapping is shared across all sequences and is Gaussian noise.\n2See supplementary material for more detailed derivation of (14) and for the equations for the gradients. 3We will use the term \u201cvariational parameters\u201d to refer only to the parameters of q(X) although the inducing points are also variational\nparameters."}, {"heading": "3 Predictions", "text": "Our algorithm models the temporal evolution of a dynamical system. It should be capable of generating completely new sequences or reconstructing missing observations from partially observed data. For generating novel sequence given training data the model requires a time vector t\u2217 as input and computes a density p(Y\u2217|Y, t, t\u2217). For reconstruction of partially observed data the time stamp information is additionally accompanied by a partially observed sequence Y p\u2217 \u2208 RN\u2217\u00d7Dp from the whole Y\u2217 = (Y p\u2217 , Y m\u2217 ), where p and m are set of indices indicating the present (i.e. observed) and missing dimensions of Y\u2217 respectively, so that p \u222am = {1, . . . , D}. We reconstruct the missing dimensions by computing the Bayesian predictive distribution p(Y m\u2217 |Y p \u2217 , Y, t\u2217, t). The predictive densities can also be used as estimators for tasks like generative Bayesian classification. Whilst time stamp information is always provided, in the next section we drop its dependence to avoid notational clutter."}, {"heading": "3.1 Predictions Given Only the Test Time Points", "text": "To approximate the predictive density, we will need to introduce the underlying latent function values F\u2217 \u2208 RN\u2217\u00d7D (the noisy-free version of Y\u2217) and the latent variables X\u2217 \u2208 RN\u2217\u00d7Q. We write the predictive density as\np(Y\u2217|Y ) = \u222b p(Y\u2217, F\u2217, X\u2217|Y\u2217, Y )dF\u2217dX\u2217 = \u222b p(Y\u2217|F\u2217)p(F\u2217|X\u2217, Y )p(X\u2217|Y )dF\u2217dX\u2217. (16)\nThe term p(F\u2217|X\u2217, Y ) is approximated by the variational distribution q(F\u2217|X\u2217) = \u222b \u220f\nd\u2208D p(f\u2217,d|ud, X\u2217)q(ud)dud = \u220f d\u2208D q(f\u2217,d|X\u2217), (17)\nwhere q(f\u2217,d|X\u2217) is a Gaussian that can be computed analytically, since in our variational framework the optimal setting for q(ud) is also found to be a Gaussian (see suppl. material for complete forms). As for the term p(X\u2217|Y ) in eq. (16), it is approximated by a Gaussian variational distribution q(X\u2217),\nq(X\u2217) = Q\u220f q=1 q(x\u2217,q) = Q\u220f q=1 \u222b p(x\u2217,q|xq)q(xq)dxq = Q\u220f q=1 \u3008p(x\u2217,q|xq)\u3009q(xq) , (18)\nwhere p(x\u2217,q|xq) is a Gaussian found from the conditional GP prior (see [9]) and q(X) is also Gaussian. We can, thus, work out analytically the mean and variance for (18), which turn out to be:\n\u00b5x\u2217,q = K\u2217N \u00b5\u0304q (19) var(x\u2217,q) = K\u2217\u2217 \u2212K\u2217N (Kt + \u039b\u22121q )\u22121KN\u2217. (20)\nwhere K\u2217N = kx(t\u2217, t), K\u2217N = K>\u2217N and K\u2217\u2217 = kx(t\u2217, t\u2217). Notice that these equations have exactly the same form as found in standard GP regression problems. Once we have analytic forms for the posteriors in (16), the predictive density is approximated as\np(Y\u2217|Y ) = \u222b p(Y\u2217|F\u2217)q(F\u2217|X\u2217)q(X\u2217)dF\u2217dX\u2217 = \u222b p(Y\u2217|F\u2217) \u3008q(F\u2217|X\u2217)\u3009q(X\u2217) dF\u2217, (21)\nwhich is a non-Gaussian integral that cannot be computed analytically. However, following the same argument as in [9, 13], we can calculate analytically its mean and covariance:\nE(F\u2217) = B>\u03a8\u22171 (22) Cov(F\u2217) = B> ( \u03a8\u22172 \u2212\u03a8\u22171(\u03a8\u22171)> ) B + \u03a8\u22170I \u2212 Tr [( K\u22121MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a8\u22172 ] I, (23)\nwhere B = \u03b2 (KMM + \u03b2\u03a82) \u22121 \u03a8>1 Y , \u03a8 \u2217 0 = \u3008kf (X\u2217, X\u2217)\u3009, \u03a8\u22171 = \u3008KM\u2217\u3009 and \u03a8\u22172 = \u3008KM\u2217K\u2217M \u3009. All expectations are taken w.r.t. q(X\u2217) and can be calculated analytically, while KM\u2217 denotes the cross-covariance matrix between the training inducing inputs X\u0303 and X\u2217. The \u03a8 quantities are calculated analytically (see suppl. material). Finally, since Y\u2217 is just a noisy version of F\u2217, the mean and covariance of (21) is just computed as: E(Y\u2217) = E(F\u2217) and Cov(Y\u2217) = Cov(F\u2217) + \u03b2\u22121IN\u2217 ."}, {"heading": "3.2 Predictions Given the Test Time Points and Partially Observed Outputs", "text": "The expression for the predictive density p(Y m\u2217 |Y p \u2217 , Y ) is similar to (16),\np(Y m\u2217 |Y p\u2217 , Y ) = \u222b p(Y m\u2217 |Fm\u2217 )p(Fm\u2217 |X\u2217, Y p\u2217 , Y )p(X\u2217|Y p\u2217 , Y )dFm\u2217 dX\u2217, (24)\nand is analytically intractable. To obtain an approximation, we firstly need to apply variational inference and approximate p(X\u2217|Y p\u2217 , Y ) with a Gaussian distribution. This requires the optimisation of a new variational lower bound that accounts for the contribution of the partially observed data Y p\u2217 . This lower bound approximates the true marginal likelihood p(Y p\u2217 , Y ) and has exactly analogous form with the lower bound computed only on the training data Y . Moreover, the variational optimisation requires the definition of the variational distribution q(X\u2217, X) which needs to be optimised and is fully correlated across X and X\u2217. After the optimisation, the approximation to the true posterior p(X\u2217|Y p\u2217 , Y ) is given from the marginal q(X\u2217). A much faster but less accurate method would be to decouple the test from the training latent variables by imposing the factorisation q(X\u2217, X) = q(X)q(X\u2217). This is not used, however, in our current implementation."}, {"heading": "4 Handling Very High Dimensional Datasets", "text": "Our variational framework avoids the typical cubic complexity of Gaussian processes allowing relatively large training sets (thousands of time points, N ). Further, the model scales only linearly with the number of dimensions D. Specifically, the number of dimensions only matters when performing calculations involving the data matrix Y . In the final form of the lower bound (and consequently in all of the derived quantities, such as gradients) this matrix only appears in the form Y Y > which can be precomputed. This means that, when N D, we can calculate Y Y > only once and then substitute Y with the SVD (or Cholesky decomposition) of Y Y >. In this way, we can work with an N \u00d7N instead of an N \u00d7D matrix. Practically speaking, this allows us to work with data sets involving millions of features. In our experiments we model directly the pixels of HD quality video, exploiting this trick."}, {"heading": "5 Experiments", "text": "We consider two different types of high dimensional time series, a human motion capture data set consisting of different walks and high resolution video sequences. The experiments are intended to explore the various properties of the model and to evaluate its performance in different tasks (prediction, reconstruction, generation of data). Matlab source code for repeating the following experiments is available on-line from http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/vargplvm/."}, {"heading": "5.1 Human Motion Capture Data", "text": "We followed [14, 15] in considering motion capture data of walks and runs taken from subject 35 in the CMU motion capture database. We treated each motion as an independent sequence. The data set was constructed and preprocessed as described in [15]. This results in 2,613 separate 59-dimensional frames split into 31 training sequences with an average length of 84 frames each.\nThe model is jointly trained, as explained in section 2.3, on both walks and runs, i.e. the algorithm learns a common latent space for these motions. At test time we investigate the ability of the model to reconstruct test data from a previously unseen sequence given partial information for the test targets. This is tested once by providing only the dimensions which correspond to the body of the subject and once by providing those that correspond to the legs. We compare with results in [15], which used MAP approximations for the dynamical models, and against nearest neighbour. We can also indirectly compare with the binary latent variable model (BLV) of [14] which used a slightly different data preprocessing. We assess the performance using the cumulative error per joint in the scaled space defined in [14] and by the root mean square error in the angle space suggested by [15]. Our model was initialized with nine latent dimensions. We performed two runs, once using\nthe Matern covariance function for the dynamical prior and once using the RBF. From table 1 we see that the variational Gaussian process dynamical system considerably outperforms the other approaches. The appropriate latent space dimensionality for the data was automatically inferred by our models. The model which employed an RBF covariance to govern the dynamics retained four dimensions, whereas the model that used the Matern kept only three. The other latent dimensions were completely switched off by the ARD parameters. The best performance for the legs and the body reconstruction was achieved by the VGPDS model that used the Matern and the RBF covariance function respectively."}, {"heading": "5.2 Modeling Raw High Dimensional Video Sequences", "text": "For our second set of experiments we considered video sequences. Such sequences are typically preprocessed before modeling to extract informative features and reduce the dimensionality of the problem. Here we work directly with the raw pixel values to demonstrate the ability of the VGPDS to model data with a vast number of features. This also allows us to directly sample video from the learned model.\nFirstly, we used the model to reconstruct partially observed frames from test video sequences4. For the first video discussed here we gave as partial information approximately 50% of the pixels while for the other two we gave approximately 40% of the pixels on each frame. The mean squared error per pixel was measured to compare with the k\u2212nearest neighbour (NN) method, for k \u2208 (1, .., 5) (we only present the error achieved for the best choice of k in each case). The datasets considered are the following: firstly, the \u2018Missa\u2019 dataset, a standard benchmark used in image processing. This is 103,680-dimensional video, showing a woman talking for 150 frames. The data is challenging as there are translations in the pixel space. We also considered an HD video of dimensionality 9\u00d7105 that shows an artificially created scene of ocean waves as well as a 230, 400\u2212dimensional video showing a dog running for 60 frames. The later is approximately periodic in nature, containing several paces from the dog. For the first two videos we used the Matern and RBF kernel respectively to model the dynamics and interpolated to reconstruct blocks of frames chosen from the whole sequence. For the \u2018dog\u2019 dataset we constructed a compound kernel kx = kx(rbf) + kx(periodic), where the RBF term is employed to capture any divergence from the approximately periodic pattern. We then used our model to reconstruct the last 7 frames extrapolating beyond the original video. As can be seen in table 2, our method outperformed NN in all cases. The results are also demonstrated visually in figure 1 and the reconstructed videos are available in the supplementary material.\n4\u2018Missa\u2019 dataset: cipr.rpi.edu. \u2018Ocean\u2019: cogfilms.com. \u2018Dog\u2019: fitfurlife.com. See details in supplementary. The logo appearing in the \u2018dog\u2019 images in the experiments that follow, has been added with post-processing.\nAs can be seen in figure 1, VGPDS predicts pixels which are smoothly connected with the observed image, whereas the NN method cannot fit the predicted pixels in the overall context.\nAs a second task, we used our generative model to create new samples and generate a new video sequence. This is most effective for the \u2018dog\u2019 video as the training examples were approximately periodic in nature. The model was trained on 60 frames (time-stamps [t1, t60]) and we generated the new frames which correspond to the next 40 time points in the future. The only input given for this generation of future frames was the time stamp vector, [t61, t100]. The results show a smooth transition from training to test and amongst the test video frames. The resulting video of the dog continuing to run is sharp and high quality. This experiment demonstrates the ability of the model to reconstruct massively high dimensional images without blurring. Frames from the result are shown in figure 2. The full video is available in the supplementary material."}, {"heading": "6 Discussion and Future Work", "text": "We have introduced a fully Bayesian approach for modeling dynamical systems through probabilistic nonlinear dimensionality reduction. Marginalizing the latent space and reconstructing data using Gaussian processes results in a very generic model for capturing complex, non-linear correlations even in very high dimensional\ndata, without having to perform any data preprocessing or exhaustive search for defining the model\u2019s structure and parameters.\nOur method\u2019s effectiveness has been demonstrated in two tasks; firstly, in modeling human motion capture data and, secondly, in reconstructing and generating raw, very high dimensional video sequences. A promising future direction to follow would be to enhance our formulation with domain-specific knowledge encoded, for example, in more sophisticated covariance functions or in the way that data are being preprocessed. Thus, we can obtain application-oriented methods to be used for tasks in areas such as robotics, computer vision and finance."}, {"heading": "Acknowledgments", "text": "Research was partially supported by the University of Sheffield Moody endowment fund and the Greek State Scholarships Foundation (IKY). We also thank Colin Litster and \u201cFit Fur Life\u201d for allowing us to use their video files as datasets."}, {"heading": "A Derivation of the variational bound", "text": "We wish to approximate the marginal likelihood:\np(Y |t) = \u222b p(Y, F,X|t)dXdF, (25)\nby computing a lower bound: Fv(q,\u03b8) = \u222b q(\u0398) log\np(Y, F,X |t) q(\u0398) dXdF, (26)\nThis can be achieved by first augmenting the joint probability density of our model with inducing inputs X\u0303 along with their corresponding function values U :\np(Y, F, U,X, X\u0303|t) = D\u220f d=1 p(yd|fd)p(fd|ud,X )p(ud|X\u0303)p(X|t) (27)\nwhere p(ud|X\u0303) = \u220fD d=1N (ud|0,KMM ) . For simplicity, X\u0303 is dropped from our expressions for the rest of this supplementary material. Note that after including the inducing points, p(fd|ud, X) remains analytically tractable and it turns out to be [9]):\np(fd|ud, X) = N ( fd|KNMK\u22121MMud,KNN \u2212KNMK \u22121 MMKMN ) . (28)\nWe are now able to define a variational distribution q(\u0398) which factorises as: For tractability we now define a variational density, q(\u0398):\nq(\u0398) = q(F,U,X) = q(F |U,X)q(U)q(X) = D\u220f d=1 p(fd|ud, X)q(ud)q(X), (29)\nwhere q(X) = \u220fQ q=1N (xq|\u00b5q, Sq). Now, we return to (26) and replace the joint distribution with its augmented version (27) and the variational distribution with its factorised version (29):\nFv(q,\u03b8) = \u222b q(\u0398) log\np(Y, F, U,X|t) q(F,U,X) dXdF,\n= \u222b D\u220f d=1 p(fd|ud, X)q(ud)q(X) log \u220fD d=1 p(yd|fd) p(fd|ud,X )p(ud|X\u0303)p(X|t)\u220fD d=1 p(fd|ud, X)q(ud)q(X)) dXdF\n= \u222b D\u220f d=1 p(fd|ud, X)q(ud)q(X) log \u220fD d=1 p(yd|fd)p(ud|X\u0303)\u220fD d=1 q(ud)q(X)) dXdF,\n\u2212 \u222b D\u220f\nd=1\nq(X) log q(X)\np(X|t) dX\n= F\u0302v \u2212 KL(q \u2016 p), (30)\nwith F\u0302v = \u222b q(X) log p(Y |F )p(F |X) dXdF = \u2211D d=1 F\u0302d. Both terms in (30) are analytically tractable, with the first having the same analytical solution as the one derived in [8]. Further calculations in the the F\u0302v term reveal that the optimal setting for q(ud) is also a Gaussian. More specifically, we have:\nF\u0302v = \u222b q(ud) log e \u3008logN(yd|ad,\u03b2\u22121Id)\u3009 q(X)p(ud)\nq(ud) dud \u2212A (31)\nwhere A is a collection of remaining terms and ad is the mean of (28). (31) is a KL-like quantity and, therefore, q(ud) is optimally set to be the quantity appearing in the numerator of the above equation. So:\nq(ud) = e \u3008logN(yd|ad,\u03b2\u22121Id)\u3009 q(X)p(ud),\nexactly as in [8]. This is a Gaussian distribution since p(ud) = N (ud|0,KMM ). The complete form of the jensen\u2019s lower bound turns out to be:\nFv(q,\u03b8) = D\u2211 d=1 F\u0302d(q,\u03b8)\u2212 KL(q \u2016 p)\n= D\u2211 d=1 log\n( (\u03b2) N 2 |KMM | 1 2\n(2\u03c0) N 2 |\u03b2\u03a82 + KMM | 1 2\ne\u2212 1 2y T dWyd ) \u2212 \u03b2\u03c80\n2 + \u03b2 2 Tr ( K\u22121MM\u03a82 ) \u2212 Q\n2 log |Kt | \u2212\n1\n2 Q\u2211 q=1 [ Tr ( Kt \u22121Sq ) + Tr ( Kt \u22121\u00b5q\u00b5 T q )] + 1 2 Q\u2211 q=1 log |Sq |+ const (32)\nwhere the last line corresponds to the KL term. Also:\n\u03a80 = Tr(\u3008KNN \u3009q(X )) , \u03a81 = \u3008KNM \u3009q(X ) , \u03a82 = \u3008KMN KNM \u3009q(X ) (33)\nThe \u03a8 quantities can be computed analytically as in [8]."}, {"heading": "B Derivatives of the variational bound", "text": "Before giving the expressions for the derivatives of the variational bound (30), it should be reminded that the variational parameters \u00b5q and Sq (for all qs) have been reparametrised as Sq = ( K\u22121t + diag(\u03bbq) )\u22121 and \u00b5q = Kt\u00b5\u0304q , where the function diag(\u00b7) transforms a vector into a square diagonal matrix and vice versa. Given the above, the set of the parameters to be optimised is (\u03b8f ,\u03b8x, {\u00b5\u0304q,\u03bbq}Qq=1, X\u0303 . The gradient w.r.t the inducing points X\u0303 , however, has exactly the same form as for \u03b8f and, therefore, is not presented here. Also notice that from now on we will often use the term \u201cvariational parameters\u201d to refer to the new quantities \u00b5\u0304q and \u03bbq .\nSome more notation:\n1. \u03bbq is a scalar, an element of the vector \u03bbq which, in turn, is the main diagonal of the diagonal matrix \u039bq .\n2. Sij , Sq;ij the element of Sq found in the i-th row and j-th column.\n3. sq , {Sq;ii}Ni=1, i.e. it is a vector with the diagonal of Sq .\nB.1 Derivatives w.r.t the variational parameters\n\u03d1Fv \u03d1\u00b5\u0304q = Kt ( \u03d1F\u0302 \u03d1\u00b5q \u2212 \u00b5\u0304q ) and \u03d1Fv \u03d1\u03bbq = \u2212(Sq \u25e6 Sq) ( \u03d1F\u0302 \u03d1sq + 1 2 \u03bbq ) . (34)\nwhere:\nF\u0302(q,\u03b8) \u03d1\u00b5q = \u2212\u03b2D 2 \u03d1\u03a80 \u03d1\u00b5q + \u03b2Tr ( \u03d1\u03a8T1 \u03d1\u00b5q Y Y T\u03a81A \u22121 )\n+ \u03b2 2 Tr [ \u03d1\u03a82 \u03d1\u00b5q ( DK\u22121MM \u2212 \u03b2 \u22121DA\u22121 \u2212A\u22121\u03a8T1 Y Y T\u03a81A\u22121 )]\n(35)\n\u03d1F\u0302(q,\u03b8) \u03d1Sq;i,j = \u2212\u03b2D 2 \u03d1\u03a80 \u03d1Sq;i,j + \u03b2Tr ( \u03d1\u03a8T1 \u03d1Sq;i,j Y Y T\u03a81A \u22121 )\n+ \u03b2 2 Tr [ \u03d1\u03a82 \u03d1Sq;i,j ( DK\u22121MM \u2212 \u03b2 \u22121DA\u22121 \u2212A\u22121\u03a8T1 Y Y T\u03a81A\u22121 )]\n(36)\nwith A = \u03b2\u22121KMM + \u03a82.\nB.2 Derivatives w.r.t \u03b8 = (\u03b8f ,\u03b8x) and \u03b2 Given that the KL term involves only the temporal prior, its gradient w.r.t the parameters \u03b8f is zero. Therefore:\n\u03d1Fv \u03d1\u03b8f = \u03d1F\u0302 \u03d1\u03b8f\n(37)\nwith:\n\u03d1F\u0302 \u03d1\u03b8f = const\u2212 \u03b2D 2 \u03d1\u03a80 \u03d1\u03b8f + \u03b2Tr ( \u03d1\u03a8T1 \u03d1\u03b8f Y Y T\u03a81A \u22121 )\n+ 1 2 Tr [ \u03d1KMM \u03d1\u03b8f ( DK\u22121MM \u2212 \u03b2 \u22121DA\u22121 \u2212A\u22121\u03a8T1 Y Y T\u03a81A\u22121 \u2212 \u03b2DK\u22121MM\u03a82K \u22121 MM )] + \u03b2\n2 Tr [ \u03d1\u03a82 \u03d1\u03b8f ( DK\u22121MM \u2212 \u03b2 \u22121DA\u22121 \u2212A\u22121\u03a8T1 Y Y T\u03a81A\u22121 )]\n(38)\nThe expression above is identical for the derivatives w.r.t the inducing points. For the gradients w.r.t the \u03b2 term, we have a similar expression:\n\u03d1F\u0302 \u03d1\u03b2 = 1 2 [ D ( Tr(K\u22121MM\u03a82) + (N \u2212M)\u03b2 \u22121 \u2212\u03a80 ) \u2212 Tr(Y Y >) + Tr(A\u22121\u03a8>1 Y Y >\u03a81)\n+\u03b2\u22122DTr(KMMA\u22121) + \u03b2\u22121Tr ( K\u22121MMA \u22121\u03a8>1 Y Y >\u03a81A \u22121) ] (39) In contrast to the above, the term F\u0302v does involve parameters \u03b8x, because it involves the variational parameters that are now reparametrized withKt, which in turn depends on \u03b8x. To demonstrate that, we will forget for a moment the reparametrization of Sq and we will express the bound as F (\u03b8x, \u00b5q(\u03b8x)) (where \u00b5q(\u03b8x) = Kt\u00b5\u0304q) so as to show explicitly the dependency on the variational mean which is now a function of \u03b8x. Our calcu-\nlations must now take into account the term ( \u03d1F\u0302(\u00b5q) \u03d1\u00b5q )> \u03d1\u00b5q(\u03b8x) \u03d1\u03b8x\nthat is what we \u201cmiss\u201d when we consider \u00b5q(\u03b8x) = \u00b5q:\n\u03d1Fv(\u03b8x, \u00b5q(\u03b8x)) \u03d1\u03b8x = \u03d1Fv(\u03b8x,\u00b5q) \u03d1\u03b8x + ( \u03d1F\u0302(\u00b5q) \u03d1\u00b5q )> \u03d1\u00b5q(\u03b8x) \u03d1\u03b8x\n= \u03d1F\u0302(\u00b5q) \u03d1\u03b8x + \u03d1(\u2212KL)(\u03b8x,\u00b5q(\u03b8x)) \u03d1\u03b8x + ( \u03d1F\u0302(\u00b5q) \u03d1\u00b5q )> \u03d1\u00b5q(\u03b8x) \u03d1\u03b8x (40)\nWe do the same for Sq and then we can take the resulting equations and replace \u00b5q and Sq with their equals so as to take the final expression which only contains \u00b5\u0304q and \u03bbq:\n\u03d1Fv(\u03b8x, \u00b5q(\u03b8x), Sq(\u03b8x)) \u03d1\u03b8x\n= Tr [[ \u2212 1\n2\n( B\u0302qKtB\u0302q + \u00b5\u0304q\u00b5\u0304 > q ) + ( I \u2212 B\u0302qKt ) diag ( \u03d1F\u0302 \u03d1sq )( I \u2212 B\u0302qKt )> ]\u03d1Kt \u03d1\u03b8x ]\n+ ( \u03d1F\u0302(\u00b5q) \u03d1\u00b5q )> \u03d1Kt \u03d1\u03b8x \u00b5\u0304q (41)\nwhere B\u0302q = \u039b 1 2 q B\u0303\u22121q \u039b 1 2 q . and B\u0303q = I + \u039b 1 2 q Kt\u039b 1 2 q . Note that by using this B\u0303q matrix (which has eigenvalues bounded below by one) we have an expression which, when implemented, leads to more numerically stable computations, as explained in [9] page 45-46."}, {"heading": "C Predictions", "text": "C.1 Predictions given only the test time points To approximate the predictive density, we will need to introduce the underlying latent function values F\u2217 \u2208 RN\u2217\u00d7D (the noisy-free version of Y\u2217) and the latent variables X\u2217 \u2208 RN\u2217\u00d7Q. We write the predictive density as\np(Y\u2217|Y ) = \u222b p(Y\u2217, F\u2217, X\u2217|Y\u2217, Y )dF\u2217dX\u2217 = \u222b p(Y\u2217|F\u2217)p(F\u2217|X\u2217, Y )p(X\u2217|Y )dF\u2217dX\u2217. (42)\nThe term p(F\u2217|X\u2217, Y ) is approximated according to q(F\u2217|X\u2217) = \u222b \u220f\nd\u2208D p(f\u2217,d|ud, X\u2217)q(ud)dud = \u220f d\u2208D q(f\u2217,d|X\u2217), (43)\nwhere q(f\u2217,d|X\u2217) is a Gaussian that can be computed analytically.The term p(X\u2217|Y ) in eq. (42) is approximated by a Gaussian variational distribution q(X\u2217),\np(X\u2217|Y ) \u2248 \u222b p(X\u2217|X)q(X)dX = \u3008p(X\u2217|X)\u3009q(X) = q(X\u2217) = Q\u220f q=1 q(x\u2217,q), (44)\nwhere p(X\u2217,q|X) can be found from the conditional GP prior (see [9]). We can then write\nx\u2217,q = \u03b1xq + , (45) where \u03b1 = K\u2217NK\u22121t and \u223c N ( 0,K\u2217\u2217 \u2212K\u2217NK\u22121t KN\u2217 ) . Also, Kt = kx(t, t), K\u2217N = kx(t\u2217, t) and K\u2217\u2217 = kx(t\u2217t\u2217). Given the above, we know a priori that (44) is a Gaussian and by taking expectations over q(X) in the r.h.s. of (45) we find the mean and covariance of q(X\u2217). Substituting for the equivalent forms of \u00b5q and Sq from section 2.2 we obtain the final solution\n\u00b5x\u2217,q = k\u2217N \u00b5\u0304q (46) var(x\u2217,q) = k\u2217\u2217 \u2212 k\u2217N (Kt + \u039b\u22121q )\u22121kN\u2217. (47)\n(42) can then be written as: p(Y\u2217|Y ) = \u222b p(Y\u2217|F\u2217)q(F\u2217|X\u2217)q(X\u2217)dF\u2217dX\u2217 = \u222b p(Y\u2217|F\u2217) \u3008q(F\u2217|X\u2217)\u3009q(X\u2217) dF\u2217 (48)\nAlthough the expectation appearing in the above integral is not a Gaussian, its moments can be found analytically [9, 13],\nE(F\u2217) = B>\u03a8\u22171 (49) Cov(F\u2217) = B> (\u03a8\u22172 \u2212\u03a8\u22171(\u03a8\u22171)>)B + \u03a8\u22170I \u2212 Tr [( K\u22121MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a8\u22172 ] I, (50)\nwhere B = \u03b2 (KMM + \u03b2\u03a82) \u22121 \u03a8>1 Y , \u03a8 \u2217 0 = \u3008kf (X\u2217, X\u2217)\u3009, \u03a8\u22171 = \u3008KM\u2217\u3009 and \u03a8\u22172 = \u3008KM\u2217K\u2217M \u3009. All expectations are taken w.r.t. q(X\u2217) and can be calculated analytically, while KM\u2217 denotes the cross-covariance matrix between the training inducing inputs X\u0303 and X\u2217. Finally, since Y\u2217 is just a noisy version of F\u2217, the mean and covariance of (48) is just computed as: E(Y\u2217) = E(F\u2217) and Cov(Y\u2217) = Cov(F\u2217) + \u03b2\u22121IN\u2217 .\nC.2 Predictions given the test time points and partially observed outputs The expression for the predictive density p(Y m\u2217 |Y p \u2217 , Y ) follows exactly as in section C.1 but we need to compute probabilities for Y m\u2217 instead of Y\u2217 and Y is replaced with (Y, Y p \u2217 ) in all conditioning sets. Similarly, F is replaced with Fm. Now q(X\u2217) cannot be found analytically as in section C.1; instead, it is optimised so that Y p\u2217 are taken into account. This is done by maximising the variational lower bound on the marginal likelihood:\np(Y p\u2217 , Y ) = \u222b p(Y p\u2217 , Y |X\u2217, X)p(X\u2217, X)dX\u2217dX\n= \u222b p(Y m|X)p(Y p\u2217 , Y p|X\u2217, X)p(X\u2217, X)dX\u2217dX,\nNotice that here, unlike the main paper, we work with the likelihood after marginalising F , for simplicity. Assuming a variational distribution q(X\u2217, X) and using Jensen\u2019s inequality we obtain the lower bound\u222b\nq(X\u2217, X) log p(Y m|X)p(Y p\u2217 , Y p|X\u2217, X)p(X\u2217, X)\nq(X\u2217, X) dX\u2217dX\n= \u222b q(X) log p(Y m|X)dX + \u222b q(X\u2217, X) log p(Y p \u2217 , Y p|X\u2217, X)dX\u2217dX\n\u2212 KL[q(X\u2217, X)||p(X\u2217, X)] (51)\nThis quantity can now be maximized in the same manner as for the bound of the training phase. Unfortunately, this means that the variational parameters that are already optimised from the training procedure cannot be used here because X and X\u2217 are coupled in q(X\u2217, X). A much faster but less accurate method would be to decouple the test from the training latent variables by imposing the factorisation q(X\u2217, X) = q(X)q(X\u2217). Then, equation (51) would break into terms containing X , X\u2217 or both. The ones containing only X could then be treated as constants.\nD Additional results from the experiments"}], "references": [{"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 1783\u20131816, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1816}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "In NIPS, p. 2004, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Gaussian process dynamical models", "author": ["J.M. Wang", "D.J. Fleet", "A. Hertzmann"], "venue": "In NIPS, pp. 1441\u2013 1448, MIT Press, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Gaussian process dynamical models for human motion", "author": ["J.M. Wang", "D.J. Fleet", "A. Hertzmann"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, pp. 283\u2013298, Feb. 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "In International Conference in Machine Learning, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models", "author": ["J. Ko", "D. Fox"], "venue": "Auton. Robots, vol. 27, pp. 75\u201390, July 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M. Titsias"], "venue": "JMLR W&CP, vol. 5, pp. 567\u2013574, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian Gaussian process latent variable model", "author": ["M. Titsias", "N.D. Lawrence"], "venue": "Journal of Machine Learning Research - Proceedings Track, vol. 9, pp. 844\u2013851, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Introduction to Gaussian processes", "author": ["D.J.C. MacKay"], "venue": "Neural Networks and Machine Learning (C. M. Bishop, ed.), NATO ASI Series, pp. 133\u2013166, Kluwer Academic Press, 1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Pattern Recognition and Machine Learning (Information", "author": ["C.M. Bishop"], "venue": "Science and Statistics). Springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "The variational Gaussian approximation revisited", "author": ["M. Opper", "C. Archambeau"], "venue": "Neural Computation, vol. 21, no. 3, pp. 786\u2013792, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaussian process priors with uncertain inputs - application to multiple-step ahead time series forecasting", "author": ["A. Girard", "C.E. Rasmussen", "J. Qui\u00f1onero-Candela", "R. Murray-Smith"], "venue": "Neural Information Processing Systems, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling human motion using binary latent variables", "author": ["G.W. Taylor", "G.E. Hinton", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems, p. 2007, MIT Press, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 2, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 3, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 4, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 5, "context": "Ko and Fox [6] further extend these models for fully Bayesian filtering in a robotics setting.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In this paper we build on recent developments in variational approximations for Gaussian processes [7, 8] to introduce a variational Gaussian process dynamical system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous lower bound on the marginal likelihood.", "startOffset": 99, "endOffset": 105}, {"referenceID": 7, "context": "In this paper we build on recent developments in variational approximations for Gaussian processes [7, 8] to introduce a variational Gaussian process dynamical system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous lower bound on the marginal likelihood.", "startOffset": 99, "endOffset": 105}, {"referenceID": 8, "context": "1 Instead we would like to infer them in a fully Bayesian non-parametric fashion using Gaussian processes [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "In our experiments, we will focus on the squared exponential covariance function (RBF), the Matern 3/2 which is only once differentiable, and a periodic covariance function [9, 10] which can be used when data exhibit strong periodicity.", "startOffset": 173, "endOffset": 180}, {"referenceID": 9, "context": "In our experiments, we will focus on the squared exponential covariance function (RBF), the Matern 3/2 which is only once differentiable, and a periodic covariance function [9, 10] which can be used when data exhibit strong periodicity.", "startOffset": 173, "endOffset": 180}, {"referenceID": 7, "context": "This, as in the variational Bayesian formulation of the GP-LVM [8], enables an automatic relevance determination procedure (ARD), i.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "[5, 3]) marginalise out only F and seek a MAP solution for X .", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[5, 3]) marginalise out only F and seek a MAP solution for X .", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "In the next section we describe how efficient variational approximations can be applied to marginalize X by extending the framework of [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "Following a standard procedure [11], we introduce a variational distribution q(\u0398) and compute the Jensen\u2019s lower bound Fv on the logarithm of (9),", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "As shown in [8], this intractability is removed by applying the \u201cdata augmentation\u201d principle.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "By dropping X\u0303 from our expressions, we write the augmented GP prior analytically (see [9]) as p(fd|ud, X) = N ( fd|KNMK MMud,KNN \u2212KNMK \u22121 MMKMN ) .", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "(12) A key result in [8] is that a tractable lower bound (computed analogously to (10)) can be obtained through the variational density", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Titsias and Lawrence [8] assume full independence for q(X) and the variational covariances are diagonal matrices.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Therefore, the first term in (14) has the same analytical solution as the one derived in [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 11, "context": "However, by reparametrizing our O ( N ) variational parameters according to the framework described in [12] we can obtain a set of O(N) less correlated variational parameters.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "where p(x\u2217,q|xq) is a Gaussian found from the conditional GP prior (see [9]) and q(X) is also Gaussian.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "However, following the same argument as in [9, 13], we can calculate analytically its mean and covariance: E(F\u2217) = B\u03a81 (22) Cov(F\u2217) = B> ( \u03a82 \u2212\u03a81(\u03a81) ) B + \u03a80I \u2212 Tr [( K\u22121 MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a82 ] I, (23)", "startOffset": 43, "endOffset": 50}, {"referenceID": 12, "context": "However, following the same argument as in [9, 13], we can calculate analytically its mean and covariance: E(F\u2217) = B\u03a81 (22) Cov(F\u2217) = B> ( \u03a82 \u2212\u03a81(\u03a81) ) B + \u03a80I \u2212 Tr [( K\u22121 MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a82 ] I, (23)", "startOffset": 43, "endOffset": 50}, {"referenceID": 13, "context": "1 Human Motion Capture Data We followed [14, 15] in considering motion capture data of walks and runs taken from subject 35 in the CMU motion capture database.", "startOffset": 40, "endOffset": 48}, {"referenceID": 13, "context": "We can also indirectly compare with the binary latent variable model (BLV) of [14] which used a slightly different data preprocessing.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "We assess the performance using the cumulative error per joint in the scaled space defined in [14] and by the root mean square error in the angle space suggested by [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "CL / CB are the leg and body datasets as preprocessed in [14], L and B the corresponding datasets from [15].", "startOffset": 57, "endOffset": 61}], "year": 2011, "abstractText": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.", "creator": "TeX"}}}