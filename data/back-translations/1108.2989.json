{"id": "1108.2989", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2011", "title": "A Theory of Multiclass Boosting", "abstract": "Boosting combines weak classifiers into high-precision predictors. Although the case of binary classification is well understood, multiclass setting lacks the \"right\" requirements for the weak classifier or the idea of the most efficient boosting algorithms. In this paper, we create a broad and general framework within which we specify and identify the optimal requirements for the weak classifier, and design the most effective ones in a sense, and promote algorithms that meet such requirements.", "histories": [["v1", "Mon, 15 Aug 2011 13:26:26 GMT  (2070kb,D)", "http://arxiv.org/abs/1108.2989v1", "A preliminary version appeared in NIPS 2010"]], "COMMENTS": "A preliminary version appeared in NIPS 2010", "reviews": [], "SUBJECTS": "stat.ML cs.AI", "authors": ["indraneel mukherjee", "robert e schapire"], "accepted": true, "id": "1108.2989"}, "pdf": {"name": "1108.2989.pdf", "metadata": {"source": "CRF", "title": "A Theory of Multiclass Boosting", "authors": ["Indraneel Mukherjee", "Robert E. Schapire"], "emails": ["imukherj@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "Keywords: Multiclass, boosting, weak learning condition, drifting games\n1. Introduction\nBoosting (Schapire and Freund, 2012) refers to a general technique of combining rules of thumb, or weak classifiers, to form highly accurate combined classifiers. Minimal demands are placed on the weak classifiers, so that a variety of learning algorithms, also called weak-learners, can be employed to discover these simple rules, making the algorithm widely applicable. The theory of boosting is well-developed for the case of binary classification. In particular, the exact requirements on the weak classifiers in this setting are known: any algorithm that predicts better than random on any distribution over the training set is said to satisfy the weak learning assumption. Further, boosting algorithms that minimize loss as efficiently as possible have been designed. Specifically, it is known that the Boost-bymajority (Freund, 1995) algorithm is optimal in a certain sense, and that AdaBoost (Freund and Schapire, 1997) is a practical approximation.\nSuch an understanding would be desirable in the multiclass setting as well, since many natural classification problems involve more than two labels, e.g. recognizing a digit from its image, natural language processing tasks such as part-of-speech tagging, and object recognition in vision. However, for such multiclass problems, a complete theoretical understanding of boosting is lacking. In particular, we do not know the \u201ccorrect\u201d way to define the requirements on the weak classifiers, nor has the notion of optimal boosting been explored in the multiclass setting.\nStraightforward extensions of the binary weak-learning condition to multiclass do not work. Requiring less error than random guessing on every distribution, as in the binary case, turns out to be too weak for boosting to be possible when there are more than two labels. On the other hand, requiring more than 50% accuracy even when the number of labels is much larger than two is too stringent, and simple weak classifiers like decision stumps fail\nar X\niv :1\n10 8.\n29 89\nv1 [\nst at\n.M L\n] 1\n5 A\nug 2\nto meet this criterion, even though they often can be combined to produce highly accurate classifiers (Freund and Schapire, 1996a). The most common approaches so far have relied on reductions to binary classification (Allwein et al., 2000), but it is hardly clear that the weak-learning conditions implicitly assumed by such reductions are the most appropriate.\nThe purpose of a weak-learning condition is to clarify the goal of the weak-learner, thus aiding in its design, while providing a specific minimal guarantee on performance that can be exploited by a boosting algorithm. These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers, which in turn can help prevent overfitting. Furthermore, boosting algorithms that more efficiently and effectively minimize training error may prevent underfitting, which can also be important.\nIn this paper, we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner. Unlike much, but not all, of the previous work on multiclass boosting, we focus specifically on the most natural, and perhaps weakest, case in which the weak classifiers are genuine classifiers in the sense of predicting a single multiclass label for each instance. Our new framework allows us to express a range of weak-learning conditions, both new ones and most of the ones that had previously been assumed (often only implicitly). Within this formalism, we can also now finally make precise what is meant by correct weak-learning conditions that are neither too weak nor too strong.\nWe focus particularly on a family of novel weak-learning conditions that have an especially appealing form: like the binary conditions, they require performance that is only slightly better than random guessing, though with respect to performance measures that are more general than ordinary classification error. We introduce a whole family of such conditions since there are many ways of randomly guessing on more than two labels, a key difference between the binary and multiclass settings. Although these conditions impose seemingly mild demands on the weak-learner, we show that each one of them is powerful enough to guarantee boostability, meaning that some combination of the weak classifiers has high accuracy. And while no individual member of the family is necessary for boostability, we also show that the entire family taken together is necessary in the sense that for every boostable learning problem, there exists one member of the family that is satisfied. Thus, we have identified a family of conditions which, as a whole, is necessary and sufficient for multiclass boosting. Moreover, we can combine the entire family into a single weak-learning condition that is necessary and sufficient by taking a kind of union, or logical or, of all the members. This combined condition can also be expressed in our framework.\nWith this understanding, we are able to characterize previously studied weak-learning conditions. In particular, the condition implicitly used by AdaBoost.MH (Schapire and Singer, 1999), which is based on a one-against-all reduction to binary, turns out to be strictly stronger than necessary for boostability. This also applies to AdaBoost.M1 (Freund and Schapire, 1996a), the most direct generalization of AdaBoost to multiclass, whose conditions can be shown to be equivalent to those of AdaBoost.MH in our setting. On the other hand, the condition implicit to the SAMME algorithm by Zhu et al. (2009) is too weak in the sense that even when the condition is satisfied, no boosting algorithm can guarantee to drive down the training error. Finally, the condition implicit to AdaBoost.MR (Schapire\nand Singer, 1999; Freund and Schapire, 1996a) (also called AdaBoost.M2) turns out to be exactly necessary and sufficient for boostability.\nEmploying proper weak-learning conditions is important, but we also need boosting algorithms that can exploit these conditions to effectively drive down error. For a given weak-learning condition, the boosting algorithm that drives down training error most efficiently in our framework can be understood as the optimal strategy for playing a certain two-player game. These games are non-trivial to analyze. However, using the powerful machinery of drifting games (Freund and Opper, 2002; Schapire, 2001), we are able to compute the optimal strategy for the games arising out of each weak-learning condition in the family described above. Compared to earlier work, our optimality results hold more generally and also achieve tighter bounds. These optimal strategies have a natural interpretation in terms of random walks, a phenomenon that has been observed in other settings (Abernethy et al., 2008; Freund, 1995).\nWe also analyze the optimal boosting strategy when using the minimal weak learning condition, and this poses additional challenges. Firstly, the minimal weak learning condition has multiple natural formulations \u2014 e.g., as the union of all the conditions in the family described above, or the formulation used in AdaBoost.MR \u2014 and each formulation leading to a different game specification. A priori, it is not clear which game would lead to the best strategy. We resolve this dilemma by proving that the optimal strategies arising out of different formulations of the same weak learning condition lead to algorithms that are essentially equally good, and therefore we are free to choose whichever formulation leads to an easier analysis without fear of suffering in performance. We choose the union of conditions formulation, since it leads to strategies that share the same interpretation in terms of random walks as before. However, even with this choice, the resulting games are hard to analyze, and although we can explicitly compute the optimum strategies in general, the computational complexity is usually exponential. Nevertheless, we identify key situations under which efficient computation is possible.\nThe game-theoretic strategies are non-adaptive in that they presume prior knowledge about the edge, that is, how much better than random are the weak classifiers. Algorithms that are adaptive, such as AdaBoost, are much more practical because they do not require such prior information. We show therefore how to derive an adaptive boosting algorithm by modifying the game-theoretic strategy based on the minimal condition. This algorithm enjoys a number of theoretical guarantees. Unlike some of the non-adaptive strategies, it is efficiently computable, and since it is based on the minimal weak learning condition, it makes minimal assumptions. In fact, whenever presented with a boostable learning problem, this algorithm can approach zero training error at an exponential rate. More importantly, the algorithm is effective even beyond the boostability framework. In particular, we show empirical consistency, i.e., the algorithm always converges to the minimum of a certain exponential loss over the training data, whether or not the dataset is boostable. Furthermore, using the results in (Mukherjee et al., 2011) we can show that this convergence occurs rapidly.\nOur focus in this paper is only on minimizing training error, which, for the algorithms we derive, provably decreases exponentially fast with the number of rounds of boosting under boostability assumptions. Such results can be used in turn to derive bounds on the generalization error using standard techniques that have been applied to other boosting\nalgorithms (Schapire et al., 1998; Freund and Schapire, 1997; Koltchinskii and Panchenko, 2002). Consistency in the multiclass classification setting has been studied by Tewari and Bartlett (2007) and has been shown to be trickier than binary classification consistency. Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset.\nWe present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting.\nPrevious work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997). Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 (Freund and Schapire, 1997), as well as AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). Other approaches include the work by Eibl and Pfeiffer (2005); Zhu et al. (2009). There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998). Two game-theoretic perspectives have been applied to boosting. The first one (Freund and Schapire, 1996b; Ra\u0308tsch and Warmuth, 2005) views the weak-learning condition as a minimax game, while drifting games (Schapire, 2001; Freund, 1995) were designed to analyze the most efficient boosting algorithms. These games have been further analyzed in the multiclass and continuous time setting in (Freund and Opper, 2002).\n2. Framework\nWe introduce some notation. Unless otherwise stated, matrices will be denoted by bold capital letters like M, and vectors by bold small letters like v. Entries of a matrix and vector will be denoted as M(i, j) or v(i), while M(i) will denote the ith row of a matrix. Inner product of two vectors u,v is denoted by \u3008u,v\u3009. The Frobenius inner product of two matrices Tr(MM\u2032) will be denoted by M \u2022M\u2032, where M\u2032 is the transpose of M. The indicator function is denoted by 1 [\u00b7]. The set of all distributions over the set {1, . . . , k} will be denoted by \u2206 {1, . . . , k}, and in general, the set of all distributions over any set S will be denoted by \u2206(S).\nIn multiclass classification, we want to predict the labels of examples lying in some set X. We are provided a training set of labeled examples {(x1, y1), . . . , (xm, ym)}, where each example xi \u2208 X has a label yi in the set {1, . . . , k}.\nBoosting combines several mildly powerful predictors, called weak classifiers, to form a highly accurate combined classifier, and has been previously applied for multiclass classification. In this paper, we only allow weak classifier that predict a single class for each example. This is appealing, since the combined classifier has the same form, although it\ndiffers from what has been used in much previous work. Later we will expand our framework to include multilabel weak classifiers, that may predict multiple labels per example.\nWe adopt a game-theoretic view of boosting. A game is played between two players, Booster and Weak-Learner, for a fixed number of rounds T . With binary labels, Booster outputs a distribution in each round, and Weak-Learner returns a weak classifier achieving more than 50% accuracy on that distribution. The multiclass game is an extension of the binary game. In particular, in each round t:\n\u2022 Booster creates a cost-matrix Ct \u2208 Rm\u00d7k, specifying to Weak-Learner that the cost of classifying example xi as l is Ct(i, l). The cost-matrix may not be arbitrary, but should conform to certain restrictions as discussed below.\n\u2022 Weak-Learner returns some weak classifier ht : X \u2192 {1, . . . , k} from a fixed space ht \u2208 H so that the cost incurred is\nCt \u2022 1ht = m\u2211 i=1 Ct(i, ht(xi)),\nis \u201csmall enough\u201d, according to some conditions discussed below. Here by 1h we mean the m\u00d7 k matrix whose (i, j)-th entry is 1 [h(i) = j].\n\u2022 Booster computes a weight \u03b1t for the current weak classifier based on how much cost was incurred in this round.\nAt the end, Booster predicts according to the weighted plurality vote of the classifiers returned in each round:\nH(x) M = argmax\nl\u2208{1,...,k} fT (x, l), where fT (x, l)\nM = T\u2211 t=1 1 [ht(x) = l]\u03b1t. (1)\nBy carefully choosing the cost matrices in each round, Booster aims to minimize the training error of the final classifer H, even when Weak-Learner is adversarial. The restrictions on cost-matrices created by Booster, and the maximum cost Weak-Learner can suffer in each round, together define the weak-learning condition being used. For binary labels, the traditional weak-learning condition states: for any non-negative weights w(1), . . . , w(m) on the training set, the error of the weak classfier returned is at most (1/2\u2212 \u03b3/2) \u2211 iwi. Here \u03b3 parametrizes the condition. There are many ways to translate this condition into our language. The one with fewest restrictions on the cost-matrices requires labeling correctly should be less costly than labeling incorrectly:\n\u2200i : C(i, yi) \u2264 C(i, y\u0304i) (here y\u0304i 6= yi is the other binary label),\nwhile the restriction on the returned weak classifier h requires less cost than predicting randomly: \u2211\ni C(i, h(xi)) \u2264 \u2211 i {( 1 2 \u2212 \u03b3 2 ) C(i, y\u0304i) + ( 1 2 + \u03b3 2 ) C(i, yi) } .\nBy the correspondence w(i) = C(i, y\u0304i)\u2212 C(i, yi), we may verify the two conditions are the same.\nWe will rewrite this condition after making some simplifying assumptions. Henceforth, without loss of generality, we assume that the true label is always 1. Let Cbin \u2286 Rm\u00d72 consist of matrices C which satisfy C(i, 1) \u2264 C(i, 2). Further, let Ubin\u03b3 \u2208 Rm\u00d72 be the matrix whose each row is (1/2 + \u03b3/2, 1/2\u2212 \u03b3/2). Then, Weak-Learner searching space H satisfies the binary weak-learning condition if: \u2200C \u2208 Cbin, \u2203h \u2208 H : C \u2022 ( 1h \u2212Ubin\u03b3 ) \u2264 0. There are two main benefits to this reformulation. With linear homogeneous constraints, the mathematics is simplified, as will be apparent later. More importantly, by varying the restrictions Cbin on the cost vectors and the matrix Ubin, we can generate a vast variety of weak-learning conditions for the multiclass setting k \u2265 2 as we now show.\nLet C \u2286 Rm\u00d7k and let B \u2208 Rm\u00d7k be a matrix which we call the baseline. We say a weak classifier space H satisfies the condition (C,B) if\n\u2200C \u2208 C, \u2203h \u2208 H : C \u2022 (1h \u2212B) \u2264 0, i.e., m\u2211 i=1 C(i, h(i)) \u2264 m\u2211 i=1 \u3008C(i),B(i)\u3009 . (2)\nIn (2), the variable matrix C specifies how costly each misclassification is, while the baseline B specifies a weight for each misclassification. The condition therefore states that a weak classifier should not exceed the average cost when weighted according to baseline B. This large class of weak-learning conditions captures many previously used conditions, such as the ones used by AdaBoost.M1 (Freund and Schapire, 1996a), AdaBoost.MH (Schapire and Singer, 1999) and AdaBoost.MR (Freund and Schapire, 1996a; Schapire and Singer, 1999) (see below), as well as novel conditions introduced in the next section.\nBy studying this vast class of weak-learning conditions, we hope to find the one that will serve the main purpose of the boosting game: finding a convex combination of weak classifiers that has zero training error. For this to be possible, at the minimum the weak classifiers should be sufficiently rich for such a perfect combination to exist. Formally, a collection H of weak classifiers is boostable if it is eligible for boosting in the sense that there exists a distribution \u03bb on this space that linearly separates the data: \u2200i : argmaxl\u2208{1,...,k} \u2211 h\u2208H \u03bb(h)1 [h(xi) = l] = yi. The weak-learning condition plays two roles. It rejects spaces that are not boostable, and provides an algorithmic means of searching for the right combination. Ideally, the second factor will not cause the weak-learning condition to impose additional restrictions on the weak classifiers; in that case, the weaklearning condition is merely a reformulation of being boostable that is more appropriate for deriving an algorithm. In general, it could be too strong, i.e. certain boostable spaces will fail to satisfy the conditions. Or it could be too weak i.e., non-boostable spaces might satisfy such a condition. Booster strategies relying on either of these conditions will fail to drive down error, the former due to underfitting, and the latter due to overfitting. Later we will describe conditions captured by our framework that avoid being too weak or too strong. But before that, we show in the next section how our flexible framework captures weak learning conditions that have appeared previously in the literature.\n3. Old conditions\nIn this section, we rewrite, in the language of our framework, the weak learning conditions explicitly or implicitly employed in the multiclass boosting algorithms SAMME (Zhu et al., 2009), AdaBoost.M1 (Freund and Schapire, 1996a), and AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). This will be useful later on for comparing the strengths and weaknesses of the various conditions. We will end this section with a curious equivalence between the conditions of AdaBoost.MH and AdaBoost.M1.\nRecall that we have assumed the correct label is 1 for every example. Nevertheless, we continue to use yi to denote the correct label in this section.\n3.1 Old conditions in the new framework\nHere we restate, in the language of our new framework, the weak learning conditions of four algorithms that have earlier appeared in the literature.\nSAMME. The SAMME algorithm (Zhu et al., 2009) requires less error than random guessing on any distribution on the examples. Formally, a space H satisfies the condition if there is a \u03b3\u2032 > 0 such that,\n\u2200d(1), . . . , d(m) \u2265 0, \u2203h \u2208 H : m\u2211 i=1 d(i)1 [h(xi) 6= yi] \u2264 (1\u2212 1/k \u2212 \u03b3\u2032) m\u2211 i=1 d(i). (3)\nDefine a cost matrix C whose entries are given by\nC(i, j) = { d(i) if j 6= yi, 0 if j = yi.\nThen the left hand side of (3) can be written as\nm\u2211 i=1 C(i, h(xi)) = C \u2022 1h.\nNext let \u03b3 = (1\u2212 1/k)\u03b3\u2032 and define baseline U\u03b3 to be the multiclass extension of Ubin,\nU\u03b3(i, l) =\n{ (1\u2212\u03b3) k + \u03b3 if l = yi,\n(1\u2212\u03b3) k if l 6= yi.\nThen the right hand side of (3) can be written as\nm\u2211 i=1 \u2211 l 6=yi C(i, l)U\u03b3(i, l) = C \u2022U\u03b3 ,\nsince C(i, yi) = 0 for every example i. Define CSAM to be the following collection of cost matrices:\nCSAM M= { C : C(i, l) = { 0 if l = yi,\nti if l 6= yi, for non-negative t1, . . . , tm.\n}\nUsing the last two equations, (3) is equivalent to \u2200C \u2208 CSAM, \u2203h \u2208 H : C \u2022 ( 1h \u2212U\u03b3 ) \u2264 0.\nTherefore, the weak-learning condition of SAMME is given by (CSAM,U\u03b3).\nAdaBoost.M1 Adaboost.M1 (Freund and Schapire, 1997) measures the performance of weak classifiers using ordinary error. It requires 1/2 + \u03b3/2 accuracy with respect to any non-negative weights d(1), . . . , d(m) on the training set:\nm\u2211 i=1 d(i)1 [h(xi) 6= yi] \u2264 (1/2\u2212 \u03b3/2) m\u2211 i=1 d(i), (4)\ni.e. m\u2211 i=1 d(i)Jh(xi) 6= yiK \u2264 \u2212\u03b3 m\u2211 i=1 d(i).\nwhere J\u00b7K is the \u00b11 indicator function, taking value +1 when its argument is true, and \u22121 when false. Using the transformation\nC(i, l) = Jl 6= yiKd(i) (5)\nwe may rewrite (5) as\n\u2200C \u2208 Rm\u00d7k satisfying 0 \u2264 \u2212C(i, yi) = C(i, l) for l 6= yi, (6)\n\u2203h \u2208 H : m\u2211 i=1 C(i, h(xi)) \u2264 \u03b3 m\u2211 i=1 C(i, yi)\ni.e. \u2200C \u2208 CM1, \u2203h \u2208 H : C \u2022 ( 1h \u2212BM1\u03b3 ) \u2264 0, (7)\nwhere BM1\u03b3 (i, l) = \u03b31 [l = yi], and CM1 \u2286 Rm\u00d7k consists of matrices satisfying the constraints in (6).\nAdaBoost.MH AdaBoost.MH (Schapire and Singer, 1999) is a popular multiclass boosting algorithm that is based on the one-against-all reduction, and was originally designed to use weak-hypotheses that return a prediction for every example and every label. The implicit weak learning condition requires that for any matrix with non-negative entries d(i, l), the weak-hypothesis should achieve 1/2 + \u03b3 accuracy\nm\u2211 i=1 1 [h(xi) 6= yi] d(i, yi) +\u2211 l 6=yi 1 [h(xi) = l] d(i, l)  \u2264 ( 1 2 \u2212 \u03b3 2 ) m\u2211 i=1 k\u2211 l=1 d(i, l).\n(8)\nThis can be rewritten as\nm\u2211 i=1 \u22121 [h(xi) = yi] d(i, yi) +\u2211 l 6=yi 1 [h(xi) = l] d(i, l)  \u2264\nm\u2211 i=1  ( 1 2 \u2212 \u03b3 2 )\u2211 l 6=yi d(i, l)\u2212 ( 1 2 + \u03b3 2 ) d(i, yi)  .\nUsing the mapping\nC(i, l) = { d(i, l) if l 6= yi \u2212d(i, l) if l = yi,\ntheir weak-learning condition may be rewritten as follows\n\u2200C \u2208 Rm\u00d7k satisfying C(i, yi) \u2264 0, C(i, l) \u2265 0 for l 6= yi, (9) \u2203h \u2208 H : m\u2211 i=1 C(i, h(xi)) \u2264 m\u2211 i=1  ( 1 2 + \u03b3 2 ) C(i, yi) + ( 1 2 \u2212 \u03b3 2 )\u2211 l 6=yi C(i, l)  . (10) Defining CMH to be the space of all cost matrices satisfying the constraints in (9), the above condition is the same as\n\u2200C \u2208 CMH, \u2203h \u2208 H : C \u2022 ( 1h \u2212BMH\u03b3 ) \u2264 0,\nwhere BMH\u03b3 (i, l) = (1/2 + \u03b3Jl = yiK/2). AdaBoost.MR AdaBoost.MR (Schapire and Singer, 1999) is based on the all-pairs multiclass to binary reduction. Like AdaBoost.MH, it was originally designed to use weakhypotheses that return a prediction for every example and every label. The weak learning condition for AdaBoost.MR requires that for any non-negative cost-vectors {d(i, l)}l 6=yi , the weak-hypothesis returned should satisfy the following:\nm\u2211 i=1 \u2211 l 6=yi (1 [h(xi) = l]\u2212 1 [h(xi) = yi]) d(i, l) \u2264 \u2212\u03b3 m\u2211 i=1 \u2211 l 6=yi d(i, l)\ni.e. m\u2211 i=1 \u22121 [h(xi) = yi]\u2211 l 6=yi d(i, l) + \u2211 l 6=yi 1 [h(xi) = l] d(i, l)  \u2264 \u2212\u03b3 m\u2211 i=1 \u2211 l 6=yi d(i, l).\nSubstituting\nC(i, l) = { d(i, l) l 6= yi \u2212 \u2211\nl 6=yi d(i, l) l = yi,\nwe may rewrite AdaBoost.MR\u2019s weak-learning condition as \u2200C \u2208 Rm\u00d7k satisfying C(i, l) \u2265 0 for l 6= yi, C(i, yi) = \u2212 \u2211 l 6=yi C(i, l), (11)\n\u2203h \u2208 H : m\u2211 i=1 C(i, h(xi)) \u2264 \u2212 \u03b3 2 m\u2211 i=1 \u2212C(i, yi) +\u2211 l 6=yi C(i, l)  . Defining CMR to be the collection of cost matrices satisfying the constraints in (11), the above condition is the same as\n\u2200C \u2208 CMR,\u2203h \u2208 H : C \u2022 ( 1h \u2212BMR\u03b3 ) \u2264 0,\nwhere BMR\u03b3 (i, l) = Jl = yiK\u03b3/2.\n3.2 A curious equivalence\nWe show that the weak learning conditions of AdaBoost.MH and AdaBoost.M1 are identical in our framework. This is surprising because the original motivations behind these algorithms were completely different. AdaBoost.M1 is a direct extension of binary AdaBoost to the multiclass setting, whereas AdaBoost.MH is based on the one-against-all multiclass to binary reduction. This equivalence is a sort of degeneracy, and arises because the weak classifiers being used predict single labels per example. With multilabel weak classifiers, for which AdaBoost.MH was originally designed, the equivalence no longer holds.\nThe proofs in this and later sections will make use of the following minimax result, that is a weaker version of Corollary 37.3.2 of (Rockafellar, 1970).\nTheorem 1 (Minimax Theorem) Let C,D be non-empty closed convex subsets of Rm,Rn respectively, and let K be a linear function on C \u00d7D. If either C or D is bounded, then\nmin v\u2208D max u\u2208C K(u, v) = max u\u2208C min v\u2208D K(u, v).\nLemma 2 A weak classifier space H satisfies (CM1,BM1\u03b3 ) if and only if it satisfies (CMH,BMH\u03b3 ).\nProof We will refer to (CM1,BM1\u03b3 ) by M1 and (CMH,BMH\u03b3 ) by MH for brevity. The proof is in three steps.\nStep (i): If H satisfies MH, then it also satisfies M1. This follows since any constraint (4) imposed by M1 on H can be reproduced by MH by plugging the following values of d(i, l) in (8)\nd(i, l) =\n{ d(i) if l = yi\n0 if l 6= yi.\nStep (ii): If H satisfies M1, then there is a convex combination H\u03bb\u2217 of the matrices 1h \u2208 H, defined as\nH\u03bb\u2217 M = \u2211 h\u2208H \u03bb\u2217(h)1h,\nsuch that\n\u2200i : ( H\u03bb\u2217 \u2212BMH\u03b3 ) (i, l) { \u2265 0 if l = yi \u2264 0 if l 6= yi.\n(12)\nIndeed, Theorem 1 yields\nmin \u03bb\u2208\u2206(H) max C\u2208CM1\nC \u2022 ( H\u03bb \u2212BM1\u03b3 ) = max\nC\u2208CM1 min h\u2208H\nC \u2022 ( 1h \u2212BM1\u03b3 ) \u2264 0, (13)\nwhere the inequality is a restatement of our assumption that H satisfies M1. If \u03bb\u2217 is a minimizer of the minmax expression, then H\u03bb\u2217 must satisfy\n\u2200i : H\u03bb\u2217(i, l)\n{ \u2265 12 + \u03b3 2 if l = yi\n\u2264 12 \u2212 \u03b3 2 if l 6= yi,\n(14)\nor else some choice of C \u2208 CM1 can cause C \u2022 ( H\u03bb\u2217 \u2212BM1 ) to exceed 0. In particular, if H\u03bb\u2217(i0, l) < 1/2 + \u03b3/2, then( H\u03bb\u2217 \u2212BM1\u03b3 ) (i0, yi0) <\n\u2211 l 6=yi0 ( H\u03bb\u2217 \u2212BM1\u03b3 ) (i0, l).\nNow, if we choose C \u2208 CM1 as\nC(i, l) =  0 if i 6= i0 1 if i = i0, l 6= yi0 \u22121 if i = i0, l = yi0 ,\nthen, C \u2022 ( H\u03bb\u2217 \u2212BM1\u03b3 ) = \u2212 ( H\u03bb\u2217 \u2212BM1\u03b3 ) (i0, yi0) + \u2211 l 6=yi0 ( H\u03bb\u2217 \u2212BM1\u03b3 ) (i0, l) > 0,\ncontradicting the inequality in (13). Therefore (14) holds. Eqn. (12), and thus Step (ii), now follows by observing that BMH\u03b3 , by definition, satisfies\n\u2200i : BMH\u03b3 (i, l) =\n{ 1 2 + \u03b3 2 if l = yi\n1 2 \u2212 \u03b3 2 if l 6= yi.\nStep (iii) If there is some convex combination H\u03bb\u2217 satisfying (12), then H satisfies MH. Recall that BMH consists of entries that are non-positive on the correct labels and non-negative for incorrect labels. Therefore, (12) implies\n0 \u2265 max C\u2208CMH\nC \u2022 ( H\u03bb\u2217 \u2212BMH\u03b3 ) \u2265 min\n\u03bb\u2208\u2206(H) max C\u2208CMH C \u2022\n( H\u03bb \u2212BMH\u03b3 ) .\nOn the other hand, using Theorem 1 we have\nmin \u03bb\u2208\u2206(H) max C\u2208CMH\nC \u2022 ( H\u03bb \u2212BMH\u03b3 ) = max\nC\u2208CMH min h\u2208H\nC \u2022 ( 1h \u2212BMH\u03b3 ) .\nCombining the two, we get\n0 \u2265 max C\u2208CMH min h\u2208H\nC \u2022 ( 1h \u2212BMH\u03b3 ) ,\nwhich is the same as saying that H satisfies MH\u2019s condition. Steps (ii) and (iii) together imply that if H satisfies M1, then it also satisfies MH. Along with Step (i), this concludes the proof.\n4. Necessary and sufficient weak-learning conditions\nThe binary weak-learning condition has an appealing form: for any distribution over the examples, the weak classifier needs to achieve error not greater than that of a random player who guesses the correct answer with probability 1/2+\u03b3/2. Further, this is the weakest condition under which boosting is possible as follows from a game-theoretic perspective (Freund and Schapire, 1996b; Ra\u0308tsch and Warmuth, 2005) . Multiclass weak-learning conditions with similar properties are missing in the literature. In this section we show how our framework captures such conditions.\n4.1 Edge-over-random conditions\nIn the multiclass setting, we model a random player as a baseline predictor B \u2208 Rm\u00d7k whose rows are distributions over the labels, B(i) \u2208 \u2206 {1, . . . , k}. The prediction on example i is a sample from B(i). We only consider the space of edge-over-random baselines Beor\u03b3 \u2286 Rm\u00d7k who have a faint clue about the correct answer. More precisely, any baseline B \u2208 Beor\u03b3 in this space is \u03b3 more likely to predict the correct label than an incorrect one on every example i: \u2200l 6= 1, B(i, 1) \u2265 B(i, l) + \u03b3, with equality holding for some l, i.e.:\nB(i, 1) = max {B(i, l) + \u03b3 : l 6= 1} .\nNotice that the edge-over-random baselines are different from the baselines used by earlier weak learning conditions discussed in the previous section.\nWhen k = 2, the space Beor\u03b3 consists of the unique player Ubin\u03b3 , and the binary weaklearning condition is given by (Cbin,Ubin\u03b3 ). The new conditions generalize this to k > 2. In particular, define Ceor to be the multiclass extension of Cbin: any cost-matrix in Ceor should put the least cost on the correct label, i.e., the rows of the cost-matrices should come from the set { c \u2208 Rk : \u2200l, c(1) \u2264 c(l) } . Then, for every baseline B \u2208 Beor\u03b3 , we introduce the condition (Ceor,B), which we call an edge-over-random weak-learning condition. Since C\u2022B is the expected cost of the edge-over-random baseline B on matrix C, the constraints (2) imposed by the new condition essentially require better than random performance.\nAlso recall that we have assumed that the true label yi of example i in our training set is always 1. Nevertheless, we may occasionally continue to refer to the true labels as yi.\nWe now present the central results of this section. The seemingly mild edge-over-random conditions guarantee boostability, meaning weak classifiers that satisfy any one such condition can be combined to form a highly accurate combined classifier.\nTheorem 3 (Sufficiency) If a weak classifier space H satisfies a weak-learning condition (Ceor,B), for some B \u2208 Beor\u03b3 , then H is boostable.\nProof The proof is in the spirit of the ones in (Freund and Schapire, 1996b). Applying Theorem 1 yields\n0 \u2265 max C\u2208Ceor min h\u2208H C \u2022 (1h \u2212B) = min \u03bb\u2208\u2206(H) max C\u2208Ceor C \u2022 (H\u03bb \u2212B) ,\nwhere the first inequality follows from the definition (2) of the weak-learning condition. Let \u03bb\u2217 be a minimizer of the min-max expression. Unless the first entry of each row of (H\u03bb\u2217 \u2212B) is the largest, the right hand side of the min-max expression can be made arbitrarily large by choosing C \u2208 Ceor appropriately. For example, if in some row i, the jth0 element is strictly larger than the first element, by choosing\nC(i, j) =  \u22121 if j = 1 1 if j = j0\n0 otherwise,\nwe get a matrix in Ceor which causes C \u2022 (H\u03bb\u2217 \u2212B) to be equal to C(i, j0) \u2212 C(i, 1) > 0, an impossibility by the first inequality.\nTherefore, the convex combination of the weak classifiers, obtained by choosing each weak classifier with weight given by \u03bb\u2217, perfectly classifies the training data, in fact with a margin \u03b3.\nOn the other hand, the family of such conditions, taken as a whole, is necessary for boostability in the sense that every eligible space of weak classifiers satisfies some edge-over-random condition.\nTheorem 4 (Relaxed necessity) For every boostable weak classifier space H, there exists a \u03b3 > 0 and B \u2208 Beor\u03b3 such that H satisfies the weak-learning condition (Ceor,B).\nProof The proof shows existence through non-constructive averaging arguments. We will reuse notation from the proof of Theorem 3 above. H is boostable implies there exists some distribution \u03bb\u2217 \u2208 \u2206(H) such that\n\u2200j 6= 1, i : H\u03bb\u2217(i, 1)\u2212H\u03bb\u2217(i, j) > 0.\nLet \u03b3 > 0 be the minimum of the above expression over all possible (i, j), and let B = H\u03bb\u2217 . Then B \u2208 Beor\u03b3 , and\nmax C\u2208Ceor min h\u2208H C \u2022 (1h \u2212B) \u2264 min \u03bb\u2208\u2206(H) max C\u2208Ceor C \u2022 (H\u03bb \u2212B) \u2264 max C\u2208Ceor C \u2022 (H\u03bb\u2217 \u2212B) = 0,\nwhere the equality follows since by definition H\u03bb\u2217 \u2212B = 0. The max-min expression is at most zero is another way of saying that H satisfies the weak-learning condition (Ceor,B) as in (2).\nTheorem 4 states that any boostable weak classifier space will satisfy some condition in our family, but it does not help us choose the right condition. Experiments in Section 10 suggest ( Ceor,U\u03b3 ) is effective with very simple weak-learners compared to popular boosting algorithms. (Recall U\u03b3 \u2208 Beor\u03b3 is the edge-over-random baseline closest to uniform; it has weight (1\u2212\u03b3)/k on incorrect labels and (1\u2212\u03b3)/k+\u03b3 on the correct label.) However, there are theoretical examples showing each condition in our family is too strong.\nTheorem 5 For any B \u2208 Beor\u03b3 , there exists a boostable space H that fails to satisfy the condition (Ceor,B).\nProof We provide, for any \u03b3 > 0 and edge-over-random baseline B \u2208 Beor\u03b3 , a dataset and weak classifier space that is boostable but fails to satisfy the condition (Ceor,B).\nPick \u03b3\u2032 = \u03b3/k and set m > 1/\u03b3\u2032 so that bm(1/2 + \u03b3\u2032)c > m/2. Our dataset will have m labeled examples {(0, y0), . . . , (m\u2212 1, ym\u22121)}, and m weak classifiers. We want the following symmetries in our weak classifiers:\n\u2022 Each weak classifier correctly classifies bm(1/2 + \u03b3\u2032)c examples and misclassifies the rest.\n\u2022 On each example, bm(1/2 + \u03b3\u2032)c weak classifiers predict correctly.\nNote the second property implies boostability, since the uniform convex combination of all the weak classifiers is a perfect predictor.\nThe two properties can be satisfied by the following design. A window is a contiguous sequence of examples that may wrap around; for example\n{i, (i+ 1) mod m, . . . , (i+ k) mod m}\nis a window containing k elements, which may wrap around if i+ k \u2265 m. For each window of length bm(1/2 + \u03b3\u2032)c create a hypothesis that correctly classifies within the window, and misclassifies outside. This weak-hypothesis space has size m, and has the required properties.\nWe still have flexibility as to how the misclassifications occur, and which cost-matrix to use, which brings us to the next two choices:\n\u2022 Whenever a hypothesis misclassifies on example i, it predicts label\ny\u0302i M = argmin {B(i, l) : l 6= yi} . (15)\n\u2022 A cost-matrix is chosen so that the cost of predicting y\u0302i on example i is 1, but for any other prediction the cost is zero. Observe this cost-matrix belongs to Ceor.\nTherefore, every time a weak classifier predicts incorrectly, it also suffers cost 1. Since each weak classifier predicts correctly only within a window of length bm(1/2 + \u03b3\u2032)c, it suffers cost dm(1/2\u2212 \u03b3\u2032)e. On the other hand, by the choice of y\u0302i in (15),\nB(i, y\u0302i) = min {B(i, 1)\u2212 \u03b3,B(i, 2), . . . , B(i, k)}\n\u2264 1 k {B(i, 1)\u2212 \u03b3 +B(i, 2) +B(i, 3) + . . .+B(i, k)} = 1/k \u2212 \u03b3/k.\nSo the cost of B on the chosen cost-matrix is at most m(1/k\u2212 \u03b3/k), which is less than the cost dm(1/2\u2212 \u03b3\u2032)e \u2265 m(1/2\u2212 \u03b3/k) of any weak classifier whenever the number of labels k is more than two. Hence our boostable space of weak classifiers fails to satisfy (Ceor,B).\nTheorems 4 and 5 can be interpreted as follows. While a boostable space will satisfy some edge-over-random condition, without further information about the dataset it is not possible to know which particular condition will be satisfied. The kind of prior knowledge required to make this guess correctly is provided by Theorem 3: the appropriate weak learning condition is determined by the distribution of votes on the labels for each example that a target weak classifier combination might be able to get. Even with domain expertise, such knowledge may or may not be obtainable in practice before running boosting. We therefore need conditions that assume less.\n4.2 The minimal weak learning condition\nA perhaps extreme way of weakening the condition is by requiring the performance on a cost matrix to be competitive not with a fixed baseline B \u2208 Beor\u03b3 , but with the worst of them:\n\u2200C \u2208 Ceor,\u2203h \u2208 H : C \u2022 1h \u2264 max B\u2208Beor\u03b3 C \u2022B. (16)\nCondition (16) states that during the course of the same boosting game, Weak-Learner may choose to beat any edge-over-random baseline B \u2208 Beor\u03b3 , possibly a different one for every round and every cost-matrix. This may superficially seem much too weak. On the contrary, this condition turns out to be equivalent to boostability. In other words, according to our criterion, it is neither too weak nor too strong as a weak-learning condition. However, unlike the edge-over-random conditions, it also turns out to be more difficult to work with algorithmically.\nFurthermore, this condition can be shown to be equivalent to the one used by AdaBoost.MR (Schapire and Singer, 1999; Freund and Schapire, 1996a). This is perhaps remarkable since the latter is based on the apparently completely unrelated all-pairs multiclass to binary reduction. In Section 3 we saw that the MR condition is given by (CMR,BMR\u03b3 ), where CMR consists of cost-matrices that put non-negative costs on incorrect labels and whose rows sum up to zero, while BMR\u03b3 \u2208 Rm\u00d7k is the matrix that has \u03b3 on the first column and \u2212\u03b3 on all other columns. Further, the MR condition, and hence (16), can be shown to be neither too weak nor too strong.\nTheorem 6 (MR) A weak classifier space H satisfies AdaBoost.MR\u2019s weak-learning condition (CMR,BMR\u03b3 ) if and only if it satisfies (16). Moreover, this condition is equivalent to being boostable.\nProof We will show the following three conditions are equivalent:\n(A) H is boostable\n(B) \u2203\u03b3 > 0 such that \u2200C \u2208 Ceor, \u2203h \u2208 H : C \u2022 1h \u2264 max B\u2208Beor\u03b3 C \u2022B\n(C) \u2203\u03b3 > 0 such that \u2200C \u2208 CMR, \u2203h \u2208 H : C \u2022 1h \u2264 C \u2022BMR.\nWe will show (A) implies (B), (B) implies (C), and (C) implies (A) to achieve the above. (A) implies (B): Immediate from Theorem 2.\n(B) implies (C): Suppose (B) is satisfied with 2\u03b3. We will show that this implies H satisfies (CMR,BMR\u03b3 ). Notice CMR \u2282 Ceor. Therefore it suffices to show that\n\u2200C \u2208 CMR,B \u2208 Beor2\u03b3 : C \u2022 ( B\u2212BMR\u03b3 ) \u2264 0.\nNotice that B \u2208 Beor2\u03b3 implies B\u2032 = B\u2212BMR\u03b3 is a matrix whose largest entry in each row is in the first column of that row. Then, for any C \u2208 CMR, C \u2022B\u2032 can be written as\nC \u2022B\u2032 = m\u2211 i=1 k\u2211 j=2 C(i, j) ( B\u2032(i, j)\u2212B\u2032(i, 1) ) .\nSince C(i, j) \u2265 0 for j > 1, and B\u2032(i, j)\u2212B\u2032(i, 1) \u2264 0, we have our result. (C) implies (A): Applying Theorem 1\n0 \u2265 max C\u2208CMR min h\u2208H\nC \u2022 ( 1h \u2212BMR\u03b3 ) = min\n\u03bb\u2208\u2206(H) max C\u2208CMR C \u2022\n( H\u03bb \u2212BMR\u03b3 ) .\nFor any i0 and l0 6= 1, the following cost-matrix C satisfies C \u2208 CMR,\nC(i, l) =  0 if i 6= i0 or l 6\u2208 {1, l0} 1 if i = i0, l = l0\n\u22121 if i = i0, l = 1.\nLet \u03bb belong to the argmin of the min max expression. Then C \u2022 ( H\u03bb \u2212BMR\u03b3 ) \u2264 0 implies H\u03bb(i0, 1) \u2212H\u03bb(i0, l0) \u2265 2\u03b3. Since this is true for all i0 and l0 6= 1, we conclude that the (CMR,BMR\u03b3 ) condition implies boostability.\nThis concludes the proof of equivalence.\nNext, we illustrate the strengths of our minimal weak-learning condition through concrete comparisons with previous algorithms.\nComparison with SAMME. The SAMME algorithm of Zhu et al. (2009) requires the weak classifiers to achieve less error than uniform random guessing for multiple labels; in our language, their weak-learning condition is (CSAM,U\u03b3), as shown in Section 3, where CSAM consists of cost matrices whose rows are of the form (0, t, t, . . .) for some non-negative t. As is well-known, this condition is not sufficient for boosting to be possible. In particular, consider the dataset {(a, 1), (b, 2)} with k = 3,m = 2, and a weak classifier space consisting of h1, h2 which always predict 1, 2, respectively (Figure 1). Since neither classifier distinguishes between a, b we cannot achieve perfect accuracy by combining them in any way. Yet, due to the constraints on the cost-matrix, one of h1, h2 will always manage non-positive cost while random always suffers positive cost. On the other hand our weak-learning condition allows the Booster to choose far richer cost matrices. In particular, when the cost matrix C \u2208 Ceor is given by\n1 2 3\na \u22121 +1 0 b +1 \u22121 0,\nboth classifiers in the above example suffer more loss than the random player U\u03b3 , and fail to satisfy our condition.\nComparison with AdaBoost.MH. AdaBoost.MH (Schapire and Singer, 1999) was designed for use with weak hypotheses that on each example return a prediction for every label. When used in our framework, where the weak classifiers return only a single multiclass prediction per example, the implicit demands made by AdaBoost.MH on the weak classifier space turn out to be too strong. To demonstrate this, we construct a classifier space that satisfies the condition (Ceor,U\u03b3) in our family, but cannot satisfy AdaBoost.MH\u2019s\nweak-learning condition. Note that this does not imply that the conditions are too strong when used with more powerful weak classifiers that return multilabel multiclass predictions.\nConsider a space H that has, for every (1/k + \u03b3)m element subset of the examples, a classifier that predicts correctly on exactly those elements. The expected loss of a randomly chosen classifier from this space is the same as that of the random player U\u03b3 . Hence H satisfies this weak-learning condition. On the other hand, it was shown in Section 3 that AdaBoost.MH\u2019s weak-learning condition is the pair (CMH,BMH\u03b3 ), where CMH consists of cost matrices with non-negative entries on incorrect labels and non-positive entries on real labels, and where each row of the matrix BMH\u03b3 is the vector (1/2 + \u03b3/2, 1/2\u2212 \u03b3/2, . . . , 1/2\u2212 \u03b3/2). A quick calculation shows that for any h \u2208 H, and C \u2208 CMH with \u22121 in the first column and zeroes elsewhere, C \u2022 ( 1h \u2212BMH\u03b3 ) = 1/2\u2212 1/k. This is positive when k > 2, so that H fails to satisfy AdaBoost.MH\u2019s condition.\nWe have seen how our framework allows us to capture the strengths and weaknesses of old conditions, describe a whole new family of conditions and also identify the condition making minimal assumptions. In the next few sections, we show how to design boosting algorithms that employ these new conditions and enjoy strong theoretical guarantees.\n5. Algorithms\nIn this section we devise algorithms by analyzing the boosting games that employ weaklearning conditions in our framework. We compute the optimum Booster strategy against a completely adversarial Weak-Learner, which here is permitted to choose weak classifiers without restriction, i.e. the entire space Hall of all possible functions mapping examples to labels. By modeling Weak-Learner adversarially, we make absolutely no assumptions on the algorithm it might use. Hence, error guarantees enjoyed in this situation will be universally applicable. Our algorithms are derived from the very general drifting games framework (Schapire, 2001) for solving boosting games, which in turn was inspired by Freund\u2019s Boost-by-majority algorithm (Freund, 1995), which we review next.\nThe OS Algorithm. Fix the number of rounds T and a weak-learning condition (C,B). We will only consider conditions that are not vacuous, i.e., at least some classifier space satisfies the condition, or equivalently, the space Hall satisfies (C,B). Additionally, we assume the constraints placed by C are on individual rows. In other words, there is some subset C0 \u2286 Rk of all possible rows, such that a cost matrix C belongs to the collection C if and only if each of its rows belongs to this subset:\nC \u2208 C \u21d0\u21d2 \u2200i : C(i) \u2208 C0. (17)\nFurther, we assume C0 forms a convex cone i.e c, c\u2032 \u2208 C0 implies tc + t\u2032c\u2032 \u2208 C0 for any nonnegative t, t\u2032. This also implies that C is a convex cone. This is a very natural restriction, and is satisfied by the space C used by the weak learning conditions of AdaBoost.MH, AdaBoost.M1, AdaBoost.MR, SAMME as well as every edge-over-random condition. 1 For simplicity of presentation we fix the weights \u03b1t = 1 in each round. With fT defined\n1. All our results hold under the weaker restriction on the space C, where the set of possible cost vectors C0 for a row i could depend on i. For simplicity of exposition, we stick to the more restrictive assumption that C0 is common across all rows.\nas in (1), whether the final hypotheses output by Booster makes a prediction error on an example i is decided by whether an incorrect label received the maximum number of votes, fT (i, 1) \u2264 maxkl=2 fT (i, l). Therefore, the optimum Booster payoff can be written as\nmin C1\u2208C\nmax h1\u2208Hall:\nC1\u2022(1h1\u2212B)\u22640\n. . . min CT\u2208C\nmax hT\u2208Hall:\nCT \u2022(1hT\u2212B)\u22640\n1\nm m\u2211 i=1 Lerr(fT (xi, 1), . . . , fT (xi, k)). (18)\nwhere the function Lerr : Rk \u2192 R encodes 0-1 error\nLerr(s) = 1 [ s(1) \u2264 max\nl>1 s(l)\n] . (19)\nIn general, we will also consider other loss functions L : Rk \u2192 R such as exponential loss, hinge loss, etc. that upper-bound error and are proper : i.e. L(s) is increasing in the weight of the correct label s(1), and decreasing in the weights of the incorrect labels s(l), l 6= 1.\nDirectly analyzing the optimal payoff is hard. However, Schapire (2001) observed that the payoffs can be very well approximated by certain potential functions. Indeed, for any b \u2208 Rk define the potential function \u03c6bt : Rk \u2192 R by the following recurrence:\n\u03c6b0 = L\n\u03c6bt (s) = min c\u2208C0 max p\u2208\u2206{1,...,k}\nEl\u223cp [ \u03c6bt\u22121 (s + el) ] s.t. El\u223cp [c(l)] \u2264 \u3008b, c\u3009 ,\n(20)\nwhere l \u223c p denotes that label l is sampled from the distribution p, and el \u2208 Rk is the unitvector whose lth coordinate is 1 and the remaining coordinates zero. Notice the recurrence uses the collection of rows C0 instead of the collection of cost matrices C. When there are T \u2212 t rounds remaining (that is, after t rounds of boosting), these potential functions compute an estimate \u03c6bT\u2212t(st) of whether an example x will be misclassified, based on its current state st consisting of counts of votes received so far on various classes:\nst(l) = t\u22121\u2211 t\u2032=1 1 [ht\u2032(x) = l] . (21)\nNotice this definition of state assumes that \u03b1t = 1 in each round. Sometimes, we will choose the weights differently. In such cases, a more appropriate definition is the weighted state ft \u2208 Rk, tracking the weighted counts of votes received so far:\nft(l) = t\u22121\u2211 t\u2032=1 \u03b1t\u20321 [ht\u2032(x) = l] . (22)\nHowever, unless otherwise noted, we will assume \u03b1t = 1, and so the definition in (21) will suffice.\nThe recurrence in (20) requires the max player\u2019s response p to satisfy the constraint that the expected cost under the distribution p is at most the inner-product \u3008c,b\u3009. If there is no\ndistribution satisfying this requirement, then the value of the max expression is \u2212\u221e. The existence of a valid distribution depends on both b and c and is captured by the following:\n\u2203p \u2208 \u2206 {1, . . . , k} : El\u223cp [c(l)] \u2264 \u3008c,b\u3009 \u21d0\u21d2 min l c(l) \u2264 \u3008b, c\u3009 . (23)\nIn this paper, the vector b will always correspond to some row B(i) of the baseline used in the weak learning condition. In such a situation, the next lemma shows that a distribution satisfying the required constraints will always exist.\nLemma 7 If C0 is a cone and (17) holds, then for any row b = B(i) of the baseline and any cost vector c \u2208 C0, (23) holds unless the condition (C,B) is vacuous.\nProof We show that if (23) does not hold, then the condition is vacuous. Assume that for row b = B(i0) of the baseline, and some choice of cost vector c \u2208 C0, (23) does not hold. We pick a cost-matrix C \u2208 C, such that no weak classifier h can satisfy the requirement (2), implying the condition must be vacuous. The ith0 row of the cost matrix is c, and the remaining rows are 0. Since C0 is a cone, 0 \u2208 C0 and hence the cost matrix lies in C. With this choice for C, the condition (2) becomes\nc(h(xi)) = C (i, h(xi)) \u2264 \u3008C(i),B(i)\u3009 = \u3008c,b\u3009 < min l c(l),\nwhere the last inequality holds since, by assumption, (23) is not true for this choice of c,b. The previous equation is an impossibility, and hence no such weak classifier h exists, showing the condition is vacuous.\nLemma 7 shows that the expression in (20) is well defined, and takes on finite values. We next record an alternate dual form for the same recurrence which will be useful later.\nLemma 8 The recurrence in (20) is equivalent to\n\u03c6bt (s) = min c\u2208C0 k max l=1\n{ \u03c6bt\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,b\u3009) } . (24)\nProof Using Lagrangean multipliers, we may convert (20) to an unconstrained expression as follows:\n\u03c6bt (s) = min c\u2208C0 max p\u2208\u2206{1,...,k} min \u03bb\u22650\n{ El\u223cp [ \u03c6bt\u22121 (s + el) ] \u2212 \u03bb (El\u223cp [c(l)]\u2212 \u3008c,b\u3009) } .\nApplying Theorem 1 to the inner min-max expression we get\n\u03c6bt (s) = min c\u2208C0 min \u03bb\u22650 max p\u2208\u2206{1,...,k}\n{ El\u223cp [ \u03c6bt\u22121 (s + el) ] \u2212 (El\u223cp [\u03bbc(l)]\u2212 \u3008\u03bbc,b\u3009) } .\nSince C0 is a cone, c \u2208 C0 implies \u03bbc \u2208 C0. Therefore we may absorb the Lagrange multiplier into the cost vector:\n\u03c6bt (s) = min c\u2208C0 max p\u2208\u2206{1,...,k}\nEl\u223cp [ \u03c6bt\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,b\u3009) ] .\nFor a fixed choice of c, the expectation is maximized when the distribution p is concentrated on a single label that maximizes the inner expression, which completes our proof.\nThe dual form of the recurrence is useful for optimally choosing the cost matrix in each round. When the weak learning condition being used is (C,B), Schapire (2001) proposed a Booster strategy, called the OS strategy, which always chooses the weight \u03b1t = 1, and uses the potential functions to construct a cost matrix Ct as follows. Each row Ct(i) of the matrix achieves the minimum of the right hand side of (24) with b replaced by B(i), t replaced by T \u2212 t, and s replaced by current state st(i):\nCt(i) = argmin c\u2208C0 k max l=1\n{ \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,B(i)\u3009) } . (25)\nThe following theorem, proved in the appendix, provides a guarantee for the loss suffered by the OS algorithm, and also shows that it is the game-theoretically optimum strategy when the number of examples is large. Similar results have been proved by Schapire (2001), but our theorem holds much more generally, and also achieves tighter lower bounds.\nTheorem 9 (Extension of results in (Schapire, 2001)) Suppose the weak-learning condition is not vacuous and is given by (C,B), where C is such that, for some convex cone C0 \u2286 Rk, the condition (17) holds. Let the potential functions \u03c6bt be defined as in (20), and assume the Booster employs the OS algorithm, choosing \u03b1t = 1 and Ct as in (25) in each round t. Then the average potential of the states,\n1\nm m\u2211 i=1 \u03c6 B(i) T\u2212t (st(i)) ,\nnever increases in any round. In particular, the loss suffered after T rounds of play is at most\n1\nm m\u2211 i=1 \u03c6 B(i) T (0). (26)\nFurther, under certain conditions, this bound is nearly tight. In particular, assume the loss function does not vary too much but satisfies\nsup s,s\u2032\u2208ST\n|L(s)\u2212 L(s\u2032)| \u2264 (L, T ), (27)\nwhere ST , a subset of { s \u2208 Rk : \u2016s\u2016\u221e \u2264 T } , is the set of all states reachable in T iterations, and (L, T ) is an upper bound on the discrepancy of losses between any two reachable states when the loss function is L and the total number of iterations is T . Then, for any \u03b5 > 0, when the number of examples m is sufficiently large,\nm \u2265 T (L, T ) \u03b5 , (28)\nno Booster strategy can guarantee to achieve in T rounds a loss that is \u03b5 less than the bound (26).\nIn order to implement the near optimal OS strategy, we need to solve (25). This is computationally only as hard as evaluating the potentials, which in turn reduces to computing the recurrences in (20). In the next few sections, we study how to do this when using various losses and weak learning conditions.\n6. Solving for any fixed edge-over-random condition\nIn this section we show how to implement the OS strategy when the weak learning condition is any fixed edge-over-random condition: (C,B) for some B \u2208 Beor\u03b3 . By our previous discussions, this is equivalent to computing the potential \u03c6bt by solving the recurrence in (20), where the vector b corresponds to some row of the baseline B. Let \u2206k\u03b3 \u2286 \u2206 {1, . . . , k} denote the set of all edge-over-random distributions on {1, . . . , k} with \u03b3 more weight on the first coordinate:\n\u2206k\u03b3 = {b \u2208 \u2206 {1, . . . , k} : b(1)\u2212 \u03b3 = max {b(2), . . . , b(k)}} . (29)\nNote, that Beor\u03b3 consists of all matrices whose rows belong to the set \u2206k\u03b3 . Therefore we are interested in computing \u03c6b, where b is an arbitrary edge-over-random distribution: b \u2208 \u2206k\u03b3 . We begin by simplifying the recurrence (20) satisfied by such potentials, and show how to compute the optimal cost matrix in terms of the potentials.\nLemma 10 Assume L is proper, and b \u2208 \u2206k\u03b3 is an edge-over-random distribution. Then the recurrence (20) may be simplified as\n\u03c6bt (s) = El\u223cb [\u03c6t\u22121 (s + el)] . (30)\nFurther, if the cost matrix Ct is chosen as follows\nCt(i, l) = \u03c6 b T\u2212t\u22121(st(i) + el), (31)\nthen Ct satisfies the condition in (25), and hence is the optimal choice.\nProof Let Ceor0 \u2286 Rk denote all vectors c satisfying \u2200l : c(1) \u2264 c(l). Then, we have\n\u03c6bt (s) = min c\u2208Ceor0 max p\u2208\u2206{1,...,k}\nEl\u223cp [\u03c6t\u22121 (s + el)]\ns.t. El\u223cp[c(l)] \u2264 El\u223cb [c(l)] , ( by (20) )\n= min c\u2208Ceor0 max p\u2208\u2206 min \u03bb\u22650\n{ El\u223cp [ \u03c6bt\u22121 (s + el) ] + \u03bb (El\u223cb [c(l)]\u2212 El\u223cp[c(l)]) } (Lagrangean)\n= min c\u2208Ceor0 min \u03bb\u22650 max p\u2208\u2206\nEl\u223cp [ \u03c6bt\u22121 (s + el) ] + \u03bb \u3008b\u2212 p, c\u3009 (Theorem 1)\n= min c\u2208Ceor0 max p\u2208\u2206\nEl\u223cp [ \u03c6bt\u22121 (s + el) ] + \u3008b\u2212 p, c\u3009 (absorb \u03bb into c)\n= max p\u2208\u2206 min c\u2208Ceor0\nEl\u223cp [ \u03c6bt\u22121 (s + el) ] + \u3008b\u2212 p, c\u3009 (Theorem 1) .\nUnless b(1)\u2212p(1) \u2264 0 and b(l)\u2212p(l) \u2265 0 for each l > 1, the quantity \u3008b\u2212 p, c\u3009 can be made arbitrarily small for appropriate choices of c \u2208 Ceor0 . The max-player is therefore forced to constrain its choices of p, and the above expression becomes\nmax p\u2208\u2206\nEl\u223cp [ \u03c6bt\u22121 (s + el) ] s.t. b(l)\u2212 q(l) { \u2265 0 if l = 1, \u2264 0 if l > 1.\nLemma 6 of (Schapire, 2001) states that if L is proper (as defined here), so is \u03c6bt ; the same result can be extended to our drifting games. This implies the optimal choice of p in the above expression is in fact the distribution that puts as small weight as possible in the first coordinate, namely b. Therefore the optimum choice of p is b, and the potential is the same as in (30).\nWe end the proof by showing that the choice of cost matrix in (31) is optimum. Theorem 9 states that a cost matrix Ct is the optimum choice if it satisfies (25), that is, if the expression\nk max l=1\n{ \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 (Ct(i, l)\u2212 \u3008Ct(i),B(i)\u3009) } (32)\nis equal to\nmin c\u2208C0 k max l=1\n{ \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,B(i)\u3009) } = \u03c6 B(i) T\u2212t (s) , (33)\nwhere the equality in (33) follows from (24). If Ct(i) is chosen as in (31), then, for any label l, the expression within max in (32) evaluates to\n\u03c6 B(i) T\u2212t\u22121 (s + el) \u2212 ( \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 \u3008Ct(i),B(i)\u3009 ) = \u3008B(i),Ct(i)\u3009 = El\u223cB(i) [Ct(i, l)]\n= El\u223cB(i) [ \u03c6 B(i) T\u2212t\u22121 (s + el) ] = \u03c6\nB(i) T\u2212t(s),\nwhere the last equality follows from (30). Therefore the max expression in (32) is also equal to \u03c6 B(i) T\u2212t(s), which is what we needed to show.\nEq. (31) in Lemma 10 implies the cost matrix chosen by the OS strategy can be expressed in terms of the potentials, which is the only thing left to calculate. Fortunately, the simplification (30) of the drifting games recurrence, allows the potentials to be solved completely in terms of a random-walk Rtb(x). This random variable denotes the position of a particle after t time steps, that starts at location x \u2208 Rk, and in each step moves in direction el with probability b(l).\nCorollary 11 The recurrence in (30) can be solved as follows: \u03c6bt (s) = E [ L ( Rtb(s) )] . (34)\nProof Inductively assuming \u03c6bt\u22121(x) = E [ L(Rt\u22121b (x)) ] ,\n\u03c6t(s) = El\u223cb [ L(Rt\u22121b (s) + el) ] = E [ L(Rtb(s)) ] .\nThe last equality follows by observing that the random position Rt\u22121b (s) + el is distributed as Rtb(s) when l is sampled from b.\nLemma 10 and Corollary 11 together imply:\nTheorem 12 Assume L is proper and b \u2208 \u2206k\u03b3 is an edge-over-random distribution. Then the potential \u03c6bt , defined by the recurrence in (20), has the solution given in (34) in terms of random walks.\nBefore we can compute (34), we need to choose a loss function L. We next consider two options for the loss \u2014 the non-convex 0-1 error, and exponential loss.\nExponential Loss. The exponential loss serves as a smooth convex proxy for discontinuous non-convex 0-1 error (19) that we would ultimately like to bound, and is given by\nLexp\u03b7 (s) = k\u2211 l=2 e\u03b7(sl\u2212s1). (35)\nThe parameter \u03b7 can be thought of as the weight in each round, that is, \u03b1t = \u03b7 in each round. Then notice that the weighted state ft of the examples, defined in (22), is related to the unweighted states st as ft(l) = \u03b7st(l). Therefore the exponential loss function in (35) directly measures the loss of the weighted state as\nLexp(ft) = k\u2211 l=2 eft(l)\u2212ft(1). (36)\nBecause of this correspondence, the optimal strategy with the loss function Lexp and \u03b1t = \u03b7 is the same as that using loss Lexp\u03b7 and \u03b1t = 1. We study the latter setting so that we may use the results derived earlier. With the choice of the exponential loss Lexp\u03b7 , the potentials are easily computed, and in fact have a closed form solution.\nTheorem 13 If Lexp\u03b7 is as in (35), where \u03b7 is non-negative, then the solution in Theorem 12 evaluates to \u03c6bt (s) = \u2211k l=2(al) te\u03b7l(sl\u2212s1), where al = 1\u2212 (b1 + bl) + e\u03b7bl + e\u2212\u03b7b1.\nThe proof by induction is straightforward. By tuning the weight \u03b7, each al can be always made less than 1. This ensures the exponential loss decays exponentially with rounds. In particular, when B = U\u03b3 (so that the condition is (Ceor,U\u03b3)), the relevant potential \u03c6t(s) or \u03c6t(f) is given by\n\u03c6t(s) = \u03c6t(f) = \u03ba(\u03b3, \u03b7) t k\u2211 l=2 e\u03b7(sl\u2212s1) = \u03ba(\u03b3, \u03b7)t k\u2211 l=2 efl\u2212f1 (37)\nwhere\n\u03ba(\u03b3, \u03b7) = 1 + (1\u2212 \u03b3) k\n( e\u03b7 + e\u2212\u03b7 \u2212 2 ) \u2212 ( 1\u2212 e\u2212\u03b7 ) \u03b3. (38)\nThe cost-matrix output by the OS algorithm can be simplified by rescaling, or adding the same number to each coordinate of a cost vector, without affecting the constraints it imposes on a weak classifier, to the following form\nC(i, l) = { (e\u03b7 \u2212 1) e\u03b7(sl\u2212s1) if l > 1, (e\u2212\u03b7 \u2212 1) \u2211k l=2 e \u03b7(sl\u2212s1) if l = 1.\nUsing the correspondence between unweighted and weighted states, the above may also be rewritten as:\nC(i, l) = { (e\u03b7 \u2212 1) efl\u2212f1 if l > 1, (e\u2212\u03b7 \u2212 1) \u2211k l=2 e fl\u2212f1 if l = 1. (39)\nWith such a choice, Theorem 9 and the form of the potential guarantee that the average loss\n1\nm m\u2211 i=1 Lexp\u03b7 (st(i)) = 1 m m\u2211 i=1 Lexp(ft(i)) (40)\nof the states changes by a factor of at most \u03ba (\u03b3, \u03b7) every round. Therefore the final loss, which upper bounds the error, i.e., the fraction of misclassified training examples, is at most (k\u22121)\u03ba (\u03b3, \u03b7)T . Since this upper bound holds for any value of \u03b7, we may tune it to optimize the bound. Setting \u03b7 = ln (1 + \u03b3), the error can be upper bounded by (k \u2212 1)e\u2212T\u03b32/2.\nZero-one Loss. There is no simple closed form solution for the potential when using the zero-one loss Lerr (19). However, we may compute the potentials efficiently as follows. To compute \u03c6bt (s), we need to find the probability that a random walk (making steps according to b) of length t in Zk, starting at s will end up in a region where the loss function is 1. Any such random walk will consist of xl steps in direction el where the non-negative \u2211 l xl = t.\nThe probability of each such path is \u220f l b xl l . Further, there are exactly ( t\nx1,...,xk\n) such paths.\nStarting at state s, such a path will lead to a correct answer only if s1 + x1 > sl + xl for each l > 1. Hence we may write the potential \u03c6bt (s) as\n\u03c6bt (s) = 1\u2212 t\u2211\nx1,...,xk\n( t\nx1,...,xk\n)\u220fk l=1 b xl l\ns.t. x1 + . . .+ xk = t\n\u2200l : xl \u2265 0 \u2200l : xl + sl \u2264 x1 + s1.\nSince the xl\u2019s are restricted to be integers, this problem is presumably hard. In particular, the only algorithms known to the authors that take time logarithmic in t is also exponential in k. However, by using dynamic programming, we can compute the summation in time polynomial in |sl|, t and k. In fact, the runtime is always O(t3k), and at least \u2126(tk).\nThe bounds on error we achieve, although not in closed form, are much tighter than those obtainable using exponential loss. The exponential loss analysis yields an error upper bound of (k \u2212 1)e\u2212T\u03b32/2. Using a different initial distribution, Schapire and Singer (1999) achieve the slightly better bound \u221a (k \u2212 1)e\u2212T\u03b32/2. However, when the edge \u03b3 is small and the number of rounds are few, each bound is greater than 1 and hence trivial. On the other hand, the bounds computed by the above dynamic program are sensible for all values of k, \u03b3 and T . When b is the \u03b3-biased uniform distribution b = (1\u2212\u03b3k +\u03b3, 1\u2212\u03b3 k , 1\u2212\u03b3 k , . . . , 1\u2212\u03b3 k ) a table containing the error upper bound \u03c6bT (0) for k = 6, \u03b3 = 0 and small values for the number of rounds T is shown in Figure 2(a); note that with the exponential loss, the bound is always 1 if the edge \u03b3 is 0. Further, the bounds due to the exponential loss analyses seem to imply that the dependence of the error on the number of labels is monotonic. However, a plot of the tighter bounds with edge \u03b3 = 0.1, number of rounds T = 10 against various values of k, shown in Figure 2(b), indicates that the true dependence is more complicated. Therefore the tighter analysis also provides qualitative insights not obtainable via the exponential loss bound.\n7. Solving for the minimal weak learning condition\nIn the previous section we saw how to find the optimal boosting strategy when using any fixed edge-over-random condition. However as we have seen before, these conditions can be stronger than necessary, and therefore lead to boosting algorithms that require additional assumptions. Here we show how to compute the optimal algorithm while using the weakest weak learning condition, provided by (16), or equivalently the condition used by AdaBoost.MR, (CMR,BMR\u03b3 ). Since there are two possible formulations for the minimal condition, it is not immediately clear which to use to compute the optimal boosting strategy. To resolve this, we first show that the optimal boosting strategy based on any formulation of a necessary and sufficient weak learning condition is the same. Having resolved this ambiguity, we show how to compute this strategy for the exponential loss and 0-1 error using the first formulation.\n7.1 Game-theoretic equivalence of necessary and sufficient weak-learning conditions\nIn this section we study the effect of the weak learning condition on the game-theoretically optimal boosting strategy. We introduce the notion of game-theoretic equivalence between two weak learning conditions, that determines if the payoffs (18) of the optimal boosting strategies based on the two conditions are identical. This is different from the usual notion of equivalence between two conditions, which holds if any weak classifier space satisfies both conditions or neither condition. In fact we prove that game-theoretic equivalence is a\nbroader notion; in other words, equivalence implies game-theoretic equivalence. A special case of this general result is that any two weak learning conditions that are necessary and sufficient, and hence equivalent to boostability, are also game-theoretically equivalent. In particular, so are the conditions of AdaBoost.MR and (16), and the resulting optimal Booster strategies enjoy equally good payoffs. We conclude that in order to derive the optimal boosting strategy that uses the minimal weak-learning condition, it is sound to use either of these two formulations.\nThe purpose of a weak learning condition (C,B) is to impose restrictions on the WeakLearner\u2019s responses in each round. These restrictions are captured by subsets of the weak classifier space as follows. If Booster chooses cost-matrix C \u2208 C in a round, the WeakLearner\u2019s response h is restricted to the subset SC \u2286 Hall defined as\nSC = { h \u2208 Hall : C \u2022 1h \u2264 C \u2022B } .\nThus, a weak learning condition is essentially a family of subsets of the weak classifier space. Further, smaller subsets mean fewer options for Weak-Learner, and hence better payoffs for the optimal boosting strategy. Based on this idea, we may define when a weak learning condition (C1,B1) is game-theoretically stronger than another condition (C2,B2) if the following holds: For every subset SC2 in the second condition (that is C2 \u2208 C2), there is a subset SC1 in the first condition (that is C1 \u2208 C1), such that SC1 \u2286 SC2 . Mathematically, this may be written as follows:\n\u2200C1 \u2208 C1,\u2203C2 \u2208 C2 : SC1 \u2286 SC2 .\nIntuitively, a game theoretically stronger condition will allow Booster to place similar or stricter restrictions on Weak-Learner in each round. Therefore, the optimal Booster payoff using a game-theoretically stronger condition is at least equally good, if not better. It therefore follows that if two conditions are both game-theoretically stronger than each other, the corresponding Booster payoffs must be equal, that is they must be game-theoretically equivalent.\nNote that game-theoretic equivalence of two conditions does not mean that they are identical as families of subsets, for we may arbitrarily add large and \u201cuseless\u201d subsets to the two conditions without affecting the Booster payoffs, since these subsets will never be used by an optimal Booster strategy. In fact we next show that game-theoretic equivalence is a broader notion than just equivalence.\nTheorem 14 Suppose (C1,B1) and (C2,B2) are two equivalent weak learning conditions, that is, every space H satisfies both or neither condition. Then each condition is gametheoretically stronger than the other, and hence game-theoretically equivalent.\nProof We argue by contradiction. Assume that despite equivalence, the first condition (without loss of generality) includes a particularly hard subset SC1 \u2286 Hall,C1 \u2208 C1 which is not smaller than any subset in the second condition. In particular, for every subset SC2 ,C2 \u2208 C2 in the second condition is satisfied by some weak classifier hC2 not satisfying the hard subset in the first condition: hC2 \u2208 SC2 \\ SC1 . Therefore, the space\nH = {hC2 : C2 \u2208 C2} ,\nformed by just these classifiers satisfies the second condition, but has an empty intersection with SC1 . In other words, H satisfies the second but not the first condition, a contradiction to their equivalence.\nAn immediate corollary is the game theoretic equivalence of necessary and equivalent conditions.\nCorollary 15 Any two necessary and sufficient weak learning conditions are game-theoretically equivalent. In particular the optimum Booster strategies based on AdaBoost.MR\u2019s condition (CMR,BMR\u03b3 ) and (16) have equal payoffs. Therefore, in deriving the optimal Booster strategy, it is sound to work with either AdaBoost.MR\u2019s condition or (16). In the next section, we actually compute the optimal strategy using the latter formulation.\n7.2 Optimal strategy with the minimal conditions\nIn this section we compute the optimal Booster strategy that uses the minimum weak learning condition provided in (16). We choose this instead of AdaBoost.MR\u2019s condition because this description is more closely related to the edge-over-random conditions, and the resulting algorithm has a close relationship to the ones derived for fixed edge-overrandom conditions, and therefore more insightful. However, this formulation does not state the condition as a single pair (C,B), and therefore we cannot directly use the result of Theorem 9. Instead, we define new potentials and a modified OS strategy that is still nearly optimal, and this constitutes the first part of this section. In the second part, we show how to compute these new potentials and the resulting OS strategy.\n7.2.1 Modified potentials and OS strategy\nThe condition in (16) is not stated as a single pair (Ceor,B), but uses all possible edgeover-random baselines B \u2208 Beor\u03b3 . Therefore, we modify the definitions (20) of the potentials accordingly to extract an optimal Booster strategy. Recall that \u2206k\u03b3 is defined in (29) as the set of all edge-over-random distributions which constitute the rows of edge-over-random baselines B \u2208 Beor\u03b3 . Using these, define new potentials \u03c6t(s) as follows:\n\u03c6t(s) = min c\u2208Ceor0 max b\u2208\u2206k\u03b3 max p\u2208\u2206{1,...,k}\nEl\u223cp [\u03c6t\u22121 (s + el)]\ns.t. El\u223cp[c(l)] \u2264 \u3008b, c\u3009 . (41)\nThe main difference between (41) and (20) is that while the older potentials were defined using a fixed vector b corresponding to some row in the fixed baseline B, the new definition takes the maximum over all possible rows b \u2208 \u2206k\u03b3 that an edge-over-random baseline B \u2208 Beor\u03b3 may have. As before, we may write the recurrence in (41) in its dual form\n\u03c6t(s) = min c\u2208Ceor0 max b\u2208\u2206k\u03b3 k max l=1 {\u03c6t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,b\u3009)} . (42)\nThe proof is very similar to that of Lemma 8 and is omitted. We may now define a new OS strategy that chooses a cost-matrix in round t analogously:\nCt(i) \u2208 argmin c\u2208Ceor0 max b\u2208\u2206k\u03b3 k max l=1 {\u03c6t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,b\u3009)} . (43)\nwhere recall that st(i) denotes the state vector (defined in (21)) of example i. With this strategy, we can show an optimality result very similar to Theorem 9.\nTheorem 16 Suppose the weak-learning condition is given by (16). Let the potential functions \u03c6bt be defined as in (41), and assume the Booster employs the modified OS strategy, choosing \u03b1t = 1 and Ct as in (43) in each round t. Then the average potential of the states,\n1\nm m\u2211 i=1 \u03c6T\u2212t (st(i)) ,\nnever increases in any round. In particular, the loss suffered after T rounds of play is at most \u03c6T (0).\nFurther, for any \u03b5 > 0, when the loss function satisfies (27) and the number of examples m is as large as in (28), no Booster strategy can guarantee to achieve less than \u03c6T (0) \u2212 \u03b5 loss in T rounds.\nThe proof is very similar to that of Theorem 9 and is omitted.\n7.2.2 Computing the new potentials.\nHere we show how to compute the new potentials. The resulting algorithms will require exponential time, and we provide some empirical evidence showing that this might be necessary. Finally, we show how to carry out the computations efficiently in certain special situations.\nAn exponential time algorithm. Here we show how the potentials may be computed as the expected loss of some random walk, just as we did for the potentials arising with fixed edge-over-random conditions. The main difference is there will be several random walks to choose from.\nWe first begin by simplifying the recurrence (41), and expressing the optimal cost matrix in (43) in terms of the potentials, just as we did in Lemma 10 for the case of fixed edgeover-random conditions.\nLemma 17 Assume L is proper. Then the recurrence (41) may be simplified as\n\u03c6t(s) = max b\u2208\u2206k\u03b3\nEl\u223cb [\u03c6t\u22121 (s + el)] . (44)\nFurther, if the cost matrix Ct is chosen as follows:\nCt(i, l) = \u03c6T\u2212t\u22121(st(i) + el), (45)\nthen Ct satisfies the condition in (43).\nThe proof is very similar to that of Lemma 10 and is omitted. Eq. (45) implies that, as before, computing the optimal Booster strategy reduces to computing the new potentials. One computational difficulty created by the new definitions (41) or (44) is that they require infinitely many possible distributions b \u2208 \u2206k\u03b3 to be considered. We show that we may in fact restrict our attention to only finitely many of such distributions described next.\nAt any state s and number of remaining iterations t, let \u03c0 be a permutation of the coordinates {2, . . . , k} that sorts the potential values:\n\u03c6t\u22121 ( s + e\u03c0(k) ) \u2265 \u03c6t\u22121 ( s + e\u03c0(k\u22121) ) \u2265 . . . \u2265 \u03c6t\u22121 ( s + e\u03c0(2) ) . (46)\nFor any permutation \u03c0 of the coordinates {2, . . . , k}, let b\u03c0a denote the \u03b3-biased uniform distribution on the a coordinates {1, \u03c0k, \u03c0k\u22121, . . . , \u03c0k\u2212a+2}:\nb\u03c0a(l) =  1\u2212\u03b3 a + \u03b3 if l = 1 1\u2212\u03b3 a if l \u2208 {\u03c0k, . . . , \u03c0k\u2212a+2}\n0 otherwise.\n(47)\nThen, the next lemma shows that we may restrict our attention to only the distributions {b\u03c02 , . . . ,b\u03c0k} when evaluating the recurrence in (44).\nLemma 18 Fix a state s and remaining rounds of boosting t. Let \u03c0 be a permutation of the coordinates {2, . . . , k} satisfying (46), and define b\u03c0a as in (47). Then the recurrence (44) may be simplified as follows:\n\u03c6t(s) = max b\u2208\u2206k\u03b3 El\u223cb [\u03c6t\u22121 (s + el)] = max 2\u2264a\u2264k El\u223cb\u03c0a [\u03c6t\u22121 (s + el)] . (48)\nProof Assume (by relabeling the coordinates if necessary) that \u03c0 is the identity permutation, that is, \u03c0(2) = 2, . . . , \u03c0(k) = k. Observe that the right hand side of (44) is at least as much the right hand side of (48) since the former considers more distributions. We complete the proof by showing that the former is also at most the latter.\nBy (44), we may assume that some optimal b satisfies\nb(k) = \u00b7 \u00b7 \u00b7 = b(k \u2212 a+ 2) = b(1)\u2212 \u03b3, b(k \u2212 a+ 1) \u2264 b(1)\u2212 \u03b3,\nb(k \u2212 a) = \u00b7 \u00b7 \u00b7 = b(2) = 0.\nTherefore, b is a distribution supported on a+1 elements, with the minimum weight placed on element k \u2212 a+ 1. This implies b(k \u2212 a+ 1) \u2208 [0, 1/(a+ 1)].\nNow, El\u223cb [\u03c6t\u22121(s + el)] may be written as\n\u03b3 \u00b7 \u03c6t\u22121(s + e1) + b(k \u2212 a+ 1)\u03c6t\u22121(s + ek\u2212a+1)\n+ (1\u2212 \u03b3 \u2212 b(k \u2212 a+ 1))\u03c6t\u22121(s + e1) + \u03c6t\u22121(s + ek\u2212a+2) + . . . \u03c6t\u22121(s + ek) a = \u03b3 \u00b7 \u03c6t\u22121(s + e1) + b(k \u2212 a+ 1)\n1\u2212 \u03b3 \u03c6t\u22121(s + ek\u2212a+1) + (1\u2212 \u03b3) {(\n1\u2212 b(k \u2212 a+ 1) 1\u2212 \u03b3\n) \u03c6t\u22121(s + e1) + \u03c6t\u22121(s + ek\u2212a+2) + . . . \u03c6t\u22121(s + ek)\na } Replacing b(k \u2212 a + 1) by x in the above expression, we get a linear function of x. When restricted to [0, 1/(a+ 1)] the maximum value is attained at a boundary point. For x = 0, the expression becomes\n\u03b3 \u00b7 \u03c6t\u22121(s + e1) + (1\u2212 \u03b3) \u03c6t\u22121(s + e1) + \u03c6t\u22121(s + ek\u2212a+2) + . . . \u03c6t\u22121(s + ek)\na .\nFor x = 1/(a+ 1), the expression becomes\n\u03b3 \u00b7 \u03c6t\u22121(s + e1) + (1\u2212 \u03b3) \u03c6t\u22121(s + e1) + \u03c6t\u22121(s + ek\u2212a+1) + . . . \u03c6t\u22121(s + ek)\na+ 1 .\nSince b(k \u2212 a + 1) lies in [0, 1/(a + 1)], the optimal value is at most the maximum of the two. However each of these last two expressions is at most the right hand side of (48), completing the proof.\nUnraveling (48), we find that \u03c6t(s) is the expected loss of the final state reached by some random walk of t steps starting at state s. However, the number of possibilities for the random-walk is huge; indeed, the distribution at each step can be any of the k\u22121 possibilities b\u03c0a for a \u2208 {2, . . . , k}, where the parameter a denotes the size of the support of the \u03b3biased uniform distribution chosen at each step. In other words, at a given state s with t rounds of boosting remaining, the parameter a determines the number of directions the optimal random walk will consider taking; we will therefore refer to a as the degree of the random walk given (s, t). Now, the total number of states reachable in T steps is O ( T k\u22121 ) . If the degree assignment every such state, for every value of t \u2264 T is fixed in advance, a = {a(s, t) : t \u2264 T, s reachable}, we may identify a unique random walk Ra,t(s) of length t starting at step s. Therefore the potential may be computed as\n\u03c6t(s) = max a\nE [ Ra,t(s) ] . (49)\nA dynamic programming approach for computing (49) requires time and memory linear in the number of different states reachable by a random walk that takes T coordinate steps: O(T k\u22121). This is exponential in the dataset size, and hence impractical. In the next two sections we show that perhaps there may not be any way of computing these efficiently in general, but provide efficient algorithms in certain special cases.\nHardness of evaluating the potentials. Here we provide empirical evidence for the hardness of computing the new potentials. We first identify a computationally easier problem, and show that even that is probably hard to compute. Eq. (48) implies that if the potentials were efficiently computable, the correct value of the degree a could be determined efficiently. The problem of determining the degree a given the state s and remaining rounds t is therefore easier than evaluating the potentials. However, a plot of the degrees against states and remaining rounds, henceforth called a degree map, reveals very little structure that might be captured by a computationally efficient function.\nWe include three such degree maps in Figure 3. Only three classes k = 3 are used, and the loss function is 0-1 error. We also fix the number T of remaining rounds of boosting and the value of the edge \u03b3 for each plot. For ease of presentation, the 3-dimensional states s = (s1, s2, s3) are compressed into 2-dimensional pixel coordinates (u = s2\u2212s1, v = s3\u2212s2). It can be shown that this does not take away information required to evaluate the potentials or the degree at any pixel (u, v). Further, only those states are considered whose compressed coordinates u, v lie in the range [\u2212T, T ]; in T rounds, these account for all the reachable states. The degrees are indicated on the plot by colors. Our discussion in the previous sections implies that the possible values of the degree is 2 or 3. When the degree at a pixel (u, v) is 3, the pixel is colored green, and when the degree is 2, it is colored black.\n\u221220 \u221210 0 10 20\n\u22122 0\n\u22121 0\n0 10\n20\nT = 20\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u221240 \u221220 0 20 40\n\u22124 0\n\u22122 0\n0 20\n40\nT = 50\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u221240 \u221220 0 20 40\n\u22124 0\n\u22122 0\n0 20\n40\nT = 50\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\nFigure 3: Green pixels have degree 3, black pixels have degree 2. Each step is diagonally down (left), and up (if x < y) and right (if x > y) and both when degree is 3. The rightmost figure uses \u03b3 = 0.4, and the other two \u03b3 = 0. The loss function is 0-1.\n!10\n!5\n0\n5\n10\n!10\n!5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT=3\n\u221210 \u22125\n0 5\n10\n\u221210\n\u22125\n0\n5\n10\n0.2\n0.4\n0.6\n0.8\nT=20\nFigure 4: Optimum recurrence value. We set \u03b3 = 0. Surface is irregular for smaller values of T , but smoother for larger values, admitting hope for approximation.\nNote that a random walk over the space s \u2208 R3 consisting of distributions over coordinate steps {(1, 0, 0), (0, 1, 0), (0, 0, 1)} translates to a random walk over (u, v) \u2208 R2 where each step lies in the set {(\u22121,\u22121), (1, 0), (0, 1)}. In Figure 3, these correspond to the directions diagonally down, up or right. Therefore at a black pixel, the random walk either chooses between diagonally down and up, or between diagonally down and right, with probabilities {1/2 + \u03b3/2, 1/2\u2212 \u03b3/2}. On the other hand, at a green pixel, the random walk chooses among diagonally down, up and right with probabilities (\u03b3+(1\u2212\u03b3)/3, (1\u2212\u03b3)/3, (1\u2212\u03b3)/3). The degree maps are shown for varying values of T and the edge \u03b3. While some patterns emerge for the degrees, such as black or green depending on the parity of u or v, the authors found the region near the line u = v still too complex to admit any solution apart from a brute-force computation.\nWe also plot the potential values themselves in Figure 4 against different states. In each plot, the number of iterations remaining, T , is held constant, the number of classes is chosen to be 3, and the edge \u03b3 = 0. The states are compressed into pixels as before, and the\npotential is plotted against each pixel, resulting in a 3-dimensional surface. We include two plots, with different values for T . The surface is irregular for T = 3 rounds, but smoother for 20 rounds, admitting some hope for approximation.\nAn alternative approach would be to approximate the potential \u03c6t by the potential \u03c6 b t for some fixed b \u2208 \u2206k\u03b3 corresponding to some particular edge-over-random condition. Since \u03c6t \u2265 \u03c6bt for all edge-over-random distributions b, it is natural to approximate by choosing b that maximizes the fixed edge-over-random potential. (It can be shown that this b corresponds to the \u03b3-biased uniform distribution.) Two plots of comparing the potential values at 0, \u03c6T (0) and maxb \u03c6 b T (0), which correspond to the respective error upper bounds, is shown in Figure 5. In the first plot, the number of classes k is held fixed at 6, and the values are plotted for different values of iterations T . In the second plot, the number of classes vary, and the number of iterations is held at 10. Both plots show that the difference in the values is significant, and hence maxb \u03c6 b T (0) would be a rather optimistic upper bound on the error when using the minimal weak learning condition.\nIf we use exponential loss (35), the situation is not much better. The degree maps for varying values of the weight parameter \u03b7 against fixed values of edge \u03b3 = 0.1, rounds remaining T = 20 and number of classes k = 3 are plotted in Figure 6. Although the patterns are simple, with the degree 3 pixels forming a diagonal band, we found it hard to prove this fact formally, or compute the exact boundary of the band. However the plots suggest that when \u03b7 is small, all pixels have degree 3. We next find conditions under which this opportunity for tractable computation exists.\nEfficient computation in special cases. Here we show that when using the exponential loss, if the edge \u03b3 is very small, then the potentials can be computed efficiently. We first show an intermediate result. We already observed empirically that when the weight parameter \u03b7 is small, the degrees all become equal to k. Indeed, we can prove this fact.\nLemma 19 If the loss function being used is exponential loss (35) and the weight parameter \u03b7 is small compared to the number of rounds\n\u03b7 \u2264 1 4 min\n{ 1\nk \u2212 1 ,\n1\nT\n} , (50)\nthen the optimal value of the degree a in (48) is always k. Therefore, in this situation, the potential \u03c6t using the minimal weak learning condition is the same as the potential \u03c6 u t using the \u03b3-biased uniform distribution u,\nu = ( 1\u2212 \u03b3 k + \u03b3, 1\u2212 \u03b3 k , . . . , 1\u2212 \u03b3 k ) , (51)\nand hence can be efficiently computed.\nProof We show \u03c6t = \u03c6 u t by induction on the remaining number t of boosting iterations. The base case holds since, by definition, \u03c60 = \u03c6 u 0 = L exp \u03b7 . Assume, inductively that\n\u03c6t\u22121(s) = \u03c6 u t\u22121(s) = \u03ba(\u03b3, \u03b7) t\u22121 k\u2211 l=2 e\u03b7(sl\u2212s1), (52)\nwhere the second equality follows from (37). We show that\n\u03c6t(s) = El\u223cu [\u03c6t\u22121(s + el)] . (53)\nBy the inductive hypothesis and (30), the right hand side of (53) is in fact equal to \u03c6ut , and we will have shown \u03c6t = \u03c6 u t . The proof will then follow by induction.\nIn order to show (53), by Lemma 18, it suffices to show that the optimal degree a maximizing the right hand side of (48) is always k:\nEl\u223cb\u03c0a [\u03c6t\u22121 (s + el)] \u2264 El\u223cb\u03c0k [\u03c6t\u22121 (s + el)] . (54)\nBy (52), \u03c6t\u22121 (s + el0) may be written as \u03c6t\u22121(s) + \u03ba(\u03b3, \u03b7) t\u22121 \u00b7 \u03bel0 , where the term \u03bel0 is:\n\u03bel0 = { (e\u03b7 \u2212 1)e\u03b7(sl0\u2212s1) if l0 6= 1, (e\u2212\u03b7 \u2212 1) \u2211k l=2 e \u03b7(sl\u2212s1) if l0 = 1.\nTherefore (54) is the same as: El\u223cb\u03c0a [\u03bel] \u2264 El\u223cb\u03c0k [\u03bel]. Assume (by relabeling if necessary) that \u03c0 is the identity permutation on coordinates {2, . . . , k}. Then the expression El\u223cb\u03c0a [\u03bel] can be written as\nEl\u223cb\u03c0a [\u03bel] = ( 1\u2212 \u03b3 a + \u03b3 ) \u03be1 + k\u2211 l=k\u2212a+2 ( 1\u2212 \u03b3 a ) \u03bel\n= \u03b3\u03be1 + (1\u2212 \u03b3)\n{ \u03be1 + \u2211k l=k\u2212a+2 \u03bel\na\n} .\nIt suffices to show that the term in curly brackets is maximized when a = k. We will in fact show that the numerator of the term is negative if a < k, and non-negative for a = k, which will complete our proof. Notice that the numerator can be written as\n(e\u03b7 \u2212 1)\n{ k\u2211\nl=k\u2212a+2 e\u03b7(sl\u2212s1)\n} \u2212 (1\u2212 e\u2212\u03b7)\nk\u2211 l=2 e\u03b7(sl\u2212s1)\n= (e\u03b7 \u2212 1)\n{ k\u2211\nl=k\u2212a+2 e\u03b7(sl\u2212s1) \u2212 k\u2211 l=2 e\u03b7(sl\u2212s1)\n} + {\n(e\u03b7 \u2212 1)\u2212 (1\u2212 e\u2212\u03b7) } k\u2211 l=2 e\u03b7(sl\u2212s1)\n= { e\u03b7 + e\u2212\u03b7 \u2212 2 } k\u2211 l=2 e\u03b7(sl\u2212s1) \u2212 (e\u03b7 \u2212 1) { k\u2212a+1\u2211 l=2 e\u03b7(sl\u2212s1) } .\nWhen a = k, the second summation disappears, and we are left with a non-negative expression. However when a < k, the second summation is at least e\u03b7(s2\u2212s1). Since t \u2264 T , and in t iterations the absolute value of any state coordinate |st(l)| is at most T , the first summation is at most (k \u2212 1)e2\u03b7T and the second summation is at least e\u22122\u03b7T . Therefore the previous expression is at most\n(k \u2212 1) ( e\u03b7 + e\u2212\u03b7 \u2212 2 ) e2\u03b7T \u2212 (e\u03b7 \u2212 1)e\u22122\u03b7T\n= (e\u03b7 \u2212 1)e\u22122\u03b7T { (k \u2212 1)(1\u2212 e\u2212\u03b7)e4\u03b7T \u2212 1 } .\nWe show that the term in curly brackets is negative. Firstly, using ex \u2265 1 + x, we have 1\u2212 e\u2212\u03b7 \u2264 \u03b7 \u2264 1/(4(k \u2212 1)) by choice of \u03b7. Therefore it suffices to show that e4\u03b7T < 4. By choice of \u03b7 again, e4\u03b7T \u2264 e1 < 4. This completes our proof.\nThe above lemma seems to suggest that under certain conditions, a sort of degeneracy occurs, and the optimal Booster payoff (18) is nearly unaffected by whether we use the minimal weak learning condition, or the condition (Ceor,U\u03b3). Indeed, we next prove this fact.\nTheorem 20 Suppose the loss function is as in Lemma 19, and for some parameter \u03b5 > 0, the number of examples m is large enough\nm \u2265 Te 1/4\n\u03b5 . (55)\nConsider the minimal weak learning condition (16), and the fixed edge-over-random condition (Ceor,U\u03b3) corresponding to the \u03b3-biased uniform baseline U\u03b3. Then the optimal booster payoffs using either condition is within \u03b5 of each other.\nProof We show that the OS strategies arising out of either condition is the same. In other words, at any iteration t and state st, both strategies play the same cost matrix and enforce the same constraints on the response of Weak-Learner. The theorem will then follow if we can invoke Theorems 9 and 16. For that, the number of examples needs to be as large as in (28). The required largeness would follow from (55) if the loss function satisfied (27) with (L, T ) at most exp(1/4). Since the largest discrepancy in losses between two states reachable in T iterations is at most e\u03b7T \u2212 0, the bound follows from the choice of \u03b7 in (50). Therefore, it suffices to show the equivalence of the OS strategies corresponding to the two weak learning conditions.\nWe first show both strategies play the same cost-matrix. Lemma 19 states that the potential function using the minimal weak learning condition is the same as when using the fixed condition (Ceor,U\u03b3): \u03c6t = \u03c6ut , where u is as in (51). Since, according to (31) and (45), given a state st and iteration t, the two strategies choose cost matrices that are identical functions of the respective potentials, by the equivalence of the potential functions, the resulting cost matrices must be the same.\nEven with the same cost matrix, the two different conditions could be imposing different constraints on Weak-Learner, which might affect the final payoff. For instance, with the baseline U\u03b3 , Weak-Learner has to return a weak classifier h satisfying\nCt \u2022 1h \u2264 Ct \u2022U\u03b3 ,\nwhereas with the minimal condition, the constraint on h is\nCt \u2022 1h \u2264 max B\u2208Beor\u03b3 Ct \u2022B.\nIn order to show that the constraints are the same we therefore need to show that for the common cost matrix Ct chosen, the right hand side of the two previous expressions are the same:\nCt \u2022U\u03b3 = max B\u2208Beor\u03b3 Ct \u2022 Beor\u03b3 . (56)\nWe will in fact show the stronger fact that the equality holds for every row separately:\n\u2200i : \u3008Ct(i),u\u3009 = max b\u2208\u2206k\u03b3 \u3008Ct(i),b\u3009 . (57)\nTo see this, first observe that the choice of the optimal cost matrix Ct in (45) implies the following identity\n\u3008Ct(i),b\u3009 = El\u223cb [\u03c6T\u2212t\u22121(st(i) + el)] .\nOn the other hand, (48) and Lemma 19 together imply that the distribution b maximizing the right hand side of the above is the \u03b3-biased uniform distribution, from which (57) follows. Therefore, the constraints placed on Weak-Learner by the cost-matrix Ct is the same whether we use minimum weak learning condition or the fixed condition (Ceor,U\u03b3).\nOne may wonder why \u03b7 would be chosen so small, especially since the previous theorem indicates that such choices for \u03b7 lead to degeneracies. To understand this, recall that \u03b7 represents the size of the weights \u03b1t chosen in every round, and was introduced as a tunable parameter to help achieve the best possible upper bound on zero-one error. More precisely, recall that the exponential loss Lexp\u03b7 (s) of the unweighted state, defined in (35), is equal to the exponential loss Lexp(f) on the weighted state, defined in (36), which in turn is an upper bound on the error Lerr(fT ) of the final weighted state fT . Therefore the potential value \u03c6T (0) based on the exponential loss L exp \u03b7 is an upper bound on the minimum error attainable after T rounds of boosting. At the same time, \u03c6T (0) is a function of \u03b7. Therefore, we may tune this parameter to attain the best bound possible. Even with this motivation, it may seem that a properly tuned \u03b7 will not be as small as in Lemma 19, especially since it can be shown that the resulting loss bound \u03c6T (0) will always be larger than a fixed constant (depending on \u03b3, k), no matter how many rounds T of boosting is used. However, the next result identifies conditions under which the tuned value of \u03b7 is indeed as small as in Lemma 19. This happens when the edge \u03b3 is very small, as is described in the next theorem. Intuitively, a weak classifier achieving small edge has low accuracy, and a low weight reflects Booster\u2019s lack of confidence in this classifier.\nTheorem 21 When using the exponential loss function (35), and the minimal weak learning condition (16), the loss upper bound \u03c6T (0) provided by Theorem 16 is more than 1 and hence trivial unless the parameter \u03b7 is chosen sufficiently small compared to the edge \u03b3:\n\u03b7 \u2264 k\u03b3 1\u2212 \u03b3 . (58)\nIn particular, when the edge is very small: \u03b3 \u2264 min { 1\n2 ,\n1\n8k min\n{ 1\nk ,\n1\nT\n}} , (59)\nthe value of \u03b7 needs to be as small as in (50).\nProof Comparing solutions (49) and (34) to the potentials corresponding to the minimal weak learning condition and a fixed edge-over-random condition, we may conclude that the loss bound \u03c6T (0) is in the former case is larger than \u03c6 b T (0), for any edge-over-random distribution b \u2208 \u2206k\u03b3 . In particular, when b is set to be the \u03b3-biased uniform distribution u, as defined in (51), we get \u03c6T (0) \u2265 \u03c6uT (0). Now the latter bound, according to (37), is \u03ba(\u03b3, \u03b7)T , where \u03ba is defined as in (38). Therefore, to get non-trivial loss bounds which are at most 1, we need to choose \u03b7 such that \u03ba(\u03b3, \u03b7) \u2264 1. By (38), this happens when(\n1\u2212 e\u2212\u03b7 ) \u03b3 \u2265 ( e\u03b7 + e\u2212\u03b7 \u2212 2 )(1\u2212 \u03b3 k ) i.e., k\u03b3\n1\u2212 \u03b3 \u2265 e \u03b7 + e\u2212\u03b7 \u2212 2 1\u2212 e\u2212\u03b7 = e\u03b7 \u2212 1 \u2265 \u03b7.\nTherefore (58) holds. When \u03b3 is as small as in (59), then 1\u2212 \u03b3 \u2264 12 , and therefore, by (58), the bound on \u03b7 becomes identical to that in (59).\nThe condition in the previous theorem, that of the existence of only a very small edge, is the most we can assume for most practical datasets. Therefore, in such situations, we can compute the optimal Booster strategy that uses the minimal weak learning conditions. More importantly, using this result, we derive, in the next section, a highly efficient and practical adaptive algorithm, that is, one that does not require any prior knowledge about the edge \u03b3, and will therefore work with any dataset.\n8. Variable edges\nSo far we have required Weak-Learner to beat random by at least a fixed amount \u03b3 > 0 in each round of the boosting game. In reality, the edge over random is larger initially, and gets smaller as the OS algorithm creates harder cost matrices. Therefore requiring a fixed edge is either unduly pessimistic or overly optimistic. If the fixed edge is too small, not enough progress is made in the initial rounds, and if the edge is too large, Weak-Learner fails to meet the weak-learning condition in latter rounds. We fix this by not making any assumption about the edges, but instead adaptively responding to the edges returned by Weak-Learner. In the rest of the section we describe the adaptive procedure, and the resulting loss bounds guaranteed by it.\nThe philosophy behind the adaptive algorithm is a boosting game where Booster and Weak Learner no longer have opposite goals, but cooperate to reduce error as fast as possible. However, in order to create a clean abstraction and separate implementations of the boosting algorithms and the weak learning procedures as much as possible, we assume neither of the players has any knowledge of the details of the algorithm employed by the other player. In particular Booster may only assume that Weak Learner\u2019s strategy is barely strong enough to guarantee boosting. Therefore, Booster\u2019s demands on the weak classifiers returned by Weak Learner should be minimal, and it should send the weak learning algorithm the \u201ceasiest\u201d cost matrices that will ensure boostability. In turn, Weak Learner may only assume a very weak Booster strategy, and therefore return a weak classifier that performs as well as possible with respect to the cost matrix sent by Booster.\nAt a high level, the adaptive strategy proceeds as follows. At any iteration, based on the states of the examples and number of remaining rounds of boosting, Booster chooses the game-theoretically optimal cost matrix assuming only infinitesimal edges in the remaining rounds. Intuitively, Booster has no high expectations of Weak Learner, and supplies it the easiest cost matrices with which it may be able to boost. However, in the adaptive setting, Weak-Learner is no longer adversarial. Therefore, although only infinitesimal edges are anticipated by Booster, Weak Learner cooperates in returning weak classifiers that achieve as large edges as possible, which will be more than just inifinitesimal. Based on the exact edge received in each round, Booster chooses the weight \u03b1t adaptively to reach the most favourable state possible. Therefore, Booster plays game theoretically assuming an adversarial Weak Learner and expecting only the smallest edges in the future rounds, although Weak Learner actually cooperates, and Booster adaptively exploits this favorable behavior as much as possible. This way the boosting algorithm remains robust to a poorly\nperforming Weak Learner, and yet can make use of a powerful weak learning algorithm whenever possible.\nWe next describe the details of the adaptive procedure. With variable weights we need to work with the weighted state ft(i) of each example i, defined in (22). To keep the compuations tractable, we will only be working with the exponential loss Lexp(f) on the weighted states. We first describe how Booster chooses the cost-matrix in each round. Following that we describe how it adaptively computes the weights in each round based on the edge of the weak classifier received.\nChoosing the cost-matrix. As discussed before, at any iteration t and state ft Booster assumes that it will receive an infinitesimal edge \u03b3 in each of the remaining rounds. Since the step size is a function of the edge, which in turn is expected to be the same tiny value in each round, we may assume that the step size in each round will also be some fixed value \u03b7. We are therefore in the setting of Theorem 21, which states that the parameter \u03b7 in the exponential loss function (35) should also be tiny to get any non-trivial bound. But then the loss function satisfies the conditions in Lemma 19, and by Theorem 20, the game theoretically optimal strategy remains the same whether we use the minimal condition or (Ceor,U\u03b3). When using the latter condition, the optimal choice of the cost-matrix at iteration t and state ft, according to (39), is\nCt(i, l) = { (e\u03b7 \u2212 1) eft\u22121(i,j)\u2212ft\u22121(i,1) if l > 1, (e\u2212\u03b7 \u2212 1) \u2211k j=2 e ft\u22121(i,j)\u2212ft\u22121(i,1) if l = 1. (60)\nFurther, when using the condition (Ceor,U\u03b3), the average potential of the states ft(i), according to (37), is given by the average loss (40) of the state times \u03ba(\u03b3, \u03b7)T\u2212t, where the function \u03ba is defined in (38). Our goal is to choose \u03b7 as a function of \u03b3 so that \u03ba(\u03b3, \u03b7) is as small as possible. Now, there is no lower bound on how small the edge \u03b3 may get, and, anticipating the worst, it makes sense to choose an infinitesimal \u03b3, in the spirit of (Freund, 2001). Eq. (38) then implies that the choice of \u03b7 should also be infinitesimal. Then the above choice of the cost matrix becomes the following (after some rescaling):\nCt(i, l) = lim \u03b7\u21920\nC\u03b7(i, l) M =\n1\n\u03b7 { (e\u03b7 \u2212 1) eft\u22121(i,j)\u2212ft\u22121(i,1) if l > 1, (e\u2212\u03b7 \u2212 1) \u2211k j=2 e ft\u22121(i,j)\u2212ft\u22121(i,1) if l = 1.\n=\n{ eft\u22121(i,j)\u2212ft\u22121(i,1) if l > 1,\n\u2212 \u2211k\nj=2 e ft\u22121(i,j)\u2212ft\u22121(i,1) if l = 1.\n(61)\nWe have therefore derived the optimal cost matrix played by the adaptive boosting strategy, and we record this fact.\nLemma 22 Consider the boosting game using the minimal weak learning condition (16). Then, in iteration t at state ft, the game-theoretically optimal Booster strategy chooses the cost matrix Ct given in (61).\nWe next show how to adaptively choose the weights \u03b1t.\nAdaptively choosing weights. Once Weak Learner returns a weak classifier ht, Booster chooses the optimum weight \u03b1t so that the resulting states ft = ft\u22121 +\u03b1t1ht are as favorable as possible, that is, minimizes the total potential of its states. By our previous discussions, these are proportional to the total loss given by Zt = \u2211m i=1 \u2211k l=2 e\nft(i,l)\u2212ft(i,1). For any choice of \u03b1t, the difference Zt\u2212Zt\u22121 between the total loss in rounds t\u22121 and t is given by\n(e\u03b1t \u2212 1) \u2211 i\u2208S\u2212 eft\u22121(i,ht(i))\u2212ft\u22121(i,1) \u2212 ( 1\u2212 e\u2212\u03b1t ) \u2211 i\u2208S+ Lexp(ft\u22121(i))\n= (e\u03b1t \u2212 1)At\u2212 \u2212 ( 1\u2212 e\u2212\u03b1t ) At+\n= ( At+e \u2212\u03b1t +At\u2212e \u03b1t ) \u2212 ( At+ +A t \u2212 ) ,\nwhere S+ denotes the set of examples that ht classifies correctly, S\u2212 the incorrectly classified examples, and At\u2212, A t + denote the first and second summations, respectively. Therefore, the task of choosing \u03b1t can be cast as a simple optimization problem minimizing the previous expression. In fact, the optimal value of \u03b1t is given by the following closed form expression\n\u03b1t = 1\n2 ln ( At+ At\u2212 ) . (62)\nWith this choice of weight, one can show (with some straightforward algebra) that the total loss of the state falls by a factor less than 1. In fact the factor is exactly\n(1\u2212 ct)\u2212 \u221a c2t \u2212 \u03b42t , (63)\nwhere ct = (A t + +A t \u2212)/Zt\u22121, (64)\nand \u03b4t is the edge of the returned classifier ht on the supplied cost-matrix Ct. Notice that the quantity ct is at most 1, and hence the factor (63) can be upper bounded by \u221a 1\u2212 \u03b42t . We next show how to compute the edge \u03b4t. The definition of the edge depends on the weak learning condition being used, and in this case we are using the minimal condition (16). Therefore the edge \u03b4t is the largest \u03b3 such that the following still holds\nCt \u2022 1h \u2264 max B\u2208Beor\u03b3 Ct \u2022B.\nHowever, since Ct is the optimal cost matrix when using exponential loss with a tiny value of \u03b7, we can use arguments in the proof of Theorem 20 to simplify the computation. In particular, eq. (56) implies that the edge \u03b4t may be computed as the largest \u03b3 satisfying the following simpler inequality\n\u03b4t = sup { \u03b3 : Ct \u2022 1ht \u2264 Ct \u2022U\u03b3 } = sup { \u03b3 : Ct \u2022 1ht \u2264 \u2212\u03b3\nm\u2211 i=1 k\u2211 l=2 eft\u22121(i,l)\u2212ft\u22121(i,1)\n}\n=\u21d2 \u03b4t = \u03b3 : Ct \u2022 1ht = \u2212\u03b3 m\u2211 i=1 k\u2211 l=2 eft\u22121(i,l)\u2212ft\u22121(i,1) =\u21d2 \u03b4t = \u2212Ct \u2022 1ht\u2211m\ni=1 \u2211k l=2 e ft\u22121(i,l)\u2212ft\u22121(i,1) = \u2212Ct \u2022 1ht Zt , (65)\nwhere the first step follows by expanding Ct \u2022U\u03b3 . We have therefore an adaptive strategy which efficiently reduces error. We record our results.\nLemma 23 If the weight \u03b1t in each round is chosen as in (62), and the edge \u03b4t is given by (65), then the total loss Zt falls by the factor given in (63), which is at most \u221a 1\u2212 \u03b42t .\nThe choice of \u03b1t in (62) is optimal, but depends on quantities other than just the edge \u03b4t. We next show a way of choosing \u03b1t based only on \u03b4t that still causes the total loss to drop by a factor of \u221a 1\u2212 \u03b42t .\nLemma 24 Suppose cost matrix Ct is chosen as in (61), and the returned weak classifier ht has edge \u03b4t i.e. Ct \u2022 1ht \u2264 Ct \u2022U\u03b4t. Then choosing any weight \u03b1t > 0 for ht makes the loss Zt at most a factor\n1\u2212 1 2 (e\u03b1t \u2212 e\u2212\u03b1t)\u03b4t + 1 2 (e\u03b1t + e\u2212\u03b1t \u2212 2)\nof the previous loss Zt\u22121. In particular by choosing\n\u03b1t = 1\n2 ln ( 1 + \u03b4t 1\u2212 \u03b4t ) , (66)\nthe drop factor is at most \u221a\n1\u2212 \u03b42t .\nProof We borrow notation from earlier discussions. The edge-condition implies\nAt\u2212 \u2212At+ = Ct \u2022 1ht \u2264 Ct \u2022U\u03b4t = \u2212\u03b4tZt\u22121 =\u21d2 At+ \u2212At\u2212 \u2265 \u03b4tZt\u22121.\nOn the other hand, the drop in loss after choosing ht with weight \u03b1t is( 1\u2212 e\u2212\u03b1t ) At+ \u2212 (e\u03b1t \u2212 1)At\u2212\n=\n( e\u03b1t \u2212 e\u2212\u03b1t\n2\n)( At+ \u2212At\u2212 ) \u2212 ( e\u03b1t + e\u2212\u03b1t \u2212 2\n2\n)( At+ +A t \u2212 ) .\nWe have already shown that At+ \u2212 At\u2212 \u2265 \u03b4tZt\u22121. Further, At+ + At\u2212 is at most Zt\u22121. Therefore the loss drops by a factor of at least\n1\u2212 1 2 (e\u03b1t \u2212 e\u2212\u03b1t)\u03b4t + 1 2 (e\u03b1t + e\u2212\u03b1t \u2212 2) = 1 2\n{ (1\u2212 \u03b4t)e\u03b1t + (1 + \u03b4t)e\u2212\u03b1t } .\nTuning \u03b1t as in (66) causes the drop factor to be at least \u221a 1\u2212 \u03b42t .\nAlgorithm 1 contains pseudocode for the adaptive algorithm, and includes both ways of choosing \u03b1t. We call both versions of this algorithm AdaBoost.MM. With the approximate way of choosing the step length in (67), AdaBoost.MM turns out to be identical to AdaBoost.M2 (Freund and Schapire, 1997) or AdaBoost.MR (Schapire and Singer, 1999), provided the weak classifier space is transformed in an appropriate way to be acceptable by AdaBoost.M2 or AdaBoost.MR. We emphasize that AdaBoost.MM and AdaBoost.M2 are products of very different theoretical considerations, and this similarity should be viewed as a coincidence arising because of the particular choice of loss function, infinitesimal edge and approximate step size. For instance, when the step sizes are chosen instead as in (68), the training error falls more rapidly, and the resulting algorithm is different.\nAs a summary of all the discussions in the section, we record the following theorem.\nAlgorithm 1 AdaBoost.MM\nRequire: Number of classes k, number of examples m. Require: Training set {(x1, y1), . . . , (xm, ym)} with yi \u2208 {1, . . . , k} and xi \u2208 X.\n\u2022 Initialize m\u00d7 k matrix f0(i, l) = 0 for i = 1, . . . ,m, and l = 1, . . . , k. for t = 1 to T do \u2022 Choose cost matrix Ct as follows:\nCt(i, l) = { eft\u22121(i,l)\u2212ft\u22121(i,yi) if l 6= yi, \u2212 \u2211\nl 6=yi e ft\u22121(i,j)\u2212ft\u22121(i,yi) if l = 1.\n\u2022 Receive weak classifier ht : X \u2192 {1, . . . , k} from weak learning algorithm \u2022 Compute edge \u03b4t as follows:\n\u03b4t = \u2212 \u2211m\ni=1Ct(i, ht(xi))\u2211m i=1 \u2211 l 6=yi e ft\u22121(i,l)\u2212ft\u22121(i,yi)\n\u2022 Choose \u03b1t either as \u03b1t = 1\n2 ln ( 1 + \u03b4t 1\u2212 \u03b4t ) , (67)\nor, for a slightly bigger drop in the loss, as\n\u03b1t = 1\n2 ln\n(\u2211 i:ht(xi)=yi \u2211 l 6=yi e\nft\u22121(i,l)\u2212ft\u22121(i,yi)\u2211 i:ht(xi)6=yi e ft\u22121(i,ht(xi))\u2212ft\u22121(i,yi)\n) (68)\n\u2022 Compute ft as: ft(i, l) = ft\u22121(i, l) + \u03b1t1 [ht(xi) = l] .\nend for \u2022 Output weighted combination of weak classifiers FT : X \u00d7 {1, . . . , k} \u2192 R defined as:\nFT (x, l) = T\u2211 t=1 \u03b1t1 [ht(x) = l] . (69)\n\u2022 Based on FT , output a classifier HT : X \u2192 {1, . . . , k} that predicts as\nHT (x) = k\nargmax l=1 FT (x, l). (70)\nTheorem 25 The boosting algorithm AdaBoost.MM, shown in Algorithm 1, is the optimal strategy for playing the adaptive boosting game, and is based on the minimal weak learning condition. Further if the edges returned in each round are \u03b41, . . . , \u03b4T , then the error after T\nrounds is (k \u2212 1) \u220fT t=1 \u221a 1\u2212 \u03b42t \u2264 (k \u2212 1) exp { \u2212(1/2) \u2211T t=1 \u03b4 2 t } .\nIn particular, if a weak hypothesis space is used that satisfies the optimal weak learning condition (16), for some \u03b3, then the edge in each round is large, \u03b4t \u2265 \u03b3, and therefore the error after T rounds is exponentially small, (k \u2212 1)e\u2212T\u03b32/2.\nThe theorem above states that as long as the minimal weak learning condition is satisfied, the error will decrease exponentially fast. Even if the condition is not satisfied, the error rate will keep falling rapidly provided the edges achieved by the weak classifiers are relatively high. However, our theory so far can provide no guarantees on these edges, and therefore it is not clear what is the best error rate achievable in this case, and how quickly it is achieved. The assumptions of boostability, and hence our minimal weak learning condition does not hold for the vast majority of practical datasets, and as such it is important to know what happens in such settings. In particular, an important requirement is empirical consistency, where we want that for any given weak classifier space, the algorithm converge, if allowed to run forever, to the weighted combination of classifiers that minimizes error on the training set. Another important criterion is universal consistency, which requires that the algorithm converge, when provided sufficient training data, to the classifier combination that minimizes error on the test dataset. In the next section, we show that AdaBoost.MM satisfies such consistency requirements. Both the choice of the minimal weak learning condition as well as the setup of the adaptive game framework will play crucial roles in ensuring consistency. These results therefore provide evidence that game theoretic considerations can have strong statistical implications.\n9. Consistency of the adaptive algorithm\nThe goal in a classification task is to design a classifier that predicts with high accuracy on unobserved or test data. This is usually carried out by ensuring the classifier fits training data well without being overly complex. Assuming the training and test data are reasonably similar, one can show that the above procedure achieves high test accuracy, or is consistent. Here we work in a probabilistic setting that connects training and test data by assuming both consist of examples and labels drawn from a common, unknown distribution.\nConsistency for multiclass classification in the probabilistic setting has been studied by Tewari and Bartlett (2007), who show that, unlike in the binary setting, many natural approaches fail to achieve consistency. In this section, we show that AdaBoost.MM described in the previous section avoids such pitfalls and enjoys various consistency results. We begin by laying down some standard assumptions and setting up some notation. Then we prove our first result showing that our algorithm minimizes a certain exponential loss function on the training data at a fast rate. Next, we build upon this result and improve along two fronts: firstly we change our metric from exponential loss to the more relevant classification error metric, and secondly we show fast convergence on not just training data, but also the test set. For the proofs, we heavily reuse existing machinery in the literature.\nThroughout the rest of this section we consider the version of AdaBoost.MM that picks weights according to the approximate rule in (67). All our results most probably hold with the other rule for picking weights in (68) as well, but we did not verify that. These results hold without any boostability requirements on the space H of weak classifiers, and are therefore widely applicable in practice. While we do not assume any weak learning condition, we will require a fully cooperating Weak Learner. In particular, we will require that in each round Weak Learner picks the weak classifier suffering minimum cost with respect to the cost matrix provided by the boosting algorithm, or equivalently achieves the highest edge as defined in (65). Such assumptions are both necessary and standard in the literature, and are frequently met in practice.\nIn order to state our results, we will need to setup some notation. The space of examples will be denoted by X , and the set of labels by Y = {1, . . . , k}. We also fix a finite weak classifier space H consisting of classifiers h : X \u2192 Y. We will be interested in functions F : X \u00d7 Y \u2192 R that assign a score to every example and label pair. Important examples of such functions are the weighted majority combinations (69) output by the adaptive algorithm. In general, any such combination of the weak classifiers in space H is specified by some weight function \u03b1 : H \u2192 R; the resulting function is denoted by F\u03b1 : X \u00d7 Y \u2192 R, and satisfies:\nF\u03b1(x, l) = \u2211 h\u2208H \u03b1(h)1 [h(x) = l] .\nWe will be interested in measuring the average exponential loss of such functions. To measure this, we introduce the r\u0302isk operator:\nr\u0302isk(F ) M =\n1\nm m\u2211 i=1 \u2211 l 6=yi eF (xi,l)\u2212F (xi,yi). (71)\nWith this setup, we can now state our simplest consistency result, which ensures that the algorithm converges to a weighted combination of classifiers in the space H that achieves the minimum exponential loss over the training set at an efficient rate.\nLemma 26 The r\u0302isk of the predictions FT , as defined in (69), converges to that of the optimal predictions of any combination of the weak classifiers in H at the rate O(1/T ):\nr\u0302isk(FT )\u2212 inf \u03b1:H\u2192R\nr\u0302isk(F\u03b1) \u2264 C\nT , (72)\nwhere C is a constant depending only on the dataset.\nA slightly stronger result would state that the average exponential loss when measured with respect to the test set, and not just the empirical set, also converges. The test set is generated by some target distribution D over example label pairs, and we introduce the riskD operator to measure the exponential loss for any function F : X \u00d7Y \u2192 R with respect to D:\nriskD(F ) = E(x,y)\u223cD \u2211 l 6=y eF (x,l)\u2212F (x,y)  .\nWe show this stronger result holds if the function FT is modified to the function F\u0304T : X \u00d7 Y \u2192 R that takes values in the range [0,\u2212C], for some large constant C:\nF\u0304T (x, l) M = max { \u2212C,FT (x, l)\u2212max\nl\u2032 FT (x, l\n\u2032) } . (73)\nLemma 27 If F\u0304T is as in (73), and the number of rounds T is set to Tm = \u221a m, then its riskD converges to the optimal value as m\u2192\u221e with high probability:\nPr [ riskD ( F\u0304Tm ) \u2264 inf\nF :X\u00d7Y\u2192R riskD(F ) +O\n( m\u2212c )] \u2265 1\u2212 1\nm2 , (74)\nwhere c > 0 is some absolute constant, and the probability is over the draw of training examples.\nWe prove Lemmas 26 and 27 by demonstrating a strong correspondence between AdaBoost.MM and binary AdaBoost, and then leveraging almost identical known consistency results for AdaBoost (Bartlett and Traskin, 2007). Our proofs will closely follow the exposition in Chapter 12 of (Schapire and Freund, 2012) on the consistency of AdaBoost, and are deferred to the appendix.\nSo far we have focused on riskD, but a more desirable consistency result would state that the test error of the final classifier output by AdaBoost.MM converges to the Bayes optimal error. The test error is measured by the errD operator, and is given by\nerrD(H) = Pr (x,y)\u223cD\n[H(x) 6= y] . (75)\nThe Bayes optimal classifier Hopt is a classifier achieving the minimum error among all possible classifying functions\nerrD(Hopt) = inf H:X\u2192Y errD(H), (76)\nand we want our algorithm to output a classifier whose errD approaches errD(Hopt). In designing the algorithm, our main focus was on reducing the exponential loss, captured by riskD and r\u0302isk. Unless these loss functions are aligned properly with classification error, we cannot hope to achieve optimal error. The next result shows that our loss functions are correctly aligned, or more technically Bayes consistent. In other words, if a scoring function F : X \u00d7 Y \u2192 R is close to achieving optimal riskD, then the classifier H : X \u2192 Y derived from it as follows:\nH(x) \u2208 argmax l\u2208Y F (x, y), (77)\nalso approaches the Bayes optimal error.\nLemma 28 Suppose F is a scoring function achieving close to optimal risk\nriskD(F ) \u2264 inf F \u2032:X\u00d7Y\u2192R\nriskD(F \u2032) + \u03b5, (78)\nfor some \u03b5 \u2265 0. If H is the classifier derived from it as in (77), then it achieves close to the Bayes optimal error\nerrD(H) \u2264 errD(Hopt) + \u221a 2\u03b5. (79)\nProof The proof is similar to that of Theorem 12.1 in (Schapire and Freund, 2012), which in turn is based on the work by Zhang (2004) and Bartlett et al. (2006). Let p(x) = Pr(x\u2032,y\u2032)\u223cD (x\n\u2032 = x) denote the the marginalized probability of drawing example x from D, and let pxy = Pr(x\u2032,y\u2032)\u223cD [y\n\u2032 = y|x\u2032 = x] denote the conditional probability of drawing label y given we have drawn example x. We first rewrite the difference in errors between H and Hopt using these probabilities. Firstly note that the accuracy of any classifier H\n\u2032 is given by \u2211\nx\u2208X D(x,H \u2032(x)) = \u2211 x\u2208X p(x)pxH\u2032(x).\nIf X \u2032 is the set of examples where the predictions ofH andHopt differ, X \u2032 = {x \u2208 X : H(x) 6= Hopt(x)}, then we may bound the error differences as\nerrD(H)\u2212 errD(Hopt) = \u2211 x\u2208X \u2032 p(x) ( pxHopt(x) \u2212 p x H(x) ) . (80)\nWe next relate this expression to the difference of the losses. Notice that for any scoring function F \u2032, the riskD can be rewritten as follows :\nriskD(F \u2032) = \u2211 x\u2208X p(x) \u2211 l<l\u2032 { pxl e F \u2032(x,l\u2032)\u2212F \u2032(x,l) + pxl\u2032e F \u2032(x,l)\u2212F \u2032(x,l\u2032) } .\nDenote the inner summation in curly brackets by Ll,l \u2032\nF \u2032 (x), and notice this quantity is minimized if\neF \u2032(x,l)\u2212F \u2032(x,l\u2032) = \u221a pxl /p x l\u2032 , i.e., if F \u2032(x, l)\u2212 F \u2032(x, l\u2032) = 12 ln p x l \u2212 1 2 ln p x l\u2032 .\nTherefore, defining F \u2217(x, l) = 12 ln p x l leads to a riskD minimizing function F \u2217. Furthermore, for any example and pair of labels l, l\u2032, the quantity Ll,l \u2032\nF \u2217(x) is at most L l,l\u2032\nF (x), and therefore the difference in losses of F \u2217 and F may be lower bounded as follows:\n\u03b5 \u2265 riskD(F )\u2212 riskD(F \u2217) = \u2211 x\u2208X p(x) \u2211 l 6=l\u2032 ( Ll,l \u2032 F \u2212 L l,l\u2032 F \u2217 ) \u2265\n\u2211 x\u2208X \u2032 p(x) { L H(x),Hopt(x) F \u2212 L H(x),Hopt(x) F \u2217 } . (81)\nWe next study the term in the curly brackets for a fixed x. Let A and B denote H(x) and Hopt(x), respectively. We have already seen that L A,B F \u2217 = 2 \u221a pxAp x B. Further, by definition of Bayes optimality, pxA \u2265 pxB. On the other hand, since x \u2208 X \u2032, we know that B 6= A, and hence, F (x,A) \u2265 F (x,B). Let eF (x,B)\u2212F (x,A) = 1 + \u03b7, for some \u03b7 \u2265 0. The quantity LA,BF may be lower bounded as:\nLA,BF = p x Ae F (x,B)\u2212F (x,A) + pxBe F (x,A)\u2212F (x,B)\n= (1 + \u03b7)pxA + (1 + \u03b7) \u22121pxB \u2265 (1 + \u03b7)pxA + (1\u2212 \u03b7)pxB = pxA + p x B + \u03b7(p x A \u2212 pxB) \u2265 pxA + pxB.\nCombining we get\nLA,BF \u2212 L A,B F \u2217 \u2265 p x A + p x B \u2212 2 \u221a pxAp x B = (\u221a pxA \u2212 \u221a pxB )2 .\nPlugging back into (81) we get\n\u2211 x\u2208X \u2032 p(x) (\u221a pxH(x) \u2212 \u221a pxHopt(x) )2 \u2264 \u03b5. (82)\nNow we connect (80) to the previous expression as follows\n{errD(H)\u2212 errD(Hopt)}2\n= {\u2211 x\u2208X \u2032 p(x) ( pxHopt(x) \u2212 p x H(x) )}2\n\u2264 (\u2211 x\u2208X \u2032 p(x) )(\u2211 x\u2208X \u2032 p(x) ( pxHopt(x) \u2212 p x H(x) )2) (Cauchy-Schwartz)\n\u2264 \u2211 x\u2208X \u2032 p(x) (\u221a pxHopt(x) \u2212 \u221a pxH(x) )2(\u221a pxHopt(x) + \u221a pxH(x) )2 (83)\n\u2264 2 \u2211 x\u2208X \u2032 p(x) (\u221a pxHopt(x) \u2212 \u221a pxH(x) )2 (84) \u2264 2\u03b5, (by (82))\nwhere (83) holds since \u2211 x\u2208X \u2032 p(x) = Pr (x\u2032,y\u2032)\u223cD [ x\u2032 \u2208 X \u2032 ] \u2264 1,\nand (84) holds since\npxH(x) + p x Hopt(x)\n= Pr (x\u2032,y\u2032)\u223cD\n[ y\u2032 \u2208 {H(x), Hopt(x)} |x ] \u2264 1\n=\u21d2 \u221a pxH(x) + \u221a pxHopt(x) \u2264 \u221a 2.\nTherefore, errD(H)\u2212 errD(Hopt) \u2264 \u221a 2\u03b5.\nNote that the classifier H\u0304T , derived from the truncated scoring function F\u0304T in the manner provided in (77), makes identical predictions to, and hence has the same errD as, the classifier HT output by the adaptive algorithm. Further, Lemma 27 seems to suggest that F\u0304T satisfies the condition in (78), which, combined with our previous observation errD(H) = errD(H\u0304T ), would imply HT approaches the optimal error. However, the condition (78) requires achieving optimal risk over all scoring functions, and not just ones achievable as a combination of weak classifiers in H. Therefore, in order to use Lemma 28, we require the weak classifier space to be sufficiently rich, so that some combination of the weak classifiers in H attains riskD arbitrarily close to the minimum attainable by any function:\ninf \u03b1:H\u2192R riskD(F\u03b1) = inf F :X\u00d7Y\u2192R riskD(F ). (85)\nThe richness condition, along with our previous arguments and Lemma 27, immediately imply the following result.\nTheorem 29 If the weak classifier space H satisfies the richness condition (85), and the number of rounds T is set to \u221a m, then the error of the final classifier HT approaches the Bayes optimal error:\nPr [ errD ( H\u221am ) \u2264 errD(Hopt) +O ( m\u2212c )] \u2265 1\u2212 1\nm2 , (86)\nwhere c > 0 is some positive constant, and the probability is over the draw of training examples.\nA consequence of the theorem is our strongest consistency result:\nCorollary 30 Let Hopt be the Bayes optimal classifier, and let the weak classifier space H satisfy the richness condition (85). Suppose m example and label pairs {(x1, y1), . . . , (xm, ym)} are sampled from the distribution D, the number of rounds T is set to be \u221a m, and these are supplied to AdaBoost.MM. Then, in the limit m \u2192 \u221e, the final classifier H\u221am output by AdaBoost.MM achieves the Bayes optimal error almost surely:\nPr [{\nlim m\u2192\u221e\nerrD(H\u221am) } = errD(Hopt) ] = 1, (87)\nwhere the probability is over the randomness due to the draw of training examples.\nThe proof of Corollary 30, based on the Borel-Cantelli Lemma, is very similar to that of Corollary 12.3 in (Schapire and Freund, 2012), and so we omit it. When k = 2, AdaBoost.MM is identical to AdaBoost. For Theorem 29 to hold for AdaBoost, the richness assumption (85) is necessary, since there are examples due to Long and Servedio (2010) showing that the theorem may not hold when that assumption is violated.\nAlthough we have seen that technically AdaBoost.MM is consistent under broad assumptions, intuitively perhaps it is not clear what properties were responsible for this desirable behavior. We next briefly study the high level ingredients necessary for consistency in boosting algorithms.\nKey ingredients for consistency. We show here how both the choice of the loss function as well as the weak learning condition play crucial roles in ensuring consistency. If the loss function were not Bayes consistent as in Lemma 28, driving it down arbitrarily could still lead to high test error. For example, the loss employed by SAMME (Zhu et al., 2009) does not upper bound the error, and therefore although it can manage to drive down its loss arbitrarily when supplied by the dataset discussed in Figure 1, although its error remains high.\nEqually important is the weak learning condition. Even if the loss function is chosen to be error, so that it is trivially Bayes consistent, choosing the wrong weak learning condition could lead to inconsistency. In particular, if the weak learning condition is stronger than necessary, then, even on a boostable dataset where the error can be driven to zero, the boosting algorithm may get stuck prematurely because its stronger than necessary demands cannot be met by the weak classifier space. We have already seen theoretical examples of\nsuch datasets, and we will see some practical instances of this phenomenon in the next section.\nOn the other hand, if the weak learning condition is too weak, then a lazy Weak Learner may satisfy the Booster\u2019s demands by returning weak classifiers belonging only to a nonboostable subset of the available weak classifier space. For instance, consider again the dataset in Figure 1, and assume that this time the weak classifier space is much richer, and consists of all possible classifying functions. However, in any round, Weak Learner searches through the space, first trying hypotheses h1 and h2 shown in the figure, and only if neither satisfy the Booster, search for additional weak classifiers. In that case, any algorithm using SAMME\u2019s weak learning condition, which is known to be too weak and satisfiable by just the two hypotheses {h1, h2}, would only receive h1 or h2 in each round, and therefore be unable to reach the optimum accuracy. Of course, if the Weak Learner is extremely generous and helpful, then it may return the right collection of weak classifiers even with a null weak learning condition that places no demands on it. However, in practice, many Weak Learners used are similar to the lazy weak learner described since these are computationally efficient.\nTo see the effect of inconsistency arising from too weak learning conditions in practice, we need to test boosting algorithms relying on such datasets on significantly hard datasets, where only the strictest Booster strategy can extract the necessary service from Weak Learner for creating an optimal classifier. We did not include such experiments, and it will be an interesting empirical conjecture to be tested in the future. However, we did include experiments that illustrate the consequence of using too strong conditions, and we discuss those in the next section.\n10. Experiments\nIn the final section of this paper, we report preliminary experimental results on 13 UCI datasets: letter, nursery, pendigits, satimage, segmentation, vowel, car, chess, connect4, forest, magic04, poker, abalone. These datasets are all multiclass except for magic04, have a wide range of sizes, contain all combinations of real and categorical features, have different number of examples to number of features per example ratios, and are drawn from a variety of real-life situations. Most sets come with prespecified train and test splits which we use; if not, we picked a random 4 : 1 split. Throughout this section by MM we refer to the version of AdaBoost.MM studied in the consistency section, which uses the approximate step size (67).\nThere were two kinds of experiments. In the first, we took a standard implementation M1 of Adaboost.M1 with C4.5 as weak learner, and the Boostexter implementation MH of Adaboost.MH using stumps (Schapire and Singer, 2000), and compared it against our method MM with a naive greedy tree-searching weak-learner Greedy. The size of trees to be used can be specified to our weak learner, and was chosen to be the of the same order as the tree sizes used by M1. The test-error after 500 rounds of boosting for each algorithm and dataset is bar-plotted in Figure 7. The performance is comparable with M1 and far better than MH (understandably since stumps are far weaker than trees), even though our weak-learner is very naive. The convergence rates of error with rounds of M1 and MM are also comparable, as shown in Figure 8 (we omitted the curve for MH since it lay far above both M1 and MM).\nWe next investigated how each algorithm performs with less powerful weak-learners. We modified MH so that it uses a tree returning a single multiclass prediction on each example. For MH and MM we used the Greedy weak learner, while for M1 we used a more powerfulvariant Greedy-Info whose greedy criterion was information gain rather than error (we also ran M1 on top of Greedy but Greedy-Info consistently gave better results so we only report the latter). We tried all tree-sizes in the set {10, 20, 50, 100, 200, 500, 1000, 2000, 4000} up to the tree-size used by M1 on C4.5 for each data-set. We plotted the error of each algorithm against tree size for each data-set in Figure 9. As predicted by our theory, our algorithm succeeds in boosting the accuracy even when the tree size is too small to meet the stronger weak learning assumptions of the other algorithms. More insight is provided by plots in Figure 10 of the rate of convergence of error with rounds when the tree size allowed is very small (5). Both M1 and MH drive down the error for a few rounds. But since boosting keeps creating harder distributions, very soon the small-tree learning algorithms Greedy and Greedy-Info are no longer able to meet the excessive requirements of M1 and MH respectively. However, our algorithm makes more reasonable demands that are easily met by Greedy.\n11. Conclusion\nIn summary, we create a new framework for studying multiclass boosting. This framework is very general and captures the weak learning conditions implicitly used by many earlier multiclass boosting algorithms as well as novel conditions, including the minimal condition under which boosting is possible. We also show how to design boosting algorithms relying on these weak learning conditions that drive down training error rapidly. These algorithms are the optimal strategies for playing certain two player games. Based on this game-theoretic approach, we also design a multiclass boosting algorithm that is consistent, i.e., approaches the minimum empirical risk, and under some basic assumptions, the Bayes optimal test error. Preliminary experiments show that this algorithm can achieve much lower error compared to existing algorithms when used with very weak classifiers.\nAlthough we can efficiently compute the game-theoretically optimal strategies under most conditions, when using the minimal weak learning condition, and non-convex 0-1 error as loss function, we require exponential computational time to solve the corresponding boosting games. Boosting algorithms based on error are potentially far more noise tolerant than those based on convex loss functions, and finding efficiently computable near-optimal strategies in this situation is an important problem left for future work. Further, we primarily work with weak classifiers that output a single multiclass prediction per example, whereas weak hypotheses that make multilabel multiclass predictions are typically more powerful. We believe that multilabel predictions do not increase the power of the weak learner in our framework, and our theory can be extended without much work to include such hypotheses, but we do not address this here. Finally, it will be interesting to see if the notion of minimal weak learning condition can be extended to boosting settings beyond classification, such as ranking.\nAcknowledgments\nThis research was funded by the National Science Foundation under grants IIS-0325500 and IIS-1016029.\nAppendix\nOptimality of the OS strategy\nHere we prove Theorem 9. The proof of the upper bound on the loss is very similar to the proof of Theorem 2 in (Schapire, 2001). For the lower bound, a similar result is proven in Theorem 3 in (Schapire, 2001). However, the proof relies on certain assumptions that may not hold in our setting, and we instead follow the more direct lower bounding techniques in Section 5 of (Mukherjee and Schapire, 2010).\nWe first show that the average potential of states does not increase in any round. The dual form of the recurrence (24) and the choice of the cost matrix Ct in (25) together ensure that for each example i,\n\u03c6 B(i) T\u2212t (st(i)) = k max l=1\n{ \u03c6 B(i) T\u2212t\u22121 (st(i) + el)\u2212 (Ct(i)(l)\u2212 \u3008Ct(i),B(i)\u3009) } \u2265 \u03c6B(i)T\u2212t\u22121 ( st(i) + eht(xi) ) \u2212 (Ct(i, ht(xi))\u2212 \u3008Ct(i),B(i)\u3009) .\nSumming up the inequalities over all examples, we get\nm\u2211 i=1 \u03c6 B(i) T\u2212t\u22121 ( st(i) + eht(xi) ) \u2264 m\u2211 i=1 \u03c6 B(i) T\u2212t (st(i)) + m\u2211 i=1 {Ct(i, ht(xi))\u2212 \u3008Ct(i),B(i)\u3009}\nThe first two summations are the total potentials in round t+ 1 and t, respectively, and the third summation is the difference in the costs incurred by the weak-classifier ht returned in iteration t and the baseline B. By the weak learning condition, this difference is nonpositive, implying that the average potential does not increase.\nNext we show that the bound is tight. In particular choose any accuracy parameter \u03b5 > 0, and total number of iterations T , and let m be as large as in (28). We show that in any iteration t \u2264 T , based on Booster\u2019s choice of cost-matrix C, an adversary can choose a weak classifier ht \u2208 Hall such that the weak learning condition is satisfied, and the average potential does not fall by more than an amount \u03b5/T . In fact, we show how to choose labels l1, . . . , lm such that the following hold simultaneously:\nm\u2211 i=1 C(i, li) \u2264 m\u2211 i=1 \u3008C(i),B(i)\u3009 (88)\nm\u2211 i=1 \u03c6 B(i) T\u2212t (st(i)) \u2264 m\u03b5 T + m\u2211 i=1 \u03c6 B(i) T\u2212t\u22121 (st(i) + eli) (89)\nThis will imply that the final potential or loss is at least \u03b5 less than the bound in (26). We first construct, for each example i, a distribution pi \u2208 \u2206 {1, . . . , k} such that the size of the support of pi is either 1 or 2, and\n\u03c6 B(i) T\u2212t(st(i)) = El\u223cpi [ \u03c6 B(i) T\u2212t\u22121 (st(i) + el) ] . (90)\nTo satisfy (90), by (20), we may choose pi as any optimal response of the max player in the minmax recurrence when the min player chooses C(i):\npi \u2208 argmax p\u2208Pi\n{ El\u223cp [ \u03c6 B(i) t\u22121 (s + el) ]} (91)\nwhere Pi = {p \u2208 \u2206 {1, . . . , k} : El\u223cp [C(i, l)] \u2264 \u3008C(i),B(i)\u3009} . (92)\nThe existence of pi is guaranteed, since, by Lemma 7, the polytope Pi is non-empty for each i. The next result shows that we may choose pi to have a support of size 1 or 2.\nLemma 31 There is a pi satisfying (91) with either 1 or 2 non-zero coordinates.\nProof Let p\u2217 satisfy (91), and let its support set be S. Let \u00b5i denote the mean cost under this distribution: \u00b5i = El\u223cp\u2217 [C(i, l)] \u2264 \u3008C(i),B(i)\u3009 . If the support has size at most 2, then we are done. Further, if each non-zero coordinate l \u2208 S of p\u2217 satisfies C(i, l) = \u00b5i, then the distribution pi that concentrates all its weight on the label lmin \u2208 S minimizing \u03c6B(i)t\u22121 (s + elmin) is an optimum solution with support of size 1. Otherwise, we can pick labels lmin1 , l min 2 \u2208 S such that\nC(i, lmin1 ) < \u00b5i < C(i, l min 2 ).\nThen we may choose a distribution q supported on these two labels with mean \u00b5i:\nEl\u223cq [C(i, l)] = q(lmin1 )C(i, lmin1 ) + q(lmin2 )C(i, lmin2 ) = \u00b5i.\nChoose \u03bb as follows:\n\u03bb = min\n{ p\u2217(lmin1 )\nq(lmin1 ) , p\u2217(lmin2 ) q(lmin2 )\n} ,\nand write p\u2217 = \u03bbq + (1\u2212 \u03bb)p. Then both p,q belong to the polytope Pi, and have strictly fewer non-zero coordinates than p\u2217. Further, by linearity, one of q,p is also optimal. We repeat the process on the new optimal distribution till we find one which has only 1 or 2 non-zero entries.\nWe next show how to choose the labels l1, . . . , lm using the distributions pi. For each i, let { l+i , l \u2212 i } be the support of pi so that\nC ( i, l+i ) \u2264 El\u223cpi [C(i, l)] \u2264 C ( i, l\u2212i ) .\n(When pi has only one non-zero element, then l + i = l \u2212 i .) For brevity, we use p + i and p \u2212 i to denote pi ( l+i ) and pi ( l\u2212i ) , respectively. If the costs of both labels are equal, we assume without loss of generality that pi is concentrated on label l \u2212 i :\nC ( i, l\u2212i ) \u2212 C ( i, l\u2212i ) = 0 =\u21d2 p+i = 0, p \u2212 i = 1. (93)\nWe will choose each label li from the set { l\u2212i , l + i } . In fact, we will choose a partition S+, S\u2212 of the examples 1, . . . ,m and choose the label depending on which side S\u03be, for \u03be \u2208 {\u2212,+}, of the partition element i belongs to:\nli = l \u03be i if i \u2208 S\u03be.\nIn order to guide our choice for the partition, we introduce parameters ai, bi as follows:\nai = C(i, l \u2212 i )\u2212 C(i, l + i ), bi = \u03c6 B(i) T\u2212t\u22121 ( st(i) + el\u2212i ) \u2212 \u03c6B(i)T\u2212t\u22121 ( st(i) + el+i ) .\nNotice that for each example i and each sign-bit \u03be \u2208 {\u22121,+1}, we have the following relations:\nC(i, l\u03bei ) = El\u223cpi [C(i, l)]\u2212 \u03be(1\u2212 p \u03be i )ai (94)\n\u03c6 B(i) T\u2212t\u22121 ( st(i) + el\u03bei ) = El\u223cpi [ \u03c6 B(i) T\u2212t(i, l) ] \u2212 \u03be(1\u2212 p\u03bei )bi. (95)\nThen the cost incurred by the choice of labels can be expressed in terms of the parameters ai, bi as follows:\u2211\ni\u2208S+ C(i, l+i ) + \u2211 i\u2208S\u2212 C(i, l\u2212i ) = \u2211 i\u2208S+ { El\u223cpi [C(i, l)]\u2212 ai + p + i ai }\n+ \u2211 i\u2208S\u2212 { El\u223cpi [C(i, l)] + p + i ai }\n= m\u2211 i=1 El\u223cpi [C(i, l)] +  m\u2211 i=1 p+i ai \u2212 \u2211 i\u2208S+ ai  \u2264\nm\u2211 i=1 \u3008C(i),B(i)\u3009+  m\u2211 i=1 p+i ai \u2212 \u2211 i\u2208S+ ai  , (96) where the first equality follows from (94), and the inequality follows from the constraint on pi in (92). Similarly, the potential of the new states is given by\u2211\ni\u2208S+\n\u03c6 B(i) T\u2212t\u22121 ( st(i) + el+i ) + \u2211 i\u2208S\u2212 \u03c6 B(i) T\u2212t\u22121 ( st(i) + el\u2212i ) (97)\n= \u2211 i\u2208S+ { El\u223cpi [ \u03c6 B(i) T\u2212t\u22121 (st(i) + el) ] \u2212 bi + p+i bi } + \u2211 i\u2208S\u2212 { El\u223cpi [ \u03c6 B(i) T\u2212t\u22121 (st(i) + el) ] + p+i bi } = m\u2211 i=1 El\u223cpi [ \u03c6 B(i) T\u2212t\u22121 (st(i) + el) ] +  m\u2211 i=1 p+i bi \u2212 \u2211 i\u2208S+ bi\n =\nm\u2211 i=1 \u03c6 B(i) T\u2212t (st(i)) +  m\u2211 i=1 p+i bi \u2212 \u2211 i\u2208S+ bi  , (98) where the first equality follows from (95), and the last equality from an optimal choice of pi satisfying (90). Now, (96) and (98) imply that in order to satisfy (88) and (89), it suffices to choose a subset S+ satisfying\n\u2211 i\u2208S+ ai \u2265 m\u2211 i=1 p+i ai, \u2211 i\u2208S+ bi \u2264 m\u03b5 T + m\u2211 i=1 p+i bi. (99)\nWe simplify the required conditions. Notice the first constraint tries to ensure that S+ is big, while the second constraint forces it to be small, provided the bi are non-negative. However, if bi < 0 for any example i, then adding this example to S+ only helps both inequalities. In other words, if we can always construct a set S+ satisfying (99) in the case where the bi are non-negative, then we may handle the more general situation by just adding the examples i with negative bi to the set S+ that would be constructed by considering only the examples {i : bi \u2265 0}. Therefore we may assume without loss of generality that the bi\nare non-negative. Further, assume (by relabeling if necessary) that a1, . . . , am\u2032 are positive and am\u2032+1, . . . am = 0, for some m\n\u2032 \u2264 m. By (93), we have p+i = 0 for i > m\u2032. Therefore, by assigning the examples m\u2032 + 1, . . . ,m to the opposite partition S\u2212, we can ensure that (99) holds if the following is true:\n\u2211 i\u2208S+ ai \u2265 m\u2032\u2211 i=1 p+i ai, (100)\n\u2211 i\u2208S+ bi \u2264 m\u2032 max i=1 |bi|+ m\u2032\u2211 i=1 p+i bi, (101)\nwhere, for (101), we additionally used that, by the choice of m (28) and the bound on loss variation (27), we have\nm\u03b5\nT \u2265 (L, T ) \u2265 bi for i = 1, . . . ,m.\nThe next lemma shows how to construct such a subset S+, and concludes our lower bound proof.\nLemma 32 Suppose a1, . . . , am\u2032 are positive and b1, . . . , bm\u2032 are non-negative reals, and p+1 , . . . , p + m\u2032 \u2208 [0, 1] are probabilities. Then there exists a subset S+ \u2286 {1, . . . ,m\n\u2032} such that (100) and (101) hold.\nProof Assume, by relabeling if necessary, that the following ordering holds:\na(1)\u2212 b(1) a(1) \u2265 \u00b7 \u00b7 \u00b7 \u2265 a(m \u2032)\u2212 b(m\u2032) a(m\u2032) . (102)\nLet I \u2264 m\u2032 be the largest integer such that\na1 + a2 + \u00b7 \u00b7 \u00b7+ aI < m\u2032\u2211 i=1 p+i ai. (103)\nSince the p+i are at most 1, I is in fact at most m \u2032\u22121. We will choose S+ to be the first I+1 examples S+ = {1, . . . , I + 1}. Observe that (100) follows immediately from the definition of I. Further, (101) will hold if the following is true\nb1 + b2 + \u00b7 \u00b7 \u00b7+ bI \u2264 m\u2032\u2211 i=1 p+i bi, (104)\nsince the addition of one more example I + 1 can exceed this bound by at most bI+1 \u2264 maxm\n\u2032 i=1|bi|. We prove (104) by showing that the left hand side of this equation is not much\nmore than the left hand side of (103). We first rewrite the latter summation differently. The inequality in (103) implies we can pick p\u0303+1 , . . . , p\u0303 + m\u2032 \u2208 [0, 1] (e.g., by simply scaling the p+i \u2019s appropriately) such that\na1 + . . .+ aI = m\u2032\u2211 i=1 p\u0303+i ai (105)\nfor i = 1, . . . ,m\u2032: p\u0303+i \u2264 pi. (106)\nBy subtracting off the first I terms in the right hand side of (105) from both sides we get\n(1\u2212 p\u0303+1 )a1 + \u00b7 \u00b7 \u00b7+ (1\u2212 p\u0303 + I )aI = p\u0303 + I+1aI+1 + \u00b7 \u00b7 \u00b7+ p\u0303 + m\u2032am\u2032 .\nSince the terms in the summations are non-negative, we may combine the above with the ordering property in (102) to get\n(1\u2212 p\u0303+1 )a1 ( a1 \u2212 b1 a1 ) + \u00b7 \u00b7 \u00b7+ (1\u2212 p\u0303+I )aI ( aI \u2212 bI aI ) \u2265 p\u0303+I+1aI+1 ( aI+1 \u2212 bI+1\naI+1\n) + \u00b7 \u00b7 \u00b7+ p\u0303+m\u2032am\u2032 ( am\u2032 \u2212 bm\u2032\nam\u2032\n) . (107)\nAdding the expression\np\u0303+1 a1 ( a1 \u2212 b1 a1 ) + \u00b7 \u00b7 \u00b7+ p\u0303+I aI ( aI \u2212 bI aI ) to both sides of (107) yields\nI\u2211 i=1 ai ( ai \u2212 bi ai ) \u2265 m\u2032\u2211 i=1 p\u0303+i ai ( ai \u2212 bi ai )\ni.e. I\u2211 i=1 ai \u2212 I\u2211 i=1 bi \u2265 m\u2032\u2211 i=1 p\u0303+i ai \u2212 m\u2032\u2211 i=1 p\u0303+i bi\ni.e. I\u2211 i=1 bi \u2264 m\u2032\u2211 i=1 p\u0303+i bi, (108)\nwhere the last inequality follows from (105). Now (104) follows from (108) using (106) and the fact that the bi\u2019s are non-negative.\nThis completes the proof of the lower bound.\nConsistency proofs\nHere we sketch the proofs of Lemmas 26 and 27. Our approach will be to relate our algorithm to AdaBoost and then use relevant known results on the consistency of AdaBoost. We first describe the correspondence between the two algorithms, and then state and connect the relevant results on AdaBoost to the ones in this section.\nFor any given multiclass dataset and weak classifier space, we will obtain a transformed binary dataset and weak classifier space, such that the run of AdaBoost.MM on the original dataset will be in perfect correspondence with the run of AdaBoost on the transformed dataset. In particular, the loss and error on both the training and test set of the combined classifiers produced by our algorithm will be exactly equal to those produced by AdaBoost, while the space of functions and classifiers on the two datasets will be in correspondence.\nIntuitively, we transform our multiclass classification problem into a single binary classification problem in a way similar to the all-pairs multiclass to binary reduction. A very similar reduction was carried out by Freund and Schapire (1997). Borrowing their terminology, the transformed dataset roughly consists of mislabel triples (x, y, l) where y is the\ntrue label of the example and l is an incorrect example. The new binary label of a mislabel triple is always \u22121, signifying that l is not the true label. A multiclass classifier becomes a binary classifier that predict \u00b11 on the mislabel triple (x, y, l) depending on whether the prediction on x matches label l; therefore error on the transformed binary dataset is low whenever the multiclass accuracy is high. The details of the transformation are provided in Figure 11.\nSome of the properties between the functions and their transformed counterparts are described in the next lemma, showing that we are essentially dealing with similar objects.\nLemma 33 The following are identities for any scoring function F : X \u00d7 Y \u2192 R and weight function \u03b1 : H \u2192 R:\nr\u0302isk (F\u03b1) = \u02dc\u0302 risk ( F\u0303\u03b1\u0303 ) (109)\nriskD ( F\u0304 ) = r\u0303iskD ( \u00af\u0303 F ) . (110)\nThe proofs involve doing straightforward algebraic manipulations to verify the identities and are omitted.\nThe next lemma connects the two algorithms. We show that the scoring function output by AdaBoost when run on the transformed dataset is the transformation of the function output by our algorithm. The proof again involves tedious but straightforward checking of details and is omitted.\nLemma 34 If AdaBoost.MM produces scoring function F\u03b1 when run for T rounds with the training set S and weak classifier space H, then AdaBoost produces the scoring function F\u0303\u03b1\u0303 when run for T rounds with the training set S\u0303 and space H\u0303. We assume that for both the algorithms, Weak Learner returns the weak classifier in each round that achieves the maximum edge. Further we consider the version of AdaBoost.MM that chooses weights according to the approximate rule (67).\nWe next state the result for AdaBoost corresponding to Lemma 26 , which appears in (Mukherjee et al., 2011). .\nLemma 35 [Theorem 8 in (Mukherjee et al., 2011)] Suppose AdaBoost produces the scoring function F\u0303\u03b1\u0303 when run for T rounds with the training set S\u0303 and space H\u0303. Then\u02dc\u0302\nrisk ( F\u0303\u03b1\u0303 ) \u2264 inf\n\u03b2\u0303:H\u0303\u2192R\n\u02dc\u0302 risk ( F\u0303 \u03b2\u0303 ) + C/T, (111)\nwhere the constant C depends only on the dataset.\nThe previous lemma, along with (109) immediately proves Lemma 26. The result for AdaBoost corresponding to Lemma 27 appears in (Schapire and Freund, 2012).\nLemma 36 (Theorem 12.2 in (Schapire and Freund, 2012)) Suppose AdaBoost produces the scoring function F\u0303 when run for T = \u221a m rounds with the training set S\u0303 and space H\u0303. Then Pr [ riskD ( \u00af\u0303 F ) \u2264 inf\nF\u0303 \u2032:X\u0303\u2192R riskD(F\u0303 \u2032) +O\n( m\u2212c )] \u2265 1\u2212 1\nm2 , (112)\nwhere the constant C depends only on the dataset.\nThe proof of Lemma 27 follows immediately from the above lemma and (110).\nReferences\nJacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal stragies and minimax lower bounds for online convex games. In COLT, pages 415\u2013424, 2008.\nErin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of Machine Learning Research, 1: 113\u2013141, 2000.\nPeter L. Bartlett and Mikhail Traskin. AdaBoost is consistent. Journal of Machine Learning Research, 8:2347\u20132368, 2007.\nPeter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, March 2006.\nAlina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments. In Algorithmic Learning Theory: 20th International Conference, pages 247\u2013262, 2009.\nThomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via errorcorrecting output codes. Journal of Artificial Intelligence Research, 2:263\u2013286, January 1995.\nGu\u0308nther Eibl and Karl-Peter Pfeiffer. Multiclass boosting for weak classifiers. Journal of Machine Learning Research, 6:189\u2013210, 2005.\nYoav Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293\u2013318, June 2001.\nYoav Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256\u2013285, 1995.\nYoav Freund and Manfred Opper. Continuous drifting games. Journal of Computer and System Sciences, pages 113\u2013132, 2002.\nYoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148\u2013 156, 1996a.\nYoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages 325\u2013332, 1996b.\nYoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, August 1997.\nTrevor Hastie and Robert Tibshirani. Classification by pairwise coupling. Annals of Statistics, 26(2):451\u2013471, 1998.\nV. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30(1), February 2002.\nPhilip M. Long and Rocco A. Servedio. Random classification noise defeats all convex potential boosters. Machine Learning, 78:287\u2013304, 2010.\nIndraneel Mukherjee and Robert E. Schapire. Learning with continuous experts using drifting games. Theoretical Computer Science, 411(29-30):2670\u20132683, June 2010.\nIndraneel Mukherjee, Cynthia Rudin, and Robert E. Schapire. The rate of convergence of AdaBoost. In The 24th Annual Conference on Learning Theory, 2011.\nGunnar Ra\u0308tsch and Manfred K. Warmuth. Efficient margin maximizing with boosting. Journal of Machine Learning Research, 6:2131\u20132152, 2005.\nR. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.\nRobert E. Schapire. Drifting games. Machine Learning, 43(3):265\u2013291, June 2001.\nRobert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197\u2013227, 1990.\nRobert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.\nRobert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135\u2013168, May/June 2000.\nRobert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297\u2013336, December 1999.\nRobert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5): 1651\u20131686, October 1998.\nAmbuj Tewari and Peter L. Bartlett. On the Consistency of Multiclass Classification Methods. Journal of Machine Learning Research, 8:1007\u20131025, May 2007.\nTong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, 32(1):56\u2013134, 2004.\nJi Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class AdaBoost. Statistics and Its Interface, 2:349360, 2009."}], "references": [{"title": "Optimal stragies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "AdaBoost is consistent", "author": ["Peter L. Bartlett", "Mikhail Traskin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Traskin.,? \\Q2007\\E", "shortCiteRegEx": "Bartlett and Traskin.", "year": 2007}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Error-correcting tournaments", "author": ["Alina Beygelzimer", "John Langford", "Pradeep Ravikumar"], "venue": "In Algorithmic Learning Theory: 20th International Conference,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Solving multiclass learning problems via errorcorrecting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Multiclass boosting for weak classifiers", "author": ["G\u00fcnther Eibl", "Karl-Peter Pfeiffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Eibl and Pfeiffer.,? \\Q2005\\E", "shortCiteRegEx": "Eibl and Pfeiffer.", "year": 2005}, {"title": "An adaptive version of the boost by majority algorithm", "author": ["Yoav Freund"], "venue": "Machine Learning,", "citeRegEx": "Freund.,? \\Q2001\\E", "shortCiteRegEx": "Freund.", "year": 2001}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and Computation,", "citeRegEx": "Freund.,? \\Q1995\\E", "shortCiteRegEx": "Freund.", "year": 1995}, {"title": "Continuous drifting games", "author": ["Yoav Freund", "Manfred Opper"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Opper.,? \\Q2002\\E", "shortCiteRegEx": "Freund and Opper.", "year": 2002}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "In Machine Learning: Proceedings of the Thirteenth International Conference,", "citeRegEx": "Freund and Schapire.,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1996}, {"title": "Game theory, on-line prediction and boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "In Proceedings of the Ninth Annual Conference on Computational Learning Theory,", "citeRegEx": "Freund and Schapire.,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Hastie and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1998}, {"title": "The rate of convergence", "author": ["Indraneel Mukherjee", "Cynthia Rudin", "Robert E. Schapire"], "venue": "Theoretical Computer Science,", "citeRegEx": "Mukherjee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2010}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Schapire.,? \\Q2005\\E", "shortCiteRegEx": "Schapire.", "year": 2005}, {"title": "Improved boosting algorithms using confidence-rated", "author": ["Robert E. Schapire", "Yoram Singer"], "venue": "categorization. Machine Learning,", "citeRegEx": "Schapire and Singer.,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer.", "year": 2000}, {"title": "Boosting the margin", "author": ["Robert E. Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "venue": "predictions. Machine Learning,", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "On the Consistency of Multiclass Classification", "author": ["Ambuj Tewari", "Peter L. Bartlett"], "venue": null, "citeRegEx": "Tewari and Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 1998}, {"title": "Statistical behavior and consistency of classification methods based on convex", "author": ["Tong Zhang"], "venue": "ods. Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Zhang.", "year": 2007}, {"title": "Multi-class AdaBoost", "author": ["Zhu", "Hui Zou", "Saharon Rosset", "Trevor Hastie"], "venue": "Annals of Statistics,", "citeRegEx": "Zhu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 8, "context": "Specifically, it is known that the Boost-bymajority (Freund, 1995) algorithm is optimal in a certain sense, and that AdaBoost (Freund and Schapire, 1997) is a practical approximation.", "startOffset": 52, "endOffset": 66}, {"referenceID": 12, "context": "Specifically, it is known that the Boost-bymajority (Freund, 1995) algorithm is optimal in a certain sense, and that AdaBoost (Freund and Schapire, 1997) is a practical approximation.", "startOffset": 126, "endOffset": 153}, {"referenceID": 1, "context": "The most common approaches so far have relied on reductions to binary classification (Allwein et al., 2000), but it is hardly clear that the weak-learning conditions implicitly assumed by such reductions are the most appropriate.", "startOffset": 85, "endOffset": 107}, {"referenceID": 1, "context": "The most common approaches so far have relied on reductions to binary classification (Allwein et al., 2000), but it is hardly clear that the weak-learning conditions implicitly assumed by such reductions are the most appropriate. The purpose of a weak-learning condition is to clarify the goal of the weak-learner, thus aiding in its design, while providing a specific minimal guarantee on performance that can be exploited by a boosting algorithm. These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers, which in turn can help prevent overfitting. Furthermore, boosting algorithms that more efficiently and effectively minimize training error may prevent underfitting, which can also be important. In this paper, we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner. Unlike much, but not all, of the previous work on multiclass boosting, we focus specifically on the most natural, and perhaps weakest, case in which the weak classifiers are genuine classifiers in the sense of predicting a single multiclass label for each instance. Our new framework allows us to express a range of weak-learning conditions, both new ones and most of the ones that had previously been assumed (often only implicitly). Within this formalism, we can also now finally make precise what is meant by correct weak-learning conditions that are neither too weak nor too strong. We focus particularly on a family of novel weak-learning conditions that have an especially appealing form: like the binary conditions, they require performance that is only slightly better than random guessing, though with respect to performance measures that are more general than ordinary classification error. We introduce a whole family of such conditions since there are many ways of randomly guessing on more than two labels, a key difference between the binary and multiclass settings. Although these conditions impose seemingly mild demands on the weak-learner, we show that each one of them is powerful enough to guarantee boostability, meaning that some combination of the weak classifiers has high accuracy. And while no individual member of the family is necessary for boostability, we also show that the entire family taken together is necessary in the sense that for every boostable learning problem, there exists one member of the family that is satisfied. Thus, we have identified a family of conditions which, as a whole, is necessary and sufficient for multiclass boosting. Moreover, we can combine the entire family into a single weak-learning condition that is necessary and sufficient by taking a kind of union, or logical or, of all the members. This combined condition can also be expressed in our framework. With this understanding, we are able to characterize previously studied weak-learning conditions. In particular, the condition implicitly used by AdaBoost.MH (Schapire and Singer, 1999), which is based on a one-against-all reduction to binary, turns out to be strictly stronger than necessary for boostability. This also applies to AdaBoost.M1 (Freund and Schapire, 1996a), the most direct generalization of AdaBoost to multiclass, whose conditions can be shown to be equivalent to those of AdaBoost.MH in our setting. On the other hand, the condition implicit to the SAMME algorithm by Zhu et al. (2009) is too weak in the sense that even when the condition is satisfied, no boosting algorithm can guarantee to drive down the training error.", "startOffset": 86, "endOffset": 3517}, {"referenceID": 9, "context": "However, using the powerful machinery of drifting games (Freund and Opper, 2002; Schapire, 2001), we are able to compute the optimal strategy for the games arising out of each weak-learning condition in the family described above.", "startOffset": 56, "endOffset": 96}, {"referenceID": 0, "context": "These optimal strategies have a natural interpretation in terms of random walks, a phenomenon that has been observed in other settings (Abernethy et al., 2008; Freund, 1995).", "startOffset": 135, "endOffset": 173}, {"referenceID": 8, "context": "These optimal strategies have a natural interpretation in terms of random walks, a phenomenon that has been observed in other settings (Abernethy et al., 2008; Freund, 1995).", "startOffset": 135, "endOffset": 173}, {"referenceID": 12, "context": "algorithms (Schapire et al., 1998; Freund and Schapire, 1997; Koltchinskii and Panchenko, 2002).", "startOffset": 11, "endOffset": 95}, {"referenceID": 2, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset.", "startOffset": 42, "endOffset": 70}, {"referenceID": 12, "context": "The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 116, "endOffset": 143}, {"referenceID": 12, "context": "M2 (Freund and Schapire, 1997), as well as AdaBoost.", "startOffset": 3, "endOffset": 30}, {"referenceID": 1, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 4, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 5, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 13, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 8, "context": "The first one (Freund and Schapire, 1996b; R\u00e4tsch and Warmuth, 2005) views the weak-learning condition as a minimax game, while drifting games (Schapire, 2001; Freund, 1995) were designed to analyze the most efficient boosting algorithms.", "startOffset": 143, "endOffset": 173}, {"referenceID": 9, "context": "These games have been further analyzed in the multiclass and continuous time setting in (Freund and Opper, 2002).", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": ", 1998; Freund and Schapire, 1997; Koltchinskii and Panchenko, 2002). Consistency in the multiclass classification setting has been studied by Tewari and Bartlett (2007) and has been shown to be trickier than binary classification consistency.", "startOffset": 8, "endOffset": 170}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 43, "endOffset": 940}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 43, "endOffset": 958}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997). Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 (Freund and Schapire, 1997), as well as AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). Other approaches include the work by Eibl and Pfeiffer (2005); Zhu et al.", "startOffset": 43, "endOffset": 1251}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997). Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 (Freund and Schapire, 1997), as well as AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). Other approaches include the work by Eibl and Pfeiffer (2005); Zhu et al. (2009). There are also more general approaches that can be applied to boosting including (Allwein et al.", "startOffset": 43, "endOffset": 1270}, {"referenceID": 12, "context": "M1 (Freund and Schapire, 1997) measures the performance of weak classifiers using ordinary error.", "startOffset": 3, "endOffset": 30}, {"referenceID": 18, "context": "The SAMME algorithm of Zhu et al. (2009) requires the weak classifiers to achieve less error than uniform random guessing for multiple labels; in our language, their weak-learning condition is (C,U\u03b3), as shown in Section 3, where CSAM consists of cost matrices whose rows are of the form (0, t, t, .", "startOffset": 23, "endOffset": 41}, {"referenceID": 8, "context": "Our algorithms are derived from the very general drifting games framework (Schapire, 2001) for solving boosting games, which in turn was inspired by Freund\u2019s Boost-by-majority algorithm (Freund, 1995), which we review next.", "startOffset": 186, "endOffset": 200}, {"referenceID": 15, "context": "However, Schapire (2001) observed that the payoffs can be very well approximated by certain potential functions.", "startOffset": 9, "endOffset": 25}, {"referenceID": 15, "context": "When the weak learning condition being used is (C,B), Schapire (2001) proposed a Booster strategy, called the OS strategy, which always chooses the weight \u03b1t = 1, and uses the potential functions to construct a cost matrix Ct as follows.", "startOffset": 54, "endOffset": 70}, {"referenceID": 15, "context": "When the weak learning condition being used is (C,B), Schapire (2001) proposed a Booster strategy, called the OS strategy, which always chooses the weight \u03b1t = 1, and uses the potential functions to construct a cost matrix Ct as follows. Each row Ct(i) of the matrix achieves the minimum of the right hand side of (24) with b replaced by B(i), t replaced by T \u2212 t, and s replaced by current state st(i): Ct(i) = argmin c\u2208C0 k max l=1 { \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,B(i)\u3009) } . (25) The following theorem, proved in the appendix, provides a guarantee for the loss suffered by the OS algorithm, and also shows that it is the game-theoretically optimum strategy when the number of examples is large. Similar results have been proved by Schapire (2001), but our theorem holds much more generally, and also achieves tighter lower bounds.", "startOffset": 54, "endOffset": 752}, {"referenceID": 15, "context": "Using a different initial distribution, Schapire and Singer (1999) achieve the slightly better bound \u221a (k \u2212 1)e\u2212T\u03b32/2.", "startOffset": 40, "endOffset": 67}, {"referenceID": 7, "context": "Now, there is no lower bound on how small the edge \u03b3 may get, and, anticipating the worst, it makes sense to choose an infinitesimal \u03b3, in the spirit of (Freund, 2001).", "startOffset": 153, "endOffset": 167}, {"referenceID": 12, "context": "M2 (Freund and Schapire, 1997) or AdaBoost.", "startOffset": 3, "endOffset": 30}, {"referenceID": 18, "context": "Consistency for multiclass classification in the probabilistic setting has been studied by Tewari and Bartlett (2007), who show that, unlike in the binary setting, many natural approaches fail to achieve consistency.", "startOffset": 91, "endOffset": 118}, {"referenceID": 2, "context": "MM and binary AdaBoost, and then leveraging almost identical known consistency results for AdaBoost (Bartlett and Traskin, 2007).", "startOffset": 100, "endOffset": 128}, {"referenceID": 6, "context": "1 in (Schapire and Freund, 2012), which in turn is based on the work by Zhang (2004) and Bartlett et al.", "startOffset": 19, "endOffset": 85}, {"referenceID": 3, "context": "1 in (Schapire and Freund, 2012), which in turn is based on the work by Zhang (2004) and Bartlett et al. (2006). Let p(x) = Pr(x\u2032,y\u2032)\u223cD (x \u2032 = x) denote the the marginalized probability of drawing example x from D, and let py = Pr(x\u2032,y\u2032)\u223cD [y \u2032 = y|x\u2032 = x] denote the conditional probability of drawing label y given we have drawn example x.", "startOffset": 89, "endOffset": 112}, {"referenceID": 7, "context": "3 in (Schapire and Freund, 2012), and so we omit it. When k = 2, AdaBoost.MM is identical to AdaBoost. For Theorem 29 to hold for AdaBoost, the richness assumption (85) is necessary, since there are examples due to Long and Servedio (2010) showing that the theorem may not hold when that assumption is violated.", "startOffset": 19, "endOffset": 240}, {"referenceID": 16, "context": "MH using stumps (Schapire and Singer, 2000), and compared it against our method MM with a naive greedy tree-searching weak-learner Greedy.", "startOffset": 16, "endOffset": 43}, {"referenceID": 7, "context": "A very similar reduction was carried out by Freund and Schapire (1997). Borrowing their terminology, the transformed dataset roughly consists of mislabel triples (x, y, l) where y is the", "startOffset": 44, "endOffset": 71}], "year": 2011, "abstractText": "Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the \u201ccorrect\u201d requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.", "creator": "TeX"}}}