{"id": "1202.3743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Belief change with noisy sensing in the situation calculus", "abstract": "Since actions undertaken by agents cause constant changes in agents \"beliefs, the question of how to manage these changes is a very important question. Shapiro et al. [22] is one of the studies that has considered this issue. However, within this framework, the problem of noise perception, which often occurs in real-life applications, is not taken into account. Consequently, sound perceptions within this framework lead to an agent facing an inconsistent situation, and as a result, the agent cannot proceed any further. In this paper, we examine how noise perceptual actions can be dealt with in repeated changes of belief within the situation. We expand the framework proposed in [22] to include the ability to handle noise perceptions. We show that an agent can still detect the actual situation if the ratio of noise perceptual actions to exact perceptual actions is limited. We prove that our framework of repeated beliefs can be summed up by all of us [22], if we are able to do so in a more precise way.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (288kb)", "http://arxiv.org/abs/1202.3743v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jianbing ma", "weiru liu", "paul miller"], "accepted": false, "id": "1202.3743"}, "pdf": {"name": "1202.3743.pdf", "metadata": {"source": "CRF", "title": "Belief change with noisy sensing in the situation calculus", "authors": ["Jianbing Ma", "Weiru Liu"], "emails": ["jma03@qub.ac.uk,", "w.liu@qub.ac.uk,", "p.miller@ecit.qub.ac.uk"], "sections": [{"heading": null, "text": "Situation calculus has been applied widely in artificial intelligence to model and reason about actions and changes in dynamic systems. Since actions carried out by agents will cause constant changes of the agents\u2019 beliefs, how to manage these changes is a very important issue. Shapiro et al. [22] is one of the studies that considered this issue. However, in this framework, the problem of noisy sensing, which often presents in real-world applications, is not considered. As a consequence, noisy sensing actions in this framework will lead to an agent facing inconsistent situation and subsequently the agent cannot proceed further. In this paper, we investigate how noisy sensing actions can be handled in iterated belief change within the situation calculus formalism. We extend the framework proposed in [22] with the capability of managing noisy sensings. We demonstrate that an agent can still detect the actual situation when the ratio of noisy sensing actions vs. accurate sensing actions is limited. We prove that our framework subsumes the iterated belief change strategy in [22] when all sensing actions are accurate. Furthermore, we prove that our framework can adequately handle belief introspection, mistaken beliefs, belief revision and belief update even with noisy sensing, as done in [22] with accurate sensing actions only."}, {"heading": "1 Introduction", "text": "Situation calculus, introduced by John McCarthy [14, 15], has been applied widely to model and reason about actions and changes in dynamic systems. It was reinterpreted in [17] as basic action theories which are comprised of a set of foundational axioms defining the space of situations, unique-name axioms for actions, action preconditions and effects axioms, and the initial situation axioms [8]. The\nwell known frame problem is solved by a set of special action effects axioms called successor state axioms.\nSince actions carried out by agents cause constant changes of the agents\u2019 beliefs, developing strategies of managing belief changes triggered by actions is an important issue. The problem of iterated belief change within the framework of situation calculus has been investigated widely, e.g., [19, 20, 22, 13]. In [22], a new framework exceeding previous approaches was proposed in which a plausibility value is attached to every situation. This way, the framework is able to deal with nested beliefs, belief introspection, mistaken beliefs, and it can also handle belief revision and belief update (two types of belief changes) together in a seamless way.\nAlthough this framework has many distinct advantages, it suffers from some drawbacks. In this framework, a set of initial situations which an agent considers possible are given, then every time the agent performs a sensing action, situations that do not match the sensing result (e.g., a sensing result shows the light is on, while it is considered off in the situation) are discarded. A key assumption of the framework is that all sensing actions must be accurate, an assumption that is too strong in real-world scenarios. When sensing actions are not accurate, situations that perfectly match the actual situation are in fact discarded. Discarding such situations results in inconsistency and makes subsequent reasoning unproceedable.\nLet us illustrate this with the following example.\nExample 1 (adapted from [22]) Assume that the initial situation S0 is InR1(S0) \u2227 \u00acLight1(S0) \u2227 \u00acLight2(S0) which states that the agent is in Room 1 (InR1(S0)), the lights in both Room 1 and Room 2 (assume there are only two rooms) are off. However, the agent does not know the actual situation, e.g., which room it is in and whether the lights in these rooms are on/off. So it considers two possible situations S1 and S2 at the beginning where\nS1 = \u00acInR1(S1) \u2227 Light1(S1) \u2227 Light2(S1) S2 = InR1(S2) \u2227 \u00acLight1(S2) \u2227 \u00acLight2(S2)\nIt is easy to see that S2 perfectly matches the real situation\nS0. Not knowing the truth, the agent assigns S1 and S2 with plausibility values 0 and 1 respectively, which are the \u03ba-rankings in [24] such that the lower the plausibility value is, the more plausible the situation is. The bottom-half of Fig.1 illustrates these three situations.\nThe agent now believes that the most plausible situation is S1. To confirm, the agent performs a SenseLight action which senses whether the light is on in the room that the agent is currently located. A SenseLight (SL) action is accurate if it returns the true state of the situation and is noisy1 when it returns a wrong result. In Fig. 1, a Noisy SenseLight action is abbreviated as NSL. Given that the agent is in Room 1 and the light in Room 1 is off, this sensing action is noisy which prompts the agent to believe that S1 matches this sensing result whilst S2 does not. As a consequence, with this framework in [22], S2, the right situation, will be discarded. Now with only S1 considered possible, if the agent performs another sensing action, SenseRoom (which senses which room the agent is located) and it is accurate, then the agent immediately find that it faces an inconsistency (that is, the result of SenseRoom tells the agent it is in room R1 while the only possible situation shows it is not in R1) and cannot proceed. That is, noisy sensing cannot be handled properly by the framework.\nIn this paper, we extend the framework in [22] to manage noisy sensing actions. By using non-fixed plausibility values for situations, our formalism does not need to discard any situations but only changes their plausibility values, which makes it possible to accept noisy sensing actions. An important result is that we prove that rare noisy sensing\n1Note that the agent itself does not know whether a sensing action is accurate or noisy. The sensing is accurate or not is independent to the agent and the statistics of the accuracy of the sensing device (or sensing method) can be obtained in some way, such as training.\ndoes not play crucial roles in detecting the environment. More precisely, if the ratio of noisy sensing w.r.t. accurate sensing is relatively small, then an agent should be able to detect the actual situation. Furthermore, when the ratio is not that small but is restricted to a certain degree, the agent is still able to obtain a degree of chance for detecting the actual situation. On the other hand, when every sensing action is accurate, our extended framework is capable of discovering what can be derived from the framework in [22]. In addition, a set of desirable properties are proved to hold for this new formalism. In summary, we have the following main contributions.\n\u2022 With the new framework, an agent can continue to proceed with noisy sensing actions.\n\u2022 We prove that it does not affect the detection of the actual situation if there are only a few noisy sensing actions. Moreover, if the ratio of noisy sensing actions is restricted to a certain degree, then the framework has a very good chance to detect the actual situation.\n\u2022 When all sensing actions are accurate, the beliefs that can be induced by the framework of [22] can also be induced from our framework.\n\u2022 Belief introspection, mistaken beliefs, belief revision and update can all be well handled in our framework.\nThe rest of the paper is organized as follows. We provide the preliminaries on situation calculus in Sec. 2. In Sec. 3, some definitions and axioms needed by our framework are introduced. In Sec. 4, properties of our framework are proposed. Sec. 5 extends Example 1 to illustrate our extended framework. Finally, we discuss some related work and conclude the paper in Sec. 6 and Sec. 7, respectively."}, {"heading": "2 Background", "text": "The framework in [22] is based on an extension of the action theory [17] stemming from situation calculus [14, 15]. Here we introduce the notion of situation calculus from [22] which includes a belief operator [19, 20].\nThe situation calculus is a predicate calculus language for representing dynamically changing domains. A situation represents a snapshot of the domain. There is a set of initial situations corresponding to the ways agents believe what the domain might be initially. The actual initial state is represented by a distinguished initial situation constant, S0, which may or may not be among the set of initial situations believed by an agent. The term do(a, s) denotes the unique situation that results from the agent performing action a in situation s.\nA predicate (or function) whose value may change upon a situation (its last argument), is called a fluent. For instance,\nwe use the fluent InR1(s) to represent that the agent is in room R1 in situation s. The effects of actions on fluents are defined using successor state axioms [17], which provide a succinct representation for both effect axioms and frame axioms [14, 15]. For instance, in Example 1, if there is an action Leave which takes the agent from the current room to the other room, then the successor state axiom for InR1 is: InR1 ( do(a, s)\n) \u2261((\n\u00acInR1(s) \u2227 a = Leave ) \u2228 ( InR1(s) \u2227 a 6= Leave )) .\nThis axiom says that the agent will be in Room 1 after doing action a in s iff either it is in Room 2 and leaves for Room 1 or is currently in Room 1 and does not leave.\nLevesque [9] introduced a predicate, SF(a, s), to describe the result of performing the binary-valued sensing action a. SF(a, s) holds (returns true) iff the sensor associated with a returns the sensing value 1 in situation s. Each sensing action senses some property of the domain. The property sensed by an action is associated with the action using a guarded sensed fluent axiom [5]. For example, the following two axioms InR1(s) \u2192 ( SF(SenseLight, s) \u2261 Light1(s)\n) \u00acInR1(s) \u2192 ( SF(SenseLight, s) \u2261 Light2(s)\n) can be used to specify that the SenseLight action senses whether the light is on in the room the agent is currently located in Example 1. Let \u03c6[s] denote that fluent \u03c6 is believed at s, and M(a, s) = SF(a, s) \u2261 \u03c6[s] denote that the sensing result matches s, if a is used to sense \u03c6.\nScherl and Levesque [19, 20] defined a successor state axiom for B, an accessibility relation on situations based on the possible-worlds semantics by [16], that shows how actions, including sensing actions, affect the beliefs of an agent.\nB ( s\u2032\u2032, do(a, s) ) \u2261 \u2203s\u2032, (1)[\nB(s, s\u2032) \u2227 s\u2032\u2032 = do(a, s\u2032) \u2227 ( SF(a, s\u2032) \u2261 SF(a, s) )] .\nThe situations s\u2032\u2032 that are B-related to do(a, s) are the ones resulting from doing action a in a situation s\u2032, s.t., the sensor associated with a has the same value in s\u2032 as it has in s, where (informally) a situation s is B-related to another situation s\u2032 if in situation s, the agent considers s\u2032 is also possible. It follows the Kripke semantics.\nSimilar to [22], we take the following conventions about the guarded action theories \u03a3 consisting of: (A) successor state axioms for each fluent, and guarded sensed fluent axioms for each action; (B) unique names axioms for actions, and domain-independent foundational axioms; and (C) initial state axioms which describe the initial state of the domain and the initial beliefs of agents. A domaindependent fluent means a fluent other than B or pl (the plausibility of situations that will be defined later), and a domain-dependent formula is one that only mentions domain-dependent fluents. We further assume that there is only one agent acting in a chosen domain, although the\nframework is capable of accommodating multiple agents. In addition, we assume that there are only finite initial situations considered by the agent."}, {"heading": "3 Definitions and Axioms", "text": "In this section, we extend the framework in [22] to include a non-fixed plausibility operator to account for iterated belief changes in the situation calculus. The non-fixed plausibilities of situations enable us not to discard any situations, and hence the accessibility relation B becomes very succinct. Surprisingly, it has a more expressive power than that in [22].\nThe revised accessibility relation B is defined as follows.\nB ( s\u2032\u2032, do(a, s) ) \u2261 \u2203s\u2032, [ B(s, s\u2032) \u2227 s\u2032\u2032 = do(a, s\u2032) ] . (2)\nIt is clearly much simpler than Equation 1. Equation 2 ensures that if a set of situations are B-related, then their successors are also B-related, and so on. That is, if initially we have a set of B-related situations, then after the first action is taken, the successors of these situations are also B-related; after the second action is taken, the 2nd set of successors are again B-related.\nWe define a plausibility function pl for each situation s to measure how plausible an agent considers s to be. The pl function is in line with Spohn\u2019s ordinal conditional functions whose range is the set of natural numbers including 0. A lower pl(s) value indicates a higher plausibility level of s. The pl functions for the initial situations are given, with at least one s having pl(s) = 0 showing the greatest plausibility level, using an initial state axiom as:\nAxiom 1 (Initial State Axiom) \u2203s, Init(s) \u2227 pl(s) = 0.\nLet m def = maxs:Init(s)pl(s) be the maximum pl value (hence the minimal plausibility level) of the initial situations and ta,s def = mins\u2032:B(s\u2032,s)\u2227M(a,s\u2032)=truepl(s\n\u2032) be the minimum pl value among the B-related situations of s that match the sensing result of a. Plausibility values of successor situations are defined by the following successor state axiom:\nAxiom 2 (Successor State Axiom) pl ( do(a, s) ) = { pl(s)\u2212 ta,s a \u2208 SA \u2227 M(a, s) = true pl(s) + m + 1 a \u2208 SA \u2227M(a, s) = false pl(s) a 6\u2208 SA,\nHere and subsequently, we use SA to denote the set of all sensing actions, hence a \u2208SA indicates that a is a sensing action. The successor state axiom for pl says that if a is not a sensing action, then the successor situation has the same plausibility as its predecessor; if a is a sensing action and the current situation matches the sensing result, then the plausibility level of the successor situation should increase\n(i.e., the plausibility value decreases), and the plausibility of the successor of the most plausible situation is changed to 0 (for normalization); if a is a sensing action but the current situation does not match the sensing result, then the plausibility level of its successor should decrease, hence the plausibility value is increased by m + 1 to make sure it be greater than any of B-related situations that match the sensing result. This axiom follows a similar manner to OCF conditionalization [24].\nThis successor state axiom for pl follows the spirit of the intuition stated for the plausibility settings in [22] that if the accessible situation agrees with the actual situation upon the result of a sensing action, the plausibility of the accessible should increase (i.e., its \u03ba-ranking should decrease), otherwise the plausibility should decrease. However, this intuition was not implemented in the pl function in [22] because it is considered in conflict with positive and negative introspection of beliefs [22]. Instead, the framework adopted a fixed plausibility value for each situation and its successors (although discarding some situations makes the beliefs changeable). Our Axiom 2 can be seen as an improvement over this fixed value framework and offers an opportunity to allow flexible changes of plausibilities of situations and hence beliefs (and still be able to handle introspections).\nThe belief operator Bel is defined as follows.\nDefinition 1\nBel(\u03c6, s) def = \u2200s\u2032 ( B(s\u2032, s) \u2227 pl(s\u2032) = 0 \u2192 \u03c6[s\u2032] ) .\nThat is, \u03c6 is believed at s when it holds at all the most plausible situations that are B-related to s. This is intuitively the same as the one defined in [22]. The only difference is that in our framework, the plausibility values are not fixed and we always have B-related situations which have a plausibility value 0.\nFor initializing the B-related situations, we have\nAxiom 3 \u2200s, s\u2032, Init(s) \u2227 Init(s\u2032) \u2192 B(s\u2032, s).\nThat is, all initial situations are B-related to each other. Note that this axiom and the definition of B relations together ensure that an initial situation will not be accessible by a non-initial situation.\nCoupling with Equation 2, we have Theorem 1 \u2200s, s\u2032, B(s\u2032, s) \u2192 ( \u2200s\u2032\u2032, B(s\u2032\u2032, s\u2032) \u2261 B(s\u2032\u2032, s) ) .\nThat is, B is transitive and Euclidean, hence the positive and negative introspection of beliefs holds in our framework.\nActually, Equation (2) and Axiom 3 imply that all situations having the same number of antecedents are mutually\naccessible. This means that the B-relation does not play a crucial role in this paper. But we keep this relation to make the analog with related work obvious."}, {"heading": "4 Properties", "text": "In this section, we first present the key contribution of the paper, noisy sensing tolerance, which is beyond previous approaches. We then provide the counterparts of properties given in [22] which demonstrate that our framework faithfully extend their framework to noisy sensing situations. To make the comparisons easier to follow, in the corresponding subsections below, we have adopted many notations and results proposed in [22]."}, {"heading": "4.1 Noisy Sensing Tolerance", "text": "In this subsection, we show that our framework can deal with noisy sensing properly. More precisely, we prove that in the sense of probability, rare noisy sensing actions do not prevent an agent from deriving the actual situation.\nTo differentiate accurate sensing from noisy ones, for any sensing action a, let aT and aF denote that a is accurate or noisy, respectively2. Let P be a probability function that measures the statistical outcome of the accuracy of sensing actions performed by an agent [2]. For instance, P (a = aT ) \u2248 1 is interpreted as in a large number of experimental runs, the chance of a being accurate is almost certain and P (a = aF ) \u2248 0 means it is very rare that a returns a false result.\nDefinition 2 A situation calculus about a domain is called sensing-sensitive if there is a sequence (multi-set) of actions SEQ = {a1, \u00b7 \u00b7 \u00b7 , an} such that:\n\u2022 \u2200ai \u2208 SEQ, if ai \u2208 SA, then ai = aTi . \u2022 Let S\u2217 be the actual situation3, and\nS be any situation, then S \u2261 S\u2217 iff SF ( ai, do(ai\u22121, do(ai\u22122, \u00b7 \u00b7 \u00b7 , S) \u00b7 \u00b7 \u00b7) ) =\nSF ( ai, do(ai\u22121, do(ai\u22122, \u00b7 \u00b7 \u00b7 , S\u2217) \u00b7 \u00b7 \u00b7) ) , \u2200ai \u2208 SA \u2229 SEQ.\nSensing-sensitive means that the actual situation can be uniquely determined by an agent after taking a sequence of accurate sensing actions and other physically executable actions. This issue was not explicitly considered in [22] but could be seen as a default assumption for their framework.\n2Again here the phrase accurate or noisy is for the sensing device. The agent does not know whether each sensing is accurate or noisy, and he does not need to know. All he needs is the ability to train the device to obtain its probability of accuracy.\n3The actual situation is the true state of the current environment. Generally an agent does not know the actual situation, but it is able to know given this sensing-sensitive property.\nExample 2 (Example 1 Cont\u2019) Suppose all sensing actions are accurate, then the actual situation could be detected by a sequence of actions SEQ = {SR, SL, LEAVE, SL} where SR indicates that the agent senses whether it is in Room 1; LEAVE indicates that the agent leaves the current room for the other room; and SL is the same as in Example 1.\nIf SR,SL, SL are all accurate, then it is easy to see that S2 satisfies that for any sensing action ai in SEQ (i.e., SR, SL,SL), s.t., SF ( ai, do(ai\u22121, do(ai\u22122, \u00b7 \u00b7 \u00b7 , S2) \u00b7 \u00b7 \u00b7) ) =\nSF ( ai, do(ai\u22121, do(ai\u22122, \u00b7 \u00b7 \u00b7 , S0) \u00b7 \u00b7 \u00b7) ) , while S1 does not.\nLet sn be a situation believed by an agent after n actions, and Sn0 be the actual situation at the time these n actions were taken. Let I(X) be an indication function s.t. I(X) = 1 if \u03a3 |= X and I(X) = 0 otherwise. Now we show that the agent is able to detect the actual situation in the long run.\nTheorem 2 If P (a = aT |a \u2208 SA) \u2248 1 and if the framework is sensing-sensitive, then\n\u2200\u03c6, s, limn\u2192+\u221e \u2211n i=1 I(Bel(\u03c6,sn)\u2261Bel(\u03c6,Sn0 ))\nn = 1.\nThat is, if the noisy sensing actions are rare, and the framework is able to detect the actual situation by a sequence of accurate sensing actions, then after taking a finite number of actions, it can be sure that the actual situation can be detected in a probabilistic manner.\nThe proof is not difficult when the agent executes actions SEQ = {a1, \u00b7 \u00b7 \u00b7 , ak} repeatedly. Indeed for any sensing action a, let P (a = aT |a \u2208 SA) > 1 \u2212 \u00b2 where \u00b2 > 0 is an arbitrarily small real number, then it is easy to show that the expected accuracy rate for question whether the situation the agent detected is the actual situation, is no less than 1 \u2212 k\u00b2. Therefore, from a mathematical result on stochastic process, when n \u2192 +\u221e, \u2211n i=1\nI(Bel(\u03c6,sn)\u2261Bel(\u03c6,Sn0 )) n\nreduces to the above expected rate and from \u00b2 \u2192 0, we get the result.\nAs a corollary, if there is only a limited number of noisy sensing actions, then the agent is supposed to be able to detect the actual situation too. Let |M | denote the cardinality of a set M .\nCorollary 1 If |{a : a \u2208 SA \u2227 a = aF }| < +\u221e and the framework is sensing-sensitive, then \u2203N > 0, \u2200n > N , sn \u2261 Sn0 . However, if the probability of noisy sensing actions cannot be ignored, we have the following result.\nTheorem 3 If the framework is sensing-sensitive, then \u2200\u03c6, s, limn\u2192+\u221e \u2211n i=1 I(Bel(\u03c6,sn)\u2261Bel(\u03c6,Sn0 ))\nn \u2265\u220f ai\u2208SEQ P (ai = a T i ).\nThat is, even if the noisy sensing actions appear frequently,\nwe may still have a (relatively good) chance to obtain the true situation in the long run."}, {"heading": "4.2 Recovering the Beliefs in (Shapiro et al. 2011)", "text": "Similar to [22], for clarity, if there is no confusion, the situation argument of a fluent is often omitted in a belief operator, e.g., Bel(InR1, s). For comparison, let \u03a3S denote a guarded action theory used in [22] (with different definitions of B relations and successive state axioms for pl) in contrast to using \u03a3 for a guarded action theory in our framework. Similarly, we use plS and BelS for plausibility functions and beliefs in [22].\nIn [22], the successor state axiom for pl is defined as:\nplS(do(a, s)) = plS(s). (3)\nIts belief operator is defined as: BelS(\u03c6, s) def = \u2200s\u2032,\nB(s\u2032, s) \u2227 (\u2200s\u2032\u2032, B(s\u2032\u2032, s) \u2192 plS(s\u2032) \u2264 plS(s\u2032\u2032) ) \u2192 \u03c6[s\u2032].\nTwo axioms for initializing the B-related situations are used [22] to complete its framework. Axiom 4 [22] Init(s) \u2227 B(s\u2032, s) \u2192 ( \u2200s\u2032\u2032, B(s\u2032\u2032, s\u2032) \u2261 B(s\u2032\u2032, s) ) .\nThis axiom requires that B-relations to be transitive and Euclidean initially. The aim is to obtain positive and negative introspection of beliefs.\nAxiom 5 [22] Init(s) \u2227B(s\u2032, s) \u2192 Init(s\u2032).\nThis axiom says that an agent considers nothing happened initially, hence any situations that are B-related to an initial situation are themselves initial.\nNow we have the following result.\nTheorem 4 If \u2200a \u2208 SA, a = aT , then for any \u03c6, s, if \u03a3S |= BelS(\u03c6, s), then \u03a3 |= Bel(\u03c6, s).\nThis theorem shows that when all sensing actions are accurate, our framework truly discovers the beliefs which can be induced by the framework in [22].\nThe proof of the above theorem is not difficult. In fact, it is easy to show that if all sensing actions are accurate, then the situations that do not match the sensing results will henceforth have no chance to influence the change of beliefs, just like being discarded by the B-relations as done in [22]."}, {"heading": "4.3 Belief Revision", "text": "Belief revision studies how an agent\u2019s beliefs can be changed based on some new information if the new information must be believed. Sensing actions are a way of obtaining certain new information that an agent can use to\nrevise its beliefs about the actual situation without actually changing the environment. Sensing actions do not change any fluents, instead they modify the pl values corresponding to the degrees of beliefs of an agent. Therefore studying belief revision in situation calculus is a natural course for managing an agent\u2019s beliefs. In the following, we assume that for each formula \u03c6 to be revised, there is a corresponding sensing action.\nDefinition 3 (Uniform formula, adapted from [22]) A formula is uniform if it contains no unbound variables.\nDefinition 4 (Revision action for \u03c6, adapted from [22]) A revision action A for a uniform formula \u03c6 with respect to action theory \u03a3 is a sensing action that satisfies the following condition for every domain-dependent fluent F : \u03a3 |= [\u2200s, SF (A, s) \u2261 \u03c6[s]] \u2227 [\u2200s\u2200\u2212\u2192x , F (\u2212\u2192x , s) \u2261 F (\u2212\u2192x , do(A, s))], \u2212\u2192x is the set of arguments of F .\nIt means that A is a sensing action for \u03c6 which does not change any physical fluent. The following two theorems provided in [22] also hold in our framework.\nTheorem 5 Let \u03c6 be a domain-dependent, uniform formula, and A be a revision action for \u03c6 w.r.t. \u03a3, then we have: \u03a3 |= [\u2200s, \u03c6[s] \u2192 Bel(\u03c6, do(A, s))] \u2227[\u2200s,\u00ac\u03c6[s] \u2192 Bel(\u00ac\u03c6, do(A, s))].\nThis theorem proves that revision in our framework is handled adequately. That is, if the sensor indicates that \u03c6 holds, then the agent will believe that \u03c6 holds after performing A. Conversely, if the sensor shows that \u03c6 does not hold, then the agent will believe \u00ac\u03c6 after performing A. This theorem is also consistent with the framework in [19, 20].\nTheorem 6 Let A be a revision action for domaindependent, uniform formula \u03c6 w.r.t. \u03a3, then the following sentence is satisfiable: \u03a3\u222a{Bel(\u00ac\u03c6, S0), Bel(\u03c6, do(A, S0)),\u00acBel(FALSE, do(A, S0))}.\nThis theorem shows that even if the agent believes \u00ac\u03c6 in S0, it will believe \u03c6 after performing A when action A senses that \u03c6 is true, and still maintains consistent beliefs (\u00acBel(FALSE, do(A, S0)))."}, {"heading": "4.4 Introspection", "text": "Like [19, 20, 22], our framework supports belief introspection. Theorem 7 \u03a3 |= [Bel(\u03c6, s) \u2192 Bel(Bel(\u03c6), s)] \u2227[\u00acBel(\u03c6, s) \u2192 Bel(\u00acBel(\u03c6), s)].\nThis is not surprising, since in our framework the Brelation is transitive and Euclidean.\nNote that in [22], the beliefs are induced from the most plausible B-related situations instead of from all B-related situations. The proof of the above theorem is simply similar to the proof of Theorem 26 in [22].\nIn [22], it is argued that variations of their formalization where plausibility values are updated lead to problems with introspection. They may produce counterintuitive results about future beliefs (cf. [22] for details). Note that variations mentioned in [22] have plausibilities between Baccessible situations, while our plausibility is assigned to single situations, so our framework does not have such weakness, as can be seen from Theorem 4 (showing that we will not produce counterintuitive results since the framework [22] does not) and Theorem 7 (showing that introspection holds)."}, {"heading": "4.5 Awareness of errors", "text": "As in [22], suppose that an agent believes \u00ac\u03c6 in s, however after performing a revision action A in s, the agent discovers that \u03c6 is true and believes \u03c6. Then in do(A, s), the agent should believe that in the previous situation s, \u03c6 was true, but it believed \u03c6 was false. In other words, the agent should realize that it was mistaken about \u03c6 in s.\nDefinition 5 ([22]) Prev(\u03c6, s) def= \u2203a, s\u2032, s.t., s = do(a, s\u2032) \u2227 \u03c6[s\u2032].\nPrev(\u03c6, s) denotes that \u03c6 was held in the situation immediately before s.\nThe following theorem provided in [22] also holds here.\nTheorem 8 Let A be a revision action for a domaindependent, uniform formula \u03c6 w.r.t. \u03a3, then: \u03a3 |= \u2200s,Bel(\u00ac\u03c6, s) \u2227 Bel(\u03c6, do(A, s)) \u2192 Bel(Prev(\u03c6 \u2227 Bel(\u00ac\u03c6)), do(A, s)).\nThe ability of belief update [22] is also provable from our framework. We omitted it here due to space limitation."}, {"heading": "5 Example", "text": "We now extend Example 1 to illustrate our framework.\nExample 3 Assumption: one agent with three actions: the agent leaves the current room and enters the other room (LEAVE); the agent senses whether it is in Room 1 (SR); the agent senses whether the light is on in the room it is currently located (SL). And for noisy sensing, we annotate it as NSL. Note that the agent itself does not know whether the sensing is accurate or not.\nThe successor state axioms and guarded sensed fluent axioms for the example are as follows: Light1 ( do(a, s) ) \u2261 Light1(s)\nLight2 ( do(a, s) ) \u2261 Light2(s) InR1 ( do(a, s)\n) \u2261((\n\u00acInR1(s) \u2227 a = Leave ) \u2228 ( InR1(s) \u2227 a 6= Leave )) TRUE \u2192 ( SF(LEAVE, s) \u2261 TRUE\n) InR1(s) \u2192 ( SF(SL, s) \u2261 Light1(s)\n) \u00acInR1(s) \u2192 ( SF(SL, s) \u2261 Light2(s)\n) TRUE \u2192 ( SF(SR, s) \u2261 InR1(s) )\nThe initial states including the actual initial state are stated in Example 1. The initial state axioms for S1, S2 are:( \u2203s, Init(s) \u2227 pl(s) = 0 ) \u2227 ( \u2200s, Init(s) \u2227 pl(s) = 0 \u2192 \u00acInR1(s) \u2227 Light1(s) \u2227 Light2(s) ) ( \u2203s, Init(s) \u2227 pl(s) = 1 ) \u2227 ( \u2200s, Init(s) \u2227 pl(s) = 1 \u2192 InR1(s) \u2227 \u00acLight1(s) \u2227 \u00acLight2(s) )\nState axioms for other states are similar and omitted here. For simplicity, we only give an informal explanation of the first two steps in the process depicted by Figure 2.\n\u2022 The first action is a noisy light sensing, which gives the result that the light is on in the room the agent is located. As the agent thinks it is in Room 2, S1 matches the sensing result whilst S2 does not. Based on Axiom 2 (Sec. 3), the plausibility value of S12 increases to 2.\n\u2022 The second action is the room sensing, which tells the agent that it is in Room 1 not Room 2. Hence the agent reassigns S22 with plausibility 0 and S 2 1 with 1. Now\nits belief is in accord with S22 .\nIn fact, we can see that the action sequence SR,SL, LEAVE, SL makes the situation calculus sensing sensitive. Also note that the framework in [22] cannot proceed after the sensing action SR."}, {"heading": "6 Related Work", "text": "In [3], the problem of noisy sensors is also studied and a probabilistic method is applied for such situations. That is, the probability of a sensing action result follows a Gaussian distribution. In addition, the beliefs induced from the situations are also probabilistic. This approach is extended in [23] to study its properties and allows for using conditional probability densities in the noisy sensor readings. To some extent it could be seen as a probabilistic counterpart of our framework although noisy sensing tolerance is not addressed in these papers.\nIn [21], the noisy sensor problem is also studied by adding plausibility values into the B-relation, i.e., B(s\u2032, n, s) which means that in s, the agent thinks s\u2032 is possible with a plausibility value n. Similar to our approach, noisy sensing results will affect the n value based on whether the situation matches the sensing result. However, in [22], it is discussed that this kind of accessibility relations, together\nwith the transitive and Euclidean condition required by belief introspection, conflicts with any reasonable plausibility update scheme for accurate sensors.\nIn [6], a fluent calculus framework is proposed to deal with the problem of observations contradicting the model which is to some extent similar to noisy sensing. However, in the formalism, the state ranking change axioms need the details of actions which make the formalism somehow lose generality."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed a framework which can deal with noisy sensing actions and can handle nested beliefs, belief introspection, mistaken beliefs, belief revision and belief update. In addition, we show that rare noisy sensing does not prevent an agent from detecting the actual situation, and limited noisy sensing allows for a certain degree of detecting the actual situation. Moreover, we show that our framework induces what can be discovered by the framework of [22] when all sensing actions are accurate.\nDue to space limitation, in this paper, we do not provide the comparison of our framework with the various belief change postulates, e.g., AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc. However, we can prove that our framework satisfies most of the postulates mentioned above. Especially, DP\u2019s C2 postulate is satisfied in our framework, whilst in [22], C2 cannot be defined.\nFor future work, we want to extend our framework to multiple agent scenarios. It is also very interesting to study whether an agent can realize that the previous sensing is inaccurate in belief introspection. In addition, a study of the relationship between our framework and the Partially Observable Markov Decision Process (POMDP) [18] could be helpful."}], "references": [{"title": "On the logic of theory change: Partial meet functions for contraction and revision", "author": ["C.E. Alchourr\u00f3n", "P. G\u00e4rdenfors", "D. Makinson"], "venue": "Symbolic Logic,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "From statistical knowledge bases to degrees of belief", "author": ["F. Bacchus", "A. Grove", "J. Halpern", "D. Koller"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Reasoning about noisy sensors and effectors in the situation calculus", "author": ["F. Bacchus", "J. Halpern", "H. Levesque"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "On the logic of iterated belief revision", "author": ["A. Darwiche", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Progression using regression and sensors", "author": ["G. De Giacomo", "H. Levesque"], "venue": "In Procs. of IJCAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Representing beliefs in the fluent calculus", "author": ["Y. Jin", "M. Thielscher"], "venue": "In Procs. of ECAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Propositional knowledge base revision and minimal change", "author": ["H. Katsuno", "A.O. Mendelzon"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "A semantic characterization of a useful fragment of the situation calculus with knowledge", "author": ["G. Lakemeyer", "H. Levesque"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "What is planning in the presence of sensing", "author": ["H. Levesque"], "venue": "In Procs. of AAAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Modeling belief change on epistemic states", "author": ["J. Ma", "W. Liu"], "venue": "In Proc. of 22th Flairs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A framework for managing uncertain inputs: an axiomization of rewarding", "author": ["J. Ma", "W. Liu"], "venue": "Inter. J. of Appro. Reason.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "A belief revision framework for revising epistemic states with partial epistemic states", "author": ["J. Ma", "W. Liu", "S. Benferhat"], "venue": "In Proc. of AAAI\u201910,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Handling sequential observations in intelligent surveillance", "author": ["J. Ma", "W. Liu", "P. Miller"], "venue": "In Procs. of SUM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Situations, Actions and Causal Laws", "author": ["J. McCarthy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1963}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["J. McCarthy", "P. Hayes"], "venue": "In Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1969}, {"title": "Semantic considerations on nonmonotonic logic", "author": ["R. Moore"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression", "author": ["R. Reiter"], "venue": "In Aritificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John Mc- Carthy,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Extending DT- Golog to deal with POMDPs", "author": ["G. Rens", "A. Ferrein", "E. van der Poel"], "venue": "In Procs. 19th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The frame problem and knowledge-producing actions", "author": ["R. Scherl", "H. Levesque"], "venue": "In Procs. of AAAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Knowledge, action, and the frame problem", "author": ["R. Scherl", "H. Levesque"], "venue": "Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Belief change with noisy sensing and introspection", "author": ["S. Shapiro"], "venue": "In Procs. of NRAC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Iterated belief change in the situation calculus", "author": ["S. Shapiro", "M. Pagnucco", "Y. Lesp\u00e9rance", "H. Levesque"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Knowledge, belief, and noisy sensing in the situation calculus. In www.cs.jhu.edu/ psimari/publications/simari msc.pdf", "author": ["P. Simari"], "venue": "Master Thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Ordinal conditional functions: A dynamic theory of epistemic states. Causation in Decision", "author": ["W. Spohn"], "venue": "Belief Change, and Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}], "referenceMentions": [{"referenceID": 21, "context": "[22] is one of the studies that considered this issue.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We extend the framework proposed in [22] with the capability of managing noisy sensings.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "We prove that our framework subsumes the iterated belief change strategy in [22] when all sensing actions are accurate.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Furthermore, we prove that our framework can adequately handle belief introspection, mistaken beliefs, belief revision and belief update even with noisy sensing, as done in [22] with accurate sensing actions only.", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "Situation calculus, introduced by John McCarthy [14, 15], has been applied widely to model and reason about actions and changes in dynamic systems.", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "Situation calculus, introduced by John McCarthy [14, 15], has been applied widely to model and reason about actions and changes in dynamic systems.", "startOffset": 48, "endOffset": 56}, {"referenceID": 16, "context": "It was reinterpreted in [17] as basic action theories which are comprised of a set of foundational axioms defining the space of situations, unique-name axioms for actions, action preconditions and effects axioms, and the initial situation axioms [8].", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "It was reinterpreted in [17] as basic action theories which are comprised of a set of foundational axioms defining the space of situations, unique-name axioms for actions, action preconditions and effects axioms, and the initial situation axioms [8].", "startOffset": 246, "endOffset": 249}, {"referenceID": 18, "context": ", [19, 20, 22, 13].", "startOffset": 2, "endOffset": 18}, {"referenceID": 19, "context": ", [19, 20, 22, 13].", "startOffset": 2, "endOffset": 18}, {"referenceID": 21, "context": ", [19, 20, 22, 13].", "startOffset": 2, "endOffset": 18}, {"referenceID": 12, "context": ", [19, 20, 22, 13].", "startOffset": 2, "endOffset": 18}, {"referenceID": 21, "context": "In [22], a new framework exceeding previous approaches was proposed in which a plausibility value is attached to every situation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Example 1 (adapted from [22]) Assume that the initial situation S0 is InR1(S0) \u2227 \u00acLight1(S0) \u2227 \u00acLight2(S0) which states that the agent is in Room 1 (InR1(S0)), the lights in both Room 1 and Room 2 (assume there are only two rooms) are off.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "Not knowing the truth, the agent assigns S1 and S2 with plausibility values 0 and 1 respectively, which are the \u03ba-rankings in [24] such that the lower the plausibility value is, the more plausible the situation is.", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "As a consequence, with this framework in [22], S2, the right situation, will be discarded.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "In this paper, we extend the framework in [22] to manage noisy sensing actions.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "On the other hand, when every sensing action is accurate, our extended framework is capable of discovering what can be derived from the framework in [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "\u2022 When all sensing actions are accurate, the beliefs that can be induced by the framework of [22] can also be induced from our framework.", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "The framework in [22] is based on an extension of the action theory [17] stemming from situation calculus [14, 15].", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "The framework in [22] is based on an extension of the action theory [17] stemming from situation calculus [14, 15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "The framework in [22] is based on an extension of the action theory [17] stemming from situation calculus [14, 15].", "startOffset": 106, "endOffset": 114}, {"referenceID": 14, "context": "The framework in [22] is based on an extension of the action theory [17] stemming from situation calculus [14, 15].", "startOffset": 106, "endOffset": 114}, {"referenceID": 21, "context": "Here we introduce the notion of situation calculus from [22] which includes a belief operator [19, 20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Here we introduce the notion of situation calculus from [22] which includes a belief operator [19, 20].", "startOffset": 94, "endOffset": 102}, {"referenceID": 19, "context": "Here we introduce the notion of situation calculus from [22] which includes a belief operator [19, 20].", "startOffset": 94, "endOffset": 102}, {"referenceID": 16, "context": "The effects of actions on fluents are defined using successor state axioms [17], which provide a succinct representation for both effect axioms and frame axioms [14, 15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "The effects of actions on fluents are defined using successor state axioms [17], which provide a succinct representation for both effect axioms and frame axioms [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 14, "context": "The effects of actions on fluents are defined using successor state axioms [17], which provide a succinct representation for both effect axioms and frame axioms [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 8, "context": "Levesque [9] introduced a predicate, SF(a, s), to describe the result of performing the binary-valued sensing action a.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "The property sensed by an action is associated with the action using a guarded sensed fluent axiom [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 18, "context": "Scherl and Levesque [19, 20] defined a successor state axiom for B, an accessibility relation on situations based on the possible-worlds semantics by [16], that shows how actions, including sensing actions, affect the beliefs of an agent.", "startOffset": 20, "endOffset": 28}, {"referenceID": 19, "context": "Scherl and Levesque [19, 20] defined a successor state axiom for B, an accessibility relation on situations based on the possible-worlds semantics by [16], that shows how actions, including sensing actions, affect the beliefs of an agent.", "startOffset": 20, "endOffset": 28}, {"referenceID": 15, "context": "Scherl and Levesque [19, 20] defined a successor state axiom for B, an accessibility relation on situations based on the possible-worlds semantics by [16], that shows how actions, including sensing actions, affect the beliefs of an agent.", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "Similar to [22], we take the following conventions about the guarded action theories \u03a3 consisting of: (A) successor state axioms for each fluent, and guarded sensed fluent axioms for each action; (B) unique names axioms for actions, and domain-independent foundational axioms; and (C) initial state axioms which describe the initial state of the domain and the initial beliefs of agents.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "In this section, we extend the framework in [22] to include a non-fixed plausibility operator to account for iterated belief changes in the situation calculus.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Surprisingly, it has a more expressive power than that in [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "This axiom follows a similar manner to OCF conditionalization [24].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "This successor state axiom for pl follows the spirit of the intuition stated for the plausibility settings in [22] that if the accessible situation agrees with the actual situation upon the result of a sensing action, the plausibility of the accessible should increase (i.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "However, this intuition was not implemented in the pl function in [22] because it is considered in conflict with positive and negative introspection of beliefs [22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "However, this intuition was not implemented in the pl function in [22] because it is considered in conflict with positive and negative introspection of beliefs [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "This is intuitively the same as the one defined in [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "We then provide the counterparts of properties given in [22] which demonstrate that our framework faithfully extend their framework to noisy sensing situations.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "To make the comparisons easier to follow, in the corresponding subsections below, we have adopted many notations and results proposed in [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "Let P be a probability function that measures the statistical outcome of the accuracy of sensing actions performed by an agent [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 21, "context": "This issue was not explicitly considered in [22] but could be seen as a default assumption for their framework.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Similar to [22], for clarity, if there is no confusion, the situation argument of a fluent is often omitted in a belief operator, e.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "For comparison, let \u03a3S denote a guarded action theory used in [22] (with different definitions of B relations and successive state axioms for pl) in contrast to using \u03a3 for a guarded action theory in our framework.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "Similarly, we use plS and BelS for plausibility functions and beliefs in [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "In [22], the successor state axiom for pl is defined as:", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Two axioms for initializing the B-related situations are used [22] to complete its framework.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "Axiom 4 [22] Init(s) \u2227 B(s\u2032, s) \u2192 ( \u2200s\u2032\u2032, B(s\u2032\u2032, s\u2032) \u2261 B(s\u2032\u2032, s) ) .", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Axiom 5 [22] Init(s) \u2227B(s\u2032, s) \u2192 Init(s\u2032).", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "This theorem shows that when all sensing actions are accurate, our framework truly discovers the beliefs which can be induced by the framework in [22].", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "In fact, it is easy to show that if all sensing actions are accurate, then the situations that do not match the sensing results will henceforth have no chance to influence the change of beliefs, just like being discarded by the B-relations as done in [22].", "startOffset": 251, "endOffset": 255}, {"referenceID": 21, "context": "Definition 3 (Uniform formula, adapted from [22]) A formula is uniform if it contains no unbound variables.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Definition 4 (Revision action for \u03c6, adapted from [22]) A revision action A for a uniform formula \u03c6 with respect to action theory \u03a3 is a sensing action that satisfies the following condition for every domain-dependent fluent F : \u03a3 |= [\u2200s, SF (A, s) \u2261 \u03c6[s]] \u2227 [\u2200s\u2200\u2212\u2192x , F (\u2212\u2192x , s) \u2261 F (\u2212\u2192x , do(A, s))], \u2212\u2192x is the set of arguments of F .", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "The following two theorems provided in [22] also hold in our framework.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "This theorem is also consistent with the framework in [19, 20].", "startOffset": 54, "endOffset": 62}, {"referenceID": 19, "context": "This theorem is also consistent with the framework in [19, 20].", "startOffset": 54, "endOffset": 62}, {"referenceID": 18, "context": "Like [19, 20, 22], our framework supports belief introspection.", "startOffset": 5, "endOffset": 17}, {"referenceID": 19, "context": "Like [19, 20, 22], our framework supports belief introspection.", "startOffset": 5, "endOffset": 17}, {"referenceID": 21, "context": "Like [19, 20, 22], our framework supports belief introspection.", "startOffset": 5, "endOffset": 17}, {"referenceID": 21, "context": "Note that in [22], the beliefs are induced from the most plausible B-related situations instead of from all B-related situations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "The proof of the above theorem is simply similar to the proof of Theorem 26 in [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "In [22], it is argued that variations of their formalization where plausibility values are updated lead to problems with introspection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] for details).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Note that variations mentioned in [22] have plausibilities between Baccessible situations, while our plausibility is assigned to single situations, so our framework does not have such weakness, as can be seen from Theorem 4 (showing that we will not produce counterintuitive results since the framework [22] does not) and Theorem 7 (showing that introspection holds).", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "Note that variations mentioned in [22] have plausibilities between Baccessible situations, while our plausibility is assigned to single situations, so our framework does not have such weakness, as can be seen from Theorem 4 (showing that we will not produce counterintuitive results since the framework [22] does not) and Theorem 7 (showing that introspection holds).", "startOffset": 303, "endOffset": 307}, {"referenceID": 21, "context": "As in [22], suppose that an agent believes \u00ac\u03c6 in s, however after performing a revision action A in s, the agent discovers that \u03c6 is true and believes \u03c6.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "Definition 5 ([22]) Prev(\u03c6, s) def = \u2203a, s\u2032, s.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "The following theorem provided in [22] also holds here.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "The ability of belief update [22] is also provable from our framework.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Also note that the framework in [22] cannot proceed after the sensing action SR.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "In [3], the problem of noisy sensors is also studied and a probabilistic method is applied for such situations.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "This approach is extended in [23] to study its properties and allows for using conditional probability densities in the noisy sensor readings.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "In [21], the noisy sensor problem is also studied by adding plausibility values into the B-relation, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "However, in [22], it is discussed that this kind of accessibility relations, together", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "In [6], a fluent calculus framework is proposed to deal with the problem of observations contradicting the model which is to some extent similar to noisy sensing.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "Moreover, we show that our framework induces what can be discovered by the framework of [22] when all sensing actions are accurate.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 182, "endOffset": 190}, {"referenceID": 10, "context": ", AGM belief revision postulates [1], KM belief update postulates [7], DP iterated belief revision postulates [4], epistemic state revision postulates [12], belief change postulates [10, 11], etc.", "startOffset": 182, "endOffset": 190}, {"referenceID": 21, "context": "Especially, DP\u2019s C2 postulate is satisfied in our framework, whilst in [22], C2 cannot be defined.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "In addition, a study of the relationship between our framework and the Partially Observable Markov Decision Process (POMDP) [18] could be helpful.", "startOffset": 124, "endOffset": 128}], "year": 2011, "abstractText": "Situation calculus has been applied widely in artificial intelligence to model and reason about actions and changes in dynamic systems. Since actions carried out by agents will cause constant changes of the agents\u2019 beliefs, how to manage these changes is a very important issue. Shapiro et al. [22] is one of the studies that considered this issue. However, in this framework, the problem of noisy sensing, which often presents in real-world applications, is not considered. As a consequence, noisy sensing actions in this framework will lead to an agent facing inconsistent situation and subsequently the agent cannot proceed further. In this paper, we investigate how noisy sensing actions can be handled in iterated belief change within the situation calculus formalism. We extend the framework proposed in [22] with the capability of managing noisy sensings. We demonstrate that an agent can still detect the actual situation when the ratio of noisy sensing actions vs. accurate sensing actions is limited. We prove that our framework subsumes the iterated belief change strategy in [22] when all sensing actions are accurate. Furthermore, we prove that our framework can adequately handle belief introspection, mistaken beliefs, belief revision and belief update even with noisy sensing, as done in [22] with accurate sensing actions only.", "creator": " TeX output 2011.06.15:2022"}}}