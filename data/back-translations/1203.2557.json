{"id": "1203.2557", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2012", "title": "On the Necessity of Irrelevant Variables", "abstract": "This paper examines the impact of relevant and irrelevant Boolean variables on the accuracy of classifiers. It starts from the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources, when the relevant variables have a small advantage over random guess.The main result is that algorithms that rely predominantly on irrelevant variables have error probabilities that quickly go to 0 when algorithms that restrict the use of irrelevant variables have errors below a positive constant. We also show that precise learning is possible even when there are so few examples that it is not possible to determine with a high degree of certainty whether a single variable is relevant or not.", "histories": [["v1", "Mon, 12 Mar 2012 17:17:34 GMT  (212kb,D)", "https://arxiv.org/abs/1203.2557v1", "A preliminary version of this paper appeared in the proceedings of ICML'11"], ["v2", "Tue, 13 Mar 2012 18:29:37 GMT  (212kb,D)", "http://arxiv.org/abs/1203.2557v2", "A preliminary version of this paper appeared in the proceedings of ICML'11"], ["v3", "Fri, 8 Jun 2012 23:50:17 GMT  (207kb,D)", "http://arxiv.org/abs/1203.2557v3", "A preliminary version of this paper appeared in the proceedings of ICML'11"]], "COMMENTS": "A preliminary version of this paper appeared in the proceedings of ICML'11", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david p helmbold", "philip m long"], "accepted": true, "id": "1203.2557"}, "pdf": {"name": "1203.2557.pdf", "metadata": {"source": "CRF", "title": "On the Necessity of Irrelevant Variables", "authors": ["David P. Helmbold", "Philip M. Long"], "emails": ["dph@soe.ucsc.edu", "plong@sv.nec-labs.com"], "sections": [{"heading": null, "text": "Keywords: Feature Selection, Generalization, Learning Theory"}, {"heading": "1. Introduction", "text": "When creating a classifier, a natural inclination is to only use variables that are obviously relevant since irrelevant variables typically decrease the accuracy of a classifier. On the other hand, this paper shows that the harm from irrelevant variables can be much less than the benefit from relevant variables and therefore it is possible to learn very accurate classifiers even when almost all of the variables are irrelevant. It can be advantageous to continue adding variables, even as their prospects for being relevant fade away. We show this with theoretical analysis and experiments using artificially generated data.\nWe provide an illustrative analysis that isolates the effects of relevant and irrelevant variables on a classifier\u2019s accuracy. We analyze the case in which variables complement one another, which we formalize using the common assumption of conditional independence given the class label. We focus on the situation where relatively few of the many variables are relevant, and the relevant variables are only weakly predictive.1 Under these conditions, algorithms that cast a wide net can succeed while more selective algorithms fail.\n1. Note that in many natural settings the individual variables are only weakly associated with the class label. This can happen when a lot of measurement error is present, as is seen in microarray data.\nc\u00a92012 David P. Helmbold and Philip M. Long.\nar X\niv :1\n20 3.\n25 57\nv3 [\ncs .L\nG ]\n8 J\nWe prove upper bounds on the error rate of a very simple learning algorithm that may include many irrelevant variables in its hypothesis. We also prove a contrasting lower bound on the error of every learning algorithm that uses mostly relevant variables. The combination of these results show that the simple algorithm\u2019s error rate approaches zero in situations where every algorithm that predicts with mostly relevant variables has an error rate greater than a positive constant.\nOver the past decade or so, a number of empirical and theoretical findings have challenged the traditional rule of thumb described by Bishop (2006) as follows.\nOne rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model.\nThe Support Vector Machine literature (see Vapnik, 1998) views algorithms that compute apparently complicated functions of a given set of variables as linear classifiers applied to an expanded, even infinite, set of features. These empirically perform well on test data, and theoretical accounts have been given for this. Boosting and Bagging algorithms also generalize well, despite combining large numbers of simple classifiers \u2013 even if the number of such \u201cbase classifiers\u201d is much more than the number of training examples (Quinlan, 1996; Breiman, 1998; Schapire et al., 1998). This is despite the fact that Friedman et al. (2000) showed the behavior of such classifiers is closely related to performing logistic regression on a potentially vast set of features (one for each possible decision tree, for example).\nSimilar effects are sometimes found even when the features added are restricted to the original \u201craw\u201d variables. Figure 1, which is reproduced from (Tibshirani et al., 2002), is one example. The curve labelled \u201cte\u201d is the test-set error, and this error is plotted as a function of the number of features selected by the Shrunken Centroids algorithm. The best accuracy is obtained using a classifier that depends on the expression level of well over 1000 genes, despite the fact that there are only a few dozen training examples.\nIt is impossible to tell if most of the variables used by the most accurate classifier in Figure 1 are irrelevant. However, we do know which variables are relevant and irrelevant in synthetic data (and can generate as many test examples as desired). Consider for the moment a simple algorithm applied to a simple source. Each of two classes is equally likely, and there are 1000 relevant boolean variables, 500 of which agree with the class label with probability 1/2 + 1/10, and 500 which disagree with the class label with probability 1/2 + 1/10. Another 99000 boolean variables are irrelevant. The algorithm is equally simple: it has a parameter \u03b2, and outputs the majority vote over those features (variables or their negations) that agree with the class label on a 1/2 + \u03b2 fraction of the training examples. Figure 2 plots three runs of this algorithm with 100 training examples, and 1000 test examples. Both the accuracy of the classifier and the fraction of relevant variables are plotted against the number of variables used in the model, for various values of \u03b2.2 Each time, the best accuracy is achieved when an overwhelming majority of the variables used in the model are irrelevant, and those models with few (< 25%) irrelevant variables perform far worse. Furthermore, the best accuracy is obtained with a model that uses many\n2. In the first graph, only the results in which fewer than 1000 features were chosen are shown, since including larger feature sets obscures the shape of the graph in the most interesting region, where relatively few features are chosen.\nmore variables than there are training examples. Also, accuracy over 90% is achieved even though there are few training examples and the correlation of the individual variables with the class label is weak. In fact, the number of examples is so small and the correlations are so weak that, for any individual feature, it is impossible to confidently tell whether or not the feature is relevant.\nAssume classifier f consists of a vote over n variables that are conditionally independent given the class label. Let k of the variables agree with the class label with probability 1/2 + \u03b3, and the remaining n\u2212 k variables agree with the label with probability 1/2. Then the probability that f is incorrect is at most\nexp\n( \u22122\u03b32k2\nn\n) (1)\n(as shown in Section 3). The error bound decreases exponentially in the square of the number of relevant variables. The competing factor increases only linearly with the number of irrelevant variables. Thus, a very accurate classifier can be obtained with a feature set consisting predominantly of irrelevant variables.\nIn Section 4 we consider learning from training data where the variables are conditionally independent given the class label. Whereas Equation (1) bounded the error as a function of the number of variables n and relevant variables k in the model, we now use capital N and capital K for the total number of variables and number of relevant variables in the data. The N \u2212 K irrelevant variables are independent of the label, agreeing with it with probability 1/2. The K relevant variables either agree with the label with probability 1/2 + \u03b3 or with probability 1/2 \u2212 \u03b3. We analyze an algorithm that chooses a value \u03b2 \u2265 0 and outputs a majority vote over all features that agree with the class label on at least 1/2+\u03b2 of the training examples (as before, each feature is either a variable or its negation). Our Theorem 3 shows that if \u03b2 \u2264 \u03b3 and the algorithm is given m training examples, then the probability that it makes an incorrect prediction on an independent test example is at\nmost\n(1 + o(1)) exp ( \u22122\u03b32K ( [1\u2212 8e\u22122(\u03b3\u2212\u03b2)2m \u2212 \u03b3)]2+ 1 + 8(N/K)e\u22122\u03b22m + \u03b3 )) , (2)\nwhere [z]+ def = max{z, 0}. (Throughout the paper, the \u201cbig Oh\u201d and other asymptotic notation will be for the case where \u03b3 is small, K\u03b3 is large, and N/K is large. Thus the edge of the relevant features and the fraction of features that are relevant both approach zero while the total number of relevant features increases. If K is not large relative to 1/\u03b32, even the Bayes optimal classifier is not accurate. No other assumptions about the relationship between the parameters are needed.)\nWhen \u03b2 \u2264 \u03b3/2 and the number m of training examples satisfies m \u2265 c/\u03b32 for an absolute constant c, we also show in Theorem 8 that the error probability is at most\n(1 + o(1)) exp ( \u2212\u03b32K2/N ) . (3)\nIf N = o(\u03b32K2), this error probability goes to zero. With only \u0398(1/\u03b32) examples, an algorithm cannot even tell with high confidence whether a relevant variable is positively or negatively associated with the class label, much less solve the more difficult problem of determining whether or not a variable is relevant. Indeed, this error bound is also achieved using \u03b2 = 0, when, for each variable Xi, the algorithm includes either Xi or its negation in the vote.3 Because bound (3) holds even when \u03b2 = 0, it can be achieved by an algorithm that does not use knowledge of \u03b3 or K.\nOur upper bounds illustrate the potential rewards for algorithms that are \u201cinclusive\u201d, using many of the available variables in their classifiers \u2013 even when this means that most variables in the model are irrelevant. We also prove a complementary lower bound that illustrates the potential cost when algorithms are \u201cexclusive\u201d. We say that an algorithm is \u03bb-exclusive if the expectation of the fraction of the variables used in its model that are relevant is at least \u03bb. We show that any \u03bb-exclusive policy has an error probability bounded below by \u03bb/4 as K and N/K go to infinity and \u03b3 goes to 0 in such a way that the error rate obtained by the more \u201cinclusive\u201d setting \u03b2 = \u03b3/2 goes to 0. In particular, no \u03bb-exclusive algorithm (where \u03bb is a positive constant) can achieve a bound like (3).\nRelationship to Previous Work Donoho and Jin (see Donoho and Jin, 2008; Jin, 2009) and Fan and Fan (2008), building on a line of research on joint testing of multiple hypotheses (see Abramovich et al., 2006; Addario-Berry et al., 2010; Donoho and Jin, 2004, 2006; Meinshausen and Rice, 2006), performed analyses and simulations using sources with elements in common with the model studied here, including conditionally independent variables and a weak association between the variables and the class labels. Donoho and Jin also pointed out that their algorithm can produce accurate hypotheses while using many more irrelevant features than relevant ones. The main theoretical results proved in their papers describe conditions that imply that, if the relevant variables are too small a fraction of all the variables, and the number of examples is too small, then learning is impossible. The\n3. To be precise, the algorithm includes each variable or its negation when \u03b2 = 0 and m is odd, and includes both the variable and its negation when m is even and the variable agrees with the class label exactly half the time. But, any time both a variable and its negation are included, their votes cancel. We will always use the smaller equivalent model obtained by removing such canceling votes.\nemphasis of our theoretical analysis is the opposite: algorithms can tolerate a large number of irrelevant variables, while using a small number of examples, and algorithms that avoid irrelevant variables, even to a limited extent, cannot learn as effectively as algorithms that cast a wider net. In particular, ours is the first analysis that we are aware of to have a result qualitatively like Theorem 13, which demonstrates the limitations of exclusive algorithms.\nFor the sources studied in this paper, there is a linear classifier that classifies most random examples correctly with a large margin, i.e. most examples are not close to the decision boundary. The main motivation for our analysis was to understand the effects of relevant and irrelevant variables on generalization, but it is interesting to note that we get meaningful bounds in the extreme case that m = \u0398(1/\u03b32), whereas the margin-based bounds that we are aware of (such as Schapire et al. (1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples. But their bound for Naive Bayes is also vacuous when m = \u0398(1/\u03b32). Bickel and Levina (2004) studied the case in which the class conditional distributions are Gaussians, and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case, especially when the number of variables is large. Bu\u0308hlmann and Yu (2002) analyzed the variance-reduction benefits of Bagging with primary focus on the benefits of the smoother classifier that is obtained when ragged classifiers are averaged. As such it takes a different form than our analysis.\nOur analysis demonstrates that certain effects are possible, but how important this is depends on how closely natural learning settings resemble our theoretical setting and the extent to which our analysis can be generalized. The conditional independence assumption is one way to express the intuitive notion that variables are not too redundant. A limit on the redundancy is needed for results like ours since, for example, a collection of \u0398(k) perfectly correlated irrelevant variables would swamp the votes of the k relevant variables. On the other hand, many boosting algorithms minimize the potential for this kind of effect by choosing features in later iterations that make errors on different examples then the previously chosen features. One relaxation of the conditional independence assumption is to allow each variable to conditionally depend on a limited number r of other variables, as is done in the formulation of the Lovasz Local Lemma (see Alon et al., 1992). As partial illustration of the robustness of the effects analyzed here, we generalize upper bound (1) to\nthis case in Section 6.1. There we prove an error bound of c(r+ 1) exp ( \u22122\u03b32k2 n(r+1) ) when each variable depends on most r others. There are a number of ways that one could imagine relaxing the conditional independence assumption while still proving theorems of a similar flavor. Another obvious direction for generalization is to relax the strict categorization of variables into irrelevant and (1/2 + \u03b3)-relevant classes. We believe that many extensions of this work with different coverage and interpretability tradeoffs are possible. For example, our proof techniques easily give similar theorems when each relevant variable has a probability between 1/2 + \u03b3/2 and 1/2 + 2\u03b3 of agreeing with the class label (as discussed in Section 6.2). Most of this paper uses the cleanest and simplest setting in order to focus attention on the main ideas.\nWe state some useful tail bounds in the next section, and Section 3 analyzes the error of simple voting classifiers. Section 4 gives bounds on the expected error of hypotheses learned from training data while Section 5 shows that, in certain situations, any exclusive algorithm must have high error while the error of some inclusive algorithms goes to 0. In Section 6.1 we bound the accuracy of voting classifiers under a weakened independence assumption and in Section 6.2 we consider relaxation of the assumption that all relevant variables have the same edge."}, {"heading": "2. Tail bounds", "text": "This section gathers together the several tail bounds that will be used in various places in the analysis. These bounds all assume that U1, U2, . . . , U` are ` independent {0, 1}-valued random variables and U = \u2211` i=1 Ui. We start with some upper bounds.\n\u2022 The Hoeffding bound, (see Pollard, 1984):\nP [ 1\n` U \u2212 E\n( 1\n` U\n) \u2265 \u03b7 ] \u2264 e\u22122\u03b72`. (4)\n\u2022 The Chernoff bound, (Angluin and Valiant, 1979; Motwani and Raghavan, 1995, see) and Appendix A.1. For any \u03b7 > 0:\nP[U > (1 + \u03b7)E(U)] < exp ( \u2212(1 + \u03b7)E(U) ln ( 1 + \u03b7\ne\n)) . (5)\n\u2022 For any 0 \u2264 \u03b7 \u2264 4 (see Appendix A.1):\nP[U > (1 + \u03b7)E(U)] < exp ( \u2212\u03b72E(U)/4 ) . (6)\n\u2022 For any 0 < \u03b4 \u2264 1 (see Appendix A.2):\nP[U > 4E(U) + 3 ln(1/\u03b4)] < \u03b4. (7)\nWe also use the following lower bounds on the tails of distributions.\n\u2022 If P[Ui = 1] = 1/2 for all i, \u03b7 > 0, and ` \u2265 1/\u03b72 then (see Appendix A.3):\nP [ 1\n` U \u2212 1 ` E (U) \u2265 \u03b7\n] \u2265 1\n7\u03b7 \u221a `\nexp ( \u22122\u03b72` ) \u2212 1\u221a\n` . (8)\n\u2022 If P[Ui = 1] = 1/2 for all i, then for all 0 \u2264 \u03b7 \u2264 1/8 such that \u03b7` is an integer4 (see Appendix A.4):\nP [ 1\n` U \u2212 1 ` E(U) \u2265 \u03b7\n] \u2265 1\n5 e\u221216\u03b7 2`. (9)\n4. For notational simplicity we omit the floors/ceilings implicit in the use of this bound.\n\u2022 A consequence of Slud\u2019s Inequality (1977) gives the following (see Appendix A.5). If 0 \u2264 \u03b7 \u2264 1/5 and P[Ui = 1] = 1/2 + \u03b7 for all i then:\nP [ 1\n` U < 1/2\n] \u2265 1\n4 e\u22125\u03b7 2`. (10)\nNote that the constants in the above bounds were chosen to be simple and illustrative, rather than the best possible."}, {"heading": "3. The accuracy of models containing relevant and irrelevant variables", "text": "In this section we analyze the accuracy of the models (hypotheses) produced by the algorithms in Section 4. Each example is represented by a vector of N binary variables and a class designation. We use the following generative model:\n\u2022 a random class designation from {0, 1} is chosen, with both classes equally likely, then\n\u2022 each of K relevant variables are equal to the class designation with probability 1/2+\u03b3 (or with probability 1/2\u2212 \u03b3), and\n\u2022 the remaining N \u2212K irrelevant variables are equal to the class label with probability 1/2;\n\u2022 all variables are conditionally independent given the class designation.\nWhich variables are relevant and whether each one is positively or negatively correlated with the class designations are chosen arbitrarily ahead of time.\nA feature is either a variable or its complement. The 2(N \u2212 K) irrelevant features come from the irrelevant variables, the K relevant features agree with the class labels with probability 1/2 + \u03b3, and the K misleading features agree with the class labels with probability 1/2\u2212 \u03b3.\nWe now consider modelsM predicting with a majority vote over a subset of the features. We use n for the total number of features in modelM, k for the number of relevant features, and ` for the number of misleading features (leaving n \u2212 k \u2212 ` irrelevant features). Since the votes of a variable and its negation \u201ccancel out,\u201d we assume without loss of generality that models include at most one feature for each variable. Recall that [z]+ def = max{z, 0}.\nTheorem 1 LetM be a majority vote of n features, k of which are relevant and ` of which are misleading (and n\u2212 k\u2212 ` are irrelevant). The probability that M predicts incorrectly is\nat most exp\n( \u22122\u03b32[k \u2212 `]2+\nn\n) .\nProof: If ` \u2265 k then the exponent is 0 and the bound trivially holds. Suppose k > `. ModelM predicts incorrectly only when at most half of its features are correct. The expected fraction of correct voters is 1/2 + \u03b3(k\u2212`)n , so, forM\u2019s prediction to be incorrect, the fraction of correct voters must be at least \u03b3(k\u2212`)/n less than its expectation. Applying (4), this probability is at most\nexp\n( \u22122\u03b32(k \u2212 `)2\nn\n) .\nThe next corollary shows that even models where most of the features are irrelevant can\nbe highly accurate. Corollary 2 If \u03b3 is a constant, k \u2212 ` = \u03c9( \u221a n) and k = o(n), then the accuracy of the model approaches 100% while its fraction of irrelevant variables approaches 1 (as n\u2192\u221e).\nFor example, the conditions of Corollary 2 are satisfied when \u03b3 = 1/4, k = 2n2/3 and ` = n2/3."}, {"heading": "4. Learning", "text": "We now consider the problem of learning a model M from data. We assume that the algorithm receives m i.i.d. examples generated as described in Section 3. One test example is independently generated from the same distribution, and we evaluate the algorithm\u2019s expected error : the probability over training set and test example that its model makes an incorrect prediction on the test example (the \u201cprediction model\u201d of Haussler et al. (1994)).\nWe define M\u03b2 to be the majority vote5 of all features that equal the class label on at least 1/2 +\u03b2 of the training examples. To keep the analysis as clean as possible, our results in this section apply to algorithms that chose \u03b2 as a function of the number of features N , the number of relevant features K, the edge of the relevant features \u03b3, and training set size m, and then predict with M\u03b2. Note that this includes the algorithm that always choses \u03b2 = 0 regardless of N , K, \u03b3 and m.\nRecall that asymptotic notation will concern the case in which \u03b3 is small, K\u03b3 is large, and N/K is large.\nThis section proves two theorems bounding the expected error rates of learned models. One can compare these bounds with a similar bound on the Bayes Optimal predictor that \u201cknows\u201d which features are relevant. This Bayes Optimal predictor for our generative model is a majority vote of the K relevant features, and has an error rate bounded by e\u22122\u03b3\n2K (a bound as tight as the Hoeffding bound).\nTheorem 3 If 0 \u2264 \u03b2 \u2264 \u03b3, then the expected error rate of M\u03b2 is at most\n(1 + o(1)) exp ( \u22122\u03b32K ( [1\u2212 8e\u22122(\u03b3\u2212\u03b2)2m \u2212 \u03b3]2+ 1 + 8(N/K)e\u22122\u03b22m + \u03b3 )) .\nOur proof of Theorem 3 starts with lemmas bounding the number of misleading, irrelevant, and relevant features in M\u03b2. These lemmas use a quantity \u03b4 > 0 that will be determined later in the analysis.\nLemma 4 With probability at least 1 \u2212 \u03b4, the number of misleading features in M\u03b2 is at most 4Ke\u22122(\u03b3+\u03b2) 2m + 3 ln(1/\u03b4).\nProof: For a particular misleading feature to be included in M\u03b2, Algorithm A must overestimate the probability that misleading feature equals the class label by at least \u03b2 + \u03b3.\n5. If M\u03b2 is empty or the vote is tied then any default prediction, such as 1, will do.\nApplying (4), this happens with probability at most e\u22122(\u03b2+\u03b3) 2m, so the expected number of misleading features in M\u03b2 is at most Ke\u22122(\u03b2+\u03b3) 2m. Since each misleading feature is associated with a different independent variable, we can apply (7) with E(U) \u2264 Ke\u22122(\u03b2+\u03b3)2m to get the desired result.\nLemma 5 With probability at least 1 \u2212 2\u03b4, the number of irrelevant features in M\u03b2 is at most 8Ne\u22122\u03b2 2m + 6 ln(1/\u03b4).\nProof: For a particular positive irrelevant feature to be included inM\u03b2, Algorithm A must overestimate the probability that the positive irrelevant feature equals the class label by \u03b2. Applying (4), this happens with probability at most e\u22122\u03b2\n2m, so the expected number of irrelevant positive features in M\u03b2 is at most (N \u2212K)e\u22122\u03b2\n2m. All of the events that variables agree with the label, for various variables, and various examples, are independent. So the events that various irrelevant variables are included in M\u03b2 are independent. Applying (7) with E(U) = (N\u2212K)e\u22122\u03b2 2m gives that, with probability at least 1\u2212 \u03b4, the number of irrelevant positive features inM\u03b2 is at most 4(N \u2212K)e\u22122\u03b2 2m.\nA symmetric analysis establishes the same bound on the number of negative irrelevant features in M\u03b2. Adding these up completes the proof.\nLemma 6 With probability at least 1\u2212 \u03b4, the number of relevant features in M\u03b2 is at least K \u2212 4Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 3 ln(1/\u03b4).\nProof: For a particular relevant feature to be excluded from M\u03b2, Algorithm A must underestimate the probability that the relevant feature equals the class label by at least \u03b3 \u2212 \u03b2. Applying (4), this happens with probability at most e\u22122(\u03b3\u2212\u03b2)2m, so the expected number of relevant variables excluded from M\u03b2 is at most Ke\u22122(\u03b3\u2212\u03b2)\n2m. Applying (7) as in the preceding two lemmas completes the proof.\nLemma 7 The probability that M\u03b2 makes an error is at most\nexp \u22122\u03b32 [ K \u2212 8Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 6 ln(1/\u03b4) ]2 +\nK + 8Ne\u22122\u03b22m + 6 ln(1/\u03b4) + 4\u03b4. for any \u03b4 > 0 and 0 \u2264 \u03b2 \u2264 \u03b3.\nProof: The bounds of Lemmas 4, 5, and 6 simultaneously hold with probability at least 1 \u2212 4\u03b4. Thus the error probability of M\u03b2 is at most 4\u03b4 plus the probability of error given that all three bounds hold. Plugging the three bounds into Theorem 1, (and over-estimating the number n of variables in the model with K plus the bound of Lemma 5 on the number of irrelevant variables) gives a bound on M\u03b2\u2019s error probability of\nexp \u22122\u03b32 [ (K \u2212 4Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 3 ln(1/\u03b4))\u2212 (4Ke\u22122(\u03b3+\u03b2)2m + 3 ln(1/\u03b4)) ]2 +\nK + 8Ne\u22122\u03b22m + 6 ln(1/\u03b4)\n (11)\nwhen all three bounds hold. Under-approximating (\u03b3 + \u03b2)2 with (\u03b3 \u2212 \u03b2)2 and simplifying yields:\n(11) \u2264 exp \u22122\u03b32 [ K \u2212 8Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 6 ln(1/\u03b4) ]2 +\nK + 8Ne\u22122\u03b22m + 6 ln(1/\u03b4)  . Adding 4\u03b4 completes the proof.\nWe are now ready to prove Theorem 3.\nProof (of Theorem 3): Using\n\u03b4 = exp ( \u2212\u03b3K\n6 ) in Lemma 7 bounds the probability that M\u03b2 makes a mistake by\nexp \u22122\u03b32 [ K \u2212 8Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 \u03b3K ]2 +\nK + 8Ne\u22122\u03b22m + \u03b3K\n+ 4 exp(\u2212\u03b3K 6 )\n< exp \u22122\u03b32K [ 1\u2212 8e\u22122(\u03b3\u2212\u03b2)2m \u2212 \u03b3 ]2 +\n1 + 8NK e \u22122\u03b22m + \u03b3\n+ 4 exp(\u2212\u03b3K 6 ) . (12)\nThe first term is at least e\u22122\u03b3 2K , and\n4 exp ( \u2212\u03b3K\n6\n) = o ( e\u22122\u03b3 2K )\nas \u03b3 \u2192 0 and \u03b3K \u2192\u221e, so (12) implies the bound\n(1 + o(1)) exp \u22122\u03b32K [ 1\u2212 8e\u22122(\u03b3\u2212\u03b2)2m \u2212 \u03b3 ]2 +\n1 + 8NK e \u22122\u03b22m + \u03b3  as desired.\nThe following theorem bounds the error in terms of just K, N , and \u03b3 when m is sufficiently large.\nTheorem 8 Suppose algorithm A produces models M\u03b2 where 0 \u2264 \u03b2 \u2264 c\u03b3 for a constant c \u2208 [0, 1).\n\u2022 Then there is a constant b (depending only on c) such that whenever m \u2265 b/\u03b32 the error of A\u2019s model is at most (1 + o(1)) exp ( \u2212\u03b3 2K2\nN\n) .\n\u2022 If m = \u03c9(1/\u03b32) then the error of A\u2019s model is at most (1 + o(1)) exp ( \u2212(2\u2212o(1))\u03b32K2\nN\n) .\nProof Combining Lemmas 4 and 6 with the upper bound of N on the number of features in M\u03b2 as in Lemma 7\u2019s proof gives the following error bound on M\u03b2\nexp \u22122\u03b32 [ K \u2212 8Ke\u22122(\u03b3\u2212\u03b2)2m \u2212 6 ln(1/\u03b4) ]2 +\nN + 2\u03b4 for any \u03b4 > 0. Setting\n\u03b4 = exp ( \u2212\u03b3K\n6 ) and continuing as in the proof of Theorem 3 gives the bound\n(1 + o(1)) exp \u22122\u03b32K2 [ 1\u2212 2 ( 4e\u22122(\u03b3\u2212\u03b2) 2m + \u03b3 )]2 +\nN  . (13) For the first part of the theorem, it suffices to show that the [\u00b7 \u00b7 \u00b7 ]2+ term is at least 1/2. Recalling that our analysis is for small \u03b3, the term inside the [\u00b7 \u00b7 \u00b7 ]+ of (13) is at least\n1\u2212 8e\u22122(1\u2212c)2\u03b32m \u2212 o(1).\nWhen\nm \u2265 ln (32) 2(1\u2212 c)2\u03b32 , (14)\nthis term is at least 3/4 \u2212 o(1), and thus its square is at least 1/2 for small enough \u03b3, completing the proof of the first part of the theorem.\nTo see the second part of the theorem, since m \u2208 \u03c9(1/\u03b32), the term of (13) inside the [\u00b7 \u00b7 \u00b7 ]+ is 1\u2212 o(1).\nBy examining inequality (14), we see that the constant b in Theorem 8 can be set to ln(32)/2(1\u2212 c)2.\nLemma 9 The expected number of irrelevant variables in M\u03b2 is at least (N \u2212K)e\u221216\u03b2 2m.\nProof Follows from inequality (9).\nCorollary 10 If K, N , and m are functions of \u03b3 such that\n\u03b3 \u2192 0, K2/N \u2208 \u03c9(ln(1/\u03b3)/\u03b32), K = o(N) and\nm = 2 ln(32)/\u03b32\nthen if an algorithm outputs M\u03b2 using a \u03b2 in [0, \u03b3/2], it has an error that decreases superpolynomially (in \u03b3), while the expected fraction of irrelevant variables in the model goes to 1.\nNote that Theorem 8 and Corollary 10 include non-trivial error bounds on the model M0 that votes all N variables (for odd sample size m)."}, {"heading": "5. Lower bound", "text": "Here we show that any algorithm with an error guarantee like Theorem 8 must include many irrelevant features in its model. The preliminary version of this paper (Helmbold and Long, 2011) contains a related lower bound for algorithms that choose \u03b2 as a function of N , K, m, and \u03b3, and predict with M\u03b2. Here we present a more general lower bound that applies to algorithms outputting arbitrary hypotheses. This includes algorithms that use weighted voting (perhaps with L1 regularization). In this section we\n\u2022 set the number of features N , number of relevant features K, and sample size m as a functions of \u03b3 in such a way that Corollary 10 applies, and\n\u2022 prove a constant lower bound for these combinations of values that holds for \u201cexclusive\u201d algorithms (defined below) when \u03b3 is small enough.\nThus, in this situation, \u201cinclusive\u201d algorithms relying on many irrelevant variables have error rates going to zero while every \u201cexclusive\u201d algorithm has an error rate bounded below by a constant.\nThe proofs in this section assume that all relevant variables are positively correlated with the class designation, so each relevant variable agrees with the class designation with probability 1/2 + \u03b3. Although not essential for the results, this assumption simplifies the definitions and notation6. We also set m = 2 ln(32)/\u03b32. This satisfies the assumption of Theorem 8 when \u03b2 \u2264 \u03b3/2 (see Inequality (14)).\nDefinition 11 We say a classifier f includes a variable xi if there is an input (x1, ..., xN ) such that\nf(x1, ..., xi\u22121, xi, xi+1, ..., xN ) 6= f(x1, ..., xi\u22121, 1\u2212 xi, xi+1, ..., xN ).\nLet V (f) be the set of variables included in f .\nFor a training set S, we will refer to the classifier output by algorithm A on S as A(S). Let R be the set of relevant variables.\nDefinition 12 We say that an algorithm A is \u03bb-exclusive7 if for every positive N , K, \u03b3, and m, the expected fraction of the variables included in its hypothesis that are relevant is\nat least \u03bb, i.e. E ( |V (A(S)) \u2229R| |V (A(S))| ) \u2265 \u03bb.\n6. The assumption that each relevant variable agrees with the class label with probability 1/2 + \u03b3 gives a special case of the generative model described in Section 4, so the lower bounds proven here also apply to that more general setting. 7. The proceedings version of this paper (Helmbold and Long, 2011) used a different definition of \u03bb-exclusive.\nOur main lower bound theorem is the following.\nTheorem 13 If\nK = 1\n\u03b32 exp\n( ln(1/\u03b3)1/3 ) N = K exp ( ln(1/\u03b3)1/4\n) m = 2 ln(32)\n\u03b32\nthen for any constant \u03bb > 0 and any \u03bb-exclusive algorithm A, the error rate of A is lower bounded by \u03bb/4\u2212 o(1) as \u03b3 goes to 0.\nNotice that this theorem provides a sharp contrast to Corollary 10. Corollary 10 shows that inclusive A using models M\u03b2 for any 0 \u2264 \u03b2 \u2264 \u03b3/2 have error rates that goes to zero super-polynomially fast (in 1/\u03b3) under the assumptions of Theorem 13.\nThe values of K and N in Theorem 13 are chosen to make the proof convenient, but other values would work. For example, decreasing K and/or increasing N would make the lower bound part of Theorem 13 easier to prove. There is some slack to do so while continuing to ensure that the upper bound of Corollary 10 goes to 0.\nAs the correlation of variables with the label over the sample plays a central role in our analysis, we will use the following definition.\nDefinition 14 If a variable agrees with the class label on 1/2 + \u03b7 of the training set then it has (empirical) edge \u03b7.\nThe proof of Theorem 13 uses a critical value of \u03b2, namely \u03b2\u2217 = \u03b3 ln(N/K)/10 ln(32), with the property that both:\nE (|M\u03b2\u2217 \u2229R|) E (|M\u03b2\u2217 |) \u2192 0 (15)\nE (|M\u03b2\u2217 \u2229R|) \u2208 o(1/\u03b32) (16)\nas \u03b3 \u2192 0. Intuitively, (15) means that any algorithm that uses most of the variables having empirical edge at least \u03b2\u2217 cannot be \u03bb-exclusive. On the other hand, (16) implies that if the algorithm restricts itself to variables with empirical edges greater than \u03b2\u2217 then it does not include enough relevant variables to be accurate. The proof must show that arbitrary algorithms frequently include either too many irrelevant variables to be \u03bb-exclusive or too few relevant ones to be accurate. See Figure 3 for some useful facts about \u03b3, m, and \u03b2\u2217.\nTo prove the lower bound, borrowing a technique from Ehrenfeucht et al. (1989), we will assume that the K relevant variables are randomly selected from the N variables, and lower bound the error with respect to this random choice, along with the training and test data. This will then imply that, for each algorithm, there will be a choice of the K relevant variables giving the same lower bound with respect only to the random choice of the training and test data. We will always use relevant variables that are positively associated with the class label, agreeing with it with probability 1/2 + \u03b3.\nProof [of Theorem 13] Fix any learning algorithm A, and let A(S) be the hypothesis produced by A from sample S. Let n(S) be the number of variables included in A(S) and let \u03b2(S) be the n(S)\u2019th largest empirical (w.r.t. S) edge of a variable.\nLet q\u03b3 be the probability that \u03b2(S) \u2265 \u03b2\u2217 = \u03b3 ln(N/K)/10 ln(32). We will show in Section 5.2 that if A is \u03bb-exclusive then \u03bb \u2264 q\u03b3 + o(1) (as \u03b3 goes to 0). We will also show in Section 5.3 that the expected error of A is at least q\u03b3/4\u2212 o(1) as \u03b3 goes to 0. Therefore any \u03bb-exclusive algorithm A has an expected error rate at least \u03bb/4\u2212o(1) as \u03b3 goes to 0.\nBefore attacking the two parts of the proof alluded to above, we need a subsection providing some basic results about relevant variables and optimal algorithms."}, {"heading": "5.1 Relevant Variables and Good Hypotheses", "text": "This section proves some useful facts about relevant variables and good hypotheses. The first lemma is a lower bound on the accuracy of a model in terms of the number of relevant variables.\nLemma 15 If \u03b3 \u2208 [0, 1/5] then any classifier using k relevant variables has an error probability at least 14e \u22125\u03b32k.\nProof: The usual Naive Bayes calculation (see Duda et al., 2000) implies that the optimal classifier over a certain set V of variables is a majority vote over V \u2229R. Applying the lower tail bound (10) then completes the proof.\nOur next lemma shows that, given a sample, the probability that a variable is relevant (positively correlated with the class label) is monotonically increasing in its empirical edge.\nLemma 16 For two variables xi and xj, and any training set S of m examples,\n\u2022 P[xi relevant | S] > P[xj relevant | S] if and only if the empirical edge of xi in S is greater than the empirical edge of xj in S, and\n\u2022 P[xi relevant | S] = P[xj relevant | S] if and only if the empirical edge of xi in S is equal to the empirical edge of xj in S.\nProof Since the random choice of R does not effect that marginal distribution over the labels, we can generate S by picking the labels for all the examples first, then R, and finally the values of the variables on all the examples. Thus if we can prove the lemma after conditioning on the values of the class labels, then scaling all of the probabilities by 2\u2212m would complete the proof. So, let us fix the values of the class labels, and evaluate probabilities only with respect to the random choice of the relevant variables R, and the values of the variables.\nLet \u2206 = P[xi \u2208 R|S]\u2212 P[xj \u2208 R|S] .\nFirst, by subtracting off the probabilities that both variables are relevant, we have \u2206 = P [ xi \u2208 R, xj 6\u2208 R \u2223\u2223S]\u2212 P[xi 6\u2208 R, xj \u2208 R\u2223\u2223S] . Let ONE be the event that exactly one of xi or xj is relevant. Then\n\u2206 = (P [ xi \u2208 R, xj 6\u2208 R \u2223\u2223S,ONE]\u2212 P[xi 6\u2208 R, xj \u2208 R\u2223\u2223S,ONE])P[ONE] . So \u2206 > 0 if and only if\n\u2206\u2032 def = P [ xi \u2208 R, xj 6\u2208 R \u2223\u2223S,ONE]\u2212 P[xi 6\u2208 R, xj \u2208 R\u2223\u2223S,ONE] > 0 (and similarly for \u2206 = 0 if and only if \u2206\u2032 = 0). If Q is the distribution obtained by conditioning on ONE, then\n\u2206\u2032 = Q [ xi \u2208 R, xj 6\u2208 R \u2223\u2223S]\u2212 Q[xi 6\u2208 R, xj \u2208 R\u2223\u2223S] . Let Si be the values of variable i in S, and define Sj similarly for variable j. Let S \u2032 be the values of the other variables. Since we have already conditioned on the labels, after also conditioning on ONE (i.e., under the distribution Q), the pair (Si, Sj) is independent of S\u2032. For each Si we have P[Si | xi 6\u2208 R] = Q[Si | xi 6\u2208 R]. Furthermore, by symmetry,\nQ [ xi \u2208 R, xj 6\u2208 R \u2223\u2223S\u2032] = Q[xi 6\u2208 R, xj \u2208 R\u2223\u2223S\u2032] = 1 2 .\nThus, by using Bayes\u2019 Rule on each term, we have \u2206\u2032 = Q [ xi \u2208 R, xj 6\u2208 R \u2223\u2223Si, Sj , S\u2032]\u2212 Q[xi 6\u2208 R, xj \u2208 R\u2223\u2223Si, Sj , S\u2032] = Q [ Si, Sj \u2223\u2223xi \u2208 R, xj 6\u2208 R, S\u2032]\u2212 Q[Si, Sj\u2223\u2223xi 6\u2208 R, xj \u2208 R, S\u2032] 2Q[Si, Sj |S\u2032]\n= (1/2 + \u03b3)mi(1/2\u2212 \u03b3)m\u2212mi \u2212 (1/2 + \u03b3)mj (1/2\u2212 \u03b3)m\u2212mj\n2m+1Q[Si, Sj ] ,\nwhere mi and mj are the numbers of times that variables xi and xj agree with the label in sample S. The proof concludes by observing that \u2206\u2032 is positive exactly when mi > mj and zero exactly when mi = mj .\nBecause, in this lower bound proof, relevant variables are always positively associated with the class label, we will use a variant of M\u03b2 which only considers positive features.\nDefinition 17 Let V\u03b2 be a vote over the variables with empirical edge at least \u03b2.\nWhen there is no chance of confusion, we will refer to the set of variables in V\u03b2 also as V\u03b2 (rather than V (V\u03b2)).\nWe now establish lower bounds on the probability of variables being included in V\u03b2 (here \u03b2 can be a function of \u03b3, but does not depend on the particular sample S).\nLemma 18 If \u03b3 \u2264 1/8 and \u03b2 \u2265 0 then the probability that a given variable has empirical edge at least \u03b2 is at least\n1 5 exp\n( \u221216\u03b22m ) ."}, {"heading": "If in addition m \u2265 1/\u03b22, then the probability that a given variable has empirical edge at", "text": "least \u03b2 is at least\n1\n7\u03b2 \u221a m\nexp ( \u22122\u03b22m ) \u2212 1\u221a\nm .\nProof: Since relevant variables agree with the class label with probability 1/2 + \u03b3, the probability that a relevant variable has empirical edge at least \u03b2 is lower bounded by the probability that an irrelevant variable has empirical edge at least \u03b2. An irrelevant variable has empirical edge at least \u03b2 only when it agrees with the class on 1/2 + \u03b2 of the sample. Applying Bound (9), this happens with probability at least 15 exp ( \u221216\u03b22m ) . The second part uses Bound (8) instead of (9). We now upper bound the probability of a relevant variable being included in V\u03b2, again for \u03b2 that does not depend on S.\nLemma 19 If \u03b2 \u2265 \u03b3, the probability that a given relevant variable has empirical edge at least \u03b2 is at most e\u22122(\u03b2\u2212\u03b3) 2m.\nProof: Use (4) to bound the probability that a relevant feature agrees with the class label \u03b2 \u2212 \u03b3 more often than its expected fraction of times."}, {"heading": "5.2 Bounding \u03bb-Exclusiveness", "text": "Recall that n(S) is the number of variables used by A(S), and \u03b2(S) is the edge of the variable whose rank, when the variables are ordered by their empirical edges, is n(S). We will show that: if A(S) is \u03bb-exclusive, then there is reasonable probability that \u03b2(S) is at least the critical value \u03b2\u2217 = \u03b3 ln(N/K)/5b. Specifically, if A is \u03bb-exclusive, then, for any small enough \u03b3, we have P[\u03b2(S) \u2265 \u03b2\u2217] > \u03bb/2.\nSuppose, given the training set S, the variables are sorted in decreasing order of empirical edge (breaking ties arbitrarily, say using the variable index). Let VS,k consist of the first k variables in this sorted order, the \u201ctop k\u201d variables.\nSince for each sample S and each variable xi, the probability P [ xi relevant \u2223\u2223S] decreases as the empirical edge of xi decreases (Lemma 16), the expectation E ( |VS,k \u2229R| |VS,k| \u2223\u2223\u2223\u2223 S) is non-increasing with k.\nFurthermore, Lemma 16 also implies that for each sample S, we have\nE ( |V (A(S)) \u2229R| |V (A(S))| \u2223\u2223\u2223\u2223 S) \u2264 E( |VS,n(S) \u2229R||VS,n(S)| \u2223\u2223\u2223\u2223 S) .\nTherefore, by averaging over samples, for each \u03b3 we have\nE ( |V (A(S)) \u2229R| |V (A(S))| ) \u2264 E ( |VS,n(S) \u2229R| |VS,n(S)| ) . (17)\nNote that the numerators in the expectations are never greater than the denominators. We will next give upper bounds on |V\u03b2\u2217 \u2229R| and lower bounds on |V\u03b2\u2217 | that each hold with probability 1\u2212 \u03b3.\nThe next step is a high-confidence upper bound on |V\u03b2\u2217 \u2229 R|. From Lemma 19, the probability that a particular relevant variable is in V\u03b2\u2217 is at most (recall that m = b/\u03b32)\ne\u22122(\u03b2 \u2217\u2212\u03b3)2m = e\u22122b(\u03b2 \u2217/\u03b3\u22121)2\n= exp \u22122b( ln(1/\u03b3)1/4 5b \u2212 1 )2 = exp ( \u22122 ln(1/\u03b3)1/2\n25b +\n4 ln(1/\u03b3)1/4\n5 \u2212 2b\n)\n< exp\n( \u22122 ln(1/\u03b3)1/2\n25b +\n4 ln(1/\u03b3)1/4\n5\n) .\nLet prel = exp ( \u22122 ln(1/\u03b3)1/2 25b + 4 ln(1/\u03b3)1/4 5 ) be this upper bound, and note that prel drops to 0 as \u03b3 goes to 0, but a rate slower than \u03b3 for any . The number of relevant variables in V\u03b2\u2217 has a binomial distribution with parameters K and p where p < prel. The standard deviation of this distribution is\n\u03c3 = \u221a Kp(1\u2212 p) < \u221a Kp <\nexp ( ln(1/\u03b3)1/3\n2\n)\u221a prel\n\u03b3 . (18)\nUsing the Chebyshev bound,\nP[|X \u2212 E(X)| > a\u03c3] \u2264 1 a2\n(19)\nwith a = 1/ \u221a \u03b3 gives that\nP [ |V\u03b2\u2217 \u2229R| \u2212Kp >\n\u03c3 \u221a \u03b3\n] \u2264 \u03b3\nP [ |V\u03b2\u2217 \u2229R| > Kprel +\n\u03c3 \u221a \u03b3\n] \u2264 \u03b3. (20)\nSince \u03c3 < \u221a Kp < \u221a Kprel by (18), we have \u03c3 \u221a Kprel < Kprel. Substituting the values\nof K and prel into the square-root yields\nKprel > \u03c3 \u00d7 exp\n( ln(1/\u03b3)1/3\n2\n) exp ( \u2212 ln(1/\u03b3)1/2\n25b + 2 ln(1/\u03b3)1/4 5 ) \u03b3\n> \u03c3/ \u221a \u03b3,\nfor small enough \u03b3. Combining with (20), we get that\nP[|V\u03b2\u2217 \u2229R| > 2Kprel] \u2264 \u03b3 (21)\nholds for small enough \u03b3.\nUsing similar reasoning, we now obtain a lower bound on the expected number of variables in V\u03b2\u2217 . Lemma 18 shows that, for each variable, the probability of the variable having empirical edge \u03b2\u2217 is at least\n1\n7\u03b2\u2217 \u221a m\nexp ( \u22122\u03b2\u22172m ) \u2212 1\u221a\nm =\n5 \u221a b\n7 ln(1/\u03b3)1/4 exp\n( \u22122ln(1/\u03b3) 1/2\n25b\n) \u2212 \u03b3\u221a\nb\n>\n\u221a b\n2 ln(1/\u03b3)1/4 exp\n( \u22122ln(1/\u03b3) 1/2\n25b\n)\nfor sufficiently small \u03b3. Since the empirical edges of different variables are independent, the probability that at least n variables have empirical edge at least \u03b2\u2217 is lower bounded by the probability of at least n successes from the binomial distribution with parameters N and pirrel where\npirrel =\n\u221a b\n2 ln(1/\u03b3)1/4 exp\n( \u22122ln(1/\u03b3) 1/2\n25b\n) .\nIf, now, we define \u03c3 to be the standard deviation of this binomial distribution, then, like before, \u03c3 = \u221a Npirrel(1\u2212 pirrel) < \u221a Npirrel, and Npirrel/2 > \u03c3 \u221a Npirrel/2\n= \u03c3 2 \u00d7 exp((1/2)(ln(1/\u03b3)\n1/4 + ln(1/\u03b3)1/3))\n\u03b3 \u00d7 b\n1/4\n\u221a 2 ln(1/\u03b3)1/8 exp\n( \u2212 ln(1/\u03b3) 1/2\n25b\n) ,\nso that, for small enough \u03b3, Npirrel/2 > \u03c3/ \u221a \u03b3. Therefore applying the Chebyshev bound (19) with a = 1/ \u221a \u03b3 gives (for sufficiently small \u03b3)\nP [ |V\u03b2\u2217 | <\nNpirrel 2\n] \u2264 P[|V\u03b2\u2217 | < Npirrel \u2212 \u03c3/ \u221a \u03b3] < \u03b3. (22)\nRecall that\nq\u03b3 = P[\u03b2(S) \u2265 \u03b2\u2217] = P[n(S) \u2264 |V\u03b2\u2217 |] .\nIf A is \u03bb-exclusive then, using (17), we have \u03bb \u2264 E ( |V (A(S)) \u2229R|) |V (A(S))| ) \u2264 E ( |VS,n(S) \u2229R| |VS,n(S)| ) \u2264 (1\u2212 q\u03b3)E ( |VS,n(S) \u2229R| |VS,n(S)|\n\u2223\u2223\u2223\u2223 |V\u03b2\u2217 | < n(S))+ q\u03b3 \u2264 (1\u2212 q\u03b3)E ( |V\u03b2\u2217 \u2229R| |V\u03b2\u2217 |\n\u2223\u2223\u2223\u2223 |V\u03b2\u2217 | < n(S))+ q\u03b3 \u2264 (1\u2212 q\u03b3) ( 2Kprel Npirrel/2 + 2\u03b3 ) + q\u03b3\nwhere we use the upper and lower bounds from Equations (21) and (22) that each hold with probability 1\u2212 \u03b3. Note that the ratio\n2Kprel Npirrel/2\n\u2264 2 eln(1/\u03b3)\n1/3\n\u03b32 exp\n( \u22122 ln(1/\u03b3)1/2\n25b +\n4 ln(1/\u03b3)1/4\n5 ) eln(1/\u03b3) 1/3 eln(1/\u03b3) 1/4\n4\u03b32\n\u221a b\nln(1/\u03b3)1/4 exp\n( \u22122ln(1/\u03b3) 1/2\n25b\n)\n=\n8 ln(1/\u03b3)1/4 exp\n( \u22122 ln(1/\u03b3)1/2\n25b +\n4 ln(1/\u03b3)1/4\n5 ) \u221a b eln(1/\u03b3) 1/4 exp ( \u22122ln(1/\u03b3) 1/2\n25b\n)\n=\n8 ln(1/\u03b3)1/4 exp\n( \u2212 ln(1/\u03b3)1/4\n5 ) \u221a b\nwhich goes to 0 as \u03b3 goes to 0. Therefore, \u03bb \u2264 E ( |V (A(S)) \u2229R|) |V (A(S))| ) \u2264 q\u03b3 + o(1)\nwhich implies that, q\u03b3 = P[\u03b2(S) \u2265 \u03b2\u2217] \u2265 \u03bb\u2212 o(1)\nas \u03b3 goes to 0."}, {"heading": "5.3 Large Error", "text": "Call a variable good if it is relevant and its empirical edge is at least \u03b2\u2217 in the sample. Let p be the probability that a relevant variable is good. Thus the number of good variables is binomially distributed with parameters K and p. We have that the expected number of good variables is pK and the variance is Kp(1 \u2212 p) < Kp. By Chebyshev\u2019s inequality, we have\nP [ # good vars \u2265 Kp+ a \u221a Kp ] \u2264 P [ # good vars \u2265 Kp+ a \u221a Kp(1\u2212 p) ] \u2264 1 a2 , (23)\nand setting a = \u221a Kp, this gives\nP[# good vars \u2265 2Kp] \u2264 1 Kp . (24)\nBy Lemma 19, Kp \u2264 Ke\u22122(\u03b2\u2217\u2212\u03b3)2m = Ke\u22122b(ln(1/\u03b3)1/4/5b\u22121) 2 , so\nln(Kp) \u2264 lnK \u2212 2b\n( ln(1/\u03b3)1/2\n25b2 \u2212 2 ln(1/\u03b3)\n1/4\n5b + 1\n)\nln(Kp) \u2264 2 ln(1/\u03b3) + ln(1/\u03b3)1/3 \u2212 2b\n( ln(1/\u03b3)1/2\n25b2 \u2212 2 ln(1/\u03b3)\n1/4\n5b + 1\n) .\nSo for small enough \u03b3,\nln(Kp) \u2264 2 ln(1/\u03b3)\u2212 ln(1/\u03b3) 1/2\n25b\nand thus Kp \u2208 o(1/\u03b32). So if Kp > 1/\u03b3, then with probability at least 1\u2212 \u03b3, there are less than 2Kp \u2208 o(1/\u03b32)\ngood variables. On the other hand, if Kp < 1/\u03b3, then, setting a = \u221a\n1/\u03b3 in bound (23) gives that the probability that there are more than 2/\u03b3 good variables is at most \u03b3. So in either case the probability that there are more than 2\n\u03b32 exp\n( \u2212 ln(1/\u03b3)1/2/25b ) good variables is at\nmost \u03b3 (for small enough \u03b3). So if P[\u03b2(S) \u2265 \u03b2\u2217] \u2265 q\u03b3 , then with probability at least q\u03b3 \u2212 \u03b3 algorithm A is using a hypothesis with at most 2 \u03b32 exp ( \u2212 ln(1/\u03b3)1/2/25b ) relevant variables. Applying Lemma 15 yields the following lower bound on the probability of error:\n(q\u03b3 \u2212 \u03b3) 1\n4 exp\n( \u221210 exp ( \u2212 ln(1/\u03b3)1/2/25b )) . (25)\nSince the limit of (25) for small \u03b3 is q\u03b3/4, this completes the proof of Theorem 13."}, {"heading": "6. Relaxations of some assumptions", "text": "To keep the analysis clean, and facilitate the interpretation of the results, we have analyzed an idealized model. In this section, we briefly consider the consequences of some relaxations of our assumptions."}, {"heading": "6.1 Conditionally dependent variables", "text": "Theorem 1 can be generalized to the case in which there is limited dependence among the variables, after conditioning on the class designation, in a variety of ways. For example, suppose that there is a degree-r graph G whose nodes are variables, and such that, conditioned on the label, each variable is independent of all variables not connected to it by an edge in G. Assume that k variables agree with the label with probability 1/2 + \u03b3, and the n \u2212 k agree with the label with probability 1/2. Let us say that a source like this has r-local dependence. Then applying a Chernoff-Hoeffding bound for such sets of random\nvariables due to Pemmaraju (2001), if r \u2264 n/2, one gets a bound of c(r + 1) exp ( \u22122\u03b32k2 n(r+1) ) the probability of error."}, {"heading": "6.2 Variables with different strengths", "text": "We have previously assumed that all relevant variables are equally strongly associated with the class label. Our analysis is easily generalized to the situation when the strengths of associations fall in an interval [\u03b3min, \u03b3max]. Thus relevant variables agree with the class label with probability at least 1/2 + \u03b3min and misleading variables agree with the class label with probability at least 1/2 \u2212 \u03b3max. Although a sophisticated analysis would take each variable\u2019s degree of association into account, it is possible to leverage our previous analysis with a simpler approach. Using the 1/2 + \u03b3min and 1/2\u2212 \u03b3max underestimates on the probability that relevant variables and misleading variables agree with the class label leads to an analog of Theorem 1. This analog says that models voting n variables, k of which are relevant and ` of which are misleading, have error probabilities bounded by\nexp\n( \u22122[\u03b3mink \u2212 \u03b3max`]2+\nn\n) .\nWe can also use the upper and lower bounds on association to get high-confidence bounds (like those of Lemmas 4 and 6) on the numbers of relevant and misleading features in models M\u03b2. This leads to an analog of Theorem 3 bounding the expected error rate of M\u03b2 by\n(1 + o(1)) exp \u22122\u03b32minK [ 1\u2212 4(1 + \u03b3max/\u03b3min)e\u22122(\u03b3min\u2212\u03b2) 2m \u2212 \u03b3min ]2 +\n1 + 8NK e \u22122\u03b22m + \u03b3min  when 0 \u2264 \u03b2 \u2264 \u03b3 and \u03b3max \u2208 o(1). Note that \u03b3 in Theorem 3 is replaced by \u03b3min here, and \u03b3max only appears in the 4(1 + \u03b3max/\u03b3min) factor (which replaces an \u201c8\u201d in the original theorem).\nContinuing to mimic our previous analysis gives analogs to Theorem 8 and Corollary 10. These analogs imply that if \u03b3max/\u03b3min is bounded then algorithms using small \u03b2 perform well in the same limiting situations used in Section 5 to bound the effectiveness of exclusive algorithms.\nA more sophisticated analysis keeping better track of the degree of association between relevant variables and the class label may produce better bounds. In addition, if the variables have varying strengths then it makes sense to consider classifiers that assign different voting weights to the variables based on their estimated strength of association with the class label. An analysis that takes account of these issues is a potentially interesting subject for further research."}, {"heading": "7. Conclusions", "text": "We analyzed learning when there are few examples, a small fraction of the variables are relevant, and the relevant variables are only weakly correlated with the class label. In this situation, algorithms that produce hypotheses consisting predominately of irrelevant variables can be highly accurate (with error rates going to 0). Furthermore, this inclusion of many irrelevant variables is essential. Any algorithm limiting the expected fraction of irrelevant variables in its hypotheses has an error rate bounded below by a constant. This\nis in stark contrast with many feature selection heuristics that limit the number of features to a small multiple of the number of examples, or that limit the classifier to use variables that pass stringent statistical tests of association with the class label.\nThese results have two implications on the practice of machine learning. First, they show that the engineering practice of producing models that include enormous numbers of variables is sometimes justified. Second, they run counter to the intuitively appealing view that accurate class prediction \u201cvalidates\u201d the variables used by the predictor."}, {"heading": "Acknowledgements", "text": "We thank Aravind Srinivasan for his help."}, {"heading": "Appendix A. Appendices", "text": "A.1 Proof of (5) and (6)\nEquation 4.1 from (Motwani and Raghavan, 1995) is\nP[U > (1 + \u03b7)E(U)] < (\ne\u03b7\n(1 + \u03b7)1+\u03b7\n)E(U) (26)\nand holds for independent 0-1 valued Ui\u2019s each with (possibly different) probabilities pi = P (Ui = 1) where 0 < pi < 1 and \u03b7 > 0. Taking the logarithm of the RHS, we get\nln(RHS) = E(U) (\u03b7 \u2212 (1 + \u03b7) ln(1 + \u03b7)) (27) < E(U) (\u03b7 + 1\u2212 (1 + \u03b7) ln(1 + \u03b7)) = \u2212E(U)(\u03b7 + 1)(ln(1 + \u03b7)\u2212 1),\nwhich implies (5). From (26), when 0 \u2264 \u03b7 \u2264 4 (since \u03b7 \u2212 (1 + \u03b7) ln(1 + \u03b7) < \u2212\u03b72/4 there), P[U > (1 + \u03b7)E(U)] < exp ( \u2212\u03b72E(U)/4 ) showing (6).\nA.2 Proof of (7)\nUsing (5) with \u03b7 = 3 + 3 ln(1/\u03b4)/E(U) gives\nP[U > 4E(U) + 3 ln(1/\u03b4)] < exp ( \u2212(4E(U) + 3 ln \u03b4) ln ( 4 + 3 ln(1/\u03b4)/E(U)\ne )) < exp ( \u2212(3 ln(1/\u03b4) ln ( 4\ne )) < exp (\u2212 ln(1/\u03b4)) = \u03b4\nusing the fact that ln(4/e) \u2248 0.38 > 1/3.\nA.3 Proof of (8)\nThe following is a straightforward consequence of the Berry-Esseen inequality.\nLemma 20 ((see DasGupta, 2008, Theorem 11.1)) Under the assumptions of Section 2 with each P[Ui = 1] = 1/2, let:\nTi = 2(Ui \u2212 1/2),\nT =\n\u221a 1\n` \u2211\u0300 i=1 Ti, and\nZ be a standard normal random variable.\nThen for all \u03b7, we have \u2223\u2223P[T > \u03b7]\u2212 P[Z > \u03b7]\u2223\u2223 \u2264 1\u221a\n` .\nLemma 21 ((Feller, 1968, Chapter VII, section 1)) If Z is a standard normal random variable and x > 0, then\n1\u221a 2\u03c0\n( 1\nx \u2212 1 x3\n) e\u2212x\n2/2 < P[Z > x] < 1\u221a 2\u03c0\n( 1\nx\n) e\u2212x 2/2.\nNow, to prove (8), let M = 1` \u2211` i=1(Ui \u2212 1 2) and let Z be a standard normal random\nvariable. Then Lemma 20 implies that, for all \u03ba\u2223\u2223\u2223P[2\u221a`M > \u03ba]\u2212 P[Z > \u03ba]\u2223\u2223\u2223 \u2264 1\u221a ` . Using \u03ba = 2\u03b7 \u221a `,\nP[M > \u03b7] \u2265 P [ Z > 2\u03b7 \u221a ` ] \u2212 1\u221a\n` . (28)\nApplying Lemma 21, we get\nP [ Z > 2\u03b7 \u221a ` ] \u2265 1\u221a\n2\u03c0\n( 1 2\u03b7 \u221a ` \u2212 ( 1 2\u03b7 \u221a ` )3) e\u22122\u03b7 2`.\nSince ` \u2265 1/\u03b72, we get\nP [ Z > 2\u03b7 \u221a ` ] \u2265 1\u221a\n2\u03c0\n( 1\n2 \u2212 1 8\n) 1\n\u03b7 \u221a ` e\u22122\u03b7 2`\n\u2265 1 7\u03b7 \u221a ` e\u22122\u03b7 2`.\nCombining with (28) completes the proof of (8).\nA.4 Proof of (9)\nWe follow the proof of Proposition 7.3.2 in (Matous\u030cek and Vondrak, 2011, Page 46).\nLemma 22 For n even, let U1, . . . , Un be i.i.d. RVs with P[U1 = 0] = P[U1 = 1] = 1/2 and U = \u2211n i=1. Then for integer t \u2208 [0, n 8 ],\nP [ U \u2265 n 2 + t ] \u2265 1 5 e\u221216t 2/n.\nProof Let integer m = n/2.\nP[U \u2265 m+ t] = 2\u22122m m\u2211 j=t ( 2m m+ j ) (29)\n\u2265 2\u22122m 2t\u22121\u2211 j=t ( 2m m+ j ) (30)\n= 2\u22122m 2t\u22121\u2211 j=t ( 2m m ) \u00b7 m m+ j \u00b7 m\u2212 1 m+ j \u2212 1 \u00b7 \u00b7 \u00b7 m\u2212 j + 1 m+ 1\n(31)\n\u2265 1 2 \u221a m 2t\u22121\u2211 j=t j\u220f i=1 ( 1\u2212 j m+ 1 ) using ( 2m m ) \u2265 22m/2 \u221a m (32)\n\u2265 t 2 \u221a m\n( 1\u2212 2t\nm\n)2t (33)\n\u2265 t 2 \u221a m e\u22128t 2/m since 1\u2212 x \u2265 e\u22122x for 0 \u2264 x \u2264 1/2. (34)\nFor t \u2265 12 \u221a m, the last expression is at least 14e \u221216t2/n. Note that P[U = m] = 2\u22122m ( 2m m ) \u2264 1/ \u221a \u03c0m. Thus for 0 \u2264 t < 12 \u221a m, we have\nP[U \u2265 m+ t] \u2265 1 2 \u2212 tP[U = m] (35)\n\u2265 1 2 \u2212 1 2 \u221a m 1\u221a \u03c0m\n(36)\n\u2265 1 2 \u2212 1 2 \u221a \u03c0 \u2248 0.218 \u2265 1 5 \u2265 1 5 e\u221216 2/n (37)\nThus the bound 15e \u221216t2/n holds for all 0 \u2264 t \u2264 m/4.\nA.5 Proof of (10)\nThe proof of (10) uses the next two lemmas and and follows the proof of Lemma 5.1 in (Anthony and Bartlett, 1999).\nLemma 23 (Slud\u2019s Inequality, (Slud, 1977)) Let B be a binomial (`, p) random variable with p \u2264 1/2. Then for `(1\u2212 p) \u2265 j \u2265 `p,\nP[B \u2265 j] \u2265 P [ Z \u2265 j \u2212 `p\u221a\n`p(1\u2212 p) ] where Z is a standard normal random variable.\nLemma 24 ((see Anthony and Bartlett, 1999, Appendix 1)) If Z is a standard normal and x > 0 then\nP[Z \u2265 x] \u2265 1 2\n( 1\u2212 \u221a 1\u2212 e\u2212x2 ) .\nRecall that in (10) U the sum of the ` i.i.d. boolean random variables, each of which is 1 with probability 12 +\u03b7. Let B be a random variable with the binomial (`, 1 2\u2212\u03b7) distribution.\nP [ 1\n` U < 1/2\n] = P[B \u2265 `/2]\n\u2265 P [ N \u2265 `/2\u2212 `(1/2\u2212 \u03b7)\u221a\n`(1/2 + \u03b7)(1/2\u2212 \u03b7)\n] Slud\u2019s Inequality\n= P [ N \u2265 2\u03b7 \u221a `\u221a\n(1\u2212 4\u03b72)\n]\n\u2265 1 2\n( 1\u2212 \u221a 1\u2212 exp ( \u2212 4\u03b7 2`\n1\u2212 4\u03b72\n))\n\u2265 1 4 exp\n( \u2212 4\u03b7 2`\n1\u2212 4\u03b72\n) since 1\u2212 \u221a 1\u2212 x > x/2\n\u2265 1 4\nexp ( \u22125\u03b72` ) when \u03b7 \u2264 1/5\ncompleting the proof of (10)."}], "references": [{"title": "Boosting with diverse base classifiers", "author": ["S. Dasgupta", "P.M. Long"], "venue": "COLT,", "citeRegEx": "Dasgupta and Long.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Long.", "year": 2003}, {"title": "On the necessity of irrelevant variables", "author": ["J. Jin"], "venue": "Information and Computation,", "citeRegEx": "Jin.,? \\Q1994\\E", "shortCiteRegEx": "Jin.", "year": 1994}, {"title": "Empirical margin distributions and bounding the gen", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": null, "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2009\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2009}, {"title": "Randomized Algorithms", "author": ["R. Motwani", "P. Raghavan"], "venue": null, "citeRegEx": "2006", "shortCiteRegEx": "2006", "year": 1995}, {"title": "logistic regression and naive bayes", "author": ["S. Pemmaraju"], "venue": "bounds. RANDOM,", "citeRegEx": "Pemmaraju.,? \\Q2001\\E", "shortCiteRegEx": "Pemmaraju.", "year": 2001}, {"title": "Bagging, boosting and C4.5", "author": ["Helmbold", "Long J. Quinlan"], "venue": null, "citeRegEx": "Helmbold and Quinlan.,? \\Q1996\\E", "shortCiteRegEx": "Helmbold and Quinlan.", "year": 1996}], "referenceMentions": [{"referenceID": 3, "context": "Over the past decade or so, a number of empirical and theoretical findings have challenged the traditional rule of thumb described by Bishop (2006) as follows. One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model. The Support Vector Machine literature (see Vapnik, 1998) views algorithms that compute apparently complicated functions of a given set of variables as linear classifiers applied to an expanded, even infinite, set of features. These empirically perform well on test data, and theoretical accounts have been given for this. Boosting and Bagging algorithms also generalize well, despite combining large numbers of simple classifiers \u2013 even if the number of such \u201cbase classifiers\u201d is much more than the number of training examples (Quinlan, 1996; Breiman, 1998; Schapire et al., 1998). This is despite the fact that Friedman et al. (2000) showed the behavior of such classifiers is closely related to performing logistic regression on a potentially vast set of features (one for each possible decision tree, for example).", "startOffset": 142, "endOffset": 980}, {"referenceID": 1, "context": "Relationship to Previous Work Donoho and Jin (see Donoho and Jin, 2008; Jin, 2009) and Fan and Fan (2008), building on a line of research on joint testing of multiple hypotheses (see Abramovich et al.", "startOffset": 41, "endOffset": 106}, {"referenceID": 1, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al.", "startOffset": 8, "endOffset": 42}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case.", "startOffset": 43, "endOffset": 88}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples.", "startOffset": 43, "endOffset": 239}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples. But their bound for Naive Bayes is also vacuous when m = \u0398(1/\u03b32). Bickel and Levina (2004) studied the case in which the class conditional distributions are Gaussians, and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case, especially when the number of variables is large.", "startOffset": 43, "endOffset": 493}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples. But their bound for Naive Bayes is also vacuous when m = \u0398(1/\u03b32). Bickel and Levina (2004) studied the case in which the class conditional distributions are Gaussians, and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case, especially when the number of variables is large. B\u00fchlmann and Yu (2002) analyzed the variance-reduction benefits of Bagging with primary focus on the benefits of the smoother classifier that is obtained when ragged classifiers are averaged.", "startOffset": 43, "endOffset": 766}, {"referenceID": 4, "context": "Then applying a Chernoff-Hoeffding bound for such sets of random variables due to Pemmaraju (2001), if r \u2264 n/2, one gets a bound of c(r + 1) exp ( \u22122\u03b32k2 n(r+1) )", "startOffset": 82, "endOffset": 99}], "year": 2012, "abstractText": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.", "creator": "LaTeX with hyperref package"}}}