{"id": "1203.5262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2012", "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", "abstract": "Currently, computers are used to solve complex tasks and problems ranging from simple calculations to intensive digital image processing and complex algorithmic optimization problems to mathematically demanding weather forecasting problems. ASR, short for Automatic Speech Recognition, is another computer problem whose purpose is to recognize and convert human speech into text that can be processed by a computer. Although ASR has many versatile and ubiquitous applications in the real world, it is still relatively flawed and not perfectly solved as it is prone to produce spelling errors in recognized text, especially when the ASR system operates in a noisy environment, its vocabulary is limited and its input language is poor or low quality. This paper proposes a post-edited ASR error correction method based on Microsoft's N-gram data sets for recognizing and correcting spelling errors generated by ASR systems.", "histories": [["v1", "Fri, 23 Mar 2012 14:51:05 GMT  (832kb)", "http://arxiv.org/abs/1203.5262v1", "LACSC - Lebanese Association for Computational Sciences -this http URL"]], "COMMENTS": "LACSC - Lebanese Association for Computational Sciences -this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["youssef bassil", "paul semaan"], "accepted": false, "id": "1203.5262"}, "pdf": {"name": "1203.5262.pdf", "metadata": {"source": "CRF", "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", "authors": ["Youssef Bassil", "Paul Semaan"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Artificial Intelligence, Natural Language Processing, Speech Recognition and Synthesis, Error Correction\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n1INTRODUCTION ith the advancement of information technologies, computers are no more exclusively used for performing mathematical and scientific operations. Instead, miscellaneous applications are now possible, allowing computers to solve versatile problems pertaining to different fields and domains. ASR short for Automatic Speech Recognition has been a subject of great focus and attention in recent years as it has been studied and researched by severalscientists, universities, and research centers. Inherently, ASR converts spoken words represented mathematically as an acoustic waveform into text that can be processed by a computer [1].Speech-ToText (STT), Automated Telephone Services (ATS), Voice User Interface (VUI), Voice-driven Home Automation (Domotics), and Speech Dictation are few ASR applications to mention.\nIn spite of the great advantages and benefits of ASR, it isstill error-prone and imperfect as it produces spelling errors in the recognized output text. Commonly, ASR errors are manifested as linguistic mistakes and misspellings visible at the final output of the system. These errors are often caused by the extreme noise in theenvironment, the bad quality of the speech, the fluctuating utterance of the dialogue, and the small size of\nthe ASR vocabulary [2], [3]. Numerous error-correction methods and algorithms were devised to help fight against ASR errors, some of themrely on post-processing the output text and correcting it manually;whereas others rely on building improved acoustic models to increase the precision of speech recognition [4]. Regardless of all these attempts focused on reducing the ASR error rate, results are not yet convincing and speech recognition systems still suffer a major degradation in performance.\nThis paper proposes a post-editing ASR error correction method for detecting and correcting non-word and real-word errors generated by ASR recognition systems, based on data extracted from Microsoft Web NGram dataset [5]. Principally, the Microsoft Web N-Gram datasetenclosespetabytes ofn-gram word counts and statistics retrieved from the Internet and Bing search engine [6], and is appropriate for carrying out text spelling correction. The proposed approach is a postediting process which spell-checks the final recognized output text after the input wave has been completely converted.Itis majorly composed of three foremost algorithms: An error detection algorithmthat detects nonword errors in the ASRoutput text using unigram information from Microsoft Web N-Gram dataset; a candidate corrections generationalgorithmthatgenerates possible correction spellings for the misspelled words using a character-based2-gram model; and a contextsensitivereal-word error correction algorithmthatpicks out the closest candidate for correction using 5-gram counts from Microsoft Web N-Gram dataset. In effect, as the proposed techniquemakes use of real-world web-scale \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  Youssef Bassil is the Chief Science Officer of the Lebanese Association for Computational Sciences, (LACSC), reg. no. 957, 2011, Beirut, Lebanon.  Paul Semaan is a senior researcher at the Lebanese Association for Computational Sciences, (LACSC), reg. no. 957, 2011, Beirut, Lebanon.\nW\ndata, it can significantly decrease the ASR error rate and consequently improve theperformance of ASR systems.\n2AUTOMATIC SPEECH RECOGNITION As defined by many textbooks [7], [8], and [9], automatic speech recognition systems also known as ASR, receive some speech signals as input and generate a corresponding readable text transcript as output. Put differently, it simply converts spoken words into text. Figure 1 shows a speech recognition system in which a voice W usually generated by a speaker such a human person, propagates as a waveform into the communication channel where it is analyzed and processed to eventually be transformed into a readable text W\u2019. \u00a0\nEssentially,\u00a0an\u00a0ASR\u00a0system\u00a0is\u00a0often\u00a0implemented\u00a0using\u00a0 a\u00a0Hidden\u00a0Markov\u00a0Model\u00a0 (HMM)\u00a0 [10],\u00a0 [11]\u00a0based\u00a0on\u00a0 the\u00a0 notion\u00a0 of\u00a0 noisy\u00a0 channel\u00a0 [12].\u00a0 The\u00a0 concept\u00a0 behind\u00a0 the\u00a0 noisy\u00a0 channel\u00a0 model\u00a0 is\u00a0 to\u00a0 consider\u00a0 the\u00a0 input\u00a0 acoustic\u00a0 waveform\u00a0 as\u00a0 a\u00a0 noisy\u00a0 signal\u00a0 which\u00a0 has\u00a0 been\u00a0 distorted\u00a0 somehow\u00a0 during\u00a0 transmission.\u00a0 The\u00a0 quintessence\u00a0 of\u00a0 this\u00a0 approach\u00a0 is\u00a0 that\u00a0 if\u00a0 one\u00a0 could\u00a0 know\u00a0 how\u00a0 the\u00a0 original\u00a0 waveform\u00a0 was\u00a0 distorted,\u00a0 it\u00a0 is\u00a0 then\u00a0 easy\u00a0 to\u00a0 find\u00a0 the\u00a0 original\u00a0 input.The\u00a0 Hidden\u00a0 Markov\u00a0 Model\u00a0 is\u00a0 a\u00a0 special\u00a0 type\u00a0of\u00a0weighted\u00a0finite\u2010state\u00a0automata,\u00a0defined\u00a0by\u00a0a\u00a0set\u00a0of\u00a0 N\u00a0 states\u00a0Q=q1q1\u2026qn;\u00a0a\u00a0 start\u00a0 state\u00a0q0\u00a0and\u00a0an\u00a0end\u00a0 state\u00a0qf;\u00a0a\u00a0 sequence\u00a0 of\u00a0 input\u00a0 observations\u00a0 O=o1o2\u2026or;\u00a0 a\u00a0 set\u00a0 of\u00a0 transitions\u00a0 from\u00a0one\u00a0 state\u00a0 to\u00a0another\u00a0based\u00a0on\u00a0 the\u00a0 input\u00a0 observations;\u00a0 and\u00a0 two\u00a0 types\u00a0 of\u00a0 probabilities:\u00a0 The\u00a0 prior\u00a0 probability\u00a0and\u00a0 the\u00a0 likelihood\u00a0probability.\u00a0The\u00a0 former\u00a0 is\u00a0 associated\u00a0with\u00a0every\u00a0transition\u00a0and\u00a0indicates\u00a0how\u00a0likely\u00a0 a\u00a0transition\u00a0is\u00a0to\u00a0be\u00a0taken.It\u00a0is\u00a0represented\u00a0by\u00a0a\u00a0transition\u00a0 probability\u00a0matrix\u00a0A=a11a12\u2026an1\u2026anm\u00a0where\u00a0 aij\u00a0 represents\u00a0 the\u00a0 probability\u00a0 of\u00a0 transiting\u00a0 from\u00a0 state\u00a0 i\u00a0 to\u00a0 state\u00a0 j.\u00a0 The\u00a0 latter\u00a0is\u00a0denoted\u00a0by\u00a0B=bi(ot),\u00a0and\u00a0consists\u00a0of\u00a0a\u00a0sequence\u00a0of\u00a0 observations\u00a0 likelihood\u00a0 that\u00a0 indicates\u00a0 the\u00a0 probability\u00a0 of\u00a0 an\u00a0observation\u00a0otbeing\u00a0emerged\u00a0from\u00a0state\u00a0i.\u00a0\nFundamentally,\u00a0 an\u00a0 ASR\u00a0 system\u00a0 is\u00a0 a\u00a0 blend\u00a0 of\u00a0 four\u00a0 logical\u00a0 modules\u00a0 [7],\u00a0 each\u00a0 of\u00a0 which\u00a0 has\u00a0 a\u00a0 particular\u00a0 algorithm,\u00a0purpose,\u00a0and\u00a0 inner\u2010workings,\u00a0and\u00a0 they\u00a0are\u00a0 in\u00a0 order:\u00a0 The\u00a0 signal\u00a0 processing\u00a0 module,\u00a0 the\u00a0 acoustic\u00a0 modeling\u00a0module,\u00a0 the\u00a0 language\u00a0modeling\u00a0module,\u00a0 and\u00a0 the\u00a0decoding\u00a0module.\u00a0Figure\u00a02\u00a0depicts\u00a0a\u00a0block\u00a0diagram\u00a0of\u00a0 an\u00a0automatic\u00a0speech\u00a0recognition\u00a0system\u00a0comprising\u00a0 four\u00a0 functional\u00a0modules.\u00a0\nThe\u00a0 first\u00a0 is\u00a0 the\u00a0 signal\u00a0 processing\u00a0 module,\u00a0 in\u00a0 which\u00a0 spectral\u00a0 features\u00a0 are\u00a0 extracted\u00a0 from\u00a0 the\u00a0 input\u00a0 acoustic\u00a0 waveform\u00a0by\u00a0sampling\u00a0and\u00a0capturing\u00a0small\u00a0frames\u00a0out\u00a0of\u00a0 the\u00a0 input\u00a0 signal\u00a0 on\u00a0 a\u00a0 maximum\u00a0 interval\u00a0 of\u00a0 20\u00a0 milliseconds.\u00a0 The\u00a0 spectral\u00a0 features\u00a0 help\u00a0 building\u00a0 phone\u00a0 and\u00a0sub\u2010phone\u00a0classifiers.\u00a0Phones\u00a0are\u00a0individual\u00a0symbols\u00a0 that\u00a0model\u00a0the\u00a0pronunciation\u00a0of\u00a0a\u00a0word.Computationally,\u00a0 the\u00a0 input\u00a0signal\u00a0 is\u00a0 first\u00a0converted\u00a0 from\u00a0analog\u00a0 to\u00a0digital,\u00a0 then\u00a0 the\u00a0power\u00a0 of\u00a0 its\u00a0high\u00a0 frequencies\u00a0 is\u00a0 amplified\u00a0 in\u00a0 a\u00a0 process\u00a0 called\u00a0 Pre\u2010emphasis\u00a0 with\u00a0 the\u00a0 objective\u00a0 of\u00a0 increasing\u00a0the\u00a0speech\u00a0detection\u00a0accuracy.\u00a0Afterwards,\u00a0the\u00a0 windowing\u00a0process\u00a0is\u00a0introduced\u00a0to\u00a0divide\u00a0the\u00a0signal\u00a0into\u00a0 frames\u00a0 of\u00a0 signal\u00a0 speeches\u00a0 having\u00a0 a\u00a0 particular\u00a0 length\u00a0 usually\u00a025ms.\u00a0These\u00a0frames\u00a0are\u00a0separated\u00a0from\u00a0each\u00a0other\u00a0 by\u00a0 an\u00a0 offset\u00a0 called\u00a0 frame\u00a0 shift\u00a0 usually\u00a0 of\u00a0 length\u00a0 10ms.\u00a0 Next,\u00a0 the\u00a0Discrete\u00a0Fourier\u00a0Transform\u00a0 (DFT)\u00a0 is\u00a0applied\u00a0 to\u00a0 transform\u00a0 the\u00a0 previous\u00a0 signal\u00a0 frames\u00a0 into\u00a0 a\u00a0 complex\u00a0 number\u00a0 representing\u00a0 the\u00a0 phase\u00a0 and\u00a0 magnitude\u00a0 of\u00a0 the\u00a0 frequency\u00a0 of\u00a0 that\u00a0 frame.\u00a0 Finally,\u00a0 the\u00a0 cepstrum\u00a0 is\u00a0 calculated\u00a0 via\u00a0 the\u00a0 Inverse\u00a0 DFT\u00a0 which\u00a0 significantly\u00a0 improves\u00a0 the\u00a0 accuracy\u00a0 and\u00a0 performance\u00a0 of\u00a0 phones\u00a0 recognition.\u00a0The\u00a0result\u00a0of\u00a0this\u00a0module\u00a0is\u00a039\u00a0features\u00a0called\u00a0 Mel\u00a0 Frequency\u00a0 Cepstral\u00a0 Coefficients\u00a0 (MFCC),\u00a0 which\u00a0 uniquely\u00a0 identify\u00a0 a\u00a0discrete\u00a0 acoustic\u00a0phone\u00a0 in\u00a0 the\u00a0 input\u00a0 sound.\u00a0\nThe\u00a0 second\u00a0 is\u00a0 the\u00a0 acoustic\u00a0 modeling\u00a0 (AM)\u00a0 module,\u00a0 which\u00a0 computes\u00a0 the\u00a0 likelihood\u00a0 of\u00a0 the\u00a0 observed\u00a0 spectral\u00a0 feature\u00a0 vectors\u00a0 given\u00a0 linguistic\u00a0 units\u00a0 (phones).For\u00a0 instance,\u00a0 it\u00a0 computes\u00a0 the\u00a0 likelihood\u00a0 P(o|q)\u00a0 of\u00a0 a\u00a0 specific\u00a0 feature\u00a0 vector\u00a0 o\u00a0 given\u00a0 a\u00a0 particular\u00a0 HMM\u00a0 stage\u00a0 q\u00a0 that\u00a0 represents\u00a0 a\u00a0 particular\u00a0 phone\u00a0 x.\u00a0 For\u00a0 this\u00a0 purpose,\u00a0 a\u00a0 lexicon\u00a0 of\u00a0 words\u00a0 with\u00a0 their\u00a0 corresponding\u00a0 phones\u00a0 (a\u00a0 sequence\u00a0of\u00a0pronunciations),\u00a0 is\u00a0used\u00a0 to\u00a0help\u00a0 recognizing\u00a0 the\u00a0spoken\u00a0words.\u00a0\nThe\u00a0 third\u00a0 is\u00a0 the\u00a0 language\u00a0 modeling\u00a0 (LM)\u00a0 module,\u00a0 which\u00a0 computes\u00a0 the\u00a0 prior\u00a0 probability\u00a0 P(W)that\u00a0 approximates\u00a0how\u00a0 likely\u00a0a\u00a0given\u00a0 sentence\u00a0 is\u00a0 to\u00a0occur\u00a0 in\u00a0 the\u00a0 language.\u00a0 P(W)\u00a0 is\u00a0 usually\u00a0 calculated\u00a0 using\u00a0 the\u00a0 probabilistic\u00a0 n\u2010gram\u00a0model\u00a0 [13]\u00a0which\u00a0predicts\u00a0 the\u00a0next\u00a0 word,\u00a0letter,\u00a0or\u00a0phone\u00a0in\u00a0a\u00a0given\u00a0sequence.\u00a0In\u00a0short,\u00a0an\u00a0n\u2010 gram\u00a0 is\u00a0 simply\u00a0 a\u00a0 collocation\u00a0 of\u00a0 words\u00a0 that\u00a0 is\u00a0 n\u00a0 words\u00a0 long.\u00a0\nThe\u00a0 fourth\u00a0 is\u00a0 the\u00a0 decoding\u00a0module,\u00a0which\u00a0 joins\u00a0 the\u00a0 observation\u00a0 likelihood\u00a0 P(O|W)\u00a0 resulted\u00a0 from\u00a0 of\u00a0 the\u00a0 acoustic\u00a0 model\u00a0 and\u00a0 the\u00a0 prior\u00a0 probability\u00a0 P(W)\u00a0 resulted\u00a0 from\u00a0 the\u00a0 language\u00a0 model\u00a0 to\u00a0 deduce\u00a0 the\u00a0 most\u00a0 likely\u00a0 output\u00a0 textW\u2019.\u00a0 The\u00a0 products\u00a0 of\u00a0 all\u00a0 probabilities\u00a0 are\u00a0 calculated\u00a0 P(O|W)*P(W)\u00a0 and\u00a0 the\u00a0 one\u00a0 with\u00a0 the\u00a0 greatest\u00a0 value\u00a0 is\u00a0 selected\u00a0 as\u00a0 the\u00a0 output\u00a0 textW\u2019.\u00a0 Figure\u00a0 3\u00a0 is\u00a0 the\u00a0 mathematical\u00a0 equation\u00a0 for\u00a0 calculating\u00a0 and\u00a0 choosing\u00a0 the\u00a0 most\u00a0probable\u00a0output\u00a0textW\u2019.\u00a0\nFig.3.Equation for Finding the Product of the Likelihood and the Prior\nProbability\n3ERRORS IN SPEECH RECOGNITION SYSTEMS An\u00a0early\u00a0experiment\u00a0conducted\u00a0at\u00a0IBM\u00a0research\u00a0labs\u00a0[14]\u00a0 to\u00a0 calculate\u00a0 the\u00a0 number\u00a0 of\u00a0 errors\u00a0 generated\u00a0 by\u00a0 ASR\u00a0 systems,\u00a0 showed\u00a0 an\u00a0 average\u00a0 of\u00a0 105\u00a0 errors\u00a0 being\u00a0 committed\u00a0 per\u00a0 minute.Essentially,\u00a0 ASR\u00a0 errors\u00a0 are\u00a0 relatively\u00a0due\u00a0to\u00a0five\u00a0factors:The\u00a0first\u00a0factor\u00a0is\u00a0noise\u00a0which\u00a0 is\u00a0 tightly\u00a0 associated\u00a0withthe\u00a0 condition\u00a0of\u00a0 the\u00a0 location\u00a0 in\u00a0 which\u00a0 speech\u00a0 recognition\u00a0 is\u00a0 being\u00a0 performed.For\u00a0 instance,\u00a0 if\u00a0 the\u00a0 recognition\u00a0 process\u00a0 occurs\u00a0 in\u00a0 a\u00a0 silent\u00a0 environment,\u00a0 it\u00a0 yieldsto\u00a0more\u00a0 accurate\u00a0 results\u00a0 than\u00a0 if\u00a0 it\u00a0 would\u00a0have\u00a0been\u00a0occurred\u00a0in\u00a0a\u00a0noisy\u00a0environment.\u00a0Noise\u00a0 adds\u00a0 extra\u00a0 signals\u00a0 to\u00a0 the\u00a0 speech;\u00a0 and\u00a0 hence,it\u00a0 alters\u00a0 the\u00a0 content\u00a0 of\u00a0 the\u00a0 input\u00a0 waveform\u00a0 making\u00a0 it\u00a0 hard\u00a0 to\u00a0 be\u00a0 interpreted.The\u00a0second\u00a0factor\u00a0 is\u00a0 the\u00a0 type\u00a0of\u00a0speech\u00a0being\u00a0 recognized.\u00a0 In\u00a0 fact,\u00a0 there\u00a0 exist\u00a0 two\u00a0 types\u00a0 of\u00a0 speech:\u00a0 thediscrete\u00a0 speechalso\u00a0 called\u00a0 isolated\u2010word\u00a0 speech\u00a0 in\u00a0 which\u00a0 spoken\u00a0 words\u00a0 are\u00a0 separated\u00a0 by\u00a0 silent\u00a0 pauses,\u00a0 andthe\u00a0continuous\u00a0speech\u00a0which\u00a0contains\u00a0non\u2010segmented\u00a0 endless\u00a0 sequence\u00a0 of\u00a0words\u00a0 that\u00a0 are\u00a0more\u00a0difficult\u00a0 to\u00a0 be\u00a0 separated\u00a0 and\u00a0 distinguished\u00a0 by\u00a0 the\u00a0 system.\u00a0Continuous\u00a0 speech\u00a0 thus\u00a0 imposes\u00a0 more\u00a0 complications\u00a0 on\u00a0 the\u00a0 recognitionprocess\u00a0 whichresult\u00a0 in\u00a0 an\u00a0 increase\u00a0 in\u00a0 the\u00a0 ASRerror\u00a0 rate.\u00a0 The\u00a0 third\u00a0 factor\u00a0 is\u00a0 the\u00a0 speech\u00a0 utterance\u00a0 which\u00a0ranges\u00a0from\u00a0read\u00a0speech\u00a0to\u00a0conversational\u00a0speech.\u00a0 Conversational\u00a0 speech\u00a0 is\u00a0 more\u00a0 problematic\u00a0 to\u00a0 handle\u00a0 since\u00a0 it\u00a0 is\u00a0 spontaneous\u00a0 and\u00a0 may\u00a0 contain\u00a0 defects\u00a0 in\u00a0 pronunciation.The\u00a0fourth\u00a0factor\u00a0is\u00a0the\u00a0dialect\u00a0of\u00a0thespeech\u00a0 which\u00a0varies\u00a0from\u00a0speaker\u00a0to\u00a0speaker\u00a0as\u00a0each\u00a0person\u00a0has\u00a0 unique\u00a0spectral\u00a0 features.\u00a0ASR\u00a0systems\u00a0can\u00a0sometimes\u00a0be\u00a0 characterized\u00a0 as\u00a0 speaker\u2010dependentand\u00a0 speaker\u2010 independent\u00a0 systems.\u00a0 A\u00a0 speaker\u2010independent\u00a0 system\u00a0 operates\u00a0 for\u00a0different\u00a0 speakers\u00a0 and\u00a0 for\u00a0different\u00a0 type\u00a0 of\u00a0 speakers.\u00a0Such\u00a0 systems\u00a0are\u00a0more\u00a0challenging\u00a0 to\u00a0develop\u00a0 and\u00a0 implement,\u00a0 and\u00a0 are\u00a0 subject\u00a0 to\u00a0higher\u00a0 error\u00a0 rate.The\u00a0 fifth\u00a0 factor\u00a0 is\u00a0 the\u00a0 size\u00a0 of\u00a0 the\u00a0 vocabulary\u00a0 that\u00a0 anASR\u00a0 system\u00a0 can\u00a0 recognize.\u00a0 Basically,\u00a0 and\u00a0 since\u00a0 the\u00a0 acoustic\u00a0 model\u00a0 (AM)\u00a0 is\u00a0based\u00a0on\u00a0an\u00a0 internal\u00a0dictionaryor\u00a0 lexicon\u00a0 of\u00a0 words\u00a0 with\u00a0 their\u00a0 corresponding\u00a0 phones\u00a0 and\u00a0 pronunciations\u00a0 that\u00a0 are\u00a0 necessary\u00a0 to\u00a0 match\u00a0 a\u00a0 spoken\u00a0 word\u00a0with\u00a0an\u00a0entry\u00a0 in\u00a0 the\u00a0 lexicon,\u00a0 the\u00a0 larger\u00a0 the\u00a0 size\u00a0of\u00a0 the\u00a0 vocabulary\u00a0 thelexiconhas,\u00a0 the\u00a0 less\u00a0 is\u00a0 to\u00a0 be\u00a0 the\u00a0ASR\u00a0 error\u00a0rate.\u00a0A\u00a0small\u00a0vocabulary\u00a0can\u00a0lead\u00a0to\u00a0a\u00a0situation\u00a0often\u00a0 known\u00a0 as\u00a0 OOV\u00a0 short\u00a0 for\u00a0 Out\u2010Of\u2010Vocabulary\u00a0 which\u00a0\nusually\u00a0occurs\u00a0when\u00a0a\u00a0 spoken\u00a0word\u00a0 is\u00a0not\u00a0 found\u00a0 in\u00a0 the\u00a0 acoustic\u00a0 model\u2019s\u00a0 dictionary.This\u00a0 would\u00a0 in\u00a0 consequence\u00a0 cause\u00a0 theASR\u00a0 system\u00a0 to\u00a0 failrecognizingthe\u00a0 input\u00a0 vocal\u00a0 word.\u00a0"}, {"heading": "4 LIMITATIONS OF ASR BUILT-INDICTIONARIES", "text": "The\u00a0 vocabulary\u00a0 size\u00a0 and\u00a0 the\u00a0 number\u00a0 of\u00a0 distinct\u00a0 words\u00a0 that\u00a0an\u00a0ASRsystem\u00a0can\u00a0recognize\u00a0play\u00a0a\u00a0turning\u00a0point\u00a0 in\u00a0 determining\u00a0 the\u00a0 overall\u00a0 error\u00a0 rate\u00a0 of\u00a0 the\u00a0 system.\u00a0 Applications\u00a0 with\u00a0 fewer\u00a0 terms\u00a0 like\u00a0 \u201cyes\u201d\u00a0 and\u00a0 \u201cno\u201d,\u00a0 or\u00a0 digits\u00a0 like\u00a0\u201c1,2,3...9\u201d\u00a0are\u00a0easier\u00a0 to\u00a0handle\u00a0 than\u00a0 those\u00a0with\u00a0 large\u00a0 volume\u00a0 of\u00a0 terms\u00a0 such\u00a0 as\u00a0 continuous\u00a0 dictation\u00a0 systems\u00a0 that\u00a0sometimes\u00a0require\u00a0 the\u00a0recognition\u00a0of\u00a0a\u00a0hive\u00a0 of\u00a0 terms\u00a0 and\u00a0 words.\u00a0 Such\u00a0 systems\u00a0 are\u00a0 called\u00a0 LVCSR\u00a0 (Large\u2010Vocabulary\u00a0Continuous\u00a0 Speech\u00a0Recognition)\u00a0 and\u00a0 they\u00a0usually\u00a0need\u00a0to\u00a0recognize\u00a0between\u00a020,000\u00a0and\u00a060,000\u00a0 terms\u00a0 while\u00a0 achieving\u00a0 a\u00a0 good\u00a0 level\u00a0 of\u00a0 accuracy\u00a0 and\u00a0 a\u00a0 minimal\u00a0 amount\u00a0 of\u00a0 errors.\u00a0 Since\u00a0 ASR\u00a0 systemsare\u00a0 often\u00a0 built\u00a0 on\u00a0 traditional\u00a0 dictionariesthat\u00a0 do\u00a0 not\u00a0 cover\u00a0 all\u00a0 wordsin\u00a0 the\u00a0 language,\u00a0 theysuffer\u00a0 from\u00a0 severe\u00a0 Out\u2010Of\u2010 Vocabulary(OOV)\u00a0deficiencies.\u00a0The\u00a0reasons\u00a0behind\u00a0OOV\u00a0 can\u00a0be\u00a0summarized\u00a0by\u00a0the\u00a0following:\u00a0\nThe\u00a0 first\u00a0 reason\u00a0 is\u00a0 thatASRsystems\u00a0 lack\u00a0 a\u00a0 comprehensive\u00a0 dictionary\u00a0 that\u00a0 can\u00a0 cover\u00a0 every\u00a0 single\u00a0 word\u00a0in\u00a0the\u00a0language.\u00a0For\u00a0instance,\u00a0the\u00a0Oxford\u00a0dictionary\u00a0 embraces\u00a0 171,476\u00a0 words\u00a0 in\u00a0 current\u00a0 use,\u00a0 and\u00a0 47,156\u00a0 obsolete\u00a0 words,\u00a0 in\u00a0 addition\u00a0 to\u00a0 their\u00a0 derivatives\u00a0 which\u00a0 count\u00a0around\u00a09,500\u00a0words.\u00a0This\u00a0suggests\u00a0 that\u00a0 there\u00a0 is,\u00a0at\u00a0 the\u00a0 very\u00a0 least,\u00a0 a\u00a0 quarter\u00a0 of\u00a0 a\u00a0 million\u00a0 distinct\u00a0 English\u00a0 words.\u00a0Besides,\u00a0spoken\u00a0languages\u00a0may\u00a0have\u00a0one\u00a0or\u00a0more\u00a0 varieties\u00a0 each\u00a0 with\u00a0 dissimilar\u00a0 words,\u00a0 for\u00a0 instance,\u00a0 the\u00a0 German\u00a0 language\u00a0 has\u00a0 two\u00a0 varieties,\u00a0 a\u00a0 new\u2010spelling\u00a0 variance\u00a0 and\u00a0 an\u00a0 old\u2010spelling\u00a0 variance.\u00a0 Likewise,\u00a0 the\u00a0 Armenian\u00a0 language\u00a0 has\u00a0 three\u00a0 varieties\u00a0 each\u00a0 with\u00a0 a\u00a0 number\u00a0of\u00a0deviating\u00a0words:\u00a0Eastern\u00a0Armenian,\u00a0Western\u00a0 Armenian,\u00a0 and\u00a0 Grabar.\u00a0 Therefore,\u00a0 it\u00a0 is\u00a0 obvious\u00a0 that\u00a0 languages\u00a0 are\u00a0not\u00a0uniform,\u00a0 in\u00a0 a\u00a0 sense\u00a0 that\u00a0 they\u00a0 are\u00a0not\u00a0 standardized\u00a0and\u00a0thereby\u00a0cannot\u00a0be\u00a0supported\u00a0by\u00a0a\u00a0single\u00a0 dictionary.\u00a0\nThe\u00a0 second\u00a0 reason\u00a0 is\u00a0 that\u00a0 regular\u00a0 dictionaries\u00a0 normally\u00a0 target\u00a0 a\u00a0 specific\u00a0 language\u00a0 in\u00a0 that\u00a0 they\u00a0 cannot\u00a0 support\u00a0multiple\u00a0languages\u00a0simultaneously.\u00a0For\u00a0instance,\u00a0 the\u00a0Oxford\u00a0dictionary\u00a0only\u00a0 targets\u00a0 the\u00a0English\u00a0 language.\u00a0 The\u00a0 Hachette\u00a0 dictionary\u00a0 targets\u00a0 the\u00a0 French\u00a0 language,\u00a0 while\u00a0 the\u00a0 Al\u00a0 Kamel\u00a0 dictionary\u00a0 targets\u00a0 the\u00a0 Arabic\u00a0 language.\u00a0 Therefore,\u00a0 it\u00a0 is\u00a0 unquestionably\u00a0 impossible\u00a0 to\u00a0 create\u00a0 an\u00a0 international\u00a0 dictionary\u00a0 pertaining\u00a0 to\u00a0 all\u00a0 languages\u00a0of\u00a0the\u00a0world.\u00a0\nThe\u00a0 third\u00a0 reason\u00a0 is\u00a0 that\u00a0 conventional\u00a0dictionaries\u00a0do\u00a0 not\u00a0 expansively\u00a0 support\u00a0 proper\u00a0 and\u00a0 personal\u00a0 names,\u00a0 names\u00a0 of\u00a0 countries,\u00a0 regions,\u00a0 geographical\u00a0 locations,\u00a0 technical\u00a0 keywords,\u00a0 domain\u00a0 specific\u00a0 terms,\u00a0 and\u00a0 acronyms.\u00a0For\u00a0instance,\u00a0an\u00a0ordinary\u00a0dictionary\u00a0can\u00a0falsely\u00a0 detect\u00a0 \u201cAndrew\u00a0 Jackson\u201d,\u00a0 \u201cIntel\u201d,\u00a0 and\u00a0 \u201cRenault\u201d\u00a0 as\u00a0 incorrect\u00a0words.\u00a0Relatedly,\u00a0 technical\u00a0 terminologies\u00a0 such\u00a0 as\u00a0 \u201cUSB\u201d,\u00a0 \u201cSATA\u201d,\u00a0 and\u00a0 \u201cTexel\u201d,\u00a0 and\u00a0names\u00a0 of\u00a0diseases\u00a0 such\u00a0 as\u00a0 \u201cLeukemia\u201d,\u00a0 \u201cParkinson\u201d,\u00a0 and\u00a0 \u201cCholera\u201d\u00a0 canbe\u00a0 falsely\u00a0detected\u00a0 as\u00a0misspellings\u00a0 too.\u00a0 In\u00a0 total,\u00a0 it\u00a0 is\u00a0nearly\u00a0\nimpracticable\u00a0 to\u00a0 compile\u00a0 a\u00a0 universal\u00a0 lexicon\u00a0 containing\u00a0 words\u00a0from\u00a0all\u00a0existing\u00a0domains\u00a0and\u00a0fields.\u00a0\nThe\u00a0 fourth\u00a0 and\u00a0 last\u00a0 reason\u00a0 is\u00a0 that\u00a0 the\u00a0 content\u00a0 of\u00a0 standard\u00a0 dictionaries\u00a0 is\u00a0 static\u00a0 in\u00a0 a\u00a0 way\u00a0 that\u00a0 it\u00a0 is\u00a0 not\u00a0 constantly\u00a0 updated\u00a0 with\u00a0 new\u00a0 emerging\u00a0 words\u00a0 unless\u00a0 manually\u00a0edited,\u00a0and\u00a0 thus,\u00a0 it\u00a0cannot\u00a0keep\u00a0pace\u00a0with\u00a0 the\u00a0 immense\u00a0dynamic\u00a0breeding\u00a0of\u00a0new\u00a0words\u00a0and\u00a0terms.\u00a0\u00a0\nFor\u00a0 all\u00a0 the\u00a0 aforementioned\u00a0 reasons,\u00a0 attaining\u00a0 better\u00a0 speech\u00a0 recognition\u00a0 results\u00a0 greatly\u00a0 require\u00a0 finding\u00a0 a\u00a0 universal,\u00a0 all\u2010inclusive,\u00a0 multi\u2010language,\u00a0 and\u00a0 dynamic\u00a0 dictionary\u00a0 embracing\u00a0 a\u00a0 colossal\u00a0 volume\u00a0 of\u00a0 real\u2010world\u00a0 entries,\u00a0words,\u00a0terms,\u00a0proper\u00a0nouns,\u00a0expressions,\u00a0jargons,\u00a0 and\u00a0terminologies.\u00a0\n5STATE-OF-THE-ART ASR ERROR CORRECTION TECHNIQUES Different\u00a0error\u00a0correction\u00a0techniques\u00a0exist,\u00a0whose\u00a0purpose\u00a0 is\u00a0 to\u00a0 detect\u00a0 and\u00a0 correct\u00a0misspelled\u00a0words\u00a0 generated\u00a0 by\u00a0 ASR\u00a0 systems.\u00a0 Broadly,\u00a0 they\u00a0 can\u00a0 be\u00a0 broken\u00a0 down\u00a0 into\u00a0 several\u00a0 categories:\u00a0 Manual\u00a0 error\u00a0 correction,\u00a0 error\u00a0 correction\u00a0 based\u00a0 on\u00a0 alternative\u00a0 hypothesis,\u00a0 error\u00a0 correction\u00a0 based\u00a0 on\u00a0 pattern\u00a0 learning,\u00a0 and\u00a0 post\u2010editing\u00a0 error\u00a0correction.\u00a0\nIn\u00a0manual\u00a0error\u00a0correction,\u00a0a\u00a0staff\u00a0of\u00a0people\u00a0is\u00a0hired\u00a0to\u00a0 review\u00a0the\u00a0output\u00a0transcript\u00a0generated\u00a0by\u00a0the\u00a0ASR\u00a0system\u00a0 and\u00a0correct\u00a0the\u00a0misspelled\u00a0words\u00a0manually\u00a0by\u00a0hand.\u00a0This\u00a0 is\u00a0 to\u00a0 some\u00a0extent\u00a0 considered\u00a0 laborious,\u00a0 time\u00a0 consuming,\u00a0 and\u00a0error\u2010prone\u00a0as\u00a0the\u00a0human\u00a0eye\u00a0may\u00a0miss\u00a0some\u00a0errors.\u00a0\nAnother\u00a0category\u00a0of\u00a0error\u00a0correction\u00a0 is\u00a0 the\u00a0alternative\u00a0 hypothesis\u00a0in\u00a0which\u00a0an\u00a0error\u00a0is\u00a0replaced\u00a0by\u00a0an\u00a0alternative\u00a0 word\u2010correction\u00a0called\u00a0hypothesis.\u00a0The\u00a0chief\u00a0drawback\u00a0of\u00a0 this\u00a0method\u00a0is\u00a0that\u00a0the\u00a0hypothesis\u00a0is\u00a0usually\u00a0derived\u00a0from\u00a0 a\u00a0 lexicon\u00a0 of\u00a0words;\u00a0 and\u00a0 hence,\u00a0 it\u00a0 is\u00a0 susceptible\u00a0 to\u00a0 high\u00a0 out\u2010of\u2010vocabulary\u00a0 rate.\u00a0 In\u00a0 that\u00a0 context,\u00a0 Setlur,\u00a0 Sukkar,\u00a0 andJacob[15]\u00a0 proposed\u00a0 an\u00a0 algorithm\u00a0 that\u00a0 treats\u00a0 each\u00a0 utterance\u00a0of\u00a0the\u00a0spoken\u00a0word\u00a0as\u00a0hypothesis\u00a0and\u00a0assigns\u00a0it\u00a0 a\u00a0confidence\u00a0score\u00a0during\u00a0the\u00a0recognition.\u00a0The\u00a0hypothesis\u00a0 which\u00a0bypasses\u00a0a\u00a0specific\u00a0threshold\u00a0is\u00a0to\u00a0be\u00a0selected\u00a0as\u00a0the\u00a0 correct\u00a0 output\u00a0word.\u00a0 The\u00a0 experiments\u00a0 showed\u00a0 that\u00a0 the\u00a0 error\u00a0 rate\u00a0 was\u00a0 reduced\u00a0 by\u00a0 a\u00a0 factor\u00a0 of\u00a0 0.13%.\u00a0 Likewise,\u00a0 Zhou,\u00a0Meng,\u00a0 and\u00a0Lo[16]\u00a0proposed\u00a0 another\u00a0 algorithm\u00a0 to\u00a0 detect\u00a0 and\u00a0 correct\u00a0misspellings\u00a0 in\u00a0ASR\u00a0 systems.\u00a0 In\u00a0 this\u00a0 approach,\u00a0 twenty\u00a0 alternative\u00a0 words\u00a0 are\u00a0 generated\u00a0 for\u00a0 every\u00a0 single\u00a0 word\u00a0 error\u00a0 and\u00a0 treated\u00a0 as\u00a0 utterance\u00a0 hypotheses.\u00a0Then,\u00a0a\u00a0linear\u00a0scoring\u00a0system\u00a0is\u00a0used\u00a0to\u00a0score\u00a0 every\u00a0 utterance\u00a0 with\u00a0 certain\u00a0 mutual\u00a0 information\u00a0 representing\u00a0 the\u00a0 frequency\u00a0or\u00a0 the\u00a0number\u00a0of\u00a0occurrence\u00a0 of\u00a0 this\u00a0 specific\u00a0 utterance\u00a0 in\u00a0 the\u00a0 input\u00a0 waveform.\u00a0 Next,\u00a0 utterances\u00a0 are\u00a0 ranked\u00a0 according\u00a0 to\u00a0 their\u00a0 scores.\u00a0 The\u00a0 utterance\u00a0 that\u00a0 received\u00a0 the\u00a0 highest\u00a0 score\u00a0 is\u00a0 chosen\u00a0 to\u00a0 substitute\u00a0 the\u00a0 detected\u00a0 error.\u00a0 Experiments\u00a0 conducted,\u00a0 indicated\u00a0a\u00a0decrease\u00a0in\u00a0the\u00a0error\u00a0rate\u00a0by\u00a0a\u00a0factor\u00a0of\u00a00.8%.\u00a0\nPattern\u00a0 learning\u00a0 error\u00a0 correction\u00a0 is\u00a0yet\u00a0 another\u00a0 type\u00a0 of\u00a0error\u00a0correction\u00a0 techniques\u00a0 in\u00a0which\u00a0error\u00a0detection\u00a0 is\u00a0 done\u00a0 through\u00a0 finding\u00a0 patterns\u00a0 that\u00a0 are\u00a0 considered\u00a0 erroneous.\u00a0The\u00a0system\u00a0is\u00a0first\u00a0trained\u00a0using\u00a0a\u00a0set\u00a0of\u00a0error\u00a0 words\u00a0belonging\u00a0 to\u00a0a\u00a0specific\u00a0domain.\u00a0Subsequently,\u00a0 the\u00a0 system\u00a0builds\u00a0up\u00a0detection\u00a0rules\u00a0that\u00a0can\u00a0pinpoint\u00a0errors\u00a0 once\u00a0they\u00a0occur.\u00a0At\u00a0recognition\u00a0time,\u00a0the\u00a0ASR\u00a0system\u00a0can\u00a0\ndetect\u00a0 linguistic\u00a0 errors\u00a0 by\u00a0 validating\u00a0 the\u00a0 output\u00a0 text\u00a0 against\u00a0 its\u00a0 predefined\u00a0 rules.\u00a0 The\u00a0 drawback\u00a0 of\u00a0 this\u00a0 approach\u00a0 is\u00a0 that\u00a0 it\u00a0 is\u00a0 domain\u00a0 specific;\u00a0 and\u00a0 thus,\u00a0 the\u00a0 number\u00a0of\u00a0words\u00a0that\u00a0can\u00a0be\u00a0recognized\u00a0by\u00a0the\u00a0system\u00a0is\u00a0 minimal.\u00a0 In\u00a0 this\u00a0 perspective,\u00a0 Mangu\u00a0 and\u00a0 Padmanabhan[17]\u00a0 proposed\u00a0 a\u00a0 transformation\u2010based\u00a0 learning\u00a0 algorithm\u00a0 for\u00a0 ASR\u00a0 error\u00a0 correction.\u00a0 The\u00a0 algorithm\u00a0 exploits\u00a0 confusion\u00a0 network\u00a0 to\u00a0 learn\u00a0 error\u00a0 patterns\u00a0while\u00a0 the\u00a0 system\u00a0 is\u00a0 offline.\u00a0At\u00a0 run\u2010time,\u00a0 these\u00a0 learned\u00a0 rules\u00a0 assist\u00a0 in\u00a0 selecting\u00a0an\u00a0alternative\u00a0 correction\u00a0 to\u00a0replace\u00a0the\u00a0detected\u00a0error.\u00a0Similarly,\u00a0Kaki,\u00a0Sumita,\u00a0and\u00a0 H.\u00a0Iida[18]\u00a0proposed\u00a0an\u00a0error\u00a0correction\u00a0algorithm\u00a0based\u00a0 on\u00a0 pattern\u00a0 learning\u00a0 to\u00a0 detect\u00a0 misspellings\u00a0 and\u00a0 on\u00a0 similarity\u00a0 string\u00a0 matching\u00a0 algorithm\u00a0 to\u00a0 correct\u00a0 misspellings.\u00a0 In\u00a0 this\u00a0 technique,\u00a0 the\u00a0 output\u00a0 recognized\u00a0 transcript\u00a0 is\u00a0 searched\u00a0 for\u00a0 potential\u00a0 misspelled\u00a0 words.\u00a0 Once\u00a0 an\u00a0 error\u00a0 pattern\u00a0 is\u00a0 detected,\u00a0 the\u00a0 similarity\u00a0 string\u00a0 algorithm\u00a0 is\u00a0applied\u00a0 to\u00a0suggest\u00a0a\u00a0correction\u00a0 for\u00a0 the\u00a0error\u00a0 word.\u00a0Experiments\u00a0were\u00a0 executed\u00a0on\u00a0 a\u00a0 Japanese\u00a0 speech\u00a0 and\u00a0 the\u00a0 results\u00a0 indicated\u00a0 an\u00a0 overall\u00a0 8.5%\u00a0 reduction\u00a0 in\u00a0 ASR\u00a0 errors.\u00a0 In\u00a0 a\u00a0 parallel\u00a0 effort,\u00a0 statistical\u2010based\u00a0 pattern\u00a0 learning\u00a0techniques\u00a0were\u00a0also\u00a0developed.Jung,\u00a0Jeong,\u00a0and\u00a0 Lee[19]\u00a0employed\u00a0the\u00a0noisy\u00a0channel\u00a0model\u00a0to\u00a0detect\u00a0error\u00a0 patterns\u00a0 in\u00a0the\u00a0output\u00a0text.\u00a0Unlike\u00a0other\u00a0pattern\u00a0 learning\u00a0 techniques\u00a0 which\u00a0 exploit\u00a0 word\u00a0 tokens,\u00a0 this\u00a0 approach\u00a0 applies\u00a0 pattern\u00a0 learning\u00a0 on\u00a0 smaller\u00a0 units,\u00a0 namely\u00a0 individual\u00a0 characters.\u00a0 The\u00a0 global\u00a0 outcome\u00a0 was\u00a0 a\u00a0 40%\u00a0 improvement\u00a0 in\u00a0 the\u00a0 error\u00a0 correction\u00a0 rate.\u00a0 Furthermore,\u00a0 Sarma\u00a0 and\u00a0Palmer[20]\u00a0proposed\u00a0 a\u00a0method\u00a0 for\u00a0detecting\u00a0 errors\u00a0based\u00a0on\u00a0 statistical\u00a0 co\u2010occurrence\u00a0of\u00a0words\u00a0 in\u00a0 the\u00a0 output\u00a0 transcript.\u00a0 The\u00a0 idea\u00a0 revolves\u00a0 around\u00a0 contextual\u00a0 information\u00a0which\u00a0states\u00a0that\u00a0a\u00a0word\u00a0usually\u00a0appears\u00a0in\u00a0a\u00a0 text\u00a0with\u00a0 some\u00a0highly\u00a0 co\u2010occurred\u00a0words.\u00a0As\u00a0a\u00a0 result,\u00a0 if\u00a0 an\u00a0 error\u00a0 occurs\u00a0 within\u00a0 a\u00a0 specific\u00a0 set\u00a0 of\u00a0 words,\u00a0 the\u00a0 correction\u00a0 can\u00a0 be\u00a0 statistically\u00a0 deduced\u00a0 from\u00a0 the\u00a0 co\u2010 occurred\u00a0words\u00a0that\u00a0often\u00a0appear\u00a0in\u00a0the\u00a0same\u00a0set.\u00a0\u00a0\nThe\u00a0final\u00a0type\u00a0of\u00a0error\u00a0correction\u00a0is\u00a0post\u2010editing.\u00a0In\u00a0this\u00a0 approach,\u00a0an\u00a0extra\u00a0 layer\u00a0 is\u00a0appended\u00a0 to\u00a0 the\u00a0ASR\u00a0system\u00a0 with\u00a0 the\u00a0 intention\u00a0 of\u00a0 detecting\u00a0 and\u00a0 correcting\u00a0 misspellings\u00a0 in\u00a0 the\u00a0 final\u00a0output\u00a0 text\u00a0after\u00a0 the\u00a0recognition\u00a0 of\u00a0 the\u00a0 speech\u00a0 is\u00a0 completed.\u00a0 The\u00a0 advantage\u00a0 of\u00a0 this\u00a0 technique\u00a0 is\u00a0 that\u00a0 it\u00a0 is\u00a0 loosely\u00a0 coupled\u00a0 with\u00a0 the\u00a0 inner\u00a0 signal\u00a0and\u00a0recognition\u00a0algorithms\u00a0of\u00a0the\u00a0ASR\u00a0system;\u00a0and\u00a0 thus,\u00a0it\u00a0is\u00a0easy\u00a0to\u00a0be\u00a0implemented\u00a0and\u00a0integrated\u00a0into\u00a0an\u00a0 existing\u00a0ASR\u00a0system\u00a0and\u00a0can\u00a0also\u00a0benefit\u00a0from\u00a0other\u00a0error\u00a0 correction\u00a0explorations\u00a0done\u00a0in\u00a0sister\u00a0fields\u00a0such\u00a0as\u00a0OCR,\u00a0 NLP,\u00a0 and\u00a0 machine\u00a0 translation.\u00a0 As\u00a0 an\u00a0 initial\u00a0 attempt,\u00a0 Ringger\u00a0 and\u00a0Allen[21]\u00a0proposed\u00a0 a\u00a0post\u2010processor\u00a0model\u00a0 for\u00a0 discovering\u00a0 statistical\u00a0 error\u00a0 patterns\u00a0 and\u00a0 correct\u00a0 errors.\u00a0 The\u00a0 post\u2010processor\u00a0 was\u00a0 trained\u00a0 on\u00a0 data\u00a0 from\u00a0 a\u00a0 specific\u00a0 domain\u00a0 to\u00a0 spell\u2010check\u00a0 articles\u00a0 belonging\u00a0 to\u00a0 the\u00a0 same\u00a0domain.\u00a0The\u00a0actual\u00a0design\u00a0is\u00a0composed\u00a0of\u00a0a\u00a0channel\u00a0 model\u00a0 to\u00a0 detect\u00a0 errors\u00a0 generated\u00a0 during\u00a0 the\u00a0 speech\u00a0 recognition\u00a0 phase,\u00a0 and\u00a0 a\u00a0 language\u00a0 model\u00a0 to\u00a0 provide\u00a0 spelling\u00a0 suggestions\u00a0 for\u00a0 those\u00a0 detected\u00a0 errors.\u00a0 As\u00a0 outcome,\u00a0 around\u00a0 20%\u00a0 improvement\u00a0 in\u00a0 the\u00a0 error\u00a0 correction\u00a0rate\u00a0was\u00a0achieved.\u00a0On\u00a0the\u00a0other\u00a0hand,\u00a0Ringger\u00a0 and\u00a0 Allen[22]\u00a0 proposed\u00a0 a\u00a0 post\u2010editing\u00a0 model\u00a0 named\u00a0 SPEECHPP\u00a0 to\u00a0 correct\u00a0 word\u00a0 errors\u00a0 generated\u00a0 by\u00a0 ASR\u00a0 systems.\u00a0 The\u00a0model\u00a0 uses\u00a0 a\u00a0 noisy\u00a0 channel\u00a0 to\u00a0 detect\u00a0 and\u00a0\ncorrect\u00a0errors;\u00a0it\u00a0also\u00a0uses\u00a0the\u00a0Viterbi\u00a0search\u00a0algorithm\u00a0to\u00a0 implement\u00a0 the\u00a0 language\u00a0 model.\u00a0 Moreover,\u00a0 the\u00a0 system\u00a0 leverages\u00a0 a\u00a0 fertility\u00a0model\u00a0 to\u00a0 deal\u00a0with\u00a0 split\u00a0 or\u00a0merged\u00a0 errors\u00a0 such\u00a0 as\u00a0 1\u2010to\u20102\u00a0 or\u00a0 2\u2010to\u20101\u00a0mapping\u00a0 errors.\u00a0Another\u00a0 attempt\u00a0was\u00a0presented\u00a0by\u00a0Brandow\u00a0and\u00a0Strzalkowski[23]\u00a0 in\u00a0whichthe\u00a0text\u00a0generated\u00a0by\u00a0the\u00a0ASR\u00a0system\u00a0is\u00a0collected\u00a0 and\u00a0 aligned\u00a0 with\u00a0 the\u00a0 correct\u00a0 transcription\u00a0 of\u00a0 the\u00a0 same\u00a0 text.\u00a0 In\u00a0 a\u00a0 training\u00a0 process,\u00a0 a\u00a0 set\u00a0 of\u00a0 correction\u00a0 rules\u00a0 are\u00a0 generated\u00a0 from\u00a0 these\u00a0 transcription\u00a0 texts\u00a0 and\u00a0 validated\u00a0 against\u00a0 a\u00a0 generic\u00a0 corpus.The\u00a0 Rules\u00a0 that\u00a0 are\u00a0 void\u00a0 or\u00a0 invalid\u00a0 are\u00a0 discarded.\u00a0 The\u00a0 system\u00a0 loops\u00a0 for\u00a0 several\u00a0 iterations\u00a0 until\u00a0 all\u00a0 rules\u00a0 get\u00a0 verified.\u00a0 Finally,\u00a0 a\u00a0 post\u2010 editing\u00a0 stage\u00a0 is\u00a0 employed\u00a0which\u00a0 exploits\u00a0 these\u00a0 rules\u00a0 to\u00a0 detect\u00a0 and\u00a0 correct\u00a0 misspelled\u00a0 words\u00a0 generated\u00a0 by\u00a0 theASRsystem.\u00a0\n6PROPOSED METHOD This\u00a0 paper\u00a0 proposes\u00a0 a\u00a0 novelpost\u2010editing\u00a0 ASRcontext\u2010 sensitive\u00a0error\u00a0correction\u00a0method\u00a0based\u00a0on\u00a0Microsoft\u00a0Web\u00a0 N\u2010Gram\u00a0 dataset\u00a0 [5]\u00a0 for\u00a0 detecting\u00a0 and\u00a0 correcting\u00a0 non\u2010 word\u00a0and\u00a0real\u2010word\u00a0errors\u00a0produced\u00a0byASRsystems.\u00a0The\u00a0 proposed\u00a0method\u00a0uses\u00a0a\u00a0post\u2010editing\u00a0approach\u00a0 in\u00a0 that\u00a0 it\u00a0 spell\u2010checks\u00a0the\u00a0output\u00a0transcript\u00a0of\u00a0the\u00a0ASR\u00a0system\u00a0after\u00a0 the\u00a0 input\u00a0 speech\u00a0has\u00a0been\u00a0converted\u00a0 into\u00a0 text.\u00a0Microsoft\u00a0 who\u00a0owns\u00a0Bing\u00a0search\u00a0engine\u00a0[6]\u00a0already\u00a0developed\u00a0and\u00a0 published\u00a0a\u00a0set\u00a0of\u00a0online\u00a0public\u00a0APIs\u00a0and\u00a0Web\u00a0Services\u00a0to\u00a0 give\u00a0access\u00a0to\u00a0their\u00a0indexed\u00a0web\u00a0data.\u00a0The\u00a0Microsoft\u00a0Web\u00a0 N\u2010Gram\u00a0dataset\u00a0 is\u00a0a\u00a0database\u00a0containingreal\u2010world\u00a0web\u2010 scale\u00a0 dynamic\u00a0 data\u00a0 stored\u00a0 as\u00a0 word\u00a0 n\u2010gram\u00a0 sequences\u00a0 with\u00a0 their\u00a0 corresponding\u00a0 counts,worthwhile\u00a0 in\u00a0 solving\u00a0 manycomputational\u00a0 linguisticsproblems.\u00a0 Since\u00a0Microsoft\u00a0 dataset\u00a0houses\u00a0a\u00a0huge\u00a0volume\u00a0of\u00a0data\u00a0crawled\u00a0from\u00a0real\u2010 world\u00a0web\u00a0pages\u00a0and\u00a0documents\u00a0postedon\u00a0the\u00a0Internet,\u00a0it\u00a0 is\u00a0 overflowedwith\u00a0 proper\u00a0 names,\u00a0 technical\u00a0 keywords,\u00a0 domain\u00a0specific\u00a0terms,\u00a0acronyms,\u00a0special\u00a0expressions,\u00a0and\u00a0 terminologies,that\u00a0 altogether\u00a0 can\u00a0mimic\u00a0 a\u00a0wide\u2010ranging\u00a0 dictionary\u00a0 thatcan\u00a0 covermost\u00a0 of\u00a0 the\u00a0 words\u00a0 in\u00a0 the\u00a0 language.\u00a0\nPredominantly,\u00a0 the\u00a0proposed\u00a0error\u00a0correction\u00a0method\u00a0 combines\u00a0 three\u00a0 algorithms:\u00a0 The\u00a0 error\u00a0 detection\u00a0 algorithm,the\u00a0 candidate\u00a0 corrections\u00a0generationalgorithm,\u00a0 and\u00a0 thecontext\u2010sensitive\u00a0 error\u00a0 correction\u00a0 algorithm.\u00a0 Figure\u00a04shows\u00a0 thelogical\u00a0block\u00a0diagramfor\u00a0 the\u00a0proposed\u00a0 ASR\u00a0error\u00a0correction\u00a0method.\u00a0"}, {"heading": "6.1 The Error Detection Algorithm", "text": "The\u00a0 error\u00a0 detectionalgorithmdetects\u00a0 non\u2010word\u00a0 spelling\u00a0 errors\u00a0 in\u00a0 the\u00a0ASR\u00a0output\u00a0 text.\u00a0Formally,\u00a0 these\u00a0errors\u00a0are\u00a0 denoted\u00a0 byE={e1,e2,e3,ep}\u00a0 where\u00a0 edenotes\u00a0 an\u00a0 non\u2010word\u00a0 error,\u00a0 and\u00a0 p\u00a0 denotes\u00a0 the\u00a0 total\u00a0 number\u00a0 of\u00a0 detected\u00a0 errors.The\u00a0 ASR\u00a0 output\u00a0 text\u00a0 is\u00a0 denoted\u00a0 byA={a1,a2,a3,at}\u00a0 wherea\u00a0 is\u00a0a\u00a0word\u00a0or\u00a0 term\u00a0 in\u00a0 the\u00a0ASRoutput\u00a0 text,\u00a0and\u00a0 tis\u00a0 the\u00a0 total\u00a0 number\u00a0 of\u00a0 words.\u00a0 The\u00a0 algorithm\u00a0 works\u00a0 as\u00a0 follows:\u00a0 it\u00a0 first\u00a0 starts\u00a0 by\u00a0 validating\u00a0 every\u00a0 word\u00a0 ai\u00a0 in\u00a0 Aagainst\u00a0Microsoft\u00a0Web\u00a0N\u2010Gram\u00a0dataset;\u00a0if\u00a0an\u00a0entry\u00a0for\u00a0ai\u00a0 is\u00a0 found\u00a0 in\u00a0 the\u00a0 dataset,\u00a0 then\u00a0 ai\u00a0 is\u00a0 assumedto\u00a0 bespelled\u00a0 correctly,\u00a0and\u00a0therefore\u00a0no\u00a0spelling\u00a0correction\u00a0is\u00a0required.\u00a0 In\u00a0contrast,\u00a0if\u00a0no\u00a0entry\u00a0exists\u00a0for\u00a0the\u00a0word\u00a0aiin\u00a0the\u00a0dataset,\u00a0 then\u00a0ai\u00a0is\u00a0assumed\u00a0to\u00a0be\u00a0misspelled,\u00a0and\u00a0thusit\u00a0requires\u00a0a\u00a0 spelling\u00a0correction.\u00a0In\u00a0due\u00a0course,all\u00a0the\u00a0detectedspelling\u00a0 errors\u00a0are\u00a0grouped\u00a0in\u00a0a\u00a0list,denoted\u00a0byE={e1,e2,e3,ep}\u00a0where\u00a0 p\u00a0 is\u00a0 the\u00a0 total\u00a0number\u00a0of\u00a0non\u2010word\u00a0errors\u00a0detected\u00a0 in\u00a0 the\u00a0 ASRoutput\u00a0 text.\u00a0The\u00a0 pseudo\u2010codefor\u00a0 the\u00a0 error\u00a0detection\u00a0 algorithm\u00a0is\u00a0given\u00a0below.\u00a0\n\u00a0 FunctionErrorDetection\u00a0(A)\u00a0 {\u00a0 //\u00a0split\u00a0the\u00a0ASR\u00a0text\u00a0on\u00a0space\u00a0and\u00a0return\u00a0word\u00a0tokens\u00a0 WSplit(A,\u201c\u201d)\u00a0\u00a0 \u00a0 for(i0\u00a0to\u00a0i\u00a0<\u00a0N)\u00a0\u00a0//\u00a0detect\u00a0all\u00a0word\u00a0tokens\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//\u00a0search\u00a0for\u00a0W[i]\u00a0in\u00a0Microsoft\u00a0N\u2010Gram\u00a0dataset\u00a0 RSearch(MicrosoftDataset\u00a0,\u00a0W[i])\u00a0\u00a0 \u00a0 if(R\u00a0==\u00a0true)\u00a0\u00a0//\u00a0mean\u00a0W[i]\u00a0was\u00a0found\u00a0in\u00a0Microsoft\u00a0\u00a0 dataset(i.e.\u00a0correctly\u00a0spelled)\u00a0\ni\u00a0i+1\u00a0\u00a0//\u00a0go\u00a0to\u00a0the\u00a0next\u00a0word\u00a0tokenW[i+1]\u00a0 else\u00a0\u00a0//\u00a0W[i]\u00a0is\u00a0misspelled\u00a0and\u00a0thus\u00a0a\u00a0correction\u00a0is\u00a0required\u00a0 //\u00a0go\u00a0to\u00a0the\u00a0candidate\u00a0corrections\u00a0generation\u00a0algorithm\u00a0 GenerateCandidates(W[i])\u00a0\u00a0\u00a0 }\u00a0 }\u00a0\n\u00a0 6.2 The Candidate Corrections Generation Algorithm The\u00a0candidate\u00a0corrections\u00a0generationalgorithm\u00a0generates\u00a0 a\u00a0 list\u00a0 of\u00a0 possible\u00a0 spelling\u00a0 correctionsfor\u00a0 the\u00a0 errors\u00a0 that\u00a0 were\u00a0 previously\u00a0 detected\u00a0 by\u00a0 the\u00a0 error\u00a0 detection\u00a0 algorithm.\u00a0 The\u00a0 list\u00a0 of\u00a0 candidates\u00a0 is\u00a0 denoted\u00a0 by\u00a0 C={c11,c12,c13,c1q,\u2026,cb1,cb2,cb3,cbq}\u00a0where\u00a0p\u00a0denotes\u00a0a\u00a0particular\u00a0 candidate\u00a0spelling,\u00a0b\u00a0denotes\u00a0the\u00a0total\u00a0number\u00a0of\u00a0detected\u00a0 non\u2010word\u00a0ASR\u00a0errors,\u00a0and\u00a0q\u00a0denotes\u00a0the\u00a0 total\u00a0number\u00a0of\u00a0 candidate\u00a0 corrections\u00a0 generated\u00a0 for\u00a0 a\u00a0 particular\u00a0 error.\u00a0 Computationally,\u00a0 the\u00a0 algorithm\u00a0 generates\u00a0 candidate\u00a0 corrections\u00a0 using\u00a0 a\u00a0 2\u2010gram\u00a0 character\u2010based\u00a0 model\u00a0 thatsearches\u00a0 for\u00a0 unigrams\u00a0 in\u00a0 Microsoft\u00a0 Web\u00a0 N\u2010Gram\u00a0 dataset\u00a0 having\u00a0 2\u2010gram\u00a0 character\u00a0 sequences\u00a0 in\u00a0 common\u00a0 with\u00a0theerror\u00a0word.\u00a0\u00a0\nFor\u00a0example,\u00a0considering\u00a0a\u00a0speech\u00a0that\u00a0was\u00a0converted\u00a0 by\u00a0 theASR\u00a0system\u00a0 into\u00a0\u201cwatch\u00a0episodes\u00a0of\u00a0your\u00a0 favorite\u00a0 shaws\u00a0 and\u00a0 more\u201d,\u00a0 in\u00a0 which\u00a0 the\u00a0 word\u00a0 \u201cshows\u201d\u00a0 was\u00a0 misrecognized\u00a0as\u00a0\u201cshaws\u201d.\u00a0Converting\u00a0the\u00a0word\u00a0\u201cshaws\u201d\u00a0 into\u00a0 2\u2010gram\u00a0 character\u00a0 sequences\u00a0would\u00a0 produce:\u00a0 \u201csh\u201d\u00a0 ,\u00a0 \u201cha\u201d\u00a0 ,\u00a0\u201caw\u201d\u00a0 ,\u00a0\u201cws\u201d.\u00a0The\u00a0task\u00a0of\u00a0the\u00a0candidate\u00a0corrections\u00a0 generation\u00a0algorithmis\u00a0 to\u00a0 find\u00a0a\u00a0series\u00a0of\u00a0unigrams\u00a0 from\u00a0 Microsoft\u00a0Web\u00a0N\u2010Gram\u00a0datasetthat\u00a0enclose\u00a0 these\u00a02\u2010gram\u00a0 sequences.\u00a0Table\u00a01\u00a0 shows\u00a0a\u00a0possible\u00a0 set\u00a0of\u00a0unigrams\u00a0 for\u00a0 the\u00a0error\u00a0word\u00a0\u201cshaws\u201d\u00a0retrieved\u00a0from\u00a0Microsoft\u00a0Web\u00a0N\u2010 Gram\u00a0dataset.\u00a0\u00a0 \u00a0\nNow\u00a0 the\u00a0 task\u00a0 is\u00a0 to\u00a0 find\u00a0 the\u00a0 unigrams\u00a0 having\u00a0 the\u00a0 highest\u00a0number\u00a0 of\u00a0 common\u00a0 2\u2010gram\u00a0 character\u00a0 sequences\u00a0 with\u00a0the\u00a0error\u00a0word\u00a0\u201cshaws\u201d.\u00a0As\u00a0there\u00a0might\u00a0be\u00a0hundreds\u00a0 of\u00a0 unigrams,\u00a0 the\u00a0 algorithm\u00a0 only\u00a0 selects\u00a0 the\u00a0 top\u00a0 8\u00a0 unigrams\u00a0 as\u00a0 candidate\u00a0 corrections.\u00a0 Table\u00a0 2\u00a0 outlines\u00a0 the\u00a0 list\u00a0of\u00a0 the\u00a0 top\u00a08\u00a0candidate\u00a0corrections\u00a0 for\u00a0 the\u00a0error\u00a0word\u00a0 \u201cshaws\u201d.\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0\n\u00a0 \u00a0\ngeneration\u00a0algorithm\u00a0is\u00a0given\u00a0below.\u00a0 \u00a0\nFunctionGenerateCandidates\u00a0(word)\u00a0 {\u00a0 //\u00a0create\u00a02\u2010gram\u00a0character\u00a0sequences\u00a0and\u00a0put\u00a0them\u00a0in\u00a0a\u00a0 aSplit2Grams(word)\u00a0\u00a0\u00a0 \u00a0 for(i0\u00a0to\u00a0i\u00a0<\u00a0N)\u00a0\u00a0//\u00a0for\u00a0all\u00a02\u2010gram\u00a0sequences\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\u00a0 //\u00a0look\u00a0for\u00a0unigrams\u00a0having\u00a0a[i]\u00a0as\u00a0substring,\u00a0(i.e.\u00a0 //unigrams\u00a0sharing\u00a02\u2010gram\u00a0sequence\u00a0with\u00a0the\u00a0error\u00a0word\u00a0\n\u00a0 L[i]\u00a0Substring(MicrosoftDataset,\u00a0a[i])\u00a0\u00a0 \u00a0 i\u00a0i+1\u00a0 }\u00a0 \u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//\u00a0select\u00a0the\u00a0top\u00a08\u00a0unigrams\u00a0sharing\u00a02\u2010gram\u00a0character\u00a0\u00a0 sequences\u00a0with\u00a0the\u00a0error\u00a0word\u00a0 candidatescommonUnigrams(L)\u00a0\u00a0\u00a0 \u00a0 //\u00a0go\u00a0to\u00a0the\u00a0error\u00a0correction\u00a0algorithm\u00a0 ErrorCorrection(candidates)\u00a0\u00a0\u00a0 }\u00a0 \u00a0 6.3 The Error Correction Algorithm The\u00a0 error\u00a0 correctionalgorithm\u00a0 selects\u00a0 the\u00a0 best\u00a0 candidate\u00a0 spelling\u00a0 as\u00a0 a\u00a0 correction\u00a0 for\u00a0 every\u00a0 detected\u00a0 error.\u00a0 The\u00a0 algorithm\u00a0 first\u00a0 starts\u00a0 by\u00a0 considering\u00a0 each\u00a0 generated\u00a0 candidate\u00a0correctioncir\u00a0with\u00a04\u00a0words\u00a0that\u00a0directly\u00a0precede\u00a0 the\u00a0 initial\u00a0error\u00a0 in\u00a0 the\u00a0ASR\u00a0output\u00a0 text.\u00a0The\u00a0 result\u00a0 is\u00a0a\u00a05\u00a0 words\u00a0 sentence\u00a0 denoted\u00a0 as\u00a0 Lr=\u201cai\u20104ai\u20103ai\u20102ai\u20101\u00a0 \u00a0 cir\u00a0 \u201d\u00a0 where\u00a0 Ldenotes\u00a0 a\u00a0 sentence\u00a0made\u00a0 out\u00a0 of\u00a0 5\u00a0words,\u00a0 a\u00a0 denotes\u00a0 a\u00a0 word\u00a0 preceding\u00a0 the\u00a0 original\u00a0 ASR\u00a0 error,\u00a0 c\u00a0 denotes\u00a0 a\u00a0 particular\u00a0 candidate\u00a0 correction\u00a0 for\u00a0 a\u00a0 particular\u00a0 error,\u00a0 i\u00a0 denotes\u00a0 the\u00a0 ith\u00a0word\u00a0 that\u00a0precedes\u00a0 the\u00a0originalASR\u00a0error,\u00a0 and\u00a0 r\u00a0denotes\u00a0 the\u00a0 rth\u00a0candidate\u00a0correction.\u00a0Subsequently,\u00a0 the\u00a0 algorithm\u00a0 searches\u00a0 for\u00a0 every\u00a0Lr\u00a0 inMicrosoft\u00a0Web\u00a0N\u2010 Gram\u00a0dataset.\u00a0The\u00a0candidate\u00a0cir\u00a0in\u00a0sentence\u00a0Lr\u00a0having\u00a0the\u00a0 highest\u00a0number\u00a0of\u00a0occurrence\u00a0 in\u00a0Microsoft\u00a0Web\u00a0N\u2010Gram\u00a0 dataset\u00a0 is\u00a0selected\u00a0 to\u00a0replace\u00a0 the\u00a0originally\u00a0detected\u00a0ASR\u00a0 error.\u00a0\nThe\u00a0 proposed\u00a0 algorithm\u00a0 is\u00a0 context\u2010sensitive\u00a0 as\u00a0 it\u00a0 depends\u00a0on\u00a0real\u2010world\u00a0data\u00a0statistics\u00a0from\u00a0Microsoft\u00a0Web\u00a0 N\u2010Gram\u00a0 dataset,\u00a0 largelydug\u00a0 up\u00a0 from\u00a0 the\u00a0 Internet.\u00a0\nAccordingly,\u00a0and\u00a0back\u00a0to\u00a0the\u00a0previous\u00a0example,\u00a0in\u00a0spiteof\u00a0 the\u00a0fact\u00a0that\u00a0the\u00a0candidate\u00a0\u201chaws\u201d\u00a0is\u00a0a\u00a0valid\u00a0correction\u00a0for\u00a0 the\u00a0 error\u00a0 word\u00a0 \u201cshaws\u201d,\u00a0 candidate\u00a0 \u201cshows\u201d\u00a0 will\u00a0 be\u00a0 favoredand\u00a0 selected\u00a0 as\u00a0 a\u00a0 spelling\u00a0 correction\u00a0 instead\u00a0 of\u00a0 \u201chaws\u201d\u00a0 since\u00a0 the\u00a0 sentence\u00a0 \u201cwatch\u00a0 episodes\u00a0 of\u00a0 your\u00a0 favorite\u00a0haws\u00a0 and\u00a0more\u201d\u00a0would\u00a0occur\u00a0 fewer\u00a0 times\u00a0over\u00a0 the\u00a0 Internet\u00a0 than\u00a0 the\u00a0 sentence\u00a0 \u201cwatch\u00a0 episodes\u00a0 of\u00a0 your\u00a0 favorite\u00a0 shows\u00a0and\u00a0more\u201d.\u00a0Table\u00a03\u00a0outlinesthe\u00a0variousLr\u00a0 5\u2010gram\u00a0 sentences\u00a0 from\u00a0Microsoft\u00a0Web\u00a0N\u2010Gram\u00a0 dataset,\u00a0 each\u00a0 enclosing\u00a0 a\u00a0 word\u00a0 from\u00a0 the\u00a0 list\u00a0 of\u00a0 candidate\u00a0 corrections.\u00a0 \u00a0\ngiven\u00a0below.\u00a0 \u00a0\nFunctionErrorCorrection\u00a0(candidates)\u00a0 {\u00a0 for(i0\u00a0to\u00a0i\u00a0<\u00a0N)\u00a0\u00a0//\u00a0process\u00a0all\u00a0candidate\u00a0corrections\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\u00a0 //\u00a0concatenate\u00a0together\u00a0the\u00a0ith\u00a0candidate\u00a0with\u00a0the\u00a0four\u00a0\u00a0 preceding\u00a0words\u00a0 //\u00a0A\u00a0is\u00a0a\u00a0global\u00a0array\u00a0containing\u00a0the\u00a0original\u00a0ASR\u00a0output\u00a0text\u00a0\nL\u00a0Concatenate(A[j\u20104]\u00a0,\u00a0A[j\u20103]\u00a0,\u00a0A[j\u20102]\u00a0,\u00a0A[j\u20101]\u00a0,\u00a0\u00a0 candidates[i]\u00a0)\u00a0\u00a0\n\u00a0 //\u00a0find\u00a0L\u00a0in\u00a0Microsoft\u00a0N\u2010gram\u00a0dataset\u00a0and\u00a0returns\u00a0its\u00a0frequency\u00a0 frequency[i]\u00a0\u00a0Search(MicrosoftDataset\u00a0,\u00a0L)\u00a0\u00a0 \u00a0 \u00a0i\u00a0\u00a0i+1\u00a0 }\u00a0 \u00a0 pMaxFrequency(frequency)\u00a0\u00a0\u00a0 //\u00a0return\u00a0the\u00a0index\u00a0p\u00a0of\u00a0the\u00a0candidate\u00a0whose\u00a0L\u00a0has\u00a0the\u00a0\u00a0 highest\u00a0frequency\u00a0 \u00a0 //\u00a0return\u00a0the\u00a0correction\u00a0for\u00a0the\u00a0ASR\u00a0error\u00a0 RETURN\u00a0candidates[p]\u00a0\u00a0\u00a0 }\u00a0\n7EXPERIMENTS & RESULTS For\u00a0 evaluation\u00a0 purposes,\u00a0 five\u00a0 different\u00a0 English\u00a0 articles\u00a0 each\u00a0 composed\u00a0 of\u00a0 around\u00a0 100\u00a0words\u00a0were\u00a0 read\u00a0 by\u00a0 five\u00a0 different\u00a0 speakers.\u00a0 Those\u00a0 articlesare\u00a0 from\u00a0 various\u00a0 domains\u00a0 including\u00a0 information\u00a0 technology,\u00a0 engineering,\u00a0 medicine,\u00a0 business,\u00a0 and\u00a0 sports.\u00a0 In\u00a0 sum,\u00a0 they\u00a0 consist\u00a0 of\u00a0 around500\u00a0 words\u00a0 comprisingregular\u00a0 dictionary\u00a0 words,\u00a0\nproper\u00a0 names,\u00a0 domain\u00a0 specific\u00a0 terms,\u00a0 special\u00a0 terminologies,\u00a0 and\u00a0 technical\u00a0 jargons.\u00a0 The\u00a0ASR\u00a0 software\u00a0 used\u00a0 to\u00a0 perform\u00a0 the\u00a0 speech\u00a0 recognition\u00a0 is\u00a0 based\u00a0 on\u00a0 Microsoft\u00a0 Speech\u00a0 Application\u00a0 Programming\u00a0 Interface\u00a0 (SAPI\u00a05.0)\u00a0engine\u00a0[24].\u00a0SAPI\u00a05.0\u00a0is\u00a0a\u00a0freely\u2010redistributable\u00a0 API\u00a0developed\u00a0by\u00a0Microsoft\u00a0and\u00a0released\u00a0in\u00a02000\u00a0to\u00a0allow\u00a0 the\u00a0use\u00a0of\u00a0speech\u00a0recognition\u00a0and\u00a0speech\u00a0synthesis\u00a0within\u00a0 Windows\u00a0 applications.\u00a0 Such\u00a0 applications\u00a0 include\u00a0 Microsoft\u00a0 Office,\u00a0 Microsoft\u00a0 Narrator,\u00a0 and\u00a0 Microsoft\u00a0 Speech\u00a0Server.\u00a0\nThe\u00a0outcome\u00a0of\u00a0performing\u00a0 speech\u00a0 recognition\u00a0using\u00a0 SAPIwasaround\u00a0106\u00a0total\u00a0errors\u00a0out\u00a0of\u00a0500total\u00a0words\u00a0for\u00a0 the\u00a0five\u00a0articles,\u00a0making\u00a0the\u00a0average\u00a0of\u00a0ASR\u00a0errors\u00a0around\u00a0 21\u00a0errors\u00a0per\u00a0100\u2010words\u00a0article.\u00a0As\u00a0a\u00a0result,\u00a0the\u00a0error\u00a0rate\u00a0 is\u00a021%\u00a0distributed\u00a0as\u00a014%non\u2010word\u00a0errors\u00a0and\u00a086%\u00a0real\u2010 word\u00a0 errors.Table\u00a0 4delineates\u00a0 the\u00a0 results\u00a0 obtained\u00a0 for\u00a0 SAPI\u00a0 including\u00a0 the\u00a0 total\u00a0number\u00a0 of\u00a0non\u2010word\u00a0 and\u00a0 real\u2010 word\u00a0errors.\u00a0 \u00a0\nPost\u2010editing\u00a0 the\u00a0 obtained\u00a0 results\u00a0 using\u00a0 the\u00a0 proposed\u00a0 method\u00a0 resulted\u00a0 in\u00a0 94\u00a0 total\u00a0 errors\u00a0 being\u00a0 corrected\u00a0 successfully,\u00a0among\u00a0which\u00a012\u00a0were\u00a0non\u2010word\u00a0errors\u00a0and\u00a0 82\u00a0were\u00a0real\u2010word\u00a0errors.\u00a0As\u00a0a\u00a0result,\u00a0around\u00a089%\u00a0of\u00a0 the\u00a0 total\u00a0errors\u00a0were\u00a0corrected;\u00a0around\u00a080%\u00a0of\u00a0total\u00a0non\u2010word\u00a0 errors\u00a0were\u00a0corrected;\u00a0and\u00a0around\u00a090%\u00a0of\u00a0total\u00a0real\u2010word\u00a0 errors\u00a0 were\u00a0 corrected\u00a0 successfully.\u00a0 Table\u00a0 5\u00a0 outlines\u00a0 the\u00a0 obtained\u00a0test\u00a0results\u00a0for\u00a0the\u00a0proposed\u00a0method.\u00a0 \u00a0\n\u00a0 In\u00a0retrospect,\u00a0the\u00a0proposed\u00a0method\u00a0clearlyoutmatched\u00a0 the\u00a0Microsoft\u00a0 SAPI\u00a0 engine\u00a0 as\u00a0 its\u00a0 error\u00a0 rate\u00a0was\u00a0 around\u00a0 21%\u00a0 for\u00a0 500\u2010words\u00a0 articles\u00a0 (106\u00a0 errors\u00a0 out\u00a0 of\u00a0 500\u00a0 total\u00a0 words);\u00a0while\u00a0the\u00a0error\u00a0rate\u00a0for\u00a0the\u00a0proposed\u00a0method\u00a0was\u00a0 around2.4%\u00a0 (12\u00a0 errors\u00a0 out\u00a0 of\u00a0 106\u00a0were\u00a0 left\u00a0uncorrected,\u00a0 making\u00a0 the\u00a0error\u00a0rate\u00a0500*2.4%=12\u00a0errors\u00a0out\u00a0of\u00a0500\u00a0 total\u00a0 words).\u00a0These\u00a0 exceptional\u00a0 results\u00a0 are\u00a0 chiefly\u00a0due\u00a0 to\u00a0 the\u00a0 bigamount\u00a0 of\u00a0 5\u2010gram\u00a0 data\u00a0 in\u00a0 Microsoft\u00a0 Web\u00a0 N\u2010Gram\u00a0\ndataset\u00a0which\u00a0were\u00a0 used\u00a0 by\u00a0 the\u00a0 proposed\u00a0method\u00a0 as\u00a0 a\u00a0 dictionary\u00a0 to\u00a0 perform\u00a0 spelling\u00a0 detection\u00a0 and\u00a0 correction.\u00a0 As\u00a0 the\u00a0 content\u00a0 ofMicrosoft\u00a0 Web\u00a0 N\u2010Gram\u00a0 dataset\u00a0 is\u00a0 minedfrom\u00a0 the\u00a0 World\u00a0 Wide\u00a0 Web,\u00a0 it\u00a0 is\u00a0 comprehensivelypacked\u00a0 with\u00a0 real\u2010world\u00a0 data\u00a0 encompassing\u00a0 regular\u00a0 dictionary\u00a0 words,\u00a0 in\u00a0 addition\u00a0 to\u00a0 proper\u00a0 names,\u00a0 domain\u00a0 specific\u00a0 terms,\u00a0 special\u00a0 terminologies,\u00a0 uncommonacronyms,\u00a0 and\u00a0 technical\u00a0 jargons\u00a0and\u00a0expressions\u00a0that\u00a0can\u00a0cover\u00a0millions\u00a0of\u00a0words\u00a0 along\u00a0with\u00a0their\u00a0possible\u00a0sequences\u00a0in\u00a0the\u00a0language.\u00a0\n8CONCLUSIONS AND FUTURE WORK This\u00a0 paper\u00a0 presented\u00a0 an\u00a0 original\u00a0 context\u2010sensitive\u00a0 error\u00a0 correction\u00a0 method\u00a0 for\u00a0 detecting\u00a0 and\u00a0 correcting\u00a0 speech\u00a0 recognition\u00a0 non\u2010word\u00a0 and\u00a0 real\u2010word\u00a0 errors.\u00a0 The\u00a0 proposed\u00a0 system\u00a0 is\u00a0 based\u00a0 on\u00a0 Microsoft\u00a0 Web\u00a0 N\u2010Gram\u00a0 dataset\u00a0 which\u00a0 incorporateslarge\u00a0 amount\u00a0 of\u00a0 real\u2010world\u00a0 word\u00a0 sequences\u00a0 and\u00a0 n\u2010gram\u00a0 statistics\u00a0 initially\u00a0 extracted\u00a0 from\u00a0 the\u00a0World\u00a0Wide\u00a0Web,\u00a0 and\u00a0necessary\u00a0 to\u00a0 effectively\u00a0 correct\u00a0 misspellings\u00a0 in\u00a0 any\u00a0 sort\u00a0 of\u00a0 text.\u00a0 Experiments\u00a0 conducted\u00a0 on\u00a0 several\u00a0 spoken\u00a0 articles\u00a0 showed\u00a0 a\u00a0 notable\u00a0 decrease\u00a0 in\u00a0 the\u00a0ASR\u00a0error\u00a0 rate.\u00a0Practically,\u00a0 the\u00a0error\u00a0 rate\u00a0 using\u00a0the\u00a0proposed\u00a0method\u00a0was\u00a0around\u00a02.4%,\u00a0generating\u00a0 only\u00a012\u00a0errors\u00a0out\u00a0of\u00a0500\u00a0 total\u00a0words;\u00a0whereas,\u00a0 the\u00a0error\u00a0 rate\u00a0 for\u00a0other\u00a0existing\u00a0methods\u00a0 such\u00a0as\u00a0 the\u00a0SAPI\u00a0engine\u00a0 was\u00a0 around\u00a0 21%,\u00a0 generating\u00a0 106\u00a0 errors\u00a0 out\u00a0 of\u00a0 500\u00a0 total\u00a0 words.IntegratingMicrosoft\u00a0Web\u00a0N\u2010Gram\u00a0dataset\u00a0into\u00a0the\u00a0 proposed\u00a0 algorithms\u00a0 increased\u00a0 drastically\u00a0 the\u00a0 rate\u00a0 of\u00a0 error\u00a0 correction\u00a0 as\u00a0 this\u00a0 datasetholdsan\u00a0 extensivenumber\u00a0 of\u00a0words\u00a0and\u00a0accurate\u00a0 statistics\u00a0about\u00a0word\u00a0associations\u00a0 that\u00a0 almost\u00a0 cover\u00a0 the\u00a0 entire\u00a0 vocabulary\u00a0 of\u00a0 the\u00a0 language\u00a0 including\u00a0 regular\u00a0words,proper\u00a0 names,\u00a0 domain\u00a0 specific\u00a0 terms,\u00a0 technical\u00a0 terminologies,\u00a0 scientific\u00a0 acronyms,\u00a0 special\u00a0 expressions,\u00a0 and\u00a0 a\u00a0 lot\u00a0 of\u00a0 word\u00a0 sequences\u00a0 that\u00a0 haveoriginallyoccurred\u00a0on\u00a0the\u00a0web.\u00a0\nAs\u00a0 for\u00a0 future\u00a0 work,\u00a0 the\u00a0 proposed\u00a0 methodis\u00a0 to\u00a0 be\u00a0 parallelized\u00a0 in\u00a0 an\u00a0 attempt\u00a0 to\u00a0 boostitsexecution\u00a0 time.\u00a0 Often,\u00a0 parallel\u00a0 algorithms,\u00a0 more\u00a0 specifically\u00a0 multithreaded\u00a0 algorithms\u00a0 can\u00a0 take\u00a0 the\u00a0 most\u00a0 out\u00a0 of\u00a0 multiprocessor\u00a0large\u2010scale\u00a0computing\u00a0machines\u00a0to\u00a0deliver\u00a0 very\u00a0 fast\u00a0 real\u2010time\u00a0 computations,\u00a0 necessary\u00a0 to\u00a0 perform\u00a0 extensive\u00a0error\u00a0correction\u00a0 for\u00a0 long\u00a0 textat\u00a0high\u00a0speed\u00a0and\u00a0 quality.\u00a0"}, {"heading": "ACKNOWLEDGMENT", "text": "This\u00a0research\u00a0was\u00a0funded\u00a0by\u00a0the\u00a0Lebanese\u00a0Association\u00a0for\u00a0 Computational\u00a0Sciences\u00a0(LACSC),\u00a0Beirut,\u00a0Lebanon\u00a0under\u00a0 the\u00a0 \u201cWeb\u2010Scale\u00a0 Speech\u00a0 Recognition\u00a0 Research\u00a0 Project\u00a0 \u2013\u00a0 WSSRRP2011\u201d.\u00a0\nREFERENCES [1] T.Dutoit,\u00a0An\u00a0Introduction\u00a0to\u00a0Text\u2010to\u2010Speech\u00a0Synthesis\u00a0(Text,\u00a0Speech\u00a0\nand\u00a0Language\u00a0Technology),\u00a0Springer,\u00a01sted,\u00a01997.\u00a0 [2] L.\u00a0 Deng\u00a0 ,\u00a0 X.\u00a0 Huang,\u00a0 \u201cChallenges\u00a0 in\u00a0 adopting\u00a0 speech\u00a0\nrecognition\u201d,\u00a0Communications\u00a0of\u00a0the\u00a0ACM,\u00a0vol.\u00a047,\u00a0no.\u00a01,\u00a0pp.\u00a060\u2013 75,\u00a02004.\u00a0\n[3] M.\u00a0Forsberg,\u201cWhy\u00a0 Speech\u00a0Recognition\u00a0 is\u00a0Difficult\u201d,\u00a0Chalmers\u00a0 University\u00a0of\u00a0Technology,\u00a02003.\u00a0\n[4] Alexander\u00a0 I.\u00a0Rudnicky,\u00a0Alexander\u00a0G.\u00a0Hauptmann,\u00a0Kaifu\u00a0Lee,\u00a0 \u201cSurvey\u00a0of\u00a0Current\u00a0Speech\u00a0Technology\u201d,\u00a0Communications\u00a0of\u00a0the\u00a0 ACM,vol.\u00a037,\u00a0no.\u00a03,\u00a0pp.\u00a052\u201357,\u00a01994.\u00a0\n[5] Microsoft\u00a0 Corporation,\u00a0 Microsoft\u00a0 Web\u00a0 N\u2010Gram\u00a0 Service,\u00a0 http://web\u2010ngram.research.microsoft.com/info/,\u00a02010.\u00a0\n[6] Microsoft\u00a0 Corporation,\u00a0 Bing\u00a0 Web\u00a0 Search\u00a0 Engine,\u00a0 http://www.bing.com/,\u00a02011.\u00a0\n[7] D.Jurafsky,\u00a0 J.Martin,\u00a0 Speech\u00a0 and\u00a0 Language\u00a0 Processing,\u00a0 2nded,\u00a0 Prentice\u00a0Hall,\u00a02008.\u00a0\n[8] Schuetze\u00a0 Manning,\u00a0 Foundations\u00a0 of\u00a0 Statistical\u00a0 Natural\u00a0 Language\u00a0 Processing,\u00a0The\u00a0MIT\u00a0Press,\u00a01999.\u00a0\n[9] Ian\u00a0 McLoughlin,\u00a0 Applied\u00a0 Speech\u00a0 and\u00a0 Audio\u00a0 Processing:\u00a0 With\u00a0 Matlab\u00a0Examples,\u00a0Cambridge\u00a0University\u00a0Press,\u00a02009.\u00a0\n[10] Lawrence\u00a0R.\u00a0Rabiner,\u00a0 \u201cA\u00a0 tutorial\u00a0 on\u00a0Hidden\u00a0Markov\u00a0Models\u00a0 and\u00a0selected\u00a0applications\u00a0 in\u00a0speech\u00a0recognition\u201d,\u00a0Proceedings\u00a0of\u00a0 the\u00a0IEEE,\u00a0vol.\u00a077,no.\u00a02,\u00a0pp.\u00a0257\u2013286,\u00a01989.\u00a0\n[11] X.\u00a0 Huang,\u00a0 M.\u00a0 Jack,\u00a0 and\u00a0 Y.\u00a0 Ariki,\u00a0 Hidden\u00a0 Markov\u00a0 Models\u00a0 for\u00a0 Speech\u00a0Recognition,\u00a0Edinburgh\u00a0University\u00a0Press,\u00a01990.\u00a0\n[12] C.Shannon,\u00a0\u201cA\u00a0Mathematical\u00a0Theory\u00a0of\u00a0Communication\u201d,\u00a0Bell\u00a0 System\u00a0Technical\u00a0Journal,\u00a0vol.\u00a027,\u00a0no.3,\u00a0pp.\u00a0379\u2010423,\u00a01948.\u00a0\n[13] A.A.\u00a0 Markov,\u00a0 \u201cEssaid\u2019unerecherchestatistiquesur\u00a0 le\u00a0 texte\u00a0 du\u00a0 roman\u00a0\u201cEug\u00e8neOneguine\u201d\u201d,\u00a0Bull.\u00a0Acad.\u00a0Imper.\u00a0Sci.\u00a0St.\u00a0Petersburg,\u00a0 vol.\u00a07,\u00a01913.\u00a0\n[14] J.R.\u00a0 Lewis,\u00a0 \u201cEffect\u00a0 of\u00a0 Error\u00a0 Correction\u00a0 Strategy\u00a0 on\u00a0 Speech\u00a0 Dictation\u00a0 Throughput\u201d,\u00a0 Proceedings\u00a0 of\u00a0 the\u00a0 Human\u00a0 Factors\u00a0 and\u00a0 Ergonomics\u00a0Society,\u00a0pp.\u00a0457\u2010461,\u00a01999.\u00a0\n[15] A.\u00a0R.\u00a0Setlur,\u00a0R.\u00a0A.\u00a0Sukkar,\u00a0and\u00a0J.\u00a0Jacob,\u00a0\u201cCorrecting\u00a0recognition\u00a0 errors\u00a0via\u00a0discriminative\u00a0utterance\u00a0verification\u201d,\u00a0 In\u00a0Proceedings\u00a0 of\u00a0the\u00a0International\u00a0Conference\u00a0on\u00a0Spoken\u00a0Language\u00a0Processing,\u00a0pp.\u00a0 602\u2013605,\u00a0Philadelphia,\u00a0PA,\u00a01996.\u00a0\n[16] Z.\u00a0 Zhou,\u00a0 H.\u00a0 M.\u00a0 Meng,\u00a0 and\u00a0 W.\u00a0 K.\u00a0 Lo,\u00a0 \u201cA\u00a0 multi\u2010pass\u00a0 error\u00a0 detection\u00a0and\u00a0correction\u00a0 framework\u00a0 for\u00a0Mandarin\u00a0LVCSR\u201d,\u00a0 In\u00a0 Proceedings\u00a0 of\u00a0 the\u00a0 International\u00a0 Conference\u00a0 on\u00a0 Spoken\u00a0 Language\u00a0 Processing,\u00a0pp.\u00a01646\u20131649,\u00a0Pittsburgh,\u00a0PA,\u00a02006.\u00a0\n[17] L.\u00a0Mangu\u00a0and\u00a0M.\u00a0Padmanabhan,\u00a0\u201cError\u00a0corrective\u00a0mechanisms\u00a0 for\u00a0 speech\u00a0 recognition\u201d,\u00a0 In\u00a0Proceedings\u00a0 of\u00a0 the\u00a0 IEEE\u00a0 International\u00a0 Conference\u00a0on\u00a0Acoustics,\u00a0Speech,\u00a0and\u00a0Signal\u00a0Processing,\u00a0vol\u00a01,\u00a0pp.\u00a0 29\u201332,\u00a0Salt\u00a0Lake\u00a0City,\u00a0UT,\u00a02001.\u00a0\n[18] S.\u00a0Kaki,\u00a0E.\u00a0Sumita,\u00a0and\u00a0H.\u00a0Iida,\u00a0\u201cA\u00a0method\u00a0for\u00a0correcting\u00a0errors\u00a0 in\u00a0speech\u00a0recognition\u00a0using\u00a0 the\u00a0statistical\u00a0 features\u00a0of\u00a0character\u00a0 co\u2010occurrence\u201d,\u00a0 In\u00a0 COLING\u2010ACL,\u00a0 pp.\u00a0 653\u2013657,\u00a0 Montreal,\u00a0 Quebec,\u00a0Canada,\u00a01998.\u00a0\n[19] S.\u00a0 Jung,\u00a0 M.\u00a0 Jeong,\u00a0 and\u00a0 G.\u00a0 G.\u00a0 Lee,\u00a0 \u201cSpeech\u00a0 recognition\u00a0 error\u00a0 correction\u00a0 using\u00a0 maximum\u00a0 entropy\u00a0 language\u00a0 model\u201d,\u00a0 In\u00a0 Proceedings\u00a0 of\u00a0 the\u00a0 International\u00a0 Conference\u00a0 on\u00a0 Spoken\u00a0 Language\u00a0 Processing,\u00a0pp.\u00a02137\u20132140,\u00a0Jeju\u00a0Island,\u00a0Korea,\u00a02004.\u00a0\n[20] A.\u00a0Sarma\u00a0and\u00a0D.\u00a0D.\u00a0Palmer,\u00a0\u201cContext\u2010based\u00a0speech\u00a0recognition\u00a0 error\u00a0 detection\u00a0 and\u00a0 correction\u201d,\u00a0 In\u00a0 Proceedings\u00a0 of\u00a0 the\u00a0 Human\u00a0 Language\u00a0Technology\u00a0Conference\u00a0of\u00a0 the\u00a0North\u00a0American\u00a0Chapter\u00a0of\u00a0 the\u00a0Association\u00a0 for\u00a0Computational\u00a0 Linguistics,\u00a0pp.\u00a0 85\u201388,\u00a0Boston,\u00a0 MA,\u00a02004.\u00a0\n[21] E.\u00a0 K.\u00a0 Ringger\u00a0 and\u00a0 J.\u00a0 F.\u00a0 Allen,\u00a0 \u201cError\u00a0 correction\u00a0 via\u00a0 a\u00a0 post\u2010 processor\u00a0 for\u00a0continuous\u00a0speech\u00a0recognition\u201d,\u00a0 In\u00a0Proceedings\u00a0of\u00a0 the\u00a0 IEEE\u00a0 International\u00a0Conference\u00a0on\u00a0Acoustics,\u00a0Speech,\u00a0and\u00a0Signal\u00a0 Processing,\u00a0vol.\u00a01,\u00a0pp.\u00a0427\u2013430,\u00a0Atlanta,\u00a0GA,\u00a01996.\u00a0\n[22] E.\u00a0K.\u00a0Ringger,\u00a0J.\u00a0F.\u00a0Allen,\u00a0\u201cA\u00a0Fertility\u00a0Channel\u00a0Model\u00a0for\u00a0Post\u2010 Correction\u00a0of\u00a0Continuous\u00a0Speech\u00a0Recognition\u201d,\u00a0In\u00a0Proceedings\u00a0of\u00a0 the\u00a0Fourth\u00a0International\u00a0Conference\u00a0on\u00a0Spoken\u00a0Language\u00a0Processing,\u00a0 vol.2,\u00a0pp.\u00a0897\u2010900,\u00a01996.\u00a0\n[23] R.\u00a0L.Brandow,\u00a0T.\u00a0Strzalkowski,\u00a0\u201cImproving\u00a0speech\u00a0recognition\u00a0 through\u00a0 text\u2010based\u00a0 linguistic\u00a0 post\u2010processing\u201d,\u00a0 United\u00a0 States,\u00a0 Patent\u00a06064957,\u00a02000.\u00a0\n[24] Microsoft\u00a0Corporation,\u00a0Microsoft\u00a0Speech\u00a0Application\u00a0 Programming\u00a0Interface\u00a0(SAPI\u00a05.0)\u00a0engine,\u00a0 http://www.microsoft.com/download/en/details.aspx?displayla ng=en&id=10121,\u00a02011.\u00a0\n\u00a0"}], "references": [{"title": "Challenges in adopting speech recognition", "author": ["L. Deng", "X. Huang"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Speech Recognition is Difficult", "author": ["M. Forsberg", "\u201cWhy"], "venue": "Chalmers University of Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Survey of Current Speech Technology", "author": ["Alexander I. Rudnicky", "Alexander G. Hauptmann", "Kaifu Lee"], "venue": "Communications of the ACM,vol", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Schuetze Manning"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Applied Speech and Audio Processing: With Matlab Examples", "author": ["Ian McLoughlin"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A tutorial on Hidden Markov Models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77,no", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Hidden Markov Models for Speech Recognition, Edinburgh", "author": ["X. Huang", "M. Jack", "Y. Ariki"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Essaid\u2019unerecherchestatistiquesur le texte du roman \u201cEug\u00e8neOneguine\u201d", "author": ["A.A. Markov"], "venue": "Bull. Acad. Imper. Sci. St. Petersburg,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1913}, {"title": "Effect of Error Correction Strategy on Speech Dictation Throughput", "author": ["J.R. Lewis"], "venue": "Proceedings of the Human Factors and Ergonomics Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Correcting recognition errors via discriminative utterance verification", "author": ["A.R. Setlur", "R.A. Sukkar", "J. Jacob"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "A multi\u2010pass error detection and correction framework for Mandarin LVCSR", "author": ["Z. Zhou", "H.M. Meng", "W.K. Lo"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Error corrective mechanisms for speech recognition", "author": ["L. Mangu", "M. Padmanabhan"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "A method for correcting errors in speech recognition using the statistical features of character co\u2010occurrence", "author": ["S. Kaki", "E. Sumita", "H. Iida"], "venue": "In COLING\u2010ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Speech recognition error correction using maximum entropy language model", "author": ["S. Jung", "M. Jeong", "G.G. Lee"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Context\u2010based speech recognition error detection and correction", "author": ["A. Sarma", "D.D. Palmer"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Error correction via a post\u2010 processor for continuous speech recognition", "author": ["E.K. Ringger", "J.F. Allen"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "A Fertility Channel Model for Post\u2010 Correction of Continuous Speech Recognition", "author": ["E.K. Ringger", "J.F. Allen"], "venue": "In Proceedings of the Fourth International Conference on Spoken Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Improving speech recognition through text\u2010based linguistic post\u2010processing", "author": ["R.L.Brandow", "T. Strzalkowski"], "venue": "United States, Patent 6064957,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "These errors are often caused by the extreme noise in theenvironment, the bad quality of the speech, the fluctuating utterance of the dialogue, and the small size of the ASR vocabulary [2], [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "These errors are often caused by the extreme noise in theenvironment, the bad quality of the speech, the fluctuating utterance of the dialogue, and the small size of the ASR vocabulary [2], [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "Numerous error-correction methods and algorithms were devised to help fight against ASR errors, some of themrely on post-processing the output text and correcting it manually;whereas others rely on building improved acoustic models to increase the precision of speech recognition [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 3, "context": "As defined by many textbooks [7], [8], and [9], automatic speech recognition systems also known as ASR, receive some speech signals as input and generate a corresponding readable text transcript as output.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "As defined by many textbooks [7], [8], and [9], automatic speech recognition systems also known as ASR, receive some speech signals as input and generate a corresponding readable text transcript as output.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Essentially, an ASR system is often implemented using a Hidden Markov Model  (HMM)  [10],  [11] based on  the notion  of  noisy  channel  [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "Essentially, an ASR system is often implemented using a Hidden Markov Model  (HMM)  [10],  [11] based on  the notion  of  noisy  channel  [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "P(W)  is  usually  calculated  using  the probabilistic  n\u2010gram model  [13] which predicts  the next word, letter, or phone in a given sequence.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "An early experiment conducted at IBM research labs [14] to  calculate  the  number  of  errors  generated  by  ASR systems,  showed  an  average  of  105  errors  being committed  per  minute.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "In  that  context,  Setlur,  Sukkar, andJacob[15]  proposed  an  algorithm  that  treats  each utterance of the spoken word as hypothesis and assigns it a confidence score during the recognition.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "Likewise, Zhou, Meng,  and Lo[16] proposed  another  algorithm  to detect  and  correct misspellings  in ASR  systems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "In  this  perspective,  Mangu  and Padmanabhan[17]  proposed  a  transformation\u2010based learning  algorithm  for  ASR  error  correction.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Iida[18] proposed an error correction algorithm based on  pattern  learning  to  detect  misspellings  and  on similarity  string  matching  algorithm  to  correct misspellings.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "Jung, Jeong, and Lee[19] employed the noisy channel model to detect error patterns  in the output text.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "Furthermore, Sarma  and Palmer[20] proposed  a method  for detecting errors based on  statistical  co\u2010occurrence of words  in  the output  transcript.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "As  an  initial  attempt, Ringger  and Allen[21] proposed  a post\u2010processor model for  discovering  statistical  error  patterns  and  correct errors.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "On the other hand, Ringger and  Allen[22]  proposed  a  post\u2010editing  model  named SPEECHPP  to  correct  word  errors  generated  by  ASR systems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Another attempt was presented by Brandow and Strzalkowski[23] in whichthe text generated by the ASR system is collected and  aligned  with  the  correct  transcription  of  the  same text.", "startOffset": 57, "endOffset": 61}], "year": 2012, "abstractText": "At the present time, computers are employed to solve complex tasks and problems ranging from simple calculations to intensive digital image processing and intricate algorithmic optimization problems to computationally-demanding weather forecasting problems. ASR short for Automatic Speech Recognition is yet another type of computational problem whose purpose is to recognize human spoken speech and convert it into text that can be processed by a computer. Despite that ASR has many versatile and pervasive real-world applications,it is still relatively erroneous and not perfectly solved as it is prone to produce spelling errors in the recognized text, especially if the ASR system is operating in a noisy environment, its vocabulary size is limited, and its input speech is of bad or low quality. This paper proposes a post-editing ASR error correction method based on MicrosoftN-Gram dataset for detecting and correcting spelling errors generated by ASR systems. The proposed method comprises an error detection algorithm for detecting word errors; a candidate corrections generation algorithm for generating correction suggestions for the detected word errors; and a context-sensitive error correction algorithm for selecting the best candidate for correction. The virtue of using the Microsoft N-Gram dataset is that it contains real-world data and word sequences extracted from the web which canmimica comprehensive dictionary of words having a large and all-inclusive vocabulary. Experiments conducted on numerous speeches, performed by different speakers, showed a remarkable reduction in ASR errors. Future research can improve upon the proposed algorithm so much so that it can be parallelized to take advantage of multiprocessor and distributed systems.", "creator": "PScript5.dll Version 5.2.2"}}}