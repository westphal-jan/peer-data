{"id": "1204.0136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2012", "title": "Near-Optimal Algorithms for Online Matrix Prediction", "abstract": "In several online prediction problems of current interest, the comparison class consists of matrices with limited entries. Another important example is online cooperative filtering, where a widely used comparison class is the set of matrices with a small trace standard. In this work, we isolate a property of matrices that we call (beta, tau) decomposition, and derive from it an efficient online learning algorithm that has a regret limit of O * (sqrt (beta tau T) for all problems where the comparison class consists of (beta, tau) decomposition matrices. By analyzing the decomposition of sectional matrices, triangular matrices, and low trace standard matrices, we show near optimal limits for this problem.", "histories": [["v1", "Sat, 31 Mar 2012 21:15:28 GMT  (23kb)", "http://arxiv.org/abs/1204.0136v1", "25 pages"]], "COMMENTS": "25 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["elad hazan", "satyen kale", "shai shalev-shwartz"], "accepted": false, "id": "1204.0136"}, "pdf": {"name": "1204.0136.pdf", "metadata": {"source": "CRF", "title": "Near-Optimal Algorithms for Online Matrix Prediction", "authors": ["Elad Hazan", "Satyen Kale", "Shai Shalev-Shwartz"], "emails": ["ehazan@ie.technion.ac.il.", "sckale@us.ibm.com."], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n01 36\nv1 [\ncs .L\nG ]\n3 1\nM ar\n2 01\n2\n\u221a \u03b2 \u03c4 T ) for all problems in which the comparison class is composed of (\u03b2, \u03c4)-decomposable matrices. By analyzing the decomposability of cut matrices, triangular matrices, and low tracenorm matrices, we derive near optimal regret bounds for online max-cut, online gambling, and online collaborative filtering. In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011]."}, {"heading": "1 Introduction", "text": "We consider online learning problems in which on each round the learner receives (it, jt) \u2208 [m]\u00d7 [n] and should return a prediction in [\u22121, 1]. For example, in the online collaborative filtering problem, m is the number of users, n is the number of items (e.g., movies), and on each online round the learner should predict a number in [\u22121, 1] indicating how much user it \u2208 [m] likes item jt \u2208 [n]. Once the learner makes the prediction, the environment responds with a loss function, \u2113t : [\u22121, 1] \u2192 R, that assesses the correctness of the learner\u2019s prediction.\nA natural approach for the learner is to maintain a matrix Wt \u2208 [\u22121, 1]m\u00d7n, and to predict the corresponding entry, Wt(it, jt). The matrix is updated based on the loss function and the process continues.\nWithout further structure, the above setting is equivalent to mn independent prediction problems - one per user-item pair. However, it is usually assumed that there is a relationship between the different matrix entries - e.g. similar users prefer similar movies. This can be modeled in the online learning setting by assuming that there is some fixed matrix W, in a restricted class of matrices W \u2286 [\u22121, 1]m\u00d7n, such that the strategy which always predicts W (it, jt) has a small cumulative loss. A common choice for W in the collaborative filtering application is to be the set\n\u2217Technion - Israel Institute of Technology. ehazan@ie.technion.ac.il. \u2020IBM T. J. Watson Research Center. sckale@us.ibm.com. \u2021Hebrew University. shais@cs.huji.ac.il.\nof matrices with a trace norm of at most \u03c4 (which intuitively requires the prediction matrix to be of low rank). As usual, rather than assuming that some W \u2208 W has a small cumulative loss, we require that the regret of the online learner with respect to W will be small. Formally, after T rounds, the regret of the learner is\nRegret :=\nT\u2211\nt=1\n\u2113t(Wt(it, jt))\u2212 min W\u2208W\nT\u2211\nt=1\n\u2113t(W (it, jt)),\nand we would like the regret to be as small as possible. A natural question is what properties of W enables us to derive an efficient online learning algorithm that enjoys low regret, and how does the regret depend on the properties of W. In this paper we define a property of matrices, called (\u03b2, \u03c4)-decomposability, and derive an efficient online learning algorithm that enjoys a regret bound of O\u0303( \u221a \u03b2 \u03c4 T ) for any problem in which W \u2282 [\u22121, 1]m\u00d7n and every matrix W \u2208 W is (\u03b2, \u03c4)-decomposable. Roughly speaking, W is (\u03b2, \u03c4)decomposable if a symmetrization of it can be written as P\u2212N where both P and N are positive semidefinite, have sum of traces bounded by \u03c4 , and have diagonal elements bounded by \u03b2.\nWe apply this technique to three online learning problems.\n1. Online max-cut: On each round, the learner receives a pair of graph nodes (i, j) \u2208 [n]\u00d7 [n], and should decide whether there is an edge connecting i and j. Then, it receives a binary feedback. The comparison class is the set of all cuts of the graph, which can be encoded as the set of matrices {WA : A \u2282 [n]}, where WA(i, j) indicates if (i, j) crosses the cut defined by A or not. It is possible to achieve a regret of O( \u221a nT ) for this problem by a non-efficient\nalgorithm (simply refer to each A as an expert and apply a prediction with expert advice algorithm). Our algorithm yields a nearly optimal regret bound of O( \u221a\nn log(n)T ) for this problem. This is the first efficient algorithm that achieves near optimal regret.\n2. Online Gambling: On each round, the learner receives a pair of teams (i, j) \u2208 [n] \u00d7 [n], and should predict whether i is going to beat j in an upcoming matchup or vice versa. The comparison class is the set of permutations over the teams, where a permutation will predict that i is going to beat j if i appears before j in the permutation. Permutations can be encoded naturally as matrices, where W (i, j) is either 1 (if i appears before j in the permutation) or 0. Again, it is possible to achieve a regret of O( \u221a\nn log(n)T ) by a non-efficient algorithm (that simply treats each permutation as an expert). Our algorithm yields a nearly optimal regret bound of O( \u221a n log3(n)T ). This resolves an open problem posed in Abernethy [2010],\nKleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem. In fact, Kanade and Steinke [2012] tried to show computational hardness for this problem by reducing the problem of online agnostic learning of halfspaces in a restricted setting to it. This paper shows that the problem is in fact tractable.\n3. Online Collaborative Filtering: We already mentioned this problem previously. We consider the comparison class W = {W \u2208 [\u22121, 1]m\u00d7n : \u2016W\u2016\u22c6 \u2264 \u03c4}, where \u2016 \u00b7 \u2016\u22c6 is the trace norm. Without loss of generality assume m \u2264 n. Our algorithm yields a nearly optimal regret bound of O( \u221a \u03c4 \u221a n log(n)T ). Since for this problem one typically has \u03c4 = \u0398(n), we\ncan rewrite the regret bound as O( \u221a n3/2 log(n)T ). In contrast, a direct application of the\nonline mirror descent framework to this problem yields a regret of O( \u221a \u03c42T ) = O( \u221a n2T ). The latter is a trivial bound since the bound becomes meaningful only after T \u2265 n2 rounds (which means that we saw the entire matrix).\nRecently, Cesa-Bianchi and Shamir [2011] proposed a rather different algorithm with regret bounded by O(\u03c4 \u221a n) but under the additional assumption that each entry (i, j) is seen only once. In addition, while both the runtime of our method and the Cesa-Bianchi and Shamir [2011] method is polynomial, the runtime of our method is significantly smaller: for m \u2248 n, each iteration of our method can be implemented in O\u0303(n3) time (see Section 6), whereas the runtime of each iteration in their algorithm is at least \u2126(n4) and can be significantly larger depending on the specific implementation.1\nFinally, we derive (nearly) matching lower bounds for the three problems. In particular, our lower bound for the online collaborative filtering problem implies that the sample complexity of learning matrices with bounded entries and trace norm of \u0398(n) is \u2126(n3/2). This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011]."}, {"heading": "2 Problem statements and main results", "text": "We start with the definition of (\u03b2, \u03c4)-decomposability. For this, we first define a symmetrization operator.\nDefinition 1 (Symmetrization). Given an m\u00d7 n non-symmetric matrix W its symmetrization is the (m+ n)\u00d7 (m+ n) matrix:\nsym(W) :=\n[ 0 W\nW\u22a4 0\n]\n.\nIf m = n and W is symmetric, then sym(W) := W.\nThe main property of matrices we rely on is (\u03b2, \u03c4)-decomposability, which we define below.\nDefinition 2 ((\u03b2, \u03c4)-decomposability). An m \u00d7 n matrix W is (\u03b2, \u03c4)-decomposable if there exist symmetric, positive semidefinite matrices P,N \u2208 Rp\u00d7p, where p is the order of sym(W), such that the following conditions hold:\nsym(W) = P\u2212N, Tr(P) + Tr(N) \u2264 \u03c4,\n\u2200i \u2208 [p] : P (i, i), N(i, i) \u2264 \u03b2.\nWe say that a set of matrices W is (\u03b2, \u03c4)-decomposable if every matrix in W is (\u03b2, \u03c4)-decomposable. 1Specifically, each iteration in their algorithm requires solving n empirical risk minimization problems over the hypothesis space of m\u00d7 n matrices with a bounded trace norm (in their notation, to obtain the optimal bound, one should set T = n2 and \u03b7 \u2265 1/n, and then should solve \u03b7T empirical risk minimization problems per iteration). It is not clear what is the optimal runtime of solving each such empirical risk minimization problem. We believe that it is impossible to obtain a solver which is significantly faster than n4.\nIn the above, the parameter \u03b2 stands for a bound on the diagonal elements of P and N, while the parameter \u03c4 stands for the trace of P andN. It is easy to verify that ifW is (\u03b2, \u03c4)-decomposable then so is its convex hull, conv(W). Throughout this paper, we assume for technical convenience that \u03b2 \u2265 1.2\nThere is an intriguing connection between the (\u03b2, \u03c4)-decomposition for a rectangular matrix W and its max-norm and trace norm: the least possible \u03b2 in any (\u03b2, \u03c4)-decomposition exactly equals half the max-norm of W (see Theorem 21), and the least possible \u03c4 in any (\u03b2, \u03c4)-decomposition exactly equals twice the trace-norm of W (see Theorem 23).\nOur first contribution is a generic low regret algorithm for online matrix prediction with a (\u03b2, \u03c4)-decomposable comparison class. We also assume that all the matrices in the comparison class have bounded entries. Formally, we consider the following problem.\nOnline Matrix Prediction\nparameters: \u03b2 \u2265 1, \u03c4 \u2265 0, G \u2265 0 input: A set of matrices, W \u2282 [\u22121, 1]m\u00d7n, which is (\u03b2, \u03c4)-decomposable for t = 1, 2, . . . , T adversary supplies a pair of indices (it, jt) \u2208 [m]\u00d7 [n] learner picks Wt \u2208 conv(W) and outputs the prediction Wt(it, jt) adversary supplies a convex, G-Lipschitz, loss function \u2113t : [\u22121, 1] \u2192 R learner pays \u2113t(Wt(it, jt))\nTheorem 1. There exists an efficient algorithm for Online Matrix Prediction which enjoys the regret bound\nRegret \u2264 2G \u221a \u03c4\u03b2 log(2p)T ,\nwhere p is the order of sym(W) for any matrix W \u2208 W.\nThe Online Matrix Prediction problem captures several specific problems considered in the literature, given in the next few subsections."}, {"heading": "2.1 Online Max-Cut", "text": "Recall that on each round of online max-cut, the learner should decide whether two vertices of a graph, (it, jt) are joined by an edge or not. The learner outputs a number y\u0302t \u2208 [\u22121, 1] which is to be interpreted as a randomized prediction in {\u22121, 1}: predict 1 with probability 1+y\u0302t2 and \u22121 with the remaining probability. The adversary then supplies the true outcome, yt \u2208 {\u22121, 1}, where yt = 1 indicates the outcome \u201c(it, jt) are joined by an edge\u201d, and yt = \u22121 the opposite outcome. The loss suffered by the learner is the absolute loss,\n\u2113t(y\u0302t) = 1\n2 |y\u0302t \u2212 yt|,\nwhich can be also interpreted as the probability that a randomized prediction according to y\u0302t will not equal the true outcome yt.\n2The condition \u03b2 \u2265 1 is not a serious restriction since for any (\u03b2, \u03c4 )-decomposition of W, viz. sym(W) = P\u2212N, we have \u03b2 \u2265 |P (i, j)|, |N(i, j)| for all (i, j) since P,N 0; and so 2\u03b2 \u2265 |P (i, j) \u2212 N(i, j)| = |W (i, j)|. Thus, if we make the reasonable assumption that there is some W \u2208 W with |W (i, j)| = 1 for some (i, j), then \u03b2 \u2265 1\n2 is necessary.\nThe comparison class is W = {WA|A \u2286 [n]}, where\nWA(i, j) =\n{\n1 if ((i \u2208 A) and (j /\u2208 A)) or ((j \u2208 A) and (i /\u2208 A)) \u22121 otherwise.\nThat is, WA(i, j) indicates if (i, j) crosses the cut defined by A or not. The following lemma (proved in Appendix C) formalizes the relationship of this online problem to the max-cut problem:\nLemma 2. Consider an online sequence of loss functions {\u2113t} as above. Let\nW\u2217 = arg min W\u2208W\n\u2211\nt\n\u2113t(W (it, jt)) .\nThen W\u2217 = WA for the set A that determines the max cut in the weighted graph over [n] nodes whose weights are given by wij = \u2211\nt:(it,jt)=(i,j) yt for every (i, j).\nA regret bound of O( \u221a nT ) is attainable for this problem as follows via an exponential time algorithm: consider the set of all 2n cuts in the graph. For each cut defined by A, consider a decision rule or \u201cexpert\u201d that predicts according to the matrix WA. Standard bounds for the experts algorithm imply the O( \u221a nT ) regret bound.\nA simple way to get an efficient algorithm is to replace W with the class of all matrices in {\u22121, 1}n\u00d7n. This leads to n2 different prediction tasks, each of which corresponds to the decision if there is an edge between two nodes, which is efficiently solvable. However, the regret with respect to this larger comparison class scales like O( \u221a n2T ).\nAnother popular approach for circumventing the hardness is to replace W with the set of matrices whose trace-norm is bounded by \u03c4 = n. However, applying the online mirror descent algorithmic framework with an appropriate squared-Schatten norm regularization, as described in [Kakade et al., 2010], leads to a regret bound that again scales like O( \u221a n2T ).\nIn contrast, our Online Matrix Prediction algorithm yields an efficient solution for this problem, with a regret that scales like \u221a\nn log(n)T . The regret bound of the algorithm follows from the following:\nLemma 3. W is (1, n)-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 4. There is an efficient algorithm for the online max-cut problem with regret bounded by 2 \u221a n log(n)T .\nWe prove (in Appendix 5) that the upper bound is near-optimal:\nTheorem 5. For any algorithm for the online max-cut problem, there is a sequence of entries (it, jt) and loss functions \u2113t for t = 1, 2, . . . , T such that the regret of the algorithm is at least\u221a\nnT/16."}, {"heading": "2.2 Collaborative Filtering with Bounded Trace Norm", "text": "In this problem, the comparison set W is the following set of m \u00d7 n matrices with trace norm bounded by some parameter \u03c4 :\nW := {W \u2208 [\u22121, 1]m\u00d7n : \u2016W\u2016\u22c6 \u2264 \u03c4}. (1)\nWithout loss of generality we assume that m \u2264 n. As before, applying the technique of Kakade et al. [2010] leads to a regret bound that scales as\u221a\n\u03c42T , which leads to trivial results in the most relevant case where \u03c4 = \u0398( \u221a mn). In contrast, we\ncan obtain a much better result based on the following lemma. Lemma 6. The class W given in (1) is ( \u221a m+ n, 2\u03c4)-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 7. There is an efficient algorithm for the online collaborative filtering problem with regret bounded by 2G \u221a 2\u03c4 \u221a n+m log(2(m+ n))T ), assuming that for all t the loss function is GLipschitz.\nThis upper bound is near-optimal, as we can also show (in Appendix 5) the following lower bound on the regret:\nTheorem 8. For any algorithm for online collaborative filtering problem with trace norm bounded by \u03c4 , there is a sequence of entries (it, jt) and G-Lipschitz loss functions \u2113t for t = 1, 2, . . . , T such that the regret of the algorithm is at least G \u221a\n1 2\u03c4\n\u221a nT .\nIn fact, the technique used to prove the above lower bound also implies a lower bound on the sample complexity of collaborative filtering in the batch setting (proved in Appendix 5). Theorem 9. The sample complexity of learning W in the batch setting, is \u2126(\u03c4\u221an/\u03b52). In particular, when \u03c4 = \u0398(n), the sample complexity is \u2126(n1.5/\u03b52).\nThis matches an upper bound given by Shamir and Shalev-Shwartz [2011]. The question of determining the sample complexity of W in the batch setting has been posed as an open problem by Shamir (who conjectured that it scales like n1.5) and Srebro (who conjectured that it scales like n4/3)."}, {"heading": "2.3 Online gambling", "text": "In the gambling problem, we define the comparison set W as the following set of n \u00d7 n matrices. First, for every permutation \u03c0 : [n] \u2192 [n], define the matrix W\u03c0 as:\nW\u03c0(i, j) =\n{\n1 if \u03c0(i) \u2264 \u03c0(j) 0 otherwise.\nThen the set W is defined as\nW := {W\u03c0 : \u03c0 is a permutation of [n]}. (2)\nOn round t, the adversary supplies a pair (it, jt) with it 6= jt, and the learner outputs as a prediction y\u0302t = Wt(it, jt) \u2208 [0, 1], where we interpret y\u0302t as the probability that it will beat jt. The adversary then supplies the true outcome, y\u0302t \u2208 {0, 1}, where y\u0302t = 1 indicates the outcome \u201cit beats jt\u201d, and y\u0302t = 0 the opposite outcome. The loss suffered by the learner is the absolute loss,\n\u2113t(yt) = |yt \u2212 y\u0302t|,\nwhich can be also interpreted as the probability that a randomized prediction according to y\u0302t will not equal to the true outcome yt.\nAs before, we tackle the problem by analyzing the decomposability of W.\nLemma 10. The class W given in (2) is (O(log(n)), O(n log(n)))-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 11. There is an efficient algorithm for the online gambling problem with regret bounded by O( \u221a n log3(n)T ).\nThis upper bound is near-optimal, as Kleinberg et al. [2010] essentially prove the following lower bound on the regret:\nTheorem 12. For any algorithm for the online gambling problem, there is a sequence of entries (it, jt) and labels yt, for t = 1, 2, . . . , T , such that the regret of the algorithm is at least \u2126( \u221a n log(n)T )."}, {"heading": "3 The Algorithm for Online Matrix Prediction", "text": "In this section we prove Theorem 1 by constructing an efficient algorithm for Online Matrix Prediction and analyze its regret. We start by describing an algorithm for Online Linear Optimization (OLO) over a certain set of matrices and with a certain set of linear loss functions. We show later that the Online Matrix Prediction problem can be reduced to this online convex optimization problem."}, {"heading": "3.1 The (\u03b2, \u03c4, \u03b3)-OLO problem", "text": "In this section, all matrices are in the space of real symmetric matrices of size N \u00d7 N , which we denote by SN\u00d7N .\nOn each round of online linear optimization, the learner chooses an element from a convex set K and the adversary responds with a linear loss function. In our case, the convex set K is a subset of the set of matrices with bounded trace and diagonal values:\nK \u2286 {X \u2208 SN\u00d7N : X 0, \u2200i \u2208 [N ] : Xii \u2264 \u03b2, Tr(X) \u2264 \u03c4}.\nWe assume for convenience that \u03c4N I \u2208 K. The loss function on round t is the function X 7\u2192 X \u2022 Lt def= \u2211 i,j X(i, j)Lt(i, j), where Lt is a matrix from the following set of matrices:\nL = {L \u2208 SN\u00d7N : L2 def= LL is a diagonal matrix s.t. Tr(L2) \u2264 \u03b3}.\nWe call the above setting a (\u03b2, \u03b3, \u03c4)-OLO problem. As usual, we analyze the regret of the algorithm\nRegret := T\u2211\nt=1\nXt \u2022 Lt \u2212 min X\u2208K\nT\u2211\nt=1\nX \u2022 Lt ,\nwhere X1, . . . ,XT are the predictions of the learner. Below we describe and analyze an algorithm for the (\u03b2, \u03b3, \u03c4)-OLO problem. The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K. The projection operation is defined with respect to the quantum relative entropy divergence:\n\u2206(X,A) = Tr(X log(X)\u2212X log(A)\u2212X+A).\nAlgorithm 1 Matrix Multiplicative Weights with Quantum Relative Entropy Projections\n1: Input: \u03b7 2: Initialize X1 = \u03c4 N I. 3: for t = 1, 2, . . . , T : do 4: Play the matrix Xt. 5: Obtain loss matrix Lt. 6: Update Xt+1 = argminX\u2208K \u2206(X, exp(log(Xt)\u2212 \u03b7Lt)). 7: end for\nAlgorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness):\nTheorem 13. Suppose \u03b7 is chosen so that \u03b7\u2016Lt\u2016 \u2264 1 for all t (where \u2016Lt\u2016 is the spectral norm of Lt). Then\nRegret \u2264 \u03b7 T\u2211\nt=1\nXt \u2022 L2t + \u03c4 log(N)\n\u03b7 .\nEquipped with the above we are ready to prove a regret bound for (\u03b2, \u03b3, \u03c4)-OLO.\nTheorem 14. Assume T \u2265 \u03c4 log(N)\u03b2 . Then, applying Algorithm 1 with \u03b7 = \u221a \u03c4 log(N) \u03b2\u03b3T on a (\u03b2, \u03b3, \u03c4)OLO problem yields an efficient algorithm whose regret is at most 2 \u221a \u03b2\u03b3\u03c4 log(N)T .\nProof. Clearly, Algorithm 1) can be implemented in polynomial time since the update of step 6 is a convex optimization problem. To analyze the regret of the algorithm we rely on Theorem 13. By the definition of K and L, we get that Xt \u2022 L2t \u2264 \u03b2\u03b3. Hence, the regret bound becomes\nRegret \u2264 \u03b7\u03b2\u03b3T + \u03c4 log(N) \u03b7 .\nSubstituting the value of \u03b7, we get the stated regret bound. One technical condition is that the above regret bound holds as long as \u03b7 is chosen small enough so that for all t, we have \u03b7\u2016Lt\u2016 \u2264 1. Now \u2016Lt\u2016 \u2264 \u2016Lt\u2016F = \u221a Tr(L2t ) \u2264 \u221a \u03b3. Thus, for T \u2265 \u03c4 log(N)\u03b2 , the technical condition is satisfied for \u03b7 = \u221a\n\u03c4 log(N) \u03b2\u03b3T ."}, {"heading": "3.2 An Algorithm for the Online Matrix Prediction Problem", "text": "In this section we describe a reduction from the Online Matrix Prediction problem (with a (\u03b2, \u03c4)decomposable comparison class) to a (\u03b2, 4G2, \u03c4)-OCO problem with N = 2p. The regret bound of the derived algorithm will follow directly from Theorem 14.\nWe now describe the reduction. To simplify our notation, let q be m if W contains nonsymmetric matrices and q = 0 otherwise. Note that the definition of sym(W) implies that for a pair of indices (i, j) \u2208 [m]\u00d7 [n], their corresponding indices in sym(W) are (i, j + q).\nGiven any matrix W \u2208 W we embed its symmetrization sym(W) (which has size p \u00d7 p) into the set of 2p\u00d72p positive semidefinite matrices as follows. Since W admits a (\u03b2, \u03c4)-decomposition, there exist P,N 0 such that sym(W) = P \u2212 N, Tr(P) + Tr(N) \u2264 \u03c4 , and for all i \u2208 [p], P (i, i), N(i, i) \u2264 \u03b2. The embedding of W in S2p\u00d72p, denoted \u03c6(W), is defined to be the matrix3\n\u03c6(W) =\n[ P 0\n0 N\n]\n.\nIt is easy to verify that \u03c6(W) belongs to the convex set K defined below:\nK := { X \u2208 S2p\u00d72p s.t.\nX 0 (3) \u2200i \u2208 [2p] : X(i, i) \u2264 \u03b2 Tr(X) \u2264 \u03c4 \u2200(i, j) \u2208 [m]\u00d7 [n] : (X(i, j + q)\u2212X(p+ i, p + j + q)) \u2208 [\u22121, 1] }\nWe shall run the OLO algorithm with the set K. On round t, if the adversary gives the pair (it, jt), then we predict y\u0302t = Xt(it, jt + q)\u2212Xt(p+ it, p+ jt + q) . The last constraint defining K simply ensures that y\u0302t \u2208 [\u22121, 1]. While this constraint makes the quantum relative entropy projection onto K more complex, in Appendix 6 we show how we can leverage the knowledge of (it, jt) to get a very fast implementation.\nNext we describe how to choose the loss matrices Lt using the subderivative of \u2113t. Given the loss function \u2113t, let g be a subderivative of \u2113t at y\u0302t. Since \u2113t is convex and G-Lipschitz, we have that |g| \u2264 G. Define Lt \u2208 S2p\u00d72p as follows:\nLt(i, j) =\n \n\ng if (i, j) = (it, jt + q) or (i, j) = (jt + q, it) \u2212g if (i, j) = (p + it, p+ jt + q) or (i, j) = (p + jt + q, p+ it) 0 otherwise.\n(4)\nNote that L2t is a diagonal matrix, whose only non-zero diagonal entries are (it + q, it + q), (jt + q, jt+q), (p+it+q, p+it+q), and (p+jt+q, p+jt+q), all equalling g 2. Hence, Tr(L2t ) = 4g 2 \u2264 4G2.\n3Note that this mapping depends on the choice of P and N for each matrix W \u2208 W. We make an arbitrary choice for each W.\nTo summarize, the Online Matrix Prediction algorithm will be as follows:\nAlgorithm 2 Matrix Multiplicative Weights for Online Matrix Prediction\n1: Input: \u03b2, \u03c4,G,m, n, p, q (see text for definitions) 2: Set: \u03b3 = 4G2, N = 2p, \u03b7 = \u221a\n\u03c4 log(N) \u03b2\u03b3T\n3: Let K be as defined in (3) 4: Initialize X1 = \u03c4 N I. 5: for t = 1, 2, . . . , T : do 6: Adversary supplies a pair of indices (it, jt) \u2208 [m]\u00d7 [n]. 7: Predict y\u0302t = Xt(it, jt + q)\u2212Xt(p+ it, p+ jt + q). 8: Obtain loss function \u2113t : [\u22121, 1] \u2192 R and pay \u2113t(y\u0302t). 9: Let g be a sub-derivative of \u2113t at y\u0302t\n10: Let Lt be as defined in (4) 11: Update Xt+1 = argminX\u2208K \u2206(X, exp(log(Xt)\u2212 \u03b7Lt)). 12: end for\nTo analyze the algorithm, note that for any W \u2208 W,\n\u03c6(W) \u2022 Lt = 2g(P (it, jt)\u2212N(it, jt)) = 2gW (it, jt),\nand Xt \u2022 Lt = 2g(Xt(it, jt + q)\u2212Xt(p + it, p+ jt + q)) = 2gy\u0302t.\nSo for any W \u2208 W, we have\nXt \u2022 Lt \u2212 \u03c6(W) \u2022 Lt = 2g(y\u0302t \u2212W (it, jt)) \u2265 2(\u2113t(y\u0302t)\u2212 \u2113t(W (it, jt))),\nby the convexity of \u2113t(\u00b7). This implies that for any W \u2208 W,\nT\u2211\nt=1\n\u2113t(y\u0302t)\u2212 \u2113t(W (it, jt)) \u2264 1\n2\n[ T\u2211\nt=1\nXt \u2022 Lt \u2212 \u03c6(W) \u2022 Lt ]\n\u2264 1 2 \u00b7 RegretOLO.\nThus, the regret of the Online Matrix Prediction problem is at most half the regret in the (\u03b2, 4G2, \u03c4)OLO problem."}, {"heading": "3.2.1 Proof of Theorem 1", "text": "Following our reduction, we can now appeal to Theorem 14. For T \u2265 \u03c4 log(2p)\u03b2 , the bound of Theorem 14 applies and gives a regret bound of 2G \u221a\n\u03c4\u03b2 log(2p)T . For T < \u03c4 log(2p)\u03b2 , note that in any round, the regret can be at most 2G, since the subderivatives of the loss functions are bounded in absolute value by G and the domain is [\u22121, 1], so the regret is bounded by 2GT < 2G \u221a\n\u03c4\u03b2 log(2p)T since \u03b2 \u2265 1. Thus, we have proved the regret bound stated in Theorem 1."}, {"heading": "4 Decomposability Proofs", "text": "In this section we prove the decomposability results for the comparison classes corresponds to maxcut, collaborative filtering, and gambling. All the three decompositions we give are optimal up to constant factors."}, {"heading": "4.1 Proof of Lemma 3 (max-cut)", "text": "We need to show that every matrix WA \u2208 W admits a (1, n)-decomposition. We can rewrite WA = \u2212wAw\u22a4 where wA \u2208 Rn is the vector such that\nWA(i) =\n{\n1 if i \u2208 A \u22121 otherwise.\nSince WA is already symmetric, sym(WA) = WA = \u2212wAw\u22a4A. Thus we can choose P = 0 and N = wAw \u22a4 A. These are positive semidefinite matrices with diagonals bounded by 1 and sum of traces equals to n, which concludes the proof. Since Tr(wAw \u22a4 A) = n, this (1, n)-decomposition is optimal."}, {"heading": "4.2 Proof of Lemma 6 (collaborative filtering)", "text": "We need to show that every matrixW \u2208 W, i.e. anm\u00d7nmatrix over [\u22121, 1] with \u2016W\u2016\u22c6 \u2264 \u03c4 , admits a ( \u221a m+ n, 2\u03c4)-decomposition. The ( \u221a m+ n, 2\u03c4)-decomposition of W is a direct consequence of the following theorem, setting Y = sym(W), with p = m + n, and the fact that \u2016sym(W)\u2016\u22c6 = 2\u2016W\u2016\u22c6 (see Lemma 19). Theorem 15. Let Y be a p\u00d7 p symmetric matrix with entries in [\u22121, 1]. Then Y can be written as Y = P\u2212N where P and N are both positive semidefinite matrices with diagonal entries bounded by \u221a p, and Tr(P) + Tr(N) = \u2016Y\u2016\u22c6.\nProof. Let\nY = \u2211\ni\n\u03bbiviv \u22a4 i\nbe the eigenvalue decomposition of Y. We now show that\nP = \u2211\ni: \u03bbi\u22650 \u03bbiviv\n\u22a4 i and N =\n\u2211\ni: \u03bbi<0\n\u2212\u03bbiviv\u22a4i\nsatisfy the required conditions. Clearly Tr(P)+Tr(N) = \u2211\ni |\u03bbi| = \u2016Y\u2016\u22c6. Define abs(Y) = P+N =\u2211 i |\u03bbi|viv\u22a4i . Note that\nabs(Y)2 = \u2211\ni\n\u03bb2iviv \u22a4 i = Y 2.\nWe now show that all entries (and in particular, the diagonal entries) of abs(Y) are bounded in magnitude by \u221a p. Since P and N are both positive semidefinite, their diagonal elements must be non-negative, so we conclude that the diagonal entries of P and N are bounded by \u221a p as well.\nSince all the entries of Y are bounded in magnitude by 1, it follows that all entries of Y2 are bounded in magnitude by p. In particular, the diagonal entries of Y2 are bounded by p. Since these diagonal entries are equal to the squared lengths of the rows of abs(Y), it follows that each entry of abs(Y) is bounded in magnitude by \u221a p.\nThis decomposition is optimal up to constant factors. Consider the matrix W formed by taking m = \u03c4\u221a\nn rows of an n \u00d7 n Hadamard matrix. In Theorem 20 (proved in Appendix D), we prove\nthat any (\u03b2, \u03c4\u0303 )-decomposition of sym(W) must have \u03b2\u03c4\u0303 \u2265 14\u03c4 \u221a n. Since the regret bound depends on the product \u03b2\u03c4\u0303 , we conclude that the decomposition obtained from Theorem 15 is optimal up to a constant factor."}, {"heading": "4.3 Proof of Lemma 10 (gambling)", "text": "We need to show that every matrix W \u2208 W, i.e. an n \u00d7 n matrix W\u03c0 for some permutation \u03c0 : [n] \u2192 [n], admits a (O(log(n)), O(n log(n)))-decomposition. One minor change that needs to be made to Algorithm 2 is that the last constraint in (3) needs to be changed to\n\u2200(i, j) \u2208 [n]\u00d7 [n] : (X(i, j + q)\u2212X(p + i, p + j + q)) \u2208 [0, 1],\nto ensure that the prediction lies in [0, 1] rather than [\u22121, 1]. The analysis remains intact, and so does the regret bound.\nWe now give the decomposition. The following upper triangular matrix T plays a pivotal role:\nT (i, j) =\n{\n1 if i \u2264 j 0 otherwise.\nThe reason this matrix is so important is because any matrix W\u03c0 is obtained by permuting the rows and columns of T. In particular, let P\u03c0 be the permutation matrix defined by the permutation \u03c0, i.e.\nP\u03c0(i, j) =\n{\n1 if j = \u03c0(i)\n0 otherwise.\nThen it is easy to check that W\u03c0 = P\u03c0TP \u22a4 \u03c0 .\nUsing this fact, we get [ P\u03c0 0\n0 P\u03c0\n]\n\ufe38 \ufe37\ufe37 \ufe38\nQ\u03c0\nsym(T) [ P\u22a4\u03c0 0 0 P\u22a4\u03c0 ] = [ P\u03c0 0 0 P\u03c0 ] [ 0 T T\u22a4 0 ] [ P\u22a4\u03c0 0 0 P\u22a4\u03c0 ]\n=\n[ 0 P\u03c0TP \u22a4 \u03c0\nP\u03c0T \u22a4P\u22a4\u03c0 0\n]\n=\n[ 0 W\u03c0\nW\u22a4\u03c0 0\n]\n= sym(W\u03c0).\nNow, note thatQ\u03c0 is a permutation matrix (viz. the one defined by the permutation \u03c0 \u2032 : [2n] \u2192 [2n] defined as \u03c0\u2032(i) = \u03c0(i) for 1 \u2264 i \u2264 n, and \u03c0\u2032(i) = \u03c0(i\u2212 n) +n for n < i \u2264 2n). Thus, if T admits a (\u03b2, \u03c4)-decomposition, sym(T) = P\u2212N, then\nsym(W\u03c0) = Q\u03c0sym(T)Q \u22a4 \u03c0 = Q\u03c0PQ \u22a4 \u03c0 \u2212Q\u03c0NQ\u22a4\u03c0\nis a (\u03b2, \u03c4)-decomposition for sym(W\u03c0). This is because the diagonal entries of Q\u03c0PQ \u22a4 \u03c0 (resp. Q\u03c0NQ \u22a4 \u03c0 ) are simply a permutation (viz. \u03c0 \u2032) of the diagonal entries of P (resp. N). Since\nABA\u22a4 0 if B 0 for any matrix A, the matrices Q\u03c0PQ\u22a4\u03c0 and Q\u03c0PQ\u22a4\u03c0 are both positive semidefinite.\nSo now we show that T admits a (O(log(n)), O(n log(n)))-decomposition. For convenience, we assume that n is a power of 2, i.e. n = 2k for some integer k \u2265 0. For n that are not a power of 2, we can readily obtain a decomposition by the following observation: if we take the smallest power of 2 that is larger than n, say 2k, and consider the symmetrized triangular matrix for 2k, then sym(T) can be expressed as a principal submatrix of it. Then taking the corresponding principal submatrices from the decomposition for the triangular matrix for 2k we obtain a decomposition for n. This uses the fact that principal submatrices of positive semidefinite matrices are positive semidefinite as well.\nTheorem 16. Let n = 2k for some integer k \u2265 0. Then T admits a (k+1, 4n(k+1))-decomposition.\nProof. We show that sym(T) can be written as a difference of positive semidefinite matrices with diagonals bounded by k+1. The bound on the sum of traces, 4n(k+1), of the two matrices follows trivially.\nWe use a recursive construction. Let the triangular matrix for n = 2k be denoted by Tk. For k = 0, the following is a decomposition for T0 with diagonals bounded by 1:\nsym(T0) = [ 0 1 1 0 ] = [ 1 1 1 1 ] \u2212 [ 1 0 0 1 ] .\nSo now assume that k > 0 and we have a decomposition for Tk\u22121 with diagonals bounded by k, i.e.\nsym(Tk\u22121) =\n[ 0 Tk\u22121\nT\u22a4k\u22121 0\n]\n= P\u2212N,\nwhere P,N 0, and for all i \u2208 [2k], P (i, i), N(i, i) \u2264 k. We need the following block decomposition of P and N into contiguous 2k\u22121 \u00d7 2k\u22121 blocks as follows:\nP =\n[ PA PB\nPC PD\n]\nand N =\n[ NA NB\nNC ND\n]\n.\nThen we have the following decomposition of sym(Tk). All the blocks in the decomposition below are of size 2k\u22121 \u00d7 2k\u22121.\nsym(Tk) =\n\n  \n0 0 0 1\n0 0 0 0\n0 0 0 0\n1 0 0 0\n\n   +\n\n  \n0 0 Tk\u22121 0 0 0 0 Tk\u22121\nT\u22a4k\u22121 0 0 0 0 T\u22a4k\u22121 0 0\n\n   .\nNow, consider the following decompositions of the two matrices above as a difference of positive semidefinite matrices. For the first matrix, the diagonals in the decomposition are bounded by 1:\n\n  \n0 0 0 1\n0 0 0 0\n0 0 0 0 1 0 0 0\n\n   =\n\n  \n1 0 0 1\n0 0 0 0\n0 0 0 0 1 0 0 1\n\n   \u2212\n\n  \n1 0 0 0\n0 0 0 0\n0 0 0 0 0 0 0 1\n\n   .\nFor the second matrix, the diagonals in the decomposition are bounded by k.\n\n  \n0 0 Tk\u22121 0 0 0 0 Tk\u22121\nT\u22a4k\u22121 0 0 0 0 T\u22a4k\u22121 0 0\n\n   =\n\n  \nPA 0 PB 0\n0 PA 0 PB\nPC 0 PD 0\n0 PC 0 PD\n\n   \u2212\n\n  \nNA 0 NB 0\n0 NA 0 NB\nNC 0 ND 0\n0 NC 0 ND\n\n   .\nIt is easy to verify that the matrices in the decomposition above are positive semidefinite, since each is a sum of two positive semidefinite matrices. For example:\n\n  \nPA 0 PB 0\n0 PA 0 PB\nPC 0 PD 0\n0 PC 0 PD\n\n   =\n\n  \nPA 0 PB 0\n0 0 0 0 PC 0 PD 0\n0 0 0 0\n\n   +\n\n  \n0 0 0 0 0 PA 0 PB\n0 0 0 0 0 PC 0 PD\n\n   .\nAdding the two decompositions, we get a decomposition for sym(Tk) as a difference of two positive semidefinite matrices. The diagonal entries of these two matrices are bounded by k+1, as required.\nThis decomposition is optimal up to constant factors. This is because the singular values of T are 1\n2 cos( k\u03c0 2n+1\n) for k = 1, 2, . . . , n (see Elkies [2011]). This implies that \u2016T\u2016\u22c6 = \u0398(n log(n)). Thus,\nthe best \u03b2 one can get is \u0398(log(n)), and the best \u03c4 is \u0398(n log(n))."}, {"heading": "5 Lower bounds", "text": "In this section we prove the lower bounds stated in Section 2."}, {"heading": "5.1 Online Max Cut", "text": "We prove Theorem 5, which we restate here for convenience:\nTheorem 5 restated: For any algorithm for the online max cut problem, there is a sequence of entries (it, jt) and loss functions \u2113t for t = 1, 2, . . . , T such that the regret of the algorithm is at least \u221a nT/16.\nProof. Consider the following stochastic adversary. Divide up the time period T into n/2 equal size4 intervals Ti, for i \u2208 [n/2], corresponding to the n/2 pairs of indices (i, i + n/2) for i \u2208 [n/2]. For every i \u2208 [n/2] and for each t \u2208 Ti, the adversary sets (it, jt) = (i, i + n/2) and yt to be a Rademacher random variable independent of all other such variables. Clearly, the expected regret of any algorithm for the online max cut problem equals T2 .\nNow, define the following subset of vertices A: for every i \u2208 [n/2], consider Si = \u2211\nt\u2208Ti yt. If Si < 0, include both i, i + n/2 \u2208 A, else only include i \u2208 A. By construction, the matrix WA has the following property for all i \u2208 [n/2]:\nWA(i, i+ n/2) = sgn(Si).\n4We assume for convenience that n 2 and 2T n are integers.\nUsing the definition of \u2113t and the fact that |Ti| = 2T/n, we obtain\nE\n  \u2211\nt\u2208Ti\n\u2113t(WA(i, i+ n/2))\n\n = E\n  \u2211\nt\u2208Ti\n( 1 2 \u2212 sgn(Si) 2 yt )\n\n\n= E\n[ T\nn \u2212 |Si| 2\n] \u2264 T n \u2212 \u221a T 4n ,\nwhere we used Khintchine\u2019s inequality: if X is a sum of k independent Rademacher random variables, then E[|X|] \u2265 \u221a k/2. Summing up over all i \u2208 [n/2], we get that\nE\n[ T\u2211\nt=1\n\u2113t(WA(it, jt))\n]\n\u2264 n 2\n[\nT n \u2212 \u221a T 4n\n]\n= T\n2 \u2212\n\u221a\nnT 16 .\nHence the expected regret of the algorithm is at least \u221a\nnT 16 . In particular, there is a setting of the\ny\u0302t variables so that the regret of the algorithm is at least \u221a nT 16 ."}, {"heading": "5.2 Online Collaborative Filtering with Bounded Trace Norm", "text": "We start with the proof of Theorem 8, which we restate here for convenience:\nTheorem 8 restated: For any algorithm for online collaborative filtering problem with trace norm bounded by \u03c4 , there is a sequence of entries (it, jt) and loss functions \u2113t for t = 1, 2, . . . , T such that the regret of the algorithm is at least G \u221a\n1 2\u03c4\n\u221a nT .\nProof. First, we may assume that \u03c4 \u2264 m\u221an: this is because for any matrix W \u2208 [\u22121, 1]m\u00d7n, we have\n\u2016W\u2016\u22c6 \u2264 \u221a rank(W)\u2016W\u2016F \u2264 \u221a m \u00b7 \u221a mn = m \u221a n,\nsince rank(W) \u2264 m. So now we focus on the sub-matrix formed by the first \u03c4\u221a n rows5 and all n\ncolumns. This sub-matrix has \u03c4 \u221a n entries.\nConsider the following stochastic adversary. Divide up the time period T into \u03c4 \u221a n intervals of\nlength T \u03c4 \u221a n , indexed by \u03c4\n\u221a n pairs (i, j) corresponding to the entries of the sub-matrix. For every\n(i, j), and for every round t in the interval Iij corresponding to (i, j), we set the loss function to be \u2113t(W) = \u03c3tGWij, where \u03c3t \u2208 {\u22121, 1} is a Rademacher random variable chosen independently of all other such variables. Note that the absolute value of derivative of the loss function is G.\nClearly, any algorithm for OCF has expected loss 0. Now consider the matrix W\u22c6 where\n\u2200i \u2208 [\n\u03c4\u221a n\n]\n, j \u2208 [n] : W \u22c6ij = \u2212sgn ( \u2211 t\u2208Iij\u03c3t ) ,\nand all entries in rows i > \u03c4\u221a n are set to 0. Since rank(W\u22c6) \u2264 \u03c4\u221a n , we have\n\u2016W\u22c6\u2016\u22c6 \u2264 \u221a rank(W\u22c6) \u00b7 \u2016W\u22c6\u2016F \u2264 \u221a \u03c4\u221a n \u00b7 \u221a \u03c4 \u221a n = \u03c4,\n5For convenience, we assume that \u03c4\u221a n and T \u03c4 \u221a n are integers.\nso W\u22c6 \u2208 W.6 The expected loss of W\u22c6 is\n\u2211\nij\nE\n  \u2211\nt\u2208Iij\n\u03c3tGW \u22c6 ij\n  = G \u2211\nij\nE\n\n\u2212 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nt\u2208Iij\n\u03c3t \u2223 \u2223 \u2223 \u2223 \u2223 \u2223  \n\u2265 \u2212G \u2211\nij\n\u221a\n1 2 |Iij |\n= \u2212G\u03c4 \u221a n \u00b7\n\u221a\nT\n2\u03c4 \u221a n\n= \u2212G \u221a 1\n2 \u03c4 \u221a nT ,\nwhere the inequality above is again due to Khintchine\u2019s inequality. Hence, the expected regret of the algorithm is at least G \u221a\n1 2\u03c4 \u221a nT . In particular, there is a specific assignment of values to \u03c3t\nsuch that the regret of the algorithm is at least G \u221a\n1 2\u03c4\n\u221a nT .\nThe construction we used for deriving the above lower bound can be easily adapted to derive a lower bound on the sample complexity of learning the class W in the batch setting. This is formalized in Theorem 9, which we restate here for convenience.\nTheorem 9 restated The sample complexity of learningW in the batch setting, is \u2126(\u03c4\u221an/\u03b52). In particular, when \u03c4 = \u0398(n), the sample complexity is \u2126(n1.5/\u03b52).\nProof. For simplicity, let us choose m = n. Let k = \u03c4/ \u221a n and fix some small \u03b5. Define a family of distributions over [n]2 \u00d7 {\u22121, 1} as follows. Each distribution is parameterized by a matrix W such that there is some I \u2282 [n], with |I| = k, where W (i, j) \u2208 {\u22121, 1} for i \u2208 I and W (i, j) = 0 for i /\u2208 I. Now, the probability to sample an example (i, j, y) is\n( 1 2 + 2\u03b5 ) 1 kn if i \u2208 I and y = W (i, j),\nis ( 1 2 \u2212 2\u03b5 ) 1 kn if i \u2208 I and y = \u2212W (i, j), and the probability is 0 in all other cases.\nAs in the proof of Theorem 8, any matrix defining such distribution is in W. Furthermore, if we consider the absolute loss function: \u2113(W, (i, j, y)) = 12 |W (i, j)\u2212 y|, then the expected loss of W with respect to the distribution it defines is\nE [ 1 2 |W (i, j) \u2212 y| ] = 12 \u2212 2\u03b5 .\nIn contrast, by standard no-free-lunch arguments, no algorithm can know to predict an entry (i, j) with error smaller than 12 \u2212 \u03b5 without observing \u2126(1/\u03b52) examples from this entry. Therefore, no algorithm can have an error smaller than 12 \u2212 \u03b5 without receiving \u2126(kn/\u03b52) examples."}, {"heading": "6 Implementation Details", "text": "In general, the update rule in Algorithm 1 is a convex optimization problem and can be computed in polynomial time. We now give the following more efficient implementation which takes essentially O\u0303(p3) time per round. This is based on the following theorem that is essentially proved in Tsuda et al. [2006]:\n6This construction is tight: e.g. if W\u22c6 is formed by taking t\u221a n rows of an n\u00d7 n Hadamard matrix.\nTheorem 17. The optimal solution of argminX\u2208K \u2206(X,Y), where Y is a given symmetric matrix, and K := {X \u2208 Sn\u00d7n : Aj \u2022X \u2264 bj for j = 1, 2, . . . ,m}, is given by\nX\u22c6 = exp(log(Y)\u2212\u2211mj=1\u03b1\u22c6jA\u2032j),\nwhere A\u2032j = 1 2 (Aj +A \u22a4 j ), and \u03b1 \u22c6 = \u3008\u03b1\u22c61, \u03b1\u22c62, . . . , \u03b1\u22c6m\u3009 is given by\n\u03b1 \u22c6 = arg max \u2200j\u2208[m]: \u03b1j\u22650 \u2212Tr(exp(log(Y)\u2212\u2211mj=1\u03b1jA\u2032j))\u2212 \u2211m j=1\u03b1jbj.\nThe idea is to avoid taking projections on the set K in each round. If the chosen entry in round t is (it, jt), then we compute Xt as\nXt = arg min X\u2208Kt\n\u2206(X, exp(log(Xt\u22121 \u2212 \u03b7Lt\u22121)),\nwhere the polytope Kt is defined as\nKt := { X \u2208 S2p\u00d72p s.t.\nX(it, it) +X(jt + q, jt + q) +X(p + it, p+ it) +X(p + jt + q, p+ jt + q) \u2264 4\u03b2 X(it, jt + q)\u2212X(p + it, p+ jt + q)) \u2264 1 X(p+ it, p + jt + q))\u2212X(it, jt + q) \u2264 1 Tr(X) \u2264 \u03c4 }\nThe observation is that this suffices for the regret bound of Theorem 14 to hold since the optimal point in hindsight X\u22c6 \u2208 Kt for all t (see the proof of Theorem 13).\nNote that Kt is defined using just 4 constraints, and hence the dual problem given in Theorem 17 has only 4 variables \u03b1j . Thus, standard convex optimization techniques (say, the ellipsoid method) can be used to solve the dual problem to \u03b5-precision in O(log(1/\u03b5)) iterations, each of which requires computing the gradient and/or the Hessian of the objective, which can be done in O(p3) time via the eigendecomposition, leading to an O\u0303(p3) time algorithm overall.\nMore precisely, the iteration count for convex optimization methods have logarithmic dependence on the range of the \u03b1j variables. Since Tr(Xt\u22121) \u2264 \u03c4 , we see (using the Golden-Thompson inequality [Golden, 1965, Thompson, 1965]) that\nTr(exp(log(Xt\u22121 \u2212 \u03b7Lt\u22121))) \u2264 Xt\u22121 \u2022 exp(\u2212\u03b7Lt\u22121) \u2264 3\u03c4.\nThus, setting all \u03b1j = 0, the dual objective value is at least \u22123\u03c4 . Since bj \u2265 1 for all j, we get that the optimal values of \u03b1j are all bounded by 3\u03c4 . Thus, the range of all \u03b1j can be set to [0, 3\u03c4 ], giving a O(log( \u03c4\u03b5 )) bound on the number of iterations."}, {"heading": "7 Conclusions", "text": "In recent years the FTRL (Follow The Regularized Leader) paradigm has become the method of choice for proving regret bounds for online learning problems. In several online learning problems a direct application of this paradigm has failed to give tight regret bounds due to suboptimal \u201cconvexification\u201d of the problem. This unsatisfying situation occurred in mainstream applications, such as online collaborative filtering, but also in basic prediction settings such as the online max cut or online gambling settings.\nIn this paper we single out a common property of these unresolved problems: they involve structured matrix prediction, in the sense that the matrices involved have certain nice decompositions. We give a unified formulation for three of these structured matrix prediction problems which leads to near-optimal convexification. Applying the standard FTRL algorithm, Matrix Multiplicative Weights, now gives efficient and near optimal regret algorithms for these problems. In the process we resolve two COLT open problems. The main conclusion of this paper is that spectral analysis in matrix predictions tasks can be surprisingly powerful, even when the connection between the spectrum and the problem may not be obvious on first sight (such as in the online gambling problem).\nWe leave open the question of bridging the logarithmic gap between known upper and lower bounds for regret in these structured prediction problems. Note that since all the three decompositions in this paper are optimal up to constant factors, one cannot close the gap by improving the decomposition; some fundamentally different algorithm seems necessary. It would also be interesting to see more applications of the (\u03b2, \u03c4)-decomposition for other online matrix prediction problems."}, {"heading": "A Matrix Multiplicative Weights Algorithm", "text": "For the sake of completeness, we prove Theorem 13. The setting is as follows. We have an online convex optimization problem where the decision set is a convex subset K of N \u00d7N positive semidefinite matrices of trace bounded by \u03c4 , viz. for all X \u2208 K, we have X 0 and Tr(X) \u2264 \u03c4 . We assume for convenience that \u03c4N I \u2208 K. In each round t, the learner produces a matrix Xt \u2208 K, and the adversary supplies a loss matrix Lt \u2208 RN\u00d7N , which is assumed to be symmetric. The loss of the learner is Xt \u2022 Lt. The goal is to minimize regret defined as\nRegret := T\u2211\nt=1\nXt \u2022 Lt \u2212 min X\u2208K\nT\u2211\nt=1\nX \u2022 Lt.\nConsider Algorithm 1. We now prove Theorem 13, which we restate here for convenience:\nTheorem 18. Suppose \u03b7 is chosen so that \u03b7\u2016Lt\u2016 \u2264 1 for all t. Then\nRegret \u2264 \u03b7 T\u2211\nt=1\nXt \u2022 L2t + \u03c4 log(N)\n\u03b7 .\nProof. Consider any round t. Let X \u2208 K be any matrix. We use the quantum relative entropy, \u2206(X,Xt), as a potential function. We have\n\u2206(X, exp(log(Xt)\u2212 \u03b7Lt))\u2212\u2206(X,Xt) = \u03b7X \u2022 Lt \u2212 Tr(Xt) + Tr(exp(log(Xt)\u2212 \u03b7Lt)). (5)\nNow quantum relative entropy projection onto the set K is a Bregman projection, and hence the Generalized Pythagorean inequality applies (see Tsuda et al. [2006]):\n\u2206(X,Xt+1) + \u2206(Xt+1, exp(log(Xt)\u2212 \u03b7Lt))) \u2264 \u2206(X, exp(log(Xt)\u2212 \u03b7Lt))),\nand since \u2206(Xt+1, exp(log(Xt)\u2212 \u03b7Lt))) \u2265 0, we get that\n\u2206(X,Xt+1) \u2264 \u2206(X, exp(log(Xt)\u2212 \u03b7Lt))).\nHence from (5) we get\n\u2206(X,Xt+1)\u2212\u2206(X,Xt) \u2264 \u03b7X \u2022 Lt \u2212Tr(Xt) + Tr(exp(log(Xt)\u2212 \u03b7Lt)). (6)\nNow, using the Golden-Thompson inequality [Golden, 1965, Thompson, 1965], we have\nTr(exp(log(Xt)\u2212 \u03b7Lt)) \u2264 Tr(Xt exp(\u2212\u03b7Lt))\nNext, using the fact that exp(A) I+A+A2 for \u2016A\u2016 \u2264 1,7 we obtain\nTr(Xt exp(\u2212\u03b7Lt)) \u2264 Tr(Xt(I\u2212 \u03b7Lt + \u03b72L2t ) = Tr(Xt)\u2212 \u03b7Xt \u2022 Lt + \u03b72Xt \u2022 L2t .\nCombining the above and plugging into (6) we get\n\u2206(X,Xt+1)\u2212\u2206(X,Xt) \u2264 \u03b7X \u2022 Lt \u2212 \u03b7Xt \u2022 Lt + \u03b72Xt \u2022 L2t . (7)\nSumming up from t = 1 to T , and rearranging, we get\nRegret \u2264 \u03b7 T\u2211\nt=1\nXt \u2022 L2t + \u2206(X,X1)\u2212\u2206(X,XT+1)\n\u03b7\n\u2264 \u03b7 T\u2211\nt=1\nXt \u2022 L2t + \u03c4 log(N)\n\u03b7 ,\nsince \u2206(X,XT+1) \u2265 0 and\n\u2206(X,X1) = X \u2022 (log(X)\u2212 log( \u03c4N I))\u2212 Tr(X) + \u03c4 = X \u2022 log( 1\u03c4X) + log(\u03c4)Tr(X)\u2212 log( \u03c4N )Tr(X)\u2212 Tr(X) + \u03c4 \u2264 Tr(X)(log(N)\u2212 1) + \u03c4 \u2264 \u03c4 log(N).\nThe first inequality above follows because Tr(X) \u2264 \u03c4 , so log( 1\u03c4X) \u227a 0. The second inequality uses Tr(X) \u2264 \u03c4 ."}, {"heading": "B Technical Lemmas and Proofs", "text": "Lemma 19. For m\u00d7 n non-symmetric matrices W, if W = U\u03a3V\u22a4 is the singular value decomposition of W, then\nsym(W) =\n[ 1\u221a 2 U 1\u221a 2 U\n1\u221a 2 V \u2212 1\u221a 2 V\n][ \u03a3 0\n0 \u2212\u03a3\n][ 1\u221a 2 U\u22a4 1\u221a 2 V\u22a4\n1\u221a 2 U\u22a4 \u2212 1\u221a 2 V\u22a4\n]\nis the eigenvalue decomposition of sym(W). In particular, \u2016sym(W)\u2016\u22c6 = 2\u2016W\u2016\u22c6. 7To see this, note that we can write A = VDV\u22a4 for some orthonormal V and diagonal D. Therefore,\nI+A+A2 \u2212 eA = V ( I+D+D2 \u2212 eD ) V \u22a4 .\nNow, by the inequality 1 + a + a2 \u2212 ea \u2265 0, which holds for all a \u2264 1, we obtain that all elements of the diagonal matrix ( I+D+D2 \u2212 eD ) are non-negative.\nProof. By the block matrix multiplication rule we have [\n1\u221a 2 U 1\u221a 2 U 1\u221a 2 V \u2212 1\u221a 2 V\n][ \u03a3 0\n0 \u2212\u03a3\n] [ 1\u221a 2 U\u22a4 1\u221a 2 V\u22a4\n1\u221a 2 U\u22a4 \u2212 1\u221a 2 V\u22a4\n]\n=\n[ 1\u221a 2 U\u03a3 \u2212 1\u221a 2 U\u03a3\n1\u221a 2 V\u03a3 1\u221a 2 V\u03a3\n] [ 1\u221a 2 U\u22a4 1\u221a 2 V\u22a4\n1\u221a 2 U\u22a4 \u2212 1\u221a 2 V\u22a4\n]\n=\n[ 0 U\u03a3V\u22a4\nV\u03a3U\u22a4 0\n]\n=\n[ 0 W\nW\u22a4 0\n]\n.\nIn addition, it is easy to check that the columns of\n[ 1\u221a 2 U 1\u221a 2 U\n1\u221a 2 V \u2212 1\u221a 2 V\n]\nare orthonormal. It follows\nthat the above form is the eigendecomposition of sym(W). Therefore, for any Schatten norm: \u2016sym(W)\u2016 = 2\u2016\u03a3\u2016 = 2\u2016W\u2016, which concludes our proof."}, {"heading": "C The optimal cut in the Online Max Cut problem", "text": "We prove Lemma 2, which we restate here for convenience.\nLemma 2 restated Consider an online sequence of loss functions {\u2113t = 12 |yt \u2212 y\u0302y|}. Let\nW\u2217 = arg min W\u2208W\n\u2211\nt\n\u2113t(W (it, jt)) .\nThen W\u2217 = WA for the set A that determines the max cut in the weighted graph over [n] nodes whose weights are given by wij = \u2211\nt:(it,jt)=(i,j) yt for every (i, j).\nProof. Consider WA. For each pair (i, j) let c + ij , c \u2212 ij be the total number of iterations in which the pair (i, j) appeared in the adversarial sequence with yt = 1 or yt = \u22121 respectively. Since y\u0302t \u2208 [\u22121, 1] we can rewrite the total loss as:\n\u2211\nt\n\u2113t(WA(it, jt)) = 1\n2\n\u2211\n(i,j)\n[c+ij \u00b7 (1\u2212WA(i, j)) + c\u2212ij \u00b7 (1 +WA(i, j))]\n= 1\n2\n\u2211\n(i,j)\nWA(i, j) \u00b7 (c\u2212ij \u2212 c+ij) + CT\n= \u22121 2\n\u2211\n(i,j)\nWA(i, j) \u00b7 wij + CT\nWhere CT is a constant which is independent of WA. Hence, minimizing the above expression is equivalent to maximizing the expression:\n\u2211\n(i,j)\nWA(i, j) \u00b7 wij = 2 \u00b7 \u2211\n(i,j): WA(i,j)=1\nwij \u2212 \u2211\n(i,j)\nwij.\nSince \u2211\n(i,j)wij is a constant independent of A, the cut which maximizes this expression is the maximum cut in the weighted graph over the weights wij ."}, {"heading": "D Optimality of Decomposition for Collaborative Filtering", "text": "In this section, we prove the following theorem:\nTheorem 20. Consider the matrix W formed by taking m = \u03c4\u221a n rows of an n \u00d7 n Hadamard matrix. This matrix has \u2016W\u2016\u22c6 = \u03c4 , and any (\u03b2, \u03c4\u0303 )-decomposition for sym(W) has\n\u03b2\u03c4\u0303 \u2265 1 4 \u03c4 \u221a n.\nProof. Since the rows of W are orthogonal to each other, the m singular values of W all equal \u221a n, and thus \u2016W\u2016\u22c6 = m \u221a n = \u03c4 . Further, the SVD of W is (here, Im is the m\u00d7m identity matrix):\nW = Im( \u221a nIm)(\n1\u221a n W).\nUsing Lemma 19 the eigendecomposition of sym(W) can be written as\nsym(W) = U( \u221a nIm)U \u22a4 +V(\u2212 \u221a nIm)V \u22a4,\nwhere U = [ 1\u221a\n2 Im, 1\u221a 2n W]\u22a4 and V = [ 1\u221a 2 Im,\u2212 1\u221a2nW] \u22a4\nare p\u00d7m matrices with orthonormal columns. Let sym(W) = P \u2212 N be a (\u03b2, \u03c4\u0303 )-decomposition. Now consider the following matrices: first, define the p\u00d7 p diagonal matrix\nD :=\n[ 1\u221a 2m Im 0\n0 \u221a mn\n2 \u221a 2\u03c4\u0303 In\n]\n.\nFinally, define the p\u00d7 p positive semidefinite matrix\nY := DUU\u22a4D.\nSince U has orthonormal columns we have UU\u22a4 Ip, and so\nY DIpD = D2.\nNow, consider\nY \u2022 sym(W) = Y \u2022 (P\u2212N) \u2264 Y \u2022P (\u2235 Y,N 0, so Y \u2022N \u2265 0) \u2264 D2 \u2022P (\u2235 Y D2)\n=\nm\u2211\ni=1\n1\n2m P (i, i) +\np \u2211\ni=m+1\nmn\n8\u03c4\u0303 P (i, i)\n\u2264 1 2 \u03b2 + mn 8\u03c4\u0303 \u03c4,\nsince P (i, i) \u2264 \u03b2 for all i and Tr(P) \u2264 \u03c4 . We also have\nY \u2022 sym(W) = Tr(DUU\u22a4Dsym(W)) = Tr(UU\u22a4Dsym(W)D)\n=\n\u221a n\n4\u03c4\u0303 Tr(UU\u22a4sym(W)) (\u2235 Dsym(W)D = sym(\n\u221a n 4\u03c4\u0303 W))\n=\n\u221a n\n4\u03c4\u0303 Tr(UU\u22a4[U(\n\u221a nIm)U \u22a4 +V(\u2212 \u221a nIm)V \u22a4])\n= mn\n4\u03c4\u0303 ,\nsince U\u22a4V = 0. Putting the above two inequalities together, we have\nmn 4\u03c4\u0303 \u2264 1 2 \u03b2 + mn 8\u03c4\u0303 ,\nwhich implies that\n\u03b2\u03c4\u0303 \u2265 1 4 mn = 1 4 \u03c4 \u221a n\nas required."}, {"heading": "E Relation between (\u03b2, \u03c4)-decomposition, max-norm and trace-", "text": "norm\nIn this section, we consider m\u00d7n non-symmetric matrix W. The max-norm of W is defined to be (see Lee et al. [2010]) the value of the following SDP:\nmin t [\nY1 W W\u22a4 Y2\n]\n0\n\u2200i \u2208 [m], j \u2208 [n] : Y1(i, i), Y2(j, j) \u2264 t. (8)\nThe least possible \u03b2 in any (\u03b2, \u03c4)-decomposition for W is given by the following SDP:\nmin \u03b2 [\n0 W\nW\u22a4 0\n]\n= P\u2212N\nP, N 0 \u2200i \u2208 [m+ n] : P (i, i), N(i, i) \u2264 \u03b2. (9)\nTheorem 21. The least possible \u03b2 in any (\u03b2, \u03c4)-decomposition exactly equals half the max-norm of W.\nProof. Let t\u2217 and \u03b2\u2217 be the optima of SDPs (8) and (9) respectively. Let Y1, Y2 be the optimal solution to SDP (8), so that for all i \u2208 [m], j \u2208 [n] we have Y1(i, i), Y2(j, j) \u2264 t\u2217. Consider the matrices\nP = 1\n2\n[ Y1 W\nW\u22a4 Y2\n]\nand N = 1\n2\n[ Y1 \u2212W\n\u2212W\u22a4 Y2\n]\n.\nUsing the feasibility of Y1, Y2 and Lemma 22, we get that P,N 0. Thus this is a feasible solution to SDP (9). Hence, we conclude that t\u2217 \u2265 2\u03b2\u2217.\nNow let P, N be the optimal solution to SDP (8), so that for all i \u2208 [m + n] we have P (i, i), N(i, i) \u2264 \u03b2\u2217. Consider the blocks of P and N formed by the first m indices and the last n indices:\nP =\n[ PA PB\nPC PD\n]\nand N =\n[ NA NB\nNC ND\n]\n.\nSince N 0, by Lemma 22 the following matrix is positive semidefinite as well:\nN\u2032 := [ NA \u2212NB \u2212NC ND ]\n0.\nSo P+N\u2032 0, i.e. P+N\u2032 = [ PA +NA W\nW\u22a4 PD +ND\n]\n0.\nThus, Y1 = P A +NA and Y2 = P D +ND is a feasible solution to SDP (8). Now for all i \u2208 [m] we have Y1(i, i) \u2264 PA(i, i) + NA(i, i) \u2264 2\u03b2\u2217, and similarly for all j \u2208 [n] we have Y2(j, j) \u2264 2\u03b2\u2217. Thus, we conclude that t\u2217 \u2264 2\u03b2\u2217.\nLemma 22. Let P be a positive semidefinite matrix of order m+ n and let\nP =\n[ PA PB\nPC PD\n]\n.\nbe the block decomposition of P formed by the first m indices and the last n indices. Then the following matrix is positive semidefinite:\nP\u2032 := [ PA \u2212PB \u2212PC PD ] .\nProof. Since P 0, there are vectors vi, for all i, j \u2208 [m + n] such that P (i, j) = vi \u00b7 vj. Then consider the vectors\nwi :=\n{\nvi if i \u2208 [m] \u2212vi otherwise.\nIt is easy to check that for all i, j \u2208 [m + n] we have P \u2032(i, j) = wi \u00b7 wj. Thus, we conclude that P\u2032 0.\nFinally, we show the connection between the trace-norm and the least possible \u03c4 in any (\u03b2, \u03c4)decomposition:\nTheorem 23. The least possible \u03c4 in any (\u03b2, \u03c4)-decomposition exactly equals twice the trace-norm of W.\nProof. Let \u03c4\u2217 be the least possible value of \u03c4 in any (\u03b2, \u03c4)-decomposition, and let P,N be positive semidefinite matrices such that sym(W) = P \u2212 N and Tr(P) + Tr(N) = \u03c4\u2217. Then by triangle inequality, we have\n\u2016sym(W)\u2016\u22c6 \u2264 \u2016P\u2016\u22c6 + \u2016N\u2016\u22c6.\nSince \u2016sym(W)\u2016\u22c6 = 2\u2016W\u2016\u22c6, \u2016P\u2016\u22c6 = Tr(P), and \u2016N\u2016\u22c6 = Tr(N), we conclude that \u03c4\u2217 \u2265 2\u2016W\u2016\u22c6. Now, let\nsym(W) = \u2211\ni\n\u03bbiviv \u22a4 i\nbe the eigenvalue decomposition of sym(W). Now consider the positive semidefinite matrices\nP = \u2211\ni: \u03bbi\u22650 \u03bbiviv\n\u22a4 i and N =\n\u2211\ni: \u03bbi<0\n\u2212\u03bbiviv\u22a4i .\nClearly sym(W) = P\u2212N, and\nTr(P) + Tr(N) = \u2211\ni\n|\u03bbi| = \u2016sym(W)\u2016\u22c6 = 2\u2016W\u2016\u22c6.\nHence, \u03c4\u2217 \u2264 2\u2016W\u2016\u22c6, completing the proof."}], "references": [{"title": "Can we learn to gamble efficiently", "author": ["J. Abernethy"], "venue": "In COLT,", "citeRegEx": "Abernethy.,? \\Q2010\\E", "shortCiteRegEx": "Abernethy.", "year": 2010}, {"title": "Lower Bounds for the Helmholtz Function", "author": ["S. Golden"], "venue": "Physical Review,", "citeRegEx": "Golden.,? \\Q1965\\E", "shortCiteRegEx": "Golden.", "year": 1965}, {"title": "Regularization techniques for learning with matrices", "author": ["S. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": null, "citeRegEx": "Kakade et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2010}, {"title": "Learning hurdles for sleeping experts", "author": ["V. Kanade", "T. Steinke"], "venue": "In Innovations in Theoretical Computer Science,", "citeRegEx": "Kanade and Steinke.,? \\Q2012\\E", "shortCiteRegEx": "Kanade and Steinke.", "year": 2012}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "Machine learning,", "citeRegEx": "Kleinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2010}, {"title": "Practical large-scale optimization for max-norm regularization", "author": ["J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J.A. Tropp"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "author": ["O. Shamir", "S. Shalev-Shwartz"], "venue": "In 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Shamir and Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Shalev.Shwartz.", "year": 2011}, {"title": "Sample complexity of trace-norm", "author": ["O. Shamir", "N. Srebro"], "venue": "In COLT,", "citeRegEx": "Shamir and Srebro.,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Srebro.", "year": 2011}, {"title": "Inequality with applications in statistical mechanics", "author": ["C.J. Thompson"], "venue": "Journal of Mathematical Physics,", "citeRegEx": "Thompson.,? \\Q1965\\E", "shortCiteRegEx": "Thompson.", "year": 1965}, {"title": "Matrix exponentiated gradient updates for on-line learning and bregman projection", "author": ["K. Tsuda", "G. Ratsch", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsuda et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsuda et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al.", "startOffset": 75, "endOffset": 92}, {"referenceID": 0, "context": "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors.", "startOffset": 75, "endOffset": 117}, {"referenceID": 0, "context": "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011].", "startOffset": 75, "endOffset": 385}, {"referenceID": 0, "context": "This resolves an open problem posed in Abernethy [2010], Kleinberg et al.", "startOffset": 39, "endOffset": 56}, {"referenceID": 0, "context": "This resolves an open problem posed in Abernethy [2010], Kleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem.", "startOffset": 39, "endOffset": 81}, {"referenceID": 0, "context": "This resolves an open problem posed in Abernethy [2010], Kleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem. In fact, Kanade and Steinke [2012] tried to show computational hardness for this problem by reducing the problem of online agnostic learning of halfspaces in a restricted setting to it.", "startOffset": 39, "endOffset": 295}, {"referenceID": 6, "context": "This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011].", "startOffset": 64, "endOffset": 97}, {"referenceID": 6, "context": "This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011].", "startOffset": 64, "endOffset": 158}, {"referenceID": 2, "context": "However, applying the online mirror descent algorithmic framework with an appropriate squared-Schatten norm regularization, as described in [Kakade et al., 2010], leads to a regret bound that again scales like O( \u221a n2T ).", "startOffset": 140, "endOffset": 161}, {"referenceID": 2, "context": "As before, applying the technique of Kakade et al. [2010] leads to a regret bound that scales as \u221a \u03c42T , which leads to trivial results in the most relevant case where \u03c4 = \u0398( \u221a mn).", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "This matches an upper bound given by Shamir and Shalev-Shwartz [2011]. The question of determining the sample complexity of W in the batch setting has been posed as an open problem by Shamir (who conjectured that it scales like n1.", "startOffset": 37, "endOffset": 70}, {"referenceID": 4, "context": "This upper bound is near-optimal, as Kleinberg et al. [2010] essentially prove the following lower bound on the regret:", "startOffset": 37, "endOffset": 61}, {"referenceID": 9, "context": "The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K.", "startOffset": 68, "endOffset": 88}, {"referenceID": 9, "context": "The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K.", "startOffset": 68, "endOffset": 114}, {"referenceID": 9, "context": "Algorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness): Theorem 13.", "startOffset": 66, "endOffset": 86}, {"referenceID": 9, "context": "Algorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness): Theorem 13.", "startOffset": 66, "endOffset": 109}, {"referenceID": 9, "context": "This is based on the following theorem that is essentially proved in Tsuda et al. [2006]: This construction is tight: e.", "startOffset": 69, "endOffset": 89}], "year": 2012, "abstractText": "In several online prediction problems of recent interest the comparison class is composed of matrices with bounded entries. For example, in the online max-cut problem, the comparison class is matrices which represent cuts of a given graph and in online gambling the comparison class is matrices which represent permutations over n teams. Another important example is online collaborative filtering in which a widely used comparison class is the set of matrices with a small trace norm. In this paper we isolate a property of matrices, which we call (\u03b2, \u03c4)decomposability, and derive an efficient online learning algorithm, that enjoys a regret bound of \u00d5( \u221a \u03b2 \u03c4 T ) for all problems in which the comparison class is composed of (\u03b2, \u03c4)-decomposable matrices. By analyzing the decomposability of cut matrices, triangular matrices, and low tracenorm matrices, we derive near optimal regret bounds for online max-cut, online gambling, and online collaborative filtering. In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011].", "creator": "LaTeX with hyperref package"}}}