{"id": "1204.5810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2012", "title": "Geometry of Online Packing Linear Programs", "abstract": "The n columns arrive in random order, and the goal is to irrevocably set the corresponding decision variables when they arrive to obtain a workable solution that maximizes the expected reward. Previous (1 -\\ epsilon) competitive algorithms dictated that the right side of the LP must be Omega ((((m /\\ epsilon ^ 2) log (n /\\ epsilon)), a limit that deteriorates with the number of columns and rows. However, dependence on the number of columns in the single-line case is not required, and known lower limits for the general case are also independent of n.", "histories": [["v1", "Thu, 26 Apr 2012 02:06:44 GMT  (48kb,D)", "http://arxiv.org/abs/1204.5810v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["marco molinaro", "r ravi"], "accepted": false, "id": "1204.5810"}, "pdf": {"name": "1204.5810.pdf", "metadata": {"source": "CRF", "title": "Geometry of Online Packing Linear Programs", "authors": ["Marco Molinaro", "R. Ravi"], "emails": [], "sections": [{"heading": null, "text": "n ), a bound\nthat worsens with the number of columns and rows. However, the dependence on the number of columns is not required in the single-row case and known lower bounds for the general case are also independent of n.\nOur goal is to understand whether the dependence on n is required in the multi-row case, making it fundamentally harder than the single-row version. We refute this by exhibiting an algorithm which is (1\u2212 )-competitive as long as the right-hand sides are \u2126(m 2 2 log m ). Our techniques refine previous PAClearning based approaches which interpret the online decisions as linear classifications of the columns based on sampled dual prices. The key ingredient of our improvement comes from a non-standard covering argument together with the realization that only when the columns of the LP belong to few 1-d subspaces we can obtain small such covers; bounding the size of the cover constructed also relies on the geometry of linear classifiers. General packing LP\u2019s are handled by perturbing the input columns, which can be seen as making the learning problem more robust."}, {"heading": "1 Introduction", "text": "Traditional optimization models usually assume that the input is known a priori. However, in most applications, the data is either revealed over time or only coarse information about the input is known, often modeled in terms of a probability distribution. Consequently, much effort has been directed towards understanding the quality of solutions that can be obtained without full knowledge of the input, which led to the development of online and stochastic optimization [7, 6]. Emerging problems such as allocating advertisement slots to advertisers and yield management in the internet are of inherent online nature and have further accelerated this development [1].\nLinear programming is arguably the most important and thus well-studied optimization problem. Therefore, understanding the limitations of solving linear programs when complete data is not available is a fundamental theoretical problem with a slew of applications, including the ad allocation and yield management problems above. Indeed, a simple linear program with one uniform knapsack constraint, the Secretary Problem, was one of the first online problems to be considered and an optimal solution was already obtained by the early 60\u2019s [13, 15]. Although the single knapsack case is currently well-understood under different models of how information is revealed [4], much less is known about problems with multiple knapsacks and only recently algorithms with solution guarantees have been developed [14, 1, 10].\nThe Model. We study online packing LP\u2019s in the random permutation model. Consider a fixed but unknown LP with n columns a1, a2, . . . , an \u2208 [0, 1]m, whose associated variables are constrained to be in [0, 1], and\nar X\niv :1\n20 4.\n58 10\nv1 [\ncs .D\nS] 2\n6 A\npr 2\n01 2\nm packing constraints:\nOPT = max n\u2211 t=1 \u03c0txt\nn\u2211 t=1 atxt \u2264 B (LP)\nxt \u2208 [0, 1] .\nColumns are presented in uniformly random order, and when a column is presented we are required to irrevocably choose the value of its corresponding variable. We assume that the number of columns n is known.1 The goal is to obtain a feasible solution while maximizing its value. We use OPT to denote the optimum value of the (offline) LP.\nBy scaling down rows as necessary, we assume without loss of generality that all entries of B are the same, which we also denote with some overload of notation byB. Due to the packing nature of the problem, we also assume without loss of generality that all the \u03c0t\u2019s are non-negative and all the at\u2019s are non-zero: we can simply ignore columns which do not satisfy the first property and always set to 1 the variables associated to the remaining columns which do not satisfy the second property. Finally, we assume that the columns at\u2019s are in general position: for all p \u2208 Rm, there are at most m different t \u2208 [n] such that \u03c0t = pat. Notice that perturbing the input randomly by a tiny amount achieves this property with probability one, while the effect of the perturbation is absorbed in our approximation guarantees [11, 1].\nRelated work. The random permutation model has grown in popularity [16, 11, 4] since it avoids strong lower bounds of the pessimistic adversarial-order model [8] while still capturing the lack of total information a priori. Different online problems have already been studied in this model, including bin-packing [19], matchings [18, 16], the AdWords Problem [11] and different generalizations of the Secretary Problem [4, 2, 5, 24, 17]. Closest to our work are packing problems with a single knapsack constraint. In [20], Kleinberg considered the B-Choice Secretary Problem, where the goal is to select at most B items coming online in random order to maximize profit. The author presented an algorithm with competitive ratio 1 \u2212 O(1/ \u221a B) and showed that 1\u2212 \u2126(1/ \u221a B) is best possible. Generalizing the B-Choice Secretary Problem, Babaioff et\nal. [3] considered the online knapsack problem and presented a (1/10e)-competitive algorithm. Notice that in both cases the competitive ratio does not depend on n.\nDespite all these works, the first result for more general online packing LP\u2019s here was only recently obtained by Feldman et al. [14] and Agrawal et al. [1]. The first paper presents an algorithm that obtains with high probability a solution of value at least (1 \u2212 )OPT whenever B \u2265 \u2126(m logn 3 ) and OPT \u2265 \u2126(\u03c0maxm logn ), where \u03c0max is the largest profit. In the second paper, the authors present an algorithm which obtains a solution of expected value at least (1 \u2212 )OPT under the weaker assumptions B \u2265 \u2126 ( m 2 log n )\nor OPT \u2265 \u2126 ( \u03c0maxm2 2 log n ) . One other way of stating this result is that the algorithm obtains a solution\nwith competitive ratio 1 \u2212 O( \u221a\nm log(n) logB B ); notice that the guarantee degrades as n increases. The cur-\nrent lower bound on B to allow (1 \u2212 )-competitive algorithms is B \u2265 logm 2\n, also presented in [1]. We remark that these algorithms actually work for more general allocation problems, where a set of columns representing various options arrive at each step and the solution may choose at most one of the options.\n1Actually knowing n up to (1\u00b1 ) factor is enough. This assumption is required to allow algorithms with non-trivial competitive ratio [11].\nBoth of the above algorithms use a connection between solving the online LP and PAC-learning [9] a linear classification of its columns, which was initiated by Devanur and Hayes [11] in the context of the AdWords problem. Here we further explore this connection and our improved bounds can be seen as a consequence of making the learning algorithm more robust by suitably changing the input LP. Robustness is a topic well-studied in learning theory [12, 21], although existing results do not seem to apply directly to our problem. We remark that a component of robustness more closely related to the standard PAC-learning literature is used in [11].\nIn recent work, Devanur et al. [10] consider the weaker i.i.d. model for the general allocation problem. While in the random permutation model one can think that columns are sampled without replacement, in the i.i.d. model they are sampled with replacement. Making use of the independence between samples, Devanur et al. substantially improve requirement on B to \u2126( log(m/ )\n2 ) while showing that the lower bound \u2126 ( logm 2 ) still holds in this model. We remark, however, that these models can present very different behaviors: as a simple example, consider an LP with n columns, m = 1 constraints and budget B = 1, where only one of the columns has \u03c01 = a1 = 1 and all others have \u03c0i = ai = 0; in the random permutation model the expected value of the optimal solution is 1, while in the i.i.d. model this value is 1\u2212 (1\u2212 1/n)n \u2192 1\u2212 1/e. The competitiveness of the algorithm of [10] under the permutation model is still unknown and was left as an open problem by the authors. Our results. Our focus is to understand how large B is required to be in order to allow (1\u2212 )-competitive algorithms. In particular, the requirements for B in the above algorithms degrade as the number of columns in the LP increases, while the the lower bound does not. With the trend of handling LP\u2019s with larger number of columns (e.g. columns correspond to the keywords in the ad allocation problem, which in turn correspond to visits of a search engine\u2019s webpage), this gap is very unsatisfactory from a practical point of view. Furthermore, given that guarantees for the single knapsack case do not depend on the number of columns, it is important to understand if the multi-knapsack case is fundamentally more difficult. In this work, we give a precise indication of why the latter problem was resistant to arguments used in the single knapsack case, and overcome this difficulty to exhibit an algorithm with dimension-independent guarantee.\nWe show that a modification of the DPA algorithm from [1] that we call Robust DPA obtains a (1\u2212 )- competitive solution for online packing LP\u2019s withm constraints in the random permutation model whenever B \u2265 \u2126(m2 2 log m ). Another way of stating this result is that the algorithm has competitive ratio 1 \u2212 O(m \u221a logB/ \u221a B). Contrasting to previous results, our guarantee does not depend on n and in the case\nm = 1 matches the bounds for the B-Choice Secretary Problem up to lower order terms. We finally remark that we can replace the requirement B \u2265 \u2126(m2 2 log m ) by OPT \u2265 \u2126( \u03c0maxm3 2 log m ) exactly as done in Section 5.1 of [1]. High-level outline. As mentioned before, we use the connection between solving an online LP and PAClearning a good linear classification of its columns; in order to obtain the improved guarantee, we focus on tightening the bounds for the generalization error of the learning problem. More precisely, solving the LP can be seen as classifying the columns into 0/1, which corresponds to setting their associated variable to 0/1. Consider a family X \u2286 {0, 1}n of linear classifications of the columns. Our algorithms sample a set S of columns and learn a classification xS \u2208 X which is \u201cgood\u201d for the columns S (i.e., obtains large proportional revenue while not filling up the proportionally scaled budget too much). The goal is to upper bound the probability that xS is not good for the whole LP; this is typically done via a union bound over the classifications in X [11, 1].\nTo obtain improved guarantees, we refine this bound using an argument akin to covering: we consider witnesses (Section 2.2), which are representatives of groups of \u2018similar\u2019 bad classifications that can be used to bound the probability that any classification in the group is learned; for that we need to use a non-standard\nmeasure of similarity between classifications which is based on the budget of the LP. The problem is that, when the columns (\u03c0t, at)\u2019s do not lie in a two-dimensional subspace of Rm, the set X may contain a large number of mutually dissimilar bad classifications; this is a roadblock for obtaining a small set of witnesses. In stark contrast, when these columns do lie in a two-dimensional subspace (e.g., m = 1), these classifications have a much nicer structure which indeed allows a small set of witnesses. This indicates that the latter learning problem is intrinsically more robust than the former, which seem to precisely capture the increased difficulty in obtained good bounds for the multi-row case.\nMotivated by this discussion we first consider LP\u2019s whose columns at\u2019s lie in few one-dimensional subspaces (Section 2). For each of these subspaces, we are able to approximate the classifications induced in the columns lying in the subspace by considering a small subset of the induced classifications; patching together these partial classifications gives us a witness set for X . However, this strategy as stated does not make use of the fact that the subspaces are embedded in an m-dimensional space, and hence leads to large witness sets. By establishing a connection between the \u201cuseful\u201d patching possibilities with faces of a hyperplane arrangement in Rm (Lemma 2.12), we are able to make use of the dimension of the host space and exhibit witness sets of much smaller sizes, which leads to improved bounds.\nFor a general packing LP, we perturb the columns at\u2019s to make them lie in few one-dimensional subspaces that form an \u2018 -net\u2019 of the space, while not altering the feasibility and optimality of the LP by more than a (1\u00b1 ) factor (Section 3). Finally, we tighten the bound by using the idea of periodically recomputing the classification, following [1] (Section 4)."}, {"heading": "2 OTP for almost 1-dim columns", "text": "In this section we describe and analyze the algorithm OTP (One-Time Pricing) over LP\u2019s whose columns are contained in few 1-dimensional subspaces of Rm. The overall goal is to find an appropriate dual (perhaps infeasible) solution p for (LP) and use it to classify the columns of the LP. More precisely, given p \u2208 Rm, we define x(p)t = 1 if \u03c0t > pat and x(p)t = 0 otherwise. Thus, x(p) is the result of classifying the columns (\u03c0t, at)\u2019s with the homogeneous hyperplane in Rm+1 with normal (\u22121, p). The motivation behind this classification is that it selects the columns which have positive reduced cost with respect to the dual solution p, or alternatively, it solves to optimality the Lagrangian relaxation using p as multipliers.\nSampling LP\u2019s. In order to obtain a good dual solution p we use the (random) LP consisting on the first s columns of (LP) with appropriately scaled right-hand side.\nmax s\u2211 t=1 \u03c0\u03c3(t)x\u03c3(t) ((s, \u03b4)-LP)\ns\u2211 t=1 a\u03c3(t)x\u03c3(t) \u2264 s n \u03b4B\nx\u03c3(t) \u2208 [0, 1] t = 1, . . . , s.\nmin s\nn \u03b4B m\u2211 i=1 pi + s\u2211 t=1 \u03b1\u03c3(t) ((s, \u03b4)-Dual)\npa\u03c3(t) + \u03b1\u03c3(t) \u2265 \u03c0\u03c3(t) t = 1, . . . , s p \u2265 0 \u03b1 \u2265 0.\nHere \u03c3 denotes the random permutation of the columns of the LP. We use OPT(s, \u03b4) to denote the optimal value of (s, \u03b4)-LP and OPT(s) to denote the optimal value of (s, 1)-LP.\nThe static pricing algorithm OTP of [1] can then be described as follows.2\n1. Wait for the first n columns of the LP (indexed by \u03c3(1), \u03c3(2), . . . , \u03c3( n)) and solve ( n, 1\u2212 )-Dual. 2To simplify the exposition, we assume that n is an integer.\nLet (p, \u03b1) be the obtained dual optimal solution.\n2. Use the classification given by p as above by setting x\u03c3(t) = x(p)\u03c3(t) for t = n + 1, n + 2, . . . for as long as the solution obtained remains valid. From this point on set all further variables to zero.\nNote that by definition this algorithm outputs a feasible solution with probability one. Our goal is then to analyze the quality of the solution produced, ultimately leading to the following theorem.\nTheorem 2.1 Fix \u2208 (0, 1]. Suppose that there areK \u2265 m 1-dim subspaces of Rm containing the columns at\u2019s and that B \u2265 \u2126 ( m 3 log K ) . Then algorithm OTP returns a feasible solution with expected value at least (1\u2212 5 )OPT.\nLet S = {\u03c3(1), . . . , \u03c3( n)} be the (random) index set of the columns sampled by OTP. We use pS to denote the optimal dual solution obtained by OTP; notice that pS is completely determined by S. To simplify the notation, we also use xS to denote x(pS).\nNotice that, for all the scenarios where xS is feasible, the solution returned by OTP is identical to xS with its components xS\u03c3(1), . . . , x S \u03c3( n) set to zero. Given this observation and the fact that E[ \u2211 t\u2264 n \u03c0\u03c3(t)x S \u03c3(t)] \u2264\nOPT, one can prove that the following lemma implies Theorem 2.1.\nLemma 2.2 Fix \u2208 (0, 1]. Suppose that there are K \u2265 m 1-dim subspaces of Rm containing the columns at\u2019s and that B \u2265 \u2126 ( m 3 log K ) . Then with probability at least (1 \u2212 ), xS is a feasible solution for (LP) with value at least (1\u2212 3 )OPT."}, {"heading": "2.1 Connection to PAC learning", "text": "We assume from now on that B \u2265 \u2126(m 3 log K ). Let X = {x(p) : p \u2208 R m +} \u2286 {0, 1}n denote the set of all possible linear classifications of the LP columns which can be generated by OTP. With slight overload in the notation, we identify a vector x \u2208 {0, 1}n with the subset of [n] corresponding to its support.\nDefinition 2.3 (Bad solution) Given a scenario, we say that xS is bad if it does not satisfy the properties of Lemma 2.2, namely xS is either infeasible or has value less than (1 \u2212 3 )OPT. We say that xS is good otherwise.\nAs noted in previous work, since our decisions are made based on reduced costs it suffices to analyze the budget occupation (or complementary slackness) of the solution in order to understand its value. To make this precise, given x \u2208 {0, 1}n let ai(x) = \u2211 t\u2208x a t i be its occupation of the ith budget and let a S i (x) = 1\n\u2211 t\u2208x\u2229S a t i be its appropriately scaled occupation of ith budget in the sampled LP (recall |S| = n).\nLemma 2.4 Consider a scenario where xS satisfies: (i) for all i \u2208 [m], ai(xS) \u2264 B and (ii) for all i \u2208 [m] with pSi > 0, ai(x S) \u2265 (1\u2212 3 )B. Then xS is good.\nMoreover, since we are making decisions based on the optimal reduced cost for the sampled LP, our solution satisfies the above properties for the sampled LP.\nLemma 2.5 In every scenario, xS satisfies the following: (i) for all i \u2208 [m], aSi (xS) \u2264 (1 \u2212 )B and (ii) for every i \u2208 [m] with pSi > 0, aSi (xS) \u2265 (1\u2212 2 )B.\nGiven that ai(x) = E[aSi (x)] for all x, the idea is to use concentration inequalities to argue that the conditions in Lemma 2.4 hold with good probability. Although concentration of aSi (x) for fixed x can be achieved via Chernoff-type bounds, the quantity aSi (x\nS) has undesired correlations; obtaining an effective bound is the main technical contribution of this paper.\nDefinition 2.6 (Badly learnable) For a given scenario, we say that x \u2208 X can be badly learned for budget i if either (i) aSi (x) \u2264 (1\u2212 )B and ai(x) > B or (ii) aSi (x) \u2265 (1\u2212 2 )B and ai(x) < (1\u2212 3 )B.\nEssentially these are the classifications which look good for the sampled ( n, 1\u2212 )-LP but are actually bad for (LP). Putting Lemmas 2.4 and 2.5 together and unraveling the definitions gives that\nPr ( xS is bad ) \u2264 Pr  \u2228 i\u2208[m],x\u2208X x can be badly learned for budget i  . Notice that the right-hand side of this inequality does not depend on xS , it is only a function of how skewed aSi (x) is as compared to its expectation ai(x).\nUsually the right-hand side in the previous equation is upper bounded by taking a union bound over all its terms [1]. Unfortunately this is too wasteful: when x and x\u2032 are \u201csimilar\u201d there is a large overlap between the scenarios where aSi (x) is skewed and those where a S i (x \u2032) is skewed. In order to obtain improved guarantees, we introduce in the next section a new way of bounding the right-hand side of the above expression."}, {"heading": "2.2 Similarity via witnesses", "text": "First, we partition the classifications which can be badly learned for budget i into two sets, depending on why they are bad: for i \u2208 [m], let X+i = {x \u2208 X : ai(x) > B} and X \u2212 i = {x \u2208 X : ai(x) < (1\u2212 3 )B}. In order to simplify the notation, given a set x we define skewmi( , x) to be the event that aSi (x) \u2264 (1\u2212 )B and skewpi( , x) to be the event that a S i (x) \u2265 (1 \u2212 2 )B. Notice that if x \u2208 X + i , then skewmi( , x) is the event that aSi (x) is significantly smaller than its expectation (skewed in the minus direction), while for x \u2208 X\u2212i skewpi( , x) is the event that aSi (x) is significantly larger than its expectation (skewed in the plus direction). These definitions directly give the equivalence\nPr  \u2228 i,x\u2208X x can be badly learned for budget i  = Pr  \u2228 i,x\u2208X+i skewmi( , x) \u2228 \u2228 i,x\u2208X\u2212i skewpi( , x)  . In order to introduce the concept of witnesses, consider two sets x, x\u2032, say, in X+i . Take a subset w \u2286 x \u2229 x\u2032; the main observation is that, since at \u2265 0 for all t, for all scenarios we have aSi (w) \u2264 aSi (x) and aSi (w) \u2264 aSi (x\u2032). In particular, the event skewmi( , x) \u2228 skewmi( , x\u2032) is contained in skewm( , w). The set w serves as a witness for scenarios which are skewed for either x or x\u2032; if additionally ai(w) reasonably larger than (1 \u2212 )B, we can then use concentration inequalities over skewmi( , w) in order to bound probability of skewm( , x)\u2228 skewm( , x\u2032). This ability of bounding multiple terms of the right-hand side of (2.2) simultaneously is what gives an improvement over the naive union bound.\nDefinition 2.7 (Witness) We say that W+i is a witness set for X + i if: (i) for all w \u2208 W + i , ai(w) \u2265 (1 \u2212\n/2)B and (ii) for all x \u2208 X+i there is w \u2208 W + i contained in x. Similarly, we say thatW \u2212 i is a witness set\nfor X\u2212i if: (i) for all w \u2208 W \u2212 i , ai(w) \u2264 (1\u2212 3 /2)B and (ii) for all x \u2208 X \u2212 i there is w \u2208 W \u2212 i containing x.\nAs indicated by the previous discussion, given witness setsW+i andW \u2212 i for X + i and X \u2212 i , we directly\nget the bound\nPr  \u2228 i,x\u2208X+i skewm( , x) \u2228 \u2228 i,x\u2208X\u2212i skewp( , x)  \u2264 Pr  \u2228 i,w\u2208W+i skewm( , w) \u2228 \u2228 i,w\u2208W\u2212i skewp( , w)  . (2.1)\nPutting together the last three displayed equations and using Chernoff-type bounds, we can get an upper estimate on the probability that xS is bad in terms of the size of witnesses sets.\nLemma 2.8 Suppose that, for all i \u2208 [m], there are witness sets for X+i and X \u2212 i of size at most M . Then Pr(xS is bad ) \u2264 8mM exp ( \u2212 3B33 ) .\nOne natural choice of a witness set for, say, X+i is the collection of all of its minimal sets; unfortunately this may not give a witness set of small enough size. But notice that a witness set need not be a subset of X+i (or even X ). Allowing elements outside X+i gives the flexibility of obtaining witnesses which are associated to multiple \u201csimilar\u201d minimal elements of X+i , which is effective in reducing the size of witness sets."}, {"heading": "2.3 Small witness sets for almost 1-dim columns", "text": "Given the previous lemma, our task is to find small witness sets. Unfortunately, when the (\u03c0t, at)\u2019s lie in a space of dimension at least 3, X+i and X \u2212 i may contain many (\u2126(n)) disjoint sets (see Figure 5.1), which shows that in general we cannot find small witness sets directly. This sharply contrasts with the case where the (\u03c0t, at)\u2019s lie in a 2-dimensional subspace of Rm+1, where one can show that X is a union of 2 chains with respect to inclusion. In the special case where the at\u2019s lie in a 1-dimensional subspace of Rm, we show that X is actually a single chain (Lemma 2.10) and therefore we can takeW+i as the minimal set of X + i and W\u2212i as the maximal set of X \u2212 i .\nDue to the above observations, we focus on LP\u2019s whose at\u2019s lie in few 1-dimensional subspaces. In this case, X+i and X \u2212 i are sufficiently well-behaved so that we can find small (independent of n) witness sets.\nLemma 2.9 Suppose that there are K \u2265 m 1-dimensional subspaces of Rm which contain the at\u2019s. Then there are witness sets for X+i and X \u2212 i of size at most (O( K log K )) m.\nAssuming the hypothesis of the lemma, partition the index set [n] into C1, C2, . . . , CK such that for all j \u2208 [K] the columns {at}t\u2208Cj belong to the same 1-dimensional subspace. Equivalently, for each j \u2208 [K] there is a vector cj of `\u221e-norm 1 such that for all t \u2208 Cj we have at = \u2016at\u2016\u221ecj . An important observation is that now we can order the columns (locally) by the ratio of profit over budget occupation: without loss of generality assume that for all j \u2208 [K] and t, t\u2032 \u2208 Cj with t < t\u2032, we have \u03c0t\u2016at\u2016\u221e \u2265 \u03c0t\u2032 \u2016at\u2032\u2016\u221e .3\nGiven a classification x, we use x|Cj to denote its projection onto the coordinates in Cj ; so x|Cj is the induced classification on columns with indices in Cj . Similarly, we define X|Cj = {x|Cj : x \u2208 X} as the set of all classifications induced in the columns in Cj . The most important structure that we get from working with 1-d subspaces, which is implied by the local order of the columns, is the following.\nLemma 2.10 For each j \u2208 [K], the sets in X|Cj are prefixes of Cj . 3Notice that this ratio is well-defined since by assumption at 6= 0 for all t \u2208 [n].\nTo simplify the notation fix i \u2208 [m] for the rest of this section, so we aim at providing witness sets forX+i and X\u2212i . The idea is to group the classifications according to their budget occupation caused by the different column classes Cj\u2019s. To make this formal, start by covering the interval [0, B +m] with intervals {I`}`\u2208L, where I0 = [0, B4K ) and I` = [ B 4K (1 + 4) `\u22121, B4K (1 + 4) `) for ` > 0 and L = {0, . . . , dlog1+ /4 8K e} (note that since B \u2265 m, we have B + m \u2264 2B). Define B`i,j as the set of partial classifications y \u2208 X |Cj whose budget occupation ai(y) lies in the interval I`. For v \u2208 LK define the family of classifications Bvi = {(y1, y2, . . . , yK) : yj \u2208 B vj i,j}. The Bvi \u2019s then provide the desired grouping of the classifications. Note that the Bvi \u2019s may include classifications not in X and may not include classifications in X which have occupation ai(.) greater than B +m.\nNow consider a non-empty Bvi . Let wvi be the inclusion-wise smallest element in Bvi . Notice that such unique smallest element exists: since X|Cj is a chain, so is B vj i,j , and hence w v i is the product (over j) of the smallest elements in the sets {Bvji,j}j . Similarly, let wvi denote the largest element in Bvi . Intuitively, wvi and wvi will serve as witnesses for all the sets in Bvi .\nFinally, define the witness sets by adding the wvi and w v i \u2019s of appropriate size corresponding to mean-\ningful Bvi \u2019s: set W + i = {wvi : v \u2208 LK ,Bvi \u2229 X 6= \u2205, ai(wvi ) \u2265 (1 \u2212 /2)B} and W \u2212 i = {wvi : v \u2208 LK ,Bvi \u2229 X 6= \u2205, ai(wvi ) \u2264 (1\u2212 3 /2)B}. It is not too difficult to see that, say,W+i is a witness set for X + i : If x \u2208 X + i belongs to some Bvi , then wvi belongs toW + i and is easily shown to be a witness for x. However, if x does not belong to any Bvi , by having too large ai(x), the idea is to find x\u2032 \u2286 x which belongs to some Bvi and to X , and then use wvi as a witness for x. We note that considering Bvi \u2019s for side lengths at most B +m and only adding witnesses for Bvi \u2019s which intersect X are crucially used for bounding the size ofW + i andW \u2212 i .\nLemma 2.11 The setsW+i andW \u2212 i are witness sets for X + i and X \u2212 i .\nBounding the size of witness sets. Clearly the witness setsW+i andW \u2212 i have size at most |L|K . Although this size is independent of n, it is still unnecessarily large since it only uses locally (for each Cj) the fact that X consists of linear classifications; in particular, it does not use the dimension of the ambient space Rm. Now we sketch the argument for an improved bound, and details are provided in the appendix.\nFirst notice that the partial classification x(p)|Cj is completely defined by the value pcj . Thus, if J \u2286 [K] is such that the directions {cj}j\u2208J form a basis of Rm then knowing pcj for all j \u2208 J completely determines the whole classification x(p). Similarly, if we know that x(p)|Cj \u2208 B vj i for all j \u2208 J , then for each j /\u2208 J we should have fewer possible Bvji \u2019s where the partial classification x(p)|Cj can belong to; this indicates that some of the sets {Bvi }v\u2208LK do not contain any element from X , which implies a reduced size for the witness sets.\nIn order to capture this idea, we focus on the space of dual vectors p and define the sets P `j = {p \u2208 Rm+ : x(p)|Cj \u2208 B`i,j} and P v = {p \u2208 Rm+ : x(p) \u2208 Bvi }. Notice that P v = \u2229jP vj j and that Bvi is empty iff P v is. The main step is to show that each P `j is a polyhedron with \u201cfew\u201d facets, which uses the definition of x(p) and Lemma 2.10. We then consider the arrangement of the hyperplanes which are facet-defining for the P `j \u2019s and conclude that the P\nv\u2019s are given by unions of the cells in this arrangement; classical bounds on the number of cells in a hyperplane arrangement in Rm then allow us to upper bound the number of nonempty P v\u2019s. This gives the following.\nLemma 2.12 At most (O(K log K )) m of the Bvi \u2019s contain an element from X .\nThis lemma implies thatW+i andW \u2212 i each has size at most (O( K log K )) m, which then proves Lemma 2.9. Finally, applying Lemma 2.8 we conclude the proof of Lemma 2.2."}, {"heading": "3 Robust OTP", "text": "In this section we consider (LP) with columns that may not belong to few 1-dimensional subspaces. Given the results of the previous section we would like to perturb the columns of this LP so that it belongs to few 1-dim subspaces, and such that an approximate solution for this perturbed LP is also an approximate solution for the original one. More precisely, we obtain a set of vectorsQ \u2286 Rm and transform each column at into a column a\u0303t which is a scaling of a vector in Q, and we let the rewards \u03c0t remain unchanged. The crucial observation is that the solutions of an LP are robust to slight changes in the the constraint matrix.\nLemma 3.1 Consider real numbers \u03c01, . . . , \u03c0n and vectors a1, . . . , an and a\u03031, . . . , a\u0303n in Rm+ such that \u2016a\u0303t \u2212 at\u2016\u221e \u2264 m+1\u2016a\nt\u2016\u221e. If x is an -approximate solution for (LP) with columns (\u03c0t, a\u0303t) and right-hand side (1\u2212 )B, then x is a 2 -approximate solution for the LP (LP).\nPerturbing the columns. To simplify the notation, set \u03b4 = m+1 ; for simplicity of exposition we assume that 1/\u03b4 is integral. When constructing Q we want the rays spanned by the each of its vectors to be \u201cuniform\u201d over Rm+ . Using `\u221e as normalization, let Q be a \u03b4-net of the unit `\u221e sphere, namely let Q be the vectors in {0, \u03b4, 2\u03b4, 3\u03b4, . . . , 1}m which have `\u221e norm 1. Note that |Q| = (O(m ))\nm. Given a vector at \u2208 Rm we let a\u0303t = \u2016at\u2016\u221eqt, where qt is the vector in Q closest (in `\u221e) to a t\n\u2016at\u2016\u221e . By definition of Q, for every vector v \u2208 Rm with \u2016v\u2016\u221e = 1 there is a vector q \u2208 Q with \u2016v \u2212 q\u2016\u221e \u2264 \u03b4. It then follows from positive homogeneity of norms that the a\u0303t\u2019s satisfy the property required in Lemma 3.1: \u2016at \u2212 a\u0303t\u2016\u221e \u2264 \u03b4\u2016at\u2016\u221e. Algorithm Robust OTP. One way to think of the algorithm Robust OTP is that it works in two phases. First, it transforms the vectors at into a\u0303t as described above. Then it returns the solution obtained by running the algorithm OTP over the LP with columns (\u03c0t, a\u0303t) and right-hand side (1\u2212 )B. Notice that this algorithm can indeed be implemented to run in an online fashion.\nPutting together the discussion in the previous paragraphs and the guarantee of OTP for almost 1-dim columns given by Theorem 2.1 with K = |Q| = (O(m )) m, we obtain the following theorem. Theorem 3.2 Fix \u2208 (0, 1] and suppose B \u2265 \u2126 ( m2 3 log m ) . Then algorithm Robust OTP returns a\nsolution to the online (LP) with expected value at least (1\u2212 10 )OPT."}, {"heading": "4 Robust DPA", "text": "In this section we describe our final algorithm, which has an improved dependence on 1/ . Following [1], the idea is to update the dual vector used in the classification as new columns arrive: we use the first 2i n columns to classify columns 2i n + 1, . . . , 2i+1 n. This leads to improved generalization bounds, which in turn give the reduced dependence on 1/ . The algorithm Robust DPA (as the algorithm DPA) can be seen as a combination of solutions to multiple sampled LP\u2019s, obtained via a modification of OTP denoted by (s, \u03b4)-OTP.\nAlgorithm (s, \u03b4)-OTP. This algorithm aims at solving the program (2s, 1)-LP and can be described as follows: it finds an optimal dual solution (p, \u03b1) for (s, (1 \u2212 \u03b4))-LP and sets x\u03c3(t) = x(p)\u03c3(t) for t = s+ 1, s+ 2, . . . , t\u2032 \u2264 2s such that t\u2032 is the maximum one guaranteeing \u22112s t=s+1 a\n\u03c3(t)x\u03c3(t) \u2264 snB. The analysis of (s, \u03b4)-OTP is similar to the one employed for OTP. The main difference is that this\nalgorithm tries to approximate the value of the random LP (2s, 1)-LP. This requires a partition of the bad\nclassifications which is more refined than simply splitting into X+i and X \u2212 i , and witness sets need to be redefined appropriately. Nonetheless, using these ideas we can prove the following guarantee for (s, \u03b4)OTP. Again let S = {\u03c3(1), \u03c3(2), . . . , \u03c3(s)} be the random index set of the first s columns of the LP, let T = {\u03c3(s+ 1), \u03c3(s+ 2), . . . , \u03c3(2s)} and U = S \u222a T . We use \u03c0U to denote the vector (\u03c0t)t\u2208U .\nLemma 4.1 Suppose that there are K \u2265 m 1-dim subspaces of Rm containing the columns at\u2019s. Fix an integer s and a real number \u03b4 \u2208 (0, 1/10) such that \u03b42sBn \u2265 \u2126(m ln K \u03b4 ). Then algorithm (s, \u03b4)-OTP returns a solution x satisfying aTi (x) \u2264 B for all i \u2208 [m] with probability 1 and with expected value E[\u03c0Ux] \u2265 (1\u2212 3\u03b4)E[OPT(2s)]\u2212 E[OPT(s)]\u2212 \u03b42OPT.\nAlgorithm Robust DPA. In order to simplify the description of the algorithm, we assume in this section that log(1/ ) is an integer.\nAgain the algorithm Robust DPA can be thought as acting in two phases. In the first phase it converts the vectors at into a\u0303t, just as in the first phase of Robust OTP. In the second phase, for i = 0, . . . , log(1/ )\u22121, it runs ( 2in, \u221a /2i)-OTP over (LP) with columns (\u03c0t, a\u0303t) and right-hand side (1\u2212 )B to obtain the solution\nxi. The algorithm finally returns the solution x consisting of the \u201cunion\u201d of xi\u2019s: x = \u2211\ni x i.\nNote that the second phase corresponds exactly to using the first 2in columns to classify the columns 2in+1, . . . , 2i+1n. This relative increase in the size of the training data for each learning problem allow us to reduce the dependence of B on in each of the iterations, while the error from all the iterations telescope and are still bounded as before. Furthermore, notice that Robust DPA can be implemented to run online.\nThe analysis of Robust DPA reduces to that of (s, \u03b4)-OTP. That is, using the definition of the parameters of (s, \u03b4)-OTP used in Robust DPA and Lemma 4.1, it is routine to check that the algorithm produces a feasible solution which has expected value (1\u2212 )OPT. This is formally stated in the following theorem.\nTheorem 4.2 Fix \u2208 (0, 1/100) and suppose that B \u2265 \u2126(m2 2 ln m ). Then the algorithm Robust DPA returns a solution to the online LP (LP) with expected value at least (1\u2212 50 )OPT."}, {"heading": "5 Open problems", "text": "A very interesting open question is whether the techniques introduced in this work can be used to obtain improved algorithms for generalized allocation problems [14]. The difficulty in this problem is that the classifications of the columns are not linear anymore; they essentially come from a conjunction of linear classifiers. Given this additional flexibility, having the columns in few 1-dimensional subspaces does not seem to impose strong enough properties in the classifications. It would be interesting to find the appropriate geometric structure of the columns in this case.\nOf course a direct open question is to improve the lower or upper bound on the dependence on the right-hand side B to obtain (1 \u2212 )-competitive algorithms. One possibility is to investigate how much the techniques presented here can be pushed and what are their limitations. Another possibility is to analyze the performance of the algorithm from [10] under the random permutation model."}, {"heading": "A Bernstein inequality for sampling without replacement", "text": "Lemma A.1 (Theorem 2.14.19 in [25]) Let Y = {Y1, . . . , Yn} be a set of real numbers in the interval [0, 1] and let 0 < < 1. Let S be a random subset of Y of size s and let YS = \u2211 i\u2208S Yi. Setting \u00b5 = 1 n \u2211 i Yi and\n\u03c32 = 1n \u2211 i(Yi \u2212 \u00b5)2, we have that for every \u03c4 > 0\nPr(|YS \u2212 s\u00b5| \u2265 \u03c4) \u2264 2 exp ( \u2212 \u03c4 2\n2s\u03c32 + \u03c4 ) Notice that, since the Yi\u2019s belong to the interval [0, 1], we can upper bound the variance by the mean as\nfollows:\n\u03c32 \u2264 1 n \u2211 i |Yi \u2212 \u00b5| \u2264 1 n (\u2211 i |Yi|+ \u2211 i |\u00b5| ) = 2\u00b5.\nThis gives the following corollary.\nCorollary A.2 Consider the conditions of the previous lemma. Then for all \u03c4 > 0\nPr(|YS \u2212 s\u00b5| \u2265 \u03c4) \u2264 2 exp ( \u2212 \u03c4 2\n4s\u00b5+ \u03c4\n) ."}, {"heading": "B Proof of Lemmas 2.4 and 2.5", "text": "Proof of Lemma 2.4: Fix a scenario \u03c3 for the duration of the proof. By assumption xS is feasible for (LP), so it suffices to show that it attains value at least (1 \u2212 3 )OPT. For that, consider (LP) with a modified right-hand side:\nmax n\u2211 t=1 \u03c0txt\nn\u2211 t=1 atixt \u2264 ai(xS) \u2200i \u2208 [m] (modLP)\nx \u2208 [0, 1]n.\nConsider the Lagrangian relaxation L(p, x) = \u2211 n t=1 \u03c0txt \u2212 \u2211m i=1 pi( \u2211 n t=1 a t ixt \u2212 ai(xS)). Notice that xS is an optimal solution for maxx\u2208[0,1]n L(pS , x), which is at least the OPT(modLP), the optimum value of LP (modLP). Since xS is clearly feasible for (modLP), it follows that xS is an optimal solution for the latter.\nNow let x\u2217 be an optimal solution for (LP). Since ai(xS) \u2265 (1 \u2212 3 )B for all i, and since at \u2265 0 for all t, it follows that (1 \u2212 3 )x\u2217 is feasible for (modLP). By linearity of the objective function we get that OPT(modLP) \u2265 (1\u2212 3 ) \u2211n t=1 \u03c0tx \u2217 t = (1\u2212 3 )OPT and the result follows. Proof of Lemma 2.5: Fix a scenario \u03c3 for the duration of the proof. Let x\u2217 be an optimal solution for ( n, (1\u2212 ))-LP in complementary slackness with pS . If pSat > \u03c0t, the corresponding constraint in the dual is loose and by complementary slackness we get x\u2217t = 0. If p\nSat < \u03c0t, then for dual feasibility we have \u03b1\u2217t > 0 and by complementary slackness we have x \u2217 t = 1.\nFrom the definition of xS we get that xS \u2264 x\u2217 and, since the at\u2019s are non-negative, the feasibility of x\u2217 implies that aSi (x\nS) \u2264 (1 \u2212 )B for all i \u2208 [m]. Moreover, from our assumption that the input is in general position we get that there are at most m values of t such that pSat = \u03c0t. Therefore, xS and x\u2217 differ in at most m positions and from primal complementary slackness we get that whenever pS > 0, aSi (x\nS) \u2265 aSi (x\u2217)\u2212m = (1\u2212 )B \u2212m \u2265 (1\u2212 2 )B, where the last inequality follows from the fact that B \u2265 1 . This concludes the proof of the lemma."}, {"heading": "C Proof of Lemma 2.8", "text": "The following simple inequalities will be helpful.\nObservation C.1 For , \u03b1, \u03b2 \u2265 0, 1\u2212\u03b1 1+\u03b2 \u2265 1\u2212 (\u03b1+ \u03b2) and 1\u2212\u03b1 1\u2212\u03b2 \u2264 1\u2212 (\u03b1\u2212 \u03b2) .\nCombining equations (2.1), (2.2) and (2.1) and union bounding over all terms in the disjunction, we have that\nPr ( xS is bad ) \u2264 \u2211 i,w\u2208W+i Pr (skewm( , w)) + \u2211 i,w\u2208W\u2212i Pr (skewp( , w)) .\nThus, it suffices to show that for all w \u2208 W+i (respectively w \u2208 W \u2212 i ), the event skewm( , w) (resp. skewp( , w)) occurs with probability at most 2 exp ( \u2212 3B33 ) .\nTake w \u2208 W+i . By definition of this set, ai(w) \u2265 (1 \u2212 2)B, so the event skewm( , w) is contained in the event that aSi (w) \u2264 (1 \u2212 )ai(w)/(1 \u2212 2), which is contained in the event a S i (w) \u2264 (1 \u2212 2)ai(w).\nUsing Corollary A.2 with \u03c4 = 2ai(w)/2, we obtain that Pr(skewm( , w)) \u2264 2 exp ( \u2212 3B33 ) .\nSimilarly, take w \u2208 W\u2212i , such that ai(w) \u2264 (1\u2212 3 2 )B. It is easy to check that the event skewp( , w) is contained in aSi (w) \u2265 (1+ 2)ai(w), so using Corollary A.2 with \u03c4 = 2B/2 we get that Pr(skewm( , w)) \u2264\n2 exp ( \u2212 3B33 ) . This concludes the proof of the lemma."}, {"heading": "D Proof of Lemma 2.10", "text": "Fix j \u2208 [K]. Consider a set x \u2208 X and let p be a dual vector such that x(p) = x. Let t\u2032 be the last index of Cj which belongs to x|Cj ; this implies that \u03c0t\u2032 > pat \u2032 = pcj\u2016at\u2032\u2016\u221e, or alternatively \u03c0t\u2032\u2016at\u2032\u2016\u221e > pc j . By the ordering of the columns, for all t \u2208 Cj smaller than t\u2032 we have \u03c0t\u2016at\u2016\u221e \u2265 \u03c0t\u2032 \u2016at\u2032\u2016\u221e > pcj and hence t \u2208 x|Cj . By definition of t\u2032 it follows that x|Cj = {t \u2208 Cj : t \u2264 t\u2032}, a prefix of Cj ; this concludes the proof."}, {"heading": "E Proof of Lemma 2.11", "text": "We prove thatW+i is a witness set for X + i ; the proof thatW \u2212 i is a witness set for X \u2212 i is analogous.\nFirst, we claim that for all x \u2208 X+i , there is x\u2032 \u2208 X such that x\u2032 \u2286 x and ai(x\u2032) \u2208 [B,B + m]. To see this, let p be such that x = x(p). For \u03bb \u2265 0, define p\u03bb = p + \u03bbei, where ei denotes the ith canonical vector. We have that ai(x(p0)) > B (since x(p) \u2208 X+i ) and ai(x(p\u221e)) = 0 (since columns with ati > 0 will at have at some point p\u03bbat \u2265 \u03c0t). Due to the assumption that the input is in general position, whenever ai(x(p\n\u03bb)) is discontinuous (as a function of \u03bb \u2265 0) the right and the left limits differ by at most m. It then follows that there is \u03bb \u2265 0 such that ai(x(p\u03bb)) \u2208 [B,B +m], and since x(p\u03bb) \u2286 x for all \u03bb \u2265 0 the claim follows.\nSo take a classification x \u2208 X+i and let x\u2032 be as above. The fact that ai(x\u2032) \u2264 B + m and the nonnegativity of the at\u2019s imply that there is an ` \u2208 LK such that x\u2032 \u2208 B`i . Since w` is the unique smallest set in B`i , clearly x\u2032 \u2286 w`. To show that w` \u2208 W + i , it suffices to argue that ai(w\n`) \u2265 (1\u2212 /2)B. Since w`, x\u2032 \u2208 B`i , for all j such that `j > 0 we have ai(w`|Cj ) \u2265 ai(x\u2032|Cj )/(1 + 4). Moreover, for j\nsuch that ` = 0 we have ai(x(p)|Cj ) < B4K . Adding over all j \u2208 [K] gives\nai(w `) \u2265\n( 1\n1 + 4 )ai(x(p))\u2212 \u2211 j:`j=0 ai(x(p)|Cj )  \u2265 B 1 + 4 \u2212 B 4 \u2265 ( 1\u2212 2 ) B,\nwhere the third inequality follows from Observation C.1. Thus, w` \u2208 W+i . Since this property holds for all x \u2208 X+i , we conclude thatW + i is a witness set for X + i ."}, {"heading": "F Proof of Lemma 2.12", "text": "Recall the definitions of P v (for v \u2208 LK) and P `j (for j \u2208 [m], ` \u2208 L). It suffices to prove that at most (O(K log K ))\nm of the families P v\u2019s are non-empty. Since x(p) \u2208 Bvi if and only if for all j \u2208 [K] we have x(p)|Cj \u2208 B vj i,j , it follows that P v = \u22c2 j P vj j . Let \u03c4 `j denote the first index in Cj such that the prefix {t \u2208 Cj : t \u2264 \u03c4 `j } occupies the budget i to an extent in I`. Using Lemma 2.10 and the fact that the at\u2019s are non-negative, we get that B`i,j is the set of all prefixes of Cj which contain \u03c4 `j but do not contain \u03c4 `+1 j . Moreover, notice that the set x(p)|Cj contains \u03c4 `j if and only if \u03c0\u03c4`j > pa \u03c4`j . It then follows from these observations we can express the set P `j using linear inequalities: P `j = {p \u2208 Rm+ : \u03c0\u03c4`j > pa \u03c4`j , \u03c0\u03c4`+1j \u2264 pa\u03c4 `+1 j }. Since P v = \u22c2 j P vj j , we have that P v is given by the intersection of halfspaces defined by hyperplanes of the form \u03c0\u03c4`j = pa \u03c4`j and pk = 0 (k \u2208 [m]).\nSo consider the arrangement given by all hyperplanes {\u03c0\u03c4`j = pa \u03c4`j }j\u2208[K],`\u2208L and {pi = 0}mi=1. Given a face F in this arrangement and a set P v, either F is contained in P v or these sets are disjoint. Since the faces of the arrangement cover Rm, it follows that each non-empty P v contains at least one of these faces.\nNotice that the arrangement is defined by K|L| + m \u2264 O(Km log K ) hyperplanes, where the last inequality uses the fact that log(1 + 4) \u2265 log(1 + 1 4) holds (by concavity) for \u2208 [0, 1]. It is known that\nan arrangement with h \u2265 m hyperplanes in Rm has at most ( eh m )m faces (see Section 6.1 of [22] and page 82 of [23]). Using the conclusion of the previous paragraph, we get that there are at most (O(K log K )) m non-empty P v\u2019s and the result follows."}, {"heading": "G Proof of Lemma 3.1", "text": "Let LP1 denote the LP with columns (\u03c0t, a\u0303t) and right-hand side (1 \u2212 )B and LP2 denote the LP with columns (\u03c0t, at) and right-hand side B.\nLet x be an -approximate solution for LP1. Notice that we can upper bound \u2016at \u2212 a\u0303t\u2016\u221e as a function of \u2016a\u0303t\u2016\u221e:\n\u2016a\u0303t\u2016\u221e \u2265 \u2016at\u2016\u221e \u2212 \u2016at \u2212 a\u0303t\u2016\u221e \u2265 m \u2016at \u2212 a\u0303t\u2016\u221e,\nwhere the first inequality follows from triangle inequality. That is, we have \u2016at \u2212 a\u0303t\u2016\u221e \u2264 m\u2016a\u0303 t\u2016\u221e.\nGiven this bound, it is easy to see that x is feasible for LP2:\u2211 t atixt \u2264 \u2211 t (a\u0303ti + \u2016ati \u2212 a\u0303ti\u2016)xt \u2264 (1\u2212 )B + \u2211 t \u2016at \u2212 a\u0303t\u2016\u221ext \u2264 (1\u2212 )B + m \u2211 t \u2016a\u0303t\u2016\u221ext \u2264 B,\nwhere the last inequality uses the fact that \u2211\nt \u2016a\u0303t\u2016\u221ext \u2264 \u2016a\u0303t\u20161xt \u2264 mB, since x is a feasible solution and the a\u0303t\u2019s are non-negative.\nIn order to show that x is a 2 -approximate solution for LP2, it suffices to show that the optimum of LP1 is at least 1/(1 + ) times the optimum of the LP2, since then x will be within a factor of (1\u2212 )/(1 + ) \u2265 (1\u2212 2 ) the optimum of LP2. So let x\u2217 be an optimal solution for LP2. Using the same argument as before, it is easy to see that x\u2217/(1 + ) is feasible for LP1; this concludes the proof of the lemma."}, {"heading": "H Proof of Lemma 4.1", "text": "The proof uses the same ideas used in the analysis of OTP, although some definitions need to be changed slightly.\nRecall that S = {\u03c3(1), \u03c3(2), . . . , \u03c3(s)}, T = {\u03c3(s+ 1), \u03c3(s+ 2), . . . , \u03c3(2s)} and U = S \u222a T . Again we use pS to denote the dual vector used by (s, \u03b4)-OTP for its classification, and set xS = x(pS). With slight abuse in the notation, we often see xS as a (possibly infeasible) solution for (2s, 1)-LP, which means that we truncate the vector xS to the first 2s coordinates xS\u03c3(1), . . . , x S \u03c3(2s).\nAs before, we focus on proving the following lemma; the proof that this lemma implies Lemma 4.1 is presented at the end of this section.\nLemma H.1 Suppose that there are K \u2265 m 1-dim subspaces of Rm containing the columns at\u2019s. Fix an integer s and a real number \u03b4 \u2208 (0, 1/10) such that \u03b42sBn \u2265 \u2126(m ln K \u03b4 ). Then with probability at least (1\u2212 \u03b42), xS satisfies aTi (xS) \u2264 B for all i \u2208 [m] and has value \u03c0UxS \u2265 (1\u2212 3\u03b4)OPT(2s).\nIn a given scenario, we now say that xS is bad if aTi (s S) > B for some i \u2208 [m] or if \u03c0UxS(1 \u2212 3\u03b4)OPT(2s). In this scenario, now a classification x \u2208 X can be badly learned for budget i due to infeasibility if aSi (x) \u2264 (1 \u2212 \u03b4)B and aTi (x) > B; x can be badly learned for budget i due to value if aSi (x) \u2265 (1 \u2212 2\u03b4)B and aUi (x) < (1 \u2212 3\u03b4)B. Then x can be badly learned for budget i if it falls into any of the above cases. The following is the appropriate modification of Lemma 2.4 for our current setting, and can be proved exactly in the same way.\nLemma H.2 Consider a scenario where xS satisfies the following: (i) for all i \u2208 [m], aTi (xS) \u2264 B and (ii) for all i \u2208 [m] with pSi > 0, aUi (xS) \u2265 (1\u2212 3\u03b4)B. Then xS is good.\nDue to our definitions, this lemma implies that inequality (2.1) still hold.\nWitness sets. In the analysis of OTP, each x \u2208 X could be badly learned for budget i due to either infeasibility or (exclusively) due to value, which motivated the definitions of X+i and X \u2212 i . Now the same x can be badly learned for budget i due to both conditions. Therefore, we introduce two different partition of X , which tells why a classification is unlikely to be badly learned due to the appropriate condition. That is, we define X+i = {x \u2208 X : ai(x) > (1 \u2212 \u03b4)B + \u03b4B 2 } and Y + i = {x \u2208 X : ai(x) \u2264 (1 \u2212 \u03b4)B + \u03b4B 2 } as the partition associated to the infeasibility condition and X\u2212i = {x \u2208 X : ai(x) < (1 \u2212 2\u03b4)B \u2212 \u03b4B 2 } and Y\u2212i = {x \u2208 X : ai(x) \u2265 (1\u2212 2\u03b4)B \u2212 \u03b4B 2 } as the partition associated to the value condition. For example, X\u2212i is the set of classifications which are unlikely to be infeasible because of a small ai(.) value. Also, note that these classifications are all based on the total budget occupation rather than on the budget occupation in the first 2s columns only.\nGiven this more refined tagging of elements in X , we also need to redefine witness sets. We say that (W+i ,W \u2212 i ,Z + i ,Z \u2212 i ) are witness sets for (X + i ,X \u2212 i ,Y \u2212 i ,Y + i ) respectively if they satisfy the following:\nw \u2208 W+i \u21d2 ai(w) \u2265 (1\u2212 \u03b4)B + \u03b4B\n4 , x \u2208 X+i \u21d2 \u2203w \u2208 W + i : w \u2286 x\nw \u2208 Z+i \u21d2 ai(w) \u2265 (1\u2212 2\u03b4)B \u2212 3\u03b4B 4 , x \u2208 Y\u2212i \u21d2 \u2203w \u2208 Z + i : w \u2286 x w \u2208 W\u2212i \u21d2 ai(w) \u2264 (1\u2212 2\u03b4)B \u2212 \u03b4B\n4 , x \u2208 X\u2212i \u21d2 \u2203w \u2208 W + i : x \u2286 w\nw \u2208 Z\u2212i \u21d2 ai(w) \u2264 (1\u2212 \u03b4)B + 3\u03b4B\n4 , x \u2208 Y+i \u21d2 \u2203w \u2208 W + i : x \u2286 w .\nAgain to simplify the notation, given a set x we define skewmSi (\u03b4, x) to be the event that a S i (x) \u2264 (1 \u2212 \u03b4)B, skewpSi (\u03b4, x) to be the event that aSi (x) \u2265 (1 \u2212 \u03b4)B and similarly replacing the set S by the sets T and U . The following expression, which is the analogous to (2.2)-(2.1), establishes the connection between the events where classifications can be badly learned and witness sets:\n\u2228 x\u2208X {x can be badly learned for budget i} \u2286  \u2228 w\u2208W+i skewmS(\u03b4, w)  \u2228  \u2228 w\u2208Z+i skewmU (3\u03b4, w)  \u2228\n \u2228 w\u2208W\u2212i skewpS(2\u03b4, w)  \u2228  \u2228 w\u2208Z\u2212i skewpT (0, w)  . (H.2)\nTo see that this expression holds, take x \u2208 X . Suppose that x \u2208 X+i and let w \u2208 W + i be contained in x. Then the event {x can be badly learned for budget i due to infeasibility} is contained in skewmS(\u03b4, w). Similarly, if x \u2208 Y+i letw \u2208 Z \u2212 i contain x; then the event {x can be badly learned for budget i due to infeasibility} is contained in skewmT (0, w). The reasoning for the event {x can be badly learned for budget i due to value} is similar.\nThe following is analogous to Lemma 2.8.\nLemma H.3 Suppose that, for all i \u2208 [m], there are witness sets for (X+i ,X \u2212 i ,Y + i ,Y \u2212 i ) of size at most M . Then Pr(xS is bad ) \u2264 8mM exp ( \u2212 \u03b42sB136n ) .\nGood witness sets. We now construct witness sets of size at most (O(K\u03b4 log K \u03b4 )) m, so Lemma H.1 will follow directly from Lemma H.3. The development mirrors that of Section 2.3. Let C1, C2, . . . , CK be a partition of the index set [n] such that for all j, the columns {at}t\u2208Cj belong to the same 1-dimensional subspace.\nCover the interval [0, B+m] with intervals {I`}`\u2208L, where I0 = [0, \u03b4B8K ) and I` = [ \u03b4B 8K (1+ \u03b4 8) `\u22121, \u03b4B8K (1+\n\u03b4 8) `) for ` > 0 and L = {0, . . . , dlog1+\u03b4/8 16K\u03b4 e + 1}. Define B ` i,j as the set of classifications x \u2208 X |Cj whose occupation ai(x) lies in the interval I`. Finally, for ` \u2208 LK , define the family of boxes B`i = \u220f j B `j i,j .\nGiven ` \u2208 L, let w`(j) be the smallest set in X|Cj which has ai(w`(j)) \u2208 I` and for ` \u2208 LK define the set w` as the union of the sets w`j (j)\u2019s (or equivalently, as the concatenation of the vectors w`j (j)\u2019s). Similarly, for ` \u2208 L let w`(j) be the largest set in X|Cj which has ai(w`(j)) \u2208 I` and for ` \u2208 LK define the set w` as the union of the sets w`j (j)\u2019s.\nNow we construct the witness sets as before. SetW+i = {w` : ai(w`) \u2265 (1\u2212\u03b4)B+ \u03b4B 4 ,B ` i\u2229X 6= \u2205}, set\nZ+i = {w` : ai(w`) \u2265 (1\u22122\u03b4)B\u2212 3\u03b4B 4 ,B ` i\u2229X 6= \u2205}, setW \u2212 i = {w` : ai(w`) \u2264 (1\u22122\u03b4)B\u2212 \u03b4B 4 ,B ` i\u2229X 6= \u2205} and finally set Z\u2212i = {w` : ai(w`) \u2264 (1\u2212 \u03b4)B + 3\u03b4B 4 ,B ` i \u2229 X 6= \u2205}.\nFollowing the same steps as in the proof of Lemma 2.11, one can check that (W+i ,W \u2212 i ,Z + i ,Z \u2212 i ) are\nwitness sets for (X+i ,X \u2212 i ,Y + i ,Y \u2212 i ). Moreover, the proof of Lemma 2.12 can be used to show that, for a fixed i \u2208 [m], at most (eK\u03b4 log K \u03b4 )\nm of the B`i \u2019s contain an element of X , which then imposes the same upper bound on the size of the witness sets. This concludes the proof of Lemma H.1.\nProof of Lemma 4.1: Let x be the solution returned by (s, \u03b4)-OTP and let E denote the event that xS is good. For any scenario in E , we have x\u03c3(t) = xS\u03c3(t) for all t = s+ 1, s+ 2, . . . , 2s. Therefore, we get that\nE [ 2s\u2211 t=1 \u03c0\u03c3(t)x\u03c3(t) ] \u2265 E [ 2s\u2211 t=1 \u03c0\u03c3(t)x\u03c3(t) | E ] Pr(E)\n\u2265 E [ 2s\u2211 t=1 \u03c0\u03c3(t)x S \u03c3(t) | E ] Pr(E)\u2212 E[OPT(s) | E ] Pr(E)\n\u2265 E [ 2s\u2211 t=1 \u03c0\u03c3(t)x S \u03c3(t) | E ] Pr(E)\u2212 E[OPT(s)]. (H.3)\nTo lower bound the first term in the right hand side we use again the definition of E :\nE [ 2s\u2211 t=1 \u03c0\u03c3(t)x S \u03c3(t) | E ] \u2265 (1\u2212 3\u03b4)E[OPT(2s) | E ] Pr(E)\nand\nE[OPT(2s)] = E[OPT(2s) | E ] Pr(E) + E[OPT(2s) | E ] Pr(E) \u2264 E[OPT(2s) | E ] Pr(E) + \u03b42OPT, where the last inequality uses Lemma H.1. Combining the previous two inequalities give that E [\u22112s\nt=1 \u03c0\u03c3(t)x S \u03c3(t) | E\n] \u2265\n(1\u2212 3\u03b4)E[OPT(2s)]\u2212 \u03b42OPT, and the result follows from equation (H.3)."}, {"heading": "I Proof of Theorem 4.2", "text": "Let LP1 denote the LP with columns (\u03c0t, a\u0303t) and right-hand side B\u0303 = (1\u2212 )B and LP2 denote the LP with columns (\u03c0t, at) and right-hand side B. We show that Robust DPA returns a (1\u2212 21.5 )-approximation for LP1, and the theorem will follow from Lemma 3.1.\nFirst we show that the returned solution x is feasible for LP1. By definition of the algorithm, aj(xi) \u2264 2iB\u0303 for all i, j. By linearity, aj(x) = \u2211 i aj(x i) \u2264 B\u0303 \u2211log(1/ )\u22121 i=0 2 i \u2264 B\u0303.\nIn order to verify the value of the returned solution, we first show that \u03b4 2sB n \u2265 \u2126(m ln K \u03b4 ) in every call to (s, \u03b4)-OTP made by Robust DPA. As in Section 3, the columns a\u0303t\u2019s belong to at most K = O(m ) m 1-dim subspaces. Since B \u2265 \u2126(m2 2 ln m ), we have that for each i = 0, . . . , log(1/ ) \u2212 1 setting s = 2 in\nand \u03b4 = \u221a /2i satisfies the expression \u03b4\n2sB n \u2265 \u2126(m ln K \u03b4 ). Then applying Lemma 4.1 we get that for all i = 0, . . . , log(1/ )\u22121, E[\u03c0xi] \u2265 (1\u22123 \u221a\n2i )E[OPT( 2i+1n)]\u2212 E[OPT( 2in)]\u2212 OPT\n2i . By linearity of the objective value and of expectations\nE[\u03c0x] = \u2211 i E[\u03c0xi] \u2265 \u2212E[OPT( n)]\u2212 log(1/ )\u22122\u2211 i=0 ( 3 \u221a 2i ) E[OPT( n2i+1)] + (1\u2212 3 \u221a 2 \u2212 )OPT.\nLemma 2.4 of [1] states that E[OPT(s)] \u2264 snOPT for all s \u2265 0. Employing this observation, we get\nE[\u03c0x] \u2265 OPT\u2212 OPT 3\u221a2 + 2 + 3\u221a log(1/ )\u22122\u2211 i=0 2i/2+1  . Since the summation in the expression can be upper bounded by 2 \u221a 2 log(1/ )\n\u221a 2\u22121 \u2264 5\u221a , we get that E[\u03c0\u0303x] \u2265\n(1\u2212 21.5 )OPT. This concludes the proof of the theorem."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "<lb>We consider packing LP\u2019s with m rows where all constraint coefficients are normalized to be in the<lb>unit interval. The n columns arrive in random order and the goal is to set the corresponding decision<lb>variables irrevocably when they arrive so as to obtain a feasible solution maximizing the expected reward.<lb>Previous (1\u2212 )-competitive algorithms require the right-hand side of the LP to be \u03a9(<lb>2 log n ), a bound<lb>that worsens with the number of columns and rows. However, the dependence on the number of columns<lb>is not required in the single-row case and known lower bounds for the general case are also independent<lb>of n.<lb>Our goal is to understand whether the dependence on n is required in the multi-row case, making it<lb>fundamentally harder than the single-row version. We refute this by exhibiting an algorithm which is<lb>(1\u2212 )-competitive as long as the right-hand sides are \u03a9(<lb>2<lb>2 log m ). Our techniques refine previous PAC-<lb>learning based approaches which interpret the online decisions as linear classifications of the columns<lb>based on sampled dual prices. The key ingredient of our improvement comes from a non-standard<lb>covering argument together with the realization that only when the columns of the LP belong to few 1-d<lb>subspaces we can obtain small such covers; bounding the size of the cover constructed also relies on the<lb>geometry of linear classifiers. General packing LP\u2019s are handled by perturbing the input columns, which<lb>can be seen as making the learning problem more robust.", "creator": "LaTeX with hyperref package"}}}