{"id": "1205.5075", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2012", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "abstract": "Although promising, most existing work uses convex methods that may be suboptimal in terms of accuracy of character selection and parameter estimation. In this paper, we expand a non-convex paradigm for group trait selection motivated by applications that require the identification of the underlying group structure and the simultaneous selection of characteristics. The main contributions in this article are two: (1) statistically, we are introducing a non-convex sparse model for group trait selection that can reconstruct the oracle estimator. Therefore, consistent selection of characteristics and parameter estimates can be achieved; (2) mathematically, we propose an efficient algorithm that is applicable to major problems. Numerical results suggest that the proposed non-convex method can be compared favorably with its competitors on synthetic data and real-world applications, thereby achieving the desired goal of delivering high performance.", "histories": [["v1", "Wed, 23 May 2012 00:02:01 GMT  (22kb)", "http://arxiv.org/abs/1205.5075v1", null], ["v2", "Fri, 18 Jan 2013 21:06:49 GMT  (22kb)", "http://arxiv.org/abs/1205.5075v2", "Accepted by the 30th International Conference on Machine Learning (ICML 2013)"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shuo xiang", "xiaoshen tong", "jieping ye"], "accepted": true, "id": "1205.5075"}, "pdf": {"name": "1205.5075.pdf", "metadata": {"source": "CRF", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "authors": ["Shuo Xiang", "Xiaotong Shen", "Jieping Ye"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n50 75\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "During the past decade, sparse feature selection has been extensively investigated, on both optimization algorithms [1] and statistical properties [28, 20, 3]. When the data possesses certain group structure, sparse modeling has been explored in [24, 16, 13] for group feature selection. The group lasso [24] proposes an L2-regularization method for each group, which ultimately yields a group-wisely sparse model. The utility of such a method has been demonstrated in detecting splice sites [23]\u2014an important step in gene finding and theoretically justified in [13]. The sparse group lasso [11] enables to encourage sparsity at the level of both features and groups simultaneously.\nIn the literature, most approaches use convex methods due to globality of the solution and tractable computation. However, this may lead to suboptimal results. Recent studies demonstrate that nonconvex methods, for instance, the truncated L1-penalty [19, 15, 27], may have potential to deliver superior performance than the standard L1-formulation. In addition, [19] suggests that a constrained version of nonconvex regularization is slightly more preferable than its regularization counterpart due to theoretical merits.\nIn this article, we investigate the sparse group feature selection (SGFS) through a constrained nonconvex\nformulation. Ideally, we wish to optimize the following L0-model:\nminimize x\n1 2 \u2016Ax\u2212 y\u201622\nsubject to\np\u2211\nj=1\nI(|xj | 6= 0) \u2264 s1\n|G|\u2211\nj=1\nI(\u2016xGj\u20162 6= 0) \u2264 s2,\n(1)\nwhere A is an n by p data matrix with its columns representing different features. x = (x1, \u00b7 \u00b7 \u00b7 , xp) is partitioned into |G| non-overlapping groups {xGi} and I(\u00b7) is the indicator function. The advantage of the L0-model (1) lies in its complete control on two levels of sparsity (s1, s2), which are the numbers of features and groups respectively. However, a problem like (1) is known to be NP-hard [17].\nThis paper develops an efficient nonconvex method, which is a computational surrogate of the L0-method described above and has theoretically guaranteed performance. We contribute in two aspects: (i) statistically, the proposed method retains the merits of the L0 approach (1) in the sense that the oracle estimator can be reconstructed, which leads to consistent feature selection and parameter estimation; (ii) computationally, our efficient optimization tool enables to treat large-scale problems."}, {"heading": "2 Nonconvex Formulation and Computation", "text": "One major difficulty of solving (1) comes from nonconvex and discrete constraints, which require enumerating all possible combinations of features and groups to achieve the optimal solution. Therefore we approximate these constraints by their continuous computational surrogates:\nminimize x\n1 2 \u2016Ax\u2212 y\u201622\nsubject to\np\u2211\nj=1\nJ\u03c4 (|xj |) \u2264 s1,\n|G|\u2211\ni=1\nJ\u03c4 (\u2016xGi\u20162) \u2264 s2,\n(2)\nwhere J\u03c4 (z) = min(|z|/\u03c4, 1) is a truncated L1-function approximating the L0-function [19, 26], and \u03c4 > 0 is a tuning parameter such that J\u03c4 (z) approximates the indicator function I(|z| 6= 0) as \u03c4 approaches zero.\nTo solve the nonconvex problem (2), we develop a Difference of Convex (DC) algorithm based on a decomposition of each nonconvex constraint function into a difference of two convex functions; for instance,\np\u2211\nj=1\nJ\u03c4 (|xj |) = S1(x)\u2212 S2(x),\nwhere\nS1(x) = 1\n\u03c4\np\u2211\nj=1\n|xj |\nand\nS2(x) = 1\n\u03c4\np\u2211\nj=1\nmax{|xj | \u2212 \u03c4, 0}\nare convex in x. Then each trailing convex function, say S2(x), is replaced by its affine minorant at the previous iteration S1(x)\u2212 S2(x\u0302(m\u22121))\u2212\u2207S2(x\u0302(m\u22121))T (x\u2212 x\u0302(m\u22121)), (3) which yields an upper approximation of the constraint function \u2211p j=1 J\u03c4 (|xj |) as follows:\n1\n\u03c4\np\u2211\nj=1\n|xj | \u00b7 I(|x\u0302(m\u22121)j | \u2264 \u03c4) + p\u2211\nj=1\nI(|x\u0302(m\u22121)j | > \u03c4) \u2264 s1. (4)\nSimilarly, the second nonconvex constraint in (2) can be approximated by\n1\n\u03c4\n|G|\u2211\nj=1\n\u2016xGj\u20162 \u00b7 I(\u2016x\u0302(m\u22121)Gj \u20162 \u2264 \u03c4) + |G|\u2211\nj=1\nI(\u2016x\u0302(m\u22121)Gj \u20162 > \u03c4) \u2264 s2. (5)\nNote that both (4) and (5) are convex constraints, which result in a convex subproblem as follows:\nminimize x\n1 2 \u2016Ax\u2212 y\u201622\nsubject to 1 \u03c4 \u2016xT1(x\u0302(m\u22121))\u20161 \u2264 s1 \u2212 (p\u2212 |T1(x\u0302(m\u22121))|)\n1 \u03c4 \u2016xT3(x\u0302(m\u22121))\u2016G \u2264 s2 \u2212 (|G| \u2212 |T2(x\u0302(m\u22121))|),\n(6)\nwhere T1, T2 and T3 are the support sets 1 defined as:\nT1(x) = {i : |xi| \u2264 \u03c4} T2(x) = {i : \u2016xGi\u20162 \u2264 \u03c4} T3(x) = {i : xi \u2208 xGj , j \u2208 T2(x)},\n\u2016xT1\u20161 and \u2016xT3\u2016G denote the corresponding value restricted on T1 and T3 respectively, and \u2016x\u2016G =\u2211|G| i=1 \u2016xGi\u20162. Solving (6) would provide us an updated solution, denoted as x\u0302(m). Such procedure is iterated until the objective value is no longer decreasing, indicating that a local minimizer is achieved. The DC algorithm is summarized in Algorithm 1, from which we can see that efficient computation of (6) is critical to the overall DC routine. We defer detailed discussion of this part to Section 4.\nAlgorithm 1 DC programming for solving (2)\nInput: A, y, s1, s2 Output: solution x to (2) 1: (Initialization) Initialize x\u0302(0). 2: (Iteration) At iteration m, compute x\u0302(m) by optimizing (6). 3: (Stopping Criterion) Terminate when the objective function stops decreasing."}, {"heading": "3 Theoretical Results", "text": "This section investigates theoretical aspects of the proposed method. More specifically, we demonstrate that the oracle estimator x\u0302o, the least squares estimator based on the true model, can be reconstructed. As a result, consistent selection as well as optimal parameter estimation can be achieved.\n1Support sets indicate that the elements outside these sets have no effect on the particular items in the constraints of (6).\nFor better presentation, we introduce some notations that would be only utilized in this section. Let C = (Gi1 , \u00b7 \u00b7 \u00b7 , Gik) be the collection of groups that contain nonzero elements. Let AGj = AGj (x) and A = A(x) denote the indices of nonzero elements of x in group Gj and in entire x respectively. Define\nSj,i = {x \u2208 S : (AC , C) 6= (AC0 , C0), |A| = j, |C| = i},\nwhere S is the feasible region of (2) and C0 represents the true nonzero groups. The following assumptions are needed for obtaining consistent reconstruction of the oracle estimator:\nAssumption 1 (Separation condition). Define\nCmin(x 0) = inf\nx\u2208S \u2212 log(1 \u2212 h2(x,x0)) max(|C0 \\ C|, 1) ,\nthen for some constant c1 > 0,\nCmin(x 0) \u2265 c1 log |G|+ log s01 n ,\nwhere\nh(x,x0) = (1 2\n\u222b (g1/2(x, y)\u2212 g1/2(x0, y))2d\u00b5(y) )1/2\nis the Hellinger-distance for densities with respect to a dominating measure \u00b5.\nAssumption 2 (Complexity of the parameter space). For some constants c0 > 0 and any 0 < t < \u03b5 \u2264 1,\nH(t,Fj,i) \u2264 c0 max((log(|G|+ s01))2, 1)|Bj,i| log(2\u03b5/t),\nwhere Bj,i = Sj,i \u2229 {x \u2208 h(x,x0) \u2264 2\u03b5} is a local parameter space and Fj,i = {g1/2(x, y) : x \u2208 Bj,i} is a collection of square-root densities. H(\u00b7,F) is the bracketing Hellinger metric entropy of space F [14].\nAssumption 3. For some positive constants d1, d2, d3 with d1 > 10,\n\u2212 log(1\u2212 h2(x,x0)) \u2265 \u2212d1 log(1\u2212 h2(x\u03c4 ,x0))\u2212 d3\u03c4d2p,\nwhere x\u03c4 = (x1I(|x1| \u2265 \u03c4), \u00b7 \u00b7 \u00b7 , xpI(|xp| \u2265 \u03c4)).\nWith the above assumptions hold, we can conclude the following non-asymptotic probability error bound regarding the reconstruction of the oracle estimator x\u0302o.\nTheorem 1. Suppose that Assumptions 2 and 3 hold. For a global minimizer of (2) x\u0302 with (s1, s2) = (s 0 1, s 0 2) and \u03c4 \u2264 ( (d1\u221210)Cmin(x0)\nd3d\n)1/d2 , the following result hold:\nP ( x\u0302 6= x\u0302o ) \u2264 exp ( \u2212 c2nCmin(x0) + 2(log |G|+ log s01) ) .\nMoreover, with Assumption 1 hold, P ( x\u0302 = x\u0302o ) \u2192 1 and\nEh2(x\u0302,xo) = (1 + o(1))max(Eh2(x\u0302o,x0), s01 n )\nas n\u2192 \u221e, |G| \u2192 \u221e.\nTheorem 1 states that the oracle estimator x\u0302o can be accurately reconstructed, which in turn yields feature selection consistency as well as the recovery of the performance of the oracle estimator in parameter estimation. Moreover, according to Assumption 1, such conclusion still holds when s01|G| grows in the order of exp(c\u221211 nCmin) . This is in contrast to existing conclusions on consistent feature selection, where the number of candidate features should be no larger than exp(c\u2217n) for some c\u2217 [28]. In this sense, the number\nof candidate features is allowed to be much larger when an additional group structure is incorporated, particularly when each group contains considerable redundant features.\nTo our knowledge, our theory for the grouped selection is the first of this kind. However, it has a root in feature selection. The large deviation approach used here is applicable to derive bounds for feature selection consistency. In such a situation, the result agrees with the necessary condition for feature selection consistency for any method, except for the constants independent of the sample size [19]. In other words, the required conditions are weaker than those for L1-regularization [21]. The use of the Hellinger-distance is mainly to avoid specifying a sub-Gaussian tail of the random error. This means that the result continues to hold even when the error does not have a sub-Gaussian tail."}, {"heading": "4 Optimization Procedures", "text": "As mentioned in Section 2, efficient computation of the convex subproblem (6) is of critical importance for the proposed DC algorithm. Note that (6) has an identical form of the constrained sparse group lasso problem:\nminimize x\n1 2 \u2016Ax\u2212 y\u201622\nsubject to \u2016x\u20161 \u2264 s1 \u2016x\u2016G \u2264 s2\n(7)\nexcept that x is restricted to the two support sets. As to be shown in Section 4.3, an algorithm for solving (6) can be obtained through only a few modifications on that of (7). Therefore, we first focus on solving (7)."}, {"heading": "4.1 Accelerated Gradient Method", "text": "For large-scale problems, the dimensionality of data can be very high, therefore first-order optimization is often preferred. We adapt the well-known accelerated gradient method (AGM) [18, 2], which is commonly used due to its fast convergence rate.\nTo apply AGM to our formulation (7), the crucial step is to solve the following Sparse Group Lasso Projection (SGLP):\nminimize x\n1 2 \u2016x\u2212 v\u201622\nsubject to \u2016x\u20161 \u2264 s1 (C1) \u2016x\u2016G \u2264 s2 (C2),\n(8)\nwhich is an Euclidean projection onto a convex set and a special case of (7) when A is the identity. For convenience, let C1 and C2 denote the above two constraints in what follows.\nSince the AGM is a standard framework whose efficiency mainly depends on that of the projection step, we leave the detailed description of AGM in the supplement and introduce the efficient algorithm for this projection step (8)."}, {"heading": "4.2 Efficient Projection", "text": "We begin with some special cases of (8). If only C1 exists, (8) becomes the well-known L1-ball projection [9], whose optimal solution is denoted as Ps11 (v), standing for the projection of v onto the L1-ball with radius s1. On the other hand, if only C2 is involved, it becomes the group lasso projection, denoted as Ps2G . Moreover, we say a constraint is active, if and only if an equality holds at the optimal solution x\u2217; otherwise, it is inactive.\nPreliminary results are summarized in Lemma 1:\nLemma 1. Denote a global minimizer of (8) as x\u2217. Then the following results hold:\n1. If both C1 and C2 are inactive, then x \u2217 = v.\n2. If C1 is the only active constraint, i.e., \u2016x\u2217\u20161 = s1, \u2016x\u2217\u2016G < s2, then x\u2217 = Ps11 (v) 3. If C2 is the only active constraint, i.e., \u2016x\u2217\u20161 < s1, \u2016x\u2217\u2016G = s2, then x\u2217 = Ps2G (v)"}, {"heading": "4.2.1 Computing x\u2217 from the optimal dual variables", "text": "Lemma 1 describes a global minimizer when either constraint is inactive. Next we consider the case in which both C1 and C2 are active. By the convex duality theory [6], there exist unique non-negative dual variables \u03bb\u2217 and \u03b7\u2217 such that x\u2217 is also the global minimizer of the following regularized problem:\nminimize x\n1 2 \u2016x\u2212 v\u201622 + \u03bb\u2217\u2016x\u20161 + \u03b7\u2217\u2016x\u2016G, (9)\nwhose solution is given by the following Theorem.\nTheorem 2 ([11]). The optimal solution x\u2217 of (9) is given by\nx\u2217Gi = max{\u2016v\u03bb \u2217 Gi\u20162 \u2212 \u03b7\u2217, 0} v\u03bb\n\u2217 Gi\n\u2016v\u03bb\u2217Gi\u20162 i = 1, 2, \u00b7 \u00b7 \u00b7 , |G| (10)\nwhere v\u03bb \u2217\nGi is computed via soft-thresholding [8] vGi with threshold \u03bb\n\u2217 as follows:\nv\u03bb \u2217 Gi = SGN(vGi) \u00b7max{|vGi | \u2212 \u03bb\u2217, 0},\nwhere SGN(\u00b7) is the sign function and all the operations are taken element-wisely.\nTheorem 2 gives an analytical solution of x\u2217 in an ideal situation when the values of \u03bb\u2217 and \u03b7\u2217 are given. Unfortunately, this is not the case and the values of \u03bb\u2217 and \u03b7\u2217 need to be computed directly from (8). Based on Theorem 2, we have the following conclusion characterizing the relations between the dual variables:\nCorollary 1. The following equations hold:\n\u2016x\u2217\u20161 = |G|\u2211\ni=1\nmax{\u2016v\u03bb\u2217Gi\u20162 \u2212 \u03b7\u2217, 0} \u2016v\u03bb\u2217Gi\u20161 \u2016v\u03bb\u2217Gi\u20162 = s1 (11)\n\u2016x\u2217\u2016G = |G|\u2211\ni=1\nmax{\u2016v\u03bb\u2217Gi\u20162 \u2212 \u03b7\u2217, 0} = s2 . (12)\nSuppose \u03bb\u2217 is given, then computing \u03b7\u2217 from (12) amounts to solving a median finding problem, which can be done in linear time [9].\nFinally, we treat the case of unknown \u03bb\u2217 (thus unknown \u03b7\u2217). We propose an efficient bisection approach to compute it."}, {"heading": "4.2.2 Computing \u03bb\u2217: bisection", "text": "Given an initial guess (estimator) of \u03bb\u2217, says \u03bb\u0302, one may perform bisection to locate the optimal \u03bb\u2217, provided that there exists an oracle procedure indicating if the optimal value is greater than \u03bb\u03022. This bisection method can estimate \u03bb\u2217 in logarithm time. Next, we shall design an oracle procedure.\n2An upper bound and a lower bound of \u03bb\u2217 should be provided in order to perform the bisection. These bounds can be easily derived from the assumption that both C1 and C2 are active.\nLet the triples (x\u2217, \u03bb\u2217, \u03b7\u2217) = SGLP(v, s1, s2)\nbe the optimal solution of (8) with both constraints active, i.e., \u2016x\u2217\u20161 = s1, \u2016x\u2217\u2016G = s2, with (\u03bb\u2217, \u03b7\u2217) be the optimal dual variables. Consider the following two sparse group lasso projections:\n(x, \u03bb, \u03b7) = SGLP(v, s1, s2), (x\u2032, \u03bb\u2032, \u03b7\u2032) = SGLP(v, s\u20321, s \u2032 2).\nThe following key result holds.\nTheorem 3. If \u03bb \u2264 \u03bb\u2032 and s2 = s\u20322, then s1 \u2265 s\u20321.\nTheorem 3 gives the oracle procedure with its proof presented in the supplement. For a given estimator \u03bb\u0302, we compute its corresponding \u03b7\u0302 from (12) and then s\u03021 from (11), satisfying (x\u0302, \u03bb\u0302, \u03b7\u0302) = SGLP(v, s\u03021, s2). Then s\u03021 is compared with s1. Clearly, by Theorem 3, if s\u03021 \u2264 s1, the estimator \u03bb\u0302 is no less than \u03bb\u2217. Otherwise, s\u03021 > s1 means \u03bb\u0302 < \u03bb\n\u2217. In addition, from (11) we know that s\u03021 is a continuous function of \u03bb\u0302. Together with the monotonicity given in Theorem 3, a bisection approach can be employed to calculate \u03bb\u2217. Algorithm 2 gives a detailed description.\n4.3 Solving Restricted version of (7)\nFinally, we modify the above procedures to compute the optimal solution of the restricted problem (6). To apply the accelerated gradient method, we consider the following projection step:\nminimize x\n1 2 \u2016x\u2212 v\u201622\nsubject to \u2016xT1\u20161 \u2264 s1 (C1) \u2016xT3\u2016G \u2264 s2 (C2).\n(13)\nOur first observation is: T3(x) \u2282 T1(x), since if an element of x lies in a group whose L2-norm is less than \u03c4 , then the absolute value of this element must also be less than \u03c4 . Secondly, from the decomposable nature of the objective function, we conclude that:\nx\u2217j = { vj if j \u2208 (T1)c v\u03bb \u2217\nj if j \u2208 T1\\T3,\nsince there are no constraints on xj if it is outside T1 and involves only the L1-norm constraint if j \u2208 T1\\T3. Following routine calculations as in [9], we obtain the following results similar to (11) and (12):\ns1 = \u2211\ni\u2208T2 max{\u2016v\u03bb\u2217Gi\u20162 \u2212 \u03b7\u2217, 0} \u2016v\u03bb\u2217Gi\u20161 \u2016v\u03bb\u2217Gi\u20162 + \u2211 j\u2208T1\\T3 v\u03bb \u2217 j (14)\ns2 = \u2211\ni\u2208T2 max{\u2016v\u03bb\u2217Gi\u20162 \u2212 \u03b7\u2217, 0}. (15)\nBased on (14) and (15), we design a similar bisection approach to compute \u03bb\u2217 and thus (x\u2217)T3 , as in Algorithm 2. Details are deferred to the supplement.\nAlgorithm 2 Sparse Group Lasso Projection Algorithm\nInput: v, s1, s2 Output: an optimal solution x to the Sparse Group Projection Problem\nFunction SGLP(v, s1, s2)\n1: if \u2016x\u20161 \u2264 s1 and \u2016x\u2016G \u2264 s2 then 2: return v 3: end if 4: xC1 = Ps11 (v) 5: xC2 = Ps2G (v) 6: xC12= bisec(v, s1, s2) 7: if \u2016xC1\u2016G \u2264 s2 then 8: return xC1 9: else if \u2016xC2\u20161 \u2264 s1 then\n10: return xC2 11: else 12: return xC12 13: end if\nFunction bisec(v, s1, s2)\n1: Initialize up, low and tol 2: while up\u2212 low > tol do 3: \u03bb\u0302 = (low + up)/2 4: if (12) has a solution \u03b7\u0302 given v\u03bb\u0302 then\n5: calculate s\u03021 using \u03b7\u0302 and \u03bb\u0302. 6: if s\u03021 \u2264 s1 then 7: up = \u03bb\u0302 8: else\n9: low = \u03bb\u0302 10: end if 11: else 12: up = \u03bb\u0302 13: end if 14: end while 15: \u03bb\u2217 = up 16: Solve (12) to get \u03b7\u2217 17: Calculate x\u2217 from \u03bb\u2217 and \u03b7\u2217 via (10) 18: return x\u2217"}, {"heading": "5 Significance", "text": "This section is devoted to a brief discussion of advantages of our work statistically and computationally. Moreover, it explains why the proposed method is useful to perform efficient and interpretable feature selection with a given natural group structure.\nInterpretability. The parameters in (2) are highly interpretable in that s1 and s2 are upper bounds of the number of nonzero elements as well as that of groups. This is advantageous, especially in the presence of certain prior knowledge regarding the number of features and/or that of groups. However, such an interpretation vanishes with convex methods such as lasso or sparse group lasso, in which incorporating such prior knowledge often requires repeated trials of different parameters.\nParameter tuning. Typically, tuning parameters for good generalization usually requires considerable amount work due to a large number of choices of parameters. However, tuning in (1) may search through integer values in a bounded range, and can be further simplified when certain prior knowledge is available.\nThis permits more efficient tuning than its regularization counterpart. Based on our limited experience, we note that \u03c4 does not need to be tuned precisely as we may fix at some small values.\nPerformance and Computation. Although our model (2) is proposed as a computational surrogate of the ideal L0-method, its performance can also be theoretically guaranteed, i.e., consistent feature selection can be achieved. Moreover, the computation of our model is much more efficient and applicable to large-scale applications."}, {"heading": "6 Empirical Evaluation", "text": "This section performs numerical experiments to evaluate the proposed methods in terms of the efficiency and accuracy of sparse group feature selection. Evaluations are conducted on a PC with i7-2600 CPU, 8.0 GB memory and 64-bit Windows operating system."}, {"heading": "6.1 Evaluation of Projection Algorithms", "text": "Since the DC programming and the accelerated gradient methods are both standard, the efficiency of the proposed nonconvex formulation (2) depends on the projection step in (8). Therefore, we focus on evaluating the projection algorithms and comparing with two popular projection algorithms: Alternating Direction Multiplier Method (ADMM) [5] and Dykstra\u2019s projection algorithm [7]. We provide a detailed derivation of adapting these two algorithms to our formulation in the supplement.\nTo evaluate the efficiency, we first generate the vector v whose entries are uniformly distributed in [\u221250, 50] and the dimension of v, denoted as p, is chosen from the set {102, 103, 104, 105, 106}. Next we partition the vector into 10 groups of equal size. Finally, s2 is set to 5 log(p) and s1, the radius of the L1-ball, is computed by \u221a 10 2 s2 (motivated by the fact that s1 \u2264 \u221a 10s2).\nFor a fair comparison, we run our projection algorithm until converge and record the minimal objective value as f\u2217. Then we run ADMM and Dykstra\u2019s algorithm until their objective values become close to ours. More specifically, we terminate their iterations as soon as fADMM \u2212 f\u2217 \u2264 10\u22123 and fDykstra \u2212 f\u2217 \u2264 10\u22123, where fADMM and fDykstra stand for the objective value of ADMM and Dykstra\u2019s algorithm respectively. Table 1 summarizes the average running time of all three algorithms over 100 replications.\nNext we demonstrate the accuracy of our projection algorithm. Toward this end, the general convex optimization toolbox CVX [12] is chosen as the baseline. Following the same strategy of generating data, we report the distance (computed from the Euclidean norm \u2016 \u00b7 \u20162) between optimal solution of the three projection algorithms and that of the CVX. Note that the projection is strictly convex with a unique global optimal solution.\nFor ADMM and Dykstra\u2019s algorithm, the termination criterion is that the relative difference of the objective values between consecutive iterations is less than a threshold value. Specifically, we terminate the iteration if |f(xk\u22121)\u2212 f(xk)| \u2264 10\u22127f(xk\u22121). For our projection algorithm, we set the tol in Algorithm 2 to be 10\u22127. The results are summarized in Table 2. Powered by second-order optimization algorithms, CVX can\nprovide fast and accurate solution for problems of moderate size but would suffer from great computational burden for large-scale ones. Therefore we only report the results up to 5, 000 dimensions.\nFrom Tables 1 and 2, we note that both ADMM and our algorithm yield more accurate solution than that of Dykstra\u2019s. For projections of moderate size, all three algorithms perform well. However, for large-scale ones, our advantage on efficiency is evident."}, {"heading": "6.2 Performance on Synthetic Data", "text": ""}, {"heading": "6.2.1 Experimental Setup", "text": "We generate a 60\u00d7 100 matrix A, whose entries follow i.i.d standard normal distribution. The 100 features (columns) are partitioned into 10 groups of equal size. The ground truth vector x0 possesses nonzero elements only in 4 of the 10 groups. To further enhance sparsity, in each nonzero group of x0, only t (t \u2264 10) elements are nonzero, where t is uniformly distributed from [1, 5]. Finally y is generated according to Ax0 + z with z following distribution N (0, 0.52), where A and y are divided into training and testing set of equal size.\nWe fit our method to the training set and compare with lasso, group lasso and sparse group lasso. The tuning parameters of the convex methods are selected from {0.01, 0.1, 1, 10}, whereas for our method, the number of nonzero groups is selected from the set {2, 4, 6, 8} and the number of features is chosen from {2s2, 4s2, 6s2, 8s2}. Leave-one-out cross-validation is conducted over the training set for choosing the best tuning parameter for all the methods."}, {"heading": "6.2.2 Results and Discussions", "text": "We use following metrics for evaluation:\n\u2022 Estimation error: \u2016x\u0302\u2212 x0\u201622 \u2022 Prediction error: \u2016Ax\u0302\u2212 y\u0303\u201622 \u2022 Group precision: |T2(x\u0302) \u2229 T2(x0)|/|T2(x\u0302)|\n\u2022 Group recall: |T2(x\u0302) \u2229 T2(x0)|/|T2(x0)|\nwhere x\u0302 is the estimator obtained from (2) and y\u0303 is an independent vector following the same distribution as y. The group precision and recall demonstrate the capability of recovering the group structure from data. We report the results in Table 3 and observe that our model generally exhibits better performance. Note that although our model does not provide the best result on the metric of group recall, the group precision of our model is significantly better than the others, illustrating the fact that the three convex methods recover more redundant groups."}, {"heading": "6.3 Performance on Real-world Application", "text": "Our method is further evaluated on the application of examining Electroencephalography (EEG) correlates of genetic predisposition to alcoholism [10]. EEG records the brain\u2019s spontaneous electrical activity by measuring the voltage fluctuations over multiple electrodes placed on the scalp. This technology has been widely used in clinical diagnosis, such as coma, brain death and genetic predisposition to alcoholism. In fact, encoded in the EEG data is a certain group structure, since each electrode records the electrical activity of a certain region of the scalp. Identifying and utilizing such spatial information has the potential of increasing stability of a prediction.\nThe training set contains 200 samples of 16384 dimensions, sampled from 64 electrodes placed on subject\u2019s scalps at 256 Hz (3.9-msec epoch) for 1 second. Therefore, the data can naturally be divided into 64 groups of size 256. We apply the lasso, group lasso, sparse group lasso and our method on the training set and adapt the 5-fold cross-validation for selecting tuning parameters. More specifically, for lasso and group lasso, the candidate tuning parameters are specified by 10 parameters3 sampled using the logarithmic scale from the parameter spaces, while for the sparse group lasso, the parameters form a 10 \u00d7 10 grid4, sampled from the parameter space in logarithmic scale. For our method, the number of groups is selected from the set: s2 = {30, 40, 50} and s1, the number of features is chosen from the set {50s2, 100s2, 150s2}. The accuracy of classification together with the number of selected features and groups over a test set, which also contains 200 samples, are reported in Table 4. Clearly our method achieves the best performance of classification with the least number of groups. Note that, although lasso\u2019s performance is almost as good as ours with even less features, however, it fails to identify the underlying group structure in the data, as revealed by the fact all 64 groups are selected."}, {"heading": "7 Conclusion and Future Work", "text": "This paper expands a nonconvex paradigm into sparse group feature selection. In particular, theoretical properties on the accuracy of selection and parameter estimation are analyzed. In addition, an efficient optimization scheme is developed based on the DC programming, accelerated gradient method and efficient\n3\u03bblasso = logspace(10 \u22123, 1), \u03bbglasso = logspace(10 \u22122, 1) 4The product space of \u03bblasso \u00d7 \u03bbglasso\nprojection. The efficiency and efficacy of the proposed method are validated on both synthetic data and real-world applications.\nThe proposed method will be further investigated on real-world applications involving the group structure. Moreover, extending the proposed model to multi-modal multi-task learning [25] is another promising direction."}, {"heading": "1 Proof of Theorem 1", "text": "The proof uses a large deviation probability inequality of [22] to treat one-sided log-likelihood ratios with constraints.\nLet S = { x\u03c4 : \u2016x\u03c4\u20160 \u2264 s01, \u2016x\u03c4\u20160,G \u2264 s02 } , \u2016x\u20160 = \u2211p j=1 I(|xj | 6= 0) is the L0-norm of x, and \u2016x\u20160,G =\u2211|G|\nj=1 I(\u2016xj\u20162 6= 0) is the L0-norm over the groups. Now we partition S. Note that for C \u2282 (G1, \u00b7 \u00b7 \u00b7 , G|G|), it can be partitioned into C = (C \\ C0) \u222a (C \u2229 C0). Then\nS = s02\u22c3\ni=0\n\u22c3\nC\u2208Bi SAC ,C ,\nwhere SAC ,C = { x\u03c4 \u2208 S : C(x) = C = (Gi1 , \u00b7 \u00b7 \u00b7 , Gik), \u2211 j |AGj | \u2264 s01 } , and Bi = {C 6= C0 : |C0 \\ C| = i, |C| \u2264 s02}, with |Bi| = ( s02 s02\u2212i )\u2211i j=0 (|G|\u2212s02 j ) ; i = 0, \u00b7 \u00b7 \u00b7 , s02.\nTo bound the error probability, let L(x) = \u2212 12\u2016Ax\u2212 y\u20162 be the likelihood. Note that\n{x\u0302 6= x\u0302o} \u2286 {L(x\u0302)\u2212 L(x\u0302o) \u2265 0} \u2286 {L(x\u0302)\u2212 L(x0) \u2265 0}.\nThis together with {x\u0302 6= x\u0302o} \u2286 {x\u0302 \u2208 S} implies that\n{x\u0302 6= x\u0302o} \u2286 {L(x\u0302)\u2212 L(x0) \u2265 0} \u2229 {x\u0302 \u2208 S}.\nConsequently,\nI \u2261 P ( x\u0302 6= x\u0302o )\n\u2264 P ( L(x\u0302)\u2212 L(x0) \u2265 0; x\u0302 \u2208 S )\n\u2264 s02\u2211\ni=1\n\u2211\nC\u2208Bi\n\u2211\nSAC,C\nP \u2217 (\nsup x\u2208SAC,C\n( L(x)\u2212 L(x0) ) \u2265 0\n)\n\u2264 s02\u2211\ni=1\ns01\u2211\nj=1\n\u2211\n|C|=i,|AG|=j P \u2217\n( sup{ \u2212log(1\u2212h2(x,x0))\u2265max(i,1)Cmin(x0)\u2212d3\u03c4d2p,x\u2208SAC,C } ( L(x)\u2212 L(x0) ) \u2265 0 ) ,\nwhere P \u2217 is the outer measure and the last two inequalities use the fact that SAC ,C \u2286 {x \u2208 SAC ,C : max(|C0 \\C|, 1)Cmin(x0) \u2264 \u2212 log(1\u2212h2(x,x0))} \u2286 {\u2212 log(1\u2212h2(x,x0)) \u2265 d1 max(i, 1)Cmin(x0)\u2212 d3\u03c4d2p}, under Assumption 3.\nFor I, we apply Theorem 1 of [22] to bound each term. Towards this end, we verify their entropy condition (3.1) for the local entropy over SAC ,C for |C| = 1, \u00b7 \u00b7 \u00b7 , s02 and |A| = 1, \u00b7 \u00b7 \u00b7 , s01. Under Assumption 2 \u03b5 = \u03b5n,p = (2c0) 1/2c\u221214 log(2 1/2/c3) log p( s01 n ) 1/2 satisfies there with respect to \u03b5 > 0, that is,\nsup {0\u2264|A|\u2264p0}\n\u222b 21/2\u03b5\n2\u22128\u03b52 H1/2(t/c3,Fji)dt \u2264 p1/20 21/2\u03b5 log(2/21/2c3) \u2264 c4n1/2\u03b52. (16)\nfor some constant c3 > 0 and c4 > 0, say c3 = 10 and c4 = (2/3)5/2 512 . By Assumption 2, Cmin(x 0) \u2265 \u03b52n,p0,p implies (16), provided that s01 \u2265 (2c0)1/2c\u221214 log(21/2/c3).\nNote that |Bi| = ( s02 s02\u2212i )\u2211i j=0 (|G|\u2212s02 j ) \u2264 (|G|(|G| \u2212 s02)i \u2264 (|G|2/4)i by the binomial coefficients formula.\nMoreover, \u2211s01 j=1 2 jij \u2264 is01 , and \u2211j1+\u00b7\u00b7\u00b7+ji=j ( j j1,\u00b7\u00b7\u00b7ji ) 2j = (2i)j using the Multinomial Theorem. By Theorem 1 of [22], there exists a constant c2 > 0, say c2 = 4 27 1 1926 ,\nI \u2264 s02\u2211\ni=1\n|Bi| s01\u2211\nj=1\n\u2211\n(j1,\u00b7\u00b7\u00b7ji)\n( j\nj1, \u00b7 \u00b7 \u00b7 ji\n) 2j1 \u00b7 \u00b7 \u00b7 2ji exp ( \u2212 c2niCmin(x0) )\n\u2264 s02\u2211\ni=1\nexp ( \u2212 c2niCmin(x0) + 2i(log |G|+ log s01) )\n\u2264 exp ( \u2212 c2nCmin(x0) + 2(log |G|+ log s01) ) .\nLet G = {x\u0302 6= x\u03020}. For the risk property, Eh2(x\u0302,x0) = Eh2(x\u03020,x0)+Eh2(x\u0302,x0)I(G) is upper bounded by\nEh2(x\u0302,x0) + exp ( \u2212 c2nCmin(x0) + 2(log |G|+ log s01) ) = (1 + o(1))Eh2(x\u03020,x0),\nusing the fact that h(x\u0302,x0) \u2264 1. This completes the proof."}, {"heading": "2 Proof of Theorem 3", "text": "We utilize an intermediate lemma from [4]:\nLemma 2. Let X be a metric space and U be a normed space. Suppose that for all x \u2208 X, the function \u03c8(x, \u00b7) is differentiable and that \u03c8(x, Y ) and DY \u03c8(x, Y ) (the partial derivative of \u03c8(x, Y ) with respect to Y ) are continuous on X \u00d7 U . Let \u03a6 be a compact subset of X. Define the optimal value function as \u03c6(Y ) = infx\u2208\u03a6 \u03c8(x, Y ). The optimal value function \u03c6(Y ) is directionally differentiable. In addition, if for any Y \u2208 U , \u03c8(\u00b7, Y ) has a unique minimizer x(Y ) over \u03a6, then \u03c6(Y ) is differentiable at Y and the gradient of \u03c6(Y ) is given by \u03c6\u2032(Y ) = DY \u03c8(x(Y ), Y ).\nProof of Theorem 3. For the proof, an intermediate lemma will be used, with its details given in the Appendix. Since both constraints are active, if (x, \u03bb, \u03b7) = SGLP(v, s1, s2), then x and \u03bb are also the optimal solutions to the following problem:\nmaximize \u03bb minimize x\u2208X\n\u03c8(x, \u03bb) = 1\n2 \u2016x\u2212 v\u201622 + \u03bb(\u2016x\u20161 \u2212 s1),\nwhere X = {x : \u2016x\u2016G \u2264 s2}. By Lemma 2, \u03c6(\u03bb) = infx\u2208X \u03c8(x, \u03bb) is differentiable with the derivative given by \u2016x\u20161. In addition, as a pointwise infimum of a concave function, so does \u03c6(\u03bb) [6] and its derivative, \u2016x\u20161, is non-increasing. Therefore s1 = \u2016x\u20161 is non-decreasing as \u03bb becomes smaller. This completes the proof.\n3 Algorithm for Solving (13)\nWe give a detailed description of algorithm for solving the restricted projection (13) in Algorithm 3.\nAlgorithm 3 Restricted Sparse Group Lasso Projection Algorithm\nInput: v, s1, s2, T1, T3 Output: an optimal solution x to the Restricted Sparse Group Projection Problem (13)\nFunction RSGLP(v, s1, s2, T1, T3)\n1: if \u2016xT1\u20161 \u2264 s1 and \u2016xT3\u2016G \u2264 s2 then 2: return v 3: end if 4: x (T1) c\nC1 = v(T1)\nc\n, xT1C1 = P s1 1 (v T1)\n5: x (T3)\nc\nC2 = v(T3)\nc\n, xT3C2 = P s2 G (v T3)\n6: x (T1)\nc\nC12 = v(T1)\nc\n, xT1C12= bisec(v, s1, s2, T1, T3)\n7: if \u2016xT3C1\u2016G \u2264 s2 then 8: return xC1 9: else if \u2016xT1C2\u20161 \u2264 s1 then\n10: return xC2 11: else 12: return xC12 13: end if\nFunction bisec(v, s1, s2, T1, T3)\n1: Initialize up, low and tol 2: while up\u2212 low > tol do 3: \u03bb\u0302 = (low + up)/2 4: if (15) has a solution \u03b7\u0302 given v\u03bb\u0302 then\n5: calculate s\u03021 using \u03b7\u0302 and \u03bb\u0302. 6: if s\u03021 \u2264 s1 then 7: up = \u03bb\u0302 8: else\n9: low = \u03bb\u0302 10: end if 11: else 12: up = \u03bb\u0302 13: end if 14: end while 15: \u03bb\u2217 = up 16: Solve (15) to get \u03b7\u2217 17: Calculate (x\u2217)T1 from \u03bb\u2217 and \u03b7\u2217. 18: return (x\u2217)T1"}, {"heading": "4 Accelerated Gradient Method", "text": "The AGM procedure is listed in Algorithms 4, in which f(x) is the objective function 12\u2016Ax \u2212 y\u201622 with \u2207f(x) denotes its gradient at x. In addition, fL,u(x) is the linearization of f(x) at u defined as follows:\nfL,u(x) = f(u) +\u2207f(u)T (x\u2212 u) + L\n2 \u2016x\u2212 u\u201622.\nAlgorithm 4 Accelerated Gradient Method [18, 2] for (7)\nInput: A, y, s1, s2, L0, x0, Output: solution x to (7) 1: Initialize: L0, x1 = x0, \u03b1\u22121 = 0, \u03b10 = 1, t = 0. 2: repeat\n3: t = t+ 1, \u03b2t = \u03b1t\u22122\u22121 \u03b1t\u22121 , ut = xt + \u03b2t(xt \u2212 xt\u22121) 4: Line search: Find the smallest L = 2jLt\u22121 such that\nf(xt+1) \u2264 fL,ut(xt+1),\nwhere xt+1 = SGLP(ut \u2212 1L\u2207f(ut), s1, s2) 5: \u03b1t+1 = 1+ \u221a 1+4\u03b12t 2 , Lt = L. 6: until Converge 7: return xt"}, {"heading": "5 ADMM Projection algorithm", "text": "ADMM is widely chosen for its capability of decomposing coupled variables/constraints, which is exactly the case in our projection problem. Before applying ADMM, we transform (8) into an equivalent form as follows:\nminimize x\n1 2 \u2016x\u2212 v\u201622\nsubject to \u2016u\u20161 \u2264 s1 \u2016w\u2016G \u2264 s2 u = x,w = x.\nThe augmented Lagrangian is:\nL(x,\u03bb,\u03b7) = 1 2 \u2016x\u2212 v\u201622 + \u03bbT (u\u2212 x) + \u03b7T (w \u2212 x)\n+ \u03c1\n2 (\u2016u\u2212 x\u201622 + \u2016w \u2212 x\u201622).\nUtilize the scaled form [5], i.e., let \u03bb = \u03bb\u03c1 , \u03b7 = \u03b7 \u03c1 , we can obtain an equivalent augmented Lagrangian:\nL(x,\u03bb,\u03b7) = 1 2 \u2016x\u2212 v\u201622 + \u03c1 2 (\u2016x\u2212 u\u2212 \u03bb\u201622 + \u2016x\u2212w \u2212 \u03b7\u201622)\n\u2212 \u03c1 2 (\u2016\u03bb\u201622 + \u2016\u03b7\u201622).\nNow we calculate the optimal x, \u03bb and \u03b7 through alternating minimization. For fixed u and w, the optimal x possesses a closed-form solution:\nx = 1\n1 + 2\u03c1 (v + \u03c1(u+ \u03bb +w + \u03b7)) .\nFor fixed x and u, finding the optimal w is a group lasso projection:\nminimize w\n1 2 \u2016w \u2212 (x\u2212 \u03b7)\u201622\nsubject to \u2016w\u2016G \u2264 s2 (17)\nFor fixed x and w, finding the optimal u amounts to solve an L1-ball projection:\nminimize u\n1 2 \u2016u\u2212 (x\u2212 \u03bb)\u201622\nsubject to \u2016u\u20161 \u2264 s1. (18)\nThe update of multipliers is standard as follows:\n\u03bb = \u03bb+ u\u2212 x \u03b7 = \u03b7 +w \u2212 x (19)\nAlgorithm 5 summarizes the above procedure. Note that, the value of the penalty term \u03c1 is fixed in Algorithm 5. However, in our implementation, we increase \u03c1 whenever necessary to obtain faster convergence.\nAlgorithm 5 ADMM [5] for (8)\nInput: v, s1, s2 Output: an optimal solution x to (8) 1: Initialize: x0, u0, w0, \u03bb0, \u03b70, t = 0, \u03c1 > 0 2: repeat\n3: t = t+ 1 4: xt = 1 1+2\u03c1 (v + \u03c1(ut\u22121 + \u03bbt\u22121 +wt\u22121 + \u03b7t\u22121)) 5: wt = Ps2G (xt \u2212 \u03b7t\u22121) 6: ut = Ps11 (xt \u2212 \u03bbt\u22121) 7: \u03bbt = \u03bbt\u22121 + ut \u2212 xt, \u03b7t = \u03b7t\u22121 +wt \u2212 xt. 8: until Converge 9: return xt"}, {"heading": "6 Dykstra\u2019s Algorithm", "text": "Dykstra\u2019s algorithm is a general scheme to compute the projection onto intersections of convex sets. It is carried out by taking Euclidean projections onto each convex set alternatively in a smart way and is guaranteed to converge for least squares objective function [7]. The details of applying Dykstra\u2019s Algorithm to our projection problem are listed in Algorithm 6.\nAlgorithm 6 Dykstra\u2019s Algorithm [7] for (8)\nInput: v, s1, s2 Output: an optimal solution x to (8) 1: Initialize: x0 = v, p0 = 0, q0 = 0, t = 0 2: repeat\n3: t = t+ 1 4: yt\u22121 = Ps2G (xt\u22121 + pt\u22121) 5: pt = xt\u22121 + pt\u22121 \u2212 yt\u22121 6: xt = Ps11 (yt\u22121 + qt\u22121) 7: qt = yt\u22121 + qt\u22121 \u2212 xt 8: until Converge 9: return xt"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Sparse feature selection has been demonstrated to be effective in handling high-dimensional data.<lb>While promising, most of the existing works use convex methods, which may be suboptimal in terms<lb>of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex<lb>paradigm to sparse group feature selection, which is motivated by applications that require identifying<lb>the underlying group structure and performing feature selection simultaneously. The main contributions<lb>of this article are twofold: (1) statistically, we introduce a nonconvex sparse group feature selection<lb>model which can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter<lb>estimation can be achieved; (2) computationally, we propose an efficient algorithm that is applicable to<lb>large-scale problems. Numerical results suggest that the proposed nonconvex method compares favorably<lb>against its competitors on synthetic data and real-world applications, thus achieving desired goal of<lb>delivering high performance.", "creator": "LaTeX with hyperref package"}}}