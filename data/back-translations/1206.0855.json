{"id": "1206.0855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2012", "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch", "abstract": "Partially observable Markov decision-making processes have been widely used to provide models of decision-making problems in the real world. In this paper, we will present a method where a slightly different version of them, the so-called Mixed Observability Markov Decision Process, MOMDP, will be added to our problem. Essentially, we aim to provide a behavioral model of how intelligent actors interact with the musical pitch environment, and we will show how MOMDP can shed some light on building a decision-making model for musical pitches.", "histories": [["v1", "Tue, 5 Jun 2012 09:35:44 GMT  (1283kb,D)", "http://arxiv.org/abs/1206.0855v1", "In 5th International Workshop on Machine Learning and Music, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)"]], "COMMENTS": "In 5th International Workshop on Machine Learning and Music, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["pouyan rafiei fard", "keyvan yahya"], "accepted": false, "id": "1206.0855"}, "pdf": {"name": "1206.0855.pdf", "metadata": {"source": "META", "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch", "authors": ["Pouyan Rafiei Fard"], "emails": ["rafieifard@ce.sharif.edu", "kxy054@bham.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Partially observable Markov decision processes (POMDPs) have been widely used to provide models for real-world decision making problems. They provide a mathematical framework to model the interaction between the agent and its environment. One of the most notable characteristics of POMDPs is their ability to keep planning in dynamic environments and under uncertainty (Ong et al., 2010). To our knowledge, only a few authors have previously mentioned MDPs and POMDPs in the field of computer music. Among them, we could mention (Martin et al. , 2010) who demonstrated the use of POMDPs to control musical behaviour in different conditions.\nIn this paper, we propose a novel model for interaction of the agents with musical pitch environment\nbased on a variant of POMDPs called mixed observ-\nability Markov decision process (Ong et al., 2010).\nIn 5th International Workshop on Machine Learning and Music, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nFirst, we mention the theoretical background of our work. In section 3, we propose our model for musical pitch based on MOMDPs. Section 4 addresses some implementation issues and presents an experiment to evaluate our model. Finally, we make our concluding remarks and discuss about the prospective potential developments of this models and its applications."}, {"heading": "2. The Basic Idea of MOMDP", "text": "Beside the standard models of POMDP, there is a model called MOMDP that makes a slightly different with the former one. The latter is basically a factored POMDP which benefits from factorizing its states. In a MOMDP model, a state s is factored into two different variables x, y. So by writing s = (x, y) we mean that s is consisted of two variables such that x stands for fully observable state and y stands for partially observable state. Thus having been factorized, we would have a mixed system space S = X \u00d7 Y where X is the state of all values for x and either does Y for y.\nFig. 1. The standard POMDP model (left) and the MOMDP model (right). A MOMDP state s is factored into two variables: s = (x, y), where x is fully observable and y is partially observable.\nmodel a MOMDP. In a MOMDP, the fully observable state components are represented as a single state variable x, while the partially observable components are represented as another state variable y. Thus (x, y) specifies the complete system state, and the state space is factored as S = X \u00d7 Y , where X is the space of all possible values for x and Y is the space of all possible values for y. In our AUV example, x represents the depth and the orientation (d, \u03b8), and y represents the horizontal position p. Formally a MOMDP model is specified as a tuple (X ,Y ,A,O, TX , TY , Z,R, \u03b3). The conditional probability function TX (x, y, a, x\u2032) = p(x\u2032|x, y, a) gives the probability that the fully observable state variable has value x\u2032 if the robot takes action a in state (x, y), and TY(x, y, a, x\u2032, y\u2032) = p(y\u2032|x, y, a, x\u2032) gives the probability that the partially observable state variable has value y\u2032 if the robot takes action a in state (x, y) and the fully observable state variable has value x\u2032. Compared with the standard POMDP model, the MOMDP model uses a factored state-space representation X \u00d7 Y , with the corresponding probabilistic state-transition functions TX and TY . All other aspects remain the same. See Fig. 1 for a comparison.\u2217 So far, the changes introduced by the MOMDP model seem mostly notational. The computational advantages become apparent when we consider the belief space B. Since the state variable x is fully observable and known exactly, we only need to maintain a belief bY , a probability distribution on the state variable y. Any belief b \u2208 B on the complete system state s = (x, y) is then represented as (x, bY). Let BY denote the space of all beliefs on y. We now associate with each value x of the fully observable state variable a belief space for y: BY(x) = {(x, bY) | bY \u2208 BY}. BY(x) is a subspace in B, and B is a union of these subspaces: B =\u22c3\nx\u2208X BY(x). Observe that while B has |X ||Y| dimensions, where |X | and |Y| are the number of states in X and Y , each BY(x) has only |Y| dimensions. Effectively we represent the high-dimensional space B as a union of lowerdimensional subspaces. When the uncertainty in a system is small, specifically, when |Y| is small, the MOMDP model leads to dramatic improvement in computational efficiency, due to the reduced dimensionality of the space. Now consider how we would represent and execute a\n\u2217A MOMDP can be regarded as an instance of dynamic Bayesian network (DBN). Following the DBN methodology, we could factor x or y further, but this may lead to difficulty in value function representation.\nMOMDP policy. As mentioned in Section II-A, a POMDP policy can be represented as a value function V (b) = max\u03b1\u2208\u0393(\u03b1 \u00b7 b), where \u0393 is a set of \u03b1-vectors. In a MOMDP, a belief is given by (x, bY), and the belief space B is union of subspaces BY(x) for x \u2208 X . Correspondingly, a MOMDP value function V (x, bY) is represented as a collection of \u03b1vector sets: {\u0393Y(x) | x \u2208 X}, where for each x, \u0393Y(x) is a set of \u03b1-vectors defined over BY(x). To evaluate V (x, bY), we first find the right \u03b1-vector set \u0393Y(x) using the x value and then find the maximum \u03b1-vector from the set:\nV (x, bY) = max\u03b1\u2208\u0393Y(x)(\u03b1 \u00b7 bY). (3) In general, any value function V (b) = max\u03b1\u2208\u0393(\u03b1 \u00b7 b) can be represented in this new form, as stated in the theorem below. Theorem 1: Let B = \u22c3x\u2208X BY(x) be the belief space of a MOMDP with state space X\u00d7Y . If V (b) = max\u03b1\u2208\u0393(\u03b1\u00b7b) is any value function over B in the standard POMDP form, then V (b) is equivalent to a MOMDP value function V \u2032(x, bY) = max\u03b1\u2208\u0393Y (x)(\u03b1\u00b7bY) such that for any b = (x, bY) with b \u2208 B, x \u2208 X , and bY \u2208 BY(x), V (b) = V \u2032(x, bY).\u2020. Geometrically, each \u03b1-vector set \u0393Y(x) represents a restriction of the POMDP value function V (b) to the subspace BY(x): Vx(bY) = max\u03b1\u2208\u0393Y (x)(\u03b1 \u00b7 bY). In a MOMDP, we compute only these lower-dimensional restrictions {Vx(bY) | x \u2208 X}, because B is simply a union of subspaces BY(x) for x \u2208 X . A comparison of (1) and (3) also indicates that (3) often results in faster policy execution, because action selection can be performed more efficiently. First, each \u03b1-vector in \u0393Y(x) has length |Y|, while each \u03b1-vector in \u0393 has length |X ||Y|. Furthermore, in a MOMDP value function, the \u03b1vectors are partitioned into groups according to the value of x. We only need to calculate the maximum over \u0393Y(x), which is potentially much smaller than \u0393 in size. In summary, by factoring out the fully and partially observable state variables, a MOMDP model reveals the internal structure of the belief space as a union of lowerdimensional subspaces. We want to exploit this structure and perform all operations on beliefs and value functions in these lower-dimensional subspaces rather than the original belief space. Before we describe the details of our algorithm, let us first look at how MOMDPs can be used to handle uncertainty commonly encountered in robotic systems."}, {"heading": "B. Modeling Robotic Tasks with MOMDPs", "text": "Sensor limitations are a major source of uncertainty in robotic systems and are closely related to observability. If a robot\u2019s state consists of several components, some are fully observable, possibly due to accurate sensing, but others are not. This is a natural case for modeling with MOMDPs. All fully observable components are grouped together and modeled by the variable x. The other components are modeled by the variable y.\n\u2020Due to space limitations, the proofs of all the theorems are provided in the full version of the paper, which will be available on-line at http: //motion.comp.nus.edu.sg/papers/rss09.pdf\nar X\niv :1\n20 6.\n08 55\nv1 [\ncs .A\nI] 5\nJ un\n2 01"}, {"heading": "3. The Proposed Model", "text": "From music theory, we know that any compound interval can be decomposed into some octaves and a simple interval which this idea can also be brought to any other simple intervals. In our MOMDP-based model, the agent makes its decisions according to the states that it receives from the environment which is here the musical pitch space. A MOMDP is denoted by the tuple (X,Y,A,O, Tx, Ty, Z,R, \u03b3). The relationship between these quantities and the musical concepts of our model are given elaborately in the following.\nAt each time step the environment is in a state s \u2208 S where s = (x, y) and x \u2208 X is a fully observable state whereas y \u2208 Y is a partially observable one. In our model, a fully observable state represents a musical pitch in which the agent is having a precise estimate of its frequency at the time t plus the interval the agent is supposed to make. For the sake of simplicity, we only consider the natural musical pitches and the main intervals not beyond the octave interval. So, we have S = {\u2032C \u2032,\u2032D\u2032,\u2032E\u2032,\u2032 F \u2032,\u2032G\u2032,\u2032A\u2032,\u2032B\u2032}\u00d7 {\u20321st\u2032,\u2032 2nd\u2032,\u2032 3rd\u2032, ..,\u2032 7th\u2032}.A is the set of actions available to the agent. Here an action a \u2208 A stands for a making a transition via a musical interval decomposition. In each state we define the possible actions with a set of decompositions. Relatively, the environment lies in partially observable states y \u2208 Y as the intermediate state regarding which one of actions the agent makes. Technically, the space of partially observable states is the same space for fully observable states. The parameter O is a set of observations that the agent makes which is the possible values of this parameter is the same as values from X and Y . Finally, The R parameter is R1\u03b3+R2, where \u03b3 is the discount factor while R1 is the reward for this first interval decomposition and R2 is the reward given to the second one."}, {"heading": "4. Experiment: Reinforcing patterns", "text": "For testing our model, we developed a Q-Learning algorithm (Watkins, 1989) to perform a similar task to which was done in (Cont, 2008). We made interactions with the system by feeding a relative pitch pattern as depicted in Figure 2., into the system. For this learning experiment, we set the learning parameters \u03b1 = 0.4, \u03b3 = 0.5 and N = 20 as the number of interactions. For a better demonstration of musical learning, the results are presented as intervals and notes. Thus, the y-axis of Figure 3. indicates the intervals and the x-axis is for the notes and for each interval-note pair. The gray-scale values show the learned Q-value and the intensity of these them shows the policy learned\nby the agent. Also, the values indicated with red rectangles are the values which was originally fed into the system via the pitch contour."}, {"heading": "5. Conclusion", "text": "The results of our experiments imply that our agent efficiently learned a behaviour policy. In addition, from Figure 3. we can see that not only our agent learned the given pitch contour (shown by red rectangles) but also some other state-action pairs. This is mainly happened because our method benefits from factorizing each state into a couple of fully-observable and partially-observable states. So, this approach will obviously help to have a faster convergence of the agent which is interacting with musical pitch environment."}], "references": [{"title": "Planning under Uncertainty for Robotic Tasks with Mixed Observability", "author": ["S.C.W. Ong", "S.W. Png", "Hsu D", "W.S. Lee"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Ong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2010}, {"title": "Partially Observable Markov Decision Processes for Interactive Music Systems", "author": ["A. Martin", "C. Jin", "A. van Schaik", "W.L. Martens"], "venue": "In Proceedings of the International Computer Music Conference (ICMC", "citeRegEx": "Martin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2010}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": "PhD thesis, Cambridge University,", "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}, {"title": "Modeling Musical Anticipation: From the time of music to the music of time", "author": ["A. Cont"], "venue": "PhD thesis, University of Paris 6 and University of California in San Diego,", "citeRegEx": "Cont,? \\Q2008\\E", "shortCiteRegEx": "Cont", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "One of the most notable characteristics of POMDPs is their ability to keep planning in dynamic environments and under uncertainty (Ong et al., 2010).", "startOffset": 130, "endOffset": 148}, {"referenceID": 0, "context": "In this paper, we propose a novel model for interaction of the agents with musical pitch environment based on a variant of POMDPs called mixed observability Markov decision process (Ong et al., 2010).", "startOffset": 181, "endOffset": 199}, {"referenceID": 0, "context": "y (adapted from (Ong et al., 2010)).", "startOffset": 16, "endOffset": 34}, {"referenceID": 3, "context": "For testing our model, we developed a Q-Learning algorithm (Watkins, 1989) to perform a similar task to which was done in (Cont, 2008).", "startOffset": 122, "endOffset": 134}], "year": 2012, "abstractText": "Partially observable Markov decision processes have been widely used to provide models for real-world decision making problems. In this paper, we will provide a method in which a slightly different version of them called Mixed observability Markov decision process, MOMDP, is going to join with our problem. Basically, we aim at offering a behavioural model for interaction of intelligent agents with musical pitch environment and we will show that how MOMDP can shed some light on building up a decision making model for musical pitch conveniently.", "creator": "LaTeX with hyperref package"}}}