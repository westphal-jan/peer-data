{"id": "1206.3232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "AND/OR Importance Sampling", "abstract": "In contrast to the meaning sampling, AND / OR meaning stores samples in the AND / OR space and then extracts a new mean from the stored samples. We demonstrate that samples with AND / OR meaning can have a lower variance than samples with meaning, providing a theoretical justification for preferring them to a meaning sampling. Our empirical analysis shows that in many cases, samples with AND / OR meaning are much more accurate than samples with meaning.", "histories": [["v1", "Wed, 13 Jun 2012 12:33:40 GMT  (365kb)", "http://arxiv.org/abs/1206.3232v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vibhav gogate", "rina dechter"], "accepted": false, "id": "1206.3232"}, "pdf": {"name": "1206.3232.pdf", "metadata": {"source": "CRF", "title": "AND/OR Importance Sampling", "authors": ["Vibhav Gogate"], "emails": ["vgogate@ics.uci.edu", "dechter@ics.uci.edu"], "sections": [{"heading": null, "text": "The paper introduces AND/OR importance sampling for probabilistic graphical models. In contrast to importance sampling, AND/OR importance sampling caches samples in the AND/OR space and then extracts a new sample mean from the stored samples. We prove that AND/OR importance sampling may have lower variance than importance sampling; thereby providing a theoretical justification for preferring it over importance sampling. Our empirical evaluation demonstrates that AND/OR importance sampling is far more accurate than importance sampling in many cases."}, {"heading": "1 Introduction", "text": "Many problems in graphical models such as computing the probability of evidence in Bayesian networks, solution counting in constraint networks and computing the partition function in Markov random fields are summation problems, defined as a sum of a function over a domain. Because these problems are NP-hard, sampling based techniques are often used to approximate the sum. The focus of the current paper is on importance sampling.\nThe main idea in importance sampling [Geweke, 1989, Rubinstein, 1981] is to transform the summation problem to that of computing a weighted average over the domain by using a special distribution called the proposal (or importance) distribution. Importance sampling then generates samples from the proposal distribution and approximates the true average over the domain by an average over the samples; often referred to as the sample average. The sample average is simply a ratio of the sum of sample weights and the number of samples, and it can be computed in a memory-less fashion since it requires keeping only these two quantities in memory.\nThe main idea in this paper is to equip importance sampling with memoization or caching in order to exploit conditional\nindependencies that exist in the graphical model. Specifically, we cache the samples on an AND/OR tree or graph [Dechter and Mateescu, 2007] which respects the structure of the graphical model and then compute a new weighted average over that AND/OR structure, yielding, as we show, an unbiased estimator that has a smaller variance than the importance sampling estimator. Similar to AND/OR search [Dechter and Mateescu, 2007], our new AND/OR importance sampling scheme recursively combines samples that are cached in independent components yielding an increase in the effective sample size which is part of the reason that its estimates have lower variance.\nWe present a detailed experimental evaluation comparing importance sampling with AND/OR importance sampling on Bayesian network benchmarks. We observe that the latter outperforms the former on most benchmarks and in some cases quite significantly.\nThe rest of the paper is organized as follows. In the next section, we describe preliminaries on graphical models, importance sampling and AND/OR search spaces. In sections 3, 4 and 5 we formally describe AND/OR importance sampling and prove that its sample mean has lower variance than conventional importance sampling. Experimental results are described in section 6 and we conclude with a discussion of related work and summary in section 7."}, {"heading": "2 Preliminaries", "text": "We represent sets by bold capital letters and members of a set by capital letters. An assignment of a value to a variable is denoted by a small letter while bold small letters indicate an assignment to a set of variables.\nDefinition 2.1 (belief networks). A belief network (BN) is a graphical model R = (X,D,P), where X = {X1, . . . ,Xn} is a set of random variables over multi-valued domains D = {D1, . . . ,Dn}. Given a directed acyclic graph G over X, P = {Pi}, where Pi = P(Xi|pa(Xi)) are conditional probability tables (CPTs) associated with each Xi. pa(Xi) is the set of parents of the variable Xi in G. A belief network represents a probability distribution over X, P(X) =\n\u220fni=1P(Xi|pa(Xi)). An evidence set E = e is an instantiated subset of variables.The moral graph (or primal graph) of a belief network is the undirected graph obtained by connecting the parent nodes and removing direction. Definition 2.2 (Probability of Evidence). Given a belief network R and evidence E = e, the probability of evidence P(E = e) is defined as:\nP(e) = \u2211 X\\E\nn\n\u220f j=1 P(X j|pa(X j))|E=e (1)\nThe notation h(X)|E=e stands for a function h over X \\E with the assignment E = e."}, {"heading": "2.1 AND/OR search spaces", "text": "We can compute probability of evidence by search, by accumulating probabilities over the search space of instantiated variables. In the simplest case, this process defines an OR search tree, whose nodes represent partial variable assignments. This search space does not capture the structure of the underlying graphical model. To remedy this problem, [Dechter and Mateescu, 2007] introduced the notion of AND/OR search space. Given a bayesian network R = (X,D,P), its AND/OR search space is driven by a pseudo tree defined below. Definition 2.3 (Pseudo Tree). Given an undirected graph G = (V,E), a directed rooted tree T = (V,E) defined on all its nodes is called pseudo tree if any arc of G which is not included in E is a back-arc, namely it connects a node to an ancestor in T . Definition 2.4 (Labeled AND/OR tree). Given a graphical model R = \u3008X,D,P\u3009, its primal graph G and a backbone pseudo tree T of G, the associated AND/OR search tree, has alternating levels of AND and OR nodes. The OR nodes are labeled Xi and correspond to the variables. The AND nodes are labeled \u3008Xi,xi\u3009 and correspond to the value assignments in the domains of the variables. The structure of the AND/OR search tree is based on the underlying backbone tree T . The root of the AND/OR search tree is an OR node labeled by the root of T .\nEach OR arc, emanating from an OR node to an AND node is associated with a label which can be derived from the CPTs of the bayesian network\n[Dechter and Mateescu, 2007]. Each OR node and AND node is also associated with a value that is used for computing the quantity of interest.\nSemantically, the OR states represent alternative assignments, whereas the AND states represent problem decomposition into independent subproblems, all of which need be solved. When the pseudo-tree is a chain, the AND/OR search tree coincides with the regular OR search tree. The probability of evidence can be computed from a labeled AND/OR tree by recursively computing the value of all nodes from leaves to the root [Dechter and Mateescu, 2007]. Example 2.5. Figure 1(a) shows a bayesian network over seven variables with domains of {0,1}. F and G are evidence nodes. Figure 1(c) shows the AND/OR-search tree for the bayesian network based on the Pseudo-tree in Figure 1(b). Note that because F and G are instantiated, the search space has only 5 variables."}, {"heading": "2.2 Computing Probability of Evidence Using Importance Sampling", "text": "Importance sampling [Rubinstein, 1981] is a simulation technique commonly used to evaluate the sum, M = \u2211x\u2208X f (x) for some real function f . The idea is to generate samples x1, . . . ,xN from a proposal distribution Q (satisfying f (x) > 0 \u21d2 Q(x) > 0) and then estimate M as follows:\nM = \u2211 x\u2208X f (x) = \u2211 x\u2208X\nf (x)\nQ(x) Q(x) = EQ[\nf (x)\nQ(x) ] (2)\nM\u0302 = 1\nN\nN\n\u2211 i=1\nw(xi) , where w(xi) = f (xi)\nQ(xi) (3)\nw is often referred to as the sample weight. It is known that the expected value E(M\u0302) = M [Rubinstein, 1981].\nTo compute the probability of evidence by importance sampling, we use the substitution:\nf (x) = n\n\u220f j=1 P(X j|pa(X j))|E=e (4)\nSeveral choices are available for the proposal distribution Q(x) ranging from the prior distribution as in likelihood weighting to more sophisticated alternatives such as\nIJGP-Sampling [Gogate and Dechter, 2005] and EPIS-BN [Yuan and Druzdzel, 2006] where the output of belief propagation is used to compute the proposal distribution.\nAs in prior work [Cheng and Druzdzel, 2000], we assume that the proposal distribution is expressed in a factored product form: Q(X) = \u220fni=1Qi(Xi|X1, . . . ,Xi\u22121) = \u220fni=1Qi(Xi|Yi), where Yi \u2286 {X1, . . . ,Xi\u22121}, Qi(Xi|Yi) = Q(Xi|X1, . . . ,Xi\u22121) and |Yi| < c for some constant c. We can generate a full sample from Q as follows. For i = 1 to n, sample Xi = xi from the conditional distribution Q(Xi|X1 = x1, . . . ,Xi\u22121 = xi\u22121) and set Xi = xi."}, {"heading": "3 AND/OR importance sampling", "text": "We first discuss computing expectation by parts; which forms the backbone of AND/OR importance sampling. We then present the AND/OR importance sampling scheme formally and derive its properties."}, {"heading": "3.1 Estimating Expectation by Parts", "text": "In Equation 2, the expectation of a multi-variable function is computed by summing over the entire domain. This method is clearly inefficient because it does not take into account the decomposition of the multi-variable function as we illustrate below.\nConsider the tree graphical model given in Figure 2(a). Let A = a and B = b be the evidence variables. Let Q(ZXY ) = Q(Z)Q(X |Z)Q(Y |Z) be the proposal distribution. For simplicity, let us assume that f (Z) = P(Z), f (XZ) = P(Z|X)P(A = a|X) and f (YZ) = P(Z|Y )P(B = b|Y ). We can express probability of evidence P(a,b) as:\nP(a,b) = \u2211 XYZ\nf (Z) f (XZ) f (YZ)\nQ(Z)Q(X |Z)Q(Y |Z) Q(Z)Q(X |Z)Q(Y |Z)\n= E\n[ f (Z) f (XZ) f (YZ)\nQ(Z)Q(X |Z)Q(Y |Z)\n] (5)\nWe can decompose the expectation in Equation 5 into smaller components as follows:\nP(a,b) = \u2211 Z\nf (Z)Q(Z)\nQ(Z) (\n\u2211 X\nf (XZ)Q(X |Z)\nQ(X |Z)\n)(\n\u2211 Y\nf (YZ)Q(Y |Z)\nQ(Y |Z)\n) (6)\nThe quantities in the two brackets in Equation 6 are, by definition, conditional expectations of a function over X and Y respectively given Z. Therefore, Equation 6 can be written as:\nP(a,b) = \u2211 Z\nf (Z) Q(Z) E\n[ f (XZ)\nQ(X |Z) |Z\n] E [ f (YZ)\nQ(Y |Z) |Z\n] Q(Z) (7)\nBy definition, Equation 7 can be written as:\nP(a,b) = E\n[ f (Z)\nQ(Z) E\n[ f (XZ)\nQ(X |Z) |Z\n] E [ f (YZ)\nQ(Y |Z) |Z\n]] (8)\nWe will refer to Equations of the form 8 as expectation by parts borrowing from similar terms such as integration and summation by parts. If the domain size of all variables is d = 3, for example, computing expectation using Equation 5 would require summing over d3 = 33 = 27 terms while computing the same expectation by parts would require summing over d+d2 +d2 = 3+32 +32 = 21 terms. Therefore, exactly computing expectation by parts is clearly more efficient.\nImportance sampling ignores the decomposition of expectation while approximating it by the sample average. Our new algorithm estimates the true expectation by decomposing it into several conditional expectations and then approximating each by an appropriate weighted average over the samples. Since computing expectation by parts is less complex than computing expectation by summing over the domain; we expect that approximating it by parts will be easier as well. We next illustrate how to estimate expectation by parts on our example Bayesian network given in Figure 2(a).\nAssume that we are given samples (z1,x1,y1), . . . ,(zN ,xN ,yN) generated from Q decomposed according to Figure 2(a). For simplicity, let {0,1} be the domain of Z and let Z = 0 and Z = 1 be sampled N0 and\nN1 times respectively. We can approximate E [ f (XZ) Q(X |Z) |Z ] and E [\nf (YZ) Q(Y |Z) |Z ] by \u0302gX (Z = j) and \u0302gY (Z = j) defined\nbelow:\n\u0302gX (Z = j) = 1\nN j\nN\n\u2211 i=1\nf (xi,Z = j)I(xi,Z = j)\nQ(xi,Z = j)\n\u0302gY (Z = j) = 1\nN j\nN\n\u2211 i=1\nf (yi,Z = j)I(yi,Z = j)\nQ(yi,Z = j) (9)\nwhere I(xi,Z = j) (or I(yi,Z = j)) is an indicator function which is 1 iff the tuple (xi,Z = j) ( or (yi,Z = j) ) is generated in any of the N samples and 0 otherwise.\nFrom Equation 8, we can now derive the following unbiased estimator for P(a,b):\nP\u0302(a,b) = 1\nN\n1\n\u2211 j=0\nN j f (Z = j) \u0302gX (Z = j) \u0302gY (Z = j)\nQ(Z = j) (10)\nImportance sampling on the other hand would estimate P(a,b) as follows:\nP\u0303(a,b) = 1\nN\n1\n\u2211 j=0\nN j f (Z = j)\nQ(Z = j)\n\u00d7 1\nN j\nN\n\u2211 i=1\nf (xi,Z = j) f (yi,Z = j)\nQ(xi|Z = j)Q(Y i|Z = j) I(xi,yi,Z = j) (11)\nwhere I(xi,yi,Z = j) is an indicator function which is 1 iff the tuple (xi,yi,Z = j) is generated in any of the N samples and 0 otherwise.\nEquation 10 which is an unbiased estimator of expectation by parts given in Equation 8 provides another rationale for preferring it over the usual importance sampling estimator given by Equation 11. In particular in Equation 10, we estimate two functions defined over the random variables X |Z = z and Y |Z = z respectively from the generated samples. In importance sampling, on the other hand, we estimate a function over the joint random variable XY |Z = z using the generated samples. Because the samples for X |Z = z and Y |Z = z are considered independently in Equation 10, N j samples drawn over the joint random variable XY |Z = z in Equation 11 correspond to a larger set N j \u2217N j = N 2 j of virtual samples. We know that [Rubinstein, 1981] the variance (and therefore the mean-squared error) of an unbiased estimator decreases with an increase in the effective sample size. Consequently, our new estimation technique will have lower error than the conventional approach.\nIn the following subsection, we discuss how the AND/OR structure can be used for estimating expectation by parts yielding the AND/OR importance sampling scheme."}, {"heading": "3.2 Computing Sample Mean in AND/OR-space", "text": "In this subsection, we formalize the ideas of estimating expectation by parts on a general AND/OR tree starting with some required definitions. We define the notion of an AND/OR sample tree which is restricted to the generated samples and which will be used to compute the AND/OR sample mean. The labels on this AND/OR tree are set to account for the importance weights. Definition 3.1 (Arc Labeled AND/OR Sample Tree). Given a a graphical model R = \u3008X,D,P\u3009, a pseudo-tree T (V,E) , a proposal distribution Q = \u220fni=1Q(Xi|Anc(Xi)) such that Anc(Xi) is a subset of all ancestors of Xi in T , a sequence of assignments (samples) S and a complete AND/OR search tree \u03c6T , an AND/OR sample tree SAOT is constructed from \u03c6T by removing all edges and corresponding nodes which are not in S i.e. they are not sampled.\nThe Arc-label for an OR node Xi to an AND node Xi = xi in SAOT is a pair \u3008w,#\u3009 where:\n\u2022 w = P(Xi=xi,anc(xi)) Q(Xi=xi|anc(xi)) is called the weight of the arc.\nanc(xi) is the assignment of values to all variables\nfrom the node Xi to the root node of SAO and P(Xi = xi,anc(xi)) is the product of all functions in R that mention Xi but do not mention any variable ordered below it in T given (Xi = xi,anc(xi)).\n\u2022 # is the frequency of the arc. Namely, it is equal to the number of times the assignment (Xi = xi,anc(xi)) is sampled.\nExample 3.2. Consider again the Bayesian network given in Figure 2(a). Assume that the proposal distribution Q(XYZ) is uniform. Figure 2(b) shows four hypothetical random samples drawn from Q. Figure 2(c) shows the AND/OR sample tree over the four samples. Each arc from an OR node to an AND node in the AND/OR sample tree is labeled with appropriate frequencies and weights according to Definition 3.1. Figure 2(c) shows the derivation of arc-weights for two arcs.\nThe main virtue of arranging the samples on an AND/OR sample tree is that we can exploit the independencies to define the AND/OR sample mean.\nDefinition 3.3 (AND/OR Sample Mean). Given a AND/OR sample tree with arcs labeled according to Definition 3.1, the value of a node is defined recursively as follows. The value of leaf AND nodes is \u201d1\u201d and the value of leaf OR nodes is \u201d0\u201d. Let C(n) denote the child nodes and v(n) denotes the value of node n. If n is a AND node then: v(n) = \u220fn\u2032\u2208C(n) v(n \u2032) and if n is a OR node then\nv(n) = \u2211n\u2032\u2208C(n)(#(n,n\n\u2032)w(n,n\u2032)v(n\u2032))\n\u2211n\u2032\u2208C(n) #(n,n \u2032)\nThe AND/OR sample mean is the value of the root node.\nWe can show that the value of an OR node is equal to an unbiased estimate of the conditional expectation of the variable at the OR node given an assignment from the root to the parent of the OR node. Since all variables, except the evidence variables are unassigned at the root node, the value of the root node equals the AND/OR sample mean which is an unbiased estimate of probability of evidence. Formally,\nTHEOREM 3.4. The AND/OR sample mean is an unbiased estimate of probability of evidence.\nExample 3.5. The calculations involved in computing the sample mean on the AND/OR sample tree on our example\nBayesian network given in Figure 2 are shown in Figure 3. Each AND node and OR node in Figure 3 is marked with a value that is computed recursively using definition 3.3. The value of OR nodes X and Y given Z = j \u2208 {0,1} is equal to \u0302gX (Z = j) and \u0302gY (Z = j) respectively defined in Equation 9. The value of the root node is equal to the AND/OR sample mean which is equal to the sample mean computed by parts in Equation 10.\nAlgorithm 1 AND/OR Importance Sampling\nInput: an ordering O = (X1, . . . ,Xn),a Bayesian network BN and a proposal distribution Q Output: Estimate of Probability of Evidence\n1: Generate samples x1, . . . ,xN from Q along O. 2: Build a AND/OR sample tree SAOT for the samples x 1, . . . ,xN\nalong the ordering O. 3: Initialize all labeling functions \u3008w,#\u3009 on each arc from an Or-\nnode n to an And-node n\u2032 using Definition 3.1. 4: FOR all leaf nodes i of SAOT do 5: IF And-node v(i)= 1 ELSE v(i)=0 6: For every node n from leaves to the root do 7: Let C(n) denote the child nodes of node n 8: IF n = \u3008X ,x\u3009 is a AND node, then v(n) = \u220fn\u2032\u2208C(n) v(n\n\u2032) 9: ELSE if n = X is a OR node then\nv(n) = \u2211n\u2032\u2208C(n)(#(n,n\n\u2032)w(n,n\u2032)v(n\u2032))\n\u2211n\u2032\u2208C(n) #(n,n \u2032)\n.\n10: Return v(root node)\nWe now have the necessary definitions to formally present the AND/OR importance sampling scheme (see Algorithm 1). In Steps 1-3, the algorithm generates samples from Q and stores them on an AND/OR sample tree. The algorithm then computes the AND/OR sample mean over the AND/OR sample tree recursively from leaves to the root in Steps 4\u2212 9. We can show that the value v(n) of a node in the AND/OR sample tree stores the sample average of the subproblem rooted at n, subject to the current variable instantiation along the path from the root to n. If n is the root, then v(n) is the AND/OR sample mean which is our AND/OR estimator of probability of evidence. Finally, we summarize the complexity of computing AND/OR sample mean in the following theorem:\nTHEOREM 3.6. Given N samples and n variables (with\nconstant domain size), the time complexity of computing AND/OR sample mean is O(nN) (same as importance sampling) and its space complexity is O(nN) (the space complexity of importance sampling is constant)."}, {"heading": "4 Variance Reduction", "text": "In this section, we prove that the AND/OR sample mean may have lower variance than the sample mean computed using importance sampling (Equation 3). THEOREM 4.1 (Variance Reduction). Variance of AND/OR sample mean is less than or equal to the variance of importance sampling sample mean.\nProof. The details of the proof are quite complicated and therefore we only provide the intuitions involved. As noted earlier the guiding principle of AND/OR sample mean is to take advantage of conditional independence in the graphical model. Let us assume that we have three random variables X, Y and Z with the following relationship: X and Y are independent of each other given Z (similar to our example Bayesian network). The expression for variance derived here can be used in an induction step (induction is carried on the nodes of the pseudo tree) to prove the theorem.\nIn this case, importance sampling generates samples ((x1,y1,z1), . . . ,(xN ,yN ,zN)) along the order \u3008Z,X,Y\u3009 and estimates the mean as follows:\n\u00b5 IS(XYZ) = \u2211Ni=1 x iyizi\nN (12)\nWithout loss of generality, let {z1,z2} be the domain of Z and let these values be sampled N1 and N2 times respectively. We can rewrite Equation 12 as follows:\n\u00b5 IS(XYZ) = 1\nN\n2\n\u2211 j=1\nN jzj \u2211Ni=1 x iyiI(z j,x i,yi)\nN j (13)\nwhere I(z j,x i,yi) is an indicator function which is 1 iff the partial assignment (z j,x i,yi) is generated in any of the N samples and 0 otherwise.\nAND/OR sample mean is defined as:\n\u00b5AO(XYZ) = 1N\n2\n\u2211 j=1 N jz j\n( \u2211Ni=1 x iI(z j ,x i)\nN j\n)( \u2211Ni=1 y iI(z j ,y i)\nN j\n) (14)\nwhere I(x j,zi) (and similarly I(y j,zi)) is an indicator func-\ntion which equals 1 when one of the N samples contains the tuple (x j,zi) (and similarly (y j,zi))) and is 0 otherwise.\nBy simple algebraic manipulations, we can prove that the variance of estimator \u00b5 IS(XYZ) is given by:\nVar(\u00b5 IS(XYZ)) =\n( 2\n\u2211 j=1\nz2jQ(zj) ( \u00b5(X|z j) 2V (Y|z j)+\n\u00b5(Y|z j) 2V (X|z j)+V (X|z j)V (Y|zj) )) /N\u2212\u00b52XYZ/N (15)\nSimilarly, the variance of AND/OR sample mean is given by:\nVar(\u00b5AO(XYZ)) =\n( 2\n\u2211 j=1\nz2jQ(zj) ( \u00b5(X|z j) 2V (Y|z j)\n+ \u00b5(Y|z j) 2V (X|z j)+\nV (X|z j)V (Y|zj)\nN j\n)) /N\u2212\u00b52XYZ/N (16)\nwhere \u00b5(X|z j) and V (X|z j) are the conditional mean and variance respectively of X given Z = z j. Similarly, \u00b5(Y|z j) and V (Y|z j) are the conditional mean and variance respectively of Y given Z = z j.\nFrom Equations 15 and 16, if N j = 1 for all j, then we can see that the Var(\u00b5AO(XYZ)) =Var(\u00b5 IS(XYZ)). However if N j > 1,Var(\u00b5\nAO(XYZ)) <Var(\u00b5 IS(XYZ)). This proves that the variance of AND/OR sample mean is less than or equal to the variance of conventional sample mean on this special case. As noted earlier using this case in induction over the nodes of a general pseudo-tree completes the proof."}, {"heading": "5 Estimation in AND/OR graphs", "text": "Next, we describe a more powerful algorithm for estimating mean in AND/OR-space by moving from AND/OR-trees to AND/OR graphs as presented in [Dechter and Mateescu, 2007]. An AND/OR-tree may contain nodes that root identical subtrees. When such unifiable nodes are merged, the tree becomes a graph and its size becomes smaller. Some unifiable nodes can be identified using contexts defined below.\nDefinition 5.1 (Context). Given a belief network and the corresponding AND/OR search tree SAOT relative to a pseudo-tree T , the context of any AND node \u3008Xi,xi\u3009 \u2208 SAOT , denoted by context(Xi), is defined as the set of ancestors of Xi in T , that are connected to Xi and descendants of Xi.\nThe context minimal AND/OR graph is obtained by merging all the context unifiable AND nodes. The size of the largest context is bounded by the tree width w\u2217 of the pseudo-tree [Dechter and Mateescu, 2007]. Therefore, the time and space complexity of a search algorithm traversing the context-minimal AND/OR graph is O(exp(w\u2217)). Example 5.2. For illustration, consider the contextminimal graph in Figure 1(e) of the pseudo-tree from Figure 1(c). Its size is far smaller that that of the AND/OR tree from Figure 2(c) (30 nodes vs. 38 nodes). The contexts of the nodes can be read from the pseudo-tree in Figure 1(b) as follows: context(A) = {A}, context(B) = {B,A}, context(C) = {C,B,A}, context(D) = {D,C,B} and context(E) = {E,A,B}.\nThe main idea in AND/OR-graph estimation is to store all samples on an AND/OR-graph instead of an AND/OR-tree.\nSimilar to an AND/OR sample tree, we can define an identical notion of an AND/OR sample graph.\nDefinition 5.3 ( Arc labeled AND/OR sample graph). Given a complete AND/OR graph \u03c6G and a set of samples S , an AND/OR sample graph SAOG is obtained by removing all nodes and arcs not in S from \u03c6G. The labels on SAOG are set similar to that of an AND/OR sample tree (see Definition 3.1).\nExample 5.4. The bold edges and nodes in Figure 1(c) define an AND/OR sample tree. The bold edges and nodes in Figure 1(d) define an AND/OR sample graph corresponding to the same samples that define the AND/OR sample tree in Figure 1(c).\nThe algorithm for computing the sample mean on AND/OR sample graphs is identical to the algorithm for AND/ORtree (Steps 4-10 of Algorithm 1). The main reason in moving from trees to graphs is that the variance of the sample mean computed on an AND/OR sample graph can be even smaller than that computed on an AND/OR sample tree. More formally, THEOREM 5.5. Let V (\u00b5AOG), V (\u00b5AOT ) and V (\u00b5IS) be the variance of AND/OR sample mean on an AND/OR sample graph, variance of AND/OR sample mean on an AND/OR sample tree and variance of sample mean of importance sampling respectively. Then given the same set of input samples:\nV (\u00b5AOG) \u2264V (\u00b5AOT ) \u2264V (\u00b5IS)\nWe omit the proof due to lack of space.\nTHEOREM 5.6 (Complexity of computing AND/OR graph sample mean). Given a graphical model with n variables, a psuedo-tree with treewidth w\u2217 and N samples, the time complexity of AND/OR graph sampling is O(nNw\u2217) while its space complexity is O(nN)."}, {"heading": "6 Experimental Evaluation", "text": ""}, {"heading": "6.1 Competing Algorithms", "text": "The performance of importance sampling based algorithms is highly dependent on the proposal distribution [Cheng and Druzdzel, 2000]. It was shown that computing the proposal distribution from the output of a Generalized Belief Propagation scheme of Iterative Join Graph Propagation (IJGP) yields better empirical performance than other available choices [Gogate and Dechter, 2005]. Therefore, we use the output of IJGP to compute the proposal distribution Q. The complexity of IJGP is time and space exponential in its i-bound, a parameter that bounds cluster sizes. We use a i-bound of 5 in all our experiments.\nWe experimented with three sampling algorithms for benchmarks which do not have determinism: (a) (pure) IJGP-sampling, (b) AND/OR-tree IJGP-sampling and (c) AND/OR-graph IJGP-sampling. Note that the underlying scheme for generating the samples is identical in all the\nmethods. What changes is the method of accumulating the samples and deriving the estimates. On benchmarks which have zero probabilities or determinism, we use the SampleSearch scheme introduced by [Gogate and Dechter, 2007] to overcome the rejection problem. We experiment with the following versions of SampleSearch on deterministic networks: (a) pure SampleSearch, (b) AND/OR-tree SampleSearch and (c) AND/OR-graph SampleSearch."}, {"heading": "6.1.1 Results", "text": "We experimented with three sets of benchmark belief networks (a) Random networks, (b) Linkage networks and (c) Grid networks. Note that only linkage and grid networks have zero probabilities on which we use SampleSearch.The exact P(e) for most instances is available from the UAI 2006 competition web-site.\nOur results are presented in Figures 4-6. Each Figure shows approximate probability of evidence as a function of time. The bold line in each Figure indicates the exact probability of evidence. The reader can visualize the error from the distance between the approximate curves and the exact line. For lack of space, we show only part of our results. Each Figure shows the number of variables n, the maximum-domain size d and the number of evidence nodes |E| for the respective benchmark.\nRandom Networks From Figures 4(a) and 4(b), we see that AND/OR-graph sampling is better than AND/OR-tree sampling which in turn is better than pure IJGP-sampling. However there is not much difference in the error because the proposal distribution seems to be a very good approximation of the posterior.\nGrid Networks All Grid instances have 1444 binary nodes and between 5-10 evidence nodes. From Figures 5(a) and 5(b), we can see that AND/OR-graph SampleSearch and AND/OR-tree SampleSearch are substantially better than pure SampleSearch.\nLinkage Networks The linkage instances are generated by converting a Pedigree to a Bayesian network [Fishelson and Geiger, 2003]. These networks have between 777-2315 nodes with a maximum domain size of 36. Note that it is hard to compute exact probability of evidence in these networks [Fishelson and Geiger, 2003]. We observe from Figures 6(a),(b) (c) and (d) that AND/ORgraph SampleSearch is substantially more accurate than AND/OR-tree SampleSearch which in turn is substantially more accurate than pure SampleSearch. Notice the logscale in Figures 6 (a)-(d) which means that there is an order of magnitude difference between the errors. Our results suggest that AND/OR-graph and tree estimators yield far better performance than conventional estimators especially on problems in which the proposal distribution is a bad approximation of the posterior distribution."}, {"heading": "7 Related Work and Summary", "text": "The work presented in this paper is related to the work by [Hernndez and Moral, 1995, Kj\u00e6rulff, 1995, Dawid et al., 1994] who perform sampling based inference on a junction tree. The main idea in these papers is to perform message passing on a junction tree by substituting messages which are too hard to compute exactly by their sampling-based approx-\nimations. [Kj\u00e6rulff, 1995, Dawid et al., 1994] use Gibbs sampling while [Hernndez and Moral, 1995] use importance sampling to approximate the messages. Similar to recent work on Rao-Blackwellised sampling such as [Bidyuk and Dechter, 2003, Paskin, 2004, Gogate and Dechter, 2005], variance reduction is achieved in these junction tree based sampling schemes because of some exact computations; as dictated by the Rao-Blackwell theorem. AND/OR estimation, however, does not require\nexact computations to achieve variance reduction. In fact, variance reduction due to Rao-Blackwellisation is orthogonal to the variance reduction achieved by AND/OR estimation and therefore the two could be combined to achieve more variance reduction. Also, unlike our work which focuses on probability of evidence, the focus of these aforementioned papers was on belief updating.\nTo summarize, the paper introduces a new sampling based estimation technique called AND/OR importance sampling. The main idea of our new scheme is to derive statistics on the generated samples by using an AND/OR tree or graph that takes advantage of the independencies present in the graphical model. We proved that the sample mean computed on an AND/OR tree or graph may have smaller variance than the sample mean computed using the conventional approach. Our experimental evaluation is preliminary but quite promising showing that on most instances AND/OR sample mean has lower error than importance sampling and sometimes by significant margins."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the NSF under award numbers IIS-0331707, IIS-0412854 and IIS-0713118 and the NIH grant R01-HG004175-02."}], "references": [{"title": "An empirical study of w-cutset sampling for bayesian networks", "author": ["Bidyuk", "Dechter", "B. 2003] Bidyuk", "R. Dechter"], "venue": "In Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI-03)", "citeRegEx": "Bidyuk et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bidyuk et al\\.", "year": 2003}, {"title": "Ais-bn: An adaptive importance sampling algorithm for evidential reasoning in large bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000] Cheng", "M.J. Druzdzel"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Hybrid propagation in junction trees", "author": ["Dawid et al", "A.P. 1994] Dawid", "U. Kjaerulff", "S.L. Lauritzen"], "venue": "In IPMU\u201994,", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "AND/OR search spaces for graphical models", "author": ["Dechter", "Mateescu", "R. 2007] Dechter", "R. Mateescu"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 2007}, {"title": "Optimizing exact genetic linkage computations", "author": ["Fishelson", "Geiger", "M. 2003] Fishelson", "D. Geiger"], "venue": "RECOMB", "citeRegEx": "Fishelson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fishelson et al\\.", "year": 2003}, {"title": "Approximate inference algorithms for hybrid bayesian networks with discrete constraints. UAI2005", "author": ["Gogate", "Dechter", "V. 2005] Gogate", "R. Dechter"], "venue": null, "citeRegEx": "Gogate et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2005}, {"title": "Samplesearch: A scheme that searches for consistent samples", "author": ["Gogate", "Dechter", "V. 2007] Gogate", "R. Dechter"], "venue": "AISTATS", "citeRegEx": "Gogate et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2007}, {"title": "Mixing exact and importance sampling propagation algorithms in dependence graphs", "author": ["Hernndez", "Moral", "L.D. 1995] Hernndez", "S. Moral"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Hernndez et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hernndez et al\\.", "year": 1995}, {"title": "Importance sampling algorithms for Bayesian networks: Principles and performance", "author": ["Yuan", "Druzdzel", "C. 2006] Yuan", "M.J. Druzdzel"], "venue": "Mathematical and Computer Modelling", "citeRegEx": "Yuan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2006}], "referenceMentions": [], "year": 2008, "abstractText": "The paper introduces AND/OR importance sampling for probabilistic graphical models. In contrast to importance sampling, AND/OR importance sampling caches samples in the AND/OR space and then extracts a new sample mean from the stored samples. We prove that AND/OR importance sampling may have lower variance than importance sampling; thereby providing a theoretical justification for preferring it over importance sampling. Our empirical evaluation demonstrates that AND/OR importance sampling is far more accurate than importance sampling in many cases.", "creator": "gnuplot 4.2 patchlevel 2 "}}}