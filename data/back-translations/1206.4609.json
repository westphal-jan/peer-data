{"id": "1206.4609", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "On multi-view feature learning", "abstract": "Recently, there has been a growing interest in learning traits from spatio-temporal, binocular, or other multi-observation data, with the goal of encoding the relationship between images, not the content of a single image. We offer an analysis of Multiview Feature Learning that shows that hidden variables encode transformations by detecting rotation angles in the spaces of their own that are shared between multiple image faults. Our analysis helps explain recent experimental results that show that transformation-specific traits arise when complex cell models are trained on video. Our analysis also shows that transformation-invariant traits can occur as a by-product of learning representations of transformations.", "histories": [["v1", "Mon, 18 Jun 2012 14:45:17 GMT  (402kb)", "http://arxiv.org/abs/1206.4609v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["roland memisevic"], "accepted": true, "id": "1206.4609"}, "pdf": {"name": "1206.4609.pdf", "metadata": {"source": "META", "title": "On multi-view feature learning", "authors": ["Roland Memisevic"], "emails": ["ro@cs.uni-frankfurt.de"], "sections": [{"heading": "1. Introduction", "text": "Feature learning (AKA dictionary learning, or sparse coding) has gained considerable attention in computer vision in recent years, because it can yield image representations that are useful for recognition. However, although recognition is important in a variety of tasks, a lot of problems in vision involve the encoding of the relationship between observations not single observations. Examples include tracking, multi-view geometry, action understanding or dealing with invariances.\nA variety of multi-view feature learning models have recently been suggested as a way to learn features that encode relations between images. The basic idea behind these models is that hidden variables sum over products of filter responses applied to two observations x and y and thereby correlate the responses.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nAdapting the filters based on synthetic transformations on images was shown to yield transformationspecific features like phase-shifted Fourier components when training on shifted image pairs, or \u201ccircular\u201d Fourier components when training on rotated image pairs (Memisevic & Hinton, 2010). Task-specific filterpairs emerge when training on natural transformations, like facial expression changes (Susskind et al., 2011) or natural video (Taylor et al., 2010), and they were shown to yield state-of-the-art recognition performance in these domains. Multi-view feature learning models are also closely related to energy models of complex cells (Adelson & Bergen, 1985), which, in turn, have been successfully applied to video understanding, too (Le et al., 2011). They have also been used to learn within-image correlations by letting input and output images be the same (Ranzato & Hinton, 2010; Bergstra et al., 2010).\nCommon to all these methods is that they deploy products of filter responses to learn relations. In this paper, we analyze the role of these multiplicative interactions in learning relations. We also show that the hidden variables in a multi-view feature learning model represent transformations by detecting rotation angles in eigenspaces that are shared among the transformations. We focus on image transformations here, but our analysis is not restricted to images.\nOur analysis has a variety of practical applications, that we investigate in detail experimentally: (1) We can train complex cell and energy models using conditional sparse coding models and vice versa, (2) It is possible to extend multi-view feature learning to model sequences of three or more images instead of just two, (3) It is mandatory that hidden variables pool over multiple subspaces to work properly, (4) Invariant features can be learned by separating pooling within subspaces from pooling across subspaces. Our analysis is related to previous investigations of energy models and of complex cells (for example, (Fleet et al., 1996; Qian, 1994)), and it extends this line of work to more general transformations than local translation."}, {"heading": "2. Background on multi-view sparse coding", "text": "Feature learning1 amounts to encoding an image patch x using a vector of latent variables z = \u03c3(WTx), where each column of W can be viewed as a linear feature (\u201cfilter\u201d) that corresponds to one hidden variable zk, and 2 where \u03c3 is a non-linearity, such as the sigmoid \u03c3(a) = ( 1 + exp(a) ) \u22121 . To adapt the parameters, W , based on a set of example patches {x\u03b1} one can use a variety of methods, including maximizing the average sparsity of z, minimizing a form of reconstruction error, maximizing the likelihood of the observations via Gibbs sampling, and others (see, for example, (Hyva\u0308rinen et al., 2009) and references therein).\nTo obtain hidden variables z, that encode the relationship between two images, x and y, one needs to represent correlation patterns between two images instead. This is commonly achieved by computing the sum over products of filter responses:\nz = WT ( UTx ) \u2217 ( V Ty )\n(1)\nwhere \u201c\u2217\u201d is element-wise multiplication, and the columns of U and V contain image filters that are learned along with W from data (Memisevic & Hinton, 2010). Again, one may apply an element-wise non-linearity to z. The hidden units are \u201cmulti-view\u201d variables that encode transformations not the content of single images, and they are commonly referred to as \u201cmapping units\u201d.\nTraining the model parameters, (U, V,W ), can be achieved by minimizing the conditional reconstruction error of y keeping x fixed or vice versa (Memisevic, 2011), or by conditional variants of maximum likelihood (Ranzato & Hinton, 2010; Memisevic & Hinton, 2010). Training the model on transforming randomdot patterns yields transformation-specific features, such as phase-shifted Fourier features in the case of translation and circular harmonics in the case of rotation (Memisevic & Hinton, 2010; Memisevic, 2011). Eq. 1 can also be derived by factorizing the parameter tensor of a conditional sparse coding model (Memisevic & Hinton, 2010). An illustration of the model is shown in Figure 1 (a).\n1We use the terms \u201cfeature learning\u201d, \u201cdictionary learning\u201d and \u201csparse coding\u201d synonymously in this paper. Each term tends to come with a slightly different meaning in the literature, but for the purpose of this work the differences are negligible.\n2In practice, it is common to add constant bias terms to the linear mapping. In the following, we shall refrain from doing so to avoid cluttering up the derivations. We shall instead think of data and hidden variables as being in \u201chomogeneous notation\u201d with an extra, constant 1-dimension."}, {"heading": "2.1. Energy models", "text": "Multi-view feature learning is closely related to energy models and to models of complex cells (Adelson & Bergen, 1985; Fleet et al., 1996; Kohonen; Hyva\u0308rinen & Hoyer, 2000). The activity of a hidden unit in an energy model is typically defined as the sum over squared filter responses, which may be written\nz = WT ( BTx ) \u2217 ( BTx )\n(2)\nwhere B contains image filters in its columns. W is usually constrained such that each hidden variable, zk, computes the sum over only a subset of all products. This way, hidden variables can be thought of as encoding the norm of a projection of x onto a subspace3. Energy models are also referred to as \u201csubspace\u201d or \u201csquare-pooling\u201d models.\nFor our analysis, it is important to note that, when we apply an energy model to the concatenation of two images, x and y, we obtain a response that is closely related to the response of a multi-view sparse coding model (cf., Eq. 1): Let bf denote a single column of matrix B. Furthermore, let uf denote the part of the filter bf that gets applied to image x, and let vf denote the part that gets applied to image y, so that bTf [x;y] = u T f x + v T f y. Hidden unit activities, zk, then take the form\nzk = \u2211\nf\nWfk ( uTf x+ v T f y )2 = 2\n\u2211\nf\nWfk ( uTf x )( vTf y )\n+ \u2211\nf\nWfk ( uTf x )2 + \u2211\nf\nWfk ( vTf y )2\n(3)\nThus, up to the quadratic terms in Eq. 3, hidden unit activities are the same as in a multi-view feature learning model (Eq. 1). As we shall discuss in Section 3.5, the quadratic terms do not significantly change the behavior of the hidden units as compared to multi-view sparse coding models. An illustration of the energy model is shown in Figure 1 (b)."}, {"heading": "3. Eigenspace analysis", "text": "We now show that hidden variables turn into subspace rotation detectors when the models are trained on transformed image pairs. To simplify the analysis, we shall restrict our attention to transformations, L, that are orthogonal, that is, LTL = LLT = I, where I is the identity matrix. In other words, L\u22121 = LT. Linear transformations in \u201cpixel-space\u201d are also\n3It is also common to apply non-linearities, such as a square-root, to the activity zk.\nknown as warp. Note that practically all relevant spatial transformations, like translation, rotation or local shifts, can be expressed approximately as an orthogonal warp, because orthogonal transformations subsume, in particular, all permutations (\u201cshuffling pixels\u201d).\nAn important fact about orthogonal matrices is that the eigen-decomposition L = UDUT is complex, where eigenvalues (diagonal of D) have absolute value 1 (Horn & Johnson, 1990). Multiplying by a complex number with absolute value 1 amounts to performing a rotation in the complex plane, as illustrated in Figure 1 (c) and (d). Each eigenspace associated with L is also referred to as invariant subspace of L (as application of L will keep eigenvectors within the subspace).\nApplying an orthogonal warp is thus equivalent to (i) projecting the image onto filter pairs (the real and imaginary parts of each eigenvector), (ii) performing a rotation within each invariant subspace, and (iii) projecting back into the image-space. In other words, we can decompose an orthogonal transformation into a set of independent, 2-dimensional rotations. The most well-known examples are translations: A 1Dtranslation matrix contains ones along one of its secondary diagonals, and it is zero elsewhere4. The eigenvectors of this matrix are Fourier-components (Gray, 2005), and the rotation in each invariant subspace amounts to a phase-shift of the corresponding Fourierfeature. This leaves the norm of the projections onto the Fourier-components (the power spectrum of the signal) constant, which is a well known property of\n4To be exactly orthogonal it has to contain an additional one in another place, so that it performs a rotation with wrap-around.\ntranslations.\nIt is interesting to note that the imaginary and real parts of the eigenvectors of a translation matrix correspond to sine and cosine features, respectively, reflecting the fact that Fourier components naturally come in pairs. These are commonly referred to as quadrature pairs in the literature. The same is true of Gabor features, which represent local translations (Qian, 1994; Fleet et al., 1996). However, the property that eigenvectors come in pairs is not specific to translations. It is shared by all transformations that can be represented by an orthogonal matrix. (Bethge et al., 2007) use the term generalized quadrature pair to refer to the eigen-features of these transformations."}, {"heading": "3.1. Commuting warps share eigenspaces", "text": "A central observation to our analysis is that eigenspaces can be shared among transformations. When eigenspaces are shared, then the only way in which two transformations differ, is in the angles of rotation within the eigenspaces. In this case, we can represent multiple transformations with a single set of features as we shall show.\nAn example of a shared eigenspace is the Fourier-basis, which is shared among translations (Gray, 2005). Less obvious examples are Gabor features which may be thought of as the eigenbases of local translations, or features that represent spatial rotations. Formally, a set of matrices share eigenvectors if they commute (Horn & Johnson, 1990). This can be seen by considering any two matrices A and B with AB = BA and with \u03bb, v an eigenvalue/eigenvector pair of B with multiplicity one. It holds that BAv = ABv = \u03bbAv.\nTherefore, Av is also an eigenvector of B with the same eigenvalue."}, {"heading": "3.2. Extracting transformations", "text": "Consider the following task: Given two images x and y, determine the transformation L that relates them, assuming that L belongs to a given class of transformations.\nThe importance of commuting transformations for our analysis is that, since they share an eigenbasis, any two transformations differ only in the angles of rotation in the joint eigenspaces. As a result, one may extract the transformation from the given image pair (x,y) simply by recovering the angles of rotation between the projections of x and y onto the eigenspaces. To this end, consider the real and complex parts vR and vI of some eigen-feature v = vR+ ivI, where i = \u221a \u22121. The real and imaginary coordinates of the projection pvx of x onto the invariant subspace associated with v are given by vTRx and v T I x, respectively. For the projection pvy of the output image onto the invariant subspace, they are vTRy and v T I y.\nLet \u03c6x and \u03c6y denote the angles of the projections of x and y with the real axis in the complex plane. If we normalize the projections to have unit norm, then the cosine of the angle between the projections, \u03c6y \u2212 \u03c6x, may be written\ncos(\u03c6y \u2212 \u03c6x) = cos\u03c6y cos\u03c6x + sin\u03c6y sin\u03c6x by a trigonometric identity. This is equivalent to computing the inner product between two normalized projections (cf. Figure 1 (c) and (d)). In other words, to estimate the (cosine of) the angle of rotation between the projections of x and y, we need to sum over the product of two filter responses."}, {"heading": "3.3. The subspace aperture problem", "text": "Note, however, that normalizing each projection to 1 amounts to dividing by the sum of squared filter responses, an operation that is highly unstable if a projection is close to zero. This will be the case, whenever one of the images is almost orthogonal to the invariant subspace. This, in turn, means that the rotation angle cannot be recovered from the given image, because the image is too close to the axis of rotation. One may view this as a subspace-generalization of the wellknown aperture problem beyond translation, to the set of orthogonal transformations. Normalization would ignore this problem and provide the illusion of a recovered angle even when the aperture problem makes the detection of the transformation component impossible. In the next section we discuss how one may\novercome this problem by rephrasing the problem as a detection task."}, {"heading": "3.4. Mapping units as rotation detectors", "text": "For each eigenvector, v, and rotation angle, \u03b8, define the complex output image filter\nv\u03b8 = exp(i\u03b8)v\nwhich represents a projection and simultaneous rotation by \u03b8. This allows us to define a subspace rotation-detector with preferred angle \u03b8 as follows:\nr\u03b8 = (vTRy)(v \u03b8 R T x) + (vTI y)(v \u03b8 I T x) (4)\nwhere subscripts R and I denote the real and imaginary part of the filters like before. If projections are normalized to length 1, we have\nr\u03b8 = cos\u03c6y cos(\u03c6x + \u03b8) + sin\u03c6y sin(\u03c6x + \u03b8)\n= cos(\u03c6y \u2212 \u03c6x \u2212 \u03b8), (5)\nwhich is maximal whenever \u03c6y \u2212 \u03c6x = \u03b8, thus when the observed angle of rotation, \u03c6y \u2212\u03c6x, is equal to the preferred angle of rotation, \u03b8. However, like before, normalizing projections is not a good idea because of the subspace aperture problem. We now show that mapping units are well-suited to detecting subspace rotations, if a number of conditions are met.\nIf features and data are contrast normalized, then the projections will depend only on how well the image pair represents a given subspace rotation. The value r\u03b8, in turn, will depend (a) on the transformation (via the subspace angle) and (b) on the content of the images (via the angle between each image and the invariant subspace). Thus, the output of the detector factors in both, the presence of a transformation and our ability to discern it.\nThe fact that r\u03b8 depends on image content makes it a suboptimal representation of the transformation. However, note that r\u03b8 is a \u201cconservative\u201d detector, that takes on a large value only if an input image pair (x,y) complies with its transformation. We can therefore define a content-independent representation by pooling over multiple detectors r\u03b8 that represent the same transformation but respond to different images.\nTherefore, by stacking eigenfeatures v and v\u03b8 in matrices U and V , respectively, we may define the representation t of a transformation, given two images x and y, as\nt = WTP ( UTx ) \u2217 ( V Ty )\n(6)\nwhere P is a band-diagonal within-subspace pooling matrix, and W is an appropriate across-subspace pooling matrix that supports content-independence.\nFurthermore, the following conditions need to be met: (1) Images x and y are contrast-normalized, (2) For each row uf of U there exists \u03b8 such that the corresponding row vf of V can be written vf = exp(i\u03b8)uf . In other words, filter pairs are related through rotations only.\nEq. 6 takes the same form as inference in a multiview feature learning model (cf., Eq. 1), if we absorb the within-subspace pooling matrix P into W . Learning amounts to identifying both the subspaces and the pooling matrix, so training a multi-view feature learning model can be thought of as performing multiple simultaneous diagonalizations of a set of transformations. When a dataset contains more than one transformation class, learning involves partitioning the set of orthogonal warps into commutative subsets and simultaneously diagonalizing each subset. Note that, in practice, complex filters can be represented by learning two-dimensional subspaces in the form of filter pairs. It is uncommon, albeit possible, to learn actually complex-valued features in practice.\nIt is interesting to note that condition (2) above implies that filters are normalized to have the same lengths. Imposing a norm constraint has been a common approach to stabilizing learning (Ranzato & Hinton, 2010; Memisevic, 2011; Susskind et al., 2011), but it has not been clear why imposing norm constraints help. Pooling over multiple subspaces may, in addition to providing contentindependent representations, also help deal with edge effects and noise, as well as with the fact that learned transformations may not be exactly orthogonal. In practice, it is also common to apply a sigmoid nonlinearity after computing mapping unit activities, so that the output of a hidden variable can be interpreted as a probability.\nNote that diagonalizing a single transformation, L, would amount to performing a kind of canonical correlations analysis (CCA), so learning a multi-view feature learning model may be thought of as performing multiple canonical correlation analyzes with tied features. Similarly, modeling within-image structure by setting x = y (Ranzato & Hinton, 2010) would amount to learning a PCA mixture with tied weights. In the same way that neural networks can be used to implement CCA and PCA up to a linear transformation, the result of training a multi-view feature learning model is a simultaneous diagonalization only up to a linear transformation."}, {"heading": "3.5. Relation to energy models", "text": "By concatenating images x and y, as well as filters v and v\u03b8, we may approximate the subspace rotation detector (Eq. 4) with the response of an energy detector:\nr\u03b8 = ( (vR Ty) + (v\u03b8R T x) )2 + ( (vI Ty) + (v\u03b8I T x) )2\n= 2 ( (vR Ty)(v\u03b8R T x) + (vI Ty)(v\u03b8I T x) )\n+ (vR Ty)2 + (v\u03b8R T x)2 + (vI Ty)2 + (v\u03b8I T x)2\n(7)\nEq. 7 is equivalent to Eq. 4 up to the four quadratic terms. The four quadratic terms are equal to the sum of the squared norms of the projections of x and y onto the invariant subspace. Thus, like the norm of the projections, they contribute information about the discernibility of transformations. This makes the energy response depend more on the alignment of the images with its subspace. However, like for the inner product detector (Eq. 4), the peak response is attained when both images reside within the detector\u2019s subspace and when their projections are rotated by the detectors preferred angle \u03b8.\nBy pooling over multiple rotation detectors, r\u03b8, we obtain the equivalent of an energy response (Eq. 3). This shows that energy models applied to the concatenation of two images are well-suited to modeling transformations, too. It is interesting to note that both, multiview sparse coding models (Taylor et al., 2010) and energy models (Le et al., 2011) were recently shown to yield highly competitive performance in action recognition tasks, which require the encoding of motion in videos."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Learning quadrature pairs", "text": "Figure 2 shows random subsets of input/output filter pairs learned from rotations of random dot images (top plot), and from a mixed dataset, consisting of random rotations and random translations (bottom plot). We separate the two layers of pooling, W and P , and we constrain P to be band-diagonal with entries Pi,i = Pi,i+1 = 1 and 0 elsewhere. Thus, filters need to come in pairs, which we expect to be approximately in quadrature (each pairs spans the subspace associated with a rotation detector r\u03b8). Figure 2 shows that this is indeed the case after training. Here, we use a modification of a higher-order autoencoder (Memisevic, 2011) for training, but we expect simi-\nlar results to hold for other multi-view models5. We discuss an application of separating pooling in detail in Section 5. Note that for the mixed dataset, both the set of rotations and the set of translations are sets of commuting warps (up to edge effects), but rotations do not commute with translations and vice versa. The figure shows that the model has learned to separate out the two types of transformation by devoting a subset of filters to encoding rotations and another subset to modeling translations."}, {"heading": "4.2. Learning \u201ceigenmovies\u201d", "text": "Both energy models and cross-correlation models can be applied to more than two images: Eq. 4 may be modified to contain all cross-terms, or all the ones that are deemed relevant (for example, adjacent frames, which would amount to a \u201cMarkov\u201d-type gating model of a video). Alternatively, for the energy mechanism, we can compute the square of the concatenation of more than two images in place of Eq. 7, in which case, we obtain the detector response\nr = (\n\u2211\ns\nvsR T xs )2 + (\n\u2211\ns\nvsI T xs\n)2\n= \u2126+ \u2211\nst\n(\nvsR T xs\n)( vtR T xt ) + \u2211\nst\n(\nvsI T xs\n)( vtI T xt )\n(8)\n5Code and datasets are available at http://www.cs.toronto.edu/~rfm/code/multiview/index.html\nwhere \u2126 contains the quadratic terms of the energy model. In analogy to Section 3.4, for the detector to function properly, features will need to satisfy vsR = exp(i\u03b8s)vR and v s I = exp(i\u03b8s)vI for appropriate filters vR and vI .\nWe verify that training on videos leads to filters which approximately satisfy this condition as follows: We use a gated autoencoder where we set U = V , and we set x = y to the concatenation of the 10 frames. In contrast to Section 4.1, we use a single (full) pooling matrix W . Figure 3 shows subsets of learned filters after training the model on shifted random dots (top) and natural movies cropped from the van Hateren database (van Hateren & Ruderman, 1998) (center). The learned filter-sequences represent repeated phaseshifts as expected. Thus, they form the \u201ceigenmovies\u201d of each respective transformation class.\nEq. 8 implies that learning videos requires consistency between the filters across time. Each factor \u2013 corresponding to a sequence of T filters \u2013 can model only the repeated application of the same transformation. An inhomogeneous sequence that involves multiple different types of transformation can only be modeled by devoting separate filter sets to homogeneous subsequences. We verify that this is what happens during training, by using 10-frame videos showing random dots that first rotate at a constant speed for 5 frames, then translate at a constant speed for the remaining 5 frames. Orientation, speed and direction vary across movies. We trained a gated autoencoder like in the previous experiment. The bottom plot in Figure 3 shows that the model learns to decompose the movies into (a) Fourier filters which are quiet in the first half of a movie and (b) rotation features which are quiet in the second half of the movie."}, {"heading": "5. Learning invariances by learning transformations", "text": "Our analysis suggests that detector responses, r\u03b8, will not be affected by transformations they are tuned to, as these will only cause a rotation within the detector\u2019s subspace, leaving the norm of the projections unchanged. Any other transformation (like showing a completely different image pair), however, may change the representation: Projections may get larger or smaller as the transformation changes the degree of alignment of the images with the invariant subspaces.\nThis suggests, that we can learn features that are invariant with respect to one type of transformation and at the same time selective with respect to any other type of transformation as follows: we separate\nthe two pooling-levels into a band-matrix P and a full matrix W (cf., Section 4.1). After training the model on a transformation class, the first-level pooling activities PT ( UTx ) \u2217 ( V Ty )\ncomputed from a test image pair (x,y) will constitute a transformation invariant code for this pair. Alternatively, we can use PT ( UTx ) \u2217 ( V Tx )\nif the test data does not come in the form of pairs but consists only of single images. In this case we obtain a representation of the nulltransformation, but it will still be invariant.\nWe tested this approach on the \u201crotated MNIST\u201ddataset from (Larochelle et al., 2007), which consists of 72000 MNIST digit images of size 28 \u00d7 28 pixels that are rotated by arbitrary random angles (\u2212180 to 180 degrees; 12000 train-, 60000 test-cases, classes range from 0 \u2212 9). Since the number of training cases is fairly large, most exemplars are represented at most angles, so even linear classifiers perform well (Larochelle et al., 2007). However, when reducing the number of training cases, the number of potential matches for any test case dramatically reduces, so classification error rates become much worse when using raw images or standard features.\nWe used a gated auto-encoder with 2000 factors and 200 mapping units, which we trained on image pairs showing rotating random-dots. Figure 4 shows the error rates when using subspace features (responses of the first layer pooling units) with subspace dimen-\nsion 2. We used the features in a logistic regression classifier vs. k-nearest neighbors on the original images (we also tried logistic regression on raw images and logistic regression as well as nearest neighbors on 200-dimensional PCA-features, but the performance is worse in all these cases). The learned features are similar to the features shown in Figure 2, so they are not tuned to digits, and they are not trained discriminatively. They nevertheless consistently perform about as well as, or better than, nearest neighbor. Also, even at half the original dataset size, the subspace features still attain about the same classification performance as raw images on the whole training set. All parameters were set using a fixed hold-out set of 2000 images. The experiment shows that the rotation detectors, r\u03b8, are affected sufficiently by the aperture problem, such that they are selective to image content while being invariant to rotation. This shows that we can \u201charness the aperture problem\u201d to learn invariant features."}, {"heading": "6. Conclusions", "text": "We analyzed multi-view sparse coding models in terms of the joint eigenspaces of a set of transformations. Our analysis helps understand why Fourier features and circular Fourier features emerge when training transformation models on shifts and rotations, and why square-pooling models work well in action and\nmotion recognition tasks. Our analysis furthermore shows how the aperture problem implies that we can learn invariant features as a by-product of learning about transformations.\nThe fact that squaring nonlinearities and multiplicative interactions can support the learning of relations suggests that these may help increase the role of statistical learning in vision in general. By learning about relations we may extend the applicability of sparse coding models beyond recognizing objects in static, single images, towards tasks that involve the fusion of multiple views, including inference about geometry."}, {"heading": "Acknowledgments", "text": "This work was supported by the German Federal Ministry of Education and Research (BMBF) in the project 01GQ0841 (BFNT Frankfurt)."}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "Adelson and Bergen,? \\Q1985\\E", "shortCiteRegEx": "Adelson and Bergen", "year": 1985}, {"title": "Suitability of V1 energy models for object classification", "author": ["Bergstra", "James", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me"], "venue": "Neural Computation, pp", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Unsupervised learning of a steerable basis for invariant image representations", "author": ["M Bethge", "S Gerwinn", "Macke", "JH"], "venue": "In Human Vision and Electronic Imaging XII. SPIE,", "citeRegEx": "Bethge et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bethge et al\\.", "year": 2007}, {"title": "Toeplitz and circulant matrices: a review", "author": ["Gray", "Robert M"], "venue": "Commun. Inf. Theory,", "citeRegEx": "Gray and M.,? \\Q2005\\E", "shortCiteRegEx": "Gray and M.", "year": 2005}, {"title": "Matrix Analysis", "author": ["Horn", "Roger A", "Johnson", "Charles R"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1990}, {"title": "Emergence of phaseand shift-invariant features by decomposition of natural images into independent feature subspaces", "author": ["Hyv\u00e4rinen", "Aapo", "Hoyer", "Patrik"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision", "author": ["Hyv\u00e4rinen", "Aapo", "J. Hurri", "Hoyer", "Patrik O"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Courville", "Aaron", "Bergstra", "James", "Bengio", "Yoshua"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "In Proc. CVPR,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Gradient-based learning of higherorder image features", "author": ["Memisevic", "Roland"], "venue": "In Proceedings of the International Conference on Computer Vision,", "citeRegEx": "Memisevic and Roland.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2011}, {"title": "Learning to represent spatial transformations with factored higherorder Boltzmann machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Computing stereo disparity and motion with known binocular cell properties", "author": ["Qian", "Ning"], "venue": "Neural Computation,", "citeRegEx": "Qian and Ning.,? \\Q1994\\E", "shortCiteRegEx": "Qian and Ning.", "year": 1994}, {"title": "Modeling Pixel Means and Covariances Using Factorized ThirdOrder Boltzmann Machines", "author": ["Ranzato", "Marc\u2019Aurelio", "Hinton", "Geoffrey E"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "R. Memisevic", "G. Hinton", "M. Pollefeys"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Susskind et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2011}, {"title": "Convolutional learning of spatiotemporal features", "author": ["Taylor", "W. Graham", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "In Proc. European Conference on Computer Vision,", "citeRegEx": "Taylor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Independent component analysis of natural image sequences yields spatiotemporal filters similar to simple cells in primary visual cortex", "author": ["L. van Hateren", "J. Ruderman"], "venue": "Proc. Biological Sciences,", "citeRegEx": "Hateren and Ruderman,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman", "year": 1998}], "referenceMentions": [{"referenceID": 13, "context": "Task-specific filterpairs emerge when training on natural transformations, like facial expression changes (Susskind et al., 2011) or natural video (Taylor et al.", "startOffset": 106, "endOffset": 129}, {"referenceID": 14, "context": ", 2011) or natural video (Taylor et al., 2010), and they were shown to yield state-of-the-art recognition performance in these domains.", "startOffset": 25, "endOffset": 46}, {"referenceID": 8, "context": "Multi-view feature learning models are also closely related to energy models of complex cells (Adelson & Bergen, 1985), which, in turn, have been successfully applied to video understanding, too (Le et al., 2011).", "startOffset": 195, "endOffset": 212}, {"referenceID": 1, "context": "They have also been used to learn within-image correlations by letting input and output images be the same (Ranzato & Hinton, 2010; Bergstra et al., 2010).", "startOffset": 107, "endOffset": 154}, {"referenceID": 6, "context": "To adapt the parameters, W , based on a set of example patches {x\u03b1} one can use a variety of methods, including maximizing the average sparsity of z, minimizing a form of reconstruction error, maximizing the likelihood of the observations via Gibbs sampling, and others (see, for example, (Hyv\u00e4rinen et al., 2009) and references therein).", "startOffset": 289, "endOffset": 313}, {"referenceID": 2, "context": "(Bethge et al., 2007) use the term generalized quadrature pair to refer to the eigen-features of these transformations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Imposing a norm constraint has been a common approach to stabilizing learning (Ranzato & Hinton, 2010; Memisevic, 2011; Susskind et al., 2011), but it has not been clear why imposing norm constraints help.", "startOffset": 78, "endOffset": 142}, {"referenceID": 14, "context": "It is interesting to note that both, multiview sparse coding models (Taylor et al., 2010) and energy models (Le et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 8, "context": ", 2010) and energy models (Le et al., 2011) were recently shown to yield highly competitive performance in action recognition tasks, which require the encoding of motion in videos.", "startOffset": 26, "endOffset": 43}, {"referenceID": 7, "context": "We tested this approach on the \u201crotated MNIST\u201ddataset from (Larochelle et al., 2007), which consists of 72000 MNIST digit images of size 28 \u00d7 28 pixels that are rotated by arbitrary random angles (\u2212180 to 180 degrees; 12000 train-, 60000 test-cases, classes range from 0 \u2212 9).", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "Since the number of training cases is fairly large, most exemplars are represented at most angles, so even linear classifiers perform well (Larochelle et al., 2007).", "startOffset": 139, "endOffset": 164}], "year": 2012, "abstractText": "Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.", "creator": "LaTeX with hyperref package"}}}