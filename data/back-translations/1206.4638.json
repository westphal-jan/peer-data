{"id": "1206.4638", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Efficient Euclidean Projections onto the Intersection of Norm Balls", "abstract": "As a key building block of these methods, this paper suggests an efficient operator for the Euclidean projection to the intersection of $\\ ell _ 1 $and $\\ ell _ {1, q} $norm balls $(q = 2\\ text {or}\\ infty) $. We show that the projection can be reduced to finding the root of an auxiliary function that is piecemeal smooth and monotonous. Therefore, a bisection algorithm is sufficient to solve the problem. We show that the temporal complexity of our solution is $O (n + g\\ log g) $for $q = 2 $and $O (n\\ log n) $for $q =\\ infty $, where $n $is the dimensionality of the vector to be projected and $g $is the number of groups to be resolved; we clearly confirm that this projection method is more efficient than other methods based on efficiency.", "histories": [["v1", "Mon, 18 Jun 2012 15:16:28 GMT  (179kb)", "http://arxiv.org/abs/1206.4638v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["adams wei yu", "hao su", "fei-fei li"], "accepted": true, "id": "1206.4638"}, "pdf": {"name": "1206.4638.pdf", "metadata": {"source": "CRF", "title": "Efficient Euclidean Projections onto the Intersection of Norm Balls", "authors": ["Hao Su", "Wei Yu", "Li Fei-Fei"], "emails": ["haosu@cs.stanford.edu", "wyu@cs.hku.hk", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "The sparse-inducing norms have been powerful tools for learning robust models with limited data in highdimensional space. By imposing such norms as the\n* Indicates equal contributions.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nconstraints to the optimization task, one could bias the model towards learning sparse solutions, which in many cases has been proven to be statistically effective. Typical sparse-inducing norms include the \u21131 norm and \u21131,q norm (defined in Sec 3) (Liu & Ye, 2010); the former encourages element-wise sparsity and the latter encourages group-wise sparsity. In a variety of contexts, the two types of sparsity pattern exist simultaneously. For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks (Jalali et al., 2010). One natural approach is to formalize an optimization problem constrained by \u21131 norm and \u21131,q norm together, so that \u21131 norm induces the sparsity in features of each task and \u21131,q norm couples the sparsity across tasks(Friedman et al., 2010).\nProjection-based algorithms such as Projected Gradient (Bertsekas, 1999), Nesterov\u2019s optimal first-order method (Beck & Teboulle, 2009; Nesterov, 2007) and Projected Quasi-Newton (Schmidt et al., 2009) are major approaches to minimize a convex function with constraints. These algorithms minimize the objective function iteratively. By invoking the projection operation, the point of each iteration is guaranteed to be in the constraint set. Therefore, the projection serves as a key building block for such type of method.\nIn this paper, we study the problem of projecting a point in a high-dimensional space into the constraint set formed by the \u21131 and \u21131,q norms simultaneously, in particular, q = 2 or \u221e. We choose q = 2 or \u221e because these two types of norms are the most widely used group-sparsity inducing norms(Bach et al., 2011). Our Euclidean projection operator P\u03c41,\u03c42(1,q)+1(c) can be formulated as\nP\u03c41,\u03c42(1,q)+1(c) = argminx {\u2225x\u2212 c\u2225 2 2 | \u2225x\u22251,q \u2264 \u03c41, \u2225x\u22251 \u2264 \u03c42}.\n(1)\nwhere c is the point to be projected, \u2225x\u22251,q and \u2225x\u22251\nare the \u21131,q norm and \u21131 norm of x (Sec 3).\nWe formalize the projection as a convex optimization problem and show that the solution can be parameterized by the dual variables and the parametrization has an intuitive geometrical interpretation. Since the dual problem optimizes a concave objective, intuitively we can solve the dual variables through 2-D grid search. However, this method is costly and inaccurate. Further inspection reveals that seeking the optimal dual variable can be associated with finding the unique root of a 1-D auxiliary function. As a main result of this paper, we prove that this function is piecewise smooth and strictly monotonic. Therefore, it is sufficient to adopt a bisection algorithm to handle it efficiently. We then theoretically analyze its time complexity, which are O(n+ g log g) for q = 2 and O(n log n) for q = \u221e.\nHaving obtained an efficient projection algorithm, we can embed it in the projection-based optimization methods to efficiently find the \u201csimultaneous sparse\u201d solution to the following problem:\nmin w\nf(w) s.t. \u2225w\u22251,q \u2264 \u03c41, \u2225w\u22251 \u2264 \u03c42 (2)\nwhere f(w) is a convex function. We illustrate this point by experimentally solving regression problems with the above constraints.\nThe main contribution of this paper can be summarized as follows. Firstly, we are the first to propose a specific method that is highly efficient in time and memory usage for the composite norm projection problem. Secondly, we derive a bound to the time complexity of our algorithm and theoretically show that the algorithm enjoys fast convergence rate. This result is supported by the experiments using synthetic data and real data."}, {"heading": "2. Related Work", "text": "There has been a lot of research on efficient projection operators to norm balls. Projection onto the \u21132 ball is straightforward since we only need to rescale the point towards the origin. Linear time projection algorithms for \u21131 norm and \u21131,2 norm are proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al., 2009), respectively. For the \u21131,\u221e norm, (Quattoni et al., 2009) proposes a method with O(n log n) complexity and (Sra, 2010) introduces a method with weak linear time complexity.\nThe problem of projecting a point onto the intersection of convex sets has been studied since decades ago. In particular, various alternating direction methods have been proposed(Han, 1988; Perkins, 2002; Schmidt & Murphy, 2010). For example, Dykstra\u2019s algorithm (Dykstra, 1983) and ADMM\n(Gabay & Mercier, 1976) are variants of the alternating projection algorithm which successively projects the point onto the convex sets until convergence. Another approach to solve the projection problem is by modeling it as a Second-Order Cone Programming (SOCP) problem and solve it by the Interior Point solver. Although these algorithms could be applied, empirical results reveal their slow convergence rate and poor scalability. Recently, (Gong et al., 2011) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space. However, the problem is quite different from ours and their method could not be trivially applied. Consequently, a specific method tailored for our problem is needed."}, {"heading": "3. Notations and Definitions", "text": "We start by introducing the notation and definitions that will be used throughout the paper.\nGiven c \u2208 Rn, the \u21131, \u21132 and \u2113\u221e norms of c are defined as \u2225c\u22251 = \u2211n i=1 |ci|, \u2225c\u22252 = \u221a\u2211n i=1 c 2 i and \u2225c\u2225\u221e = max1\u2264i\u2264n{|ci|} respectively.\nIn addition, the indices of c are divided into g disjoint groups. Thus c can be written as c = [c\u03031; . . . ; c\u0303g], where each c\u0303i is a subvector of c. We define the \u21131,qnorm of c as \u2225c\u22251,q \u2261 \u2211g i=1 \u2225c\u0303i\u2225q. The (\u21131+\u21131,q)-norm ball is defined as the intersection of \u21131-norm ball and \u21131,q-norm ball.\nThe Euclidean projections onto the \u21131, \u21131,q and (\u21131 + \u21131,q) norm balls are denoted as P\u03c421 (\u00b7),P \u03c41 (1,q)(\u00b7) and P\u03c41,\u03c42(1,q)+1(\u00b7), respectively.\nFinally, we introduce three functions, SGN(c) = c \u2225c\u22252 , MAX(c,d) = [max(c1, d1); . . . ;max(cn, dn)], and MIN(c,d) = [min(c1, d1); . . . ;min(cn, dn)], where ci, di is the ith element of c and d."}, {"heading": "4. Euclidean Projection on the", "text": "(\u21131 + \u21131,q)-norm Ball\nIn this section, we will introduce our approach of Euclidean projection onto the (\u21131 + \u21131,2)-norm ball and (\u21131 + \u21131,\u221e)-norm ball. Due to space constraints, we leave most proofs in the appendix except Theorem 1."}, {"heading": "4.1. Euclidean Projection on the", "text": "(\u21131 + \u21131,2)-norm Ball\nIn this section, we first formulate the projection on the (\u21131 + \u21131,2)-norm ball as a convex optimization problem (Sec 4.1.1). We then parameterize the solution by dual variables and provide an intuitive geometrical interpretation (Sec 4.1.2). Finally, we determine\nthe optimal dual variable values by finding the unique zero point of a monotonic function with a bisection algorithm (Sec 4.1.3)."}, {"heading": "4.1.1. Problem Formulation", "text": "The Euclidean projection in Rn for the (\u21131+\u21131,2)-norm ball can be formalized as\nmin x\n1 2 \u2225c\u2212 x\u222522 s.t. \u2225x\u22251,2 \u2264 \u03c41, \u2225x\u22251 \u2264 \u03c42 (3)\nwhere \u03c41 (\u03c42) specifies the radius of \u21131,2 (\u21131) norm ball.\nUsing the following proposition, we can reflect the point c to the positive orthant by simply setting ci := |ci| and later recover the original optimizer by setting x\u2217i := sign(ci) \u00b7 x\u2217i . Therefore, we simply assume ci \u2265 0, i = 1, 2, ..., n from now on.\nProposition 1 Let x\u2217 be the optimizer of problem (3), then x\u2217i ci \u2265 0, i = 1, 2, ..., n."}, {"heading": "4.1.2. Parameterizing the Solution by Optimal Dual Variables", "text": "We can parameterize the solution x\u2217 by the optimal dual variables \u03bb\u22171 and \u03bb \u2217 2 as shown in the following lemma, so that the KKT system (Sec A.1 in the appendix) is satisfied (Bach et al., 2011).\nProposition 2 Suppose x\u2217, \u03bb\u22171 and \u03bb \u2217 2 are the primal and dual solutions respectively, then\nx\u0303\u2217k =SGN(MAX(c\u0303k \u2212 \u03bb\u22172e\u0303k, 0\u0303k)) \u00b7max(\u2225MAX(c\u0303k \u2212 \u03bb\u22172e\u0303k, 0\u0303k)\u22252 \u2212 \u03bb\u22171, 0)\n(4)\nwhere e\u0303k is a vector of all 1s which has the same dimension as c\u0303k.\nThe solution has an intuitive geometrical interpretation. x\u0303\u2217k is obtained by first translating c\u0303k by MIN(c\u0303k, \u03bb \u2217 2e\u0303) units towards the origin and then shrinking by a factor of max(\u2225MAX(c\u0303k\u2212\u03bb\u22172e\u0303k, 0\u0303k)\u22252\u2212\u03bb\u22171, 0). The geometrical interpretation is illustrated for the simple case where n = 2, g = 1 in Figure 1. According to Proposition 1, it is sufficient to consider the projection in the positive orthant. We divide the region outside the constraint set into three sets (Region I, II and III in Figure 1). The projection in Region I corresponds to the degenerated case when \u03bb\u22172 = 0 and thus x\u2217 = P\u03c421 (c) (A1 is projected to B1). The projection in Region II corresponds to the degenerated case when \u03bb\u22171 = 0, and thus x\n\u2217 = P\u03c411,2(c) (A2 is projected to B2). The projection in Region III corresponds to the case when \u03bb\u22171 > 0 and \u03bb \u2217 2 > 0, where we should employ P(1,2),1 (A3 is projected to B3). In this simple setting with only one group, we assume\nci\u2212\u03bb\u22172 > 0, i = 1, 2, \u2225c\u03031\u2212\u03bb\u22172e\u03031\u22252 > \u03bb\u22171, and thus (4) is reduced to x\u0303\u22171 = SGN(c\u03031\u2212\u03bb\u22172e\u03031) \u00b7 (\u2225c\u03031\u2212\u03bb\u22172e\u03031\u22252\u2212\u03bb\u22171). One can find that x\u0303\u22171 (OB3 in Figure 1) is actually a contraction of the translated unit vector SGN(c\u03031 \u2212 \u03bb\u22172e\u03031) by a factor (\u2225c\u03031 \u2212 \u03bb\u22172e\u03031\u22252 \u2212 \u03bb\u22171). Hence the projection path is separated into two segments. The first segment is called Translation Path, which is c\u03031 \u2212\u03bb\u22172e\u03031 (A3C3) of height \u03bb \u2217 2 in Figure 1. The second segment is called Stretch Path, which is C3B3 of length \u03bb \u2217 1 in Figure 1."}, {"heading": "4.1.3. Determining the Dual Variables", "text": "So far, we have transformed the Euclidean projection problem into determining the optimal dual variables \u03bb\u22171 and \u03bb \u2217 2. In this section, we discuss how to determine the variables case by case. We first consider the trivial cases when at least one of \u03bb\u22171 and \u03bb \u2217 2 equals to zero.\nCase 1: \u03bb\u22171 = 0 and \u03bb \u2217 2 = 0. This is the case when c\nis already in the (\u21131 + \u21131,2)-norm ball (the shaded area in Figure 1) and no projection is needed. We can test this case by checking the \u21131 and \u21131,2 norms of c. Case 2: \u03bb\u22171 > 0 and \u03bb \u2217 2 = 0. This is the case when\nx\u2217 = P\u03c411,2(c) (Region II in Figure 1). We can test this case by checking whether P\u03c411,2(c) lies in the \u21131-norm ball. Case 3: \u03bb\u22171 = 0 and \u03bb \u2217 2 > 0. This is the case when\nx\u2217 = P\u03c421 (c) (Region I in Figure 1). We can test\nthis case by checking whether P\u03c421 (c) lies in the \u21131,2-norm ball. Now we discuss the non-trivial case when \u03bb\u22171 > 0 and \u03bb\u22172 > 0 (Region III in Figure 1). According to the complementary-slackness condition of the KKT system (See (3) and (4) in the appendix), the solution satisfies\ng\u2211 i=1 \u2225x\u0303\u2217i \u22252 = \u03c41, n\u2211 j=1 |x\u2217j | = \u03c42. (5)\nSubstitute (4) into (5) and we get the two equations of \u03bb1 and \u03bb2:\n\u03c41 = \u2211\ni\u2208S\u03bb1,\u03bb2\n[\u2225MAX(c\u0303i \u2212 \u03bb2e\u0303i, 0\u0303i)\u22252 \u2212 \u03bb1] (6)\n\u03c42 = \u2211\ni\u2208S\u03bb1,\u03bb2\n(1\u2212 \u03bb1 \u2225MAX(c\u0303i \u2212 \u03bb2e\u0303i, 0\u0303i)\u22252 )\n\u00b7 \u2211\nj\u2208Si\u03bb2\n(ci,j \u2212 \u03bb2) (7)\nwhere S\u03bb1,\u03bb2 = {i | \u2225MAX(c\u0303i \u2212 \u03bb2e\u0303i, 0\u0303i)\u22252 > \u03bb1} and Si\u03bb2 = {j | ci,j > \u03bb2}, i = 1, 2, . . . , g. Now the task is to find a pair (\u03bb\u22171, \u03bb \u2217 2) which satisfies (6) and (7) simultaneously.\n(6) implicitly defines a function \u03bb1(\u03bb2) and use this fact we obtain the following equation:\n\u03bb1(\u03bb2) =\n\u2211 i\u2208S\u03bb1,\u03bb2\n\u2225MAX(c\u0303i \u2212 \u03bb2e\u0303i, 0\u0303i)\u22252 \u2212 \u03c41 |S\u03bb1,\u03bb2 | (8)\nNote that (8) does not define an explicit function \u03bb1(\u03bb2) since \u03bb1 also appears on the right side of (8). For a detailed proof that \u03bb1 is an implicit function of \u03bb2, please check Lemma 2 and Lemma 3 in the appendix.\nBy substituting \u03bb1(\u03bb2) into (7), it is easy to see that solving the equation system (6) and (7) is equivalent to finding the zero point of the following function:\nf(\u03bb2) = \u2211\ni\u2208S\u03bb1(\u03bb2),\u03bb2\n(1\u2212 \u03bb1(\u03bb2) \u2225MAX(c\u0303i \u2212 \u03bb2e\u0303i, 0\u0303i)\u22252 )\n\u00b7 \u2211\nj\u2208Si\u03bb2\n(ci,j \u2212 \u03bb2)\u2212 \u03c42 (9)\nThe following theorem states that f(\u03bb2) is continuous, piece-wise smooth and monotone. The fact immediately leads to a bisection algorithm to efficiently find the zero point.\nTheorem 1 1) f is a continuous piece-wise smooth function in (0,max{ci,j}); 2) f is monotonically decreasing and it has a unique root in (0,max{ci,j}).\nWe leave the proof of the continuity and piecewise smooth property in the appendix (Lemma 5 in the appendix). Here we just prove the monotonicity of f(\u03bb2).\nProof: Because f(\u03bb2) is continuous and piecewise smooth in (0,max{ci,j}), it is sufficient to prove that f \u2032(\u03bb2) \u2264 0 for \u03bb2 \u2208 R+\\E , where E is a set containing finite points as defined in Lemma 4 in the appendix. For such points, by Lemma 4, we can always find an interval (a, b) where S\u03bb1,\u03bb2 and Si\u03bb2 do not change, hence we can denote S1 = S\u03bb1,\u03bb2 and Si2 = Si\u03bb2 here for simplicity.\nDenote \u2225 \u00b7 \u2225i1 = \u2211\nj\u2208Si2 (ci,j \u2212 \u03bb2) and \u2225 \u00b7 \u2225i2 =\u221a\u2211\nj\u2208Si2 (ci,j \u2212 \u03bb2)2. Within the interval, we assume\n\u2225 \u00b7 \u2225i2 \u2265 \u03bb1(\u03bb2). Therefore,\nf \u2032(\u03bb2) = \u2212 \u2211 i\u2208S1 |Si2|+ 1 |S1| ( \u2211 i\u2208S1 \u2225 \u00b7 \u2225i1 \u2225 \u00b7 \u2225i2 )2\n+\u03bb1(\u03bb2) \u2211 i\u2208S1 |Si2| \u2212 ( \u2225\u00b7\u2225i1 \u2225\u00b7\u2225i2 )2 \u2225 \u00b7 \u2225i2\n\u2264 \u2212 \u2211 i\u2208S1 |Si2|+ 1 |S1| ( \u2211 i\u2208S1 \u2225 \u00b7 \u2225i1 \u2225 \u00b7 \u2225i2 )2\n+min{\u2225 \u00b7 \u2225i2} \u2211 i\u2208S1 |Si2| \u2212 ( \u2225\u00b7\u2225i1 \u2225\u00b7\u2225i2 )2 min{\u2225 \u00b7 \u2225i2}\n\u2264 |S1|[( 1 |S1| \u2211 i\u2208S1 \u2225 \u00b7 \u22251 \u2225 \u00b7 \u22252 )2 \u2212 1 |S1| \u2211 i\u2208S1 ( \u2225 \u00b7 \u22251 \u2225 \u00b7 \u22252 )2] \u2264 0\nSince f(0) > 0 and f(max{ci,j}) < 0 and E is finite, there exists one and only one root in [0,max{ci,j}]. Given the theorem above, it is sufficient to apply a bisection algorithm to f(\u03bb2) to find its unique root. Note that it is non-trivial to evaluate \u03bb1(\u03bb2) since (8) is not a definition of the function, as we discussed before. Now we introduce an algorithm FindLambda1 to tackle it. More specifically, we first sort all the groups MAX(c\u0303k \u2212 \u03bb2e\u0303k, 0\u0303k), k = 1, 2, ...g in ascending order w.r.t their \u21132-norms. Then we repeatedly add the group indexes one at a time to the active group set S\u03bb1,\u03bb2 , calculating the corresponding \u03bb1 and checking its validity. This process stops as soon as \u03bb1, \u03bb2 and S\u03bb1,\u03bb2 are all consistent.\nComplexity Analysis: Given the tolerance \u03f5, the bisection projection algorithm converges after no more than \u2308log2[maxi(ci)/\u03f5]\u2309 outermost iterations (lines 4- 21 in Algorithm 1). In each iteration, FindLambda1 dominates the complexity. In Algorithm 2, line 4 costs O(n) flops. While additional O(n) flops are needed for calculating the \u21132-norm of each group, the sort-\nAlgorithm 1 \u21131 + \u21131,2 Projection\n1: Input: c, group Index of each ci, \u03c41, \u03c42, \u03f5. 2: Output: \u03bb1, \u03bb2, x. 3: left = 0; right = max{ci,j}; 4: while true do 5: \u03bb2 = (left+ right)/2; 6: [\u03bb1,isLambda1Found]=FindLambda1(c, \u03bb2, group\nIndex of each ci); 7: if isLambda1Found == true then 8: Evaluate f(\u03bb2) according to (9); 9: if |f(\u03bb2)| < \u03f5 then 10: break; 11: else 12: if f(\u03bb2) < \u2212\u03f5 then 13: right = \u03bb2; 14: else 15: left = \u03bb2; 16: end if 17: end if 18: else 19: right = \u03bb2; 20: end if 21: end while 22: Calculate x according to (4);\ning in line 5 takes O(g log g) flops. Finally, the complexity of line 7 to line 14 is O(g). Therefore, the overall time complexity for the projection algorithm is \u2308log2[maxi(ci)/\u03f5]\u2309 \u00b7O(n+ g log g)."}, {"heading": "4.2. Euclidean Projection on the", "text": "(\u21131 + \u21131,\u221e)-norm Ball\nThe projection onto the (\u21131 + \u21131,\u221e)-norm ball could also be addressed by a bisection algorithm. We first introduce a variable d, and then give an equivalent formulation of the projection problem (1) for q = \u221e as follows:\nmin x,d\n1 2 \u2225c\u2212 x\u222522\ns.t. xi,j \u2264 di(i = 1, 2, ...g), g\u2211\ni=1\ndi \u2264 \u03c41,\n\u2225x\u22251 \u2264 \u03c42, xi,j \u2265 0, di \u2265 0.\n(10)\nNote that the formulation above differs from (Quattoni et al., 2009) in the additional term \u2225x\u22251 \u2264 \u03c42. Similar to Sec 4.1.2, given the KKT system (in the appendix), we can parameterize the solution x\u2217 using d\u2217 and optimal dual variables \u03bb\u22171 and \u03bb \u2217 2:\nProposition 3 Suppose x\u2217, d\u2217 and \u03bb\u22171, \u03bb \u2217 2 are the primal and dual solution respectively, then\nx\u2217i,j = min(max(ci,j \u2212 \u03bb\u22172, 0), d\u2217i ),\nd\u2217i =\n\u2211 j\u2208Si,1\n\u03bb\u22172\n(ci,j \u2212 \u03bb\u22172)\u2212 \u03bb\u22171\n|Si,1\u03bb\u22172 | ,\n\u03bb\u22171 =\n\u2211 i\u2208S\u03bb\u22172\n\u2211 j\u2208Si,1\n\u03bb\u22172\n(ci,j\u2212\u03bb\u22172)\u2212\u03c41\n|Si,1 \u03bb\u22172 |\u2211 i\u2208S\u03bb\u22172 1\n|Si,1 \u03bb\u22172 |\n,\nwhere Si\u03bb\u22172 \u2261 {j|ci,j \u2212 \u03bb \u2217 2 > 0} = S i,1 \u03bb\u22172 \u222a Si,2\u03bb\u22172 , S i,1 \u03bb\u22172 = {j|ci,j \u2212 \u03bb\u22172 > di} and S i,2 \u03bb\u22172 = {j|0 < ci,j \u2212 \u03bb\u22172 \u2264 di}.\nThe proposition above reveals that x\u2217i,j , d \u2217 and \u03bb\u22171 can all be viewed as functions of \u03bb\u22172. We can substitute the above equations into the KKT system and show that \u03bb\u22172 is the zero point of the following function\nh(\u03bb2) = \u2211 i \u2211 j min(max(ci,j \u2212 \u03bb2, 0), di(\u03bb2))\u2212 \u03c42\nWe can prove that h(\u03bb2) is a strictly monotonically decreasing function:\nTheorem 2 1) h is a continuous piece-wise smooth function in (0,max{ci,j}); 2) h is monotonically decreasing and it has a unique root in (0,max{ci,j}).\nComplexity Analysis: Based upon the above theorem, we can determine \u03bb\u22172 using the bisection Algorithm 3. For a given \u03bb2, \u2200i, j,max(ci,j \u2212 \u03bb2, 0) is determined, (Quattoni et al., 2009) shows that \u03bb\u22171 and d \u2217 can be solved with time complexity O(n log n). Therefore, the total time complexity of the bisection algorithm is \u2308log2[maxi,j(ci,j)/\u03f5]\u2309 \u00b7O(n log n)."}, {"heading": "5. Experiments", "text": "In this section, we demonstrate the efficiency and effectiveness of the proposed projection algorithm in experiments using synthetic and real-world data. Due to the space limitation, we only show the result of \u21131 + \u21131,2, and the case of \u21131 + \u21131,\u221e is shown in the appendix."}, {"heading": "5.1. Efficiency of the Proposed Projection Algorithms", "text": "We first compare our methods to Interior Point (IP) method and alternating projection methods which are also applicable in solving the Euclidean projection problem. For IP, we use a highly optimized commercial software MOSEK1. For alternating projection methods, as there is no algorithm specifically tailored for our problem, we compare with two widely used representative algorithms \u2013 Dykstra\u2019s algorithm and ADMM algorithm. Both algorithms generate a sequence of points whose limit is the orthogonal projection onto the intersection of convex sets 2.\n1We reformulate the problem as an SOCP problem. Please refer to the appendix for the SOCP formulation.\n2Check Sec G in Appendix for more details.\nNote that all methods in the comparison apply P\u03c421 (\u00b7) in Region I and P\u03c411,2(\u00b7) in Region II. Therefore, we first show the time cost for the shared modules P\u03c411,q(\u00b7) and P\u03c421 (\u00b7), and then compare the different projection methods in Region III. To estimate the expected running time of each method, we estimate the volume of the three regions by Monte Carlo method with uniform sampling distribution.\nWe generate synthetic data with different number of groups g and dimensions n. Specifically, each dimension of a point is sampled uniformly in [\u2212103, 103], and \u03c4 (2) 1 = 5, \u03c4 (2) 2 = 6, \u03c4 (\u221e) 1 = 5, \u03c4 (\u221e) 2 = 10. Each method is run for 10 times to calculate the average running time and standard deviation. To estimate the area of each region, we sampled 10,000 i.i.d points.\nAlgorithm 2 FindLambda1\n1: Input: c, \u03bb2, group Index of each ci. 2: Output: \u03bb1, isLambda1Found. 3: isLambda1Found = false; 4: for each i do xi = max(ci \u2212 \u03bb2, 0); 5: Sort the groups of x\u0303 in ascending order, w.r.t their \u21132-\nnorms; 6: sum = 0; 7: for i = g down to 1 do 8: sum = sum+ \u2225x\u0303i\u22252; 9: \u03bb1 = (sum\u2212 \u03c41)/(g \u2212 i+ 1); 10: if (i > 1 and \u2225x\u0303i\u22121\u22252 < \u03bb1 \u2264 \u2225x\u0303i\u22252) or (i == 1 and 0 < \u03bb1 \u2264 \u2225x\u03031\u22252) then 11: isLambda1Found = true; 12: break; 13: end if 14: end for\nAlgorithm 3 \u21131 + \u21131,\u221e Projection\n1: Input: c, group Index of each ci, \u03c41, \u03c42, \u03f5. 2: Output: \u03bb2, \u03bb1,x. 3: Initialization; 4: while |h(\u03bb2)| > \u03f5 do 5: if h(\u03bb2) > \u03f5 then 6: left = \u03bb2; 7: else 8: right = \u03bb2; 9: end if 10: \u03bb2 = (left+ right)/2; 11: for each i, j do xi,j = max(ci,j \u2212 \u03bb2, 0); 12: [x,d, \u03bb2, \u03bb1] = P\u03c41(1,\u221e)(x); 13: Evaluate h(\u03bb2); 14: end while\nIn our proposed projection method, as in each step the bisection algorithm halves the range of \u03bb2, we stop when the range is smaller than 10\u22129. For Dykstra\u2019s algorithm and ADMM algorithm, we stop when the \u21132-\nnorm of two consecutive projected points falls below 10\u22129. For IP, we stop it if the duality gap is below 10\u22129. Table 1 summarizes the results for q = 2.\nWe can observe from the table that: 1) our proposed technique is significantly faster than IP and alternating projection methods (Dykstra and ADMM) in Region III; 2) our method scales well to large problems.\nCompared with IP, which is the runner-up algorithm in speed, our algorithm is more memory efficient since very little extra space is needed, whereas IP introduces several groups of auxiliary variables.\nCompared with Dykstra\u2019s algorithm and ADMM algorithm, our method takes much less iterations to converge. As we discussed before, the number of iterations of our algorithm is bounded by \u2308log2[maxi(ci)/\u03f5]\u2309. When n = 1000 and g = 100, it can be calculated that our algorithm takes no more than 30 iterations. On the other hand, empirical study shows that Dykstra\u2019s algorithm takes 692\u00b1247 iterations to converge in Region III. A closer study also shows that Dykstra\u2019s algorithm suffers from a high variance in iterations, which may be related to the order of projections. For example, 692 \u00b1 247 iterations are taken if we first project onto the \u21131-norm ball, whereas 3460 \u00b1 565 iterations are taken if we first project onto the \u21131,2-norm ball.\nWe also analyze the volume of each region by Monte Carlo sampling method. Table 2 indicates that the probability for a point falling in Region III may be very high. Because our algorithm runs much faster than competitors in Region III, its expected running time can be much shorter in general."}, {"heading": "5.2. Efficiency of the Projection-based Methods in Linear Regression with Composite Norm Constraints", "text": "In this section, we show that embedded with our proposed projection operator, various projection-based method can efficiently optimize the composite norm constrained linear regression problem. These projection-based methods significantly outperform the baseline method in speed.\nWe embed our projection operator into three types of projection-based optimization framework, including Projected Gradient method (PG), Nesterov\u2019s optimal first-order method (Nesterov) and Projected QuasiNewton method (PQN).\nWe synthesize a small-sized data and a medium-sized data. For the small-sized data, we adopt the experiment setting in (Friedman et al., 2010). We create the coefficient vector w \u2208 R100 divided in ten blocks of ten, where the last four blocks are set to all ze-\nros and the first six blocks have 10, 8, 6, 4, 2 and 1 non-zero entries respectively. All the non-zero coefficients are randomly chosen as \u00b11. Then we generate N = 200 observations y \u2208 R200 by setting y = Xw+\u03f5, where X \u2208 R200\u00d7100 denotes the synthetic data points of standard Gaussian distribution with correlation 0.2 within a group and zero otherwise and \u03f5 \u2208 R200 is a Gaussian noise vector with standard deviation 4.0 in each entry. For the medium-sized data, the data generation process is similar except that the first six blocks have 100, 80, 60, 40, 20 and 10 non-zero entries respectively for 4000 observations.\nSince the generated w exhibits sparsity in both group level and individual level, it is natural to recover w by solving the following constrained linear regression problem (q = 2 or \u221e):\nmin 1\n2 \u2225y \u2212Xw\u222522 s.t. \u2225w\u22251,q \u2264 \u03c4 (q) 1 , \u2225w\u22251 \u2264 \u03c4 (q) 2\nWe choose Interior Point method as the baseline to solve the above problem for its efficiency. Embedded with our efficient projection operator, all projectionbased algorithms (PG, Nesterov and PQN) take much less time and memory than IP to converge to the same accuracy (10\u22129) (see Table 3 for time used). We note that, projection-based methods usually take much more iterations to converge than IP (see Figure 2, and the projection operator may be invoked several times per iteration. Hence, the efficiency of the projection operator greatly impact the performance.\n0 10 20 30 40 50 60 70 80 90 100 10\n\u221210\n10 \u22128\n10 \u22126\n10 \u22124\n10 \u22122\n10 0\n10 2\n10 4\nL 1 + L 1,2\nIteration\nf \u2212 f *\nIP PQN Nesterov\nPG } USE OUR ALGO\nFigure 2. Number of iterations on linear regression with (\u21131 + \u21131,2)-norm constraint for different methods. With our efficient projection operator, all three projection-based methods converge faster than IP even though they take more iterations."}, {"heading": "5.3. Classification Performance in Multi-task Learning", "text": "In this experiment, we show that using our efficient projection operator, with limited additional time cost, composite norm regularizer outperforms single norm regularizer in multi-task learning. In the multiple-task learning setting, there are r > 1 response variables (each corresponding to a task) and a common set of p features. We hypothesize that, if the relevant features for each task are sparse and there is a large overlap of these relevant features across tasks, combining \u21131,q norm and \u21131 norm will recover both the sparsity of each task and the sparsity shared across tasks.\nWe use handwritten digits recognition as test case. The input data are features of handwritten digits (0-9) extracted from a collection of Dutch utility maps(Asuncion & Newman, 2007). This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms(Jalali et al., 2010). There are r = 10 tasks, and each sample consists of p = 649 features. We use logistic regression as the classifier and constrain the classifier by \u21131 norm, \u21131,q norm and \u21131+\u21131,q norm, respectively. PQN method is used to optimize the objective function.\nWe compare the running time and classification performance of each method. The classification performance is measured by the mean and standard deviation of the classification error. Results are obtained from ten ran-\ndom samples of training and testing data with parameters chosen via cross-validation in all methods. Using our projection operator, (\u21131+\u21131,q)-norm yields the best classification result with similar running time to \u21131,q-norm (Table 4). We also test replacing our projection algorithm by the runner-up algorithm in Table 1, which is IP for q = 2 and Dykstra for q = \u221e. Unfortunately, using these projection operators, PQN could not converge within 30 minutes. These results show that a more structured yet complicated regularizer is more effective in a multiple-task learning problem and our efficient projection algorithms make it feasible."}], "references": [{"title": "Convex optimization with sparsity-inducing norms", "author": ["Bach", "Francis", "Jenatton", "Rodolphe", "Mairal", "Julien", "Obozinski", "Guillaume"], "venue": "Optimization for Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["Duchi", "John C", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In ICML, pp", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "An algorithm for restricted least squares regression", "author": ["Dykstra", "Richard L"], "venue": "JASA, 78(384):837\u2013842,", "citeRegEx": "Dykstra and L.,? \\Q1983\\E", "shortCiteRegEx": "Dykstra and L.", "year": 1983}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Arxiv preprint arXiv10010736,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["Gabay", "Daniel", "Mercier", "Bertrand"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "Gabay et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Gabay et al\\.", "year": 1976}, {"title": "Efficient euclidean projections via piecewise root finding and its application in gradient", "author": ["Gong", "Pinghua", "Gai", "Kun", "Zhang", "Changshui"], "venue": "projection. Neurocomputing,", "citeRegEx": "Gong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2011}, {"title": "A successive projection method", "author": ["Han", "Shih-Ping"], "venue": "Mathematical Programming,", "citeRegEx": "Han and Shih.Ping.,? \\Q1988\\E", "shortCiteRegEx": "Han and Shih.Ping.", "year": 1988}, {"title": "A dirty model for multi-task learning", "author": ["Jalali", "Ali", "Ravikumar", "Pradeep D", "Sanghavi", "Sujay", "Ruan", "Chao"], "venue": "In NIPS, pp", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "Efficient euclidean projections in linear time", "author": ["Liu", "Jun", "Ye", "Jieping"], "venue": "In ICML, pp", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Efficient l1/lq Norm Regularization", "author": ["Liu", "Jun", "Ye", "Jieping"], "venue": "Arxiv preprint arXiv:1009.4766,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Nesterov", "Yu"], "venue": "ReCALL,", "citeRegEx": "Nesterov and Yu.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2007}, {"title": "A convergence analysis of dykstra\u2019s algorithm for polyhedral sets", "author": ["Perkins", "Chris"], "venue": "SIAM J. Numerical Analysis,", "citeRegEx": "Perkins and Chris.,? \\Q2002\\E", "shortCiteRegEx": "Perkins and Chris.", "year": 2002}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["Quattoni", "Ariadna", "Carreras", "Xavier", "Collins", "Michael", "Darrell", "Trevor"], "venue": "In ICML, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Convex structure learning in log-linear models: Beyond pairwise potentials", "author": ["Schmidt", "Mark W", "Murphy", "Kevin P"], "venue": "In AISTATS, pp", "citeRegEx": "Schmidt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2010}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton", "author": ["Schmidt", "Mark W", "van den Berg", "Ewout", "Friedlander", "Michael P", "Murphy", "Kevin P"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2009}, {"title": "Generalized proximity and projection with norms and mixed-norms", "author": ["Sra", "Suvrit"], "venue": "In Technique Report,", "citeRegEx": "Sra and Suvrit.,? \\Q2010\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks (Jalali et al., 2010).", "startOffset": 172, "endOffset": 193}, {"referenceID": 4, "context": "One natural approach is to formalize an optimization problem constrained by l1 norm and l1,q norm together, so that l1 norm induces the sparsity in features of each task and l1,q norm couples the sparsity across tasks(Friedman et al., 2010).", "startOffset": 217, "endOffset": 240}, {"referenceID": 15, "context": "Projection-based algorithms such as Projected Gradient (Bertsekas, 1999), Nesterov\u2019s optimal first-order method (Beck & Teboulle, 2009; Nesterov, 2007) and Projected Quasi-Newton (Schmidt et al., 2009) are major approaches to minimize a convex function with constraints.", "startOffset": 179, "endOffset": 201}, {"referenceID": 0, "context": "We choose q = 2 or \u221e because these two types of norms are the most widely used group-sparsity inducing norms(Bach et al., 2011).", "startOffset": 108, "endOffset": 127}, {"referenceID": 2, "context": "Linear time projection algorithms for l1 norm and l1,2 norm are proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al.", "startOffset": 76, "endOffset": 112}, {"referenceID": 15, "context": ", 2008; Liu & Ye, 2009) and (Schmidt et al., 2009), respectively.", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "For the l1,\u221e norm, (Quattoni et al., 2009) proposes a method with O(n log n) complexity and (Sra, 2010) introduces a method with weak linear time complexity.", "startOffset": 19, "endOffset": 42}, {"referenceID": 6, "context": "Recently, (Gong et al., 2011) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space.", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "1 in the appendix) is satisfied (Bach et al., 2011).", "startOffset": 32, "endOffset": 51}, {"referenceID": 13, "context": "Note that the formulation above differs from (Quattoni et al., 2009) in the additional term \u2225x\u22251 \u2264 \u03c42.", "startOffset": 45, "endOffset": 68}, {"referenceID": 13, "context": "For a given \u03bb2, \u2200i, j,max(ci,j \u2212 \u03bb2, 0) is determined, (Quattoni et al., 2009) shows that \u03bb1 and d \u2217 can be solved with time complexity O(n log n).", "startOffset": 55, "endOffset": 78}, {"referenceID": 4, "context": "For the small-sized data, we adopt the experiment setting in (Friedman et al., 2010).", "startOffset": 61, "endOffset": 84}, {"referenceID": 8, "context": "This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms(Jalali et al., 2010).", "startOffset": 109, "endOffset": 130}, {"referenceID": 8, "context": "Our performance is on par with state-of-the-art (Jalali et al., 2010).", "startOffset": 48, "endOffset": 69}], "year": 2012, "abstractText": "Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of l1 and l1,q norm balls (q = 2 or \u221e) is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is O(n + g log g) for q = 2 and O(n log n) for q = \u221e, where n is the dimensionality of the vector to be projected and g is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projectionbased algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy.", "creator": " TeX output 2012.05.19:1810"}}}