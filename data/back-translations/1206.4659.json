{"id": "1206.4659", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "abstract": "We present a nonparametric model with maximum margin that combines the ideas of learning with maximum margin and Bayesian nonparametry to detect discriminatory latent characteristics for link prediction and automatically infer the unknown latent social dimension. By minimizing hinge loss using the linear expectation operator, we can efficiently perform posterior inferences without having to deal with a highly nonlinear link probability function; by using a fully Bayesian formulation, we can avoid the matching of regularization constants. Experimental results from real data sets seem to show the benefits inherited from learning with maximum margin and fully Bayesian nonparametry.", "histories": [["v1", "Mon, 18 Jun 2012 15:27:56 GMT  (994kb)", "http://arxiv.org/abs/1206.4659v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jun zhu"], "accepted": true, "id": "1206.4659"}, "pdf": {"name": "1206.4659.pdf", "metadata": {"source": "CRF", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "authors": ["Jun Zhu"], "emails": ["dcszj@mail.tsinghua.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "As the availability and scope of social networks and relational datasets increase, a considerable amount of attention has been devoted to the statistical analysis of such data, which is typically represented as a graph in which the vertices represent entities and edges represent links between entities. Link prediction is one fundamental problem in analyzing these social network or relational data, and its goal is to predict unseen links between entities given the observed links. Often there is extra information about links and entities such as attributes and timestamps (Liben-Nowell & Kleinberg, 2003; Backstrom & Leskovec, 2011; Miller et al., 2009) that can be used to help with prediction.\nRecently, various approaches based on probabilistic models have been developed for link prediction. One class of such models utilize a latent feature matrix and a link function (e.g., the commonly used sig-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmoid function) (Hoff, 2007; Miller et al., 2009) to define the link formation probability distribution. These latent feature models were shown to generalize latent class (Nowicki & Snijders, 2001; Airoldi et al., 2008) and latent distance (Hoff et al., 2002) models and are thus able to represent both homophily and stochastic equivalence, which are important properties commonly observed in real-world social network and relational data. The parameters for these probabilistic models are typically estimated with MLE or their posterior distributions are inferred with Monte Carlo methods. Such techniques have demonstrated competitive results on various datasets. However, to determine the unknown dimensionality of the latent feature space (or latent social space), most of the existing approaches rely on a general model selection procedure, e.g., cross-validation, which could be expensive by comparing many different settings. The work (Miller et al., 2009) is an exception, which presents a nonparametric Bayesian method to automatically infer the unknown social dimension.\nThis paper presents an alternative way to develop nonparametric latent feature relational models. Instead of defining a normalized link likelihood model, we propose to directly minimize some objective function (e.g., hinge-loss) that measures the quality of link prediction, under the principle of maximum entropy discrimination (MED) (Jaakkola et al., 1999; Jebara, 2002), which was introduced as an elegant framework to integrate max-margin learning and Bayesian generative modeling. The present work extends MED in several novel ways to solve the challenging link prediction problem. First, like (Miller et al., 2009), we use nonparametric Bayesian techniques to automatically resolve the unknown dimension of a latent social space, and thus our work represents an attempt towards uniting Bayesian nonparametrics and max-margin learning, which have been largely treated as two isolated topics. Second, we present a fully-Bayesian method to avoid tuning regularization constants. By minimizing a hinge-loss, our model avoids dealing with a\nhighly nonlinear link likelihood (e.g., sigmoid) and can be efficiently solved using variational methods, where the sub-problems of max-margin learning are solved with existing high-performance solvers. Experimental results on three real datasets appear to demonstrate that 1) using max-margin learning can significantly improve the link prediction performance, and 2) using fully-Bayesian methods, we can avoid tuning regularization constants without sacrificing the performance, and dramatically decrease running time.\nThe paper is structured as follows. Sec 2 introduces existing latent feature models, as well as a new insight about the connections between these models. Sec 3 presents the max-margin latent feature relational model and a fully-Bayesian formulation. Sec 4 presents empirical results. Finally, Sec 5 concludes."}, {"heading": "2. Latent Feature Relational Models", "text": "Assume we have an N \u00d7 N relational link matrix Y , where N is the number of entities. We consider the binary case, where the entry Yij = +1 (or Yij = \u22121) indicates the presence (or absence) of a link between entity i and entity j. We emphasize that all the latent feature models introduced below can be extended to deal with real or categorical Y . Y is not fully observed. The goal of link prediction is to learn a model from observed links such that we can predict the values of unobserved entries of Y . In some cases, we may have observed attributes Xij \u2208 RD that affect the link between i and j.\nIn a latent feature relational model, each entity is associated with a vector \u00b5i \u2208 RK , a point in a latent feature space (or latent social space). Then, the link likelihood is generally defined as\np(Yij = 1|Xij , \u00b5i, \u00b5j) = \u03a6(\u00b5+ \u03b2\u22a4Xij + \u03c8(\u00b5i, \u00b5j)), (1)\nwhere a common choice of \u03a6 is the sigmoid function, i.e., \u03a6(t) = 11+e\u2212t . For the latent distance model (Hoff et al., 2002), we have\n\u03c8(\u00b5i, \u00b5j) = \u2212d(\u00b5i, \u00b5j), where d is a distance function.\nFor the latent eigenmodel (Hoff, 2007), which generalizes the latent distance model and the latent class model for modeling symmetric relational data, we have\n\u03c8(\u00b5i, \u00b5j) = \u00b5 \u22a4 i D\u00b5j , where D is a diagonal matrix.\nIn the above models, the dimension K is unknown a priori, and a model selection procedure (e.g., cross-validation) is needed. The nonparametric latent feature relational model (LFRM) (Miller et al., 2009) leverages the recent advances in Bayesian nonparametrics to automatically infer the latent\ndimension. Moreover, LFRM differs from the above models by inferring binary latent features and defining\n\u03c8(\u00b5i, \u00b5j) = \u00b5 \u22a4 i W\u00b5j , where \u00b5i \u2208 {0, 1}\u221e.\nWe will use Z to denote a binary feature matrix, where each row corresponds to the latent feature of an entity. For LFRM, we have Z = [\u00b5\u22a41 ; \u00b7 \u00b7 \u00b7 ;\u00b5\u22a4N ]. Fully-Bayesian inference with MCMC sampling is usually performed for these models by imposing appropriate priors on latent features and model parameters. In LFRM, Indian buffet process (IBP) (Griffiths & Ghahramani, 2006) was used as the prior of Z to induce a sparse latent feature vector for each entity.\nMiller et al. discussed the expressiveness of LFRM over latent class models. Here, we provide another support for the expressiveness. For modeling symmetric relational data, we usually constrain W to be symmetric (Miller et al., 2009). Since a symmetric real matrix is diagonalizable, i.e., there exists an orthogonal matrix Q satisfying that Q\u22a4WQ is a diagonal matrix, denoted again by D, we have W = QDQ\u22a4. Thus we can treat ZQ as the effective real-valued latent features and conclude that LFRM subsumes the latent egienmodel for modeling symmetric relational data."}, {"heading": "3. Max-margin Latent Feature Models", "text": "Now, we present the max-margin latent feature model for link prediction. We first briefly review the basic concepts of MED (Jaakkola et al., 1999; Jebara, 2002)."}, {"heading": "3.1. MED", "text": "We consider binary classification, where the response variable Y takes values from {+1,\u22121}. Let X be an input feature vector and F (X; \u03b7) be a discriminant function parameterized by \u03b7. Let D = {(Xn, Yn)}Nn=1 be a training set and define h\u2113(x) = max(0, \u2113 \u2212 x), where \u2113 is a positive cost parameter. Unlike standard SVMs, which estimate a single \u03b7, MED learns a distribution p(\u03b7) by solving an entropic regularized risk minimization problem with prior p0(\u03b7)\nmin p(\u03b7)\nKL(p(\u03b7)\u2225p0(\u03b7)) + CR(p(\u03b7)), (2)\nwhere C is a positive constant; KL(p\u2225q) is the KL divergence; R(p(\u03b7)) = \u2211n h1(YnEp(\u03b7)[F (Xn; \u03b7)]) is the hinge-loss that captures the large-margin principle underlying the MED prediction rule\nY\u0302 = signEp(\u03b7)[F (X; \u03b7)]. (3)\nBy defining F as the log-likelihood ratio of a Bayesian generative model, MED provides an elegant way to\nintegrate the discriminative max-margin learning and Bayesian generative modeling. MED subsumes SVM as a special case and has been extended to incorporate latent variables (Jebara, 2002; Zhu et al., 2009) and to perform structured output prediction (Zhu & Xing, 2009). Recent work has further extended MED to unite Bayesian nonparametrics and max-margin learning (Zhu et al., 2011a;b), which have been largely treated as isolated topics, for learning better classification models. The present work contributes by introducing a novel generalization of MED to perform the challenging relational link prediction."}, {"heading": "3.2. MED Latent Feature Relational Model", "text": "Now, we present the max-margin latent feature model for link prediction. Based on the above discussions, we use the same formulations as the most general LFRM model. Specifically, we represent each entity using a set of binary features and let Z to denote the binary feature matrix, of which each row corresponds to an entity and each column corresponds to a feature. The entry Zik = 1 means that entity i has the feature k.\nIf the features Zi and Zj are given, we can naturally define the latent discriminant function as\nf(Zi, Zj ;Xij ,W, \u03b7) =ZiWZ \u22a4 j + \u03b7 \u22a4Xij (4)\n= Tr(WZ\u22a4j Zi) + \u03b7 \u22a4Xij ,\nwhere W is a real-valued matrix and the entry Wkk\u2032 is the weight that affects the link from entity i to entity j if entity i has feature k and entity j has feature k\u2032. For finite sized matrices Z with K columns, we can define the prior as a Beta-Bernoulli process (Meeds et al., 2007). In the infinite case, where Z has an infinite number of columns, we adopt the Indian buffet process (IBP) prior over the unbounded binary matrices as described in (Griffiths & Ghahramani, 2006).\nLet \u0398 = {W,\u03b7} be all the parameters. To make this model Bayesian, we also treat \u0398 as random, with a prior p0(\u0398). To make prediction, we need to get rid of the uncertainty of latent variables, and we define the effective discriminant function as an expectation\nf(Xij) = Ep(Z,\u0398)[f(Zi, Zj ;Xij ,\u0398)]. (5)\nThen, the prediction rule is Y\u0302ij = signf(Xij). Let I be the set of pairs that have observed links. The hinge loss of the expected prediction rule is\nR\u2113(p(Z,\u0398)) = \u2211\n(i,j)\u2208I h\u2113(Yijf(Xij)), (6)\nLet p0(Z) be the prior on the latent feature matrix. We define the MED latent feature relational model\n(MedLFRM) as solving the problem\nmin p(Z,\u0398)\u2208P\nKL(p(Z,\u0398)\u2225p0(Z,\u0398)) + CR\u2113(p(Z,\u0398)) (7)\nIn graphical models, it is well known that introducing auxiliary variables could simplify the inference by converting marginal dependence into conditional independence. Here, we follow this principle and introduce additional variables for the IBP prior p0(Z). One elegant way to do that is the stick-breaking representation of IBP (Teh et al., 2007). Specifically, let \u03c0k \u2208 (0, 1) be a parameter associated with column k of Z. The parameters \u03c0 are generated by a stick-breaking process, that is, \u03c01 = \u03bd1, and \u03c0k = \u03bdk\u03c0k\u22121 = \u220fk i=1 \u03bdi, where \u03bdi \u223c Beta(\u03b1, 1). Given \u03c0k, each Znk in column k is sampled independently from Bernoulli(\u03c0k). This process results in a decreasing sequence of probabilities \u03c0k, and the probability of seeing feature k decreases exponentially with k on a finite dataset. With this representation, we have the augmented MedLFRM\nmin p(\u03bd,Z,\u0398)\nKL(p(\u03bd, Z,\u0398)\u2225p0(\u03bd, Z,\u0398)) + CR\u2113(p(Z,\u0398)) (8)\nwhere p0(\u03bd, Z,\u0398) = p0(\u03bd)p(Z|\u03bd)p0(\u0398). We make two comments about the above definitions. First, we have adopted the similar method as in (Zhu et al., 2011a;b) to define the discriminant function using the expectation operator, instead of the traditional log-likelihood ratio of a Bayesian generative model with latent variables (Jebara, 2002; Lewis et al., 2006). The linearity of expectation makes our formulation simpler than the one that could be achieved by using a highly nonlinear log-likelihood ratio. Second, although a likelihood model can be defined as in (Zhu et al., 2011a;b) to perform hybrid learning, we have avoided doing that because the sigmoid link likelihood model in Eq. (1) is highly nonlinear and it could make the hybrid problem hard to solve."}, {"heading": "3.2.1. Inference with Truncated Mean-Field", "text": "The above problem has nice properties. For example, R\u2113 is a piece-wise linear functional of p and f is linear of \u0398. While sampling methods could lead to more accurate results, variational methods are usually more efficient and they also have an objective to monitor the convergence behavior. Here, we introduce a simple variational method to explore such properties, which turns out to perform well in practice. Specifically, we make the truncated mean-field assumption\np(\u03bd, Z,\u0398) = p(\u0398)\nK\u220f\nk=1\np(\u03bdk|\u03b3k)( N\u220f\ni=1\np(Zik|\u03c8ik)), (9)\nwhere p(\u03bdk|\u03b3k) = Beta(\u03b3k1, \u03b3k2), p(Zik|\u03c8ik) = Bernoulli(\u03c8ik) and K is a truncation level. Then,\nproblem (8) can be solved using an iterative procedure that alternates between:\nSolving for p(\u0398): by fixing p(\u03bd, Z), the subproblem can be equivalently written in a constrained form\nmin p(\u0398),\u03be\nKL(p(\u0398)\u2225p0(\u0398)) + C \u2211\n(i,j)\u2208I \u03beij (10)\n\u2200(i, j) \u2208 I, s.t. : Yij(Tr(E[W ]Z\u0304ij) + E[\u03b7]\u22a4Xij) \u2265 \u2113\u2212 \u03beij ,\nwhere Z\u0304ij = Ep[Z\u22a4j Zi] is the expected latent features and \u03be = {\u03beij} are slack variables. By Lagrangian duality theory, we have the optimal solution\np(\u0398) \u221d p0(\u0398) exp { \u2211\n(i,j)\u2208I \u03c9ijYij(Tr(W Z\u0304ij) + \u03b7\n\u22a4Xij) } .\nwhere \u03c9 = {\u03c9ij} are Lagrangian multipliers. For the commonly used standard normal prior p0(\u0398), we have the optimal solution\np(\u0398) = p(W )p(\u03b7) = (\u220f\nkk\u2032 N (\u039bkk\u2032 , 1)\n)(\u220f\nd\nN (\u03bad, 1) ) ,\nwhere the means are \u039bkk\u2032 = \u2211\n(i,j)\u2208I \u03c9ijYijE[ZikZjk\u2032 ] and \u03bad = \u2211 (i,j)\u2208I \u03c9ijYijX d ij . The dual problem is\nmax \u03c9\n\u2211\n(i,j)\n\u2113\u03c9ij \u2212 1 2 (\u2225\u039b\u222522 + \u2225\u03ba\u222522)\ns.t. : 0 \u2264 \u03c9ij \u2264 C, \u2200(i, j) \u2208 I.\nEquivalently, the mean parameters \u039b and \u03ba can be directly obtained by solving the primal problem\nmin \u039b,\u03ba,\u03be\n1 2 (\u2225\u039b\u222522 + \u2225\u03ba\u222522) + C\n\u2211\n(i,j)\u2208I \u03beij (11)\n\u2200(i, j) \u2208 I, s.t. : Yij(Tr(\u039bZ\u0304ij) + \u03ba\u22a4Xij) \u2265 \u2113\u2212 \u03beij ,\nwhich is a binary classification SVM. We can solve it with any existing high-performance solvers, such as SVMLight or LibSVM.\nSolving for p(\u03bd, Z): by fixing p(\u0398),the subproblem is\nmin p(\u03bd,Z)\nKL(p(\u03bd, Z)\u2225p0(\u03bd, Z)) + CR\u2113(p(Z,\u0398)).\nWith the truncated mean-field assumption, we have\nTr(\u039bZ\u0304ij) =\n{ \u03c8i\u039b\u03c8 \u22a4 j if i \u0338= j\n\u03c8i\u039b\u03c8 \u22a4 i + \u2211 k \u039bkk\u03c8ik(1\u2212 \u03c8ik) if i = j\nWe defer the evaluation of the KL-divergence to Appendix A. For p(\u03bd), since the margin constraints are not dependent on \u03bd, we can get the same solutions as in (Doshi-Velez et al., 2009).\nWe can solve for p(Z) using sub-gradient methods. Let\nIi = {j : j \u0338= i, (i, j) \u2208 I and Yijf(Xij) \u2264 \u2113} I\u2032i = {j : j \u0338= i, (j, i) \u2208 I and Yjif(Xji) \u2264 \u2113}.\nDue to the fact that \u2202xh\u2113(g(x)) equals to \u2212\u2202xg(x) if g(x) \u2264 \u2113; 0 otherwise, we have the subgradient \u2202\u03c8ikR\u2113 = \u2212 \u2211\nj\u2208Ii Yij\u039bk\u00b7\u03c8\n\u22a4 j \u2212\n\u2211\nj\u2208I\u2032i\nYji\u03c8j\u039b\u00b7k\n\u2212I(Yiif(Xii) \u2264 \u2113)Yii(\u039bkk(1\u2212 \u03c8ik) + \u039bk\u00b7\u03c8\u22a4i ),\nwhere \u039bk\u00b7 (\u039b\u00b7k) denotes the kth row (column) of \u039b, and I(\u00b7) is an indicator function. Let the subgradient equal to 0, and we get the update equation\n\u03c8ik = \u03a6 ( k\u2211\nj=1\nEp[log \u03bdj ]\u2212 L\u03bdk + C\u2202\u03c8ikR\u2113 ) . (12)\nwhere L\u03bdk is a lower bound of Ep[log(1\u2212 \u220fk\nj=1 \u03bdj)] (See Appendix A)."}, {"heading": "3.3. The Fully-Bayesian Model", "text": "MedLFRM has one regularization parameter C, which normally plays an important role in large-margin classifiers, especially on sparse and imbalanced datasets. To search a good value of C, cross-validation is a typical approach, but it could be computationally expensive by comparing many candidates. Under the probabilistic formulation, we could provide an alternative way to control model complexity automatically, at least in part. Below, we present a fully-Bayesian MedLFRM model by introducing appropriate priors for the hyper-parameters.\nNormal-Gamma Prior: For simplicity, we assume that the prior is an isotropic normal distribution1 with common mean \u00b5 and precision \u03c4\np0(\u0398|\u00b5, \u03c4) = \u220f\nkk\u2032 N (\u00b5, \u03c4\u22121)\n\u220f\nd\nN (\u00b5, \u03c4\u22121). (13)\nTo complete the model, we use a Normal-Gamma hyper-prior for \u00b5 and \u03c4 :\np0(\u00b5|\u03c4) = N (\u00b50, (n0\u03c4)\u22121), p0(\u03c4) = G(\u03bd0 2 , 2 S0 ), (14)\nwhere G is the Gamma distribution, \u00b50 is the prior mean, \u03bd0 is the prior degrees of freedom, n0 is the prior sample size, S0 is the prior sum of squared errors. We denote this Normal-Gamma distribution by NG(\u00b50, n0, \u03bd02 , 2S0 ). We note that the normal-Gamma prior has been used in a marginalized form as a heavy-tailed prior for deriving sparse estimates (Griffin & Brown, 2010). Here, we use it for automatically inferring the regularization constants, which replace the role of C in problem (8). Also, our Bayesian approach is different from\n1A more flexible prior will be the one that uses different means and variances for different components of \u0398. We leave this extension for future work.\nthe previous methods that were developed for estimating the hyper-parameters of SVM, by optimizing a logevidence (Gold et al., 2005) or an estimate of the generalization error (Chapelle et al., 2002).\nFormally, with the above hierarchical prior, we define Bayesian MedLFRM (BayesMedLFRM) as solving\nmin p(\u03bd,Z,\u00b5,\u03c4,\u0398)\n{ KL(p(\u03bd, Z, \u00b5, \u03c4,\u0398)\u2225p0(\u03bd, Z, \u00b5, \u03c4,\u0398))\n+R\u2113(p(Z,\u0398))\n}\nwhere p0(\u03bd, Z, \u00b5, \u03c4,\u0398) = p0(\u03bd, Z)p0(\u00b5, \u03c4)p0(\u0398|\u00b5, \u03c4). For this problem, we can develop a similar iterative algorithm as for MedLFRM. Specifically, the sub-step of inferring p(\u03bd, Z) does not change. For p(\u00b5, \u03c4,\u0398), the sub-problem (in equivalent constrained form) is\nmin p(\u00b5,\u03c4,\u0398),\u03be\nKL(p(\u00b5, \u03c4,\u0398)\u2225p0(\u00b5, \u03c4,\u0398)) + \u2211\n(i,j)\u2208I \u03beij\n\u2200(i, j) \u2208 I, s.t. :Yij(Tr(E[W ]Z\u0304ij) + E[\u03b7]\u22a4Xij) \u2265 \u2113\u2212 \u03beij ,\nwhich is convex but intractable to solve directly. Here, we make the mild mean-field assumption that p(\u00b5, \u03c4,\u0398) = p(\u00b5, \u03c4)p(\u0398). Then, we iteratively solve for p(\u0398) and p(\u00b5, \u03c4), as summarized below. We defer the details to Appendix B.\nFor p(\u0398), we have the mean-field update equation\np(Wkk\u2032) = N (\u039bkk\u2032 , \u03bb\u22121), p(\u03b7d) = N (\u03bad, \u03bb\u22121), (15)\nwhere \u039bkk\u2032 = E[\u00b5] + \u03bb\u22121 \u2211\n(i,j)\u2208I \u03c9ijYijE[ZikZjk\u2032 ], \u03bad = E[\u00b5] + \u03bb\u22121 \u2211 (i,j)\u2208I \u03c9ijYijX d ij , and \u03bb = E[\u03c4 ]. Similar as in MedLFRM, the mean of \u0398 can be obtained by solving the following problem\nmin \u039b,\u03ba,\u03be\n\u03bb 2 (\u2225\u039b\u2212 E[\u00b5]E\u222522 + \u2225\u03ba\u2212 E[\u00b5]e\u222522) +\n\u2211\n(i,j)\u2208I \u03beij\ns.t. :Yij(Tr(\u039bZ\u0304ij) + \u03ba \u22a4Xij) \u2265 \u2113\u2212 \u03beij , \u2200(i, j) \u2208 I,\nwhere e is a K \u00d7 1 vector with all entries being the unit 1 and E = ee\u22a4 is a K \u00d7 K matrix. Let \u039b\u2032 = \u039b \u2212 E[\u00b5]E and \u03ba\u2032 = \u03ba \u2212 E[\u00b5]e, we have the transformed problem\nmin \u039b\u2032,\u03ba\u2032,\u03be\n\u03bb 2 (\u2225\u039b\u2032\u222522 + \u2225\u03ba\u2032\u222522) +\n\u2211\n(i,j)\u2208I \u03beij (16)\n\u2200(i, j) \u2208 I, s.t. :Yij(Tr(\u039b\u2032Z\u0304ij) + (\u03ba\u2032)\u22a4Xij) \u2265 \u2113ij \u2212 \u03beij\nwhere \u2113ij = \u2113\u2212E[\u00b5]Yij(Tr(EZ\u0304ij)+e\u22a4Xij) is the adaptive cost. The problem can be solved using an existing binary SVM solver with slight changes to consider the sample-varying costs. Comparing with problem (11), we can see that BayesMedLFRM automatically infers the regularization constant \u03bb (or equivalently C), by iteratively updating the posterior distribution p(\u03c4), as explained below.\nThe mean-field update equation for p(\u00b5, \u03c4) is\np(\u00b5, \u03c4) = NG(\u00b5\u0303, n\u0303, \u03bd\u0303, S\u0303), (17)\nwhere \u00b5\u0303= K 2\u039b\u0304+D\u03ba\u0304+n0\u00b50 K2+D+n0 , n\u0303=n0+K 2+D, \u03bd\u0303=\u03bd0+K 2+D,\nS\u0303 = E[SW ] + E[S\u03b7] + S0 + n0(K 2(\u039b\u0304\u2212 \u00b5)2 +D(\u03ba\u0304\u2212 \u00b5)2) K2 +D + n0 ,\nand SW = \u2225W \u2212 W\u0304E\u222522, S\u03b7 = \u2225\u03b7 \u2212 \u03b7\u0304e\u222522. From p(\u00b5, \u03c4), we can compute the expectation and variance, which are needed in updating p(\u0398)\nE[\u00b5] = \u00b5\u0303, E[\u03c4 ] = \u03bd\u0303 S\u0303 , and Var(\u00b5) =\nS\u0303\nn\u0303(\u03bd\u0303 \u2212 2) ."}, {"heading": "4. Experiments", "text": "Now, we provide empirical studies on several real datasets to demonstrate the effectiveness of the maxmargin principle in learning latent feature relational models, as well as the effectiveness of fully-Bayesian methods in avoiding tuning the hyper-parameter C."}, {"heading": "4.1. Multi-relational Datasets", "text": "We report the results of MedLFRM and BayesMedLFRM on the two benchmark datasets which were used in (Miller et al., 2009) to evaluate the performance of latent feature relational models. One dataset contains 54 relations of 14 countries along with 90 given features of the countries, and the other one contains 26 kinship relationships of 104 people in the Alyawarra tribe in Central Australia. On average, there is a probability of about 0.21 that a link exists for each relation on the countries dataset, and the probability of a link is about 0.04 for the kinship dataset. So, the kinship dataset is extremely imbalanced (i.e., much more negative examples than positive examples). To deal with this imbalance in learning max-margin MedLFRM, we use different regularization constants for the positive (C+) and negative (C\u2212) examples. We refer the readers to (Akbani et al., 2004) for other possible choices. In our experiments, we set C+ = 10C\u2212 = 10C for simplicity and tune the parameter C. For BayesMedLFRM, this equality is held during all iterations.\nDepending on the input data, the latent features might not have interpretable meanings (Miller et al., 2009). In the experiments, we focus on the effectiveness of max-margin learning in learning latent feature relational models. We also compare with two wellestablished class-based algorithms \u2013 IRM (i.e., infinite relational model) (Kemp et al., 2006) and MMSB (i.e., mixed membership stochastic block) (Airoldi et al., 2008), both of which were tested in (Miller et al., 2009). In order to compare with their reported results,\nTable 1. AUC on the countries and kinship datasets. Bold indicates the best performance.\nCountries single Countries global Alyawarra single Alyawarra global\nSVM 0.8180 \u00b1 0.0000 0.8180 \u00b1 0.0000 \u2014 \u2014 LR 0.8139 \u00b1 0.0000 0.8139 \u00b1 0.0000 \u2014 \u2014\nMMSB 0.8212 \u00b1 0.0032 0.8643 \u00b1 0.0077 0.9005 \u00b1 0.0022 0.9143 \u00b1 0.0097 IRM 0.8423 \u00b1 0.0034 0.8500 \u00b1 0.0033 0.9310 \u00b1 0.0023 0.8943 \u00b1 0.3000 LFRM rand 0.8529 \u00b1 0.0037 0.7067 \u00b1 0.0534 0.9443 \u00b1 0.0018 0.7127 \u00b1 0.0300 LFRM w/ IRM 0.8521 \u00b1 0.0035 0.8772 \u00b1 0.0075 0.9346 \u00b1 0.0013 0.9183 \u00b1 0.0108 MedLFRM 0.9173 \u00b1 0.0067 0.9255 \u00b1 0.0076 0.9552 \u00b1 0.0065 0.9616 \u00b1 0.0045\nBayesMedLFRM 0.9178 \u00b1 0.0045 0.9260 \u00b1 0.0023 0.9547 \u00b1 0.0028 0.9600 \u00b1 0.0016\nwe use the same setup for the experiments. Specifically, for each dataset, we held out 20% of the data during training and report the AUC (i.e., area under the Receiver Operating Characteristic or ROC curve) for the held out data. As in (Miller et al., 2009), we consider two settings \u2013 \u201cglobal\u201d and \u201csingle\u201d. For the global setting, we infer a single set of latent features for all relations; and for the single setting, we infer independent latent features for each relation and the overall AUC is an average of the AUC scores of all relations.\nFor MedLFRM and BayesMedLFRM, we randomly initialize the posterior mean of W uniformly in the interval [0, 0.1]; initialize \u03c8 to uniform (i.e., 0.5) corrupted by a random noise distributed uniformly at the interval [0, 0.001]; and initialize the mean of \u03b7 to be zero. All the following results of MedLFRM and BayesMedLFRM are averages over 5 randomly initialized runs, again similar as in (Miller et al., 2009). For MedLFRM, the hyper-parameter C is selected via cross-validation during training. For BayesMedLFRM, we use a very weak hyper-prior by setting \u00b50 = 0, n0 = 1, \u03bd0 = 2, and S0 = 1. We set the cost parameter \u2113 = 9 in all experiments.\nTable 1 shows the results. We can see that in both settings and on both datasets, the max-margin based latent feature relational model MedLFRM significantly outperforms LFRM that uses a likelihoodbased approach with MCMC sampling. Comparing BayesMedLFRM and MedLFRM, we can see that using the fully-Bayesian technique with a simple NormalGamma hierarchical prior, we can avoid tuning the regularization constant C, without sacrificing the link prediction performance. To see the effectiveness of latent feature models, we also report the performance of logistic regression (LR) and linear SVM on the countries dataset, which has input features. We can see that a latent feature or latent class model generally outperforms the methods that are built on raw input features for this particular dataset.\nFigure 1 shows the performance of MedLFRM on the countries dataset when using and not using input fea-\n10 20 30 40 50 0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nTruncation Level\nA U\nC\nMedLFRM with feature MedLFRM without features\nFigure 1. AUC scores of MedLFRM with and without input features on the countries dataset.\ntures. We consider the global setting. Here, we also study the effects of truncation level K. We can see that in general using input features can boost the performance. Moreover, even if using latent features only, MedLFRM can still achieve very competitive performance, better than the performance of the likelihoodbased LFRM that uses both latent features and input features. Finally, it is sufficient to get good performance by setting the truncation level K to be larger than 40. We set K to be 50 in the experiments."}, {"heading": "4.2. Predicting NIPS coauthorship", "text": "The second experiments are done on the coauthorship data constructed from the NIPS dataset which contains a list of all papers and authors from NIPS 1- 17. To compare with LFRM, we use the same dataset as in (Miller et al., 2009), which contains 234 authors who had published with the most other people2. To better fit the symmetric coauthor link data, we restrict our models to be symmetric, i.e., the posterior mean ofW is a symmetric matrix, as in (Miller et al., 2009). For MedLFRM and BayesMedLFRM, this symmetry constraint can be easily satisfied when solving the SVM problems (11) and (16). To see the effects of the\n2The average probability of forming a link on this data is about 0.02, again very imbalanced. We tried the same strategy as for the kinship dataset by using different regularization constants. The results are not significantly different from those by using a common C. K = 80 is sufficient for these experiments.\nsymmetry constraint, we also report the results of the asymmetric MedLFRM and asymmetric BayesMedLFRM, which do not impose the symmetry constraint on the posterior mean of W . As in (Miller et al., 2009), we train the model on 80% of the data and use the remaining data for test.\nTable 2 shows the results, where the results of LFRM, IRM and MMSB were reported in (Miller et al., 2009). Again, we can see that using the discriminative max-margin training, the symmetric MedLFRM and BayesMedLFRM outperform all other likelihoodbased methods, using either latent feature or latent class models; and the fully-Bayesian MedLFRM model performs comparably with MedLFRM while avoiding tuning the hyper-parameter C. Finally, the asymmetric MedLFRM and BayesMedLFRM models perform much worse than their symmetric counterpart models, but still better than the latent class models."}, {"heading": "4.3. Stability and Running Time", "text": "Figure 2 shows the change of the objective function as well as the change of the AUC scores on test data of the countries dataset during the iterations for both MedLFRM and BayesMedLFRM. For MedLFRM, we report the results with the best C selected via cross-validation. We can see that the variational inference algorithms for both models converge quickly to a particular region. Since we use sub-gradient descent to update the distribution of Z and the subproblems of solving for p(\u0398) can in practice only be approximately solved, the objective function has some disturbance, but within a relatively very small interval. For the AUC scores, we have similar observations, namely, within several iterations, we could have very good link prediction performance. The disturbance is again maintained within a small region, which is reasonable for our approximate inference algorithms. Comparing the two models, we can see that BayesMedLFRM has similar behaviors as MedLFRM, which demonstrates the effectiveness of using fully-Bayesian techniques to automatically learn the hyper-parameter C. Figure 3 presents the results on the kinship dataset, from which we have the same observations. We omit the results on the NIPS dataset for saving space.\nFinally, Figure 4 shows the training time and test time of MedLFRM and Bayesian MedLFRM on each of the three datasets. For MedLFRM, we show the single run with the optimum parameter C, selected via inner cross-validation. We can see that using Bayesian infer-\nence, the running time does not increase much, being generally comparable with that of MedLFRM. But since MedLFRM needs to select the hyper-parameter C, it will need much more time than BayesMedLFRM to finish the entire training on a single dataset."}, {"heading": "5. Conclusions and Future Work", "text": "We have presented a discriminative max-margin latent feature relational model for link prediction. Under a Bayesian-style max-margin formulation, our work naturally integrates the ideas of Bayesian nonparametrics to automatically resolve the unknown dimensionality of a latent social space. Furthermore, we present a fully-Bayesian formulation, which can avoid tuning regularization constants. We developed efficient variational methods to perform posterior inference. Empirical results on several real datasets appear to demonstrate the benefits inherited from both max-margin learning and fully-Bayesian methods.\nOur current analysis is focusing on small static network snapshots. For future work, we are interested in learning more flexible latent feature relational models to deal with large dynamic networks and reveal more subtle network evolution patterns. We are also interested in developing Monte Carlo sampling methods, which have been widely used in previous latent feature relational models."}, {"heading": "Acknowledgements", "text": "JZ is supported by National Key Foundation R&D Projects 2012CB316301, a Starting Research Fund No. 553420003, the 221 Basic Research Plan for Young Faculties at Tsinghua University, and a Research Fund No. 20123000007 from Microsoft Research Asia."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["E. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "In NIPS, pp", "citeRegEx": "Airoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2008}, {"title": "Supervised random walks: predicting and recommending links in social networks", "author": ["L. Backstrom", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "Backstrom and Leskovec,? \\Q2011\\E", "shortCiteRegEx": "Backstrom and Leskovec", "year": 2011}, {"title": "Choosing multiple parameters for support vector machines", "author": ["O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee"], "venue": "Machine Learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi-Velez", "K. Miller", "Gael", "J. Van", "Y.W. Teh"], "venue": "In AISTATS,", "citeRegEx": "Doshi.Velez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2009}, {"title": "Bayesian approach to feature selection and parameter tuning for support vector machine classifiers", "author": ["C. Gold", "A. Holub", "P. Sollich"], "venue": "Neural Networks,", "citeRegEx": "Gold et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gold et al\\.", "year": 2005}, {"title": "Inference with normalgamma prior distributions in regression problems", "author": ["J.E. Griffin", "P.J. Brown"], "venue": "Bayesian Analysis,", "citeRegEx": "Griffin and Brown,? \\Q2010\\E", "shortCiteRegEx": "Griffin and Brown", "year": 2010}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "Latent space approaches to social network analysis", "author": ["P. Hoff", "A. Raftery", "M. Handcock"], "venue": "Journal of American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "Modeling homophily and stochastic equivalence in symmetric relational data", "author": ["P.D. Hoff"], "venue": "In NIPS,", "citeRegEx": "Hoff,? \\Q2007\\E", "shortCiteRegEx": "Hoff", "year": 2007}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "In NIPS,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Discriminative, generative and imitative learning", "author": ["T. Jebara"], "venue": "PhD Thesis,", "citeRegEx": "Jebara,? \\Q2002\\E", "shortCiteRegEx": "Jebara", "year": 2002}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffithms", "T. Yamada", "N. Ueda"], "venue": "In AAAI,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Nonstationary kernel combination", "author": ["D.P. Lewis", "T. Jebara", "W.S. Noble"], "venue": "In ICML,", "citeRegEx": "Lewis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2006}, {"title": "The link prediction problem for social networks", "author": ["D. Liben-Nowell", "J.M. Kleinberg"], "venue": "In CIKM,", "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2003\\E", "shortCiteRegEx": "Liben.Nowell and Kleinberg", "year": 2003}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R. Neal", "S. Roweis"], "venue": "In NIPS,", "citeRegEx": "Meeds et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meeds et al\\.", "year": 2007}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K. Miller", "T. Griffiths", "M. Jordan"], "venue": "In NIPS,", "citeRegEx": "Miller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["K. Nowicki", "T.A.B. Snijders"], "venue": "Journal of American Statistical Association,", "citeRegEx": "Nowicki and Snijders,? \\Q2001\\E", "shortCiteRegEx": "Nowicki and Snijders", "year": 2001}, {"title": "Stick-breaking construction of the Indian buffet process", "author": ["Y.W. Teh", "D. Gorur", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Maximum entropy discrimination markov networks", "author": ["J. Zhu", "E.P. Xing"], "venue": "JMLR, 10:2531\u20132569,", "citeRegEx": "Zhu and Xing,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Xing", "year": 2009}, {"title": "MedLDA: Maximum margin supervised topic models for regression and classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}, {"title": "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}, {"title": "Infinite latent SVM for classification and multi-task learning", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In NIPS,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "Often there is extra information about links and entities such as attributes and timestamps (Liben-Nowell & Kleinberg, 2003; Backstrom & Leskovec, 2011; Miller et al., 2009) that can be used to help with prediction.", "startOffset": 92, "endOffset": 173}, {"referenceID": 8, "context": "moid function) (Hoff, 2007; Miller et al., 2009) to define the link formation probability distribution.", "startOffset": 15, "endOffset": 48}, {"referenceID": 15, "context": "moid function) (Hoff, 2007; Miller et al., 2009) to define the link formation probability distribution.", "startOffset": 15, "endOffset": 48}, {"referenceID": 0, "context": "These latent feature models were shown to generalize latent class (Nowicki & Snijders, 2001; Airoldi et al., 2008) and latent distance (Hoff et al.", "startOffset": 66, "endOffset": 114}, {"referenceID": 7, "context": ", 2008) and latent distance (Hoff et al., 2002) models and are thus able to represent both homophily and stochastic equivalence, which are important properties commonly observed in real-world social network and relational data.", "startOffset": 28, "endOffset": 47}, {"referenceID": 15, "context": "The work (Miller et al., 2009) is an exception, which presents a nonparametric Bayesian method to automatically infer the unknown social dimension.", "startOffset": 9, "endOffset": 30}, {"referenceID": 9, "context": ", hinge-loss) that measures the quality of link prediction, under the principle of maximum entropy discrimination (MED) (Jaakkola et al., 1999; Jebara, 2002), which was introduced as an elegant framework to integrate max-margin learning and Bayesian generative modeling.", "startOffset": 120, "endOffset": 157}, {"referenceID": 10, "context": ", hinge-loss) that measures the quality of link prediction, under the principle of maximum entropy discrimination (MED) (Jaakkola et al., 1999; Jebara, 2002), which was introduced as an elegant framework to integrate max-margin learning and Bayesian generative modeling.", "startOffset": 120, "endOffset": 157}, {"referenceID": 15, "context": "First, like (Miller et al., 2009), we use nonparametric Bayesian techniques to automatically resolve the unknown dimension of a latent social space, and thus our work represents an attempt towards uniting Bayesian nonparametrics and max-margin learning, which have been largely treated as two isolated topics.", "startOffset": 12, "endOffset": 33}, {"referenceID": 7, "context": "For the latent distance model (Hoff et al., 2002), we have", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": "For the latent eigenmodel (Hoff, 2007), which generalizes the latent distance model and the latent class model for modeling symmetric relational data, we have", "startOffset": 26, "endOffset": 38}, {"referenceID": 15, "context": "The nonparametric latent feature relational model (LFRM) (Miller et al., 2009) leverages the recent advances in Bayesian nonparametrics to automatically infer the latent dimension.", "startOffset": 57, "endOffset": 78}, {"referenceID": 15, "context": "For modeling symmetric relational data, we usually constrain W to be symmetric (Miller et al., 2009).", "startOffset": 79, "endOffset": 100}, {"referenceID": 9, "context": "We first briefly review the basic concepts of MED (Jaakkola et al., 1999; Jebara, 2002).", "startOffset": 50, "endOffset": 87}, {"referenceID": 10, "context": "We first briefly review the basic concepts of MED (Jaakkola et al., 1999; Jebara, 2002).", "startOffset": 50, "endOffset": 87}, {"referenceID": 10, "context": "MED subsumes SVM as a special case and has been extended to incorporate latent variables (Jebara, 2002; Zhu et al., 2009) and to perform structured output prediction (Zhu & Xing, 2009).", "startOffset": 89, "endOffset": 121}, {"referenceID": 19, "context": "MED subsumes SVM as a special case and has been extended to incorporate latent variables (Jebara, 2002; Zhu et al., 2009) and to perform structured output prediction (Zhu & Xing, 2009).", "startOffset": 89, "endOffset": 121}, {"referenceID": 14, "context": "For finite sized matrices Z with K columns, we can define the prior as a Beta-Bernoulli process (Meeds et al., 2007).", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "One elegant way to do that is the stick-breaking representation of IBP (Teh et al., 2007).", "startOffset": 71, "endOffset": 89}, {"referenceID": 10, "context": ", 2011a;b) to define the discriminant function using the expectation operator, instead of the traditional log-likelihood ratio of a Bayesian generative model with latent variables (Jebara, 2002; Lewis et al., 2006).", "startOffset": 180, "endOffset": 214}, {"referenceID": 12, "context": ", 2011a;b) to define the discriminant function using the expectation operator, instead of the traditional log-likelihood ratio of a Bayesian generative model with latent variables (Jebara, 2002; Lewis et al., 2006).", "startOffset": 180, "endOffset": 214}, {"referenceID": 3, "context": "For p(\u03bd), since the margin constraints are not dependent on \u03bd, we can get the same solutions as in (Doshi-Velez et al., 2009).", "startOffset": 99, "endOffset": 125}, {"referenceID": 4, "context": "the previous methods that were developed for estimating the hyper-parameters of SVM, by optimizing a logevidence (Gold et al., 2005) or an estimate of the generalization error (Chapelle et al.", "startOffset": 113, "endOffset": 132}, {"referenceID": 2, "context": ", 2005) or an estimate of the generalization error (Chapelle et al., 2002).", "startOffset": 51, "endOffset": 74}, {"referenceID": 15, "context": "We report the results of MedLFRM and BayesMedLFRM on the two benchmark datasets which were used in (Miller et al., 2009) to evaluate the performance of latent feature relational models.", "startOffset": 99, "endOffset": 120}, {"referenceID": 15, "context": "Depending on the input data, the latent features might not have interpretable meanings (Miller et al., 2009).", "startOffset": 87, "endOffset": 108}, {"referenceID": 11, "context": ", infinite relational model) (Kemp et al., 2006) and MMSB (i.", "startOffset": 29, "endOffset": 48}, {"referenceID": 0, "context": ", mixed membership stochastic block) (Airoldi et al., 2008), both of which were tested in (Miller et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 15, "context": ", 2008), both of which were tested in (Miller et al., 2009).", "startOffset": 38, "endOffset": 59}, {"referenceID": 15, "context": "As in (Miller et al., 2009), we consider two settings \u2013 \u201cglobal\u201d and \u201csingle\u201d.", "startOffset": 6, "endOffset": 27}, {"referenceID": 15, "context": "All the following results of MedLFRM and BayesMedLFRM are averages over 5 randomly initialized runs, again similar as in (Miller et al., 2009).", "startOffset": 121, "endOffset": 142}, {"referenceID": 15, "context": "To compare with LFRM, we use the same dataset as in (Miller et al., 2009), which contains 234 authors who had published with the most other people.", "startOffset": 52, "endOffset": 73}, {"referenceID": 15, "context": ", the posterior mean ofW is a symmetric matrix, as in (Miller et al., 2009).", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": "As in (Miller et al., 2009), we train the model on 80% of the data and use the remaining data for test.", "startOffset": 6, "endOffset": 27}, {"referenceID": 15, "context": "Table 2 shows the results, where the results of LFRM, IRM and MMSB were reported in (Miller et al., 2009).", "startOffset": 84, "endOffset": 105}], "year": 2012, "abstractText": "We present a max-margin nonparametric latent feature relational model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.", "creator": "TeX"}}}