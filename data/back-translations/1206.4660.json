{"id": "1206.4660", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Learning with Augmented Features for Heterogeneous Domain Adaptation", "abstract": "We propose a new learning method for the adaptation of heterogeneous domains (HDA), where the data from the source domain and the target domain are represented by heterogeneous characteristics with different dimensions. We first transform the data from two domains into a common subspace using two different projection matrices to measure the similarity between the data from two domains. We then propose two new feature mapping functions to supplement the transformed data with their original characteristics and zeros. Existing learning methods (e.g. SVM and SVR) can be easily integrated into our newly proposed enhanced feature representations to effectively use the data from both domains for HDA. Using the example of the hinge loss function in SVM, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear Case and also describe its core licensing to handle the data with very high dimensions.", "histories": [["v1", "Mon, 18 Jun 2012 15:28:12 GMT  (146kb)", "http://arxiv.org/abs/1206.4660v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lixin duan", "dong xu", "ivor w tsang"], "accepted": true, "id": "1206.4660"}, "pdf": {"name": "1206.4660.pdf", "metadata": {"source": "CRF", "title": "Learning with Augmented Features for Heterogeneous Domain Adaptation", "authors": ["Lixin Duan", "Dong Xu", "Ivor W. Tsang"], "emails": ["S080003@ntu.edu.sg", "DongXu@ntu.edu.sg", "IvorTsang@ntu.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "In real-world applications, it is often expensive and time-consuming to collect the labeled data. Transfer\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nlearning (a.k.a., domain adaptation), as a new machine learning strategy, has attracted growing attention because it can learn robust classifiers with very few labeled data from the target domain by leveraging a large amount of labeled data from other existing domains (a.k.a., source domains).\nDomain adaptation methods have been successfully used for different research fields such as natural language processing and computer vision (Blitzer et al., 2006; 2007; Daume\u0301 III, 2007; Duan et al., 2010; 2012b;a; Wu & Dietterich, 2004). However, all those methods assume that the data from different domains are represented by the same type of features with the same dimension. Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) (Dai et al., 2009; Yang et al., 2009).\nIn the literature, a few works have been proposed for the HDA problem. Dai et al. (2009) proposed to learn a feature translator between the source and target domains by assuming that the data from both domains share co-occurrence attributes (i.e., text data). The same assumption was also used in (Yang et al., 2009; Zhu et al., 2011) for text-aid image clustering and classification. However, this assumption may not be well satisfied in many applications such as the object recognition task where only visual features are used. Based on structural correspondence learning (Blitzer et al., 2006), two methods (Prettenhofer & Stein, 2010; Wei & Pal, 2010) were recently proposed to extract the so-called pivot features from the source and target domains, which is specifically designed for the cross-language text classification task. And these pivot features are constructed by text words which have explicit semantic meanings.\nFor more general HDA tasks, Shi et al. (2010) proposed a method called Heterogeneous Spectral Mapping\n(HeMap) to discover a common feature subspace by learning two feature mapping matrices as well as the optimal projection of the data from both domains, in which the valuable label information is not exploited. Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valuable training labels, either. Wang et al. (2011) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intradomain similarity and the inter-domain dissimilarity. By kernelizing the method in (Saenko et al., 2010), Kulis et al. (2011) proposed to learn an asymmetric kernel transformation to transfer feature knowledge between the data from the source and target domains.\nIn this work, we propose a new method called Heterogeneous Feature Augmentation (HFA) for heterogeneous domain adaptation. Considering the data from different domains are represented by features with different dimensions, we first transform the data from the source and target domains into a common subspace by using two different projection matrices P and Q. Then, we propose two new feature mapping functions to augment the transformed data with their original features and zeros. With the new augmented feature representations, we propose to learn the projection matrices P andQ by using the standard SVM with the hinge loss function in a linear case. We also describe its kernelization in order to efficiently cope with the data with very high dimension. To simplify the nontrivial optimization problem in HFA, we introduce an intermediate variable H called as a transformation metric to combine P and Q. We then develop an alternating optimization algorithm to simultaneously solve for the dual problem of SVM and the optimal transformation metric H.\nWe summarize the main contributions of this work:\n\u2022 The newly proposed augmented features in our HFA method can be readily incorporated into different methods (e.g., SVM and SVR) to effectively utilize the patterns from two domains, making them applicable to the HDA task.\n\u2022 We simplify the nontrivial optimization problem by defining a transformation metric H and develop an effective alternating optimization algorithm. With the introduction of H, we do not explicitly solve for P and Q, which makes the common subspace invisible to us.\n\u2022 Promising results on two benchmark datasets clearly demonstrate the effectiveness of HFA for object recognition and text categorization."}, {"heading": "2. Kernel Learning for Heterogeneous Domain Adaptation", "text": "In the remainder of this paper, we use the superscript \u2032 to denote the transpose of a vector or a matrix. We define In as the n\u00d7 n identity matrix and On\u00d7m as a n\u00d7m matrix of all zeros. We also define 0n,1n \u2208 Rn as the n \u00d7 1 column vectors of all zeros and all ones, respectively. The inequality a \u2264 b means that ai \u2264 bi for i = 1, . . . , n. Moreover, a \u25e6 b denotes the elementwise product between vectors a and b, i.e., a \u25e6 b = [a1b1, . . . , anbn]\n\u2032. And H \u227d 0 means that the matrix H is positive semidefinite.\nIn this work, we assume there are only one source domain and one target domain. For some given class, we are provided with a set of labeled training samples { (xsi , ysi )| ns i=1} from the source domain as well as a limited number of labeled samples { (xti, yti)| nt i=1} from the target domain, where ysi and y t i are the labels of the samples xsi and x t i, respectively, and y s i , y t i \u2208 {1,\u22121}. The dimensions of xsi and xti are ds and dt, respectively. Note that in the HDA problem, ds \u0338= dt."}, {"heading": "2.1. Heterogeneous Feature Augmentation", "text": "Daume III (2007) proposed Feature Replication (FR) to augment the original feature space Rd into a larger space R3d by replicating the source and target data for homogeneous domain adaptation. Specifically, for any data point x \u2208 Rd, the feature mapping functions \u03c6s and \u03c6t for the source and target domains are defined as \u03c6s(x) = [x \u2032,x\u2032,0d] \u2032 and \u03c6t(x) = [x \u2032,0d,x \u2032]\u2032. Note that it is not meaningful to directly use the method in (Daume\u0301 III, 2007) for the HDA task by simply padding zeros to make the dimensions of the data from two domains become the same, because there would be no correspondences between the heterogeneous features in this case.\nTo effectively utilize the heterogeneous features from two domains, we first introduce a common subspace for the source and target data for our HDA task, in which the heterogeneous features from two domains can be compared. We define the common subspace as Rdc , where any source sample xs and target sample xt can be projected onto it by using two projection matrices P \u2208 Rdc\u00d7ds and Q \u2208 Rdc\u00d7dt , respectively. Note that promising results have been shown by incorporating original features into feature augmentation (Daume\u0301 III, 2007; Pan et al., 2010) to enhance the similarities between data from the same domain. Motivated by (Daume\u0301 III, 2007; Pan et al., 2010), we also incorporate original features in this work and then augment any source and target domain samples xs \u2208 Rds and xt \u2208 Rdt by using our newly proposed\nfeature mapping functions \u03c6s and \u03c6t as follows:\n\u03c6s(x s) = Pxsxs 0dt  and \u03c6t(xt) = Qxt0ds xt  . (1) After introducingP andQ, the data from two domains can be readily compared in the common subspace. It is worth mentioning that our newly proposed augmented features for the source and target samples in (1) can be readily incorporated into different methods (e.g., SVM and SVR), making these methods applicable for the HDA problem.\nIn the next subsection, we take SVM with the hinge loss as a showcase of our Heterogeneous Feature Augmentation method (HFA for short). As it is nontrivial to solve for the projection matrices P and Q in our learning problem, we simplify the optimization problem by introducing an intermediate variable H = [P,Q]\u2032[P,Q] such that we only need to solve for H rather than P and Q. In this way, the common subspace becomes invisible to us, which is therefore referred to as latent common subspace in this work."}, {"heading": "2.2. Proposed Method", "text": "We define feature weight vector w = [w\u2032c,w \u2032 s,w \u2032 t] \u2032 for the augmented feature space, where wc,ws and wt are also weight vectors that are defined for the common subspace, the source domain and the target domain, respectively. We then propose to learn the projection matrices P and Q as well as the weight vector w by minimizing the structural risk functional of SVM. Formally, we present the formulation of our HFA method for the HDA problem as follows:\nmin P,Q min w,b,\u03besi ,\u03be t i\n1 2 \u2225w\u22252 + C ( ns\u2211 i=1 \u03besi + nt\u2211 i=1 \u03beti ) , (2)\ns.t. ysi (w \u2032\u03c6s(x s i ) + b) \u2265 1\u2212 \u03besi , \u03besi \u2265 0; (3)\nyti(w \u2032\u03c6t(x t i) + b) \u2265 1\u2212 \u03beti , \u03beti \u2265 0; (4) \u2225P\u22252F \u2264 \u03bbp, \u2225Q\u22252F \u2264 \u03bbq,\nwhere C > 0 is a regularization parameter that regulates the loss on the training samples, and \u03bbp, \u03bbq > 0 are predefined to control the complexities of P and Q, respectively.\nTo solve (2), we first derive the dual form of the inner optimization problem in (2) with respect to w, b, \u03besi and \u03beti . Specifically, we introduce dual variables {\u03b1si | ns i=1} and {\u03b1ti| nt i=1} for the constraints in (3) and (4), respectively. By setting the derivatives of the Lagrangian of (2) with respect to w, b, \u03besi and \u03be t i to zeros, we obtain the Karush-Kuhn-Tucker (KKT) conditions as: w = \u2211ns i=1 \u03b1 s i y s i\u03c6s(x s i ) + \u2211nt i=1 \u03b1 t iy t i\u03c6t(x\nt i),\u2211ns\ni=1 \u03b1 s i y s i + \u2211nt i=1 \u03b1 t iy t i = 0 and 0 \u2264 \u03b1si , \u03b1ti \u2264 C.\nWith the KKT conditions, we arrive at the alternative optimization problem as follows:\nmin P,Q max \u03b1\n1\u2032ns+nt\u03b1\u2212 1 2 (\u03b1 \u25e6 y)\u2032KP,Q(\u03b1 \u25e6 y), (5)\ns.t. y\u2032\u03b1 = 0, 0ns+nt \u2264 \u03b1 \u2264 C1ns+nt , \u2225P\u22252F \u2264 \u03bbp, \u2225Q\u22252F \u2264 \u03bbq,\nwhere \u03b1 = [\u03b1s1, . . . , \u03b1 s ns , \u03b1 t 1, . . . , \u03b1 t nt ] \u2032 is a vector of the dual variables, y = [ys1, . . . , y s ns , y t 1, . . . , y t nt ] \u2032 is a label vector,\nKP,Q =\n[ X\u2032s(Ins +P \u2032P)Xs X \u2032 sP\n\u2032QXt X\u2032tQ \u2032PXs X \u2032 t(Int +Q \u2032Q)Xt ] is the derived kernel matrix.\nA straightforward solution to the optimization problem in (5) would be to iteratively update one of the variables \u03b1,P and Q by fixing the others. However, the dimension of the common subspace (i.e., dc) must be given beforehand in this case, and it is nontrivial to determine the optimal dc. Observing that in the kernel matrix KP,Q in (5), the projection matrices P and Q always appear in the forms of P\u2032P,P\u2032Q,Q\u2032P and Q\u2032Q, we then replace these multiplications by defining an intermediate variable H = [P,Q]\u2032[P,Q] \u2208 R(ds+dt)\u00d7(ds+dt). Obviously, H is positive semidefinite, i.e., H \u227d 0. With the introduction of H, we can throw away the parameter dc. Moreover, the common subspace becomes latent, because we do not need to explicitly solve for P and Q any more.\nWith the definition of H, we convert the optimization problem in (5) to the final formulation of our proposed HFA method as follows:\nmin H\u227d0 max \u03b1\n1\u2032ns+nt\u03b1\u2212 1 2 (\u03b1 \u25e6 y)\u2032KH(\u03b1 \u25e6 y), (6)\ns.t. y\u2032\u03b1 = 0, 0ns+nt \u2264 \u03b1 \u2264 C1ns+nt , trace(H) \u2264 \u03bb,\nwhere KH =\n[ X\u2032sXs + L \u2032 sHLs L \u2032 sHLt\nL\u2032tHLs X \u2032 tXt + L \u2032 tHLt\n] ,\nLs =\n[ Ids\nOdt\u00d7ds\n] Xs, Lt = [ Ods\u00d7dt Idt ] Xt and \u03bb =\n\u03bbp + \u03bbq. Note that given \u03b1, the optimization problem in (6) becomes the following Semidefinite Programming (SDP) problem (Vandenberghe & Boyd, 1996) by defining \u03b2 = \u03b1 \u25e6 y:\nmin H\u227d0 \u22121 2 \u03b2\u2032KH\u03b2, s.t. trace(H) \u2264 \u03bb. (7)\nThus far, we have successfully converted our original HDA problem, which learns two projection matrices P and Q, into a new problem of learning a transformation metric H. We emphasize that this new problem has two main advantages: i) it avoids determining the optimal dimension of the common subspace beforehand; and ii) as the common subspace\nbecomes latent after the introduction of H, we only have to optimize \u03b1 and H for our proposed method.\nDiscussion: There are two major limitations to the current formulation of HFA in (6): i) The transformation metric H is linear, which may not be effective for some tasks. ii) The size of H grows with the dimensions of the source and target data (i.e., ds and dt). Therefore, it is computationally infeasible to learn the linear metric H in the SDP problem (7) for some real-world applications (e.g., text categorization) with very high dimensional data. In order to effectively deal with high dimensional data, inspired by (Kulis et al., 2011), in the next subsection we will apply kernelization to the data from the source and target domains and show that (7) can be solved in a kernel space by learning a nonlinear transformation metric with its size independent of the feature dimension."}, {"heading": "2.3. Nonlinear Feature Transformation", "text": "Note that the size of the linear transformation metric H is proportional to the feature dimension, and thus it is computationally infeasible for data with a very high dimension. In this section, we will show that by applying kernelization, the transformation metric is independent of the feature dimension and grows only with the number of training data.\nAs any arbitrary feature mapping function \u03d5 can be used to derive a corresponding kernel space for the source and target data, we can replace their linear inner products with some kernel function k. Let us denote \u03a6s = [\u03d5(x s 1), . . . , \u03d5(x s ns)] and \u03a6t = [\u03d5(xt1), . . . , \u03d5(x t nt)] as the matrices of the source and target training data after mapping them into a nonlinear feature space by using \u03d5, respectively. We also define Ks = \u03a6 \u2032 s\u03a6s and Kt = \u03a6 \u2032 t\u03a6t as the kernel matrices of the training data from the source and target domains, respectively. Moreover, we denote the corresponding projection matrices for the source and target data respectively as P\u03d5 and Q\u03d5.\nTheorem 1. Assume Ks and Kt be positive definite. There exist two matrices P\u0303 \u2208 Rdc\u00d7ns and Q\u0303 \u2208 Rdc\u00d7nt such that any feasible solution P\u03d5 and Q\u03d5 to the kernelized version of (2) can be written in the form of P\u03d5 = P\u0303K \u22121/2 s \u03a6\u2032s and Q\u03d5 = Q\u0303K \u22121/2 t \u03a6 \u2032 t, respectively.\nProof. The proof can be analogously derived as for Lemma 3.1 in (Kulis et al., 2011).\nWith Theorem 1, it is easy to verify that \u2225P\u0303\u22252F = \u2225P\u03d5\u22252F \u2264 \u03bbp and \u2225Q\u0303\u22252F = \u2225Q\u03d5\u22252F \u2264 \u03bbq. Here, we apply the same trick as in Section 2.2 to avoid determining dc for the latent common subspace. That is, we define the nonlinear transformation metric H\u0303 = [P\u0303, Q\u0303]\u2032[P\u0303, Q\u0303] \u2208 R(ns+nt)\u00d7(ns+nt), and thus the size of\nAlgorithm 1 Heterogeneous Feature Augmentation Input: Labeled source samples { (xsi , ysi )| ns i=1} and labeled target samples { (xti, yti)| nt i=1} Initialization: \u03c4 \u2190 1, H\u0303[\u03c4 ] \u2190 \u03bbns+nt Ins+nt With H\u0303[\u03c4 ], solve for \u03b1[\u03c4 ] in the inner optimization problem of (8) by using SVM; while \u03c4 < Tmax do\nUpdate H\u0303[\u03c4+1] by using the projected gradient descent method with (10); With H\u0303[\u03c4+1], solve for \u03b1[\u03c4+1] in the inner optimization problem of (8) by using SVM; if the objective value of (8) converges then\nbreak; end \u03c4 \u2190 \u03c4 + 1;\nend\nOutput: H\u0303[\u03c4 ] and \u03b1[\u03c4 ]\nH\u0303 is independent of the feature dimension. We also have H\u0303 \u227d 0 and trace(H\u0303) \u2264 \u03bbp + \u03bbq = \u03bb.\nTherefore, the formulation of our proposed HFA method after applying kernelization becomes:\nmin H\u0303\u227d0 max \u03b1\n1\u2032ns+nt\u03b1\u2212 1 2 (\u03b1 \u25e6 y)\u2032KH\u0303(\u03b1 \u25e6 y), (8)\ns.t. y\u2032\u03b1 = 0, 0ns+nt \u2264 \u03b1 \u2264 C1ns+nt , trace(H\u0303) \u2264 \u03bb,\nwhere KH\u0303 =\n[ Ks + L\u0303 \u2032 sH\u0303L\u0303s L\u0303 \u2032 sH\u0303L\u0303t\nL\u0303\u2032tH\u0303L\u0303s Kt + L\u0303 \u2032 tH\u0303L\u0303t ] , L\u0303s =[\nIns Ont\u00d7ns\n] K 1/2 s and L\u0303t = [ Ons\u00d7nt Int ] K 1/2 t . For a given\n\u03b1, we also arrive at an SDP problem as follows by defining \u03b2 = \u03b1 \u25e6 y:\nmin H\u0303\u227d0 \u22121 2 \u03b2\u2032KH\u0303\u03b2, s.t. trace(H\u0303) \u2264 \u03bb. (9)"}, {"heading": "2.4. Detailed Solution", "text": "For our proposed HFA method, we develop an alternating optimization algorithm by iteratively updating \u03b1 and H\u0303 to effectively solve the problem in (8). Specifically, when updating \u03b1 at the \u03c4 -th iteration, we fix H\u0303[\u03c4 ] and solve for \u03b1[\u03c4 ] in (8) by using the standard SVM with the kernel matrix KH\u0303[\u03c4] . While updating H\u0303, we fix \u03b1[\u03c4 ] and solve for H\u0303[\u03c4+1] via SDP optimization in (9). The optimization procedure will be terminated when the value of the objective function in (8) converges.\nIn order to efficiently solve the SDP problem in (9), we also develop a simple projected gradient descent method to update H\u0303. Let us define \u03b2s = [\u03b21, . . . , \u03b2ns ] \u2032 and \u03b2t = [\u03b2ns+1, . . . , \u03b2ns+nt ] \u2032. Denoting G(H\u0303) as the\nobjective function of (9), we first obtain the derivative of G(H\u0303) with respect to H\u0303 as follows:\n\u2202G \u2202H\u0303 = \u22121 2 (L\u0303s\u03b2s + L\u0303t\u03b2t)(L\u0303s\u03b2s + L\u0303t\u03b2t) \u2032.\nThen at the \u03c4 -th iteration, H\u0303 will be updated by using the following equation:\nH\u0303[\u03c4+1] = H\u0303[\u03c4 ] \u2212 \u03b7[\u03c4 ] \u2202G\n\u2202H\u0303 \u2223\u2223\u2223 H\u0303=H\u0303[\u03c4] , (10)\nwhere \u03b7[\u03c4 ] is the step size at the \u03c4 -th iteration, which can be found by using the standard line search method (Boyd & Vandenberghe, 2004).\nWe summarize the proposed alternating optimization algorithm for HFA in Algorithm 1. After obtaining the optimal solution \u03b1 and H\u0303 to (8), we can predict any test data point x from the target domain by using the following target decision function:\nf(x) = w\u2032\u03d5(x) + b\n= (( \u03b2\u2032sL\u0303 \u2032 s+\u03b2 \u2032 tL\u0303 \u2032 t ) H\u0303 [ Ons\u00d7nt Int ] +\u03b2\u2032t ) kt+b,(11)\nwhere kt = [k(x t 1,x), . . . , k(x t nt ,x)] \u2032 and k(xi,xj) = \u03d5(xi)\n\u2032\u03d5(xj) is a predefined kernel function for two data samples xi and xj with the same feature dimension."}, {"heading": "3. Related Work", "text": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.\nTo handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011). Shi et al. (2010) proposed to learn feature mapping matrices without using the valuable data label information. While Wang et al. (2011) used the class labels of data, they assumed the data should have a manifold structure. Such manifold assumption may not exist in real-world applications.\nRecently, Kulis et al. (2011) proposed a nonlinear metric learning method to learn an asymmetric feature transformation for the source and target data with high dimensions. And the learned transformation metric is universal for all classes. However, when there exist many classes, a universal metric may not be sufficiently good for feature transformation between data from all classes. In contrast, our method incorporates the proposed augmented features into SVM to learn an individual model for each class."}, {"heading": "4. Experiments", "text": "In this section, we evaluate our proposed HFA method for object recognition and multilingual text categorization. We focus on the heterogeneous domain adaptation problem where there exist only one source domain and one target domain in which only a limited number of labeled target training samples are available. Moreover, we assume that the test data from the target domain are unseen during the training phase."}, {"heading": "4.1. Setup", "text": "Object recognition: We employ a recently released dataset1 used in (Saenko et al., 2010; Kulis et al., 2011) for this task. This dataset contains a total of 4106 images with 31 categories from three sources: amazon (web images downloaded from an online merchant), dslr (high-resolution images taken from a digital DLR camera) and webcam (low-resolution images taken from a web camera). We follow the same protocols in the previous work (Kulis et al., 2011). Specifically, SURF features (Bay et al., 2006) are extracted for all the images. The images from amazon and webcam are clustered into 800 visual words by using k-means. After vector quantization, each image is represented as a 800 dimensional histogram feature. Similarly, we represented each image from dslr as a 600-dimensional histogram feature.\nIn the experiments, dslr is used as the target domain, while amazon and webcam are considered as two individual source domains. We strictly follow the setting in (Saenko et al., 2010; Kulis et al., 2011) and randomly select 20 (resp., 8) training images per category for the source domain amazon (resp., webcam). For the target domain dslr, 3 training images are randomly selected from each category, and the remaining dslr images are used for testing. See Table 1 for a summarization of this dataset.\nText categorization: We use the Reuters multilingual dataset2 (Amini et al., 2009), which is collected by sampling parts of the Reuters RCV1 and RCV2 collections. It contains about 11K newswire articles\n1http://www.icsi.berkeley.edu/~saenko/projects. html\n2http://multilingreuters.iit.nrc.ca/ ReutersMultiLingualMultiView.htm\nfrom 6 classes in 5 languages (i.e., English, French, German, Italian and Spanish). While each document was also translated into the other four languages in this dataset, we do not use the translated documents in this work. All documents are represented as a bag of words and the TF-IDF are extracted.\nWe take Spanish as the target domain in the experiments and other four languages as individual source domains. For each class, we randomly sample 100 training documents from the source domain and m training documents from the target domain, where m = 5, 7, 10, 15 and 20. And the remaining documents in the target domain are used as the test data. Note that the method (Wang & Mahadevan, 2011) cannot handle the original high dimensional TF-IDF features. In order to compare our HFA method with theirs (Wang & Mahadevan, 2011), for documents written in each language, we perform PCA with 60% energy preserved on the TF-IDF features. We summarize this dataset in Table 2.\nBaselines: As the source and target data have different dimensions, they cannot be directly combined to train any classifiers for the target domain. Considering the number of training samples is much lower than the feature dimension, we compare our HFA method by applying kernelization with a number of baseline algorithms listed below:\n\u2022 SVM T: It utilizes the labeled samples only from the target domain to train a standard SVM classifier for each category/class. Note it is not reported in (Kulis et al., 2011).\n\u2022 KCCA (Shawe-Taylor & Cristianini, 2004): It learns a common feature subspace by maximizing the correlation between the source and target training data without using any label information. The data from both domains are projected into the common subspace. Note that KCCA was originally proposed for multi-view learning. Following (Kulis et al., 2011), we also report its results in this work.\n\u2022 HeMap (Shi et al., 2010): It finds the projection matrices for a common feature subspace as\nwell as learns the optimal projected data from both domains. But the label information of training data from both domains is not used.\n\u2022 DAMA (Wang & Mahadevan, 2011): It learns a common feature subspace by utilizing the class labels of the source and target training data for manifold alignment.\n\u2022 ARC-t (Kulis et al., 2011): It uses the labeled training data from both domains to learn an asymmetric transformation metric between the different feature spaces.\nFor KCCA, HeMap and DAMA, after learning the projection matrices, we apply SVM to train their final classifiers by using the projected training data from both domains for a given category/class. For ARC-t, we construct the kernel matrix based on the learned asymmetric transformation metric, and then SVM is also applied to train its final classifier. For all methods, we set the regularization parameter C = 1 in SVM and use the RBF kernel for fair comparison. As we only have a very limited number of labeled training samples in the target domain, the cross-validation technique cannot be effectively employed to determine the optimal parameters. Instead, for our HFA method, we empirically fix the parameter \u03bb as 100 for the object dataset and 1 for the Reuters multilingual dataset. For other methods, we validate all their parameters chosen from {0.01, 0.1, 1, 10, 100} based on the test data and report their best results.\nEvaluation metric: Following (Kulis et al., 2011), for each method we measure the classification accuracy over all categories/classes on both datasets. We randomly sample the training data for ten times and report the mean classification accuracies of all methods over the ten rounds of experiments."}, {"heading": "4.2. Classification Results", "text": "Object recognition: We report the mean and standard deviations of classification accuracies for all methods on the object dataset (Saenko et al., 2010) in Table 3. From the results, SVM T outperforms KCCA and HeMap by using only 3 labeled training samples from the target domain. The explanation is that KCCA and HeMap do not utilize the label information of the target training data to learn the feature mapping matrices. As a result, the learned common subspace is not sufficient to preserve a similar data structure as in the original feature spaces of the source and target data, which results in poor classification performances. DAMA performs only slightly better that SVM T, possibly due to the lack of\nTable 3. Means and standard deviations of classification accuracies (%) of all methods on the object dataset by using 3 labeled training samples per class from the target domain dslr. Results in boldface are significantly better than the others, judged by the t-test with a significance level at 0.05. For KCCA and ARC-t, the numbers in the parentheses are the results reported in (Kulis et al., 2011).\nSource Domain SVM T KCCA HeMap DAMA ARC-t HFA amazon\n52.9\u00b1 3.1 46.3\u00b1 2.7 (51.0) 42.8\u00b1 2.4 53.3\u00b1 2.3 53.1\u00b1 2.4 (53.2) 55.4\u00b1 2.8 webcam 46.7\u00b1 2.8 42.2\u00b1 2.6 53.2\u00b1 3.2 53.0\u00b1 3.2 54.3\u00b1 3.7\nTable 4. Means and standard deviations of classification accuracies (%) of all methods on the Reuters multilingual dataset by using 10 labeled training samples per class from the target domain Spanish. Results in boldface are significantly better than the others, judged by the t-test with a significance level at 0.05.\nSource Domain SVM T KCCA HeMap DAMA ARC-t HFA English\n72.6\u00b1 2.3 71.4\u00b1 3.2 65.7\u00b1 3.1 72.4\u00b1 2.4 72.9\u00b1 2.0 75.3\u00b1 1.7 French 72.8\u00b1 2.8 64.2\u00b1 4.2 72.8\u00b1 2.0 73.5\u00b1 1.8 75.7\u00b1 1.6 German 73.8\u00b1 2.2 64.6\u00b1 3.6 72.9\u00b1 2.3 74.7\u00b1 1.6 76.1\u00b1 1.5 Italian 73.8\u00b1 2.1 65.8\u00b1 2.3 73.3\u00b1 2.1 74.0\u00b1 2.0 75.8\u00b1 1.8\nthe strong manifold structure on this dataset. Both results of ARC-t implemented by ourselves and reported in (Kulis et al., 2011) are only comparable with those of SVM T, which shows that ARC-t is less effective for HDA on this dataset. Our HFA method outperforms the other methods under both settings, which clearly demonstrate the effectiveness of our proposed method for HDA by learning with augmented features.\nText categorization: Table 4 shows the mean and standard deviations of classification accuracies for all methods on the Reuters multilingual dataset (Amini et al., 2009) by using m = 20 labeled training samples per class from the target domain. We have a similar observation as on the object dataset that SVM T still outperforms HeMap in terms of classification accuracy. It is interesting to observe that KCCA is generally better than SVM T, which shows that it can learn a good common feature subspace on this dataset. Moreover, by using the label information, both DAMA and ARC-t perform better than SVM T under almost all the settings. Our proposed HFA method still achieves significantly better performances than others on this dataset, when judged by the t-test\nwith a significance level at 0.05.\nWe also plot the classification results of SVM T, KCCA, DAMA, ARC-t and our HFA method with respect to the number of target training samples per class (i.e., m = 5, 7, 10, 15 and 20) for each source domain in Figure 2. We do not report the results of HeMap, as they are much worse than the other methods. From the results, the performances of all methods increase when using a larger m. And the two HDA methods DAMA and ARC-t generally achieve better mean classification accuracies than SVM T except for the setting using English as the source domain. Our HFA method generally outperforms all other methods according to mean classification accuracy."}, {"heading": "4.3. Convergence Analysis", "text": "To analyze the convergence of the proposed Algorithm 1 for our HFA method, we take one setting from each of the datasets as the showcase. For the object dataset, we use the category \u201cback pack\u201d and the source domain amazon; for the Reuters multilingual dataset, the class \u201cC15\u201d is used together with the source domain English. From the results, Algorithm 1 generally takes less than 80 (resp., 40) iterations before its convergence on the object dataset (resp., the Reuters multilingual dataset). We have similar observations for other categories/classes on the two datasets as well."}, {"heading": "5. Conclusions and Future Work", "text": "We have proposed a new method called Heterogeneous Feature Augmentation (HFA) for heterogeneous domain adaptation. In HFA, we augment the heterogeneous features from the source and target domains by using two newly proposed feature mapping functions, respectively. With the augmented features, we propose\nto find the two projection matrices for the source and target data by using the standard SVM with the hinge loss in both linear and nonlinear cases. Moreover, a socalled transformation metric is introduced to simply our formulated optimization problem of HFA such that it can be effectively solved by our developed alternating optimization algorithm. Promising results of HFA have been achieved on two benchmark datasets for object recognition and text classification."}, {"heading": "Acknowledgement", "text": "This work is supported by the Singapore National Research Foundation under its Interactive & Digital Media (IDM) Public Sector R&D Funding Initiative (administered by the IDM Programme Office) and AcRF Tier-1 Research Grant (RG15/08)."}], "references": [{"title": "Learning from multiple partially observed views \u2013 an application to multilingual text categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "In NIPS,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "Gool", "L. Van"], "venue": "In ECCV,", "citeRegEx": "Bay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Translated learning: Transfer learning across different feature spaces", "author": ["W. Dai", "Y. Chen", "Xue", "G.-R", "Q. Yang", "Y. Yu"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2009}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "In ACL,", "citeRegEx": "Daum\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W. Tsang", "J. Luo"], "venue": "In CVPR,", "citeRegEx": "Duan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2010}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "T-PAMI,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Domain adaptation from multiple sources: A domain-dependent regularization approach. T-NNLS", "author": ["L. Duan", "D. Xu", "I.W. Tsang"], "venue": null, "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Learning from multiple outlooks", "author": ["M. Harel", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Harel and Mannor,? \\Q2011\\E", "shortCiteRegEx": "Harel and Mannor", "year": 2011}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Cross-domain sentiment classification via spectral feature alignment", "author": ["S.J. Pan", "X. Ni", "Sun", "J.-T", "Q. Yang", "Z. Chen"], "venue": "In WWW,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Cross-language text classification using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "In ACL,", "citeRegEx": "Prettenhofer and Stein,? \\Q2010\\E", "shortCiteRegEx": "Prettenhofer and Stein", "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Transfer learning on heterogenous feature spaces via spectral transformation", "author": ["X. Shi", "Q. Liu", "W. Fan", "P.S. Yu", "R. Zhu"], "venue": "In ICDM,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Heterogeneous domain adaptation using manifold alignment", "author": ["C. Wang", "S. Mahadevan"], "venue": "In IJCAI,", "citeRegEx": "Wang and Mahadevan,? \\Q2011\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2011}, {"title": "Cross-lingual adaptation: An experiment on sentiment classifications", "author": ["B. Wei", "C. Pal"], "venue": "In ACL,", "citeRegEx": "Wei and Pal,? \\Q2010\\E", "shortCiteRegEx": "Wei and Pal", "year": 2010}, {"title": "Improving svm accuracy by training on auxiliary data sources", "author": ["P. Wu", "T.G. Dietterich"], "venue": "In ICML,", "citeRegEx": "Wu and Dietterich,? \\Q2004\\E", "shortCiteRegEx": "Wu and Dietterich", "year": 2004}, {"title": "Heterogeneous transfer learning for image clustering via the social web", "author": ["Q. Yang", "Y. Chen", "Xue", "G.-R", "W. Dai", "Y. Yu"], "venue": "ACL/IJCNLP,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Heterogeneous transfer learning for image classification", "author": ["Y. Zhu", "Y. Chen", "Z. Lu", "S.J. Pan", "Xue", "G.-R", "Y. Yu", "Q. Yang"], "venue": "In AAAI,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Domain adaptation methods have been successfully used for different research fields such as natural language processing and computer vision (Blitzer et al., 2006; 2007; Daum\u00e9 III, 2007; Duan et al., 2010; 2012b;a; Wu & Dietterich, 2004).", "startOffset": 140, "endOffset": 236}, {"referenceID": 7, "context": "Domain adaptation methods have been successfully used for different research fields such as natural language processing and computer vision (Blitzer et al., 2006; 2007; Daum\u00e9 III, 2007; Duan et al., 2010; 2012b;a; Wu & Dietterich, 2004).", "startOffset": 140, "endOffset": 236}, {"referenceID": 5, "context": "Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) (Dai et al., 2009; Yang et al., 2009).", "startOffset": 175, "endOffset": 212}, {"referenceID": 20, "context": "Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) (Dai et al., 2009; Yang et al., 2009).", "startOffset": 175, "endOffset": 212}, {"referenceID": 20, "context": "The same assumption was also used in (Yang et al., 2009; Zhu et al., 2011) for text-aid image clustering and classification.", "startOffset": 37, "endOffset": 74}, {"referenceID": 21, "context": "The same assumption was also used in (Yang et al., 2009; Zhu et al., 2011) for text-aid image clustering and classification.", "startOffset": 37, "endOffset": 74}, {"referenceID": 2, "context": "Based on structural correspondence learning (Blitzer et al., 2006), two methods (Prettenhofer & Stein, 2010; Wei & Pal, 2010) were recently proposed to extract the so-called pivot features from the source and target domains, which is specifically designed for the cross-language text classification task.", "startOffset": 44, "endOffset": 66}, {"referenceID": 3, "context": "Dai et al. (2009) proposed to learn a feature translator between the source and target domains by assuming that the data from both domains share co-occurrence attributes (i.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "For more general HDA tasks, Shi et al. (2010) proposed a method called Heterogeneous Spectral Mapping", "startOffset": 28, "endOffset": 46}, {"referenceID": 14, "context": "By kernelizing the method in (Saenko et al., 2010), Kulis et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valuable training labels, either. Wang et al. (2011) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intradomain similarity and the inter-domain dissimilarity.", "startOffset": 0, "endOffset": 206}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valuable training labels, either. Wang et al. (2011) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intradomain similarity and the inter-domain dissimilarity. By kernelizing the method in (Saenko et al., 2010), Kulis et al. (2011) proposed to learn an asymmetric kernel transformation to transfer feature knowledge between the data from the source and target domains.", "startOffset": 0, "endOffset": 445}, {"referenceID": 12, "context": "Note that promising results have been shown by incorporating original features into feature augmentation (Daum\u00e9 III, 2007; Pan et al., 2010) to enhance the similarities between data from the same domain.", "startOffset": 105, "endOffset": 140}, {"referenceID": 12, "context": "Motivated by (Daum\u00e9 III, 2007; Pan et al., 2010), we also incorporate original features in this work and then augment any source and target domain samples x \u2208 Rs and x \u2208 Rt by using our newly proposed", "startOffset": 13, "endOffset": 48}, {"referenceID": 11, "context": "In order to effectively deal with high dimensional data, inspired by (Kulis et al., 2011), in the next subsection we will apply kernelization to the data from the source and target domains and show that (7) can be solved in a kernel space by learning a nonlinear transformation metric with its size independent of the feature dimension.", "startOffset": 69, "endOffset": 89}, {"referenceID": 11, "context": "1 in (Kulis et al., 2011).", "startOffset": 5, "endOffset": 25}, {"referenceID": 5, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 20, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 21, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011).", "startOffset": 108, "endOffset": 150}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011). Shi et al. (2010) proposed to learn feature mapping matrices without using the valuable data label information.", "startOffset": 109, "endOffset": 170}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011). Shi et al. (2010) proposed to learn feature mapping matrices without using the valuable data label information. While Wang et al. (2011) used the class labels of data, they assumed the data should have a manifold structure.", "startOffset": 109, "endOffset": 289}, {"referenceID": 11, "context": "Recently, Kulis et al. (2011) proposed a nonlinear metric learning method to learn an asymmetric feature transformation for the source and target data with high dimensions.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "Object recognition: We employ a recently released dataset used in (Saenko et al., 2010; Kulis et al., 2011) for this task.", "startOffset": 66, "endOffset": 107}, {"referenceID": 11, "context": "Object recognition: We employ a recently released dataset used in (Saenko et al., 2010; Kulis et al., 2011) for this task.", "startOffset": 66, "endOffset": 107}, {"referenceID": 11, "context": "We follow the same protocols in the previous work (Kulis et al., 2011).", "startOffset": 50, "endOffset": 70}, {"referenceID": 1, "context": "Specifically, SURF features (Bay et al., 2006) are extracted for all the images.", "startOffset": 28, "endOffset": 46}, {"referenceID": 14, "context": "We strictly follow the setting in (Saenko et al., 2010; Kulis et al., 2011) and randomly select 20 (resp.", "startOffset": 34, "endOffset": 75}, {"referenceID": 11, "context": "We strictly follow the setting in (Saenko et al., 2010; Kulis et al., 2011) and randomly select 20 (resp.", "startOffset": 34, "endOffset": 75}, {"referenceID": 0, "context": "Text categorization: We use the Reuters multilingual dataset (Amini et al., 2009), which is collected by sampling parts of the Reuters RCV1 and RCV2 collections.", "startOffset": 61, "endOffset": 81}, {"referenceID": 11, "context": "Note it is not reported in (Kulis et al., 2011).", "startOffset": 27, "endOffset": 47}, {"referenceID": 11, "context": "Following (Kulis et al., 2011), we also report its results in this work.", "startOffset": 10, "endOffset": 30}, {"referenceID": 16, "context": "\u2022 HeMap (Shi et al., 2010): It finds the projection matrices for a common feature subspace as well as learns the optimal projected data from both domains.", "startOffset": 8, "endOffset": 26}, {"referenceID": 11, "context": "\u2022 ARC-t (Kulis et al., 2011): It uses the labeled training data from both domains to learn an asymmetric transformation metric between the different feature spaces.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "Evaluation metric: Following (Kulis et al., 2011), for each method we measure the classification accuracy over all categories/classes on both datasets.", "startOffset": 29, "endOffset": 49}, {"referenceID": 14, "context": "Object recognition: We report the mean and standard deviations of classification accuracies for all methods on the object dataset (Saenko et al., 2010) in Table 3.", "startOffset": 130, "endOffset": 151}, {"referenceID": 11, "context": "For KCCA and ARC-t, the numbers in the parentheses are the results reported in (Kulis et al., 2011).", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": "Both results of ARC-t implemented by ourselves and reported in (Kulis et al., 2011) are only comparable with those of SVM T, which shows that ARC-t is less effective for HDA on this dataset.", "startOffset": 63, "endOffset": 83}, {"referenceID": 0, "context": "Text categorization: Table 4 shows the mean and standard deviations of classification accuracies for all methods on the Reuters multilingual dataset (Amini et al., 2009) by using m = 20 labeled training samples per class from the target domain.", "startOffset": 149, "endOffset": 169}], "year": 2012, "abstractText": "We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.", "creator": " TeX output 2012.05.20:1505"}}}