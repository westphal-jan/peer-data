{"id": "1206.6441", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Topic Model for Melodic Sequences", "abstract": "We study the problem of learning a probabilistic model for melodies directly from music sequences of the same genre. This is a challenging task, as one needs to grasp not only the rich temporal structure that is evident in music, but also the complex statistical dependencies between different music components. To solve this problem, we present the Variable-gram Topic Model, which combines latent topic formalism with a systematic model for context-related information. We evaluate the model using the prediction in the next step. In addition, we present a novel method of model evaluation, in which we compare model samples directly with data sequences using the Maximum Mean Discrepance of string kernels to assess how close the model distribution is to the data distribution. We show that the model performs best under both evaluation measures compared to LDA, Topic Bigram, and related independent models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (485kb)", "http://arxiv.org/abs/1206.6441v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.IR stat.ML", "authors": ["athina spiliopoulou", "amos j storkey"], "accepted": true, "id": "1206.6441"}, "pdf": {"name": "1206.6441.pdf", "metadata": {"source": "META", "title": "A Topic Model for Melodic Sequences", "authors": ["Athina Spiliopoulou", "Amos Storkey"], "emails": ["a.spiliopoulou@ed.ac.uk", "a.storkey@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Modelling the real-world complexity of music is an interesting problem for machine learning. In Western music, pieces are typically composed according to a system of musical organization, rendering musical structure as one of the fundamentals of music. Nevertheless, characterizing this structure is particularly difficult, as it depends not only on the realization of several musical elements, such as scale, rhythm and meter, but also on the relation of these elements both within single time frames and across time. This results in an infinite number of possible variations, even within pieces from the same musical genre, which are typically built according to a single musical form.\nTo tackle the problem of melody modelling we propose\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nthe Variable-gram Topic model which employs a Dirichlet Variable-Length Markov Model (Dirichlet-VMM) (Spiliopoulou & Storkey, 2011) for the parametrisation of the topic distributions over words. The DirichletVMM models the temporal structure by learning contexts of variable length that are indicative of the future. At the same time, the latent topics represent different music regimes, thus allowing us to model the different styles, tonalities and dynamics that occur in music. The model does not make any assumptions explicit to music, but it is particularly suitable in the music context, as it is able to model temporal dependencies of considerable complexity without enforcing a stationarity assumption for the data. Each sequence is modelled as a mixture of latent components (topics), and each component models Markov dependencies of different order according to the statistics of the data that are assigned to it.\nTo evaluate the performance of the model we perform a comparative analysis with related models, using two metrics. The first one is the average next-step prediction log-likelihood of test sequences under each model. The second is the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) of string kernels computed between model samples and test-data sequences. In both evaluations, we find that using topics improves performance, but it does not overcome the need for a systematic temporal model. The Variable-gram topic model, which couples these two strategies has the highest performance under both evaluation objectives.\nThe contributions of this paper are: (a) We introduce the Variable-gram Topic model, which extends the topic modelling methodology by considering conditional distributions that model contextual information of considerable complexity. (b) We introduce a novel way of evaluating generative models for discrete data. This employs the MMD of string kernels to directly compare model samples with data sequences."}, {"heading": "2. Background", "text": "A number of machine learning and statistical approaches have been suggested for music related prob-\nlems. Here we discuss methods that take as input discrete music sequences and attempt to model the melodic structure. Lavrenko & Pickens (2003) propose Markov Random Fields (MRFs) for modelling polyphonic music. The model is very general, but in order to remain tractable much information is discarded, thus making it less suitable for realistic music. Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch. The model has three internal states that are predefined according to the structure of the music genre examined. Eck & Lapalme (2008) propose an LSTM Recurrent Neural Network for modelling melody. The network is conditioned on the chord and certain previous time-steps, chosen according to the metrical boundaries. Paiement et al. (2009) provide an interesting approach that incorporates musical knowldege in the melody modelling task. They define a graphical model for melodies given chords, rhythms and a sequence of Narmour features, which are extracted from an Input-Output HMM conditioned on the rhythm.\nA very successful line of research examines the transfer of methodologies from the fields of statistical language modelling and text compression to the modelling of music. Dubnov et al. (2003) propose two dictionary-based prediction methods, Incremental Parsing (IP) and Prediction Suffix Trees (PSTs), for modelling melodies with a Variable-Length Markov model (VMM). Despite its fairly simple nature the VMM is able to capture both large and small order Markov dependencies and achieves impressive musical generations. Begleiter et al. (2004) study six different alogrithms for training a VMM. These differ in the way they handle the counting of occurences, the smoothing of unobserved events and the variable-length modelling. Spiliopoulou & Storkey (2011) propose a Bayesian formulation of the VMM, the Dirichlet-VMM, for the problem of melody modelling. The model is shown to significantly outperform a VMM trained using the PST algorithm. Finally, an interesting application of dictionary-based predictors in the music context is presented in Pearce & Wiggins (2004). They describe a multiple viewpoint system comprising a cross-product of Prediction by Partial Match (PPM) models."}, {"heading": "3. The Variable-gram Topic Model", "text": "In this section we introduce the Variable-Gram Topic model, which we later apply to melodic sequences. In the context of music modelling, documents correspond to music pieces and words correspond to notes. The Variable-Gram Topic model extends Latent Dirichlet Allocation (LDA) by employing the Dirichlet VariableLength Markov model (Dirichlet-VMM) (Spiliopoulou & Storkey, 2011) for the parametrisation of the topic\ndistributions over words. We begin with a description of the Dirichlet-VMM."}, {"heading": "3.1. The Dirichlet-VMM", "text": "The Dirichlet-VMM is a Bayesian hierarchical model for discrete sequential data defined over a finite alphabet. It models the conditional probability distribution of the next symbol given a context, where the length of the context varies according to what we actually observe. Long contexts that occur frequently in the data are used during prediction, while for infrequent ones, their shorter counterparts are used.\nSimilarly to a VMM, the model is represented by a suffix tree that stores contexts as paths starting at the root node; the deeper a node in the tree the longer the corresponding context. The depth of the tree is upper bounded by L, the maximum allowed length for a context. The tree is not complete; only contexts that occur frequently enough in the data and convey useful information for predicting the next symbol are stored. The Probabilistic Suffix Tree algorithm for constructing a VMM tree is detailed in Ron et al. (1994).\nIn contrast to the VMM, parameter estimation in the Dirichlet-VMM is driven by Bayesian inference. Let w denote a symbol from the alphabet and j index the nodes in the tree, with cj = w1 . . . w`, ` \u2208 {1, . . . , L}, denoting the context of node j. Each node j is identified by the conditional probability distribution of the next symbol given context cj , which we denote by \u03c6i|j \u2261 P (w = i|cj). In the Dirichlet-VMM this distribution is modelled through a Dirichlet prior centred at the parent node, \u03c6j \u223c Dirichlet(\u03b2\u03c6pa(j)), where \u03b2 denotes the concentration parameter of the Dirichlet distribution, pa(j) denotes the parent of node j, with corresponding context cpa(j) = w1 . . . w`\u22121, and we have used the bold notation \u03c6j to denote the parameter vector \u03c6\u00b7|j . An example Dirichlet-VMM is depicted in Figure 1.\nDue to the conjugacy of the Dirichlet distribution to the multinomial, posterior inference in this model is exact. Let \u03c6\u0302i|j \u2261 P (w = i|cj , D) denote the es-\ntimate for \u03c6i|j after observing data D. We have \u03c6\u0302j \u223c Dirichlet(\u03b2E[\u03c6\u0302pa(j)] +N\u00b7|j), where N\u00b7|j denotes the counts associated with context j in the data and E [\u00b7] denotes expectation.\nDuring prediction only the leaf nodes are used. Given an observed context, we start at the root node and follow the path labelled by successively older symbols from the context, until we reach a leaf node, from which we read the predictive distribution. The hierarchical construction of the Dirichlet-VMM allows us to maintain information coming from the shorter contexts into the predictive probabilities of the longer ones. It is related to the hierarchical Dirichlet language model (Mackay & Peto, 1995), where instead of a single fixed order we now consider n-gram statistics of variable order, by successively tying the measure of the Dirichlet prior to the statistics of lower orders."}, {"heading": "3.2. Introducing Latent Topics", "text": "The graphical model for the Variable-gram Topic model is depicted in Figure 2(b). Similarly to LDA, each document is modelled as a mixture of latent topics and the latent topics are shared among documents. Each document d has a distribution over the K latent topics parametrised by \u03b8d, where \u03b8d is defined as \u03b8k|d \u2261 P (z = k|d). On the other hand, each topic is now represented by a Dirichlet-VMM; instead of a single probability distribution over words, we now have a set of conditional probability distributions encoding contextual information of variable order. This difference from LDA is apparent in Figure 2, where we can see that in the variable-gram topic model, word wt has directed connections from both zt and the L previously observed words. These latter connections are defined in terms of the Dirichlet-VMM, which means that depending on the context we observe, we can have ` active connections, with ` \u2208 {1, . . . , L}. If we consider only first order (` = L = 1) dependencies, instead of variable order ones, then we retrieve the Bigram Topic model of Wallach (2006).\nAlgorithm 1 Generative Process for the Variablegram Topic model.\nInput: Dirichlet-VMM T , K, \u0398, \u03a6 for each document d in the corpus \u03c9 do\nfor each time-step t, t \u2208 {1, ..., Td} in d do Choose a topic zt,d \u223c Multinomial(\u03b8d) Choose a word wt,d \u223c Multinomial(\u03c6cwt,d ,zt,d)\nend for end for\nLet j index the leaf nodes of a Dirichlet-VMM, i.e. the contexts that can be used during prediction, and cwt= wt\u22121 . . . wt\u2212`, ` \u2208 {1, . . . , L} denote the context of word wt. The parameters \u03c6k characterising word generation within topic k are defined by \u03c6i|j,k\u2261P (wt = i|cwt = j, zt = k). This results in a tensor \u03a6 with KC(W \u2212 1) free parameters, where K is the number of latent topics, C the number of leaf nodes and W the number of words in the vocabulary. For simplicity we assume that the Dirichlet-VMM has the same tree structure for all topics.\nAccording to the generative process in Alogrithm 1, the joint probability of a corpus \u03c9 and a set of corresponding latent topic assignments z under a variable-gram topic model with parameters \u03a6 and \u0398 is\nP (\u03c9, z|\u03a6,\u0398) = \u220f d \u220f t P (zt,d|\u03b8d)P (wt,d|\u03c6cwt,d ,zt,d)\n= \u220f d \u220f i \u220f j \u220f k \u03c6 Ni|j,k i|j,k \u03b8 Nk|d k|d , (1)\nwhere Ni|j,k is the number of times word i has been assigned to topic k when preceded by context j, Nk|d is the number of times topic k has occured in document d and t indexes word positions (time-steps).\nThe prior over the \u0398 parameters is the same as in LDA P (\u0398|\u03b1n) = \u220f d Dirichlet(\u03b8d|\u03b1n) . (2)\nIn this work, we set n to the uniform distribution.\nThe prior over the \u03a6 parameters is now defined in terms of the Dirichlet-VMM. More specifically, let mj,k = \u03c6pa(j),k, denote the measure for the Dirichlet prior of node j in the k-th Dirichlet-VMM. We have\nP (\u03a6|\u03b2{mj,k}) = \u220f k \u220f j Dirichlet(\u03c6j,k|\u03b2mj,k) . (3)\nThis prior produces a hierarchical Dirichlet-VMM tree for each topic k. The parameters for the children of a node are coupled through the shared Dirichlet prior. Hence, within a topic, information regarding prediction is shared among all contexts, through the mutual prior\nat the root node. In this work we set the prior for the root node of each topic, m0,k, to a uniform distribution."}, {"heading": "3.3. Inference & Learning", "text": "The total probability of the model given a set of hyperparameters is\nP (\u03c9, z,\u03a6,\u0398|\u03b1n, \u03b2{mj,k}) = \u220f d\nP (\u03b8d|\u03b1n)\u00d7\u220f k,j P (\u03c6j,k|\u03b2mj,k) \u220f t P (zt,d|\u03b8d)P (wt,d|\u03c6cwt,d ,zt,d). (4)\nFrom (4) we can see that, as with LDA, given \u03b1n, the \u03b8d parameters are independent from each other and the same for all the \u03c6j,k. Similarly, the \u03c6j,k parameters are independent from each other given \u03b2 and {mj,k}. Using these independence relations and the conjugacy of the Dirichlet to the multinomial, we can integrate over the model parameters, \u0398 and \u03a6, to obtain a closed form solution for the joint probability of a corpus \u03c9 and a set of corresponding latent topic assignments z, given a set of hyperparemeters\nP (\u03c9, z|\u03b1n, \u03b2{mj,k}) =\u220f k \u220f j \u0393(\u03b2)\u220f i \u0393(\u03b2mi|j,k) \u220f i \u0393(Ni|j,k + \u03b2mi|j,k) \u0393(Nj,k + \u03b2) \u00d7\n\u220f d \u0393(\u03b1)\u220f k \u0393(\u03b1nk) \u220f k \u0393(Nk|d + \u03b1nk) \u0393(Nd + \u03b1) . (5)\nUsing (5) we can define a collapsed Gibbs sampling procedure that will allow us to infer the latent topic assignments. The procedure starts by randomly initialising the latent topic assignments, and then sequentially sampling each latent variable zt given the current values of all other latent variables z\u2212t, the data \u03c9 and a set of hyperparameters {\u03b1n, \u03b2{mj,k}}. At every Gibbs step we sample zt according to\nP (zt = k|z\u2212t,\u03c9, \u03b1n, \u03b2{mj,k}) \u221d {Ni|j,k}\u2212t + \u03b2mi,j,k {Nj,k}\u2212t + \u03b2 {Nk|d}\u2212t + \u03b1nk {Nd}\u2212t + \u03b1\n(6)\nAfter the Gibbs sampling procedure has converged, we can approximate the posterior distribution of the model parameters, \u03a6 and \u0398, through the predictive distributions\nP (\u03b8k|d|\u03c9, z, \u03b1n) = Nk|d + \u03b1nk\nNd + \u03b1 (7)\nP (\u03c6i|j,k|\u03c9, z, \u03b2{mj,k}) = Ni|j,k + \u03b2mi|j,k\nNj,k + \u03b2 (8)"}, {"heading": "4. Experiments", "text": "In the following section we evaluate the Variable-gram Topic model by comparing its performance with Latent\nDirichlet Allocation, the Bigram Topic model and the Dirichlet-VMM. First, we consider a next-step prediction task, which is commonly used for evaluation in the music context (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004).\nAlthough predictive log-likelihood is indicative of model performance, it only examines certain aspects of what a model has learnt. More specifically, log-likelihood decreases sharply if a model is overfitting, but it does not penalise as heavily a model that assigns lots of its probability mass to improbable configurations. This is problematic, as in unsupervised learning of complex data it is common for a model to underfit.\nTo address this issue, we introduce a novel framework for model evaluation which employs string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to compare samples from the model with test sequences. This evaluation is indicative of underfitting, as models that spread their probability mass outside the space of possible configurations will generate samples that do not resemble data sequences. Therefore, this framework is complementary to the log-likelihood evaluation and allows us to further understand the generative properties of a model.\nFinally, we provide a qualitative evaluation of the Variable-gram Topic model, where we analyse the inferred latent topic assignments and the learned parameters and show that the model captures musically meaningful properties, such as the key and the tempo."}, {"heading": "4.1. Experimental Setup", "text": "For our experiments we use a dataset of MIDI files comprising 264 Sottish and Irish reels from the Nottingham Folk Music Database. Roughly half of the pieces are in the G major scale and the rest in the D major and all the pieces have 4/4 meter. We use the representation of Spiliopoulou & Storkey (2011), where time is discretized in eighth notes and the MIDI values are mapped to a 26-multinomial variable, with 24 values representing pitch (C4-B5) and 2 special values representing \u201csilence\u201d and \u201ccontinuation\u201d. This representation is depicted in Figure 3.\nTo set the hyperparameters, \u03b1 and \u03b2, of the topic models we use a 10-fold cross-validation procedure and perform grid search over the product space of the values {0.01, 1, 5, 10, 50, 100}.\nWe present results for topic models with 5, 10 and 50 topics. Additionally, for the Variable-gram Topic model and the Dirichlet-VMM, we present results using two tree structures, obtained by changing the threshold that the relative frequency of a context must exceed,\nin order to include the context in the tree. The first tree is relatively shallow (threshold: 1e \u2212 03) and is referred to as .Sh, whereas the second tree is deeper (threshold: 1e\u2212 04) and is referred to by .De."}, {"heading": "4.2. Next-Step Prediction Task", "text": "Given a test corpus of melodic sequences, we want to evaluate how well a model performs in next-step prediction. The average next-step prediction log-likelihood of a test corpus \u03c9test under a model M with parameters \u03b8 is given by:\nL = 1 N N\u2211 d=1 1 Td Td\u2211 t=1 logP (wt,d|w1,d, . . . , wt\u22121,d). (9)\nIn the Dirichlet-VMM we can compute (9) exactly. For the topic models, computing the prediction loglikelihood requires a summation over the latent topic assignments for the test set, which is intractable. We approximate (9) through a sampling procedure, where we initialize \u03b8d at the prior and at each time-step we sample s topics from our current estimate of \u03b8d, use these samples to compute the log-likelihood of timestep t and subsequently update \u03b8d with the mean of the posterior distribution from each sample. Additionally we present results from two different update schemes for the distributions over words. In the first approach, denoted by S.1, we do not update the word distribution during testing, that is the information from the observed part of a test piece is only used to update the \u03b8d parameters. In the second approach, denoted by S.2, after each time-step we also update the distributions over words, by adding to \u03c6i|j,k of\nTopic Models (S.1) K = 5 K = 10 K = 50\nLDA \u22122.0484 \u22122.0355 \u22122.0282 Bigram \u22121.7658 \u22121.7558 \u22121.7071 Var-gram.Sh \u22121.5680 \u22121.5653 \u22121.5349 Var-gram.De -1.5390 -1.5427 -1.5219\nTopic Models (S.2) K = 5 K = 10 K = 50\nLDA \u22122.0480 \u22122.0351 \u22122.0280 Bigram \u22121.7575 \u22121.7433 \u22121.6966 Var-gram.Sh \u22121.1724 -0.9827 -0.9553 Var-gram.De -1.0354 \u22120.9850 \u22121.0194\nthe observed word-context, counts proportional to the posterior probability of topic k.\nTable 1 shows the average next-step prediction loglikelihood of the testdata under different models. Note that this is computed using only the first half of the test sequences, as in this genre the second half is typically exact repetition of the first half. The empirical marginal distribution, denoted as EmpMarg, is used as a simple baseline. It is the Maximum Likelihood model if we do not include any temporal dependencies or topic components. The Dir-Bigram model is a Dirichlet-VMM with L = 1 and is included as a second baseline that models first order dependencies. The first thing we can note is that introducing latent topics is useful for modelling melody, as all the topic models perform better than their non-topic counterparts and performance improves as we use more topics. This is true for both update schemes S.1 and S.2.\nA second observation is that modelling temporal dependencies is very important in melody. The DirichletVMM, which has no latent topics, but is able to capture both large and small order Markov dependencies, performs better than the Topic Bigram and LDA, which model only first order and no temporal dependencies respectively. Similarly, the Dirichlet-Bigram performs better than LDA. Therefore, the predictive information of the context is higher than that of the latent topics. This is also evident by the fact that performance improves as we consider longer contexts, with the Variable-gram topic models being consistently better than all other models for both update schemes S.1 and S.2.\nFinally, the aspect of novelty in music is particularly\napparent when we compare the performance of the Variable-gram topic models using update scheme S.1 and S.2. We can see that performance improves significantly when the distributions over notes are updated during prediction, which shows that information coming from the test piece is highly predictive of the future. This signifies that longer contexts can have different meaning across music pieces, which is expected as each piece is a unique realisation of a music idea."}, {"heading": "4.3. Maximum Mean Discrepancy of String Kernels", "text": "In this section, we present a new approach for evaluating model generation. We employ string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to estimate the distance between the model distribution, Q, and the true \u201ctheoretical\u201d data distribution, P , based on finite samples drawn i.i.d. from each. Given the two populations \u2013 model samples and test data \u2013 we first compute a similarity score between each pair of sequences, which is proportional to the number of matching subsequences. Then we quantify the distance between the two populations by comparing the intrapopulation similarity scores to the inter-population scores. A small distance indicates that a model generates many of the different substructures that occur in the data. The method cannot assess the generalisation properties of a model, but it identifies underfitting, by measuring how close are model generations to data sequences. Therefore, it provides a complementary evaluation of model performance.\nThe Maximum Mean Discrepancy (MMD) is a distance metric between probability distribution embeddings on a Reproducing Kernel Hilbert Space (RKHS). Given a set of observed data sequences X := {x1,x2, . . . ,xm} drawn i.i.d. from P and a set of sampled sequences X\u2032 := {x\u20321,x\u20322, . . . ,x\u2032n} drawn i.i.d. from Q, an unbiased empirical estimate of the squared population MMD can be computed as\nMMD2u [F ,X,X\u2032] = 1\nm(m-1) m\u2211 i=1 m\u2211 j 6=i K(xi,xj)+\n1\nn(n-1) n\u2211 i=1 n\u2211 j 6=i K(x\u2032i,x \u2032 j)\u2212 2 mn m\u2211 i=1 n\u2211 j=1 K(xi,x \u2032 j), (10)\nwhere F is a RKHS and K(x, x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009F is a positive definite kernel defined as the inner product between feature mappings \u03c6(x) \u2208 F .\nSince the MMD is defined on a RKHS, we can use the kernel trick to compare pairs of melodic sequences. String kernels naturally lend themselves to this problem, as they define a measure of similarity between\ndiscrete structures by comparing the set of matching substructures. We use the mismatch kernel (Leslie et al., 2004), K(k,m)(x,x\n\u2032), which for a pair of sequences x and x\u2032 computes the shared occurences of k-length subsequences that have at most m mismatches. This kernel has been successfully used for biological sequence classification (Leslie et al., 2004) and NLP tasks (Teo & Vishwanathan, 2006).\nIn order to avoid spurious correlations due to the \u201ccontinuation\u201d value, we map the melodic sequences to a 25-multinomial representation, where \u201ccontinuation\u201d is substituted by the observed pitch. Additionally, we report results using the normalized mismatch kernels\nK\u0303(k,m)(x,x \u2032) = K(k,m)(x,x \u2032)\u221a K(k,m)(x,x) \u221a K(k,m)(x\u2032,x\u2032) (11)\nFigure 4 shows the mean and standard deviation of the estimated squared MMD between test sequences and model samples from different models, computed using the (4, 1) mismatch kernel. Note that for the topic models we generate samples in two ways. In the first case, the topics are sampled from the prior which at each step is updated with the sampled topic. In the second case, which is denoted with the (*) symbol, we first run Gibbs sampling on the test sequences to get a set of topic allocations and then we perform sampling from the model given the topic allocations. This restricts the sampling noise, but is not directly comparable with\nthe non-topic models, as it uses information from the test pieces captured by the latent topic allocations.\nAgain, we use the empirical marginal distribution as a baseline. Additionally, the MMD between test sequences and train sequences, denoted as Train, is given as a lower bound on what can be achieved. All models outperform the empirical marginal, but none of them has learned P completely.\nAlthough the objective function for this evaluation is very different to the one for the next-step prediction task, the comparative performance of the models under this metric is analogous. The first thing we can observe is that the topic component is useful. Similar to the results from the prediction task, the Variablegram Topic model always outperforms the equivalent Dirichlet-VMM and performs better as we increase the number of topics. A second important observation is that introducing topics does not overcome the need for a systematic temporal model. The variable-gram models are consistently and notably better than the equivalent bigrams, and the variable-gram with the deeper tree structure (De) tends to outperform the shallower one (Sh). This indicates that a good model for temporal dependencies is important for both prediction and generation. Experiments using the (5, 1) and (6, 1) mismatch kernels produce equivalent results.\nFigure 5 shows the (4, 1) mismatch kernels between 10 test sequences and 10 samples from different models. We can observe that due to the lack of any temporal structure, samples from the Uniform distribution\nhave very low similarity scores, both with each other and with test sequences. On the other hand, samples from the Dirichlet-VMM are comparatively much more similar to each other than to test sequences. Samples from the topic models are less similar to each other, as different samples have different distributions over topics, thus allowing for more unique generations, while still capturing the statistical dependencies across pieces. Another interesting observation is that when given the topic allocations for the test pieces (e), the samples have significantly more shared occurences with the corresponding test pieces, suggesting that the latent topic is highly informative of the observed note. This is depicted by the high values in the diagonal of the data-samples submatrix (top-right) in subfigure (e)."}, {"heading": "4.4. Qualitative evaluation", "text": "An attractive property of Topic models when applied in NLP tasks is that they discover meaningful topics. In this section we examine aspects of the latent topic allocations and the inferred parameters of the Variablegram topic model and analyse them with respect to musical features. Figure 6 shows a scatter plot of the number of times each topic has been allocated in pieces from the key of G (x-axis) and in pieces from the key of D (y-axis) after the Gibbs sampler has converged. The model has learned to distinguish the key, as topic allocations in this plot are negatively correlated, which means that each topic tends to be allocated in pieces from a single key. Figure 7 shows the conditional distributions over notes for 2 topics that are used in pieces from the key of D for 4 different contexts. The first context is the empty string, i.e. the\ndistribution of the root node of the Dirichlet-VMM tree corresponding to each of the topics. The second context is note E5 and the other two are note E5 preceded by D5 and F#5 repsectively. We can see that the conditional distributions are successively sharper. Topic 4 tends to prefer \u201ccontinuation\u201d, thus modelling slower parts of the melody, whereas topic 1 assigns lower probability to \u201ccontinuation\u201d, especially given the longer contexts. At the same time we can observe that, depending on the context, the topics prefer different notes. For instance, if there is an upward movement in the context, D5 followed by E5 (subfigure (c)), Topic 4 wants to continue going upwards, i.e. high probability on F#5, and vice versa for a downward movement (subfigure (d))."}, {"heading": "5. Discussion", "text": "We presented the Variable-gram Topic model, which couples the latent topic formalism with an expressive model of contextual information. Using two evaluation objectives we showed that the model outperforms a number of related methods for the problem of modelling melodic sequences. Both evaluations revealed that in this setting although latent topics improve performance, they do not overcome the need for a systematic temporal model.\nAdditionally we presented a novel way of evaluating model performance, where we used the MMD of string kernels computed between data sequences and model samples to directly evaluate the model distribution. This evaluation gave the same comparative results as next-step prediction, although it addresses different aspects of model behaviour. Looking at the mismatch\nkernels in Figure 5 we can visualize the progress that a model has made and \u201chow much\u201d of the structure is still not captured.\nFinally it is interesting to investigate the aspect of novelty in music. Pearce & Wiggins (2004) moves in this direction by using the cross product of two models, one constructed using the train data and the other constructed sequentially as we observe the test sequence. Although this is applicable in a prediction task, it is not easy to actualize for generation, as the only available information comes from what the model has previously sampled."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Iain Murray and the anonymous reviewers for providing useful comments."}], "references": [{"title": "On prediction using variable order Markov models", "author": ["R. Begleiter", "R. El-Yaniv", "G. Yona"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Begleiter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Begleiter et al\\.", "year": 2004}, {"title": "Using machine-learning methods for musical style modeling", "author": ["S. Dubnov", "G. Assayag", "O. Lartillot", "G. Bejerano"], "venue": null, "citeRegEx": "Dubnov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dubnov et al\\.", "year": 2003}, {"title": "Learning musical structure directly from sequences of music", "author": ["D. Eck", "J. Lapalme"], "venue": "Technical report, Universite\u0301 de Montreal,", "citeRegEx": "Eck and Lapalme,? \\Q2008\\E", "shortCiteRegEx": "Eck and Lapalme", "year": 2008}, {"title": "A kernel method for the two-sample problem", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2006}, {"title": "Polyphonic music modeling with random fields", "author": ["V. Lavrenko", "J. Pickens"], "venue": "In ACM Multimedia,", "citeRegEx": "Lavrenko and Pickens,? \\Q2003\\E", "shortCiteRegEx": "Lavrenko and Pickens", "year": 2003}, {"title": "Mismatch string kernels for discriminative protein", "author": ["C.S. Leslie", "E. Eskin", "A. Cohen", "J. Weston", "W.S. Noble"], "venue": "classification. Bioinformatics,", "citeRegEx": "Leslie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2004}, {"title": "A hierarchical Dirichlet language model", "author": ["D.J.C. Mackay", "L.C.B. Peto"], "venue": "Natural Language Engineering,", "citeRegEx": "Mackay and Peto,? \\Q1995\\E", "shortCiteRegEx": "Mackay and Peto", "year": 1995}, {"title": "Predictive models for music", "author": ["J.F. Paiement", "Y. Grandvalet", "S. Bengio"], "venue": "Connection Science,", "citeRegEx": "Paiement et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paiement et al\\.", "year": 2009}, {"title": "Improved methods for statistical modelling of monophonic music", "author": ["M. Pearce", "G. Wiggins"], "venue": "Journal of New Music Research,", "citeRegEx": "Pearce and Wiggins,? \\Q2004\\E", "shortCiteRegEx": "Pearce and Wiggins", "year": 2004}, {"title": "The power of amnesia", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Ron et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1994}, {"title": "Comparing probabilistic models for melodic sequences", "author": ["A. Spiliopoulou", "A.J. Storkey"], "venue": "In ECML/PKDD,", "citeRegEx": "Spiliopoulou and Storkey,? \\Q2011\\E", "shortCiteRegEx": "Spiliopoulou and Storkey", "year": 2011}, {"title": "Fast and space efficient string kernels using suffix arrays", "author": ["C.H. Teo", "S.V.N. Vishwanathan"], "venue": "In ICML,", "citeRegEx": "Teo and Vishwanathan,? \\Q2006\\E", "shortCiteRegEx": "Teo and Vishwanathan", "year": 2006}, {"title": "Topic modeling: Beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "In ICML, pp. 977\u2013984", "citeRegEx": "Wallach,? \\Q2006\\E", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "Learning musical pitch structures with hierarchical hidden Markov models", "author": ["M. Weiland", "A. Smaill", "P. Nelson"], "venue": "Technical report, University of Edinburgh,", "citeRegEx": "Weiland et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weiland et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "The second is the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) of string kernels computed between model samples and test-data sequences.", "startOffset": 49, "endOffset": 71}, {"referenceID": 12, "context": "Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch. The model has three internal states that are predefined according to the structure of the music genre examined. Eck & Lapalme (2008) propose an LSTM Recurrent Neural Network for modelling melody.", "startOffset": 0, "endOffset": 216}, {"referenceID": 7, "context": "Paiement et al. (2009) provide an interesting approach that incorporates musical knowldege in the melody modelling task.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Dubnov et al. (2003) propose two dictionary-based prediction methods, Incremental Parsing (IP) and Prediction Suffix Trees (PSTs), for modelling melodies with a Variable-Length Markov model (VMM).", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM. These differ in the way they handle the counting of occurences, the smoothing of unobserved events and the variable-length modelling. Spiliopoulou & Storkey (2011) propose a Bayesian formulation of the VMM, the Dirichlet-VMM, for the problem of melody modelling.", "startOffset": 0, "endOffset": 239}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM. These differ in the way they handle the counting of occurences, the smoothing of unobserved events and the variable-length modelling. Spiliopoulou & Storkey (2011) propose a Bayesian formulation of the VMM, the Dirichlet-VMM, for the problem of melody modelling. The model is shown to significantly outperform a VMM trained using the PST algorithm. Finally, an interesting application of dictionary-based predictors in the music context is presented in Pearce & Wiggins (2004). They describe a multiple viewpoint system comprising a cross-product of Prediction by Partial Match (PPM) models.", "startOffset": 0, "endOffset": 552}, {"referenceID": 9, "context": "The Probabilistic Suffix Tree algorithm for constructing a VMM tree is detailed in Ron et al. (1994).", "startOffset": 83, "endOffset": 101}, {"referenceID": 12, "context": "If we consider only first order (` = L = 1) dependencies, instead of variable order ones, then we retrieve the Bigram Topic model of Wallach (2006). Algorithm 1 Generative Process for the Variablegram Topic model.", "startOffset": 133, "endOffset": 148}, {"referenceID": 7, "context": "First, we consider a next-step prediction task, which is commonly used for evaluation in the music context (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004).", "startOffset": 107, "endOffset": 180}, {"referenceID": 0, "context": "First, we consider a next-step prediction task, which is commonly used for evaluation in the music context (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004).", "startOffset": 107, "endOffset": 180}, {"referenceID": 3, "context": "To address this issue, we introduce a novel framework for model evaluation which employs string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to compare samples from the model with test sequences.", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "We employ string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to estimate the distance between the model distribution, Q, and the true \u201ctheoretical\u201d data distribution, P , based on finite samples drawn i.", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "We use the mismatch kernel (Leslie et al., 2004), K(k,m)(x,x \u2032), which for a pair of sequences x and x\u2032 computes the shared occurences of k-length subsequences that have at most m mismatches.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": "This kernel has been successfully used for biological sequence classification (Leslie et al., 2004) and NLP tasks (Teo & Vishwanathan, 2006).", "startOffset": 78, "endOffset": 99}], "year": 2012, "abstractText": "We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre. This is a challenging task as one needs to capture not only the rich temporal structure evident in music, but also the complex statistical dependencies among different music components. To address this problem we introduce the Variable-gram Topic Model, which couples the latent topic formalism with a systematic model for contextual information. We evaluate the model on next-step prediction. Additionally, we present a novel way of model evaluation, where we directly compare model samples with data sequences using the Maximum Mean Discrepancy of string kernels, to assess how close is the model distribution to the data distribution. We show that the model has the highest performance under both evaluation measures when compared to LDA, the Topic Bigram and related non-topic models.", "creator": "LaTeX with hyperref package"}}}