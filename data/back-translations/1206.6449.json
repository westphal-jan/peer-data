{"id": "1206.6449", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Monte Carlo Bayesian Reinforcement Learning", "abstract": "This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a finite set of hypotheses for the model parameter values and forms a discrete, partially observable Markov decision process (POMDP) whose state space is a cross-product of the state space for the learning task amplification and the sampled model parameter space. POMDP does not require conjugate distributions to represent belief, as previous work has done, and can be solved relatively easily using point-based approximation algorithms. MC-BRL naturally handles both complete and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approaches the underlying BRL task well with guaranteed performance.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (972kb)", "http://arxiv.org/abs/1206.6449v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi wang 0006", "kok sung won", "david hsu", "wee sun lee"], "accepted": true, "id": "1206.6449"}, "pdf": {"name": "1206.6449.pdf", "metadata": {"source": "META", "title": "Monte Carlo Bayesian Reinforcement Learning", "authors": ["Yi Wang", "Kok Sung Won", "David Hsu", "Wee Sun Lee"], "emails": ["WANGY@COMP.NUS.EDU.SG", "KOKSUNG@COMP.NUS.EDU.SG", "DYHSU@COMP.NUS.EDU.SG", "LEEWS@COMP.NUS.EDU.SG"], "sections": [{"heading": "1. Introduction", "text": "A major obstacle in reinforcement learning is slow convergence, requiring many trials to learn an effective policy. Model-based Bayesian reinforcement learning (BRL) provides a principled framework to tackle this difficulty. To speed up convergence, BRL encodes prior knowledge of the world in a model. It explicitly represents uncertainty in model parameters by maintaining a probability distribution over them and chooses actions that maximize the expected long-term reward with respect to this distribution. One approach to BRL is to cast it as a partially observable Markov decision process (POMDP) P (Duff, 2002). The state of P\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nis a pair (s, \u03b8), where s is the discrete world state for the reinforcement learning task and \u03b8 is the unknown continuous model parameter. POMDP policy computation automatically analyzes both aspects of each action: its reward and its contribution towards inferring unknown model parameters, thus achieving optimal trade-off between exploration and exploitation.\nDespite its elegance, this approach is not easy to use in practice. Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).\nWe propose Monte Carlo Bayesian Reinforcement Learning (MC-BRL), a simpler and more general approach to BRL, based on the following observation: although there are infinitely many parameter values, it may be possible to compute an approximately optimal policy without considering all of them, if the objective is good average performance with respect to a prior distribution b0P of model parameters. We sample a finite set of values from b0P and form a discrete POMDP P\u0302 whose state is (s, \u03b8\u0302), with \u03b8\u0302 taking values from the sampled set only. This discrete POMDP P\u0302 approximates the hybrid POMDP P . P\u0302 does not require conjugate distributions for belief representation and can be solved much more easily with existing point-based approximation algorithms, e.g., (Kurniawati et al., 2008). MCBRL also naturally handles both fully and partially observable worlds.\nWe show that MC-BRL is approximately Bayes-optimal with a bounded error in the average case. The outputsensitive bound indicates that if a small approximately optimal policy exists, then a small number of samples is sufficient for P\u0302 to approximate P well. In other words, if we treat P as a generalization of P\u0302 with a richer model parameter space, a small policy results in better generalization. This nicely mirrors similar results in learning theory. We also provide experimental results evaluating MC-BRL on four distinct domains, including one from an application in\nautonomous vehicle navigation."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. MDP and POMDP", "text": "An MDP is a tuple \u3008S,A, T,R, \u03b3\u3009, where S is a set of world states, A is a set of actions, T (s, a, s\u2032) specifies the transition probability of reaching state s\u2032 when taking action a in state s, R(s, a, s\u2032) specifies the reward received when taking action a in state s and reaching state s\u2032, and \u03b3 is a discount factor.\nA policy \u03c0 : S \u2192 A for an MDP is a function that specifies which action to take in each state s \u2208 S. The value of a policy \u03c0 is defined as the expected cumulative discounted reward\nE ( \u221e\u2211 t=0 \u03b3tR (st, \u03c0(st), st+1) ) ,\nwhere the expectation is with respect to the random variable st, the state at step t. The aim of the MDP is to find an optimal policy \u03c0? with maximum value.\nMDPs assume that the agent can directly observe the world state. POMDPs generalize MDPs by allowing partially observable states. Formally, a POMDP is a tuple \u3008S,A,O, T, Z,R, \u03b3\u3009, where S, A, T , R, \u03b3 are as defined in the case of MDP, O is a set of observations, and Z(s\u2032, a, o) is the observation function that specifies the probability of observing o when action a was taken in the previous step and the current state is s\u2032.\nIn a POMDP, the agent does not know for sure its state. Instead, it maintains a probability distribution or belief b(s) over the state space S. A policy \u03c0 : B \u2192 A for a POMDP is a mapping from the belief space to actions. The value of \u03c0 at a belief b is defined as\nV\u03c0(b) = E ( \u221e\u2211 t=0 \u03b3tR (bt, \u03c0(bt), bt+1) | b0 = b ) ,\nwhere the expectation is with respect to the random variable bt, the belief at step t. Given an initial belief b0, the aim of the POMDP is to find an optimal policy \u03c0? with maximum value at b0."}, {"heading": "2.2. Related Works", "text": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008). To maintain the posterior belief of continuous model parameters, it requires either a closed-form representation or effective approximate inference techniques. Instead of solving P directly, MC-BRL\napproximates it with a discrete POMDP P\u0302 by sampling from the prior distribution and takes advantage of the recent advances in point-based discrete POMDP algorithms. This way, we avoid the restrictive assumption of close-form belief representation and obtain a simpler and more general approach.\nSampling has been used extensively in BRL (Castro & Precup, 2007; Ross et al., 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008; Asmuth et al., 2009). However, the earlier works draw samples from the posterior distributions to speed up planning for P or to maintain beliefs efficiently. This is conceptually different from our approach, which samples hypotheses from the model parameter space a priori to form P\u0302 and works exclusively with the sampled hypotheses afterwards.\nOur theoretical result shares a similar idea with that for the (PO)MDP algorithm PEGASUS. (Ng & Jordan, 2000). The PEGASUS analysis bounds the number of samples required to find a good policy in a policy class with finite VC-dimension. Our result does not assume such a policy class. It provides an output-sensitive bound that depends on the size of the policy actually computed, instead of a worst-case bound for all policies in a class."}, {"heading": "3. Monte Carlo BRL", "text": ""}, {"heading": "3.1. BRL as POMDP", "text": "To simplify the presentation, let us first consider BRL of an MDP. Given an MDP \u3008S,A, T,R, \u03b3\u3009, the task of BRL is to find an optimal policy when the transition function T is unknown. Let \u03b8 = {\u03b8sas\u2032 |s, s\u2032 \u2208 S, a \u2208 A} denote the collection of unknown parameters of the MDP, where \u03b8sas\u2032 = T (s, a, s\u2032). It has been shown that the BRL problem can be formulated as a POMDP P = \u3008SP , AP , OP , TP , ZP , RP , \u03b3, b0P\u3009 (Duff, 2002). The state space SP = S \u00d7 \u0398 is the cross product of the MDP states S and the parameter space \u0398. A state (s, \u03b8) consists of a world state s of the MDP and a hypothesized value \u03b8 of the unknown parameter. The actions AP are identical to the actions A in the MDP. Assuming the parameter \u03b8 does not change over time, the transition function is defined as\nTP(s, \u03b8, a, s \u2032, \u03b8\u2032) = Pr(s\u2032, \u03b8\u2032|s, \u03b8, a)\n= Pr(s\u2032|s, \u03b8, a, \u03b8\u2032)Pr(\u03b8\u2032|s, \u03b8, a) = \u03b8sas\u2032\u03b4\u03b8\u03b8\u2032 ,\nwhere \u03b4\u03b8\u03b8\u2032 is the Kronecker delta that takes value 1 if \u03b8 = \u03b8\u2032 and value 0 otherwise. The observation of the POMDP P indicates the current MDP state. Therefore, we define OP = S and ZP(s\u2032, \u03b8\u2032, a, o) = \u03b4s\u2032o. The reward does not depend on the parameter \u03b8, so we have RP(s, \u03b8, a, s\n\u2032, \u03b8\u2032) = R(s, a, s\u2032). Finally, we put a prior distribution b0P(\u03b8) over \u03b8, which reflects our initial belief\nof the unknown parameter.\nThis formulation explicitly represents the uncertainty in the unknown parameter. The parameter \u03b8 forms a component of the POMDP state, which is partially observable and can be inferred based on the history of the observed MDP state/action pairs. By solving the POMDP P , one plans against both the uncertainty in the dynamics and the uncertainty in the model parameter. An optimal policy for P thus yields an optimal strategy for action selection that balances exploration with exploitation.\nSince the parameter \u03b8sas\u2032 takes continuous value, P has a hybrid state space. Two difficulties arise as a result. The first is how to efficiently maintain a belief for the continuous state variable. In order to attain a closed-form representation, most existing work assumes a conjugate prior b0P over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008). The second difficulty is how to solve the hybrid POMDP P efficiently. Although several approximate algorithms based on function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general."}, {"heading": "3.2. Algorithm", "text": "MC-BRL is motivated by the following observation. Although there are infinitely many possible values for the parameter \u03b8, it may be possible to compute an approximately optimal policy without considering all of them. MC-BRL consists of two phases, offline and online. Given a prior distribution b0(\u03b8) and a sample size K, the offline phase of the algorithm works in three steps.\n1. Sample K hypotheses ( \u03b8\u03021, \u03b8\u03022, . . . , \u03b8\u0302K ) indepen-\ndently from b0(\u03b8).\n2. Form a discrete POMDP P\u0302 = \u3008SP\u0302 , AP\u0302 , OP\u0302 , TP\u0302 , ZP\u0302 , RP\u0302 , \u03b3, b0P\u0302\u3009. The state space is the cross product SP\u0302 = S \u00d7 {1, 2, . . . ,K}. A state (s, k) consists of an MDP state s and an indicator k of the sampled hypotheses for \u03b8. The actions AP\u0302 = A and observations OP\u0302 = S are defined in the same way as in Section 3.1. The transition, observation, and reward functions are defined as TP\u0302(s, k, a, s \u2032, k\u2032) = \u03b8\u0302ksas\u2032\u03b4kk\u2032 , ZP\u0302(s \u2032, k\u2032, a, o) =\n\u03b4s\u2032o, and RP\u0302(s, k, a, s \u2032, k\u2032) = R(s, a, s\u2032), respectively. Finally, the initial belief b0P\u0302(k) is defined as the uniform distribution over {1, 2, . . . ,K}.\n3. Solve the POMDP P\u0302 and output a policy \u03c0\u0302.\nIn the online phase, the agent then follows the policy \u03c0\u0302 to select actions.\nMC-BRL sidesteps the two technical obstacles of the existing approach based on the hybrid POMDP P . The discrete POMDP P\u0302 can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons, 2005; Kurniawati et al., 2008). There is also no restrictive assumption on the form of the prior distribution b0(\u03b8). The only requirement is that it is easy to sample from.\nWe further note that P\u0302 falls into the class of mixed observability MDPs (MOMDPs). Its state (s, k) has mixed observability. While the second component k is hidden, the first component s is fully observable. It has been shown that MOMDPs admit a compact factored representation of the state space, which can be exploited to speed up POMDP planning (Ong et al., 2010). In this paper, we use SARSOP (Ong et al., 2010) to solve P\u0302 which readily takes advantage of the MOMDP representation.\nMC-BRL takes a prior distribution b0(\u03b8) as input. In practice, if we know nothing about the true parameter, we use a non-informative prior such as uniform distribution. When there is prior knowledge about the true parameter, more informative prior can be used to bias the hypotheses towards the ground truth."}, {"heading": "3.3. Generalization to Partially Observable Environments", "text": "MC-BRL can be readily generalized to BRL problems under partially observable environments. Suppose we are given a POMDP \u3008S,A,O, T, Z,R, \u03b3\u3009, and we aim to find an optimal policy when both the transition function T and the observation function Z are unknown. The unknown parameters can be denoted as a pair (\u03b8, \u03c8), where \u03b8 is as defined before, while \u03c8 = {\u03c8s\u2032ao|s\u2032 \u2208 S, a \u2208 A, o \u2208 O} denotes the observation function and \u03c8s\u2032ao = Z(s\u2032, a, o).\nMC-BRL can be naturally adapted to address this problem with two modifications to the offline phase. First, it samples the hypotheses from a joint prior distribution b0(\u03b8, \u03c8) instead of b0(\u03b8). Second, the POMDP P\u0302 is modified by setting OP\u0302 = O and ZP\u0302(s \u2032, k\u2032, a, o) = \u03c8\u0302k \u2032\ns\u2032ao. The modified observation function ZP\u0302 now incorporates the uncertainty in the unknown parameter \u03c8 of the underlying POMDP."}, {"heading": "4. Theoretical Analysis", "text": "MC-BRL uses the discrete POMDP P\u0302 to approximate the hybrid POMDP P . To analyze the quality of this approximation, we derive a probably approximately correct (PAC) bound on the regret of MC-BRL\u2019s solution, compared with the optimal solution to P .\nWe assume that a POMDP policy \u03c0 is represented as a policy graph G, which is a directed graph with labeled nodes and edges. Each node of G is labeled with an action a \u2208 A\nand has |O| outgoing edges, each labeled with a distinct observation o \u2208 O. The size of the policy \u03c0, denoted as |\u03c0|, is the number of nodes in G. To execute the policy, the agent first picks a node in G according to the initial belief. It then takes the action associated with the node, receives an observation, and transits to the next node by following the edge labeled with that observation. The process then repeats.\nThe policy graph representation allows us to establish the correspondence between policies forP and P\u0302 . If \u03c0 is a policy for P , then it is also a valid policy for P\u0302 , and vice versa, as P and P\u0302 share the same action space A and observation space O.\nSuppose that MC-BRL forms the discrete POMDP P\u0302 by takingK samples from the initial belief b0P of P . There are three policies of interest: an optimal policy \u03c0? for P , an optimal policy \u03c0\u0302? for P\u0302 , and the policy \u03c0\u0302 that MC-BRL actually computes. We want to bound the regret of \u03c0\u0302 against \u03c0?. Define V\u03c0 as the value of a policy \u03c0 for P with initial belief b0P , and V\u0302\u03c0 as the value of \u03c0 for P\u0302 with initial belief b0P\u0302 . The following theorem states our main theoretical result. The proof is given in the supplementary material1.\nTheorem 1. Suppose that \u03c0? is an optimal policy for P and \u03c0\u0302 is the policy that MC-BRL computes by taking K samples to form a discrete POMDP P\u0302 . Let Rmax = maxs,s\u2032\u2208S,a\u2208A |R(s, a, s\u2032)|. If V\u0302\u03c0\u0302? \u2212 V\u0302\u03c0\u0302 \u2264 \u03b4, then for any \u03c4 \u2208 (0, 1),\nV\u03c0? \u2212 V\u03c0\u0302 \u2264 2Rmax1\u2212\u03b3 \u221a 2((|\u03c0\u0302||O|+2) ln |\u03c0\u0302|+|\u03c0\u0302| ln |A|+ln(4/\u03c4)) K\n+\u03b4\nwith probability at least 1\u2212 \u03c4 .\nThe theorem says that MC-BRL with a small set of samples produces a good approximate solution \u03c0\u0302 to P with high probability, provided that there exists a simple approximate solution \u03c0\u0302 to P\u0302 . It is interesting to observe that although we formulate and solve the underlying reinforcement learning task as a planning problem, this analysis closely mirrors similar results in learning: if we think of P as a generalization of P\u0302 with a richer model parameter space, then the theorem implies that a small policy results in better generalization.\nThe error bound consists of two terms. The first term decays at the rate O(1/ \u221a K). We can reduce it by sampling more hypotheses from the prior, but at the cost of potentially increasing the complexity of the discrete POMDP P\u0302 and the resulting policy \u03c0\u0302. The second term \u03b4 bounds the error in the approximate solution to the discrete POMDP\n1Available at http://www.comp.nus.edu.sg/ \u02dcleews/publications/icml2012-supp.pdf.\nP\u0302 . Algorithms such as HSVI (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) output such bounds as a by-product of POMDP policy computation. We can reduce \u03b4 by running these algorithms longer towards convergence.\nIt is also important to observe that the approximate Bayesoptimality of \u03c0\u0302, quantified by V\u03c0\u0302 , guarantees the average performance of \u03c0\u0302 with respect to the prior distribution b0P of models. It does not guarantee the performance of \u03c0\u0302 on any particular model.\nOur analysis assumes a policy graph representation of POMDP policies. In practice, point-based discrete POMDP algorithms, such as HSVI and SARSOP, typically output policies represented as a set of \u03b1-vectors, which in principle can be converted to policy graphs."}, {"heading": "5. Experiments", "text": "We now experiment with MC-BRL on both fully observable and partially observable reinforcement learning tasks. First, we evaluate MC-BRL on two small synthetic domains widely used in the existing work on BRL (Sections 5.1 and 5.2). Here the standard setup requires us to measure the performance of an algorithm on particular model parameter values rather than the average performance with respect to a prior distribution of model parameters. Therefore the bound in Theorem 1 is not applicable here. Next, we test MC-BRL on two more realistic domains (Sections 5.3 and 5.4), where we measure the average performance of MC-BRL and show that it performs well in this sense, as our theoretical result guarantees.\nAll the experiments are conducted on a 16-core Intel Xeon 2.4GHz server."}, {"heading": "5.1. Chain", "text": "We start with the Chain problem used in (Dearden et al., 1998; Poupart et al., 2006). This problem consists of a chain of 5 states and 2 actions {a, b}. The actions cause the transitions between states and receive corresponding rewards, as shown in Figure 1. The actions are noisy. They slip with probability 0.2 and cause the opposite effect. The optimal policy of this problem is to always perform action a.\nWe consider two versions of the Chain problem. In the\nsemi-tied version, we assume that the structure of transitions between states in Figure 1 are given. The only unknown parameters are the 2 slipping probabilities, one for each action. In the full version, we assume that the transition function T (s, a, s\u2032) is completely unspecified. This leads to 40 unknown parameters.\nWe evaluate MC-BRL algorithm using 500 simulations with 1000 steps in each simulation. We test K = 10, 100, and 1000, and use the uniform prior to sample hypotheses. Since it is a stochastic algorithm, we rerun the offline phase of MC-BRL before each simulation, obtain a policy, and then execute that policy online. We run the offline phase up to 180 seconds. The online time is negligible.\nTable 1 reports the average (undiscounted) total rewards of MC-BRL. For comparison, we also report an upper bound on the reward that could be achieved only if we had known the true model parameters, as well as the rewards of three alternatives: the Beetle algorithm (Poupart et al., 2006), the Exploit heuristic, which never explores but takes the optimal action with respect to the expected MDP under the current belief, and Q-learning with -greedy exploration and linear learning rate. For Q-learning, we test a wide range of values from 0 to 0.5. The reward for the optimal value is reported.\nMC-BRL achieves good performance in the semi-tied version. It obtains near-optimal reward with 1000 samples and is comparable to Beetle. It outperforms Exploit and Q-learning. In the full version, MC-BRL is still better than Q-learning. However, it performs slightly worse than Beetle and is unable to improve the performance substantially with increased number of samples. Exploit performs much better than both MC-BRL and Beetle. However, Exploit relies on a myopic heuristic and does not explore well in general. For example, it performs much more poorly than MC-BRL and Beetle in the semi-tied version.\nMC-BRL\u2019s performance degrades in the full version, be-\ncause the sample size is too small to cover the neighborhood of the true parameters within the 40-dimensional parameter space using the uniform prior. To verify this, we conduct another experiment by inserting the true parameter values as one of the samples of MC-BRL. The results, denoted as MC-BRL+ in Table 1, show that MCBRL achieves good performance in this case. Constructing effective sampling strategies is an important direction for future research."}, {"heading": "5.2. Tiger", "text": "We next test MC-BRL on the Tiger problem (Kaelbling et al., 1998) with partial observability. In this problem, the agent must decide whether to open one of two doors or to listen for the position of the tiger at each time step. Opening the wrong door will cause the agent to be eaten by a tiger with a penalty of \u2212100, while opening the correct door will give a reward of 10. Listening costs\u22121 and gives the true position of the tiger with 15% error. We assume that the transition and reward functions are given, but the observation error rates are unknown.\nWe evaluate MC-BRL using 1000 simulations. Each simulation consists of 100 episodes. In each episode, the agent takes actions and receives observation sequentially. The episode ends when the agent opens a door and the position of the tiger is reset. We test MC-BRL with K = 10 and 100. Following (Ross et al., 2007), we use Dirichlet(3, 5) as the prior distribution to sample the unknown parameters. This prior corresponds to an expected error rate 37.5%. We run the offline phase of MCBRL up to 300 seconds.\nTable 2 shows the total reward gained by MC-BRL in 100 episodes, averaged over the 1000 simulations. For reference, we also include the upper bound induced by the true model, and the reward of the prior model in which the observation error rate is set to the prior expectation 37.5%. With K = 100, MC-BRL achieves performance close to the upper bound, and is far better than the prior model.\nWe further look into the evolution of the reward over episodes. Figure 2 shows the reward gained by MC-BRL per episode, averaged over the 1000 simulations. As we do not have the exact settings used in (Ross et al., 2007), we cannot directly compare with their experimental results. However, we can see that MC-BRL quickly learns the un-\nknown parameters and improves over the prior model. It achieves near-optimal performance after about 20 episodes."}, {"heading": "5.3. Iterated Prisoner\u2019s Dilemma", "text": "The Prisoner\u2019s Dilemma (Poundstone, 1992) is a well known one-shot two-player game in which each player tries to maximize his own reward by cooperating with or betraying the other. In this section, we studied its repeated version, the Iterated Prisoner\u2019s Dilemma (IPD) (Axelrod, 1984), and show that MC-BRL can achieve excellent performance on this problem.\nIn IPD, the game is played repeatedly and each player knows the history of his opponents moves. A key factor for an agent to gain high reward is the capability to model the opponent\u2019s behaviour based on history. It has been shown that any memoryless and one-stage memory opponent can be modeled using 4 parameters \u3008PS , PT , PR, PP \u3009, which are the probabilities that the opponent will cooperate in the next step, given the 4 possible situations of the current step: (1) the agent cooperates while the opponent defects (denoted by S); (2) the agent defects while the opponent cooperates (T ); (3) mutual cooperation (R); and (4) mutual defection (P ) (Kraines & Kraines, 1995).\nSuppose the agent knows the parameters of its opponent. Then the IPD can be naturally formulated as an MDP. The state of the MDP is the current move of the two players, which takes values from {S, T,R, P}. The agent needs to select between cooperating or defecting for the next move. The transition function is defined based on the parameters of the opponent. The reward depends on the next state, and is set to 0, 5, 3, 1 for S, T,R, P respectively, following the setting commonly used in IPD tournaments.\nIn reality, the parameters of the opponent are unknown. The agent needs to explore the opponent\u2019s strategy and at the same time maximize its reward. This leads to a RL problem and we apply MC-BRL to solve it.\nWe are interested in the average performance of MC-BRL when facing various opponents. Therefore, we randomly\nselect 1000 test opponents by uniformly sampling their parameters. For each opponent, we run the offline phase of MC-BRL for 180 seconds and obtain a policy. We then use the policy to play against the opponent for 300 steps and collect the total reward. This is repeated for 20 times to account for the stochastic behaviour of the opponent. For MC-BRL, we test K = 250 and 1000, and use the uniform prior to sample the parameters. We set the discount factor \u03b3 = 0.95.\nTable 3 shows the total rewards averaged over the 1000 opponents. With K = 250, MC-BRL already achieves good rewards. With K = 1000, it approaches the upper bound, which is achieved by solving the underlying MDP with the true parameters of the opponents.\nFor reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany & Kienreich, 2007). These four strategies are used to play against the same 1000 test opponents under the same setting as MC-BRL. The results are summarized in Table 3. MC-BRL achieves comparable reward to OTFT, and significantly outperforms all the others. It is interesting to note that AP, the tournament winner, performs very poorly.\nTFT, Pavlov, AP, and OTFT are all specially designed to win the IPD tournaments, while MC-BRL is a general algorithm for BRL and is not optimized for competitions. On the other hand, one should not directly translate the good performance of MC-BRL here to the IPD tournaments, as it is unlikely to face random opponents. However, MC-BRL can use more informative priors to exploit domain knowledge on the opponents, as the other algorithms do.\nWe further compare MC-BRL with Q-learning. We follow the setting suggested by (Littman & Stone, 2001). The result is shown in Table 3. We can see that MC-BRL significantly outperforms Q-learning on this task.\nWhile MC-BRL achieves good average performance,\nas our theorem guarantees, it can perform worse than other algorithms when faced with particular opponents. For instance, for the opponent parameterized by \u30080.806, 0.108, 0.596, 0.185\u3009, MC-BRL obtains a much lower reward than that of Q-learning: 596.05 versus 659.1."}, {"heading": "5.4. Intersection Navigation", "text": "This problem is motivated by an accident in the 2007 DARPA Urban Challenge (Leonard et al., 2008). In that event, two autonomous vehicles, R and A, approached an uncontrolled traffic intersection as shown in Figure 3. R had the right-of-way and proceeded. However, possibly due to sensor failure or imperfect driving strategy, A did not yield to R and caused a near-miss. This situation is quite common and occurs frequently even with human drivers. Crossing the intersection safely and efficiently without knowing the driving strategy of A poses a significant challenge.\nWe formulate the problem as a RL problem. The underlying model is a POMDP. The state consists of the positions and velocities of R and A. For simplicity, we discretize the environment into a uniform grid. In each step, the agent R can take three actions: accelerate, maintain speed, and decelerate. It then receives an observation on its own state and the state of A. Both actions and observations are noisy. The transition function is defined based on the driving strategy of A, which is unknown to the agent R. The agent receives a reward for crossing the intersection safely, and a large penalty for collision with A. A small penalty is given in each step to expedite the agent to cross the intersection faster. Due to space limitation, we give the detailed settings in the supplementary material.\nThe driving strategy of A is unknown to the agent. We parameterize the driving strategy using 4 parameters: (1) driver imperfection, \u03c3 \u2208 [0, 1], (2) driver reaction time, \u03c4 \u2208 [0.5, 2] s, (3) acceleration, a \u2208 [0.5, 3] m/s2, and (4) deceleration, d \u2208 [\u22123,\u22120.5] m/s2. A preliminary study shows that this parameterization can cover a variety of drivers such as a reckless driver who never slows down at the intersection and an impatient driver who performs a\nrolling stop near the intersection. The agent needs to learn the parameters of A and cross the intersection at the same time.\nWe test MC-BRL on this RL problem. We test a range of K values and sample the parameters from the uniform distribution. Similar to the IPD problem, we are interested in the average performance of MC-BRL with respect to different driversA. Therefore, we uniformly sampled 250 test drivers. For each driver, we run the offline phase of MCBRL for 1.5 hours and obtain a policy. We then evaluate the policy against that test driver using 200 simulations with 40 steps in each simulation.\nFigure 4 shows the average discounted total rewards with discount factor \u03b3 = 0.99. We can see that, as the sample size K increases, the performance of MC-BRL improves quickly. With K = 300, it gets close to the upper bound, which is achieved when the true parameters of the driver A are known.\nWe also compare MC-BRL to a hand-crafted intersection policy that is commonly used in the traffic modeling community (Liu & Ozguner, 2007). With K = 150 and above, MC-BRL significantly outperforms that policy. While the hand-crafted policy is not designed to handle noisy observations, we think that the performance gap between the hand-crafted policy and MC-BRL is more likely to be caused by insufficient adaptivity of the hand-crafted policy in learning the driving strategy of A.\nAs a final remark, this problem gives an example where it is more natural to define the prior over the physical properties of the environment. MC-BRL handles such priors easily, although they are challenging to specify using methods that rely on conjugate distributions."}, {"heading": "6. Conclusion", "text": "We have presented MC-BRL, a simple and general approach to Bayesian reinforcement learning. We prove that by sampling a finite set of hypotheses from the model\nparameter space, MC-BRL generates a discrete POMDP that approximates the underlying BRL problem well with guaranteed performance. We provide experimental results demonstrating strong performance of the approach in practice. Furthermore, MC-BRL naturally handles both fully and partially observable worlds.\nOne important issue for MC-BRL is to sample the model parameter space effectively. A naive method is to discretize the parameter space uniformly and treat the fixed grid points as samples. This method, however, suffers from the \u201ccurse of dimensionality\u201d and is difficult to scale up as the number of parameters increases (Poupart et al., 2006). MC-BRL takes one step further and samples a set of hypotheses independently from a given prior distribution. The promising results obtained in this work open up many possibilities for future investigation, e.g., constructing better informed prior distributions by exploiting domain knowledge and adaptive sampling."}, {"heading": "Acknowledgments", "text": "Y. Wang and D. Hsu are supported in part by MoE AcRF grant 2010-T2-2-071 and MDA GAMBIT grant R-252000-398-490. K.S. Won is supported by an NUS President\u2019s Fellowship. W.S. Lee is supported in part by the Air Force Research Laboratory, under agreement number FA2386-12-1-4031. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government."}], "references": [{"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "In UAI,", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "The Evolution of Cooperation", "author": ["R. Axelrod"], "venue": "Basic Books,", "citeRegEx": "Axelrod,? \\Q1984\\E", "shortCiteRegEx": "Axelrod", "year": 1984}, {"title": "Using linear programming for Bayesian exploration in Markov Decision Processes", "author": ["P.S. Castro", "D. Precup"], "venue": "In IJCAI,", "citeRegEx": "Castro and Precup,? \\Q2007\\E", "shortCiteRegEx": "Castro and Precup", "year": 2007}, {"title": "Model-based Bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In UAI,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes", "author": ["M.O. Duff"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Evolution of learning among pavlov strategies in a competitive environment with noise", "author": ["D. Kraines", "V. Kraines"], "venue": "Journal of Conflict Resolution,", "citeRegEx": "Kraines and Kraines,? \\Q1995\\E", "shortCiteRegEx": "Kraines and Kraines", "year": 1995}, {"title": "SARSOP: Efficient pointbased POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "In RSS,", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "A perception driven autonomous urban vehicle", "author": ["J. Leonard", "J. How", "S. Teller"], "venue": "Journal of Field Robotics,", "citeRegEx": "Leonard et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leonard et al\\.", "year": 2008}, {"title": "How to design a strategy to win an IPD tournament", "author": ["J. Li"], "venue": "In The Iterated Prisoners\u2019 Dilemma: 20 Years On,", "citeRegEx": "Li,? \\Q2007\\E", "shortCiteRegEx": "Li", "year": 2007}, {"title": "Leading best-response strategies in repeated games", "author": ["M.L. Littman", "P. Stone"], "venue": "In IJCAI Workshop on Economic Agents, Models, and Mechanisms,", "citeRegEx": "Littman and Stone,? \\Q2001\\E", "shortCiteRegEx": "Littman and Stone", "year": 2001}, {"title": "Human driver model and driver decision making for intersection driving", "author": ["Y. Liu", "U. Ozguner"], "venue": "IEEE Intelligent Vehicles Symposium,", "citeRegEx": "Liu and Ozguner,? \\Q2007\\E", "shortCiteRegEx": "Liu and Ozguner", "year": 2007}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A. Ng", "M. Jordan"], "venue": "In UAI, pp", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner\u2019s dilemma", "author": ["M. Nowak", "K. Sigmund"], "venue": "game. Nature,", "citeRegEx": "Nowak and Sigmund,? \\Q1993\\E", "shortCiteRegEx": "Nowak and Sigmund", "year": 1993}, {"title": "Planning under uncertainty for robotic tasks with mixed observability", "author": ["S.C.W. Ong", "S.W. Png", "D. Hsu", "W.S. Lee"], "venue": null, "citeRegEx": "Ong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2010}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In IJCAI, pp", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Model-based Bayesian reinforcement learning in partially observable domains", "author": ["P. Poupart", "N. Vlassis"], "venue": "In ISAIM,", "citeRegEx": "Poupart and Vlassis,? \\Q2008\\E", "shortCiteRegEx": "Poupart and Vlassis", "year": 2008}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "In ICML, pp", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Model-based Bayesian reinforcement learning in large structured domains", "author": ["S. Ross", "J. Pineau"], "venue": "In UAI,", "citeRegEx": "Ross and Pineau,? \\Q2008\\E", "shortCiteRegEx": "Ross and Pineau", "year": 2008}, {"title": "On some winning strategies for the Iterated Prisoner\u2019s Dilemma or Mr", "author": ["W. Slany", "W. Kienreich"], "venue": "Nice Guy and the Cosa Nostra. In The Iterated Prisoners\u2019 Dilemma:", "citeRegEx": "Slany and Kienreich,? \\Q2007\\E", "shortCiteRegEx": "Slany and Kienreich", "year": 2007}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R.G. Simmons"], "venue": "In UAI, pp", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "One approach to BRL is to cast it as a partially observable Markov decision process (POMDP) P (Duff, 2002).", "startOffset": 94, "endOffset": 106}, {"referenceID": 4, "context": "Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 195, "endOffset": 273}, {"referenceID": 17, "context": "Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 195, "endOffset": 273}, {"referenceID": 7, "context": ", (Kurniawati et al., 2008).", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al.", "startOffset": 50, "endOffset": 62}, {"referenceID": 21, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008).", "startOffset": 116, "endOffset": 245}, {"referenceID": 17, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008).", "startOffset": 116, "endOffset": 245}, {"referenceID": 0, "context": "Sampling has been used extensively in BRL (Castro & Precup, 2007; Ross et al., 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008; Asmuth et al., 2009).", "startOffset": 42, "endOffset": 151}, {"referenceID": 4, "context": "It has been shown that the BRL problem can be formulated as a POMDP P = \u3008SP , AP , OP , TP , ZP , RP , \u03b3, bP\u3009 (Duff, 2002).", "startOffset": 110, "endOffset": 122}, {"referenceID": 3, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 4, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 17, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 4, "context": "Although several approximate algorithms based on function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general.", "startOffset": 111, "endOffset": 164}, {"referenceID": 17, "context": "Although several approximate algorithms based on function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general.", "startOffset": 111, "endOffset": 164}, {"referenceID": 15, "context": "The discrete POMDP P\u0302 can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons, 2005; Kurniawati et al., 2008).", "startOffset": 86, "endOffset": 155}, {"referenceID": 7, "context": "The discrete POMDP P\u0302 can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons, 2005; Kurniawati et al., 2008).", "startOffset": 86, "endOffset": 155}, {"referenceID": 14, "context": "It has been shown that MOMDPs admit a compact factored representation of the state space, which can be exploited to speed up POMDP planning (Ong et al., 2010).", "startOffset": 140, "endOffset": 158}, {"referenceID": 14, "context": "In this paper, we use SARSOP (Ong et al., 2010) to solve P\u0302 which readily takes advantage of the MOMDP representation.", "startOffset": 29, "endOffset": 47}, {"referenceID": 7, "context": "Algorithms such as HSVI (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) output such bounds as a by-product of POMDP policy computation.", "startOffset": 59, "endOffset": 84}, {"referenceID": 17, "context": "We start with the Chain problem used in (Dearden et al., 1998; Poupart et al., 2006).", "startOffset": 40, "endOffset": 84}, {"referenceID": 17, "context": "(Poupart et al., 2006).", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "For comparison, we also report an upper bound on the reward that could be achieved only if we had known the true model parameters, as well as the rewards of three alternatives: the Beetle algorithm (Poupart et al., 2006), the Exploit heuristic, which never explores but takes the optimal action with respect to the expected MDP under the current belief, and Q-learning with -greedy exploration and linear learning rate.", "startOffset": 198, "endOffset": 220}, {"referenceID": 5, "context": "We next test MC-BRL on the Tiger problem (Kaelbling et al., 1998) with partial observability.", "startOffset": 41, "endOffset": 65}, {"referenceID": 1, "context": "In this section, we studied its repeated version, the Iterated Prisoner\u2019s Dilemma (IPD) (Axelrod, 1984), and show that MC-BRL can achieve excellent performance on this problem.", "startOffset": 88, "endOffset": 103}, {"referenceID": 1, "context": "For reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany & Kienreich, 2007).", "startOffset": 98, "endOffset": 113}, {"referenceID": 9, "context": "For reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany & Kienreich, 2007).", "startOffset": 227, "endOffset": 237}, {"referenceID": 8, "context": "This problem is motivated by an accident in the 2007 DARPA Urban Challenge (Leonard et al., 2008).", "startOffset": 75, "endOffset": 97}, {"referenceID": 17, "context": "This method, however, suffers from the \u201ccurse of dimensionality\u201d and is difficult to scale up as the number of parameters increases (Poupart et al., 2006).", "startOffset": 132, "endOffset": 154}], "year": 2012, "abstractText": "Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with pointbased approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.", "creator": "LaTeX with hyperref package"}}}