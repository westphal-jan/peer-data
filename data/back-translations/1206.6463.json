{"id": "1206.6463", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An Iterative Locally Linear Embedding Algorithm", "abstract": "Local Linear Embedding (LLE) is a popular method for dimension reduction. In this paper, we first show that LLE with non-negative constraint corresponds to the widespread laplactic embedding. We also propose to repeat the two steps in LLE repeatedly to improve the results. Third, we relax the kNN constraint of LLE and present a sparse learning algorithm for similarity. The final iterative LLE combines these three improvements. Extensive experimental results show that iterative LLE algorithm significantly improves both classification and cluster results.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (223kb)", "http://arxiv.org/abs/1206.6463v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["deguang kong", "chris h q ding"], "accepted": true, "id": "1206.6463"}, "pdf": {"name": "1206.6463.pdf", "metadata": {"source": "META", "title": "An Iterative Locally Linear Embedding Algorithm", "authors": ["Deguang Kong", "Chris Ding", "Heng Huang", "Feiping Nie"], "emails": ["doogkong@gmail.com", "chqding@uta.edu", "heng@uta.edu", "feipingnie@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "Recently, there have been many algorithms proposed for nonlinear dimension reduction, which include Isomap (Tenenbaum et al., 2000), locally linear embedding (LLE) (Roweis & Saul, 2000), kernel-LLE (Ham et al., 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations. Above dimension reduction algorithms usually cover two main steps: (A) for each data point, learn the local geometry information W. This W can be viewed as similarity between data points or the edge weights of a graph whose nodes are the data points. We call this W-learning, or learning the graph weights. (B) Using the learned W to embed the high-dimensional data points into a lower-dimensional space Y. We call this Y-learning, or learn the embedding. The performance of those algorithms are\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ndetermined both by learning the local information and also by constructing the mapping relations.\nIn past decades, many clustering algorithms have been proposed such as K-means, spectral clustering and its variants (Ng et al., 2001), normalized cut (Shi & Malik, 1997), ratio cut (Chan et al., 1994), etc. Among them, the use of manifold information in graph cuts has shown the state-of-the-art clustering performance.\nOne key observation is that both LLE and spectral clustering utilize the data manifold information. This motivates us to investigate deeper relations between the LLE Y-learning and spectral clustering in terms of Laplacian embedding (because the embedding is precisely the relaxed cluster indicators for the spectral clustering). Indeed, we discover that a properly modified formulation of Y-learning provides a solution which is identical to the normalized Laplacian embedding (see \u00a72.3). We incorporate this improvement into our final iterative LLE algorithm.\nAnother observation is that the data geometry information encoded in W also plays a central role in the performance of these algorithms. We investigate the W-learning process and propose a nonnegative, kernelized, sparse W-learning algorithm (see \u00a74).\nFurthermore, we propose to iteratively repeat the two mains steps (W-learning and Y-learning) to improve the results progressively. Here we use the learned embedding Y to augment the input data to learn a better W, which leads to a better Y in turn. This is repeated until the process converges (details are given in \u00a73). This iterative procedure incorporates both the improved Y learning and the improved W learning into a coherent iterative LLE algorithm.\nThe experiment results for clustering and semisupervised learning tasks on 9 datasets show clear performance improvements."}, {"heading": "2. LLE and New Formations", "text": ""}, {"heading": "2.1. Brief overview of LLE", "text": "LLE (Roweis & Saul, 2000) is a nonlinear dimension reduction approach. Suppose data X = [x1,x2, \u00b7 \u00b7 \u00b7,xn] \u2208 <\np\u00d7n, consists of n data points xi, each with dimensionality p. LLE expects each data point and its neighbors to lie on or close to a locally linear manifold, which governs how the weight coefficients W are constructed from Eq.(1). It then reconstruct each data point (low k-dimensional embedding vectors {yi}) from its neighbors via the same neighborhood relations by minimizing a quadratic cost function Eq.(2),\nmin W\n\u2211\ni\n||xi \u2212 \u2211\nj\u2208Ni\nWijxj || 2 , (1)\nmin Y\n\u2211\ni\n\u2016yi \u2212 \u2211\nj\nWijyj\u2016 2 , (2)\nwhere weight Wij summarizes the contribution of the jth data point to the construction of ith data point. Ni is the kNN neighborhood of xi. The shift invariance of Y = [y1,y2, \u00b7 \u00b7 \u00b7,yn] \u2208 <k\u00d7n is enforced by restricting\u2211 j Wij = 1."}, {"heading": "2.2. LLE Improvements in two directions", "text": "In this paper, we propose improved formulations in both main steps in LLE. (A) In the W-learning step of Eq.(1), we propose new improved formulations to learnW. We first makeW nonnegative in this section. We will further propose a kernelized sparse learning in \u00a74. (B) In the Y-learning step of Eq.(2), we propose slightly modified formulation and prove that the solution to Eq.(2) is identical to Normalized Cut or Laplacian embedding. Our iterative LLE algorithm is based on these improved formations in both LLE steps.\nTo make a connection to graph embedding, we (1) restrict W to be nonnegative, i.e., we add constraint W \u2265 0 to Eq.(1) (as done in (Wang & Zhang, 2006)); (2) we symmetrize W to obtain Z = 12 (W +W\nT ) as the graph edge weight/similarity matrix; (3) we impose D-orthogonal constraint on Y, i.e., YDYT = I, where D = diag(Ze) is a diagonal matrix containing node degrees.\nWith these three changes, the LLE equations of Eqs.(1,2) become\nmin W\n\u2211\ni\n||xi \u2212 \u2211\nj\u2208Ni\nWijxj || 2 , s.t. W \u2265 0, (3)\nmin Y\n\u2211\ni\ndi||yi \u2212 \u2211\nj\n(D\u22121Z)ijyj || 2 s.t. YDYT = I, (4)\nwhere di = Dii.\nWe note several important changes here. In Eq.(4), D\u22121 is inserted for two important reasons: (1) Note that \u2211 j(D\n\u22121Z)ijyj is the average values of yi\u2019s neighbors. Thus Eq.(4) enforces the smoothness of function {yi}. (2) It also enforces the shift invariance of obtainedY, because \u2211 j(D\n\u22121Z)ij = 1. This implies that if {y\u2217i } is an optimal solution, so is {y \u2217 i \u2212c} where c is a constant vector. Note that we add di as the weight of each point yi, for reasons immediately clear below."}, {"heading": "2.3. LLE Y-learning is identical to Normalized Cut Spectral Clustering", "text": "Now we show that LLE Y-learning formulation of Eq.(4) is identical to normalized cut.\nIn fact, this is a general result, not restricted to LLE. It holds for any symmetric nonnegative graph similarity function Z. More precisely we have theorem (1),\nTheorem 1. For any symmetric nonnegative graph similarity function Z of the formulation of Eq.(4), the optimal solution of Y is identical to the optimal solution H of normalized cut spectral clustering, given graph weight matrix Z.\nIn the following, we first briefly introduce normalized cut and present the proof of Theorem 1.\nNormalized Cut.\nNormalized cut (Shi & Malik, 1997) is an effective graph partitioning (clustering) technique to identify clusters inherent in the data, given the pairwise similarity matrix Z. It is well-known now multi-way normalized cut can be solved by the following problem,\nmin G\nTr(GT (I\u2212 Z\u0303)G) s.t. GTG = Ik, (5)\nwhere Z\u0303 = D\u2212 1 2ZD\u2212 1\n2 . and G = [g1,g2, \u00b7 \u00b7 \u00b7,gk] are relaxed cluster indicators. The optimal solution for G is the smallest k eigenvectors from (I\u2212 Z\u0303), i.e.,\n(I\u2212 Z\u0303)gk = \u00b5kgk. (6)\nThe cluster indicator H = [h1,h2, \u00b7 \u00b7 \u00b7,hk] is\nhk = D \u2212 1 2 gk, H = D \u2212 1 2G. (7)\nRelation to Laplacian Embedding.\nIt is easy to see that HT = V \u2261 [v1, \u00b7 \u00b7 \u00b7 ,vn] is identical to the solution of\nmin V\n\u2211\nij\nWij\u2016vi \u2212 vj\u2016 2 s.t. VDVT = Ik. (8)\nThis Laplacian embedding with degree normalization VDVT = Ik is effective for clustering problems because the embedding coordinates are the continuous\nrelaxation of the cluster indicators of the multi-way normalized cut spectral clustering. Similarly, Laplacian embedding using coordinates with standard normalization VVT = Ik is precisely the continuous relaxation of the cluster indicators of multi-way ratio cut spectral clustering (Chan et al., 1994); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means clustering (Zha et al., 2001; Ding & He, 2004).\nTheorem 1 can be equivalently expressed for Laplacian embedding."}, {"heading": "2.4. Proof of Theorem 1", "text": "To prove the theorem 1, we need Lemma 1.\nLemma 1. The optimal solution to Eq.(4) is,\nY\u2217 = FTD\u2212 1 2 , (9)\nwhere F = [f1, f2, ..., fk] \u2208 <n\u00d7k is the smallest k eigenvectors of (I\u2212 Z\u0303)2, Z\u0303 = D\u2212 1 2ZD\u2212 1 2 , i.e.,\n(I\u2212 Z\u0303)2fk = \u03bbkfk. (10)\nProof of Lemma 1.\nProof. Note Y = [y1,y2, \u00b7 \u00b7 \u00b7,yn] \u2208 < k\u00d7n. Let\ny\u0303i = yi \u2212 \u2211\nj\n(D\u22121Z)ijyj , (11)\nand then Y\u0303 = [y\u03031, y\u03032, \u00b7 \u00b7 \u00b7, y\u0303n] \u2208 <k\u00d7n. It is easy to see Y\u0303 = Y \u2212YZD\u22121. Now Eq.(4) can be written as\nn\u2211\ni=1\ndi||y\u0303i|| 2 =\nn\u2211\ni=1\nk\u2211\nj=1\ndiY\u0303 2 ji =\nk\u2211\nj=1\nn\u2211\ni=1\nY\u0303jiDii(Y\u0303 T )ij\n= Tr(Y\u0303DY\u0303T ) = Tr (Y \u2212YZD\u22121)D(Y \u2212YZD\u22121)T\n= Tr Y(I\u2212 ZD\u22121)D 1 2D 1 2 [Y(I\u2212 ZD\u22121)]T\n= Tr YD 1 2 (I\u2212 Z\u0303)(I\u2212 Z\u0303)D 1 2YT .\nThus Eq.(4) becomes\nmin Y\nTr(YD 1 2 (I\u2212 Z\u0303)2D 1 2YT ) s.t. YDYT = I. (12)\nTo optimize Eq.(12) is equivalent to optimize,\nmin F\nTrFT (I\u2212 Z\u0303)2F, s.t. FTF = I, (13)\nwhere F = D 1 2YT . It is easy to see the optimal solution F = [f1, f2, \u00b7 \u00b7 \u00b7, fk] for Eq.(13) is the smallest k eigenvectors from (I \u2212 Z\u0303)2, i.e., Eq.(9). Thus the optimal solution Y\u2217 = (D\u2212 1 2F)T = FTD\u2212 1 2 .\nProof of Theorem 1.\nProof. Because (I \u2212 Z\u0303) is semi-definite positive, the eigenvectors gk of Eq.(6) can be uniquely mapped to eigenvectors gk of\n(I\u2212 Z\u0303)2gk = \u00b5 2 kgk. (14)\nComparing Eq.(6) of normalized cut against Eq.(10) of LLE, one can see fi = gi, \u00b5 2 i = \u03bbi,F = G. Compared Eq.(7) of normalized cut against Eq.(9) of LLE, one can see H = YT . This completes the proof."}, {"heading": "3. An Iterative LLE Learning Algorithm (ILLE)", "text": "We now use the above results, coupled with two new schemes(A,B) to derive a new learning algorithm."}, {"heading": "3.1. Motivation of iterative LLE", "text": "(A)Iterative process of LLE\nIn LLE, starting from X, we learn W, and then learn Y as the low-dimensional embedding of data X. In this paper, we propose to use Y as the new data and iterate this process to further improve the embedding. The key observation is that the class structure of the data is more clear in Y than in X (this is the original embedding purpose of LLE). Thus we use Y as the new data and repeat this process to learn an improved Y.\n(B) Kernel generalization\nFrom experiments on several datasets, the results of using linear formulation on X for learning W in Eq.(1) are generally not as good as other state-of-art methods. Here we use the kernel trick to generalize this to arbitrary nonlinear similarity function. We re-write Eq.(1) as\nmin W\n\u2211\ni\n||\u03c6(xi)\u2212 \u2211\nj\u2208Ni\nWij\u03c6(xi)|| 2 , (15)\nwhere \u03c6(xi) is a mapping to a higher dimensional space. The important thing here is that the exact form of the mapping function is not needed; only the inner product Kij = \u3008\u03c6(xi), \u03c6(xj)\u3009 is needed.\nUsing matrix notation, the LLE of Eq.(1) can be written as min\nW ||X\u2212XWT ||2, and Eq.(15) can be written\nas\n||\u03c6(X)\u2212 \u03c6(X)WT ||2 = Tr(K \u2212WK \u2212KWT +WKWT ). (16)\nThis is useful, because once we compute Y from Eq.(4), we can build a kernel from Y and substitute it into Eq.(16) to learn a new W (and thus Z )."}, {"heading": "3.2. Proposed algorithm", "text": "By incorporating the above schemes of (A,B), we outline our iterative LLE learning algorithm as follows.\n(1)Given kernel Kt, solve for Wt with Eq.(16) or Eq.(18)1.\n(2)Given pairwise similarity Wt, solve for Yt using Lemma 1.\n(3)Given embedding Yt, compute a new kernel Kt+1 either as the final result of our algorithm (both embedding Yt and kernel Kt+1) or as input to step (1). Details of Kt+1 construction is given in \u00a73.3.\nInitially K1 is obtained from data X, we repeat above 3 steps for serval iterations to obtain a better kernel. See Algorithm 1 for more details. Note in step(1), we have two alternatives to compute Wt. Thus we have two versions of iterative LLE - one based on simply iterating LLE process, and the other based on learning a sparse kernel using algorithm of Eq.(21) in \u00a74.\nDiscussion Here we did not give the global convergence proof of this iterative LLE algorithm. The algorithm is very intuitive and natural. It is motivated by a simple observation: class structure is more clear in embedding Y than in original data X."}, {"heading": "3.3. Construction of the new kernel", "text": "In step (3) of our algorithm, once the low-dimensional embedding Yt is obtained, we have the following choices.\n(a) Construct a new kernel from Yt. There are many way to construct kernel. One possible approach is to construct the kernel KY by simply using the Gaussian Kernel, i.e., KY = e\u2212\u03b3||yi\u2212yj || 2\n, where \u03b3 is the scale parameter. Another way is to construct new kernel KY as the linear kernel in low-dimensional space, i.e., KY = YYT .\n(b) Construct the kernel Kt+1 either as the final result of our algorithm or as input to step (1). There are many choices, (b1)Kt+1 = KtY; (b2) Kernel K\nt+1 is a combination of KtY and the previous kernel K\nt. There are two way to achieve this, additively Kt+1 = Kt + KtY, or multiplicatively K\nt+1 = Kt KtY, where we use to denote the element-wise matrix multiplication, e.g., if C = A B, then Cij = Aij \u00d7Bij .\nIn choice (b1), we simply ignore the previous kernel and set the new kernel Kt+1 = KtY. Note both additive and multiplicative operations in choices (b2) ensure the new kernelKt+1 is also semi-definite positive(s.d.p)\n1 S of Eq.(18) can be viewed as pairwise similarity\nif the original kernel Kt is s.d.p.\nDiscussion In our experiments, we tried different choices. We find the results obtained from (b2) are generally better than (b1), and the multiplicative combination usually achieves better results than additive combination. Thus in our experiment we use (b2) with multiplicative combination to construct the new kernel in step 3.\nAlgorithm 1 Iterative LLE algorithm(ILLE)\nInput: Original Kernel K1 obtained from data X, maximal iteration T Output: Pairwise similarity W , embedding Y Algorithm:\n1: for t = 1 to T do 2: Compute Wt of Eq.(16) or Eq.(18) with current ker-\nnel Kt\n3: Compute Zt = 1 2 (W +Wt). 4: Compute embedding Yt using Lemma 1. 5: Compute a new kernel Kt+1 given embedding Yt. 6: end for 7: Output: Pairwise similarity W = Kt+1, embedding\nY = Yt."}, {"heading": "4. Improved W-Learning Formulation", "text": "Here we propose an improvement to the W-learning step of LLE. So far for LLE of Eq.(1) and the new kernel version of Eq.(16), we maintain the original LLE convention that W preserves the kNN structure, i.e. Wij 6= 0 for only j \u2208 Ni (kNN of object i).\nThis constraint is too strong for constructing the data similarity matrix W. Thus, in our approach, we relax this to let Wij be nonzero even if j 6\u2208 Ni. In other words, we bypass kNN entirely.\nWe now present a new approach to learn the pairwise similarity matrix S \u2208 <n\u00d7n, where Sij represents the i-th data\u2019s contribution to reconstruct data point xj . We hope the newly learned S has much clear structure. We use the symbol S to emphasize that W is learned using the new approach. Our objective function for learning S is,\nmin S\u22650 \u2016X\u2212XS\u20162 + \u03b1Tr(STS) + \u03b2||S||1,1, (17)\nwhere \u03b1 and \u03b2 are regularization parameters, ||S||1,1 = \u2211 ij |Sij |. The first term \u2016X \u2212 XS\u2016\n2 =\u2211 i ||xi\u2212 \u2211 j Sjixj || 2 is used to minimize the reconstruction error from the original data. The second term penalizes the complexity of S. The third term of L1 norm is to promote the sparsity of the solution.\nUsing mapping \u03c6: X \u2192 \u03c6(X) to map data X to a higher dimensional space in kernel machine. Eq.(17)\nbecomes\nmin S\u22650\n\u2016\u03c6(X)\u2212 \u03c6(X)S\u20162 + \u03b1Tr(STS) + \u03b2||S||1,1, (18)\nwhich is equivalent to,\nmin S\u22650\nTr(K \u2212 2KS+ STKS) + \u03b1Tr(STS) + \u03b2||S||1,1. (19)\nEq.(19) is identical to Eq.(17) when K = XTX.\nEq.(19) is a convex optimization problem and S has a unique global solution. Furthermore, Eq.(19) can be written as\nmin S\u22650\nTr[K + (\u03b2E\u2212 2K)S+ ST (K + \u03b1I)S], (20)\nwhere E is a matrix of all ones. Because K is s.d.p., by adding \u03b1I with \u03b1 > 0, (K + \u03b1I) is a well-conditioned matrix. It can be solved efficiently (see below). Usually L1 norm term is difficult to handle. Here, however, it does not add any difficulty when handled together with the nonnegativity constraint. The L1 term can be ignored entirely: ||S||1,1 = Tr(ES)."}, {"heading": "4.1. Computational algorithm for Eq.(18)", "text": "Here we present an efficient algorithm to solve Eq.(18) and prove its convergence rigorously.\nThe algorithm starts with an initial guess of S = E(E is a matrix of all ones), iteratively updates S according to\nSij \u2190 Sij Kij\n(KS+ \u03b1S)ij + \u03b2\n2\n. (21)\nThis algorithm converges very fast. The computational algorithm for Eq.(18) is very simple and can be efficiently implemented."}, {"heading": "4.2. Convergence of Updating rule of Eq.(21)", "text": "We have Theorem(2) to prove the convergence of the algorithm when K is non-negative.\nTheorem 2. Updating S using the rule of Eq.(21), the objective function of Eq.(18) monotonically decreases.\nThe proof of this theorem is lengthy and is similar to that in (Ding et al., 2010; Kong et al., 2011). We therefore skip the proof in this paper."}, {"heading": "4.3. Correctness of Updating Rule of Eq.(21)", "text": "We prove that the converged solution satisfies the Karush-Kuhn-Tucker condition of the constrained optimization theory. We have Theorem 3 to prove it.\nTheorem 3. At convergence, the converged solution S of the updating rule of Eq.(21) satisfies the KKT condition of the optimization theory.\nProof. The KKT condition for S with constraints Sij \u2265 0 is \u2202J(S) \u2202Sij Sij = 0, \u2200 i, j.\nThe derivative of J(S)(Eq.18) is \u2202J(S) \u2202Sij\n=\n(\u22122K + 2KS+ 2\u03b1S+ \u03b2E)ij . Thus the KKT condition for S is\n(\u22122K + 2KS+ 2\u03b1S+ \u03b2E) ij Sij = 0 \u2200 i, j. (22)\nOn the other hand, once S converges, according to the updating rule of Eq.(21), the converged solution S satisfies\nSij = Sij Kij\n(KS+ \u03b1S+ \u03b2 2 E) ij\n, (23)\nwhich can be written as [\u2212Kij +(KS+ \u03b1S+ \u03b2\n2 E) ij ]Sij =\n0. This is identical to Eq.(22). Thus the converged solution satisfies the KKT condition."}, {"heading": "5. Experiments", "text": "We perform the proposed algorithms on nie datasets. We do both semi-supervised learning and clustering on these datasets. We evaluate the proposed iterative LLE learning algorithm(\u00a73) and sparse similarity learning algorithm(\u00a74), and then show the embedding results from our approach.\nDataset These data sets come from a wide range of domains, including three face datasets AT&T, umist and yale (Georghiades et al., 2001), two digit datasets mnist (Lecun et al., 1998) and binalpha 1, two image scene datasets Caltec101(Caltec) (Dueck & Frey, 2007) and MSRC (Lee & Grauman, 2009), and two text datasets Newsgroup2, Reuters3. Table 1 summarizes the characteristics of them.\n1http://www.kyb.tuebingen.mpg.de/ssl-book/ benchmarks.html\n2http://people.csail.mit.edu/jrennie/20Newsgroups/ 3http://www.daviddlewis.com/resources/testcollections/\nreuters21578/\nWe show both the iterative LLE (algorithm 1 in \u00a73) and the sparse similarity learning algorithm (\u00a74) results. Given original kernel K0, S is obtained from 1- time running of sparse similarity learning algorithm in \u00a74. Then we obtain the final embedding resultsY after repeating out iterative LLE algorithm for 4 times. For step 1 of algorithm 1(\u00a73), we use kernel constructed from Eq.(18) for the subsequent iterations. For step 3 of algorithm 1(\u00a73), given current embedding Yt, we obtain the new kernel Kt+1 using choice (b2) with multiplicative combination in every iteration."}, {"heading": "5.1. Clustering Results", "text": "We use clustering algorithms to evaluate the learned Y in LLE. We compare three standard clustering algorithms: (1) normalized cut, which in the context of our iterative LLE, is simply K-means clustering on learned embeddingY; (2) spectral clustering (Ng et al., 2001), which is K-means clustering on embedding Y normalized onto unit sphere. (3) symmetric NMF, which runs\non the learned W in iterative LLE. All of results are the averages of 10 K-means clustering with random starts.\nWe use accuracy, normalized mutual information (NMI) and purity as the measurement of the clustering qualities and the results are shown in Table 2. We show the clustering results obtained from using (1) the original/input kernel (K0), (2) LLE1: results on learned Y after 1 LLE iteration. (3) LLE4: results on learned Y after 4 LLE iterations.\nFor image datasets, we use gaussian kernel K0ij = e\u2212\u03b3||xi\u2212xj || 2\n. For text datasets, we use linear kernel. We tune the graph construction parameter \u03b3 to obtain the best results from kernel K0. From Table 2, we observe that LLE1 and LLE4 consistently achieve better clustering results, as compared to the results obtained from original kernel K0."}, {"heading": "5.2. Semi-supervised learning results", "text": "We use K0, LLE1 and LLE4 results (learned W) as the input to run three semi-supervised methods: harmonic function(Zhu et al., 2003), local and global consistency(Zhou et al., 2004), green\u2019s function(Ding et al., 2007). We compare the classification accuracy of above three methods by using original kernel(K0) and the results obtained from LLE1 and LLE4 on 9 data sets. For all the methods and datasets, we randomly select 10%, 20% of labeled data for each class, and use the rest as unlabeled data. We do 10 fold and 5 fold cross validation, respectively. Finally, we report the average of the semi-supervised classification accuracy in Table 3. In all cases, we obtain higher classification accuracy by applying iterative LLE learning\nalgorithm (shown as LLE4 and LLE1)."}, {"heading": "5.3. Demonstration of embedding results", "text": "We demonstrate the advantages of iterative LLE learning algorithm (\u00a73) and sparse similarity learning algorithm (\u00a74) using two-dimensional visualization. We randomly select four digits from MNIST dataset (\u201c0\u201d, \u201c3\u201d, \u201c6\u201d, \u201c9\u201d). Given Gaussian Kernel as the input, the iterative LLE algorithm (\u00a73) and sparse similarity learning algorithm (\u00a74) are run. The other parameters are set as mentioned before. The embedding results obtained from original Gaussian Kernel K0, 4-time running of iterative LLE learning algorithm (LLE4) and 1-time running of W-learning algorithm (LLE1) are shown in Figs.(1(a),1(c),1(b)). In original\nresults from Gaussian Kernel, all images from different groups collapse together. For the results obtained from LLE4 and LLE1, the images from different groups are balanced and distributed more evenly. This indicates much better embedding results.\nInsights from experiment results. Overall, from initial/input kernel K0 to LLE1, LLE4, both clustering and semi-supervised learning results consistently improved. Comparing results obtained between LLE1 and initial/input kernel K0, the performance boost is from the learned W using the algorithm of \u00a74. Comparing results obtained between LLE1 and LLE4, the performance boost is from the iterative learning of LLE. From the statistics shown in Tables 2, 3, we observe that the boost from LLE1 to LLE4 is usually higher than that from K0 to LLE1, indicating that the iterative aspect contributes more."}, {"heading": "6. Conclusion", "text": "In summary, the main contribution of our paper is in three-fold. (1) We show that an improved Ylearning formulation of LLE is identical to normalized cut spectral clustering. (2) We present an improved W-learning algorithm that learns a nonnegative, sparse pairwise similarity from an input kernel function. (3) An iterative procedure of the above two steps is proposed to progressively refine/improve the solution. Experiments show that the iterative LLE incorporating (1,2,3) leads to better clustering and semisupervised learning results.\nAcknowledgments. This work is supported partially by NSF-CCF-0939187, NSF-CCF-0917274, NSF-DMS-0915228."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin and Niyogi,? \\Q2001\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2001}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["P.K. Chan", "M.Schlag", "J.Y. Zien"], "venue": "IEEE Trans. CADIntegrated Circuits and Systems,", "citeRegEx": "Chan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Chan et al\\.", "year": 1994}, {"title": "K-means clustering and principal component analysis", "author": ["C. Ding", "X. He"], "venue": "Int\u2019l Conf. Machine Learning (ICML),", "citeRegEx": "Ding and He,? \\Q2004\\E", "shortCiteRegEx": "Ding and He", "year": 2004}, {"title": "A learning framework using green\u2019s function and kernel regularization with application to recommender system", "author": ["C. Ding", "R. Jin", "T. Li", "H.D. Simon"], "venue": "In KDD, pp", "citeRegEx": "Ding et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2007}, {"title": "Convex and seminonnegative matrix factorizations", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ding et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2010}, {"title": "Hessian eigenmaps: New locally linear embedding techniques for high-dimensional data", "author": ["D.L. Donoho", "C. Grimes"], "venue": "Proceedings of National Academy of Science,", "citeRegEx": "Donoho and Grimes,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Grimes", "year": 2003}, {"title": "Non-metric affinity propagation for unsupervised image categorization", "author": ["D. Dueck", "B.J. Frey"], "venue": "In ICCV, pp. 1\u2013", "citeRegEx": "Dueck and Frey,? \\Q2007\\E", "shortCiteRegEx": "Dueck and Frey", "year": 2007}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "R-dimensional quadratic placement algorithm", "author": ["K.M. Hall"], "venue": "Management Science,", "citeRegEx": "Hall,? \\Q1971\\E", "shortCiteRegEx": "Hall", "year": 1971}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["J. Ham", "D.D. Lee", "S. Mika", "B. Scholkopf"], "venue": null, "citeRegEx": "Ham et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2004}, {"title": "Robust nonnegative matrix factorization using l21-norm", "author": ["D. Kong", "C.H.Q. Ding", "H. Huang"], "venue": "In CIKM, pp", "citeRegEx": "Kong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Foreground focus: Unsupervised learning from partially matching images", "author": ["Y.J. Lee", "K. Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lee and Grauman,? \\Q2009\\E", "shortCiteRegEx": "Lee and Grauman", "year": 2009}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q1997\\E", "shortCiteRegEx": "Shi and Malik", "year": 1997}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Label propagation through linear neighborhoods", "author": ["Wang", "Fei", "Zhang", "Changshui"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Spectral relaxation for K-means clustering", "author": ["H. Zha", "C. Ding", "M. Gu", "X. He", "H.D. Simon"], "venue": null, "citeRegEx": "Zha et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zha et al\\.", "year": 2001}, {"title": "Principal manifolds and nonlinear dimensionality reduction via tangent space alignment", "author": ["Z. Zhang", "Z. Zha"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Zhang and Zha,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Zha", "year": 2004}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schlkopf"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "Proc. Int\u2019l Conf. Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 16, "context": "Recently, there have been many algorithms proposed for nonlinear dimension reduction, which include Isomap (Tenenbaum et al., 2000), locally linear embedding (LLE) (Roweis & Saul, 2000), kernel-LLE (Ham et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 9, "context": ", 2000), locally linear embedding (LLE) (Roweis & Saul, 2000), kernel-LLE (Ham et al., 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations.", "startOffset": 74, "endOffset": 92}, {"referenceID": 8, "context": ", 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations.", "startOffset": 111, "endOffset": 146}, {"referenceID": 13, "context": "In past decades, many clustering algorithms have been proposed such as K-means, spectral clustering and its variants (Ng et al., 2001), normalized cut (Shi & Malik, 1997), ratio cut (Chan et al.", "startOffset": 117, "endOffset": 134}, {"referenceID": 1, "context": ", 2001), normalized cut (Shi & Malik, 1997), ratio cut (Chan et al., 1994), etc.", "startOffset": 55, "endOffset": 74}, {"referenceID": 1, "context": "Similarly, Laplacian embedding using coordinates with standard normalization VV = Ik is precisely the continuous relaxation of the cluster indicators of multi-way ratio cut spectral clustering (Chan et al., 1994); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means clustering (Zha et al.", "startOffset": 193, "endOffset": 212}, {"referenceID": 18, "context": ", 1994); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means clustering (Zha et al., 2001; Ding & He, 2004).", "startOffset": 179, "endOffset": 214}, {"referenceID": 4, "context": "The proof of this theorem is lengthy and is similar to that in (Ding et al., 2010; Kong et al., 2011).", "startOffset": 63, "endOffset": 101}, {"referenceID": 10, "context": "The proof of this theorem is lengthy and is similar to that in (Ding et al., 2010; Kong et al., 2011).", "startOffset": 63, "endOffset": 101}, {"referenceID": 7, "context": "Dataset These data sets come from a wide range of domains, including three face datasets AT&T, umist and yale (Georghiades et al., 2001), two digit datasets mnist (Lecun et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 11, "context": ", 2001), two digit datasets mnist (Lecun et al., 1998) and binalpha , two image scene datasets Caltec101(Caltec) (Dueck & Frey, 2007) and MSRC (Lee & Grauman, 2009), and two text datasets Newsgroup, Reuters.", "startOffset": 34, "endOffset": 54}, {"referenceID": 13, "context": "We compare three standard clustering algorithms: (1) normalized cut, which in the context of our iterative LLE, is simply K-means clustering on learned embeddingY; (2) spectral clustering (Ng et al., 2001), which is K-means clustering on embedding Y normalized onto unit sphere.", "startOffset": 188, "endOffset": 205}, {"referenceID": 21, "context": "Semi-supervised learning results We use K, LLE1 and LLE4 results (learned W) as the input to run three semi-supervised methods: harmonic function(Zhu et al., 2003), local and global consistency(Zhou et al.", "startOffset": 145, "endOffset": 163}, {"referenceID": 20, "context": ", 2003), local and global consistency(Zhou et al., 2004), green\u2019s function(Ding et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 3, "context": ", 2004), green\u2019s function(Ding et al., 2007).", "startOffset": 25, "endOffset": 44}], "year": 2012, "abstractText": "Locally Linear embedding (LLE) is a popular dimension reduction method. In this paper, we systematically improve the two main steps of LLE: (A) learning the graph weights W, and (B) learning the embedding Y. We propose a sparse nonnegative W learning algorithm. We propose a weighted formulation for learning Y and show the results are identical to normalized cuts spectral clustering. We further propose to iterate the two steps in LLE repeatedly to improve the results. Extensive experiment results show that iterative LLE algorithm significantly improves both classification and clustering results.", "creator": "LaTeX with hyperref package"}}}