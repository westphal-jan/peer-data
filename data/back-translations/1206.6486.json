{"id": "1206.6486", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "abstract": "Multi-task learning algorithms are typically designed on the assumption of a fixed, a priori known latent structure common to all tasks, but it is generally unclear which type of latent task structure is best suited to a particular multi-task learning problem. Ideally, the \"right\" latent task structure should be learned in a data-driven manner. We present a flexible, non-parametric Bayesian model that puts a mixture of factor analyzer structure on the tasks, and the non-parametric aspect makes the model meaningful enough to subsume many existing models of latent task structures (e.g. intermediate-regulated tasks, cluster tasks, low-level or linear / non-linear sub-space measures on tasks, etc.). In addition, it can also learn more general task structures that address the inadequacies of such models. We present a varying inference algorithm for our model. Experimental results on synthetic and real data sets to demonstrate both regression and fidelity, as well as the proposed methods of classification.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (229kb)", "http://arxiv.org/abs/1206.6486v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexandre passos", "piyush rai", "jacques wainer", "hal daum\u00e9 iii"], "accepted": true, "id": "1206.6486"}, "pdf": {"name": "1206.6486.pdf", "metadata": {"source": "CRF", "title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "authors": ["Alexandre Passos", "Piyush Rai", "Jacques Wainer"], "emails": ["apassos@cs.umass.edu", "piyush@cs.utah.edu", "wainer@ic.unicamp.br", "hal@umiacs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "Learning problems do not exist in a vacuum. Often one is tasked with developing not one, but many classifiers for different tasks. In these cases, there is often not enough data to learn a good model for each\n\u2020Contributed equally\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntask individually\u2014real-world examples are prioritizing email messages across many users\u2019 inboxes (Aberdeen et al., 2011) and recommending items to users on web sites (Ning & Karypis, 2010). In these settings it is advantageous to transfer or share information across tasks. Multitask learning (MTL) (Caruana, 1997) encompasses a range of techniques to share statistical strength across models for various tasks and allows learning even when the amount of labeled data for each individual task is very small. Most MTL methods achieve this improved performance either by assuming some notion of similarity across tasks\u2014for example, that all task parameters are drawn from a shared Gaussian prior (Chelba & Acero, 2006), have a cluster structure (Xue et al., 2007; Jacob & Bach, 2008), live on a low-dimensional subspace (Rai & Daume\u0301 III, 2010), share feature representations (Argyriou et al., 2007), or by modeling the task covariance matrix (Bonilla et al., 2007; Zhang & Yeung, 2010). Choosing the correct notion of task relatedness is crucial to the effectiveness of any MTL method. Incorrect assumptions can hurt performance and it is desirable to have a flexible model that can automatically adapt its assumptions for a given problem.\nMotivated by this, we propose a nonparametric Bayesian MTL model by representing the task parameters (e.g., the weight vectors for logistic regression models) as being generated from a nonparametric mixture of nonparametric factor analyzers. Parameters are shared only between tasks in the same cluster and, within each cluster, across a linear subspace that regularizes what is shared. Moreover, by virtue of this being a nonparametric model, various existing MTL models result as special cases of our model; for example, the weight vectors are drawn from a single shared Gaussian prior, or form clusters (equivalently, gener-\nated from a mixture of Gaussians), or live close to a subspace, etc. Our model can automatically interpolate between these assumptions as needed, providing the best fit to the given MTL problem.\nIn addition to offering a general framework for multitask learning, our proposed model also addresses several shortcomings of commonly used MTL models. For example, task clustering (Xue et al., 2007), which fits a full-covariance Gaussian mixture model over the weight vectors, is prone to overfitting on high dimensional problems as the number of learning tasks is usually much smaller than the dimensionality, making it difficult to estimate the covariance matrix. A model based on mixtures of factor analyzers, like ours, can deal with this issue by adaptively estimating the dimensionality of each component, using less parameters than in the full rank case. Likewise, models based on task subspaces (Zhang et al., 2006; Rai & Daume\u0301 III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks. Our model, based on a mixture of subspaces, circumvents these issues by allowing different groups of weight vectors to live in different subspaces when grouping all together them would not fit the data well. One can also view our model as allowing the sharing of statistical strengths at two levels: (1) by exploiting the cluster structure, and (2) by additionally exploiting the subspace structure within each cluster."}, {"heading": "2. Background", "text": "In the context of MTL, since the task relatedness structure is usually unknown, the standard solution is to try many different models, covering many similarity assumptions, with many settings of complexity for each model, and choose the one according to some model selection criteria. In this paper, we take a nonparametric Bayesian approach to this problem (using the Dirichlet Process and the Indian Buffet Process as building blocks) such that the appropriate MTL model capturing the correct task relatedness structure and the model complexity for that model will be learned in a data-driven manner side-stepping the model selection issues."}, {"heading": "2.1. The Dirichlet Process", "text": "The Dirichlet Process (DP) is a prior distribution over discrete distributions (Ferguson, 1973). Discreteness implies that if one draws samples from a distribution drawn from the DP, the samples will cluster: new samples take the same value as older samples with some positive probability. A DP is defined by two parameters: a concentration parameter \u03b1 and a base measure\nG0. The sampling process defining the DP draws the first sample from the base measure G0. Each subsequent sample would take on a new value drawn from G0 with a probability proportional to \u03b1, or reuse a previously drawn value with probability proportional to the number of samples having that value. This property makes it suitable as a prior for effectively infinite mixture models, where the number of mixtures can grow as new samples are observed. Our mixture of factor analyzers based MTL model uses the DP to model the mixture components so we do not need to specify their number a priori."}, {"heading": "2.2. The Indian Buffet Process", "text": "The Indian Buffet Process (IBP) (Griffiths & Ghahramani, 2006) and the closely related Beta Process (Thibaux & Jordan, 2007) define a distribution on a collection of sparse binary vectors of unbounded size (or, equivalently, on sparse binary matrices with one dimension fixed but the other being unbounded). Such sparse structures are commonly used in applications such as sparse factor analysis (Paisley & Carin, 2009) where we want to decompose a data matrix X such that each observation Xn \u2208 RD is represented as a sparse combination of a set of K \u226a D basis vectors (or factors) but K is not specified a priori. The generative story in the finite case is (assuming a linear Gaussian model generation):\nXn \u223c Nor(\u039bbn, \u03c3 2 XI) \u039bk \u223c Nor(0, \u03c3 2I)\nbkn \u223c Ber(\u03c0k)\n\u03c0k \u223c Bet(\u03b1/K, 1)\nIn the above, \u039b is a matrix consisting of K columns (the factors) and the factor combination is defined by the sparse binary vector bn of size K. For the more general case of factor analysis, factor combination weights are sparse real-valued vectors, so the model is of the form Xn = \u039b(sn\u2299 bn)+E, where sn is a real-valued vector of the same size as bn (Paisley & Carin, 2009) and can be given a Gaussian prior, and \u2299 is the elementwise product. Our mixture of factor analyzers based MTL model uses the IBP/Beta Process to model each factor analyzer so we do not need to specify the number of factors K a priori."}, {"heading": "3. Mixture of Factor Analyzers based Generative Model for MTL", "text": "Our proposed model assumes that the parameters (i.e., the weight vector) of each task are sampled from a mixture of factor analyzers (Ghahramani & Beal, 2000). Note that our model is defined over latent weight vectors whereas the standard mixture of factor analyzers\nis commonly defined to model observed data.\nThe weight vector \u03b8t is a sparse linear combination of K basis vectors represented by the columns of \u039bt (each column is a \u201cbasis task\u201d). The combination weights are given by ft \u2208 R\nK which we represent as st \u2299 bt where st is a real valued vector and bt is a binary valued vector, both of size K. Our model uses a BetaBernoulli/IBP prior on bt to determine K, the number of factors in each factor analyzer. The {\u00b5t,\u039bt} pair for each task is drawn from a DP, also giving the tasks a clustering property, and there will be a finite number F \u2264 T of distinct factor analyzers. Finally, \u03b5t \u223c Nor(0, 1 \u03c32 I) represents task-specific noise.\nFigure 1 shows a graphical depiction of our model and Figure 2 shows the generative story for the linear regression case . The DP base measure G0 is a product of two Gaussian priors for \u00b5t,\u039bt. In our nonparametric Bayesian model, F and K need not be known a priori ; these are inferred from the data.\nFor classification, the only change is that the first line in the generative model becomes Yt,i \u223c Ber(sig(\u03b8t \u00b7\nXt,i)), where sig(x) = 1\n1+exp(\u2212x) is the logistic func-\ntion and Ber is the Bernoulli distribution.\nA number of existing multitask learning models arise as special cases of our model as it nicely interpolates between some different and useful scenarios, depending on the actual inferred values of F and K, for a given multitask learning dataset:\n\u2022 Shared Gaussian Prior(F=1,K=0): (Chelba & Acero, 2006). This corresponds to a single factor analyzer modeling either a diagonal or fullrank Gaussian as the prior. \u2022 Cluster-based Assumption(F > 1,K=0): (Xue et al., 2007; Jacob & Bach, 2008). This corresponds to a mixture of identity-covariance or full-rank Gaussians as the prior. \u2022 Linear Subspace Assumption(F=1,K < D): (Zhang et al., 2006; Rai & Daume\u0301 III, 2010). This corresponds to a single factor analyzer with less than full rank. Note that this is also equivalent to the matrix \u0398 = {\u03b81, . . . , \u03b8T } being a rankK matrix (Argyriou et al., 2007). \u2022 Nonlinear Manifold Assumption: A mixture of linear subspaces allows modeling a nonlinear subspace (Chen et al., 2010) and can capture the case when the weight vectors live on a nonlinear manifold (Ghosn & Bengio, 2003; Agarwal et al., 2010). Moreover, in our model, the manifold\u2019s intrinsic dimensionality can be different in different parts of the ambient space (since we do not restrict K to be the same for each factor analyzer).\nOur nonparametric Bayesian model can interpolate between these cases as appropriate for a given dataset, without changing the model structure or hyperparameters. From a non-probabilistic analogy, our model can be seen as doing dictionary learning/sparse coding (Aharon et al., 2010) over the latent weight vectors (albeit, using an undercomplete dictionary setting since we assume K \u2264 min{T,D}). The model learns M dictionaries of basis tasks (one dictionary per group/cluster of tasks, and M inferred from the data) and tasks within each cluster are expressed as a\nsparse linear combination of elements from that dictionary. Our model can also be generalized further, e.g., by replacing the Gaussian prior on the low-dimensional latent task representations st \u2208 R\nK by a prior of the form P (st+1|st), one can even relax the exchangeability assumption of tasks within each group, and have tasks that are evolving with time."}, {"heading": "3.1. Variational inference", "text": "As this model is infinite and combinatorial in nature, exact inference is intractable and sampling-based inference may take too long to converge (Doshi-Velez et al., 2009; Blei & Jordan, 2006). Hence, we employ a variational mean-field algorithm to perform inference in this model. To do so, we lower-bound the marginal log-probability of Y given X using a fully factored approximating distribution Q over the model parameters \u03b8, \u00b5,\u039b, z, b, s:\nlogP (Y |X) = logEP [P (Y |X, \u03b8, \u00b5,\u039b, z, b, s)]\n\u2265 EQ[logP (Y |X)]\n\u2212EQ[logQ(Y |X)].\nTo do so, we approximate the DP and the IBP with a tractable distribution Q. For the DP we use a finite stick-breaking distribution, based on the infinite stick-breaking representation of the DP (Blei & Jordan, 2006). In this representation, we introduce, for each \u03b8t, a multinomial random variable zt that indexes the infinite set of possible mixture parameters \u00b5 and \u039b. The zt vector is nonzero on its i-th component with probability \u03c6i \u220f\nj<i(1 \u2212 \u03c6j), where \u03c6 is an infinite set of independent Bet(1, \u03b11) random variables (Bet is the Beta distribution). A finite approximation to the DP is obtained by setting a given \u03c6i to 1, which sets the probability of zj for j > i necessarily to 0. While there is a similar stick-breaking construction to the IBP (Teh et al., 2007), it is not in the exponential family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al., 2009).\nThe distribution we are approximating then (for the linear regression case) is shown in Figure 3 (top). The stick-breaking distribution SBP which is the prior for zt is such that P (zt= i) = \u03c6i \u220f j<i(1\u2212 \u03c6j).\nIn our variational distribution, we set the number of factor analyzers in the truncated stick-breaking representation to a hyperparameter F and the number of factors in each such analyzer to a truncation level hyperparameter K. After inference, if the truncation levels are set high enough, most factor analyzers (and factors within each factor analyzer) will not be used, effectively approximating the property of the infinite model that only a small finite number of components\nis ever used to model a finite data set. It is worthwhile to note that while the solution found by the variational approximation is necessarily finite and with complexity bounded by the truncation parameters, it will still implicitly perform model selection. Therefore, more often than not, it will concentrate most of its posterior mass on models with less complexity than the truncation parameters suggest. Ishwaran & James (2001) present two theorems to help choose these truncation levels, as using smaller values of F and K (particularly K, as the update equations are quadratic in K) can lead to significant savings of computing time (in our experiments, we simply set these to min{D,T}) which we found to be sufficient).\nOur approximating Q distribution is shown in Figure 3 (bottom). For the linear regression case, we treat P (Y |\u03b8) by lower-bounding it directly, without introducing an approximating distribution for Y . In the case of logistic regression, we use the lower bound by (Jaakkola & Jordan, 1996) that allows us to integrate out the logistic function.\nApart from approximating the DP with the truncated stick-breaking prior, approximating the IBP with a set of symmetric, finite Beta distributed variables, and lower-bounding the logistic function with a quadratic, all the computations involved in deriving the variational lower bound are straightforward exponentialfamily computations. Note that for Q we could use more general covariances instead of the identity matrices. In practice, we found that this did not improve classification performance, and it would imply on a significantly higher computational cost. Another less\nexpensive option however would be to use the same hyperparameter for each feature, i.e., a spherical (instead of diagonal) covariance \u03c42I which would require optimizing w.r.t. a single hyperparameter \u03c4 . The variational parameter updates are:\n\u03b3f,1 = 1 + \u2211\nt\n\u03bdzt,f\n\u03b3f,2 = \u03b11 + \u2211\nt\n\u2211\nj>f\n\u03bdzt,j\n\u03bdzt,f \u221d exp ( \u03a8(\u03b3f,1)\u2212\u03a8(\u03b3f,1 + \u03b3f,2)\n+ \u2211\nj<f\n(\u03a8(\u03b3j,2)\u2212\u03a8(\u03b3j,1 + \u03b3j,2))\n+EQ[logP (\u03b8t|zt = f)] )\n\u03c1f,k,1 = \u03b12 K + \u2211\nt\n\u03bdbt,f,k , \u03c1f,k,2 = 1 + \u2211\nt\n(1\u2212 \u03bdbt,f,k)\n\u03bdbt,f,k = sig ( \u03a8(\u03c1f,k,1)\u2212\u03a8(\u03c1f,k,2)\n+ \u03c3\u03bdzt,f\n([\n\u03bd\u03b8t \u2212 \u03bd\u00b5f \u2212 (\u03bdst,i + 1)\u03bd\u039bf,i\n\u2212 \u2211\nj 6=i\n\u03bdst,j\u03bdbt,f,j\u03bd\u039bf,j\n]T\n\u03bd\u039bf,i\u03bdst,i\n\u2212 D\n2 \u03bd2st,i \u2212\nDF\n2\n))\n\u03bdst,i = (1 + \u03c3\u03bdzt,f \u03bdbt,f,i(D + ||\u03bd\u039bf,i || 2))\u22121\n\u03bdzt,f\u03c3 (( \u03bd\u03b8t \u2212 \u03bd\u00b5f\n\u2212 0.5 \u2211\nj 6=i\n\u03bdst,f,j\u03bdbt,f,j\u03bd\u039bf,j ) T \u03bd\u039bf,i\u03bdbt,f,i\n)\n\u03bd\u00b5f =\n\u2211\nt \u03bdzt,f\u03c3(\u03bd\u03b8t \u2212 \u03bd\u039bf (\u03bdst,f \u2299 \u03bdbt,f ))\n1 + \u03c3 \u2211\nt \u03bdzt,f\n\u03bd\u039bf,i = ( 1 + \u03c3 \u2211\nt\n\u03bdzt,f \u03bdbt,f,i(1 + \u03bd 2 st,f,i\n) )\u22121\n\u03c3 \u2211\nt\n\u03bdzt,f \u03bdst,f,i\u03bdbt,f,i\n(\n\u03bd\u03b8t \u2212 \u03bd\u00b5f\n\u2212 1\n2\n\u2211\nj 6=i\n\u03bdst,f,j\u03bdbt,f,j\u03bd\u039bf,j\n)\nIn the above \u03a8 denotes the digamma function. While it is possible to update \u03bd\u03b8t analytically, the update requires inverting a matrix, and in our experiment this matrix was often ill-conditioned, so we updated \u03bd\u03b8t by optimizing the lower bound with the L-BFGS-B optimizer (Zhu et al., 1997). The optimizer is run until convergence at each iteration, warm-started with the previous value. We note that it could be replaced by any other optimizer, including gradient methods, with no changes in the above equations.\nThe complete derivations are provided in the the supplementary material.\nFor regression, the gradient of the lower bound with respect to \u03bd\u03b8t is\n\u2207L(\u03bd\u03b8t) = \u03c3 \u2211\nf\n\u03bdzt,f ( \u03bd\u03b8t \u2212 \u03bd\u00b5f \u2212 \u03bd\u039bf (\u03bdst,f \u2299 \u03bdbt,f ) )\n+\nNt \u2211\ni\n(\nYt,iXt,i \u2212Xt,iX T t,i\u03bd\u03b8t\n)\n.\nFor classification the gradient is similar, the main difference being that there is an extra factor in the Xt,iX T t,i\u03bd\u03b8t term involving the variational parameter for the lower bound of the logistic function.\nWe also optimize the lower bound w.r.t the precision parameter \u03c3 to obtain an empirical Bayes estimate:\n1 \u03c3 = \u2211\nt\n\u2211\nf\n\u03bdzt,f\n(\n||\u03bd\u03b8t \u2212 \u03bd\u00b5f \u2212 \u03bd\u039bf (\u03bdst,f \u2299 \u03bdbt,f )|| 2\nKDF\n+\n\u2211\ni \u03bdbt,f,i(\u03bd 2 st,f,i + ||\u03bd\u039bf,i || 2)\nKF +\n1\nK\n)\n.\nThe hyperparameters \u03b11 and \u03b12 are held fixed and can be optimized by cross-validation. We initialize the inference process with \u03bd\u03b8t set to the maximum likelihood solution to each task\u2019s regression or classification problem. Then we alternate updating all other parameters to convergence and updating \u03bd\u03b8t given the other parameters. The value of \u03bd\u03b8t , and hence the regression or classification accuracy, usually stabilizes after the first couple of iterations, and the only changes observed are further improvements to the lower bound. This matches behavior observed in Ando & Zhang (2005). All our experiments were run on three iterations."}, {"heading": "4. Experiments", "text": "We present results on both synthetic and real-world datasets, and on linear regression and classification settings. As a sanity check to show that our model can learn the underlying latent task structures correctly, we generated a synthetic data consisting of 5 clusters of tasks. Each cluster consists of 10 binary classification tasks, having 100 examples each. We used a 50/50 split for train/test data. Each task is represented by a weight vector of length D = 20. Figure 4 (left) shows the true correlation structure of the tasks and Figure 4 (right) shows the recovered structure by our model: it correctly infers the correct number (5) of clusters. Our model resulted in a classification accuracy of 83.2%, whereas independently learned tasks resulted in an accuracy of 79.2%.\nOur next set of experiments compare our model with a number of baseline methods on several synthetic and real-world multitask regression and multitask classification problems. Our baselines include:\n\u2022 Independently learned tasks - STL: assumes the tasks are independent (no information sharing).\n\u2022 Multitask Feature Learning - MTFL: assumes the tasks share a common set of features (Argyriou et al., 2007).\n\u2022 Shared Gaussian prior over the weight vectors - PRIOR (Chelba & Acero, 2006): assumes the tasks are drawn from a shared Gaussian prior with a unknown but fixed mean and covariance.\n\u2022 Single shared subspace - RANK (Zhang et al., 2006; Rai & Daume\u0301 III, 2010): assumes the tasks live close to a linear subspace (also equivalent to the matrix of the weight vector being low-rank).\n\u2022 DP mixture model based task clustering - DPMTL (Xue et al., 2007): assumes the weight vectors are generated from a mixture model, each component being a full-rank Gaussian.\n\u2022 Learning with Whom to Share - LWS (Kang et al., 2011). It is an integer programming based method that learn the task grouping structure (with pre-specified number of groups) and encourages the tasks within each group to share features.\nOf these baselines, MTFL and LWS were used for regression problems only since the publicly available implementations are for regression. In the experiments, we would refer to our model as MFA-MTL (Mixture of Factor Analyzers for MultiTask Learning). In all our experiments, we set the hyperparameters \u03b11 = 1 and \u03b12 = 5, as these values performed reasonably in preliminary experiments. The truncation level for the DP can be chosen to be equal to the number of tasks T , and for the IBP, to be the minimum of T and the number of features D in the data. This is often more than necessary and in most of our experiments, much smaller truncation levels were found to be sufficient.\nFor our multitask regression experiments, we compared MFA-MTL with STL, MTFL, and LWS (we skip the other baselines as they performed comparably or worse than MTFL/LWS). For this experiment,\nwe used three datasets - one synthetic dataset used in (Kang et al., 2011), and two real-world datasets used commonly in the multitask learning literature: (1) School: This dataset consists of the examination scores of 15362 students from 139 schools in London. Each school is a task so there are a total of 139 tasks for this dataset. (2) Computer: This dataset consists of a survey of 190 students about the chances of purchasing 20 different personal computers. There are a total of 190 tasks, 20 examples per task, and 13 features per example. For the synthetic data, we followed the similar procedure for train/test split as used by (Kang et al., 2011). For School and Computer datasets, we split the data equally into training and test set and further only used 20% of the training data (training set deliberately kept small as is often the case with multitask learning problems in practice). The average mean squared errors (i.e., across tasks) in predicting the responses by each method are shown in Table 1. As shown in Table 1, MFA-MTL outperforms the other baselines on all the datasets. Moreover, for the synthetic data, we found that it also inferred the number of task groups (3) correctly (the LWS baseline needs this number to be specified - we ran it with the ground truth). On the school and computer datasets, MFAMTL outperforms STL and LWS and does slightly better than MTFL. For LWS on these two datasets, we report the best results as obtained by varying the number of groups from 1 to 20.\nWe next experiment with the classification setting. For this, we chose two datasets: (1) Landmine: The landmine detection dataset is a subset of the dataset used in the symmetric multitask learning experiment by (Xue et al., 2007). It contains 19 classification tasks and the tasks are known to be clustered for this data. (2) 20ng: We did the standard training/test split of 20 Newsgroups for multitask learning, following Raina et al. (2006), and used a 50/50 split for the landmine data. The classification accuracies reported by our\nmodel and the various baselines on landmine and 20 Newsgroups datasets are shown in Table 2. As shown in Table 2, our method outperforms the various baselines. We note that 3 of them (PRIOR, RANK, and DP-MTL), which are methods proposed in prior work, are special cases of our model (as discussed in Section 3). In particular, RANK performs worse than our method, potentially because all weight vectors share the same subspace which may not be desirable if not all the tasks are related with each other. DP-MTL performs worse than our method, potentially because it fits a full-rank Gaussian for each mixture component and is especially prone to overfit if the number of tasks is smaller than the number of features.\nFinally, we investigated the behavior of different algorithms in the small training data regimes. For this, we varied the amount of training examples per task (for landmine data, we varied the fraction from 20% to 100%; for 20 Newsgroup, we varied the number of examples from 20 to 100). Results are shown in Figure 5. To uncrowd the figure, we compare only with STL and DP-MTL (the best performing baseline). In the small data regimes, our algorithm performs better as compared to both STL and DP-MTL. Another important aspect of an MTL algorithm is its asymptotic behavior in the limit of large training data per task. For this experiment, we compared MFA-MTL with STL on the school multitask regression dataset by providing each algorithm the complete training data. MFA-MTL resulted in an MSE of 261.4 as compared to STL which gave an MSE of 271.1. Therefore our algorithm tends to do comparably (in fact, marginally better) to independently learned tasks even when the amount of training data per task is sufficiently large."}, {"heading": "5. Related Work", "text": "Apart from the prior work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008). Their model assumes that tasks can be partitioned into groups and tasks within each group share a kernel. Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou\net al., 2007) (one of the baselines we used in our experiments) that assumes all tasks share the common kernel. In (Kumar & Daume\u0301 III, 2012), the authors assume that there is single set of task basis vectors (i.e., a task dictionary) and each task is a sparse combination of these basis vectors. In their model, the number of basis vectors shared between two tasks (i.e., their \u201coverlap\u201d) can be seen as the pairwise task similarity. In Kang et al. (2011), the authors proposed a model based on the assumption that the tasks exist in groups and the tasks within each group share features, which is again similar in spirit to our work (this model was one of our baselines in the experiments). In contrast, the generative model we presented in this paper offers a number of advantages over these models such as the ability to deal with missing data in a principled manner, doing automatic model complexity control in a nonparametric Bayesian setting, and being flexible enough to subsume these and many other notions as task relatedness used in multitask learning.\nAmong other related work, Canini et al. (2010) propose hierarchical Dirichlet process models as good models for human categorical learning. The idea is that one can model transfer learning by assuming that people unsupervisedly learn subgroups of known classes and use these groups to refine the knowledge of new classes by sharing subgroups via a hierarchical Dirichlet process. Our model can be seen as a discriminative analog of their generative model, where aspects of the task parameter\u2014instead of the distribution of the test examples\u2014are shared among similar tasks and the sharing structure is discovered automatically."}, {"heading": "6. Future Work and Discussion", "text": "We proposed and evaluated a nonparametric Bayesian multitask learning model that usefully interpolates between many different previously proposed models for estimating task parameters of multiple related learning problems, such as a shared Gaussian prior (Chelba & Acero, 2006), a clustering structure (Xue et al., 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al., 2010), etc. We presented a variational mean-field algorithm for this model that exhibits competitive results on a set of synthetic as well as real-world multitask learning datasets. The proposed model, by using the flexibility afforded by nonparametric Bayesian techniques, requires only minimal assumptions to be applied to any given multitask learning problem. A possible future work is studying a hierarchical Dirichlet process variant of this model where different tasks are allowed to share exactly the\nsame \u03b8 parameters, which might be beneficial in cases where training data is especially sparse or the tasks are more strongly clustered."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Emma Tosch for helpful comments in the preparation of this manuscript. Piyush Rai was support by DARPA CSSG Grant P-1076-113840. Alexandre Passos was supported in part by the CIIR, in part by IARPA via DoI/NBC contract #D11PC20152, in part by UPenn NSF medium IIS-080384, in part by DARPA) Machine Reading Program under AFRL prime contract no. FA8750-09-C-0181. The U.S. Government is authorized to reproduce and distribute reprint for Governmental purposes notwithstanding any copyright annotation thereon. Any opinions, findings, and conclusion or recommendations expressed in this material are the authors\u2019 and do not necessarily reflect those of the sponsors."}], "references": [{"title": "The learning behind gmail priority inbox", "author": ["D. Aberdeen", "O. Pacovsky", "A. Slater"], "venue": "In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds,", "citeRegEx": "Aberdeen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aberdeen et al\\.", "year": 2011}, {"title": "Learning multiple tasks using manifold regularization", "author": ["Agarwal", "Arvind", "Gerber", "Samuel", "Daum\u00e9 III", "Hal"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": null, "citeRegEx": "Aharon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2010}, {"title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "author": ["R.K. Ando", "T. Zhang"], "venue": null, "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "In NIPS,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["Argyriou", "Andreas", "Maurer", "Pontil", "Massimiliano"], "venue": "In ECML,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2006}, {"title": "Multi-task gaussian process prediction", "author": ["Bonilla", "Edwin V", "Chai", "Kian Ming A", "Williams", "Christopher K. I"], "venue": "In NIPS,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Modeling transfer learning in human categorization with the hierarchical Dirichlet process", "author": ["K.R. Canini", "M.M. Shashkov", "T.L. Griffiths"], "venue": "In ICML,", "citeRegEx": "Canini et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Canini et al\\.", "year": 2010}, {"title": "Multitask Learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["C. Chelba", "A. Acero"], "venue": "Computer Speech & Language,", "citeRegEx": "Chelba and Acero,? \\Q2006\\E", "shortCiteRegEx": "Chelba and Acero", "year": 2006}, {"title": "Compressive Sensing on Manifolds Using a Nonparametric Mixture of Factor Analyzers: Algorithm and Performance Bounds", "author": ["M. Chen", "J. Silva", "J. Paisley", "C. Wang", "D. Dunson", "L. Carin"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi-Velez", "K.T. Miller", "J. Van Gael", "Y.W. Teh", "G. Unit"], "venue": "In AISTATS,", "citeRegEx": "Doshi.Velez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2009}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Variational inference for bayesian mixtures of factor analysers", "author": ["Ghahramani", "Zoubin", "Beal", "Matthew J"], "venue": "In NIPS,", "citeRegEx": "Ghahramani et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2000}, {"title": "Infinite Latent Feature Models and the Indian Buffet Process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ishwaran and James,? \\Q2001\\E", "shortCiteRegEx": "Ishwaran and James", "year": 2001}, {"title": "A variational approach to bayesian logistic regression models and their extensions", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In AISTATS,", "citeRegEx": "Jaakkola and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1996}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["L. Jacob", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "Jacob and Bach,? \\Q2008\\E", "shortCiteRegEx": "Jacob and Bach", "year": 2008}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daum\u00e9 III"], "venue": "In ICML,", "citeRegEx": "Kumar and III,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III", "year": 2012}, {"title": "Multi-task learning for recommender systems", "author": ["X. Ning", "G. Karypis"], "venue": null, "citeRegEx": "Ning and Karypis,? \\Q2010\\E", "shortCiteRegEx": "Ning and Karypis", "year": 2010}, {"title": "Nonparametric factor analysis with beta process priors", "author": ["Paisley", "John", "Carin", "Lawrence"], "venue": "In ICML,", "citeRegEx": "Paisley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2009}, {"title": "Infinite predictor subspace models for multitask learning", "author": ["P. Rai", "H. Daum\u00e9 III"], "venue": "In AISTATS,", "citeRegEx": "Rai and III,? \\Q2010\\E", "shortCiteRegEx": "Rai and III", "year": 2010}, {"title": "Constructing informative priors using transfer learning", "author": ["R. Raina", "A.Y. Ng", "D. Koller"], "venue": "In ICML,", "citeRegEx": "Raina et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2006}, {"title": "Stick-breaking construction for the Indian buffet process", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Hierarchical beta processes and the indian buffet process", "author": ["Thibaux", "Romain", "Jordan", "Michael I"], "venue": "In AISTATS,", "citeRegEx": "Thibaux et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Thibaux et al\\.", "year": 2007}, {"title": "Multitask Learning for Classification with Dirichlet Process Priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D. Yeung"], "venue": "In UAI,", "citeRegEx": "Zhang and Yeung,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Yeung", "year": 2010}, {"title": "L-BFGSB: Fortran subroutines for large-scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Transactions on Mathetmatical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "task individually\u2014real-world examples are prioritizing email messages across many users\u2019 inboxes (Aberdeen et al., 2011) and recommending items to users on web sites (Ning & Karypis, 2010).", "startOffset": 97, "endOffset": 120}, {"referenceID": 27, "context": "Most MTL methods achieve this improved performance either by assuming some notion of similarity across tasks\u2014for example, that all task parameters are drawn from a shared Gaussian prior (Chelba & Acero, 2006), have a cluster structure (Xue et al., 2007; Jacob & Bach, 2008), live on a low-dimensional subspace (Rai & Daum\u00e9 III, 2010), share feature representations (Argyriou et al.", "startOffset": 235, "endOffset": 273}, {"referenceID": 4, "context": ", 2007; Jacob & Bach, 2008), live on a low-dimensional subspace (Rai & Daum\u00e9 III, 2010), share feature representations (Argyriou et al., 2007), or by modeling the task covariance matrix (Bonilla et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 7, "context": ", 2007), or by modeling the task covariance matrix (Bonilla et al., 2007; Zhang & Yeung, 2010).", "startOffset": 51, "endOffset": 94}, {"referenceID": 27, "context": "For example, task clustering (Xue et al., 2007), which fits a full-covariance Gaussian mixture model over the weight vectors, is prone to overfitting on high dimensional problems as the number of learning tasks is usually much smaller than the dimensionality, making it difficult to estimate the covariance matrix.", "startOffset": 29, "endOffset": 47}, {"referenceID": 28, "context": "Likewise, models based on task subspaces (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks.", "startOffset": 41, "endOffset": 106}, {"referenceID": 1, "context": "Likewise, models based on task subspaces (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks.", "startOffset": 41, "endOffset": 106}, {"referenceID": 13, "context": "The Dirichlet Process (DP) is a prior distribution over discrete distributions (Ferguson, 1973).", "startOffset": 79, "endOffset": 95}, {"referenceID": 27, "context": "\u2022 Cluster-based Assumption(F > 1,K=0): (Xue et al., 2007; Jacob & Bach, 2008).", "startOffset": 39, "endOffset": 77}, {"referenceID": 28, "context": "\u2022 Linear Subspace Assumption(F=1,K < D): (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010).", "startOffset": 41, "endOffset": 84}, {"referenceID": 4, "context": ", \u03b8T } being a rankK matrix (Argyriou et al., 2007).", "startOffset": 28, "endOffset": 51}, {"referenceID": 11, "context": "\u2022 Nonlinear Manifold Assumption: A mixture of linear subspaces allows modeling a nonlinear subspace (Chen et al., 2010) and can capture the case when the weight vectors live on a nonlinear manifold (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 1, "context": ", 2010) and can capture the case when the weight vectors live on a nonlinear manifold (Ghosn & Bengio, 2003; Agarwal et al., 2010).", "startOffset": 86, "endOffset": 130}, {"referenceID": 2, "context": "From a non-probabilistic analogy, our model can be seen as doing dictionary learning/sparse coding (Aharon et al., 2010) over the latent weight vectors (albeit, using an undercomplete dictionary setting since we assume K \u2264 min{T,D}).", "startOffset": 99, "endOffset": 120}, {"referenceID": 12, "context": "As this model is infinite and combinatorial in nature, exact inference is intractable and sampling-based inference may take too long to converge (Doshi-Velez et al., 2009; Blei & Jordan, 2006).", "startOffset": 145, "endOffset": 192}, {"referenceID": 25, "context": "While there is a similar stick-breaking construction to the IBP (Teh et al., 2007), it is not in the exponential family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": ", 2007), it is not in the exponential family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al., 2009).", "startOffset": 153, "endOffset": 179}, {"referenceID": 30, "context": "While it is possible to update \u03bd\u03b8t analytically, the update requires inverting a matrix, and in our experiment this matrix was often ill-conditioned, so we updated \u03bd\u03b8t by optimizing the lower bound with the L-BFGS-B optimizer (Zhu et al., 1997).", "startOffset": 226, "endOffset": 244}, {"referenceID": 4, "context": "\u2022 Multitask Feature Learning - MTFL: assumes the tasks share a common set of features (Argyriou et al., 2007).", "startOffset": 86, "endOffset": 109}, {"referenceID": 28, "context": "\u2022 Single shared subspace - RANK (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010): assumes the tasks live close to a linear subspace (also equivalent to the matrix of the weight vector being low-rank).", "startOffset": 32, "endOffset": 75}, {"referenceID": 27, "context": "\u2022 DP mixture model based task clustering - DPMTL (Xue et al., 2007): assumes the weight vectors are generated from a mixture model, each component being a full-rank Gaussian.", "startOffset": 49, "endOffset": 67}, {"referenceID": 19, "context": "\u2022 Learning with Whom to Share - LWS (Kang et al., 2011).", "startOffset": 36, "endOffset": 55}, {"referenceID": 19, "context": "we used three datasets - one synthetic dataset used in (Kang et al., 2011), and two real-world datasets used commonly in the multitask learning literature: (1) School: This dataset consists of the examination scores of 15362 students from 139 schools in London.", "startOffset": 55, "endOffset": 74}, {"referenceID": 19, "context": "For the synthetic data, we followed the similar procedure for train/test split as used by (Kang et al., 2011).", "startOffset": 90, "endOffset": 109}, {"referenceID": 27, "context": "For this, we chose two datasets: (1) Landmine: The landmine detection dataset is a subset of the dataset used in the symmetric multitask learning experiment by (Xue et al., 2007).", "startOffset": 160, "endOffset": 178}, {"referenceID": 24, "context": "(2) 20ng: We did the standard training/test split of 20 Newsgroups for multitask learning, following Raina et al. (2006), and used a 50/50 split for the landmine data.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "Apart from the prior work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008).", "startOffset": 149, "endOffset": 172}, {"referenceID": 4, "context": "Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou et al., 2007) (one of the baselines we used in our experiments) that assumes all tasks share the common kernel.", "startOffset": 83, "endOffset": 106}, {"referenceID": 4, "context": "Apart from the prior work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008). Their model assumes that tasks can be partitioned into groups and tasks within each group share a kernel. Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou et al., 2007) (one of the baselines we used in our experiments) that assumes all tasks share the common kernel. In (Kumar & Daum\u00e9 III, 2012), the authors assume that there is single set of task basis vectors (i.e., a task dictionary) and each task is a sparse combination of these basis vectors. In their model, the number of basis vectors shared between two tasks (i.e., their \u201coverlap\u201d) can be seen as the pairwise task similarity. In Kang et al. (2011), the authors proposed a model based on the assumption that the tasks exist in groups and the tasks within each group share features, which is again similar in spirit to our work (this model was one of our baselines in the experiments).", "startOffset": 150, "endOffset": 829}, {"referenceID": 8, "context": "Among other related work, Canini et al. (2010) propose hierarchical Dirichlet process models as good models for human categorical learning.", "startOffset": 26, "endOffset": 47}, {"referenceID": 27, "context": "We proposed and evaluated a nonparametric Bayesian multitask learning model that usefully interpolates between many different previously proposed models for estimating task parameters of multiple related learning problems, such as a shared Gaussian prior (Chelba & Acero, 2006), a clustering structure (Xue et al., 2007), reduced dimensionality (Argyriou et al.", "startOffset": 302, "endOffset": 320}, {"referenceID": 4, "context": ", 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 28, "context": ", 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 1, "context": ", 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al., 2010), etc.", "startOffset": 28, "endOffset": 72}], "year": 2012, "abstractText": "Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the \u201cright\u201d latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, meanregularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and realworld datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}