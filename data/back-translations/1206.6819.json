{"id": "1206.6819", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Robustness of Most Probable Explanations", "abstract": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instance with the highest probability given the current evidence. In this paper, we discuss the problem of finding the robustness of the MPE under individual parameter changes. Specifically, we ask the question: How much variation of a single network parameter can we afford to apply while leaving the MPE unchanged? We will describe a method that is the first of its kind to calculate this response for each parameter in the Bayesian network variable in time O (n exp (w)), where n is the number of network variables and w is its tree width.", "histories": [["v1", "Wed, 27 Jun 2012 15:39:15 GMT  (215kb)", "http://arxiv.org/abs/1206.6819v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hei chan", "adnan darwiche"], "accepted": false, "id": "1206.6819"}, "pdf": {"name": "1206.6819.pdf", "metadata": {"source": "CRF", "title": "On the Robustness of Most Probable Explanations", "authors": ["Hei Chan", "Adnan Darwiche"], "emails": ["chanhe@eecs.oregonstate.edu", "darwiche@cs.ucla.edu"], "sections": [{"heading": "1 Introduction", "text": "A Most Probable Explanation (MPE) in a Bayesian network is a complete variable instantiation which has the highest probability given current evidence [1]. Given an MPE solution for some piece of evidence, we concern ourselves in this paper with the following question: What is the amount of change one can apply to some network parameter without changing this current MPE solution? Our goal is then to deduce robustness conditions for MPE under single parameter changes. This problem falls into the realm of sensitivity analysis. Here, we treat the Bayesian network as a system which accepts network parameters as inputs, and produces the MPE as an output. Our goal is then to characterize conditions under which the output is guaranteed to be the same (or different) given a change in some input value.\nThis question is very useful in a number of application areas, including what-if analysis, in addition to the\n\u2217This work was completed while Hei Chan was at UCLA.\ndesign and debugging of Bayesian networks. For an example, consider Figure 1 which depicts a Bayesian network for diagnosing potential problems in a car. Suppose now that we have the following evidence: the dashboard test and the lights test came out positive, while the engine test came out negative. When we compute the MPE in this case, we get a scenario in which all car components are working normally. This seems to be counterintuitive as we expect the most likely scenario to indicate at least that the engine is not working. The methods developed in this paper can be used to debug this scenario. In particular, we will be able to identify the amount of change in each network parameter which is necessary to produce a different MPE solution. We will revisit this example later in the paper and discuss the specific recommendations computed by our proposed algorithm.\nPrevious results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9]. Because probability values are continuous, while MPE solutions are discrete instantiations, abrupt changes in MPE so-\nlutions may occur when we change a parameter value. This makes the sensitivity analysis of MPE quite different from previous work on the subject.\nThis paper is structured as follows. We first provide the formal definition of Bayesian networks and MPE in Section 2. Then in Section 3, we explore the relationship between the MPE and a single network parameter, and also look into the case where we change co-varying parameters in Section 4. We deduce that the relationship can be captured by two constants that are independent of the given parameter. Next in Section 5, we show how we can compute these constants for all network parameters, allowing us to automatically identify robustness conditions for MPE, and provide a complexity analysis of our proposed approach. Finally, we show some concrete examples in Section 6, and then extend our analysis to evidence change in Section 7."}, {"heading": "2 Most Probable Explanations", "text": "We will formally define most probable explanations in this section, but we specify some of our notational conventions first. We will denote variables by uppercase letters (X) and their values by lowercase letters (x). Sets of variables will be denoted by bold-face uppercase letters (X) and their instantiations by bold-face lowercase letters (x). For variable X and value x, we will often write x instead of X = x, and hence, Pr(x) instead of Pr(X = x). For a binary variable X with values true and false, we will use x to denote X = true and x\u0304 to denote X = false. Therefore, Pr(X = true) and Pr(x) represent the same probability in this case. Similarly, Pr(X = false) and Pr(x\u0304) represent the same probability. Finally, for instantiation x of variables X, we will write \u00acx to mean the set of all instantiations x? 6= x of variables X. For example, we will write Pr(x) + Pr(\u00acx) = 1.\nA Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional probability tables (CPTs), with one CPT for each network variable [1]. In the CPT for variable X with parents U, we define a network parameter \u03b8x|u for every family instantiation xu such that \u03b8x|u = Pr(x | u).\nGiven the network parameters, we can compute the probability of a complete variable instantiation x as follows:\nPr(x) = \u220f\nxu\u223cx \u03b8x|u, (1)\nwhere \u223c is the compatibility relation between instantiations, i.e., xu \u223c x means that xu is compatible with x). Now assume that we are given evidence e. A most probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the\nhighest probability [1]:\nMPE (e) def = argmax\nx\u223ce Pr(x) (2)\n= argmax x\u223ce \u220f xu\u223cx \u03b8x|u.\nWe note that the MPE may not be a unique instantiation as there can be multiple instantiations with the same highest probability. Therefore, we will define MPE (e) as a set of instantiations instead of just one instantiation. Moreover, we will sometimes use MPE (e,\u00acx) to denote the MPE instantiations that are consistent with e but inconsistent with x.\nIn the following discussion, we will find it necessary to distinguish between the MPE identity and the MPE probability. By the MPE identity, we mean the set of instantiations having the highest probability. By the MPE probability, we mean the probability assumed by a most likely instantiation, which is denoted by:\nMPEp(e) def = max\nx\u223ce Pr(x). (3)\nThis distinction is important when discussing robustness conditions for MPE since a change in some network parameter may change the MPE probability, but not the MPE identity."}, {"heading": "3 Relation Between MPE and Network Parameters", "text": "Assume that we are given evidence e and are able to find its MPE, MPE (e). We now address the following question: How much change can we apply to a network parameter \u03b8x|u without changing the MPE identity of evidence e? To simplify the discussion, we will first assume that we can change this parameter without changing any co-varying parameters, such as \u03b8x\u0304|u, but we will relax this assumption later.\nOur solution to this problem is based on some basic observations which we discuss next. In particular, we observe that complete variable instantiations x which are consistent with e can be divided into two categories:\n\u2022 Those that are consistent with xu. From Equation 1, the probability of each such instantiation x is a linear function of the parameter \u03b8x|u.\n\u2022 Those that are inconsistent with xu. From Equation 1, the probability of each such instantiation x is a constant which is independent of the parameter \u03b8x|u.\nLet us denote the first set of instantiations by \u03a3e,xu and the second set by \u03a3e,\u00ac(xu). We can then conclude that:\n\u2022 The set of most likely instantiations in \u03a3e,xu remains unchanged regardless of the value of parameter \u03b8x|u, even though the probability of such instantiations may change according to the value of \u03b8x|u. This is because the probability of each instantiation x \u2208 \u03a3e,xu is a linear function of the value of \u03b8x|u: Pr(x) = r \u00b7 \u03b8x|u, where r is a coefficient independent of the value of \u03b8x|u. Therefore, the relative probabilities among instantiations in \u03a3e,xu remain unchanged as we change the value of \u03b8x|u. Note also that the most likely instantiations in this set \u03a3e,xu are just MPE (e, xu) and their probability isMPEp(e, xu). Therefore, if we define:\nr(e, xu) def = \u2202MPEp(e, xu) \u2202\u03b8x|u , (4)\nwe will then have:\nPr(x) = r(e, xu) \u00b7 \u03b8x|u,\nfor any x \u2208 MPE (e, xu).\n\u2022 Both the identity and probability of the most likely instantiations in \u03a3e,\u00ac(xu) are independent of the value of parameter \u03b8x|u. This is because the probability of each instantiation x \u2208 \u03a3e,\u00ac(xu) is independent of the value of \u03b8x|u. Note that the most likely instantiation in this set \u03a3e,\u00ac(xu) is just MPE (e,\u00ac(xu)). We will define the probability of such an instantiation as:\nk(e, xu) def = MPEp(e,\u00ac(xu)). (5)\nGiven the above observations, MPE (e) will either be MPE (e, xu), MPE (e,\u00ac(xu)), or their union, depending on the value of parameter \u03b8x|u:\nMPE (e)\n=  MPE (e, xu), if r(e, xu) \u00b7 \u03b8x|u > k(e, xu);MPE (e,\u00ac(xu)), if r(e, xu) \u00b7 \u03b8x|u < k(e, xu);MPE (e, xu) \u222aMPE (e,\u00ac(xu)), otherwise. Moreover, the MPE probability can always be expressed as:\nMPEp(e) = max(r(e, xu) \u00b7 \u03b8x|u, k(e, xu)).\nFigure 2 plots the relation between the MPE probability MPEp(e) and the value of parameter \u03b8x|u. According to the figure, if \u03b8x|u > k(e, xu)/r(e, xu), i.e., region A of the plot, then we have MPE (e) = MPE (e, xu), and thus the MPE solutions are consistent with xu. Moreover, the MPE identity will remain unchanged as long as the value of \u03b8x|u remains greater than k(e, xu)/r(e, xu).\nMPEPr(e)\n\u03b8x|u\nk(e,xu)\nk(e,xu) / r(e,xu) 0\nRegion A Region B\nFigure 2: A plot of the relation between the MPE probability MPEp(e) and the value of parameter \u03b8x|u.\nOn the other hand, if \u03b8x|u < k(e, xu)/r(e, xu), i.e., region B of the plot, then we have MPE (e) = MPE (e,\u00ac(xu)), and thus the MPE solutions are inconsistent with xu. Moreover, the MPE identity and probability will remain unchanged as long as the value of \u03b8x|u remains less than k(e, xu)/r(e, xu).\nTherefore, \u03b8x|u = k(e, xu)/r(e, xu) is the point where there is a change in the MPE identity if we were to change the value of parameter \u03b8x|u. At this point, MPE (e) = MPE (e, xu)\u222aMPE (e,\u00ac(xu)) and we have both MPE solutions consistent with xu and MPE solutions inconsistent with xu. There are no other points where there is a change in the MPE identity. If we are able to find the constants r(e, xu) and k(e, xu) for the network parameter \u03b8x|u, we can then compute robustness conditions for MPE with respect to changes in this parameter."}, {"heading": "4 Dealing with Co-Varying Parameters", "text": "The above analysis assumed that we can change a parameter \u03b8x|u without needing to change any other parameters in the network. This is not realistic though in the context of Bayesian networks, where co-varying parameters need to add up to 1 for the network to induce a valid probability distribution. For example, if variable X has two values, x and x\u0304, we must always have:\n\u03b8x|u + \u03b8x\u0304|u = 1.\nWe will therefore extend the analysis conducted in the previous section to account for the simultaneously changes in the co-varying parameters. We will restrict our attention to binary variables to simplify the discussion, but our results can be easily extended to multivalued variables as we will show later.\nIn particular, assuming that we are changing parame-\nters \u03b8x|u and \u03b8x\u0304|u simultaneously for a binary variable X, we can now categorize all network instantiations which are consistent with evidence e into three groups, depending on whether they are consistent with xu, x\u0304u, or \u00acu. Moreover, the most likely instantiations in each group are just MPE (e, xu), MPE (e, x\u0304u), and MPE (e,\u00acu) respectively. Therefore, if x \u2208 MPE (e), then:\nPr(x) =  r(e, xu) \u00b7 \u03b8x|u, if x \u2208 MPE (e, xu);r(e, x\u0304u) \u00b7 \u03b8x\u0304|u, if x \u2208 MPE (e, x\u0304u); k(e,u), if x \u2208 MPE (e,\u00acu);\nwhere:\nr(e, xu) = \u2202MPEp(e, xu)\n\u2202\u03b8x|u ;\nr(e, x\u0304u) = \u2202MPEp(e, x\u0304u)\n\u2202\u03b8x\u0304|u ;\nk(e,u) = MPEp(e,\u00acu);\nand the MPE probability is:\nMPEp(e) = max(r(e, xu) \u00b7\u03b8x|u, r(e, x\u0304u) \u00b7\u03b8x\u0304|u, k(e,u)).\nTherefore, changing the co-varying parameters \u03b8x|u and \u03b8x\u0304|u will not affect the identity of either MPE (e, xu) orMPE (e, x\u0304u), nor will it affect the identity or probability of MPE (e,\u00acu).\nThe robustness condition of an MPE solution can now be summarized as follows:\n\u2022 If an MPE solution is consistent with xu, it remains a solution as long as the following inequalities are true:\nr(e, xu) \u00b7 \u03b8x|u \u2265 r(e, x\u0304u) \u00b7 \u03b8x\u0304|u; r(e, xu) \u00b7 \u03b8x|u \u2265 k(e,u).\n\u2022 If an MPE solution is consistent with x\u0304u, it remains a solution as long as the following inequalities are true:\nr(e, x\u0304u) \u00b7 \u03b8x\u0304|u \u2265 r(e, xu) \u00b7 \u03b8x|u; r(e, x\u0304u) \u00b7 \u03b8x\u0304|u \u2265 k(e,u).\n\u2022 If an MPE solution is consistent with \u00acu, it remains a solution as long as the following inequalities are true:\nk(e,u) \u2265 r(e, xu) \u00b7 \u03b8x|u; k(e,u) \u2265 r(e, x\u0304u) \u00b7 \u03b8x\u0304|u.\nWe note here that one can easily deduce whether an MPE solution is consistent with xu, x\u0304u, or \u00acu since it is a complete variable instantiation.\nTherefore, all we need are the constants r(e, xu) and k(e,u) for each network parameter \u03b8x|u in order to define robustness conditions for MPE. The constants k(e,u) can be easily computed from the constants r(e, xu) by observing the following:\nk(e,u) = MPEp(e,\u00acu) = max\nu?:u? 6=u MPEp(e,u?)\n= max xu?:u? 6=u MPEp(e, xu?)\n= max xu?:u? 6=u r(e, xu?) \u00b7 \u03b8x|u? . (6)\nAs the algorithm we will describe later computes the r(e, xu) constants for all family instantiations xu, the algorithm will then allow us to compute all the k(e,u) constants as well.\nAs a simple example, for the Bayesian network whose CPTs are shown in Figure 3, the current MPE solution without any evidence is A = a,B = b\u0304, and has probability .4. For the parameters in the CPT of B, we can compute the corresponding r(e, xu) constants. In particular, we have r(e, ba) = r(e, b\u0304a) = r(e, ba\u0304) = r(e, b\u0304a\u0304) = .5 in this case. The k(e,u) constants can also be computed as k(e, a) = .3 and k(e, a\u0304) = .4. Given these constants, we can easily compute the amount of change we can apply to covarying parameters, say \u03b8b|a and \u03b8b\u0304|a, such that the MPE solution remains the same. The conditions we must satisfy are:\nr(e, b\u0304a) \u00b7 \u03b8b\u0304|a \u2265 r(e, ba) \u00b7 \u03b8b|a; r(e, b\u0304a) \u00b7 \u03b8b\u0304|a \u2265 k(e, a).\nThis leads to \u03b8b\u0304|a \u2265 \u03b8b|a and \u03b8b\u0304|a \u2265 .6. Therefore, the current MPE solution will remain so as long as \u03b8b\u0304|a \u2265 .6, which has a current value of .8.\nWe close this section by pointing out that our robustness equations can be extended to multi-valued variables as follows. If variable X has values x1, . . . , xj , with j > 2, then each of the conditions we showed earlier will consist of j inequalities instead of just two. For example, if an MPE solution is consistent with x1u, it remains a solution as long as the following inequalities are true:\nr(e, x1u) \u00b7 \u03b8x1|u \u2265 r(e, x ?u) \u00b7 \u03b8x?|u for all x? 6= x1; r(e, x1u) \u00b7 \u03b8x1|u \u2265 k(e,u)."}, {"heading": "5 Computing Robustness Conditions", "text": "In this section, we will develop an algorithm for computing the constants r(e, xu) for all network parameters \u03b8x|u. In particular, we will show that they can be computed in time and space which is O(n exp(w)), where n is the number of network variables and w is its treewidth."}, {"heading": "5.1 Arithmetic Circuits", "text": "Our algorithm for computing the r(e, xu) constants is based on an arithmetic circuit representation of the Bayesian network [10]. Figure 4 depicts an arithmetic circuit for a small network consisting of two binary nodes, A and B, shown in Figure 3. An arithmetic circuit is a rooted DAG, where each internal node corresponds to multiplication (\u2217) or addition (+), and each leaf node corresponds either to a network parameter \u03b8x|u or an evidence indicator \u03bbx; see Figure 4. Operationally, the circuit can be used to compute the probability of any evidence e by evaluating the circuit while setting the evidence indicator \u03bbx to 0 if x contradicts e and setting it to 1 otherwise. Semantically though, the arithmetic circuit is simply a factored representation of an exponential-size function that captures the network distribution. For example, the circuit in Figure 4 is simply a factored representation of the following function:\n\u03bba\u03bbb\u03b8a\u03b8b|a + \u03bba\u03bbb\u0304\u03b8a\u03b8b\u0304|a + \u03bba\u0304\u03bbb\u03b8a\u0304\u03b8b|a\u0304 + \u03bba\u0304\u03bbb\u0304\u03b8a\u0304\u03b8b\u0304|a\u0304.\nThis function, called the network polynomial, includes a term for each instantiation x of network variables, where the term is simply a product of the network parameters and evidence indicators which are consistent\nwith x. Moreover, the term for x evaluates to the probability value Pr(e,x) when the evidence indicators are set according to e. Note that this function is multilinear. Therefore, a corresponding arithmetic circuit will have the property that two sub-circuits that feed into the same multiplication node will never contain a common variable. This property is important for some of the following developments."}, {"heading": "5.2 Complete Sub-Circuits and Their", "text": "Coefficients\nEach term in the network polynomial corresponds to a complete sub-circuit in the arithmetic circuit. A complete sub-circuit can be constructed recursively from the root, by including all children of each multiplication node, and exactly one child of each addition node. The bold lines in Figure 4 depict a complete sub-circuit, corresponding to the term \u03bba\u03bbb\u0304\u03b8a\u03b8b\u0304|a. In fact, it is easy to check that the circuit in Figure 4 has four complete sub-circuits, corresponding to the four terms in the network polynomial.\nA key observation about complete sub-circuits is that if a network parameter is included in a complete subcircuit, there is a unique path from the root to this parameter in this sub-circuit, even though there may be multiple paths from the root to this parameter in the original arithmetic circuit. This path is important as one can relate the value of the term corresponding to the sub-circuit and the parameter value by simply traversing the path as we show next.\nConsider now a complete sub-circuit which includes a network parameter \u03b8x|u and let \u03b1 be the unique path in this sub-circuit connecting the root to parameter \u03b8x|u. We will now define the sub-circuit coefficient w.r.t. \u03b8x|u, denoted as r, in terms of the path \u03b1 such that r \u00b7\u03b8x|u is just the value of the term corresponding to the sub-circuit.\nLet \u03a3 be the set of all multiplication nodes on this path \u03b1. The sub-circuit coefficient w.r.t. \u03b8x|u is defined as the product of all children of nodes in \u03a3 which are themselves not on the path \u03b1. Consider for example the complete sub-circuit highlighted in Figure 4 and the path from the root to the network parameter \u03b8a. The coefficient w.r.t. \u03b8a is r = \u03bba\u03bbb\u0304\u03b8b\u0304|a. Moreover, r \u00b7\u03b8a = \u03bba\u03bbb\u0304\u03b8a\u03b8b\u0304|a, which is the term corresponding to the sub-circuit."}, {"heading": "5.3 Maximizer Circuits", "text": "An arithmetic circuit can be easily modified into a maximizer circuit to compute the MPE solutions, by simply replacing each addition node with a maximization node; see Figure 5. This corresponds to a circuit\nthat computes the value of the maximum term in a network polynomial, instead of adding up the values of these terms. The value of the root will thus be the MPE probability MPEp(e). The maximizer circuit in Figure 5 is evaluated under evidence A = a, leading to an MPE probability of .4.\nTo recover an MPE solution from a maximizer circuit, all we need to do is construct the MPE sub-circuit recursively from the root, by including all children of each multiplication node, and one child c for each maximization node v, such that v and c have the same value; see Figure 5. The MPE sub-circuit will then correspond to an MPE solution. Moreover, if a parameter \u03b8x|u is in the MPE sub-circuit, and the sub-circuit coefficient w.r.t \u03b8x|u is r, then we have r \u00b7 \u03b8x|u as the probability of MPE, MPEp(e).\nConsider Figure 5 and the highlighted MPE subcircuit, evaluated under evidence A = a. The term corresponding to this sub-circuit is A = a,B = b\u0304, which is therefore an MPE solution. Moreover, we have two parameters in this sub-circuit, \u03b8a and \u03b8b\u0304|a, with coefficients .8 = (1)(.8) and .5 = (.5)(1)(1) respectively. Therefore, the MPE probability can be obtained by multiplying any of these coefficients with the corresponding parameter value, as (.8)\u03b8a = (.8)(.5) = .4 and (.5)\u03b8b\u0304|a = (.5)(.8) = .4.\n5.4 Computing r(e, xu)\nSuppose now that our goal is to compute MPE (e, xu) for some network parameter \u03b8x|u. Suppose further that \u03b11, . . . , \u03b1m are all the complete sub-circuits that\nAlgorithm 1 D-MAXC(M: a maximizer circuit, e: evidence) 1: evaluate the circuit M under evidence e; afterwards\nthe value of each node v is p[v] 2: r[v]\u2190 1 for root v of circuit M 3: r[v]\u2190 0 for all non-root nodes v in circuitM 4: for non-leaf nodes v (parents before children) do 5: if node v is a maximization node then 6: r[c]\u2190 max(r[c], r[v]) for each child c of node v 7: if node v is a multiplication node then 8: r[c]\u2190 max (r[c], r[v] \u220f c?\np[c?]) for each child c of node v, where c? are the other children of node v\ninclude \u03b8x|u. Moreover, let x1, . . . ,xm be the instantiations corresponding to these sub-circuits and let r1, . . . , rm be their corresponding coefficients w.r.t. \u03b8x|u. It then follows that the probabilities of these instantiations are r1 \u00b7 \u03b8x|u, . . . , rm \u00b7 \u03b8x|u respectively. Moreover, it follows that:\nMPEp(e, xu) = max i r1 \u00b7 \u03b8x|u, . . . , rm \u00b7 \u03b8x|u,\nand hence, from Equation 4:\n\u2202MPEp(e, xu) \u2202\u03b8x|u = r(e, xu) = max i r1, . . . , rm.\nTherefore, if we can compute the maximum of these coefficients, then we have computed the constant r(e, xu).\nAlgorithm 1 provides a procedure which evaluates the maximizer circuit and then traverses it top-down, parents before children, computing simultaneously the constants r(e, xu) for all network parameters. The procedure maintains an additional register value r[.] for each node in the circuit, and updates these registers as it visits nodes. When the procedure terminates, it is guaranteed that the register value r[\u03b8x|u] will be the constant r(e, xu). We will also see later that the register value r[\u03bbx] is also a constant which provides valuable information for the MPE solutions. Figure 6 depicts an example of this procedure.\nAlgorithm 1 can be modelled as the all-pairs shortest path procedure, with edge v \u2212\u2192 c having weight 0 = \u2212 ln 1 if v is a maximization node, and weight \u2212 ln\u03c0 if v is a multiplication node, where \u03c0 is the product of the values of the other children c? 6= c of node v. The length of the shortest path from the root to the network parameter \u03b8x|u is then \u2212 ln r(e, xu). It should be clear that the time and space complexity of the above algorithm is linear in the number of circuit nodes.1 It is well known that we can compile a\n1More precisely, this algorithm is linear in the number of circuit nodes only if the number of children per multiplication node is bounded. If not, one can use a technique which gives a linear complexity by simply storing two additional bits with each multiplication node [10].\ncircuit for any Bayesian network in O(n exp(w)) time and space, where n is the number of network variables and w is its treewidth [10]. Therefore, all constants r(e, xu) can be computed with the same complexity.\nWe close this section by pointing out that one can in principle use the jointree algorithm to compute MPEp(e, xu) = r(e, xu) \u00b7 \u03b8x|u for all family instantiations xu with the above complexity. In particular, by replacing summation with maximization in the jointree algorithm, one obtainsMPEp(e, c) for each cluster instantiation c. Projecting on the families XU in cluster C, one can then obtain MPEp(e, xu) for all family instantiations xu, which is all we need to compute robustness conditions for MPE.2 Our method above, however, is more general for two reasons:\n\u2022 The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].\n\u2022 The constants computed by the algorithm for the evidence indicators can be used to answer additional MPE queries, which results after variations on the current evidence. This will be discussed in Section 7."}, {"heading": "6 Example", "text": "We now go back to the example network in Figure 1, and compute robustness conditions for the current\n2However, in case some of the parameters are equal to 0, one needs to use a special jointree [11].\nMPE solution using the inequalities we obtain in Section 4, and an implementation of Algorithm 1. After going through the CPT of each variable, our procedure found nine possible parameter changes that would produce a different MPE solution, as shown in Figure 7. From these nine suggested changes, only three changes make sense from a qualitative point of view:\n\u2022 Decreasing the probability that the ignition is working from .9925 to at most .9133. (6th row)\n\u2022 Decreasing the probability that the engine is working given both the battery and the ignition are working from .97 to at most .9108. (1st row)\n\u2022 Decreasing the false-negative rate of the engine test from .09 to at most .0285. (9th row)\nIf we apply the first parameter change, we get a new MPE solution in which both the ignition and the engine are not working. If we apply either the second or third parameter change, we get a new MPE solution in which the engine is not working."}, {"heading": "7 MPE under Evidence Change", "text": "We have discussed in Section 5.2 the notion of a complete sub-circuit and its coefficient with respect to a network parameter \u03b8x|u which is included in the subcircuit. In particular, we have shown how each subcircuit corresponds to a term in the network polynomial, and that if a complete sub-circuit has coefficient r with respect to parameter \u03b8x|u, then r \u00b7 \u03b8x|u will be the value of the term corresponding to this sub-circuit.\nThe notion of a sub-circuit coefficient can be extended to evidence indicators as well. In particular, if a complete sub-circuit has coefficient r with respect to an evidence indicator \u03bbx which is included in the sub-circuit,\nthen r \u00b7\u03bbx will be the value of the term corresponding to this sub-circuit.\nSuppose now that \u03b11, . . . , \u03b1m are all the complete subcircuits that include \u03bbx. Moreover, let x1, . . . ,xm be the terms corresponding to these sub-circuits and let r1, . . . , rm be their corresponding coefficients with respect to \u03bbx. It then follows that the values of these terms are r1 \u00b7\u03bbx, . . . , rm \u00b7\u03bbx respectively. Moreover, it follows that:\nMPEp(e\u2212X,x) = max i r1, . . . , rm,\nwhere e \u2212 X denotes the new evidence after having retracted the value of variable X from e (if X \u2208 E, otherwise e\u2212X = e). Therefore, if we can compute the maximum of these coefficients, then we have computed MPEp(e \u2212 X,x). Note, however, that Algorithm 1 already computes the maximum of these coefficients for each \u03bbx as the evidence indicators are nodes in the maximizer circuit as well, and therefore the register value r[\u03bbx] gives us MPEp(e\u2212X,x) for every variable X and value x.\nConsider for example the circuit in Figure 6, and the coefficients computed by Algorithm 1 for the four evidence indicators. According to the above analysis, these coefficients have the following meanings:\n\u03bbx e\u2212X,x r[\u03bbx] = MPEp(e\u2212X,x) \u03bba a .4 \u03bba\u0304 a\u0304 .3 \u03bbb a, b .1 \u03bbb\u0304 a, b\u0304 .4\nFor example, the second row above tells us that the MPE probability would be .3 if the evidence was A = a\u0304 instead of A = a. In general, if we have n variables, we then have O(n) variations on the current evidence of the form e \u2212 X,x. The MPE probability of all of these variations are immediately available from the coefficients with respect to the evidence indicators.\nThe computation of these coefficients allows us to deduce the MPE identity after evidence retraction. In particular, suppose that variable X is set as x in evidence e, andMPEp(e) \u2265 MPEp(e\u2212X,x?) for all other values x? 6= x. We can then conclude that MPEp(e) = MPEp(e \u2212 X). Moreover, MPE (e) = MPE (e \u2212 X) if MPEp(e) > MPEp(e \u2212 X,x?) for all other values x? 6= x, or MPE (e) \u2282 MPE (e \u2212 X) if there exists some x? 6= x such that MPEp(e) = MPEp(e\u2212X,x?). Therefore, the current MPE solutions will remain so even after we retract X = x from the evidence. This means that X = x is not integral in the determination of the current MPE solutions given the other evidence, i.e., e\u2212X.\nThe result above also has implications on the identification of multiple MPE solutions given evidence e. In particular, suppose that variable X is not set in evidence e, then:\n\u2022 If the coefficients for the evidence indicators \u03bbx and \u03bbx\u0304 are equal, we must have both MPE solutions with X = x and MPE solutions with X = x\u0304. In fact, the coefficients must both equal the MPE probability MPEp(e) in this case.\n\u2022 If the coefficient for the evidence indicator \u03bbx is larger than the coefficient for the evidence indicator \u03bbx\u0304, then every MPE solution must have X = x.\nIn the example above, we have r[\u03bbb\u0304] > r[\u03bbb], suggesting that every MPE solution must have b\u0304 in this case."}, {"heading": "8 Conclusion", "text": "We considered in this paper the problem of finding robustness conditions for MPE solutions of a Bayesian network under single parameter changes. We were able to solve this problem by identifying some interesting relationships between an MPE solution and the network parameters. In particular, we found that the robustness condition of an MPE solution under a single parameter change depends on two constants that are independent of the parameter value. We also proposed a method for computing such constants and, therefore, the robustness conditions of MPE in O(n exp(w)) time and space, where n is the number of network variables and w is the network treewidth. Our algorithm is the first of its kind for ensuring the robustness of MPE solutions under parameter changes in a Bayesian network."}, {"heading": "Acknowledgments", "text": "This work has been partially supported by Air Force grant #FA9550-05-1-0075-P00002 and JPL/NASA grant #1272258. We would also like to thank James Park for reviewing this paper and making the observation on how to compute k(e,u) in Equation 6."}], "references": [{"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "When do numbers really matter", "author": ["Hei Chan", "Adnan Darwiche"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Sensitivity analysis in Bayesian networks: From single to multiple parameters", "author": ["Hei Chan", "Adnan Darwiche"], "venue": "In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Sensitivity analysis in discrete Bayesian networks", "author": ["Enrique Castillo", "Jos\u00e9 Manuel Guti\u00e9rrez", "Ali S. Hadi"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part A (Systems and Humans),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Using sensitivity analysis for efficient quantification of a belief network", "author": ["Veerle M.H. Coup\u00e9", "Niels Peek", "Jaap Ottenkamp", "J. Dik F. Habbema"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Making sensitivity analysis computationally efficient", "author": ["Uffe Kj\u00e6rulff", "Linda C. van der Gaag"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Sensitivity analysis for probability assessments in Bayesian networks", "author": ["Kathryn B. Laskey"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "The sensitivity of belief networks to imprecise probabilities: An experimental investigation", "author": ["Malcolm Pradhan", "Max Henrion", "Gregory Provan", "Brendan Del Favero", "Kurt Huang"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Analysing sensitivity data from probabilistic networks", "author": ["Linda C. van der Gaag", "Silja Renooij"], "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A differential approach to inference in Bayesian networks", "author": ["Adnan Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A differential semantics for jointree algorithms", "author": ["James D. Park", "Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Compiling Bayesian networks with local structure", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Compiling relational Bayesian networks for exact inference", "author": ["Mark Chavira", "Adnan Darwiche", "Manfred Jaeger"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A Most Probable Explanation (MPE) in a Bayesian network is a complete variable instantiation which has the highest probability given current evidence [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 2, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 3, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 4, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 5, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 6, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 7, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 8, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 0, "context": "A Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional probability tables (CPTs), with one CPT for each network variable [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "A most probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the highest probability [1]:", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "Our algorithm for computing the r(e, xu) constants is based on an arithmetic circuit representation of the Bayesian network [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "If not, one can use a technique which gives a linear complexity by simply storing two additional bits with each multiplication node [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "circuit for any Bayesian network in O(n exp(w)) time and space, where n is the number of network variables and w is its treewidth [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "\u2022 The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 12, "context": "\u2022 The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 10, "context": "However, in case some of the parameters are equal to 0, one needs to use a special jointree [11].", "startOffset": 92, "endOffset": 96}], "year": 2006, "abstractText": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for all parameters in the Bayesian network in time O(n exp(w)), where n is the number of network variables and w is its treewidth.", "creator": "TeX"}}}