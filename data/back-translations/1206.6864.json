{"id": "1206.6864", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Infinite Hidden Relational Models", "abstract": "In many cases, it makes sense to model a relationship symmetrically without implying a specific directional relationship. Consider the classic example of a referral system in which the evaluation of an item by a user should be symmetrically dependent on the attributes of both the user and the item. Attributes of (known) relationships are also relevant for predicting unit attributes and predicting attributes of new relationships. In referral systems, the use of relational attributes is often referred to as collaborative filtering. Also, in many applications, one might prefer to model the collaborative effect in a symmetrical way. In this paper, we present a completely symmetrical model. The key innovation is that for each object (or object) we introduce an infinitely dimensional latent variable as part of a Dirichlet Process (DP) model. We discuss conclusions in the model that are based on a Gibbs pre-signed Chinese process, i.e. a three-signatory Chinese model.", "histories": [["v1", "Wed, 27 Jun 2012 16:28:29 GMT  (280kb)", "http://arxiv.org/abs/1206.6864v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI cs.DB cs.LG", "authors": ["zhao xu", "volker tresp", "kai yu", "hans-peter kriegel"], "accepted": false, "id": "1206.6864"}, "pdf": {"name": "1206.6864.pdf", "metadata": {"source": "CRF", "title": "Infinite Hidden Relational Models", "authors": ["Zhao Xu", "Volker Tresp", "Kai Yu"], "emails": [], "sections": [{"heading": null, "text": "Relational learning analyzes the probabilistic constraints between the attributes of entities and relationships. We extend the expressiveness of relational models by introducing for each entity (or object) an infinitedimensional latent variable as part of a Dirichlet process (DP) mixture model. We discuss inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. We extended the Chinese restaurant process to be applicable to relational modeling. We discuss how information is propagated in the network of latent variables, reducing the necessity for extensive structural learning. In the context of a recommendation engine our approach realizes a principled solution for recommendations based on features of items, features of users and relational information. Our approach is evaluated in three applications: a recommendation system based on the MovieLens data set, the prediction of gene function using relational information and a medical recommendation system."}, {"heading": "1 Introduction", "text": "Relational learning (Dzeroski & Lavrac, 2001; Raedt & Kersting, 2003; Wrobel, 2001; Friedman et al., 1999) is an object oriented approach that clearly distinguishes between entities (e.g, objects), relationships and their respective attributes and represents an area of growing interest in machine learning. A simple example of a relational system is a recommendation system: based on the attributes of two entities, i.e. of the user and the item, one wants to predict relational attributes like the preference (rating, willingness to purchase, ...) of this user for this item. In many circumstances, the\nattributes of the entities are rather weak predictors in which case one can exploit the known relationship attributes to predict unknown entity or relationship attributes (Yu et al., 2004). In recommendation systems, the latter situation is often referred to as collaborative filtering. Although the unique identifier of an entity to which a relationship exists might often be used as a feature, it has the disadvantage that it does not permit the generalization to new entities. From this point of view it is more advantageous to introduce a latent variable representing unknown attributes of the entities, which is the approach pursued in this paper. Attributes of entities are now children of the corresponding entity latent variable and attributes of relationships are children of the latent variables of the entities participating in the relationship. By introducing the latent variables the ground network forms a relational network of latent variables. Thus, our hidden relational model can be viewed on as a direct generalization of hidden Markov models used in speech or hidden Markov random fields used in vision (such models are discussed, for example, in Yedidia et al. 2005). As in those models, information can propagate across the network of latent variables in the hidden relational model, which reduces the need to extensive structural model selection. Structural model selection is a major problem in relational learning due to the exponentially many features an attribute might depend on. Thus information about my grandfather can propagate to me via the latent variable of my father.\nSince each entity class might have a different number of states in its latent variables, it is natural to allow the model to determine the appropriate number of latent states in a self-organized way. This is possible by embedding the model in Dirichlet process (DP) mixture models, which can be interpreted as a mixture models with an infinite number of mixture components but where the model, based on the data, automatically reduces the complexity to an appropriate finite number of components. The DP mixture model also allows us to view our infinite hidden relational model as a\ngeneralization of nonparametric hierarchical Bayesian modeling to relational models (compare also, Xu et al., 2005). The combination of the hidden relational model and the DP mixture model is the infinite hidden relational model.\nAfter presenting related work we will briefly introduce our preferred framework for describing relational models, i.e., the directed acyclic probabilistic entity relationship (DAPER) model. In Section 4 we will describe infinite hidden relational models and in Section 5 we introduce a modified Chinese restaurant sampling process to accommodate for the relational structure. In the subsequent sections we describe experimental results applying infinite hidden relational models to encode movie preference for users, to predict the functional class of a gene, and on a medical example, modeling the relationship between patients, diagnosis and procedures. In Section 9 we will present conclusions."}, {"heading": "2 Related Work", "text": "Our approach can be related to some existing work. (Getoor et al., 2000) refined probabilistic relational models with class hierarchies, which specialized distinct probabilistic dependency for each subclass. (Rosen-Zvi et al., 2004) introduced an author-topic model for documents. The model implicitly explored the two relationships between documents and authors and document and words. (Kemp et al., 2004) showed a relational model with latent classes. (Carbonetto et al., 2005) introduced the nonparametric BLOG model, which specifies nonparametric probabilistic distributions over possible worlds defined by first-order logic. These models demonstrated good performance in certain applications. However, most are restricted to domains with simple relations. The proposed model goes beyond that by considering multiple related entities. In addition, the nonparametric nature allows the complexity of the model to be tuned by the model based on the available data set."}, {"heading": "3 The DAPER Model", "text": "The DAPER model (Heckerman et al., 2004) formulates a probabilistic framework for an entity relationship database model. The DAPER model consists of entity classes, relationship classes, attribute classes and arc classes, as well as local distribution classes and constraint classes. Figure 1 shows an example of a DAPER model for a universe of students, courses and grades. The entity classes specify classes of objects in the real world, e.g. Student and Course shown as rectangles in Figure 1. The relationship class repre-\nsents interaction among entity classes. It is shown as a diamond-shaped node with dashed lines linked to the related entity classes. For example, the relationship, Take(s, c) indicates that a student s takes a class c. Attribute classes describe properties of entities or relationships. Attribute classes are connected to the corresponding entity/relationship class by a dashed line. For example, associated with courses is the attribute class Course.Difficulty. The attribute class \u03b8 in Figure 1 represents the parameters specifying the probability of student\u2019s grade in different configurations (i.e. course\u2019s difficulty and student\u2019s IQ). The arc classes shown as solid arrows from \u201cparent\u201d to \u201cchild\u201d represent probabilistic dependencies among corresponding attributes. For example, the solid arrow from Student.IQ to Course.Grade specifies the fact that student\u2019s grade probabilistically depends on student\u2019s IQ. For more details please refer to (Heckerman et al., 2004). A relationship attribute might have the special attribute Exist with Exist= 0 indicating that the relationship does not exist (Getoor et al., 2003). Given particular instantiations of entities and relationships a ground Bayesian network can be formed which consists of all attributes in the domain linked by the resulting arc classes."}, {"heading": "4 Infinite Hidden Relational Models", "text": ""}, {"heading": "4.1 Hidden Relational Models", "text": "An example of an hidden relational model is shown\nin Figure 2. The example shows a movie recommendation system with entity classes User and Movie and relationship class Like. Furthermore, there are the attributes UserAttributes, MovieAttributes and R (rating) and various parameters and hyperparameters. The first innovation in our approach is to introduce for each entity a latent variable, in the example denoted as Zu and Zm. They can be thought of as unknown attributes of the entities and are the parents of both the entity attributes and the relationship attributes. The underlying assumption is that if the latent variable was known, both entity attributes and relationship attributes can be well predicted. The most important result from introducing the latent variables is that now information can propagate through the ground network of interconnected latent variables. Let us consider the prediction of the relationship attribute R. If both the associated user and movie have strong known attributes, those will determine the state of the latent variables and the prediction for R is mostly based on the entity attributes. In terms of a recommender system we would obtain a content-based recommendation system. Conversely, if the known attributes are weak, then the states of the latent variables for the user might be determined by the relationship attributes in relations to other movies and the states of those movies\u2019 latent variables. With the same argument, the states of the latent variables for the movie might be determined by the relationship attributes in relations to other users and the states of those users\u2019 latent variables. So by introducing the latent variables, information can globally distribute in the ground network defined by the relationship structure. This reduces the need for extensive structural learning, which is particularly difficult in relational models due to the huge number of potential parents. Note that a similar propagation of information can be observed in hidden Markov models in speech systems or in the hidden Markov random fields used in image analysis (Yedidia et a., 2005). In fact the hidden relational model can be viewed as a generalization of both for relational structures.\nWe now complete the model by introducing the parameters. First we consider the user parameters. Assume that Zu has ru states and that \u03c0u = (\u03c0u1 , . . . , \u03c0 u ru) are multinomial parameters with P (Zu = i) = \u03c0ui (\u03c0ui \u2265 0, \u2211 i \u03c0 u i = 1). The multinomial parameters are drawn from a Dirichlet prior with \u03c0u \u223c Dir(\u00b7|\u03b1u0/ru, . . . , \u03b1u0/ru).\nIn the experiments all user attributes are assumed to be discrete and independent given Zu. Thus, a particular user attribute Au with r states is a sample from a multinomial distribution with P (Au = i) = \u03c6i and\n(\u03c61, . . . , \u03c6r) \u223c Gu0 = Dir(\u00b7|\u03b2\u22171 , . . . , \u03b2\u2217r ).\nIt is also convenient to re-parameterize\n\u03b20 = r\u2211 k=1 \u03b2\u2217k \u03b2k = \u03b2\u2217k \u03b20 k = 1, . . . , r\nand \u03b2 = {\u03b21, . . . , \u03b2r}. In the application, we assume a neutral prior with \u03b2k = 1/r, which represents our prior belief in the fact that the multinomial parameters should be equal. \u03b20 is a parameter indicating how strongly we believe that the prior distribution represented by \u03b2 should be true. We finely tune \u03b20 using a cross validation procedure.\nThe parameters for the entity class Movie and the relationship class Like are defined in an equivalent way. Note, that for the relationship attribute R, ru \u00d7 rm parameter vectors \u03b3 are generated."}, {"heading": "4.2 Infinite Hidden Relational Models", "text": "The latent variables play a key role in our model and in many applications, we would expect that the model might require a large number of states for the latent variables. Consider again the movie recommendation system. With little information about past ratings all users might look the same (movies are globally liked or disliked), with more information available, one might discover certain clusters in the users (action movie aficionados, comedy aficionados, ...) but with an increasing number of past ratings the clusters might show increasingly detailed structure ultimately indicating that everyone is an individual. It thus makes sense to permit an arbitrary number of latent states by using a Dirichlet process mixture model. This permits the model to decide itself about the optimal number of states for the latent variables. In addition, the infinite hidden relational model can now also be viewed as a direct generalization of a nonparametric hierarchical Bayesian approach. For an introduction to Dirichlet processes see for example Teh et al. (2004) and Tresp (2006) . For our discussion is suffices to say that we obtain an infinite hidden relational model by simply letting the number of states approach infinity, ru \u2192 \u221e, rm \u2192 \u221e. Although a model with infinite numbers of states and parameters cannot be represented, it turns out that sampling in such model is elegant and simple, as shown in the next section.\nIn the Dirichlet mixture model, \u03b10 determines the tendency of the model to either use a large number or a small number of states in the latent variables, which is also apparent from the sampling procedures described below. In our experiments, we found that \u03b10 is rather uncritical and was fixed for all models to be equal to ten."}, {"heading": "5 Sampling in the Infinite Hidden Relational Model", "text": "Although a Dirichlet process mixture model contains an infinite number of parameters and states, the sampling procedure only deals with a growing but finite representation. This sampling procedure is based on the Chinese restaurant process (CRP) where a state of a latent variable is identified as a cluster, i.e., a table in a restaurant. We will now describe how the CRP is applied to the infinite hidden relational model. The procedure differs from the standard model by the sampling of the relational attribute where two CRP processes are coupled. We omit the description of the sampling of the attributes which is straightforward, given parameter samples.\n\u2022 The first user is assigned to the user cluster 1, Zu1 = 1; an associated parameter is generated \u03c61 \u223c Gu0 . Similarly, the first movie is assigned to the movie cluster 1, Zm1 = 1; an associated parameter is generated \u03b81 \u223c Gm0 .\n\u2022 The parameter \u03b31,1 \u223c GR0 is drawn for the attributes of the relation R between the first user cluster and the first movie cluster.\n\u2022 With probability 1/(1 + \u03b1u0 ), the second user is also assigned at the user cluster 1, Zu2 = 1, and inherits \u03c61 and \u03b31,1; with probability \u03b1 u 0/(1 + \u03b1 u 0 ) the user is\nassigned at cluster 2, Zu2 = 2, and new parameters are generated \u03c62 \u223c Gu0 and \u03b32,1 \u223c GR0 .\n\u2022 Equivalently, the second movie is generated. With probability 1/(1+\u03b1m0 ), the second movie is assigned at the movie cluster 1, Zm2 = 1, and inherits \u03b81 and \u03b31,1; with probability \u03b1m0 /(1+\u03b1 m 0 ) the movie is assigned at\ncluster 2, Zm2 = 2, and new parameters are generated \u03b82 \u223c Gm0 , \u03b31,2 \u223c GR0 and \u03b32,2 \u223c GR0 (if the second user cluster has appeared so far).\n\u2022 We continue this process, after Nu users and Nm movies have been generated, Ku user clusters andKm\nmovie clusters appear, Nui users are assigned to the user cluster i, Nmj movies are assigned to the movie cluster j.\n\u2022 New user:\n\u2013 The user Nu + 1 is assigned with probability Nui\nNu+\u03b1u0 to a previously existing cluster i and inherits \u03c6i and {\u03b3i,l}K m l=1 . Thus: Z u Nu+1 = i,\nNui \u2190 Nui + 1; \u2013 With probability\n\u03b1u0 Nu+\u03b1u0 the user is assigned to\na new cluster Ku + 1. Thus: ZuNu+1 = K u + 1, NuKu+1 = 1. \u2013 For the new user cluster, new parameters are generated: \u03c6Ku+1 \u223c Gu0 and \u03b3Ku+1,l \u223c GR0 , l = 1 : Km. Ku \u2190 Ku + 1.\n\u2022 New movie:\n\u2013 The generative process for the movie Nm + 1 is equivalent.\n\u2013 The movie Nm + 1 is assigned with probability Nmj\nNm+\u03b1m0 to a previously existing cluster j and inherits \u03b8j and {\u03b3l,j}K u l=1. Thus: Z m Nm+1 = j, Nmj \u2190 Nmj + 1;\n\u2013 With probability \u03b1m0\nNm+\u03b1m0 the movie is assigned\nto a new cluster Km+1. Thus: ZmNm+1 = K m+ 1, NmKm+1 = 1. \u2013 For the new movie cluster, new parameters are generated: \u03b8Km+1 \u223c Gm0 and \u03b3l,Km+1 \u223c GR0 , l = 1 : Ku. Km \u2190 Km + 1.\nThe previous procedure generates samples from the generative model. Now we consider sampling from a model given data, i.e. given a set of movie attributes, user attributes and ratings. We assume that the model has U users and M movies and that some instances of Au, Am, R are known. The goal is now to generate samples of the parameters \u03c6, \u03b8, \u03b3, the latent variables Zu and Zm, which allows us to then make predictions about unknown attributes. We exploit Gibbs sampling inference based on the Chinese restaurant procedure as described in the Appendix. Note, that since the attributes appear as children, unknown attributes can be marginalized out and thus removed from the model, greatly reducing the complexity. Although the DP mixture model contains an infinite number of states, in the Gibbs sampling procedure only a finite number of states is ever occupied, providing an estimate of the true underlying number of clusters (Tresp, 2006). Details on the Gibbs sampler can be found in the Appendix."}, {"heading": "6 Experiment on MovieLens", "text": "We first evaluate our model on the MovieLens data which contains movie ratings from a large number of users (Sarwar et al. 2000). The task is to predict whether a user likes a movie. There are two entity classes (User and Movie) and one relationship class (Like: users like movies). The User class has several attribute classes such as Age, Gender, Occupation. The Movie class has attribute classes such as Published-year, Genres and so on. The relationship has an additional attribute R with two states: R = 1 indicates that the user likes the movie and R = 0 indicates otherwise. The model is shown as Figure 2. In the data set, there are totally 943 users and 1680 movies. In addition, user ratings on movies are originally recorded on a five-point scale, ranging from 1 to 5. We transfer the ratings to be binary, yes if a rating higher than the average rating of the user, and vice versa. Model performance is evaluated using prediction accuracy. The experimental results are shown in Table 1. First we did experiments ignoring the attributes of the users and the items. We achieved an\naccuracy of 69.97% (E3). This is significantly better in comparison to approaches using one-sided collaborative filtering by generalizing across users (E1) leading to an accuracy of 64.22% or by generalizing across items (E2) leading to an accuracy of 64.66%. When we added information about the attributes of the users and the model, the prediction accuracy only improved insignificantly to 70.3% (E4): the reason is that the attributes are weak predictors of preferences as indicated by the bad performance of the SVM prediction (54.85% accuracy, E5) which is solely based on the attributes of the users and the items."}, {"heading": "7 Experiment on Medical Data", "text": "The second experiment is concerned with a medical domain. The proposed model is shown in Figure 3(a). The domain includes three entity classes (Patient, Diagnosis and Procedure) and two relationship classes (Make: physician is making a diagnosis and Take:patient taking a procedure). A patient typically has both multiple procedures and multiple diagnoses. The Patient class has several attribute classes including Age, Gender, PrimaryComplaint. To reduce the complexity of Figure 3(a), patient characteristics are grouped together as PatientAttributes (these attributes are not aggregated in learning and inference). The DiagnosisAttributes contain the class of the diagnosis as specified in the ICD-9 code and the ProcedureAttributes contain the class of the procedure as specified in the CPT4 code. Both the relationships between the patients and the procedures and the relationships between the patients and the diagnoses are modeled as existence uncertainty. Rpa,pr = 1 means that the patient received the procedure and Rpa,pr = 0 indicates otherwise. Equivalently, Rpa,dg = 1 means that the patient received the diagnosis and Rpa,dg = 0 indicates otherwise. In the data, there are totally 14062 patients, 703 diagnoses and 367 procedures. The infinite hidden relational model contains three DPs, one for each entity class. We compare our approach with two models. The first one is a relational model using reference uncertainty (Getoor et al., 2003) without a latent variable structure. The second comparison model is a\ncontent based Bayesian network. In this model, only the attributes of patients and procedures determine if a procedure is prescribed.\nWe test model performances by predicting the application of procedures. ROC curve is used as evaluation criteria. In the experiment we selected the top N procedures recommended by the various models. Sensitivity indicates how many percent of the actually being performed procedures were correctly proposed by the model. (1-specificity) indicates how many of the procedures that were not actually performed were recommended by the model. Along the curves, the N was varied from left to right as N = 5, 10, . . . , 50.\nIn the experiment we predict a relation between a patient and a procedure given her first procedure. The corresponding ROC curves (averaged over all patients) for the experiments are shown in Figure 4. The infinite hidden relational model (E3) exploiting all relational information and all attributes gave best performance. When we remove the attributes of the entities, the performance degrades (E2). If, in addition, we only consider the one-sided collaborative effect, the performance is even worse (E1). (E5) is the pure contentbased approach using the Bayesian network. The results show that entity attributes are a reasonable predictor but that the performance of the full model cannot be achieved. (E4) shows the results of relational model using relational uncertainty, which gave good results but did not achieve the performance of the infinite hidden relational model. Figures 5 shows the corresponding plots for a selected class of patients; patients with prime complaint respiratory problem. The results exhibit similar trends."}, {"heading": "8 Experiment on Gene Data", "text": "The third evaluation is performed on the yeast genome data set of KDD Cup 2001 (Cheng et al. 2002). The\ngoal is to evaluate the proposed algorithm to model the dependencies between relationships instantiated from various relationship classes. The genomes in several organisms have been sequenced. Genes code for proteins, the proteins localize in various parts of the cells and interact with one another to perform crucial functions. Traditionally, the functions of genes/proteins are predicted by comparing with characterized genes/proteins in sequence similarity. But only 52% of 6449 yeast proteins have been characterized. Of the remaining, only 4% show strong similarity with the known ones at the sequence level. It is therefore necessary to integrate other information to characterize genes/proteins. The goal of our experiment is to predict functions of genes based on the information not only at the genelevel but also at the protein-level. The data set consists of two relational tables that are produced from the original seven relational tables. One table speci-\nfies a variety of properties of genes or proteins. These properties include chromosome, essential, phenotype, motif, class, complex and function. Chromosome expresses the chromosome on which the gene appears. Essential specifies whether organisms with a mutation in this gene can survive. Phenotype represents the observed characteristics of organisms with differences in this gene. Class means the structural category of the protein for which this gene codes. Motif expresses the information about the amino acid sequence of the protein. The value of property complex specifies how the expression of the gene can complex with others to form a larger protein. The other table contains the information about interactions between genes. A gene typically has multiple complexes, phenotypes, classes, motifs and functions, respectively but only one property essential and one property chromosome. An example gene is shown in Table 2. To keep the multi-relational nature of the data, we restore the original data structure. There are six entity classes (Gene, Complex, Phenotype, Class, Motif and Function) and six relationship classes (Interact: genes interact with each other, Have: genes have functions, Observe: phenotype are observed for the genes, Form: which kinds of complex is formed for the genes, Belong: genes belong to structural classes, Contain: genes contain characteristic motifs). Gene class has attribute classes such as Essential, Chromosome, etc. The attributes of other entity classes are not available in the data set. A hidden attribute is added into each entity class. All relationships are modeled as existence uncertainty. Thus each relationship class has additional attribute R with two states. The state of R indicates whether the relationship exists or not. The task of function prediction of genes is therefore transformed to the relationship prediction between genes and func-\ntions. The data set totally contains 1243 genes. A subset (381 genes) is withheld for testing in the KDD Cup 2001. The remaining 862 genes are provided to participants. In the data, there are 56 complexes, 11 phenotypes, 351 motifs, 24 classes and 14 functions. There are two main challenges in the gene data set. First, there are many types of relationships. Second, there are large numbers of objects, but only a small number of known relationships.\nThe proposed model applied to the gene data is shown in Figure 3(b). The existence of any relationship depends on the hidden states of the corresponding entities. The information about a variety of relationships of Gene is propagated via the hidden attribute of Gene. The model is optimized using 862 genes, and is applied on the testing data. The experiment results are shown in Table 3. There were 41 groups that participated in the KDD Cup 2001 contest. The algorithms include naive Bayes, k-nearest neighbor, decision tree, neural network, SVM, and Bayesian networks, etc. and technologies such as feature selection, boosting, cross validation, etc., were employed. The performance of our model is comparable to the best results. The winning algorithm is a relational model based on inductive logic programming. The infinite hidden relational model is only slightly worse (probably not significantly) if compared to the winning algorithm.\nIn the second set of experiments, we investigated the influence of a variety of relationships on the prediction of functions. We perform the experiments by ignoring a specific kind of known relationships. The result is\nshown in Table 4. When a specific type of known relationship is ignored, lower accuracy indicates higher importance of this type of relationship. One observation is that the most important relationship is Complex, specifying how genes complex with another genes to form larger proteins. The second one is the interaction relationships between genes. This coincide with the lesson learned from KDD Cup 2001 that protein interaction information is less important in function prediction. This lesson is somewhat surprising since there is a general belief in biology that the knowledge about regulatory pathways is helpful to determine the functions of genes."}, {"heading": "9 Conclusions and Extensions", "text": "We have introduced the infinite hidden relational model. The model showed encouraging results on a number of data sets. We hope that infinite hidden relational model will be a useful addition to relational modeling by allowing for flexible inference in a relational network reducing the need for extensive structural model search. Implicitly, we have assumed a particular sampling scheme, i.e., that entities are independently sampled out of unspecified populations. In this context our model permits generalization but it might fail if this assumption is not reasonable or if the sampling procedure changes in the test set. We have focussed on an explicit modeling of the relation between pairs of entities but our model can easily be generalized if more than two entities are involved in a relation. As part of our future work we will explore and compare different approximate inference algorithms."}], "references": [{"title": "Nonparametric bayesian logic", "author": ["P. Carbonetto", "J. Kisynski", "N. de Freitas", "D. Poole"], "venue": "Proc. 21th UAI.", "citeRegEx": "Carbonetto et al\\.,? 2005", "shortCiteRegEx": "Carbonetto et al\\.", "year": 2005}, {"title": "KDD Cup 2001 report", "author": ["J. Cheng", "C. Hatzis", "H. Hayashi", "M. Krogel", "S. Morishita", "D. Page", "J. Sese"], "venue": "SIGKDD Explorations, 3(2):47-64.", "citeRegEx": "Cheng et al\\.,? 2002", "shortCiteRegEx": "Cheng et al\\.", "year": 2002}, {"title": "Learning probabilistic models of link structure", "author": ["L. Getoor", "N. Friedman", "D. Koller", "B. Taskar"], "venue": "Journal of Machine Learning Research, 3:679-707.", "citeRegEx": "Getoor et al\\.,? 2003", "shortCiteRegEx": "Getoor et al\\.", "year": 2003}, {"title": "From instances to classes in probabilistic relational models", "author": ["L. Getoor", "D. Koller", "N. Friedman"], "venue": "Proc. ICML 2000 Workshop on Attribute-Value and Relational Learning: Crossing the Boundarie. Stanford, CA.", "citeRegEx": "Getoor et al\\.,? 2000", "shortCiteRegEx": "Getoor et al\\.", "year": 2000}, {"title": "Probabilistic Models for Relational Data (Technical Report MSRTR-2004-30)", "author": ["D. Heckerman", "C. Meek", "D. Koller"], "venue": "Microsoft.", "citeRegEx": "Heckerman et al\\.,? 2004", "shortCiteRegEx": "Heckerman et al\\.", "year": 2004}, {"title": "Discovering Latent Classes in Relational Data (Technical Report AI Memo 2004-019)", "author": ["C. Kemp", "T. Griffiths", "J.R. Tenenbaum"], "venue": null, "citeRegEx": "Kemp et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2004}, {"title": "Probabilistic logic learning", "author": ["L.D. Raedt", "K. Kersting"], "venue": "SIGKDD Explor. Newsl., 5:31-48.", "citeRegEx": "Raedt and Kersting,? 2003", "shortCiteRegEx": "Raedt and Kersting", "year": 2003}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proc. 20th UAI, 487-494. AUAI Press.", "citeRegEx": "Rosen.Zvi et al\\.,? 2004", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Analysis of recommender algorithms for ecommerce", "author": ["B.M. Sarwar", "G. Karypis", "J.A. Konstan", "J. Riedl"], "venue": "Proc. ACM E-Commerce Conference, 158167. ACM Press.", "citeRegEx": "Sarwar et al\\.,? 2000", "shortCiteRegEx": "Sarwar et al\\.", "year": 2000}, {"title": "Hierarchical Dirichlet Processes (Technical Report 653)", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "UC Berkeley Statistics.", "citeRegEx": "Teh et al\\.,? 2004", "shortCiteRegEx": "Teh et al\\.", "year": 2004}, {"title": "An introduction to nonparametric hierarchical Bayesian modelling", "author": ["V. Tresp", "K. Yu"], "venue": "Proc. the Hamilton Summer School on Switching and Learning in Feedback Systems, 290-312. Springer.", "citeRegEx": "Tresp and Yu,? 2004", "shortCiteRegEx": "Tresp and Yu", "year": 2004}, {"title": "Dirichlet processes and nonparametric Bayesian modelling", "author": ["V. Tresp"], "venue": "Online tutorial.", "citeRegEx": "Tresp,? 2006", "shortCiteRegEx": "Tresp", "year": 2006}, {"title": "Inductive logic programming for knowledge discovery in databases", "author": ["S. Wrobel"], "venue": "S. Dzeroski and N. Lavrac (ed.), Relational Data Mining, 74-101. Springer.", "citeRegEx": "Wrobel,? 2001", "shortCiteRegEx": "Wrobel", "year": 2001}, {"title": "Dirichlet enhanced relational learning", "author": ["Z. Xu", "V. Tresp", "K. Yu", "S. Yu", "H.-P. Kriegel"], "venue": "Proc. 22nd ICML, 1004-1011. ACM Press.", "citeRegEx": "Xu et al\\.,? 2005", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory, 51(7):2282-2312.", "citeRegEx": "Yedidia et al\\.,? 2005", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "A nonparametric hierarchical bayesian framework for information filtering", "author": ["K. Yu", "V. Tresp", "S. Yu"], "venue": "Proc. 27th SIGIR, 353-360. ACM Press.", "citeRegEx": "Yu et al\\.,? 2004", "shortCiteRegEx": "Yu et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Relational learning (Dzeroski & Lavrac, 2001; Raedt & Kersting, 2003; Wrobel, 2001; Friedman et al., 1999) is an object oriented approach that clearly distinguishes between entities (e.", "startOffset": 20, "endOffset": 106}, {"referenceID": 15, "context": "In many circumstances, the attributes of the entities are rather weak predictors in which case one can exploit the known relationship attributes to predict unknown entity or relationship attributes (Yu et al., 2004).", "startOffset": 198, "endOffset": 215}, {"referenceID": 3, "context": "(Getoor et al., 2000) refined probabilistic relational models with class hierarchies, which specialized distinct probabilistic dependency for each subclass.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "(Rosen-Zvi et al., 2004) introduced an author-topic model for documents.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "(Kemp et al., 2004) showed a relational model with latent classes.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "(Carbonetto et al., 2005) introduced the nonparametric BLOG model, which specifies nonparametric probabilistic distributions over possible worlds defined by first-order logic.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "The DAPER model (Heckerman et al., 2004) formulates a probabilistic framework for an entity relationship database model.", "startOffset": 16, "endOffset": 40}, {"referenceID": 4, "context": "The DAPER model (Heckerman et al., 2004) formulates a probabilistic framework for an entity relationship database model. The DAPER model consists of entity classes, relationship classes, attribute classes and arc classes, as well as local distribution classes and constraint classes. Figure 1 shows an example of a DAPER model for a universe of students, courses and grades. The entity classes specify classes of objects in the real world, e.g. Student and Course shown as rectangles in Figure 1. The relationship class repreFigure 1: An example of DAPER model over university domain from Heckerman et al. (2004).", "startOffset": 17, "endOffset": 613}, {"referenceID": 4, "context": "For more details please refer to (Heckerman et al., 2004).", "startOffset": 33, "endOffset": 57}, {"referenceID": 2, "context": "A relationship attribute might have the special attribute Exist with Exist= 0 indicating that the relationship does not exist (Getoor et al., 2003).", "startOffset": 126, "endOffset": 147}, {"referenceID": 9, "context": "For an introduction to Dirichlet processes see for example Teh et al. (2004) and Tresp (2006) .", "startOffset": 59, "endOffset": 77}, {"referenceID": 9, "context": "For an introduction to Dirichlet processes see for example Teh et al. (2004) and Tresp (2006) .", "startOffset": 59, "endOffset": 94}, {"referenceID": 11, "context": "Although the DP mixture model contains an infinite number of states, in the Gibbs sampling procedure only a finite number of states is ever occupied, providing an estimate of the true underlying number of clusters (Tresp, 2006).", "startOffset": 214, "endOffset": 227}, {"referenceID": 8, "context": "We first evaluate our model on the MovieLens data which contains movie ratings from a large number of users (Sarwar et al. 2000).", "startOffset": 108, "endOffset": 128}, {"referenceID": 2, "context": "The first one is a relational model using reference uncertainty (Getoor et al., 2003) without a latent variable structure.", "startOffset": 64, "endOffset": 85}, {"referenceID": 1, "context": "The third evaluation is performed on the yeast genome data set of KDD Cup 2001 (Cheng et al. 2002).", "startOffset": 79, "endOffset": 98}], "year": 2006, "abstractText": "Relational learning analyzes the probabilistic constraints between the attributes of entities and relationships. We extend the expressiveness of relational models by introducing for each entity (or object) an infinitedimensional latent variable as part of a Dirichlet process (DP) mixture model. We discuss inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. We extended the Chinese restaurant process to be applicable to relational modeling. We discuss how information is propagated in the network of latent variables, reducing the necessity for extensive structural learning. In the context of a recommendation engine our approach realizes a principled solution for recommendations based on features of items, features of users and relational information. Our approach is evaluated in three applications: a recommendation system based on the MovieLens data set, the prediction of gene function using relational information and a medical recommendation system.", "creator": "TeX"}}}