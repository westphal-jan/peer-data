{"id": "1206.6869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Recognizing Activities and Spatial Context Using Wearable Sensors", "abstract": "Our model is novel in that it uses a dynamic graphical model to jointly estimate both activity and spatial context over time based on the simultaneous use of asynchronous observations, consisting of GPS measurements and measurements from a small mountable sensor board. Common conclusions are desirable as they can improve the accuracy of the model. However, an important goal in designing our overall system is to be able to make precise inference decisions while minimizing the amount of hardware that a person must carry. This minimization results in greater convenience and flexibility, reduced power consumption and therefore increased battery life and reduced costs. We show results that suggest that our common measurement model exceeds measurements from either the sensor board or GPS alone by using two types of probable inference processes, namely particle filtering and unprinted exact inference.", "histories": [["v1", "Wed, 27 Jun 2012 16:29:30 GMT  (931kb)", "http://arxiv.org/abs/1206.6869v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["amarnag subramanya", "alvin raj", "jeff a bilmes", "dieter fox"], "accepted": false, "id": "1206.6869"}, "pdf": {"name": "1206.6869.pdf", "metadata": {"source": "CRF", "title": "Recognizing Activities and Spatial Context Using Wearable Sensors", "authors": ["Amarnag Subramanya", "Alvin Raj"], "emails": [], "sections": [{"heading": null, "text": "We introduce a new dynamic model with the capability of recognizing both activities that an individual is performing as well as where that individual is located. Our approach is novel in that it utilizes a dynamic graphical model to jointly estimate both activity and spatial context over time based on the simultaneous use of asynchronous observations consisting of GPS measurements, and a small mountable sensor board. Joint inference is quite desirable as it has the ability to improve accuracy of the model and consistency of the location and activity estimates. The parameters of our model are trained on partially labeled data. We apply virtual evidence to improve data annotation, giving the user high flexibility when labeling training data. We present results indicating the performance gains achieved by virtual evidence for data annotation and the joint inference performed by our system."}, {"heading": "1 Introduction", "text": "Recent advances in wearable sensing and computing devices and in fast probabilistic inference techniques make possible the fine-grained estimation of a person\u2019s activities over extended periods of time [12]. Such technologies enable applications ranging from context aware computing [9] to support for cognitively impaired people [16] to long-term health and fitness monitoring to automatic after action reporting of military missions.\nThe focus of our work is on providing accurate information about a person\u2019s activities and environmental context in everyday environments based on wearable sensors and GPS devices. More specifically, we wish to estimate a person\u2019s motion type (such as walking, running, going upstairs/downstairs, or driving a vehicle) and whether a person is outdoors, inside a building, or in a vehicle. These ac-\ntivity estimates are additionally combined with GPS information so as to estimate the trajectory of the person along with information about which buildings the person enters. To do this, our approach assumes that the bounding boxes of buildings (i.e., maps) are known. This assumption is reasonable, given the availability of satellite images from which these bounding boxes can be extracted manually or via computational vision algorithms.\nOur main emphasis is on performing activity recognition that is not only accurate, but that also requires a minimum number of sensor devices. There are in fact a variety of systems that utilize multiple sensors and measurements taken all over the body [11, 15]. Our approach, by contrast, attempts to produce as accurate as possible activity recognition requiring only one sensing device mounted only at one location on the body. Our reasoning for reducing the total number of sensors is threefold: 1) it can be unwieldy for the person wearing the sensors to have many such sensors and battery packs mounted all over the body, 2) we wish to minimize overall system cost, and 3) we wish to extend operational time between battery replacement/recharge (equivalently, minimize online energy consumption).\nOur model is novel in several respects. We integrate sensor information from both a standard GPS device and a single sensor board (producing standard signals such as accelerometer, barometric pressure, etc.) in a semiasynchronous way. In other words, it is not required for our sensor board to produce measurements at precisely the same time points as does the GPS device. This approach therefore allows us to utilize separate, standard, cost-effective, and non-integrated components for multiple types of information, thereby reducing the overall cost of the system. Our statistical model, moreover, utilizes a dynamic Bayesian network (DBN) [7] for expressing the underlying dependencies between various types of information represented in our system, and performs efficient probabilistic inference based on a discretization of 2-D space. State space pruning allows us to perform joint inference on both a person\u2019s location and her activity.\nIn Section 2, we give a brief overview of our sensor board.\nSection 3 describes our activity model including all modeling assumptions, inference, and learning algorithms. Experiments are described in Section 5, followed by a discussion and conclusions."}, {"heading": "2 Wearable Sensor System", "text": "Our customized wearable sensor system consists of a multisensor board, a Holux GPS unit with SIRF-III chipset, and an iPAQ PDA for data storage.\nThe sensor board shown in Figure 1 is extremely compact, low-cost, and uses standard electronic components. It weighs only 121g including battery and processing hardware. Sensors include a 3-axis accelerometer, two microphones for recording speech and ambient sound, phototransistors for measuring light conditions, and temperature and barometric pressure sensors. The overall cost per sensor board is approximately USD 400. The time-stamped data collected on this device is transfered via a USB connection to an iPAQ handheld computer. GPS data is transfered from the receiver via Bluetooth to the PDA. The overall system is able to operate for more than 8 hours."}, {"heading": "3 Activity Model", "text": ""}, {"heading": "3.1 Overview", "text": "The complete DBN for our activity model is shown in Figure 2 with some of the implementation details shown in Figure 3. The variables represented in the model include GPS measurements (gk, hk), sensor-board measurements mk, the person\u2019s location lk, motion velocity vk, the type of motion sk they are performing, and their environment ek. We first describe the individual components and their relationships starting at the sensor level of the model.\nGPS measurements are separated into longitude / latitude information, gk = {g.xk, g.yk}, and horizontal dilution of precision (hdop), hk. hdop provides information about the accuracy of the location information, which depends mostly on the visibility and position of satellites. The node ok explicitly models GPS outliers. Outliers typically occur when the person is inside or close to a building or under trees. Unfortunately, outliers are not always indicated by a high hdop value, especially when the person is very close to high rise buildings.\nThe left panel in Figure 3 shows a more \u2018low-level\u2019 view of the sub-graphical model for the GPS observations. In this figure g.xk and g.yk are the GPS northing and easting readings, respectively. These are scored by the northing and easting of the current location lk, namely l.xk and l.yk, and the current value of hdop hk. It should be noted that both l.xk and l.yk are deterministic children of the hidden variable modeling location, i.e., lk. Also, ok is an observed switching parent such that when ok = {1}, the graph is exactly as shown in Figure 3, but when ok = {0}, the GPS observations are disconnected from all the their (non-switching) parents. Furthermore, the two observation streams in our system, namely, the sensor board and the GPS, are not synchronized. In particular, the GPS is the\nless frequent of the two. In order to handle this, we make use of a switching parent Ik, which disconnects gk from all its parents when Ik = 0. The likelihood of a GPS measurement is given by\np(gk|lk, hk, ok)= N (gk; lk, \u03c32hk) if ok = 1 and Ik = 1 (1)\nThat is, if the GPS observation is present at time k, and is not an outlier, then the likelihood is given by a Gaussian centered at the person\u2019s location. The variance of the Gaussian is a function of the hdop value hk. In our current implementation, we have found that setting \u03c32hk = 2 \u2217 hk works reasonably well.\nSensor-board measurements mk consist of pertinent features extracted from 3D acceleration, barometric pressure, temperature, visible and IR light intensity, and raw audio. As we will further describe in Section 3.4, we use boosted classifiers to extract probability estimates for the person\u2019s instantaneous environment and motion state. These classifier outputs provide the observation mk, which in our model depends on the current environment and motion state, as indicated by the arcs from ek and sk. More specifically, we learn one binary classifier for each motion state and environment. At each iteration, each of these classifiers generates a probability estimate pi that the current measurement corresponds to the state or environment it was trained on (see right panel in Figure 3). We then uniformly quantize the pi\u2019s into b bins. This gives us an integer observation vector mk = (msk,m e k) at time k, where msk = (m s 1,k, . . . ,m s |s|,k), and m e k = (m e 1,k, . . . ,m e |e|,k) where, every \u03b1 \u2208 mk is such that 1 \u2264 \u03b1 \u2264 b, |s| and |e| are the cardinalities of the state (activity) and environment variables respectively. In our current implementation, b = 10, i.e. each pi is uniformly quantized into 10 bins.\nWe assume each discretized binary classifier observation is independent of all else given the corresponding environment or motion state, and along with a naive Bayes like assumption, obtain the following observation model:\np(mk | sk, ek) = p(msk | sk) p(mek | ek)\n= |s|\u220f i=1 p(msi,k | sk) |e|\u220f i=1 p(mei,k | ek)(2)\nNote that in our current implementation, we assume that the observations from the sensor board are available all the time.\nMap There are many ways of expressing constraints in Bayesian networks [8], each of which can be implemented using a variety of algorithms. We choose in this work to utilize the mechanism of the observed child. Specifically, we encode position and map information using an observed child ck that implements a constraint between its two parents, namely, the current location lk and the environment ek. This variable and its conditional probability\ntable (CPT) represents knowledge about the locations of buildings in the area under consideration. Values in the CPT can be used to enforce consistency between the assignments to the location and environment variable in either a soft or hard manner. For example, we can easily enforce the constraint that when lk is outside any of the buildings in the map, a value of ek = inside would yield zero probability, in the form of p(ck = 1|ek = inside, lk) = 0 for appropriate values of lk, and where the map is encoded in the CPT p(ck|ek, lk). The above is an example of a hard constraint, but it is straightforward to extend the above to soft constraints in which case sets of variable assignments would get probabilities in this CPT other than 0 or 1.\nLocation In reality the location of a person is a continuous function of time. In [18], we show how to apply RaoBlackwellized particle filters in order to perform approximate inference using a sample-based representation of the state space. In this paper, however, we simplify the problem by discretizing the location variable. We assume that we have a fixed region over which the user moves, which is then discretized using fixed square grids of size 100cm. Currently, we do not aim to track people inside buildings since GPS is either useless or not available, and because the accelerometers on our sensor boards are not sufficiently accurate to provide adequate dead reckoning information. The location at time k solely depends on the person\u2019s previous location lk\u22121 and motion vk\u22121. Note that location is hidden during both training and testing.\nVelocity represents the motion between locations at consecutive points in time. We adopt a piecewise linear motion model on polar velocity coordinates vk = (tk, \u03b8k)T , namely translational speed tk and heading direction \u03b8k. Once again we discretized the number of possible speed and heading values. We assume that the heading at time k only depends on the previous heading and the current activity. By making the change in heading also depend on the activity, our system can model the fact that, for example, a walking user changes heading differently than a user driving a vehicle. The translational speed tk is modeled in a similar fashion to the heading, and depends on the previous speed tk\u22121 and the current motion state sk. In order to aid data sharing across activities, both the CPTs, i.e., speed and heading are implemented using sparse representations. This is because the motion patterns for various classes of activities are similar. For example, walking and going up/downstairs are similar, as going up/downstairs are essentially ramifications of walking on inclined surfaces. Sharing motion patterns across the above set of activities not only aids better training, but also speeds up inference.\nMotion states represent different types of motion a person can be involved in. In our current system, these states include s = {stationary, walking, running, driving vehicle, going up/down stairs}. The motion state sk depends on the previous\nmotion state sk\u22121 and the current environment ek. The temporal dependency allows the system to capture information such as \u201cit is very unlikely to get into the driving state right after going upstairs\u201d. It should be noted that this variable is observed during training, as we assume that we have frame level labels 1.\nEnvironment captures the person\u2019s spatial context, which is E = indoors,outdoors,vehicle. Note that due to the edge between ek and sk, there can be (both soft and hard) constraints imposed between the motion state and the environment. For example, whenever it is the case that the environment is in the indoors or outdoors state, we a priori preclude driving from being a possible value of the motion type (i.e., it has zero probability). Whenever the environment is in the vehicle state, the motion type may not be up/down stairs (but it may be stationary, for example). Moreover, other \u201csoft constraints\u201d are imposed by the fact that the two nodes are related probabilistically, and the probabilities are learned automatically (see Section 3.2). Like the motion state variable, the environment variable is observed during training."}, {"heading": "3.2 Inference", "text": "Our DBN-based probabilistic inference system uses a combination of a triangulation [4] (to produce a dynamic junction tree) and search-based methods within each clique. This allows us to utilize a form of approximate inference method that is akin to sampling, but has some differences. As each clique is constructed, we only use a subset of the states of the incoming separators. The subset is chosen indirectly using a pruning strategy \u2014 specifically, for each separator, its previous clique has had all but the top scoring k clique-states removed. This leads to a reduced separator size and thus a reduced within-clique construction procedure.\nAs a result, our inference system adaptively prunes the state space. Since the pruning threshold is chosen based on the probability of the most likely state, the number of states being tracked depends on the uncertainty of the posterior; the higher the uncertainty, the less states are pruned. This pruning strategy is essential for tractable inference and learning, since the overall state space of our model contains on an average over 100, 000 states for each time frame. If left unpruned, the state space can potentially increase exponentially over time. In our experiments, the system typically tracks only about 10,000 states, without noticeable reduction in inference and learning accuracy. Further details of this DBN inference procedure will appear in forthcoming publications. All DBN inference and incorporation of dynamic virtual evidence (see below) utilizes the graphical modeling toolkit (GMTK) system [4].\n1Note that in section 3.3 we discuss the case when frame level labels are not available"}, {"heading": "3.3 Semi-supervised Learning With Virtual Evidence", "text": "The robustness and accuracy of activity recognition models largely depends on the availability of labeled training data. A typical approach to learning such models is to collect long sequences of training data and to manually label them with the performed activities. Unfortunately, such an approach is extremely tedious and error-prone. Annotation errors are typically due to confusing different activities and due to wrong timing of transitions between activities. The second type of error is especially difficult to overcome since it is often unclear or not well defined when a transition actually occurs, or what a transition means. For instance, the exact time point when a person enters the driving vehicle state can be chosen in various ways, for instance when the person enters the car, when she sits in the car, when she turns the motor on, or when the car starts to move.\nAn obvious solution to this problem would be unsupervised training of the model parameters. Unfortunately, such an approach typically generates models that do not directly correspond to the set of activities one wants to detect. A popular solution is to use a mixture of supervised and unsupervised (i.e., semi-supervised) learning [19, 5, 6], where parts of the data are labeled and other parts are left unlabeled. While such an approach provides reasonable results, it gives the user only a limited set of options when labeling data (fully, or not at all).\nIn our system, we use the notion of virtual evidence (VE) to model a variety of data annotations. Virtual evidence was first defined in [17] as a way to depict evidence in a Bayesian network using the mechanism of hidden variables. Given a joint distribution over n variables p(x1, . . . , xn), evidence simply means that one of the variables (w.l.o.g. x1) is observed. We denote this by x\u03041, so\nthe probability distribution becomes p(x\u03041, . . . , xn) which is no longer a function of x1 since it is fixed. Essentially, any configuration of the variables where x1 6= x\u03041 is never considered. We can mimic this behavior by introducing a new virtual child variable c into the joint distribution that is always observed to be one (so c = 1), and have c interact only with x1 via the CPT p(c = 1|x1) = \u03b4x1=x\u03041 . Therefore, \u2211 x1 p(c = 1, x1, . . . , xn) = p(x\u03041, . . . , xn).\nMore general virtual evidence is formed where c interacts with two or more variables, and rather than using a delta function, we use a \u201csoft evidence\u201d function. That is, suppose c interacts with x1 and x2. We set p(c = 1|x1, x2) = f(x1, x2), where f() is an arbitrary non-negative function.2 Therefore, additional preferential treatment can be given to different combinations of x1 and x2, but unlike hard evidence, we are not insisting on only one particular pair of values. Virtual evidence can be used to incorporate soft evidence when computing posterior distributions over the most probable values of random variables (filtering, Viterbi, or MPE), and during EM training [3].\nIn this work, we employ virtual evidence as a mechanism for semi-supervised learning. Clearly, making f() a delta function mimics evidence. And making f() the same score for all values of its arguments mimics an entirely hidden variable. These two extremes capture the typical case in semi-supervised learning. Under neither of these extremes, however, f() can provide numerical preference for or against some set of values of the hidden variables reflecting a graded uncertainty about a particular label.\nWe can use the VE mechanism to represent uncertainty in the supervisory training procedure in the ways outlined at the beginning of this section. Figure 4 illustrates some of the annotations allowed in our system. First, on the left of the figure shows standard supervised labels being simulated by VE, where at any given time, f() provides a non-zero score only to one possible activity. Second, we may wish to model the fact that the labels at transitions are typically unreliable. We thus can add a smoothly varying time-inhomogeneous collection of virtual evidence functions, moving from one class label to the next \u2014 these are shown as the center set of labels in the figure. For example, if we decide that a transition starts at time k1, ends at time k2, and makes a transition from label `1 to `2, we utilize a time-dependent function p(ck = 1|ak) = fk(ak) where ak is the activity at time k, and where fk() varies smoothly between the extremes of insisting on label `1 to the same on label `2. By putting less stringent requirements on the posteriors, training can result in parameters that have a reduced variance. Third, we may have regions that are entirely missing labels, as shown as the right set of labels in the figure. In this case, we can again utilize an f() function that gradually \u201cfades out\u201d from the most recent label, and\n2Normalization is performed whenever a state distribution needs to be extracted from the system.\nfades in to the next label. There are a number of ways to do this. For example, f() can fade out to the point where it provides uniform score over all activities. Alternatively, it could fade out to the point where it only provides uniform score to the previous and the following known activity. We experiment with these approaches in Section 5."}, {"heading": "3.4 Feature Extraction", "text": "Our system uses GPS measurements and sensor-board measurements to infer a person\u2019s activities. Especially the sensor board produces a variety of discrete signals that have different properties, such as inherently different sampling rates and spectral contents \u2014 for example, raw audio has a much higher sampling rate than the barometric pressure sensor. To incorporate such heterogeneous sensor data, we employ the feature extraction process developed by Lester and colleagues [12]. Here, we only provide a high-level description of this process, more information can be found in [12].\nFirst, the signal sample rates are normalized by low-pass filtering and/or up/down-sampling to an appropriate rate so that information is not lost. Next, each signal is windowed using a window of appropriate length at 4 Hz, and in each window a feature vector is extracted, resulting in a feature vector of high dimensionality (650 in our current system). Such high-dimensional feature vectors are not possible to utilize directly in a model, and typical approaches either require dimensionality reducing linear transforms such as PCA or LDA, or alternatively feature selection. We utilize the approach taken in [12] as our starting feature extraction procedure. Essentially, for each activity we learn boosted threshold detectors, or decision stumps. In other words, for each activity, we learn a collection of decision stumps where the next stump is obtained via boosting. Each decision stump essentially acts as a weak-learner, alone incapable of making an accurate detection decision, but when combined with kindred classifiers, capable of making highly accurate decisions. This collection of decision stumps for each activity is then used to produce a final event detection probability 0 \u2264 pi \u2264 1 for activity i (as mentioned in Section 3.1). The detection probability pi is obtained by viewing each individual threshold as a decision boundary, the distance to which constitutes a margin. Considering these margins together, we can obtain an average distance to the decision boundary, which is then passed through a sigmoid function to produce a [0, 1]- valued probability. It is these probabilities that are then uniformly quantized to produce the integer observations \u2014 e.g., mai,k = 3 if 0.2 \u2264 pi < 0.3. In future work, we plan to utilize standard joint vector-quantification algorithms for this last process, but for now we found that the above simple approach worked well for our applications."}, {"heading": "4 Related Work", "text": "Recently, the estimation of activities from wearable sensors has received significant attention especially in the ubiquitous computing and artificial intelligence communities. Virtually all existing approaches incorporate either location sensors such as GPS or location-independent sensors such as accelerometers and microphones, but not both.\nBao and Intille [2] use multiple accelerometers placed on a person\u2019s body to estimate activities such as standing, walking, or running. Kern and colleagues [11, 10] and Lukowicz et al. [15] added a microphone to a similar set of accelerometers in order to extract additional context information. These techniques typically rely on Gaussian observation models and dimensionality reduction techniques such as PCA and LDA to generate observation models from the low-level sensor data or features extracted thereof (e.g., FFT). Recently, Lester et al. [12] showed how to apply boosting in order to learn activity classifiers based on the sensor data collected by the same sensor board we use in our research. These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing. None of these approaches estimates a user\u2019s spatial context or is able to consider such context so as to improve activity recognition.\nUsing location for activity recognition has been the focus of other work. For instance, Ashbrook and Starner [1] detect a person\u2019s significant places based on the time spent at a location. In [13], Liao and colleagues showed how to learn a person\u2019s outdoor transportation routines from GPS data. More recently, the same authors presented a technique for jointly determining a person\u2019s activities and her significant places [14]. However, all these approaches are limited in their accuracy due to the fact that they only rely on location information.\nIn contrast to existing work, our system combines the information from both classes of sensors. We apply a structured DBN model in order to efficiently perform joint inference over our more complex state space. The ability to incorporate multiple modalities into the estimation process results in a system that is able to extract more expressive context information from the sensor data."}, {"heading": "5 Experiments", "text": "To evaluate our system, multiple users were asked to perform a variety of activities on the University of Washington campus. These activities included walking, running, going up/down a flight of stairs, driving around in a vehicle, going indoors, etc. Users were instructed to perform the above activities in a natural way and neither the sequence of activities nor their durations was choreographed. The\nusers were asked to label the activities they performed using a hand-held device, i.e., each time a user started a new activity, she had to indicate that activity on the PDA.\nIn our current implementation we make use of a rather simple GPS outlier detector. We simply ignore GPS whenever the hdop value hk > 8. As we have a mixture of both continuous and discrete observation distributions in this model, the scores from the continuous GPS distributions tend to outweigh the scores from the discrete observation distributions. Thus we raise the scores of the GPS observation distribution to a heuristically chosen exponent 0.5 in the probability domain.\nWe extracted the locations of buildings from satellite images of the UW campus. A soft implementation is more prudent in this scenario because the reliability of the location estimate is very low in the absence of GPS (for example when the user is inside the building) and thus a hard map constraint can lead to undesirable results. In our current implementation, p(ck = 1 | ek = inside,m(lk) = inside) = 0.6 and p(ck = 1 | ek = outside,m(lk) = outside) = 0.85, where m() is a (deterministic) function that maps a given location to either inside or outside a building based on the map information. These values were obtained after doing a grid search over the likely values that maximize the accuracy over a development set that was independent of the training and test sets. Note that the above soft map implementation may also be seen as using the map information as virtual evidence on the environment and location variables. The above model was trained using the EM algorithm until convergence.\nIn all there were 6 participants in the data collection effort, resulting in 25 data traces. Each trace had an average duration of 25 minutes. We performed leave-one-out cross-validation on these data sets. In each experiment, we trained the binary adaboost classifiers and discretized the margins of the weak learners, as explained in section 3.4. These discrete features and the GPS data were then used to jointly learn the parameters of the graphical model. The models were then evaluated based on the Viterbi output on the test trace.\nFigure 5 shows a typical trace. The bounded regions in the figure represent buildings. The GPS observations are shown as circles connected by a black line. The most likely path as estimated by the system is shown using the thick, grey line when the environment is outside, and by stars when the environment is inside. It can be seen that the model is able to successfully trace the GPS and determine when and where the person enters and exits the building. Furthermore, it can be seen that the system successfully corrects the erroneous GPS measurements occurring near one building (such outliers are often not indicated by a high hdop value).\nAccuracy\nFor each trace, accuracy was determined by counting the number of correctly labeled frames divided by the total number of frames. We separately determined accuracy in estimating the person\u2019s motion state and accuracy in estimating the environment. The mean and 95% confidence intervals of the motion state and environment accuracies achieved on the 25 test traces are summarized in Table 1 and 2.\nEach row in the tables provides the accuracy for a different model. AdaBoost is the output of the boosted classifiers on the data collected by the multi-sensor board (MSB). The second row in each of the tables shows the accuracies when a HMM was used to infer the state or environment independently. Note that this system only incorporates the boosted classifiers as observations, no GPS information is used. This system is similar to the technique used to infer activities in [12]. As can be seen, the temporal transition models provided by the HMMs result in about 5% performance gain in both state and environment tasks. Slight improvements can be gained by performing joint inference over the motion state and environment, as given in the third\nrow of each of the tables.\nThe next row shows the result of fusing the MSB and GPS sensor streams. As can be seen, making use of GPS information in conjunction with the sensor board performs better than using the sensor board alone (more than 4% for motion state). This increase is mostly due to the fact that the GPS sensor provides additional information about a person\u2019s motion velocity. Finally, we present the results of adding information about the locations of buildings to the system. Although the results indicate that adding this information does not yield a significant improvement in state accuracy, we found that this system outperformed the system without map information in about 86% of the traces. Furthermore, we observed that performing joint inference over a person\u2019s state and location results in more consistent estimates. For instance, the person\u2019s location estimate is typically inside (or outside) a building whenever the environment state is estimated to be inside (or outside). This is not the case when GPS is not considered during inference.\nVirtual Evidence\nAll tests reported so far use fully labeled training data. In this section, we discuss some of our efforts on training us-\ning virtual evidence (VE) for partially labeled data. Here, we focused on learning the parameters of activity recognition based on MSB information only (i.e., no GPS information is used).\nIn order to test the robustness w.r.t. sparseness of data annotation, we proceeded as follows. The boosted classifiers were trained once based on a fixed set of training data. Such training can be performed without significant overhead, since it does not require any temporal structure or transitions between activities. When training the HMM, our experiment simulates a user who labels only a fraction of each activity in an activity sequence. To do so we first determined in the fully labeled data the time blocks in which an activity did not change. We then removed increasing amounts of training labels from the start and end of each activity block. Put differently, we removed increasing amounts of labels on either side of each transition between activities, as illustrated in Figure 6. This generated a sequence of fully labeled blocks that were interleaved with time periods that had no labels. Our experiment assumes that the user annotates at least one time frame in each activity in the sequence. Thus, between two labeled blocks, only the two activities of these blocks can occur. However, since the blocks have different durations, the transitions between activities are typically not in the middle between blocks.\nThe information about activities in the gaps between blocks was modeled using virtual evidence (see Section 3.3). To see this, let us assume that two consecutive blocks have activities s1 and s2. In the (unlabeled) region of uncertainty between them, we varied relative weights of f(sk = s1) and f(sk = s2) using different functions. These include a linear interpolation function, a uniform (equal) weight over the two activities on either side of the transition, and a uniform function over all the activities. Note that the uniform function over all activities corresponds to the standard semi-supervised setting in which some parts are fully labeled while the rest of the data has no labels at all.\nFigure 7 shows the results of these experiments. The xaxis of this figure indicates the percent of frames dropped from each activity block (around each transition). While 0% corresponds to fully labeled data, 100% corresponds to a setting when only a single frame in the middle of each activity block is labeled. The thin, horizontal line gives the result obtained when using fully labeled data. The dotted line represents the result obtained when we made use of a linear interpolation function for VE, and the thick, solid line gives the result when using an equal value for the two activities between two labeled blocks. The dashed line indicates the results achieved when treating the gaps between labeled blocks as totally unlabeled, and training was performed using EMwithout any preference over the activities in these unlabeled regions.\nAs can be seen, when using VE the system performance\nis not effected to a large extent even when virtually all the labels were dropped! In other words, these results indicate that a user has to label only one frame for each activity he performs and still achieves virtually the same performance as a system trained with fully labeled data. This is a significant result as collecting fully labeled training data can be a highly challenging exercise. The dashed line in Figure 7 indicates that such a result cannot be achieved when using a standard approach in which the unlabeled data is trained without VE. In order to further test the above hypothesis, we repeated the experiment in which all but one label was dropped for each activity segment. This time, however, we chose the time point at which the label was set randomly, in contrast to choosing it based on the distance from the transitions. In other words, this simulates a practical system in which a user only labels one arbitrary frame for every activity. We ran this experiment 20 times and the mean accuracy was 81.69% (with a 95% confidence interval of 0.26). Thus the system was able to achieve an accuracy which is not significantly below the 82.1% achieved with the fully labeled training data!\nIn addition to making data annotation easier, VE is also able to achieve higher accuracy than the fully labeled data, as indicated by the increased accuracy when dropping 5% of the labels and using linear interpolation for VE (dotted line). This is due to the fact that the transition times between activities cannot always be labeled exactly. In such cases, VE is able to learn better transition times. The performance of VE with linear interpolation (dotted line) falls below the performance of VE with equal weight on two activities (thick solid line) as more data labels are dropped. This can be explained by the fact that the linear interpolation assumes that the transition between activities occurs in the middle of an unlabeled region, which may or may not be the case."}, {"heading": "6 Conclusions", "text": "We presented an approach to estimate a person\u2019s low-level activities and spatial context using data collected by a small wearable sensor device. Our approach uses a dynamic Bayesian network to model the dependencies between the different parts of the system. This allows us to perform efficient inference and learning in the model. In experiments we show that the recognition accuracy of our system is significantly higher than the accuracy achieved by existing techniques that do not perform joint reasoning about a person\u2019s activities and spatial context. Furthermore, by taking information about the locations of buildings into account, our approach is able to correct GPS errors, thereby achieving more consistent estimates of a person\u2019s location and environment context.\nWe also show how virtual evidence (VE) can be used to model partially labeled data. The experiments indicate that\nVE can yield better results than standard EM on fully labeled data. This is mostly due to the fact that VE can model uncertainty in the transition time between activities. The experiments additionally show that it is possible to train accurate HMM models by labeling only one time frame per activity in a training sequence.\nWhile these results are extremely encouraging, they only present the first step toward fully recognizing a person\u2019s context. We believe that virtual evidence provides a powerful mechanism for smart data annotation. In future work, we intend to develop a modeling language and graphical user interface that uses VE to model various types of user annotations. For instance, we performed preliminary experiments in which VE is used to constrain the types of transitions occurring in a certain part of an activity sequence. The user can then specify, for example, that he ran at time k, drove a vehicle at time k\u2032, and stopped exactly once for an unknown period of time in between.\nOur experiments also showed that it is extremely difficult to handle outliers and biases in GPS, especially when a person moves between buildings. By improving the handling of GPS, we intend to make better use of the availability of maps and satellite images. Finally, our current system relies on a discretization of space in order to perform efficient inference and learning. We are also investigating the use of Rao-Blackwellised particle filters in order better represent a person\u2019s location in continuous space [18]."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Tanzeem Choudhury, Jonathan Lester, and Gaetano Borriello for many useful discussions and for making their feature extraction and learning code available. Additional thanks go to the Intel Research Lab in Seattle for providing the sensor boards used in this research. This work has partly been supported by DARPA\u2019s ASSIST and CALO Programmes (contract numbers: NBCH-C-05-0137, SRI subcontract 27-000968), and by the NSF Human and Social Dynamics (HSD) program under contract number IIS-0433637."}], "references": [{"title": "Using GPS to learn significant locations and predict movement across multiple users", "author": ["D. Ashbrook", "T. Starner"], "venue": "Personal and Ubiquitous Computing, 7(5)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Activity recognition from userannotated acceleration data", "author": ["L. Bao", "S Intille"], "venue": "Proc. of the International Conference on Pervasive Computing and Communications", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "On soft evidence in bayesian networks", "author": ["J. Bilmes"], "venue": "Technical Report UWEETR-2004-0016, University of Washington, Dept. of EE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "On triangulating dynamic graphical models", "author": ["J. Bilmes", "C. Bartels"], "venue": "Uncertainty in Artificial Intelligence: Pro-  ceedings of the Nineteenth Conference ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "COLT: Proceedings of the Workshop on Computational Learning Theory", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "editors", "author": ["O. Chapelle", "A. Zien", "B. Sch\u00f6lkopf"], "venue": "Semisupervised learning. MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic temporal reasoning", "author": ["T. Dean", "K. Kanazawa"], "venue": "Proc. of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": "Morgan Kaufmann", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Using context-aware computing to reduce the perceived burden of interruptions from mobile devices", "author": ["J. Ho", "S. Intille"], "venue": "Proc. of the Conference on Human Factors in Computing Systems (CHI)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Wearable sensing to annotate meeting recordings", "author": ["N. Kern", "B. Schiele", "H. Junker", "P. Lukowicz", "G. Trster"], "venue": "In The 6th International Symposium on Wearable Computers (ISWC)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Recognizing context for annotating a live life recording", "author": ["N. Kern", "B. Schiele", "A. Schmidt"], "venue": "Personal and Ubiquituous Computingd", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "A hybrid discriminative-generative approach for modeling human activities", "author": ["J. Lester", "T. Choudhury", "N. Kern", "G. Borriello", "B. Hannaford"], "venue": "Proc. of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning and inferring transportation routines", "author": ["L. Liao", "D. Fox", "H. Kautz"], "venue": "Proc. of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Location-based activity recognition", "author": ["L. Liao", "D. Fox", "H. Kautz"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Recognizing workshop activity using body worn microphones and accelerometers", "author": ["P. Lukowicz", "J. Ward", "H. Junker", "M. St\u00e4ger", "G. Tr\u00f6ster", "A. Atrash", "T. Starner"], "venue": "Proc. of Pervasive Computing", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Opportunity Knocks: a system to provide cognitive assistance with transportation services", "author": ["D. Patterson", "L. Liao", "K. Gajos", "M. Collier", "N. Livic", "K. Olson", "S. Wang", "D. Fox", "H. Kautz"], "venue": "International Conference on Ubiquitous Computing (UbiComp)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann Publishers, Inc.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "Rao- Blackwellized particle filters for recognizing activities and spatial context from wearable sensors", "author": ["A. Raj", "A. Subramanya", "J. Bilmes", "D. Fox"], "venue": "Experimental Robotics: The 10th International Symposium, Springer Tracts in Advanced Robotics (STAR). Springer Verlag", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 11, "context": "Recent advances in wearable sensing and computing devices and in fast probabilistic inference techniques make possible the fine-grained estimation of a person\u2019s activities over extended periods of time [12].", "startOffset": 202, "endOffset": 206}, {"referenceID": 8, "context": "Such technologies enable applications ranging from context aware computing [9] to support for cognitively impaired people [16] to long-term health and fitness monitoring to automatic after action reporting of military missions.", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "Such technologies enable applications ranging from context aware computing [9] to support for cognitively impaired people [16] to long-term health and fitness monitoring to automatic after action reporting of military missions.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "There are in fact a variety of systems that utilize multiple sensors and measurements taken all over the body [11, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 14, "context": "There are in fact a variety of systems that utilize multiple sensors and measurements taken all over the body [11, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 6, "context": "Our statistical model, moreover, utilizes a dynamic Bayesian network (DBN) [7] for expressing the underlying dependencies between various types of information represented in our system, and performs efficient probabilistic inference based on a discretization of 2-D space.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Map There are many ways of expressing constraints in Bayesian networks [8], each of which can be implemented using a variety of algorithms.", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "In [18], we show how to apply RaoBlackwellized particle filters in order to perform approximate inference using a sample-based representation of the state space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "Our DBN-based probabilistic inference system uses a combination of a triangulation [4] (to produce a dynamic junction tree) and search-based methods within each clique.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "All DBN inference and incorporation of dynamic virtual evidence (see below) utilizes the graphical modeling toolkit (GMTK) system [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 18, "context": ", semi-supervised) learning [19, 5, 6], where parts of the data are labeled and other parts are left unlabeled.", "startOffset": 28, "endOffset": 38}, {"referenceID": 4, "context": ", semi-supervised) learning [19, 5, 6], where parts of the data are labeled and other parts are left unlabeled.", "startOffset": 28, "endOffset": 38}, {"referenceID": 5, "context": ", semi-supervised) learning [19, 5, 6], where parts of the data are labeled and other parts are left unlabeled.", "startOffset": 28, "endOffset": 38}, {"referenceID": 16, "context": "Virtual evidence was first defined in [17] as a way to depict evidence in a Bayesian network using the mechanism of hidden variables.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Virtual evidence can be used to incorporate soft evidence when computing posterior distributions over the most probable values of random variables (filtering, Viterbi, or MPE), and during EM training [3].", "startOffset": 200, "endOffset": 203}, {"referenceID": 11, "context": "To incorporate such heterogeneous sensor data, we employ the feature extraction process developed by Lester and colleagues [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "Here, we only provide a high-level description of this process, more information can be found in [12].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "We utilize the approach taken in [12] as our starting feature extraction procedure.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Considering these margins together, we can obtain an average distance to the decision boundary, which is then passed through a sigmoid function to produce a [0, 1]valued probability.", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "Bao and Intille [2] use multiple accelerometers placed on a person\u2019s body to estimate activities such as standing, walking, or running.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "Kern and colleagues [11, 10] and Lukowicz et al.", "startOffset": 20, "endOffset": 28}, {"referenceID": 9, "context": "Kern and colleagues [11, 10] and Lukowicz et al.", "startOffset": 20, "endOffset": 28}, {"referenceID": 14, "context": "[15] added a microphone to a similar set of accelerometers in order to extract additional context information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] showed how to apply boosting in order to learn activity classifiers based on the sensor data collected by the same sensor board we use in our research.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing.", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing.", "startOffset": 83, "endOffset": 89}, {"referenceID": 11, "context": "These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing.", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing.", "startOffset": 155, "endOffset": 163}, {"referenceID": 10, "context": "These existing approaches feed the sensor data or features into static classifiers [2, 9], a bank of temporally independent HMMs [12], or multi-state HMMs [10, 11] in order to perform temporal smoothing.", "startOffset": 155, "endOffset": 163}, {"referenceID": 0, "context": "For instance, Ashbrook and Starner [1] detect a person\u2019s significant places based on the time spent at a location.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "In [13], Liao and colleagues showed how to learn a person\u2019s outdoor transportation routines from GPS data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "More recently, the same authors presented a technique for jointly determining a person\u2019s activities and her significant places [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "This system is similar to the technique used to infer activities in [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "We are also investigating the use of Rao-Blackwellised particle filters in order better represent a person\u2019s location in continuous space [18].", "startOffset": 138, "endOffset": 142}], "year": 2006, "abstractText": "We introduce a new dynamic model with the capability of recognizing both activities that an individual is performing as well as where that individual is located. Our approach is novel in that it utilizes a dynamic graphical model to jointly estimate both activity and spatial context over time based on the simultaneous use of asynchronous observations consisting of GPS measurements, and a small mountable sensor board. Joint inference is quite desirable as it has the ability to improve accuracy of the model and consistency of the location and activity estimates. The parameters of our model are trained on partially labeled data. We apply virtual evidence to improve data annotation, giving the user high flexibility when labeling training data. We present results indicating the performance gains achieved by virtual evidence for data annotation and the joint inference performed by our system.", "creator": "TeX"}}}