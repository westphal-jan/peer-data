{"id": "1206.7064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2012", "title": "Software Verification and Graph Similarity for Automated Evaluation of Students' Assignments", "abstract": "These two approaches are also useful when it comes to providing comprehensible feedback that can help students improve the quality of their programs; we also present our corresponding tools, which are publicly available and open source; the tools are based on the LLVM low-level intermediate code representation, so that they can be applied to a number of programming languages; and experimental evaluations of the proposed grading framework are carried out on a number of university courses written in the C programming language. Experimental results show that automatically generated grades correlate to a high degree with manually defined grades, suggesting that the tools presented can find real applications in study and graduation.", "histories": [["v1", "Fri, 29 Jun 2012 16:10:20 GMT  (45kb)", "http://arxiv.org/abs/1206.7064v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["milena vujosevic-janicic", "mladen nikolic", "dusan tosic", "viktor kuncak"], "accepted": false, "id": "1206.7064"}, "pdf": {"name": "1206.7064.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Viktor Kuncak"], "emails": ["milena@matf.bg.ac.rs", "nikolic@matf.bg.ac.rs", "dtosic@matf.bg.ac.rs", "viktor.kuncak@epfl.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n70 64\nv1 [\ncs .A\nI] 2\nKeywords automated grading, software verification, graph similarity, computer supported education\nThis work was partially supported by the Serbian Ministry of Science grant 174021 and by Swiss National Science Foundation grant SCOPES IZ73Z0 127979/1.\nMilena Vujos\u030cevic\u0301-Janic\u030cic\u0301 \u00b7 Mladen Nikolic\u0301 \u00b7 Dus\u030can Tos\u030cic\u0301 Faculty of Mathematics, University of Belgrade, Belgrade, Serbia E-mail: milena@matf.bg.ac.rs E-mail: nikolic@matf.bg.ac.rs E-mail: dtosic@matf.bg.ac.rs\nViktor Kuncak, School of Computer and Communication Sciences, EPFL, Station 14, CH-1015 Lausanne, Switzerland E-mail: viktor.kuncak@epfl.ch"}, {"heading": "1 Introduction", "text": "Automated evaluation of programs is beneficial for both teachers and students (Pears, Seidman, Malmi, Mannila, Adams, Bennedsen, Devlin, & Paterson, 2007). For teachers, automated evaluation is helpful in grading assignments and it leaves more time for other activities with students. For students, it provides immediate feedback which is very important in process of studying, especially in computer science where students take a challenge of making the computer follow their intentions (Nipkow, 2012). Immediate feedback is particularly helpful at first programming courses where students have frequent and deep misconceptions (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Tos\u030cic\u0301, 2008).\nBenefits of automated evaluation of programs are even more significant in the context of online learning. A number of world\u2019s leading universities offer numerous online courses. The number of students taking such courses is measured in millions and quickly growing (Allen & Seaman, 2010). In online courses, the teaching process is carried out on the computer, the contact with teacher is already minimal and hence the fast and substantial automatic feedback is especially desirable. Therefore, automation of evaluation tasks in online learning is very important.\nMost of the tools for automated evaluation of students\u2019 code are based on automated testing (Douce, Livingstone, & Orwell, 2005). Testing is used for checking functional correctness of student\u2019s solution, i.e., whether the student\u2019s program exhibits the desired behavior on selected inputs. Testing can also be used for detecting bugs. We consider bugs to be runtime errors and exclude errors that only compromise functional correctness (for example, in programming language C, some important bugs are buffer overflow, null pointer dereferencing and division by zero). Although there is a variety of software verification tools that could enhance automated bug finding in students\u2019 programs (by analyzing the code without executing it), these tools are usually too complex to use and cannot be easily adapted for educational purposes.\nIn addition to checking functional correctness, an evaluation tool may also analyze program efficiency and/or complexity by profiling. Relevant aspects of program quality are also it\u2019s design and modularity (adequate decomposition of code to functions). These issues are addressed by checking similarity to a teacher provided solution. In order to check similarity, aspects that can be analyzed are: frequencies of keywords, number of lines of code, number of variables etc. Recently, a more sophisticated approach of grading students\u2019 programs by measuring the similarity of related graphs has been proposed (Wang, Su, Wang, & Ma, 2007; Naude\u0301, Greyling, & Vogts, 2010). Recent surveys of related approaches are given elsewhere (Ala-Mutka, 2005; Ihantola, Ahoniemi, Karavirta, & Seppa\u0308la\u0308, 2010).\nIn this paper, we propose a new grading framework for automated evaluation of students\u2019 programs aiming primarily at introductory programming courses. The framework is based on merging information from three different evaluation methods: it merges results obtained by software verification (automated bug finding) and control flow graph (CFG) similarity measurement with results obtained by automated testing. The synergy between automated testing, verification, and similarity measurement improves the quality and precision of automated grading and overcoming the individual weaknesses of these approaches. Our experimental results show that our framework can lead to a grading model that highly correlates\nto manual grading and therefore gives promises for real-world applicability in education.\nWe also briefly discuss tools for software verification (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012) and CFG similarity (Nikolic\u0301, 2013), that we use for assignment evaluation. These tools, based on novel methods, are publicly available and open source.1 Both tools use the low-level intermediate code representation LLVM. Therefore, they could be applied to a number of programming languages and could be complemented with other existing LLVM based tools (e.g., tools for automated test generation). Also, the tools are enhanced with support for meaningful and comprehensible feedback to students, so they can be used both in the process of studying and in the process of grading assignments.\nOverview of the paper. Necessary background information is given in Section 2. Motivating examples for the synergy of the three proposed approaches are given in Section 3. The grading setting and the corpus used for evaluation are described in Section 4. The role of the verification techniques in automated evaluation is discussed in Section 5 and the role of structural similaritymeasurement is discussed in Section 6. An experimental evaluation of the proposed framework for automated grading is presented in Section 7. Section 8 contains information about related work. Conclusions and outlines of possible directions of future work are given in Section 9."}, {"heading": "2 Background", "text": "This section provides an overview of intermediate languages, the LLVM tool, software verification, the LAV tool, control flow graphs and graph similarity measurement.\nIntermediate languages and LLVM. An intermediate language separates concepts and semantics of a high level programming language from low level issues relevant for a specific machine. Examples of intermediate languages include the ones used in LLVM and .NET framework. LLVM2 is an open source, widely used, rich compiler framework, well suited for developing new mid-level language-independent analyses and optimizations of all sorts (Lattner & Adve, 2002). LLVM intermediate language is assembly-like language with simple RISC-like instructions. It provides easy construction of control flow graphs of program functions and of entire programs. There is a number of tools using LLVM for various purposes, including software verification. LLVM has front-ends for C, C++, Ada and Fortran, while there are external projects for translating a number of other languages to LLVM intermediate representation (e.g., Python, Ruby, Haskell, Java, D, PHP, Pure, and Lua).\nSoftware verification and LAV. Verification of software and automated bug finding are some of the greatest challenges in computer science. Software bugs cost the world economy billions of dollars annually (Tassey, 2002). Software verification\n1 http://argo.matf.bg.ac.rs/?content=lav 2 http://llvm.org/\ntools aim at automatically checking correctness properties. Different approaches to automated checking of software properties exist, such as symbolic execution (King, 1976), model checking (Clarke, 2008) and abstract interpretation (Cousot & Cousot, 1977). Software verification tools usually use automated theorem provers.\nLAV (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012) is an open-source tool for statically verifying program assertions and locating bugs such as buffer overflows, pointer errors and division by zero. LAV uses popular LLVM infrastructure. As a result, it supports several programming languages that compile into LLVM, and benefits from the robust LLVM front ends. LAV is primarily aimed at programs in the C programming language, in which the opportunities for errors are abundant. For each safety critical command, LAV generates a first order logic formula that represents its correctness condition. This formula is checked by one of the several SMT solvers (Barrett, Sebastiani, Seshia, & Tinelli, 2009) used by LAV. If a command cannot be proved to be safe, LAV translates a potential counterexample from the solver into a program trace that exhibits this error. It also extracts the values of relevant program variables along this trace. LAV was already used, to a limited extent, for automated bug finding in students\u2019 assignments (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012).\nControl flow graph. A control flow graph (CFG) is a graph-based representation of all paths that might be traversed through a program during its execution. Each node of CFG represents a sequence of commands containing only one path of execution (there are no jumps, loops, conditional statements, etc.). The control flow graphs can be produced by various tools, including LLVM. A control flow graph clearly separates the structure of the program and its contents. Therefore, it is a suitable representation for structural comparison of programs.\nGraph similarity and neighbor matching method. There are many similarity measures for graphs and their nodes (Kleinberg, 1999; Heymans & Singh, 2003; Blondel, Gajardo, Heymans, Snellart, & van Dooren, 2004; Nikolic\u0301, 2013). These measures have been successfully applied in several practical domains like ranking of query results, synonym extraction, database structure matching, construction of phylogenetic trees, analysis of social networks, etc. A short overview of similarity measures for graphs can be found in the literature (Nikolic\u0301, 2013).\nA specific similaritymeasure for graph nodes called neighbor matching, possesses properties relevant for our purpose that other similar measures lack (Nikolic\u0301, 2013). It allows similarity measure for graphs to be defined based on similarity scores of their nodes. The notion of similarity of nodes is based on the intuition that two nodes i and j of graphs A and B are considered to be similar if neighbor nodes of i can be matched to similar neighbor nodes of j. More detailed definitions follow. In the neighbor matching method, if a graph contains an edge (i, j), the node i is called an in-neighbor of node j in the graph and the node j is called an outneighbor of the node i in the graph. An in-degree id(i) of the node i is the number of in-neighbors of i, and an out-degree od(i) of the node i is the number of outneighbors of i.\nIf A and B are two finite sets of arbitrary elements, a matching of elements of sets A and B is a set of pairs M = {(i, j)|i \u2208 A, j \u2208 B} such that no element of one set is paired with more than one element of the other set. For the matching M , enumeration functions f : {1, 2, . . . k} \u2192 A and g : {1, 2, . . . k} \u2192 B are defined\nsuch that M = {(f(l), g(l))|l = 1,2, . . . , k} where k = |M |. If w(a, b) is a function assigning weights to pairs of elements a \u2208 A and b \u2208 B, the weight of a matching is the sum of weights assigned to the pairs of elements from the matching. The goal of the assignment problem is to find a matching of elements of A and B of the highest weight (if two sets are of different cardinalities, some elements of the larger set will not have corresponding elements in the smaller set). The assignment problem is usually solved by the well-known Hungarian algorithm of complexity O(mn2) where m = max(|A|, |B|) and n = min(|A|, |B|) (Kuhn, 1955), but there are also more efficient algorithms.\nThe calculation of similarity of nodes i and j, denoted xij , is based on iterative procedure given by the following equations:\nxk+1ij \u2190 sk+1in (i, j) + s k+1 out (i, j)\n2\nwhere\nsk+1in (i, j) \u2190 1\nmin\nnin \u2211\nl=1\nxkfin ij (l)gin ij (l) s k+1 out (i, j) \u2190\n1\nmout\nnout \u2211\nl=1\nxkfout ij (l)gout ij (l) (1)\nmin = max(id(i), id(j)) mout = max(od(i), od(j))\nnin = min(id(i), id(j)) nout = min(od(i), od(j))\nwhere functions f inij and g in ij are the enumeration functions of the optimal matching of in-neighbors for nodes i and j with weight function w(a, b) = xkab, and analogously for foutij and g out ij . In Equations 1, 0 0 is defined to be 1 (used in case when min = nin = 0 or mout = nout = 0). Initial similarity values x 0 ij are set to 1 for each i and j. The termination condition is maxij |x k ij \u2212 x k\u22121 ij | < \u03b5 for some chosen precision \u03b5 and the iterative algorithm is proved to converge (Nikolic\u0301, 2013). The similarity matrix [xij] reflects the similarities of nodes of two graphs A and B. The similarity of the graphs can be defined as the weight of the optimal matching of nodes from A and B divided by the number of matched nodes (Nikolic\u0301, 2013)."}, {"heading": "3 The Need for Synergy of Testing, Verification, and Similarity Measurement", "text": "Automated testing of programs is a very important part of the evaluation process. Unfortunately, the grading system is directly influenced by the choice of test cases. Also, no matter whether the test cases are automatically generated or manually designed, testing cannot guarantee neither functional correctness of a program nor the absence of bugs.\nFor checking functional correctness, combination of random testing with evaluator-supplied test cases is a common choice (Mandal, Mandal, & Reade, 2007). However, randomly generated test cases are not likely to hit a bug if it exists (Godefroid, Levin, & Molnar, 2012), while manually choosing all important test cases is not a trivial job and can be time consuming. It is not sufficient that test cases cover all important paths through the program. It is also important to\ncarefully choose values of the variables for each path \u2014 for some values along the same path a bug can be detected while for some other values the bug can stay undetected.\nAlso, manually generated test cases are designed according to the expected solutions, while the evaluator cannot predict all the important paths through the student\u2019s solution. Even running a test case that hits a certain bug (for example, a buffer overflow bug in a C program) does not necessarily lead to any visible undesired behavior if the running is done in a normal (or sandbox) environment. Finally, if one manages to hit a bug by a test case, if the bug produces the Segmentation fault message, it is not a feedback that student can easily understand and use for debugging the program. In the context of automated grading, this feedback cannot be easily used since it may have different causes. In contrast to program testing, software verification tools like Pex (Tillmann & Halleux, 2008), Klee (Cadar, Dunbar, & Engler, 2008), S2E (Chipounov, Kuznetsov, & Candea, 2011), CBMC (Clarke, Kroening, & Lerda, 2004), ESBMC (Cordeiro, Fischer, & Marques-Silva, 2009), and LAV (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012) can give much better explanations (e.g., the kind of bug and the program trace that introduces an error).\nThe example function shown at Figure 1 is extracted from a student\u2019s code written on an exam. It calculates the maximum value of each row of a matrix and writes these values into an array. This function is used in a context where the memory for the matrix is statically allocated and numbers of rows and columns are less or equal to the allocated sizes of the matrix. However, in the line 11, there is a possible buffer overflow bug, since i + 1 can exceed the allocated number of rows for the matrix. It is possible that this kind of a bug does not affect the output of the program or destroy any data, but in a slightly different context it can be harmful, so students should be warned and penalized for making such errors. The bugs like this one can be missed in testing but are easily discovered by verification tools like LAV.\nFunctional correctness and absence of bugs are not the only important aspects of students\u2019 programs. The programs are often supposed to meet certain require-\nments concerning the structure of the program, such as its modularity (adequate decomposition of code to functions) or simplicity. Figure 2 shows two solutions of different modularity or structural simplicity for two problems. Neither testing, nor software verification can be used to assess these aspects of the programs. This problem can be addressed by checking the similarity of student\u2019s solution with a teacher provided solution, i.e., by analyzing the similarity of their related graphs (e.g. CFGs) (Wang et al., 2007; Naude\u0301 et al., 2010; Nikolic\u0301, 2013).3\nFinally, using similarity only (like in (Wang et al., 2007; Naude\u0301 et al., 2010)) or even with support of a bug finding tool, would miss to penalize incorrectness of program\u2019s behavior. Figure 3 gives a simple example program, extracted from a real student\u2019s solution, that is very similar to the expected solution and without verification errors. However, this program is not functionally correct. Therefore, we conclude that the synergy of these three approaches is needed for sophisticated evaluation of students\u2019 assignments.\n3 In Figure 2, the second example could also be distinguished by profiling for large inputs, because it is quadratic in one case and linear in the other. However, profiling cannot be used to assess structural properties in general."}, {"heading": "4 Grading Setting", "text": "There may be different grading settings depending on aims of the course and goals of teachers. The setting used at an introductory course of programming in C (at University of Belgrade) is taking exams on computers and expecting from students to write working programs. In order to help students achieve this goal, each assignment is provided with several test cases which illustrate desired behavior of the solution. Students are also provided with sufficient (but limited) time for developing and testing programs. If a student fails to provide a working program that gives correct results for given test cases, his/her solution is not further examined. Otherwise, the program is tested by additional test cases (unknown to students) and a certain amount of points is given corresponding to the test cases successfully passed. Only if all these test cases are successfully passed, the program is further manually examined and may obtain additional points with respect to other features of the program (efficiency, modularity, simplicity, absence of bugs, etc).\nAll experiments described in this paper were preformed on a corpus of programs written by students on the exams, following the described grading setting. The corpus consists of 266 solutions to 15 different problems. These problems include numerical calculations, manipulations with arrays and matrices, manipulations with strings, and manipulations with data structures. Only programs that passed all test cases were included in this corpus. These programs are the main target of our automated evaluation technique since the manual grading was applied only in this case and we want to explore potentials for completely eliminating manual grading. These programs obtained 80% of the maximal score (as they passed all test cases) and additional potential 20% were given by manual inspection. The grades are expressed at the scale from 0 to 10. The corpus together with problem descriptions and the final marks are publicly available.4"}, {"heading": "5 Assignment Evaluation and Software Verification", "text": "In this section we show benefits of using software verification tool in assignment evaluation, e.g., generating useful feedback for students and providing improved assignment evaluation for teachers.\n5.1 Software verification for assignment evaluation\nNo software verification tool can report all the bugs in a program without introducing false alarms (due to the undecidability of the halting problem). False alarms (i.e., reported \u201dbugs\u201d that are not real bugs) arise as a consequence of approximations that are necessary in modeling of programs.\nThe most important approximation is concerned with dealing with loops. Different verification approaches use various techniques for dealing with loops. These techniques range from under-approximations of loops to over-approximations of loops. Under-approximation of loops, as in bounded model checking techniques\n4 http://argo.matf.bg.ac.rs/?content=lav\n(Clarke, 2008), uses a fixed number n for loop unwinding. In this case, if the code is verified successfully, it means that the original code has no bugs for n or less passes through the loop. However, it may happen that some bug remains undiscovered if the unwinding is performed an insufficient number of times. Over-approximation of loops can be done by simulation of first n and last m passes through the loop (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012) or by using abstract interpretation techniques (Cousot & Cousot, 1977). If there are no bugs detected in the over-approximated code, then the original code has no bugs too. However, in this case, a false alarm can appear after or inside a loop. On the other hand, precise dealing with loops, like in symbolic execution techniques, can be non terminating.\nFalse alarms are highly unwelcome in software development, but still are not critical \u2014 the developer can fix the problem or confirm that the reported problem is not really a bug (and both of these are situations that the developer can expect and understand). However, false alarms in assignment evaluation are rather critical and have to be eliminated. For teachers, there should be no false alarms, because the evaluation process should be as automatic and reliable as possible. For students, there should be no false alarms because they would be confused if told that something is a bug when it is not. In order to eliminate false alarms, a system may be non-terminating or may miss to report some real bugs. In assignment evaluation, the second choice is more reasonable \u2014 the tool has to be terminating, must not introduce false alarms, even if the price is missing some real bugs. These requirements make applications of software verification in education rather specific, and special care has to be taken when these techniques are applied.\n5.2 LAV for assignment evaluation\nLAV is a general purpose verification tool and has a number of options that can adapt its behavior to the desired context. When running LAV in the assignment evaluation context, most of these options can be fixed.\nThe most important choice for the user is the choice of the way in which LAV deals with loops. LAV has support for both over-approximation of loops and for fixed number of unwinding of loops (under-approximation), two common techniques for dealing with loops. Setting up the upper loop bound (if underapproximation is used), is problem dependent and should be done by the teacher for each assignment.\nWe use LAV in the following way. LAV is first invoked with its default parameters \u2014 over-approximation of loops. Since this technique can introduce false alarms, if a potential bug is found after or inside a loop, the verification is invoked again but this time with fixed unwinding parameter. If the bug is still present, then it is reported. Otherwise, the previously detected potential bug is considered to be a false alarm and it is not reported.\nIn software verification, each detected bug is important and should be reported. However, some bugs can confuse novice programmers, like the one shown in Figure 4. In this code, at the line 11, there is a possible buffer overflow. For instance, for n = 0x80000001 only 4 bytes will be allocated for the pointer array, because of an integer overflow. This is a verification error, but a teacher may decide not to consider this kind of bugs. For this purpose, LAV can be invoked in mode for students (so the bugs like this one are not reported).\nTo a limited extent, LAV was already used on students\u2019 assignments at an introductory programming course (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012). In these experiments, most of the programs from the corpus were not functionally correct. It was shown that the vast majority of bugs, produced by students, follow wrong expectations \u2014 for instance, expectations that input parameters of their programs will meet certain constraints and that memory allocation will always succeed. It is also noticed that most of the reported bugs are consequence of only few oversights. In many cases, omission of a necessary check produces several bugs in the rest of the program. Therefore, the number of bugs, as reported by a verification tool, is not a reliable indicator of program quality. This property will be taken into account in automated grading.\n5.3 Experimental evaluation\nAs discussed in Section 3, programs that successfully pass a testing phase can still contain bugs. To show that this problem is practically important, we used LAV to analyze programs from the corpus described in Section 4.\nFor each problem, LAV was ran with its default parameters, and programs with potential bugs were checked with under-approximation of loops, as described in Section 5.2.5 The results are shown in Table 1. The time that LAV spent in analyzing the programs was typically negligible.6 LAV discovered bugs in 35 solutions that successfully passed the testing. There was one bug missed by manual inspection and detected by LAV and one bug missed by LAV and detected by manual inspection. The bug missed by manual inspection was the one described in Section 3 and given in Figure 1. The bug missed by LAV was a consequence of the problem formulation which was too general to allow a precise unique upper\n5 When analyzing the solutions of problems 3, 5 and 8, only under-approximation of loops was used. This was the consequence of the formulation of the problems given to the students. Namely, the formulation of these problems contained some assumptions on input parameters. These assumptions implied that some potential bugs should not be considered (because these are not bugs when these additional assumptions are taken into account).\n6 Generally, in this context, a time limit can be given to the verification tool and if it was exceeded no bug will be reported (in order to avoid reporting false alarms) or a program can be checked using the same parameters but with another underlying solver (if applicable for the tool).\nloop unwinding parameter value for all possible solutions. There were just two false alarms produced by LAV when the default parameters were used. These false alarms were eliminated when the tool was invoked for the second time with a specified loop unwinding parameter, and hence there were no false alarms in the final outputs. In summary, the presented results show that a verification tool like LAV can be used as a complement to automated testing that improves the evaluation process.\n5.4 Feedback for students and teachers\nLAV can be used to provide a meaningful and comprehensible feedback to students while writing their programs. Information like the line number, the kind of the error, program trace that introduces the error and values of the variables along this trace, can help student improve the solution. It can also remind the student to add an appropriate check that is missing. The example given in Figure 5, extracted from a student\u2019s code written on an exam, shows the error detected by LAV and the generated hint.\nFrom the software verification support, a teacher can obtain the information if the student\u2019s program contains a bug. The teacher can use this information in grading assignments by himself. Alternatively, this information can be taken into account within the wider integrated framework for obtaining automatically proposed final grade, as discussed in Section 7."}, {"heading": "6 Assignment Evaluation and Structural Similarity of Programs", "text": "In this section we propose a similarity measure for programs based on their control flow graphs, perform its experimental evaluation, and point to ways it can be used to provide feedback for students and teachers.\n6.1 Similarity of CFGs for assignment evaluation\nTo evaluate structural properties of programs, we take the approach of comparing students\u2019 programs to solutions provided by the teacher. Student\u2019s program is considered to be good if it is similar to some of the programs provided by the teacher (Wang et al., 2007). In order to perform a comparison, a suitable program representation and a similarity measure are needed. As already noticed in Section 2, there is a control flow graph (CFG) corresponding to each program. The CFG reflects the structure of the program. Also, there is a linear code sequence attributed to each node of the CFG which we call the node content. We assume that the code is in the intermediate LLVM language. In order to measure the similarity of programs, both the similarity of graphs\u2019 structures and the similarity of node contents should be considered. We take the approach of combining the similarity of node contents with topological similarity of graph nodes described in Section 2.\nSimilarity of node contents. The node content is a sequence of LLVM instructions. A simple way of measuring the similarity of two sequences of instructions s1 and s2 is using the edit distance between them d(s1, s2) \u2014 the minimal number of insertion, deletion and substitution operations over the elements of the sequence by which one sequence can be transformed into another (Levenshtein, 1966). In order for edit distance to be computed, the cost of each insertion, deletion and substitution operation has to be defined. We define the cost of insertion and deletion of an instruction to be 1. Next, we define the cost of substitution of instruction i1 by instruction i2. Let opcode be a function that maps an instruction to its opcode (a part of instruction that specifies the operation to be performed). Let opcode(i1) and opcode(i2) be function calls. Then, the cost of substitution is 1 if i1 and i2 call different functions, and 0 if they call the same function. If opcode(i1) or opcode(i2)\nis not a function call, the cost of substitution is 1 if opcode(i1) 6= opcode(i2), and 0 otherwise. Let n1 = |s1|, n2 = |s2|, and let M be the maximal edit distance over two sequences of length n1 and n2. Then, the similarity of sequences s1 and s2 is defined as 1\u2212 d(s1, s2)/M .\nAlthough it could be argued that the proposed similarity measure is rough since it does not account for differences of instruction arguments, it is simple, easily implemented, and intuitive.\nFull similarity of nodes and similarity of CFGs. The topological similarity of nodes can be computed by the method described in Section 2. However, purely topological similarity does not account for differences of the node content. Hence, we modify the computation of topological similarity to include the apriori similarity of nodes. The modified update rule is:\nxk+1ij \u2190\n\u221a\nyij \u00b7 sk+1in (i, j) + s k+1 out (i, j)\n2\nwhere yij are the similarities of contents of nodes i and j and s k+1 in (i, j) and sk+1out (i, j) are defined by Equations 1. Also, we set x 0 ij = yij . This way, both content similarity and topological similarity of nodes are taken into consideration. The similarity of CFGs can be defined based on the node similarity matrix as described in Section 2. Note that both the similarity of nodes and the similarity of CFGs take values in the interval [0, 1].\nIt should be noted that our approach provides both the similarity measure for CFGs and the similarity measure for their nodes (xij). In addition to evaluating similarity of programs, this approach enables matching of related parts of the programs by matching the most similar nodes of CFGs. This could serve as a basis of a method for suggesting which parts of the student\u2019s program could be further improved.\n6.2 Experimental evaluation\nIn order to show that the proposed program similarity measure corresponds to some intuitive notion of program similarity, we performed the following experiment. For each program from the corpus already described in Section 4, we found the most similar program from the rest of the corpus and counted how often these programs are the solutions for the same problem. That was the case for 90% of all programs. This shows that our similarity measure performs well since with high probability, for each program, the program that is the most similar to it, corresponds to the same problem. The inspection suggests that in most cases, where the programs do not correspond to the same problem, student took an original approach to solving the problem.\nThe CFGs of the programs from the corpus are rather small. The average size of CFGs is 15 nodes. The time spent to compute the similarity of two programs is negligible. However, out of the educational context where CFGs could have thousands of nodes, the scalability might be an issue.\n6.3 Feedback for students and teachers\nThe students can benefit from program similarity evaluation while learning and exercising, assuming that the teacher provided a valid solution or set of solutions to the evaluation system. In introductory programming courses, most often a student\u2019s solution can be considered as better if it is more similar to one of the teacher\u2019s solutions (Wang et al., 2007). In Section 7 we show that the similarity measure can be used for automatic calculation of a grade (a feedback that students easily understand). Moreover, we show that there is a significant linear dependence of the grade on the similarity value. Due to that linearity, the similarity value can be considered as an intuitive feedback, but also it can be translated into descriptive estimate. For example, the feedback could be that the solution is dissimilar (0-0.5), roughly similar (0.5-0.7), similar (0.7-0.9) or very similar (0.9-1) to one of the desired solutions.\nThe teachers can use the similarity information in automated grading, as discussed in Section 7."}, {"heading": "7 Automated Grading", "text": "We believe that automated grading can be performed by calculating a linear combination of different scores measured for the student\u2019s solution. We propose a linear model for prediction of the teacher-provided grade of the following form:\ny\u0302 = \u03b11 \u00b7 x1 + \u03b12 \u00b7 x2 + \u03b13 \u00b7 x3\nwhere\n\u2013 y\u0302 is the automatically predicted grade, \u2013 x1 is a result obtained by automated testing expressed in the interval [0,1], \u2013 x2 is 1 if in the student\u2019s solution is correct as reported by the software verifi-\ncation tool, and 0 otherwise, \u2013 x3 is the maximal value of similarity between the student\u2019s solution and each\nof the teacher provided solutions (its range is [0,1]).\nIt should be noted that we do not use bug count as a parameter, as discussed in Section 5.2. Different choices for the coefficients \u03b1i, for i = 1, 2, 3 could be proposed. In our case, one simple way could be \u03b11 = 8, \u03b12 = 1, and \u03b13 = 1 since all programs in our training set won 80% of the full grade due to the success in testing. However, it is not always clear how the teacher\u2019s intuitive grading criterion can be factored to automatically measurable quantities. Teachers need not have the intuitive feeling for all the variables involved in the grading. For instance, the behavior of any of the proposed similarity measures including ours (Wang et al., 2007; Naude\u0301 et al., 2010; Nikolic\u0301, 2013) is not clear from their definitions only. So, it may be unclear how to choose weights for different variables when combining them in the final grade or if some of the variables should be nonlinearly transformed in order to be useful for grading. A natural solution is to try to tune the coefficients \u03b1i, for i = 1,2, 3 so that the behavior of the predictive model corresponds to the teacher\u2019s grading style. For that purpose, coefficients can be determined automatically using least squares linear regression (Gross, 2003) if a manually graded corpus of students\u2019 programs is provided by the teacher.\nIn our evaluation the corpus of programs was split into a training and a test set where the training set consisted of two thirds of the corpus and the test set consisted of one third of the corpus. The training set contained solutions of eight different problems and the test set contained solutions of remaining seven problems.\nDue to the nature of the corpus, for all the instances it holds x1 = 1. Therefore, while it is clear that the number of test cases the program passed (x1) is useful in automated grading, this variable can not be analyzed based on this corpus.\nThe optimal values of coefficients \u03b1i, i = 1, 2, 3, with respect to the training corpus, are determined using least squares linear regression. The obtained equation is\ny\u0302 = 6.058 \u00b7 x1 + 1.014 \u00b7 x2 + 2.919 \u00b7 x3\nThe formula for y\u0302 may seem counterintuitive. Since the minimal grade in the corpus is 8 and x1 = 1 for all instances, one would expect that it holds \u03b11 \u2248 8. The discrepancy is due to the fact that for the solutions in the corpus, the minimal value for x3 is 0.68 \u2014 since the solutions are good (they all passed the testing) there are no programs with low similarity value. Taking this into consideration, one can rewrite the formula for y\u0302 as\ny\u0302 = 8.043 \u00b7 x1 + 1.014 \u00b7 x2 + 0.934 \u00b7 x \u2032 3\nwhere x\u20323 = x3\u22120.68 1\u22120.68 so the variable x \u2032\n3 takes values from the interval [0,1]. This means that when the range of variability of both x2 and x3 is scaled to the interval [0,1], their contribution to the mark is rather similar.\nTable 2 shows the comparison between the model y\u0302 and three other models. Model y\u03021 = 8 \u00b7 x1 + x2 + x3 has predetermined parameters, model y\u03022 is trained just with verification information x2 (without similarity measure), and model y\u03023 is trained only with similarity measure x3 (without verification information). Results show that the performance of model y\u0302 on the test set (consisting of problems not appearing in the training set) is outstanding \u2014 the correlation is 0.842 and the model accounts for 71% of the variability of teacher provided grade. These results indicate a strong and reliable dependence between teacher provided grade and the variables xi, meaning that a grade can be reliably predicted by y\u0302. Also, y\u0302 is much better than other models. This shows that the approach using both verification information and graph similarity information is superior to approaches using only one source of information, and also that automated tuning of coefficients of the model provides better prediction than giving them in advance.\nInspection of solutions that yielded the biggest error in prediction suggests that the greatest source of discrepancy of automatically provided and teacher provided grades are the original solutions given by students and the solutions that the teacher did not predict in advance. However, we cannot exclude other factors apart form presence of bugs and similarity to model solutions, that govern human grading process."}, {"heading": "8 Related work", "text": "Automated testing is the most common way of evaluating students\u2019 programs (Douce et al., 2005). Test cases are usually supplied by a teacher and/or randomly generated (Mandal et al., 2007). A lot of systems use this approach, for\nexample, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).\nSoftware verification techniques are not commonly used in automated evaluation of programs. There are limited experiments on using Java PathFinder model checker for automated test case generation (Ihantola, 2007). Tools with integrated support for automated testing and verification, e.g. Ceasar (Garavel, 1998), are usually too complex and not aimed for educational purposes. To the authors\u2019 knowledge, there is no other software verification tool deployed in process of automated bug finding as a complement to automated testing of students\u2019 programs. The tool LAV was already used, to a limited extent, for finding bugs in students\u2019 programs (Vujos\u030cevic\u0301-Janic\u030cic\u0301 & Kuncak, 2012). In that work, a different sort of corpus was used, as discussed in Section 5.2. Also, that application did not aim at automated grading, and instead was made in the wider context of design and development of LAV as a general-purpose SMT-based error finding platform.\nWang et al. proposed a grading approach for assignments in C based only on program similarity (Wang et al., 2007). It relies on dependence graphs (Horwitz & Reps, 1992) as program representation. They perform various code transformations in order to standardize the representation of the program. In this approach, the similarity is calculated based on comparison of structure, statement, and size which are weighted by some predetermined coefficients. Their approach is evaluated on 10 problems, 200 solutions each, and obtain good results compared to manual grading. Manual grading was performed strictly according to the criterion that indicates how the scores are awarded for structure, statements used, and size. However, it is not quite obvious that human grading is always expressed strictly in terms of these three factors. An advantage of our approach compared to this one is automated tuning of weights corresponding to different variables used in grading, instead of using the predetermined ones. Since teachers do not need to have an intuitive feeling for different similarity measures, it may be unclear how the corresponding weights should be chosen. Also, we avoid language dependent transformations by using LLVM which makes our approach applicable to large variety of programming languages. Very similar approach to the one of Wang et al. was presented by Li et al. (Li, Pan, Zhang, Chen, Nie, & He, 2010).\nAnother approach to grading assignments based only on graph similarity measure is proposed by Naude\u0301 et al. (Naude\u0301 et al., 2010). They represent programs\nas dependence graphs and propose directed acyclic graph (DAG) similarity measure. In their approach, for each solution to be graded, several similar solutions in the training set are found and the grade is formed by combining grades of these solutions with respect to matched portions of the similar solutions. The approach was evaluated on one assignment problem and the correlation between human and machine provided grades is the same as ours. For appropriate grading they recommend at least 20 manually graded solutions of various qualities for each problem to be automatically graded. In the case of automatic grading of high quality solutions (as is the case with our corpus), using 20 manually graded solutions, their approach achieves 16.7% relative error, while with 90 manually graded solutions it achieves around 10%. The improvement that our approach provides is reflected through several indicators. We used a heterogeneous corpus of 15 problems instead of one. Our approach uses 1 to 3 model solutions for each problem to be graded and a training set for weight estimation which does not need to contain the solutions for the program to be graded. So, after the initial training has been performed, for each new problem only few model solutions should be provided. Using 1 to 3 model solutions, we achieve 10% relative error (see Table 2). Due to the use of the LLVM platform, we do not use language dependent transformations, so our approach is applicable to large number of programming languages. The similarity measure we use, called neighbor matching, is similar to the one of Naude\u0301 et al., but for our measure, important theoretical properties (e.g. convergence) are proven (Nikolic\u0301, 2013). The neighbor matching method was already applied to several problems but in all these applications its use was limited to ordinary graphs with nodes without any internal specifics. In order to be applied to CFGs, the method was modified to include node content similarity which was independently defined as described in Section 6.1.\nFinally, as a distinctive feature of our system, we are not aware of open source implementations of the similarity based approaches. A drawback in the comparison of our approach to previously described ones is that our corpus consists of high quality solutions due to the grading setting at the course.\nApart of assignment grading, regression techniques were also used for final grade forecasting with good results. For this purpose, Macfadyen et al. used data from learning management system and identified variables most useful for the prediction, e.g., number of assessments completed and number of discussion and mail messages sent (Macfadyen & Dawson, 2010). Kotsiantis performed successful forecasting based on demographic characteristics of students, results of several written assignments, and class attendance (Kotsiantis, 2012)."}, {"heading": "9 Conclusions and Further Work", "text": "We presented two techniques that can be used for improving automated evaluation of students\u2019 programs. First one is based on software verification and second one on CFG similarity measurement. Both techniques can be used for providing useful and helpful feedback to students and for improving automated grading for teachers. In our evaluation, we show that synergy of these techniques offers more information useful for automated grading than any of them independently. Also, we obtained good results in prediction of the grades for a new set of assignments. This shows that our approach can be trained to adapt to teacher\u2019s grading style on\nseveral teacher graded problems and then be used on different problems using only few model solutions per problem. An important advantage of our approach is independence of specific programming language since LLVM platform (which we use to produce intermediate code) supports large number of programming languages. We also provide the corresponding open source tools.\nIn our future work we are planning to make an integrated web-based system with support for the mentioned techniques along with compiling, automated testing, profiling and detection of plagiarism of students\u2019 programs. Also, we intend to improve feedback to students by indicating missing or redundant parts of code compared to the teacher\u2019s solution. This feature would rely on the fact that our similarity measure provides the similarity values for nodes of CFGs, and hence enables matching the parts of code between two solutions. If some parts of the solutions cannot be matched or are matched with very low similarity, this can be reported to the student. On the other hand, the similarity of the CFG with itself could reveal the repetitions of parts of the code and suggest that refactoring could be performed. We are planning to integrate LLVM-based open source tool KLEE (Cadar et al., 2008) for automated test case generation and also to add support for teacher supplied test cases.\nWe are also planning to explore potential for using software verification tools for proving functional correctness of students\u2019 programs. This task would pose new challenges. Testing, profiling, bug finding and similarity measurement are used on original students\u2019 programs, which makes the automation easy. For verification of functional correctness, the teacher would have to define correctness conditions (possibly in terms of implemented functions) and insert corresponding assertions in appropriate places in students\u2019 programs which should be possible to automate in some cases, but it is not trivial in general. In addition, for some programs it is not easy to formulate correctness conditions (for example, for programs that are expected only to print some messages on standard output)."}], "references": [{"title": "A Survey of Automated Assessment Approaches for Programming Assignments", "author": ["K.M. Ala-Mutka"], "venue": "Computer Science Education, 15, 83\u2013102.", "citeRegEx": "Ala.Mutka,? 2005", "shortCiteRegEx": "Ala.Mutka", "year": 2005}, {"title": "Learning on demand: Online education", "author": ["I.E. Allen", "J. Seaman"], "venue": "in the united states,", "citeRegEx": "Allen and Seaman,? \\Q2010\\E", "shortCiteRegEx": "Allen and Seaman", "year": 2010}, {"title": "Webtoteach: an interactive focused programming exercise system", "author": ["D. Arnow", "O. Barshay"], "venue": "Frontiers in Education, Annual,", "citeRegEx": "Arnow and Barshay,? \\Q1999\\E", "shortCiteRegEx": "Arnow and Barshay", "year": 1999}, {"title": "Satisfiability modulo theories", "author": ["C. Barrett", "R. Sebastiani", "S.A. Seshia", "C. Tinelli"], "venue": "In Handbook of Satisfiability,", "citeRegEx": "Barrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2009}, {"title": "A measure of similarity between graph vertices: Applications to synonym extraction and web searching", "author": ["V.D. Blondel", "A. Gajardo", "M. Heymans", "P. Snellart", "P. van Dooren"], "venue": "SIAM Review,", "citeRegEx": "Blondel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2004}, {"title": "Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs", "author": ["C. Cadar", "D. Dunbar", "D. Engler"], "venue": "In Proceeding OSDI\u201908 Proceedings of the 8th USENIX conference on Operating systems design and implementation. USENIX Association Berkeley", "citeRegEx": "Cadar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cadar et al\\.", "year": 2008}, {"title": "On automated grading", "author": ["B. Cheang", "A. Kurnia", "A. Lim", "Oon", "W.-C"], "venue": null, "citeRegEx": "Cheang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cheang et al\\.", "year": 2003}, {"title": "S2e: a platform for in-vivo", "author": ["V. Chipounov", "V. Kuznetsov", "G. Candea"], "venue": null, "citeRegEx": "Chipounov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chipounov et al\\.", "year": 2011}, {"title": "A tool for checking ansi-c programs", "author": ["E. Clarke", "D. Kroening", "F. Lerda"], "venue": null, "citeRegEx": "Clarke et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2004}, {"title": "Smt-based bounded model", "author": ["L. Cordeiro", "B. Fischer", "J. Marques-Silva"], "venue": null, "citeRegEx": "Cordeiro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cordeiro et al\\.", "year": 2009}, {"title": "Abstract interpretation: A unified lattice model", "author": ["P. Cousot", "R. Cousot"], "venue": null, "citeRegEx": "Cousot and Cousot,? \\Q1977\\E", "shortCiteRegEx": "Cousot and Cousot", "year": 1977}, {"title": "Automatic test-based assessment", "author": ["C. Douce", "D. Livingstone", "J. Orwell"], "venue": null, "citeRegEx": "Douce et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Douce et al\\.", "year": 2005}, {"title": "The quiver system", "author": ["C.C. Ellsworth", "Fenwick", "J.B. Jr.", "B.L. Kurtz"], "venue": null, "citeRegEx": "Ellsworth et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ellsworth et al\\.", "year": 2004}, {"title": "Automated assessment of gui programs using jewl", "author": ["J. English"], "venue": "SIGCSE", "citeRegEx": "English,? 2004", "shortCiteRegEx": "English", "year": 2004}, {"title": "Open/c\u00e6sar: An open software architecture for verification", "author": ["H. Garavel"], "venue": null, "citeRegEx": "Garavel,? \\Q1998\\E", "shortCiteRegEx": "Garavel", "year": 1998}, {"title": "Sage: Whitebox fuzzing", "author": ["P. Godefroid", "M.Y. Levin", "D.A. Molnar"], "venue": null, "citeRegEx": "Godefroid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Godefroid et al\\.", "year": 2012}, {"title": "Linear Regression", "author": ["J. Gross"], "venue": "Springer.", "citeRegEx": "Gross,? 2003", "shortCiteRegEx": "Gross", "year": 2003}, {"title": "An automatic grading scheme for simple", "author": ["J.B. Hext", "J.W. Winings"], "venue": null, "citeRegEx": "Hext and Winings,? \\Q1969\\E", "shortCiteRegEx": "Hext and Winings", "year": 1969}, {"title": "Deriving phylogenetic trees from the similarity", "author": ["M. Heymans", "A. Singh"], "venue": null, "citeRegEx": "Heymans and Singh,? \\Q2003\\E", "shortCiteRegEx": "Heymans and Singh", "year": 2003}, {"title": "The use of program dependence graphs in software", "author": ["S. Horwitz", "T. Reps"], "venue": null, "citeRegEx": "Horwitz and Reps,? \\Q1992\\E", "shortCiteRegEx": "Horwitz and Reps", "year": 1992}, {"title": "Creating and visualizing test data from programming exercises", "author": ["P. Ihantola"], "venue": null, "citeRegEx": "Ihantola,? \\Q2007\\E", "shortCiteRegEx": "Ihantola", "year": 2007}, {"title": "Grading student programs - a software testing approach", "author": ["E.L. Jones"], "venue": null, "citeRegEx": "Jones,? \\Q2001\\E", "shortCiteRegEx": "Jones", "year": 2001}, {"title": "The boss online submission", "author": ["M. Joy", "N. Griffiths", "R. Boyatt"], "venue": null, "citeRegEx": "Joy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Joy et al\\.", "year": 2005}, {"title": "Symbolic execution and program testing", "author": ["J.C. King"], "venue": "Communications of the ACM, 19 (7), 385\u2013394.", "citeRegEx": "King,? 1976", "shortCiteRegEx": "King", "year": 1976}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "Journal of the ACM, 46, 604 \u2014 632.", "citeRegEx": "Kleinberg,? 1999", "shortCiteRegEx": "Kleinberg", "year": 1999}, {"title": "Use of machine learning techniques for educational proposes: a decision support system for forecasting students\u2019 grades", "author": ["S.B. Kotsiantis"], "venue": "Artificial Intelligence Review, 34 (4), 331\u2013344.", "citeRegEx": "Kotsiantis,? 2012", "shortCiteRegEx": "Kotsiantis", "year": 2012}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly, 2 (1-2), 83\u201397.", "citeRegEx": "Kuhn,? 1955", "shortCiteRegEx": "Kuhn", "year": 1955}, {"title": "The LLVM Instruction Set and Compilation Strategy", "author": ["C. Lattner", "V. Adve"], "venue": null, "citeRegEx": "Lattner and Adve,? \\Q2002\\E", "shortCiteRegEx": "Lattner and Adve", "year": 2002}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady, 10 (8), 707\u2013710.", "citeRegEx": "Levenshtein,? 1966", "shortCiteRegEx": "Levenshtein", "year": 1966}, {"title": "Design and implementation of semantic matching based automatic scoring system for c programming language", "author": ["J. Li", "W. Pan", "R. Zhang", "F. Chen", "S. Nie", "X. He"], "venue": "In Proceedings of the Entertainment for education, and 5th international conference on E-learning and games,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Mining lms data to develop an \u201dearly warning system\u201d for educators: a proof of concept", "author": ["L.P. Macfadyen", "S. Dawson"], "venue": "Computers and Education,", "citeRegEx": "Macfadyen and Dawson,? \\Q2010\\E", "shortCiteRegEx": "Macfadyen and Dawson", "year": 2010}, {"title": "A system for automatic evaluation of c programs", "author": ["A.K. Mandal", "C.A. Mandal", "C. Reade"], "venue": "Features and interfaces. IJWLTT,", "citeRegEx": "Mandal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mandal et al\\.", "year": 2007}, {"title": "Kassandra: The automatic grading system", "author": ["U.V. Matt"], "venue": "SIGCUE Outlook, 22, 22\u201326.", "citeRegEx": "Matt,? 1994", "shortCiteRegEx": "Matt", "year": 1994}, {"title": "Automatically grading java programming assignments via reflection, inheritance, and regular expressions", "author": ["D.S. Morris"], "venue": "Frontiers in Education Conference 1, 1, T3G\u201322.", "citeRegEx": "Morris,? 2002", "shortCiteRegEx": "Morris", "year": 2002}, {"title": "Automatic grading of student\u2019s programming assignments: an interactive process and suit of programs", "author": ["D. Morris"], "venue": "Proceedings of the Frontiers in Education Conference 3, Vol. 3, pp. 1\u20136.", "citeRegEx": "Morris,? 2003", "shortCiteRegEx": "Morris", "year": 2003}, {"title": "Marking student programs using graph similarity", "author": ["K.A. Naud\u00e9", "J.H. Greyling", "D. Vogts"], "venue": "Computers and Education,", "citeRegEx": "Naud\u00e9 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Naud\u00e9 et al\\.", "year": 2010}, {"title": "Measuring similarity of graph nodes by neighbor matching", "author": ["M. Nikoli\u0107"], "venue": "Intelligent Data Analysis, Accepted for publication.", "citeRegEx": "Nikoli\u0107,? 2013", "shortCiteRegEx": "Nikoli\u0107", "year": 2013}, {"title": "Teaching semantics with a proof assistant: No more lsd trip proofs", "author": ["T. Nipkow"], "venue": "VMCAI, pp. 24\u201338.", "citeRegEx": "Nipkow,? 2012", "shortCiteRegEx": "Nipkow", "year": 2012}, {"title": "A survey of literature on the teaching of introductory programming", "author": ["A. Pears", "S. Seidman", "L. Malmi", "L. Mannila", "E. Adams", "J. Bennedsen", "M. Devlin", "J. Paterson"], "venue": "In Working group reports on ITiCSE on Innovation and technology in computer science education,", "citeRegEx": "Pears et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pears et al\\.", "year": 2007}, {"title": "Fully automatic assessment of programming exercises", "author": ["R. Saikkonen", "L. Malmi", "A. Korhonen"], "venue": "ACM Sigcse Bulletin,", "citeRegEx": "Saikkonen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Saikkonen et al\\.", "year": 2001}, {"title": "The economic impacts of inadequate infrastructure for software testing", "author": ["G. Tassey"], "venue": "Tech. rep., National Institute of Standards and Technology.", "citeRegEx": "Tassey,? 2002", "shortCiteRegEx": "Tassey", "year": 2002}, {"title": "Pex white box test generation for .net", "author": ["N. Tillmann", "J. Halleux"], "venue": "In Proc. of TAP 2008, the 2nd International Conference on Tests and Proofs,", "citeRegEx": "Tillmann and Halleux,? \\Q2008\\E", "shortCiteRegEx": "Tillmann and Halleux", "year": 2008}, {"title": "Development and evaluation of LAV: an SMT-based error finding platform. In Verified Software: Theories, Tools and Experiments (VSTTE), LNCS", "author": ["M. Vujo\u0161evi\u0107-Jani\u010di\u0107", "V. Kuncak"], "venue": null, "citeRegEx": "Vujo\u0161evi\u0107.Jani\u010di\u0107 and Kuncak,? \\Q2012\\E", "shortCiteRegEx": "Vujo\u0161evi\u0107.Jani\u010di\u0107 and Kuncak", "year": 2012}, {"title": "The role of programming paradigms in the first programming courses", "author": ["M. Vujo\u0161evi\u0107-Jani\u010di\u0107", "D. To\u0161i\u0107"], "venue": "The Teaching of Mathematics,", "citeRegEx": "Vujo\u0161evi\u0107.Jani\u010di\u0107 and To\u0161i\u0107,? \\Q2008\\E", "shortCiteRegEx": "Vujo\u0161evi\u0107.Jani\u010di\u0107 and To\u0161i\u0107", "year": 2008}, {"title": "Semantic similarity-based grading of student programs", "author": ["T. Wang", "X. Su", "Y. Wang", "P. Ma"], "venue": "Information and Software Technology,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Using testing and junit across the curriculum", "author": ["M. Wick", "D. Stevenson", "P. Wagner"], "venue": "SIGCSE Bull.,", "citeRegEx": "Wick et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wick et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 37, "context": "For students, it provides immediate feedback which is very important in process of studying, especially in computer science where students take a challenge of making the computer follow their intentions (Nipkow, 2012).", "startOffset": 203, "endOffset": 217}, {"referenceID": 0, "context": "Recent surveys of related approaches are given elsewhere (Ala-Mutka, 2005; Ihantola, Ahoniemi, Karavirta, & Sepp\u00e4l\u00e4, 2010).", "startOffset": 57, "endOffset": 122}, {"referenceID": 36, "context": "We also briefly discuss tools for software verification (Vujo\u0161evi\u0107-Jani\u010di\u0107 & Kuncak, 2012) and CFG similarity (Nikoli\u0107, 2013), that we use for assignment evaluation.", "startOffset": 110, "endOffset": 125}, {"referenceID": 40, "context": "Software bugs cost the world economy billions of dollars annually (Tassey, 2002).", "startOffset": 66, "endOffset": 80}, {"referenceID": 23, "context": "Different approaches to automated checking of software properties exist, such as symbolic execution (King, 1976), model checking (Clarke, 2008) and abstract interpretation (Cousot & Cousot, 1977).", "startOffset": 100, "endOffset": 112}, {"referenceID": 24, "context": "There are many similarity measures for graphs and their nodes (Kleinberg, 1999; Heymans & Singh, 2003; Blondel, Gajardo, Heymans, Snellart, & van Dooren, 2004; Nikoli\u0107, 2013).", "startOffset": 62, "endOffset": 174}, {"referenceID": 36, "context": "There are many similarity measures for graphs and their nodes (Kleinberg, 1999; Heymans & Singh, 2003; Blondel, Gajardo, Heymans, Snellart, & van Dooren, 2004; Nikoli\u0107, 2013).", "startOffset": 62, "endOffset": 174}, {"referenceID": 36, "context": "A short overview of similarity measures for graphs can be found in the literature (Nikoli\u0107, 2013).", "startOffset": 82, "endOffset": 97}, {"referenceID": 36, "context": "A specific similaritymeasure for graph nodes called neighbor matching, possesses properties relevant for our purpose that other similar measures lack (Nikoli\u0107, 2013).", "startOffset": 150, "endOffset": 165}, {"referenceID": 26, "context": "The assignment problem is usually solved by the well-known Hungarian algorithm of complexity O(mn) where m = max(|A|, |B|) and n = min(|A|, |B|) (Kuhn, 1955), but there are also more efficient algorithms.", "startOffset": 145, "endOffset": 157}, {"referenceID": 36, "context": "The termination condition is maxij |x k ij \u2212 x k\u22121 ij | < \u03b5 for some chosen precision \u03b5 and the iterative algorithm is proved to converge (Nikoli\u0107, 2013).", "startOffset": 138, "endOffset": 153}, {"referenceID": 36, "context": "The similarity of the graphs can be defined as the weight of the optimal matching of nodes from A and B divided by the number of matched nodes (Nikoli\u0107, 2013).", "startOffset": 143, "endOffset": 158}, {"referenceID": 44, "context": "CFGs) (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013).", "startOffset": 6, "endOffset": 60}, {"referenceID": 35, "context": "CFGs) (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013).", "startOffset": 6, "endOffset": 60}, {"referenceID": 36, "context": "CFGs) (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013).", "startOffset": 6, "endOffset": 60}, {"referenceID": 44, "context": "Finally, using similarity only (like in (Wang et al., 2007; Naud\u00e9 et al., 2010)) or even with support of a bug finding tool, would miss to penalize incorrectness of program\u2019s behavior.", "startOffset": 40, "endOffset": 79}, {"referenceID": 35, "context": "Finally, using similarity only (like in (Wang et al., 2007; Naud\u00e9 et al., 2010)) or even with support of a bug finding tool, would miss to penalize incorrectness of program\u2019s behavior.", "startOffset": 40, "endOffset": 79}, {"referenceID": 44, "context": "Student\u2019s program is considered to be good if it is similar to some of the programs provided by the teacher (Wang et al., 2007).", "startOffset": 108, "endOffset": 127}, {"referenceID": 28, "context": "A simple way of measuring the similarity of two sequences of instructions s1 and s2 is using the edit distance between them d(s1, s2) \u2014 the minimal number of insertion, deletion and substitution operations over the elements of the sequence by which one sequence can be transformed into another (Levenshtein, 1966).", "startOffset": 294, "endOffset": 313}, {"referenceID": 44, "context": "In introductory programming courses, most often a student\u2019s solution can be considered as better if it is more similar to one of the teacher\u2019s solutions (Wang et al., 2007).", "startOffset": 153, "endOffset": 172}, {"referenceID": 44, "context": "For instance, the behavior of any of the proposed similarity measures including ours (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013) is not clear from their definitions only.", "startOffset": 85, "endOffset": 139}, {"referenceID": 35, "context": "For instance, the behavior of any of the proposed similarity measures including ours (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013) is not clear from their definitions only.", "startOffset": 85, "endOffset": 139}, {"referenceID": 36, "context": "For instance, the behavior of any of the proposed similarity measures including ours (Wang et al., 2007; Naud\u00e9 et al., 2010; Nikoli\u0107, 2013) is not clear from their definitions only.", "startOffset": 85, "endOffset": 139}, {"referenceID": 16, "context": "For that purpose, coefficients can be determined automatically using least squares linear regression (Gross, 2003) if a manually graded corpus of students\u2019 programs is provided by the teacher.", "startOffset": 101, "endOffset": 114}, {"referenceID": 11, "context": "Automated testing is the most common way of evaluating students\u2019 programs (Douce et al., 2005).", "startOffset": 74, "endOffset": 94}, {"referenceID": 31, "context": "Test cases are usually supplied by a teacher and/or randomly generated (Mandal et al., 2007).", "startOffset": 71, "endOffset": 92}, {"referenceID": 32, "context": "example, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).", "startOffset": 48, "endOffset": 60}, {"referenceID": 21, "context": "example, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).", "startOffset": 190, "endOffset": 203}, {"referenceID": 33, "context": "example, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).", "startOffset": 210, "endOffset": 224}, {"referenceID": 34, "context": "example, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).", "startOffset": 231, "endOffset": 245}, {"referenceID": 13, "context": "example, PSGE (Hext & Winings, 1969), Kassandra (Matt, 1994), BOSS (Joy, Griffiths, & Boyatt, 2005), WebToTeach (Arnow & Barshay, 1999), Schemerobe (Saikkonen, Malmi, & Korhonen, 2001), TRY (Jones, 2001), HoGG (Morris, 2002), BAGS (Morris, 2003), on-line Judge (Cheang, Kurnia, Lim, & Oon, 2003), JEWL (English, 2004), Quiver (Ellsworth, Fenwick, & Kurtz, 2004), and JUnit (Wick, Stevenson, & Wagner, 2005).", "startOffset": 302, "endOffset": 317}, {"referenceID": 20, "context": "There are limited experiments on using Java PathFinder model checker for automated test case generation (Ihantola, 2007).", "startOffset": 104, "endOffset": 120}, {"referenceID": 14, "context": "Ceasar (Garavel, 1998), are usually too complex and not aimed for educational purposes.", "startOffset": 7, "endOffset": 22}, {"referenceID": 44, "context": "proposed a grading approach for assignments in C based only on program similarity (Wang et al., 2007).", "startOffset": 82, "endOffset": 101}, {"referenceID": 35, "context": "(Naud\u00e9 et al., 2010).", "startOffset": 0, "endOffset": 20}, {"referenceID": 36, "context": "convergence) are proven (Nikoli\u0107, 2013).", "startOffset": 24, "endOffset": 39}, {"referenceID": 25, "context": "Kotsiantis performed successful forecasting based on demographic characteristics of students, results of several written assignments, and class attendance (Kotsiantis, 2012).", "startOffset": 155, "endOffset": 173}, {"referenceID": 5, "context": "We are planning to integrate LLVM-based open source tool KLEE (Cadar et al., 2008) for automated test case generation and also to add support for teacher supplied test cases.", "startOffset": 62, "endOffset": 82}], "year": 2012, "abstractText": "In this paper we promote introducing software verification and control flow graph similarity measurement in automated evaluation of students\u2019 programs. We present a new grading framework that merges results obtained by combination of these two approaches with results obtained by automated testing, leading to improved quality and precision of automated grading. These two approaches are also useful in providing a comprehensible feedback that can help students to improve the quality of their programs We also present our corresponding tools that are publicly available and open source. The tools are based on LLVM low-level intermediate code representation, so they could be applied to a number of programming languages. Experimental evaluation of the proposed grading framework is performed on a corpus of university students\u2019 programs written in programming language C. Results of the experiments show that automatically generated grades are highly correlated with manually determined grades suggesting that the presented tools can find real-world applications in studying and grading.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}