{"id": "1207.2491", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2012", "title": "A Spectral Learning Approach to Range-Only SLAM", "abstract": "We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) of domain data with known similarities. This algorithm is an instance of a general spectral system from which it inherits several desirable characteristics, including statistical consistency and no local optima. Compared with popular batch optimization or multi-hypotheses tracking (MHT) methods for range-only SLAM, our spectral approach guarantees low computing requirements and good traceability. Compared with popular advanced Kalman filters (EKF) or advanced information filters (EIF) and many MHT approaches, our approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs and to a lesser extent MHT, especially for the highly non-Gaussian posteriors that are only encountered in range-only SLAM analysis, our problem solving method, including our MLAM, is not practical.", "histories": [["v1", "Tue, 10 Jul 2012 21:19:33 GMT  (2216kb,D)", "http://arxiv.org/abs/1207.2491v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO stat.ML", "authors": ["byron boots", "geoffrey j gordon"], "accepted": true, "id": "1207.2491"}, "pdf": {"name": "1207.2491.pdf", "metadata": {"source": "CRF", "title": "A Spectral Learning Approach to Range-Only SLAM", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In range-only SLAM, we are given a sequence of range measurements from a robot to fixed landmarks, and possibly a matching sequence of odometry measurements. We then attempt to simultaneously estimate the robot\u2019s trajectory and the locations of the landmarks. Popular approaches to range-only SLAM include EKFs and EIFs (Kantor & Singh, 2002; Kurth et al., 2003; Djugash & Singh, 2008; Djugash, 2010; Thrun et al., 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al., 2005; Thrun et al., 2005), and batch optimization of a likelihood function (Kehagias et al., 2006).\nIn all the above approaches, the most popular representation for a hypothesis is a list of landmark locations (mn,x,mn,y) and a list of robot poses (xt, yt, \u03b8t). Unfortunately, both the motion and measurement models are highly nonlinear in this representation, leading to computational problems: inaccurate linearizations in EKF/EIF/MHT and local optima in batch optimization approaches (see Section 2 for details). Much work has attempted to remedy this problem, e.g., by changing the hypothesis representation (Djugash, 2010) or by keeping multiple hypotheses (Djugash et al., 2005; Djugash, 2010; Thrun et al., 2005). While considerable progress has been made, none of these methods are ideal; common difficulties include the need for an extensive initialization phase, inability to recover from poor initialization, lack of performance guarantees, or excessive computational requirements.\nWe take a very different approach: we formulate range-only SLAM as a matrix factorization problem, where features of observations are linearly related to a 4- or 7-dimensional state space. This\nar X\niv :1\n20 7.\n24 91\nv1 [\ncs .L\nG ]\n1 0\nJu l 2\napproach has several desirable properties. First, we need weaker assumptions about the measurement model and motion model than previous approaches to SLAM. Second, our state space yields a linear measurement model, so we hope to lose less information during tracking to approximation errors and local optima. Third, our formulation leads to a simple spectral learning algorithm, based on a fast and robust singular value decomposition (SVD)\u2014in fact, our algorithm is an instance of a general spectral system identification framework, from which it inherits desirable guarantees including statistical consistency and no local optima. Fourth, we don\u2019t need to worry as much as previous methods about errors such as a consistent bias in odometry, or a receiver mounted at a different height from the transmitters: in general, we can learn to correct such errors automatically by expanding the dimensionality of our state space.\nAs we will discuss in Section 2, our approach to SLAM has much in common with spectral algorithms for subspace identification (Van Overschee & De Moor, 1996; Boots et al., 2010); unlike these methods, our focus on SLAM makes it easy to interpret our state space. Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al., 2003; Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).\nWe begin in Section 2 by reviewing background related to our approach. In Section 3 we present the basic spectral learning algorithm for range-only SLAM, and discuss how it relates to state space discovery for a dynamical system. We conclude in Section 4 by comparing spectral SLAM to other popular methods for range-only SLAM on real world range data collected from an autonomous lawnmower with time-of-flight ranging radios."}, {"heading": "2 Background", "text": "There are four main pieces of relevant background: first, the well-known solutions to range-only SLAM using variations of the extended Kalman filter and batch optimization; second, recentlydiscovered spectral approaches to identifying parameters of nonlinear dynamical systems; third, matrix factorization for finding structure from motion in video; and fourth, dimensionality-reduction methods for localization and mapping. Below, we will discuss the connections among these areas, and show how they can be unified within a spectral learning framework."}, {"heading": "2.1 Likelihood-based Range-only SLAM", "text": "The standard probabilistic model for range-only localization (Kantor & Singh, 2002; Kurth et al., 2003) represents robot state by a vector st = [xt, yt, \u03b8t]T; the robot\u2019s (nonlinear) motion and observation models are\nst+1 = [ xt + vt cos(\u03b8t) yt + vt sin(\u03b8t)\n\u03b8t + \u03c9t\n] + t dt,n = \u221a (mn,x \u2212 xt)2 + (mn,y \u2212 yt)2 + \u03b7t (1)\nHere vt is the distance traveled, \u03c9t is the orientation change, dt,n is the estimate of the range from the nth landmark location (mn,x,mn,y) to the current location of the robot (xt, yt), and t and \u03b7t are noise. (Throughout this paper we assume known correspondences, since range sensing systems such as radio beacons typically associate unique identifiers with each reading.)\nTo handle SLAM rather than just localization, we can extend the state to include landmark positions:\nst = [xt, yt, \u03b8t,m1,x,m1,y, . . . ,mN,x,mN,y] T (2)\nwhere N is the number of landmarks. The motion and measurement models remain the same. Given this model, we can use any standard optimization algorithm (such as Gauss-Newton) to fit the unknown robot and landmark parameters by maximum likelihood. Or, we can track these parameters online using EKFs, EIFs, or MHT methods like particle filters.\nEKFs and EIFs are a popular solution for localization and mapping problems: for each new odometry input at = [vt, \u03c9t]T and each new measurement dt, we propagate the estimate of the robot state and error covariance by linearizing the non-linear motion and measurement models. Unfortunately, though, range-only SLAM is notoriously difficult for EKFs/EIFs: since range-only sensors are not informative enough to completely localize a robot or a landmark from a small number of readings,\nnonlinearities are much worse in range-only SLAM than they are in other applications such as rangeand-bearing SLAM. In particular, if we don\u2019t have a sharp prior distribution for landmark positions, then after a few steps, the exact posterior becomes highly non-Gaussian and multimodal; so, any Gaussian approximation to the posterior is necessarily inaccurate. Furthermore, an EKF will generally not even produce the best possible Gaussian approximation: a good linearization would tell us a lot about the modes of the posterior, which would be equivalent to solving the original SLAM problem. So, practical applications of the EKF to range-only SLAM attempt to delay linearization until enough information is available, e.g., via an extended initialization phase for each landmark. Such delays simply push the problem of finding a good hypothesis onto the initialization algorithm.\nDjugash et al. proposed a polar parameterization to more accurately represent the annular and multimodal distributions typically encountered in range-only SLAM. The resulting approach is called the ROP-EKF, and is shown to outperform the ordinary (Cartesian) EKF in several real-world problems, especially in combination with multiple-hypothesis tracking (Djugash & Singh, 2008; Djugash, 2010). However, the multi-hypothesis ROP-EKF can be much more expensive than an EKF, and is still a heuristic approximation to the true posterior.\nInstead of the posterior covariance of the state (as used by the EKF), the extended information filter (EIF) maintains an estimate of the inverse covariance. The two representations are statistically equivalent (and therefore have the same failure modes). But, the inverse covariance is often approximately sparse, leading to much more efficient approximate computation (Thrun et al., 2005)."}, {"heading": "2.2 Spectral State Space Discovery and System Identification", "text": "System identification algorithms attempt to learn dynamical system parameters such as a state space, a dynamics model (motion model), and an observation model (measurement model) directly from samples of observations and actions. In the last few years, spectral system identification algorithms have become popular; these algorithms learn a state space via a spectral decomposition of a carefully designed matrix of observable features, then find transition and observation models by linear regressions involving the learned states. Originally, subspace identification algorithms were almost exclusively used for linear system identification (Van Overschee & De Moor, 1996), but recently, similar spectral algorithms have been used to learn models of partially observable nonlinear dynamical systems such as HMMs (Hsu et al., 2009; Siddiqi et al., 2010) and PSRs (Rosencrantz et al., 2004; Boots et al., 2010; Boots & Gordon, 2010; Boots et al., 2011). All of these spectral algorithms share a strategy for state space discovery: they learn a state space via a spectral decomposition of a matrix of observations (Figure 1), resulting in a linear observation function, and then they learn a model of the dynamics in the learned low-dimensional state space. This is a powerful and appealing approach: the resulting algorithms are statistically consistent, and they are easy to implement with efficient linear algebra operations. In contrast, batch optimization of likelihood (e.g., via the popular expectation maximization (EM) algorithm) is only known to be consistent if we find the global optimum of the likelihood function\u2014typically an impractical requirement.\nAs we will see in Section 3, we can view the range-only SLAM problem as an instance of spectral state space discovery. And, the Appendix (Sec. 6.3) discusses how to identify transition and measurement models given the learned states. The same properties that make spectral methods appealing for system identification carry over to our spectral SLAM algorithm: computational efficiency, statistical consistency, and finite-sample error bounds."}, {"heading": "2.3 Orthographic Structure From Motion", "text": "In some ways the orthographic structure from motion (SfM) problem in vision (Tomasi & Kanade, 1992) is very similar to the SLAM problem: the goal is to recover scene geometry and camera rotations from a sequence of images (compare with landmark geometry and robot poses from a sequence of range observations). And in fact, one popular solution for SfM is very similar to the state space discovery step in spectral state space identification. The key idea in spectral SfM is that is that an image sequence can be represented as a 2F \u00d7 P measurement matrix W , containing the horizontal and vertical coordinates of P points tracked through F frames. If the images are the result of an orthographic camera projection, then it is possible to show that rank(W ) = 3. As a consequence, the measurement matrix can be factored into the product of two matrices U and V , where U contains the 3d positions of the features and V contains the camera axis rotations (Tomasi & Kanade, 1992). With respect to system identification, it is possible to interpret the matrix U as an observation model and V as an estimate of the system state. Inspired by SfM, we reformulate range-only SLAM problem in a similar way in Section 3, and then similarly solve the problem with a spectral learning algorithm. Also similar to SfM, we examine the identifiability of our factorization, and give a metric upgrade procedure which extracts additional geometric information beyond what the factorization gives us."}, {"heading": "2.4 Dimensionality-reduction-based Methods for Mapping", "text": "Dimensionality reduction methods have recently provided an alternative to more traditional likelihood-based methods for mapping. In particular, the problem of finding a good map can be viewed as finding a (possibly nonlinear) embedding of sensor data via methods like multidimensional scaling (MDS) and manifold learning.\nFor example, MDS has been used to determine a Euclidean map of sensor locations where there is no distinction between landmark positions and robot positions (Shang et al., 2003): instead all-toall range measurements are assumed for a set of landmarks. If some pairwise measurements are not available, these measurements can be approximated by some interpolation method, e.g. the geodesic distance between the landmarks (Tenenbaum et al., 2000; Shang et al., 2003).\nOur problem differs from this previous work: in contrast to MDS, we have no landmark-to-landmark measurements and only inaccurate robot-to-robot measurements (from odometry, which may not be present, and which often has significant errors when integrated over more than a short distance). Additionally, our smaller set of measurements introduces additional challenges not present in classical MDS: linear methods can recover the positions only up to a linear transformation. This ambiguity forces changes compared to the MDS algorithm: while MDS factors the all-to-all matrix of squared ranges, in Sec. 3.1 we factor only a block of this matrix, then use either a metric upgrade step or a few global position measurements to resolve the ambiguity.\nA popular alternative to linear dimensionality reduction techniques like classical MDS is manifold learning: nonlinearly mapping sensor inputs to a feature space that \u201cunfolds\u201d the manifold on which the data lies and then applying dimensionality reduction. Such nonlinear dimensionality reduction has been used to learn maps of wi-fi networks and landmark locations when sensory data is thought to be nonlinearly related to the underlying Eucidean space in which the landmarks lie (Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007). Unlike theses approaches, we show that linear dimensionality reduction is sufficient to solve the range-only SLAM problem. (In particular, (Yairi, 2007) suggests solving range-only mapping using nonlinear dimensionality reduction. We not only show that this is unnecessary, but additionally show that linear dimensionality reduction is sufficient for localization as well.) This greatly simplifies the learning algorithm and allows us to provide strong statistical guarantees for the mapping portion of SLAM (Sec. 3.3)."}, {"heading": "3 State Space Discovery and Spectral SLAM", "text": "We start with SLAM from range data without odometry. For now, we assume no noise, no missing data, and batch processing. We will generalize below: Sec. 3.2 discusses how to recover robot orientation, Sec. 3.3 discusses noise, and Sec. 3.4 discusses missing data and online SLAM. In the Appendix (Section 6.3) we discuss learning motion and measurement models."}, {"heading": "3.1 Range-only SLAM as Matrix Factorization", "text": "Consider the matrix Y \u2208 RN\u00d7T of squared ranges, with N \u2265 4 landmarks and T \u2265 4 time steps:\nY = 1\n2\n d211 d 2 12 . . . d 2 1T d221 d 2 22 . . . d 2 2T\n... ...\n... ...\nd2N1 d 2 N2 . . . d 2 NT  (3) where dn,t is the measured distance from the robot to landmark n at time step t.\nThe most basic version of our spectral SLAM method relies on the insight that Y factors according to robot position (xt, yt) and landmark position (mn,x,mn,y). To see why, note\nd2n,t = (m 2 n,x +m 2 n,y)\u2212 2mn,x \u00b7 xt \u2212 2mn,y \u00b7 yt + (x2t + y2t ) (4)\nIf we write Cn = [(m2n,x + m 2 n,y)/2,mn,x,mn,y, 1] T and Xt = [1,\u2212xt,\u2212yt, (x2t + y2t )/2]T, it is easy to see that d2n,t = 2C T nXt. So, Y factors as Y = CX , where C \u2208 RN\u00d74 contains the positions of landmarks,\nC =  (m21,x +m 2 1,y)/2 m1,x m1,y 1 (m22,x +m 2 2,y)/2 m2,x m2,y 1 ... ... ... ...\n(m2N,x +m 2 N,y)/2 mN,x mN,y 1  (5) and X \u2208 R4\u00d7T contains the positions of the robot over time\nX =  1 . . . 1\u2212x1 . . . \u2212xT\u2212y1 . . . \u2212yT (x21 + y 2 1)/2 . . . (x 2 T + y 2 T )/2  (6) If we can recover C and X , we can read off the solution to the SLAM problem. The fact that Y \u2019s rank is at most 4 suggests that we might be able to use a rank-revealing factorization of Y , such as the singular value decomposition, to find C and X . Unfortunately, such a factorization only determines C andX up to a linear transform: given an invertible matrix S, we can write Y = CX = CS\u22121SX . Therefore, factorization can only hope to recover U = CS\u22121 and V = SX .\nTo upgrade the factors U and V to a full metric map, we have two options. If global position estimates are available for at least four landmarks, we can learn the transform S via linear regression, and so recover the original C and X . This method works as long as we know at least four landmark positions. Figure 2A shows a simulated example.\nOn the other hand, if no global positions are known, the best we can hope to do is recover landmark and robot positions up to an orthogonal transform (translation, rotation, and reflection). It turns out that Eqs. (5\u20136) provide enough additional geometric constraints to do so: in the Appendix (Sec. 6.1) we show that, if we have at least 9 time steps and at least 9 landmarks, and if each of these point sets is non-singular in an appropriate sense, then we can compute the metric upgrade in closed form. The idea is to fit a quadratic surface to the rows of U , then change coordinates so that the surface becomes the function in (5). (By contrast, the usual metric upgrade for orthographic structure from motion (Tomasi & Kanade, 1992), which uses the constraint that camera projection matrices are orthogonal, requires a nonlinear optimization.)"}, {"heading": "3.2 SLAM with Headings", "text": "In addition to location, we often want the robot\u2019s global heading \u03b8. We could get headings by postprocessing our learned positions, but in practice we can reduce variance by learning positions and headings simultaneously. We do so by adding more features to our measurement matrix: differences between successive pairs of squared distances, scaled by velocity (which we can estimate from odometry). Since we need pairs of time steps, we now have Y \u2208 R2N\u00d7T\u22121:\nY = 1\n2 \nd211 d 2 12 . . . d 2 1T\u22121\n... ...\n. . . ...\nd2N1 d 2 N2 . . . d 2 NT\u22121\nd212\u2212d 2 11\nv1\nd213\u2212d 2 12\nv2 . . .\nd21T\u2212d 2 1T\u22121\nvT\u22121 ... ... . . .\n... d2N2\u2212d 2 N1\nv1\nd2N2\u2212d 2 N3\nv2 . . .\nd2NT\u2212d 2 NT\u22121\nvT\u22121\n (7)\nAs before, we can factor Y into a robot state matrix and a landmark matrix. The key new observation is that we can write the new features in terms of cos(\u03b8) and sin(\u03b8):\nd2n,t+1 \u2212 d2n,t 2vt =\u2212 mn,x(xt+1 \u2212 xt) vt \u2212 mn,y(yt+1 \u2212 yt) vt + x2t+1 \u2212 x2t + y2t+1 \u2212 y2t 2vt\n=\u2212mn,x cos(\u03b8t)\u2212mn,y sin(\u03b8t) + x2t+1 \u2212 x2t + y2t+1 \u2212 y2t\n2vt (8)\nFrom Eq. 4 and Eq. 8 it is easy to see that Y has rank at most 7 (exactly 7 if the robot path and landmark positions are not singular): we have Y = CX , where C \u2208 RN\u00d77 contains functions of landmark positions and X \u2208 R7\u00d7T contains functions of robot state,\nAlgorithm 1 Spectral SLAM In: i.i.d. pairs of observations {ot, at}Tt=1; optional: measurement model for \u2265 4 landmarks C1:4 Out: measurement model (map) C\u0302, robot locations X\u0302 (the tth column is location at time t)\n1: Collect observations and odometry into a matrix Y\u0302 (Eq. 7)\n2: Find the the top 7 singular values and vectors: \u3008U\u0302 , \u039b\u0302, V\u0302 >\u3009 \u2190 SVD(Y\u0302 , 7) The transformed measurement matrix is C\u0302S\u22121 = U\u0302 and robot states are SX\u0302 = \u039b\u0302V\u0302 >.\n3: Find S\u0302 via linear regression (from U\u0302 to C1:4) or metric upgrade (see Appendix) and return C\u0302 = U\u0302 S\u0302 and X\u0302 = S\u0302\u22121\u039b\u0302V\u0302 >\nC =  (m21,x +m 2 1,y)/2 m1,x m1,y 1 0 0 0 ... ... ... ... ... ... ... (m2N,x +m 2 N,y)/2 mN,x mN,y 1 0 0 0\n0 0 0 0 m1,x m1,y 1 ... ... ... ... ... ... ...\n0 0 0 0 mN,x mN,y 1\n (9)\nX = \n1 . . . 1 \u2212x1 . . . \u2212xT\u22121 \u2212y1 . . . \u2212yT\u22121\n(x21 + y 2 1)/2 . . . (x 2 T\u22121 + y 2 T\u22121)/2\n\u2212 cos(\u03b81) . . . \u2212 cos(\u03b8T\u22121) \u2212 sin(\u03b81) . . . \u2212 sin(\u03b8T\u22121)\nx22\u2212x 2 1+y 2 2\u2212y 2 1\n2v1 . . .\nx2T\u2212x 2 T\u22121+y 2 T\u2212y 2 T\u22121\n2vT\u22121\n (10)\nAs with the basic SLAM algorithm in Section 3.1, we can factor Y using SVD, this time keeping 7 singular values. To make the state space interpretable, we can then look at the top part of the learned transform of C: as long as we have at least four landmarks in non-singular position, this block will have exactly a three-dimensional nullspace (due to the three columns of zeros in the top part of C). After eliminating this nullspace, we can proceed as before to learn S and make the state space interpretable: either use the coordinates of at least 4 landmarks as regression targets, or perform a metric upgrade. (See the Appendix, Sec. 6.1, for details). Once we have positions, we can recover headings as angles between successive positions."}, {"heading": "3.3 A Spectral SLAM Algorithm", "text": "The matrix factorizations of Secs. 3.1 and 3.2 suggest a straightforward SLAM algorithm, Alg. 1: build an empirical estimate Y\u0302 of Y by sampling observations as the robot traverses its environment, then apply a rank-7 thin SVD, discarding the remaining singular values to suppress noise.\n\u3008U\u0302 , \u039b\u0302, V\u0302 >\u3009 \u2190 SVD(Y\u0302 , 7) (11)\nFollowing Section 3.2, the left singular vectors U\u0302 are an estimate of our transformed measurement matrix CS\u22121, and the weighted right singular vectors \u039b\u0302V\u0302 > are an estimate of our transformed robot state SX . We can then learn S via regression or metric upgrade.\nStatistical Consistency and Sample Complexity Let M \u2208 RN\u00d7N be the true observation covariance for a randomly sampled robot position, and let M\u0302 = 1T Y\u0302 Y\u0302\n> be the empirical covariance estimated from T observations. Then the true and estimated measurement models are the top singular vectors of M and M\u0302 . Assuming that the noise in M\u0302 is zero-mean, as we include more data in our averages, we will show below that the law of large numbers guarantees that M\u0302 converges to the true covariance M . So, our learning algorithm is consistent for estimating the range of M , i.e., the\nlandmark locations. (The estimated robot positions will typically not converge, since we typically have a bounded effective number of observations relevant to each robot position. But, as we see each landmark again and again, the robot position errors will average out, and we will recover the true map.)\nIn more detail, we can give finite-sample bounds on the error in recovering the true factors. For simplicity of presentation we assume that noise is i.i.d., although our algorithm will work for any zero-mean noise process with a finite mixing time. (The error bounds will of course become weaker in proportion to mixing time, since we gain less new information per observation.) The argument (see the Appendix, Sec. 6.2, for details) has two pieces: standard concentration bounds show that each element of our estimated covariance approaches its population value; then the continuity of the SVD shows that the learned subspace also approaches its true value. The final bound is:\n|| sin \u03a8||2 \u2264 Nc \u221a 2 log(T ) T\n\u03b3 (12)\nwhere \u03a8 is the vector of canonical angles between the learned subspace and the true one, c is a constant depending on our error distribution, and \u03b3 is the true smallest nonzero eigenvalue of the covariance. In particular, this bound means that the sample complexity is O\u0303(\u03b62) to achieve error \u03b6."}, {"heading": "3.4 Extensions: Missing Data, Online SLAM, and System ID", "text": "Missing data So far we have assumed that we receive range readings to all landmarks at each time step. In practice this assumption is rarely satisfied: we may receive range readings asynchronously, some range readings may be missing entirely, and it is often the case that odometry data is sampled faster than range readings. Here we outline two methods for overcoming this practical difficulty.\nFirst, if a relatively small number of observations are missing, we can use standard approaches for factorization with missing data. For example, probabilistic PCA (Tipping & Bishop, 1999) estimates the missing entries via an EM algorithm, and matrix completion (Cande\u0300s & Plan, 2009) uses a tracenorm penalty to recover a low-rank factorization with high probability. However, for range-only data, often the fraction of missing data is high and the missing values are structural rather than random.\nThe second approach is interpolation: we divide the data into overlapping subsets and then use local odometry information to interpolate the range data within each subset. To interpolate the data, we estimate a robot path by dead reckoning. For each point in the dead reckoning path we build the feature representation [1,\u2212x,\u2212y, (x2 + y2)/2]>. We then learn a linear model that predicts a squared range reading from these features (for the data points where range is available), as in Eq. 4. Next we predict the squared range along the entire path. Finally we build the matrix Y\u0302 by averaging the locally interpolated range readings. This interpolation approach works much better in practice than the fully probabilistic approaches mentioned above, and was used in our experiments in Section 4.\nOnline Spectral SLAM The algorithms developed in this section so far have had an important drawback: unlike many SLAM algorithms, they are batch methods not online ones. The extension to online SLAM is straightforward: instead of first estimating Y\u0302 and then performing a SVD, we sequentially estimate our factors \u3008U\u0302 , \u039b\u0302, V\u0302 >\u3009 via online SVD (Brand, 2006; Boots et al., 2011).\nRobot Filtering and System Identification So far, our algorithms have not directly used (or needed) a robot motion model in the learned state space. However, an explicit motion model is required if we want to predict future sensor readings or plan a course of action. We have two choices: we can derive a motion model from our learned transformation S between latent states and physical locations, or we can learn a motion model directly from data using spectral system identification. More details about both of these approaches can be found in the Appendix, Sec. 6.3."}, {"heading": "4 Experimental Results", "text": "We perform several SLAM and robot navigation experiments to illustrate and test the ideas proposed in this paper. First we show how our methods work in theory with synthetic experiments where complete observations are received at each point in time and i.i.d. noise is sampled from a multivariate Gaussian distribution. Next we demonstrate our algorithm on data collected from a real-world robotic system with substantial amounts of missing data. Experiments were performed in Matlab, on a 2.66 GHz Intel Core i7 computer with 8 GB of RAM. In contrast to batch nonlinear optimization approaches to SLAM, the spectral learning methods described in this paper are very fast, usually taking less than a second to run."}, {"heading": "4.1 Synthetic Experiments", "text": "Our simulator randomly places 6 landmarks in a 2-D environment. A simulated robot then randomly moves through the environment for 500 time steps and receives a range reading to each one of the landmarks at each time step. The range readings are perturbed by noise sampled from a Gaussian distribution with variance equal to 1% of the range. Given this data, we apply the algorithm from Section 3.3 to solve the SLAM problem. We use the coordinates of 4 landmarks to learn the linear transform S and recover the true state space, as shown in Figure 2A. The results indicate that we can accurately recover both the landmark locations and the robot path.\nWe also investigated the empirical convergence rate of our observation model (and therefore the map) as the number of range readings increased. To do so, we generated 1000 different random pairs of environments and robot paths. For each pair, we repeatedly performed our spectral SLAM algorithm on increasingly large numbers of range readings and looked at the difference between our estimated measurement model (the robot\u2019s map) and the true measurement model, excluding the landmarks that we used for reconstruction: \u2016C\u03025:6 \u2212 C5:6\u2016F . The results are shown in Figure 2B, and show that our estimates steadily converge to the true model, corroborating our theoretical results (in Section 3.3 and the Appendix)."}, {"heading": "4.2 Robotic Experiments", "text": "We used two freely available range-only SLAM data sets collected from an autonomous lawn mowing robot (Djugash, 2010), shown in Fig. 3A.1 These \u201cPlaza\u201d datasets were collected via radio nodes from Multispectral Solutions that use time-of-flight of ultra-wide-band signals to provide inter-node ranging measurements. (Additional details on the experimental setup can be found in (Djugash, 2010).) This system produces a time-stamped range estimate between the mobile robot and stationary nodes (landmarks) in the environment. The landmark radio nodes are placed atop traffic cones approximately 138cm above the ground throughout the environment, and one node was placed on top of the center of the robot\u2019s coordinate frame (also 138cm above the ground). The robot odometry (dead reckoning) comes from an onboard fiberoptic gyro and wheel encoders. The two environmental setups, including the locations of the landmarks, the dead reckoning paths, and the ground truth paths, are shown in Figure 3B-C. The ground truth paths have 2cm accuracy according to (Djugash, 2010).\nThe two Plaza datasets that we used to evaluate our algorithm have very different characteristics. In \u201cPlaza 1,\u201d the robot travelled 1.9km, occupied 9,658 distinct poses, and received 3,529 range measurements. The path taken is a typical lawn mowing pattern that balances left turns with an equal number of right turns; this type of pattern minimizes the effect of heading error. In \u201cPlaza 2,\u201d the robot travelled 1.3km, occupied 4,091 poses, and received 1,816 range measurements. The path taken is a loop which amplifies the effect of heading error. The two data sets were both very sparse, with approximately 11 time steps (and up to 500 steps) between range readings for the worst landmark. We first interpolated the missing range readings with the method of Section 3.4. Then we applied the rank-7 spectral SLAM algorithm of Section 3.3; the results are depicted in Figure 3B-C. Qualitatively, we see that the robot\u2019s localization path conforms to the true path.\nIn addition to the qualitative results, we quantitatively compared spectral SLAM to a number of different competing range-only SLAM algorithms. The localization root mean squared error (RMSE) in meters for each algorithm is shown in Figure 4. The baseline is dead reckoning (using only the robot\u2019s odometry information). Next are several standard online range-only SLAM algorithms, summarized in (Djugash, 2010). These algorithms included the Cartesian EKF, FastSLAM (Montemerlo et al., 2002) with 5,000 particles, and the ROP-EKF (Djugash & Singh, 2008). These previous results only reported the RMSE for the last 10% of the path, which is typically the best 10% of the path (since it gives the most time to recover from initialization problems). The full path localization error can be considerably worse, particularly for the initial portion of the path\u2014see Fig. 5 (right) of (Djugash & Singh, 2008).\nWe also compared to batch nonlinear optimization, via Gauss-Newton as implemented in Matlab\u2019s fminunc (see (Kehagias et al., 2006) for details). This approach to solving the range-only SLAM problem can be very data efficient, but is subject to local optima and is very computationally intensive. We followed the suggestions of (Kehagias et al., 2006) and initialized with the dead-reckoning estimate of the robot\u2019s path. The algorithm took roughly 2.5 hours to converge on Plaza 1, and 45 minutes to converge on Plaza 2. Under most evaluation metrics, the nonlinear batch algorithm handily beats the EKF-based alternatives.\nFinally, we ran our spectral SLAM algorithm on the same data sets. In contrast to Gauss-Newton, spectral SLAM is statistically consistent, and much faster: the bulk of the computation is the fixedrank SVD, so the time complexity of the algorithm is O((2N)2T ) where N is the number of landmarks and T is the number of time steps. Empirically, spectral SLAM produced results that were comparable to batch optimization in 3-4 orders of magnitude less time (see Figure 4).\nSpectral SLAM can also be used as an initialization procedure for nonlinear batch optimization. This strategy combines the best of both algorithms by allowing the locally optimal nonlinear optimization procedure to start from a theoretically guaranteed good starting point. Therefore, the local optimum found by nonlinear batch optimization should be no worse than the spectral SLAM solution and likely much better than the batch optimization seeded by dead-reckoning. Empirically, we found this to be the case (Figure 4). If time and computational resources are scarce, then we believe that spectral SLAM is clearly the best approach; if computation is not an issue, the best results will almost\n1http://www.frc.ri.cmu.edu/projects/emergencyresponse/RangeData/index.html\ncertainly be found by refining the spectral SLAM solution using a nonlinear batch optimization procedure."}, {"heading": "5 Conclusion", "text": "We proposed a novel solution for the range-only SLAM problem that differs substantially from previous approaches. The essence of this new approach is to formulate SLAM as a factorization problem, which allows us to derive a local-minimum free spectral learning method that is closely related to SfM and spectral approaches to system identification. We provide theoretical guarantees for our algorithm, discuss how to derive an online algorithm, and show how to generalize to a full robot system identification algorithm. Finally, we demonstrate that our spectral approach to SLAM beats other state-of-the-art SLAM approaches on real-world range-only SLAM problems."}, {"heading": "Acknowledgements", "text": "Byron Boots and Geoffrey Gordon were supported by ONR MURI grant number N00014-09-11052. Byron Boots was supported by the NSF under grant number EEEC-0540865."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Metric Upgrade for Learned Map", "text": "In the main body of the paper, we assumed that global position estimates of at least four landmarks were known. When these landmarks are known, we can recover all of the estimated landmark positions and robot locations.\nIn many cases, however, no global positions are known; the best we can hope to do is recover landmark and robot positions up to an orthogonal transform (translation, rotation, and reflection). It turns out that Eqs. (5\u20136) provide enough geometric constraints to perform this metric upgrade, as long as we have at least 9 landmarks and at least 9 time steps, and as long as C and X are nonsingular in the following sense: define the matrix C2, with the same number of rows as C but 10 columns, whose ith row has elements ci,jci,k for 1 \u2264 j \u2264 k \u2264 4 (in any fixed order). Note that the rank of C2 can be at most 9: from Eq. 5, we know that c2i,2 + c 2 i,3 \u2212 2ci,4 = 0, and each of the three terms in this function is a multiple of a column of C2. We will say that C is nonsingular if C2 has rank exactly 9, i.e., is rank deficient by exactly 1 dimension. The conditions for X are analogous, swapping rows for columns.2\nTo derive the metric upgrade, suppose that we start from an N \u00d7 4 matrix U of learned landmark coordinates and an 4 \u00d7 N matrix V of learned robot coordinates from the algorithm of Sec. 3.1. And, suppose that we have at least 9 nonsingular landmarks and robot positions. We would like to transform the learned coordinates into two new matrices C and X such that\nc1 \u2248 1 (13)\nc4 \u2248 1\n2 c22 +\n1 2 c23 (14)\nx4 \u2248 1 (15)\nx1 \u2248 1\n2 x22 +\n1 2 x23 (16)\nwhere c is a row of C and x is a column of X .\nAt a high level, we first fit a quadratic surface to the rows of U , then transform this surface so that it satisfies Eq. 13\u201314, and scale the surface so that it satisfies Eq. 15. Our surface will then automatically also satisfy Eq. 16, since X must be metrically correct if C is.\nIn more detail, we first (step i) linearly transform each row of U into approximately the form (1, ri,1, ri,2, ri,3): we use linear regression to find a coefficient vector a \u2208 R4 such that Ua \u2248 1, then set R = UQ where Q \u2208 R4\u00d73 is an orthonormal basis for the nullspace of aT. After this step, our factorization is (UT1)(T\u221211 V ), where T1 = (a Q).\nNext (step ii) we fit an implicit quadratic surface to the rows of R by finding 10 coefficients bjk (for 0 \u2264 j \u2264 k \u2264 3) such that\n0 \u2248 b00 + b01ri,1 + b02ri,2 + b03ri,3 + b11r 2 i,1 + b12ri,1ri,2 + b13ri,1ri,3 + b22r 2 i,2 + b23ri,2ri,3 + b33r 2 i,3\nTo do so, we form a matrix S that has the same number of rows as U but 10 columns. The elements of row i of S are ri,jri,k for 0 \u2264 j \u2264 k \u2264 3 (in any fixed order). Here, for convenience, we define ri,0 = 1 for all i. Then we find a vector b \u2208 R10 that is approximately in the nullspace of ST by taking a singular value decomposition of S and selecting the right singular vector corresponding to the smallest singular value. Using this vector, we can define our quadratic as 0 \u2248 12r\nTHr+`Tr+b00, where r is a row of R, and the Hessian matrix H and linear part ` are given by:\nH =  12b11 b12 b13b21 12b22 b23 b31 b32 1 2b33  ` = ( b01b02 b03 ) 2For intuition, a set of landmarks or robot positions that all lie on the same quadratic surface (line, circle, parabola, etc.) will be singular. Some higher-order constraints will also lead to singularity; e.g., a set of points will be singular if they all satisfy 1\n2 (x2i + y 2 i )xi + yi = 0, since each of the two terms in this function is a\ncolumn of C2.\nOver the next few steps we will transform the coordinates in R to bring our quadratic into the form of Eq. 14: that is, one coordinate will be a quadratic function of the other two, there will be no linear or constant terms, and the quadratic part will be spherical with coefficient 12 .\nWe start (step iii) by transforming coordinates so that our quadratic has no cross-terms, i.e., so that its Hessian matrix is diagonal. Using a 3 \u00d7 3 singular value decomposition, we can factor H = MH \u2032MT so thatM is orthonormal andH \u2032 is diagonal. If we setR\u2032 = RM and `\u2032 = M`, and write r\u2032 for a row of R\u2032, we can equivalently write our quadratic as 0 = 12 (r\n\u2032)TH \u2032r\u2032+ (`\u2032)Tr\u2032+ b00, which has a diagonal Hessian as desired. After this step, our factorization is (UT1T2)(T\u221212 T \u22121 1 V ), where\nT2 = ( 1 0 0 M ) Our next step (step iv) is to turn our implicit quadratic surface into an explicit quadratic function. For this purpose we pick one of the coordinates of R\u2032 and write it as a function of the other two. In order to do so, we must have zero as the corresponding diagonal element of the HessianH \u2032\u2014else we cannot guarantee that we can solve for a unique value of the chosen coordinate. So, we will take the index j such that H \u2032jj is minimal, and set H \u2032 jj = 0. Suppose that we pick the last coordinate, j = 3. (We can always reorder columns to make this true; SVD software will typically do so automatically.) Then our quadratic becomes\n0 = 1\n2 H \u203211(r \u2032 1)\n2 + 1\n2 H \u203222(r \u2032 2) 2 + `\u20321r \u2032 1 + ` \u2032 2r \u2032 2 + ` \u2032 3r \u2032 3 + b00\nr\u20323 = \u2212 1\n`\u20323\n[ 1\n2 H \u203211(r \u2032 1)\n2 + 1\n2 H \u203222(r \u2032 2) 2 + `\u20321r \u2032 1 + ` \u2032 2r \u2032 2 + b00 ] Now (step v) we can shift and rescale our coordinates one more time to get our quadratic in the desired form: translate so that the linear and constant coefficients are 0, and rescale so that the quadratic coefficients are 12 . For the translation, we define new coordinates r\n\u2032\u2032 = r\u2032 + c for c \u2208 R3, so that our quadratic becomes\nr\u2032\u20323 = c3 \u2212 1\n`\u20323\n[ 1\n2 H \u203211(r \u2032\u2032 1 \u2212 c1)2 +\n1 2 H \u203222(r \u2032\u2032 2 \u2212 c2)2 + `\u20321(r\u2032\u20321 \u2212 c1) + `\u20322(r\u2032\u20322 \u2212 c2) + b00 ] By expanding and matching coefficients, we know c must satisfy\n0 = H \u203211 `\u20323 c1 \u2212 `\u20321 `\u20323\n(coefficient of r\u2032\u20321 )\n0 = H \u203222 `\u20323 c2 \u2212 `\u20322 `\u20323\n(coefficient of r\u2032\u20322 )\n0 = c3 \u2212 H \u203211 2`\u20323 c21 \u2212 H \u203222 2`\u20323 c22 + `\u20321 `\u20323 c1 + `\u20322 `\u20323 c2 \u2212 b00/`\u20323 (constant)\nThe first two equations are linear in c1 and c2 (and don\u2019t contain c3). So, we can solve directly for c1 and c2; then we can plug their values into the last equation to find c3. For the scaling, the coefficient of r\u2032\u20321 is now \u2212 H\u203211 2`\u20323 , and that of r\u2032\u20322 is now \u2212 H\u203222 2`\u20323 . So, we can just scale these two coordinates separately to bring their coefficients to 12 .\nAfter this step, our factorization is U \u2032V \u2032, where U \u2032 = UT1T2T3 and V \u2032 = T\u221213 T \u22121 2 T \u22121 1 V , and\nT3 =  1 0 0 0 c1 \u2212 ` \u2032 3 H\u203211 0 0\nc2 0 \u2212 ` \u2032 3\nH\u203222 0\nc3 0 0 1  The left factor U \u2032 will now satisfy Eq. 13\u201314. We still have one last useful degree of freedom: if we set C = U \u2032T4, where\nT4 =  1 0 0 00 \u00b5 0 00 0 \u00b5 0 0 0 0 \u00b52 \nfor any \u00b5 \u2208 R, then C will still satisfy Eq. 13\u201314. So (step vi), we will pick \u00b5 to satisfy Eq. 15: in particular, we set \u00b5 = \u221a mean(V \u20324,:), so that when we set X = T \u22121 4 V\n\u2032, the last row of X will have mean 1.\nIf we have 7 learned coordinates in U as in Sec. 3.2, we need to find a subspace of 4 coordinates in order to perform metric upgrade. To do so, we take advantage of the special form of the correct answer, given in Eq. 9: in the upper block of C in Eq. 9, three coordinates are identically zero. Since U is a linear transformation of C, there will be three linear functions of the top block of U that are identically zero (or approximately zero in the presence of noise). As long as the landmark positions are nonsingular, we can use SVD on the top block of U to find and remove these linear functions (by setting the smallest three singular values to zero), then proceed as above with the four remaining coordinates."}, {"heading": "6.2 Sample Complexity for the Measurement Model (Robot Map)", "text": "Here we provide the details on how our estimation error scales with the number T of training examples\u2014that is, the scaling of the difference between the estimated measurement model U\u0302 , which contains the location of the landmarks, and its population counterpart.\nOur bound has two parts. First we use a standard concentration bound (the Azuma-Hoeffding inequality) to show that each element of our estimated covariance M\u0302 = Y\u0302 Y\u0302 > approaches its population value. We start by rewriting the empirical covariance matrix as a vector summed over multiple samples:\nvec ( M\u0302 ) = 1\nT T\u2211 t=1 \u03a5:,t\nwhere \u03a5 = (Y\u0302 Y\u0302 )> is the matrix of column-wise Kronecker products of the observations Y\u0302 . We assume that each element of \u03a5 minus its expectation E\u03a5i is bounded by a constant c; we can derive c from bounds on anticipated errors in distance measurements and odometry measurements.\n|\u03a5i,t \u2212 E\u03a5i| \u2264 c, \u2200i,t Then the Azuma-Hoeffding inequality bounds the probability that the empirical sum differs too much from its population value: for any \u03b1 \u2265 0 and any i,\nP [\u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 (\u03a5i,t \u2212 E\u03a5i) \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b1 ] \u2264 2e\u2212\u03b1 2/2Tc2\nIf we pick \u03b1 = \u221a 2Tc2 log(T ), then we can rewrite the probability in terms of T :\nP\n[ 1\nT \u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 (\u03a5i,t \u2212 E\u03a5i) \u2223\u2223\u2223\u2223\u2223 \u2265 c \u221a 2 log(T ) T ] \u2264 2e\u2212 log(T )\nwhich means that the probability decreases as O( 1T ) and the threshold decreases as O\u0303( 1\u221a T ).\nWe can then use a union bound over all (2N)2 covariance elements (since Y\u0302 \u2208 R2N\u00d7T ):\nP [ \u2200i \u2223\u2223\u2223\u2223\u2223 1T T\u2211 t=1 \u03a5i,t \u2212 E\u03a5i \u2223\u2223\u2223\u2223\u2223 \u2265 c \u221a 2 log(T ) T ] \u2264 8N2/T\nThat is, with high probability, the entire empirical covariance matrix M\u0302 will be close (in max-norm) to its expectation.\nNext we use the continuity of the SVD to show that the learned subspace approaches its true value. Let M\u0302 = M + E, where E is the perturbation (so the largest element of E is bounded). Let U\u0302 be the output of SVD, and let U be the population value (the top singular vectors of the true M ). Let \u03a8 be the matrix of canonical angles between range(U) and range(U\u0302). Since we know the exact rank\nof the true M (either 4 or 7), the last (4th or 7th) singular value of M will be positive; call it \u03b3 > 0. So, by Theorem 4.4 of Stewart and Sun (Stewart & Sun, 1990),\n|| sin \u03a8||2 \u2264 ||E||2 \u03b3\nThis result uses a 2-norm bound on E, but the bound we showed above is in terms of the largest element of E. But, the 2-norm can be bounded in terms of the largest element:\n||E||2 \u2264 N max ij |Eij |\nFinally, the result is that we can bound the canonical angle: || sin \u03a8||2 \u2264 Nc \u221a 2 log(T ) T\n\u03b3\nIn other words, the canonical angle shrinks at a rate of O\u0303( 1\u221a T\n), with probability at least 1\u2212 8N 2\nT ."}, {"heading": "6.3 The Robot as a Nonlinear Dynamical System", "text": "Once we have learned an interpretable state space via the algorithm of Section 3.3, we can simply write down the nominal robot dynamics in this space. The accuracy of the resulting model will depend on how well our sensors and actuators follow the nominal dynamics, as well as how well we have learned the transformation S to the interpretable version of the state space.\nIn more detail, we model the robot as a controlled nonlinear dynamical system. The evolution is governed by the following state space equations, which generalize (1):\nst+1 = f(st, at) + t (17) ot = h(st) + \u03bdt (18)\nHere st \u2208 Rk denotes the hidden state, at \u2208 Rl denotes the control signal, ot \u2208 Rm denotes the observation, t \u2208 Rk denotes the state noise, and \u03bdt \u2208 Rm denotes the observation noise. For our range-only system, following the decomposition of Section 3, we have:\nst = \n1 \u2212xt \u2212yt\n(x2t + y 2 t )/2\n\u2212 cos(\u03b8t) \u2212 sin(\u03b8t)\nx2t+1\u2212x 2 t+y 2 t+1\u2212y 2 t\n2vt\n , ot = \nd21t/2 ...\nd2Nt/2 d21t+1\u2212d 2 1t\n2vt ...\nd2Nt+1\u2212d 2 Nt\n2vt\n , at = [ vt cos(\u03c9t) sin(\u03c9t) ] (19)\nHere vt and \u03c9t are the translation and rotation calculated from the robot\u2019s odometry. A nice property of this model is that expected observations are a linear function of state:\nh(st) = Cst (20)\nThe dynamics, however, are nonlinear: see Eq. 21, which can easily be derived from the basic kinematic motion model for a wheeled robot (Thrun et al., 2005).\nf(st, at) = \n1 \u2212xt \u2212 vt cos(\u03b8t) \u2212yt \u2212 vt sin(\u03b8t)\nx2t+y 2 t 2 + vtxt cos(\u03b8t) + vtyt sin(\u03b8t) + v2t cos 2(\u03b8t)+v 2 t sin 2(\u03b8t) 2\n\u2212 cos(\u03b8t) cos(\u03c9t) + sin(\u03b8t) sin(\u03c9t) \u2212 sin(\u03b8t) cos(\u03c9t) + cos(\u03b8t) sin(\u03c9t)\n[xt cos(\u03b8t) cos(\u03c9t)\u2212 xt sin(\u03b8t) sin(\u03c9t) + vt cos2(\u03b8t) cos(\u03c9t) + yt sin(\u03b8t) cos(\u03c9t)\u2212 yt sin(\u03c9t) cos(\u03b8t) + vt sin2(\u03b8t) cos(\u03c9t)\u2212\n2vt cos(\u03b8t) sin(\u03b8t) sin(\u03c9t)]\n (21)"}, {"heading": "6.3.1 Robot System Identification", "text": "To apply the model of Section 6.3, it is essential that we maintain states in the physical coordinate frame, and not just the linearly transformed coordinate frame\u2014i.e., C\u0302 and not U\u0302 = C\u0302S\u22121. So, to use this model, we must first learn S either by regression or by metric upgrade.\nHowever, it is possible instead to use system identification to learn to filter directly in the raw state space U\u0302 . We conjecture that it may be more robust to do so, since we will not be sensitive to errors in the metric upgrade process (errors in learning S), and since we can learn to compensate for some deviations from the nominal model of Section 6.3.\nTo derive our system identification algorithm, we can explicitly rewrite f(st, at) as a nonlinear feature-expansion map followed by a linear projection. Our algorithm will then just be to use linear regression to learn the linear part of f .\nFirst, let\u2019s look at the dynamics for the special case of S = I . Each additive term in Eq. 21 is the product of at most two terms in st and at most two terms in at. Therefore, we define \u03c6(st, at) := st \u2297 st \u2297 a\u0304t \u2297 a\u0304t, where a\u0304t = [1, at]T and \u2297 is the Kronecker product. (Many of the dimensions of \u03c6(st, at) are duplicates; for efficiency we would delete these duplicates, but for simplicity of notation we keep them.) Each additive term in Eq. 21 is a multiple of an element of \u03c6(st, at), so we can write the dynamics as:\nst+1 = N\u03c6(st, at) + t (22)\nwhere N is a linear function that picks out the correct entries to form Eq. 21.\nNow, given an invertible matrix S, we can rewrite f(st, at) as an equivalent function in the transformed state space:\nSst+1 = f\u0304(Sst, at) + S t (23)\nTo do so, we use the identity (Ax)\u2297 (By) = (A\u2297B)(x\u2297 y). Repeated application yields\n\u03c6(Sst, at) = Sst \u2297 Sst \u2297 a\u0304t \u2297 a\u0304t = (S \u2297 S \u2297 I \u2297 I)(st \u2297 st \u2297 a\u0304t \u2297 a\u0304t) = S\u0304 \u03c6(st, at) (24)\nwhere S\u0304 = S \u2297 S \u2297 I \u2297 I . Note that S\u0304 is invertible (since rank(A \u2297 B) = rank(A) rank(B)); so, we can write\nf\u0304(Sst, at) = SNS\u0304 \u22121S\u0304\u03c6(st, at) = Sf(st, at) (25)\nUsing this representation, we can learn the linear part of f , SNS\u0304\u22121, directly from our state estimates: we just do a linear regression from \u03c6(Sst, at) to Sst+1.\nFor convenience, we summarize the entire learning algorithm (state space discovery followed by system identification) as Algorithm 2."}, {"heading": "6.3.2 Filtering with the Extended Kalman Filter", "text": "Whether we learn the dynamics through system identification or simply write them down in the interpretable version of our state space, we will end up with a transition model of the form (22) and an observation model of the form (20). Given these models, it is easy to write down an EKF which tracks the robot state. The measurement update is just a standard Kalman filter update (see, e.g., (Thrun et al., 2005)), since the observation model is linear. For the motion update, we need a Taylor approximation of the expected state at time t+ 1 around the current MAP state s\u0302t, given the current action at:\nst+1 \u2212 st \u2248 N [\u03c6(s\u0302t, at) + d\u03c6ds \u2223\u2223 s\u0302t\n(st \u2212 s\u0302t)] (26) d\u03c6 ds \u2223\u2223 s\u0302 = (s\u0302\u2297 I + I \u2297 s\u0302)\u2297 a\u0304t \u2297 a\u0304t (27)\nWe simply plug this Taylor approximation into the standard Kalman filter motion update (e.g., (Thrun et al., 2005)).\nAlgorithm 2 Robot System Identification In: T i.i.d. pairs of observations {ot, at}Tt=1, measurement model for 4 landmarks C1:4 (by e.g. GPS) Out: measurement model C\u0302, motion model N\u0302 , robot states X\u0302 (the tth column is state st)\n1: Collect observations and odometry into a matrix Y\u0302 (Eq. 7) 2: Find the the top 7 singular values and vectors: \u3008U\u0302 , \u039b\u0302, V\u0302 >\u3009 \u2190 SVD(Y\u0302 , 7) 3: Find the transformed measurement matrix C\u0302S\u22121 = U\u0302 and robot states SX\u0302 = \u039b\u0302V\u0302 > 4: Compute a matrix \u03a6 with columns \u03a6t = \u03c6(Sst, at). 5: Compute dynamics: SN\u0302S\u0304\u22121 = SX\u03022:T (\u03a61:T\u22121)\u2020 6: Compute the partial S\u22121: S\u0302\u22121 = C\u221211:4 (C\u03021:4S \u22121) where C\u0302S\u22121 comes from step 3. S\u0302\u22121X\u0302 gives\nus the x, y coordinates of the states. These can be used to find X\u0302 (see Section 3.2) 7: Given X\u0302 , we can compute the full S as S = (SX\u0302)X\u0302\u2020 8: Finally, from steps 3,5, and 7, we find the interpretable measurement model (C\u0302S\u22121)S and motion model N = S\u22121(SNS\u0304\u22121)S\u0304."}], "references": [{"title": "Action respecting embedding", "author": ["M. Biggs", "A. Ghodsi", "D. Wilkinson", "M. Bowling"], "venue": "In Proceedings of the Twenty-Second International Conference on Machine Learning (pp. 65\u201372)", "citeRegEx": "Biggs et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Biggs et al\\.", "year": 2005}, {"title": "Predictive state temporal difference learning", "author": ["B. Boots", "G. Gordon"], "venue": "Advances in neural information processing systems", "citeRegEx": "Boots and Gordon,? \\Q2010\\E", "shortCiteRegEx": "Boots and Gordon", "year": 2010}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "Proceedings of the 25th National Conference on Artificial Intelligence (AAAI-2011)", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["B. Boots", "S.M. Siddiqi", "G.J. Gordon"], "venue": "Proceedings of Robotics: Science and Systems VI", "citeRegEx": "Boots et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2010}, {"title": "Fast low-rank modifications of the thin singular value decomposition", "author": ["M. Brand"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Brand,? \\Q2006\\E", "shortCiteRegEx": "Brand", "year": 2006}, {"title": "Matrix completion with noise. CoRR, abs/0903.3131", "author": ["E.J. Cand\u00e8s", "Y. Plan"], "venue": null, "citeRegEx": "Cand\u00e8s and Plan,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Plan", "year": 2009}, {"title": "Geolocation with range: Robustness, efficiency and scalabilityPhD", "author": ["J. Djugash"], "venue": null, "citeRegEx": "Djugash,? \\Q2010\\E", "shortCiteRegEx": "Djugash", "year": 2010}, {"title": "A robust method of localization and mapping using only range", "author": ["J. Djugash", "S. Singh"], "venue": "International Symposium on Experimental Robotics", "citeRegEx": "Djugash and Singh,? \\Q2008\\E", "shortCiteRegEx": "Djugash and Singh", "year": 2008}, {"title": "Further results with localization and mapping using range from radio", "author": ["J. Djugash", "S. Singh", "P.I. Corke"], "venue": "International Conference on Field and Service Robotics", "citeRegEx": "Djugash et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Djugash et al\\.", "year": 2005}, {"title": "WiFi-SLAM using Gaussian process latent variable models", "author": ["B. Ferris", "D. Fox", "N. Lawrence"], "venue": "Proceedings of the 20th international joint conference on Artifical intelligence (pp", "citeRegEx": "Ferris et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ferris et al\\.", "year": 2007}, {"title": "A spectral algorithm for learning hidden Markov models. COLT", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Factorization methods for structure from motion", "author": ["T. Kanade", "D. Morris"], "venue": "Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering", "citeRegEx": "Kanade and Morris,? \\Q1998\\E", "shortCiteRegEx": "Kanade and Morris", "year": 1998}, {"title": "Preliminary results in range-only localization and mapping", "author": ["G.A. Kantor", "S. Singh"], "venue": "Proceedings of the IEEE Conference on Robotics and Automation (ICRA", "citeRegEx": "Kantor and Singh,? \\Q2002\\E", "shortCiteRegEx": "Kantor and Singh", "year": 2002}, {"title": "Range-only SLAM with interpolated range data (Technical Report CMU-RI-TR-06-26)", "author": ["A. Kehagias", "J. Djugash", "S. Singh"], "venue": "Robotics Institute", "citeRegEx": "Kehagias et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kehagias et al\\.", "year": 2006}, {"title": "Experimental results in range-only localization with radio", "author": ["D. Kurth", "G.A. Kantor", "S. Singh"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS \u201903) (pp", "citeRegEx": "Kurth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kurth et al\\.", "year": 2003}, {"title": "FastSLAM: A factored solution to the simultaneous localization and mapping problem", "author": ["M. Montemerlo", "S. Thrun", "D. Koller", "B. Wegbreit"], "venue": "In Proceedings of the AAAI National Conference on Artificial Intelligence (pp. 593\u2013598)", "citeRegEx": "Montemerlo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Montemerlo et al\\.", "year": 2002}, {"title": "Learning low dimensional predictive representations", "author": ["M. Rosencrantz", "G.J. Gordon", "S. Thrun"], "venue": null, "citeRegEx": "Rosencrantz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosencrantz et al\\.", "year": 2004}, {"title": "Localization from mere connectivity", "author": ["Y. Shang", "W. Ruml", "Y. Zhang", "M.P.J. Fromherz"], "venue": "Proceedings of the 4th ACM international symposium on Mobile ad hoc networking & computing (pp. 201\u2013212)", "citeRegEx": "Shang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2003}, {"title": "Reduced-rank hidden Markov models", "author": ["S. Siddiqi", "B. Boots", "G.J. Gordon"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Matrix perturbation theory", "author": ["G.W. Stewart", "Sun", "J.-G"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V.D. Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Probabilistic robotics (intelligent robotics and autonomous agents)", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": null, "citeRegEx": "Thrun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 2005}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tipping and Bishop,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop", "year": 1999}, {"title": "Shape and motion from image streams under orthography: a factorization method", "author": ["C. Tomasi", "T. Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Tomasi and Kanade,? \\Q1992\\E", "shortCiteRegEx": "Tomasi and Kanade", "year": 1992}, {"title": "Factorization methods for projective structure and motion", "author": ["B. Triggs"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Triggs,? \\Q1996\\E", "shortCiteRegEx": "Triggs", "year": 1996}, {"title": "Subspace identification for linear systems: Theory, implementation, applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "Overschee and Moor,? \\Q1996\\E", "shortCiteRegEx": "Overschee and Moor", "year": 1996}, {"title": "Map building without localization by dimensionality reduction techniques", "author": ["T. Yairi"], "venue": "Proceedings of the 24th international conference on Machine learning (pp. 1071\u20131078)", "citeRegEx": "Yairi,? \\Q2007\\E", "shortCiteRegEx": "Yairi", "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "Popular approaches to range-only SLAM include EKFs and EIFs (Kantor & Singh, 2002; Kurth et al., 2003; Djugash & Singh, 2008; Djugash, 2010; Thrun et al., 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al.", "startOffset": 60, "endOffset": 160}, {"referenceID": 6, "context": "Popular approaches to range-only SLAM include EKFs and EIFs (Kantor & Singh, 2002; Kurth et al., 2003; Djugash & Singh, 2008; Djugash, 2010; Thrun et al., 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al.", "startOffset": 60, "endOffset": 160}, {"referenceID": 21, "context": "Popular approaches to range-only SLAM include EKFs and EIFs (Kantor & Singh, 2002; Kurth et al., 2003; Djugash & Singh, 2008; Djugash, 2010; Thrun et al., 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al.", "startOffset": 60, "endOffset": 160}, {"referenceID": 8, "context": ", 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al., 2005; Thrun et al., 2005), and batch optimization of a likelihood function (Kehagias et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 21, "context": ", 2005), multiple-hypothesis trackers (including particle filters and multiple EKFs/EIFs) (Djugash et al., 2005; Thrun et al., 2005), and batch optimization of a likelihood function (Kehagias et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 13, "context": ", 2005), and batch optimization of a likelihood function (Kehagias et al., 2006).", "startOffset": 57, "endOffset": 80}, {"referenceID": 6, "context": ", by changing the hypothesis representation (Djugash, 2010) or by keeping multiple hypotheses (Djugash et al.", "startOffset": 44, "endOffset": 59}, {"referenceID": 8, "context": ", by changing the hypothesis representation (Djugash, 2010) or by keeping multiple hypotheses (Djugash et al., 2005; Djugash, 2010; Thrun et al., 2005).", "startOffset": 94, "endOffset": 151}, {"referenceID": 6, "context": ", by changing the hypothesis representation (Djugash, 2010) or by keeping multiple hypotheses (Djugash et al., 2005; Djugash, 2010; Thrun et al., 2005).", "startOffset": 94, "endOffset": 151}, {"referenceID": 21, "context": ", by changing the hypothesis representation (Djugash, 2010) or by keeping multiple hypotheses (Djugash et al., 2005; Djugash, 2010; Thrun et al., 2005).", "startOffset": 94, "endOffset": 151}, {"referenceID": 3, "context": "As we will discuss in Section 2, our approach to SLAM has much in common with spectral algorithms for subspace identification (Van Overschee & De Moor, 1996; Boots et al., 2010); unlike these methods, our focus on SLAM makes it easy to interpret our state space.", "startOffset": 126, "endOffset": 177}, {"referenceID": 24, "context": "Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al.", "startOffset": 74, "endOffset": 134}, {"referenceID": 17, "context": "Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al., 2003; Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 225, "endOffset": 299}, {"referenceID": 0, "context": "Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al., 2003; Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 225, "endOffset": 299}, {"referenceID": 9, "context": "Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al., 2003; Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 225, "endOffset": 299}, {"referenceID": 26, "context": "Our approach is also related to factorization-based structure from motion (Tomasi & Kanade, 1992; Triggs, 1996; Kanade & Morris, 1998), as well as to recent dimensionality-reduction-based methods for localization and mapping (Shang et al., 2003; Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 225, "endOffset": 299}, {"referenceID": 14, "context": "The standard probabilistic model for range-only localization (Kantor & Singh, 2002; Kurth et al., 2003) represents robot state by a vector st = [xt, yt, \u03b8t]; the robot\u2019s (nonlinear) motion and observation models are", "startOffset": 61, "endOffset": 103}, {"referenceID": 6, "context": "The resulting approach is called the ROP-EKF, and is shown to outperform the ordinary (Cartesian) EKF in several real-world problems, especially in combination with multiple-hypothesis tracking (Djugash & Singh, 2008; Djugash, 2010).", "startOffset": 194, "endOffset": 232}, {"referenceID": 21, "context": "But, the inverse covariance is often approximately sparse, leading to much more efficient approximate computation (Thrun et al., 2005).", "startOffset": 114, "endOffset": 134}, {"referenceID": 10, "context": "Originally, subspace identification algorithms were almost exclusively used for linear system identification (Van Overschee & De Moor, 1996), but recently, similar spectral algorithms have been used to learn models of partially observable nonlinear dynamical systems such as HMMs (Hsu et al., 2009; Siddiqi et al., 2010) and PSRs (Rosencrantz et al.", "startOffset": 280, "endOffset": 320}, {"referenceID": 18, "context": "Originally, subspace identification algorithms were almost exclusively used for linear system identification (Van Overschee & De Moor, 1996), but recently, similar spectral algorithms have been used to learn models of partially observable nonlinear dynamical systems such as HMMs (Hsu et al., 2009; Siddiqi et al., 2010) and PSRs (Rosencrantz et al.", "startOffset": 280, "endOffset": 320}, {"referenceID": 16, "context": ", 2010) and PSRs (Rosencrantz et al., 2004; Boots et al., 2010; Boots & Gordon, 2010; Boots et al., 2011).", "startOffset": 17, "endOffset": 105}, {"referenceID": 3, "context": ", 2010) and PSRs (Rosencrantz et al., 2004; Boots et al., 2010; Boots & Gordon, 2010; Boots et al., 2011).", "startOffset": 17, "endOffset": 105}, {"referenceID": 2, "context": ", 2010) and PSRs (Rosencrantz et al., 2004; Boots et al., 2010; Boots & Gordon, 2010; Boots et al., 2011).", "startOffset": 17, "endOffset": 105}, {"referenceID": 17, "context": "For example, MDS has been used to determine a Euclidean map of sensor locations where there is no distinction between landmark positions and robot positions (Shang et al., 2003): instead all-toall range measurements are assumed for a set of landmarks.", "startOffset": 157, "endOffset": 177}, {"referenceID": 20, "context": "the geodesic distance between the landmarks (Tenenbaum et al., 2000; Shang et al., 2003).", "startOffset": 44, "endOffset": 88}, {"referenceID": 17, "context": "the geodesic distance between the landmarks (Tenenbaum et al., 2000; Shang et al., 2003).", "startOffset": 44, "endOffset": 88}, {"referenceID": 0, "context": "Such nonlinear dimensionality reduction has been used to learn maps of wi-fi networks and landmark locations when sensory data is thought to be nonlinearly related to the underlying Eucidean space in which the landmarks lie (Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 224, "endOffset": 278}, {"referenceID": 9, "context": "Such nonlinear dimensionality reduction has been used to learn maps of wi-fi networks and landmark locations when sensory data is thought to be nonlinearly related to the underlying Eucidean space in which the landmarks lie (Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 224, "endOffset": 278}, {"referenceID": 26, "context": "Such nonlinear dimensionality reduction has been used to learn maps of wi-fi networks and landmark locations when sensory data is thought to be nonlinearly related to the underlying Eucidean space in which the landmarks lie (Biggs et al., 2005; Ferris et al., 2007; Yairi, 2007).", "startOffset": 224, "endOffset": 278}, {"referenceID": 26, "context": "(In particular, (Yairi, 2007) suggests solving range-only mapping using nonlinear dimensionality reduction.", "startOffset": 16, "endOffset": 29}, {"referenceID": 4, "context": "The extension to online SLAM is straightforward: instead of first estimating \u0176 and then performing a SVD, we sequentially estimate our factors \u3008\u00db , \u039b\u0302, V\u0302 >\u3009 via online SVD (Brand, 2006; Boots et al., 2011).", "startOffset": 173, "endOffset": 206}, {"referenceID": 2, "context": "The extension to online SLAM is straightforward: instead of first estimating \u0176 and then performing a SVD, we sequentially estimate our factors \u3008\u00db , \u039b\u0302, V\u0302 >\u3009 via online SVD (Brand, 2006; Boots et al., 2011).", "startOffset": 173, "endOffset": 206}, {"referenceID": 6, "context": "We used two freely available range-only SLAM data sets collected from an autonomous lawn mowing robot (Djugash, 2010), shown in Fig.", "startOffset": 102, "endOffset": 117}, {"referenceID": 6, "context": "(Additional details on the experimental setup can be found in (Djugash, 2010).", "startOffset": 62, "endOffset": 77}, {"referenceID": 6, "context": "The ground truth paths have 2cm accuracy according to (Djugash, 2010).", "startOffset": 54, "endOffset": 69}, {"referenceID": 6, "context": "Next are several standard online range-only SLAM algorithms, summarized in (Djugash, 2010).", "startOffset": 75, "endOffset": 90}, {"referenceID": 15, "context": "These algorithms included the Cartesian EKF, FastSLAM (Montemerlo et al., 2002) with 5,000 particles, and the ROP-EKF (Djugash & Singh, 2008).", "startOffset": 54, "endOffset": 79}, {"referenceID": 13, "context": "We also compared to batch nonlinear optimization, via Gauss-Newton as implemented in Matlab\u2019s fminunc (see (Kehagias et al., 2006) for details).", "startOffset": 107, "endOffset": 130}, {"referenceID": 13, "context": "We followed the suggestions of (Kehagias et al., 2006) and initialized with the dead-reckoning estimate of the robot\u2019s path.", "startOffset": 31, "endOffset": 54}], "year": 2012, "abstractText": "We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, and many MHT ones, our approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds. Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.", "creator": "LaTeX with hyperref package"}}}