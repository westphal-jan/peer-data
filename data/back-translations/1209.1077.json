{"id": "1209.1077", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2012", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "abstract": "We study the problem of estimating a metric that is thought to be anchored in a Hilbert space in many ways in terms of optimal transport metrics. By establishing a precise link between optimal transport metrics, optimal quantization, and learning theory, we deduce new probabilistic limits for the performance of a classical algorithm in unsupervised learning (k-mean) by making a probability measurement derived from the data. In the course of the analysis, we arrive at new lower limits as well as probable upper limits of the convergence rate of the empirical law of large numbers, which, unlike existing limits, are applicable to a broad class of metrics.", "histories": [["v1", "Wed, 5 Sep 2012 19:10:09 GMT  (521kb,D)", "http://arxiv.org/abs/1209.1077v1", "13 pages, 2 figures. Advances in Neural Information Processing Systems, NIPS 2012"]], "COMMENTS": "13 pages, 2 figures. Advances in Neural Information Processing Systems, NIPS 2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["guillermo d ca\u00f1as", "lorenzo rosasco"], "accepted": true, "id": "1209.1077"}, "pdf": {"name": "1209.1077.pdf", "metadata": {"source": "CRF", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "authors": ["Guille D. Canas", "Lorenzo A. Rosasco"], "emails": ["guilledc@mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction and Motivation", "text": "In this paper we study the problem of learning from random samples a probability distribution supported on a manifold, when the learning error is measured using transportation metrics.\nThe problem of learning a probability distribution is classic in statistics and machine learning, and is typically analyzed for distributions in X = Rd that have a density with respect to the Lebesgue measure, with total variation, and L2 among the common distances used to measure closeness of two densities (see for instance [10, 32] and references therein.) The setting in which the data distribution is supported on a low dimensional manifold embedded in a high dimensional space has only been considered more recently. In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18]. A discussion on several topics related to statistics on a Riemannian manifold can be found in [26].\nIn this paper, we consider the problem of estimating, in the 2-Wasserstein sense, a distribution supported on a manifold embedded in a Hilbert space.\nar X\niv :1\n20 9.\n10 77\nv1 [\ncs .L\nG ]\nThe exact formulation of the problem, as well as a detailed discussion of related previous works are given in Section 2.\nInterestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec. 4.) In fact, as described in the sequel, some of the most widely-used algorithms for unsupervised learning, such as k-means (but also others such as PCA and k-flats), can be shown to be performing exactly the task of estimating the data-generating measure in the sense of the 2-Wasserstein distance. This close relation between learning theory, and optimal transport and quantization seems novel and of interest in its own right. Indeed, in this work, techniques from the above three fields are used to derive the new probabilistic bounds described below.\nOur technical contribution can be summarized as follows:\n(a) we prove uniform lower bounds for the distance between a measure and estimates based on discrete sets (such as the empirical measure or measures derived from algorithms such as k-means);\n(b) we provide new probabilistic bounds for the rate of convergence of the empirical law of large numbers which, unlike existing probabilistic bounds, hold for a very large class of measures;\n(c) we provide probabilistic bounds for the rate of convergence of measures derived from k-means to the data measure.\nThe structure of the paper is described at the end of Section 2, where we discuss the exact formulation of the problem as well as related previous works."}, {"heading": "2 Setup and Previous work", "text": "Consider the problem of learning a probability measure \u03c1 defined on a spaceM, from an i.i.d. sample Xn = (x1, . . . , xn) \u223c \u03c1n of size n. We assume M to be a compact, smooth d-dimensional manifold with C1 metric and volume measure \u03bbM, embedded in the unit ball of a separable Hilbert space X with inner product \u3008\u00b7, \u00b7\u3009, induced norm \u2016 \u00b7 \u2016, and distance d (for instance M = Bd2 (1) the unit ball in X = Rd.) Following [34, p. 94], let Pp(M) denote the Wasserstein space of order 1 \u2264 p <\u221e:\nPp(M) := { \u03c1 \u2208 P (M) : \u222b M \u2016x\u2016pd\u03c1(x) <\u221e } of probability measures with finite p-th moment. The p-Wasserstein distance\nWp(\u03c1, \u00b5) = inf { [E\u2016X \u2212 Y \u2016p]1/p , Law(X) = \u03c1, Law(Y ) = \u00b5 }\n(1)\nwhere the inf is over random variables X,Y with laws \u03c1, \u00b5, respectively, is the optimal expected cost of transporting points generated from \u03c1 to those\ngenerated from \u00b5, and is guaranteed to be finite in Pp(M) [34, p. 95]. The space Pp(M) with the Wp metric is itself a complete separable metric space [34]. We consider here the problem of learning probability measures \u03c1 \u2208 P2(M), where the performance is measured by the distance W2(\u03c1, \u00b7).\nThere are many possible choices of distances between probability measures [13]. Among them, Wp metrizes weak convergence (see [34] theorem 6.9), that is, in Pp(M), a sequence (\u00b5i)i\u2208N of measures converges weakly to \u00b5 iff Wp(\u00b5i, \u00b5)\u2192 0. There are other distances, such as the Le\u0301vy-Prokhorov, or the weak-* distance, that also metrize weak convergence. However, as pointed out by Villani in his excellent monograph [34, p. 98],\n1. \u201cWasserstein distances are rather strong, [...]a definite advantage over the weak-* distance\u201d.\n2. \u201cIt is not so difficult to combine information on convergence in Wasserstein distance with some smoothness bound, in order to get convergence in stronger distances.\u201d\nWasserstein distances have been used to study the mixing and convergence of Markov chains [22], as well as concentration of measure phenomena [20]. To this list we would add the important fact that existing and widely-used algorithms for unsupervised learning can be easily extended (see Sec. 4) to compute a measure \u03c1\u2032 that minimizes the distance W2(\u03c1\u0302n, \u03c1 \u2032) to the empirical measure\n\u03c1\u0302n := 1\nn n\u2211 i=1 \u03b4xi ,\na fact that will allow us to prove, in Sec. 5, bounds on the convergence of the measure induced by k-means to the population measure \u03c1.\nThe most useful versions of Wasserstein distance are p = 1, 2, with p = 1 being the weaker of the two (by Ho\u0308lder\u2019s inequality, p \u2264 q \u21d2 Wp \u2264 Wq; a discussion of p = \u221e would take us out of topic, since its behavior is markedly different.) In particular, \u201cresults in W2 distance are usually stronger, and more difficult to establish than results in W1 distance\u201d [34, p. 95]."}, {"heading": "2.1 Closeness of Empirical and Population Measures", "text": "By the empirical law of large numbers, the empirical measure converges almost surely to the population measure: \u03c1\u0302n \u2192 \u03c1 in the sense of the weak topology [33]. Since weak convergence and convergence in Wp are equivalent in Pp(M), this means that, in the Wp sense, the empirical measure \u03c1\u0302n is an arbitrarily good approximation of \u03c1, as n \u2192 \u221e. A fundamental question is therefore how fast the rate of convergence of \u03c1\u0302n \u2192 \u03c1 is."}, {"heading": "2.1.1 Convergence in expectation", "text": "The mean rate of convergence of \u03c1\u0302n \u2192 \u03c1 has been widely studied in the past, resulting in upper bounds of order EW2(\u03c1, \u03c1\u0302n) = O(n\u22121/(d+2)) [19, 8], and lower\nbounds of order EW2(\u03c1, \u03c1\u0302n) = \u2126(n\u22121/d) [29] (both assuming that the absolutely continuous part of \u03c1 is \u03c1A 6= 0, with possibly better rates otherwise).\nMore recently, an upper bound of order EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d) has been proposed [2] by proving a bound for the Optimal Bipartite Matching (OBM) problem [1], and relating this problem to the expected distance EWp(\u03c1, \u03c1\u0302n). In particular, given two independent samples Xn, Yn, the OBM problem is that of finding a permutation \u03c3 that minimizes the matching cost n\u22121 \u2211 \u2016xi \u2212 y\u03c3(i)\u2016p [24, 30]. It is not hard to show that the optimal matching cost is Wp(\u03c1\u0302Xn , \u03c1\u0302Yn ) p, where \u03c1\u0302 Xn , \u03c1\u0302 Yn are the empirical measures associated to Xn, Yn. By Jensen\u2019s inequality, the triangle inequality, and (a+ b)p \u2264 2p\u22121(ap + bp), it holds\nEWp(\u03c1, \u03c1\u0302n)p \u2264 EWp(\u03c1\u0302Xn , \u03c1\u0302Yn ) p \u2264 2p\u22121EWp(\u03c1, \u03c1\u0302n)p,\nand therefore a bound of order O(n\u2212p/d) for the OBM problem [2] implies a bound EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d). The matching lower bound is only known for a special case: \u03c1A constant over a bounded set of non-null measure [2] (e.g. \u03c1A uniform.) Similar results, with matching lower bounds are found for W1 in [11]."}, {"heading": "2.1.2 Convergence in probability", "text": "Results for convergence in probability, one of the main results of this work, appear to be considerably harder to obtain. One fruitful avenue of analysis has been the use of so-called transportation, or Talagrand inequalities Tp, which can be used to prove concentration inequalities on Wp [20]. In particular, we say that \u03c1 satisfies a Tp(C) inequality with C > 0 iff Wp(\u03c1, \u00b5)\n2 \u2264 CH(\u00b5|\u03c1),\u2200\u00b5 \u2208 Pp(M), where H(\u00b7|\u00b7) is the relative entropy [20]. As shown in [6, 5], it is possible to obtain probabilistic upper bounds on Wp(\u03c1, \u03c1\u0302n), with p = 1, 2, if \u03c1 is known to satisfy a Tp inequality of the same order, thereby reducing the problem of bounding Wp(\u03c1, \u03c1\u0302n) to that of obtaining a Tp inequality. Note that, by Jensen\u2019s inequality, and as expected from the behavior ofWp, the inequality T2 is stronger than T1 [20].\nWhile it has been shown that \u03c1 satisfies a T1 inequality iff it has a finite square-exponential moment [4, 7], no such general conditions have been found for T2. As an example, consider that, if M is compact with diameter D then, by theorem 6.15 of [34], and the celebrated Csisza\u0301r-Kullback-Pinsker inequality [27], for all \u03c1, \u00b5 \u2208 Pp(M), it is\nWp(\u03c1, \u00b5) 2p \u2264 (2D)2p\u2016\u03c1\u2212 \u00b5\u20162TV \u2264 22p\u22121D2pH(\u00b5|\u03c1),\nwhere \u2016 \u00b7\u2016TV is the total variation norm. Clearly, this implies a Tp=1 inequality, but for p \u2265 2 it does not.\nThe T2 inequality has been shown by Talagrand to be satisfied by the Gaussian distribution [31], and then slightly more generally by strictly log-concave measures [3]. However, as noted in [6], \u201ccontrary to the T1 case, there is no hope to obtain T2 inequalities from just integrability or decay estimates.\u201d Structure of this paper. In this work we obtain bounds in probability (learning rates) for the problem of learning a probability measure (in the sense of W2.)\nWe begin by establishing (lower) bounds for the convergence of empirical to population measures, which serve to set up the problem and introduce the connection between quantization and measure learning (sec. 3.) We then describe how existing unsupervised learning algorithms that compute a set (k-means, k-flats, PCA,. . . ) can be easily extended to produce a measure (sec. 4.) Due to its simplicity and widespread use, we focus here on k-means. Since the two measure estimates that we consider are the empirical measure, and the measure induced by k-means, we next set out to prove upper bounds on their convergence to the data-generating measure (sec. 5.) We arrive at these bounds by means of intermediate measures, which are related to the problem of optimal quantization. The bounds apply in a very broad setting (unlike existing bounds based on transportation inequalities, they are not restricted to log-concave measures.)"}, {"heading": "3 Learning probability measures, optimal trans-", "text": "port and quantization\nWe address the problem of learning a probability measure \u03c1 when the only observation we have at our disposal is an i.i.d. sample Xn. We begin by establishing some notation and useful intermediate results.\nGiven a closed set S \u2286 M, let \u03c0 S = \u2211 q\u2208S 1Vq(S) \u00b7 q be a nearest neighbor projection onto S (a function mapping points in X to their closest point in S), where {Vq(S) : q \u2208 S} is a Borel Voronoi partition of X such that Vq(S) \u2286 {x \u2208 X : \u2016x \u2212 q\u2016 = minr\u2208S \u2016x \u2212 r\u2016} (see for instance [15].) Since S is closed and \u2016x \u2212 \u00b7\u2016 is continuous and convex, every points x \u2208 X has a closest point in S. Since {Vq(S) : q \u2208 S} is a Borel partition, it follows that \u03c0S is a measurable map. For any \u03c1 \u2208 Pp(M), the pushforward, or image measure \u03c0S\u03c1 under the mapping \u03c0\nS is supported in S, and is such that, for Borel measurable sets A, it\nis (\u03c0 S \u03c1)(A) := \u03c1(\u03c0\u22121S (A)).\nWe now establish a connection between the expected distance to a set S, and the distance between \u03c1 and the set\u2019s induced pushforward measure. Notice that the expected distance to S is exactly the expected quantization error incurred when encoding points drawn from \u03c1 by their closest point in S. This close connection between optimal quantization and Wasserstein distance has been pointed out in the past in the statistics [28], optimal quantization [14, p. 33], and approximation theory literatures [16].\nThe following two lemmas are key tools in the reminder of the paper. The first highlights the close link between quantization and optimal transport.\nLemma 3.1. For closed S \u2286M, \u03c1 \u2208 Pp(M), 1 \u2264 p <\u221e, it holds Ex\u223c\u03c1d(x, S)p = Wp(\u03c1, \u03c0S\u03c1) p.\nNote that the key element in the above lemma is that the two measures in the expression Wp(\u03c1, \u03c0S\u03c1) must match. When there is a mismatch, the distance can only increase. That is, Wp(\u03c1, \u03c0S\u00b5) \u2265 Wp(\u03c1, \u03c0S\u03c1) for all \u00b5 \u2208 Pp(M). In fact, the following lemma shows that, among all the measures with support in S, \u03c0\nS \u03c1 is closest to \u03c1.\nLemma 3.2. For closed S, and all \u00b5 \u2208 Pp(M) with supp(\u00b5) \u2286 S, 1 \u2264 p <\u221e, it holds Wp(\u03c1, \u00b5) \u2265Wp(\u03c1, \u03c0S\u03c1).\nWhen combined, lemmas 3.1 and 3.2 indicate that the behavior of the measure learning problem is limited by the performance of the optimal quantization problem. For instance, Wp(\u03c1, \u03c1\u0302n) can only be, in the best-case, as low as the optimal quantization cost with codebook of size n. The following section makes this claim precise."}, {"heading": "3.1 Lower bounds", "text": "Consider the situation depicted in fig. 1, in which a sample X4 = {x1, x2, x3, x4} is drawn from a distribution \u03c1 which we assume here to be absolutely continuous on its support. As shown, the projection map \u03c0\nX4 sends points x to their closest\npoint in X4. The resulting Voronoi decomposition of supp(\u03c1) is drawn in shades of blue. By lemma 5.2 of [9], the pairwise intersections of Voronoi regions have null ambient measure, and since \u03c1 is absolutely continuous, the pushforward measure can be written in this case as \u03c0\nX4 \u03c1 = \u22114 j=1 \u03c1(Vj)\u03b4xj , where Vj is the\nVoronoi region of xj . Note that this decomposition is not always possible if, for instance \u03c1 has an atom falling on two Voronoi regions: both regions would count the atom as theirs, and double-counting would imply \u2211 j \u03c1(Vj) > 1. The technicalities required to correctly define \u03c1(Vj) are such that, in general, it is simpler to write \u03c0S\u03c1, even though (if S is discrete) this measure can clearly be written as a sum of deltas with appropriate masses.\nBy lemma 3.1, the distance Wp(\u03c1, \u03c0X4\u03c1) p is the (expected) quantization cost of \u03c1 when using X4 as codebook. Clearly, this cost can never be lower than the optimal quantization cost of size 4. This reasoning leads to the following lower bound between empirical and population measures.\nTheorem 3.3. For \u03c1 \u2208 Pp(M) with absolutely continuous part \u03c1A 6= 0, and 1 \u2264 p < \u221e, it holds Wp(\u03c1, \u03c1\u0302n) = \u2126(n\u22121/d) uniformly over \u03c1\u0302n, where the constants depend on d and \u03c1A only.\nProof: Let Vn,p(\u03c1) := infS\u2282M,|S|=n Ex\u223c\u03c1d(x, S)p be the optimal quantization cost of \u03c1 of order p with n centers. Since \u03c1A 6= 0, and since \u03c1 has a finite (p+ \u03b4)-th order moment, for some \u03b4 > 0 (since it is supported on the unit ball), then it is Vn,p(\u03c1) = \u0398(n\n\u2212p/d), with constants depending on d and \u03c1A (see [14, p. 78] and [16].) Since supp(\u03c1\u0302n) = Xn, it follows that\nWp(\u03c1, \u03c1\u0302n) p \u2265\nlemma 3.2 Wp(\u03c1, \u03c0Xn\u03c1) p = lemma 3.1 Ex\u223c\u03c1d(x,Xn)p \u2265 Vn,p(\u03c1) = \u0398(n\u2212p/d)\nNote that the bound of theorem 3.3 holds for \u03c1\u0302n derived from any sample Xn, and is therefore stronger than the existing lower bounds on the convergence rates of EWp(\u03c1, \u03c1\u0302n) \u2192 0. In particular, it trivially induces the known lower bound \u2126(n\u22121/d) on the expected rate of convergence.\nThe consequence of theorem 3.3 is clearly that the rate of convergence of the empirical law of large numbers is limited (in all cases), by the dimension of\nthe space in which \u03c1 is absolutely continuous. This justifies the choice of formal setting to be a d-manifold (or even Rd): by the above uniform lower bound, one is effectively forced to make a finite-dimension assumption on the space where \u03c1 is absolutely continuous."}, {"heading": "4 Unsupervised learning algorithms for learning", "text": "a probability measure\nAs described in [21], several of the most widely used unsupervised learning algorithms can be interpreted to take as input a sample Xn and output a set S\u0302k, where k is typically a free parameter of the algorithm, such as the number of means in k-means1, the dimension of affine spaces in PCA, etc. Performance is measured by the empirical quantity n\u22121 \u2211n i=1 d(xi, S\u0302k)\n2, which is minimized among all sets in some class (e.g. sets of size k, affine spaces of dimension k,. . . ) This formulation is general enough to encompass k-means and PCA, but also k-flats, non-negative matrix factorization, and sparse coding (see [21] and references therein.)\nUsing the discussion of Sec. 3, we can establish a clear connection between unsupervised learning and the problem of learning probability measures with respect to W2. Consider as a running example the k-means problem, though the argument is general. Given an input Xn, the k-means problem is to find a set |S\u0302k| = k minimizing its average distance from points in Xn. By associating to S\u0302k the pushforward measure \u03c0S\u0302k \u03c1\u0302n, we find that\n1\nn n\u2211 i=1 d(xi, S\u0302k) 2 = Ex\u223c\u03c1\u0302nd(x, S\u0302k)2 = lemma 3.1 W2(\u03c1\u0302n, \u03c0S\u0302k \u03c1\u0302n) 2. (2)\nSince k-means minimizes equation 2, it also finds the measure that is closest to \u03c1\u0302n, among those with support of size k. This connection between k-means and W2 measure approximation was, to the best of the authors\u2019 knowledge, first suggested by Pollard [28] though, as mentioned earlier, the argument carries over to many other unsupervised learning algorithms.\nWe briefly clarify the steps involved in using an existing unsupervised learning algorithm for probability measure learning. Let Uk be a parametrized algorithm (e.g. k-means) that takes a sample Xn and outputs a set Uk(Xn). The measure learning algorithm Ak :Mn \u2192 Pp(M) corresponding to Uk is defined as follows:\n1. Ak takes a sample Xn and outputs the measure \u03c0S\u0302k \u03c1\u0302n, supported on S\u0302k = Uk(Xn);\n2. since \u03c1\u0302n is discrete, then so must \u03c0S\u0302k \u03c1\u0302n be, and thusAk(Xn) = 1 n \u2211n i=1 \u03b4\u03c0S\u0302k (xi) ;\n1 In a slight abuse of notation, we refer to the k-means algorithm here as an ideal algorithm that solves the k-means problem, even though in practice an approximation algorithm may be used.\n3. in practice, we can simply store an n-vector [ \u03c0S\u0302k(x1), . . . , \u03c0S\u0302k(xn) ] , from\nwhich Ak(Xn) can be reconstructed by placing atoms of mass 1/n at each point.\nIn the case that Uk is the k-means algorithm, only k points and k masses need to be stored.\nNote that any algorithm A\u2032 that attempts to output a measure A\u2032(Xn) close to \u03c1\u0302n can be cast in the above framework. Indeed, if S\n\u2032 is the support of A\u2032(Xn) then, by lemma 3.2, \u03c0S\u2032 \u03c1\u0302n is the measure closest to \u03c1\u0302n with support in S\n\u2032. This effectively reduces the problem of learning a measure to that of finding a set, and is akin to how the fact that every optimal quantizer is a nearest-neighbor quantizer (see [15], [12, p. 350], and [14, p. 37\u201338]) reduces the problem of finding an optimal quantizer to that of finding an optimal quantizing set.\nClearly, the minimum of equation 2 over sets of size k (the output of kmeans) is monotonically non-increasing with k. In particular, since S\u0302n = Xn and \u03c0S\u0302n \u03c1\u0302n = \u03c1\u0302n, it is Ex\u223c\u03c1\u0302nd(x, S\u0302n) 2 = W2(\u03c1\u0302n, \u03c0S\u0302n \u03c1\u0302n) 2 = 0. That is, we can always make the learned measure arbitrarily close to \u03c1\u0302n by increasing k. However, as pointed out in Sec. 2, the problem of measure learning is concerned with minimizing the distance W2(\u03c1, \u00b7) to the data-generating measure. The actual performance of k-means is thus not necessarily guaranteed to behave in the same way as the empirical one, and the question of characterizing its behavior as a function of k and n naturally arises.\nFinally, we note that, while it is Ex\u223c\u03c1\u0302nd(x, S\u0302k)2 = W2(\u03c1\u0302n, \u03c0S\u0302k \u03c1\u0302n) 2 (the empirical performances are the same in the optimal quantization, and measure learning problem formulations), the actual performances satisfy\nEx\u223c\u03c1d(x, S\u0302k)2 = lemma 3.1 W2(\u03c1, \u03c0S\u0302k\u03c1) 2 \u2264 lemma 3.2 W2(\u03c1, \u03c0S\u0302k \u03c1\u0302n) 2, 1 \u2264 k \u2264 n.\nConsequently, with the identification between sets S and measures \u03c0 S \u03c1\u0302n, the set-approximation problem is, in general, different from the measure learning problem (for example, if M = Rd and \u03c1 is absolutely continuous over a set of non-null volume, it\u2019s not hard to show that the inequality is almost surely strict: Ex\u223c\u03c1d(x, S\u0302k)2 < W2(\u03c1, \u03c0S\u0302k \u03c1\u0302n)\n2 for n > k > 1.) In the remainder, we characterize the performance of k-means on the measure learning problem, for varying k, n. Although other unsupervised learning algorithms could have been chosen as basis for our analysis, k-means is one of the oldest and most widely used, and the one for which the deep connection between optimal quantization and measure approximation is most clearly manifested. Note that, by setting k = n, our analysis includes the problem of characterizing the behavior of the distance W2(\u03c1, \u03c1\u0302n) between empirical and population measures which, as indicated in Sec. 2.1, is a fundamental question in statistics (i.e. the speed of convergence of the empirical law of large numbers.)"}, {"heading": "5 Learning rates", "text": "In order to analyze the performance of k-means as a measure learning algorithm, and the convergence of empirical to population measures, we propose the decomposition shown in fig. 2. The diagram includes all the measures considered in the paper, and shows the two decompositions used to prove upper bounds. The upper arrow (green), illustrates the decomposition used to bound the distance W2(\u03c1, \u03c1\u0302n), This decomposition uses the measures \u03c0Sk\u03c1 and \u03c0Sk \u03c1\u0302n as intermediates to arrive at \u03c1\u0302n, where Sk is a k-point optimal quantizer of \u03c1, that is, a set Sk minimizing Ex\u223c\u03c1d(x, S)2 and such that |Sk| = k. The lower arrow (blue) corresponds to the decomposition of W2(\u03c1, \u03c0S\u0302k \u03c1\u0302n) (the performance of k-means), whereas the labelled black arrows correspond to individual terms in the bounds. We begin with the (slightly) simpler of the two results."}, {"heading": "5.1 Convergence rates for the empirical law of large numbers", "text": "Let Sk be the optimal k-point quantizer of \u03c1 of order two [14, p. 31]. By the triangle inequality and the identity (a+ b+ c)2 \u2264 3(a2 + b2 + c2), it follows that\nW2(\u03c1, \u03c1\u0302n) 2 \u2264 3 [ W2(\u03c1, \u03c0Sk\u03c1) 2 +W2(\u03c0Sk\u03c1, \u03c0Sk \u03c1\u0302n) 2 +W2(\u03c0Sk \u03c1\u0302n, \u03c1\u0302n) 2 ] . (3)\nThis is the decomposition depicted in the upper arrow of fig. 2. By lemma 3.1, the first term in the sum of equation 3 is the optimal k-point quantization error of \u03c1 over a d-manifold M which, using recent techniques\nfrom [16] (see also [17, p. 491]), is shown in the proof of theorem 5.1 (part a) to be of order \u0398(k\u22122/d). The remaining terms, b) and c), are slightly more technical and are bounded in the proof of theorem 5.1.\nSince equation 3 holds for all 1 \u2264 k \u2264 n, the best bound on W2(\u03c1, \u03c1\u0302n) can be obtained by optimizing the right-hand side over all possible values of k, resulting in the following probabilistic bound for the rate of convergence of the empirical law of large numbers.\nTheorem 5.1. Given \u03c1 \u2208 Pp(M) with absolutely continuous part \u03c1A 6= 0, sufficiently large n, and 0 < \u03b4 < 1, it holds\nW2(\u03c1, \u03c1\u0302n) \u2264 C \u00b7m(\u03c1A) \u00b7 n\u22121/(2d+4) \u00b7 \u03c4, with probability 1\u2212 e\u2212\u03c4 2 .\nwhere m(\u03c1A) := \u222b M \u03c1A(x) d/(d+2)d\u03bbM(x), and C depends only on d.\nProof. See Appendix."}, {"heading": "5.2 Learning rates of k-means", "text": "The key element in the proof of theorem 5.1 is that the distance between population and empirical measures can be bounded by choosing an intermediate optimal quantizing measure of an appropriate size k. In the analysis, the best bounds are obtained for k smaller than n. If the output of k-means is close to an optimal quantizer (for instance if sufficient data is available), then we would similarly expect that the best bounds for k-means correspond to a choice of k < n.\nThe decomposition of the bottom (blue) arrow in figure 2 leads to the following bound in probability.\nTheorem 5.2. Given \u03c1 \u2208 Pp(M) with absolutely continuous part \u03c1A 6= 0, and 0 < \u03b4 < 1, then for all sufficiently large n, and letting\nk = C \u00b7m(\u03c1A) \u00b7 nd/(2d+4),\nit holds\nW2(\u03c1, \u03c0S\u0302k \u03c1\u0302n) \u2264 C \u00b7m(\u03c1A) \u00b7 n \u22121/(2d+4) \u00b7 \u03c4, with probability 1\u2212 e\u2212\u03c4\n2\n.\nwhere m(\u03c1A) := \u222b M \u03c1A(x) d/(d+2)d\u03bbM(x), and C depends only on d.\nProof. See Appendix.\nNote that the upper bounds in theorem 5.1 and 5.2 are exactly the same. Although this may appear surprising, it stems from the following fact. Since S = S\u0302k is a minimizer of W2(\u03c0S \u03c1\u0302n, \u03c1\u0302n) 2, the bound d) of figure 2 satisfies:\nW2(\u03c0S\u0302k \u03c1\u0302n, \u03c1\u0302n) 2 \u2264W2(\u03c0Sk \u03c1\u0302n, \u03c1\u0302n)2\nand therefore (by the definition of c), the term d) is of the same order as c). Since f) is also of the same order as c) (see the proof of theorem 5.2), this\nmeans that, up to a small constant factor, adding the term d) to the bound of W2(\u03c1, \u03c0S\u0302k \u03c1\u0302n)\n2 does not affect the bound. Since d) is the term that takes the output measure of k-means to the empirical measure, this implies that the rate of convergence of k-means cannot be worse than that of \u03c1\u0302n \u2192 \u03c1. Conversely, bounds for \u03c1\u0302n \u2192 \u03c1 are obtained from best rates of convergence of optimal quantizers, whose convergence to \u03c1 cannot be slower than that of k-means (since the quantizers that k-means produces are suboptimal.)\nSince the bounds obtained for the convergence of \u03c1\u0302n \u2192 \u03c1 are the same as those for k-means with k of order k = \u0398(nd/(2d+4)), this implies that estimates of \u03c1 that are as accurate as those derived from an n point-mass measure \u03c1\u0302n can be derived from k point-mass measures with k n.\nFinally, we note that the introduced bounds are currently limited by the statistical bound\nsup |S|=k |W2(\u03c0S \u03c1\u0302n, \u03c1\u0302n)2\u2212W2(\u03c0S\u03c1, \u03c1)2| = lemma 3.1 sup |S|=k |Ex\u223c\u03c1\u0302nd(x, S)2\u2212Ex\u223c\u03c1d(x, S)2|\n(see for instance [21]), for which non-matching lower bounds are known. This means that, if better upper bounds can be obtained, then both bounds in theorems 5.1 and 5.2 would automatically improve."}], "references": [{"title": "On optimal matchings", "author": ["M. Ajtai", "J. Komls", "G. Tusndy"], "venue": "Combinatorica, 4:259\u2013264", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "Combinatorial optimization over two random point sets", "author": ["Franck Barthe", "Charles Bordenave"], "venue": "Technical Report", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The Gaussian isoperimetric inequality and transportation", "author": ["Gordon Blower"], "venue": "Positivity, 7:203\u2013224,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Exponential integrability and transportation cost related to logarithmic Sobolev inequalities", "author": ["S.G. Bobkov", "F. G\u00f6tze"], "venue": "Journal of Functional Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Simple bounds for the convergence of empirical and occupation measures in 1-Wasserstein", "author": ["Emmanuel Boissard"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Quantitative concentration inequalities for empirical measures on non-compact spaces", "author": ["F. Bolley", "A. Guillin", "C. Villani"], "venue": "Probability Theory and Related Fields, 137(3):541\u2013593", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Weighted Csisz\u00e1r-Kullback-Pinsker inequalities and applications to transportation inequalities", "author": ["F. Bolley", "C. Villani"], "venue": "Annales de la Faculte des Sciences de Toulouse, 14(3):331\u2013352", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deconvolution for the Wasserstein metric and geometric inference", "author": ["Claire Caillerie", "Fr\u00e9d\u00e9ric Chazal", "J\u00e9r\u00f4me Dedecker", "Bertrand Michel"], "venue": "Rapport de recherche RR-7678,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Building triangulations using -nets", "author": ["Kenneth L. Clarkson"], "venue": "In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Asymptotics for transportation cost in high dimensions", "author": ["V. Dobri", "J. Yukich"], "venue": "Journal of Theoretical Probability, 8:97\u2013118", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Vector Quantization and Signal Compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer International Series in Engineering and Computer Science. Kluwer Academic Publishers", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "On choosing and bounding probability metrics", "author": ["Alison L. Gibbs", "Francis E. Su"], "venue": "International Statistical Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Foundations of quantization for probability distributions", "author": ["Siegfried Graf", "Harald Luschgy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Distortion mismatch in the quantization of probability measures", "author": ["Siegfried Graf", "Harald Luschgy", "Gilles Pages"], "venue": "Esaim: Probability and Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Optimum quantization and its applications", "author": ["Peter M. Gruber"], "venue": "Adv. Math,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Convex and discrete geometry", "author": ["P.M. Gruber"], "venue": "Grundlehren der mathematischen Wissenschaften. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel density estimation on riemannian manifolds: Asymptotic results", "author": ["Guillermo Henry", "Daniela Rodriguez"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Mean rates of convergence of empirical measures in the Wasserstein metric", "author": ["Joseph Horowitz", "Rajeeva L. Karandikar"], "venue": "J. Comput. Appl. Math.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "K\u2013dimensional coding schemes in Hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Ricci curvature of markov chains on metric spaces", "author": ["Yann Ollivier"], "venue": "J. Funct. Anal.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Submanifold density estimation", "author": ["Arkadas Ozakin", "Alexander Gray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "The probabilistic analysis of matching heuristics", "author": ["C. Papadimitriou"], "venue": "Proc. of the 15th Allerton Conf. on Communication, Control and Computing, pages 368\u2013378", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1978}, {"title": "Kernel density estimation on", "author": ["Bruno Pelletier"], "venue": "Riemannian manifolds. Statist. Probab. Lett.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements", "author": ["Xavier Pennec"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Information and information stability of random variables and processes", "author": ["M.S. Pinsker"], "venue": "San Francisco: Holden-Day", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1964}, {"title": "Quantization and the method of k-means", "author": ["David Pollard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1982}, {"title": "Probability metrics and the stability of stochastic models", "author": ["S.T. Rachev"], "venue": "Wiley series in probability and mathematical statistics: Applied probability and statistics. Wiley", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability Theory and Combinatorial Optimization", "author": ["J.M. Steele"], "venue": "Cbms-Nsf Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Transportation cost for Gaussian and other product measures", "author": ["M. Talagrand"], "venue": "Geometric And Functional Analysis, 6:587\u2013600", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "Introduction to nonparametric estimation", "author": ["Alexandre B. Tsybakov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "On the convergence of sample probability distributions", "author": ["V.S. Varadarajan"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1958}, {"title": "Optimal Transport: Old and New", "author": ["C. Villani"], "venue": "Grundlehren der Mathematischen Wissenschaften. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Manifold Parzen Windows", "author": ["P. Vincent", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "The problem of learning a probability distribution is classic in statistics and machine learning, and is typically analyzed for distributions in X = R that have a density with respect to the Lebesgue measure, with total variation, and L2 among the common distances used to measure closeness of two densities (see for instance [10, 32] and references therein.", "startOffset": 326, "endOffset": 334}, {"referenceID": 31, "context": "The problem of learning a probability distribution is classic in statistics and machine learning, and is typically analyzed for distributions in X = R that have a density with respect to the Lebesgue measure, with total variation, and L2 among the common distances used to measure closeness of two densities (see for instance [10, 32] and references therein.", "startOffset": 326, "endOffset": 334}, {"referenceID": 34, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 22, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 17, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 25, "context": "A discussion on several topics related to statistics on a Riemannian manifold can be found in [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 155, "endOffset": 163}, {"referenceID": 15, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 155, "endOffset": 163}, {"referenceID": 33, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 183, "endOffset": 187}, {"referenceID": 33, "context": "The space Pp(M) with the Wp metric is itself a complete separable metric space [34].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "There are many possible choices of distances between probability measures [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "Among them, Wp metrizes weak convergence (see [34] theorem 6.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Wasserstein distances have been used to study the mixing and convergence of Markov chains [22], as well as concentration of measure phenomena [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Wasserstein distances have been used to study the mixing and convergence of Markov chains [22], as well as concentration of measure phenomena [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "By the empirical law of large numbers, the empirical measure converges almost surely to the population measure: \u03c1\u0302n \u2192 \u03c1 in the sense of the weak topology [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "The mean rate of convergence of \u03c1\u0302n \u2192 \u03c1 has been widely studied in the past, resulting in upper bounds of order EW2(\u03c1, \u03c1\u0302n) = O(n\u22121/(d+2)) [19, 8], and lower", "startOffset": 139, "endOffset": 146}, {"referenceID": 7, "context": "The mean rate of convergence of \u03c1\u0302n \u2192 \u03c1 has been widely studied in the past, resulting in upper bounds of order EW2(\u03c1, \u03c1\u0302n) = O(n\u22121/(d+2)) [19, 8], and lower", "startOffset": 139, "endOffset": 146}, {"referenceID": 28, "context": "bounds of order EW2(\u03c1, \u03c1\u0302n) = \u03a9(n\u22121/d) [29] (both assuming that the absolutely continuous part of \u03c1 is \u03c1A 6= 0, with possibly better rates otherwise).", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "More recently, an upper bound of order EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d) has been proposed [2] by proving a bound for the Optimal Bipartite Matching (OBM) problem [1], and relating this problem to the expected distance EWp(\u03c1, \u03c1\u0302n).", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "More recently, an upper bound of order EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d) has been proposed [2] by proving a bound for the Optimal Bipartite Matching (OBM) problem [1], and relating this problem to the expected distance EWp(\u03c1, \u03c1\u0302n).", "startOffset": 152, "endOffset": 155}, {"referenceID": 23, "context": "In particular, given two independent samples Xn, Yn, the OBM problem is that of finding a permutation \u03c3 that minimizes the matching cost n\u22121 \u2211 \u2016xi \u2212 y\u03c3(i)\u2016 [24, 30].", "startOffset": 156, "endOffset": 164}, {"referenceID": 29, "context": "In particular, given two independent samples Xn, Yn, the OBM problem is that of finding a permutation \u03c3 that minimizes the matching cost n\u22121 \u2211 \u2016xi \u2212 y\u03c3(i)\u2016 [24, 30].", "startOffset": 156, "endOffset": 164}, {"referenceID": 1, "context": "By Jensen\u2019s inequality, the triangle inequality, and (a+ b) \u2264 2p\u22121(ap + b), it holds EWp(\u03c1, \u03c1\u0302n) \u2264 EWp(\u03c1\u0302Xn , \u03c1\u0302Yn ) p \u2264 2EWp(\u03c1, \u03c1\u0302n), and therefore a bound of order O(n\u2212p/d) for the OBM problem [2] implies a bound EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d).", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "The matching lower bound is only known for a special case: \u03c1A constant over a bounded set of non-null measure [2] (e.", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": ") Similar results, with matching lower bounds are found for W1 in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "One fruitful avenue of analysis has been the use of so-called transportation, or Talagrand inequalities Tp, which can be used to prove concentration inequalities on Wp [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In particular, we say that \u03c1 satisfies a Tp(C) inequality with C > 0 iff Wp(\u03c1, \u03bc) 2 \u2264 CH(\u03bc|\u03c1),\u2200\u03bc \u2208 Pp(M), where H(\u00b7|\u00b7) is the relative entropy [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "As shown in [6, 5], it is possible to obtain probabilistic upper bounds on Wp(\u03c1, \u03c1\u0302n), with p = 1, 2, if \u03c1 is known to satisfy a Tp inequality of the same order, thereby reducing the problem of bounding Wp(\u03c1, \u03c1\u0302n) to that of obtaining a Tp inequality.", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "As shown in [6, 5], it is possible to obtain probabilistic upper bounds on Wp(\u03c1, \u03c1\u0302n), with p = 1, 2, if \u03c1 is known to satisfy a Tp inequality of the same order, thereby reducing the problem of bounding Wp(\u03c1, \u03c1\u0302n) to that of obtaining a Tp inequality.", "startOffset": 12, "endOffset": 18}, {"referenceID": 19, "context": "Note that, by Jensen\u2019s inequality, and as expected from the behavior ofWp, the inequality T2 is stronger than T1 [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "While it has been shown that \u03c1 satisfies a T1 inequality iff it has a finite square-exponential moment [4, 7], no such general conditions have been found for T2.", "startOffset": 103, "endOffset": 109}, {"referenceID": 6, "context": "While it has been shown that \u03c1 satisfies a T1 inequality iff it has a finite square-exponential moment [4, 7], no such general conditions have been found for T2.", "startOffset": 103, "endOffset": 109}, {"referenceID": 33, "context": "15 of [34], and the celebrated Csisz\u00e1r-Kullback-Pinsker inequality [27], for all \u03c1, \u03bc \u2208 Pp(M), it is", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "15 of [34], and the celebrated Csisz\u00e1r-Kullback-Pinsker inequality [27], for all \u03c1, \u03bc \u2208 Pp(M), it is", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "The T2 inequality has been shown by Talagrand to be satisfied by the Gaussian distribution [31], and then slightly more generally by strictly log-concave measures [3].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "The T2 inequality has been shown by Talagrand to be satisfied by the Gaussian distribution [31], and then slightly more generally by strictly log-concave measures [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "However, as noted in [6], \u201ccontrary to the T1 case, there is no hope to obtain T2 inequalities from just integrability or decay estimates.", "startOffset": 21, "endOffset": 24}, {"referenceID": 14, "context": "Given a closed set S \u2286 M, let \u03c0 S = \u2211 q\u2208S 1Vq(S) \u00b7 q be a nearest neighbor projection onto S (a function mapping points in X to their closest point in S), where {Vq(S) : q \u2208 S} is a Borel Voronoi partition of X such that Vq(S) \u2286 {x \u2208 X : \u2016x \u2212 q\u2016 = minr\u2208S \u2016x \u2212 r\u2016} (see for instance [15].", "startOffset": 282, "endOffset": 286}, {"referenceID": 27, "context": "This close connection between optimal quantization and Wasserstein distance has been pointed out in the past in the statistics [28], optimal quantization [14, p.", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "33], and approximation theory literatures [16].", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "2 of [9], the pairwise intersections of Voronoi regions have null ambient measure, and since \u03c1 is absolutely continuous, the pushforward measure can be written in this case as \u03c0 X4 \u03c1 = \u22114 j=1 \u03c1(Vj)\u03b4xj , where Vj is the Voronoi region of xj .", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "78] and [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "As described in [21], several of the most widely used unsupervised learning algorithms can be interpreted to take as input a sample Xn and output a set \u015ck, where k is typically a free parameter of the algorithm, such as the number of means in k-means, the dimension of affine spaces in PCA, etc.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": ") This formulation is general enough to encompass k-means and PCA, but also k-flats, non-negative matrix factorization, and sparse coding (see [21] and references therein.", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "This connection between k-means and W2 measure approximation was, to the best of the authors\u2019 knowledge, first suggested by Pollard [28] though, as mentioned earlier, the argument carries over to many other unsupervised learning algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "This effectively reduces the problem of learning a measure to that of finding a set, and is akin to how the fact that every optimal quantizer is a nearest-neighbor quantizer (see [15], [12, p.", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "from [16] (see also [17, p.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "(see for instance [21]), for which non-matching lower bounds are known.", "startOffset": 18, "endOffset": 22}], "year": 2012, "abstractText": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.", "creator": "LaTeX with hyperref package"}}}