{"id": "1211.4929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2012", "title": "Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models", "abstract": "Our method consists of two main steps: First, we identify five common syntactic patterns of variable length and use them to extract candidate segments; then we use the results of a common generative sentiment theme model to filter out the non-informative segments; and we verify the proposed method with quantitative and qualitative experiments; and in a quantitative study, our approach exceeds previous methods for creating informative segments and summaries that capture aspects of products and services as expressed in user-generated pro- and contra-lists. Our user study of ninety users resonates with this result: Individual segments that are extracted and filtered by our method are rated as more useful by users compared to previous approaches.", "histories": [["v1", "Wed, 21 Nov 2012 03:59:06 GMT  (266kb,D)", "http://arxiv.org/abs/1211.4929v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["trung v nguyen", "alice h oh"], "accepted": false, "id": "1211.4929"}, "pdf": {"name": "1211.4929.pdf", "metadata": {"source": "CRF", "title": "Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models", "authors": ["Trung Nguyen", "Alice Oh"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Online reviews of products and services are an important source of knowledge for people to make their purchasing decisions. They contain a wealth of information on various product/service aspects from diverse perspectives of consumers. However, it is a challenge for stakeholders to retrieve useful information from the enormous pool of reviews. Many automatic systems were built to address this challenge including generating aspect-based sentiment summarization of reviews [1,8,19] and comparing and ranking products with regard to their aspects [13]. In this study we focus on the problem of review summarization, which takes as input a set of user reviews for a specific product or service entity and produces a set of representative text excerpts from the reviews.\nMost work on summarization so far used sentence as the unit of summary. However, we do not need a complete sentence to understand its main communicative point. Consider the following sentence from review of a coffee maker: \u2018My mum bought me this one, and I have to say it makes really awful tasing coffee\u2019. To a buyer looking for an opinion about the coffee maker, only the part \u2018makes really awfultasing coffee\u2019 is helpful. Being able to extract such short and meaningful segments from lengthy sentences can bring significant utilities to users. It reduces their reading load as well as presents more readable summaries on devices with limited screen size such as smart phones.\nThis motivates our main research question of how to extract concise and informative text from reviews of products and services that can be used for\nar X\niv :1\n21 1.\n49 29\nv1 [\ncs .I\nR ]\n2 1\nN ov\n2 01\nsummarization. Previous work has ignored the differences in product and service reviews, which is questionable. To the best of our knowledge, this is the first work that studies and compares summarization for the two domains in details. We propose to to extract text segments that match against pre-defined syntactic patterns that occur frequently in reviews of both products and services. However, the extracted segments should be subjected to some selection or filtering procedure as not all matching candidates are likely to contain rich information. Our proposed selection mechanism is based on the observation that segments containing users\u2019 opinions and evaluations about product and service aspects carry valuable information. This motivates the use of output of joint sentiment topic models to discriminate between desirable and non-desirable text segments. Since joint sentiment topic models capture sentiments that are highly associative with aspects, they are well suited for selecting informative segments from the pool of extracted candidates.\nThe major contributions of our work are as follows.\n1. A new joint sentiment-topic model that automatically learns polarities of sentiment lexicons from reviews. 2. Identification of five frequently occuring syntactic patterns for extracting concise segments from reviews of both products and services. 3. Demonstration of the effective application of topic models to select informative variable-length segments for review summarization. 4. Production of summaries that recall important information from review entities\u2019 characteristics.\nThe rest of the paper is structured as follows. We begin with the related literature in review summarization and joint sentiment topic models in Sect. 2. Next we describe our extension to a topic model and its improvements over previous models in Sect. 3. We then introduce our proposed extraction patterns and procedures for segment selection in Sect. 4. We present our experiments and evaluation in Sect. 5 and 6 and conclude in Sect. 7."}, {"heading": "2 Related work", "text": "We first look at how text excerpts are extracted from reviews in the existing literature. Previous studies mainly generated aspect-based summary for products and services by aggregating subjective text excerpts related to each aspect. Different forms of the excerpts include sentence [8], concise phrase composing of a modifier and a header term [16], adjective-noun pair extracted based on POS tagging and the term-frequency of the pair [23], and phrase generated by rules [15]. Some limitations of these previous work are i) they only worked with the simplistic adjective-noun pairs or specific form of reviews such as short comments, and ii) experiments were carried out with reviews of services only. Our approach to extract text segments by matching variable-length linguistic patterns overcome these shortcomings and can generalize well for free-text reviews of both products and services.\nVarious methods for selecting informative text fragments were applied in previous research, such as matching against pre-defined or frequently occurring aspects [1,8], ranking frequency [23], and topic models [17,20,22]. We are interested in the application of joint sentiment topic models as they can infer sentiment words that are closely associative with an aspect. This is an important property of polarity of sentiment words as pointed out in [5,11,13,18], and recently several joint topic models have been proposed to unify the treatment of sentiment and topic (aspect) [9,11,17,21]. Applications of these models have been limited to sentiment classification for reviews, but we hypothesize that they can also be helpful in summarization. We focus our next discussion on previous joint models in comparison to our proposed model.\nOne of the earliest work is the Topic-Sentiment Model (TSM) [17], which generates a word either from a topic or one of the two additional subtopics \u2013 sentiments, but it fails to account for the intimate interplay between a topic/aspect and a sentiment. TSM is based on pLSI whereas more recent work ([9,11,20]) uses or extends Latent Dirichlet Allocation (LDA) [2]. In the Multi-Aspect Sentiment (MAS) model [20], customer ratings are incorporated as signals to guide the formation of pre-defined aspects, which can then be used to extract sentences from reviews that are related to each aspect. In the Joint Sentiment/Topic (JST) model [11], and the Aspect and Sentiment Unification Model (ASUM) [9], each word is assumed to be generated from a distribution jointly defined by a topic and a sentiment (either positive or negative). As a result, JST and ASUM learn words that are commonly associated with an aspect although the models are incapable of distinguishing between sentiment and non-sentiment lexicons. We propose a new model that leverages syntactic information to identify sentiment lexicons and automatically learn their polarities from the co-occurrences of words in a sentence. This allows the model to bootstrap using a minimum set of sentiment seed words, thereby alleviating the need for information that is expensive to obtain such as ratings of users for reviews [20] or large lists of sentiment lexicons [11]."}, {"heading": "3 A Topic Model for Learning Polarity of Sentiment Lexicons", "text": "Our key modelling assumption for reviews is that a sentence expresses an opinion toward an aspect via its sentiment component. For example, in the sentence \u2018The service was excellent\u2019, only the word \u2018excellent\u2019 carries the positive sentiment. This is not a new assumption as adjectives and adverbs are commonly considered the main source of sentiment in a sentence in existing literature. Our model leverages on this type of knowledge to locate sentiment words in a sentence with relatively high confidence."}, {"heading": "3.1 Generative Process", "text": "The formal generative process of our model for the graphical representation in Fig. 1 is as follows (see Table 1 for the list of notations).\n\u2013 For every aspect k, draw a distribution of non-sentiment words, \u03c6k \u223c Dir(\u03b2), and two distributions of sentiment words, \u03c6\u2032jk \u223c Dir(\u03b2\u2032jk), where j = 0 denotes positive polarity and j = 1 denotes negative polarity. \u2013 For each review d,\nDraw a sentiment distribution \u03c0d \u223c Dir(\u03b3) Draw a topic distribution \u03b8d \u223c Dir(\u03b1) For each sentence c in document d,\n\u2013 Choose a topic z = k \u223c Mult(\u03b8d) and a sentiment s = j \u223c Mult(\u03c0d) \u2013 Choose words w \u223c Mult(\u03c6k) to discuss aspect k and sentiment words w\u2032 \u223c Mult(\u03c6\u2032jk) to convey the sentiment j toward k.\nNotice in the graphical model that the part of a sentence which emanates the sentiment is observed. In our implementation, we treat all adjectives and adverbs as w\u2032 and remaining words as w in the generative procedure, but this is not a restriction imposed on the model. It is easy to incorporate prior knowledge about words that convey sentiment into the model. For example, we can instruct the model that words such as love, hate, enjoy, worth, disappoint are sentiment words, even though they are not adjective nor adverb.\nOur main extension deals with the word smoother \u03b2\u2032 for sentiment words. Each sentiment word i is associated with a topic dependent smoothing coefficient yki for topic k and a sentiment dependent smoothing coefficient yji for sentiment j. We then impose that\n\u03b2\u2032jki = exp(yki + yji), yki \u223c N(0, \u03c321), yji \u223c N(0, \u03c322). (1)\nThis modeling allows us to incorporate polarity of sentiment words as side information. The polarity of sentiment lexicon i in a corpus is represented by the values of yji; this is to assume that the polarity of i is its intrinsic property as the corpus is about a specific domain [3]. The topic dependent smoother yki is introduced to accommodate the different frequency of association between the sentiment word i and different aspects."}, {"heading": "3.2 Inference", "text": "In order to perform inference we alternate between two procedures: sampling and maximum a posteriori. The sampler assigns values for the latent variables: the topics and sentiments of sentences. Using a collapsed Gibbs sampler [7], new values for the topic and sentiment of a sentence c in document d are drawn from the conditional probability\np(zdc = k, sdc = j|rest) \u221d \u220f i\u2208A(dc)(n TW \\ki + \u03b2)\u220f|A(dc)|\u22121\nx=0\n\u2211V i=1 n\nTW \\ki + V \u03b2 + x\u220f\ni\u2208S(dc)(n STW \\jki + \u03b2 \u2032 jki)\u220f|S(dc)|\u22121\nx=0\n\u2211V \u2032 i=1(n STW \\jki + \u03b2 \u2032 jki) + x (nDT\\dk + \u03b1)(n DS \\dj + \u03b3) (2)\nwhere S(dc) is the set of sentiment words in c and A(dc) is the set of remaining words. The \u2019\\\u2019 notation means not counting the sentence being sampled.\nWe estimate the value for \u03b2\u2032 and y from a maximum a posteriori procedure, optimizing \u03b2\u2032 over y and the assigned values of the latent variables. The negative log prior is\n\u2212 log p(\u03b2\u2032) = S \u2211 k,i yki + T \u2211 j,i yji + \u2211 j,k,i (yki + yji) 2 2\u03c32 (3)\nwhere \u03c32 = \u03c321 + \u03c3 2 2 . The collapsed negative log likelihood (dependent on sentiment words only) is\nL\u03b2\u2032 = \u2211 j,k [ log\u0393 (n\u0304jk + \u03b2\u0304 \u2032 jk)\u2212 log\u0393 (\u03b2\u0304\u2032jki) ] + \u2211 j,k,i [ log\u0393 (\u03b2\u2032jki)\u2212 log\u0393 (nSTWjki + \u03b2\u2032jki) ] (4)\nwhere n\u0304jk = \u2211V \u2032 i=1 n STW jki , \u03b2\u0304 \u2032 jk = \u2211V \u2032 i=1 \u03b2 \u2032 jki, and \u0393 is the Gamma function. We use the L-BFGS optimizer [14] to minimize the objective function L\u03b2\u2032\u2212 log p(\u03b2\u2032) by taking its partial derivatives with respect to yki and yji.\nA sample from the Markov chain in the sampler can be used to estimate the distributions of interest. The approximate probabilities of sentiment j in document d (\u03c0\u0302dj), topic k in document d (\u03b8\u0302dk), non-sentiment word i in topic k (\u03c6\u0302ki), and sentiment word i in topic k and sentiment j (\u03c6\u0302 \u2032 jki) are\n\u03c0\u0302dj = nDSdj + \u03b3\u2211S\nj\u2032=1 n DS dj\u2032 + S\u03b3\n, \u03b8\u0302dk = nDTdk + \u03b1\u2211T\nk\u2032=1 n DT dk\u2032 + T\u03b1\n,\n\u03c6\u0302ki = nTWki + \u03b2\u2211V\ni\u2032=1 n TW ki\u2032 + V \u03b2\n, \u03c6\u0302\u2032jki = nSTWjki + \u03b2 \u2032 jki\u2211V \u2032\ni=1(n STW jki\u2032 + \u03b2 \u2032 jki\u2032)\n. (5)"}, {"heading": "3.3 Aspect and Sentiment Classification Using Output of the Model", "text": "As stated in the introduction, we attempt to use the outputs of this model to improve the selection of informative segments for summarization. We define the topic classifier of an arbitrary segment of n words G = (w1, w2, . . . , wn) as\narg max k p(k|G) = arg max k wn\u2211 i=w1 (log \u03c6\u0302ki + \u2211 j log \u03c6\u0302\u2032jki) . (6)\nTo classify the sentiment of a segment G, we use the sentiment value yji learned from the model. We define the polarity of G as\npolarity(G) := \u2211\nsentiment word i\u2208G polarity(i) = \u2211 sentiment word i\u2208G y0i \u2212 y1i . (7)\nG is classified as positive if polarity(G) >= 0 and as negative if polarity(G) < 0."}, {"heading": "4 Summarization Using Syntactic Patterns and Topic Models", "text": "In this section we present our framework for variable-length segment-based summarization of reviews. We first describe the five frequently occuring syntatic patterns in reviews that are used to extract candidate text segments. We then discuss the use of topic models in selecting meaningful segments from the set of extracted candidates. We also present an independent framework for evaluation of the summaries comprising segments regardless of the approaches."}, {"heading": "4.1 Extraction Patterns", "text": "Central to our summarization system is how to extract meaningful, informative text segments out of a sentence. We use sentence syntax to guide the extraction process by defining patterns of lexical classes for matching against text segments. The purpose is to extract semantically meaningful unit of text in a sentence that can be understood without extra context. In the particular task of summarizing\nreviews for products and services, we want to capture units that contain sentiments toward aspects. This type of segments is important because it expresses and formulates opinions about the entity being reviewed.\nBased on the above observation, we identify five most common extraction patterns to capture a variety of text segments in both product and service reviews as follows. First we use POS tagger to tag all pros and cons items available in our data sets of restaurant and coffee maker reviews (see Sect. 5.1). The pros and cons are relatively short and meaningful, and can therefore be suitable representatives of the text segments that we want to generate. The resulting sequences of tags are then ranked based on their frequency. After carefully studying the top ranked patterns we select the five most productive ones listed in Table 2.\nWe use the same notations as in regular expression, where the constituent parts correspond to lexical categories as specified by the PennTree bank. For simplicity, a single tag is used to represent different forms of a category; i.e., jj represents adjective and matches all of JJ, JJR and JJS. Also, nn matches a noun phrase rather than just a single word. We further restrict that each segment must match the longest pattern. This means, for example, a segment matching pattern 1 in a sentence is consumed and no longer available for matching pattern 5. Each pattern also has its negation form easily constructed from its positive form, hence we do not show in the table."}, {"heading": "4.2 Selecting Informative Segments using Topic Models", "text": "Candidate segments can be meaningless even if they match the defined extraction patterns. For example, \u2018final thought\u2019 and \u2018several hour\u2019 are instances of pattern 5, but they reveal no interesting information. Furthermore, the sheer number of text segments matching the patterns (Table 2) requires us to be selective in finding segments to include in summaries.\nWe observe that informative segments often contain words that convey opinions about aspects of entities. Since the aspect-sentiment intimate interplay is modeled and learned by our joint sentiment-topic model, we propose the following filters to prune less informative segments using the output the model.\nBaseline No filtering, i.e., keep all matching segments.\nAW Eliminate a segment if it does not contain one of the top X most probable words of the segment\u2019s inferred aspect. SW Eliminate a segment if it does not contain one of the top Y most probable sentiment words of the segment\u2019s inferred sentiment and aspect. RANK Rank all segments having the same inferred sentiment and aspect in order of their probabilities and eliminate the bottom half segments.\nIt is possible to use previous joint sentiment topic models, such as ASUM [9] and JST [11], for the filtering purpose. Note that ASUM and JST output word distributions for each pair of sentiment and aspect; hence, ASUM and JST are in effect both sentiment classifier and filter:\nASUM Eliminate a segment if it does not contain one of the top Z most probable words of the segment\u2019s inferred senti-aspect using the ASUM model. JST Same as ASUM except that JST is used in placed of ASUM.\nA complete procedure for summarization would need a sentiment classifier component for segments as sentiment-based summaries are preferred by users [10]. In addition to our model-based sentiment classifier, we introduce another sentiment classifier based on SentiWordNet (SWN) [4], a popular lexical resource for opinion mining, using the same approach as in [5]. For convenience, we call our model-based classifier SEN and the SWN-based classifier SWN.\nVarious procedures for retaining quality segments can then be constructed by combining different sentiment classifiers and filters. For example, we may first use SEN to classify sentiment of a segment and then use both AW and SW to discard non-qualified segments. We name such procedure SEN+AW+SW, with the convention that the output of a preceding classifier/filter is the input to the next classifier/filter whenever applicable."}, {"heading": "4.3 A Framework for Segment-based Summary Evaluation", "text": "We now introduce a framework for automatically evaluating the extraction patterns at the levels of segment and entity (a specific product or service). This framework is independent of the way segments are generated and therefore can be applied to any method that uses segment as the unit of summary.\nEach entity E has a candidate summary EC = {Y |Y matches one of the patterns } and a reference summary ER = {X|X is in the gold standard summary of E }. For Y \u2208 EC and X \u2208 ER, we measure the similarity of their content using precision and recall\nP (X,Y ) := skip2(X,Y )(|Y |\n2 ) , R(X,Y ) := skip2(X,Y )(|X| 2 ) where skip2(X,Y ) is the number of skip-bigram matches between X and Y (termed ROUGE-SU in [12]). For a candidate segment Y \u2208 EC , define R(Y ) = R(Xmax, Y ) and P (Y ) = P (Xmax, Y ) where Xmax = arg maxX\u2208ER R(X,Y ).\nFor an entity E, the average precision Pskip(E) = \u2211 Y \u2208EC P (Y )/|EC | and re-\ncall Rskip(E) = \u2211 Y \u2208EC R(Y )/|EC | tells us how similar the content of extracted\nsegments is to a reference set of segments on average. We also want to assess how many portion of the reference summary is recovered and what percentage of the candidate summary is useful. For this reason, we introduce P(E) and R(E) to measure the precision and recall for the candidate summary set EC of E:\nP (E) := \u2211 Y \u2208EC 1A{Y} |EC | , R(E) := \u2211 X\u2208ER 1B{X} |ER| (8)\nwhere 1A{Y} and 1B{X} are indicator functions; A = {Y | R(Y ) \u2265 \u03b1} and B = {X| \u2203Y \u2208 A s.t R(X,Y ) = R(Y )} where \u03b1 is a recall threshold for a candidate summary to be considered useful.\nA good measure for a reference summary of an entity E must be a combination of the segment-level recall (precision), Rskip(E) and the entity-level recall (precision), R(E). A simple combination is the average of the two, i.e, Rcb(E) = (Rskip(E) +R(E))/2 and Pcb(E) = (Pskip(E) + P (E))/2.\nSince we typically work with data that contains a large set of review entities, it is convenient to report the results using the following summarization statistics:\nPs =\n\u2211N i=1 \u2211 Y \u2208ECi P (Y )\u2211N\ni=1 |ECi | , Pe = N\u2211 i=1 P (Ei)/N , Re = N\u2211 i=1 R(Ei)/N ,\nRs =\n\u2211N i=1 \u2211 Y \u2208ECi R(Y )\u2211N\ni=1 |ECi | , P = N\u2211 i=1 Pcb(Ei)/N , R = N\u2211 i=1 Rcb(Ei)/N ."}, {"heading": "5 Experiments", "text": "We experimented using reviews of coffee makers as representative for the product domain and reviews of restaurants as representative for the service domain. We describe our data sets and experimental set-ups in 5.1. In 5.2 we give example of the topics and sentiment words learned by the model. We analyze the effectiveness of extraction patterns in 5.3 and compare the performance of different sentiment classifiers and segments filters in 5.4."}, {"heading": "5.1 Data Sets and Experimental Set-ups", "text": "Our data sets consist of restaurant reviews and coffee maker reviews. For each review, we collected its free-format text content and its pros and cons lists if available.\n\u2013 RESTAURANTS 50,000 reviews of 5,532 restaurants collected from Citysearch New York. This data is provided by Ganu, et al. [6]. \u2013 COFFEEMAKERS 23,411 reviews of 534 coffee makers collected from epinions.com.\nOur first step is to fit the joint sentiment topic model to each data set. Data is pre-processed as in other standard topic models, in which sentences are tokenized\nby the punctuations: \u2018.\u2019, \u2018!\u2019, and \u2018?\u2019. The hyperparameters are set as \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.1 for both positive and negative sentiment; the number of aspects is 7 for both corpora.\nWe incorporated prior sentiment information into the model using sentiment seed words in analogy to [9]. After running the sampler for a burnin period of 500 iterations, we interleaved it with the optimizer, optimizing over yki and yji every 100th step of sampling. We trained the model in 2000 iterations for both data sets and used the last sample in the chain in all of our experiments.\nIn the segments selection step, the maximum number of words in a sequence is set to 7 and the number of top words for AW, SW, ASUM, and JST is set to 200, 100, 300, and 300, respectively. We used a value of 0.25 for the recall threshold \u03b1 in Eq. 8. All parameters were set empirically after many experiments.\nIn order to evaluate the quality of segments and summaries using the framework in 4.3, a reference summary must be obtained for each review entity. We aggregate the pros written by all reviewers for an entity as its pros gold standard and similarly for its cons standard (duplicated entries are removed). To construct an entity\u2019s candidate summaries, the procedures in 4.2 are applied to the segments extracted from all of its reviews. The sentiment classifier in a procedure partitions the entity\u2019s segments into a positive candidate summary and a negative candidate summary. The candidates are evaluated against their counterpart reference summaries independently."}, {"heading": "5.2 Topics and Polarities of Sentiment Words Learned by the Model", "text": "Example of topics inferred by the model is given in Table 3. Each topic has three distributions where one distribution (first column) consists descriptive words about the aspect and two distributions (remaining columns) consist evaluative\nwords directing the aspect. Except the common sentiment words such as good, great, bad, wrong that are associated with most aspects due to their frequent usage, positive and negative sentiment lexicons look highly related to their corresponding aspects. For example, the model discovers that people are more likely to praise the food with delicious, best, fresh, and tasty and disapprove food that is dry, tasteless, cold or soggy. Such results can be very helpful for the exploratory purpose of understanding what aspects reviewers care and comment about.\nTable 4 demonstrates the effectiveness of our model in learning the polarities of domain-specific sentiment lexicons (the seed words used for bootstrapping are excluded). To verify this claim we compare with the SWN classifier described in 4.2 in a classification task for noun phrases. SWN leverages synsets in WordNet and so, in some sense, it captures the context-dependent sentiment of a word. We used a set of 929 positive and 236 negative noun phrases obtained from an external set of restaurant reviews in [5]. All phrases are unique and manually annotated with their true sentiments. Our classifier outperforms SWN in classification accuracy for both the positive (90.1% vs. 83.4%) and and negative (74.6% vs. 66.1%) categories. This shows that our model is quite accurate in assigning sentiment score to domain-specific lexicons compared to the more general propagation approach in SWN."}, {"heading": "5.3 Evaluation of Extraction Patterns", "text": "We now analyze how different extraction patterns behave when applied to the service domain and the product domain (Table 5 and 6). We use AW+SEN+SW procedure because it produced the best result among all methods. For reviews of restaurants, pattern 3 and 5 are the most productive with superior average precision and recall at both segment-level and entity-level compared to the rest. They account for more than half of an entity\u2019s pros and cons reference. This is probably due to the prevalence of sentences such as \u2018the service was good\u2019\nin restaurant reviews. The result is consistent with the current literature where adjectives and nouns are commonly used to detect sentiments and aspects in reviews for services. It is worth noticing that extracting any thing other than adjective-noun pairs may degrade the quality of summarization as the scores for pattern 2 and 4 are overwhelmingly low.\nThe behaviors of extraction patterns are trickier for the product domain as can be seen in Table 6. There is no dominating pattern in terms of high precision and recall at both segment and entity level. In particular, pattern 3 and 5 still recover a large portion of an entity\u2019s reference summary; however, the average quality of their matching segments (Rs) is the lowest among all patterns. Pattern 2 and 4 perform badly when used with the service domain but are more useful in the product domain, producing the highest quality segments (Rs = 66.3 and 69.4 for positive; Rs = 48.6 and 43.8 for negative). Although they do not appear as frequently in reviews as other patterns, they tend to carry more meaning in their words that it is hard to ignore them. Hence, all five patterns can contribute to the extraction of informative segments for summarization. This shows that doing summarization for products is harder than for services; and, care should be exercised when generalizing results from one domain to the other."}, {"heading": "5.4 Evaluation of Sentiment Classifiers and Segment Filters", "text": "Results in the previous section suggest to use different syntactic patterns for summarization of the service and product domains. We used patterns 1, 3, and\n5 for services and all patterns for products in all of our experiments in this section.\nWe applied seven different procedures for selecting candidate segments to compare the effects of 2 sentiment classifiers (SEN and SWN) and 5 filters (AW, SW, RANK, ASUM, and JST). The results are depicted in Table 7 and 8. The good overall performance of the Baseline+SWN procedure in both domains indicates that the proposed patterns extract good segments for summarization.\nComparing AW+SWN and AW+SEN, we see that SEN is better than SWN at sentiment classification. This result agrees with previous section, again confirming the effectiveness of our model in learning sentiments of domainspecific lexicons. AW+SWN performs better than Baseline+SWN, suggesting that top aspect words can be used to identify more informative segments. The best procedure is AW+SEN+SW, which tops all other procedures especially in the cons case. It favors segments that contain common aspect-related words and its associated sentiment lexicons, which are likely to be predominant in the pros and cons lists. ASUM has similar modelling assumption as ours and so it also produces relatively good results. However, the ability of our model to optimize sentiment polarities creates the improvement in performance. JST is even inferior to Baseline+SWN for half of the cases. This is not surprising given that the JST model is not intended for sentiment lexicons discovery; in contrast, it requires a large list of sentiment seed words to function well. Fi-\nnally, AW+SEN+RANK always has highest precision for segments but many segments are eliminated and that hurts its performance."}, {"heading": "6 Qualitative Evaluation", "text": "In this section we complement our results in previous section by qualitatively evaluating the quality of extracted segments with a user study and present example of summaries generated by our approach."}, {"heading": "6.1 Quality of Extracted Segments", "text": "We carried out an user study with 130 workers from the Amazon Mechanical Turk service. We randomly selected 123 short passages each has 4 to 6 sentences from reviews of coffee makers. The user\u2019s task is to read a passage and rate each text item as \u2018very useful\u2019, \u2018useful\u2019, \u2018somewhat useful\u2019, or \u2018useless\u2019 with reference to the passage. We included two types of items for each passage: segments extracted by our approach using the AW filter and adjective-noun phrases extracted using tagging and term-frequency as in [23]. Each user performs 6 tasks in which half of them are repetitions of others, thereby allowing us to detect users that give inconsistent ratings. We discarded users who completed their tasks in less than 90 seconds or rated half of the items inconsistently. Of the remaining 90 qualified users, 13 have not used coffee makers before whereas 60 have used for more than two months.\nIn total there were 358 unique segments, each rated 5.3 times and 470 unique word pairs, each rated 5.7 times. We converted the ratings into a numeric scale from 4 to 1 with 4 being \u2019very useful\u2019 and 1 being \u2019useless\u2019. On average, users rated the segments extracted by our method as 3.01 compared to 2.53 for the adjective-noun phrases. The higher rating is not merely due to segments having more words, as we observed that users typically give an adjective-noun word pair a same or higher rating than a segment if the two carry the same message. For example, \u2018carafe stays hot\u2019 and \u2018hot carafe\u2019 are same but the former has a rating of 2.7 whereas the latter has a rating of 3.1. Therefore the higher average rating for segments is a strong evidence that they convey more valuable information than adjective-noun word pairs.\nTable 9 elaborates further on this evidence by showing example of the segments and phrases rated as very useful by users. As can be seen, the segments are quite complete semantically whereas the phrases can be rather short in their meaning, which may require interpretation from users."}, {"heading": "6.2 Example Summaries", "text": "Below we show examples of a restaurant review and a coffee maker review together with the segments extracted as their summaries.\nReview of restaurant: The space is small but cozy, and the staff is friendly and knowledgeable. There was some great music playing, which kind of made me\nfeel like I was on vacation some place far away from Astoria. There are a lot of really great vegetarian options, as well as several authentic Turkish dishes. If you\u2019re still wasting time reading this review, stop now and head straight for Mundo. Your stomach could already be filled with tons of deliciousness.\nSummary: staff is friendly, space is small, some great music playing , several authentic Turkish dishes, really great vegetarian options.\nReview of coffee maker: I bought this machine about a week ago. I did not know which machine in the store to get, but the sales clerk helped me make the decision to buy this one. It is incredibly simple to use and the espresso is great. The crema is perfect too. My latte\u2019s rival those in coffee houses and I am saving a ton of money. The \u201dcapsules\u201d must be ordered from the Nespresso website, but they are usually at your door in 48 hours via UPS...\nSummary: incredibly simple to use, espresso is great, crema is perfect.\nIn both cases the summaries express the gist of each review relatively well. Looking at the sentence where a segment is extracted from, it can be seen that the segment conveys the main talking point of the sentence. Additionally, each segment does express an opinion about some aspect of the coffee maker or the restaurant. Recall that our key assumption in modeling reviews is that each sentence has a sentiment and an aspect. Therefore extracting segments the way we propose is likely to capture the main content of a sentence."}, {"heading": "7 Conclusions", "text": "In this paper we have describe a framework for extracting and selecting informative segments for review summarization of products and services. We extract candidate segments by matching against variable-length syntactic patterns and select the segments that contain top sentiment and aspect words learned by topic models. We proposed a new joint sentiment topic model that learns the polarity of aspect dependent sentiment lexicons. Qualitative and quantitative experiments verify that our model outperforms previous approaches in improving the quality of the extracted segments as well as the generated summaries."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022, Mar.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Old wine or warm beer: Target-specific sentiment analysis of adjectives", "author": ["A. Fahrni", "M. Klenner"], "venue": "Computational Linguistics, 2(3):60\u201363,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond the stars: Improving rating predictions using review text content", "author": ["G. Ganu", "N. Elhadad", "A. Marian"], "venue": "In WebDB,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS, 101(suppl. 1):5228\u2013 5235,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Sentiment analysis and subjectivity", "author": ["B. Liu"], "venue": "In Handbook of Natural Language Processing, Second Edition. CRC Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal", "D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming, 45:503\u2013528,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Rated aspect summarization of short comments", "author": ["Y. Lu", "C. Zhai", "N. Sundaresan"], "venue": "In WWW\u201909, pages 131\u2013140,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "Many automatic systems were built to address this challenge including generating aspect-based sentiment summarization of reviews [1,8,19] and comparing and ranking products with regard to their aspects [13].", "startOffset": 202, "endOffset": 206}, {"referenceID": 6, "context": "Different forms of the excerpts include sentence [8], concise phrase composing of a modifier and a header term [16], adjective-noun pair extracted based on POS tagging and the term-frequency of the pair [23], and phrase generated by rules [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "This is an important property of polarity of sentiment words as pointed out in [5,11,13,18], and recently several joint topic models have been proposed to unify the treatment of sentiment and topic (aspect) [9,11,17,21].", "startOffset": 79, "endOffset": 91}, {"referenceID": 4, "context": "This is an important property of polarity of sentiment words as pointed out in [5,11,13,18], and recently several joint topic models have been proposed to unify the treatment of sentiment and topic (aspect) [9,11,17,21].", "startOffset": 79, "endOffset": 91}, {"referenceID": 0, "context": "TSM is based on pLSI whereas more recent work ([9,11,20]) uses or extends Latent Dirichlet Allocation (LDA) [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Using a collapsed Gibbs sampler [7], new values for the topic and sentiment of a sentence c in document d are drawn from the conditional probability", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "We use the L-BFGS optimizer [14] to minimize the objective function L\u03b2\u2032\u2212 log p(\u03b2\u2032) by taking its partial derivatives with respect to yki and yji.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "In addition to our model-based sentiment classifier, we introduce another sentiment classifier based on SentiWordNet (SWN) [4], a popular lexical resource for opinion mining, using the same approach as in [5].", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We used a set of 929 positive and 236 negative noun phrases obtained from an external set of restaurant reviews in [5].", "startOffset": 115, "endOffset": 118}], "year": 2012, "abstractText": "We present a novel summarization framework for reviews of products and services by selecting informative and concise text segments from the reviews. Our method consists of two major steps. First, we identify five frequently occurring variable-length syntactic patterns and use them to extract candidate segments. Then we use the output of a joint generative sentiment topic model to filter out the non-informative segments. We verify the proposed method with quantitative and qualitative experiments. In a quantitative study, our approach outperforms previous methods in producing informative segments and summaries that capture aspects of products and services as expressed in the user-generated pros and cons lists. Our user study with ninety users resonates with this result: individual segments extracted and filtered by our method are rated as more useful by users compared to previous approaches by users.", "creator": "LaTeX with hyperref package"}}}