{"id": "1212.3618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2012", "title": "Machine Learning in Proof General: Interfacing Interfaces", "abstract": "We introduce ML4PG, a machine learning extension for Proof General. It enables the collection of evidence statistics on target forms, sequences of applied tactics and proof tree structures from the libraries of interactive higher proofs written in Coq and SSReflect. The data collected is clustered using state-of-the-art machine learning algorithms available in Matlab and Weka. ML4PG provides an automated interface between Proof General and Matlab / Weka. The results of clustering are used by ML4PG to provide evidence in the process of interactive proof development.", "histories": [["v1", "Fri, 14 Dec 2012 21:06:34 GMT  (1381kb,D)", "https://arxiv.org/abs/1212.3618v1", null], ["v2", "Mon, 8 Jul 2013 05:19:38 GMT  (504kb,D)", "http://arxiv.org/abs/1212.3618v2", "In Proceedings UITP 2012,arXiv:1307.1528"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["ekaterina komendantskaya", "j\\'onathan heras", "gudmund grov"], "accepted": false, "id": "1212.3618"}, "pdf": {"name": "1212.3618.pdf", "metadata": {"source": "CRF", "title": "Machine Learning in Proof General: Interfacing Interfaces", "authors": ["Ekaterina Komendantskaya", "J\u00f3nathan Heras", "Gudmund Grov"], "emails": ["katya@computing.dundee.ac.uk", "jonathanheras@computing.dundee.ac.uk", "G.Grov@hw.ac.uk"], "sections": [{"heading": null, "text": "C. Kaliszyk and C. Lu\u0308th (Eds.): 10th International Workshop on User Interfaces for Theorem Provers EPTCS 118, 2013, pp. 15\u201341, doi:10.4204/EPTCS.118.2\nc\u00a9 E. Komendantskaya, J. Heras & G. Grov This work is licensed under the Creative Commons Attribution License."}, {"heading": "1 Introduction", "text": "Over the last few decades, theorem proving has seen major developments. Automated (first-order) theorem provers (ATPs) (e.g. E [51], Vampire [49] and SPASS [57]) and SAT/SMT solvers (e.g. CVC3 [5], Yices [20] and Z3 [46]) are becoming increasingly fast and efficient [39]. Interactive (higher-order) theorem provers (ITPs) (e.g. Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].\nThe main conceptual difference between ATPs and ITPs lies in the styles of proof development: for ATPs, the proof process is primarily an automatically performed proof search, for ITPs it is mainly userdriven proof development. Nevertheless, ITPs have seen major advances in proof automation [22,27,43]. One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38]. One major success of this approach is Sledgehammer [48]: it offers Isabelle/HOL users an option to call for an ATP/SMTgenerated solution [9].\nIntegrating ITPs with ATPs requires a lot of research into methods of interfacing. Namely, the major challenge is a sound and reliable translation between inherently different languages [1, 10, 27, 38, 44]. This especially concerns interpreting outputs from ATPs back into the higher-order environment [1, 10, 44], which we will also call here backward interfacing. For example, Sledgehammer uses the results provided by external tools to guide the higher-order proofs, but leaves it to the Isabelle/HOL kernel to check that the suggested tactic combination is valid.\n\u2217The work was supported by EPSRC grant EP/J014222/1. \u2020The work was supported by EPSRC grants EP/H023852/1 and EP/H024204/1.\nIn parallel to the work mentioned above, another trend of research has been developed. It approaches the issue of improving proof automation from the perspective of statistical and machine learning methods. Several aspects of automated and interactive theorem proving can be data-mined:\n\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof de-\nvelopment in ITPs [19, 37].\nThe former trend has been more successful, mainly directed to improve premise selection in ATPs. In the case of higher-order interactive proofs, there are four main issues that make statistical data-mining challenging:\nC.1. The richer tactic language reduces the chance of finding regularities and proof patterns. ITPbased proofs involve an unlimited variety of structures and proof patterns, in comparison to ATPs, where resolution or rewriting may be the two possible rules to apply. Hence, finding statistically significant proof features becomes challenging.\nC.2. The notions of a proof may be regarded from different perspectives in ITPs: it may be seen as a transition between the subgoals [17, 55, 56], a combination of applied tactics [19], or \u2014 more traditionally \u2014 a proof tree showing the overall proof strategy [37]. Depending on the nature of the proof and application areas of the machine learning tools, each of the three aspects can be important for statistical proof pattern recognition.\nC.3. Backward interfacing \u2014 interpreting results provided by the statistical machine learning tool back into the higher-order interactive prover \u2014 can be a challenge.\nC.4. In interactive proofs, the most time-consuming and challenging part is no longer the time the prover takes to find the proof. It is the time the proof developer takes to understand and guide the proof. Therefore, when data-mining interactive proofs, we are interested not only in the final result \u2014 the successful proof, but also in the proof process, including failed and discarded derivation steps. We want machine learning to guide the process, not to diagnose or speed up already found proofs. For this, machine learning tools for ITPs need to be interactive.\nUp to now, experiments on data-mining interactive proofs were always constrained by the lack of the interactive interfacing between machine learning algorithms and the user-driven proof development. For example, in [34], there was a tool that gathered statistics, but no automated data-mining tools were used; in [37], there was a feature extraction method to data-mine proofs but it was not connected to efficient statistics gathering; in [19], these two were semi-automated.\nBecause of the inherently interactive nature of proofs in ITPs, user interfaces for ITPs play an important role in proof development. For our experiments, we chose Proof General [4] \u2014 a general-purpose, emacs-based interface for a range of higher-order theorem provers, e.g. Isabelle, Coq or Lego. Among them, we have chosen Coq [14] and its SSReflect library [22] for our experiments. Although both built upon the same language \u2014 Calculus of Inductive Constructions [15], they have distinct proof styles, analysis of which plays a special role in this paper, see Section 3.\nThis idea of maintaining a strong, convenient interface for a range of proof systems is mirrored by a similar trend in the machine learning community. As statistical methods require users to constantly interpret and monitor results computed by the statistical tools, the community has developed uniform interfaces \u2014 convenient environments in which the user can choose which machine learning algorithm to use for processing the data and for interpreting results. One such famous interface we take for our experiments is MATLAB [42] which has its own underlying programming language, and comprises\nseveral machine learning toolboxes, from general-purpose Statistical Toolbox to the specialised Neuralnetwork Toolbox. The second major machine learning interface we explore is Weka [26] \u2013 an open source, general purpose interface to run a wide variety of machine learning algorithms.\nWe have already referred to the two different meanings of the term \u201cinterfaces\u201d. On the one hand, interfacing may mean translation mechanisms connecting ITPs with other proof automation tools [1, 27, 38, 44]; and on the other hand, it is used as a synonym for user-friendly environment. In this paper, the two views on the notion of interfaces meet. Our primary goal is to integrate the state-of-the-art machine learning technology into ITPs, in order to improve user experience and productivity. However, since machine learning algorithms will need to gather statistics from the user\u2019s behaviour, and feed the results back to the user during the proof development process, this primary task will never be accomplished without machine learning becoming an integral part of the user interface.\nIn this paper, we show the results of our work on interfacing interfaces \u2014 building a user-friendly environment that integrates a range of machine learning tools provided by MATLAB and Weka into Proof General. In particular, we pay attention to addressing the challenges C.1-C.4. We implement the following vision of interfacing between ITP and machine learning, and call the result ML4PG (machine learning for Proof General), see Figure 1.1\n1. ML4PG must be able to gather statistics from interactive proofs (challenge C.3), and relate this statistics accurately to the three aspects of ITP-based proof development: goal-level, tactic-level, and proof tree level (challenge C.2). We focus on this issue in Section 2.\n2. ML4PG must automatically extract the relevant features associated with these three aspects in a form suitable for machine learning tools \u2014 that is, numerical vectors of fixed length, also known as feature vectors (challenges C.1\u2013C.2). We present a new method of proof-trace feature extraction in Section 3.\n3. ML4PG must enable the user to choose from a range of machine learning interfaces and algorithms suitable for proof data-mining (challenge C.4). As we do not assume the Proof General user to have machine learning expertise, we want to delegate a substantial amount of pre- and post-processing of statistical results to ML4PG. Section 4 deals with these questions.\n4. ML4PG must automatically connect to the chosen machine learning interface, and it should collect, appropriately analyse and interpret the output of these algorithms, at any stage of the interactive proof (challenge C.3). Note that in our work \u201cbackward interfacing\u201d from the machine learning\n1It is available at [28], where the reader can download ML4PG, user manual and examples (see also [29]).\ntools to Proof General is less demanding compared to [1, 27, 38, 44]. We do not seek a translation of statistical results into the Coq language; instead, we use the statistical results to inform the user of arising proof patterns during the proof development. As Sections 4 and 5 show, this kind of light backward interfacing can be efficiently implemented.\n5. Finally, ML4PG must interact with the user by providing relevant information about the user\u2019s current proof goal in relation to statistically similar proof patterns detected in different libraries or even across different users (challenges C.1\u2013C.4). We discuss this in Sections 4 and 5.\nThere are two aspects to this work: development of methods of interactive interfacing between the ITP and machine learning interfaces; and a more general aspect of studying the potential of machinelearning methods in proof-pattern recognition. This paper mainly focuses on the the first aspect. References [28, 29, 41] are specifically devoted to the benchmarks, evaluation and discussion of statistical proof-pattern recognition methods in theorem proving. As far as ML4PG interface engineering goes, it was important for us to make accessible a number of simple but useful options that the user with no experience in machine learning could use. Section 6 surveys related work on integration of machine learning with theorem proving. Finally, in Section 7, we conclude and discuss future extensions."}, {"heading": "2 The Three Levels of an Interactive Proof", "text": "In this section, we consider a variety of possible approaches to proof pattern recognition in ITPs; namely, we consider automated proofs from the levels of goal transitions, tactic sequences, and proof trees.\nWe start with several running examples to illustrate the kind of statistical help we expect from ML4PG. We consider the library containing various lemmas about natural numbers and lists.\nExample 2 Suppose the user starts with the following two lemmas about multiplication by 0: Lemma mult n 0:\u2200n : nat,0 = n\u22170 and Lemma mult 0 n: \u2200n : nat,0 = 0\u2217n, see left side of Tables 3 and 4 for their proofs. They state two very similar properties, however, the proofs for them are different; notably, one proof involved induction, while another involved only simplification.\nNext, suppose the user switches to the library containing lists; and needs some guidance to proceed with the proofs for Lemma app l nil: \u2200l : list A, l++[]= l, and Lemma app nil l: \u2200l : list A, []++l = l. The user asks ML4PG to \u201cstatistically match\u201d these problems to previously seen proofs in the same or in a different library. We then want ML4PG to tell the user that there are two similar lemmas in the Nat\nlibrary \u2014 namely Lemma mult n 0:\u2200n : nat,0 = n\u22170 and Lemma mult 0 n: \u2200n : nat,0 = 0\u2217n. Then the user will adapt these old proofs to complete new proofs as given on the right side of Tables 3 and 4. Note that this guidance will go further than just identifying proofs over the same data type, identifying same tactic combinations, same functions/operations or similar lemma shapes. Such guidance would be based on statistical correlation of several proof features.\nAs can be seen from these examples, the user may be interested in data-mining the proofs based on either\nD.1. transitions between subgoal-shapes (in which case Lemmas app l nil and mult n 0 of Table 3 have common patterns), or\nD.2. statistics of tactic combination (in which case Lemmas app nil l and mult 0 n of Table 4 should be identified), or\nD.3. a more general understanding of lemma content (in which case all four are similar).\nTherefore, we distinguish three levels at which pattern-recognition in ITP proofs can be approached, see also [24]:\n1. Goal-pattern recognition. Sequences of subgoals may show an apparent pattern in the structure of the formulas. This type of feature abstraction has been used for learning the inputs for automatic provers [17, 56] \u2014 which has later been extended to interactive proofs [53].\nExample 5 The left-most columns of Tables 3 and 4 should be used to gather such information about goal-patterns.\n2. Tactic-pattern recognition. Sequences of tactics applied at every level of the proof bear some apparent patterns, as well. There is always a finite number of tactics for any given proof, and therefore, they can serve as features for statistical learning. Previous work on learning proof strategies [19, 31, 32] has taken this approach. It is important to note that there may be proofs in which the goal structures do not bear any evident pattern; however, the sequence of applied tactics does. Also, as an additional complication, there is a variety of tactic combinations that may lead to a successful proof for one goal; and conversely, different goals may yield same sequences of tactics in successful proofs. Moreover, tactics often have complex configurations, which can be hidden or given as arguments (e.g. rules to apply or instantiations of variables).\nExample 6 The right-most columns of Tables 3 and 4 provide information about such tacticpatterns.\nThe disadvantage of tactic-pattern recognition is that any knowledge of when and why a tactic is applied, as well as its result is lost (except with respect to other tactic applications).\n3. Proof tree pattern recognition. Finally, there is the level of a proof tree that shows relations between different proof branches and subgoals and gives a better view of the overall proof flow; this approach was tested in [37] using multi-layer neural networks and kernels.\nExample 8 Figure 7 shows the proof tree for app l nil. An advantage of the proof tree as opposed to goal or tactic sequence, is that it distinguishes between different proof branches.\nOur second running example is based on the bigop library of SSReflect. This library is devoted to\ngeneric indexed big operations, like n \u2211\ni=0 f (i) or \u22c3 i\u2208I f (i).\nExample 9 We take three lemmas about number series:\n\u2200n,2( n\n\u2211 i=0\ni) = n(n+1); \u2200n, 2n\n\u2211 i=0|odd i\ni = n2; \u2200n, n\n\u220f 1 i = n!\nThe proofs of these three lemmas, both at the level of goals and tactics, are given in Table 10. Intuitively, they show certain similarities and dissimilarities, both at the level of goals and tactics. In the next sections, we will test how ML4PG analyses such cases.\nNext, we study how these general considerations about the levels of proof patterns are used in ML4PG to extract features used in statistical data-mining."}, {"heading": "3 Feature Extraction in ML4PG: the Proof Trace Method", "text": "In this section, we explain algorithms used by ML4PG to gather proof statistics at the levels of goals, tactics, and proof trees.\nThe discovery of statistically significant features in data is a research area of its own in machine learning, known as feature extraction, see [8]. Irrespective of the particular feature-extraction algorithm used, most pattern-recognition tools will require that the number of selected features is limited and fixed.2 We design our own method of proof feature extraction. The major challenge is to respect the above\n2\u201cSparse methods\u201d of machine-learning is an exception to this rule, see [40, 41]. We discuss the issue in Section 6.\nLemma fact prod: n \u220f 1 i = n!.\nrestriction while allowing to data-mine potentially unlimited variety of different higher-order formulas and proofs.\nWe first focus on the level of goals. ML4PG must choose the relevant features for statistical analysis. At this level, we could consider general goal properties such as \u201cgoal shape\u201d (e.g. \u201cassociative-shape\u201d or \u201ccommutative-shape\u201d), or properties like \u201cthe goal embeds a hypothesis\u201d, \u201cthe goal is embedded into a hypothesis\u201d. However, gathering such features uniformly across any set of higher-order proofs would be hard, especially when working with richer theories and dependent types.\nExample 11 Consider the proof for app l nil given in Table 4. One could say that the valuable information about the shape of the (sub)goal 4 is that it embeds the inductive hypothesis, as this fact is later used in the proof. However, for more complex examples, deciding such embeddings unambiguously during feature-extraction may be difficult, see [6]. Finally, detecting a fixed number of properties like e.g. \u201ccommutativity\u201d may apply to one type of proof libraries, e.g. natural numbers, but not to others, e.g. lists, in which case uniform comparison of proof patterns across libraries becomes hard.\nThis is why we developed a method of implicit tracking of proof properties, called the proof trace method; its early variant was used in [37]. The idea is as follows. When direct feature-extraction of the goal shapes is infeasible, we still can infer some properties of the goals when gathering statistics of how the user treats the goals. In other words, we let the term-structure show itself through the proof steps it induces. We deliberately do not pre-define the types of proof patterns that ML4PG must recognise, or define what a correlation of proof features is. Instead, we want statistical machine learning tools to suggest the user what these might be. An advantage of such proof feature extraction method is that it applies uniformly to any Coq library, and does not require any adaptation when ML4PG changes the libraries.\nAnother important feature ML4PG must be sensitive to, is the long-lasting effect of one proof step on several consecutive proof steps. The dependency between subgoals very often extends much farther than from one proof step to its immediate successor. Thus, we want to capture two dimensions of goal transformation in a proof:\n1. various traceable properties of a single (sub)-goal;\n2. transformations of each such property throughout several proof steps.\nThis is why we design two dimensional arrays as shown in Table 12 to allow for statistical datamining of the two dimensions in their relation.\nExample 13 Consider Table 12 where the correlation between mult_n_0 and app_l_nil at the goal level is shown. If we consider the table associated with app_l_nil, the fact of using the tactics induction and (simpl;trivial) may not be significant, as this combination can be applied to a variety of goals. It may be insignificant, on its own, that the top symbol of the goal was the quantifier \u2200. However, the table related to this lemma allows us to characterise the goal \u2200 l : list A, []++l=l by the 30 features (entries) of the table. Correlations of values of these features will be more likely to show significant proof patterns, if such exist.\nExample 14 As can be seen in Table 15, there is a strong correlation between the features associated with the first step in the proofs of sum_first_n, sum_first_n_odd and fact_prod. However, this strong correlation only remains between sum_first_n and fact_prod when successive proof steps are considered. This illustrates the fact that we cannot focus just on the first goal of a proof, but we have to study its proof trace to obtain relevant patterns.\nAnother advantage of the method is its focus on user interaction: ML4PG learns proof patterns specific to the user\u2019s proof style as given in the chosen library of proofs.\nOn the tactic level, ML4PG focuses on features associated with each tactic applied in a proof script. The action of the tactic-level feature extraction algorithm is shown in Table 17. It is worth noting that the structure of the goal-level Table 12 can be reused in all the systems based on the application of a sequence of tactics (e.g. Coq, Isabelle/HOL, Matita, etc.); the only difference would be the values which populate the table. On the other hand, the structure of the tactic-level table depends on the concrete system, since each ITP has its concrete set of tactics.\nThe case of Coq is special, since we can find two proof styles: plain Coq and SSReflect. Although SSReflect is an extension of Coq, this package implements a set of proof tactics designed to support the extensive use of small-scale reflection in formal proofs [22]. In addition, the behaviour of some Coq tactics has been modified (for instance, the rewrite tactic); so, the SSReflect imposes a distinct proof style. ML4PG works with both styles of Coq proofs. In the case of plain Coq, the rows of the tactic table represent the main Coq tactics (from almost 100 Coq tactics ML4PG currently distinguishes 10 most popular). The set of SSReflect tactics consists of less than 10 tactics, so we have included all of them.\nExample 16 Consider the fragments of tactic-level tables associated with the Lemma app nil l and mult O n, in Table 17. The extracted features show close correlation, as expected.\nExample 18 Fragments of tactic tables for Lemmas sum_first_n, sum_first_n_odd and fact_prod are given in Table 19. There is a strong correlation between sum_first_n and fact_prod at this level.\nFinally, ML4PG can extract the tree-level features, see Table 21. Currently, it considers the proof flow using up to the depth 5 of the proof tree.\nExample 20 The tables for Lemmas sum_first_n, sum_first_n_odd and fact_prod at the proof tree level are given in Table 21.\nThe feature extraction procedures explained in this section run in the background of ML4PG during Coq compilation. Some of the features are obtained just by inspecting the names and numbers of the applied tactics. In other cases, ML4PG needs to internally invoke Coq compiler to obtain the features, for instance, when recording types of tactic arguments. Thus, statistics related to the three proof levels is automatically gathered during the proof development.\nMachine learning algorithms expect numerical feature vectors as inputs; therefore, ML4PG converts the features into numbers. As we explained in [37], the concrete function that ML4PG uses for this purpose may vary, but the numeric conversion must be consistent. Dynamic calculation of the function that converts table features into numbers is implemented in ML4PG. In particular, we have defined 4 one-to-one functions [[ . ]]Tactic, [[ . ]]Type, [[ . ]]Top symbol and [[ . ]]hyp or lemma that assign respectively a numerical value to each tactic, type, top symbol and lemma appearing in a proof. The conversion provided by these functions is blind, assigning respectively unique consecutive integers to tactics, types, top symbols and lemmas in order of their appearance in the library. If several elements appear in a cell, the value of that cell is the concatenation of the values of each element.\nExample 22 The numerical table for Lemma sum_first_n_odd at the goal level is given in Table 23. This table is flattened into a vector to be given as input to machine learning algorithms. Namely, Table 23 gives rise to the following feature vector: [3,1,\u22122,1,6,2,7,1,\u2212444,126106,6,1,1517,4,\u2212244444,112113105176,5,0,1,1,\u221224,0,5,1,7,1, \u221244444444444,25484634152437143325432,6,0].\nOnce the feature vectors are collected, ML4PG can data-mine the proofs using different machine learning algorithms."}, {"heading": "4 Interactive Proof-Clustering in ML4PG", "text": "ML4PG is designed to prove the concept: it is possible to interface higher-order proofs with machine learning engines, and do it interactively during the proof process. Interaction with several machine learning engines and algorithms is in the core of this process. This differs from the experiments performed in the literature, see [19, 37, 41, 56], where the data-mining of proofs is performed separately from the user interface. In this section, we explain how ML4PG enables the user interaction with a range of machine learning engines and algorithms, and give some technical details of the ML4PG implementation.\nThe ML4PG user may or may not be familiar with machine learning. Either way, ML4PG must offer him a number of simple but useful options to configure machine learning tools while staying within the Proof General environment. Therefore, ML4PG takes the burden of connecting to the machine learning algorithms.\nThe first choice the user makes concerns the proof level: proofs can be data-mined at the level of goals, tactics or proof trees, as explained in the previous sections. It is worth mentioning here that there are several choices of how to run this feature extraction. One option would be to extract features on demand \u2014 that is, once the user chooses the proof level, ML4PG could re-run Coq again to complete the feature extraction. The disadvantage of this is that the proof engine will have to be re-run every time one uses ML4PG for data-mining. We made a different choice: ML4PG extracts features in the background during the interactive proofs. It does the extraction at all three proof levels whenever the proof library\nis compiled. Then, the choice of proof level in the menu just indicates which data set will be sent to the machine learning algorithms. The advantage is that the time taken by data mining does not include the proof engine run. Our experiments show that the time involved in the feature extraction during the normal Coq compilation is unnoticeable, and does not significantly slow down the proof development.\nNow ML4PG is ready to communicate with machine learning interfaces. ML4PG is built to be modular \u2014 that is, when the feature extraction of Section 3 is completed within the emacs environment, the data is gathered in the format of hash tables. The first elements of these tables are the names of the lemmas, and the second elements are the feature vectors encoded as lists of numbers (let us note that emacs is a Lisp environment; therefore, it is sensible to use lists to represent the feature vectors). However, every machine learning engine has its concrete format to represent feature vectors; therefore, it is necessary to define translators to adapt ML4PG\u2019s internal encoding of feature vectors to the concrete representation of the machine learning engine. We have defined translators for two different, but equally popular, machine learning interfaces \u2014 MATLAB and Weka. ML4PG transforms the feature vectors to a comma separated values (csv) file in the case of MATLAB; and, to arff files in the case of Weka. In principle, extending the list of machine learning engines does not require any further modifications to the feature extraction algorithm, but just defining new translators. Notice the similarity with implementation of the proof level choice: again, once the features are extracted, the ML4PG engine is flexible to use them for all sorts of data mining tasks and machine learning interfaces.\nOnce the feature vectors are in a suitable format, ML4PG can invoke the machine learning engine. The ML4PG mechanism connecting to machine learning interfaces is similar to the native mechanism of Proof General used to connect to ITPs. Namely, there is a synchronous communication between ML4PG and the machine learning interfaces, which run on the background waiting for ML4PG calls.\nThe next configuration option ML4PG offers is the choice of the particular pattern-recognition algorithm available from the chosen machine learning interface. Again, this choice is made within the proof environment of Proof General. There are several machine learning algorithms available in MATLAB and Weka. We connected ML4PG only to clustering algorithms [8] \u2014 a family of unsupervised learning methods. Unsupervised learning is chosen when no user guidance or class tags are given to the algorithm in advance.\nOne could in principle envisage supervised machine learning applications in proof pattern recognition, where the user labels every proof using some finite tags, such as \u201cfundamental lemma\u201d, \u201cauxiliary lemma\u201d, \u201cproof experiment\u201d. And, on the basis of such labels and some number of training examples, the machine learning algorithm would be able to predict labels for any new proof. Here, we do not assume existence of such labels. However, our modular approach to interfacing with Proof General implies that, if the labels are available, interfacing with supervised algorithms will not be hard for ML4PG. In fact, we envisage the feature extraction method to remain the same. Section 6 discusses related work using supervised learning in proof-pattern recognition.\nIn case of MATLAB, the algorithms included in ML4PG are the two most popular methods for clustering: K-means and Gaussian [58]. If the user selects Weka as a machine learning engine; then, he can select among K-means, FarthestFirst and simple Expectation Maximisation.\nTo improve the accuracy of the clustering algorithms, a technique called Principal Components Analysis (PCA) [33] is applied. This functionality reduces the size of feature vectors but without much loss of information. The application of techniques like PCA, known in general as dimensionality reduction procedures [58], is recommended when dealing with feature vectors whose size is higher than 15 \u2014 as in our case.\nFinally, the user can choose proof libraries that he wants to access using ML4PG. Before using them, those libraries must be exported with the mechanism provided by ML4PG. ML4PG extends the compi-\nlation procedure that Coq uses for imported libraries with the feature extraction algorithm described above. Such a mechanism checks that all the proofs of the library are finished, and generates a file which contains the list of the lemmas of the library, and three files encoding respectively the feature vectors at the goal level, tactic level and proof tree level. Subsequently, when the user chooses a library, ML4PG transforms the files to the internal encoding of feature vectors (implemented by Lisp lists) and attaches those vectors to those obtained in the current development.\nBy default, ML4PG clusters the current library, but the user can add more libraries to perform clustering. The reason for not using all the available libraries is twofold. First of all, it is a question of performance, since the time needed to obtain clusters increases with the amount of libraries. The second reason is usability, because if ML4PG uses all the available libraries for clustering, it can obtain proof patterns from lemmas which belong to libraries unknown to the user, and this may or may not be convenient.\nWe now return to the examples introduced in Section 2 to illustrate how proof patterns are shown to the ML4PG user.\nExample 24 We created a small library (70 lemmas) to help us with the initial tests of ML4PG: it contains some basic lemmas about natural numbers and lists, as well as our running examples of Tables 3 and 4. We also included efficient and inefficient proofs, and cases when similar lemmas were proven using different strategies, and different lemmas were proven using the same proof strategy. In the rest of the paper, we will call the library Initial. Figure 25 shows the result of running ML4PG on this library, with the following settings:\n\u2022 statistics were taken using goal-level feature extraction;\n\u2022 machine learning interface: Weka;\n\u2022 machine learning algorithm: K-means.\nML4PG shows that all lemmas of Tables 3 and 4 belong to the same group of proofs. It agrees with one possible interpretation of the content of these lemmas, see D.3 in Section 2. But there are other lemmas in that cluster; in particular, this cluster gathers \u201cfundamental\u201d lemmas about various operations on natural numbers involving 0 and operations on lists involving nil.\nThe example above shows one mode of working with ML4PG: that is, when a library is clustered irrespective of the current proof goal. However, it may be useful to use this technology to aid the interactive proof development. In which case, we can cluster libraries relative to a few initial proof steps for the current proof goal. Note that ML4PG graphical interface offers two menu buttons for these two options \u2013 the two right-most buttons of Figure 25. The next example illustrates this.\nExample 26 On the left side of Figure 27, an incomplete development of Lemma sum_first_n is shown. Using the bigop and binomial libraries of SSReflect (205 lemmas), ML4PG can obtain proof patterns similar to the lemma that we are proving, see right side of Figure 27. The ML4PG settings in this case were:\n\u2022 statistics was taken using goal-level feature extraction;\n\u2022 machine learning interface: MATLAB;\n\u2022 machine learning algorithm: K-means.\nAmong the suggestions provided by ML4PG, we find the Lemma fact_prod. Note that, as we have seen in Section 3, there is a high correlation between their feature vectors. Other lemmas ML4PG discovered are related to series of natural numbers (including properties about big sums and big products). Lemmas like sum_first_n_odd, where there is a restriction on the elements of the series, belong to a different cluster since the correlation with lemmas like sum_first_n and fact_prod is low.\nWe have shown flexibility, modularity and interactivity of ML4PG in interfacing with machine learning environments. These features come for free with the light version of \u201cbackward interfacing\u201d that ML4PG implements: that is, it does not translate the outputs of the clustering algorithms back into the Coq language. Its only form of backward interfacing is conversion of clustered feature vectors back into lemma names \u2014 the output shown in Figures 25 and 27.\nGenerally, interfacing the ITPs with external tools (e.g. ATPs) is a challenging task, see [1,27,38,44]. A special concern is the translation of the output produced by the external tools into the ITP. This is due to the fact that unsound translation can introduce inconsistencies in the system; see e.g. [10]. In case of interfacing with machine learning, the external tool is even more alien to ITP\u2019s syntax than ATPs. Light backward interfacing implemented in ML4PG may well be the optimal solution to the problem."}, {"heading": "5 Handling Proof Statistics in ML4PG", "text": "The previous sections highlighted two features of ML4PG \u2014 light backward interfacing and interactive handling of machine learning interfaces. To handle these features gracefully, ML4PG must offer the user a convenient environment for processing and analysing the results obtained by machine learning\nalgorithms. Clustering techniques divide data into n groups of similar objects (called clusters), where the value of n is a \u201clearning\u201d parameter provided by the user together with other inputs to the clustering algorithm. Increasing the value of n means that the algorithm will try to separate objects into more classes, and, as a consequence, each cluster will contain fewer examples with higher correlation. The frequencies of clusters can serve for analysis of their reliability. Results of one run of a clustering algorithm may differ from another, even on the same data set. This is due to the fact that clustering algorithms randomly choose examples to start from, and form clusters relative to those examples. However, it may happen that certain clusters are found repeatedly \u2014 and frequently \u2014 in different runs; then, we can use these frequencies to determine the reliable clusters.\nML4PG\u2019s tools handling statistical results include a set of programs written in MATLAB and Weka, that post-process outputs of the clustering algorithms. For each clustering algorithm the user invokes, ML4PG generates one corresponding program handling the output statistics. These various programs always have three arguments: a file and two natural numbers representing the number of clusters and frequency threshold. We explain these settings in this section.\nVarious numbers of clusters can be useful for interactive proof data-mining: this may depend on the size of the data set, and on existing similarities between the proofs. We want ML4PG to accommodate such choices. In general, small values of n are useful when searching for general proof patterns which can later be refined by increasing the value of n. However, extreme values are to be avoided: small values of n can produce meaningless proof clusters for big proof libraries; whereas trivial clusters with just one proof may be found for big values of n. Very often in machine learning, the optimal number of clusters is determined experimentally, but we cannot afford this in ML4PG setting, as we assume the user focuses mainly on the Coq proofs.\nIn the machine learning literature, there exists a number of heuristics to determine this optimal num-\nber of clusters, [58]. We used them as an inspiration to formulate our own algorithm for ML4PG, tailored to the interactive proofs. At any given stage of the interactive proof, it takes into consideration the size of the proof library and an auxiliary parameter we introduce here, called granularity. This parameter is used to calculate the optimal number of proof clusters, using the formulas of Table 28. As a result, the user does not provide the value of n, but just decides on granularity in ML4PG menu, by selecting a value between 1 and 5, where 1 stands for a low granularity (producing big and general clusters) and 5 stands for a high granularity (producing small and precise clusters).\nExample 29 Consider Example 24: there, the default granularity was 3, and the cluster contained all lemmas from Tables 3 and 4. Increasing the granularity, ML4PG discovers only Lemmas app_l_nil and mult_n_0 (see D.1 from Section 2), as well as similar proofs for plus_n_0 and minus_n_0. All of them use induction, and prove similar base cases. Note that in Section 2 we conjectured this separation of examples of Tables 3 and 4 into two clusters as a desirable feature.\nExample 30 In Example 26, ML4PG used the default granularity value of 3, to obtain ten suggestions related to the Lemma sum_first_n. If the ML4PG user increases such granularity value to 5, he obtains only one suggestion, see Figure 31: the Lemma fact_prod. Inspecting the proof of Lemma fact_prod can give an insight into how to finish the proof for sum_first_n. We notice that we can apply the Lemma big_nat_recr to our current goal and, subsequently use the inductive hypothesis. The rest of the proof is based on rewriting rules of natural numbers.\nAs implied by the above examples, the configuration of the granularity parameter can be approached in two different ways: top-down and bottom-up. The top-down approach suggests first using a small value for the granularity to obtain a general proof pattern, and then refine that pattern increasing the granularity value. On the contrary, in the bottom-up approach a high value for the granularity is used to see what the most similar lemmas are and then decrease the granularity value to see more general \u2014 and potentially less trivial \u2014 patterns.\nFinally, the third parameter ML4PG uses to analyse clustering outputs is the frequency threshold. For this purpose, ML4PG actually uses double criteria: the proximity and frequency of the cluster. The clustering algorithm output contains not only clusters but also a proximity value \u2014 a measure of how close each object in one cluster is with respect to objects in other clusters. This measure ranges from +1, indicating points that are very distant from other clusters, through 0, indicating points that are not distinctly in one cluster or another, to \u22121, indicating points that are probably assigned to the wrong cluster. We have fixed 0.5 as an accuracy threshold, and all the clusters whose measure is under such value are ignored by ML4PG. This criterion is fixed, and the user interface does not give access to it. However, the second criterion, the frequency parameter, is customizable within the interface.\nOur experience shows that analysis of frequencies may give two opposite effects.\n* On the one hand, high frequencies suggest that the proofs found in clusters have a high correlation, and that is a desirable property.\n** On the other hand, proofs with too high correlation may be too trivial for providing interesting proof hints. Therefore, it is sometimes useful to look for proof clusters with lower frequencies \u2014 as they may potentially contain those non-trivial analogies.\nExample 32 Illustrating this, in our running example, the four proofs from Tables 3 and 4 were initially found only in 6% of runs (low frequency), see Figure 25; whereas there were other clusters with high frequencies that contained trivially similar lemmas (for instance, a cluster contains all the lemmas of the shape x+0 = x where x is a term \u2014 the proofs of all these lemmas are the same).\nTo gather sufficient statistical data from proofs, ML4PG runs the chosen clustering algorithm 200 times at every call of clustering, and collects the frequencies of each cluster. After discarding those with low proximity, it calculates the frequency of the significant patterns. Once frequencies are calculated, ML4PG applies the following methodology. As item * suggests, one purpose of the frequencies is to serve as thresholds: if the number of times the cluster occurs falls below the pre-set threshold, the corresponding proofs will not be displayed to the user. On the other hand, as item ** suggests, the acceptable frequency threshold values may differ from proof to proof, and may depend on the purpose of proof pattern recognition. For this, ML4PG allows the user to vary the threshold values. At the moment, we implemented three choices: frequency parameters 1\u20133 as shown in Table 28. This particular range of thresholds comes from our experience with several Coq and SSReflect libraries. However, in line with our general modular approach to ML4PG design, we assume a wider range can be implemented, if desired. Our current choice is to keep the ML4PG interface simple and minimalistic.\nExample 33 Table 34 shows the results for different choices of algorithms and parameters for Example 26, we highlight the result presented in Figure 27.\nOur next example shows an interesting interplay between the effects of varying granularity, frequency, and proof level in the process of proof data-mining.\nExample 35 In Example 24, ML4PG shows the clusters for our four running examples from the library Initial, when using the default frequency value of 1. As Example 32 showed, when increasing the frequencies parameter, such a cluster would fall below the threshold (compare with Figure 25, where the frequency of this cluster was 6%). At the same time, when increasing the granularity parameter to 5, our four proofs will be split into two smaller clusters, each having higher frequencies. Notably, inductive proofs (see D.1 and Table 3 from Section 2 and Example 29) are separated from those by simplification (see D.2 and Table 4). The cluster containing only lemmas from Table 3 has a frequency of 47% (see the left screenshot of Figure 36). The proofs from Table 4 also form a smaller cluster, but with frequency of 7%. Therefore, both clusters are shown if the frequency parameter is 1, but we also have an option of choosing a higher frequency (15% or 30%) to discard the second, less significant, cluster.\nThis is a typical situation, small values of the granularity parameter usually produce big clusters with small frequencies. When the granularity value is increased, the big clusters are split into smaller ones with high frequencies. Note the interactive nature of this proof pattern recognition process.\nExample 37 A similar effect of increasing granularity parameter and increasing frequencies for the tactic-level proof features is shown in Figure 36. The figure also demonstrates that data-mining the same library using goal-level features and tactic-level features can bring different results. Interestingly, with increase of granularity, the goal-level clustering focuses on the examples related to lemmas in Table 3, whereas the tactic-level clustering focuses on examples related to lemmas of Table 4; as we conjectured in items D.1 and D.2 of Section 2.\nWe finish this section with a discussion of the role of this statistical analysis in our approach to the light backward interfacing. ML4PG handles the results obtained with MATLAB and Weka in a uniform way: for this purpose, we devised an XML format, see Figure 38. Using this approach, ML4PG can deal with the output generated by any system which follows this XML standard using just one program which\ntransforms the XML files into a suitable format for the user. As a consequence, ML4PG can be easily extended with new engines and machine learning algorithms.\nThe XML files returned by the machine learning engines are processed in two different ways depending on the mode of using ML4PG: that is, general clustering (as illustrated in Example 24) or goal-dependent clustering (as shown in Example 26). In both cases, the XML file is converted to a list of pairs where the first element of the pair contains the lemma names and the second element the frequency of each cluster. In the general clustering case, such a list is processed to be shown as in Figure 25. For the goal-dependent clustering, ML4PG searches for those pairs of the list where the current proof is included. If the current proof belongs to several clusters, then ML4PG takes the one with the highest frequency and displays it as shown in Figure 27."}, {"heading": "6 Integrating machine learning with theorem proving: related work", "text": "In this section, we present an overview of the integration of machine learning techniques into automated and interactive proofs. There are two machine learning styles that can be useful in this context: symbolic and statistical.\nSymbolic machine learning methods can formulate auxiliary lemmas or proof strategies from background knowledge. As we have explained in the introduction, Proof General is a general-purpose interface for a range of higher-order theorem provers, and this is probably the reason why this interface has been used in different works to integrate machine learning techniques. This is the case of IsaPlanner [18],\na generic framework for proof planning that integrates techniques like rippling [6] in the interactive theorem prover Isabelle. Also using Isabelle as a prover and Proof General as interface, PGtips [45] is a tactic recommender system based on data-mining techniques [19]. Another advisor implemented in Proof General, but in this case for the Mizar system, was incorporated into MizarMode [54] to suggest similar results which could be useful in the current proof.\nInductive provers like ACL2 [35] and Hip [50] also include symbolic machine learning techniques. In particular, ACL2 uses these methods for proving termination of programs written in Lisp [36], and Hip implements a theory discovery mechanism [13] for automatically derive and prove properties about functional programs implemented in Haskell.\nThe main drawback of the symbolic methods is their scope. These techniques are often tailored for certain fragments of first order language or a certain library, or a certain proof shape; therefore, they do not scale properly to deal with big libraries. On the other hand, statistical machine learning methods scale to big libraries without problems, but they have very weak power for generalisation.\nStatistical machine learning methods can discover data regularities based on numeric proof features. ML4PG belongs to this category of methods. Among other successful statistical tools is the method of statistical proof-premise selection [39\u201341, 53]. Applied in several ATPs, it provides statistical ratings of existing lemmas; and this makes automated rewriting more efficient.\nThis technique has also been used to integrate ITPs, ATPs and machine learning. The workflow of tools like Sledgehammer to prove a theorem consists of the following steps: (1) translation of the statement of the theorem (from Isabelle, HOL or Mizar format) to a first order format suitable for ATPs; (2) selection of the lemmas (or premises) that could be relevant to prove the theorem; (3) invocation of several ATPs to prove the result; and (4) if an ATP is successful in the proof, reconstruction of the proof in the ITP from the output generated by the ATP. An important issue in this procedure is the premise selection mechanism, especially when dealing with big libraries, since proofs of some results can be infeasible for the ATPs if they receive too many premises.\nStatistical machine learning methods are used to tackle this problem in [40, 41]. In particular, a classifier is constructed for every lemma of the library. Given two lemmas A and B, if B was used in the proof of A, a feature vector |A| is extracted from the statement of A, and is sent as a positive example to the classifier < B > constructed for B; otherwise, |A| is used as a negative example to train < B >. Note that, |A| captures statistics of A\u2019s syntactic form relative to every symbol in the library; and the resulting feature vector is a sparse (including up to 106 features). After such training, the classifier < B > can predict whether a new lemma C requires the lemma B in its proof, by testing < B > with the input vector |C|. On the basis of such predictions for all lemmas in the library, this tool constructs a hierarchy of the premises that are most likely to be used in the proof of C. Figure 39 illustrates this approach.\nTable 40 summarises the main differences between premise-selection tools and ML4PG. It is important to notice that the two methods have different approaches to handling large-size data. Premiseselection tools rely on advanced \u201csparse\u201d machine-learning algorithms to process the growing feature vectors. ML4PG achieves scaling at the stage of feature extraction, by using the proof trace method to produce compact feature vectors. As a result, ML4PG also works well with libraries of small size (and hence can interact with the user at any stage of the proof), whereas sparse methods need proof libraries of big size to perform well."}, {"heading": "7 Conclusions and Further work", "text": "In this paper, we have presented a Proof General extension, called ML4PG, to interface ITPs and machine learning engines. Our main goal was to prove that it is possible to interface higher-order interactive theorem proving with statistical machine learning; and the resulting tool can provide fast and non-trivial proof hints during the proof development. The technical highlights of ML4PG are:\n\u2022 the proof trace method is a flexible, extendable technique that gathers statistics from proofs on the basis of the relative transformations of simple parameters within several proof steps; and,\n\u2022 the light backward interfacing implemented in ML4PG automates the Proof General interaction with machine learning engines. It helps to analyse and interpret the output of machine learning algorithms; however, it avoids full translation of the statistical outputs into the prover\u2019s language.\nThe ML4PG approach has several benefits. First of all, it does not assume any knowledge of machine learning interfaces from the user; and automates initial statistical experiments (determining the number of clusters, calculating frequencies) that otherwise would have been performed by hand. The choices for various measures of cluster granularity and frequency can be easily extended in the future. Moreover, it is a modular tool which allows the user to make choices regarding approach to levels of proofs and particular statistical algorithms. By design, it allows further extensions to different machine learning environments, modes of supervised/unsupervised learning, and various learning algorithms within those modes. In addition, it is tolerant to mixing and matching different proof libraries, different notations and proof styles used across several developments.\nComparing across different proof levels and different styles of proofs, our experiments show that data-mining the goal-level features shows more interesting clusters compared to the other two feature extraction methods. We plan to improve the other two feature extraction methods in the future. Proofs in SSReflect yield more consistent classification results compared to the plain Coq style. This is due to a stricter proof discipline in SSReflect, which allows ML4PG to detect more significant proof patterns.\nThe feature extraction method implemented in ML4PG can be improved in two different ways. First of all, the proof trace method considers just the first five proof steps, losing some information. As the machine learning algorithms integrated in ML4PG require a fixed number of features whereas Coq proofs may have varied length, we can study big proofs considering either small patches of proofs (by partially reusing the proof trace method), or proofs as a whole (in this case, a sparse representation will be necessary). The numerical assignment provided by the function [[ . ]]hyp or lemma for the lemmas of the libraries gives a big value spread (especially if ML4PG works with big proof libraries). We can tackle this problem assigning closer numbers to similar lemmas. We are currently working in the implementation of these new features in ML4PG.\nML4PG can be combined with other tools to make the proof development easier. Search mechanisms implemented in Coq, such as the Search command of SSReflect [22] and Whelp [2], can find patterns in lemma statements, but ML4PG can discover proof patterns that cannot be found using existing Coq\u2019s search mechanisms, see [28].\nMoving towards symbolic machine-learning, we envisage that new lemmas or strategies could be conceptualised from the proof families obtained with ML4PG using such techniques as Rippling [6] or Theory Exploration [32].\nIn addition, we plan to integrate more machine learning methods to help in the proof process. To this aim, we need a tool which tracks not only the successful proofs, but also failed and discarded derivations steps. In this way, we could use supervised machine learning algorithms to indicate a user whether he is following a sensible strategy based on the previous experience. Supervised machine learning could also be used to discover various proof styles.\nWe are interested in increasing the number of proof assistants included in our framework. This will allow us to study proof similarities across different theorem provers. Since the interaction with theorem provers such as Isabelle or Lego is already available in Proof General, we just need the implementation of the feature extraction mechanism for them; their interaction with the machine learning engines would be the same as developed for Coq and SSReflect.\nFinally, current implementation of ML4PG is centralised; this means that the user can obtain proof clusters of the libraries available on his computer. However, we think that a client-server architecture, where the proof information is shared among several users could also be useful, especially for team-based program development.\nFor this purpose, feature extraction in ML4PG is already designed to be lemma name and notation independent."}, {"heading": "8 Acknowledgements", "text": "We are grateful to the anonymous referees for their comments and suggestions; and the following individuals and research groups for inspiring discussions on the topics of machine learning in automated and interactive theorem proving: J Strother Moore, members of the AI4FM project Alan Bundy, Cliff Jones, Leo Freitas, Ewen Maclean; and participants of Dagstuhl seminar AI meets Formal Software Development [12]."}], "references": [{"title": "A Content Based Mathematical Search Engine: Whelp", "author": ["A. Asperti", "F. Guidi", "C. Sacerdoti Coen", "E. Tassi", "S. Zacchiroli"], "venue": "Post-Proceedings of the TYPES\u201904 International Conference, Lecture Notes in Computer Science", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "The Matita interactive Theorem prover", "author": ["A. Asperti", "W. Ricciotti", "C. Sacerdoti Coen", "E. Tassi"], "venue": "International Conference on Automated Deduction (CADE\u201911), Lecture Notes in Computer Science", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Proof General: A Generic Tool for Proof Development. In: 6th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS\u201900)", "author": ["D. Aspinall"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Rippling: Meta-level Guidance for Mathematical Reasoning", "author": ["D. Basin", "A. Bundy", "D. Hutter", "A. Ireland"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Analytica - an experiment in combining theorem proving and symbolic computation", "author": ["A. Bauer", "E.M. Clarke", "X. Zhao"], "venue": "Journal of Automated Reasoning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Extending Sledgehammer with SMT Solvers", "author": ["J.C. Blanchette", "S. B\u00f6hme", "L.C. Paulson"], "venue": "International Conference on Automated Deduction (CADE-23),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Encoding monomorphic and polymorphic types. In: 19th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS\u201913)", "author": ["J.C. Blanchette", "S. B\u00f6hme", "A. Popescu", "N. Smallbone"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A Brief Overview of Agda \u2014 A Functional Language with Dependent Types. In: 22nd International Conference on Theorem Proving in Higher Order Logics (TPHOLs\u201909)", "author": ["A. Bove", "P. Dybjer", "U. Norell"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "AI meets Formal Software Development (Dagstuhl Seminar 12271)", "author": ["A. Bundy", "D. Hutter", "C.B. Jones", "J S. Moore"], "venue": "Dagstuhl Reports 2(7),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Automating Inductive Proofs using Theory Exploration", "author": ["K. Claessen", "M. Johansson", "D. Ros\u00e9n", "N. Smallbone"], "venue": "24th International Conference on Automated Deduction (CADE\u201324),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "The Calculus of Constructions", "author": ["Thierry Coquand", "G\u00e9rard P. Huet"], "venue": "Inf. Comput", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}, {"title": "Learning from Previous Proof Experience: A Survey", "author": ["J. Denzinger", "M. Fuchs", "C. Goller", "S. Schulz"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Automatic Acquisition of Search Control Knowledge from Multiple Proof Attempts", "author": ["J. Denzinger", "S. Schulz"], "venue": "Information and Computation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "IsaPlanner: A Prototype Proof Planner in Isabelle", "author": ["L. Dixon", "J.D. Fleuriot"], "venue": "International Conference on Automated Deduction (CADE\u20192003), Lecture Notes in Computer Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "The use of Data-Mining for the Automatic Formation of Tactics", "author": ["H. Duncan"], "venue": "Ph.D. thesis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "The Yices SMT solver. Available at http://yices.csl.sri.com/ tool-paper.pdf", "author": ["B. Dutertre", "L. de Moura"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Formal proof - The Four-Color Theorem", "author": ["G. Gonthier"], "venue": "Notices of the American Mathematical Society", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "An introduction to small scale reflection", "author": ["G. Gonthier", "A. Mahboubi"], "venue": "Journal of Formalized Reasoning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Kornilowicz", "A. Naumowicz"], "venue": "Journal of Formalized Reasoning", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A Statistical Relational Learning Challenge - extracting proof strategies from exemplar proofs. In: ICML\u201912 worshop on Statistical Relational Learning", "author": ["G. Grov", "E. Komendantskaya", "A. Bundy"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "The Flyspeck Project fact sheet. Project description available at http://code.google. com/p/flyspeck", "author": ["T. Hales"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall"], "venue": "SIGKDD Explorations", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A skeptic approach to combining HOL and Maple", "author": ["J. Harrison", "L. Th\u00e9ry"], "venue": "Journal of Automated Reasoning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "ML4PG: machine learning interface for Proof General", "author": ["J. Heras", "E. Komendantskaya"], "venue": "Program files and user manual", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "ML4PG in Computer Algebra Verification", "author": ["J. Heras", "E. Komendantskaya"], "venue": "Conferences on Intelligent Computer Mathematics (CICM\u201913), Lecture Notes in Computer Science", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A SAT-based procedure for verifying finite state machines in ACL2", "author": ["W.A. Hunt", "E. Reeber"], "venue": "In: 6th International workshop on the ACL2 theorem prover and its applications,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Automatic learning of proof methods in proof planning", "author": ["M. Jamnik", "C.M. Kerber"], "venue": "Benzmller", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Conjecture Synthesis for Inductive Theories", "author": ["M. Johansson", "L. Dixon", "A. Bundy"], "venue": "Journal of Automated Reasoning", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Principal Components Analysis. Springer-Verlag, doi:10.1007/978-1-4757-1904-8", "author": ["I. Joliffe"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Accumulated persistence in ACL2. http://www.cs.utexas.edu/ users/moore/acl2/current/ACCUMULATED-PERSISTENCE.html", "author": ["M. Kaufmann", "J S. Moore"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Integrating CCG analysis into ACL2. In: Eighth International Workshop on Termination, part of FLOC\u201906, doi:10.1.1.79.4636", "author": ["M. Kaufmann", "P. Manolios", "J S. Moore", "D. Vroon"], "venue": "Machine Learning in Proof General: Interfacing Interfaces", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Neural Networks for Proof-Pattern Recognition", "author": ["E. Komendantskaya", "K. Lichota"], "venue": "In: International Conference on Artificial Neural Networks (ICANN\u201912), Lecture Notes in Computer Science", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Interfacing Coq + SSReflect with GAP. In: 9th International Workshop On User Interfaces for Theorem Provers (UITP\u201910)", "author": ["V. Komendantsky", "A. Konovalov", "S. Linton"], "venue": "Electronic Notes in Theoretical Computer Science", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Overview and Evaluation of Premise Selection Techniques for Large Theory Mathematics", "author": ["D. K\u00fchlwein", "T. van Laarhoven", "E. Tsivtsivadze", "J. Urban", "T. Heskes"], "venue": "In: 6th International Joint Conference on Automated Reasoning (IJCAR\u201912), Lecture Notes in Computer Science", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Lightweight relevance filtering for machine-generated resolution problems", "author": ["J. Meng", "L.C. Paulson"], "venue": "Journal of Applied Logic", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Translating higher-order clauses to first-order clauses", "author": ["J. Meng", "L.C. Paulson"], "venue": "Journal of Automated Reasoning", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "PG Tips: A Recommender System for an Interactive Theorem Prover", "author": ["A. Mercer", "A. Bundy", "H. Duncan", "D. Aspinall"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Isabelle/HOL - A Proof Assistant for Higher-Order Logic", "author": ["T. Nipkow", "L.C. Paulson", "M. Wenzel"], "venue": "Lecture Notes in Computer Science 2283,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2002}, {"title": "Three Years of Experience with Sledgehammer, a Practical Link between Automatic and Interactive Theorem Provers", "author": ["L.C. Paulson", "J.C. Blanchette"], "venue": "In: 8th International Workshop on the Implementation of Logics (IWIL\u201910),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "The design and implementation of Vampire", "author": ["A. Riazano", "A. Voronkov"], "venue": "Artificial Intelligence Communications", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2002}, {"title": "Proving Equational Haskell Properties using Automated Theorem Provers", "author": ["D. Ros\u00e9n"], "venue": "Master\u2019s thesis,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "System description: E 0.81", "author": ["S. Schulz"], "venue": "International Joint Conference on Automated Reasoning (IJCAR\u201904), Lecture Notes in Computer Science", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Semantic Graph Kernels for Automated Reasoning", "author": ["E. Tsivtsivadze", "J. Urban", "H. Geuvers", "T. Heskes"], "venue": "SIAM Conference on Data Mining (SDM\u201911),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "MaLARea SG1- Machine Learner for Automated Reasoning with Semantic Guidance", "author": ["J. Urban", "G. Sutcliffe", "P. Pudl\u00e1k", "J. Vyskocil"], "venue": "In: 4th International Joint Conference on Automated Reasoning (IJ- CAR\u201908), Lecture Notes in Computer Science", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Combining superposition, sorts and splitting, pp. 1965\u20132013", "author": ["C. Weidenbach"], "venue": "Handbook of Automated Reasoning,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2001}, {"title": "Survey of Clustering Algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2005}], "referenceMentions": [{"referenceID": 42, "context": "E [51], Vampire [49] and SPASS [57]) and SAT/SMT solvers (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 40, "context": "E [51], Vampire [49] and SPASS [57]) and SAT/SMT solvers (e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 45, "context": "E [51], Vampire [49] and SPASS [57]) and SAT/SMT solvers (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "CVC3 [5], Yices [20] and Z3 [46]) are becoming increasingly fast and efficient [39].", "startOffset": 16, "endOffset": 20}, {"referenceID": 38, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 188, "endOffset": 204}, {"referenceID": 21, "context": "Coq [14], Isabelle/HOL [47], Agda [11], Matita [3] and Mizar [23]) have been enriched with dependent types, (co)inductive types, type classes and now provide rich programming environments [21, 25, 35, 52].", "startOffset": 188, "endOffset": 204}, {"referenceID": 18, "context": "Nevertheless, ITPs have seen major advances in proof automation [22,27,43].", "startOffset": 64, "endOffset": 74}, {"referenceID": 23, "context": "Nevertheless, ITPs have seen major advances in proof automation [22,27,43].", "startOffset": 64, "endOffset": 74}, {"referenceID": 35, "context": "Nevertheless, ITPs have seen major advances in proof automation [22,27,43].", "startOffset": 64, "endOffset": 74}, {"referenceID": 35, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 109, "endOffset": 116}, {"referenceID": 6, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 134, "endOffset": 144}, {"referenceID": 26, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 134, "endOffset": 144}, {"referenceID": 4, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 173, "endOffset": 184}, {"referenceID": 23, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 173, "endOffset": 184}, {"referenceID": 33, "context": "One particular trend is to re-enforce proof automation in ITPs by employing state-of-the-art tools from ATPs [1, 43], SAT/SMT solvers [1, 9, 30] or Computer Algebra systems [7, 27, 38].", "startOffset": 173, "endOffset": 184}, {"referenceID": 39, "context": "One major success of this approach is Sledgehammer [48]: it offers Isabelle/HOL users an option to call for an ATP/SMTgenerated solution [9].", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "One major success of this approach is Sledgehammer [48]: it offers Isabelle/HOL users an option to call for an ATP/SMTgenerated solution [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Namely, the major challenge is a sound and reliable translation between inherently different languages [1, 10, 27, 38, 44].", "startOffset": 103, "endOffset": 122}, {"referenceID": 23, "context": "Namely, the major challenge is a sound and reliable translation between inherently different languages [1, 10, 27, 38, 44].", "startOffset": 103, "endOffset": 122}, {"referenceID": 33, "context": "Namely, the major challenge is a sound and reliable translation between inherently different languages [1, 10, 27, 38, 44].", "startOffset": 103, "endOffset": 122}, {"referenceID": 36, "context": "Namely, the major challenge is a sound and reliable translation between inherently different languages [1, 10, 27, 38, 44].", "startOffset": 103, "endOffset": 122}, {"referenceID": 7, "context": "This especially concerns interpreting outputs from ATPs back into the higher-order environment [1, 10, 44], which we will also call here backward interfacing.", "startOffset": 95, "endOffset": 106}, {"referenceID": 36, "context": "This especially concerns interpreting outputs from ATPs back into the higher-order environment [1, 10, 44], which we will also call here backward interfacing.", "startOffset": 95, "endOffset": 106}, {"referenceID": 12, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 13, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 30, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 32, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 34, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 43, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 44, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 69, "endOffset": 101}, {"referenceID": 15, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 219, "endOffset": 227}, {"referenceID": 32, "context": "\u2022 proof heuristics can be data-mined to improve proof search in ATPs [16, 17, 34, 37, 41, 53, 55, 56]; \u2022 history of successful and unsuccessful proof attempts can be used to inform interactive proof development in ITPs [19, 37].", "startOffset": 219, "endOffset": 227}, {"referenceID": 13, "context": "The notions of a proof may be regarded from different perspectives in ITPs: it may be seen as a transition between the subgoals [17, 55, 56], a combination of applied tactics [19], or \u2014 more traditionally \u2014 a proof tree showing the overall proof strategy [37].", "startOffset": 128, "endOffset": 140}, {"referenceID": 44, "context": "The notions of a proof may be regarded from different perspectives in ITPs: it may be seen as a transition between the subgoals [17, 55, 56], a combination of applied tactics [19], or \u2014 more traditionally \u2014 a proof tree showing the overall proof strategy [37].", "startOffset": 128, "endOffset": 140}, {"referenceID": 15, "context": "The notions of a proof may be regarded from different perspectives in ITPs: it may be seen as a transition between the subgoals [17, 55, 56], a combination of applied tactics [19], or \u2014 more traditionally \u2014 a proof tree showing the overall proof strategy [37].", "startOffset": 175, "endOffset": 179}, {"referenceID": 32, "context": "The notions of a proof may be regarded from different perspectives in ITPs: it may be seen as a transition between the subgoals [17, 55, 56], a combination of applied tactics [19], or \u2014 more traditionally \u2014 a proof tree showing the overall proof strategy [37].", "startOffset": 255, "endOffset": 259}, {"referenceID": 30, "context": "For example, in [34], there was a tool that gathered statistics, but no automated data-mining tools were used; in [37], there was a feature extraction method to data-mine proofs but it was not connected to efficient statistics gathering; in [19], these two were semi-automated.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "For example, in [34], there was a tool that gathered statistics, but no automated data-mining tools were used; in [37], there was a feature extraction method to data-mine proofs but it was not connected to efficient statistics gathering; in [19], these two were semi-automated.", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "For example, in [34], there was a tool that gathered statistics, but no automated data-mining tools were used; in [37], there was a feature extraction method to data-mine proofs but it was not connected to efficient statistics gathering; in [19], these two were semi-automated.", "startOffset": 241, "endOffset": 245}, {"referenceID": 2, "context": "For our experiments, we chose Proof General [4] \u2014 a general-purpose, emacs-based interface for a range of higher-order theorem provers, e.", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "Among them, we have chosen Coq [14] and its SSReflect library [22] for our experiments.", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Although both built upon the same language \u2014 Calculus of Inductive Constructions [15], they have distinct proof styles, analysis of which plays a special role in this paper, see Section 3.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "The second major machine learning interface we explore is Weka [26] \u2013 an open source, general purpose interface to run a wide variety of machine learning algorithms.", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "On the one hand, interfacing may mean translation mechanisms connecting ITPs with other proof automation tools [1, 27, 38, 44]; and on the other hand, it is used as a synonym for user-friendly environment.", "startOffset": 111, "endOffset": 126}, {"referenceID": 33, "context": "On the one hand, interfacing may mean translation mechanisms connecting ITPs with other proof automation tools [1, 27, 38, 44]; and on the other hand, it is used as a synonym for user-friendly environment.", "startOffset": 111, "endOffset": 126}, {"referenceID": 36, "context": "On the one hand, interfacing may mean translation mechanisms connecting ITPs with other proof automation tools [1, 27, 38, 44]; and on the other hand, it is used as a synonym for user-friendly environment.", "startOffset": 111, "endOffset": 126}, {"referenceID": 24, "context": "1It is available at [28], where the reader can download ML4PG, user manual and examples (see also [29]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "1It is available at [28], where the reader can download ML4PG, user manual and examples (see also [29]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "tools to Proof General is less demanding compared to [1, 27, 38, 44].", "startOffset": 53, "endOffset": 68}, {"referenceID": 33, "context": "tools to Proof General is less demanding compared to [1, 27, 38, 44].", "startOffset": 53, "endOffset": 68}, {"referenceID": 36, "context": "tools to Proof General is less demanding compared to [1, 27, 38, 44].", "startOffset": 53, "endOffset": 68}, {"referenceID": 24, "context": "References [28, 29, 41] are specifically devoted to the benchmarks, evaluation and discussion of statistical proof-pattern recognition methods in theorem proving.", "startOffset": 11, "endOffset": 23}, {"referenceID": 25, "context": "References [28, 29, 41] are specifically devoted to the benchmarks, evaluation and discussion of statistical proof-pattern recognition methods in theorem proving.", "startOffset": 11, "endOffset": 23}, {"referenceID": 34, "context": "References [28, 29, 41] are specifically devoted to the benchmarks, evaluation and discussion of statistical proof-pattern recognition methods in theorem proving.", "startOffset": 11, "endOffset": 23}, {"referenceID": 20, "context": "Therefore, we distinguish three levels at which pattern-recognition in ITP proofs can be approached, see also [24]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "This type of feature abstraction has been used for learning the inputs for automatic provers [17, 56] \u2014 which has later been extended to interactive proofs [53].", "startOffset": 93, "endOffset": 101}, {"referenceID": 44, "context": "This type of feature abstraction has been used for learning the inputs for automatic provers [17, 56] \u2014 which has later been extended to interactive proofs [53].", "startOffset": 93, "endOffset": 101}, {"referenceID": 43, "context": "This type of feature abstraction has been used for learning the inputs for automatic provers [17, 56] \u2014 which has later been extended to interactive proofs [53].", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": "Previous work on learning proof strategies [19, 31, 32] has taken this approach.", "startOffset": 43, "endOffset": 55}, {"referenceID": 27, "context": "Previous work on learning proof strategies [19, 31, 32] has taken this approach.", "startOffset": 43, "endOffset": 55}, {"referenceID": 28, "context": "Previous work on learning proof strategies [19, 31, 32] has taken this approach.", "startOffset": 43, "endOffset": 55}, {"referenceID": 32, "context": "Finally, there is the level of a proof tree that shows relations between different proof branches and subgoals and gives a better view of the overall proof flow; this approach was tested in [37] using multi-layer neural networks and kernels.", "startOffset": 190, "endOffset": 194}, {"referenceID": 5, "context": "The discovery of statistically significant features in data is a research area of its own in machine learning, known as feature extraction, see [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 34, "context": "2\u201cSparse methods\u201d of machine-learning is an exception to this rule, see [40, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 3, "context": "However, for more complex examples, deciding such embeddings unambiguously during feature-extraction may be difficult, see [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 32, "context": "This is why we developed a method of implicit tracking of proof properties, called the proof trace method; its early variant was used in [37].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "We use the following notation to note when ML4PG gathers the lemma names: EL1 stands for (mul0n, big_nat1 and muln0), EL2 for (big_nat_recr, mulnDr, IH, mulnD1, addn2 and mulnC), EL3 for (exp0n, index_iota subn0 and big1_seq), EL4 for (/andP, muln0 and in_nil), EL5 for (big_mkcond, addn1, mulnDr, muln1, addn2, big_nat_recr, IH, odd2n, odd2n1, addn0, n1square and n2square) and EL6 for (factS, big_add1, IH, big_add1, big_nat_recr and mulnC); the lemmas and libraries can be found in [28].", "startOffset": 485, "endOffset": 489}, {"referenceID": 18, "context": "Although SSReflect is an extension of Coq, this package implements a set of proof tactics designed to support the extensive use of small-scale reflection in formal proofs [22].", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "Where we use notation EL\u2019, EL\u201d and EL\u201d\u2019, ML4PG gathers respectively the lemma names: (mul0n, big_nat1, muln0, big_nat_recr, mulnDr, IH, mulnD1, addn2 and mulnC), (exp0n, /index_iota, subn0, big1_seq, muln0, in_nil, big_mkcond, addn1, mulnDr, muln1, addn2, big_nat_recr, IH, odd2n, odd2n1, addn0, n1square and n2square) and (big_nil, factS, big_add1, IH, big_add1, big_nat_recr and mulnC); the lemmas and libraries can be found in [28].", "startOffset": 430, "endOffset": 434}, {"referenceID": 24, "context": "Where we use notation EL\u2019, EL\u201d and EL\u201d\u2019, ML4PG gathers respectively the lemma names: (mul0n, big_nat1 and muln0), (big_nat_recr, mulnDr, IH, mulnDl, addn2 and mulnC) and (factS, big_add1, IH, big_add1, big_nat_recr and mulnC); the lemmas and libraries can be found in [28].", "startOffset": 268, "endOffset": 272}, {"referenceID": 32, "context": "As we explained in [37], the concrete function that ML4PG uses for this purpose may vary, but the numeric conversion must be consistent.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "This differs from the experiments performed in the literature, see [19, 37, 41, 56], where the data-mining of proofs is performed separately from the user interface.", "startOffset": 67, "endOffset": 83}, {"referenceID": 32, "context": "This differs from the experiments performed in the literature, see [19, 37, 41, 56], where the data-mining of proofs is performed separately from the user interface.", "startOffset": 67, "endOffset": 83}, {"referenceID": 34, "context": "This differs from the experiments performed in the literature, see [19, 37, 41, 56], where the data-mining of proofs is performed separately from the user interface.", "startOffset": 67, "endOffset": 83}, {"referenceID": 44, "context": "This differs from the experiments performed in the literature, see [19, 37, 41, 56], where the data-mining of proofs is performed separately from the user interface.", "startOffset": 67, "endOffset": 83}, {"referenceID": 5, "context": "We connected ML4PG only to clustering algorithms [8] \u2014 a family of unsupervised learning methods.", "startOffset": 49, "endOffset": 52}, {"referenceID": 46, "context": "In case of MATLAB, the algorithms included in ML4PG are the two most popular methods for clustering: K-means and Gaussian [58].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "To improve the accuracy of the clustering algorithms, a technique called Principal Components Analysis (PCA) [33] is applied.", "startOffset": 109, "endOffset": 113}, {"referenceID": 46, "context": "The application of techniques like PCA, known in general as dimensionality reduction procedures [58], is recommended when dealing with feature vectors whose size is higher than 15 \u2014 as in our case.", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "ATPs) is a challenging task, see [1,27,38,44].", "startOffset": 33, "endOffset": 45}, {"referenceID": 33, "context": "ATPs) is a challenging task, see [1,27,38,44].", "startOffset": 33, "endOffset": 45}, {"referenceID": 36, "context": "ATPs) is a challenging task, see [1,27,38,44].", "startOffset": 33, "endOffset": 45}, {"referenceID": 7, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "ber of clusters, [58].", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "This is the case of IsaPlanner [18],", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "a generic framework for proof planning that integrates techniques like rippling [6] in the interactive theorem prover Isabelle.", "startOffset": 80, "endOffset": 83}, {"referenceID": 37, "context": "Also using Isabelle as a prover and Proof General as interface, PGtips [45] is a tactic recommender system based on data-mining techniques [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "Also using Isabelle as a prover and Proof General as interface, PGtips [45] is a tactic recommender system based on data-mining techniques [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 41, "context": "Inductive provers like ACL2 [35] and Hip [50] also include symbolic machine learning techniques.", "startOffset": 41, "endOffset": 45}, {"referenceID": 31, "context": "In particular, ACL2 uses these methods for proving termination of programs written in Lisp [36], and Hip implements a theory discovery mechanism [13] for automatically derive and prove properties about functional programs implemented in Haskell.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "In particular, ACL2 uses these methods for proving termination of programs written in Lisp [36], and Hip implements a theory discovery mechanism [13] for automatically derive and prove properties about functional programs implemented in Haskell.", "startOffset": 145, "endOffset": 149}, {"referenceID": 34, "context": "Among other successful statistical tools is the method of statistical proof-premise selection [39\u201341, 53].", "startOffset": 94, "endOffset": 105}, {"referenceID": 43, "context": "Among other successful statistical tools is the method of statistical proof-premise selection [39\u201341, 53].", "startOffset": 94, "endOffset": 105}, {"referenceID": 34, "context": "Statistical machine learning methods are used to tackle this problem in [40, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 18, "context": "Search mechanisms implemented in Coq, such as the Search command of SSReflect [22] and Whelp [2], can find patterns in lemma statements, but ML4PG can discover proof patterns that cannot be found using existing Coq\u2019s search mechanisms, see [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Search mechanisms implemented in Coq, such as the Search command of SSReflect [22] and Whelp [2], can find patterns in lemma statements, but ML4PG can discover proof patterns that cannot be found using existing Coq\u2019s search mechanisms, see [28].", "startOffset": 93, "endOffset": 96}, {"referenceID": 24, "context": "Search mechanisms implemented in Coq, such as the Search command of SSReflect [22] and Whelp [2], can find patterns in lemma statements, but ML4PG can discover proof patterns that cannot be found using existing Coq\u2019s search mechanisms, see [28].", "startOffset": 240, "endOffset": 244}, {"referenceID": 3, "context": "Moving towards symbolic machine-learning, we envisage that new lemmas or strategies could be conceptualised from the proof families obtained with ML4PG using such techniques as Rippling [6] or Theory Exploration [32].", "startOffset": 186, "endOffset": 189}, {"referenceID": 28, "context": "Moving towards symbolic machine-learning, we envisage that new lemmas or strategies could be conceptualised from the proof families obtained with ML4PG using such techniques as Rippling [6] or Theory Exploration [32].", "startOffset": 212, "endOffset": 216}, {"referenceID": 9, "context": "We are grateful to the anonymous referees for their comments and suggestions; and the following individuals and research groups for inspiring discussions on the topics of machine learning in automated and interactive theorem proving: J Strother Moore, members of the AI4FM project Alan Bundy, Cliff Jones, Leo Freitas, Ewen Maclean; and participants of Dagstuhl seminar AI meets Formal Software Development [12].", "startOffset": 407, "endOffset": 411}], "year": 2013, "abstractText": "We present ML4PG \u2014 a machine learning extension for Proof General. It allows users to gather proof statistics related to shapes of goals, sequences of applied tactics, and proof tree structures from the libraries of interactive higher-order proofs written in Coq and SSReflect. The gathered data is clustered using the state-of-the-art machine learning algorithms available in MATLAB and Weka. ML4PG provides automated interfacing between Proof General and MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints in the process of interactive proof development.", "creator": "LaTeX with hyperref package"}}}