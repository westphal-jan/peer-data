{"id": "1212.6922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2012", "title": "Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems", "abstract": "The most popular model for artificial neural networks is a multilayer perceptron (MLP), as it is able to perform classification tasks with considerable success. However, due to the complexity of the MLP structure and also problems such as local minima traps, overfitting and weight interference, it is difficult to avoid these problems by removing the hidden layers. In this paper, the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using a single-layer architecture and to propose an optimization of the Artificial Bee Colony (ABC) for FLNN training is presented. The proposed technique is intended to provide a better learning scheme for a classifier to obtain more accurate classification results.", "histories": [["v1", "Mon, 31 Dec 2012 16:40:50 GMT  (233kb)", "http://arxiv.org/abs/1212.6922v1", "6 pages, 3 figures, 4 tables"]], "COMMENTS": "6 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yana mazwin mohmad hassim", "rozaida ghazali"], "accepted": false, "id": "1212.6922"}, "pdf": {"name": "1212.6922.pdf", "metadata": {"source": "CRF", "title": "Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems", "authors": ["Yana Mazwin Mohmad Hassim", "Rozaida Ghazali"], "emails": [], "sections": [{"heading": null, "text": "a non-linear separable pattern. The most popular artificial neural networks model is a Multilayer Perceptron (MLP) as is able to perform classification task with significant success. However due to the complexity of MLP structure and also problems such as local minima trapping, over fitting and weight interference have made neural network training difficult. Thus, the easy way to avoid these problems is to remove the hidden layers. This paper presents the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using single layer architecture and propose an Artificial Bee Colony (ABC) optimization for training the FLNN. The proposed technique is expected to provide better learning scheme for a classifier in order to get more accurate classification result.\nIndex Terms\u2014 Neural Networks, Functional Link Neural Network, Learning, Artificial Bee Colony.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}, {"heading": "1 INTRODUCTION", "text": "RTIFICIAL Neural Networks have been known to be successfully applied to a variety of real world classification tasks especially in industry, business\nand science [1, 2]. Artificial Neural Networks (ANNs) especially the Multilayer Perceptron (MLP) are capable of generating complex mapping between the input and the output space in performing arbitrarily complex nonlinear decision boundaries. For a classification task, ANNs needs to be \u201ctrained\u201d for the network to be able to produce the desired input-output mapping. In training phase, a set of example data are presented to the network and the connection weights of the network are adjusted by using a learning algorithm. The purpose of the weights adjustment is to enable the network to \u201clearn\u201d so that network would adapt to the given training data.\nThe most common architecture of ANNs is the multilayer feedforward network (MLP). MLP utilize a supervised learning technique called Backpropagation for training the network. However, due to its multi-layered structure, the training speeds are typically much slower as compared to other single layer feedforward networks [3]. Problems such as local minima trapping, overfitting and weight interference also make the network training in MLP become challenging [4]. Hence, Pao [5] has introduce an alternative approach named Functional Link Neural Network (FLNN) in avoiding these problems. This approach removes the hidden layer from the ANN architecture to help in reducing the neural architectural complexity and provides them with an enhancement rep-\nresentation of input nodes for the network to be able to perfom a non-linear separable classification task[5, 6].\nIn this paper, we describe an overview of FLNN and the proposed Artificial Bee Colony (ABC) as learning algorithm in order to achieve better classification abilility. The rest of this paper is organized as follows: A background and related work regarding to the Multilayer Perceptron, FLNN and Population-based optimization technique are given in section 2. The proposed Populationbased optimization for training the FLNN is detailed in section 3. The simulation result of the proposed learning scheme is presented in section 4. Finally, the paper is concluded in section 5."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 Artificial Neural Network", "text": "Artificial Neural Networks (ANNs) are information processing model inspired by the way of human brain processes information. ANNs required knowledge through a learning process while the interneuron connection strength known as synaptic weights are used to store knowledge [7]. Therefore with these abilities, Neural Networks provides a suitable solution for pattern recognition or data classification problems.\nOne of the best known types of Neural Networks is the Multilayer Perceptron (MLP). It has one or more hidden layers in between the input and the output layer. Figure 1 illustrates the layout of MLP with single hidden layer. The function of hidden neurons is to provide the ANNs with the ability to handle non-linear input-output mapping. By adding one more hidden layer, the network is able to extract higher order statistics, which is particularly valuable when the size of the input layer is large [7]. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2022 Yana Mazwin Mohmad Hassim is with the Universiti Tun Hussein Onn Malaysia, 86400 Parit Raja, Batu Pahat, Johor, Malaysia.\n\u2022 Rozaida Ghazali is with the Universiti Tun Hussein Onn Malaysia, 86400 Parit Raja, Batu Pahat, Johor, Malaysia.\nA\nThe MLP networks are trained by adjusting the weight of connection between neurons in each layer. For training the network, MLP utilize a supervised learning technique called Backpropagation, in which the network is provided with examples of the inputs and desired outputs to be computed, and then the error (difference between actual and expected results) will be calculated. Figure 1, depicted an example of MLP with Backpropagation. With this architecture, the neurons are organized in layers and their signals are sent \u201cforward\u201d while the calculated errors are then propagated backwards. The idea of the backpropagation algorithm is to reduced error, until the networks learned the training data. The training began with random weights, and the goal is to adjust them until the minimal error is achieved."}, {"heading": "2.2 Higher Order Neural Network", "text": "Higher order Neural Networks (HONNs) is a different type of neural network with the presence of expanded input space in it single layer feed-forward architecture. HONNs contain summing unit and product units that multiply their inputs. These high order terms or product units can increase the information capacity for the input features and provides nonlinear decision boundaries to give a better classification capability than the linear neuron [8]. A major advantage of HONNs is that only one layer of trainable weight is needed to achieve nonlinear separable, unlike the typical MLP or feed-forward neural network [9].\nAlthough most neural networks models share a common goal in performing functional mapping, different network architecture may vary significantly in their ability in handle different types of problems. For some tasks, higher order architecture of some of the inputs or activations may be appropriate to help form good representation for solving the problems. HONNs are needed because ordinary feed-forward network like MLP cannot avoid the problem of slow learning, especially when involving highly complex nonlinear problems [10]."}, {"heading": "2.2.1 Functional Link Neural Network", "text": "Functional Link Neural Network (FLNN) is a class of Higher Order Neural Networks (HONNs) that utilize higher combination of its inputs [5, 6]. It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24]. In this paper, we would discuss on the FLNN for the classification task. FLNN is much more modest than MLP since it has a single-layer network compared to the MLP but still is able to handle a non-linear separable classification task. The FLNN architecture is basically a flat network without any hidden layer which has make the learning algorithm used in the network less complicated [9]. In FLNN, the input vector is extended with a suitably enhanced representation of the input nodes, thereby artificially increasing the dimension of the input space [5, 6].\nOur focused on this work is on Functional link neural networks with generic basis architecture. This architecture uses a tensor representation. In tensor model the input features of x for example xi, can be enhance into xixj, xixjxk and more with condition of i\u2264 j\u2264 k. The enhanced input features from the input layer are fed into the network and the output node s, in the output layer calculates the weighted sum of inputs and passes it through activation function to produce network output, y. Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task. Figure 2 depicts the functional link neural network structure up to second order with 3 inputs. The first order consist of the 3 inputs x1, x2 and x3, while the second order of the network is the extended input based on the product unit x1x2, x1x3, and x2x3. The learning part of this architecture on the other hand, consists of a standard Backpropagation as the training algorithm.\nIn most previous researches, the learning algorithm used for training the FLNN is the Backpropagation (BP) [4]. Table I below summarizes recent studies on FLNN for classification.\nEven though BP is the mostly used algorithm in training the FLNN, the algorithm however, has several limitations. First, it is easily get trapped in local minima especially for those non-linearly separable classification problems. Second, the convergence speed of the BP learning is too slow even if the learning goal can be achieved. Third, the convergence behavior of the BP algorithm depends on the choices of initial values of the network connection weights as well as the parameters in the algorithm such as the learning rate and momentum [4]. For these reasons, a further investigation to improve a learning algorithm used in tuning the learnable weights in FLNN is desired."}, {"heading": "2.3 Artificial Bee Colony", "text": "The Artificial Bees Algorithm is an optimization tool, which provides a population-based search procedure [30]. The ABC algorithm simulates the intelligent foraging behavior of a honey bee swarm for solving multidimensional and multimodal optimization problem [31]. In population-based search procedure, each individual population called foods positions are modified by the artificial bees while the bee\u2019s aim is to discover the places of food sources with high nectar amount and finally the one with the highest nectar.\nIn this model, the colony of artificial bees consists of three groups: which are employed bees, onlookers and scouts [32]. For each food source there is only one artificial employed bee. The number of employed bees in the colony is equal to the number of food sources around the hive. Employed bees go to their food source and come back to hive and with three information regarding the food source: 1) the direction 2) its distance from the hive and 3) the fitness and then perform waggle dance to let the colony evaluate the information. Onlookers watch the dances of employed bees and choose food sources depending on dances. After waggle dancing on the dance floor, the dancer goes back to the food source with follower bees that were waiting inside the hive. This foraging process is called local search method as the method of choosing the food source is depend on the on the experience of the employed bees and their nest mates [31]. The employed bee whose food source has been abandoned becomes a scout and starts to search for finding a new\nfood source randomly without using experience. If the nectar amount of a new source is higher than that of the previous one in their memory, they memorize the new position and forget the previous one [31]. This exploration managed by scout bees is called global search methods.\nSeveral studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem. As in classification task in data mining, ABC algorithm also provide a good performance in gathering data into classes [34]. Hence motivated by these studies, the ABC algorithm is utilized in this work as an optimization tool to optimize FLNN learning for solving classification problem."}, {"heading": "3 PROPOSED TRAINING SCHEME", "text": "Inspired by the robustness and flexibility offered by the population-based optimization algorithm, we proposed the implementation of the ABC algorithm as the learning scheme to overcome the disadvantages caused by backpropagation in the FLNN training. The proposed flowchart is presented in figure 4. In the initial process, the FLNN architecture (weight and bias) is transformed into objective function along with the training dataset. This objective function will then be fed to the ABC algorithm in order to search for the optimal weight parameters. The weight changes are then tuned by the ABC algorithm based on the error calculation (difference between actual and expected results).\nBased on the ABC algorithm, each bee represents the solutions with a particular set of weight vector. The ABC algorithm [35] used for training the FLNN is summarized as follow: 1. Cycle 0: 2. Initialize a population of scout bee with random solu-\ntion xi , i = 1,2, .. SN 3. evaluate fitness of the population a. initialize weight and bias for the FLNN 4. cycle 1: while Maximum cycle not reached,\nrepeat step 5- step 11 5. form new population vi for the employed bees using:\nwhere k is a solution in the neighbourhood of i, \u03a6 is a random number in the range [-1,1] and evaluate them.\na. Form new weight and bias for FLNN 6. Calculate fitness function on new population\n7. Apply the greedy selection process between xij and vij 8. Calculate the probability values pi for the solutions xi\nusing:\n9. If the onlookers are distributed a. Go to step 11 10. else a. Repeate step 5 until step 8 for onlookers. 11. Apply the greedy selection process for onlookers, vi 12. Determine the abandoned solution for the scout, if\nexists, and replace it with a new randomly produced solution xi using:\n13. Memorize the best solution a. Update new weight and bias for the FLNN 14. cycle=cycle+1 15. Stop when cycle = Maximum cycle 16. Save updated FLNN new weight and bias."}, {"heading": "4 SIMULATION RESULT", "text": "In order to evaluate the performance of the proposed learning scheme FLNN-ABC for classification problem, simulation experiments were carried out on a 2.30 GHz Core i5-2410M Intel CPU with 8.0 GB RAM in a 64-bit Operating System. The comparison of standard BP training and ABC algorithms is discussed based on the simulation results implemented in Matlab 2010b. In this work we considered 4 benchmark classification problems, Breast Cancer Wisconsin, PIMA Indian Diabetes and BUPA Liver Disorder.\nDuring the experiment, simulations were performed on the training of MLP architecture with Backpropagation\nalgorithm (MLP-BP), second order FLNN architecture with Backpropagation algorithm (2ndFLNN-BP) and second order FLNN architecture with ABC algorithm (2ndFLNN-ABC) as their learning scheme. The best training accuracy for every benchmark problems is taken out from these simulations. The Learning rate and momentum used for both MLP-BP and 2ndFLNN-BP were 0.3 and 0.7 with the maximum of 1000 epoch and minimum error=0.001 as for the stopping criteria. Parameters setup for the 2ndFLNN-ABC however, only involved the setting up of stopping criteria of maximum 100 cycles and minimum error=0.001. The activation function used for the network output for both MLP and 2ndFLNN is Tangent Hyperbolic sigmoid function. Table 2 below summarized the parameters considered in this simulation.\nThe reason of conduction a simulation on MLP and FLNN architectures is to provide a comparison on neural complexity between standard Artificial Neural Network and Higher Order Neural Network. In this simulation we used a single hidden layer of MLP with the numbers of hidden nodes equal to it input features while for the FLNN architecture we used a second order input enhancement (2ndFLNN) to provide a nonlinear inputoutput mapping capability. The neural complexity refers to the numbers of trainable parameters (weight and bias) needed to implement good approximation in each neural network architecture. The less numbers of parameters indicate that the network required less computational load as there are small numbers of weight and bias to be updated at every epoch or cycle and thus led to a faster convergence. Table 3 below shows the comparison of network complexity for each benchmark problems.\nTen trials were performed on each simulation of the MLP-BP, 2ndFLNN-BP and 2ndFLNN-ABC with the best training accuracy result is taken out from these 10 trials. In order to generate the Training and Test sets, each datasets were randomly divided into two equal sets (1st-Fold and 2nd-Fold). Each of these two sets was alternatively used either as a Training set or as a Test set. The average values of each datasets result were then used for comparison. Table 4 below, presents the simulation result of MLP-BP, 2ndFLNN-BP and 2ndFLNN-ABC architectures. It is shown that, the implementation of ABC as the training scheme for FLNN-ABC provides better accuracy on Test set compared to FLNN-BP and MLP-BP."}, {"heading": "5 CONCLUSION", "text": "In this work, we evaluated the FLNN-ABC model for the task of pattern classification of 2-class classification problems. The experiment has demonstrated that FLNN-ABC performs the classification task quite well. For the case of Cancer, PIMA and BUPA the simulation result shows that the proposed ABC algorithm can successfully train the FLNN for solving classification problems with better accuracy rate on unseen data. The importance of this research work is to provide an alternative training scheme for training the Functional link Neural Network instead of standard BP learning algorithm."}], "references": [{"title": "Neural networks for classification: a survey", "author": ["G.P. Zhang"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, vol. 30, pp. 451-462, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Artificial neural networks classification and clustering of methodologies and applications \u2013 literature analysis from 1995 to 2005", "author": ["S.-H. Liao", "C.-H. Wen"], "venue": "Expert Systems with Applications, vol. 32, pp. 1-11, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A functional link artificial neural network for adaptive channel equalization", "author": ["J.C. Patra", "R.N. Pal"], "venue": "Signal Processing, vol. 43, pp. 181- 195, 1995.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "A comprehensive survey on functional link neural networks and an adaptive PSO\u2013BP learning for CFLNN", "author": ["S. Dehuri", "S.-B. Cho"], "venue": "Neural Computing & Applications vol. 19, pp. 187-205, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Functional-link net computing: theory, system architecture, and functionalities", "author": ["Y.H. Pao", "Y. Takefuji"], "venue": "Computer, vol. 25, pp. 76-79, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive pattern recognition and neural networks", "author": ["Y.H. Pao"], "venue": "1989.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "The Knowledge Engineering Review vol. 13, pp. 409-412, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A New Higher-order Binary-input Neural Unit: Learning and Generalizing Effectively via Using Minimal Number of Monomials ", "author": ["E. Sahin"], "venue": "Master, Middle East Technical University of Ankara,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Functional Link Artificial Neural Network for Classification Task in Data Mining", "author": ["B.B. Misra", "S. Dehuri"], "venue": "Journal of Computer Science, vol. 3, pp. 948-955, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Regression neural network for error correction in foreign exchange forecasting and trading", "author": ["A.-S. Chen", "M.T. Leung"], "venue": "Computers &amp; Operations Research, vol. 31, pp. 1049-1068, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonlinear dynamic system identification using Legendre neural network", "author": ["J.C. Patra", "C. Bornand"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on, 2010, pp. 1-7.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks", "author": ["J.C. Patra", "A.C. Kot"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 32, pp. 505-511, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "System Identification Using Optimally Designed  Functional Link Networks via a Fast Orthogonal Search Technique", "author": ["H.M. Abbas"], "venue": "JOURNAL OF COMPUTERS, vol. 4, FEBRUARY 2009 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual particle optimized functional link neural network for real time identification of nonlinear dynamic systems", "author": ["S. Emrani"], "venue": "Industrial Electronics and Applications (ICIEA), 2010 the 5th IEEE Conference on, 2010, pp. 35-40.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved Identification of Nonlinear MIMO Plants using New Hybrid FLANN-AIS Model", "author": ["S.J. Nanda"], "venue": "Advance Computing Conference, 2009. IACC 2009. IEEE International, 2009, pp. 141-146.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Application of functional link neural network to HVAC thermal dynamic system identification", "author": ["J. Teeter", "C. Mo-Yuen"], "venue": "Industrial Electronics, IEEE Transactions on, vol. 45, pp. 170-176, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A combined neural network approach for texture classification", "author": ["P.P. Raghu"], "venue": "Neural Networks, vol. 8, pp. 975-987, 1995.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "A comparative study of three artificial neural networks for the detection and classification of gear faults ", "author": ["I.-A. Abu-Mahfouz"], "venue": "International Journal of General Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Image classification in remote sensing using functional link neural networks", "author": ["L.M. Liu"], "venue": "Image Analysis and Interpretation, 1994., Proceedings of the IEEE Southwest Symposium on, 1994, pp. 54-58.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Evolutionarily optimized features in functional link neural network for classification", "author": ["S. Dehuri", "S.-B. Cho"], "venue": "Expert Systems with Applications, vol. 37, pp. 4379-4391, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "The functional link net in structural pattern recognition", "author": ["M. Klaseen", "Y.H. Pao"], "venue": "TENCON 90. 1990 IEEE Region 10 Conference on Computer and Communication Systems, 1990, pp. 567-571 vol.2.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Unconstrained word-based approach for off-line script recognition using density-based random-vector functional-link net", "author": ["G.H. Park", "Y.H. Pao"], "venue": "Neurocomputing, vol. 31, pp. 45-65, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Development and performance evaluation of FLANN based model for forecasting of stock markets", "author": ["R. Majhi"], "venue": "Expert Systems with Applications, vol. 36, pp. 6800-6808, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Dynamic Ridge Polynomial Neural Network: Forecasting the univariate non-stationary and stationary trading signals", "author": ["R. Ghazali"], "venue": "Expert Systems with Applications, vol. 38, pp. 3765-3776, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Pattern classification with Chebyshev neural network", "author": ["A. Namatame", "N. Veda"], "venue": "International Jounal of Neural Network, vol. 3, pp. 23\u2013 31, 1992.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Finding functional links for neural networks by evolutionary computation", "author": ["S. Haring", "J. Kok"], "venue": "In: Van de Merckt Tet al (eds) BENELEARN1995, proceedings of the fifth Belgian\u2013Dutch conference on machine learning, Brussels, Belgium, 1995.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature selection for neural networks through functional links found by evolutionary computation", "author": ["S. Haring"], "venue": "In: ILiu X et al (eds) Adavnces in intelligent data analysis (IDA-97). LNCS 1280, pp. 199\u2013210, 1997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolution of functional link networks", "author": ["A. Sierra"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 5, pp. 54-65, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Genetic Feature Selection for Optimal Functional Link Artificial Neural Network in Classification", "author": ["S. Dehuri"], "venue": "presented at the Proceedings of the 9th International Conference on Intelligent Data Engineering and Automated Learning, Daejeon, South Korea, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "The Bees Algorithm", "author": ["D. Pham"], "venue": "Manufacturing Engineering Centre, Cardiff University, UK,2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "An Idea Based on Honey Bee Swarm for Numerical Optimization", "author": ["D. Karaboga"], "venue": "Erciyes University, Engineering Faculty, Computer Science Department, Kayseri/Turkiye2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "On the performance of artificial bee colony (ABC) algorithm", "author": ["D. Karaboga", "B. Basturk"], "venue": "Elsevier Applied Soft Computing, vol. 8, pp. 687-697, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "A modified Artificial Bee Colony algorithm for real-parameter optimization", "author": ["B. Akay", "D. Karaboga"], "venue": "Information Sciences, vol. In Press, Corrected Proof, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel clustering approach: Artificial Bee Colony (ABC) algorithm", "author": ["D. Karaboga", "C. Ozturk"], "venue": "Applied Soft Computing, vol. 11, pp. 652-657, 2011.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "RTIFICIAL Neural Networks have been known to be successfully applied to a variety of real world classification tasks especially in industry, business and science [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 1, "context": "RTIFICIAL Neural Networks have been known to be successfully applied to a variety of real world classification tasks especially in industry, business and science [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 2, "context": "However, due to its multi-layered structure, the training speeds are typically much slower as compared to other single layer feedforward networks [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "Problems such as local minima trapping, overfitting and weight interference also make the network training in MLP become challenging [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Hence, Pao [5] has introduce an alternative approach named Functional Link Neural Network (FLNN) in avoiding these problems.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "This approach removes the hidden layer from the ANN architecture to help in reducing the neural architectural complexity and provides them with an enhancement representation of input nodes for the network to be able to perfom a non-linear separable classification task[5, 6].", "startOffset": 268, "endOffset": 274}, {"referenceID": 5, "context": "This approach removes the hidden layer from the ANN architecture to help in reducing the neural architectural complexity and provides them with an enhancement representation of input nodes for the network to be able to perfom a non-linear separable classification task[5, 6].", "startOffset": 268, "endOffset": 274}, {"referenceID": 6, "context": "ANNs required knowledge through a learning process while the interneuron connection strength known as synaptic weights are used to store knowledge [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "By adding one more hidden layer, the network is able to extract higher order statistics, which is particularly valuable when the size of the input layer is large [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "These high order terms or product units can increase the information capacity for the input features and provides nonlinear decision boundaries to give a better classification capability than the linear neuron [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 8, "context": "A major advantage of HONNs is that only one layer of trainable weight is needed to achieve nonlinear separable, unlike the typical MLP or feed-forward neural network [9].", "startOffset": 166, "endOffset": 169}, {"referenceID": 9, "context": "HONNs are needed because ordinary feed-forward network like MLP cannot avoid the problem of slow learning, especially when involving highly complex nonlinear problems [10].", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "1 Functional Link Neural Network Functional Link Neural Network (FLNN) is a class of Higher Order Neural Networks (HONNs) that utilize higher combination of its inputs [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "1 Functional Link Neural Network Functional Link Neural Network (FLNN) is a class of Higher Order Neural Networks (HONNs) that utilize higher combination of its inputs [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 11, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 12, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 14, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 2, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 138, "endOffset": 141}, {"referenceID": 16, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 17, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 18, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 19, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 20, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 187, "endOffset": 195}, {"referenceID": 21, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 187, "endOffset": 195}, {"referenceID": 22, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 211, "endOffset": 219}, {"referenceID": 23, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 211, "endOffset": 219}, {"referenceID": 8, "context": "The FLNN architecture is basically a flat network without any hidden layer which has make the learning algorithm used in the network less complicated [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "In FLNN, the input vector is extended with a suitably enhanced representation of the input nodes, thereby artificially increasing the dimension of the input space [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "In FLNN, the input vector is extended with a suitably enhanced representation of the input nodes, thereby artificially increasing the dimension of the input space [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "In most previous researches, the learning algorithm used for training the FLNN is the Backpropagation (BP) [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "TABLE 1 previous research on FLNN Training[4]", "startOffset": 42, "endOffset": 45}, {"referenceID": 25, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 173, "endOffset": 177}, {"referenceID": 3, "context": "Third, the convergence behavior of the BP algorithm depends on the choices of initial values of the network connection weights as well as the parameters in the algorithm such as the learning rate and momentum [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 29, "context": "3 Artificial Bee Colony The Artificial Bees Algorithm is an optimization tool, which provides a population-based search procedure [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "The ABC algorithm simulates the intelligent foraging behavior of a honey bee swarm for solving multidimensional and multimodal optimization problem [31].", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "In this model, the colony of artificial bees consists of three groups: which are employed bees, onlookers and scouts [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "This foraging process is called local search method as the method of choosing the food source is depend on the on the experience of the employed bees and their nest mates [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 30, "context": "If the nectar amount of a new source is higher than that of the previous one in their memory, they memorize the new position and forget the previous one [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 30, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 31, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 32, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 33, "context": "As in classification task in data mining, ABC algorithm also provide a good performance in gathering data into classes [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "where k is a solution in the neighbourhood of i, \u03a6 is a random number in the range [-1,1] and evaluate them.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 31, "endOffset": 39}], "year": 2012, "abstractText": "Artificial Neural Networks have emerged as an important tool for classification and have been widely used to classify a non-linear separable pattern. The most popular artificial neural networks model is a Multilayer Perceptron (MLP) as is able to perform classification task with significant success. However due to the complexity of MLP structure and also problems such as local minima trapping, over fitting and weight interference have made neural network training difficult. Thus, the easy way to avoid these problems is to remove the hidden layers. This paper presents the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using single layer architecture and propose an Artificial Bee Colony (ABC) optimization for training the FLNN. The proposed technique is expected to provide better learning scheme for a classifier in order to get more accurate classification result.", "creator": "PScript5.dll Version 5.2.2"}}}