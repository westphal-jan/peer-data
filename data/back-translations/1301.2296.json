{"id": "1301.2296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "The Factored Frontier Algorithm for Approximate Inference in DBNs", "abstract": "The Factored Frontier (FF) algorithm is a simple approximate inference algorithm for Dynamic Bavarian Networks (DBNs). It is very similar to the fully factorized version of the Boyen-Koller (BK) algorithm, but instead of performing an exact update at each step after the marginalization (projection), it always works with factorized distributions. Therefore, it can be applied to models where the exact step is insoluble. We then empirically show that FF corresponds to the (oneiteration) loopy faith propagation (LBP) on the original DBN and that BK corresponds to (an iteration of) LBP on a DBN where some of the nodes converge. We then empirically show that by iterating LBP the accuracy of both FF and BK can be improved.", "histories": [["v1", "Thu, 10 Jan 2013 16:25:29 GMT  (1065kb)", "http://arxiv.org/abs/1301.2296v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kevin murphy", "yair weiss"], "accepted": false, "id": "1301.2296"}, "pdf": {"name": "1301.2296.pdf", "metadata": {"source": "CRF", "title": "The Factored Frontier Algorithm for Approximate Inference in DBNs", "authors": ["Kevin Murphy", "Yair Weiss"], "emails": ["}@cs.berkeley.edu"], "sections": null, "references": [{"title": "Approximate learning of dy\u00ad namic models", "author": ["X. Boyen", "D. Koller"], "venue": "In NIPS-11,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Tractable inference for com\u00ad plex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In UAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Probabilistic Networks and Expert Sys\u00ad", "author": ["R.G. Cowell", "A.P. Dawid", "S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Mini-buckets: a general scheme of ap\u00ad proximating approximations in automated reasoning", "author": ["R. Dechter"], "venue": "In IJCAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "On the fixed points of the max-product algorithm", "author": ["W. Freeman", "Y. Weiss"], "venue": "IEEE Trans. on Info. Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Turbo factor analysis", "author": ["B.J. Frey"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Factorial hidden Markov models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Ped\u00ad ersen. An expert system for control of waste water treatment - a pilot project", "author": ["F.V. Jensen", "U. Kjaerulff", "K.G. Olesen"], "venue": "Technical report, Univ. Aalborg, Judex Datasystemer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Triangulation of graphs - algorithms giving small total state space", "author": ["U. Kjaerulff"], "venue": "Technical Report R-", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "A computational scheme for reasoning in dynamic probabilistic networks", "author": ["U. Kjaerulff"], "venue": "In UAI\ufffdB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Modeling freeway traffic with coupled HMMs", "author": ["J. Kwon", "K. Murphy"], "venue": "Technical report, Univ. Califor\u00ad nia, Berkeley,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Turbo decoding as an instance of Pearl's 'belief prop\u00ad agation' algorithm", "author": ["R.J. McEliece", "D.J.C. MacKay", "J.F. Cheng"], "venue": "IEEE J. on Selectred Areas in Comm.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T. Minka"], "venue": "In UAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Loopy belief propagation for approximate inference: an empirical study", "author": ["K. Murphy", "Y. Weiss", "M. Jordan"], "venue": "In UAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Sys\ufffd terns: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kauf\u00ad mann,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}, {"title": "Fusion and propagation with multiple observations in belief networks", "author": ["M. Peat", "R. Shachter"], "venue": "Artifi\ufffd cial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "A tutorial on Hidden Markov Models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proc. of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1989}, {"title": "Prob\u00ad abilistic independence networks for hidden Markov probability models", "author": ["P. Smyth", "D. Beckerman", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Correctness of local probability propagation in graphical models with loops", "author": ["Y. Weiss"], "venue": "Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Correctness of belief propagation in Gaussian graphical models of arbitrary topology", "author": ["Y. Weiss", "W.T. Freeman"], "venue": "In NIPS\ufffd12,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W.T. Freeman", "andY. Weiss"], "venue": "In NIPS-13,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "A forward-backward algorithm for infer\u00ad ence in Bayesian networks and an empirical compar\u00ad ison with HMMs", "author": ["G. Zweig"], "venue": "Master's thesis, Dept. Comp. Sci., U.C. Berkeley,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}], "referenceMentions": [{"referenceID": 17, "context": "The simplest way to perform exact inference in a DBN is to convert the model to an HMM and ap\u00ad ply the forwards-backwards algorithm [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 20, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 19, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 5, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 6, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 12, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 87, "endOffset": 109}, {"referenceID": 15, "context": "In Section 4, we show how both FF and BK are related to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13] , which is the method of applying Pearl's message pass\u00ad ing algorithm [16] to a Bayes net even if it contains (undirected) cycles or loops.", "startOffset": 180, "endOffset": 184}, {"referenceID": 8, "context": "This model is originally from [9], and was modified by [2] to include (discrete) evi\u00ad dence nodes.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "This model is originally from [9], and was modified by [2] to include (discrete) evi\u00ad dence nodes.", "startOffset": 55, "endOffset": 58}, {"referenceID": 17, "context": "We start by reviewing the forwards-backwards (FB) algorithm [18] for HMMs, and then the frontier algo\u00ad rithm [23] for DBNs, since this will form the basis of our generalisation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "Nevertheless, heuristics meth\u00ad ods, such as greedy search [10], often perform as well as exhaustive search using branch and bound [23].", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "A special case of the frontier algorithm, applied to fac\u00ad torial HMMs, was published in Appendix B of [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": ") For regular1 DBNs, the frontier algorithm is equivalent to the junction tree algorithm [3, 11, 19] applied to the \"unrolled\" DEN.", "startOffset": 89, "endOffset": 100}, {"referenceID": 10, "context": ") For regular1 DBNs, the frontier algorithm is equivalent to the junction tree algorithm [3, 11, 19] applied to the \"unrolled\" DEN.", "startOffset": 89, "endOffset": 100}, {"referenceID": 18, "context": ") For regular1 DBNs, the frontier algorithm is equivalent to the junction tree algorithm [3, 11, 19] applied to the \"unrolled\" DEN.", "startOffset": 89, "endOffset": 100}, {"referenceID": 1, "context": "The Boyen-Koller algorithm [2] represents the belief state, O:t = P(Xt!Yt:t), as a product of marginals over C \"clusters\", P(Xt IYI:t) \ufffd TI\ufffd=l P(XfiYl:t), where Xf is a subset of the variables {Xi}.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "We can use a similar method for computing the backward messages in an efficient manner [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 15, "context": "Pearl's belief propagation algorithm [16] is a way of computing exact marginal posterior probabilities in graphs with no undirected cycles (loops).", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "Nevertheless, the out\u00ad standing empirical success of turbo decoding, which has be shown to be equivalent to LBP [13], has cre\u00ad ated great interest in the algorithm.", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "LBP has been empirically shown to work well on sev\u00ad eral kinds of Bayesian networks which are quite differ\u00ad ent from turbo codes [15, 7].", "startOffset": 129, "endOffset": 136}, {"referenceID": 6, "context": "LBP has been empirically shown to work well on sev\u00ad eral kinds of Bayesian networks which are quite differ\u00ad ent from turbo codes [15, 7].", "startOffset": 129, "endOffset": 136}, {"referenceID": 20, "context": "In addition, a number of theoretical results have now been proved for networks in which all nodes are Gaussian [21], for networks in which there is only a single loop [20], and for general networks but using the max-product (Viterbi) version instead of the sum-product (forwards-backwards) ver\u00ad sion of the algorithm [6].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "In addition, a number of theoretical results have now been proved for networks in which all nodes are Gaussian [21], for networks in which there is only a single loop [20], and for general networks but using the max-product (Viterbi) version instead of the sum-product (forwards-backwards) ver\u00ad sion of the algorithm [6].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "In addition, a number of theoretical results have now been proved for networks in which all nodes are Gaussian [21], for networks in which there is only a single loop [20], and for general networks but using the max-product (Viterbi) version instead of the sum-product (forwards-backwards) ver\u00ad sion of the algorithm [6].", "startOffset": 317, "endOffset": 320}, {"referenceID": 16, "context": "In particular, if the D BN is in fact an HMM, then a single FB iteration (2T N message computations) will result in the exact posteriors, whereas it requires T iterations of the decentralized protocol (each iteration comput\u00ad ing 2T N messages in parallel) to reach the same result; hence the centralized algorithm is more efficient [17].", "startOffset": 332, "endOffset": 336}, {"referenceID": 15, "context": "and 1r messages without having to do work which is exponential in the number of parents [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "The equivalence between BK and a single iteration of LBP on the clustered graph allows us to utilize the re\u00ad cent result of Yedidia et al [22] to obtain a free energy for \"iterated\" BK.", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "The analysis of [22] shows that iterated BK can only converge to zero gradient points of the Bethe free energy.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "We used a CHMM model with 10 chains trained on some real freeway traffic data using exact EM [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "We there\u00ad fore used the damping trick described in [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Perhaps the closest is the expecta\u00ad tion propagation algorithm [14] .", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The mini\u00ad bucket algorithm [4] also approximates joint distribu\u00ad tions over collections of nodes as a product of smaller terms; however, this is not an iterative algorithm, and hence cannot correct for erroneous independence as\u00ad sumptions made in the first pass.", "startOffset": 27, "endOffset": 30}], "year": 2011, "abstractText": "The Factored Frontier (FF) algorithm is a simple approximate inference algorithm for Dynamic Bayesian Networks (DBNs). It is very similar to the fully factorized version of the Boyen-Koller (BK) algorithm, but in\u00ad stead of doing an exact update at every step followed by marginalisation (projection), it always works with factored distributions. Hence it can be applied to models for which the exact update step is intractable. We show that FF is equivalent to (one iteration of) loopy belief propagation (LBP) on the origi\u00ad nal DBN, and that BK is equivalent (to one iteration of) LBP on a DBN where we clus\u00ad ter some of the nodes. We then show em\u00ad pirically that by iterating more than once, LBP can improve on the accuracy of both FF and BK. We compare these algorithms on two real-world DBNs: the first is a model of a water treatment plant, and the second is a coupled HMM, used to model freeway traffic.", "creator": "pdftk 1.41 - www.pdftk.com"}}}