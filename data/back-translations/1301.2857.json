{"id": "1301.2857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2013", "title": "SpeedRead: A Fast Named Entity Recognition Pipeline", "abstract": "Online content analysis uses algorithmic methods to identify entities in unstructured text. Machine learning as well as knowledge-based approaches form the basis of contemporary systems for extracting named entities. However, progress in adopting these approaches on a web scale has been hampered by NLP's computing costs via massive text corpora. We present SpeedRead (SR), an entity recognition pipeline that is at least ten times faster than Stanford's NLP pipeline. This pipeline consists of a high-performance Penn Treebank compliant tokenizer that comes close to the state of the art, part-of-speech (POS) tagger, and knowledge-based entity recognition.", "histories": [["v1", "Mon, 14 Jan 2013 04:01:25 GMT  (75kb,D)", "http://arxiv.org/abs/1301.2857v1", "Long paper at COLING 2012"]], "COMMENTS": "Long paper at COLING 2012", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rami al-rfou'", "steven skiena"], "accepted": false, "id": "1301.2857"}, "pdf": {"name": "1301.2857.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Steven Skiena"], "emails": ["skiena}@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "KEYWORDS: Tokenization, Part Of Speech, Named Entity Recognition, NLP pipelines.\nar X\niv :1\n30 1.\n28 57\nv1 [\ncs .C"}, {"heading": "1 Introduction", "text": "Information retrieval (IR) systems rely on text as a main source of data, which is processed using natural language processing (NLP) techniques to extract information and relations. Named entity recognition is essential in information and event-extraction tasks. Since NLP algorithms require computationally expensive operations, the NLP stages of an IR system become the bottleneck with regards to scalability (Pauls and Klein, 2011). Most of the relevant work, conducted by researchers, was limited to small corpora of news and blogs because of the limitation of the available algorithms in terms of speed. Most of the NLP pipelines use previously computed features that are generated by other NLP tasks, which adds computational cost to the overall NLP pipeline. For example, named entity recognition and parsing need POS tags; co-reference resolution requires named entities. In effect, we anticipate lower speed for future tasks.\nA conservative estimate of a sample of the web news and articles can add up to terabytes of text. On such scale, speed makes a huge difference. For example, considering the task of annotating 10 TiBs of text with POS tags and named entities using a 20 CPU cores computer cluster would take at least 4 months using the fastest NLP pipeline available for researchers, our calculations show. Using our proposed NLP pipeline the time is reduced to a week.\nSeveral projects have tried to improve the speed by using code optimization. Figure 1a shows that Stanford POS tagger has improved throughout the years, increasing its speed by more than 10 times between 2006 and 2012. However, the current speed is twice slower than the SENNA POS tagger.\n(a) POS taggers performance. (b) NER taggers performance.\nFigure 1: Performance of NLP pipelines through the years over POS and NER tagging. Stanford POS tagger uses L3W model, its speed in 2006 is slow to be apparent in the graph. Stanford tagger uses CONLL 4 classes model. SENNA pipeline was first released in 2008\nIn this paper, we present a new NLP pipeline, SpeedRead, where we integrate global knowledge extracted from large corpora with machine learning algorithms to achieve high performance. Figures 1a and 1b show that our pipeline is 10 times faster than Stanford pipeline in both tasks: POS tagging and NER tagging. Our design is built on two principles: (1) majority of the words have unique annotations and tagging them is an easy task; (2) the features extracted for the frequent words should be cached for later use by the classifier. Both principles are simple and they show how to bridge the large gap in performance between current systems and what can be achieved.\nOur work makes the following contributions:\nPhase SpeedRead Relative Speed\nTokenization 11.8 POS 11.1 NER 13.9 TOK+POS+NER 18.0\nTable 1: SpeedRead relative speed to Stanford pipeline.\n\u2022 Exposing the performance limitations of the current NLP systems: We show that there is an algorithmic room for improving performance, rather than relying solely on optimizing the code. \u2022 High performance NLP pipeline that supports English tokenization, POS tagging and named entity recognition: Novel design decisions that are not taken by most of the available tools to explore new area of the accuracy-performance space. SpeedRead is available under an open-source license. The code\u2019s organization is simple and it is written in Python for its readability benefits. This makes it easier for others to contribute and hack. \u2022 Techniques to reduce computation needed for sequence tagging tasks: We distinguish between ambiguous and non-ambiguous words. We use the larger copora to calculate the frequent words and their frequent tags. We cache the extracted features of the most frequent words to avoid unnecessary calculations and boost performance.\nFigure 2 shows the design of the SpeedRead pipeline. The first stage is tokenization followed by POS tagging that is used as an essential feature to decide the boundaries of the named entities\u2019 phrases. Once the phrases are detected, a classifier decides to which category these named entities belong to.\nThis paper is structured as follows. In Section 2, we discuss the current NLP pipelines, available to researchers. Section 3 discusses SpeedRead tokenizer\u2019s architecture, speed and accuracy. In Section 4, we discuss the status of the current state-of-art POS taggers and describe SpeedRead new POS tagger. Section 5 describes the architecture SpeedRead\u2019s named entity recognition phase. Finally, in Section 5.2, we discuss the status of the pipeline and the future improvements."}, {"heading": "1.1 Experimental Setup", "text": "All the experiments presented in this paper were conducted on a single machine that has i7 intel 920 processor running on 2.67GHz, the operating system used is Ubuntu 11.10. The time of execution is the sum of {sys, user} periods calculated by the Linux command time. The speeds that are reported are calculated by averaging the execution time of five runs without considering any initialization times."}, {"heading": "2 Related Work", "text": "There are many available natural language processing packages available for researchers under open source licenses or non-commercial ones. However, this section is not meant to review the literature of named entity recognition research as this is already available in (Nadeau and Sekine, 2007). We are trying to discuss the most popular solutions and the ones we think are interesting to present.\nStanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.\nRami\u2019s cat on the mat.\nTokenization\nRami \u2019s cat on the mat .\nPOS Tagging\nRami NNP \u2019s POS cat NN on IN the DT mat NN . . NER Chunking\nRami I \u2019s O cat O on O the O mat O . O\nNER Classification\nRami PER \u2019s O cat O on O the O mat O . O\n1\nFigure 2: SpeadRead named entity recognition pipeline. First, tokenization split the words into basic units to be processed in the later phases. POS tagging identifies to which speech categories words belong to. There are 45 part of speech category, we are mainly interested in nouns. Chunking identifies the borders of phrases that make up the named entities. In the above sentence, the named entity, Rami, is one word phrase. The last stage classifies each phrase to one of four categories; Person, Location, Organization or Miscellaneous.\nThe pipeline is rich in features, flexible for tweaking and supports many natural languages. Despite being written in Java, there are many other programming language bindings that are maintained by the community. The pipeline offers a tokenization, POS tagging, named entity recognition, parsing and co-referencing resolution. The pipeline requirements of memory and computation are non-trivial. To accommodate the various computational resources, the pipeline offers several models for each task that vary in speed, memory consumption and accuracy. In general, to achieve good performance in terms of speed, the user has to increase the memory available to the pipeline to 1-3 GiBs and choose the faster but less accurate models.\nMore recent efforts include SENNA pipeline. Even though it lacks a proper tokenizer, it offers POS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston, 2008) and parsing (Collobert, 2011). The pipeline has simple interface, high speed and small memory footprint (less than 190MiB).\nSENNA builds on the idea of deep learning of extracting useful features from unlabeled text. This unsupervised learning phase is done using auto-encoders and neural networks language models. It allows the pipeline to map words into another space of representation that has lower dimensionality. SENNA maps every word available in its 130 thousand word dictionary to a vector of 50 floating numbers. These vectors are then merged into a sentence structure using convolutional networks. The same architecture is then trained on different tasks using annotated text to generate different classifiers. The big advantage of taking this approach is the lesser amount of engineering that it requires to solve multiple problems.\nNLTK (Bird et al., 2009) is a set of tools and interfaces to other NLP packages. Its simple APIs and good documentation makes it a favorable option for students and researchers. Written in Python, NLTK does not offer great speed or close to state-of-art accuracy with its tools. On the other hand, it is well maintained and has great community support.\nWikipediaMiner (Milne and Witten, 2008) detects conceptual words and named entities; it also disambiguates the word senses. This approach can be modified to detect only the words that represent entities, then using the disambiguated sense, it can decide which class the entity belongs to. Its use of the Wikipedia interlinking information is a good example of the power\nof using knowledge-based systems. Our basic investigation shows that the current system needs large chunks of memory to load all the interlinking graph of Wikipedia and it would be hard to optimize for speed. TAGME (Ferragina and Scaiella, 2010) is extending the work of WikipediaMiner to annotate short snippets of text. They are presenting a new disambiguation system that is faster and more accurate. Their system is much simpler and takes into account the sparseness of the senses and the possible lack of unambiguous senses in short texts.\nStanford and SENNA performed the best in terms of speed and quality in our early investigation. Therefore, we will focus on both of them from now on as good representatives of a wide range of NLP packages."}, {"heading": "3 Tokenizer", "text": "The first task that an NLP pipeline has to deal with is tokenization and sentence segmentation (Webster and Kit, 1992). Tokenization target is to identify tokens in the text. Tokens are the basic units which need not to be processed in the subsequent stages. Part of the complexity of tokenization comes from the fact that the definition of what a token is, depends on the application that is being developed. Punctuation brings another level of ambiguity; commas and periods can play different roles in the text. For example, we do not need to split a number like 1,000.54 into more units whereas we need to split a comma-separated list of words. On the other hand, tokenization is important as it reduces the size of the vocabulary and improves the accuracy of the taggers by producing similar vocabulary to the one used for training.\nAs many NLP tasks\u2019 gold standards are dependent on Penn Treebank(PTB), a corpus of annotated text and parsed sentences taken from Wall Street Journal (WSJ), we opted for their tokenization scheme.\nSearching for good tokenizers, we limited our options to the ones that support Unicode. We believe that Unicode support is essential to any applications that depends on the pipeline. Stanford tokenizer and Ucto (Gompel, 2012) projects offer almost Penn Treebank (PTB) compliant tokenizers plus other variations that are richer in terms of features.\nTable 2 shows that there is a substantial gap in performance between basic white space tokenizer (words are delimited by spaces or tabs and sentences are split by new line characters) and more sophisticated tokenizers as Stanford tokenizer and Ucto. We observed that the Stanford tokenizer is 50 times slower than the baseline (WhiteSpace tokenizer), which motivated us to look at the problem again.\nThe Stanford tokenizer is implemented using JFlex, a Java alternative to Flex. The tokenizer matured over the years by adding more features and modes of operation which makes it harder for us to modify. Ucto uses C++ to compile a list of regular expressions that passes over the text multiple times.\nSpeedRead, like the Stanford tokenizer, uses a lexical analyzer to construct the tokenizer. However, we use different generating engine than the (F)lex family. SpeedRead depends on Quex (Schafer, 2012), a lexical analyzer generator, to generate our tokenizer. Quex makes different trade-off decisions than the usual lex tools when it comes to the tokenizer\u2019s generation time. Quex spends more time optimizing its internal NFA to produce a faster engine. While generating a tokenizer from a normal lex file can take few minutes, Quex takes hours for the same task. However, Quex supports Unicode in multiple ways and has similar description language to lex, but is cleaner and more powerful. The extensive multiple mode support makes\nTokenizer Word/Second Relative Speed Ucto 185,500 0.8 PTB Sed Script 214220 0.96 Stanford 222,176 1.0 SpeedRead 2,626,183 11.8 WhiteSpace 11,130,048 50.0\nTable 2: Speed of different tokenizers measured as word/second; Every tokenizer generates different number of tokens. For consistency, the original words count before tokenization used to calculate the speed. Words count is calculated using linux command wc. Execution time includes both tokenization and sentence segmentation times with the exception that the original PTB Sed Script does not do sentence segmentation. Ucto\u2019s default configuration is used. Stanford tokenizer runs with strict PTB flag turned on.\nit easy to write the lexical rules in understandable and organized way. All of that results in a fast C implementation of a Penn Treebank compliant tokenizer as Table 2 shows.\nAs a design decision, we did not support some features which we believe will not affect the accuracy of the tokenizer. Table 3 shows the features which are not implemented. While some of the features are easy to add as supporting contractions, others, involving abbreviations especially U.S., prove to be complex (Gillick, 2009).\nTable 4 shows that the accuracy of our tokenizer is Penn Treebank compliant, despite the missing features. Moreover, running SpeedRead and Stanford tokenizers over Reuters RCV1 corpus results in approximately 214, 215 million tokens consecutively."}, {"heading": "3.1 Sentence Segmentation", "text": "While PTB offers a set of rules for tokenization, their tokenizer assumes that the sentences are already segmented, which is done manually. SpeedRead\u2019s sentence segmentation uses the same rules that Stanford tokenizer uses. For instance, a period is an end of a sentence unless it is part of an acronym or abbreviation. The list of rules to detect those acronyms and abbreviations are taken from the Stanford tokenizer. Any quotations or brackets, that follow the end of the sentence, will be part of that sentence. Running SpeedRead\u2019s sentence segmentation on Reuters RCV1 generated 7.8 million sentences, while Stanford tokenizer generated 8.2 million sentences.\nTokenizer Accuracy PTB Sed Script 100.0% Stanford tokenizer 99.7% SpeedRead 99.0% White Space 0.0%\nTable 4: Accuracy of the tokenizers over the first 1000 sentence in the Penn Treebank. The gold standard was created by getting the tokenized text from the parse trees and manually segment the original text into sentences according to the parse trees. Errors in differentiating between starting and ending quotations are not considered. Not supporting MXPOST convention, replacing brackets with special tokens, is not considered necessary."}, {"heading": "4 Part of Speech Tagger (POS)", "text": "Earlier work to solve the POS tagging problem relied on lexical and local features using maximum entropy models (Toutanova and Manning, 2000). Later, more advanced models took advantage of the context words and their predicted tags (Toutanova et al., 2003) to achieve higher accuracy. As POS tagging is a sequence tagging problem, modeling the sequence into a Maximum Entropy Markov Model (MEMM) or Conditional Random Fields (CRF) model (to infer the probability of the tags\u2019 sequences) seems to be the preferred option. The probability of each tag is computed using log-linear model with features that include large enough context words and their already-computed tags. This transforms every instance of the problem into a large vector of features that is expensive to compute. Then the sequence of vectors are fed to graphical model to compute the probability of each class, using the inference rules. The size of features\u2019 vector and the inference computation are the same regardless of the complexity of the problem.\nAlthough the previous algorithms are sufficient to achieve satisfying accuracy, their computation requirements are overkill for most of the cases faced by the algorithm. For example, the has a unique POS tag that never changes depending on its position in the sentence. Moreover, more and that are frequent enough in the English text that there is a need to cache their extracted features."}, {"heading": "4.1 Algorithm", "text": "SpeedRead takes advantage of the previous observations and tries to distinguish between ambiguous and certain words. To understand such influences, we ran a Stanford POS tagger (left 3 words Model (L3W); trained on Wall Street Journal(WSJ), Sections 1-18) over a 1 GiB of news text to calculate the following dictionaries:\n\u2022 The most frequent POS tag of each token (Uni). \u2022 The most frequent POS tag of each token, given the previous POS tag (Bi). \u2022 The most frequent POS tag of each token, given the previous and next POS tags (Tri).\nUsing the above dictionaries to calculate the POS tag of a word, leads to various precision/recall scores. (Lee et al., 2011) shows that using sieves is the solution to combine several rules/dictionaries. In a sieve algorithm, there is a set of rules that are cascaded after each other. The algorithm runs the rules from the highest in precision to the lowest. The first rule, matching the problem instance, returns its computed tag immediately. SpeedRead\nimplements few sieves in the following order:\n1. Certain tokens: Given a sentence, if the percentage frequency of the most frequent tag of a token is more than a threshold (in our work, 95%) then return that tag. 2. Left and Right tags (Tri): For each token with unknown tag, return the most frequent tag, given the left and right POS tags if they are known. 3. Left tags (Bi): For each token with unknown tag, return the most frequent tag, given the left POS tag if it is known from the previous stages. 4. Token tag (Uni) : For each token with unknown tag up to this stage, return the most frequent tag. 5. Backoff tag: If the token is unknown, use regular expression tagger to deduce the tag; the regular expression tagger relies heavily on matching suffixes."}, {"heading": "4.2 Results", "text": "Table 5 shows the performance of different algorithms running on different sections of PTB. Stanford and SENNA models use sections 1-18, 19-21, 22-24 for training, development and testing datasets, respectively. Despite the simplicity of our algorithm, it achieves relatively high accuracy on the various datasets available.\nApplying more context-aware rules, SpeedRead with sieves 1-5 (SR[Tri/Bi/Uni]) implemented, shows improvement in accuracy by around 2.85% compared to just using unigrams, SpeedRead with sieves 1,4-5 (SR[Uni]). To be sure that our algorithm is robust enough and not overfitting the dataset, we calculated the dictionaries again by running SENNA POS tagger(Collobert et al., 2011) over Reuters RCV1 corpus and the results were similar.\nTables 5 and 6 show the tradeoff between accuracy and speed. Stanford pipeline offers two models with different speeds and accuracies. Since Left 3 Words model (L3W) is the preferred tagger to use in practice, we chose it to be our reference in terms of speed. L3W model runs 18 times faster than the state-of-art Bidirectional model and is only 0.4% less accurate. SpeedRead pushes the speed by another factor of 11 with only 0.5% drop in accuracy. Since the speed of some algorithms vary with the memory used, every algorithm was given enough memory that adding more memory will not affect its speed. The memory footprint is reported in the fourth column of Table 6.\nPOS Tagger Speed Relative Memory Token/Sec Speed in MiB\nStanford Bi 1389 0.04 900 Stanford L3W 28,646 1.00 450 SENNA 34,385 1.20 150 SR [Tri/Bi/Uni] 318,368 11.11 600 SR [Bi/Uni] 397,501 13.87 250 SR [Uni] 564,977 19.72 120\nTable 6: Speed of different POS taggers. The first two taggers are Stanford taggers. The first tagger runs the Bidirectional(Bi) model and the second runs the Left 3 Words (L3w) model. SpeedRead has three variations\nFigure 3: Accumulative percentage of errors made by the most frequent mistagged words. The total number of words is around 2000, the graph lists only the most frequent 1000."}, {"heading": "4.3 Error Analysis", "text": "The most common errors are functional words, such as that, more, .. which have multiple roles in speech. This confirms some of the conclusions reported by (Manning, 2011). Figure 3 shows that less than 10% of mistagged words are responsible for slightly more than 50% of the errors. Regarding unknown words, the only part of the tagger that generalizes over unseen tokens is the regular expression tagger. Regular expressions are not extensive enough to achieve high accuracy. Therefore, we are planning to implement another backoff phase for the frequent unseen words where we accumulate the sentences, containing these words, after sufficient amount of text is processed and then run Stanford/SENNA tagger over those sentences to calculate the most common tag.\nTable 7 shows the confusion matrix of the most ambiguous tags; the less ambiguous tags are clustered into one category, O. One of the biggest sources of confusion in tagging is between adjectives (JJ) and nouns (NN). Proper nouns are the second source of errors as most of the capitalized words will be mistakenly tagged as proper nouns while they are either adjectives or nouns. Such errors are the result of the weak logic implemented in the backoff tagger in SpeedRead, where regular expressions are applied in sequence returning the first match. Other types of errors are adverbs (RB) and propositions (IN). These errors are mainly because of the\nPPPPPPPRef Test\nDT IN JJ NN NNP NNPS NNS RB VBD VBG O\nDT 11094 62 3 7 3 0 0 1 0 0 13 IN 15 13329 9 1 0 0 0 88 0 0 50 JJ 1 11 7461 257 130 2 10 65 38 81 159 NN 1 5 288 17196 111 0 18 11 2 109 93 NNP 8 13 118 109 12585 264 31 8 0 2 39 NNPS 0 0 0 0 70 81 16 0 0 0 0 NNS 0 0 1 23 20 42 7922 0 0 0 53 RB 17 281 103 23 8 0 0 3892 0 1 80 VBD 0 0 8 5 4 0 0 0 4311 1 232 VBG 0 0 25 104 5 0 0 0 0 1799 0 O 26 163 154 172 47 4 107 67 174 2 45707\nTable 7: Confusion Matrix of the POS tags assigned by SpeedRead over the words of sections 22-24 of PTB. O represents all the other not mentioned tags.\nambiguity of the functional words. Functional words need deeper understanding of discourse, semantic and syntactic nature of the text. Taking into consideration the contexts around the words improves the accuracy of tagging. However, trigrams are still small to be considered sufficient context for resolving all the ambiguities."}, {"heading": "5 Named Entity Recognition (NER)", "text": "Named entity recognition is essential to understand and extract information from text. Many efforts and several shared tasks, aiming to improve named entity recognition and classification, had been made; CONLL 2000/2003 (Tjong Kim Sang and De Meulder, 2003) are some of the shared tasks that addressed the named entity recognition task. We use CONLL 2003\u2019s definition of named entity recognition and classification task. CONLL 2003 defines the chunk borders of an entity by using IOB tags, where I-TYPE means that the word is inside an entity, B-TYPE means a beginning of a new entity if the previous token is part of an entity of the same type and O for anything that is not part of an entity. For classification, the task defines four different types: Person(PER), Organization(ORG), Location(LOC) and Miscellaneous(MISC) (See Figure 4).\nWe split the task into two phases. The first is to detect the borders of the entity phrase. After the entity chunk is detected, the second phase will classify each entity phrase to either a Person, Location, Organization or Miscellaneous.\nColumbia/ORG is an American/Misc university located in New/LOC York/LOC.\nFigure 4: Annotated text after NER."}, {"heading": "5.1 Chunking", "text": "We rely on the POS tags of the phrase words to detect the phrase that constitute an entity. A word is considered to be a part of an entity: (1) if it is a demonym (our compiled list contains 320 nationalities), (2) if one of the following conjunction words {&, de, of} appearing in middle of an entity phrase or, (3) if its POS tag is NNP(S) except if it belongs to one of these sets:\n\u2022 Week days and months and their abbreviations. \u2022 Sports (our compiled list contains 182 names). \u2022 Job and profession titles (our compiled list contains 314 title). \u2022 Single Capital letters.\nThese sets are compiled using freebase.\nCONLL dataset shows a strong correlation between POS tags NNP(S) and the words that are part of entities\u2019 phrases; 86% of the words that appear in entities\u2019 phrases have NNP(S) POS tags. The remaining words are distributed among different POS tags; 6.3% are demonyms. Adding the demonyms and proper nouns guarantee 92.3% coverage of the entities\u2019 words that appear in the dataset.\nUsing POS tags as main criteria to detect the entity phrases is expected, given the importance of the POS tags for the NER task. 14 out of 16 submitted paper to CONLL 2003 used POS tags as part of their feature set.\nThe behavior of the chunking algorithm is greedy as it tries to concatenate as many consecutive words as possible into one entity phrase. A technical issue appears in detecting the borders of phrases when multiple entities appear after each other without non-entity separator. This situation can be divided into two cases. Firstly, if the two consecutive entities are of the same type. In this case, the chunking tag should be B-TYPE. Looking at the dataset, such tag appears less than 0.2% out of all the entities\u2019 tags. For example, in the original Stanford MEMM implementation, the classifier (Klein et al., 2003) generates IOB chunking tags while in the later CRF models (Finkel et al., 2005) only IO chunking tags are generated. The second case is when the phrases are of different types. In the dataset, this case appears 248 times over 34834 entities. Since both cases are not frequent enough to harm the performance of the classifiers, SpeedRead does not recognize them."}, {"heading": "5.1.1 Results", "text": "Table 8 shows F1 score of the chunking phase using different taggers to generate the POS tags. This score is calculated over the chunking tags of the words. I and B tags are considered as one class while O is left as it is. It is clear from Table 8 that using better POS taggers does not necessarily produce better results. The quality of SpeedRead POS tagging is sufficient for the chunking stage. SENNA and SpeedRead POS taggers work better for the detection phase because they are more aggressive, assigning the NNP tag to any capitalized word. On the other hand, Stanford tagger prefers to assign the tag of the lowered case shape of the word, if it is a common word."}, {"heading": "5.1.2 Error Analysis", "text": "Table 9 shows the error cases that appears in the chunking phase. The most common class of errors in the chunking phase is titles, such as {RESULTS, DIVISION, CONFERENCE, PTS, PCT}. These words seem to confuse the POS tagger. Another source of confusion for the POS tagger is the words {Women, Men} ; such words appear in the name of sports so they get assigned NNP tag. As expected, all numbers that are part of entities are not detected. Conjunction words are the second important class of errors. (Pawel and Robert, 2007) shows that conjunction words that appear in middle of entities\u2019 phrases are hard to detect and need special classification task. As most of of occurrences are part of entities and the converse is true for and, we decided to include the former and exclude the later."}, {"heading": "5.2 Classification", "text": "Classification is a harder problem than just detecting an entity. For example, \u201cWest Bank\" can belong to two classes, location and organization. Disambiguating the sense of an entity depends on the context. For instance, \u201cMr. Green\" indicates that \u201cGreen\" is a person, while \u201caround Green\" points to a location. To classify an entity, we used a logistic regression classifier, sklearn (Scikit, 2011). The features we feed to the classifier are two factors per type: \u03c6i j(T ypei , phrase j) and \u03c8i j(T ypei , contex t j). Context consists of two words that precede and follow an entity phrase. To calculate these factors:\n\u03c6i j(T ypei , phrase) = n \u220f\nk\nP(T ypei |wk) (1)\n\u03c8i j(T ypei , contex t = {wbe f ore, wa f ter}) = P(T ypei |wbe f ore)\u00d7 P(T ypei |wa f ter) (2)\nThe conditional probabilities of the types, given a specific word, are calculated using the distribution of tags frequencies over words, retrieved from the annotated Reuters RCV1 corpus. SENNA NER tagger has been used to annotate the corpus.\nTable 10 indicates the importance of the classification phase. First row shows that, given chunked input, the classification phase is able to achieve close scores to the state-of- art classifiers. However, given the chunks generated by SpeedRead, the scores drop around 9.5% in F1 scores.\nXXXXXXXXXXPhase Dataset Training Dev Test SR+Gold Chunks 90.80 91.98 87.87 SpeeRead 82.05 83.35 78.28 Stanford 99.28 92.98 89.03 SENNA 96.75 97.24 89.58\nTable 10: F1 scores calculated using conlleval.pl script for NER taggers. The table shows that SpeedRead F1 score is 10% below the sate-of-art achieved by SENNA.\nTo analyze the scores of the classification phase further, Table 11 shows a confusion matrix over the tags generated by SpeedRead. The errors that involve O are signs of chunking errors; there are 1158 chunking errors which exceed the total number of classification errors, 849.\nThe chunking errors contain more false positives than false negatives. The chunking algorithm is aggressive in considering every NNP(S) as part of an entity. That would be fine if we had a perfect POS tagger. The reality that the POS tagger has hard time classifying uppercased words in titles and camel cased words that appear at the beginning of the sentence.\nOnce non-entity is considered part of an entity phrase, the classifier has higher chance of classifying it as an ORG than any other tag. The names of the organizations contain a mix of locations and persons\u2019 names, forcing the classifier to consider any long or mix of words as an organization entity. That appears more clearly in the second most frequent category of errors. 323 words in organizations entities\u2019 names were classified as locations. This could be explained by the fact that many companies and banks name themselves after country names and their locations. For example, \u201cBank of England\" could be classified as a location because of the strong association between England and the tag location.\nTable 12 shows that Stanford pipeline has a high cost for the accuracy achieved by the classifier. SENNA achieves close accuracy with twice the speed and less memory usage. SpeedRead takes another approach by focusing on speed. We are able to speed up the pipeline to the factor of 13. SpeedRead\u2019s memory footprint is half the memory consumed by the Stanford pipeline. Even though SpeedRead\u2019s accuracy is not close to the state-of-art, it still achieves 18% increase over the CONLL 2003 baseline. Moreover, adapting the pipeline to new domains could be easily done by integrating other knowledge base sources as freebase or Wikipedia. SENNA and SpeedRead are able to calculate POS tags at the end of the NER phase without extra computation while that is not true of Stanford pipeline standalone NER application. Using Stanford corenlp pipeline does not guarantee better execution time.\nNER Tagger Token/Sec Relative Memory Speed MiB\nStanford 11,612 1.00 1900 SENNA 18,579 2.13 150 SpeedRead 153,194 13.9 950\nTable 12: Speed of different NER taggers. SpeedRead is faster by 13.9 times using half the memory consumed by Stanford.\nConclusion and Future Work\nOur success in implementing a high performance tokenizer and POS tagger shows that it is possible to use simple algorithms and conditional probabilities, accumulated from a large corpora, to achieve good classification and chunking accuracies.\nThis could lead to a general technique of approximating any sequence tagging problem using sufficiently large dictionaries of conditional probabilities of contexts and inputs. This approximation has the advantage of speeding up the calculations and opens the horizon for new applications where scalability matters.\nExpanding this approach to other languages depends on the availability of other high accurate taggers in these languages. We are looking to infer these conditional probabilities from a global knowledge base as freebase or the interlinking graph of Wikipedia.\nSpeedRead is available under GPLv3 license and it is available to download from www.textmap. org/speedread. We anticipate that it will be useful to large spectrum of named entity recognition applications."}], "references": [{"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Collobert,? 2011", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, ICML \u201908, pages 160\u2013167, New York, NY, USA. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["P. Ferragina", "U. Scaiella"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM \u201910, pages 1625\u20131628, New York, NY, USA. ACM.", "citeRegEx": "Ferragina and Scaiella,? 2010", "shortCiteRegEx": "Ferragina and Scaiella", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In ACL, pages 363\u2013370.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Sentence boundary detection and the problem with the u.s. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short", "author": ["D. Gillick"], "venue": "Association for Computational Linguistics", "citeRegEx": "Gillick,? \\Q2009\\E", "shortCiteRegEx": "Gillick", "year": 2009}, {"title": "Named entity recognition with character-level models", "author": ["D. Klein", "J. Smarr", "H. Nguyen", "C.D. Manning"], "venue": "Daelemans, W. and Osborne, M., editors, Proceedings of CoNLL-2003, pages 180\u2013183. Edmonton, Canada.", "citeRegEx": "Klein et al\\.,? 2003", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Stanford\u2019s multi-pass sieve coreference resolution system at the conll-2011 shared task", "author": ["H. Lee", "Y. Peirsman", "A. Chang", "N. Chambers", "M. Surdeanu", "D. Jurafsky"], "venue": "Proceedings of the CoNLL-2011 Shared Task.", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Gelbukh, A", "author": ["C.D. Manning"], "venue": "F., editor, CICLing (1), volume 6608 of Lecture Notes in Computer Science, pages 171\u2013189. Springer.", "citeRegEx": "Manning,? 2011", "shortCiteRegEx": "Manning", "year": 2011}, {"title": "An effective, low-cost measure of semantic relatedness obtained from wikipedia links", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of AAAI 2008.", "citeRegEx": "Milne and Witten,? 2008", "shortCiteRegEx": "Milne and Witten", "year": 2008}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, 30(1):3\u201326.", "citeRegEx": "Nadeau and Sekine,? 2007", "shortCiteRegEx": "Nadeau and Sekine", "year": 2007}, {"title": "Faster and smaller n-gram language models", "author": ["A. Pauls", "D. Klein"], "venue": "Lin, D., Matsumoto, Y., and Mihalcea, R., editors, ACL, pages 258\u2013267. The Association for Computer Linguistics.", "citeRegEx": "Pauls and Klein,? 2011", "shortCiteRegEx": "Pauls and Klein", "year": 2011}, {"title": "Handling conjunctions in named entities", "author": ["M. Pawel", "D. Robert"], "venue": "Lingvisticae Investigationes, 30(1):49\u201368. Schafer, F.-R. (2012). Quex - fast universal lexical analyzer generator. http://quex. sourceforge.net.", "citeRegEx": "Pawel and Robert,? 2007", "shortCiteRegEx": "Pawel and Robert", "year": 2007}, {"title": "Scikit-learn: Machine learning in python", "author": ["Scikit", "S.-l. D."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Scikit and D.,? 2011", "shortCiteRegEx": "Scikit and D.", "year": 2011}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Daelemans, W. and Osborne, M., editors, Proceedings of CoNLL-2003, pages 142\u2013147. Edmonton, Canada.", "citeRegEx": "Sang and Meulder,? 2003", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL \u201903, pages 173\u2013180, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP \u201900, pages 63\u201370, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova and Manning,? 2000", "shortCiteRegEx": "Toutanova and Manning", "year": 2000}, {"title": "Tokenization as the initial phase in nlp", "author": ["J.J. Webster", "C. Kit"], "venue": "Proceedings of the 14th conference on Computational linguistics - Volume 4, COLING \u201992, pages 1106\u20131110, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Webster and Kit,? 1992", "shortCiteRegEx": "Webster and Kit", "year": 1992}], "referenceMentions": [{"referenceID": 12, "context": "Since NLP algorithms require computationally expensive operations, the NLP stages of an IR system become the bottleneck with regards to scalability (Pauls and Klein, 2011).", "startOffset": 148, "endOffset": 171}, {"referenceID": 11, "context": "However, this section is not meant to review the literature of named entity recognition research as this is already available in (Nadeau and Sekine, 2007).", "startOffset": 129, "endOffset": 154}, {"referenceID": 17, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 16, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 7, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 5, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 8, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 2, "context": "Even though it lacks a proper tokenizer, it offers POS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston, 2008) and parsing (Collobert, 2011).", "startOffset": 122, "endOffset": 150}, {"referenceID": 1, "context": "Even though it lacks a proper tokenizer, it offers POS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston, 2008) and parsing (Collobert, 2011).", "startOffset": 163, "endOffset": 180}, {"referenceID": 0, "context": "NLTK (Bird et al., 2009) is a set of tools and interfaces to other NLP packages.", "startOffset": 5, "endOffset": 24}, {"referenceID": 10, "context": "WikipediaMiner (Milne and Witten, 2008) detects conceptual words and named entities; it also disambiguates the word senses.", "startOffset": 15, "endOffset": 39}, {"referenceID": 4, "context": "TAGME (Ferragina and Scaiella, 2010) is extending the work of WikipediaMiner to annotate short snippets of text.", "startOffset": 6, "endOffset": 36}, {"referenceID": 18, "context": "The first task that an NLP pipeline has to deal with is tokenization and sentence segmentation (Webster and Kit, 1992).", "startOffset": 95, "endOffset": 118}, {"referenceID": 6, "context": ", prove to be complex (Gillick, 2009).", "startOffset": 22, "endOffset": 37}, {"referenceID": 17, "context": "Earlier work to solve the POS tagging problem relied on lexical and local features using maximum entropy models (Toutanova and Manning, 2000).", "startOffset": 112, "endOffset": 141}, {"referenceID": 16, "context": "Later, more advanced models took advantage of the context words and their predicted tags (Toutanova et al., 2003) to achieve higher accuracy.", "startOffset": 89, "endOffset": 113}, {"referenceID": 8, "context": "(Lee et al., 2011) shows that using sieves is the solution to combine several rules/dictionaries.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "To be sure that our algorithm is robust enough and not overfitting the dataset, we calculated the dictionaries again by running SENNA POS tagger(Collobert et al., 2011) over Reuters RCV1 corpus and the results were similar.", "startOffset": 144, "endOffset": 168}, {"referenceID": 9, "context": "This confirms some of the conclusions reported by (Manning, 2011).", "startOffset": 50, "endOffset": 65}, {"referenceID": 7, "context": "For example, in the original Stanford MEMM implementation, the classifier (Klein et al., 2003) generates IOB chunking tags while in the later CRF models (Finkel et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 5, "context": ", 2003) generates IOB chunking tags while in the later CRF models (Finkel et al., 2005) only IO chunking tags are generated.", "startOffset": 66, "endOffset": 87}, {"referenceID": 13, "context": "(Pawel and Robert, 2007) shows that conjunction words that appear in middle of entities\u2019 phrases are hard to detect and need special classification task.", "startOffset": 0, "endOffset": 24}], "year": 2013, "abstractText": "Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebankcompliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer.", "creator": "LaTeX with hyperref package"}}}