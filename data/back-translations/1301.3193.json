{"id": "1301.3193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Learning Graphical Model Parameters with Approximate Marginal Inference", "abstract": "Likelihood-based learning of graphical models faces challenges in terms of computational complexity and robustness versus model specifications. This paper examines methods that directly adjust parameters to maximize a measure of the accuracy of predicted margins, taking into account both model and follow-up approximations during training. Experiments on imaging problems suggest that marginalization-based learning performs better than probability-based approaches to difficult problems where the model that fits is approximate.", "histories": [["v1", "Tue, 15 Jan 2013 01:07:14 GMT  (8070kb)", "http://arxiv.org/abs/1301.3193v1", "To Appear, IEEE Transactions on Pattern Analysis and Machine Intelligence"]], "COMMENTS": "To Appear, IEEE Transactions on Pattern Analysis and Machine Intelligence", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["justin domke"], "accepted": false, "id": "1301.3193"}, "pdf": {"name": "1301.3193.pdf", "metadata": {"source": "CRF", "title": "Learning Graphical Model Parameters with Approximate Marginal Inference", "authors": ["Justin Domke"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n31 93\nv1 [\ncs .L\nG ]\n1 5\nJa n\n20 13\nIndex Terms\u2014Graphical Models, Conditional Random Fields, Machine Learning, Inference, Segmentation."}, {"heading": "1 INTRODUCTION", "text": "G RAPHICAL models are a standard tool in image pro-cessing, computer vision, and many other fields. Exact inference and inference are often intractable, due to the high treewidth of the graph. Much previous work involves approximations of the likelihood. (Section 4). In this paper, we suggest that parameter learning can instead be done using \u201cmarginalization-based\u201d loss functions. These directly quantify the quality of the predictions of a given marginal inference algorithm. This has two major advantages. First, approximation errors in the inference algorithm are taken into account while learning. Second, this is robust to model mis-specification. The contributions of this paper are, first, the general framework of marginalization-based fitting as implicit differentiation. Second, we show that the parameter gradient can be computed by \u201cperturbation\u201d\u2013 that is, by re-running the approximate algorithm twice with the parameters perturbed slightly based on the current loss. Third, we introduce the strategy of \u201ctruncated fitting\u201d. Inference algorithms are based on optimization, where one iterates updates until some convergence threshold is reached. In truncated fitting, algorithms are derived to fit the marginals produced after a fixed number of updates, with no assumption of convergence. We show that this leads to significant speedups. We also derive a variant of this that can apply to likelihood based learning. Finally, experimental results confirm that marginalization based learning gives better results on difficult problems where inference approximations and model mis-specification are most significant."}, {"heading": "2 SETUP", "text": ""}, {"heading": "2.1 Markov Random Fields", "text": "Markov random fields are probability distributions that may be written as\np(x) = 1\nZ\n\u220f\nc\n\u03c8(xc) \u220f\ni\n\u03c8(xi). (1)\nThis is defined with reference to a graph, with one node for each random variable. The first product in Eq. 1 is over the set of cliques c in the graph, while the second is over all individual variables. For example, the graph\nx1 x2\nx3 x4\nx5 x6\ncorresponds to the distribution\np(x) = 1\nZ \u03c8(x1, x2)\u03c8(x2, x3, x5)\u03c8(x3, x4)\u03c8(x5, x6) \u00d7 \u03c8(x1)\u03c8(x2)\u03c8(x3)\u03c8(x4)\u03c8(x5)\u03c8(x6).\nEach function \u03c8(xc) or \u03c8(xi) is positive, but otherwise arbitrary. The factor Z ensures normalization. The motivation for these types of models is the Hammersley\u2013Clifford theorem [1], which gives specific conditions under which a distribution can be written as in Eq. 1. Those conditions are that, first, each random variable is conditionally independent of all others, given its immediate neighbors and, secondly, that each configuration x has nonzero probability. Often, domain knowledge about conditional independence can be used to build a reasonable graph, and the factorized representation in an MRF reduces the curse of dimensionality encountered in modeling a high-dimensional distribution."}, {"heading": "2.2 Conditional Random Fields", "text": "One is often interested in modeling the conditional probability of x, given observations y. For such problems, it is natural to define a Conditional Random Field [2]\np(x|y) = 1\nZ(y)\n\u220f\nc\n\u03c8(xc,y) \u220f\ni\n\u03c8(xi,y).\n2 Here, \u03c8(xc,y) indicates that the value for a particular configuration xc depends on the input y. In practice, the form of this dependence is application dependent."}, {"heading": "2.3 Inference Problems", "text": "Suppose we have some distribution p(x|y), we are given some input y, and we need to guess a single output vector x\u2217. What is the best guess? The answer clearly depends on the meaning of \u201cbest\u201d. One framework for answering this question is the idea of a Bayes estimator [3]. One must specify some utility function U(x,x\u2032), quantifying how \u201chappy\u201d one is to have guessed x if the true output is x\u2032. One then chooses x\u2217 to maximize the expected utility\nx\u2217 = argmax x\n\u2211\nx\u2032\np(x\u2032|y)U(x,x\u2032).\nOne natural utility function is an indicator function, giving one for the exact value x\u2032, and zero otherwise. It is easy to show that for this utility, the optimal estimate is the popular Maximum a Posteriori (MAP) estimate.\nTheorem. If U(x,x\u2032) = I[x = x\u2032], then\nx\u2217 = argmax x p(x|y).\nLittle can be said in general about if this utility function truly reflects user priorities. However, in highdimensional applications, there are reasons for skepticism. First, the actual maximizing probability p(x\u2217|y) in a MAP estimate might be extremely small, so much so that astronomical numbers of examples might be necessary before one could expect to exactly predict the true output. Second, this utility does not distinguish between a prediction that contains only a single error at some component xj , and one that is entirely wrong. An alternative utility function, popular for imaging problems, quantifies the Hamming distance, or the number of components of the output vector that are correct. Maximizing this results in selecting the most likely value for each component independently.\nTheorem. If U(x,x\u2032) = \u2211 i I[xi = x \u2032 i], then\nx\u2217i = argmax xi p(xi|y). (2)\nThis appears to have been originally called Maximum Posterior Marginal (MPM) inference [4], though it has been reinvented under other names [5]. From a computational perspective, the main difficulty is not performing the trivial maximization in Eq. 2, but rather computing the marginals p(xi|y). The marginal-based loss functions introduced in Section 4.2 can be motivated by the idea that at test time, one will use an inference method similar to MPM where one in concerned only with the accuracy of the marginals. The results of MAP and MPM inference will be similar if the distribution p(x|y) is heavily \u201cpeaked\u201d at a single configuration x. Roughly, the greater the entropy of p(x|y), the more there is to be gained in integrating\nover all possible configurations, as MPM does. A few papers have experimentally compared MAP and MPM inference [6], [7]."}, {"heading": "2.4 Exponential Family", "text": "The exponential family is defined by\np(x; \u03b8) = exp ( \u03b8 \u00b7 f(x) \u2212A(\u03b8) ) ,\nwhere \u03b8 is a vector of parameters, f(x) is a vector of sufficient statistics, and the log-partition function\nA(\u03b8) = log \u2211\nx\nexp\u03b8 \u00b7 f(x). (3)\nensures normalization. Different sufficient statistics f(x) define different distributions. The exponential family is well understood in statistics. Accordingly, it is useful to note that a Markov random field (Eq. 1) is a member of the exponential family, with sufficient statistics consisting of indicator functions for each possible configuration of each clique and each variable [8], namely,\nf(X) = {I[Xc = xc]|\u2200c,xc} \u222a {I[Xi = xi]|\u2200i, xi}.\nIt is useful to introduce the notation \u03b8(xc) to refer to the component of \u03b8 corresponding to the indicator function I[Xc = xc], and similarly for \u03b8(xi). Then, the MRF in Eq. 1 would have \u03c8(xc) = e\u03b8(xc) and \u03c8(xi) = e\n\u03b8(xi). Many operations on graphical models can be more elegantly represented using this exponential family representation. A standard problem in the exponential family is to compute the mean value of f ,\n\u00b5(\u03b8) = \u2211\nx\np(x; \u03b8)f(x),\ncalled the \u201cmean parameters\u201d. It is easy to show these are equal to the gradient of the log-partition function.\ndA d\u03b8 = \u00b5(\u03b8). (4)\nFor an exponential family corresponding to an MRF, computing \u00b5 is equivalent to computing all the marginal probabilities. To see this, note that, using a similar notation for indexing \u00b5 as for \u03b8 above,\n\u00b5(xc; \u03b8) = \u2211\nX\np(X; \u03b8)I[Xc = xc] = p(xc; \u03b8).\nConditional distributions can be represented by thinking of the parameter vector \u03b8(y;\u03b3) as being a function of the input y, where \u03b3 are now the free parameters rather than \u03b8. (Again, the nature of the dependence of \u03b8 on y and \u03b3 will vary by application.) Then, we have that\np(x|y;\u03b3) = exp ( \u03b8(y;\u03b3) \u00b7 f(x) \u2212A(\u03b8(y;\u03b3)) ) , (5)\nsometimes called a curved conditional exponential family.\n3"}, {"heading": "2.5 Learning", "text": "The focus of this paper is learning of model parameters from data. (Automatically determining graph structure remains an active research area, but is not considered here.) Specifically, we take the goal of learning to be to minimize the empirical risk\nR(\u03b8) = \u2211\nx\u0302\nL ( \u03b8, x\u0302 ) , (6)\nwhere the summation is over all examples x\u0302 in the dataset, and the loss function L(\u03b8, x\u0302) quantifies how well the distribution defined by the parameter vector \u03b8 matches the example x\u0302. Several loss functions are considered in Section 4. We assume that the empirical risk will be fit by some gradient-based optimization. Hence, the main technical issues in learning are which loss function to use and how to compute the gradient dL\nd\u03b8 .\nIn practice, we will usually be interested in fitting conditional distributions. Using the notation from Eq. 5, we can write this as\nR(\u03b3) = \u2211\n(y\u0302,x\u0302)\nL ( \u03b8(y\u0302,\u03b3), x\u0302 ) .\nNote that if one has recovered dL d\u03b8 , dL d\u03b3 is immediate from the vector chain rule as\ndL d\u03b3 =\nd\u03b8T\nd\u03b3\ndL d\u03b8 . (7)\nThus, the main technical problems involved in fitting a conditional distribution are similar to those for a generative distribution: One finds \u03b8 = \u03b8(y\u0302,\u03b3), computes the L and dL\nd\u03b8 on example x\u0302 exactly as in the generative\ncase, and finally recovers dL d\u03b3 from Eq. 7. So, for simplicity, y and \u03b3 will largely be ignored in the theoretical developments below."}, {"heading": "3 VARIATIONAL INFERENCE", "text": "This section reviews approximate methods for computing marginals, with notation based on Wainwright and Jordan [8]. For readability, all proofs in this section are postponed to Appendix A. The relationship between the marginals and the logpartition function in Eq. 4 is key to defining approximate marginalization procedures. In Section 3.1, the exact variational principle shows that the (intractable) problem of computing the log-partition function can be converted to a (still intractable) optimization problem. To derive a tractable marginalization algorithm one approximates this optimization, yielding some approximate log-partition function A\u0303(\u03b8). The approximate marginals are then taken as the exact gradient of A\u0303. We define the reverse mapping \u03b8(\u00b5) to return some parameter vector that yields that marginals \u00b5. While this will in general not be unique [8, sec. 3.5.2], any two vectors that produce the same marginals \u00b5will also yield the same distribution, and so p(x; \u03b8(\u00b5)) is unambiguous."}, {"heading": "3.1 Exact Variational Principle", "text": "Theorem (Exact variational principle). The log-partition function can also be represented as\nA(\u03b8) = max \u00b5\u2208M \u03b8 \u00b7 \u00b5+H(\u00b5), (8)\nwhere M = {\u00b5\u2032 : \u2203\u03b8,\u00b5\u2032 = \u00b5(\u03b8)}\nis the marginal polytope, and\nH(\u00b5) = \u2212 \u2211\nx\np(x; \u03b8(\u00b5)) log p(x; \u03b8(\u00b5))\nis the entropy.\nIn treelike graphs, this optimization can be solved efficiently. In general graphs, however, it is intractable in two ways. First, the marginal polytope M becomes difficult to characterize. Second, the entropy is intractable to compute. Applying Danskin\u2019s theorem to Eq. 8 yields that\n\u00b5(\u03b8) = dA\nd\u03b8 = argmax \u00b5\u2208M \u03b8 \u00b7 \u00b5+H(\u00b5). (9)\nThus, the partition function (Eq. 8) and marginals (Eq. 9) can both be obtained from solving the same optimization problem. This close relationship between the logpartition function and marginals is heavily used in the derivation of approximate marginalization algorithms. To compute approximate marginals, first, derive an approximate version of the optimization in Eq. 8. Next, take the exact gradient of this approximate partition function. This strategy is used in both of the approximate marginalization procedures considered here: mean field and tree-reweighted belief propagation."}, {"heading": "3.2 Mean Field", "text": "The idea of mean field is to approximate the exact variational principle by replacingM with some tractable subset F \u2282 M, such that F is easy to characterize, and for any vector \u00b5 \u2208 F we can exactly compute the entropy. To create such a set F , instead of considering the set of mean vectors obtainable from any parameter vector (which characterizes M), consider a subset of tractable parameter vectors. The simplest way to achieve this to restrict consideration to parameter vectors \u03b8 with \u03b8(xc) = 0 for all factors c.\nF = {\u00b5\u2032 : \u2203\u03b8,\u00b5\u2032 = \u00b5(\u03b8), \u2200c, \u03b8(xc) = 0}.\nIt is not hard to see that this corresponds to the set of fully-factorized distributions. Note also that this is (in non-treelike graphs) a non-convex set, since it has the same convex hull as M, but is a proper subset. So, the mean field partition function approximation is based on the optimization\nA\u0303(\u03b8) = max \u00b5\u2208F \u03b8 \u00b7 \u00b5+H(\u00b5), (10)\n4 with approximate marginals corresponding to the maximizing vector \u00b5, i.e.\n\u00b5\u0303(\u03b8) = argmax \u00b5\u2208F \u03b8 \u00b7 \u00b5+H(\u00b5). (11)\nSince this is maximizing the same objective as the exact variational principle, but under a more restricted constraint set, clearly A\u0303(\u03b8) \u2264 A(\u03b8). Here, since the marginals are coming from a fullyfactorized distribution, the exact entropy is available as\nH(\u00b5) = \u2212 \u2211\ni\n\u2211\nxi\n\u00b5(xi) log \u00b5(xi). (12)\nThe strategy we use to perform the maximization in Eq. 10 is block-coordinate ascent. Namely, we pick a coordinate j, then set \u00b5(xj) to maximize the objective, leaving \u00b5(xi) fixed for all i 6= j. The next theorem formalizes this.\nTheorem (Mean Field Updates). A local maximum of Eq. 10 can be reached by iterating the updates\n\u00b5(xj) \u2190 1\nZ exp\n( \u03b8(xj) + \u2211\nc:j\u2208c\n\u2211\nxc\\j\n\u03b8(xc) \u220f\ni\u2208c\\j\n\u00b5(xi) ) ,\nwhere Z is a normalizing factor ensuring that \u2211\nxj\n\u00b5(xj) = 1."}, {"heading": "3.3 Tree-Reweighted Belief Propagation", "text": "Whereas mean field replaced the marginal polytope with a subset, tree-reweighted belief propagation (TRW) replaces it with a superset, L \u2283 M. This clearly can only increase the value of the approximate log-partition function. However, a further approximation is needed, as the entropy remains intractable to compute for an arbitrary mean vector \u00b5. (It is not even defined for \u00b5 6\u2208 M.) Thus, TRW further approximates the entropy with a tractable upper bound. Taken together, these two approximations yield a tractable upper bound on the log-partition function. Thus, TRW is based on the optimization problem\nA\u0303(\u03b8) = max \u00b5\u2208L \u03b8 \u00b7 \u00b5+ H\u0303(\u00b5). (13)\nAgain, the approximate marginals are simply the maximizing vector \u00b5, i.e.,\n\u00b5\u0303(\u03b8) = argmax \u00b5\u2208L \u03b8 \u00b7 \u00b5+ H\u0303(\u00b5). (14)\nThe relaxation of the local polytope used in TRW is the local polytope,\nL = {\u00b5 : \u2211\nxc\\i\n\u00b5(xc) = \u00b5(xi), \u2211\nxi\n\u00b5(xi) = 1}. (15)\nSince any valid marginal vector must obey these constraints, clearly M \u2282 L. However, L in general also contains unrealizable vectors (though on trees L = M).\nThus, the marginal vector returned by TRW may, in general, be inconsistent in the sense that no joint distribution yields those marginals. The entropy approximation used by TRW is\nH\u0303(\u00b5) = \u2211\ni\nH(\u00b5i)\u2212 \u2211\nc\n\u03c1cI(\u00b5c), (16)\nwhere H(\u00b5i) = \u2212 \u2211\nxi \u00b5(xi) log\u00b5(xi) is the univariate\nentropy corresponding to variable i, and\nI(\u00b5c) = \u2211\nxc\n\u00b5(xc) log \u00b5(xc) \u220f\ni\u2208c \u00b5(xi) (17)\nis the mutual information corresponding to the variables in the factor c. The motivation for this approximation is that if the constants \u03c1c are selected appropriately, this gives an upper bound on the true entropy.\nTheorem (TRW Entropy Bound). Let Pr(G) be a distribution over tree structured graphs, and define \u03c1c = Pr(c \u2208 G). Then, with H\u0303 as defined in Eq. 16,\nH\u0303(\u00b5) \u2265 H(\u00b5).\nThus, TRW is maximizing an upper bound on the exact variational principle, under an expanded constraint set. Since both of these changes can only increase the maximum value, we have that A\u0303(\u03b8) \u2265 A(\u03b8). Now, we consider how to actually compute the approximate log-partition function and associated marginals. Consider the message-passing updates\nmc(xi) \u221d \u2211\nxc\\i\ne 1 \u03c1c \u03b8(xc)\n\u220f\nj\u2208c\\i\ne\u03b8(xj) \u220f d:j\u2208dmd(xj) \u03c1d\nmc(xj) , (18)\nwhere \u201c\u221d\u201d is used as an assignment operator to means assigning after normalization.\nTheorem (TRW Updates). Let \u03c1c be as in the previous theorem. Then, if the updates in Eq. 18 reach a fixed point, the marginals defined by\n\u00b5(xc) \u221d e 1 \u03c1c \u03b8(xc)\n\u220f\ni\u2208c\ne\u03b8(xi) \u220f d:i\u2208dmd(xi) \u03c1d\nmc(xi) ,\n\u00b5(xi) \u221d e \u03b8(xi)\n\u220f\nd:i\u2208d\nmd(xi) \u03c1d\nconstitute the global optimum of Eq. 13.\nSo, if the updates happen to converge, we have the solution. Meltzer et al. show [9] that on certain graphs made up of monotonic chains, an appropriate ordering of messages does assure convergence. (The proof is essentially that under these circumstances, message passing is equivalent to coordinate ascent in the dual.) TRW simplifies into loopy belief propagation by choosing \u03c1c = 1 everywhere, though the bounding property is lost.\n5"}, {"heading": "4 LOSS FUNCTIONS", "text": "For space, only a representative sample of prior work can be cited. A recent review [10] is more thorough. Though, technically, a \u201closs\u201d should be minimized, we continue to use this terminology for the likelihood and its approximations, where one wishes to maximize. For simplicity, the discussion below is for the generative setting. Using the same loss functions for training a conditional model is simple (Section 2.5)."}, {"heading": "4.1 The Likelihood and Approximations", "text": "The classic loss function would be the likelihood, with\nL(\u03b8,x) = log p(x; \u03b8) = \u03b8 \u00b7 f(x) \u2212A(\u03b8). (19)\nThis has the gradient\ndL d\u03b8 = f(x)\u2212 \u00b5(\u03b8). (20)\nOne argument for the likelihood is that it is efficient; given a correct model, as data increases it converges to true parameters at an asymptotically optimal rate [11]. Some previous work uses tree structured graphs where marginals may be computed exactly [12]. Of course, in high-treewidth graphs, the likelihood and its gradient will be intractable to compute exactly, due to the presence of the log-partition function A(\u03b8) and marginals \u00b5(\u03b8). This has motivated a variety of approximations. The first is to approximate the marginals \u00b5 using Markov chain Monte Carlo [13], [14]. This can lead to high computational expense (particularly in the conditional case, where different chains must be run for each input). Contrastive Divergence [15] further approximates these samples by running the Markov chain for only a few steps, but started at the data points [16]. If the Markov chain is run long enough, these approaches can give an arbitrarily good approximation. However, Markov chain parameters may need to be adjusted to the particular problem, and these approaches are generally slower than those discussed below."}, {"heading": "4.1.1 Surrogate Likelihood", "text": "A seemingly heuristic approach would be to replace the marginals in Eq. 20 with those from an approximate inference method. This approximation can be quite principled if one thinks instead of approximating the logpartition function in the likelihood itself (Eq. 19). Then, the corresponding approximate marginals will emerge as the exact gradient of this surrogate loss. This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25]. However, the terminology of \u201csurrogate likelihood\u201d is not widespread and in most cases, only the gradient is computed, meaning the optimization cannot use line searches. If one uses a log-partition approximation that provides a bound on the true log-partition function, the\nsurrogate likelihood will then bound the true likelihood. Specifically, mean field based surrogate likelihood is an upper bound on the true likelihood, while TRW-based surrogate likelihood is a lower bound."}, {"heading": "4.1.2 Expectation Maximization", "text": "In many applications, only a subset of variables may be observed. Suppose that we want to model x = (z,h) where z is observed, but h is hidden. A natural loss function here is the expected maximization (EM) loss\nL(\u03b8, z) = log p(z; \u03b8) = log \u2211\nh\np(z,h; \u03b8).\nIt is easy to show that this is equivalent to\nL(\u03b8, z) = A(\u03b8, z)\u2212A(\u03b8), (21)\nwhere A(\u03b8, z) = log \u2211\nh exp\u03b8 \u00b7 f(z,h) is the log-partition function with z \u201cclamped\u201d to the observed values. If all variables are observed A(\u03b8, z) reduces to \u03b8 \u00b7 f(z). If on substitutes a variational approximation for A(\u03b8, z), a \u201cvariational EM\u201d algorithm [8, Sec. 6.2.2] can be recovered that alternates between computing approximate marginals and parameter updates. Here, because of the close relationship to the surrogate likelihood, we designate \u201csurrogate EM\u201d for the case where A(\u03b8, z) and A(\u03b8)may both be approximated and the learning is done with a gradient-based method. To obtain a bound on the true EM loss, care is required. For example, lowerbounding A(\u03b8, z) using mean field, and upper-bounding A(\u03b8) using TRW means a lower-bound on the true EM loss. However, using the same approximation for both A(\u03b8) and A(\u03b8, z) appears to work well in practice [26]."}, {"heading": "4.1.3 Saddle-Point Approximation", "text": "A third approximation of the likelihood is to search for a \u201csaddle-point\u201d. Here, one approximates the gradient in Eq. 20 by running a (presumably approximate) MAP inference algorithm, and then imagining that the marginals put unit probability at the approximate MAP solution, and zero elsewhere [27], [28], [21]. This is a heuristic method, but it can be expected to work well when the estimated MAP solution is close to the true MAP and the conditional distribution p(x|y) is strongly \u201cpeaked\u201d."}, {"heading": "4.1.4 Pseudolikelihood", "text": "Finally, there are two classes of likelihood approximations that do not require inference. The first is the classic pseudolikelihood [29], where one uses\nL(\u03b8,x) = \u2211\ni\nlog p(xi|x\u2212i; \u03b8).\nThis can be computed efficiently, even in high treewidth graphs, since conditional probabilities are easy to compute. Besag [29] showed that, under certain conditions, this will converge to the true parameter vector as the amount of data becomes infinite. The pseudolikelihood has been used in many applications [30], [31].\n6 Instead of the probability of individual variables given all others, one can take the probability of patches of variables given all others, sometimes called the \u201cpatch\u201d pseudolikelihood [32]. This interpolates to the exact likelihood as the patches become larger, though some type of inference is generally required."}, {"heading": "4.1.5 Piecewise Likelihood", "text": "More recently, Sutton and McCallum [33] suggested the piecewise likelihood. The idea is to approximate the logpartition function as a sum of log-partition functions of the different \u201cpieces\u201c of the graph. There is flexibility in determining which pieces to use. In this paper, we will use pieces consisting of each clique and each variable, which worked better in practice than some alternatives. Then, one has the surrogate partition function\nA\u0303(\u03b8) = \u2211\nc\nAc(\u03b8) + \u2211\ni\nAi(\u03b8),\nAc(\u03b8) = log \u2211\nxc\ne\u03b8(xc), Ai(\u03b8) = log \u2211\nxi\ne\u03b8(xi).\nIt is not too hard to show that A(\u03b8) \u2264 A\u0303(\u03b8). In practice, it is sometimes best to make some heuristic adjustments to the parameters after learning to improve test-time performance [34], [35]."}, {"heading": "4.2 Marginal-based Loss Functions", "text": "Given the discussion in Section 4.1, one might conclude that the likelihood, while difficult to optimize, is an ideal loss function since, given a well-specified model, it will converge to the true parameters at asymptotically efficient rates. However, this conclusion is complicated by two issues. First, of course, the maximum likelihood solution is computationally intractable, motivating the approximations above. A second issue is that of model mis-specification. For many types of complex phenomena, we will wish to fit a model that is approximate in nature. This could be true because the conditional independencies asserted by the graph do not exactly hold, or because the parametrization of factors is too simplistic. These approximations might be made out of ignorance, due to a lack of knowledge about the domain being studied, or deliberately because the true model might have too many degrees of freedom to be fit with available data. In the case of an approximate model, no \u201ctrue\u201d parameters exist. The idea of marginal-based loss functions is to instead consider how the model will be used. If one will compute marginals at test-time \u2013 perhaps for MPM inference (Section 2.3) \u2013 it makes sense to maximize the accuracy of these predictions. Further, if one will use an approximate inference algorithm, it makes sense to optimize the accuracy of the approximate marginals. This essentially fits into the paradigm of empirical risk minimization [36], [37]. The idea of training a probabilistic model using an alternative loss to the likelihood goes back at least to Bahl et al. in the late 1980s [38].\nThere is reason to think the likelihood is somewhat robust to model mis-specification. In the infinite data limit, it finds the \u201cclosest\u201d solution in the sense of KLdivergence since, if q is the true distribution, then\nKL(q||p) = const.\u2212 E q log p(x; \u03b8)."}, {"heading": "4.2.1 Univariate Logistic Loss", "text": "The univariate logistic loss [39] is defined by\nL(\u03b8,x) = \u2212 \u2211\ni\nlog \u00b5(xi; \u03b8),\nwhere we use the notation \u00b5 to indicate that the loss is implicitly defined with respect to the marginal predictions of some (possibly approximate) algorithm, rather than the true marginals. This measures the mean accuracy of all univariate marginals, rather than the joint distribution. This loss can be seen as empirical risk minimization of the KL-divergence between the true marginals and the predicted ones, since\n\u2211\ni\nKL(qi||\u00b5i) = \u2211\ni\n\u2211\nxi\nq(xi) log q(xi)\n\u00b5(xi; \u03b8)\n= const.\u2212 E q\n\u2211\ni\nlog\u00b5(xi; \u03b8).\nIf defined on exact marginals, this is a type of composite likelihood [40]."}, {"heading": "4.2.2 Smoothed Univariate Classification Error", "text": "Perhaps the most natural loss in the conditional setting would be the univariate classification error,\nL(\u03b8,x) = \u2211\ni\nS (\nmax x\u2032i 6=xi\n\u00b5(xi; \u03b8)\u2212 \u00b5(xi; \u03b8) ) ,\nwhere S(\u00b7) is the step function. This exactly measures the number of components of x that would be incorrectly predicted if using MPM inference. Of course, this loss is neither differentiable nor continuous, which makes it impractical to optimize using gradient-based methods. Instead Gross et al. [5] suggest approximating with a sigmoid function S(t) = (1 + exp(\u2212\u03b1t))\u22121, where \u03b1 controls approximation quality. There is evidence [36], [5] that the smoothed classification loss can yield parameters with lower univariate classification error under MPM inference. However, our experience is that it is also more prone to getting stuck in local minima, making experiments difficult to interpret. Thus, it is not included in the experiments below. Our experience with the univariate quadratic loss [41] is similar."}, {"heading": "4.2.3 Clique Losses", "text": "Any of the above univariate losses can be instead taken based on cliques. For example, the clique logistic loss is\nL(\u03b8,x) = \u2212 \u2211\nc\nlog\u00b5(xc; \u03b8),\n7 10 0 10 1 10 2 10 30 0.1 0.2 0.3\n# training data\nm ea\nn te\nst e\nrr or\nshift of 0\n10 0\n10 1\n10 2\n10 3\n# training data\nshift of 3\n10 0\n10 1\n10 2\n10 3\n# training data\nshift of 10\nlikelihood clique logistic univariate logistic\nFigure 1: Mean test error of various loss functions trained with exact inference. In the case of a wellspecified model (shift of zero), the likelihood performs essentially identically to the marginal-based loss functions. However, when mis-specification is introduced, quite different estimates result.\nwhich may be seen as empirical risk minimization of the mean KL-divergence of the true clique marginals to the predicted ones. An advantage of this with an exact model is consistency. Simple examples show cases where a model predicts perfect univariate marginals, despite the joint distribution being very inaccurate. However, if all clique marginals are correct, the joint must be correct, by the standard moment matching conditions for the exponential family [8]."}, {"heading": "4.2.4 Hidden variables", "text": "Marginal-based loss functions can accommodate hidden variables by simply taking the sum in the loss over the observed variables only. A similar approach can be used with the pseudolikelihood or piecewise likelihood."}, {"heading": "4.3 Comparison with Exact Inference", "text": "To compare the effects of different loss functions in the presence of model mis-specification, this section contains a simple example where the graphical model takes the following \u201cchain\u201d structure:\n. . .\nx1 x2 x3 x4 xn\ny1 y2 y3 y4 yn\nHere, exact inference is possible, so comparison is not complicated by approximate inference. All variables are binary. Parameters are generated by taking \u03b8(xi) randomly from the interval [\u22121,+1] for all i and xi. Interaction parameters are taken as \u03b8(xi, xj) = t when xi = xj , and \u03b8(xi, xj) = \u2212t when xi 6= xj , where t is randomly chosen from the interval [\u22121,+1] for all (i, j). Interactions \u03b8(yi, yj) and \u03b8(xi, yi) are chosen in the same way. To systematically study the effects of differing \u201camounts\u201d of mis-specification, after generating data, we apply various circular shifts to x. Thus, the data no longer corresponds exactly the the structure of the graphical model being fit. Thirty-two different random distributions were created. For each, various quantities of data were generated\nby Markov chain Monte Carlo, with shifts introduced after sampling. The likelihood was fit using the closedform gradient (Sec. 4.1), while the logistic losses were trained using a gradient obtained via backpropagation (Sec. 7). Fig. 1 shows the mean test error (estimated on 1000 examples), while Fig. 2 shows example marginals. We see that the performance of all methods deteriorates with mis-specification, but the marginal-based loss functions are more resistant to these effects."}, {"heading": "4.4 MAP-Based Training", "text": "Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25]. This paper focuses on applications that use marginal inference, and that may need to accommodate hidden variables, and so concentrates on likelihood and marginalbased losses."}, {"heading": "5 IMPLICIT FITTING", "text": "We now turn to the issue of how to train high-treewidth graphical models to optimize the performance of a marginal-based loss function, based on some approximate inference algorithm. Now, computing the value of the loss for any of the marginal-based loss functions is not hard. One can simply run the inference algorithm and plug the resulting marginal into the loss. However, we also require the gradient dL\nd\u03b8 .\nOur first result is that the loss gradient can be obtained by solving a sparse linear system. Here, it is useful to introduce notation to distinguish the loss L, defined in terms of the parameters \u03b8 from the loss Q, defined directly in terms of the marginals \u00b5. (Note that though the notation suggests the application to marginal inference, this is a generic result.)\nTheorem. Suppose that\n\u00b5(\u03b8) := argmax \u00b5:B\u00b5=d \u03b8 \u00b7 \u00b5+H(\u00b5). (22)\nDefine L(\u03b8,x) = Q(\u00b5(\u03b8),x). Then, letting D = d 2H\nd\u00b5d\u00b5T ,\ndL d\u03b8 = ( D\u22121BT (BD\u22121BT )\u22121BD\u22121 \u2212D\u22121 )dQ d\u00b5 .\nA proof may be found in Appendix B. This theorem states that, essentially, once one has computed the predicted marginals, the gradient of the loss with respect to marginals dQ\nd\u00b5 can be transformed into the gradient\nof the loss with respect to parameters dL d\u03b8 through the solution of a sparse linear system. The optimization in Eq. 22 takes place under linear constraints, which encompasses the local polytope used in TRW message-passing (Eq. 15). This theorem does not apply to mean field, as F is not a linear constraint set when viewed as a function of both clique and univariate marginals. In any case, the methods developed below are simpler to use, as they do not require explicitly forming the constraint matrix B or solving the linear system.\n8 0 20 40 60 80 100 120 140 160 180 200 0 0.5 1 true marginals eplacements p (y i = 1 |x )\ni\n0 20 40 60 80 100 120 140 160 180 200 0\n0.5\n1 shift of 3\np (y\ni =\n1 |x )\ni\n0 20 40 60 80 100 120 140 160 180 200 0\n0.5\n1 shift of 10\np (y\ni =\n1 |x )\ni\nFigure 2: Exact and predicted marginals for an example input. Predicted marginals are trained using 1000 data. With low shifts, all loss functions lead to accurate predicted marginals. However, the univariate and clique logistic loss are more resistant to the effects of model mis-specification. Legends as in Fig. 1."}, {"heading": "6 PERTURBATION", "text": "This section observes that variational methods have a special structure that allows derivatives to be calculated without explicitly forming or inverting a linear system. We have, by the vector chain rule, that\ndL d\u03b8 =\nd\u00b5T\nd\u03b8\ndQ d\u00b5 . (23)\nA classic trick in scientific computing is to efficiently compute Jacobian-vector products by finite differences. The basic result is that, for any vector v,\nd\u00b5\nd\u03b8T v = lim r\u21920\n1\nr\n( \u00b5(\u03b8 + rv) \u2212 \u00b5(\u03b8) ) ,\nwhich is essentially just the definition of the derivative of \u00b5 in the direction of v. Now, this does not immediately seem helpful, since Eq. 23 requires d\u00b5 T\nd\u03b8 , not d\u00b5 d\u03b8T . How-\never, with variational methods, these are symmetric. The simplest way to see this is to note that\nd\u00b5\nd\u03b8T =\nd\nd\u03b8T\n(\ndA\nd\u03b8\n)\n= dA\nd\u03b8d\u03b8T .\nDomke [46] lists conditions for various classes of entropies that guarantee that A will be differentiable. Combining the above three equations, the loss gradient is available as the limit\ndL d\u03b8 = lim r\u21920 1 r ( \u00b5(\u03b8 + r dQ d\u00b5 )\u2212 \u00b5(\u03b8) ) . (24)\nIn practice, of course, the gradient is approximated using some finite r. The simplest approximation, onesided differences, simply takes a single value of r in Eq. 24, rather than a limit. More accurate results at the\ncost of more calls to inference, are given using two-sided differences, with\ndL d\u03b8 \u2248 1 2r ( \u00b5(\u03b8 + r dQ d\u00b5 )\u2212 \u00b5(\u03b8 \u2212 r dQ d\u00b5 ) ) ,\nwhich is accurate to order o(r2). Still more accurate results are obtained with \u201cfour-sided\u201d differences, with\ndL d\u03b8 \u2248 1 12r ( \u2212\u00b5(\u03b8 + 2r dQ d\u00b5 ) + 8\u00b5(\u03b8 + r dQ d\u00b5 )\n\u2212 8\u00b5(\u03b8 \u2212 r dQ\nd\u00b5 ) + \u00b5(\u03b8 \u2212 2r\ndQ d\u00b5 ) ) ,\nwhich is accurate to order o(r4) [47]. Alg. 1 shows more explicitly how the loss gradient could be calculated, using two-sided differences. The issue remains of how to calculate the step size r. Each of the approximations above becomes exact as r \u2192 0. However, as r becomes very small, numerical error eventually dominates. To investigate this issue experimentally, we generated random models on a 10\u00d710 binary grid, with each parameter \u03b8(xi) randomly chosen from a standard normal, while each interaction parameter \u03b8(xi, xj) was chosen randomly from a normal with a standard deviation of s. In each case, a random value x was generated, and the \u201ctrue\u201d loss gradient was estimated by standard (inefficient) 2-sided finite differences, with inference re-run after each component of \u03b8 is perturbed independently. To this, we compare one, two, and four-sided perturbations. In all cases, the step size is, following Andrei [48], taken to be r = m\u01eb 1 3 ( 1+ ||\u03b8||\u221e ) /||dQ d\u00b5\n||\u221e, where \u01eb is machine epsilon, and m is a multiplier that we will vary. Note that the optimal power of \u01eb will depend on the finite difference scheme; 13 is optimal for two-sided differences [49, Sec. 8.1]. All calculations take place in double-precision with\n9 Algorithm 1 Calculating dL d\u03b8 by perturbation (two-sided).\ninference run until marginals changed by a threshold of less than 10\u221215. Fig. 3 shows that using many-sided differences leads to more accuracy, at the cost of needing to run inference more times to estimate a single loss gradient. In the following experiments, we chose twosided differences with a multiplier of 1 as a reasonable tradeoff between accuracy, simplicity, and computational expense. Welling and Teh used sensitivity of approximate beliefs to parameters to approximate joint probabilities of non-neighboring variables [50]."}, {"heading": "7 TRUNCATED FITTING", "text": "The previous methods for computing loss gradients are derived under the assumption that the inference optimization is solved exactly. In an implementation, of course, some convergence threshold must be used. Different convergence thresholds can be used in the learning stage and at test time. In practice, we have observed that too loose a threshold in the learning stage can lead to a bad estimated risk gradient, and learning\nterminating with a bad search direction. Meanwhile, a loose threshold can often be used at test time with few consequences. Usually, a difference of 10\u22123 in estimated marginals has little practical impact, but this can still be enough to prevent learning from succeeding [51]. It seems odd that the learning algorithm would spend the majority of computational effort exploring tight convergence levels that are irrelevant to the practical performance of the model. Here, we define the learning objective in terms of the approximate marginals obtained after a fixed number of iterations. To understand this, one may think of the inference process not as an optimization, but rather as a large, nonlinear function. This clearly leads to a well-defined objective function. Inputting parameters, applying the iterations of either TRW or mean field, computing predicted marginals, and finally a loss are all differentiable operations. Thus, the loss gradient is efficiently computable, at least in principle, by reverse-mode automatic differentiation (autodiff), an approach explored by Stovanov et al. [36], [52]. In preliminary work, we experimented with autodiff tools, but found these to be unsatisfactory for our applications for two reasons. Firstly, these tools impose a computational penalty over manually derived gradients. Secondly, autodiff stores all intermediate calculations, leading to large memory requirements. The methods derived below use less memory, both in terms of constant factors and big-O complexity. Nevertheless, some of these problems are issues with current implementations of reverse-mode autodiff, avoidable in theory. Both mean field and TRW involve steps where we first take a product of a set of terms, and then normalize. We define a \u201cbacknorm\u201d operator, which is useful in taking derivatives over such operations, by\nbacknorm(g, c) = c\u2299 (g \u2212 g \u00b7 c).\nThis will be used in the algorithms here. More discussion on this point can be found in Appendix C."}, {"heading": "7.1 Back Mean Field", "text": "The first backpropagating inference algorithm, back mean field, is shown as Alg. 2. The idea is as follows: Suppose we start with uniform marginals, run N iterations of mean field, and then\u2013 regardless of if mean field has converged or not\u2013 take predictedmarginals and plug them into one of the marginal-based loss functions. Since each step in this process is differentiable, this specifies the loss as a differentiable function of model parameters. We want the exact gradient of this function.\nTheorem. After execution of back mean field,\n\u2190\u2212 \u03b8 (xi) = dL\nd\u03b8(xi) and\n\u2190\u2212 \u03b8 (xc) = dL\nd\u03b8(xc) .\nA proof sketch is in Appendix C. Roughly speaking, the proof takes the form of a mechanical differentiation of each step of the inference process.\n10\nAlgorithm 2 Back Mean Field 1) Initialize \u00b5 uniformly. 2) Repeat N times for all j:\na) Push the marginals \u00b5j onto a stack.\nb) \u00b5(xj) \u221d exp ( \u03b8(xj) + \u2211\nc:j\u2208c\n\u2211\nxc\\j\n\u03b8(xc) \u220f\ni\u2208c\\j\n\u00b5(xi) )\n3) Compute L, \u2190\u2212\u00b5 (xj) = dLd\u00b5(xj) and \u2190\u2212\u00b5 (xc) = dL d\u00b5(xc) . 4) Initialize \u2190\u2212 \u03b8 (xi) \u2190 0, \u2190\u2212 \u03b8 (xc) \u2190 0.\n5) Repeat N times for all j (in reverse order): a) \u2190\u2212\u03bdj \u2190 backnorm(\u2190\u2212\u00b5j , \u00b5j)\nb) \u2190\u2212 \u03b8 (xj) \u2190 \u2190\u2212 \u03b8 (xj) + \u2190\u2212\u03bd (xj) c) \u2190\u2212 \u03b8 (xc) \u2190 \u2190\u2212 \u03b8 (xc) + \u2190\u2212\u03bd (xj) \u220f\ni\u2208c\\j\n\u00b5(xi) \u2200c : j \u2208 c\nd) \u2190\u2212\u00b5 (xi) \u2190 \u2190\u2212\u00b5 (xi) + \u2211\nxc\\i\n\u2190\u2212\u03bd (xj)\u03b8(xc) \u220f\nk\u2208c\\{i,j}\n\u00b5(xk)\n\u2200c : j \u2208 c, \u2200i \u2208 c\\j\ne) Pull marginals \u00b5j from the stack. f) \u2190\u2212\u00b5j(xj) \u2190 0\nNote that, as written, back mean field only produces univariate marginals, and so cannot cope with loss functions making use of clique marginals. However, with mean field, the clique marginals, are simply the product of univariate marginals: \u00b5(xc) = \u220f\ni\u2208c \u00b5(xi). Hence, any loss defined on clique marginals can equivalently be defined on univariate marginals."}, {"heading": "7.2 Back TRW", "text": "Next, we consider truncated fitting with TRW inference. As above, we will assume that some fixed number N of inference iterations have been run, and we want to define and differentiate a loss defined on the current predicted marginals. Alg. 3 shows the method.\nTheorem. After execution of back TRW,\n\u2190\u2212 \u03b8 (xi) = dL\nd\u03b8(xi) and\n\u2190\u2212 \u03b8 (xc) = dL\nd\u03b8(xc) .\nAgain, a proof sketch is in Appendix C. If one uses pairwise factors only, uniform appearance probabilities of \u03c1 = 1, removes all reference to the stack, and uses a convergence threshold in place of a fixed number of iterations, one obtains essentially Eaton and Ghahramani\u2019s back belief propagation [53, extended version, Fig. 5]. Here, we refer to the general strategy of using full (non-truncated) inference as \u201cbackpropagation\u201d, either with LBP, TRW, or mean field."}, {"heading": "7.3 Truncated Likelihood & Truncated EM", "text": "Applying the truncated fitting strategies to any of the marginal-based loss functions is simple. Applying it to the likelihood or EM loss, however, is not so straightforward. The reason is that these losses (Eqs. 19 and 21) are\nAlgorithm 3 Back TRW. 1) Initialize m uniformly. 2) Repeat N times for all pairs (c, i), with i \u2208 c:\na) Push the messages mc(xi) onto a stack.\nb) mc(xi) \u221d \u2211\nxc\\i e\n1 \u03c1c \u03b8(xc) \u220f\nj\u2208c\\i e \u03b8(xj)\n\u220f\nd:j\u2208d\nmd(xj) \u03c1d\nmc(xj)\n3) \u00b5(xc) \u221d e 1 \u03c1c \u03b8(xc) \u220f i\u2208c e \u03b8(xi)\n\u220f\nd:i\u2208d\nmd(xi) \u03c1d\nmc(xi) \u2200c\n4) \u00b5(xi) \u221d e\u03b8(xi) \u220f d:i\u2208dmd(xi) \u03c1d \u2200i 5) Compute L, \u2190\u2212\u00b5 (xi) = dLd\u00b5(xi) and \u2190\u2212\u00b5 (xc) = dL d\u00b5(xc)\n. 6) For all c,\na) \u2190\u2212\u03bd (xc) \u2190 backnorm(\u2190\u2212\u00b5c, \u00b5c) b) \u2190\u2212 \u03b8 (xc) + \u2190 1\n\u03c1c\n\u2190\u2212\u03bd (xc)\nc) \u2190\u2212 \u03b8 (xi) + \u2190 \u2211\nxc\\i\n\u2190\u2212\u03bd (xc) \u2200i \u2208 c\nd) \u2190\u2212md(xi) + \u2190 \u03c1d\u2212Ic=d\nmd(xi)\n\u2211\nxc\\i\n\u2190\u2212\u03bd \u2200i \u2208 c, \u2200d : i \u2208 d\n7) For all i, a) \u2190\u2212\u03bd (xi) \u2190 backnorm(\u2190\u2212\u00b5i, \u00b5i)\nb) \u2190\u2212\u03b8 (xi) + \u2190 \u2190\u2212\u03bd (xi)\nc) \u2190\u2212md(xi) + \u2190 \u03c1d \u2190\u2212\u03bd (xi) md(xi)\n\u2200d : i \u2208 d\n8) Repeat N times for all pairs (c, i) (in reverse order)\na) s(xc) \u2190 e 1 \u03c1c \u03b8(xc) \u220f j\u2208c\\i e \u03b8(xj)\n\u220f\nd:j\u2208d\nmd(xj) \u03c1d\nmc(xj)\nb) \u2190\u2212\u03bd (xi) \u2190 backnorm(\u2190\u2212\u2212mci,mci) c) \u2190\u2212 \u03b8 (xc) + \u2190 1\n\u03c1c s(xc) \u2190\u2212\u03bd (xi) mc(xi)\nd) \u2190\u2212 \u03b8 (xj) + \u2190 \u2211\nxc\\j s(xc) \u2190\u2212\u03bd (xi) mc(xi)\n\u2200j \u2208 c\\i\ne) \u2190\u2212md(xj) + \u2190 \u03c1d\u2212Ic=d\nmd(xj)\n\u2211\nxc\\j s(xc) \u2190\u2212\u03bd (xi) mc(xi)\n\u2200j \u2208 c\\i, \u2200d : j \u2208 d f) Pull messages mc(xi) from the stack.\ng) \u2190\u2212mc(xi) \u2190 0\ndefined, not in terms of predicted marginals, but in terms of partition functions. Nevertheless, we wish to compare to these losses in the experiments below. As we found truncation to be critical for speed, we instead derive a variant of truncated fitting. The basic idea is to define a \u201ctruncated partition function\u201d. This is done by taking the predicted marginals, obtained after a fixed number of iterations, and plugging them into the entropy approximations used either for mean field (Eq. 12) or TRW (Eq. 16). The approximate entropy H\u0303 is then used in defining a truncated partition function as\nA\u0303(\u03b8) = \u03b8 \u00b7 \u00b5(\u03b8)\u2212 H\u0303(\u00b5(\u03b8)).\nAs we will see below, with too few inference iterations, using this approximation can cause the surrogate likelihood to diverge. To see why, imagine an extreme case where zero inference iterations are used. This results in the loss L(\u03b8,x) = \u03b8\u00b7(f(x)\u2212\u00b50)+H\u0303(\u00b50),where \u00b50 are the\n11\ninitial marginals. As long as the mean of f(x) over the dataset is not equal to \u00b50, arbitrary loss can be achieved. With hidden variables, A(\u03b8, z) is defined similarly, but with the variables z \u201cclamped\u201d to the observed values. (Those variables will play no role in determining \u00b5(\u03b8))."}, {"heading": "8 EXPERIMENTS", "text": "These experiments consider three different datasets with varying complexity. In all cases, we try to keep the features used relatively simple. This means some sacrifice in performance, relative to using sophisticated features tuned more carefully to the different problem domains. However, given that our goal here is to gauge the relative performance of the different algorithms, we use simple features for the sake of experimental transparency. We compare marginal-based learning methods to the surrogate likelihood/EM, the pseudolikelihood and piecewise likelihood. These comparisons were chosen because, first, they are the most popular in the literature (Sec. 4). Second, the surrogate likelihood also requires marginal inference, meaning an \u201capples to apples\u201d comparison using the same inference method. Third, these methods can all cope with hidden variables, which appear in our third dataset. In each experiment, an \u201cindependent\u201d model, trained using univariate features only with logistic loss was used to initialize others. The smoothed classification loss, because of more severe issues with local minima, was initialized using surrogate likelihood/EM."}, {"heading": "8.1 Setup", "text": "All experiments here will be on vision problems, using a pairwise, 4-connected grid. Learning uses the LBFGS optimization algorithm. The values \u03b8 are linearly parametrized in terms of unary and edge features. Formally, we will fit two matrices, F and G, which determine all unary and edge features, respectively. These can be expressed most elegantly by introducing a bit more notation. Let \u03b8i denote the set of parameter values \u03b8(xi) for all values xi. If u(y, i) denotes the vector of unary features for variable i given input image y, then\n\u03b8i = Fu(y, i).\nSimilarly, let \u03b8ij denote the set of parameter values \u03b8(xi, xj) for all xi, xj . If v(y, i, j) is the vector of edge features for pair (i, j), then\n\u03b8ij = Gv(y, i, j).\nOnce dL d\u03b8 has been calculated (for whichever loss and method), we can easily recover the gradients with respect to F and G by\ndL dF = \u2211\ni\ndL d\u03b8i u(y, i)T , dL dG = \u2211\nij\ndL\nd\u03b8ij v(y, i, j)T ."}, {"heading": "8.2 Binary Denoising Data", "text": "In a first experiment, we create a binary denoising problem using the Berkeley Segmentation Dataset. Here, we took 132 200 \u00d7 300 images from the Berkeley segmentation dataset, binarized them according to if each pixel is above the image mean. The noisy input values are then generated as yi = xi(1\u2212tni ) + (1\u2212xi)t n i , where xi is the true binary label, and ti \u2208 [0, 1] is random. Here, n \u2208 (1,\u221e) is the noise level, where lower values correspond to more noise. Thirty-two images were used for training, and 100 for testing. This is something of a\n12\ntoy problem, but the ability to systematically vary the noise level is illustrative. As unary features u(y, i), we use only two features: a constant of 1, and the noisy input value at that pixel. For edge features v(y, i, j), we also use two features: one indicating that (i, j) is a horizontal edge, and one indicating that (i, j) is a vertical edge. The effect is that vertical and horizontal edges have independent parameters. For learning, we use full back TRW and back mean field (without message-storing or truncation) for marginal-based loss functions, and the surrogate likelihood with the gradient computed in the direct form (Eq. 20). In all cases, a threshold on inference of 10\u22124 is used. Error rates are shown in Tab. 1, while predicted marginals for an example test image are shown in Fig. 4. We compare against an independent model, which can be seen as truncated fitting with zero iterations or, equivalently, logistic regression at the pixel level. We see that for low noise levels, all methods perform well, while for high noise levels, the marginal-based losses outperform the surrogate likelihood and pseudolikelihood by a considerable margin. Our interpretation of this is that model mis-specification is more pronounced with high noise, and other losses are less robust to this."}, {"heading": "8.3 Horses", "text": "Secondly, we use the Weizman horse dataset, consisting of 328 images of horses at various resolutions. We use 200 for training and 128 for testing. The set of possible labels xi is again binary\u2013 either the pixel is part of a horse or not. For unary features u(y, i), we begin by computing the RGB intensities of each pixel, along with the normalized vertical and horizontal positions. We expand these 5 initial features into a larger set using sinusoidal expansion [54]. Specifically, denote the five original features by s. Then, we include the features sin(c\u00b7s) and cos(c\u00b7s) for all binary vectors c of the appropriate length. This results in an expanded set of 64 features. To these we append\n(a) Input (b) True Labels\n(c) Surr. Like. TRW (d) U. Logistic TRW (e) Sm. Class \u03bb=50 TRW\n(f) Surr. Like. MNF (g) U. Logistic MNF (h) Independent\nFigure 5: Predicted marginals for a test image from the horses dataset. Truncated learning uses 40 iterations.\na 36-component Histogram of Gradients [55], for a total of 100 features. For edge features between i and j, we use a set of 21 \u201cbase\u201d features: A constant of one, the l2 norm of the difference of the RGB values at i and j, discretized as above 10 thresholds, and the maximum response of a Sobel edge filter at i or j, again discretized using 10 thresholds. To generate the final feature vector v(y, i, j), this is increased into a set of 42 features. If (i, j) is a horizontal edge, the first half of these will contain the base features, and the other half will be zero. If (i, j) is a vertical edge, the opposite situation occurs. This essentially allows for different parametrization of vertical and horizontal edges. In a first experiment, we train models with truncated fitting with various numbers of iterations. The pseudolikelihood and piecewise likelihood use a convergence threshold of 10\u22125 for testing. Several trends are visible in Tab. 2. First, with less than 20 iterations, the truncated surrogate likelihood diverges, and produces errors around 0.4. Second, TRW consistently outperforms mean field. Finally, marginal-based loss functions outperform the others, both in terms of training and test errors. Fig. 5 shows predicted marginals for an example test image. On this dataset, the pseudolikelihood, piecewise likelihood, and surrogate likelihood based on mean field are outperformed by an independent model, where each label is predicted by input features independent of all others."}, {"heading": "8.4 Stanford Backgrounds Data", "text": "Our final experiments consider the Stanford backgrounds dataset. This consists of 715 images of resolu-\n13\n(a) Input Image (b) True Labels\ntion approximately 240\u00d7 320. Most pixels are labeled as one of eight classes, with some unlabeled. The unary features u(y, i) we use here are identical to those for the horses dataset. In preliminary experiments, we tried training models with various resolutions. We found that reducing resolution to 20% of the original after computing features, then upsampling the predicted marginals yielded significantly better results than using the original resolution. Hence, this is done for all the following experiments. Edge features are identical to those for the horses dataset, except only based on the difference of RGB intensities, meaning 22 total edge features v(y, i, j). In a first experiment, we compare the performance of truncated fitting, perturbation, and back-propagation, using 100 images from this dataset for speed. We train with varying thresholds for perturbation and backpropagation, while for truncated learning, we use various numbers of iterations. All models are trained with TRW to fit the univariate logistic loss. If a bad searchdirection is encountered, L-BFGS is re-initialized. Results are shown in Fig. 7. We see that with loose thresholds, perturbation and back-propagation experience learning failure at sub-optimal solutions. Truncated fitting is far\nmore successful; using more iterations is slower to fit, but leads to better performance at convergence. In a second experiment, we train on the entire dataset, with errors estimated using five-fold cross validation. Here, an incremental procedure was used, where first a subset of 32 images was trained on, with 1000 learning iterations. The number of images was repeatedly doubled, with the number of learning iterations halved. In practice this reduced training time substantially. Results are shown in Fig. 6. These results use a ridge regularization penalty of \u03bb on all parameters. (This is relative to the empirical risk, as measured per pixel.) For EM, and marginal based loss functions, we set this as \u03bb = 10\u22123. We found in preliminary experiments that using a smaller regularization constant caused truncated EM to diverge even with 10 iterations. The pseudolikelihood and piecewise benefit from less regularization, and so we use \u03bb = 10\u22124 there. Again the marginal based loss functions outperform others. In particular, they also perform quite well even with 5 iterations, where truncated EM diverges."}, {"heading": "9 CONCLUSIONS", "text": "Training parameters of graphical models in a high treewidth setting involves several challenges. In this paper, we focus on three: model mis-specification, the necessity of approximate inference, and computational complexity. The main technical contribution of this paper is several methods for training based on the marginals predicted by a given approximate inference algorithm. These methods take into account model mis-specification and inference approximations. To combat computational complexity, we introduce \u201ctruncated\u201d learning, where the inference algorithm only needs to be run for a fixed number of iterations. Truncation can also be applied, somewhat heuristically, to the surrogate likelihood. Among previous methods, we experimentally find the surrogate likelihood to outperform the pseudolikelihood or piecewise learning. By more closely reflecting the test criterion of Hamming loss, marginal-based loss functions perform still better, particularly on harder problems (Though the surrogate likelihood generally displays smaller train/test gaps.) Additionally marginalbased loss functions are more amenable to truncation, as the surrogate likelihood diverges with too few iterations.\n14"}, {"heading": "10 BIOGRAPHY", "text": "Justin Domke obtained a PhD degree in Computer Science from the University of Maryland, College Park in 2009. From 2009 to 2012, he was an Assistant Professor at Rochester Institute of Technology. Since 2012, he is a member of the Machine Learning group at NICTA.\n15"}, {"heading": "11 APPENDIX A: VARIATIONAL INFERENCE", "text": "Theorem (Exact variational principle). The log-partition function can also be represented as\nA(\u03b8) = max \u00b5\u2208M \u03b8 \u00b7 \u00b5+H(\u00b5),\nwhere\nM = {\u00b5\u2032 : \u2203\u03b8,\u00b5\u2032 = \u00b5(\u03b8)}\nis the marginal polytope, and\nH(\u00b5) = \u2212 \u2211\nx\np(x; \u03b8(\u00b5)) log p(x; \u03b8(\u00b5))\nis the entropy.\nProof of the exact variational principle: As A is convex, we have that\nA(\u03b8) = sup \u00b5\n\u03b8 \u00b7 \u00b5\u2212A\u2217(\u00b5)\nwhere A\u2217(\u00b5) = inf\n\u03b8 \u03b8 \u00b7 \u00b5\u2212A(\u03b8)\nis the conjugate dual. Now, since dA/d\u03b8 = \u00b5(\u03b8), if \u00b5 6\u2208 M, then the infimum for A\u2217 is unbounded above. For \u00b5 \u2208 M, the infimum will be obtained at \u03b8(\u00b5). Thus\nA\u2217(\u00b5) =\n{\n\u221e \u00b5 6\u2208 M \u03b8(\u00b5) \u00b7 \u00b5\u2212A(\u03b8(\u00b5)) \u00b5 \u2208 M .\nNow, for \u00b5 \u2208 M, we can see by substitution that\nA\u2217(\u00b5) =\u03b8(\u00b5) \u00b7 \u2211\nx\np(x; \u03b8(\u00b5))f(x) \u2212A(\u03b8(\u00b5))\n= \u2211\nx\np(x; \u03b8(\u00b5))(\u03b8(\u00b5) \u00b7 f(x) \u2212A(\u03b8(\u00b5)))\n= \u2211\nx\np(x; \u03b8(\u00b5)) log p(x; \u03b8(\u00b5)) = \u2212H(\u00b5).\nAnd so, finally,\nA\u2217(\u00b5) =\n{\n\u221e \u00b5 6\u2208 M \u2212H(\u00b5) \u00b5 \u2208 M , (25)\nwhich is equivalent to the desired result.\nTheorem (Mean Field Updates). A local maximum of Eq. 14 can be reached by iterating the updates\n\u00b5(xj) \u2190 1\nZ exp\n( \u03b8(xj) + \u2211\nc:j\u2208c\n\u2211\nxc\\j\n\u03b8(xc) \u220f\ni\u2208c\\j\n\u00b5(xi) ) ,\nwhere Z is a normalizing factor ensuring that \u2211\nxj \u00b5(xj) = 1.\nProof of Mean Field Updates: The first thing to note is that for \u00b5 \u2208 F , several simplifying results hold, which are easy to verify, namely\np(x; \u03b8(\u00b5)) = \u220f\ni\n\u00b5(xi)\nH(\u00b5) =\u2212 \u2211\ni\n\u2211\nxi\n\u00b5(xi) log \u00b5(xi).\n\u00b5(xc) = \u220f\ni\u2208c\n\u00b5(xi).\nNow, let A\u0303 denote the approximate partition function that results from solving Eq. 8 with the marginal polytope replaced by F . By substitution of previous results, we can see that this reduces to an optimization over univariate marginals only.\nA\u0303(\u03b8) = max {\u00b5(xi)}\n\u2211\ni\n\u2211\nxi\n\u03b8(xi)\u00b5(xi) + \u2211\nc\n\u2211\nxc\n\u03b8(xc)\u00b5(xc)\n\u2212 \u2211\ni\n\u2211\nxi\n\u00b5(xi) log\u00b5(xi). (26)\nNow, form a Lagrangian, enforcing that \u2211\nxj \u00b5(xj) = 1.\nL = \u2211\nj\n\u2211\nxj\n\u03b8(xj)\u00b5(xj) + \u2211\nc\n\u2211\nxc\n\u03b8(xc) \u220f\ni\u2208c\n\u00b5(xi)\n\u2212 \u2211\nj\n\u2211\nxj\n\u00b5(xj) log \u00b5(xj) + \u2211\nj\n\u03bbj(1 \u2212 \u2211\nxj\n\u00b5(xj)).\nSetting dL/d\u00b5(xj) = 0, solving for \u00b5(xj), we obtain\n\u00b5(xj) \u221d exp ( \u03b8(xj) + \u2211\nc:j\u2208c\n\u2211\nxc\\j\n\u03b8(xc) \u220f\ni\u2208c\\j\n\u00b5(xi) ) ,\nmeaning this is a local minimum. Normalizing by Z gives the result. Note that only a local maximum results since the mean-field objective is non-concave[8, Sec. 5.4]. Two preliminary results are needed to prove the TRW entropy bound.\nLemma. Let \u00b5G be the \u201cprojection\u201d of \u00b5 onto a subgraph G, defined by\n\u00b5G = {\u00b5(xi)\u2200i} \u222a {\u00b5(xc)\u2200c \u2208 G}.\nThen, for \u00b5 \u2208 M,\nH(\u00b5G) \u2265 H(\u00b5).\nProof: First, note that, by Eq. 25, for \u00b5 \u2208 M,\nH(\u00b5) = \u2212A\u2217(\u00b5) = \u2212 inf \u03b8 (\u03b8 \u00b7 \u00b5\u2212A(\u03b8)).\nNow, the entropy of \u00b5G could be defined as an infimum only over the parameters \u03b8 corresponding to the cliques in G. Equivalently, however, we can define it as a constrained optimization\nH(\u00b5G) = \u2212 inf \u03b8:\u03b8c=0 \u2200c 6\u2208G\n( \u03b8 \u00b7 \u00b5G \u2212A(\u03b8) ) .\nSince the infimum for H(\u00b5G) takes place over a more constrained set, but \u00b5 and \u00b5G are identical on all the\n16\ncomponents where \u03b8 may be nonzero, we have the result. Our next result is that the approximate entropy considered in Eq. 16 is exact for tree-structured graphs, when \u03c1c = 1.\nLemma. For \u00b5 \u2208 M for a marginal polytope M corresponding to a tree-structured graph,\nH(\u00b5) = \u2211\ni\nH(\u00b5i)\u2212 \u2211\nc\nI(\u00b5c).\nProof: First, note that for any any tree structured distribution can be factored as\np(x) = \u220f\ni\np(xi) \u220f\nc\np(xc) \u220f\ni\u2208c p(xi) .\n(This is easily shown by induction.) Now, recall our definition of H :\nH(\u00b5) = \u2212 \u2211\nx\np(x; \u03b8(\u00b5)) log p(x; \u03b8(\u00b5))\nSubstituting the tree-factorized version of p into the equation yields\nH(\u00b5) = \u2212 \u2211\nx\np(x; \u03b8(\u00b5)) log p(x; \u03b8(\u00b5))\n= \u2212 \u2211\nx\n\u2211\ni\np(x; \u03b8(\u00b5)) log p(xi; \u03b8(\u00b5))\n\u2212 \u2211\nx\n\u2211\nc\np(x; \u03b8(\u00b5)) log p(xc; \u03b8(\u00b5)) \u220f\ni\u2208c p(xi; \u03b8(\u00b5))\n= \u2212 \u2211\ni\n\u2211\nxi\n\u00b5(xi) log\u00b5(xi)\n\u2212 \u2211\nc\n\u2211\nxc\n\u00b5(xc) log \u00b5(xc) \u220f\ni\u2208c \u00b5(xi)\nFinally, combining these two lemmas, we can show the main result, that the TRW entropy is an upper bound.\nTheorem (TRW Entropy Bound). Let Pr(G) be a distribution over tree structured graphs, and define \u03c1c = Pr(c \u2208 G). Then, with H\u0303 as defined in Eq. 16,\nH\u0303(\u00b5) \u2265 H(\u00b5).\nProof: The previous Lemma shows that for any specific tree G,\nH(\u00b5G) \u2265 H(\u00b5).\nThus, it follows that\nH(\u00b5) \u2264 \u2211\nG\nPr(G)H(\u00b5G)\n= \u2211\nG\nPr(G) (\n\u2211\ni\nH(\u00b5i)\u2212 \u2211\nc\u2208G\nI(\u00b5c) )\n= \u2211\ni\nH(\u00b5i)\u2212 \u2211\nc\n\u03c1cI(\u00b5c)\nTheorem (TRW Updates). Let \u03c1c be as in the previous theorem. Then, if the updates in Eq. 18 reach a fixed point, the marginals defined by\n\u00b5(xc) \u221d e 1 \u03c1c \u03b8(xc)\n\u220f\ni\u2208c\ne\u03b8(xi) \u220f d:i\u2208dmd(xi) \u03c1d\nmc(xi) ,\n\u00b5(xi) \u221d e \u03b8(xi)\n\u220f\nd:i\u2208d\nmd(xi) \u03c1d\nconstitute the global optimum of Eq. 13. Proof: The TRW optimization is defined by\nA\u0303(\u03b8) = max \u00b5\u2208L \u03b8 \u00b7 \u00b5+ H\u0303(\u00b5).\nConsider the equivalent optimization\nmax\u00b5 \u03b8 \u00b7 \u00b5+ H\u0303(\u00b5)\ns.t. 1 = \u2211\nxi\n\u00b5(xi)\n\u00b5(xi). = \u2211\nxc\\i\n\u00b5(xc)\nwhich makes the constraints of the local polytope explicit First, we form a Lagrangian, and consider derivatives with respect to \u00b5, for fixed Lagrange multipliers.\nL =\u03b8 \u00b7 \u00b5+H(\u00b5) + \u2211\ni\n\u03bbi(1\u2212 \u2211\nxi\n\u00b5(xi))\n+ \u2211\nc\n\u2211\nxi\n\u03bbc(xi) (\n\u2211\nxc\\i\n\u00b5(xc)\u2212 \u00b5(xi) )\ndL\nd\u00b5(xc) =\u03b8(xc) + \u03c1c\n(\n\u2211\ni\u2208c\n( 1 + log \u2211\nx\u2032 i\n\u00b5(xi,x \u2032 \u2212i)\n)\n\u2212 1\u2212 log \u00b5(xc) ) + \u2211\ni\u2208c\n\u03bbc(xi)\ndL\nd\u00b5(xi) =\u03b8(xi)\u2212 1\u2212 log\u00b5(xi)\u2212 \u03bbi \u2212\n\u2211\nc:i\u2208c\n\u03bbc(xi)\nSetting these derivatives equal to zero, we can solve for the log-marginals in terms of the Lagrange multipliers:\n\u03c1c log\u00b5(xc) =\u03b8(xc) + \u03c1c (\n\u2211\ni\u2208c\n( 1 + log \u2211\nx\u2032 i\n\u00b5(xi,x \u2032 \u2212i)\n) \u2212 1 )\n+ \u2211\ni\u2208c\n\u03bbc(xi)\nlog\u00b5(xi) =\u03b8(xi)\u2212 1\u2212 \u03bbi \u2212 \u2211\nc:i\u2208c\n\u03bbc(xi)\nNow, at a solution, we must have that \u00b5(xi) = \u2211\nxc\\i \u00b5(xc). This leads first to the the constraint that\n17\nlog\u00b5(xc) = 1\n\u03c1c \u03b8(xc) +\n\u2211\ni\u2208c\n( 1 + log\u00b5(xi) + 1\n\u03c1c \u03bbc(xi)\n)\n\u2212 1\n= 1\n\u03c1c \u03b8(xc) +\n\u2211\ni\u2208c\n( 1 + \u03b8(xi)\u2212 1\u2212 \u03bbi \u2212 \u2211\nc:i\u2208c\n\u03bbc(xi)\n+ 1\n\u03c1c \u03bbc(xi)\n)\n\u2212 1.\nNow, define the \u201cmessages\u201d in terms of the Lagrange multipliers as\nmc(xi) = e \u2212 1 \u03c1c \u03bbc(xi).\nIf the appropriate values of the messages were known, then we could solve for the clique-wise marginals as\n\u00b5(xc) \u221d e 1 \u03c1c \u03b8(xc)\n\u220f\ni\u2208c\ne\u03b8(xi) exp ( 1\n\u03c1c \u03bbc(xi)\n)\n\u00d7 \u220f\nd:i\u2208d\nexp ( \u2212\u03bbd(xi)) )\n= e 1 \u03c1c \u03b8(xc)\n\u220f\ni\u2208c\ne\u03b8(xi) \u220f d:i\u2208dmd(xi) \u03c1d\nmc(xi) .\nThe univiariate marginals are available simply as\n\u00b5(xi) \u221d exp ( \u03b8(xi)\u2212 \u2211\nd:i\u2208d\n\u03bbd(xi) )\n= e\u03b8(xi) \u220f\nd:i\u2208d\nexp(\u2212\u03bbd(xi) )\n= e\u03b8(xi) \u220f\nd:i\u2208d\nmd(xi) \u03c1d .\nWe may now derive the actual propagation. At a valid solution, the Lagrange multipliers (and hence the messages) must be selected so that the constraints are satisfied. In particular, we must have that \u00b5(xi) = \u2211\nxc\\i \u00b5(xc). From the constraint, we can derive con-\nstraints on neighboring sets of messages.\n\u00b5(xi) = \u2211\nxc\\i\n\u00b5(xc)\ne\u03b8(xi) \u220f\nd:i\u2208d\nmd(xi) \u03c1d \u221d\n\u2211\nxc\\i\ne 1 \u03c1c \u03b8(xc)\n\u00d7 \u220f\ni\u2208c\ne\u03b8(xi) \u220f d:i\u2208d md(xi) \u03c1d\nmc(xi) (27)\nNow, the left hand side of this equation cancels one term from the product on line 27, except for the denominator of mc(xi). This leads to the constraint of\nmc(xi) \u221d \u2211\nxc\\i\ne 1 \u03c1c \u03b8(xc)\n\u220f\nj\u2208c\\i\ne\u03b8(xj) \u220f d:j\u2208dmd(xj) \u03c1d\nmc(xj) .\nThis is exactly the equation used as a fixed-point equation in the TRW algorithm."}, {"heading": "12 APPENDIX B: IMPLICIT DIFFERENTIATION", "text": "Theorem. Suppose that\n\u00b5(\u03b8) := argmax \u00b5:B\u00b5=d \u03b8 \u00b7 \u00b5+H(\u00b5).\nDefine L(\u03b8,x) = Q(\u00b5(\u03b8),x). Then, letting D = d 2H\nd\u00b5d\u00b5T ,\ndL d\u03b8 = ( D\u22121BT (BD\u22121BT )\u22121BD\u22121 \u2212D\u22121 )dQ d\u00b5 .\nProof: First, recall the implicit differentiation theorem. If the relationship between a and b is implicitly determined by f(a,b) = 0, then\ndbT\nda = \u2212\ndfT\nda\n(dfT\ndb\n)\u22121 .\nIn our case, given the Lagrangian\nL = \u03b8 \u00b7 \u00b5+H(\u00b5) + \u03bbT (B\u00b5\u2212 d),\nOur implicit function is determined by the constraints that dL\nd\u00b5 = 0 and dL d\u03bb = 0. That is, it must be true that\ndL d\u00b5 = \u03b8 + dH d\u00b5 +BT\u03bb = 0\ndL d\u03bb = B\u00b5\u2212 d = 0.\nThus, our implicit function is\nf\n([\n\u00b5\n\u03bb\n])\n=\n[\n\u03b8 + dH d\u00b5 +BT\u03bb\nB\u00b5\u2212 d\n]\n=\n[\n0\n0\n]\nTaking derivatives, we have that\nd\n[\n\u00b5\n\u03bb\n]T\nd\u03b8 = \u2212\n(dfT\nd\u03b8\n) ( dfT\nd\n[\n\u00b5\n\u03bb\n]\n)\u22121\nTaking the terms on the right hand side in turn, we have\ndfT\nd\u03b8 =\nd\n[\n\u03b8 + dH d\u00b5 +BT\u03bb\nB\u00b5\u2212 d\n]T\nd\u03b8 =\n[\nI 0\n]T\ndfT\nd\n[\n\u00b5\n\u03bb\n] =\n[\nd2H d\u00b5d\u00b5T BT\nB 0\n]\nd\n[\n\u00b5\n\u03bb\n]T\nd\u03b8 = \u2212\n[\nI 0\n]T [\nd2H d\u00b5d\u00b5T BT\nB 0\n]\u22121\n(28)\nThis means that \u2212 d\u00b5 T\nd\u03b8 is the upper-left block of the\ninverse of the matrix on the right hand side of Eq. 28. It is well known that if\nM =\n[\nE F G H\n]\n,\n18\nthen the upper-left block of M\u22121 is\nE\u22121 + E\u22121F (H\u2212GE\u22121F )\u22121GE\u22121.\nSo, we have that\nd\u00b5T\nd\u03b8 = D\u22121BT (BD\u22121BT )BD\u22121 \u2212D\u22121, (29)\nwhere D := d 2H\nd\u00b5d\u00b5T .\nThe result follows simply from substituting Eq. 29 into the chain rule\ndL d\u03b8 =\nd\u00b5T\nd\u03b8\ndQ d\u00b5 ."}, {"heading": "13 APPENDIX C: TRUNCATED FITTING", "text": "Several simple lemmas will be useful below. A first one considers the case where we have a \u201cproduct of powers\u201d.\nLemma (Products of Powers). Suppose that b = \u220f i a pi i . Then db\ndai = pi ai b.\nNext, both mean-field and TRW involve steps where we first take a product of a set of terms, and then normalize. The following lemma is useful in dealing with such operations.\nLemma (Normalized Products). Suppose that bi = \u220f\nj aij and ci = bi/ \u2211 j aij . Then,\ndci dajk = ( Ii=j \u2212 ci ) cj ajk .\nCorollary. Under the same conditions,\ndL\ndajk = cj ajk ( dL dcj \u2212 \u2211\ni\ndL dci ci ) .\nAccordingly, we find it useful to define the operator\nbacknorm(g, c) = c\u2299 (g \u2212 g \u00b7 c).\nThis can be used as follows. Suppose that we have calculated\u2190\u2212c = dL\ndc . Then, if we set\u2190\u2212\u03bd = backnorm(\u2190\u2212c , c),\nand we have that dL dajk = \u2190\u2212\u03bdj ajk\n. An important special case of this is where ajk =\nexp fjk. Then, we have simply that dLdfjk = \u2190\u2212\u03bdj .\nAnother important special case is where ajk = f \u03c1 jk.\nThen, we have that dajk dfjk = \u03c1f\u03c1\u22121jk , and so dL dfjk = \u03c1 \u2190\u2212\u03bdj fjk .\nTheorem. After execution of back mean field,\n\u2190\u2212 \u03b8 (xi) = dL\nd\u03b8(xi) and\n\u2190\u2212 \u03b8 (xc) = dL\nd\u03b8(xc) .\nProof sketch: The idea is just to mechanically differentiate each step of the algorithm, computing the derivative of the computed loss with respect to each intermediate quantity. First, note that we can re-write the main mean-field iteration as\n\u00b5(xj) \u221d exp ( \u03b8(xj) )\n\u220f\nc:j\u2208c\n\u220f\nxc\\j\nexp ( \u03b8(xc) \u220f\ni\u2208c\\j\n\u00b5(xi) )) . (30)\nNow, suppose we have the derivative of the loss with respect to this intermediate vector of marginals \u2190\u2212\u00b5j . We wish to \u201cpush back\u201d this derivative on the values affecting these marginals, namely \u03b8(xj), \u03b8(xc) (for all c such that j \u2208 c), and \u00b5(xi) (for all i 6= j such that \u2203c : {i, j} \u2208 c). To do this, we take two steps: 1) Calculate the derivative with respect to the value on the righthand side of Eq. 30 before normalization. 2) Calculate the derivative of this un-normalized quantity with respect to \u03b8(xj), \u03b8(xc) and \u00b5(xi). Now, define \u03bdj to be the vector of marginals produced by Eq. 30 before normalization. Then, by the discussion above, \u2190\u2212\u03bdj = backnorm(\u2190\u2212\u00b5j ,\u00b5j). This completes step 1. Now, with \u2190\u2212\u03bdj in hand, we can immediately calculate the backpropagation of \u2190\u2212\u00b5j on \u03b8 as\n\u2190\u2212 \u03b8 (xj) = \u2190\u2212\u03bd (xj).\nThis, follows from the fact that dL da = dL dea ea, where \u03b8(xj) plays the role of a, and ea plays the role of \u03bd(xj). Similarly, we can calculate that\ndL\nd\u03b8(xc) \u220f i\u2208c\\j \u00b5(xi) = \u2190\u2212\u03bd (xj).\nThus, since\nd\u03b8(xc) \u220f i\u2208c\\j \u00b5(xi)\nd\u03b8(xc) =\n\u220f\ni\u2208c\\j\n\u00b5(xi),\nwe have that\n\u2190\u2212 \u03b8 (xc) = \u2190\u2212\u03bd (xj) \u220f\ni\u2208c\\j\n\u00b5(xi).\nSimilarly, for any xc that \u201cmatches\u201d xi (in the sense that the same value xi is present as the appropriate component of xc),\nd\u03b8(xc) \u220f k\u2208c\\j \u00b5(xk)\nd\u00b5(xi) = \u03b8(xc)\n\u220f\nk\u2208c\\{i,j}\n\u00b5(xk).\nFrom which we have\n\u2190\u2212\u00b5 (xi) = \u2211\nxc\\i\n\u2190\u2212\u03bd (xj)\u03b8(xc) \u220f\nk\u2208c\\{i,j}\n\u00b5(xk),\nmeaning this is a local minimum. Normalizing by Z gives the result.\nTheorem. After execution of back TRW,\n\u2190\u2212 \u03b8 (xi) = dL\nd\u03b8(xi) and\n\u2190\u2212 \u03b8 (xc) = dL\nd\u03b8(xc) .\nProof sketch: Again, the idea is just to mechanically differentiate each step of the algorithm. Since the marginals are derived in terms of the messages, we\n19\nmust first take derivatives with respect to the marginalproducing steps. First, consider step 3, where predicted clique marginals are computed. Defining \u2190\u2212\u03bd (xc) = backnorm(\u2190\u2212\u00b5c, \u00b5c), we have that\n\u2190\u2212 \u03b8 (xc) = 1\n\u03c1c\n\u2190\u2212\u03bd (xc)\n\u2190\u2212 \u03b8 (xi) = \u2211\nxc\\i\n\u2190\u2212\u03bd (xc)\n\u2190\u2212md(xi) = \u03c1d \u2212 I[c = d]\nmd(xi)\n\u2211\nxc\\i\n\u2190\u2212\u03bd\nNext, consider step 4, where predicted univariate marginals are computed. Defining, \u2190\u2212\u03bd (xi) = backnorm(\u2190\u2212\u00b5i, \u00b5i), we have\n\u2190\u2212 \u03b8 (xi) = \u2190\u2212\u03bd (xi)\n\u2190\u2212md(xi) = \u03c1d\n\u2190\u2212\u03bd (xi) md(xi) .\nFinally, we consider the main propagation, in step 2. Here, we recompute the intermediate quantity\ns(xc) = e 1 \u03c1c \u03b8(xc)\n\u220f\nj\u2208c\\i\ne\u03b8(xj)\n\u220f\nd:j\u2208d\nmd(xj) \u03c1d\nmc(xj) .\nAfter this, consider the step where when pair (c, i) is being updated. We first compute\ndL\ndm0c(ci) =\n\u2190\u2212\u03bd (xi) mc(xi) ,\nwhere m0c(xi) is defined as the value of the marginal before normalization, and\n\u2190\u2212\u03bd (xi) = backnorm(\u2190\u2212\u2212mci,mci).\n(See the Normalized Products Lemma above.) Given this, we can consider the updates required to gradients of \u03b8(xc), \u03b8(xi) and md(xj) in turn. First, we have that the update to \u2190\u2212 \u03b8 (xc) should be\ndL\ndm0c(xi)\ndm0c(xi)\nd\u03b8(xc)\n= \u2190\u2212\u03bd (xi)\nmc(xi)\n1\n\u03c1c s(xc),\nwhich is the update present in the algorithm. Next, the update to \u2190\u2212 \u03b8 (xj) should be\n\u2211\nxi\ndL\ndm0c(xi)\ndm0c(xi)\nd\u03b8(xj)\n= \u2211\nxi\n\u2190\u2212\u03bd (xi)\nmc(xi)\n\u2211\nxc\\{i,j}\ns(xc)\n= \u2211\nxc\\j\n\u2190\u2212\u03bd (xi) mc(xi) s(xc).\nIn terms of the incoming messages, consider the update to \u2190\u2212md(xj), where j 6= i, j \u2208 d, and d 6= c. This will be\n\u2211\nxi\ndL\ndm0c(xi)\ndm0c(xi) dmd(xj)\n= \u2211\nxi\n\u2190\u2212\u03bd (xi)\nmc(xi)\n\u2211\nxc\\{i,j}\n\u03c1d md(xj) s(xc)\n= \u03c1d\nmd(xj)\n\u2211\nxc\\j\ns(xc) \u2190\u2212\u03bd (xi)\nmc(xi) .\nFinally, consider the update to \u2190\u2212mc(xj), where j 6= i. This will have the previous update, plus the additional term, considering the presence of mc(xj) in the denominator of the main TRW update, of\n\u2211\nxi\n\u2190\u2212\u03bd (xi)\nmc(xi)\n\u2211\nxc\\{i,j}\n\u03c1d md(xj) s(xc)\n= \u03c1d\nmd(xj)\n\u2211\nxc\\j\n\u2190\u2212\u03bd (xi) mc(xi) s(xc).\nNow, after the update has taken place, the messages mc(xi) are reverted to their previous values. As these values have not (yet) influenced any other variables, they are initialized with \u2190\u2212mc(xi) = 0.\n20\n21\n22\n(a) Input Image (b) True Labels\n(a) Input Image (b) True Labels"}], "references": [{"title": "Spatial interaction and the statistical analysis of lattice systems", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), vol. 36, no. 2, pp. 192\u2013236, 1974.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "ICML, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Model distortions in bayesian MAP reconstruction", "author": ["M. Nikolova"], "venue": "Inverse Problems and Imaging, vol. 1, no. 2, pp. 399\u2013422, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic solution of ill-posed problems in computational vision", "author": ["J. Marroquin", "S. Mitter", "T. Poggio"], "venue": "Journal of the American Statistical Association, vol. 82, no. 397, pp. 76\u201389, 1987.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1987}, {"title": "Training conditional random fields for maximum labelwise accuracy", "author": ["S.S. Gross", "O. Russakovsky", "C.B. Do", "S. Batzoglou"], "venue": "NIPS, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploiting inference for approximate parameter learning in discriminative fields: An empirical study", "author": ["S. Kumar", "J. August", "M. Hebert"], "venue": "EMMCVPR, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Measuring uncertainty in graph cut solutions", "author": ["P. Kohli", "P. Torr"], "venue": "Computer Vision and Image Understanding, vol. 112, no. 1, pp. 30\u201338, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn., vol. 1, no. 1-2, pp. 1\u2013305, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergent message passing algorithms - a unifying view", "author": ["T. Meltzer", "A. Globerson", "Y. Weiss"], "venue": "UAI, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured learning and prediction in computer vision", "author": ["S. Nowozin", "C.H. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision, vol. 6, pp. 185\u2013365, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "On parameter learning in CRF-based approaches to object class image segmentation", "author": ["S. Nowozin", "P.V. Gehler", "C.H. Lampert"], "venue": "ECCV, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning flexible features for conditional random fields", "author": ["L. Stewart", "X. He", "R.S. Zemel"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 8, pp. 1415\u20131426, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Markov chain monte carlo maximum likelihood", "author": ["C. Geyer"], "venue": "Symposium on the Interface, 1991.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "On contrastive divergence learning", "author": ["M. Carreira-Perpinan", "G. Hinton"], "venue": "AISTATS, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Fields of experts", "author": ["S. Roth", "M.J. Black"], "venue": "International Journal of Computer Vision, vol. 82, no. 2, pp. 205\u2013229, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the \u201cwrong\u201d graphical model: benefits in the computation-limited setting", "author": ["M.J. Wainwright"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 1829\u20131859, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1829}, {"title": "Efficiently learning random fields for stereo vision with sparse message passing", "author": ["J.J. Weinman", "L.C. Tran", "C.J. Pal"], "venue": "ECCV, 2008, pp. 617\u2013630.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Random field model for integration of local information and global information", "author": ["T. Toyoda", "O. Hasegawa"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 8, pp. 1483\u20131489, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to combine bottom-up and top-down segmentation", "author": ["A. Levin", "Y. Weiss"], "venue": "International Journal of Computer Vision, vol. 81, no. 1, pp. 105\u2013118, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting inference for approximate parameter learning in discriminative fields: An empirical study", "author": ["S. Kumar", "J. August", "M. Hebert"], "venue": "EMMCVPR, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Figure/ground assignment in natural images", "author": ["X. Ren", "C. Fowlkes", "J. Malik"], "venue": "ECCV, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Accelerated training of conditional random fields with stochastic gradient methods", "author": ["S.V.N. Vishwanathan", "N.N. Schraudolph", "M.W. Schmidt", "K.P. Murphy"], "venue": "ICML, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning probabilistic models for contour completion in natural images", "author": ["X. Ren", "C. Fowlkes", "J. Malik"], "venue": "International Journal of Computer Vision, vol. 77, no. 1-3, pp. 47\u201363, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Scene understanding with discriminative structured prediction", "author": ["J. Yuan", "J. Li", "B. Zhang"], "venue": "CVPR, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Scene segmentation with crfs learned from partially labeled images", "author": ["J.J. Verbeek", "B. Triggs"], "venue": "NIPS, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning conditional random fields for stereo", "author": ["D. Scharstein", "C. Pal"], "venue": "CVPR, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Using combination of statistical models and multilevel structural information for detecting urban areas from a single gray-level image", "author": ["P. Zhong", "R. Wang"], "venue": "IEEE T. Geoscience and Remote Sensing, vol. 45, no. 5-2, pp. 1469\u20131482, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series D (The Statistician), vol. 24, no. 3, pp. 179\u2013195, 1975.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1975}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "CVPR, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative random fields", "author": ["S. Kumar", "M. Hebert"], "venue": "International Journal of Computer Vision, vol. 68, no. 2, pp. 179\u2013201, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning in gibbsian fields: How accurate and how fast can it be?", "author": ["S.C. Zhu", "X. Liu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Piecewise training for undirected models", "author": ["C. Sutton", "A. McCallum"], "venue": "UAI, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust model-based scene interpretation by multilayered context information", "author": ["S. Kim", "I.-S. Kweon"], "venue": "Computer Vision and Image Understanding, vol. 105, no. 3, pp. 167\u2013187, 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J.M. Winn", "C. Rother", "A. Criminisi"], "venue": "Int. J. of Comput. Vision, vol. 81, no. 1, pp. 2\u201323, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["V. Stoyanov", "A. Ropson", "J. Eisner"], "venue": "AISTATS, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning convex inference of marginals", "author": ["J. Domke"], "venue": "UAI, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "A new algorithm for the estimation of hidden markov model parameters", "author": ["L.R. Bahl", "P.F. Bron", "P.V. de Souza", "R.L. Mercer"], "venue": "ICASSP, 1988.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1988}, {"title": "An alternate objective function for Markovian fields", "author": ["S. Kakade", "Y.W. Teh", "S. Roweis"], "venue": "ICML, 2002.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Composite likelihood methods", "author": ["B.G. Lindsay"], "venue": "Contemporary Mathematics, vol. 80, pp. 221\u2013239, 1988.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1988}, {"title": "Learning convex inference of marginals", "author": ["J. Domke"], "venue": "UAI, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative models for multi-class object layout", "author": ["C. Desai", "D. Ramanan", "C.C. Fowlkes"], "venue": "International Journal of Computer Vision, vol. 95, no. 1, pp. 1\u201312, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning CRFs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "ECCV, 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Hierarchical image-region labeling via structured learning", "author": ["J.J. McAuley", "T.E. de Campos", "G. Csurka", "F. Perronnin"], "venue": "BMVC, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Scene segmentation via low-dimensional semantic representation and conditional random field", "author": ["W. Yang", "B. Triggs", "D. Dai", "G.-S. Xia"], "venue": "HAL, Tech. Rep., 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Implicit differentiation by perturbation", "author": ["J. Domke"], "venue": "NIPS, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate Solution Methods in Engineering Mechanics", "author": ["A. Boresi", "K. Chong"], "venue": "Elsevier Science Inc.,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1991}, {"title": "Accelerated conjugate gradient algorithm with finite difference hessian/vector product approximation for unconstrained optimization", "author": ["N. Andrei"], "venue": "J. Comput. Appl. Math., vol. 230, no. 2, pp. 570\u2013582, 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Linear response algorithms for approximate inference in graphical models", "author": ["M. Welling", "Y.W. Teh"], "venue": "Neural Computation, vol. 16, pp. 197\u2013221, 2004.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Parameter learning with truncated message-passing", "author": ["J. Domke"], "venue": "CVPR, 2011.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Minimum-risk training of approximate CRF-based NLP systems", "author": ["V. Stoyanov", "J. Eisner"], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 0}, {"title": "Choosing a variable to clamp", "author": ["F. Eaton", "Z. Ghahramani"], "venue": "AISTATS, 2009.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Value function approximation in reinforcement leanring using the fourier basis", "author": ["G. Konidaris", "S. Osentoski", "P. Thomas"], "venue": "AAAI, 2011.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The motivation for these types of models is the Hammersley\u2013Clifford theorem [1], which gives specific conditions under which a distribution can be written as in Eq.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "For such problems, it is natural to define a Conditional Random Field [2] p(x|y) = 1 Z(y) \u220f", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "One framework for answering this question is the idea of a Bayes estimator [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "(2) This appears to have been originally called Maximum Posterior Marginal (MPM) inference [4], though it has been reinvented under other names [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "(2) This appears to have been originally called Maximum Posterior Marginal (MPM) inference [4], though it has been reinvented under other names [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "A few papers have experimentally compared MAP and MPM inference [6], [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "A few papers have experimentally compared MAP and MPM inference [6], [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "1) is a member of the exponential family, with sufficient statistics consisting of indicator functions for each possible configuration of each clique and each variable [8], namely,", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "3 VARIATIONAL INFERENCE This section reviews approximate methods for computing marginals, with notation based on Wainwright and Jordan [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 8, "context": "show [9] that on certain graphs made up of monotonic chains, an appropriate ordering of messages does assure convergence.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "A recent review [10] is more thorough.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Some previous work uses tree structured graphs where marginals may be computed exactly [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "The first is to approximate the marginals \u03bc using Markov chain Monte Carlo [13], [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "The first is to approximate the marginals \u03bc using Markov chain Monte Carlo [13], [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Contrastive Divergence [15] further approximates these samples by running the Markov chain for only a few steps, but started at the data points [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Contrastive Divergence [15] further approximates these samples by running the Markov chain for only a few steps, but started at the data points [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 18, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 20, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 21, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 203, "endOffset": 207}, {"referenceID": 23, "context": "This \u201csurrogate likelihood\u201d [17] approximation appears to be the most widely used loss in imaging problems, with marginals approximated by either mean field [18], [19], TRW [20] or LBP [21], [22], [23], [24], [25].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "However, using the same approximation for both A(\u03b8) and A(\u03b8, z) appears to work well in practice [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "20 by running a (presumably approximate) MAP inference algorithm, and then imagining that the marginals put unit probability at the approximate MAP solution, and zero elsewhere [27], [28], [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "20 by running a (presumably approximate) MAP inference algorithm, and then imagining that the marginals put unit probability at the approximate MAP solution, and zero elsewhere [27], [28], [21].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "20 by running a (presumably approximate) MAP inference algorithm, and then imagining that the marginals put unit probability at the approximate MAP solution, and zero elsewhere [27], [28], [21].", "startOffset": 189, "endOffset": 193}, {"referenceID": 27, "context": "The first is the classic pseudolikelihood [29], where one uses L(\u03b8,x) = \u2211", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "Besag [29] showed that, under certain conditions, this will converge to the true parameter vector as the amount of data becomes infinite.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "The pseudolikelihood has been used in many applications [30], [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "The pseudolikelihood has been used in many applications [30], [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "Instead of the probability of individual variables given all others, one can take the probability of patches of variables given all others, sometimes called the \u201cpatch\u201d pseudolikelihood [32].", "startOffset": 186, "endOffset": 190}, {"referenceID": 31, "context": "5 Piecewise Likelihood More recently, Sutton and McCallum [33] suggested the piecewise likelihood.", "startOffset": 58, "endOffset": 62}, {"referenceID": 32, "context": "In practice, it is sometimes best to make some heuristic adjustments to the parameters after learning to improve test-time performance [34], [35].", "startOffset": 135, "endOffset": 139}, {"referenceID": 33, "context": "In practice, it is sometimes best to make some heuristic adjustments to the parameters after learning to improve test-time performance [34], [35].", "startOffset": 141, "endOffset": 145}, {"referenceID": 34, "context": "This essentially fits into the paradigm of empirical risk minimization [36], [37].", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "This essentially fits into the paradigm of empirical risk minimization [36], [37].", "startOffset": 77, "endOffset": 81}, {"referenceID": 36, "context": "in the late 1980s [38].", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "1 Univariate Logistic Loss The univariate logistic loss [39] is defined by L(\u03b8,x) = \u2212 \u2211", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "If defined on exact marginals, this is a type of composite likelihood [40].", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "[5] suggest approximating with a sigmoid function S(t) = (1 + exp(\u2212\u03b1t)), where \u03b1 controls approximation quality.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "There is evidence [36], [5] that the smoothed classification loss can yield parameters with lower univariate classification error under MPM inference.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "There is evidence [36], [5] that the smoothed classification loss can yield parameters with lower univariate classification error under MPM inference.", "startOffset": 24, "endOffset": 27}, {"referenceID": 39, "context": "Our experience with the univariate quadratic loss [41] is similar.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "However, if all clique marginals are correct, the joint must be correct, by the standard moment matching conditions for the exponential family [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 40, "context": "4 MAP-Based Training Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 41, "context": "4 MAP-Based Training Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "4 MAP-Based Training Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "4 MAP-Based Training Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "4 MAP-Based Training Another class of methods explicitly optimize the performance of MAP inference [42], [43], [44], [45], [25].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "Domke [46] lists conditions for various classes of entropies that guarantee that A will be differentiable.", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": ", which is accurate to order o(r) [47].", "startOffset": 34, "endOffset": 38}, {"referenceID": 46, "context": "In all cases, the step size is, following Andrei [48], taken to be r = m\u01eb 1 3 (", "startOffset": 49, "endOffset": 53}, {"referenceID": 47, "context": "Welling and Teh used sensitivity of approximate beliefs to parameters to approximate joint probabilities of non-neighboring variables [50].", "startOffset": 134, "endOffset": 138}, {"referenceID": 48, "context": "Usually, a difference of 10 in estimated marginals has little practical impact, but this can still be enough to prevent learning from succeeding [51].", "startOffset": 145, "endOffset": 149}, {"referenceID": 34, "context": "[36], [52].", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[36], [52].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "The noisy input values are then generated as yi = xi(1\u2212ti ) + (1\u2212xi)t n i , where xi is the true binary label, and ti \u2208 [0, 1] is random.", "startOffset": 120, "endOffset": 126}, {"referenceID": 51, "context": "We expand these 5 initial features into a larger set using sinusoidal expansion [54].", "startOffset": 80, "endOffset": 84}], "year": 2013, "abstractText": "Likelihood based-learning of graphical models faces challenges of computational-complexity and robustness to model misspecification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learning performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature.", "creator": "LaTeX with hyperref package"}}}