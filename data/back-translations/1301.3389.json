{"id": "1301.3389", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "abstract": "Non-negative matrix factorization (NMF) has become a popular approach to machine learning for many problems in the fields of word processing, speech and image processing, bioinformatics, and seismic data analysis, to name a few. In the NMF, a matrix of non-negative data is approximated by the low product of two matrices with non-negative entries. In this paper, the approximate quality is measured by the Kullback-Leibler divergence between the data and their low reconstruction. The existence of the simple multiplicative update algorithm (MU) for calculating the matrix factors has contributed to the success of the NMF. Despite the availability of algorithms that exhibit faster convergence, the MU remains popular due to its simplicity. In this paper, it is proposed to show a diagonalized newton algorithm (DNA) of faster convergence, while the implementation of simple and highly available hardware remains applicable to various data sets, resulting in significant data algorithms being publicly available.", "histories": [["v1", "Tue, 15 Jan 2013 15:59:46 GMT  (249kb)", "http://arxiv.org/abs/1301.3389v1", "8 pages + references; International Conference on Learning Representations, 2013"], ["v2", "Mon, 18 Mar 2013 09:15:29 GMT  (251kb)", "http://arxiv.org/abs/1301.3389v2", "8 pages + references; International Conference on Learning Representations, 2013"]], "COMMENTS": "8 pages + references; International Conference on Learning Representations, 2013", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["hugo van hamme"], "accepted": true, "id": "1301.3389"}, "pdf": {"name": "1301.3389.pdf", "metadata": {"source": "CRF", "title": "The Diagonalized Newton Algorithm for Non- negative Matrix Factorization", "authors": ["Hugo Van hamme"], "emails": ["hugo.vanhamme@esat.kuleuven.be"], "sections": [{"heading": null, "text": "Non-negative matrix factorization (NMF) has become a popular machine 6 learning approach to many problems in text mining, speech and image 7 processing, bio-informatics and seismic data analysis to name a few. In 8 NMF, a matrix of non-negative data is approximated by the low-rank 9 product of two matrices with non-negative entries. In this paper, the 10 approximation quality is measured by the Kullback-Leibler divergence 11 between the data and its low-rank reconstruction. The existence of the 12 simple multiplicative update (MU) algorithm for computing the matrix 13 factors has contributed to the success of NMF. Despite the availability of 14 algorithms showing faster convergence, MU remains popular due to its 15 simplicity. In this paper, a diagonalized Newton algorithm (DNA) is 16 proposed showing faster convergence while the implementation remains 17 simple and suitable for high-rank problems. The DNA algorithm is applied 18 to various publicly available data sets, showing a substantial speed-up on 19 modern hardware. 20\n21\n1 Introduction 22\nNon-negative matrix factorization (NMF) denotes the process of factorizing a N\u00d7T data 23 matrix V of non-negative real numbers into the product of a N\u00d7R matrix W and a R\u00d7T 24 matrix H, where both W and H contain only non-negative real numbers. Taking a column-25 wise view of the data, i.e. each of the T columns of V is a sample of N-dimensional vector 26 data, the factorization expresses each sample as a (weighted) addition of columns of W, 27 which can hence be interpreted as the R parts that make up the data [1]. Hence, NMF can be 28 used to learn data representations from samples. In [2], speaker representations are learnt 29 from spectral data using NMF and subsequently applied to separate their signals. Another 30 example in speech processing is [3] and [4], where phone representations are learnt using a 31 convolutional extention of NMF. In [5], time-frequency representations reminiscent of 32 formant traces are learnt from speech using NMF. In [6], NMF is used to learn acoustic 33 representations for words in a vocabulary acquisition and recognition task. Applied to image 34 processing, local features are learnt from examples with NMF in order to represent human 35 faces in a detection task [7]. 36\nIn this paper, the metric to measure the closeness of reconstruction Z = WH to its target V is 37 measured by their Kullback-Leibler divergence: 38\n39\nV, Z = \u2212 , + , , (1) Given a data matrix V, the matrix factors W and H are then found by minimizing cost 40 function (1). The multiplicative updates (MU) algorithm proposed in [1] solves exactly this 41 problem in an iterative manner. Its simplicity and the availability of many implementations 42 make it a popular algorithm to date to solve NMF problems. However, there are some draw-43 backs to the algorithm. Firstly, it only converges locally and is not guaranteed to yield the 44 global minimum of the cost function. It is hence sensitive to the choice of the initial guesses 45 for W and H. Secondly, MU is very slow to converge. The goal of this paper is to speed up 46 the convergence while the local convergence property is retained. The resulting 47 Diagonalized Newton Algorithm (DNA) uses only simple element-wise operations, such that 48 its implementation requires only a few tens of lines of code, while memory requirements and 49 computational efforts for a single iteration are about the double of an MU update. 50\nThe faster convergence rate is obtained by applying Newton\u2019s method to minimize 51 dKL(V,WH) over W and H in alternation. Newton updates have been explored for the Frobe-52 nius norm to measure the distance between V and Z in e.g. [8]-[13]. For the Kullback-53 Leibler divergence, fewer studies are available. Since each optimization problem is multi-54 variate, Newton updates typically imply solving sets of linear equations in each iteration. In 55 [16], the Hessian is reduced by refraining from second order updates for the parameters 56 close to zero. In the proposed method, matrix inversion is avoided by diagonalizing the 57 Hessian matrix. A similar idea was recently explored in [17], but leading to different update 58 formulae. Of course, such an approximation may affect the convergence rate adversely. Also, 59 Newton algorithms only show (quadratic) convergence when the estimate is sufficiently 60 close to the local minimum and therefore need regularization, e.g. Levenberg-Marquardt as 61 in [14], or step size control as in [15] and [16]. In DNA, these convergence issues are 62 addressed by computing both the MU and Newton solutions and selecting the one with the 63 smallest cost. Hence, since the cost is non-decreasing under MU, it will also be under DNA 64 updates. This robust safety net can be constructed fairly efficiently because the variables 65 required to compute the MU have already been computed in the Newton update. The net 66 result is that DNA iterations are only about two to three times as slow as MU iterations, both 67 on a CPU and on a GPU. The experimental analysis shows that the increased convergence 68 rate generally dominates over the increased cost per iteration such that overall balance is 69 positive and can lead to speedups of up to a factor 6. 70"}, {"heading": "2 NMF formulation 71", "text": "To induce sparsity on the matrix factors, the KL-divergence is often regularized, i.e. one 72 seeks to minimize: 73\nV, W + \u03c1 ,\n+ \u03bb \u210e ,\n(2)\nsubject to non-negativity constraints on all entries of W and H. Here, \u03c1 and \u03bb are non-negative re-74 gularization parameters. 75\nMinimizing the regularized KL-divergence (2) is achieved by alternating updates of W and H for 76 which the cost is non-increasing. Hence, the updates are: 77\n\u2190 arg min # \u2265 % & V, W # + \u03bb \u210e # , ' (3)\n( \u2190 arg min(# \u2265 % & V, ( # + \u03c1 # , ' (4)\nBecause of the symmetry property dKL(V,WH) = dKL(V t ,H t W t ), where superscript-t denotes 78\nmatrix transpose, it suffices to consider only the update on H. Furthermore, because of the 79 summation over all columns in (1), minimization (3) splits up into T independent optimiza-80 tion problems. Let v denote any column of V and let h denote the corresponding column of H, 81 then the following is the core minimization problem to be considered: 82\n) \u2190 arg min)# \u2265 % V, W ) # + \u03bb* )# (5)\nwhere 1 denotes a vector of ones of appropriate length. The solution of (5) should satisfy the KKT 83 conditions, i.e. for all r with hr > 0 84\n+ v , W ) + \u03bb* ) +\u210e = \u2212 W h + + \u03bb = 0\nwhere hr denotes the r-th component of h. If hr = 0, the partial derivative is positive. Hence the 85 product of hr and the partial derivative is always zero, i.e. for r = 1 \u2026 R: 86 \u210e W h \u2212 \u210e & + \u03bb' = 0 (6) Since W-columns with all-zeros do not contribute to Z, it can be assumed that column sums of W 87 are non-zero, so the above can be recast as: 88 \u210e W q W 1 + \u03bb \u2212 \u210e = 0 where qn = vn / (Wh)n. To facilitate the derivations below, the following notations are introduced: 89 1 = W q W 1 + \u03bb \u2212 1 (7) which are functions of h via q. The KKT conditions are hence recast as 90 1 \u210e = 0 for r = 1 \u2026 R (8) Finally, summing (6) over r yields 91\n( * + \u03bb \u210e =\n(9)\nwhich is satisfied for any guess h by renormalizing: 92 \u210e \u21e0 \u210e v 1) ( * + \u03bb (10)"}, {"heading": "2.1 Multiplicat ive updates 93", "text": "For the more generic class of Bregman divergences, it was shown in a.o. [19] that multipli-94 cative updates (MU) are non-decreasing at each update of W and H. For KL-divergence, MU 95 are identical to a fixed point update of (6), i.e. 96 \u210e \u21e0 \u210e 1 + 1 = \u210e W q W 1 + \u03bb (11) Update (11) has two fixed points: hr = 0 and ar = 0. In the former case, the KKT conditions imply 97 that ar is negative. 98"}, {"heading": "2.2 Newton updates 99", "text": "To find the stationary points of (2), R equations (8) need to be solved for h. In general, let g(h) be 100 an R-dimensional vector function of an R-dimensional variable h. Newton\u2019s update then states: 101\n) \u21e0 ) \u2212 \u22078 9:8 ) (12)\nwith 102 \u22078 ; = \u2202 ) \u2202\u210e; Applied to equations (8): 103\n\u22078 ; = 1;= ; \u2212 \u210e W 1 + \u03bb > > >; () >?> (13)\nwhere \u03b4rl is Kronecker\u2019s delta. To avoid the matrix inversion in update (12), the last term in 104 equation (13) is diagonalized, which is equivalent to solving the r-th equation in (8) for hr with all 105 other components fixed. With 106\n@ = 1 W 1 + \u03bb ? W h ? (14)\nwhich is always positive, an element-wise Newton update for h is obtained: 107 \u210e \u21e0 \u210e \u210e @ \u210e @ \u2212 1 (15) Notice that this update does not automatically satisfy (9), so updates should be followed by a 108 renormalization (10). One needs to pay attention to the fact that Newton updates will attract 109 towards both local minima and local maxima. Like for the EM-update, hr = 0 and ar = 0 are the 110 only fixed points of update (15), which are now shown to be locally stable. In case the optimizer is 111 at hr = 0, ar is negative by the KKT conditions, and update (15) will indeed decrease hr. In a 112 sufficiently small neighborhood of a point where the gradient vanishes, i.e. ar = 0, update (15) will 113 increase (decrease) hr if and only if (11) increases (decreases) its estimate. Since if (11) never 114 increases the cost, update (15) attracts to a minimum. 115\nHowever, this only guarantees local convergence for per-element updates and Newton methods 116 are known to suffer from potentially small convergence regions. This also applies to update (15), 117 which can indeed result in limit cycles in some cases. In the next subsections, two measures are 118 taken to respectively increase the convergence region and to make the update non-increasing. 119\n2.3 Step size l imitat ion 120\nWhen ar is positive, update (15) may not be well-behaved in the sense that its denominator can 121 become negative or zero. Therefore, it is bounded below by a function with the same local 122 behavior around zero: 123\n\u210e @ \u210e @ \u2212 1 = 1 1 \u2212 1 \u210e @ \u2265 1 + 1 \u210e @\nHence, if ar \u2265 0, the following update is used:\n(16)\n\u210e \u21e0 \u210e 1 + 1 \u210e @ = \u210e + 1 @ (17)\nFinally, step sizes are further limited by flooring resp. ceiling the multiplicative gain applied to hr 124 in update (15) and (17) (see Algorithm 1, steps 11 and 24 for details). 125\n2.4 Non- increase of the cost 126\nDespite the measures taken in section 2.3, the divergence can still increase under the Newton 127 update. A very safe option is to compute the EM update additionally and compare the cost 128 function value for both updates. If the EM update is be better, the Newton update is rejected and 129 the EM update is taken instead. This will guarantee non-increase of the cost function. The 130 computational cost of this operation is dominated by evaluating the KL-divergence, not in 131 computing the update itself. 132\n3 The Diagonalized Newton Algorithm for KLD-NMF 133\nIn Algorithm 1, the arguments given above are joined to form the Diagonalized Newton Algorithm 134\n(DNA) for NMF with Kullback-Leibler divergence cost. Matlab TM code is available from 135 www.esat.kuleuven.be/psi/spraak/downloads both for the case when V is sparse or dense. 136\n137 Algorithm 1: pseudocode for the DNA KLD-NMF algorithm. \u2298 and \u2299 are element-wise division 138 and multiplication respectively and [x]\u03b5 = max(x,\u03b5). Steps not labelsed with \u201cMU\u201d is the additional 139 code required for DNA. 140\nInput: data V, initial guess for W and H, regularization weights \u03c1 and \u03bb. 141 MU - Step 1: divide the r-th column of W by \u2211 + \u03bb. Multiply the r-th row of H by the same number. 142 MU - Step 2: Z = WH 143 MU - Step 3: D = E\u2298 F 144 Repeat until convergence 145 MU - Step 4: precompute (\u2a00( 146 MU - Step 5: H = ( D \u2212 1 147 MU - Step 6: IJ = + H\u2a00 148 MU - Step 7: FIJ = ( IJ 149 MU - Step 8: DKL = E\u2298 FIJ 150 MU - Step 9: MIJ = * NE\u2a00log DIJ P 151 Step 10: DQ = E\u2298 R\u2a00R ; S = (\u2a00( TDQ 152 Step 11: UVW = \u2299 XS\u2298 S \u2212 H YZfor the entries for which A < 0 153 UVW = +min H \u2298 \u2299S , [ for the entries for which A \u2265 0 154\nmultiply t-th column of HDNA with the t-th entry of * E \u2298 * UVW 155 Step 12: FUVW = ( UVW 156 Step 13: D\\]^ = E\u2298 F\\]^ 157 Step 14: MUVW = * NE\u2a00log DUVW P 158 Step 15: copy H, Z and Q from: 159 HDNA, ZDNA and QDNA for the columns for which dDNA < dMU 160 HEM, ZEM and QEM for the columns for which dDNA \u2265 dMU 161 MU - Step 16: divide (multiply) the r-th row (column) of H (W) by \u2211 \u210e + \u03c1. 162 Step 17: precompute \u2a00 163 MU - Step 18: H = D T \u2212 1 164 MU - Step 19: (IJ = (+ H\u2a00( 165 MU - Step 20: FIJ = (IJ 166 MU - Step 21: DKL = E\u2298 FIJ 167 MU - Step 22: MIJ = NE\u2a00log DIJ P* 168 Step 23: DQ = E\u2298 R\u2a00R ; S = DQ \u2a00 T 169 Step 24: (UVW = (\u2299 XS\u2298 S \u2212 H YZfor the entries for which A < 0 170 (UVW = ( +min H\u2298 \u2299 S , [( for the entries for which A \u2265 0 171\nmultiply the n-th row of WDNA with the n-th entry of E* \u2298 (UVW* 172 Step 25: FUVW = (UVW 173 Step 26: D\\]^ = E\u2298 F\\]^ 174 Step 27: MUVW = NE\u2a00log DUVW P* 175 Step 28: copy W, Z and Q from: 176 WDNA, ZDNA and QDNA for the rows for which dDNA < dMU 177 WEM, ZEM and QEM for the rows for which dDNA \u2265 dMU 178 MU - Step 29: divide(multiply) the r-th column (row) of W (H) by \u2211 + \u03bb. 179 180\nNotice that step 9, 14 22 and 27 require some care for the zeros in V, which should not contribute 181 to the cost. In terms of complexity, the most expensive steps are the computation of A, B, ZMU and 182\nZDNA, which require O(NRT) operations. All other steps require O(NR), O(RT) or O(NR) 183 operations. Hence, it is expected that a DNA iteration is about twice as slow as MU iteration. On 184 modern hardware, parallelization may however distort this picture and hence experimental 185 verification is requied. 186\n4 Experiments 187\nDNA and MU are run on several publicly available 1 data sets. In all cases, W is initialized with a 188 random matrix with uniform distribution, normalized column-wise. Then H is initialized as W t V 189 and one MU iteration is performed. The same initial values are used for both algorithms. Sparsity 190\nis not included in this study, so \u03c1 = \u03bb = 0. The algorithm parameters are set to \u03b5 = 0.01 and \u03b1=4. 191\nCPU timing measurements are obtained on a quad-core AMD TM Opteron 8356 processor running 192\nthe MATLAB TM code available at www.esat.kuleuven.be/psi/spraak/downloads which uses the 193 built-in parallelization capability. Timing measurements on the graphical processing unit (GPU) 194 are obtained on a TESLA C2070 running MATLAB and Accelereyes Jacket v2.3. 195\n4.1 Dense data matrices 196\nThe first dataset considered is a set of 400 frontal face greyscale 64\u00d764 images of 40 people 197\nshowing 10 different expressions. The resulting 4096\u00d7165 dense matrix is decomposed with 198 factors of a common dimension R of 10, 20, 40 and 80. Figure 1 shows the KL divergence as a 199 function of iteration number and CPU time as measured on the CPU. The superiority of DNA is 200 obvious: for instance, at R = 40, DNA reaches the same divergence after 33 iterations as MU 201 obtains after 500 iterations. This implies a speed-up of a factor 15 in terms of iterations or 6.3 in 202 terms of CPU time. 203\nThe second test case is the CMU PIE dataset which consists of 11554 greyscale images of 32\u00d732 207 pixels showing human faces under different illumination conditions and poses. The data are 208\nshaped to a dense 1024\u00d711554 matrix and a decomposition of rank R = 10, 20, 40 and 80 are 209 attempted with the MU and DNA algorithms. The proposed DNA still outperforms MU, but by a 210 smaller margin. 211\n1 www.cad.zju.edu.cn/home/dengcai/Data/data.html\nAn overview of the time required for a single iteration on both data sets is given in Table 1. For 215 MU, the first row lists the time if the KL divergence is not computed as this is not required if the 216 number of iterations is fixed in advance instead of stopping the algorithm based on a decrease in 217 KLD. The table shows that the computational cost of MU can be reduced by about a third by not 218 computing KLD. Compared to MU with cost calculation, DNA requires typically about 2.5 to 3 219 times more time per iteration on the CPU. On the GPU, the ratio is rather 2 to 2.5. 220\n4.2 Sparse data matr ices 221\nThe third matrix considered originates from the NIST Topic Detection and Tracking Corpus 222 (TDT2). For 10212 documents (columns of V), the frequency of 36771 terms (rows of V) was 223\ncounted leading to a sparse 36771\u00d710212 matrix with only 0.35% non-zero entries. The fourth 224\nmatrix originates from the Newsgroup corpus results in a 61188\u00d718774 sparse frequency matrix 225 with 0.2% non-zeros. Both for MU and DNA a MATLAB implementation using the sparse matrix 226 class was made. In this case, an iteration of DNA is twice as slow a MU iteration. Again, the 227 convergence of both algorithms is shown in Figure 1. In this case, DNA is only marginally faster 228 than MU in terms of CPU time. 229\n230\nDNA 269 15.5 280 15.9 319 17.9 425 23.6 1180 71 1430 76 1720 95 1960 125\n233 234\n5 Conclusions 237\nThe DNA algorithm is based on Newton\u2019s method for solving the stationarity conditions of the 238 constrained optimization problem implied by NMF. This paper only addresses the Kullback-239 Leibler divergence as a cost function. To avoid matrix inversion, a diagonal approximation is 240 made, resulting in element-wise updates. Experimental verification on publicly available matrices 241 with a CPU and GPU MATLAB implementation for dense data matrices and a CPU MATLAB 242 implementation for sparse data matrices show that, depending on the case and matrix sizes, DNA 243 iterations are 2 to 3 times slower than MU iterations. In most cases, the diagonal approximation is 244 good enough such that faster convergence is observed and a net gain results. 245\nSince Newton updates can in general not ensure monotonic decrease of the cost function, the step 246 size was controlled with a brute force strategy of falling back to MU in case the cost is increased. 247 More refined step damping methods could speed up DNA by avoiding evaluations of the cost 248 function, which is next on the research agenda. 249\nAc kno w ledg me nts 250\nThis work is supported by IWT-SBO project 100049 (ALADIN) and by KU Leuven research 251 grant OT/09/028(VASI). 252\nReferences 253\n[1] D. Lee, and H. Seung, \u201cAlgorithms for non-negative matrix factorization,\u201d Advances in Neural 254 Information Processing Systems, vol. 13, pp. 556\u2013562, 2001. 255\n[2] B. Raj, R. Singh and P. Smaragdis, \u201cRecognizing Speech from Simultaneous Speakers\u201d, in proceedings 256 of Eurospeech, pp. 3317-3320, Lisbon, Portugal, September 2005 257\n[3] P. Smaragdis, \u201cConvolutive Speech Bases and Their Application to Supervised Speaker Separation,\u201d 258 IEEE Transactions on Audio, Speech and Language Processing, vol. 15, pp. 1-12, January 2007 259\n[4] P. D. O'Grady and B. A. Pearlmutter, \u201cDiscovering Speech Phones Using Convolutive Non-negative 260 Matrix Factorisation with a Sparseness Constraint.,\u201d Neurocomputing, vol. 72, no. 1-3, pp. 88-101, December 261 2008, ISSN 0925-2312. 262\n[5] M. Van Segbroeck and H. Van hamme, \u201cUnsupervised learning of time-frequency patches as a noise-263 robust representation of speech,\u201d Speech Communication, volume 51, no. 11, pp. 1124-1138, November 264 2009. 265\n[6] H. Van hamme, \u201cHAC-models: a Novel Approach to Continuous Speech Recognition,\u201d In Proc. 266 International Conference on Spoken Language Processing, pp. 2554-2557, Brisbane, Australia, September 267 2008. 268\n[7] X. Chen, L. Gu, S. Z. Li and H.-J. Zhang, \u201cLearning representative local features for face detection,\u201d in 269 proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 270 1126-1131, Kauai, HI, USA, December 2001. 271\n[8] D. Kim, S. Sra and I. S. Dhillon, \u201cFast Projection-Based Methods for the Least Squares Nonnegative 272 Matrix Approximation Problem,\u201d Statistical Analy Data Mining, vol. 1, 2008 273\n[9] C.-J. Lin, \u201cProjected gradient methods for non-negative matrix factorization,\u201d Neural Computation, vol. 274 19, pp. 2756-2779, 2007 275\n[10] R. Zdunek, A. H. Phan and A. Cichocki, \u201cDamped Newton Iterations for Nonnegative Matrix 276 Factorization,\u201d Australian Journal of Intelligent Information Processing Systems, 12(1), pp. 16-22, 2010 277\n[11] Y. Zheng, and Q. Zhang, \u201cDamped Newton based Iterative Non-negative Matrix Factorization for 278 Intelligent Wood Defects Detection,\u201d Journal of software, vol. 5, no. 8, pp. 899-906, August 2010. 279\n[12] P. Gong, and C. Zhang, \u201cEfficient Nonnegative Matrix Factorization via projected Newton method\u201d, 280 Pattern Recognition, vol. 45, no. 9, pp. 3557-3565, September 2012. 281\n[13] S. Bellavia, M. Macconi, and B. Morini, \u201cAn interior point Newton-like method for nonnegative least-282 squares problems with degenerate solution,\u201d Numerical Linear Algebra with Applications, vol. 13, no. 10, pp. 283 825-846, December 2006. 284\n[14] R. Zdunek and A. Cichocki, \u201cNon-Negative Matrix Factorization with Quasi-Newton Optimization,\u201d 285 Lecture Notes in Computer Science, Artificial Intelligence and Soft Computing 4029, pp. 870-879, 2006 286\n[15] R. Zdunek and A. Cichocki, \"Nonnegative Matrix Factorization with Constrained Second-Order 287 Optimization\", Signal Processing, vol. 87, pp. 1904-1916, 2007 288\n[16] G. Landi and E. Loli Piccolomini, \u201cA projected Newton-CG method for nonnegative astronomical image 289 deblurring,\u201d Numerical Algorithms, no. 48, pp. 279\u2013300, 2008 290\n[17] L. Li, G. Lebanon and H. Park, \u201cFast Bregman Divergence NMF using Taylor Expansion and 291 Coordinate Descent,\u201d in proceedings of the 18th ACM SIGKDD Conference on Knowledge Discovery and 292 Data Mining, 2012 293\n[18] A. Cichocki, S. Cruces, and S.-I. Amari, \u201cGeneralized Alpha-Beta Divergences and Their Application to 294 Robust Nonnegative Matrix Factorization,\u201d Entropy, vol. 13, pp. 134-170, 2011; doi:10.3390/e13010134 295\n[19] I. S. Dhillon and S. Sra, \u201cGeneralized Nonnegative Matrix Approximations with Bregman Divergences,\u201d 296 Neural Information Proc. Systems, pp. 283-290, 2005 297"}], "references": [{"title": "Algorithms for non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Advances in Neural  254 Information Processing Systems, vol. 13, pp. 556\u2013562, 2001.  255", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Recognizing Speech from Simultaneous Speakers\u201d, in proceedings  256 of Eurospeech", "author": ["B. Raj", "R. Singh", "P. Smaragdis"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Convolutive Speech Bases and Their Application to Supervised Speaker Separation", "author": ["P. Smaragdis"], "venue": " 258 IEEE Transactions on Audio, Speech and Language Processing, vol. 15, pp. 1-12, January 2007  259", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Discovering Speech Phones Using Convolutive Non-negative  260 Matrix Factorisation with a Sparseness Constraint", "author": ["P.D. O'Grady", "B.A. Pearlmutter"], "venue": "Neurocomputing, vol. 72, no. 1-3, pp. 88-101, December  261 2008, ISSN 0925-2312.  262", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised learning of time-frequency patches as a noise-  263 robust representation of speech", "author": ["M. Van Segbroeck", "H. Van hamme"], "venue": "Speech Communication, volume 51, no. 11, pp. 1124-1138, November 264 2009.  265", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "HAC-models: a Novel Approach to Continuous Speech Recognition", "author": ["H. Van hamme"], "venue": "Proc.  266 International Conference on Spoken Language Processing, pp. 2554-2557, Brisbane, Australia, September 267 2008.  268", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning representative local features for face detection", "author": ["X. Chen", "L. Gu", "S.Z. Li", "H.-J. Zhang"], "venue": " 269 proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp.  270 1126-1131, Kauai, HI, USA, December 2001.  271", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast Projection-Based Methods for the Least Squares Nonnegative 272 Matrix Approximation Problem", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Statistical Analy Data Mining, vol. 1, 2008  273", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Computation, vol. 274 19, pp. 2756-2779, 2007  275", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Damped Newton Iterations for Nonnegative Matrix 276 Factorization", "author": ["R. Zdunek", "A.H. Phan", "A. Cichocki"], "venue": "Australian Journal of Intelligent Information Processing Systems, 12(1), pp. 16-22, 2010  277", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Damped Newton based Iterative Non-negative Matrix Factorization for 278 Intelligent Wood Defects Detection", "author": ["Y. Zheng", "Q. Zhang"], "venue": "Journal of software, vol. 5, no. 8, pp. 899-906, August 2010.  279", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonnegative Matrix Factorization via projected Newton method", "author": ["P. Gong", "C. Zhang", "\u201cEfficient"], "venue": "Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An interior point Newton-like method for nonnegative least-  282 squares problems with degenerate solution", "author": ["S. Bellavia", "M. Macconi", "B. Morini"], "venue": "Numerical Linear Algebra with Applications, vol. 13, no. 10, pp.  283 825-846, December 2006.  284", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Non-Negative Matrix Factorization with Quasi-Newton Optimization", "author": ["R. Zdunek", "A. Cichocki"], "venue": " 285 Lecture Notes in Computer Science, Artificial Intelligence and Soft Computing 4029, pp. 870-879, 2006  286", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonnegative Matrix Factorization with Constrained Second-Order  287 Optimization", "author": ["R. Zdunek", "A. Cichocki"], "venue": "Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A projected Newton-CG method for nonnegative astronomical image  289 deblurring", "author": ["G. Landi", "E. Loli Piccolomini"], "venue": "Numerical Algorithms, no. 48, pp. 279\u2013300, 2008  290", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast Bregman Divergence NMF using Taylor Expansion and  291 Coordinate Descent", "author": ["L. Li", "G. Lebanon", "H. Park"], "venue": "proceedings of the 18 ACM SIGKDD Conference on Knowledge Discovery and 292 Data Mining, 2012  293", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalized Alpha-Beta Divergences and Their Application to  294 Robust Nonnegative Matrix Factorization", "author": ["A. Cichocki", "S. Cruces", "S.-I. Amari"], "venue": "Entropy, vol. 13, pp. 134-170, 2011; doi:10.3390/e13010134  295", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized Nonnegative Matrix Approximations with Bregman Divergences", "author": ["I.S. Dhillon", "S. Sra"], "venue": "296 Neural Information Proc. Systems, pp. 283-290, 2005  297", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "each of the T columns of V is a sample of N-dimensional vector 26 data, the factorization expresses each sample as a (weighted) addition of columns of W, 27 which can hence be interpreted as the R parts that make up the data [1].", "startOffset": 225, "endOffset": 228}, {"referenceID": 1, "context": "In [2], speaker representations are learnt 29 from spectral data using NMF and subsequently applied to separate their signals.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Another 30 example in speech processing is [3] and [4], where phone representations are learnt using a 31 convolutional extention of NMF.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "Another 30 example in speech processing is [3] and [4], where phone representations are learnt using a 31 convolutional extention of NMF.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "In [5], time-frequency representations reminiscent of 32 formant traces are learnt from speech using NMF.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [6], NMF is used to learn acoustic 33 representations for words in a vocabulary acquisition and recognition task.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Applied to image 34 processing, local features are learnt from examples with NMF in order to represent human 35 faces in a detection task [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "The multiplicative updates (MU) algorithm proposed in [1] solves exactly this 41 problem in an iterative manner.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "[8]-[13].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[8]-[13].", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "In 55 [16], the Hessian is reduced by refraining from second order updates for the parameters 56 close to zero.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "A similar idea was recently explored in [17], but leading to different update 58 formulae.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Levenberg-Marquardt as 61 in [14], or step size control as in [15] and [16].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "Levenberg-Marquardt as 61 in [14], or step size control as in [15] and [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "Levenberg-Marquardt as 61 in [14], or step size control as in [15] and [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "[19] that multipli94 cative updates (MU) are non-decreasing at each update of W and H.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "5 Non-negative matrix factorization (NMF) has become a popular machine 6 learning approach to many problems in text mining, speech and image 7 processing, bio-informatics and seismic data analysis to name a few. In 8 NMF, a matrix of non-negative data is approximated by the low-rank 9 product of two matrices with non-negative entries. In this paper, the 10 approximation quality is measured by the Kullback-Leibler divergence 11 between the data and its low-rank reconstruction. The existence of the 12 simple multiplicative update (MU) algorithm for computing the matrix 13 factors has contributed to the success of NMF. Despite the availability of 14 algorithms showing faster convergence, MU remains popular due to its 15 simplicity. In this paper, a diagonalized Newton algorithm (DNA) is 16 proposed showing faster convergence while the implementation remains 17 simple and suitable for high-rank problems. The DNA algorithm is applied 18 to various publicly available data sets, showing a substantial speed-up on 19 modern hardware. 20 21 1 Introduction 22 Non-negative matrix factorization (NMF) denotes the process of factorizing a N\u00d7T data 23 matrix V of non-negative real numbers into the product of a N\u00d7R matrix W and a R\u00d7T 24 matrix H, where both W and H contain only non-negative real numbers. Taking a column25 wise view of the data, i.e. each of the T columns of V is a sample of N-dimensional vector 26 data, the factorization expresses each sample as a (weighted) addition of columns of W, 27 which can hence be interpreted as the R parts that make up the data [1]. Hence, NMF can be 28 used to learn data representations from samples. In [2], speaker representations are learnt 29 from spectral data using NMF and subsequently applied to separate their signals. Another 30 example in speech processing is [3] and [4], where phone representations are learnt using a 31 convolutional extention of NMF. In [5], time-frequency representations reminiscent of 32 formant traces are learnt from speech using NMF. In [6], NMF is used to learn acoustic 33 representations for words in a vocabulary acquisition and recognition task. Applied to image 34 processing, local features are learnt from examples with NMF in order to represent human 35 faces in a detection task [7]. 36 In this paper, the metric to measure the closeness of reconstruction Z = WH to its target V is 37 measured by their Kullback-Leibler divergence: 38 39 V, Z = \u2212 , + , , (1) Given a data matrix V, the matrix factors W and H are then found by minimizing cost 40 function (1). The multiplicative updates (MU) algorithm proposed in [1] solves exactly this 41 problem in an iterative manner. Its simplicity and the availability of many implementations 42 make it a popular algorithm to date to solve NMF problems. However, there are some draw43 backs to the algorithm. Firstly, it only converges locally and is not guaranteed to yield the 44 global minimum of the cost function. It is hence sensitive to the choice of the initial guesses 45 for W and H. Secondly, MU is very slow to converge. The goal of this paper is to speed up 46 the convergence while the local convergence property is retained. The resulting 47 Diagonalized Newton Algorithm (DNA) uses only simple element-wise operations, such that 48 its implementation requires only a few tens of lines of code, while memory requirements and 49 computational efforts for a single iteration are about the double of an MU update. 50 The faster convergence rate is obtained by applying Newton\u2019s method to minimize 51 dKL(V,WH) over W and H in alternation. Newton updates have been explored for the Frobe52 nius norm to measure the distance between V and Z in e.g. [8]-[13]. For the Kullback53 Leibler divergence, fewer studies are available. Since each optimization problem is multi54 variate, Newton updates typically imply solving sets of linear equations in each iteration. In 55 [16], the Hessian is reduced by refraining from second order updates for the parameters 56 close to zero. In the proposed method, matrix inversion is avoided by diagonalizing the 57 Hessian matrix. A similar idea was recently explored in [17], but leading to different update 58 formulae. Of course, such an approximation may affect the convergence rate adversely. Also, 59 Newton algorithms only show (quadratic) convergence when the estimate is sufficiently 60 close to the local minimum and therefore need regularization, e.g. Levenberg-Marquardt as 61 in [14], or step size control as in [15] and [16]. In DNA, these convergence issues are 62 addressed by computing both the MU and Newton solutions and selecting the one with the 63 smallest cost. Hence, since the cost is non-decreasing under MU, it will also be under DNA 64 updates. This robust safety net can be constructed fairly efficiently because the variables 65 required to compute the MU have already been computed in the Newton update. The net 66 result is that DNA iterations are only about two to three times as slow as MU iterations, both 67 on a CPU and on a GPU. The experimental analysis shows that the increased convergence 68 rate generally dominates over the increased cost per iteration such that overall balance is 69 positive and can lead to speedups of up to a factor 6. 70 2 NMF formulation 71 To induce sparsity on the matrix factors, the KL-divergence is often regularized, i.e. one 72 seeks to minimize: 73 V, W + \u03c1 , + \u03bb h , (2) subject to non-negativity constraints on all entries of W and H. Here, \u03c1 and \u03bb are non-negative re74 gularization parameters. 75 Minimizing the regularized KL-divergence (2) is achieved by alternating updates of W and H for 76 which the cost is non-increasing. Hence, the updates are: 77 \u2190 arg min # \u2265 % & V, W # + \u03bb h # , ' (3) ( \u2190 arg min (# \u2265 % & V, ( # + \u03c1 # , ' (4) Because of the symmetry property dKL(V,WH) = dKL(V t ,H t W t ), where superscript-t denotes 78 matrix transpose, it suffices to consider only the update on H. Furthermore, because of the 79 summation over all columns in (1), minimization (3) splits up into T independent optimiza80 tion problems. Let v denote any column of V and let h denote the corresponding column of H, 81 then the following is the core minimization problem to be considered: 82 ) \u2190 arg min )# \u2265 % V, W ) # + \u03bb* )# (5) where 1 denotes a vector of ones of appropriate length. The solution of (5) should satisfy the KKT 83 conditions, i.e. for all r with hr > 0 84 + v , W ) + \u03bb* ) +h = \u2212 \u0004W h + + \u03bb = 0 where hr denotes the r-th component of h. If hr = 0, the partial derivative is positive. Hence the 85 product of hr and the partial derivative is always zero, i.e. for r = 1 ... R: 86 h W h \u2212 h & + \u03bb' = 0 (6) Since W-columns with all-zeros do not contribute to Z, it can be assumed that column sums of W 87 are non-zero, so the above can be recast as: 88 h W q W 1 + \u03bb \u2212 h = 0 where qn = vn / (Wh)n. To facilitate the derivations below, the following notations are introduced: 89 1 = W q W 1 + \u03bb \u2212 1 (7) which are functions of h via q. The KKT conditions are hence recast as 90 1 h = 0 for r = 1 ... R (8) Finally, summing (6) over r yields 91 ( * + \u03bb h = (9) which is satisfied for any guess h by renormalizing: 92 h \u21e0 h v 1 ) ( * + \u03bb (10) 2.1 Multiplicat ive updates 93 For the more generic class of Bregman divergences, it was shown in a.o. [19] that multipli94 cative updates (MU) are non-decreasing at each update of W and H. For KL-divergence, MU 95 are identical to a fixed point update of (6), i.e. 96 h \u21e0 h 1 + 1 = h W q W 1 + \u03bb (11) Update (11) has two fixed points: hr = 0 and ar = 0. In the former case, the KKT conditions imply 97 that ar is negative. 98 2.2 Newton updates 99 To find the stationary points of (2), R equations (8) need to be solved for h. In general, let g(h) be 100 an R-dimensional vector function of an R-dimensional variable h. Newton\u2019s update then states: 101 ) \u21e0 ) \u2212 \u22078 9:8 ) (12) with 102 \u22078 ; = \u2202 ) \u2202h; Applied to equations (8): 103 \u22078 ; = 1;= ; \u2212 h W 1 + \u03bb > > >; () >? > (13) where \u03b4rl is Kronecker\u2019s delta. To avoid the matrix inversion in update (12), the last term in 104 equation (13) is diagonalized, which is equivalent to solving the r-th equation in (8) for hr with all 105 other components fixed. With 106 @ = 1 W 1 + \u03bb ? W h ? (14) which is always positive, an element-wise Newton update for h is obtained: 107 h \u21e0 h h @ h @ \u2212 1 (15) Notice that this update does not automatically satisfy (9), so updates should be followed by a 108 renormalization (10). One needs to pay attention to the fact that Newton updates will attract 109 towards both local minima and local maxima. Like for the EM-update, hr = 0 and ar = 0 are the 110 only fixed points of update (15), which are now shown to be locally stable. In case the optimizer is 111 at hr = 0, ar is negative by the KKT conditions, and update (15) will indeed decrease hr. In a 112 sufficiently small neighborhood of a point where the gradient vanishes, i.e. ar = 0, update (15) will 113 increase (decrease) hr if and only if (11) increases (decreases) its estimate. Since if (11) never 114 increases the cost, update (15) attracts to a minimum. 115 However, this only guarantees local convergence for per-element updates and Newton methods 116 are known to suffer from potentially small convergence regions. This also applies to update (15), 117 which can indeed result in limit cycles in some cases. In the next subsections, two measures are 118 taken to respectively increase the convergence region and to make the update non-increasing. 119 2.3 Step size l imitat ion 120 When ar is positive, update (15) may not be well-behaved in the sense that its denominator can 121 become negative or zero. Therefore, it is bounded below by a function with the same local 122 behavior around zero: 123 h @ h @ \u2212 1 = 1 1 \u2212 1 h @ \u2265 1 + 1 h @ Hence, if ar \u2265 0, the following update is used: (16) h \u21e0 h 1 + 1 h @ = h + 1 @ (17) Finally, step sizes are further limited by flooring resp. ceiling the multiplicative gain applied to hr 124 in update (15) and (17) (see Algorithm 1, steps 11 and 24 for details). 125 2.4 Nonincrease of the cost 126 Despite the measures taken in section 2.3, the divergence can still increase under the Newton 127 update. A very safe option is to compute the EM update additionally and compare the cost 128 function value for both updates. If the EM update is be better, the Newton update is rejected and 129 the EM update is taken instead. This will guarantee non-increase of the cost function. The 130 computational cost of this operation is dominated by evaluating the KL-divergence, not in 131 computing the update itself. 132 3 The Diagonalized Newton Algorithm for KLD-NMF 133 In Algorithm 1, the arguments given above are joined to form the Diagonalized Newton Algorithm 134 (DNA) for NMF with Kullback-Leibler divergence cost. Matlab TM code is available from 135 www.esat.kuleuven.be/psi/spraak/downloads both for the case when V is sparse or dense. 136 137 Algorithm 1: pseudocode for the DNA KLD-NMF algorithm. \u2298 and \u2299 are element-wise division 138 and multiplication respectively and [x]\u03b5 = max(x,\u03b5). Steps not labelsed with \u201cMU\u201d is the additional 139 code required for DNA. 140 Input: data V, initial guess for W and H, regularization weights \u03c1 and \u03bb. 141 MU Step 1: divide the r-th column of W by \u2211 \r + \u03bb. Multiply the r-th row of H by the same number. 142 MU Step 2: Z = WH 143 MU Step 3: D = E\u2298 F 144 Repeat until convergence 145 MU Step 4: precompute (\u2a00( 146 MU Step 5: H = ( D \u2212 1 147 MU Step 6: IJ = + H\u2a00 148 MU Step 7: FIJ = ( IJ 149 MU Step 8: DKL = E\u2298 FIJ 150 MU Step 9: MIJ = * NE\u2a00log DIJ P 151 Step 10: DQ = E\u2298 R\u2a00R ; S = (\u2a00( TDQ 152 Step 11: UVW = \u2299 XS\u2298 S \u2212 H YZfor the entries for which A < 0 153 UVW = +min H \u2298 \u2299S , [ for the entries for which A \u2265 0 154 multiply t-th column of HDNA with the t-th entry of * E \u2298 * UVW 155 Step 12: FUVW = ( UVW 156 Step 13: D\\]^ = E\u2298 F\\]^ 157 Step 14: MUVW = * NE\u2a00log DUVW P 158 Step 15: copy H, Z and Q from: 159 HDNA, ZDNA and QDNA for the columns for which dDNA < dMU 160 HEM, ZEM and QEM for the columns for which dDNA \u2265 dMU 161 MU Step 16: divide (multiply) the r-th row (column) of H (W) by \u2211 h + \u03c1. 162 Step 17: precompute \u2a00 163 MU Step 18: H = D T \u2212 1 164 MU Step 19: (IJ = (+ H\u2a00( 165 MU Step 20: FIJ = (IJ 166 MU Step 21: DKL = E\u2298 FIJ 167 MU Step 22: MIJ = NE\u2a00log DIJ P* 168 Step 23: DQ = E\u2298 R\u2a00R ; S = DQ \u2a00 T 169 Step 24: (UVW = (\u2299 XS\u2298 S \u2212 H YZfor the entries for which A < 0 170 (UVW = ( +min H\u2298 \u2299 S , [( for the entries for which A \u2265 0 171 multiply the n-th row of WDNA with the n-th entry of E* \u2298 (UVW* 172 Step 25: FUVW = (UVW 173 Step 26: D\\]^ = E\u2298 F\\]^ 174 Step 27: MUVW = NE\u2a00log DUVW P* 175 Step 28: copy W, Z and Q from: 176 WDNA, ZDNA and QDNA for the rows for which dDNA < dMU 177 WEM, ZEM and QEM for the rows for which dDNA \u2265 dMU 178 MU Step 29: divide(multiply) the r-th column (row) of W (H) by \u2211 \r + \u03bb. 179 180 Notice that step 9, 14 22 and 27 require some care for the zeros in V, which should not contribute 181 to the cost. In terms of complexity, the most expensive steps are the computation of A, B, ZMU and 182 ZDNA, which require O(NRT) operations. All other steps require O(NR), O(RT) or O(NR) 183 operations. Hence, it is expected that a DNA iteration is about twice as slow as MU iteration. On 184 modern hardware, parallelization may however distort this picture and hence experimental 185 verification is requied. 186", "creator": "PScript5.dll Version 5.2.2"}}}