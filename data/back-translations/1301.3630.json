{"id": "1301.3630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for detecting agents \"behavior based on observing their sequential decision-making behavior in interaction with the environment. We model the problem that agents face as a Markov decision process (MDP), and then model the observed behavior of agents in terms of the degree of rationality for optimal forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for cluster or classification models. Experimental studies with GridWorld, a navigation problem, and the secretarial problem, an optimal stop problem, suggest that reward vectors found in IRL may be a good basis for pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use characteristics observed in government action space suggest that detection problems may be superior.", "histories": [["v1", "Wed, 16 Jan 2013 09:01:47 GMT  (78kb)", "https://arxiv.org/abs/1301.3630v1", null], ["v2", "Mon, 21 Jan 2013 09:26:22 GMT  (77kb)", "http://arxiv.org/abs/1301.3630v2", null], ["v3", "Mon, 18 Feb 2013 06:06:08 GMT  (64kb)", "http://arxiv.org/abs/1301.3630v3", null], ["v4", "Wed, 20 Mar 2013 21:18:07 GMT  (69kb)", "http://arxiv.org/abs/1301.3630v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qifeng qiao", "peter a beling"], "accepted": false, "id": "1301.3630"}, "pdf": {"name": "1301.3630.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Peter A. Beling"], "emails": ["qq2r@virginia.edu", "pb3a@virginia.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n36 30\nv4 [\ncs .L\nG ]\n2 0"}, {"heading": "1 Introduction", "text": "The availability of sensing technologies, such as digital cameras, global position system, infrared sensors and others, makes the computer easily access the data recording the interaction between the agents and the environment. The new web technology also provides a large amount of background knowledge that describes the user behavior on the internet. More recent research has begun to build the behavior recognition system using the real-world data to understand the users\u2019 behavior.\nMany approaches have been proposed to understand the agents\u2019 goals/plans from the observation of the decision behavior. The entire trace of actions is to be recognized and matched against a plan library or a set of possible goals/plans. Despite of the success of these methods, they assume that the plan library, a set of possible goals or some behavior model are known beforehand and provided as an input. Goal information is often completely unknown in practice, however, and so it is difficult to model goals accurately.\nConsider some examples in the real-world. Human behavior contains more complex structure and relations. E.g. Kautz pointed out two basic structure for behavior: decomposition and abstraction [9]. Behavior can be decomposed into several events. For behavior recognition, Israeli security systems evaluate a series of events to reach a conclusion. Are they merely loitering in the area? Are they wearing a warm coat on a hot day? When a list of behavior hits a certain number, the system recognizes a potential threat. This method highly depends on the empirical experience and hardly recognizes the precise goal of the observed agent. To recommend personalized advertisement, the web companies hope to understand users\u2019 interest by analyzing the web browsing history. Are users interested in cameras, if they have booked a hotel recently? There is considerable interest in categorizing the users according to their interest, while it is more difficult to infer the precise goals of the users. Due to the time and space limitation for advertisement, effective identification of the\nuser\u2019s interest is of high importance for the companies. Another motivation for a new problem comes from domains like high frequency trading of stocks and commodities, where there is considerable interest in identifying new market players and algorithms based on observations of trading actions, but little hope in learning the precise strategies employed by these agents [22, 14].\nIn this paper, we propose a new problem, termed Behavior Pattern Recognition(BPR), that involves recognizing agents based on observation of their behavior in a sequential decision making setting. Broadly, the problem is to classify or cluster the agents according to the patterns of decision behavior that are learned from the samples of agents\u2019 decision-making process. The recognition problem can be framed as a classification or clustering problem: (1) Given observations of decision trajectories consisting of sequential actions and given a label for each trajectory indicating which behavior pattern that agent has, the problem is to determine the behavior pattern for an agent with unlabeled trajectories. (2) Given only observations of the decision trajectories, the problem is to assign trajectories to clusters on the basis of similarity of behavior patterns.\nA direct solution to BPR problem is to program some heuristic rules to recognize the behavior by decomposing complex behavior into a series of simple events and then evaluating them to reach a conclusion. However, programming the rules is hard. In contrast to the manually coded rules, we propose a learning model for BPR problem, characterizing the decision behavior with high level features in terms of the underlying goals. Consider the problem of image recognition as an illustration of behavior recognition. In that problem, a computer learns to categorize images by representing every image as a multi-dimensional feature vector that consists of the components such as RGB color, texture, shape parameters or other advanced metrics. The key point of characterizing the behavior is how to effectively find a high level vector that represents the sequential behavior and encodes the information on patterns. From the perspective of decision-making process, the underlying goal of an agent is considered as an abstract representation of the behavior. If a decision-making process is modeled by MDP, the reward function is assumed to encode the goal of that agent.\nIRL [13] addresses the task of learning a reward function for given MDP that is consistent with observations of optimal decision making for the process. An assumption is that the expert\u2019s goal/intention can be characterized by the reward function. If the expert is rational, the demonstration behavior should aim to maximize the long-term accumulative reward. We study the use of IRL to characterize the decision behavior, modeling the problem faced by the agents as a MDP and assuming the reward function of the MDP model as a high-level abstraction of the decision behavior. The motivation is that even when the true behavior is not rational and we can\u2019t learn the precise goals/decisionstrategies, we still can categorize the agents by learning the reward functions that make MDP models approximate the observed behavior.\nIRL has received increasing attention in the machine learning field in recent years. Most of this work is focused on apprenticeship learning, in which IRL is used as the core method for finding decision policies consistent with observed behavior [1, 12, 20]. A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].\nOn two well-know sequential decision-making problems, we compare our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space. Our main contributions include: (1) identification of a new learning task that categorizes agents by learning their behavior patterns; (2) design of simple methods to solve the BPR problem that characterize behavior in original observation space; (3) development of a new model-based method to solve the BPR problem in MDP reward space; and (4) observation that our new method using reward space provides a formal way to solve the behavior recognition problem and performs superior to other methods."}, {"heading": "2 Preliminaries", "text": "We define the input of BPR problem as a tuple B = (D1, D2, . . . DN), where Dn, n \u2208 {1, 2, . . . , N} is the observation of the n\u2212 th agent. For a classification problem, Dn = (On, yn), where On is a set of observed decision trajectories and yn is the class label for the n \u2212 th agent.\nThe agents, who have the same behavior patterns, are given the same class label. Similarly, in a clustering problem, Dn only consists of the observed decision trajectories.\nWe define the set of decision trajectories On = {hjn}, j = 1, 2, . . . , |On|, where each trajectory h j n is defined as a series of state and action pairs: {(s, a)tn}, t = 1, 2, . . . , |h j n|. Here, the s denotes the state for the decision problem and the a means the action selected by the agent at state s.\nTo determine a label for an agent, we may develop a model to decompose the observed behavior into several events. Each event can be described by a complete or part of a decision trajectory. When a list of events hits a certain number, the model recognizes a label for an agent. However, this method requires a lot of domain knowledge and human experience to program the heuristic rules.\nAnother way to solve BPR problem is to effectively represent the problem in a multi-dimensional space and then apply the learning algorithms. The decision-making process can be characterized in two layers. The outer layer characterizes the behavior by calculating some statistic information on the observed state and action. The inner layer is an abstraction of the behavior, which is related to the goal or the internal mind of the agent that determines the behavior fundamentally."}, {"heading": "3 Simple Representation of Behavior Recognition Problem", "text": "In this section, we describe two methods in outer layer that categorize the decision-making agents just based on observation.\nThe first method is called feature trajectory (FT). Assume the length of a decision trajectory is H . The vector to characterize the behavior in j \u2212 th decision trajectory is written as follows.\nf(hjn) = [s1, a1, s2, a2, . . . , sH , aH ],\nwhere si, i \u2208 {1, 2, . . . , H} is a discrete random variable meaning the state index at i\u2212 th decision stage, and ai represents the action selected at state si. E.g., we have a problem that can be defined by 3 states and 2 actions. Then si \u2208 {1, 2, 3} and ai \u2208 {1, 2}. In the observation, every trajectory starts from the same initial state. Given the observation set On for n\u2212 th agent, the feature vector fn is obtained by computing this equation: fn = 1|On| \u2211|On| j=1 f(h j n), where the vector f(h j n) is preprocessed by scale-normalization before averaging.\nThen, the n\u2212th agent is represented by a feature vector fn. Consider a supervised learning problem. Given a real valued input vector fn \u2208 F and a category label yn \u2208 Y , we aim to learn a function h : F \u2192 Y .\nThe second method is called feature expectation (FE), which has been widely used by apprenticeship learning as a representation of the averaged long-term performance. Assume a basis function \u03c6 : S \u2192 [0, 1]d, where S denotes the state space. The feature expectation fn =\n1 |On| \u2211|On| j=1 \u2211 st\u2208h j n \u03b3t\u03c6(st), where \u03b3 \u2208 (0, 1) is a discount factor. The associated apprenticeship learning algorithms aim to find a policy that performs as well as demonstrations by minimizing the distance between their feature expectations. Here, we only use the observed state sequence to compute the feature expectation vector for an agent, where the \u03b3 is manually defined constant, e.g. 0.95. Then, the n\u2212 th agent can be represented by the vector fn that is obtained from On."}, {"heading": "4 A New Representation Model", "text": "To solve the BPR problem with high-level feature representation, we propose to use the following steps.\n1. Given the BPR problem with input B, we use the set {On}, n \u2208 {1, 2, . . . , N} to construct the state space S and action space A for the decision-making problem.\n2. For n \u2212 th observed agent, we assume an MDP model M = (S,A, Rn, \u03b3,P), where Rn is the unknown reward function for this agent, \u03b3 is the constant discount factor, and P = {Pa}a\u2208A is a set of transition probability matrices Pa for action a \u2208 A. The entries of Pa, written as Pa(s, s\u2032), give the probability of transitioning to state s\u2032 \u2208 S from state s \u2208 S given the action is a. The rows of Pa, denoted Pa(s, :), give a probability vector\nof transitioning from state s to all the states in S. The P can be modeled using prior knowledge of the problem or estimated from the observed decision trajectories {On}. In a finite state space the reward functionRn may be considered as a vector, rn, whose elements give the reward in each state. Here we expect that there exists an unknown reward function that can make the MDP find a policy as similar as the observed behavior.\n3. Apply IRL algorithms to learn the reward vector rn for n\u2212 th agent.\n4. Estimate the reward vectors for every agent. Then the supervised learning problem is written as: given the real valued input vector rn \u2208 R and the category label yn \u2208 Y , we aim to learn a function h : R \u2192 Y .\n5. Given a new observed agent, we repeat step 1-3 to get the reward vector for the agent and then predict the label for the behavior pattern using estimated function h : R \u2192 Y .\nIn MDP model, a policy is defined as a mapping \u03c0 : S \u2192 A. The value function for a policy \u03c0 is V \u03c0(s0) = E[ \u2211\u221e t=0 \u03b3\ntR(st)|p(s0), \u03c0] where p(s0) is the distribution of the initial state and the action at state st is determined by policy \u03c0. Similarly, the Q function is defined as Q(s, a) = R(s)+\u03b3 \u2211\ns\u2032\u2208S Pa(s, s \u2032)V \u03c0(s\u2032). At state s, an optimal action is selected by a\u2217 = maxa\u2208A Q(s, a).\nAn instance of the IRL problem is written as a triplet B = (M \\ r, p(r),O), where M \\ r is a MDP model without the reward function and p(r) is prior knowledge on the reward. The vector p(r) can be a non-informative prior if we have no knowledge about the reward function or a Gaussian or other distribution if we model the reward as a specific stochastic process.\nWe use an MDP to model the decision problem faced by an agent under observation. The reality of the agent\u2019s decision problem and process may differ from the MDP model, but we interpret every observed decision of the agent as the choice of an action in the MDP. The dynamics of the environment in the MDP are described by the transition probabilities P . These probabilities may be interpreted as being a prior, if known in advance, or as an estimation of the agent\u2019s beliefs of the dynamics. Next, we will show how to learn the reward functions by employing some exiting IRL algorithms."}, {"heading": "5 Bayesian framework for IRL", "text": "Most existing IRL algorithms assume that the agents are perfectly rational and the observed behavior is optimal. Prominent examples include the model in [13], which we term linear IRL (LIRL) because of its linear nature, WMAL in [20], and PROJ in [1]. In these algorithms, the reward function is written linearly in terms of features as R(s) =\n\u2211d i=1 \u03c9i\u03c6i(s) = \u03c9 T\u03c6(s), where \u03c6 : S \u2192 [0, 1]d\nand \u03c9T = [\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9d].\nOur computational framework uses Bayesian IRL to estimate the reward vectors in a MDP, which was initially proposed in [6]. The posterior over reward function for n\u2212 th agent is written as\np(rn|On) = p(On|rn)p(rn) \u221d\n|On|\u220f\nj=1\n\u220f\n(s,a)\u2208hjn\np(a|s, rn).\nThen, the IRL problem is written as maxrn log p(On|rn)+log p(rn). For many problems, however, the computation of p(rn|On) may be complicated and some algorithms use Markov chain Monte Carlo (MCMC) to sample the posterior probability. Considering the computation complexity to deal with a large number of IRL problems, we choose the IRL algorithms that have well defined likelihood function to reduce the computation cost."}, {"heading": "5.1 IRL with Boltzmann Distribution", "text": "To model the likelihood function, some IRL algorithm in [2], which we call maximum likelihood IRL (MLIRL), uses Boltzmann distribution to calculate p(a|s, rn) using p(a|s, rn) = e Q(s,a) \u2211\na\u2208A eQ(s,a)\n."}, {"heading": "5.2 IRL with Gaussian Process", "text": "IRL algorithm, which is called GPIRL in [15], uses preference relations to model the likelihood function P (On|rn) and assumes the rn is generated by Gaussian process for n\u2212 th observed agent.\nGiven a state, we assume that the optimal action is selected according to Bellman optimality. The preference relation is defined as follows.\nAt state s, \u2200a\u0302, a\u030c \u2208 A, we define the action preference relation as:\n1. Action a\u0302 is weakly preferred to a\u030c, denoted as a\u0302 s a\u030c, if Q(s, a\u0302) \u2265 Q(s, a\u030c);\n2. Action a\u0302 is strictly preferred to a\u030c, denoted as a\u0302 \u227bs a\u030c, if Q(s, a\u0302) > Q(s, a\u030c);\n3. Action a\u0302 is equivalent to a\u030c, denoted as a\u0302 \u223cs a\u030c, if and only if a\u0302 s a\u030c and a\u030c s a\u0302.\nGiven the observation set On, we have a group of preference relations at each state s, which is written as\nE \u2261 { (a\u0302 \u227bs a\u030c), a\u0302 \u2208 A\u0302, a\u030c \u2208 A \\ A\u0302 } \u222a { (a\u0302 \u223cs a\u0302 \u2032), a\u0302, a\u0302\u2032 \u2208 A\u0302 } ,\nwhere A\u0302 \u2208 A is the action subspace for state s obtained from the set On.\nLet r be the vector of rn containing the reward for m possible actions at T observed states. We have\nr = (ra1(s1), ..., ra1(sT ) \ufe38 \ufe37\ufe37 \ufe38 , . . . , ram(s1), . . . , ram(sT ) \ufe38 \ufe37\ufe37 \ufe38 )\n= ( ra1 , \u00b7 \u00b7 \u00b7 , ram),\nwhere T = |S| and ram , \u2200m \u2208 {1, 2, . . . , |A|}, denotes the reward with respect to m-th action.\nConsider ram as a Gaussian process if, for any {s1, \u00b7 \u00b7 \u00b7 , sT } \u2208 S, the random variables {ram(s1), \u00b7 \u00b7 \u00b7 , ram(sT )} are normally distributed. We denote by kam(sc, sd) the function generating the value of entry (c, d) for covariance matrix Kam , which leads to ram \u223c N(0,Kam). Then the joint prior probability of the reward is a product of multivariate Gaussian, namely p(r|S) =\n\u220f|A| m=1 p(ram |S) and r \u223c N(0,K). Note that r is completely specified by the posi-\ntive definite covariance matrix K.\nA simple strategy is to assume that the |A| latent processes are uncorrelated. Then the covariance matrix K is block diagonal in the covariance matrices { K1, ...,K|A| } . In practice, we use a squared exponential kernel function, written as:\nkam(sc, sd) = e 1 2 (sc\u2212sd)Mam (sc\u2212sd) + \u03c32am\u03b4(sc, sd),\nwhere Mam = \u03baamIT and IT is an identity matrix of size T . The function \u03b4(sc, sd) = 1, when sc = sd; otherwise \u03b4(sc, sd) = 0. Under this definition the covariance is almost unity between variables whose inputs are very close in the Euclidean space, and decreases as their distance increases.\nThen, the GPIRL algorithm estimates the reward function by iteratively conducting the following two main steps:\n1. Get estimation of rMAP by maximizing the posterior p(rn|On), which is equal to minimize \u2212 log p(On|rn) \u2212 log p(rn|\u03b8), where \u03b8 = (\u03baam , \u03c3am) is the hyper-parameter controlling the Gaussian process, and p(On|rn) = \u220f p((a\u0302 \u227bs a\u030c)) \u220f p((a\u0302 \u223cs a\u0302\n\u2032)). Above optimization problem has been proved to be convex programming in [15].\n2. Find the optimized hyper-parameters by applying gradient decent optimization method to maximize log p(On|\u03b8, rMAP ), which is the Laplace approximation of p(\u03b8|On)."}, {"heading": "6 Experimentation", "text": "Our experiments are designed to evaluate the performance of IRL algorithms in behavior recognition in comparison to methods that use simple feature representations obtained directly from observation space. We study two problems, GridWorld and the secretary problem. GridWorld provides insight into the task of recognizing machine agents for decision problems that may be modeled using MDP\nmodels under the strict rationality assumption. The secretary problem provides an environment in which the agents do not act with respect to the solution of an MDP. Agents in the secretary problem employ heuristic decision rules derived from experimental study of human behavior in psychology and economics.\nTo evaluate the recognition performance, we use the following algorithms: (1) Clustering: Kmeans [7]; (2) Classification: Support vector machine (SVM), K-nearest neighbors (KNN), Fisher discriminant analysis (FDA) and logistic regression (LR) [7]. We use clustering accuracy [21] and Normalized Mutual Information (NMI) [19] to compare clustering results.\n6.1 GridWorld problem\nIn the GridWorld problem, which is used as a benchmark experiment by Ng and Russell in [13], an agent starts from a given square and moves towards a destination square. The agent has five actions to take: moving in the four cardinal directions or staying put. With probability 0.65 the agent moves to its chosen location, with probability 0.15 it stays in the same location regardless of chosen action, and with probability 0.2 it moves in a random cardinal direction.\nThe IRL problem for GridWorld is to recover the reward structure given the observations of agent actions. To produce these observations, we first simulate the agent\u2019s behavior using the optimal solution of an MDP to decide how to move in the GridWorld. We then collect observation data by sampling the simulated movement. Note that the reward function of the MDP used for simulating the agents is not known to the IRL learner.\nWe investigated the behavior recognition problem in terms of clustering and classification on a 10\u00d7 10 GridWorld problem. Experiments were conducted according to the steps in Algorithm 1.\nAlgorithm 1 GridWorld experimentation steps\n1: Input the variables S ,A and P . Design two ground truth reward functions written as r\u22171 and r \u2217 2 . 2: Simulate agents and sample their behavior. 3: for i = 1 \u2192 2 do 4: for j = 1 \u2192 200 do 5: Model an agent using M = (S ,A,P , rij , \u03b3), where the reward rij = r\u2217i + random Gaussian noise. 6: Sample decision trajectories Oij , and make the ground truth label yij = 0, if i = 1; yij = 1, if i = 2. 7: end for 8: end for 9: IRL has access to the problem B = (S ,A,P , \u03b3,Oij) for this agent, and then infers the reward rij .\n10: Recognize these agents based on the learned rij .\nThe simulated agents in our experiments have hybrid destinations. A small number of short decision trajectories tends to present challenges to action feature methods, which is an observation of particular interest. Additionally, the length of trajectories may have a substantial impact on performance. If the length is so long that the observed agent reaches the destination in every trajectory, the problem can be easily solved based on observations. Thus, we evaluate and compare performance by making the length of decision trajectory small.\n|On| FE FT PROJ GPIRL 4 0.0077 0.0012 0.0068 0.0078 8 0.0114 0.0016 0.0130 0.0932 16 0.0177 0.0014 0.0165 0.7751 20 0.0340 0.0573 0.0243 0.8113 30 0.0321 0.0273 0.0365 0.8119 40 0.0361 0.0459 0.0389 0.8123 60 0.0387 0.0467 0.0388 0.8149 80 0.0441 0.1079 0.0421 0.8095\n100 0.0434 0.1277 0.0478 0.8149 200 0.0502 0.1649 0.0498 0.8149\nTable 1: NMI scores for GridWorld problem\n4 8 16 20 30 40 60 80 100 200 40\n50\n60\n70\n80\n90\n100\nNumber of sampled decision trajectories\nC lu\nst er\nin g\nA cc\nur ac\ny\nFE FT GPIRL PROJ\nFigure 1: Clustering accuracy\nTable 6.1 displays NMI scores and Figure1 shows clustering accuracy. The length of the trajectory is limited to six steps, as we assume the observation is incomplete and the learner does not have sufficient information to infer the goal directly. Results are averaged over 100 replications. Clustering performance improves with increasing number of observations. When the number of observations is small, GPIRL method achieves high clustering accuracy and NMI scores due to the advantage of finding more accurate reward functions that can well characterize the decision behavior. The IRL algorithms, such as PROJ and WMAL, are not effective in this problem because the length of the observed decision trajectory is too small to provide a feature expectation that is a good approximation to the agent\u2019s long-term goals. Considering the utilization of feature learning algorithms to improve the simple feature representations, we also did experiments with PCA-based features where the projection sub-space is spanned by those eigenvectors that correspond to the principal components c = 10, 20, . . . , 90 for FE and c = 2, 4, 6, 8, 10 for FT. No significant changes in the clustering NMI scores and accuracy scores are observed. Therefore, we do not show the performance of PCA-based features in Table 6.1and Figure 1.\nFigure2 displays classification accuracy for a binary classification problem in which there are four hundred agents coming from two groups of decision strategies. The results are averaged over 100 replications with tenfold cross-validations. Four popular classifiers (SVM, KNN, FDA and LR) are employed to evaluate the classification performance. Results suggest that the classifiers based on IRL perform better than the simple methods, such as FT and FE, particularly when the number of observed trajectories and the length of the trajectory are small. The results support our hypothesis that recovered reward functions constitute an effective and robust feature space for clustering or classification analysis in a behavior pattern recognition setting."}, {"heading": "6.2 Secretary problem", "text": "The secretary problem is a sequential decision-making problem in which the binary decision to either stop or continue a search is made on the basis of objects already seen. As suggested by the name,\nAlgorithm 2 Experimentation with Secretary Problem\n1: Given a heuristic rule with a parameter h, k or \u2113. 2: Add random Gaussian noise to the parameter, which is written as p\u0302. 3: Generate new secretary problem with X applications and let n\u2212 th agent solve these problems using this\nheuristic rule with its own parameter p\u0302. Save the observed decision trajectories into On. 4: Model the secretary problem in terms of an MDP consisting of the following components:\n1. State space S = {1, 2, . . . , X}, where s \u2208 S means that at time s the current applicant is a candidate. 2. Action space A consisting of two actions: reject and accept. 3. Transition probability P , computed as follows: given the reject action, the probability of transi-\ntioning from state si to sj , p(sj|si), is si\nsj(sj\u22121) if sj \u2265 si, and 0 otherwise; given the accept\naction, the probability of transitioning from state si to sj , p(sj |si), is 1 if si = sj , and 0 otherwise.\n4. The discount factor \u03b3 is a selected constant. 5. The reward function is unknown.\n5: Infer the reward function by solving an IRL problem B = (S ,A,P , \u03b3,On).\nthe problem is usually cast in the context of interviewing applicants for a secretarial position. The decision maker interviews a randomly-ordered sequence of applicants one at a time. The applicant pool is such that the interviewer can unambiguously rank each applicant in terms of quality relative to the others seen up to that point. After each interview, the decision maker chooses either to move on to the next applicant, forgoing any opportunity to hire the current applicant, or to hire the current applicant, which terminates the process. If the process goes as far as the final applicant, he or she must be hired. Thus the decision maker chooses one and only one applicant. The objective is to maximize the probability that the accepted applicant is, in fact, the best in the pool.\nTo test our hypotheses on BPR, an idea experiment would involve recognizing individual human decision makers on the basis of observations of hiring decisions that they make in secretary problem simulations. Experiments with human decision making for the secretary problem are reported on in [18, 17], but raw data consisting of decision maker action trajectories is not available. However, a major conclusion of these studies is that the decisions made by the humans largely can be explained in terms of three decision strategies, each of which uses the concept of a candidate. An applicant is said to a candidate he or she is the best applicant seen so far. The decision strategies of interest are the:\n1. Cutoff rule (CR) with cutoff value h, in which the agent will reject the first h\u2212 1 applicants and accept the next candidate;\n2. Successive non-candidate counting rule (SNCCR) with parameter value k, in which the agent will accept the first candidate who follows k successive non-candidate applicants since the last candidate; and\n3. Candidate counting rule (CCR) with parameter value \u2113, in which the agent selects the next candidate once \u2113 candidates have been seen.\nThe optimal decision strategy for the secretary problem is to use CR with a parameter that can be computed using dynamic programming for any value of n, the number of secretaries. As n grows, the optimal parameter converges to n/e and yields a probability of successfully choosing the best applicant that converges to 1/e. Thus only one of the three decision strategies enumerated above can be viewed as optimal, and that only for a single parameter value out of the continuum of possible values. Human actions are usually suboptimal and tend to look like mixtures of CR (with a nonoptimal parameter), SNCCR, and CCR [18].\nAs a surrogate for the action trajectories of humans, we use agents that we generate action trajectories for randomly sampled secretary problems using CR, SNCCR, and CCR. For a given decision rule (CR, SNCCR, CCR), we simulate a group of agents that adopt this rule, differentiating individuals in a group by adding Gaussian noise to the rule\u2019s parameter. The details of the process are given in Algorithm 2. We use IRL and observed actions to learn reward functions for the MDP model given in Algorithm 2. It is critical to understand that the state space for this MDP model captures nothing of the history of candidates, and as a consequence is wholly inadequate for the purposes of\nmodeling SNCCR and CCR. In other words, for general parameters, neither SNCCR nor CCR can be expressed as a policy for the MDP in Algorithm 2. (There does exist an MDP in which all three of the decision rules can be expressed as policies, but the state space for this model is exponentially larger.) Hence, for two of the rules, the processes that we use to generate data and the processes we use to learn are distinct.\nAs an initial set of experiments, we generated an equal number of agents from each rule. All the heuristic rules use the same parameter value. We have compared the method using statistical feature representations obtained from the raw decision trajectories and our IRL model-based method. We employ 10 fold cross-validation to obtain the average accuracy, and it is always 100% .\nGiven that perfect classification performance was achieved by all algorithms, the problem of recognizing across decision rules appears to be quite easy. A more challenging problem is to recognize variations in strategy within a single decision rule. For each rule, we conducted recognition experiments in which 300 agents were simulated, 100 each for three distinct values of the rule parameter. Individuals were differentiated by adding random noise to the parameter. Here, we show the comparison of the clustering performance between the simple method called FE and our MDP model-based method. In Figure3, the left figure displays an area marked \u201cuncertainty\u201d for the method called FE, while the right figure shows that the reward vectors have lower variance in the same group and higher variance between different groups.Figure3 intuitively demonstrates that when the agents\u2019 behavior is represented in the reward space, the recognition problem becomes easier to solve.\nTable 2 summarizes the NMI scores for using K-means clustering algorithm to recognize variations in strategy within one heuristic decision rule. The column called H in Table 2 records the number of decision trajectories that have been sampled for training. Table 2 proves that the feature representation in reward space is almost always better than the representation with statistical features computed\nfrom the raw observation data. Moreover, the reward space can particularly better characterize the behavior when the scale of the observation data is small. Note that though the MDP model cannot generate the policy that is consistent with the SNCCR and CCR rules, the reward vectors learned in the MDP environment still make the clustering problem easier to solve.\nFigure4 shows a binary classification result of using PROJ algorithm to learn the reward functions for the agents in Secretary problem and then categorize the agents into two groups. In this classification experiment, the users\u2019 ground truth label is either cutoff decision rule or random strategy that makes random decisions."}, {"heading": "7 Conclusions", "text": "We have proposed the use of IRL to solve the problem of behavior pattern recognition. The observed agent does not have to make decisions based on an MDP. However, we model the agent\u2019s behavior in an MDP environment and assume that the reward function has encoded the agent\u2019s underlying decision strategies. Numerical experiments on GridWorld and the secretary problem suggest that the advantage that IRL enjoys over action space methods is more pronounced when observations are limited and incomplete. We also note that there is seems to be a positive correlation between the success of IRL algorithms in apprenticeship learning (cf. [15]) and their success in the behavior recognition problem. To some degree, this relationship parallels results from [10, 5], where apprenticeship learning benefits from a learning structure that based on sophisticated methods for task decomposition or hierarchical identification of skill trees.\nValidation of the ideas proposed here can come only through experimentation with more difficult problems. Of particular importance would be problems involving human decision makers or other real-world scenarios, such as periodic investment, gambling, or stock trading."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Apprenticeship learning about multiple intentions", "author": ["Monica Babes-Vroman", "Vukosi Marivate", "Kaushik Subramanian", "Michael Litman"], "venue": "In the 28th International Conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Bootstrapping apprenticeship learning", "author": ["Abdeslam Boularias", "Brahim Chaib-draa"], "venue": "In Advances in Neural Information Processing Systems 24. MIT press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Map inference for bayesian inverse reinforcement learning", "author": ["Jaedeug Choi", "Kee-Eung Kim"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Automatic task decomposition and state abstraction from demonstration", "author": ["Luis C. Cobo", "Charles Lee Isbell Jr.", "Andrea Lockerd Thomaz"], "venue": "In AAMAS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Bayesian inverse reinforcement learning", "author": ["Ramachandran Deepak", "Amir Eyal"], "venue": "In Proc. 20th International Joint Conf. on Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Inverse optimal control with linearlysolvable mdps", "author": ["Krishnamurthy Dvijotham", "Emanuel Todorov"], "venue": "In Proc. 27th International Conf. on Machine learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A Formal Theory of Plan Recognition", "author": ["H. Kautz"], "venue": "PhD thesis, University of Rochester", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["G.D. Konidaris", "S.R. Kuindersma", "R.A. Grupen", "A.G. Barto"], "venue": "International Journal of Robotics Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Feature construction for inverse reinforcement learning", "author": ["Sergey Levine", "Zoran Popovic", "Vladlen Koltun"], "venue": "In Advances in Neural Information Processing Systems 24. MIT press,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["Gergely Neu", "Csaba Szepesvari"], "venue": "In Proc. Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y. Ng", "Stuart Russell"], "venue": "In Proc. 17th International Conf. on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "An agent based model of the e-mini s&p 500: Applied to flash crash analysis", "author": ["Mark Paddrik", "Roy Hayes", "Andrew Todd", "Steve Yang", "Peter Beling", "William Scherer"], "venue": "IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Inverse reinforcement learning via convex programming", "author": ["Qifeng Qiao", "Peter A. Beling"], "venue": "In Americon Control Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Maximum margin planning", "author": ["Nathan D. Ratliff", "J. Andrew Bagnell", "Martin A. Zinkevich"], "venue": "Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "The relationship between risk attitudes and heuristics in search tasks: A laboratory experiment", "author": ["Daniel Schunk", "Joachim Winter"], "venue": "Journal of Economic Behavior and Organization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Sequential decision making with relative ranks: An experimental investigation of the \u2019secretary problem", "author": ["Darryl A. Seale"], "venue": "Organizational Behavior and Human Decision Process,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Cluster ensembles? a knowledge reuse framework for combining multiple partitions", "author": ["Alexander Strehl", "Joydeep Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Umar Syed", "Robert E. Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Maximum margin clustering", "author": ["Linli Xu", "James Neufeld", "Bryce Larson", "Dale Schuurmans"], "venue": "In Advanced Neural Information Process Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Behavior based learning in identifying high frequency trading strategies", "author": ["Steve Yang", "Mark Paddrik", "Roy Hayes", "Andrew Todd", "Andrei Kirilenko", "Peter Beling", "William Scherer"], "venue": "IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Kautz pointed out two basic structure for behavior: decomposition and abstraction [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 20, "context": "Another motivation for a new problem comes from domains like high frequency trading of stocks and commodities, where there is considerable interest in identifying new market players and algorithms based on observations of trading actions, but little hope in learning the precise strategies employed by these agents [22, 14].", "startOffset": 315, "endOffset": 323}, {"referenceID": 12, "context": "Another motivation for a new problem comes from domains like high frequency trading of stocks and commodities, where there is considerable interest in identifying new market players and algorithms based on observations of trading actions, but little hope in learning the precise strategies employed by these agents [22, 14].", "startOffset": 315, "endOffset": 323}, {"referenceID": 11, "context": "IRL [13] addresses the task of learning a reward function for given MDP that is consistent with observations of optimal decision making for the process.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Most of this work is focused on apprenticeship learning, in which IRL is used as the core method for finding decision policies consistent with observed behavior [1, 12, 20].", "startOffset": 161, "endOffset": 172}, {"referenceID": 10, "context": "Most of this work is focused on apprenticeship learning, in which IRL is used as the core method for finding decision policies consistent with observed behavior [1, 12, 20].", "startOffset": 161, "endOffset": 172}, {"referenceID": 18, "context": "Most of this work is focused on apprenticeship learning, in which IRL is used as the core method for finding decision policies consistent with observed behavior [1, 12, 20].", "startOffset": 161, "endOffset": 172}, {"referenceID": 14, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 206, "endOffset": 209}, {"referenceID": 2, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 230, "endOffset": 233}, {"referenceID": 9, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 256, "endOffset": 260}, {"referenceID": 13, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 283, "endOffset": 287}, {"referenceID": 3, "context": "A number of IRL algorithms and modeling constructs have been proposed for apprenticeship learning or imitation learning, including Maxmargin planning [16], gradient tuning methods [12], linear solvable MDP [8], bootstrap learning [3], feature construction [11], Gaussian process IRL [15] and Bayesian inference [4].", "startOffset": 311, "endOffset": 314}, {"referenceID": 0, "context": "Assume a basis function \u03c6 : S \u2192 [0, 1], where S denotes the state space.", "startOffset": 32, "endOffset": 38}, {"referenceID": 11, "context": "Prominent examples include the model in [13], which we term linear IRL (LIRL) because of its linear nature, WMAL in [20], and PROJ in [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Prominent examples include the model in [13], which we term linear IRL (LIRL) because of its linear nature, WMAL in [20], and PROJ in [1].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "Prominent examples include the model in [13], which we term linear IRL (LIRL) because of its linear nature, WMAL in [20], and PROJ in [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "In these algorithms, the reward function is written linearly in terms of features as R(s) = \u2211d i=1 \u03c9i\u03c6i(s) = \u03c9 \u03c6(s), where \u03c6 : S \u2192 [0, 1] and \u03c9 = [\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9d].", "startOffset": 131, "endOffset": 137}, {"referenceID": 5, "context": "Our computational framework uses Bayesian IRL to estimate the reward vectors in a MDP, which was initially proposed in [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "1 IRL with Boltzmann Distribution To model the likelihood function, some IRL algorithm in [2], which we call maximum likelihood IRL (MLIRL), uses Boltzmann distribution to calculate p(a|s, rn) using p(a|s, rn) = e Q(s,a) \u2211 a\u2208A eQ(s,a) .", "startOffset": 90, "endOffset": 93}, {"referenceID": 13, "context": "2 IRL with Gaussian Process IRL algorithm, which is called GPIRL in [15], uses preference relations to model the likelihood function P (On|rn) and assumes the rn is generated by Gaussian process for n\u2212 th observed agent.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Above optimization problem has been proved to be convex programming in [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "We use clustering accuracy [21] and Normalized Mutual Information (NMI) [19] to compare clustering results.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We use clustering accuracy [21] and Normalized Mutual Information (NMI) [19] to compare clustering results.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "1 GridWorld problem In the GridWorld problem, which is used as a benchmark experiment by Ng and Russell in [13], an agent starts from a given square and moves towards a destination square.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "Experiments with human decision making for the secretary problem are reported on in [18, 17], but raw data consisting of decision maker action trajectories is not available.", "startOffset": 84, "endOffset": 92}, {"referenceID": 15, "context": "Experiments with human decision making for the secretary problem are reported on in [18, 17], but raw data consisting of decision maker action trajectories is not available.", "startOffset": 84, "endOffset": 92}, {"referenceID": 16, "context": "Human actions are usually suboptimal and tend to look like mixtures of CR (with a nonoptimal parameter), SNCCR, and CCR [18].", "startOffset": 120, "endOffset": 124}], "year": 2013, "abstractText": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents\u2019 behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}