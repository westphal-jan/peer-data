{"id": "1301.6719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Approximate Planning for Factored POMDPs using Belief State Simplification", "abstract": "We are interested in the problem of planning factored POMDPs. Building on the recent results of Kearns, Mansour and Ng, we provide a planning algorithm for factored POMDPs that exploits the trade-off between accuracy and efficiency introduced by Boyen and Koller in simplifying the state of belief.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:38 GMT  (434kb)", "http://arxiv.org/abs/1301.6719v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david a mcallester", "satinder singh"], "accepted": false, "id": "1301.6719"}, "pdf": {"name": "1301.6719.pdf", "metadata": {"source": "CRF", "title": "Approximate Planning for Factored POMDPs using Belief State Simplification", "authors": ["David A. McAllester", "Satinder Singh"], "emails": ["}@research.att.com"], "sections": [{"heading": null, "text": "We are interested in the problem of plan ning for factored POMOPs. Building on the recent results of Kearns, Mansour and Ng, we provide a planning algorithm for fac tored POMOPs that exploits the accuracy efficiency tradeoff in the belief state sim plifi cation introduced by Boyen and Koller.\n1 INTRODUCTION\nA large number of problems of sequential decision making in uncertain environments from artificial intel ligence, operations-research, and control can be mod eled as Markov decision processes (MOPs) [7, 1]. In such problems, at each time step the agent observes the state of the environment and then executes an ac tion causing a reward for the agent and a stochastic transition in the state of the environment. In a finite horizon MOP, the goal is to choose actions so as to maximize the expected sum of rewards up to the given horizon. A number of algorithms for solving MOPs are available [7, 1]. The complexity of such algorithms is typically some low-order polynomial in the number of states in the environment and of the decision-making horizon [1]. Unfortunately, in many real-world problems the agent's sensors provide only partial information about the state of the environment. This partial informa tion is called an observation and is generated stochas tically from the state. Such problems are modeled as partially observable MOPs or POMOPs [4, 6]. In a finite-horizon POMOP the goal is again to choose ac tions so as to maximize the expected sum of rewards up to the given horizon. At each point in time the se quence of observations made by the agent determines a probability distribution over states of the environ ment. Such a probability distribution is called a belief state. It is well known that the problem of planning in\nan arbitrary POMOP can be reduced to the problem of planning in the corresponding \"belief state\" MOP. The number of belief-states is infinite and therefore it is not possible to iterate over belief-states as is nor mally done in MOP algorithms such as value itera tion. However, for a finite-horizon POMOP, various computable forms of value iteration have been defined using piecewise linear representations of value func tions [6, 4]. Such algorithms yield a representation of a policy that assigns actions to all belief-states. How ever, their worst-case complexity is doubly-exponential in the horizon.\nNote that if the number of observations and actions is finite, then for a finite-horizon problem with a given initial belief state only a finite number of belief-states are reachable. This leads to a straightforward algo rithm for selecting actions at reachable belief-states whose complexity is only singly exponential in the horizon. This algorithm is best formalized by viewing a POMOP as a game between the agent and the en vironment. First the agent selects an action and then the environment randomly selects the next observa tion. The best action can be selected by searching this game tree to the given horizon. Associated with each node in the game tree is a belief state. Computing the belief state resulting from a given action requires O(ISI2) operations (see, e.g., [4]), where S is the set of possible states of the environment. The number of nodes in the tree is O[(IAIIOI)H], where A is the set of actions, 0 is the set of possible observations, and H is the given horizon, i.e., H is the number of allowed actions by the agent. This gives a total run time of O[ISI2(IAIIOI)H]. We can not expect to do better than singly exponential in the horizon because it has been shown that, for horizons polynomial in the size of the PO MOP, it is PSPACE hard to determine if the agent can achieve a particular expected total reward [5]. Although we cannot expect to do better than singly eJ<ponential in the horizon, dependence on lSI and 101 can be significantly improved. First, we consider the\n410 McAllester and Singh\ndependence on 101. In the game between the agent and the environment it is possible to approximately compute the expected reward achievable from a given action by stochastically sampling the observation \"re sponses\" a sufficient number of times. This yields an algorithm that achieves an expected total reward no less than o from optimal and whose complexity is O[ISI2(poly(!) IAI)H]. This represents a significant im provement when 101 is large. This approach was de veloped by Kearns et a!. for the case of MDPs and we apply it here to the case of POMDPs.\nNext we consider the dependence on lSI. To see the significance of the dependence on lSI consider the case of a factored POMDP, i.e., a POMDP in which the state of the environment is described by the values of a set of state variables. The number of states, i.e., lSI, is then exponential in the number of state vari ables. For example, if each state variable has two pos sible values, and the number of state variables is n, then lSI is 2n and so the running time of the above al gorithm is 0[22n(poly( t) IAI)H]. For simplicity, from here on we assume that the state variables have only two possible values. While the sampling algorithm can be used with arbitrarily large observation sets, it is restricted to factored POMDPs with a small number of state variables. The dependence on lSI (or n) can be improved by associating each node in the search tree with a more efficiently computable simplified be lief state approximating the true belief state at that node. One straightforward form of belief state sim plification is to approximate each belief state, i.e., a distribution on 2n states, by its product of marginals. Note that simply representing an arbitrary belief state requires 2n -1 parameters while a product of marginals state can be represented with n parameters. Unfortu nately, the standard product-of-marginals simplifica tion incurs an error that grows with the number of state variables.\nFollowing Boyen and Koller [2] we note that a more realistic simplification processes divides the state vari ables into a bounded number of classes and then uses a product of marginals simplification which treats each class of variables as a single \"metavariable\" with an ex ponential number of values. Under such simplification, representing a single belief state still requires an expo nentially large data structure. However, k-class belief state simplification cuts the exponent by a factor of k. Furthermore, Boyen and Koller show that though the difference between the simplified belief state and the true belief state can grow over time, in rapidly mixing POMDPs the expectation of this difference remains bounded. As our main result, we give a precise anal ysis relating the quality of belief state approximation to the quality of decision-making.\nIn summary, in this paper we consider some fixed but arbitrary belief state simplifier and associated notion of simplified belief state. The choice of simplifier, e.g., the choice of the number of variable classes, should reflect a trade-off between the accuracy of simplifica tion and the space and time required to represent and manipulate simplified states. We state the accuracy of our algorithm as a function of the accuracy of sim plification. The run time of our algorithm is the time required for a certain number of operations on sim plified belief states. Hence our algorithm inherits the accuracy-efficiency trade-off inherent in the choice of the simplifier.\n2 Problem Definition and Notation\nWe now define some notation for MDPs. The initial state is denoted by so. The probability of a transition to state s' conditional upon taking action a in state s is denoted P(s'la, s). We write R, ::; Rmax for the reward in state s. Following Kearns et a!. we work with a discount fac tor rather than a finite horizon. We take the agent's goal to be that of selecting actions so as to maximize the expected discounted summed reward, i.e., the ex pected value of the infinite sum I:\ufffdo \u00b7-/r1 where rt is the reward at time step t, and 0 ::; 1 < 1 is a discount factor. This sum can be no larger than I:\ufffdo 11 Rmax which equals \ufffd::;\u00b7. The quantity 1_:, is analogous to a horizon time - rewards that occur significantly later than 1_:, have no significant impact on the dis counted summed reward and our running time will be exponential in 1_:-y . A POMDP consists of an underlying MDP plus a set of possible observations. The probability of observa tion a when the underlying state iss is denoted P(ols). For technical convenience we assume the agent knows the initial state and so we can ignore any initial obser vation.\nHere we associate a POMDP with two MDPs - the true belief state MDP and a simplified belief state MDP. The simplified belief state MDP is an approx imation of the true belief state MDP using simplified belief states which can be computed and manipulated more efficiently. Both the true belief state MD P and the simplified belief state MDP can be viewed as a game between the agent and an environment. The games associated with the true belief state MDP and the simplified belief state MDP differ in the expected reward and the probability distributions over the next observation at each node in the tree. To precisely com pare these two MDPs it is technically convenient to use the game positions as states. A game position where the agent is to select the next action (a move for the\nagent) is defined by a history, i.e., a sequence of action observation pairs.\nWe now formally define the true belief state MDP M and the the simplified belief state MDP M. Both M and M have histories (game positions) as states. In both cases each history is associated with a belief state, i.e., a probability distribution over states of the under lying MOP. M uses the true belief state f3 defined as follows where 6(so) denotes the probability distribu tion in which ail probability mass is concentrated at the single state so:\n{3(0) =<l(so), f3(u; <a,a>) =U(f3(u),<a,a>),\nU( </>, <a, a>)( s') P(s'l\u00a2, <a, a>)\nP(a\ufffd, 4>) L ,P(s)P(s'ia, s)P(ais') \u2022ES where P(oia, \u00a2) = I;,, I;, \u00a2(s)P(s'ia, s)P(ois'). M uses a simplified mapping fj defined as follows: \ufffd(0)=:S(b\"(so)), \ufffd(u; <a,o>)=:S(U(\ufffd(u),<a,a>)) (1)\nwhere S is a given mapping from belief-states to sim plified belief-states - a belief state \u00a2 will be called simplified if S( \u00a2) = \u00a2. Now we define transition probabilities P and P and reward function R and R for M and M respectively. For each history p and for each action a, the history gets extended by <a, o> where o is sampled according to P(-la, p) and P(\u00b7la, p) as follows:\nP(oia, p) = L f3(p)(s) L P(s'ia, s)P(ois'), s'\nP(oia, p) = L P(p)(s) L P(s'ia, s)P(ois'). ,,\nThe reward functions are defined as follows: Rp = I:, f3(p)(s)R,, and Rp = I;, fj(p)(s)R,. A policy is a function from agent-move positions in the game tree (histories) to agent actions. Although the optimal policy for an MOP is deterministic, the Kearns et a!. algorithm described in the following sec tion is based on stochastic sampling and hence we need to accommodate stochastic policies, i.e., policies which associate each position (history) with a probability dis tribution over the next action. The expected return starting from history p and following stochastic policy Jl. is denoted as VI' ( p) in M and VI' (p) in M. Adapting standard MOP definitions we have that the following recursive relationships are true for all p.\nV\"(p) = R(f3(p)) + 'Y L P(a!JJ, p)P(aia, p)V\"(p; <a, o>) <!>,\u00a2>\nSimilarly we define optimal value functions V * (p) = max\" V \ufffd'(p), and V*(p) =maxi' V\ufffd'(p).\nPlanning for Factored POMDPs 411\n3 The Planning Algorithm\nOur algorithm is, with minor adaptations, that of Kearns et a!. Our contribution is not the algorithm itself but rather the analysis of the algorithm when applied to the simplified belief state MOP M. This analysis is given in later sections. Here we describe the algorithm and state the basic results of Kearns et a!.\nThe algorithm defines a stochastic policy, which we denote by A8, which takes a history and computes an action. Given 6, -y and Rmax the algorithm computes a horizon H and a sample size C in a manner described below. Then given a history the algorithm searches the game tree below the node defined by that history to an additional depth of H . The algorithm searches all possible agent actions at each agent move andsamples C observations at each environment move. l\\t each agent-move position p the algorithm computes a value Q(p, a, d) for each possible action a using the following relations. Q(p, a, d)=\n{ R(p) if d = 0\nR(p) + 'Yi; LoEO(p,a) maXb Q(p; <a, a>, b, d- 1) otherwise\nHere, O(p, a) is a set of C samples of observations from P( alp, a). The algorithm then selects the root action to be argmaxbQ(p, b, H ). Kearns et a!. prove the following theorem.\nTheorem 1 (Kearns et al.) If\nH\nc\nthen !VA' (p)-V*(p)l::; 6.\nNote that the number of belief states computed by the algorithm is the number of nodes in the tree searched which is O[(IAIC)H].\n4 Summary of the Analysis\nOur main results are two analyses of the policy A8 computed by the algorithm of the preceding section. The first analysis, given in section 5, shows that under accurate belief state simplification, and for a rapidly mixing PO MOP, a near-optimal policy in M performs near-optimally in M indefinitely into the future. The\n412 McAllester and Singh\nsecond analysis, given in section 6 shows that indepen dent of the mixing rate of the POMDP, under accurate simplification the policy A5 is near-optimal for M at the beginning.\nTo state our results we need a measure of the accuracy of simplification and the mixing rate of the POMDP. First, recall that the KL-divergence between two dis tributions I)! and <1>, denoted D(<I>IIIJ!) is defined by D(<I>IIIJ!) = 2:\ufffd <l>(x) log2 !f;;. Boyen and Koller con sider belief state simplification satisfying a certain ap proximation property. Intuitively, we would like to say that the simplification function S has the prop erty that for any belief state rf> we have that S( rf>) is near rf>. However, because KL-divergence does not sat isfy the triangle inequality we follow Boyen and Koller in assuming that for certain pairs of belief states rf> and 1/J we have that D(?/JIIS(rf>))- D(?/JIIrf>) is small. More precisely, we take 1jJ to be f3(p;<a,o>) and rf> to be U(\ufffd(p), <a, o>). We say that Sis I<L-<-approximate for M if for all histories p and action-observation pairs <a, o> we have the following.\nD(,B(p; <a, o> )l[t3(p; <a, o> ))-D(,l3(p)IIU(t3(p), <a, o>)) :S < Recall that the .C1-distance between two distribu tions <I> and I)!, denoted II <I> - I)! ll1, is defined by 1 1<1>- 1J!II1 = 2:,, I<I>(x)- IJ!(x)l. A simplifier S will be called .C1-E-approximate if ll6(so)-S(6(so)))lll \ufffd E and for all simplified belief states rf> we have IIU( rf>) - S(U(\u00a2>))111 \ufffd L Following Boyen and Koller, we will call a POMDP ry-mixing if for any two underly ing states s1 and s2 and any action a we have that 2:,, min(P(sala, sl), P(sala, s2)) 2: TJ, or equivalently, IIP(sla, sl)- P(sla, s2)ll1 \ufffd 2- 2ry. Now we can state the results of the two analyses of A5.\nTheorem 2 (Tracking Near Optimality) For KL-E approximate simplification and any ry-mixing POMDP, we have that for all t 2: 0,\nA' I A' \u2022 3Rmax {2f EIPI=t V (p)- V (p)l \ufffd 6 + (1 - \ufffd')2 v ry' where the expectation is taken over histories generated by running A\" in the true POMDP.\nTheorem 3 (Drifting Near Optimality) For .C1+ approximate simplification and any POMDP we have the following for all t 2: 0.\nEA' IVA'( )- V*( )I 6 1 2ERmax 12ERmaxt IPI=t p p :::: + (1- 1)3 + (1 -1)2\nThe expectation is taken as in theorem 2.\n5 Tracking Near Optimality\nIn this section we prove theorem 2 which states, in essence, that for accurate belief state simplification,\nand rapidly mixing POMDPs, the policy A5 is near optimal. The main component of the analysis is a value transfer lemma stating that for accurate sim plification and rapidly mixing MDPs we have that for any policy J.l the simplified value v\ufffd(p) is near the true value V\ufffd\"(p) (under expectation over p). Most of this section involves the proof of this value transfer lemma.\nOur departure point is the tracking theorem of Boyen and Koller stated below. In the following theorem, and throughout the remainder of this paper, the expecta tions are taken over histories p generated by running J.l in the true belief state MDP (using P(ola, p) rather than P(ola , p)).\nTheorem 4 (Boyen&I<oller 98} For any ry-mixing POMDP, any I<L-E-approximate simplifier for that POMDP, any t 2: 0, and any policy J.L, we have the following.\nThis theorem bounds the expected KL-divergence from the true belief state to the approximate belief state. Our first step is to convert this statement about KL-divergence into a statement about .C1-distance.\nLemma 5 For any ry-mixing POMDP, any I<L-\u00a3 approximate S for that POMDP, any t 2: 0, and any policy J.l we have the following.\nProof: For any two distributions P and Q we have the following [3].\nD(PI IQ) 2: \ufffd(liP-Qlh)2 This implies the following.\nf > TJ\n>\n> E\ufffdl=tD(f3(p) ll\ufffd(p)) 1 \ufffd ' 2 2EIPI=t(ll,6(p)-,6(p)ll1) 1 ' 2 2(E\ufffdI=tllf3(p)-,B(p)lll)\nWhich implies the lemma. 0\nOur objective is to bound the true value of A5, i.e., the value under P(ola,p) and Rp rather than P(ola,p) and Rp. The next step is to bound the (expected) difference between these fundamental quantities.\nLemma 6 For any ry-mixing POMDP, any KL-E approximate S for that POMDP, any policy J.l, and\nany t 2: 0:\n\ufffd f2i E\ufffdl=tiiP(ola,p)- P(ola,p)lh \ufffd V -:q\u00b7\nProof: Note that For any p IIP(ola, p)- P(ola, p)lh\nLo I L,(i3(P)(s)- ffi(p)(s)) I:,, P(s'la, s)P(ols')l ::0: I: I: l(/3(p)(s)- ffi(p)(s))l I: , P(s'la, s)P(ols') = 2:::: li3(p)(s)- ffi(p)(s)l Lo L,, P(s'la, s)P(ols')\nI:, l/3(p)(s)- \ufffd(p)(s)l = lli3(P)- \ufffd(P)II'\nwhich together with lemma 5 implies the result. 0\nLemma 7 For any I)-mixing POMDP, any KL-f approximate S for that POMDP, any policy Jl., and any t 2: 0:\nProof: For any p\nIRp- Rpl = I \ufffd)i3(P)(s)- \ufffd(p)(s))R,I < L 1(,(3(p)(s)- ffi(p)(s))IRmax = Rmaxll,l3(p)- \ufffd(P)I\\1\nwhich together with lemma 5 implies the result. 0\nWe can now prove the value transfer lemma.\nLemma 8 {Value Tmnsfer Lemma) For any I)-mixing POMDP, any KL-E-approximate S for that POMDP, and any policy Jl. we have the following.\nProof: Define\ufffd\ufffd to be E\ufffdl=tiV11(p)- V11(p)l, and\ufffd to be maxt \ufffd1. For all t we have the following.\n-\"t El\u2022I=\u2022IV\"(p)-V\"(P)I \ufffd Ei'\u2022l=tiR(p)- R(p)J\n+\ufffdE\ufffd\u2022I=t \ufffd P(ai\"' p) I Lo P(ola, p)V-\"(p; <a, o>) - Lo P(o!a, p)V\ufffd' (p; <a, o>) \ufffd Rmu/\u00a5\n+\ufffdE\ufffdPI=t 2:::: P(ai\"' p) L P(oJa, p) I a o V'\"(p; <a, o>) -V\ufffd-'(p; <a, o>) \"\"\" I P( oJo, p) I +1EjPI=\ufffd L P(al\ufffd-t. p) .l...J V\ufffd-'(p; <a, o>) -P(oia, p) a o\nPlanning for Factored POMDPs 413\n:S; Rmax\ufffd+I.O. +oVmaxEfpJ=t L P(ai\", p) L I\na o\nP(oJa, p) -i'(oia, p)\nTherefore \ufffd 5 Vmaxj'\u00a5 + \ufffd . which implies that \ufffd < \ufffd fii - \ufffd fii. - !-') v \ufffd\u00b7 11-\"1)2 v \ufffd. Corollary 9 For any I)-mixing POMDP, any KL-f approximate S for that POMDP, and any policies Jl.J and J1.2:\nEll'_ IV1'2(p) _ Vll>(p)l < Rmax fii. IPI-t - (1 _ /)2 V ry Proof: Let J1.3 be the policy defined by Jl.3 (p) = fl. I (p) if IPI < t and Jl.2(P) otherwise. We now have the fol lowing.\nE\ufffdi=tiVII>(p)- Vll>(p)l = E\ufffdi=tiVII3(p)- Vll3(p)l The result now follows from lemma 8 applied to J1.3. 0\nLemma 10 For any I)-mixing POMDP, any KL-f approximate S for that POMDP, any t 2: 0, and any policy A we have the following.\nA I \ufffd \u2022 ( ) \u2022 ( ) I 2Rmax [2{ EIPI=t V p -V p \ufffd (1- 1)2 v ry\nProof: Let Jl.* be the optimal policy for the belief state MDP and let p\u2022 be the optimal policy for the simplified belief state MDP. If V* (p) 2: V* (p) then VII' (p) 2: vi<' (p) 2: VII' (p) and so IV*(p)- V*(p)l \ufffd IV11' (p)- VII' (p)l. Similarly, for V*(p) < V*(p) we have IV'(p)- V'(p)l \ufffdIV\"' (p)- vi<' (P)I. In either case we have the following.\nIV'(p)-v'(p)l \ufffdIV\"' (p)-v\"' (p)I+IV11\u2022 (p)-v11\u2022 (p)l We now get the desired result by taking the expecta tion over p and bounding the resulting expectation of the right hand side above using corollary 9. 0 Finally, we can prove theorem 2 by noting that E\ufffd\ufffd=t IV A' (p) -V \u2022 (p) I can be no greater than the sum A6 A6 \ufffd A6 A' \ufffdA' ' of EIPI=tiV (p)- V (P)I, EIPl=tiV (p)- V*(p)l, and E\ufffd\ufffd=t!V'(p)- V*(p)l.\n6 Drifting Near Optimality\nIn this section we prove theorem 3 which states, in essence, that for accurate belief state simplification,\n414 McAllester and Singh\nand for any POMDP, the policy A6 is near-optimal near the beginning. Without the assumption of rapid mixing it is possible that errors due to simplification accumulate with time. However, in the discounted case studied here the value of a given history is only sensi tive to errors within an effective planning horizon de termined by the discount factor. We first bound how rapidly errors due to simplification can accumulate and then use this bound to prove the value transfer lemma.\nIn the appendix we prove the following lemma which is analogous to that of Boyen and Koller except that it gives a bound on .C1-distance rather than KL divergence and the bound increases with time.\nLemma 11 Under .C1-E-approximate simplification we have that for any (possibly unmixing) POMDP, for all t, and for all Jl,\nAs in Section 5 the above bound on .C1-distance yields ' ' fJ ' bound on Rand P, namely EIPi=tiRp-Rp l \ufffd 4EtRmax, J1. ' and EIPI=tiiP$'-P$'ll1 \ufffd 4d.\nNext we prove the value transfer lemma.\nLemma 12 (Value Transfer Lemma) For any POMDP, and for any E-approximate S for that POMDP, for any policy jl:\nProof: Let\ufffd\ufffd= E\ufffdi=tiVJl.(p)-V\ufffd'(p)l. First we will show that\nA, E\ufffdi=tiV\"(p)- V\"(P)I \ufffd E\ufffdi=tiR(,B(p))- R(ffi(p))i\n2::\u00ab.<> Pp( <a, o> )V\"(p; <a, o>)\n(2)\n+E\ufffdI=< 'Y , , - I:<t<.<> Pp( <a, o> )V\"(p; <a, o>) \ufffd 4e(t + l)Rmax\n+'Y El,>l=, L Pp( <a, o>) I <t<,o> V\"(p; <a, o>) -V\"(p; <a, o>) +E\ufffd1=,'Y L:v\"(p;<a,o>) , I Pp( <a, o>)\n<t<,<> -Pp( <a, o>) \ufffd 4e(t + 1)Rmax +')'At+!\nI Pp( <a, o>) +'YVmaxE\ufffdI=t L <t<,<> -Pp( <a, o>)\n< 4<(t + 1)Rmax +')'At+! + ')'4<(t + 1)Vmax = 4<(t + 1)Vmax +')'At+!\nNow we show that\ufffd\ufffd 4E(t+1)Vmax+l\ufffdt+1 implies that \ufffd < \ufffd + \ufffd t - 1-'Y (1-\"1)2. Plugging \ufffdt+1 \ufffd Vmax into Equation 2 we get \ufffd\ufffd \ufffd 4E(t + 1) Vmax +!Vmax. By continuing this unfolding in the limit we get that \ufffd\ufffd \ufffd 4EVmax\ufffd\ufffdo li (t + 1 + i). Summing this series gives \ufffd\ufffd < + \ufffd . D - 1-'Y (1-\"1)2 The proofs of corollary 9 and lemma 10 can be used here to show that for all t 2: 0 we have the following.\nAs with theorem 2, theorem 3 now follows from the observation that E\ufffd;=tiVA'(p) - V*(p)l can be no greater than the sum of E\ufffd;=,IVA'(p) - vA'(p)l, A' 'A' ' A' ' EIPi=tiV (p)-V*(p)l, and EIPi=tiV*(p)-V*(p)l.\n7 Conclusion and Future Work\nWe showed that the straightforward application of Kearns et al.'s planning algorithm to POMDPs leads to a more efficient algorithm than existing algorithms for solving POMDPs, at least for problems with large observation sets and tractable belief-state computa tion. For problems in which exact belief-state compu tation is too expensive, by building on the Kearns et a!. algorithm, and the work on belief state simplifi cation by Boyen and Koller, we have established that the accuracy-efficiency trade-off in belief state simpli fication can be used to achieve an accuracy efficiency trade-off in planning. Although we do not yet have a reasonable planning algorithm for factored POMDPs that is polynomial in the number of state variables, belief state simplification can in principle dramatically reduce the exponent in the running time of the expo nential planning algorithm.\nA significant open problem is to find some addi tional conditions under which truly polynomial (in the number of state variables) POMDP planning can be achieved. It seems possible that if one imposes a \"smoothness\" criterion on the reward function, e.g., that the reward is a sum of local rewards, then poly nomial factored POMDP planning can be done.\nReferences\n[1] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Dis tributed Computation: Numerical Methods. Prentice Hall, Englewood Cliffs, N J, 1989.\n[2] X. Boyen and D. Koller. Tractable inference for com plex stochastic processes. In Proceedings of the 14th Annual Conference on Uncertainty in AI (UAI), Wis consin, 1998.\n[3] T. M. Cover and J. A. Thomas. Elements of Informa tion Theory. Wiley-lnterscience, 1991.\n[ 4] M. L. Littman. Algorithms for Sequential Decision Making. PhD thesis, Brown University, 1996.\n[5] C. H. Papadimitriou and J. N. Tsitsiklis. The com plexity of markov decision processes. Mathematics of Operations Research, 12(3):441-450, August 1987.\n[6] E. J. Sondik. The optimal control of partially observ able Markov processes over the infinite horizon: dis counted case. Operations Research, 26:282-304, 1978.\n[7] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.\nA Proof of Lemma 1 1\nFirst we prove some general lemmas about \u00a3.1 distance and then prove Lemma 11.\nLemma 13 Let {3 and \ufffd be two distributions on the same setS and let P(x\\s) be a conditional probability function on X x S, i.e., P(x\\s) E [0, 1] and Lx P(x\\s) = 1 for any fixed s . We have the following where P(x\\s){3(s) denotes the obvious distribution on X x S.\nProof:\n\\\\P(x\\s){3(s)- P(x\\s)\ufffd(s)\\\\1 = \\\\{3- \ufffd1\\1\n\\\\P(xls){3(s)- P(x\\s)\ufffd(s)\\\\1 = 2::::2:::: \\P(x\\s){3(s) - P(x\\s)\ufffd(s)\\\nX = L 1,8(s)- \ufffd(s)\\ L P(xls)\n= L lf3(s)- \ufffd(s)l\n= 11{3- \ufffdlh\nLemma 14 Let P and Q be any two distributions on X x Y. Let P(x) denote the marginal distribution on X, i.e., P(x) = Ly P(x, y), and similarly for Q(x). We then have the following.\nIIP(x)- Q(x)l\\1 :5 1\\P(x, y)- Q(x, Y)l\\1\nProof:\nIIP(x)- Q(x)l\\1 LILP(x,y)- LQ(x, y)l X y y\n< L L IP(x, y)- Q(x, y)l y\nIIP(x, y)- Q(x, y)l\\1\nPlanning for Factored POMDPs 415\nLemma 15 Let P and Q be any two distributions on X x 0. Let P(o) denote the marginal distribution on 0, i.e., P(o) = Lx P(x, o), and similarly for Q(o). We then have the following.\nEo-P(o) 1\\P(xlo)- Q(x\\o)\\\\1 :5 IIP(x, o)- Q(x, o)l\\1 + IIP(o)- Q(o)l\\1\nProof:\nEo-P(o) 1\\P(xlo)- Q(x\\o)\\\\1 = \" P(o)\" I P(x, o) _ Q(x, o) I L...J L...J P(o) Q(o) 0 X \"P( J \" I P(x, o) _ Q(x, o)l ::; L...J 0 L...J P(o) P(o) 0 X \"P( J \" IQ(x, o) _ Q(x, o) l + L...J 0 L...J P(o) Q(o) 0 X = L L IP(x, o)- Q(x, o)l 0 X \"P( J\"I Q(x, o)- Q(x, o) l + L...J 0 L...J P(o) Q(o) 0 X = IIP(x, o)- Q(x, o)l\\1 + L P(o) L Q(x, o)\\ P\ufffdo)- Q\ufffdo) I 0 X = IIP(x, o)- Q(x, o)l\\1 + L P(o)\\ P\ufffdo)- Q\ufffdo) \\2:::: Q(x, o) 0 X = \\\\P(x, o)- Q(x, o)\\\\1 \" 1 1 + L...J P(o)\\ P(o) - Q(o) IQ(o) 0 = \\\\P(x, o)- Q(x, o)\\\\1 + L IQ(o)- P(o)\\ 0 = IIP(x, o)- Q(x, o)ll1 + \\\\Q(o)- P(o)l\\1\nWe need one more lemma before proving Lemma 11. We assume a fixed (stochastic) policy iJ defined by the proba bilities P(a\\p, iJ). We define P(w, crlp, s, t,p) to be be the probability that if we assume that the hidden state at time I PI is s and then run forward for t additional steps in the underlying MDP we generate additional history cr and end in final state w. If {3 is a belief state we let P( w, <Tip, {3, t, iJ) be L, f3(s)P(w,cr\\p, s,t , p). We also let P(wi<T,p, {3,iJ) be the probability of w given cr under to the joint distribution P( w, <T\\p, {3, t, p). We say that {3; is simplified if S({3;) = {3;. We say that a belief state <5; is pre-simplified if it can be written as U( <a, o> , {3;) for some <a, o> and simplified {3;. We say that S is \u00a3.,-<-approximate if for any pre-simplified <5; we have IIS(<S;)- .5;\\\\1 ::; L Lemma 16 For any \u00a3.,-<-approximate function S, pres irnplified belief state <5, history p, and policy iJ we have the following where the expectation is taken over the marginal on <T of the joint distribution defined by P( w, <TIP, <5, t, iJ).\nEu\\\\P(w\\<T,p, <S,t,p)- P(w\\<T,p, S(<S), t,p)ll' :52<\n416 McAllester and Singh\nProof: By lemma 15 we have the following.\nEaiiP( wi<T, p, o, t, I')- P( wi<T, p, S(o), t, l')lb ::; IIP(w, ,.lp,o, t, l')- P(w, ,.lp,S(o),t,l')ll'\n+liP( \"'IP, o, t, I') - P( \"'IP, S( o), t, I' )I b\nCombining lemmas 13 and 14 we have the following. IIP(w, <Tjp, 6, t, I')- P(w, <Tip, S(6), t, 1')11,\n::; llo- S(o)lb ::; ' IIP(,.Ip,o, t, I')- P(.,.IP, S(o), t,l')li, ::; llo- s(o)ll, ::; ,\nThese together give the desired result. 0\nLet the functions {3 and \ufffd be defined as in the paper and let the expectations be taken over I'Uillring I' in the true belief-state MDP. We now prove Lemma 11 (restated here). Lemma 11 If S is \u00a3,-\u20ac-approximate then for any t > 0 and for all I' we have the following. -\nProof: Note that P(wi<T, p,/3,1') satisfies the following conditions.\nP(wl0,p,f3,!') = f3(w)\nP(wl <a,o>; <T,p, /3, 1') = P( wi<T, p; <a, o>, U( <a, o>, /3), I')\nThis implies that P(wi<T, p, {3, I') is independent of I' and p and hence can be written as P( wi<T, {3). Let P( wi<T, /3) be the estimate of P( wi<T, /3) gotten by running with simplified intermediate belief states. More formally this estimate is defined by the following equations.\nP(wl0, /3) = f3(w)\nP(wl <a,o>; <T,/3) = P(wj<T, S(U(<a,o>, /3))) We now prove the following general statement for any p and simplified belief state {3 and where Ej';f=,!(<T) denotes the expectation of f (\"') over the histories \"' defined by P(<Tip, {3, t, !J).\n(3) The proof is by induction on t. For t = 0 the result is immediate. Now assume the result fort and consider t+ 1. In the following o abbreviates U( <a, o>, /3) and P( <a, o>) abbreviates P(<a,o> lp, /3, /J).\nt.p.\ufffd,t = Ej';f=<+IIIP( wi<T,/3) - P( wl<l', f3)1b = L:: P(<a,o>)Ej';:j,;\ufffd\u00b7'IIP(wl.,.',o)\n-P(wl.,.', S(o))lb < \"'\\;\"' P(<a o>)Ep;4;>.,'>,6IIP(wl,.' o) -\ufffd ' ]a']=t ' 4;>.,1>\n-P(wl,.', S(o))lb + L:: P(<a, o>)Ej';:j,;\ufffd\u00b7'IIP(wi,.', S(6))\n-P( wl.,.', S( o)) II' ::; 2<+ L:: P(<a,o>)Ej';:j,;\ufffd\u00b7'IIP(wi,.',S(o))\n-P(wl,.', S(o))lb = 2\u20ac + L:: P( <a, o>)\nL:: P(<Tip; <a, o>, o, I')IIP( wl,.', S(o) ]cr'l=t\n-P(wl,.', S(o))lb ::; 2\u20ac + L:: P( <a, o>)\n]cr']=t -P(,.Ip; <a, o>, S(o), 1')1\n+ L:: P( <a, o>) L:: P(<Tip; <a, o>, S(o), I') \ufffd.\u00a2> ]a']=t\nliP( wl,.', S( 6)) - P( wl,.', S( o) )II, = 4\u20ac + L:: P( <a, o>)\nEj';:j,;\ufffd\u00b7s(o) IIP(wl.,.', S(o)) -f>(wl,.', S(o))lb\n= 4\u20ac + 4\u20act = 4\u20ac(t + 1)\nWe now show that 3 implies the main theorem. Let II denote the initial belief state that has all its mass on state so.\nE\ufffdl=< lif3(p)- \ufffd(P)Ii, 0 II \u2022 = E1\ufffd1=,IIP(wlp, II)- P(wlp,S(II))Ib\n= L:: P(pi0, II, t,I')IIP(wlp,II)- f>(wlp,S(II))Ib IPI=t\n:5 L:: 2IP(pl0, II, t, I')- P(pl0, S(II), t, 1')1 IPI=t\n+ L:: P(pl0, S(II), t, I')IIP(wlp, II)- P(wlp, S(II))Ib IPI=t\n:5 2\u2022 + E\ufffd\ufffd\ufffd \ufffdl liP( wlp, II)- P( wlp, S(II))Ii, ::; 2<+ E\ufffd\ufffd\ufffd\ufffdl IIP(wlp, II)- P(wlp, S(II))Ib\n+E\ufffd\ufffd\ufffd\ufffd)IIP(wlp,S(II))- P(wlp, S(II))Ib ::; 3\u20ac + 4\u20act::; 4<(t + 1)"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "We are interested in the problem of plan\u00ad<lb>ning for factored POMOPs. Building on<lb>the recent results of Kearns, Mansour and<lb>Ng, we provide a planning algorithm for fac\u00ad<lb>tored POMOPs that exploits the accuracy\u00ad<lb>efficiency tradeoff in the belief state sim plifi\u00ad<lb>cation introduced by Boyen and Koller.", "creator": "pdftk 1.41 - www.pdftk.com"}}}