{"id": "1302.2277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2013", "title": "A Time Series Forest for Classification and Feature Extraction", "abstract": "We propose a method for classifying time series, called entropy and distance increase. Experimental studies show that the entropy increase criterion improves the accuracy of TSF. TSF uses random samples at each tree node and has linear computational complexity over the length of a time series, and can be built using parallel computing techniques such as the multi-core computing used here. The time meaning curve is also proposed to capture the important time characteristics useful for classification. Experimental studies show that TSF with simple features such as mean, deviation, and inclination outperforms strong competitors such as nearby classifiers with dynamic time distortion, is computationally efficient, and can provide insight into time characteristics.", "histories": [["v1", "Sat, 9 Feb 2013 22:56:45 GMT  (630kb)", "https://arxiv.org/abs/1302.2277v1", null], ["v2", "Mon, 18 Feb 2013 00:10:56 GMT  (651kb)", "http://arxiv.org/abs/1302.2277v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["houtao deng", "george runger", "eugene tuv", "martyanov vladimir"], "accepted": false, "id": "1302.2277"}, "pdf": {"name": "1302.2277.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Houtao Deng", "George Runger"], "emails": ["hdeng3@asu.edu", "hdeng3@asu.edu", "george.runger@asu.edu", "eugene.tuv@intel.com", "vladimir.martyanov@intel.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n22 77\nv2 [\ncs .L\nA tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.\nKeywords: decision tree; ensemble; Entrance gain; interpretability; large margin; time series classification;\n\u2217Corresponding author: hdeng3@asu.edu Email addresses: hdeng3@asu.edu (Houtao Deng), george.runger@asu.edu\n(George Runger), eugene.tuv@intel.com (Eugene Tuv), vladimir.martyanov@intel.com (Martyanov Vladimir)\nPreprint submitted to Elsevier February 19, 2013"}, {"heading": "1. Introduction", "text": "Time series classification has been playing an important role in many disciplines such as finance [25] and medicine [2]. Although one can treat the value of each time point as a feature and use a regular classifier such as onenearest-neighbor (NN) with Euclidean distance for time series classification, the classifier may be sensitive to the distortion of the time axis and can lead to unsatisfactory accuracy performance. One-nearest-neighbor with dynamic time warping (NNDTW) is robust to the distortion of the time axis and has proven exceptionally difficult to beat [20]. However, NNDTW provides limited insights into the temporal characteristics useful for distinguishing time series from different classes.\nThe temporal features calculated over time series intervals [15], referred to as interval features, can capture the temporal characteristics, and can also handle the distortion in the time axis. For example, in the two-class time series shown in Figure 1, the time series from one of the classes have sudden changes between time 201 and time 400 but not in the same time points. An interval feature such as the standard deviation between time 201 and time 400 is able to distinguish the two-class time series.\nPrevious work [15] has built decision trees on interval features. However, a large number of interval features can be extracted from time series, and there can be a large number of candidate splits to evaluate at each tree node. Class-based measures (e.g., entropy gain), which evaluate the ability of separating the classes, are commonly used to select the best split in a node. However, there can be many splits having the same ability of separating the classes. Therefore, measures able to further distinguish these splits are desirable. Also, given a large number of features/splits, an efficient and accurate classifier that can provide insights into the temporal characteristics is valuable.\nTo this end, we propose a novel tree-ensemble classifier: time series forest (TSF). TSF employs a new measure called the Entrance (entropy and distance) gain to identify high-quality splits. We show that TSF using Entrance gain outperforms TSF using entropy gain and also two NNDTW algorithms. By using a random feature sampling strategy, TSF has computational complexity linear in the time series length. Furthermore, we propose the temporal importance curve to capture the temporal characteristics informative for time series classification.\nThe remainder of this paper is organized as follows. Section 2 presents the\ndefinition of the problem and related work. Section 3 introduces the interval features. Section 4 describes the TSF method. Section 5 demonstrates the effectiveness and efficiency of TSF by experiments. Conclusions are drawn in Section 6."}, {"heading": "2. Definition and Related Work", "text": "Given N training time series instances (examples): {e1, ..., ei, ..., eN} and the corresponding class labels {y1, ..., yi, ..., yN}, where yi \u2208 {1, 2, ..., C}, the objective of time series classification is to predict the class labels for testing instances. Here we assume the values of time series are measured at equally-spaced intervals, and also assume the training and testing time series instances are of the same length M .\nTime series classification methods can be divided into instance-based and feature-based methods. Instance-based classifiers predict a testing instance based on its similarity to the training instances. Among instance-based classifiers, nearest-neighbor classifiers with Euclidean distance (NNEuclidean) or dynamic time warping (NNDTW) have been widely and successfully used\n[12, 21, 8, 24]. Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and is considered as a strong solution for time series problems [13]. Instancebased classifiers can be accurate, but they provide limited insights into the temporal characteristics useful for classification.\nFeature-based classifiers build models on temporal features, and potentially can be more interpretable than instance-based classifiers. Featurebased classifiers commonly consist of two steps: defining the temporal features and training a classifier based on the temporal features defined. Nanopoulos et al. [11] extracted statistical features such as the mean and deviation of an entire time series, and then used a multi-layer perceptron neural network for classification. This method only captured the global properties of time series. Local properties, potentially informative for classification, were ignored. Geurts [7] extracted local temporal properties after discretizing the time series. Rodr\u0301\u0131guez et al. [15] boosted binary stumps on temporal features from intervals of the time series and Rodr\u0301\u0131guez and Alonso [14], Rodr\u0301\u0131guez et al. [16] applied classifiers such as a decision tree and a SVM on the temporal features extracted from the boosted binary stumps. However, only binary stumps were boosted, and the effect of using more complex base learners, such as decision trees, should be studied [15] (but larger tree models impact the computational complexity). Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node. However, the number of candidate splits is generally large, and, thus, there can be multiple splits having the same ability of separating the classes. Consequently, additional measures able to further distinguish these features are desirable. Ye and Keogh [23] briefly discussed strategies of introducing additional measures to break ties, but it was in a different context.\nRecently, Ye and Keogh [23] proposed time series shapelets to perform interpretable time series classification. Shapelets are time series subsequences which are in some sense maximally representative of a class [23]. Ye and Keogh [23], Xing et al. [22], Lines et al. [10] have successfully shown that time series shapelets can produce highly interpretable results. In term of accuracy, Lines et al. [10] showed that the shapelet approach is comparable to NNDTW for nine data sets investigated.\nEruhimov et al. [5] considered a massive number of features. The feature sets were derived from statistical moments, wavelets, Chebyshev coefficients, PCA coefficients, and the original values of time series. The method can be accurate, but is hard to interpret and computationally expensive. The\nobjective of our work is to produce an effective and efficient classifier that uses/yields a set of simple features that can contribute to the domain knowledge. For example, in manufacturing applications, specific properties of the time series signals that discriminate conforming from un-conforming products are invaluable to diagnose, correct, and improve processes."}, {"heading": "3. Interval Features", "text": "Interval features are calculated from a time series interval, e.g., \u201cthe interval between time 10 and time 30\u201d. Many types of features over a time interval can be considered, but one may prefer simple and interpretable features such as the mean and standard deviation, e.g., \u201cthe average of the time series segment between time 10 and time 30\u201d.\nLet K be the number of feature types and fk(\u00b7) (k = 1, 2, ..., K) be the kth type. Here we consider three types: f1 = mean, f2 = standard deviation, f3 = slope. Let fk(t1, t2) for 1 \u2264 t1 \u2264 t2 \u2264 M denote the kth interval feature calculated over the interval between t1 and t2. Let vi be the value at time i for a time series example. Then the three interval features for the example are calculated as follows:\nf1(t1, t2) =\n\u2211t2 i=t1 vi\nt2 \u2212 t1 + 1 (1)\nf2(t1, t2) =\n\n\n\n\u221a\n\u2211t2 i=t1 (vi\u2212f1(t1,t2)) 2\nt2\u2212t1 t2 > t1\n0 t2 = t1\n(2)\nf3(t1, t2) =\n{\n\u03b2\u0302 t2 > t1 0 t2 = t1 (3)\nwhere \u03b2\u0302 is the slope of the least squares regression line of the training set {(t1, vt1), (t1 + 1, vt1+1). . . . , (t2, vt2)}.\nInterval features have been shown to be effective for time series classification [15, 14, 16]. However, the interval feature space is large (O(M2)). Rodr\u0301\u0131guez et al. [15] considered using only intervals of lengths equal to powers of two, and, therefore, reduced the feature space to O(M logM). Here we consider the random sampling strategy used in a random forest [1] that reduces the feature space to O(M) at each tree node."}, {"heading": "4. Time Series Forest Classifier", "text": ""}, {"heading": "4.1. Splitting criterion", "text": "A time series tree is the base component of a time series forest, and the splitting criterion is used to determine the best way to split a node in a tree. A candidate split S in a time series tree node tests the following condition (for simplicity and without loss of generality, we assume the root node here):\nfk(t1, t2) \u2264 \u03c4 (4)\nfor a threshold \u03c4 . The instances satisfying the condition are sent to the left child node. Otherwise, the instances are sent to the right child node.\nLet {fnk (t1, t2), n \u2208 1, 2, ..., N} denote the set of values of fk(t1, t2) for all training instances at the node. To obtain a good threshold \u03c4 in equation 4, one can sort the feature values of all the training instances and then select the best threshold from the midpoints between pairs of consecutive values, but this can be too costly [14]. We consider the strategy employed in Rodr\u0301\u0131guez and Alonso [14]. The candidate thresholds for a particular type feature fk are formed such that the range of [minNn=1(f n k (t1, t2)), max N n=1(f n k (t1, t2)] is divided into equal-width intervals. The number of candidate thresholds is denoted as \u03ba and is fixed, e.g., 20. The best threshold is then selected from the candidate thresholds. In this manner, sorting is avoided, and only \u03ba tests are needed.\nFurthermore, a splitting criterion is needed to define the best split S\u2217: f\u2217(t \u2217 1, t \u2217 2) \u2264 \u03c4 \u2217. We employ a combination of entropy gain and a distance measure as the splitting criterion. Entropy gain are commonly used as the splitting criterion in tree models. Denote the proportions of instances corresponding to classes {1, 2, ..., C} at a tree node as {\u03b31, \u03b32, ..., \u03b3C}, respectively. The entropy at the node is defined as\nEntropy = \u2212\u03a3Cc=1\u03b3c log \u03b3c (5)\nThe entropy gain \u25b3Entropy for a split is then the difference between the weighted sum of entropy at the child nodes and the entropy at the parent node, where the weight at a child node is the proportion of instances assigned to that child node.\n\u25b3Entropy evaluates the usefulness of separating the classes. However, in time series classification, the number of candidate splits can be large, and there are often cases where multiple candidate splits have the same\n\u25b3Entropy. Therefore we consider an additional measure called Margin, which calculates the distance between a candidate threshold and its nearest feature value. The Margin of split fk(t1, t2) \u2264 \u03c4 is calculated as\nMargin = min n=1,2,...,N\n|fnk (t1, t2)\u2212 \u03c4 | (6)\nwhere fnk (t1, t2) is the value of fk(t1, t2) for the n th instance at the node. A new splitting criterion E, referred to as the Entrance (entropy and distance) gain, is defined as a combination of \u25b3Entropy and Margin.\nE = \u25b3Entropy + \u03b1 \u00b7Margin (7) where \u03b1 is small enough so that the only role for \u03b1 in the model is to break ties that can occur from the entropy gain alone. Alternatively, one can store the values of \u25b3Entropy and Margin for a split, and use Margin to break ties when another split has the same \u25b3Entropy.\nClearly, the split with the maximum E should be selected to split the node. Furthermore, Margin and E are sensitive to the scale of the features, and we employ the following strategy if different types of features have different scales. For each feature type fk, select the split with the maximum Entrance gain. To compare the best splits from different feature types, the split with the maximum \u25b3Entropy is selected. If the best splits from different feature types have the same maximum \u25b3Entropy, one of the best splits is randomly selected.\nFigure 2 illustrates the intuition behind the criterion E. The figure shows, in one dimension, six instances from three classes in different symbols/colors. Three candidate splits S1, S2 and S3 are also shown in the figure. Clearly, all splits have the same \u25b3Entropy, but one may prefer S3 because S3 has a larger margin than S1 and S2. The Entrance gain is able to choose S3 as the best split.\nAlgorithm 1 sample() function: randomly samples a set of intervals < T1, T2 >, where T1 is the set of starting time points of intervals, and T2 is the set of ending points. The function RandSampNoRep(set, samplesize) randomly selects samplesize elements from set without replacement.\nT1 = \u2205, T2 = \u2205 W = RandSampNoRep({1, ...,M}, \u221a M) for w in set W do T1 = RandSampNoRep({1, ...,M \u2212 w + 1}, \u221a M \u2212 w + 1)\nfor t1 in set T1 do T2 = T2 \u22c3\n(t1 + w \u2212 1) end for\nend for return < T1, T2 >\nAlgorithm 2 tree(data): Time series tree. For simplicity of the algorithm, we assume different types of features are on the same scale so that E can be compared.\n< T1, T2 >=sample() calculate Thresholdk, the set of candidate thresholds for each feature type k E\u2217 = 0, \u25b3Entropy\u2217 = 0, t\u22171 = 0, t2\u2217 = 0, \u03c4\u2217 = 0, f\u2217 = \u2205 for < t1, t2 > in set < T1, T2 > do\nfor k in 1:K do for \u03c4 in Thresholdk do\ncalculate \u25b3Entropy and E for fk(t1, t2) \u2264 \u03c4 if E > E\u2217 then\nE\u2217 = E, \u25b3Entropy\u2217 = \u25b3Entropy, t\u22171 = t1, t\u22172 = t2, \u03c4\u2217 = \u03c4 , f\u2217 = fk end if\nend for\nend for\nend for if \u25b3Entropy\u2217 = 0 then label this node as a leaf and return\nend if dataleft \u2190 time series with f\u2217(t\u22171, t\u22172) \u2264 \u03c4\u2217 dataright \u2190 time series with f\u2217(t\u22171, t\u22172) > \u03c4\u2217 tree(dataleft) tree(dataright)"}, {"heading": "4.2. Time Series Tree and Time Series Forest", "text": "The construction of a time series tree follows a top-down, recursive strategy similar to standard decision tree algorithms, but uses the Entrance gain as the splitting criterion. Furthermore, the random sampling strategy employed in random forest (RF) [1] is considered here. At each node, RF only tests \u221a p features randomly sampled from the complete feature set consisting of p features. In each time series tree node, we consider randomly sampling O( \u221a M) interval sizes and O( \u221a M) starting positions. Therefore, the feature space is reduced to only O(M). The sampling algorithm is illustrated in Algorithm 1.\nThe time series tree algorithm is shown in Algorithm 2. For simplicity, we assume different types of features are on the same scale so that E can be compared. If different types of features have different scales, the previous mentioned strategy can be used, that is, for each feature type fk, select the split with the maximum Entrance gain. To compare the best splits from different feature types, the split with the maximum \u25b3Entropy is selected. Furthermore, a node is labeled as a leaf if there is no improvement on the entropy gain (e.g. all features have the same value or all instances belong to the same class).\nA time series forest (TSF) is a collection of time series trees. A TSF predicts a testing instance to be the majority class according to the votes from all time series trees."}, {"heading": "4.3. Computational Complexity", "text": "Let nij denote the number of instances in the j th node at the ith depth in a time series tree. At each node, calculating the splitting criterion of a single interval feature has complexity O(nij\u03ba), where \u03ba is the number of candidate thresholds. As O(M) interval features are randomly selected for evaluation, the complexity for evaluating the features at a node is O(nijM\u03ba). As \u03ba is considered as a constant, the complexity at a node is O(nijM).\nThe total number of instances at each depth is at most N (i.e., \u2211 j n i j \u2264\nN). Therefore, at the ith depth in the tree, the complexity is O( \u2211 j n i jM) \u2264 O(NM). Assuming the maximum depth of a tree model is O(logN) [19], the complexity of a time series tree becomes O(MN logN). Therefore, the complexity of a TSF with nTree time series trees is at most O(nTreeMN logN), linear in the length of time series."}, {"heading": "4.4. Temporal Importance Curve", "text": "TSF consists of multiple trees and is difficult to understand. Here we propose the temporal importance curve to provide insights into time series classification. At each node of TSF, the entropy gain can be calculated for the interval feature used for splitting. For a time index in the time series, one can add the entropy gain of all the splits associated with the time index for a particular type of feature. That is, for a feature type fk, the importance score for time index t can be calculated as\nImpk(t) = \u2211\nt1\u2264t\u2264t2,\u03bd\u2208SN\n\u25b3Entropy(fk(t1, t2), \u03bd) (8)\nwhere SN is the set of split nodes in TSF, and \u25b3Entropy(fk(t1, t2), \u03bd) is the entropy gain for feature fk(t1, t2) at node \u03bd. Note\u25b3Entropy(fk(t1, t2), \u03bd) = 0 if fk(t1, t2) is not used for splitting node \u03bd. Furthermore, one temporal importance curve is generated for each feature type. Consequently, for the mean, standard deviation and slope features, we calculate the mean, standard deviation, and slope temporal importance curves, respectively.\nTo investigate the temporal importance curve, we simulated two data sets, each with 1000 time points and two classes. For the first data set the time series have the same distribution so that no feature is useful for separating the classes. The time series values from both classes are normally distributed with zero mean and unit variance. The time series and the importance curves from TSF using Entrance gain are shown in Figure 3(a). It can be seen that all curves have larger values in the middle.\nNote that the number of intervals that include time index t in a time series is Num(t) = t(M \u2212 t + 1) (9) Consequently, different time indices are associated with different numbers of intervals. The number of intervals for each time index for time series with 1000 time points is plotted in Figure 3(b). The indices in the middle have more intervals than the indices on the edges of the time series. Because Impk(t) is calculated by adding the entropy gain of all the splits associated with time index t for feature fk, it can be biased towards the time points having more interval features (particularly if no feature is important for classification).\nFor the second data set the time series from the two classes have different means in interval [201, 250], and different standard deviations in interval\n[501, 550]. The temporal importance curves from TSF using Entrance gain are shown in Figure 4(a). The curves for the mean and slope have peaks in interval [201, 250], and the curve for the standard deviation has a peak in interval [501, 550]. Therefore, these curves capture the important temporal characteristics.\nWe also built TSF using entropy gain, and the corresponding temporal importance curves are shown in Figure 4(b). Although the curves also have peaks in interval [201, 250], the curves have long tails. Indeed, the entropy gain is not able to distinguish many interval features. For example, the mean feature for interval [201,250], and the mean feature for interval [201,400] have the same entropy gain as both can distinguish the two classes of time series. However, the mean feature for interval [201,250] has a larger E than the mean feature for interval [201,400]. Consequently, TSF using Entrance gain is able to capture the temporal characteristics more accurately."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Experimental Setup", "text": "The main functions of the TSF algorithm were implemented in Matlab, while computationally expensive subfunctions such as interval feature calculations were written in C. The parameters were set as follows: the number of trees = 500, f(\u00b7) = {mean, standard deviation, slope}, and the number of candidate thresholds \u03ba = 20. TSF was applied to a set of time series benchmark data sets [9] summarized in Table 1. The training/testing split setting is the same as in Keogh et al. [9]. The experiments were run on a computer with four cores and the TSF algorithm was built in parallel.\nThe purpose of the experiments is to answer the following questions: (1) Does the Entrance gain criterion improve the accuracy performance and how is the accuracy performance of TSF compared to other time series classifiers? (2) Is TSF computationally efficient? (3) Can the temporal importance curves provide some insights about the temporal characteristics useful for classification?"}, {"heading": "5.2. Results", "text": "We investigated the performance of TSF using the Entrance gain criterion (denoted as TSF) and using the original entropy gain criterion (denoted as TSF-entropy), respectively. We also considered alternative classifiers for comparison: random forest [1] applied to the interval features with sizes\npower of two (interRF), the 1-nearest-neighbor (NN) classifier with Euclidean distance (NNEuclidean), the 1-NN Best warping window DTW (DTWBest)\n[12] and the 1-NN DTW with no warping window (DTWNoWin) methods\nacquired directly from Keogh et al. [9]. DTWBest has a fixed window limiting the window width and searches for the best window size, while DTWNoWin does not use such a window.\nThe classification error rates are shown in Table 2. To compare multiple classifiers to TSF over multiple data sets, we used the procedure for comparing multiple classifiers with a control over multiple data sets suggested by Dems\u030car [3], i.e., the Friedman test [6] followed by the Bonferroni-Dunn test [4] if the Friedman test shows a significant difference between the classifiers. In our case, the Friedman test shows that there is a significant difference between the six classifiers at the 0.001 level. Therefore, we proceeded with the Bonferroni-Dunn test.\nFor the Bonferroni-Dunn test, the performance of two classifiers is different at the \u03b1 level if the their average ranks differ by at least the critical difference (CD):\nz\u03b1 = q\u03b1\n\u221a\nNclassifier(Nclassifier + 1)\n6Ndata (10)\nwhere Nclassifier is the number of classifiers in the comparison (six classifiers in our experiments), Ndata is the number of data sets (45 data sets in our experiments), and q\u03b1 is the critical value for the two-tailed Bonferroni-Dunn test for multiple classifier comparison with a control. Note q0.05 = 2.576 and q0.1 = 2.326 (Table 5(b) in Dems\u030car [3]), then according to Equation 10, z0.05 = 1.016 and z0.1 = 0.917. The average rank of each classifier, and the difference between the average ranks of TSF and each competitor are shown in Table 2. According to the rank difference, there is a significant difference between TSF and competitors NNEuclidean, DTWNoWin and interRF at the 0.1 level.\nIn addition to the multi-classifier comparison procedure, we also considered Wilcoxon signed ranks test [18] suggested for comparing a pair of classifiers, as the resolution for the multi-classifier comparison procedure can be too low to distinguish two classifiers with significantly different performance, but with close average ranks. For example, for six classifiers and 45 data sets, assume classifier A always ranks the first and classifier B always ranks the second. Although classifier A is always better than classifier B, the average ranks of classifier A and classifier B differ by only one, and therefore there is no significant difference between the two classifiers at the 0.05 level according to the two-tailed Bonferroni-Dunn test.\nThe p-values of the Wilcoxon signed ranks tests between TSF and each\ncompetitor are shown in Table 2. It can be seen there is a significant difference between TSF and all other competitors: TSF-entropy, interRF, NNEuclidean, DTWNoWin and DTWBest at the 0.1 level.\nNext consider the robustness of TSF accuracy to the number of trees. Figure 5 shows the error rate of each data set versus the number of trees, and the average error rate over all data sets versus the number of trees (represented by the thicker red line). The error rates tend to decrease as the number of trees increases, but the change is relatively small for most data sets after 100 trees.\nThe GunPoint and Wafter time series and their corresponding temporal importance curves (mean, standard deviation and slope) are shown in Figure 6. For the GunPoint time series, the mean temporal importance curve captures the characteristic that the two classes have different means in interval [60,100]. The standard deviation and slope temporal importance curves, respectively, capture the characteristics that the two classes have different standard deviations and slopes in the left and right sides of the time se-\nries. For the Wafer time series, the standard deviation temporal importance curve captures the sudden changes of the time series of class 1 near the 100th point. Consequently, the temporal importance curve is able to provide insights into the temporal characteristics useful for distinguishing time series from different classes."}, {"heading": "5.3. Computational Complexity", "text": "First consider the computational complexity of TSF with regard to the length of time series. We selected the data sets with more than 1000 time points. For each data set, \u03bbM of the time points were randomly sampled, where M is the length of the time series, and \u03bb is a multiplier. The computational times for different values of \u03bb are shown in Figure 7(a). Next consider the computational complexity of TSF with regard to the number of training instances. Data sets with more than 1000 training instances were selected. For each data set, \u03bbN of the time points were randomly sampled, where N is the number of training instances. The computational times for different values of \u03bb are shown in Figure 7(b). It can be seen that the computational time tends to be linear both in the time series length and in the number of training instances.\nTherefore, TSF is a computationally efficient classifier for time series. Furthermore, in the current TSF implementation, the interval features are dynamically calculated at each node, as pre-computing the interval features would need O(M2) features to be stored. It should noted, however, dynamic calculation can lead to repeated calculations of the interval features. Therefore, the implementation can be further improved by storing the interval features already calculated to avoid repeated calculations."}, {"heading": "6. Conclusions", "text": "Both high accuracy and interpretability are desirable for classifiers. Previous classifiers such as NNDTW can be accurate, but provide limited insights into the temporal characteristics. Interval features can be used to capture temporal characteristics, however, the huge feature space can result in many splits having the same entropy gain. Furthermore, the computational complexity becomes a concern when the feature space becomes large.\nTime series forest (TSF) proposed here addresses the challenges by using the following two strategies. Firstly, TSF uses a new splitting criterion named Entrance gain that combines the entropy gain and a distance measure to\nidentify high-quality splits. Experimental studies on 45 benchmark data sets show that the Entrance gain improves the accuracy of TSF. Secondly, TSF randomly samples O(M) features from O(M2) features, and thus makes the computational complexity linear in the time series length. In addition, each tree in TSF is grown independently, and, therefore, modern parallel computing techniques can be leveraged to speed up TSF.\nTSF is an ensemble of trees and is not easy to understand. However, we propose the temporal importance curve, calculated from TSF, to capture the informative interval features. The temporal importance curve enables one to identify the important temporal characteristics.\nTSF uses simple summary statistical features, but outperforms widely used alternatives. More complex features, such as wavelets, can be also used in the framework of TSF, which potentially can further improve the accuracy performance, but at the cost of interpretability.\nIn summary, TSF is an accurate, efficient time series classifier, and is able to provide insights on the temporal characteristics useful for distinguishing time series from different classes. We also note that TSF assumes that the time series are of the same length. Given a set of time series with different lengths, techniques such as dynamic time warping can be used to align the time series into the same length. Still, directly handling time series with varying lengths would make TSF more convenient to use, and future work\nincludes such an extension."}, {"heading": "Acknowledgements", "text": "This research was partially supported by ONR grant N00014-09-1-0656. We also wish to thank the editor and anonymous reviewers for their valuable comments."}], "references": [{"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Constrained mixture estimation for analysis and robust classification of clinical time series", "author": ["I. Costa", "A. Sch\u00f6nhuth", "C. Hafemeister", "A. Schliep"], "venue": "Bioinformatics 25 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "The Journal of Machine Learning Research 7 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1961}, {"title": "Constructing high dimensional feature space for time series classification", "author": ["V. Eruhimov", "V. Martyanov", "E. Tuv"], "venue": "in: Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparison of alternative tests of significance for the problem of m rankings", "author": ["M. Friedman"], "venue": "The Annals of Mathematical Statistics 11 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1940}, {"title": "Pattern extraction for time series classification", "author": ["P. Geurts"], "venue": "in: Proceedings of the 5th European conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y. Jeong", "M. Jeong", "O. Omitaomu"], "venue": "Pattern Recognition 44 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The ucr time series classification/clustering", "author": ["E. Keogh", "X. Xi", "L. Wei", "C. Ratanamahatana"], "venue": "homepage: www.cs.ucr.edu/ \u0303eamonn/time series data/", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "A shapelet transform for time series classification", "author": ["J. Lines", "L.M. Davis", "J. Hills", "A. Bagnall"], "venue": "in: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature-based classification of time-series data", "author": ["A. Nanopoulos", "R. Alcock", "Y. Manolopoulos"], "venue": "International Journal of Computer Research 10 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Making time-series classification more accurate using learned constraints", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Three myths about dynamic time warping data mining", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Interval and dynamic time warping-based decision trees", "author": ["J. Rod\u0155\u0131guez", "C. Alonso"], "venue": "in: Proceedings of the 2004 ACM symposium on Applied computing, ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Boosting interval based literals", "author": ["J. Rod\u0155\u0131guez", "C. Alonso", "H. Bostr\u00f6m"], "venue": "Intelligent Data Analysis 5 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machines of interval-based features for time series classification", "author": ["J. Rod\u0155\u0131guez", "C. Alonso", "J. Maestro"], "venue": "Knowledge-Based Systems 18 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing 26 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin 1 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1945}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast time series classification using numerosity reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "in: Proceedings of the 23rd international conference on Machine learning (ICML), ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Early prediction on time series: a nearest neighbor approach", "author": ["Z. Xing", "J. Pei", "P. Yu"], "venue": "in: Proceedings of the 21st International Joint Conference on Artifical Intelligence (IJCAI), Morgan Kaufmann", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting interpretable features for early classification on time series", "author": ["Z. Xing", "J. Pei", "P.S. Yu", "K. Wang"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "in: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Dynamic time warping constraint learning for large margin nearest neighbor classification", "author": ["D. Yu", "X. Yu", "Q. Hu", "J. Liu", "A. Wu"], "venue": "Information Sciences 181 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised classification of share price trends", "author": ["Z. Zeng", "H. Yan"], "venue": "Information Sciences 178 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "Time series classification has been playing an important role in many disciplines such as finance [25] and medicine [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "Time series classification has been playing an important role in many disciplines such as finance [25] and medicine [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 19, "context": "One-nearest-neighbor with dynamic time warping (NNDTW) is robust to the distortion of the time axis and has proven exceptionally difficult to beat [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "The temporal features calculated over time series intervals [15], referred to as interval features, can capture the temporal characteristics, and can also handle the distortion in the time axis.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "Previous work [15] has built decision trees on interval features.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 23, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 16, "context": "Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and is considered as a strong solution for time series problems [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and is considered as a strong solution for time series problems [13].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "[11] extracted statistical features such as the mean and deviation of an entire time series, and then used a multi-layer perceptron neural network for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Geurts [7] extracted local temporal properties after discretizing the time series.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "[15] boosted binary stumps on temporal features from intervals of the time series and Rod\u0155\u0131guez and Alonso [14], Rod\u0155\u0131guez et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] boosted binary stumps on temporal features from intervals of the time series and Rod\u0155\u0131guez and Alonso [14], Rod\u0155\u0131guez et al.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "[16] applied classifiers such as a decision tree and a SVM on the temporal features extracted from the boosted binary stumps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "However, only binary stumps were boosted, and the effect of using more complex base learners, such as decision trees, should be studied [15] (but larger tree models impact the computational complexity).", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 13, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 15, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 22, "context": "Ye and Keogh [23] briefly discussed strategies of introducing additional measures to break ties, but it was in a different context.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Recently, Ye and Keogh [23] proposed time series shapelets to perform interpretable time series classification.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Shapelets are time series subsequences which are in some sense maximally representative of a class [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Ye and Keogh [23], Xing et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "[22], Lines et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] have successfully shown that time series shapelets can produce highly interpretable results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] showed that the shapelet approach is comparable to NNDTW for nine data sets investigated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] considered a massive number of features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 13, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 15, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 14, "context": "[15] considered using only intervals of lengths equal to powers of two, and, therefore, reduced the feature space to O(M logM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Here we consider the random sampling strategy used in a random forest [1] that reduces the feature space to O(M) at each tree node.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "To obtain a good threshold \u03c4 in equation 4, one can sort the feature values of all the training instances and then select the best threshold from the midpoints between pairs of consecutive values, but this can be too costly [14].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "We consider the strategy employed in Rod\u0155\u0131guez and Alonso [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Furthermore, the random sampling strategy employed in random forest (RF) [1] is considered here.", "startOffset": 73, "endOffset": 76}, {"referenceID": 18, "context": "Assuming the maximum depth of a tree model is O(logN) [19], the complexity of a time series tree becomes O(MN logN).", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "TSF was applied to a set of time series benchmark data sets [9] summarized in Table 1.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We also considered alternative classifiers for comparison: random forest [1] applied to the interval features with sizes", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Table 2: The error rates of TSF using the splitting criterion: Entrance gain (TSF) or entropy gain (TSF-entropy), random forest with 500 trees applied to the interval features with sizes power of two (interRF), 1-NN with Euclidean distance (NNEuclidean), 1-NN with the best warping window DTW (DTWBest) [12], and 1-NN DTW with no warping window (DTWNoWin).", "startOffset": 303, "endOffset": 307}, {"referenceID": 11, "context": "[12] and the 1-NN DTW with no warping window (DTWNoWin) methods", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "To compare multiple classifiers to TSF over multiple data sets, we used the procedure for comparing multiple classifiers with a control over multiple data sets suggested by Dem\u0161ar [3], i.", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": ", the Friedman test [6] followed by the Bonferroni-Dunn test [4] if the Friedman test shows a significant difference between the classifiers.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": ", the Friedman test [6] followed by the Bonferroni-Dunn test [4] if the Friedman test shows a significant difference between the classifiers.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "326 (Table 5(b) in Dem\u0161ar [3]), then according to Equation 10, z0.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "In addition to the multi-classifier comparison procedure, we also considered Wilcoxon signed ranks test [18] suggested for comparing a pair of classifiers, as the resolution for the multi-classifier comparison procedure can be too low to distinguish two classifiers with significantly different performance, but with close average ranks.", "startOffset": 104, "endOffset": 108}], "year": 2013, "abstractText": "A tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.", "creator": "LaTeX with hyperref package"}}}