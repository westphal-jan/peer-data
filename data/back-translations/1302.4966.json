{"id": "1302.4966", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Probabilistic Exploration in Planning while Learning", "abstract": "Sequential decision-making tasks with incomplete information are characterized by the exploration problem, namely the trade-off between further exploration to learn more about the environment and the immediate utilization of the resulting information for decision-making. Within artificial intelligence, there is an increasing interest in exploring planning-while-learning algorithms for these decision-making tasks. In this paper, we focus in particular on the exploration problem in the areas of reinforcement learning and Q-Learning. Existing exploration strategies for Q-Learning are of a heuristic nature and exhibit limited scalability in tasks with large (or infinite) state and scope for action. Efficient experimentation is required to solve uncertainties when comparing potential plans (i.e. exploration). The experiment should be sufficient to select a locally optimal plan (i.e. exploitation) with statistical significance. For this purpose, we develop a probable algorithm to determine the probability of an optimal exploration strategy, as close as possible to a selection method as necessary as possible for determining a reconnaissance strategy.", "histories": [["v1", "Wed, 20 Feb 2013 15:22:12 GMT  (673kb)", "http://arxiv.org/abs/1302.4966v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["grigoris i karakoulas"], "accepted": false, "id": "1302.4966"}, "pdf": {"name": "1302.4966.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Exploration in Planning while Learning", "authors": ["Grigoris I. Karakoulas"], "emails": ["grigoris@ai.iit.nrc.ca"], "sections": [{"heading": null, "text": "continuous flow of events in time. Effective decision-mak ing requires resolution of uncertainty as early as possible. The\n. te?dency to minimize losses resulting from wrong predictions of future events necessitates the division of the problem solution into steps. A decision at each step must make use of the information from the evolution of the events experienced thus far, but that evolution, in fact, depends on the type of decision made at each step.\n1 INTRODUCTION\nSequential decision tasks with incomplete information have long been studied in decision theory and control the ory. Within artificial intelligence there has been increasing interest in studying these tasks, especially in the areas of planning (Dean & Wellman, 1991) and machine learning (Barto et al., 1989). In all these contexts, an agent that is given a goal to achieve in a partially known environment, plans its actions while learning enough about the environ ment in order to enable that goal. Such an agent should be able to represent and reason about change, uncertainty and the value or utility of its plans. Most importantly, though, it should be able to deal at any time with the trade-off between further exploration (also called identification probing) and satisfactory exploitation (also called control: hypothesis selection, planning) of the accrued informa tion. Several ideas for exploration strategies have been developed in the areas of statistical decision theory, opti mal experiment design and adaptive control. There is cur rently an influx of these ideas into decision-theoretic planning (Russell & Wefald, 1991; Drapper et al., 1994; Pemberton & Korf, 1994), concept learning (Scott & Markovitch, 1993), speed-up learning (Gratch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994), sys tem identification (Cohn, 1994; Dean et al., 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).\n\ufffdany decision-making tasks are inherently sequential smce they are characterized by two features: incomplete knowledge and steps. These two features are intercon nectr..d. This is because most real-world decision problems occur within complex and uncertain environments in a\nIn this paper we focus on exploration in reinforcement learning. The latter is a paradigm within machine learning appropriate to planning-while-learning tasks that has been shown to produce good solutions in domains such as games (Tesauro, 1992) and robotics (Mahadevan & Con nell, 1993). The goal of reinforcement learning is to deter-\nProbabilistic Exploration in Planning while Learning 353\nmine a plan (i.e. a mapping from states of the environment into actions) that optimizes the expected value of a perfor mance measure. An example of such a measure is the total long-term reward accrued from following a plan. The dis tribution of the performance measure of each plan depends on the dynamics of the environment which are assumed unknown. Furthermore, in tasks with large state and action spaces the search for an optimal plan within the space of possible plans is intractable. There is therefore need for efficient exploration that can be used to gather observa tions about the behavior of the environment These obser vations should also be sufficient for selecting a plan which is probabilistically close to a locally optimal one.\nThe exploration strategies that have been developed for reinforcement learning are largely of a heuristic nature (Thrun, 1992). They also have limited scaleability in tasks with large (or infinite) state and action spaces. The main idea of this paper is as follows. Since in a planning-while learning task an agent operates in a partially known envi ronment, exploration should be guided by the effects of uncertainty on the performance estimates of plans. During exploration the agent can probe the environment to gather samples of state values. Our goal is to develop a probabi listic algorithm for deciding at each stage of the task how much sampling is needed for exploration in order that the plan selected at each stage be, with arbitrarily high proba bility, arbitrarily close to a locally optimal plan. The algo rithm should be incremental so that its performance should monotonically improve with time as more computational resources are allocated to exploration.\n1.1 Q-LEARNING FOR PLANNING WHILE LEARNING\nQ-learning (Watkins, 1989) is the reinforcement learning algorithm that has been most studied both theoretically and practically. This is mainly due to its origination from the concepts and principles of dynamic programming (Bellman, 1957). Because of this relation with DP, Q learning integrates planning and learning into a single algorithm in contrast to other reinforcement learning methods.\nLet us define X as the state space of the environment, A as the action space and P as the state transition model of the environment mapping elements of X X A into proba bility distributions over X. r (x1, a1) is a reward func tion specifying the immediate reward that an agent receives by applying action '!t at state \ufffd\ufffd\u00b7 A pol\ufffdcy 7t (i.e. a plan) is defined as 1t: X\ufffd A. Gtven a pohcy 7t from the set of possible policies II, the value of an initial state x0, V 1t (x0) , is the expected sum of rewards which are discounted by how far into the future they occur. Thus,\nwhere y, 0 < y < 1 , is the discount factor and a1 = 1t(x1). The Q-learning algorithm is based on the idea of maintain ing for each state and action pair an estimate of Q (x ,a ) The latter is an action-value function Q1t Sf. x 1 A \ufffd 9t that gives the expected discounted c\ufffdulative reward (reinforcement) for performing action a1 in state x1 and con?n\ufffdng with \ufffdlie\ufffd 7t th\ufffdter. According to the DP pnnctple of opbmality, the optunal value function Q1t.(x1,a1) can be written as:\nwhere the expected value of the cumulative discounted reward from applying the optimal policy 1t* at state x1 + 1 and thereafter, is given by\nIf the state transition model is known, then the value itera tion algorithm of DP can be applied so that at each itera tion of the algorithm the Q-value of a state and action pair can be updated by\nwhere the expectation in (4.1) is over all possible next states. Successive iterations over the above two equations yield, in the limit, the optimal Q1t* function and hence the optimal policy 1t* . Unfortunately, the probability distributions of the state transition model are usually unknown. And even if these distributions were known, the task of identifying the opti mal policy would be intractable (i.e. the \"curse of dimen sionality\" (Bellman, 1957)).\nIn Q-learning the action model is assumed unknown. The agent only observes at each time step the value of the cur rent state. This value could also be sampled from a random function. In that case, however, the agent does not need to know the stochastic characteristics of this sampling. The same applies to the value of the immediate reward, as this may also be determined probabilistically. The surface of the Q \u2022 function is learned by applying the rule 1t\n354 Karakoulas\nQt + 1 (xt, at) = ( 1 - f3t) Qt (xt, at)+\nan infinite number of times to all possible state and action pairs. In (5.1) (31, 0 < f3t < 1 , is the learning :\"'U:\u00b7 The optimal policy 1t* is then obtained from the proJeCtion of the state space on the performance surface of Qx* . Learn ing of the Q * function can be intractable when the space X X A is ui\\ large for visiting all state and action pairs sufficiently enough. A simple exploration strategy like choosing an action according to a particular distribution (e.g. random walk, Boltzman distribution etc.) is inher ently exponential especially in stochastic domains (White head, 1991). There has been work on learning state transition models and/or utilizing knowledge generated from the Q-learning process in order to guide exploration (Sutton, 1990; Lin, 1991; Thrun, 1992). Although these exploration strategies have been shown to enhance the effectiveness of Q-learning, their efficiency can be ques tioned in complex tasks. This is because most of these strategies seek to perform exhaustive exploration. Further more, they do not provide any probabilistic guarantee for improvement of the policy being learned.\nFor this reason, we propose and develop a probabilistic exploration algorithm based on a selection procedure from sequential statistical analysis. Within each Q-learning iter ation the algorithm uses this statistical procedure and the current estimates of the Qx functions to decide how much to explore within a possibly infinite set of policies. At the end of each iteration, the algorithm is probabilistically guaranteed to find a solution approximately close to a locally optimal one.\nSection 2 presents the proposed exploration strategy. Sec tion 3 shows how this strategy is incorporated into robust Q-learning, an algorithm specifically developed for adap tive planning in noisy and uncertain environments (Karak oulas, 1995a). Section 4 reports on an experiment for evaluating the performance of the proposed exploration strategy in robust Q-learning. Related and future work is discussed in Section 5. Conclusions are given in Section 6.\n2 THE PROPOSED EXPLORATION STRATEGY We assume that the agent has access to a stochastic dis crete-time dynamic system that provides an approximation of the state of the environment at each time. The approxi mation need only be good enough for evaluating the rela tive performance of the policies. The system is in general of the form1\nwhere\n(6)\nx is the state of the system at time t and it may t summarize past information that is relevant for future optimization; a is the action selected at time t according to the t policy function a1 = 1t (xt) ; e is the vector of parameters whose values are assumed unknown;\nw is a random parameter (also called disturbance t or noise).\nActions can be discrete or real-valued. It should be noted that when the set of actions is infinite, the problem of searching for the best action in Q-learning becomes extremely difficult\nThe agent does not know the probability distribution of the state of the system in (6). At any time the agent can apply an action and get a sample of possible next states as a result of the stochasticity of the system. It can then use this sample and the current estimate of the Qx function to sta tistically evaluate the likely effectiveness of the respective policy 1t . In standard Q-learning, whenever the agent applies an action to the environment it observes only one value of the next possible state. In our approach, on the other hand, the agent receives a sample of values of the next state through the partial model of (6). The latter, therefore, acts as an oracle for the agent.\nDue to this deviation from standard Q-learning we intro duce the notion of q (x , a ) . The latter represents the 1t t t dti rti . expected discounted cumulative rewar or pe ormmg action a = 1t(X) in the state with particular value X1 and conJnuing wi& policy 1t thereafter. The Q7t function can then be defined as the expected value of the distribu tion of q-values, i.e.\nBy this definition, the sum inside the brackets in (2) is equal to q'lt*. (x1, at) . The variance of the distribution \ufffdf q-values is denoted by cr\ufffd . Both the mean and the van ance are assumed unknowH'. The agent can use samples of state values and the current estimates of q-values to com pute estimates of the mean and variance. These estimates are denoted by Q1t and s\ufffdn respectively. I When the state of the environment is discrete, as it is usually the case with AI planning problems, the stochastic process of the state of the envi ronment can be approximated by the state transition model.\nProbabilistic Exploration in Planning while Learning 355\nDuring exploration the agent seeks a policy 1t' E fl such that\n1t' = argmax1t e 0Q1t (x1, a1) (8)\nThe policy function 1t(x1) is assumed to be defined in terms of a set of parameters c. In the case of real-valued actions the function can be for example of the linear form 1t(X1) = c \u00b7 x1\u2022 The values of the parameters are unknown. An instance of parameter values identifies a par ticular policy 1t . As the set of possible policies rr is infi nite, the common solution for pruning the space of alternatives is the search technique of hill-climbing. In particular, the technique of steepest-ascent hill-climbing can climb the gradient of Q-values in (7) by selecting the policy having the highest Q-value with respect to the cur rent policy. An apparent limitation of this approach is that it requires the probability density function of x1 to be expressed in an analytic form. Even when such informa tion is available its use in this type of search makes the problem computationally intractable. To overcome this serious limitation we follow an empirical approach to the exploration problem.\nAs already mentioned samples of state values can be used to derive estimates of the mean and variance of the q-value distribution of a policy 1t . A probabilistic algorithm can be built which, given the samples, can as efficiently as possible hill-climb from an initial policy 1t to one that is, with high probability, a local optimum. The search is per formed using a set of transformations of an initial policy 1t, T = { T;(1t)} . Each T. maps a policy 1t into th I. ' I ano er po 1cy 1t . Such mapping can be performed for example through a perturbation of parameter values of the policy function at the point identified by 1t .\nEfficiency of the search refers to the bounded number of samples that are sufficient for the probabilistic algorithm to output a solution with statistical significance. Such effi ciency can be achieved by incorporating a sequential sta tistical procedure (Govindarajulu, 1987) into the hill climbing algorithm. Our interest in such procedures is due to their ability to reach an inference earlier than a fixed sample size procedure. In the latter, the size of the sample is fixed prior to any statistical experiment. The distinct characteristic of a sequential procedure is that the number of observations required to terminate a statistical experi ment is a random variable as it depends on the outcome of the observations from the experiment Inference in sequential statistical procedures is performed via a statisti cal test called a stopping rule. This rule determines the sufficient number of observations that need to be made in order that the null hypothesis of the statistical test is rejected with a specific degree of error. The number of observations that have been made when the stopping rule\nis satisfied, is called the stopping time. A sequential statis tical procedure can, therefore, meet our requirement for an incremental exploration algorithm.\nLet us define the local policy improvement operator /:1t\ufffd1t by\n{ T; (1t) if Q; (x,. a1) - Qlt (x,. 1t (x,)) > 0 1 (1t) = 1t otherwise (9)\nwhere T; ( 1t) is policy transformation i from a countably infinite set T = { T i ( 1t) } in a neighborhood of the current pol\ufffdcy 1t . Q \ufffd denotes the Q function of policy transformabon T; (1tJ . Also a1 = Ti (1t (x1)) \u2022 Because of the stochasticity of the environment, the ine quality between the Q-values in (9) can only be satisfied with a particular level of statistical confidence. Since no probability density functions are assumed, this inequality should be empirically assessed from the random samples of states. For this purpose we next introduce the sequential selection procedure by Dudewicz and Dalal (Dudewicz & Dalal, 1975).\nConsider the problem of selecting from k populations ( k \ufffd 2) - each being distributed as N ( Jl\u00b7, <J 7) with d 2 , I l Jl; an <J; unknown- the populabon that has the largest mean. The selection of the largest mean is done with probability at least p* whenever the difference between the top two means is at least equal to some value E . That is\nP {correct selection} 2: p*\nif (JJ. [k] - J.1 [k-1] ) \ufffd E (10)\nwhere J.1 [ 1] \ufffd .. . \ufffd J.1 [ k 1. denotes the ordered sequence of means and p* = 1 - 0 with a being the error proba bility.\nIn our case a population i corresponds to the population of mean Q-values, H2;} , of policy transformation Ti ( 1t) , i = 1, . . . , k. Each of the mean Q-values is estimated by\n(11)\nwhere N is the size of a sample of state values and hence of a sample of q-values. Each {2 i is an unbiased estimator of Q i \u2022 From mathematical statistics we have that for any random variable y\n356 Karakoulas\n(12)\nThe sequential statistical selection problem of (10) suits our purpose of selecting from a set of k populations, { \ufffd;} i = 1, ... , k, the one with the highest Q-value, with probability of correct selection at least p* . Since ( 10) cannot be satisfied by a sequential procedure that involves only a single stage of sampling, Dudewicz and Dalal have constructed a two-stage procedure for deter mining the minimum size of the sample for each popula tion. The procedure is based on a multivariate t distribution for defining the probability p* of correct selection in (10).\nThis sequential selection procedure can form the basis of a probabilistic hill-climbing algorithm. When a policy is selected at the end of an iteration of the algorithm, the pro cedure is again applied in the next iteration for a set of local transformations of the newly selected policy. The search terminates at the iteration where the selection pro cedure selects the same policy as at the last iteration. That policy is a probably locally optimal one. The error of each stage a i can be set such that the total error over all stages a is less than some pre-specified constant. The probabilistic hill-climbing (PHC) algorithm is pre sented in Figure 1. When the algorithm is invoked for exploration during a Q-leaming iteration it is initialized with the policy of the previous iteration. For ease of nota tion we denote the mean of q-values, \ufffdi defined in (11), with 11;. CO counts the iterations of the PHC algorithm. The value of \u00a3c.o is dynamically determined for each set of transformations T according to the values of the polic\u00a5, improvement operators of the set. The symbol r. 1 denotes the smallest integer greater than or equal to the quantity enclosed. The values of h are given by tables in Dudewicz and Dalal (1975). Specific values for the t . . are also suggested in that paper. At each iteration of tJ\u00a5e algorithm the selection procedure starts with n0 , ('!Q \ufffd 2) , samples Tl;p ... , Tl;n from each population { \ufffdi} . Additional samples are &ken according to the stopping time n; of each population. The policy transfor mation T r kl ( 1t) with the maximum T1 r kl value is then selected. The probability of this selection being the correct one is:\nP {Q[kJ -Q[k-1] \ufffde} \ufffd 1-a (13)\nIf several top Q i values are less than \u00a3 -close, the above procedure may not select the policy with the highest Q value. The amount of error from the selection depends ori \u00a3 . In this case the selection of a policy that is not the best one, is not, however, a drawback since the goal of PHC algorithm is to explore. In fact, the parameters \u00a3 and a of\nAlgorithm PHC (1t, 'I', N, a) C.0+- 0 ; While there is a set of policy transformations { T; ( 1t ) } do\nFor each policy transformation T; ( 1t) in the set do Take n0 samples Tt;p .. . , 11;110 from population HM ; Calculate the variance of the n0 samples\n\"o\ns: = (1/(n0-1)) \u00b7 L (Tt;r'l;); j=l Calculate stopping time n; = max { n0 + 1, f( ;\ufffd Tl} ; Take n;- n0 additional samples of Tt;j;\nII;\nCalculate '1; = L 't;i. Tt;r j = 1 Choose the policy transformation T [i:J (1t) with\nk = argmaxr(Tl;) ;\n1t +- T [tJ (1t) ; C.O+-C.O+l;\nReturn as output policy 1t .\nFigure 1: The probabilistic hill-climbing (PHC) algorithm.\nthe algorithm can be used to control exploration according to the degree of learning.\n3 EXPLORATION IN ROBUST Q LEARNING In a planning task an agent should be able to reason about uncertainty in its model of the environment as well as about the effects of that uncertainty on finding a satisfac tory plan. The concept of robustness refers to three issues that capture the effects of uncertainty on the performance of a plan. These issues are: (i) the stability of the environ ment's behavior under the plan, (ii) the expected total reward of the plan and (iii) the variability in the total reward as an indicator of sensitivity to uncertainty. Rea soning about the three issues can be done by evaluating the agent's attitude to the risk that is involved when fol lowing the course of action of a particular plan within the partially known environment.\nIn general, risk can be considered as one's willingness to bet against the odds of a chance prospect (e.g. a lottery).\nProbabilistic Exploration in Planning while Learning 357\nWe adopt the concept of risk aversion as stated by Dia mond and Stiglitz (1974) in order to construct a measure of robustness that reflects the agent's attitude to the risk associated with a plan. Assuming a utility function with a constant absolute risk-aversion parameter we derive the following utility measure\nwhere U 1t is the total discounted reward from policy 1t , U x is its mean, cri is its variance and <p, 0 < <p < 1 , is the risk-aversion parameter. According to (14), in a situa tion of increasing risk where the mean value of U x is pre served but its variance is increased (a mean-preserving increase in risk), a risk-averse agent would feel worse-off by a degree equal to ( 1 - <p) I <p . Using (14) we can build a robust Q-leaming algorithm (Karakoulas, 1995a) in which the reward at each iteration is defined as\nR,(x,,a,) = <pR,- (1- <p) s; I\n(15)\nwhere R t and S; are the mean and variance of the imme diate reward froDf applying action a1 to the environment. They are estimated from a sample of states of the environ ment. The counterparts of equations (5.1) and (5.2) for updating the Q-values are\nq,+ 1 (x,, a,) = q1 (x1, a,) + P,P (x1) \u2022\n[R (x1, a,) + yV, (xt+ 1) - q1 (x,, a,)] (16.1)\nV, (xt+ 1) = maxx' e T(x) (16.2)\n[ Q, (x,+ 1' 1t1 (x,+ 1)) = <pQ,- ( 1 - <p) s!J In (16.1) the probabilities P (x1) are estimated from the sample of states at time t using Bayes' rule. Equations (16.1) and (16.2) define the updating rule of robust Q leaming.\nTo see whether the PHC algorithm can be employed in robust Q-learning, we write the formula of the Q function in (16.2) in terms of a random variable with expected value 1.1 and variance cr2\n<piJ.- ( 1 - <p) cr2 (17) It can be shown that the random variable 11 ,\n11 = <pJl- ( 1 - <p) cr2 (18)\n1. Initialize; 2. For all t do:\n(i) Create a sample S 1 of current instances of states; (ii) Search probabilistically via PHC for the locally\noptimal policy x ; (iii)Apply policy x to the sample S 1 ; obtain new\nsample S1+ 1; (iv) Estimate the reward R from the sample by\n(15); (v) Update the Q value of sample S1 and policy x:\nupdate the q-value of each instance in the sample by (16.1)- (16.2); match instance to clusters of policy x ; merge instance if matching conditions satis fied;\nwhere t1 and cr2 are sample estimates of the mean and variance, is normally distributed with mean and variance that depend on 1.1 and cr2 . Hence, they are assumed unknown.\nWe can therefore apply the PHC algorithm as given in Fig ure 1 by using (18) for the definition of the random vari able 11i of a policy transformation Ti (1t) . Since 11i is an unbiased estimator of Q i , within each iteration of PHC the sequential statistical procedure finds the minimum number of observ,3tions for each 11 i and selects the policy with the highest Q -value at a particular level of statistical significance.\nFor completeness of exposition we present the basic steps of the robust Q-leaming algorithm in Figure 2 (for more details see {Karakoulas, 1993; 1995)). The steps (v) and (vi) of the algorithm refer to the function approximator that is used for generalization of the q-values over real valued state and action spaces. For each policy clusters are formed to approximate the Q function of that policy. Given a sample of states, the q-values of the sample under a particular policy are estimated by matching each state of the sample with the states already stored in the clusters of the policy.\n358 Karakoulas\n4 EXPERIMENTAL RESULTS The purpose of the experiments reported in this section is to demonsttate the effectiveness of the PHC algorithm that has been implemented as the exploration sttategy of robust Q-learning. Thus, we have run two experiments, in one applying this exploration strategy and in the other apply ing the simple and most used semi-uniform exploration sttategy. The latter str.Dtegy chooses, at each time, the pol icy with the highest Q -value with a predefined probabil ity \ufffd , and a random policy with probability 1 - \ufffd . The experiments are performed on an adaptive control task in which the environment is approximated by the fol lowing partially known model2\n(19.1)\n(19.2)\nx3, t + 1 = x1, t + 1 + u \u00b7 x2, t + 1 (19.3) The state of the model is a vector of three variables. There is only one action variable a1 which the agent can use in order to control the state of the model. There is uncertainty in the model since the exact values of the parameters of the model K, \"'\u00b7 \ufffd and u are assumed unknown. The parameters get random values from uniform distributions in [0.6,0.9], [0.1,0.4], [0.4,0.6] and [1.5,2.5] respectively. At any time t values of the state of the environment are computed by applying Monte Carlo simulations. 1n these simulations the parameter values are randomized accord ing to the aforementioned distributions.\nThe control task of the agent is: given a shock upon the environment through the state variable X 1 1 , find the opti mal policy that drives the environment back to its initial state. In adaptive control theory, the control task in linear systems with uncertain parameters - such as the one of this experiment- is usually considered as a non-linear stochastic control task. This is because a closed-form solu tion of the optimal policy does not generally exist For this reason, good suboptimal policies are sought in practice. In addition, in such task the trade-off between exploration and exploitation is crucial for finding a good solution (Kumar, 1985).\nLet us assume that the initial values of the state variables are zero. The goal of the agent must be reflected in its util ity function that penalizes whenever either x2, 1 or x3, 1\n2 Both the model and the control task have been of particular interest in the field of economic dynamics and control (Kemball-Cook, 1993; Kara koulas, 1993). We present them here by abstracting them from any domain-specific details.\ndeviates from its initial value. The utility (or cost) function is assumed to be of the form\n00\nU = L, l[ t1x\ufffd 1 + t2xi. ,] (20) t = 0\nwhere the coefficients t 1 and t2 have negative values for transforming the original minimization problem into a maximization one. The action variable a1 in this task is defined by the following policy\n(21)\nWe assume that the policy is a linear function\n(22)\nSuch linear policy functions are common in optimal con trol problems because they are robust under uncertainty and they are easy to implement. In the two experiments, the coefficients in (20) had values t1=-5 and 't2=-5. The discount rate was set to 0.988. The risk-aversion parameter cp in (15) and (16.2) was set to 0.5 giving an equal weight to both the mean and variance. In the first experiment, the parameters of the PHC algo rithm were set to 7t = 0 (i.e. no policy initially assumed), N was set to 50 and 0 was set to 0.04. Because of the linear policy function in (22) a policy transforma tion Ti (1t) generates a new policy from a policy 7t by moving the gradient of (22) by a small step.\nIn the second experiment the parameter of the semi-uni form exploration strategy \ufffd was set to 0.1. Thus, the best policy was chosen with probability 0.9 and any other pol icy in the set of possible policies was chosen with proba bility 0.1. To enable this randomization in policy selection we constructed a finite subset of policies from the original infinite set. The policies in the subset were defined by (22) with coefficient values in the discretized range -2.4,- 2.39 , ... ,0.29 ,0.3.\nThe results of the two experiments are presented in Fig ures 3 and 4. The curves from the learning algorithm with the semi-uniform distribution are depicted with dashed lines. The experiments were run for 30 time-periods. Both learning algorithms converged to the optimal policy a1 = ( -0.69) \u00b7 x1, 1\u2022 This is the same policy that was found by Kemball-Cook (1993). He used a control theory approach for solving this problem. Figure 3 shows the convergence of the two algorithms to the policy rule as a percentage of the learning run. Figure 4 shows the conver gence of the algorithms in terms of the cumulative reward obtained from following the learned policy averaged over\nProbabilistic Exploration in Planning while Learning 359\n-0$\nI -IJI \ufffd \\..- f-u 'II I -2.0 . 20 40 10\n__ ot ..... .\nFigure 1: Convergence to the optimal policy\n-0$\n1 t 1 -1.0 ..J\n-UL-----\ufffd\ufffd--\ufffd--\ufffd----\ufffd--\ufffd----\ufffd----\ufffd--\ufffd------\ufffd\ufffd----\ufffd\ufffd--\ufffd 0 20 40 \ufffd 10 \ufffd\n--olloomlqllllll\nFigure 2: Convergence of the average cumulative reward from the optimal policy\nthe number of times this policy was active. In both figures the learning algorithm with the PHC exploration con verges faster than the algorithm with the semi-uniform explomtion by a factor of three. This seems to be in agree ment with empirical studies in machine learning that have demonstrated dmstic reduction in the number of training experiences when the explomtion strategies for selecting training experience use information about the current state of the search of the theory space (see (Scott and Marko vitch, 1993) for more on this).\n5 DISCUSSION The explomtion strategies that have been developed for speed-up learning (Gmtch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994) are also based on sequential statistical analysis. The selection procedures involved are of only one stage and they are not therefore appropriate for our selection problem which requires a two- or a multi-stage procedure.\nKaelbling (1993) has developed a statistical algorithm for exploration in reinforcement learning. The algorithm works by keeping statistics on the number of times a given action has been executed and the proportion of times that it has succeeded in terms of yielding a fixed reward. Based on these statistics the algorithm constructs confidence intervals for the expected reward for each of the feasible actions; and uses the upper bound of the intervals for choosing actions and for updating the estimates of value functions. The confidence intervals are estimated from the standard normal distribution or from non-parametric sta tistical techniques. In these interval estimation procedure<: the size of the sample is predefined whereas in PHC it is determined on-line according to the observations gathered through explomtion. This enables the PHC algorithm to efficiently perform experimentation and to exploit this experimentation for finding with statistical significance a locally optimal policy. In addition, the PHC algorithm can handle real-valued actions. The relative performance of the two algorithms needs to be empirically evaluated.\n360 Karakoulas\nIn decision-theoretic planning Pemberton and Korf (1994) have proposed separate heuristic functions for exploration and decision-making in incremental real-time search algo rithms. Draper et al. (1994) have developed a probabilistic planning algorithm that perfonns both infonnation-pro ducing actions and contingent planning actions. Our exploration strategy could be applied to these planning tasks as part of a Q-learning algorithm. Of course, the search space and the transformation operators of PHC must be appropriately defined in tenns of the actions of each task. We plan to examine the perfonnance of the PHC algorithm within a Q-learning algorithm that has recently been developed for the task of cost-effective clas sification (Karakoulas, 1995b). This is a planning-while learning task in which the exploratory actions (e.g. diag nostic tests) have a cost associated with them.\nThe PHC algorithm is related in principle to the fully polynomial randomized approximation schemes that have been developed for approximating solutions of enumera tion and reliability problems (Karp & Luby, 1983; Jerrum & Sinclair, 1988). These problems are in general intracta ble. The algorithms run in time polynomial in the size of the search space and output an estimate of the solution which is, with high probability, \u00a3-close to the solution. Jerrum and Sinclair (1988) envisage the application of their algorithm to the process of simulated annealing. This process has been used in combination with the Boltzman distribution for controlling exploration in reinforcement learning.\nIn work related to our robust Q-learning method, Heger (1994) has proposed a Q-learning algorithm based on the minimax criterion. The latter defines the most risk-averse control strategy. In contrast, in our approach the risk-aver sion parameter is used to trade-off the most risk-averse cri terion represented by the variance of q-values with the most risk-neutral criterion represented by the expectation of q-values. Hence, different types of risk-averse strategies can be realized by appropriately setting the value of the risk-aversion parameter. It is worth pointing out that our robustness criterion considers only the variance of q-val ues due to sampling error. We plan to extend this criterion by including the bias factor due to the estimation error.\n6 CONCLUSION In this paper we have examined the problem of explora tion that occurs when applying Q-learning for planning while-learning tasks in uncertain environments. We pro posed a strategy that uses infonnation about the effects of uncertainty on the evaluation of alternative policies in order to guide exploration in Q-learning. A probabilistic hill-climbing (PHC) algorithm was developed for imple menting the strategy. The algorithm iterates over a two stage sequential statistical procedure that finds the mini-\nmum number of observations required for selecting a locally optimal policy with a particular level of statistical confidence. The sequential procedure makes the algorithm incremental. Furthermore, the assumptions of the proce dure do not impose any restrictions on its applicability in Q-learning. For this reason, we were able to incorporate the procedure in robust Q-learning which is a Q-learning algorithm based on risk-averse Q functions. The effective ness of the exploration strategy was tested by applying robust Q-learning on a realistic adaptive control task. Two experiments were perfonned for comparing the perfor mance of the learning algorithm using PHC and using the typical semi-unifonn distribution. The learning algorithm with PHC converged faster by a factor of three. Future work will examine the applicability of the exploration strategy in the planning-while-learning task of cost-effec tive classification.\nAcknowledgements\nThanks to Martin Brooks, Innes Ferguson and the anony mous referees for their very helpful comments on earlier versions of this paper.\nReferences\nBarto, A.G., Sutton, R.S. & Watkins, CJ.C.H. (1989). Learning and sequential decision-making. In Advances in Neural Information Processing Systems 2, pp. 686-693. Morgan Kaufmann.\nBellman, R.E. (1957). Dynamic Programming. Princeton University Press.\nCohn, D. (1994). Neural network exploration using opti mal experiment design. In Advances in Neural Informa tion Processing Systems 6, Morgan Kaufmann. Dean, T.L. and Wellman, M.P. (1991). Planning and Con trol. Morgan Kaufmann.\nDean, T., Angluin, D., Bayse, K., Engelson, J., Kaelbling, L., Kokevis, E. & Maron, 0. (1995) Inferring finite autom ata with output functions and an application to map learn ing. Machine Learning, 18,81-108.\nDiamond, P. and Stiglitz, J. (1974). Inc reases in risk and in risk aversion, Journal of Economic Theory, 8, 337-360.\nDraper, D., Hanks, S. & Weld, D. (1994). Probabilistic planning with infonnation gathering and contingent exe cution. In AAAI 1994 Spring Symposium on Decision-The oretic Planning. AAAI Press. Dudewicz, E. & Dalal, S. (1975). Allocation of observa tions in ranking and selection with unequal variances. Sankhyd: The Indian Journal of Statistics, B, 37,28-78.\nGovindarajulu, Z. (1987). The Sequential Statistical Anal ysis. American Academic Press.\nGratch, J. & DeJong, G. (1992). COMPOSER: A probabi listic solution to the utility problem in speed-up learning. In Proceedings of AAA/92, pp.235-240.\nProbabilistic Exploration in Planning while Learning 361\nGratch, J., Chien, S. & DeJong, G. (1994). Improving learning performance through rational resource allocation. In Proceedings of AAA/94, pp.576-581.\nGreiner, R. & Jurisica, I. (1992). A statistical approach to solving the E BL utility problem. In Proceedings of AAA/92, pp.241-248.\nJerrum, M. & Sinclair, A. (1988). Conductance and the rapid mixing pro perty for markov chains: The approxima tion of the permanent resolved. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Comput ing, Chicago, pp.235-244. AC M.\nKaelbling, L.P. (1993). Learning in Embedded Systems. M I T Press.\nKarakoulas, G. (1993). A machine learning approach to planning for economic systems. In Proceedings of the Third International Workshop on Artificial Intelligence in Economics and Management, Portland, Oregon.\nKarakoulas, G. (1995a). Robust reinforcement learning for continuous state and action spaces. In preparation. Karakoulas, G. (1995b). A Q-learning approach to cost effective classification. Technical Report, Knowledge Sys tems Laboratory, National Re search Council Canada.\nKarp, R. M. & Luby, M. (1983). Monte-Carlo algorithms for enumeration and reliability problems. In Proceedings of 24th Annual Symposium on Foundations of Computer Science, pp.56-64, IEEE Computer Society Press. Kemball-Cook, D. (1993). The design of macroeconomic policy under uncertainty. Ph. D. Thesis, London Business School.\nKumar, P.R. (1985). A survey of some results in stochastic adaptive control. SIAM Journal of Control and Optimiza tion, 23, 329-375.\nLin, L. (1991). Self-improvement based on reinforcement learning frameworks. In Proceedings of Eighth Interna tional Workshop on Machine Learning, pp.328-332.\nMahadevan, S., & Connell, J.H. (1993). Robot Learning. Kluwer Academic Publishers.\nPemberton, J., & Korf, R. (1994). Incremental search algorithms for real-time decision-making. In Proceedings of the Second Conference on AI Planning Systems. Russell, S. & Wefald, E. (1991). Do the Right Thing. M I T Press\nScott, P. & Markovitch, S. (1993). Experience selection and problem choice in an exploratory learning system. Machine Learning, 12,49-67.\nSutton, R.S. (1990). Integrated architectures for learning, planning and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning, ML-90. Morgan Kauf mann.\nTe sauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257-277.\nThrun, S. (1992). Efficient exploration in reinforcement learning. Technical Report C M U-CS-92-102, School of Computer Science, Carnegie- Mellon University.\nWatkins, CJ.C.H. (1989). Learning from delayed rewards. Ph. D. thesis, King's College, Cambridge University, UK. Whitehead, S. D. (1991). Complexity and coo peration in Q-learning. Technical Report, Department of Computer Science, University of Rochester."}], "references": [{"title": "Learning and sequential decision-making", "author": ["A.G. Barto", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Barto and Sutton,? \\Q1989\\E", "shortCiteRegEx": "Barto and Sutton", "year": 1989}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Neural network exploration using opti\u00ad mal experiment design", "author": ["D. Cohn"], "venue": "Advances in Neural Informa\u00ad tion Processing Systems 6, Morgan Kaufmann. Dean, T.L. and Wellman, M.P. (1991). Planning and Con\u00ad trol. Morgan Kaufmann.", "citeRegEx": "Cohn,? 1994", "shortCiteRegEx": "Cohn", "year": 1994}, {"title": "Inc reases in risk and in risk aversion", "author": ["J. Stiglitz"], "venue": "Journal of Economic Theory,", "citeRegEx": "P. and Stiglitz,? \\Q1974\\E", "shortCiteRegEx": "P. and Stiglitz", "year": 1974}, {"title": "Probabilistic planning with infonnation gathering and contingent exe\u00ad cution", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": "AAAI", "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "The Sequential Statistical Anal\u00ad ysis", "author": ["Z. Govindarajulu"], "venue": "American Academic Press.", "citeRegEx": "Govindarajulu,? 1987", "shortCiteRegEx": "Govindarajulu", "year": 1987}, {"title": "COMPOSER: A probabi\u00ad listic solution to the utility problem in speed-up learning", "author": ["J. Gratch", "G. DeJong"], "venue": "In Proceedings of AAA/92,", "citeRegEx": "Gratch and DeJong,? \\Q1992\\E", "shortCiteRegEx": "Gratch and DeJong", "year": 1992}, {"title": "Probabilistic Exploration in Planning while Learning", "author": ["J. Gratch", "S. Chien", "G. DeJong"], "venue": "In Proceedings of AAA/94,", "citeRegEx": "Gratch et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Gratch et al\\.", "year": 1994}, {"title": "A machine learning approach", "author": ["L.P. AC M. Kaelbling"], "venue": "Twentieth Annual ACM Symposium on Theory of Comput\u00ad ing,", "citeRegEx": "Kaelbling,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "A Q-learning approach to cost\u00ad effective classification", "author": ["G. Karakoulas"], "venue": "Technical Report, Knowledge Sys\u00ad tems Laboratory, National Re search Council Canada. Karp, R. M. & Luby, M. (1983). Monte-Carlo algorithms for enumeration and reliability problems. In Proceedings", "citeRegEx": "Karakoulas,? 1995b", "shortCiteRegEx": "Karakoulas", "year": 1995}, {"title": "A survey of some results in stochastic adaptive control", "author": ["P.R."], "venue": "SIAM Journal of Control and Optimiza\u00ad tion, 23, 329-375. Lin, L. (1991). Self-improvement based on reinforcement learning frameworks. In Proceedings of Eighth Interna\u00ad", "citeRegEx": "P.R.,? 1985", "shortCiteRegEx": "P.R.", "year": 1985}, {"title": "Robot Learning", "author": ["S. Mahadevan", "J.H. Connell"], "venue": "Workshop on Machine Learning,", "citeRegEx": "Mahadevan and Connell,? \\Q1993\\E", "shortCiteRegEx": "Mahadevan and Connell", "year": 1993}, {"title": "Integrated architectures for learning, planning and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1990\\E", "shortCiteRegEx": "Sutton", "year": 1990}, {"title": "Practical issues in temporal difference learning", "author": ["G. mann. Te sauro"], "venue": "Machine Learning,", "citeRegEx": "sauro,? \\Q1992\\E", "shortCiteRegEx": "sauro", "year": 1992}, {"title": "Efficient exploration in reinforcement learning", "author": ["S. Thrun"], "venue": "Technical Report C M U-CS-92-102, School of Computer Science, Carnegie- Mellon University. Watkins, CJ.C.H. (1989). Learning from delayed rewards. Ph. D. thesis, King's College, Cambridge University, UK.", "citeRegEx": "Thrun,? 1992", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Complexity and coo peration in Q-learning", "author": ["S.D. Whitehead"], "venue": "Technical Report, Department of Computer Science, University of Rochester.", "citeRegEx": "Whitehead,? 1991", "shortCiteRegEx": "Whitehead", "year": 1991}], "referenceMentions": [{"referenceID": 7, "context": ", 1994; Pemberton & Korf, 1994), concept learning (Scott & Markovitch, 1993), speed-up learning (Gratch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994), sys\u00ad tem identification (Cohn, 1994; Dean et al.", "startOffset": 96, "endOffset": 166}, {"referenceID": 2, "context": ", 1994), sys\u00ad tem identification (Cohn, 1994; Dean et al., 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 33, "endOffset": 64}, {"referenceID": 14, "context": ", 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 35, "endOffset": 65}, {"referenceID": 8, "context": ", 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 35, "endOffset": 65}, {"referenceID": 14, "context": "The exploration strategies that have been developed for reinforcement learning are largely of a heuristic nature (Thrun, 1992).", "startOffset": 113, "endOffset": 126}, {"referenceID": 1, "context": "This is mainly due to its origination from the concepts and principles of dynamic programming (Bellman, 1957).", "startOffset": 94, "endOffset": 109}, {"referenceID": 1, "context": "the \"curse of dimen\u00ad sionality\" (Bellman, 1957)).", "startOffset": 32, "endOffset": 47}, {"referenceID": 12, "context": "There has been work on learning state transition models and/or utilizing knowledge generated from the Q-learning process in order to guide exploration (Sutton, 1990; Lin, 1991; Thrun, 1992).", "startOffset": 151, "endOffset": 189}, {"referenceID": 14, "context": "There has been work on learning state transition models and/or utilizing knowledge generated from the Q-learning process in order to guide exploration (Sutton, 1990; Lin, 1991; Thrun, 1992).", "startOffset": 151, "endOffset": 189}, {"referenceID": 5, "context": "Such effi\u00ad ciency can be achieved by incorporating a sequential sta\u00ad tistical procedure (Govindarajulu, 1987) into the hill\u00ad climbing algorithm.", "startOffset": 88, "endOffset": 109}, {"referenceID": 10, "context": "o is dynamically determined for each set of transformations T according to the values of the polic\u00a5,\u00ad improvement operators of the set. The symbol r. 1 denotes the smallest integer greater than or equal to the quantity enclosed. The values of h are given by tables in Dudewicz and Dalal (1975). Specific values for the t .", "startOffset": 115, "endOffset": 294}, {"referenceID": 10, "context": "The results of the two experiments are presented in Fig\u00ad ures 3 and 4. The curves from the learning algorithm with the semi-uniform distribution are depicted with dashed lines. The experiments were run for 30 time-periods. Both learning algorithms converged to the optimal policy a1 = ( -0.69) \u00b7 x1, 1\u2022 This is the same policy that was found by Kemball-Cook (1993). He used a control theory approach for solving this problem.", "startOffset": 25, "endOffset": 365}, {"referenceID": 7, "context": "The explomtion strategies that have been developed for speed-up learning (Gmtch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994) are also based on sequential statistical analysis.", "startOffset": 73, "endOffset": 142}, {"referenceID": 7, "context": "The explomtion strategies that have been developed for speed-up learning (Gmtch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994) are also based on sequential statistical analysis. The selection procedures involved are of only one stage and they are not therefore appropriate for our selection problem which requires a two- or a multi-stage procedure. Kaelbling (1993) has developed a statistical algorithm for exploration in reinforcement learning.", "startOffset": 122, "endOffset": 382}, {"referenceID": 4, "context": "Draper et al. (1994) have developed a probabilistic planning algorithm that perfonns both infonnation-pro\u00ad", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "sification (Karakoulas, 1995b).", "startOffset": 11, "endOffset": 30}], "year": 2011, "abstractText": "Sequential decision tasks with incomplete infor\u00ad mation are characterized by the exploration prob\u00ad lem; namely the trade-off between further exploration for learning more about the environ\u00ad ment and immediate exploitation of the accrued information for decision-making. Within artificial intelligence, there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks. In this paper we focus on the exploration problem in reinforcement learn\u00ad ing and Q-learning in particular. The existing exploration strategies for Q-learning are of a heu\u00ad ristic nature and they exhibit limited scaleability in tasks with large (or infinite) state and action spaces. Efficient experimentation is needed for resolving uncertainties when possible plans are compared (i.e. exploration). The experimenta\u00ad tion should be sufficient for selecting with statis\u00ad tical significance a locally optimal plan (i.e. exploitation). For this purpose, we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is, with arbitrarily high probability, arbi\u00ad trarily close to a locally optimal one. Due to its generality the algorithm can be employed for the exploration strategy of robust Q-learning. An experiment on a relatively complex control task shows that the proposed exploration strategy per\u00ad forms better than a typical exploration strategy. continuous flow of events in time. Effective decision-mak\u00ad ing requires resolution of uncertainty as early as possible. The . te?dency to minimize losses resulting from wrong predictions of future events necessitates the division of the problem solution into steps. A decision at each step must make use of the information from the evolution of the events experienced thus far, but that evolution, in fact, depends on the type of decision made at each step.", "creator": "pdftk 1.41 - www.pdftk.com"}}}