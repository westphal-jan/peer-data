{"id": "1302.6210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2013", "title": "A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting", "abstract": "Improving the robustness and accuracy of prediction models for time series is an active area of research. Recently, artificial neural networks (ANNs) have found extensive applications in many practical prediction problems. However, the standard ANN training algorithm for reverse propagation has some critical problems, such as a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, the lack of suitable selection methods for training parameters, etc. To overcome these drawbacks, various improved training methods have been developed in the literature, but none of them can yet be guaranteed to be the best for all problems. In this essay, we propose a novel weighted ensemble scheme that intelligently combines several training parameters to increase ANN prediction accuracies. The weight for each training algorithm is determined by the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series clearly show that our two unalgorithms are better than our two suggested time series.", "histories": [["v1", "Mon, 25 Feb 2013 20:09:19 GMT  (419kb)", "http://arxiv.org/abs/1302.6210v1", "8 pages, 4 figures, 2 tables, 26 references, international journal"]], "COMMENTS": "8 pages, 4 figures, 2 tables, 26 references, international journal", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ratnadip adhikari", "r k agrawal"], "accepted": false, "id": "1302.6210"}, "pdf": {"name": "1302.6210.pdf", "metadata": {"source": "META", "title": "A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting", "authors": ["Ratnadip Adhikari", "R. K. Agrawal"], "emails": [], "sections": [{"heading": null, "text": "forecasting models is an active area of research. Recently, Artificial Neural Networks (ANNs) have found extensive applications in many practical forecasting problems. However, the standard backpropagation ANN training algorithm has some critical issues, e.g. it has a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, lack of proper training parameters selection methods, etc. To overcome these drawbacks, various improved training methods have been developed in literature; but, still none of them can be guaranteed as the best for all problems. In this paper, we propose a novel weighted ensemble scheme which intelligently combines multiple training algorithms to increase the ANN forecast accuracies. The weight for each training algorithm is determined from the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series depicts that our proposed technique reduces the mentioned shortcomings of individual ANN training algorithms to a great extent. Also it achieves significantly better forecast accuracies than two other popular statistical models.\nGeneral Terms Time Series Forecasting, Artificial Neural Network, Ensemble Technique, Backpropagation.\nKeywords Time Series Forecasting, Artificial Neural Network, Ensemble, Backpropagation, Training Algorithm, ARIMA, Support Vector Machine."}, {"heading": "1. INTRODUCTION", "text": "Analysis and forecasting of time series is of fundamental importance in many practical domains. In time series forecasting, the historical observations are carefully studied to build up a proper model which is then used to forecast unseen future values [1]. Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133]. During the last two decades, Artificial Neural Networks (ANNs) have been widely used as attractive and effective alternative tools for time series modeling and forecasting [2,4]. Originally motivated by the intelligent neural structure of human brains, ANNs have gradually found extensive applications in solving a broad range of nonlinear problems and have drawn increasing attentions of research community. Their most distinguishing feature is the nonlinear, nonparametric, data-driven and self-\nadaptive nature [2,4,5]. ANNs do not require any a priori knowledge of the associated statistical data distribution process. They adaptively construct the appropriate model from only the raw data, learn from training experiences, and then intelligently generalize the acquired knowledge to predict the nature of unseen future events. Due to these outstanding properties, they have become a favorite choice for time series researchers. At present, various ANN-based forecasting techniques exist in literature. Some excellent reviews on recent trends and developments in ANN forecasting methodology can be found in the works of Zhang et al. [4], Kamruzzaman et al. [5] and Adya and Collopy [6].\nThe performance of an ANN model is very sensitive to the proper selection of network architecture, training algorithm, the number of hidden layers, the number of nodes in each layer, the proper activation functions, the significant time lags, etc. [2,4]. The selection of a suitable network training algorithm is perhaps the most critical task in ANN modeling. So far, the classic backpropagation, developed by Rumelhart et al. [7] is the bestknown training method. It updates the network weights and biases in the direction of the most rapid decrease of the error function, i.e. negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7]. Despite its simplicity and popularity, this algorithm suffers from a number of drawbacks, which are listed here [4,5]:\n It has a very slow convergence rate and so requires a lot of computational time for large-scale problems.\n There exists no robust technique for the optimal selection of the corresponding training parameters.\n The error surface of the standard backpropagation algorithm has a very complex pattern.\n Often, the backpropagation algorithm gets stuck at the local minimum solution instead of the desired global one.\nTo get rid of these weaknesses, several improved or modified versions of backpropagation have been proposed in literature. Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms. Recently, Particle Swarm Optimization (PSO) [13,14] has also received considerable attentions in this area; e.g. Jha et al. [14] has effectively used two PSO-based training algorithms (viz. PSO-Trelea1 and PSO-Trelea2) for predicting a financial time series. Although, the modified algorithms have improved the performance of backpropagation training on many occasions [15,16], they could not overcome all its drawbacks.\nFor example, at present no algorithm can unconditionally guarantee a global optimal solution. Moreover, there is no straightforward way of selecting the best training algorithm specific to a particular problem [4].\nIn this paper, we propose a weighted ensemble scheme to combine multiple training algorithms. It is a well-known fact that combining forecasts improves the overall accuracy much better than the individual methods [17]. Seven different backpropagation techniques, which are mentioned above are considered for building our ensemble. The weight assigned to each of them is inversely proportional to the forecast errors obtained by the corresponding ANN on validation dataset. The final forecast of this combined ANN model is calculated as the weighted arithmetic mean of the forecasts obtained from individual training algorithms. To evaluate and compare the performance of our ensemble technique, we consider four real-world time series and two other popular statistical models, viz. Autoregressive Integrated Moving Average (ARIMA) and Support Vector Machine (SVM). The forecast errors of all the models are evaluated in terms of Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE).\nThe rest of the paper is organized as follows. Section 2 describes the ANN methodology for time series forecasting and various network training algorithms. Our proposed ensemble scheme is explained in Section 3. Section 4 describes two popular statistical time series forecasting models. Obtained experimental forecast results and model comparisons are reported in Section 5. Finally, Section 6 concludes our paper."}, {"heading": "2. ARTIFICIAL NEURAL NETWORKS", "text": "The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135]. These are characterized by the feedforward architecture of an input layer, one or more hidden layers, and an output layer. The nodes in each layer are connected to those in the immediate next layer by acyclic links. In practical applications, it is enough to consider a single hidden layer structure [2,4,5].\nThe output of an MLP with p input and h hidden nodes is\nexpressed as [2,4]:\n0 0\n1 1\n.\nph\nt j j ij t i\nj i y G F y      \n           \n  \n  (1)\nHere,  1,2, ,t iy i p   are the network inputs; , ij jk  are the\nconnection weights  1,2, , ; 1,2, ,i p j h   ; 0 0, j  are the\nbias terms, and , F G are respectively the hidden and output layer\nactivation functions. Normally, logistic and identity functions are\nrespectively used for F and ,G i.e. F(x)=1 \u2044 (1+exp(-x)) and G(x)=x. The model, given by the expression (1) is commonly referred as a (p, h, 1) ANN model [14]."}, {"heading": "2.1 Backpropagation Training", "text": "Training is the iterative process for determining optimal network weights and biases. In this phase, the ANN model gradually learns from successive input patterns and target values and accordingly modifies the weights and biases. In a time series forecasting\nproblem with training dataset  1 2, , , ,Ny y y a (p, h, 1) ANN model consists of (N-p) training patterns with input vectors\n1 1, , , T i i i i py y y      Y  and targets ,i py  (i=1, 2,\u2026, N-p). The\nbackpropagation is a supervised training algorithm in which network weights and bias updating is carried out through the minimization of the error function [4,,5]:\n  2\n1\n1 \u02c6 2\nN\nt t\nt p\nE y y\n \n  (2)\nwhere ty is the network output, calculated by Eq. (1) and \u02c6ty is the corresponding target. The algorithm starts with an initial vector w0 of weights and biases which is updated at each step (epoch) i according to the gradient descent rule:\n  1\n1\n.i i i\ni i i\nE  \n            w w w w w w w w\n(3)\nHere, \u03b7 and \u03b1 are the learning rate and momentum factor respectively. The training process continues until some predefined minimum error or maximum number of epochs is reached. The obtained final values of wi are used for all future predictions.\nVarious classes of improved backpropagation technique have been developed in literature. These improvements are done from different perspectives, such as robustness, direction of weights and bias updates, convergence rate, nature of error surface, local and global minima, etc. Here, we briefly discuss seven important network training algorithms, used in literature."}, {"heading": "2.1.1 The RPROP Algorithm", "text": "In the steepest descent method, often the gradient achieves a very small value, causing negligible changes in weights and biases, even though these are actually far from their optimal values. To remove this drawback, the RPROP training algorithm was suggested. It considers only the signs of partial derivatives of the error functions to determine the directions of weight changes [9,18]. The magnitudes of derivatives have no effect on weight updates. This algorithm is very efficient and simple to apply."}, {"heading": "2.1.2 The Conjugate Gradient Algorithms", "text": "Although the steepest descent direction provides the fastest decrease of the performance function but it does not necessarily means fastest convergence. Due to this fact, in conjugate gradient methods, a search is performed in conjugate directions of the\ngradient. An efficient method of this kind is the Scaled Conjugate Gradient (SCG) algorithm, developed by Moller [10]. It is used in this paper as one of the constituent training algorithm."}, {"heading": "2.1.3 The Quasi-Newton Algorithms", "text": "This class of methods uses second order derivatives for weights and bias modifications. The optimum search direction is computed\nthrough  1 ,E H w where H is the Hessian matrix of the error\nfunction. However, due to the expensive computational demand, the Hessian matrix is not calculated directly; rather it is assumed to be a function of the gradient which is iteratively approximated [18]. The most successful and widely applied quasi Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18]. Two others of this kind, used in the present paper are: the Levenberg-Marquardt (LM) [8] and One Step Secant (OSS) [11] algorithms. In particular, LM is so far the fastest training method in terms of convergence rate; however, it requires enormous amount of storage memory and mathematical computations [18]."}, {"heading": "2.1.4 PSO-Based Training Algorithms", "text": "PSO is an evolutionary optimization technique which is originally inspired from the intelligent working paradigm in birds flocks and fish schools [13,14]. From different variations of the basic PSO algorithm, in this paper we use PSO-Trelea1 and PSO-Trelea2, suggested by I. Trelea [19]. Consider a (p, h, q) neural network structure with N particles in the swarm. Each particle represents an individual ANN structure with dimension D=h(p+q+1)+q, the total number of network parameters. The PSO algorithm begins by assigning randomized positions and velocities to each particle. The particles are moved through the D-dimensional search space until some error minimization criterion is satisfied. Every particle evaluates a fitness function (error function in this study) for it. The movements of the particles are governed by two best positions, viz. the personal and global, which are respectively the current personal best fitness of each particle and the current overall best fitness achieved across the whole swarm. Using these two values, the position and velocity for the dth dimension of the ith particle is updated as follows:\n            \n1 2 1 2\n1 2\n1\n1 1 .\n; 2\nid id d id\nid id id\nid gd d\nv t av t b p x t\nx t x t v t\nb p b p b b p b\nb b\n                  \n(4)\nHere, xid, vid, and pid are respectively the position, velocity and personal best position of the ith particle at the dth dimension; pgd is the global best position, obtained at the dth dimension and a, b are two tuning parameters, which have two sets of values found for PSO-Trelea1 (a=0.6, b=1.7) and PSO-Trelea2 (a=0.729, b=1.494). In practice, 24 to 30 swarm particles are considered [14,19]."}, {"heading": "3. PROPOSED ENSEMBLE TECHNIQUE", "text": "Consider a time series  1 2, , , ,NY y y y  which is divided\ninto three subsets, viz. validation, training, and testing. These are used for selecting the best forecasting model, estimating the model parameters and assessing the out-of-sample forecast accuracy of the fitted model, respectively. Let L1, L2,\u2026, Lk are k\n(preferably an odd number) training algorithms to be used after determining the proper ANN structure for the time series. Our ensemble approach is now described below.\nThe determined ANN model is trained with every Li on the training dataset. These k trained ANNs are then used to forecast the validation set and their subsequent forecast performances are measured using three error statistics, defined as:\n \n1\n2\n1\n1\n1 \u02c6Mean Absolute Error (MAE)=\n1 \u02c6Mean Squared Error (MSE)=\n\u02c61 Mean Absolute Percentage Error (MAPE)= 100\nn\nt t\nt n\nt t\nt n\nt t\ntt\ny y n\ny y n\ny y\nn y\n\n\n\n    \n   \n   \n\n\n\nwhere, ty and \u02c6ty are the actual and forecasted observations, respectively and n is the size of the forecasted dataset. Now, the weight for each training algorithm Li is computed as:\n exp , 1\nMAE MSE MAPE\ni i\ni i i i\nw g\ng      \n(5)\nwhere, the terms in the denominator of gi refer to the errors obtained by the ANN model, trained with the training algorithm Li on the validation dataset.\nEquation (5) ensures that the weight assigned to a training algorithm is inversely proportional to its combined error (i.e. the sum of MSE, MAE, MAPE) on the validation set; so, the more error, the less weight and vice versa. The exponential function in weight calculation is used as it will regularize the effect of different magnitudes of the three error statistics.\nAfter calculating the weights, each of the k ANN models are individually trained on the whole in-sample dataset (i.e. training and validation sets combined) and their out-of-sample forecast for the test set is recorded. Let Di be the vector of out-of-sample forecast values obtained by the ANN model, trained with Li; the dimension of Di (i=1, 2,\u2026, k) is equal to the size of the test set. The final forecast vector D produced by our ensemble technique is the weighted arithmetic mean of all the k ANN forecasts, i.e.\n1 1 2 2\n1 2 1 1\n.\nk k\nk k i i i\nk i i\nw w w w w\nw w w                          D D D D D   (6)\nOur proposed technique is a homogeneous ensemble, as it uses the same model (ANN) with different methods (i.e. training algorithms). The necessary steps in the proposed ensemble scheme are outlined below:\nAlgorithm: Ensemble of Multiple ANN Training Methods\n1. Divide the time series into appropriate validation, training, and testing sets. 2. Select k (preferably odd) training algorithms L1, L2,\u2026, Lk. 3. Determine the proper ANN model and network parameters\nusing training and validation datasets.\n4. Train the ANN with each Li to forecast the validation observations. 5. Calculate the weight wi for each Li by Eq. (5).\n6. Train the ANN model with each Li on the whole in-sample dataset (i.e. training and validation sets combined). 7. Use each trained ANN to forecast the testing dataset and record the individual forecast vectors Di (i=1, 2,\u2026, k). 8. Calculate the combined forecasts using Eq. (6)."}, {"heading": "4. TWO STATISTICAL MODELS", "text": "In order to compare the forecast accuracy of our proposed method, two other well-known models, viz. Autoregressive Integrated Moving Average (ARIMA) and Support Vector Machine (SVM) are used in this paper. The present section gives a brief description about these two models."}, {"heading": "4.1 ARIMA Models", "text": "The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4]. These are actually a generalization of the Autoregressive Moving Average (ARMA) models [1] to deal with nonstationary time series. Mathematically, an ARIMA (p, d, q) model can be represented as follows:\n    \n \n \n1\n1\n1\n1\nwhere,\n1 .\n1\nand\nd t t\np i\ni\ni q\nj j\nj\nt t\nL L y L\nL L\nL L\nLy y\n  \n \n \n\n\n\n          \n     \n\n\n(7)\nHere, p, d, q are the orders of the model, which refers to the autoregressive, degree of differencing and moving average processes, respectively; yt is actual time series and \u03b5t is a random noise process; \u03c6(L) and \u03b8(L) are lagged polynomials of orders p, q with coefficients \u03c6i, \u03b8i (i=1, 2,\u2026, p; j=1, 2,\u2026, q), respectively and L is the lag (or backshift) operator. This model transforms a nonstaionary time series to a stationary one by successively (d times) differencing it. Usually, a single differencing is sufficient for most practical time series. The suitable ARIMA model is estimated through the famous Box-Jenkins methodology, which includes three iterative steps, viz. model building, parameter estimation, and diagnostic checking [1,2]. For seasonal time series forecasting, a variation of the basic ARIMA model, commonly known as the SARIMA(p,d,q)\u00d7(P,D,Q)s model (s is the seasonal period) was developed by Box and Jenkins [1], which is also used in this paper."}, {"heading": "4.2 SVM Model", "text": "SVM is a new statistical learning theory, developed by Vapnik and co-workers at the AT & T Bell laboratories in 1995 [21]. It is based on the Structural Risk Minimization (SRM) principle and its aim is to find a decision rule with good generalization ability through selecting some special data points, known as support vectors [21,22]. Time series forecasting is a branch of Support Vector Regression (SVR) in which an optimal separating hyperplane is constructed to correctly classify real-valued outputs.\nGiven a training dataset of N points   1\n, N\ni i i y  x with ,ni x \niy  , SVM attempts to approximate the unknown data\ngeneration function in the following form: f(x)=w\u00b7\u03c6(x)+b, where w is the weight vector, \u03c6 is the nonlinear mapping to a higher dimensional feature space and b is the bias term. Using the Vapnik\u2019s \u03b5-insensitive loss function [21,22], the SVM regression is converted to a Quadratic Programming Problem (QPP) to minimize the empirical risk:\n   2* * 1 1 , , 2\nN\ni i i i\ni\nJ C   \n\n  w w (8)\nwhere, C is the positive regularization constant and i , * i are\nthe positive slack variables. After solving the associated QPP,\nthe optimal decision hyperplane is given by:\n     * opt 1 ,\nsN\ni i i\ni\ny K b \n\n  x x x (9)\nwhere, *,i i  are the Lagrange multipliers (i=1, 2,\u2026, Ns), K(x, xi)\nis the kernel function, Ns is the number of support vectors and bopt is the optimal bias. Usually, a Radial Basis Function (RBF) kernel, given by K(x, y)=exp(\u2013||x\u2013y||2 \u20442\u03c32) (\u03c3 is a tuning\nparameter) is preferred [22,23]. The proper selection of the model\nparameters C and \u03c3 is crucial for effectiveness of SVM. Following\nother works [22,23], grid search and cross validation techniques\nare used in this paper for finding optimal SVM parameters."}, {"heading": "5. EXPERIMENTS AND DISCUSSIONS", "text": "To empirically examine the effectiveness of our proposed ensemble technique, four widely popular real world time series from different domains are used. These are\u2014the Canadian lynx, the Wolf\u2019s sunspot, the monthly international airline passengers, and the monthly Australian sales of red wine time series. All these four datasets are collected from the Time Series Data Library (TSDL) repository [20]. Table 1 gives the necessary descriptions about them and Fig. 2 shows the corresponding time plots.\nThe selection of a proper validation set is crucial for the success of our ensemble scheme. Here, we choose the lengths of the validation datasets in such a way that they approximately match with the lengths of the corresponding test sets for the four time series. All experiments in this paper are implemented through MATLAB. The appropriate ANN structures are determined on the basis of common model selection criteria, as discussed in details by Zhang et al. [4]. The suitable ANN model for each dataset is trained for 2000 epochs with every training algorithm. The PSO toolbox, developed by Birge [24] is used for implementing PSO-Trelea1 and PSO-Trelea2. Following previous studies [14], the number of swarm particles is chosen from the range of 24 to 30.\nsunspots, (c) Airline passengers, (d) Red wine sales. The lynx and sunspots datasets are stationary and exhibit regular patterns. In particular, the sunspot series has a cycle of length approximately 11 years. In this study, the (7, 5, 1) and (4, 4, 1) ANN structures are found to be most suitable for lynx and sunspot series, respectively. Our findings agree with those by other works regarding these two time series [2]. Also, the ARIMA(12, 0, 0) (i.e. AR(12)) model, as employed by Hipel and McLeod is used for the lynx data and the ARIMA(9, 0, 0) (i.e. AR(9)) model [2] is used for the sunspot data. As suggested by Zhang [2], the logarithms (to the base 10) of the lynx data are used in the present analysis.\nThe airline passengers and red wine sales are nonstationary series, having monthly seasonal fluctuations (i.e. s=1 2 ) with upward trends, as can be seen from Fig. 2(c) and 2(d). The seasonality in both series is strong and of multiplicative nature. The airline passenger series has been used by many researchers [1,26] for modeling trend and seasonal effect and now it is considered as a benchmark for seasonal datasets. Box and Jenkins were the first to determine that SARIMA(0,1,1)\u00d7(0,1,1)12 is the best stochastic model for the airline passenger series [1]. The same model is used in this paper for the airline data and incidentally it is found suitable for the red wine dataset too.\nFor ANN modeling of these two series, the Seasonal ANN (SANN) structure, developed by Hamzacebi [26] is considered in this paper. The unique characteristic of this model is that it uses the seasonal component of a time series to determine the number of input and output nodes. Also it can directly track the seasonal effect in the data without removing it. For a seasonal time series with period s, the SANN assumes a (s, h, s) ANN structure, h being the number of hidden nodes. This model is quite simple to understand and implement, yet very efficient in modeling seasonal data, as shown by the research work [26].\n1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nN u\nm b\ne r\no f\nly n\nx t\nra p\np e d\n1 27 53 79 105 131 157 183 209 235 261 287 0\n50\n100\n150\n200\nO b\nse rv\ne d\nn u\nm b\ne r\no f\nsu n\nsp o\nts\n(a)\n(b)\nTo evaluate the forecasting performances of all the fitted models, the two error measures, viz. MSE and MAPE are considered. The ANN model for each time series with every training algorithm is executed 50 times with different initial values for network weights and biases. The final errors are chosen as the best among these 50 runs. The boxplot in Fig. 3 presents a concise diagrammatic depiction of the relative reduction in MAPE (all four time series combined), achieved through our proposed ensemble technique. A similar result is also observed in case of the obtained forecast MSE values.\nThe summary of the obtained error measures for all four datasets is presented in Table 2. In particular, the errors for airline and red wine datasets are given in transformed scales (obtained MSE=original MSE \u00d710\u20134).\nFrom Table 2, it is evident that the forecast errors obtained by our proposed ANN ensemble technique for all four datasets are much less than those obtained by ARIMA and SVM models. These empirical findings strongly support the fact that by combining multiple training algorithms the forecasting accuracy of an ANN model can be significantly improved. In this paper we use the term Forecast Diagram to refer the graph which shows the actual and forecasted observations for a time series.\nThe obtained forecast diagrams for all four datasets are presented in Fig. 4."}, {"heading": "6. CONCLUSIONS", "text": "During the last two decades ANNs have been extensively used for many time series forecasting problems. The wide popularity of ANNs in forecasting community can be credited to their many distinctive and excellent characteristics. However, the standard backpropagation network training method often suffers from a number of inherent drawbacks, such as: the complex pattern of error surfaces, slow convergence rates, getting stuck at local minima, etc. Although various improvements of the basic backpropagation technique have been developed in literature, but none of them could overcome all its shortcomings. Moreover, at present there is no rigorous way to select a best training algorithm specific to a particular problem.\nIn view of these facts, a novel weighted ensemble technique for combining multiple ANN training algorithms is proposed in this paper. The assignment of weights is based on the forecast performance of a training algorithm on the validation dataset. In this paper, seven different training algorithms are used for combining. The experiments conducted on four real world time series suggest that the ANN forecasting accuracies are significantly improved through this ensemble method. Moreover, it is also observed that this combined training algorithm performs much better than each of the individual ones. In future, the effectiveness of our proposed scheme can be further examined for other varieties of time series forecasting problems and with other model combination techniques."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "The first author would like to thank the Council of Scientific and Industrial Research (CSIR) for the obtained financial assistance, which helped a lot while performing this research.\n0 2 4 6 8 10 12 14 2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\n3.6\nN u\nm b\ne r\no f\nly n\nx t\nra p\np e d\nARIMA SVM Ensemble Actual\n(a)"}, {"heading": "8. REFERENCES", "text": "[1] G.E.P. Box, G.M. Jenkins, Time Series Analysis: Forecasting\nand Control, 3rd ed. Holden-Day, California, 1970.\n[2] G.P. Zhang, \u2015Time series forecasting using a hybrid ARIMA and neural network model,\u2016 Neurocomputing 50,\npp.159\u2013175, 2003\n[3] G.P. Zhang, \u2015A neural network ensemble method with jittered training data for time series forecasting,\u2016\nInformation Sciences 177, pp. 5329\u20135346, 2007.\n[4] G. Zhang, B.E. Patuwo, M.Y. Hu, \u2015Forecasting with artificial neural networks: The state of the art,\u2016 International Journal\nof Forecasting 14, pp.35\u201362, 1998.\n[5] J. Kamruzzaman, R. Begg, R. Sarker, Artificial Neural Networks in Finance and Manufacturing, Idea Group\nPublishing, 2006.\n[6] M. Adya, F. Collopy, \u2015How effective are neural networks at forecasting and prediction? A review and evaluation,\u2016\nJournal of Forecasting 17, pp. 481\u2013495, 1998.\n[7] D.E. Rumelhart, G.E. Hinton, R. J. Williams, \u2015Learning representations by back-propagating errors,\u2016 Nature 323\n(6188), pp. 533-536, 1986.\n[8] M. Hagan, M. Menhaj, \"Training feedforward networks with the marquardt algorithm,\" IEEE Transactions on Neural\nNetworks, vol. 5, no. 6, pp. 989\u2013993, November 1994.\n[9] M. Reidmiller, H. Braun, \"A direct adaptive method for faster backpropagation learning: The rprop algorithm,\" In\nProceedings of the IEEE Int. Conference on Neural Networks (ICNN), San Francisco, pp. 586\u2013591, 1993.\n[10] M.F. Moller, \"A scaled conjugate gradient algorithm for fast supervised learning,\" Neural Networks 6, pp. 525\u2013533, 1993.\n[11] R. Battiti, \"One step secant conjugate gradient,\" Neural Computation 4, pp. 141\u2013166, 1992.\n[12] J.E. Dennis, R.B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear Equations,\nEnglewood Cliffs, NJ: Prentice-Hall, 1983.\n[13] J. Kennedy, R.C. Eberhart, Y. Shi, Swarm Intelligence, Morgan Kaufmann, San Francisco, CA, 2001.\n[14] G.K. Jha, P. Thulasiraman, R.K. Thulasiram, \u2015PSO based neural network for time series forecasting,\u2016 In Proceedings of\nthe IEEE International Joint Conference on Neural Networks, Atlanta, Georgia, USA, pp. 1422\u20131427 June 14\u2013 19, 2009.\n[15] R. Fletcher, Practical Methods of Optimization, 2nd ed. John Wiley, Chichester, 1987.\n[16] C. de Groot, D. Wurtz, \u2015Analysis of univariate time series with connectionist nets: a case study of two classical\nexamples,\u2016 Neurocomputing 3, pp. 177\u2013192, 1991.\n[17] J. Scott Armstrong, Combining Forecasts, Principles of Forecasting: A Handbook for Researchers and Practitioners;\nJ. Scott Armstrong (ed.): Norwell, MA: Kluwer Academic Publishers, 2001.\n[18] H. Demuth, M. Beale, M. Hagan, Neural Network Toolbox User's Guide, Natic, MA, the MathWorks, 2010.\n[19] I. Trelea, \"The particle swarm optimization algorithm: convergence analysis and parameter selection,\" Information\nProcessing Letters 85, pp. 317\u2013325, 2003.\n[20] R.J. Hyndman, Time Series Data Library, URL: http://robjhyndman.com/TSDL/, January, 2010.\n[21] V. Vapnik, Statistical Learning Theory, New York, SpringerVerlag, 1995.\n[22] J.A.K. Suykens and J. Vandewalle, \u2015Least squares support vector machines classifiers\u2016, Neural Processing Letters, vol.\n9, no. 3, pp. 293\u2013300, 1999.\n[23] Y. Fan, P. Li, Z. Song, \u2015Dynamic least square support vector machine\u2016, Proceedings of the 6th World Congress on\nIntelligent Control and Automation (WCICA), Dalian, China, pp. 4886-4889, June 21\u201323, 2006.\n[24] Birge, B.: PSOt-A Particle Swarm Optimization Toolbox for use with Matlab, Proceedings of the IEEE Swarm\nIntelligence Symposium, pp. 182-186. Indianapolis, Indiana, USA, 2003.\n[25] K.W. Hipel, A.I. McLeod, Time Series Modelling of Water Resources and Environmental Systems, Amsterdam,\nElsevier, 1994.\n[26] C. Hamzacebi, \u2015Improving artificial neural networks performance in seasonal time series forecasting,\u2016 Information\nSciences 178, pp. 4550\u20134559, 2008."}], "references": [{"title": "Time Series Analysis: Forecasting and Control", "author": ["G.E.P. Box", "G.M. Jenkins"], "venue": "3 ed. Holden-Day, California", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Time series forecasting using a hybrid ARIMA and neural network model,", "author": ["G.P. Zhang"], "venue": "Neurocomputing 50,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A neural network ensemble method with jittered training data for time series forecasting,", "author": ["G.P. Zhang"], "venue": "Information Sciences", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Forecasting with artificial neural networks: The state of the art,", "author": ["G. Zhang", "B.E. Patuwo", "M.Y. Hu"], "venue": "International Journal of Forecasting 14,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Artificial Neural Networks in Finance and Manufacturing", "author": ["J. Kamruzzaman", "R. Begg", "R. Sarker"], "venue": "Idea Group Publishing", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "F", "author": ["M. Adya"], "venue": "Collopy, \u2015How effective are neural networks at forecasting and prediction? A review and evaluation,\u2016 Journal of Forecasting 17, pp. 481\u2013495", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "R", "author": ["D.E. Rumelhart", "G.E. Hinton"], "venue": "J. Williams, \u2015Learning representations by back-propagating errors,\u2016 Nature 323 (6188), pp. 533-536", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Training feedforward networks with the marquardt algorithm,", "author": ["M. Hagan", "M. Menhaj"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "H", "author": ["M. Reidmiller"], "venue": "Braun, \"A direct adaptive method for faster backpropagation learning: The rprop algorithm,\" In Proceedings of the IEEE Int. Conference on Neural Networks (ICNN), San Francisco, pp. 586\u2013591", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning,", "author": ["M.F. Moller"], "venue": "Neural Networks", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "One step secant conjugate gradient,", "author": ["R. Battiti"], "venue": "Neural Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Numerical Methods for Unconstrained Optimization and Nonlinear Equations", "author": ["J.E. Dennis", "R.B. Schnabel"], "venue": "Englewood Cliffs, NJ: Prentice-Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1983}, {"title": "Swarm Intelligence", "author": ["J. Kennedy", "R.C. Eberhart", "Y. Shi"], "venue": "Morgan Kaufmann, San Francisco, CA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "P", "author": ["G.K. Jha"], "venue": "Thulasiraman, R.K. Thulasiram, \u2015PSO based neural network for time series forecasting,\u2016 In Proceedings of the IEEE International Joint Conference on Neural Networks, Atlanta, Georgia, USA, pp. 1422\u20131427 June 14\u2013 19", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Practical Methods of Optimization", "author": ["R. Fletcher"], "venue": "2 ed. John Wiley, Chichester", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Analysis of univariate time series with connectionist nets: a case study of two classical examples,", "author": ["C. de Groot", "D. Wurtz"], "venue": "Neurocomputing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "Combining Forecasts", "author": ["J. Scott Armstrong"], "venue": "Principles of Forecasting: A Handbook for Researchers and Practitioners; J. Scott Armstrong (ed.): Norwell, MA: Kluwer Academic Publishers, 2001.  1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67  0  50  100  150 200 O  b  se  rv  e  d  n  u  m  b  e  r  o  f  su  n  sp  o  ts  ARIMA SVM Ensemble Actual (b) 0  2  4  6  8  10 12  350  400  450  500  550  600  650 N  u  m  b  e  r  o  f  p  a  ss  e  n  g  e  rs  (  '0  0  0  s)  ARIMA SVM Ensemble Actual ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2500}, {"title": "Neural Network Toolbox User's Guide", "author": ["H. Demuth", "M. Beale", "M. Hagan"], "venue": "Natic, MA, the MathWorks", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "The particle swarm optimization algorithm: convergence analysis and parameter selection,", "author": ["I. Trelea"], "venue": "Information Processing Letters", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Time Series Data Library", "author": ["R.J. Hyndman"], "venue": "URL: http://robjhyndman.com/TSDL/, January", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "New York, Springer- Verlag", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Least squares support vector machines classifiers", "author": ["J.A.K. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters, vol. 9, no. 3, pp. 293\u2013300", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Dynamic least square support vector machine", "author": ["Y. Fan", "P. Li", "Z. Song"], "venue": "Proceedings of the 6 World Congress on Intelligent Control and Automation (WCICA), Dalian, China, pp. 4886-4889, June 21\u201323", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "PSOt-A Particle Swarm Optimization Toolbox for use with Matlab", "author": ["B. Birge"], "venue": "Proceedings of the IEEE Swarm Intelligence Symposium,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Time Series Modelling of Water Resources and Environmental Systems", "author": ["K.W. Hipel", "A.I. McLeod"], "venue": "Amsterdam, Elsevier", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Improving artificial neural networks performance in seasonal time series forecasting,", "author": ["C. Hamzacebi"], "venue": "Information Sciences", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "In time series forecasting, the historical observations are carefully studied to build up a proper model which is then used to forecast unseen future values [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 0, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 1, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 1, "context": "During the last two decades, Artificial Neural Networks (ANNs) have been widely used as attractive and effective alternative tools for time series modeling and forecasting [2,4].", "startOffset": 172, "endOffset": 177}, {"referenceID": 3, "context": "During the last two decades, Artificial Neural Networks (ANNs) have been widely used as attractive and effective alternative tools for time series modeling and forecasting [2,4].", "startOffset": 172, "endOffset": 177}, {"referenceID": 1, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 4, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "[4], Kamruzzaman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and Adya and Collopy [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] and Adya and Collopy [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "[2,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[2,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[7] is the bestknown training method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 4, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 6, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "Despite its simplicity and popularity, this algorithm suffers from a number of drawbacks, which are listed here [4,5]:", "startOffset": 112, "endOffset": 117}, {"referenceID": 4, "context": "Despite its simplicity and popularity, this algorithm suffers from a number of drawbacks, which are listed here [4,5]:", "startOffset": 112, "endOffset": 117}, {"referenceID": 7, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 223, "endOffset": 227}, {"referenceID": 12, "context": "Recently, Particle Swarm Optimization (PSO) [13,14] has also received considerable attentions in this area; e.", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "Recently, Particle Swarm Optimization (PSO) [13,14] has also received considerable attentions in this area; e.", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "[14] has effectively used two PSO-based training algorithms (viz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Although, the modified algorithms have improved the performance of backpropagation training on many occasions [15,16], they could not overcome all its drawbacks.", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "Although, the modified algorithms have improved the performance of backpropagation training on many occasions [15,16], they could not overcome all its drawbacks.", "startOffset": 110, "endOffset": 117}, {"referenceID": 3, "context": "Moreover, there is no straightforward way of selecting the best training algorithm specific to a particular problem [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 16, "context": "It is a well-known fact that combining forecasts improves the overall accuracy much better than the individual methods [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 2, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 3, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 4, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 1, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 3, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 1, "context": "The output of an MLP with p input and h hidden nodes is expressed as [2,4]:", "startOffset": 69, "endOffset": 74}, {"referenceID": 3, "context": "The output of an MLP with p input and h hidden nodes is expressed as [2,4]:", "startOffset": 69, "endOffset": 74}, {"referenceID": 13, "context": "The model, given by the expression (1) is commonly referred as a (p, h, 1) ANN model [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "The backpropagation is a supervised training algorithm in which network weights and bias updating is carried out through the minimization of the error function [4,,5]:", "startOffset": 160, "endOffset": 166}, {"referenceID": 4, "context": "The backpropagation is a supervised training algorithm in which network weights and bias updating is carried out through the minimization of the error function [4,,5]:", "startOffset": 160, "endOffset": 166}, {"referenceID": 8, "context": "It considers only the signs of partial derivatives of the error functions to determine the directions of weight changes [9,18].", "startOffset": 120, "endOffset": 126}, {"referenceID": 17, "context": "It considers only the signs of partial derivatives of the error functions to determine the directions of weight changes [9,18].", "startOffset": 120, "endOffset": 126}, {"referenceID": 9, "context": "An efficient method of this kind is the Scaled Conjugate Gradient (SCG) algorithm, developed by Moller [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "However, due to the expensive computational demand, the Hessian matrix is not calculated directly; rather it is assumed to be a function of the gradient which is iteratively approximated [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The most successful and widely applied quasi Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 17, "context": "The most successful and widely applied quasi Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 7, "context": "Two others of this kind, used in the present paper are: the Levenberg-Marquardt (LM) [8] and One Step Secant (OSS) [11] algorithms.", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "Two others of this kind, used in the present paper are: the Levenberg-Marquardt (LM) [8] and One Step Secant (OSS) [11] algorithms.", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "In particular, LM is so far the fastest training method in terms of convergence rate; however, it requires enormous amount of storage memory and mathematical computations [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "4 PSO-Based Training Algorithms PSO is an evolutionary optimization technique which is originally inspired from the intelligent working paradigm in birds flocks and fish schools [13,14].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "4 PSO-Based Training Algorithms PSO is an evolutionary optimization technique which is originally inspired from the intelligent working paradigm in birds flocks and fish schools [13,14].", "startOffset": 178, "endOffset": 185}, {"referenceID": 18, "context": "Trelea [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "In practice, 24 to 30 swarm particles are considered [14,19].", "startOffset": 53, "endOffset": 60}, {"referenceID": 18, "context": "In practice, 24 to 30 swarm particles are considered [14,19].", "startOffset": 53, "endOffset": 60}, {"referenceID": 0, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 1, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 3, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 0, "context": "These are actually a generalization of the Autoregressive Moving Average (ARMA) models [1] to deal with nonstationary time series.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "model building, parameter estimation, and diagnostic checking [1,2].", "startOffset": 62, "endOffset": 67}, {"referenceID": 1, "context": "model building, parameter estimation, and diagnostic checking [1,2].", "startOffset": 62, "endOffset": 67}, {"referenceID": 0, "context": "For seasonal time series forecasting, a variation of the basic ARIMA model, commonly known as the SARIMA(p,d,q)\u00d7(P,D,Q) model (s is the seasonal period) was developed by Box and Jenkins [1], which is also used in this paper.", "startOffset": 186, "endOffset": 189}, {"referenceID": 20, "context": "2 SVM Model SVM is a new statistical learning theory, developed by Vapnik and co-workers at the AT & T Bell laboratories in 1995 [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "It is based on the Structural Risk Minimization (SRM) principle and its aim is to find a decision rule with good generalization ability through selecting some special data points, known as support vectors [21,22].", "startOffset": 205, "endOffset": 212}, {"referenceID": 21, "context": "It is based on the Structural Risk Minimization (SRM) principle and its aim is to find a decision rule with good generalization ability through selecting some special data points, known as support vectors [21,22].", "startOffset": 205, "endOffset": 212}, {"referenceID": 20, "context": "Using the Vapnik\u2019s \u03b5-insensitive loss function [21,22], the SVM regression is converted to a Quadratic Programming Problem (QPP) to minimize the empirical risk:", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "Using the Vapnik\u2019s \u03b5-insensitive loss function [21,22], the SVM regression is converted to a Quadratic Programming Problem (QPP) to minimize the empirical risk:", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "Usually, a Radial Basis Function (RBF) kernel, given by K(x, y)=exp(\u2013||x\u2013y|| \u20442\u03c3) (\u03c3 is a tuning parameter) is preferred [22,23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 22, "context": "Usually, a Radial Basis Function (RBF) kernel, given by K(x, y)=exp(\u2013||x\u2013y|| \u20442\u03c3) (\u03c3 is a tuning parameter) is preferred [22,23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 21, "context": "Following other works [22,23], grid search and cross validation techniques are used in this paper for finding optimal SVM parameters.", "startOffset": 22, "endOffset": 29}, {"referenceID": 22, "context": "Following other works [22,23], grid search and cross validation techniques are used in this paper for finding optimal SVM parameters.", "startOffset": 22, "endOffset": 29}, {"referenceID": 19, "context": "All these four datasets are collected from the Time Series Data Library (TSDL) repository [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The PSO toolbox, developed by Birge [24] is used for implementing PSO-Trelea1 and PSO-Trelea2.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Following previous studies [14], the number of swarm particles is chosen from the range of 24 to 30.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Our findings agree with those by other works regarding these two time series [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "AR(9)) model [2] is used for the sunspot data.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "As suggested by Zhang [2], the logarithms (to the base 10) of the lynx data are used in the present analysis.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The airline passenger series has been used by many researchers [1,26] for modeling trend and seasonal effect and now it is considered as a benchmark for seasonal datasets.", "startOffset": 63, "endOffset": 69}, {"referenceID": 25, "context": "The airline passenger series has been used by many researchers [1,26] for modeling trend and seasonal effect and now it is considered as a benchmark for seasonal datasets.", "startOffset": 63, "endOffset": 69}, {"referenceID": 0, "context": "Box and Jenkins were the first to determine that SARIMA(0,1,1)\u00d7(0,1,1) is the best stochastic model for the airline passenger series [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 25, "context": "For ANN modeling of these two series, the Seasonal ANN (SANN) structure, developed by Hamzacebi [26] is considered in this paper.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "This model is quite simple to understand and implement, yet very efficient in modeling seasonal data, as shown by the research work [26].", "startOffset": 132, "endOffset": 136}], "year": 2011, "abstractText": "Enhancing the robustness and accuracy of time series forecasting models is an active area of research. Recently, Artificial Neural Networks (ANNs) have found extensive applications in many practical forecasting problems. However, the standard backpropagation ANN training algorithm has some critical issues, e.g. it has a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, lack of proper training parameters selection methods, etc. To overcome these drawbacks, various improved training methods have been developed in literature; but, still none of them can be guaranteed as the best for all problems. In this paper, we propose a novel weighted ensemble scheme which intelligently combines multiple training algorithms to increase the ANN forecast accuracies. The weight for each training algorithm is determined from the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series depicts that our proposed technique reduces the mentioned shortcomings of individual ANN training algorithms to a great extent. Also it achieves significantly better forecast accuracies than two other popular statistical models. General Terms Time Series Forecasting, Artificial Neural Network, Ensemble Technique, Backpropagation.", "creator": "Microsoft\u00ae Office Word 2007"}}}