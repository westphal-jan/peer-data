{"id": "1303.0339", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2013", "title": "Learning Hash Functions Using Column Generation", "abstract": "Recently, a number of approaches have been developed to learn data-dependent hash functions. In this thesis, we propose a method based on column generation to learn data-dependent hash functions based on proximity comparison information. Considering a number of triplets encoding the paired proximity comparison information, our method learns hash functions that maintain the relative comparison relationships in the data as well as possible within the large margin learning framework. The learning method is implemented using column generation and therefore is called CGHash. Each iteration of the column generation method selects the best hash function. Unlike most other hash methods, our method generalizes naturally to new data points and has a training goal that is convex, ensuring that the global optimum can be identified. Experiments show that the proposed method is compact when tested with the cobinary technology and that its performance is tested with the cobinary.", "histories": [["v1", "Sat, 2 Mar 2013 03:01:46 GMT  (1602kb,D)", "http://arxiv.org/abs/1303.0339v1", "9 pages, published in International Conf. Machine Learning, 2013"]], "COMMENTS": "9 pages, published in International Conf. Machine Learning, 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xi li", "guosheng lin", "chunhua shen", "anton van den hengel", "anthony r dick"], "accepted": true, "id": "1303.0339"}, "pdf": {"name": "1303.0339.pdf", "metadata": {"source": "META", "title": "Learning Hash Functions Using Column Generation", "authors": ["Xi Li", "Guosheng Lin", "Chunhua Shen", "Anton van den Hengel"], "emails": ["xi.li03@adelaide.edu.au", "guosheng.lin@adelaide.edu.au", "chunhua.shen@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au", "anthony.dick@adelaide.edu.au"], "sections": [{"heading": null, "text": "Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning datadependent hash functions have been developed. In this work, we propose a column generation based method for learning datadependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.\n* indicates equal contributions.\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s)."}, {"heading": "1. Introduction", "text": "The explosive growth in the volume of data to be processed in applications such as web search and multimedia retrieval increasingly demands fast similarity search and efficient data indexing/storage techniques. Considerable effort has been spent on designing hashing methods which address both the issues of fast similarity search and efficient data storage (for example, (Andoni & Indyk, 2006; Weiss et al., 2008; Zhang et al., 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)). A hashing-based approach constructs a set of hash functions that map highdimensional data samples to low-dimensional binary codes. These binary codes can be easily loaded into the memory in order to allow rapid retrieval of data samples. Moreover, the pairwise Hamming distance between these binary codes can be efficiently computed by using bit operations, which are well supported by modern processors, thus enabling efficient similarity calculation on large-scale datasets. Hash-based approaches have thus found a wide range of applications, including object recognition (Torralba et al., 2008), information retrieval (Zhang et al., 2010b), local descriptor compression (Strecha et al., 2011), image matching (Korman & Avidan, 2011), and many more. Recently a number of effective hashing methods have been developed which construct a variety of hash functions, mainly on the assumption that semantically similar data samples should have similar binary codes, such as random projection-based locality sensitive hashing (LSH) (Andoni & Indyk, 2006), boosting learning-based similarity sensitive coding (SSC) (Shakhnarovich et al., 2003), and spectral hashing of Weiss et al. (2008) which is inspired by Laplacian eigenmap.\nIn more detail, spectral hashing (Weiss et al., 2008) optimizes a graph Laplacian based objective function such that in the learned low-dimensional binary\nar X\niv :1\n30 3.\n03 39\nv1 [\ncs .L\nG ]\n2 M\nar 2\n01 3\nspace, the local neighborhood structure of the original dataset is best preserved. SSC (Shakhnarovich et al., 2003) makes use of boosting to adaptively learn an embedding of the original space, represented by a set of weak learners or hash functions. This embedding aims to preserve the pairwise affinity relationships of training duplets (i.e., pairs of samples in the original space). These approaches have demonstrated that, in general, data-dependent hashing is superior to data-independent hashing with a typical example being LSH (Andoni & Indyk, 2006).\nFollowing this vein, here we learn hash functions using side information that is generally presented in a set of triplet-based constraints. Note that the triples used for training can be generated in an either supervised or unsupervised fashion. The fundamental idea is to learn optimal hash functions such that, when using the learned weighted Hamming distance, the relative distance comparisons of the form \u201cpoint x is closer to x+ than to x\u2212\u201d are satisfied as well as possible (x+ and x\u2212 are respectively relevant and irrelevant samples to x). This type of relative proximity comparisons have been successfully applied to learn quadratic distance metrics (Schultz & Joachims, 2004; Shen et al., 2012). Usually this type of proximity relationships do not require explicit class labels and thus are easier to obtain than either the class labels or the actual distances between data points. For instance, in content based image retrieval, to collect feedback, users may be required to report whether image x looks more similar to x+ than it is to a third image x\u2212. This task is typically much easier than to label each individual image. Formally, we are given a set C = {(xi,x+i ,x \u2212 i )|d(xi,x + i ) < d(xi,x \u2212 i )}, i = 1, 2, \u00b7 \u00b7 \u00b7 , where d(\u00b7, \u00b7) is some similarity measure (e.g., Euclidean distance in the original space; or semantic similarity measure provided by a user). As explained, one may not explicitly know d(\u00b7, \u00b7); instead, one may only be able to provide sparse proximity relationships. Using such a set of constraints, we formulate a learning problem in the large-margin framework. By using a convex surrogate loss function, a convex optimization problem is obtained, but has an exponentially large number of variables. Column generation is thus employed to efficiently and optimally solve the formulated optimization problem.\nThe main contribution of this work is to propose a novel hash function learning framework which has the following desirable properties. (i) The formulated optimization problem can be globally optimized. We show that column generation can be used to iteratively find the optimal hash functions. The weights of all the selected hash functions for calculating the weighted\nHamming distance are updated at each iteration. (ii) The proposed framework is flexible and can accommodate various types of constraints. We show how to learn hash functions based on proximity comparisons. Furthermore, the framework can accommodate different types of loss functions as well as regularization terms. Also, our hashing framework can use different types of hash functions such as linear functions, decision stumps/trees, RBF kernel functions, etc.\nRelated work Loosely speaking, hashing methods may be categorized into two groups: data-independent and data-dependent. Without using any training data, data-independent hashing methods usually generate a set of hash functions using randomization. For instance, LSH of Andoni & Indyk (2006) use random projection and thresholding to generate binary codes in the Hamming space, where the mutually close data samples in the Euclidean space are likely to have similar binary codes. Recently, Kulis & Grauman (2009) propose a kernelized version of LSH, which is capable of capturing the intrinsic relationships between data samples using kernels instead of linear inner products. In terms of learning methodology, datadependent hashing methods can make use of unsupervised, supervised or semi-supervised learning techniques to learn a set of hash functions that generate the compact binary codes. As for unsupervised learning, two typical approaches are used to obtain such compact binary codes, including thresholding the real-valued low-dimensional vectors (after dimensionality reduction) and direct optimization of a Hamming distance based objective function (e.g., spectral hashing (Weiss et al., 2008), self-taught hashing (Zhang et al., 2010b)). The spectral hashing (SPH) method directly optimizes a graph Laplacian objective function in the Hamming space. Inspired by SPH, Zhang et al. (2010b) developed the self-taught hashing (STH) method. At the first step of STH, Laplacian graph embedding is used to generate a sequence of binary codes for each sample. By viewing these binary codes as binary classification labels, a set of hash functions are obtained by training a set of bit-specific linear support vector machines. Liu et al. (2011) proposed a scalable graph-based hashing method which uses a small-size anchor graph to approximate the original neighborhood graph and alleviates the computational limitation of spectral hashing.\nAs for the supervised learning case, a number of hashing methods take advantage of labeled training samples to build data-dependent hash functions. These hashing methods often formulate hash function learning as a classification problem. For example, Salakhutdinov & Hinton (2009) proposed the restricted Boltz-\nmann machine (RBM) hashing method using a multilayer deep learning technique for binary code generation. Strecha et al. (2011) use Fisher linear discriminant analysis (LDA) to embed the original data samples into a lower-dimensional space, where the embedded data samples are binarized using thresholding. Boosting methods have also been employed to develop hashing methods such as SSC (Shakhnarovich et al., 2003) and Forgiving Hash (Baluja & Covell, 2008), both of which learn a set of weak learners as hash functions in the boosting framework. It is demonstrated in (Torralba et al., 2008) that some datadependent hashing methods like stacked RBM and boosting SSC perform much better than LSH on largescale databases of millions of images. Wang et al. (2012) proposed a semi-supervised hashing method, which aims to ensure the smoothness of similar data samples and the separability of dissimilar data samples. More recently, Liu et al. (2012) introduced a kernel-based supervised hashing method, where the hashing functions are nonlinear kernel functions.\nThe closest work to ours might be boosting based SSC hashing (Shakhnarovich et al., 2003), which also learns a set of weighted hash functions through boosting learning. Ours differs SSC in the learning procedure. The resulting optimization problem of our CGHash is based on the concept of margin maximization. We have derived a meaningful Lagrange dual problem such that column generation can be applied to solve the semi-infinite optimization problem. In contrast, SSC is built on the learning procedure of AdaBoost, which employs stage-wise coordinate-descent optimization. The weights associated with selected hash functions (corresponding weak classifiers in AdaBoost) are not fully updated at each iteration. Also the information used for training is different. We have used distance comparison information and SSC uses pairwise information. In addition, our work can accommodate various types of constraints, and can flexibly adapt to different types of loss functions as well as regularization terms. It is unclear, for example, how SSC can accommodate different types regularization that may encode useful prior information. In this sense our CGHash is much more flexible. Next, we present our main results."}, {"heading": "2. The proposed algorithm", "text": "Given a set of training samples xm \u2208 RD, (m = 1, 2, . . . ), we aim to learn a set of hash functions hj(x) \u2208 H, j = 1, 2, . . . `, for mapping these training samples to a low-dimensional binary space, being described by a set of binary codewords bm, (m = 1, 2, . . . ). Here each bm is an `-dimensional binary vec-\ntors. In the low-dimensional binary space, the codewords bm\u2019s are supposed to preserve the underlying proximity information of corresponding xi\u2019s in the original high-dimensional space. Next we learn such hash functions {hj(x)}`j=1 within the large-margin learning framework.\nFormally, suppose that we are given a set of triplets {(xi,x+i ,x \u2212 i )} |I| i=1 with xi,x + i ,x \u2212 i \u2208 RD and I being the triplet index set. These triplets encode the proximity comparison information such that the distance/dissimilarity between xi and x + i is smaller than that between xi and x \u2212 i . Now we need to define the weighted Hamming distance for the learned binary codes: dH(x, z) = \u2211` j=1 wj |hj(x)\u2212hj(z)|, where wj is a non-negative weight factor associated with the j-th hash function. In our experiments, we have generated the triplets set as: xi and x + i belong to the same class and xi and x \u2212 i belong to different classes. As discussed, these triplets may be sparsely provided by users in applications such as image retrieval. So we want the constraints dH(xi,x + i ) < dH(xi,x \u2212 i ) to be satisfied as well as possible. For notational simplicity, we define a [i] j = |hj(xi) \u2212 hj(x \u2212 i )| \u2212 |hj(xi) \u2212 hj(x + i )| and dH(xi,x \u2212 i )\u2212 dH(xi,x + i ) = w >ai with\nai = [a [i] 1 , a [i] 2 , . . . , a [i] ` ] >. (1)\nIn what follows, we describe the details of our hashing algorithm using different types of convex loss functions and regularization norms. In theory, any convex loss and regularization can be used in our hashing framework. More details of our hashing algorithm can be found in Algorithm 1 and the supplementary file (Li et al., 2013)."}, {"heading": "2.1. Learning hashing functions with the hinge loss", "text": "Hashing with l1 norm regularization Using the hinge loss, we define the following large-margin optimization problem:\nmin w,\u03be\n\u2211|I| i=1 \u03bei + C\u2016w\u20161\ns.t. w < 0, \u03be < 0; dH(xi,x \u2212 i )\u2212 dH(xi,x + i ) \u2265 1\u2212 \u03bei, \u2200i,\n(2)\nwhere \u2016\u00b7\u20161 is the 1-norm, w = (w1, w2, . . . , w`)> is the weight vector; \u03be is the slack variable; C is a parameter controlling the trade-off between the training error and model capacity, and the symbol \u2018<\u2019 indicates elementwise inequalities. The optimization problem (2) can be rewritten as:\nmin w,\u03be\n\u2211|I| i=1 \u03bei + C1 >w\ns.t. w < 0; a>i w \u2265 1\u2212 \u03bei, \u03bei \u2265 0, \u2200i, (3)\nwhere 1 is the all-one column vector. The correspond-\nAlgorithm 1 Hashing using column generation Input: Training triplets {(xi,x+i ,x \u2212 i )}, i = 1, 2 \u00b7 \u00b7 \u00b7 and `, the number of hash functions. Output: Learned hash functions {hj(x)}`j=1 and the associated weights w. Initialize: u\u2190 1|I| . for j = 1 to ` do\n1. Find the best hash function hj(\u00b7) by solving the subproblem (11). 2. Add hj(\u00b7) to the hash function set; 3. Update ai, \u2200i as in (1); 4. Solve the primal problem for w (using LBFGS-B\n(Zhu et al., 1997)) and obtain the dual variable u using KKT condition (10).\nendfor\ning dual problem is:\nmax u\n1>u, s.t. Au 4 C1, 0 4 u 4 1, (4)\nwhere the matrix A = (a1,a2, . . . ,a|I|) \u2208 R`\u00d7|I| and the symbol \u20184\u2019 indicates element-wise inequalities.\nHashing with l\u221e norm regularization The primal problem is formulated as:\nmin w,\u03be\n\u2211|I| i=1 \u03bei + C\u2016w\u2016\u221e\ns.t. w < 0,a>i w \u2265 1\u2212 \u03bei, \u03bei \u2265 0,\u2200i. We can make the l\u221e regularization a constraint,\nmin w,\u03be\n\u2211|I| i=1 \u03bei\ns.t. w < 0, \u2016w\u2016\u221e \u2264 C \u2032;a>i w \u2265 1\u2212 \u03bei, \u03bei \u2265 0, \u2200i. (5)\nThe dual form of the above optimization problem is:\nmin u,q \u22121>u + C \u20321>q s.t. Au 4 q, 0 4 u 4 1,\n(6)\nwhere C \u2032 is a positive constant."}, {"heading": "2.2. Hashing with a general convex loss function", "text": "Here we derive the algorithm for learning hash functions with general convex loss. We assume that the general convex loss function f(\u00b7) is smooth (exponential, logistic, squared hinge loss etc.) although our algorithm can be easily extended to non-smooth loss functions.\nHashing with l1 norm regularization Assume that we want to find a set of hash functions such that the set of constraints dH(xi,x \u2212 i ) \u2212 dH(xi,x + i ) = w\n>ai > 0, i = 1, 2 . . . hold as well as possible. These constraints do not have to be all strictly satisfied. Now, we need to define the margin \u03c1i = w\n>ai, and we want to maximize the margin with regularization. Using l1\nnorm regularization to control the capacity, we may define the primal optimization problem as:\nmin w,\u03c1 |I|\u2211 i=1 f(\u03c1i) +C\u2016w\u20161, s.t. w < 0; \u03c1i = a>i w, \u2200i. (7) Here f(\u00b7) is a smooth convex loss function; w = [w1, w2, . . . , w`]\n> is the weight vector that we are interested in optimizing. C is a parameter controlling the trade-off between the training error and model capacity.\nAlso without this regularization, one can always make w arbitrarily large to make the convex loss approach zero when all constraints are satisfied. Here because the possibility of hash functions can be extremely large or even infinite, we are not able to directly solve the problem (7). We can use the column generation technique to iteratively and approximately solve the original problem. Column generation is a technique originally used for large scale linear programming problems. Demiriz et al. (2002) used this method to design boosting algorithms. At each iteration, one column\u2014 a variable in the primal or a constraint in the dual problem\u2014is added when solving the restricted problem. Till one can not find any column violating the constraint in the dual, the solution of the restricted problem is identical to the optimal solution. Here we only need to obtain an approximate solution and in order to learn compact codes, we only care about the first few (e.g, 60) selected hash functions. In theory, if we run the column generation with a sufficient number of iterations, one can obtain a sufficiently accurate solution (up to a preset precision or no more hash functions can be found to improve the solution).\nWe need to derive a meaningful Lagrange dual in order to use column generation. The Lagrangian is:\nL = |I|\u2211 i=1 f(\u03c1i) + C1 >w \u2212 p>w + |I|\u2211 i=1 ui(a > i w \u2212 \u03c1i)\n= C1>w \u2212 p>w + |I|\u2211 i=1 ui(a > i w)\u2212 (u>\u03c1\u2212 |I|\u2211 i=1 f(\u03c1i)),\nwhere p < 0 and u are Lagrange multipliers. With the definition of Fenchel conjugate (Boyd & Vandenberghe, 2004), we have the following relation:\ninf \u03c1 L = \u2212sup \u03c1 (u>\u03c1\u2212\n\u2211|I| i=1f(\u03c1i)) = \u2212 \u2211|I| i=1f \u2217(ui) and\nin order to have a finite infimum, C1 \u2212 p + Au = 0 must hold. So we have p < 0, Au < \u2212C1. Here the matrix A is defined in (4).\nConsequently, the corresponding dual problem of (7) can be written as:\nmin u\n\u2211|I| i=1 f \u2217(ui), s.t. Au < \u2212C1. (8)\nHere f\u2217(\u00b7) is the Fenchel conjugate of f(\u00b7). By reversing the sign of u, we can reformulate (8) as its equivalent form:\nmin u\n\u2211|I| i=1f\n\u2217(\u2212ui), s.t. Au 4 C1. (9) Since we assume that f(\u00b7) is smooth, the KarushKuhn-Tucker (KKT) condition establishes the connection between (9) and (7) at optimality: u?i = \u2212f \u2032(\u03c1?i ),\u2200i. (10) In other words, the dual variable is determined by the gradient of the loss function in the primal. So if we solve the primal problem (7), from the primal solution w?, we can calculate the dual solution u? using (10). But the other way around may not be true.\nThe core idea of column generation is to generate a small subset of variables, each of which is sequentially found by selecting the most violated dual constraints in the dual optimization problem (9). This process is equivalent to inserting several primal variables into the primal optimization problem (7). Here, the subproblem for generating the most violated dual constraint (i.e., to find the best hash function) can be defined as:\nh?(\u00b7) = arg max h(\u00b7)\u2208H \u2211|I| i=1 uia [i]\n= arg max h(\u00b7)\u2208H\n\u2211|I| i=1 ui(|h(xi)\u2212 h(x\u2212i )| \u2212 |h(xi)\u2212 h(x + i )|).\n(11)\nIn order to obtain a smoothly differentiable objective function, we reformulate (11) into the following equivalent form:\nargmax h(\u00b7)\u2208H |I|\u2211 i=1 ui[(h(xi)\u2212 h(x\u2212i )) 2 \u2212 (h(xi)\u2212 h(x+i )) 2].\n(12)\nThe equivalence between (11) and (12) can be trivially established.\nTo globally solve the optimization problem (12) is in general difficult. In the case of decision stumps as hash functions, we can usually exhaustively enumerate all the possibilities and find the globally best one. In the case of linear perception as hash functions, h(x) takes the form of sgn(v>x+b) where sgn(\u00b7) is the sign function. As a result, the binary hash codes are easily computed by (1+h(x))/2. In practice, we relax h(x) = sgn(v>x + b) to h(x) = tanh(v>x + b) with tanh(\u00b7) being the hyperbolic tangent function. For notional simplicity, let \u03c4i+ and \u03c4i\u2212 denote tanh(v\n>xi + b) \u2212 tanh(v>x+i +b) and tanh(v\n>xi+b)\u2212tanh(v>x\u2212i +b), respectively. Then we have the following optimization\nproblem:\nh?(\u00b7) =\narg max h(\u00b7)\u2208H |I|\u2211 i=1 ui[(h(xi)\u2212 h(x\u2212i )) 2 \u2212 (h(xi)\u2212 h(x+i )) 2]\n= arg max v,b |I|\u2211 i=1 ui(\u03c4 2 i\u2212 \u2212 \u03c42i+). (13) The above optimization problem can be efficiently solved by using LBFGS (Zhu et al., 1997) after feature normalization. The initialization of LBFGS can be guided by LSH (Andoni & Indyk, 2006). Namely, we first generate a set of candidate samples such that v \u223c N(0, 1) and b \u223c U(\u22121, 1) with N(\u00b7) and U(\u00b7) respectively being the normal and uniform distributions. Then, we use the best candidate sample as the initialization that maximizes the objective function (13). In our experiments, we have used linear perception as hash functions.\nHashing with l\u221e norm regularization We show here that we can also use other regularization terms such as the l\u221e norm. With the l\u221e norm regularization, the primal problem is defined as:\nmin w,\u03c1\n\u2211|I| i=1 f(\u03c1i) + C\u2016w\u2016\u221e, s.t. w < 0; \u03c1i = a > i w, \u2200i.\n(14) This optimization problem is equivalent to:\nmin w,\u03c1\n\u2211|I| i=1f(\u03c1i), s.t. \u2016w\u2016\u221e \u2264 C \u2032;w < 0; \u03c1i = a > i w,\u2200i,\n(15) where C \u2032 is a properly selected constant, related to C in (14). Due to w < 0 and \u2016w\u2016\u221e \u2264 C \u2032, we obtain 0 4 w 4 C \u20321. Therefore, the Lagrangian can be written as:\nL = |I|\u2211 i=1 f(\u03c1i)+q >w\u2212C \u2032q>1\u2212p>w+ |I|\u2211 i=1 ui(a > i w\u2212\u03c1i), where p, q, u are Lagrange multipliers. Similar to the `1 norm case, we can easily derive the dual problem as:\nmin u,q\n\u2211|I| i=1f \u2217(ui) + C \u20321>q, s.t. Au < \u2212q. (16)\nBy reversing the sign of u, we can reformulate (16) as its equivalent form:\nmin u,q\n\u2211|I| i=1f \u2217(\u2212ui) + C \u20321>q, s.t. Au 4 q. (17)\nThe KKT condition in this l\u221e regularized case is the same as (10). Also the rule to generate the best hash function (i.e., the most violated constraint in (17)) remains the same as in the l1 norm case that we have discussed. Note that both the primal problems (7) and (15) can be efficiently solved using quasi-Newton methods such as L-BFGS-B (Zhu et al., 1997) by eliminating the auxiliary variable \u03c1.\nExtension To demonstrate the flexibility of the proposed framework, we show an example that considers an addition pairwise information. Assume that we have information about a set of duplets that they are neighbors to each other or they are from the same class. So the distance between these duplets should be minimized. We can easily include such a term in our objective function. Formally, let us denote the duplet set as D = {(xk,x+k )} and we want to minimize the divergence \u2211|D| k=1 dH(xk,x + k ) =\u2211\nj wj( \u2211 k |hj(xk) \u2212 hj(x + k )|) = \u2211 j sjwj with sj =\u2211|D|\nk=1 |hj(xk) \u2212hj(x + k )| being a nonnegative constant given hj(\u00b7). If we use this term to replace the l1 regularization term \u2211 j wj in the primal (7), all of our analysis still holds and Algorithm 1 is still applicable with minimal modification, because the new term can be simply seen as a weighted l1 norm."}, {"heading": "3. Experimental results", "text": "Experimental setup In order to evaluate the proposed column generation hashing method (referred to as CGHash), we have conducted a set of experiments on six benchmark datasets. To train data-dependent hash functions, each dataset is randomly split into a training subset and a testing subset. This training/testing split is repeated 5 times, and the average\nperformance over these 5 trials is reported here.\nIn the experiments, the proposed hashing method is implemented by using the squared hinge loss function with the l1 regularization norm (as shown in the supplementary file). Moreover, the triplets used for learning hash functions are generated in the same way as (Weinberger et al., 2006). Specifically, given a training sample, we select the K nearest neighbors from its associated same-label training samples as relevant samples, and then choose the K nearest neighbors from its associated different-label training samples as irrelevant samples (K = 30 for the SCENE-15 dataset and K = 10 for the other datasets). The trade-off control factor C is cross-validated. We found that, in a wide range, the trade-off control factor C does not have a significant impact on the performance.\nCompeting methods To demonstrate the effectiveness of the proposed hashing method (CGHash), we compare with some other state-of-the-art hashing methods quantitatively. For simplicity, they are respectively referred to as LSH (Locality Sensitive Hashing (Andoni & Indyk, 2006)), SSC (Supervised Similarity Sensitive Coding (Torralba et al., 2008) as a modified version of (Shakhnarovich et al., 2003)), LSI (Latent Semantic Indexing (Deerwester et al., 1990)), LCH (Laplacian Co-Hashing (Zhang et al., 2010a)), SPH (Spectral Hashing (Weiss et al., 2008)), STH\n(Self-Taught hashing (Zhang et al., 2010b)), AGH (Anchor Graph Hashing (Liu et al., 2011)), BREs (Supervised Binary Reconstructive Embedding (Kulis & Darrell, 2009)), SPLH (Semi-Supervised Learning Hashing (Wang et al., 2012)), and ITQ (Iterative Quantization (Gong et al., 2012)). Making a comparison with the above competing methods can verify the effect of learning hashing functions and show the performance differences in the context of hashing methods.\nEvaluation criteria For a quantitative performance comparison, we introduce the following three evaluation criteria: i) precision-recall curve; ii) proportion of true neighbors in top-k retrieval; and iii) Knearest-neighbor classification. In the experiments, the aforementioned retrieval performance scores are averaged over all test queries in the dataset. For i), the precision-recall curve is computed as follows: precision = #retrieved relevant sampels\n#all retrieved samples and recall =\n#retrieved relevant sampels #all relevant samples . For ii), the proportion of true neighbors in top-k retrieval is calculated as: #retrieved true neighbors k . For iii), each test sample is classified by a majority voting inK-nearest-neighbor classification.\nQuantitative comparison results Figs. 1\u20136 show the retrieval and classification performances of all the hashing methods using different code lengths on the six datasets. In each of these figures, we report quan-\ntitative comparison results of all the hashing methods in the following three aspects: 1) the average precisionrecall performances using the maximum code length, and the average precisions together with standard deviations (as shown in the legend of each figure); 2) the average performances using different code lengths in the proportion of the true nearest neighbors with top-50 retrieval, and the average proportion results together with their standard deviations in the case of the maximum code length (as shown in the legend of each figure); and 3) the average K-nearest-neighbor classification performances using different code lengths, and the average classification results together with their standard deviations in the case of the maximum code length (as shown in the legend of each figure).\nFrom Figs. 1\u20136, we clearly see that the proposed CGHash obtains the larger areas under the precisionrecall curves than the competing hashing methods. In addition, we observe that CGHash achieves the higher proportions of the true nearest neighbors with top50 retrieval at most times. Moreover, it is seen that CGHash has lower classification errors than the competing methods in most cases.\nFig. 7 shows the retrieval and classification performances of the proposed CGHash using different values of K on the SCENE-15 dataset. It is seen from Fig. 7 that in general the performance is improved as K increases.\nBesides, Fig. 8 shows two retrieval examples on the\nMNIST and LABELME datasets. From Fig. 8, we observe that CGHash obtains the visually accurate nearest-neighbor-search results.\nConclusion We have proposed a novel hashing\nmethod that is implemented using column generationbased convex optimization. By taking into account a set of constraints on the triplet-based relative ranking, the proposed hashing method is capable of learning compact hash codes. Such a set of constraints are incorporated into the large-margin learning framework. Hash functions are then learned iteratively using column generation. Experimental results on several datasets have shown that the proposed hashing method achieves improved performance compared with state-of-the-art hashing methods in nearestneighbor classification, precision-recall, and proportion of true nearest neighbors retrieved.\nThis work is in part supported by ARC grants LP120200485 and FT120100969."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "In Proc. IEEE Symp. Foundations of Computer Science,", "citeRegEx": "Andoni and Indyk,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk", "year": 2006}, {"title": "Learning to hash: forgiving hash functions and applications", "author": ["S. Baluja", "M. Covell"], "venue": "Data Mining & Knowledge Discovery,", "citeRegEx": "Baluja and Covell,? \\Q2008\\E", "shortCiteRegEx": "Baluja and Covell", "year": 2008}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "J. American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Machine Learning,", "citeRegEx": "Demiriz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Demiriz et al\\.", "year": 2002}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Coherency sensitive hashing", "author": ["S. Korman", "S. Avidan"], "venue": "In Proc. Int. Conf. Computer Vision,", "citeRegEx": "Korman and Avidan,? \\Q2011\\E", "shortCiteRegEx": "Korman and Avidan", "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "In Proc. Adv. Neural Information Process. Systems,", "citeRegEx": "Kulis and Darrell,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell", "year": 2009}, {"title": "Kernelized localitysensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "In Proc. Int. Conf. Computer Vision, pp", "citeRegEx": "Kulis and Grauman,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Grauman", "year": 2009}, {"title": "Supplementary document: Effectively learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "A. Dick"], "venue": "paper.html,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.F. Chang"], "venue": "In Proc. Int. Conf. Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.G. Jiang", "S.F. Chang"], "venue": "In Proc. IEEE Conf. Computer Vision & Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "In Proc. Int. Conf. Machine Learning,", "citeRegEx": "Norouzi and Fleet,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet", "year": 2011}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "Schultz and Joachims,? \\Q2004\\E", "shortCiteRegEx": "Schultz and Joachims", "year": 2004}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "In Proc. Int. Conf. Computer Vision, pp", "citeRegEx": "Shakhnarovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2003}, {"title": "Positive semidefinite metric learning using boostinglike algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Strecha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Strecha et al\\.", "year": 2011}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "In Proc. IEEE Conf. Computer Vision & Pattern Recognition,", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Semisupervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "Weinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In Proc. Adv. Neural Information Process. Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Laplacian co-hashing of terms and documents", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. Eur. Conf. Information Retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. ACM SIGIR Conf., pp", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 20, "context": "Considerable effort has been spent on designing hashing methods which address both the issues of fast similarity search and efficient data storage (for example, (Andoni & Indyk, 2006; Weiss et al., 2008; Zhang et al., 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)).", "startOffset": 161, "endOffset": 289}, {"referenceID": 5, "context": "Considerable effort has been spent on designing hashing methods which address both the issues of fast similarity search and efficient data storage (for example, (Andoni & Indyk, 2006; Weiss et al., 2008; Zhang et al., 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)).", "startOffset": 161, "endOffset": 289}, {"referenceID": 17, "context": "Hash-based approaches have thus found a wide range of applications, including object recognition (Torralba et al., 2008), information retrieval (Zhang et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 16, "context": ", 2010b), local descriptor compression (Strecha et al., 2011), image matching (Korman & Avidan, 2011), and many more.", "startOffset": 39, "endOffset": 61}, {"referenceID": 14, "context": "Recently a number of effective hashing methods have been developed which construct a variety of hash functions, mainly on the assumption that semantically similar data samples should have similar binary codes, such as random projection-based locality sensitive hashing (LSH) (Andoni & Indyk, 2006), boosting learning-based similarity sensitive coding (SSC) (Shakhnarovich et al., 2003), and spectral hashing of Weiss et al.", "startOffset": 357, "endOffset": 385}, {"referenceID": 5, "context": ", 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)). A hashing-based approach constructs a set of hash functions that map highdimensional data samples to low-dimensional binary codes. These binary codes can be easily loaded into the memory in order to allow rapid retrieval of data samples. Moreover, the pairwise Hamming distance between these binary codes can be efficiently computed by using bit operations, which are well supported by modern processors, thus enabling efficient similarity calculation on large-scale datasets. Hash-based approaches have thus found a wide range of applications, including object recognition (Torralba et al., 2008), information retrieval (Zhang et al., 2010b), local descriptor compression (Strecha et al., 2011), image matching (Korman & Avidan, 2011), and many more. Recently a number of effective hashing methods have been developed which construct a variety of hash functions, mainly on the assumption that semantically similar data samples should have similar binary codes, such as random projection-based locality sensitive hashing (LSH) (Andoni & Indyk, 2006), boosting learning-based similarity sensitive coding (SSC) (Shakhnarovich et al., 2003), and spectral hashing of Weiss et al. (2008) which is inspired by Laplacian eigenmap.", "startOffset": 55, "endOffset": 1259}, {"referenceID": 20, "context": "In more detail, spectral hashing (Weiss et al., 2008) optimizes a graph Laplacian based objective function such that in the learned low-dimensional binary ar X iv :1 30 3.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "SSC (Shakhnarovich et al., 2003) makes use of boosting to adaptively learn an embedding of the original space, represented by a set of weak learners or hash functions.", "startOffset": 4, "endOffset": 32}, {"referenceID": 15, "context": "This type of relative proximity comparisons have been successfully applied to learn quadratic distance metrics (Schultz & Joachims, 2004; Shen et al., 2012).", "startOffset": 111, "endOffset": 156}, {"referenceID": 20, "context": ", spectral hashing (Weiss et al., 2008), self-taught hashing (Zhang et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 20, "context": ", spectral hashing (Weiss et al., 2008), self-taught hashing (Zhang et al., 2010b)). The spectral hashing (SPH) method directly optimizes a graph Laplacian objective function in the Hamming space. Inspired by SPH, Zhang et al. (2010b) developed the self-taught hashing (STH)", "startOffset": 20, "endOffset": 235}, {"referenceID": 10, "context": "Liu et al. (2011) proposed a scalable graph-based hashing method which uses a small-size anchor graph to approximate the original neighborhood graph and alleviates the computational limitation of spectral hashing.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "Boosting methods have also been employed to develop hashing methods such as SSC (Shakhnarovich et al., 2003) and Forgiving Hash (Baluja & Covell, 2008), both of which learn a set of weak learners as hash functions in the boosting framework.", "startOffset": 80, "endOffset": 108}, {"referenceID": 17, "context": "It is demonstrated in (Torralba et al., 2008) that some datadependent hashing methods like stacked RBM and boosting SSC perform much better than LSH on largescale databases of millions of images.", "startOffset": 22, "endOffset": 45}, {"referenceID": 13, "context": "Strecha et al. (2011) use Fisher linear discriminant analysis (LDA) to embed the original data samples into a lower-dimensional space, where the embedded data samples are binarized using thresholding.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Boosting methods have also been employed to develop hashing methods such as SSC (Shakhnarovich et al., 2003) and Forgiving Hash (Baluja & Covell, 2008), both of which learn a set of weak learners as hash functions in the boosting framework. It is demonstrated in (Torralba et al., 2008) that some datadependent hashing methods like stacked RBM and boosting SSC perform much better than LSH on largescale databases of millions of images. Wang et al. (2012) proposed a semi-supervised hashing method, which aims to ensure the smoothness of similar data samples and the separability of dissimilar data samples.", "startOffset": 81, "endOffset": 456}, {"referenceID": 10, "context": "More recently, Liu et al. (2012) introduced a kernel-based supervised hashing method, where the hashing functions are nonlinear kernel functions.", "startOffset": 15, "endOffset": 33}, {"referenceID": 14, "context": "The closest work to ours might be boosting based SSC hashing (Shakhnarovich et al., 2003), which also learns a set of weighted hash functions through boosting learning.", "startOffset": 61, "endOffset": 89}, {"referenceID": 9, "context": "More details of our hashing algorithm can be found in Algorithm 1 and the supplementary file (Li et al., 2013).", "startOffset": 93, "endOffset": 110}, {"referenceID": 23, "context": "Solve the primal problem for w (using LBFGS-B (Zhu et al., 1997)) and obtain the dual variable u using KKT condition (10).", "startOffset": 46, "endOffset": 64}, {"referenceID": 4, "context": "Demiriz et al. (2002) used this method to design boosting algorithms.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "The above optimization problem can be efficiently solved by using LBFGS (Zhu et al., 1997) after feature normalization.", "startOffset": 72, "endOffset": 90}, {"referenceID": 23, "context": "Note that both the primal problems (7) and (15) can be efficiently solved using quasi-Newton methods such as L-BFGS-B (Zhu et al., 1997) by eliminating the auxiliary variable \u03c1.", "startOffset": 118, "endOffset": 136}, {"referenceID": 19, "context": "Moreover, the triplets used for learning hash functions are generated in the same way as (Weinberger et al., 2006).", "startOffset": 89, "endOffset": 114}, {"referenceID": 17, "context": "For simplicity, they are respectively referred to as LSH (Locality Sensitive Hashing (Andoni & Indyk, 2006)), SSC (Supervised Similarity Sensitive Coding (Torralba et al., 2008) as a modified version of (Shakhnarovich et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 14, "context": ", 2008) as a modified version of (Shakhnarovich et al., 2003)), LSI (Latent Semantic Indexing (Deerwester et al.", "startOffset": 33, "endOffset": 61}, {"referenceID": 3, "context": ", 2003)), LSI (Latent Semantic Indexing (Deerwester et al., 1990)), LCH (Laplacian Co-Hashing (Zhang et al.", "startOffset": 40, "endOffset": 65}, {"referenceID": 20, "context": ", 2010a)), SPH (Spectral Hashing (Weiss et al., 2008)), STH", "startOffset": 33, "endOffset": 53}, {"referenceID": 10, "context": ", 2010b)), AGH (Anchor Graph Hashing (Liu et al., 2011)), BREs (Supervised Binary Reconstructive Embedding (Kulis & Darrell, 2009)), SPLH (Semi-Supervised Learning Hashing (Wang et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 18, "context": ", 2011)), BREs (Supervised Binary Reconstructive Embedding (Kulis & Darrell, 2009)), SPLH (Semi-Supervised Learning Hashing (Wang et al., 2012)), and ITQ (Iterative Quantization (Gong et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 5, "context": ", 2012)), and ITQ (Iterative Quantization (Gong et al., 2012)).", "startOffset": 42, "endOffset": 61}], "year": 2013, "abstractText": "Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning datadependent hash functions have been developed. In this work, we propose a column generation based method for learning datadependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets. * indicates equal contributions. Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).", "creator": "LaTeX with hyperref package"}}}