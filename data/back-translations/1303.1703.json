{"id": "1303.1703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Concept-based indexing in text information retrieval", "abstract": "This lexical search results in inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantically focused query approaches attempt to solve this problem by relying on concepts rather than indexing and retrieval keywords. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this problem by proposing an indexing-level solution. Specifically, we propose a novel approach to semantic indexing based on concepts identified from a linguistic resource. Specifically, our approach is based on the sharing of WordNet and WordNetDomain's lexical databases for concept identification. In addition, we propose a semantic-based concept based on a novel concept based on a central definition of the results of our system.", "histories": [["v1", "Thu, 7 Mar 2013 14:46:17 GMT  (472kb)", "http://arxiv.org/abs/1303.1703v1", "18 pages, 5 tables, 3 figures"]], "COMMENTS": "18 pages, 5 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["fatiha boubekeur", "wassila azzoug"], "accepted": false, "id": "1303.1703"}, "pdf": {"name": "1303.1703.pdf", "metadata": {"source": "CRF", "title": "CONCEPT-BASED INDEXING IN TEXT INFORMATION RETRIEVAL", "authors": ["Fatiha Boubekeur", "Wassila Azzoug"], "emails": ["amirouchefatiha@mail.ummto.dz", "azzoug_w@umbb.dz"], "sections": [{"heading": null, "text": "DOI : 10.5121/ijcsit.2013.5110 119\nTraditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexicalfocused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.\nKEYWORDS\nInformation retrieval, Concept based indexing, concept weighting, Word Sense Disambiguation, WordNet, WordNetDomains."}, {"heading": "1. INTRODUCTION", "text": "Information retrieval (IR) is concerned with selecting from a collection of documents, those that are likely to be relevant to a user\u2019s information need expressed using a query. Three basic functions are carried out in an information retrieval system (IRS): document and information need representation, and matching of these representations. Document representation is usually called indexing. The main objective of document indexing is to associate a document with a descriptor represented by a set of features manually assigned or automatically derived from its content. Representing the user\u2019s information need involves a one step or multi-step query formulation by means of prior terms expressed by the user and/or additive information driven by iterative query improvements like relevance feedback [1]. The main goal of document-query matching, also called query evaluation, is to estimate the relevance of a document to the given query. Most of IR models handle during this step an approximate matching process using the frequency distribution of query terms over the documents to compute the relevance score. This score is used as a criterion to rank the list of documents returned to the user in response to his query.\nTraditional IRS are based on the well known technique of \u201cbag of words\u201d (BOW) representation expressing the fact that both documents and queries are represented as bags of lexical entities, namely keywords. A keyword may be a simple word (as in \u201ccomputer\u201c ) or a compound word (as in \u201ccomputer science\u201d). Weights are associated with document or query keywords [2], [3] to express their importance in the considered material. The weighting scheme is generally based on variations of the well known tf*idf formula [4].\nA key characteristic of such systems is that the degree of document-query matching depends on the number of shared keywords. This leads to a \u201clexical focused\u201d relevance estimation which is less effective than a \u201csemantic focused\u201d one [5]. Indeed, in such IRS, relevant documents are not retrieved if they do not share words with the query, and irrelevant documents that have common words with the query are retrieved even if these words have not the same meaning in the document and the query. The problems mainly stem from the richness in terms of expressive power, yet the synonymy and polysemy inherent in natural language.\nTo address this shortcoming, in this paper, we propose a novel approach to indexing and weighting documents and queries using semantic entities, the concepts, in addition to lexical entities, the keywords. In our concept-based approach, concepts are identified from the content of the document (or the query), and weighted according to both their frequency distribution and their semantic similarity (relatedness) to other concepts in the document (or the query). To identify accurate concepts in the considered material, the approach relies on a two-steps word sense disambiguation (WSD) approach based on the joint use of WordNet [6] and its extension WordNetDomains [7].\nThe remainder of the paper is structured as follows: Section 2 introduces an essential background about WordNet, WordNetDomains and WordNet-based semantic similarity measures. Section 3 discusses related work in the area of concept-based indexing and situates our contribution. Section 4 details our approach. Experimental results are presented in section 5. Section 6 concludes the paper."}, {"heading": "2. BACKGROUND", "text": ""}, {"heading": "2.1. WordNet", "text": "WordNet [6] is an electronic lexical database which covers the majority of nouns, verbs, adjectives and adverbs of the English language, which it structured in a network of nodes and links.\nA node also called synset, is a set of synonymous terms that are interchangeable in a context. A synset represents a concept or a word sense. A synset is lexically represented by a term from its synonyms. Almost each WordNet synset has a gloss expressed in English that defines that synset. A synset's gloss may also contain comments and/or one or more examples of how the words in the synset are used [8]. Table 1 presents the synsets associated with the word \"bank\", among which the first synset, {depository financial institution, bank, banking concern, banking company}, is defined by the gloss: -- (a financial institution that accepts deposits and channels the money into lending activities\u201d; \"he cashed a check at the bank\"; \"that bank holds the mortgage on my home\").\nA link represents a semantic relation between two synsets. The majority of WordNet\u2019s relations connect terms from the same part of speech (POS). Thus, WordNet really consists of four sub-\nnets, one each for nouns, verbs, adjectives and adverbs, with few cross-POS pointers1. For example, the main encoded relations among noun synsets are the following:\n\u2212 the subsumption relation or is-a relation (also called hypernymy/hyponymy) associates a general concept (the hypernym) to a more specific one (its hyponym). For example, the noun bank#n#12 has as hyponyms credit union, Federal Reserve Bank, agent bank, commercial bank, state bank, etc. The is-a relation thus organizes WordNet synsets into a hierarchy. \u2212 the part-whole relation (or Meronymy/holonymy), associates a concept Y (holonym) to its part X (meronym). For example, building is a holonym of window. And conversely window is a meronym of building."}, {"heading": "2.2. WordNet-Based Semantic Relatedness Measures", "text": "Numerous approaches to measuring semantic relatedness between WordNet synsets are proposed in the literature, which are classified into two main types: path-based measures and information content-based measures.\n\u2212 In path-based measures, WordNet is viewed as a graph of concepts and semantic relatedness between two concepts is measured through edge-counting (ie. path length) between their corresponding nodes in the graph. The underlying principle is that the shortest the path from one node to another is, the more similar the concepts are. Leackock and Chodrow measure [9] and Wu-Palmer measure [10] range from this category.\n1 http://wordnet.princeton.edu/ 2bank#n#1 refers to the first sense of the noun bank in WordNet.\n\u2212 In information content-based measures, semantic relatedness between two concepts is measured through the information they share in common. Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.\nWe refer the interested reader to [14] for an overview on the cited measures."}, {"heading": "2.3. WordNetDomains", "text": "WordNetDomains [7] is an extension of WordNet lexical database that results from the annotation of each synset with one or more domain label from a set of 176 domains hierarchically organized through the subsumption (specialization/generalization) relation (for example, Tennis is a more specific domain than Sport, and Architecture is a more general domain than Buildings).\nPart of the WordNetDomains hierarchy is given in Table 3. The Top-Level domain is the root of this hierarchy. Factotum is a functional domain (as opposed to semantic one) that includes generic synsets which are hard to classify in any particular domain, and and Stop senses synsets (such as colors, numbers, etc.) which appeared frequently in different contexts [15]. Factotum is independent from the Top-Level domain and its hierarchy."}, {"heading": "3. RELATED WORK", "text": "Concept-based indexing represents both documents and queries using semantic entities, the concepts, instead of (or in addition to) lexical entities, the keywords. Retrieval is then performed in this conceptual space. Concept-based indexing approaches hold the promise that representing documents and queries (or enhancing their BOW representation) using concepts will result in a retrieval model that is less dependent on the index terms [16]. Indeed, in such a model, documents could be retrieved even when the same concept is described by different terms in the query and the documents, thus alleviating the synonymy problem and increasing recall3 [17]. Similarly, if the correct concepts are chosen for ambiguous words appearing in the query and in the documents, non-relevant documents would not be retrieved, thus alleviating the polysemy problem and increasing precision3.\nConcept-based indexing relies on concepts identified from the content of the document and the queries based on linguistic knowledge resources (such as dictionaries, thesauri, ontologies, etc.). These concepts describe the content of a given text to different extents [18]. To capture this characteristic, each concept is assigned a weight that reflects its relative importance in the indexed text. The indexing process thus runs in two main steps: (1) concept identification and (2) concept weighting.\n3 Recall and precision are two measures used to estimate the effectiveness of an IRS respectively in terms of the ratio of relevant documents that are retrieved, and the ratio of retrieved documents that are relevant."}, {"heading": "3.1. Concept Identification", "text": "Concept identification aims at assigning documents terms4 to the corresponding entries in the ontology (or any other linguistic resource). For this aim, representative keywords are first identified in each document, using classical indexing techniques (tokenization, lemmatization, stop words elimination, etc.). More complex processes can also be integrated to recognize multiword features (nominal phrases, collocations ...) [19]. These terms are then mapped onto the ontology in order to identify the corresponding concepts (or senses). An ambiguous (polysemic) term may correspond to several entries (senses) in the ontology, it must be disambiguated. To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24]. The underlying idea is to estimate the \u201csemantic relatedness\u201d betwen each sense associated with the target term and the other senses from its local context. Formally, the disambiguation process relies on the computation of a score for each concept on the basis of its semantic relatedness to other concepts in the document context. The concept which maximizes the score is then retained as the correct sense of the target term in the document. Approaches in [22], [23], [25], [26], [27] are based on these principles.\nTo disambiguate an ambiguous word, Voorhees [22], classified each synset of this word based on the number of words collocated between a neighborhood of this synset in the WordNet is-a hierarchy and the local context (the sentence in which the word occurs) of the target word. The best classified synset is then considered as the appropriate sense of the ambiguous word.\nIn a similar approach, Katz et al [26] defined the local context of a word as the ordered list of words starting from the closest useful word to the left or right neighborhood until the target word. To disambiguate an word, Katz et al. first extract all non-empty words (called selectors) from the local context of the target word. The set S of selectors is then compared to WordNet synsets. The synset that shares a maximum number of words with S is selected as the appropriate sense of the target word.\nTo disambiguate an ambiguous word, Khan et al. [27], proposed an approach based on the semantic closeness of concepts. The semantic closeness of two concepts is calculated by a score based on their mutual minimal distance in a given domain-oriented ontology. The concepts that have the highest score are then selected.\nBased on the principle that among the various possible senses (WordNet synsets) of a word, the most appropriate one maximizes its relations with other possible senses in the document, Baziz et al. [23], assigned a score to each sense of each ambiguous word. The score of a given sense is obtained by summing the values of its semantic relatedness to other possible senses in the document. The sense having the highest score is then selected as the appropriate sense of the associated word.\nIn our approach proposed in [25], the score associated with a possible sense of a word is based on its semantic similarities with other candidate concepts (senses) in the document, balanced by the occurrence frequencies of the considered features.\nIn a more recent work, the authors in [28] proposed a domain-oriented disambiguation approach that relies on first identifying the correct domain of a word in the document based on WordNetDomains, and then disambiguating the word in the identified domain based on WordNet.\n4 \u201cterms\u201d refers to simple words (keywords) or multi-words (collocations, noun phrases, etc.)\nThe correct domain of a word is selected on the basis of its frequency distribution in the context of the target word.\nBased on a similar principle, we proposed in [29] to disambiguate a domain on the basis of its semantic relatedness to other domains associated with other terms in the same context. Disambiguating a word in its selected domain is based on a score related to the intensity of its semantic relatedness to other words from its context."}, {"heading": "3.2. Concept Weighting", "text": "The concepts weighting aims at evaluating the importance of each concept in a document\u2019s (or query) content. This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie. its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].\nIn statistical weighting approaches, concepts are considered through the terms which represent them. Hence, concepts weighting consists on terms weighting. The weighting approaches of Harrathi et al. [30], Baziz et al. [23] and Voorhees [22] rely on this principle. Based on the extended vector space model introduced in [9], in which every vector consists of a set of subvectors of various concept types (called ctypes), Voorhees [22] proposed to weight the concepts using a normalized classic tf*idf scheme. The approach proposed by Baziz et al. [23], extends the tf*idf scheme to taking into account the compound terms (or multi-words). Indeed, in this socalled Cf*idf approach, the weight of a compound term is based on the cumulative frequency of the term itself and of its components. In [30], the proposed tf*ief weighting scheme is an adapted version of tf*idf to concepts weighting relatively to a given element of an XML document.\nIn semantic weighting approaches, concepts are considered through the senses they represent. Hence, concepts weighting aims at evaluating the importance of the corresponding senses in the document\u2019s (or query) content. In [31], [32], this importance is estimated through the number of semantic relations between the target concept and other concepts in a document. These relations are also weighted in [18]. In [31], the number of relations a concept has with other concepts in the document defines its centrality. The authors in [32] combine centrality and specificity to estimate the importance of a concept in a document. Concept specificity represents its depth in the WordNet hierarchy. In our work presented in [25], we proposed to weight compound terms (representing concepts) on the basis of their semantic relatedness to their corresponding sub-terms (components) and sur-terms (containers). Practically, the weight of a term is estimated through a probabilistic measurement of the relatedness of all its possible senses to other senses associated with its sub-terms and sur-terms taking into account their respective frequencies in the document. In our semantic indexing approach proposed in [24], the importance of a concept in a document is expressed through its (cumulative) semantic relatedness to other concepts in the document. This has been combined with the statistical frequency measure in our proposal in [29].\nIn the present paper, we propose a novel concept-based indexing approach that relies on the joint use of WordNet and WordNetDomains for extracting representative concepts from documents and queries, and assigning them semantic weights that reflect their importance in the indexed materials."}, {"heading": "4. CONCEPT-BASED DOCUMENT INDEXING", "text": ""}, {"heading": "4.1. Definitions and Notations", "text": "Let m be a word of the text to be indexed. \u2212 We call instance of m, each occurrence mi of m in the given text. \u2212 An instance of word is a word. It is associated with a single part of speech (noun, verb, adverb, etc.) in the sentence where it appears. \u2212 An instance mi of word m appears in a sentence. The set of index terms of this sentence defines the local context of mi which is noted .iL\u03b6 \u2212 The global context of mi is the union of all local contexts in which mi appears with the same part of speech. Each global context thus defines a different meaning for m. The global context of mi is noted by .i\u03b6\n\u2212 The local expression of mi of size s+1 is the character string obtained from concatenating the word mi and the s successive words located immediately to the right of mi by using an underscore (_) between them.\n\u2212 The size of a local expression is the number of words it contains."}, {"heading": "4.2. Description of the Approach", "text": "The key objective of our approach is to represent the document (respectively the query) by a semantic index composed of two types of terms: concepts and orphan keywords.\n\u2212 Concepts are (unambiguous) WordNet entries (synsets) identified from the text of the document. The concepts are denoted by either simple words or collocations. \u2212 Orphan keywords are (non-empty) simple words of the document that do not have entries in WordNet.\nConcepts are first identified in the document (respectively in the query) by means of an identification/disambiguation process (which also allows to identifying orphan keywords) and then weighted. Thus, the indexing process mainly runs in two steps: first step is concept identification, and second step is the concept weighting."}, {"heading": "4.2.1 Concept Identification", "text": "The main objective of this step is to identify the representative concepts of a given document (respectively query). Concepts are entries in WordNet which correspond to the representative terms of the indexed document (respectively query). Concepts identification implies (a) identifying index terms from the considered text and then mapping them to WordNet synsets, and (b) disambiguating ambiguous terms (an ambiguous term corresponds to more than one synset)."}, {"heading": "4.2.1.1 Identifying Index Terms", "text": "The purpose of this step is to identify the set of representative terms (words or collocations) of the considered material relying on WordNet. This step begins by the identification of collocations. For this aim, we first built a list Coloc of all WordNet\u2019s collocations. Then, for each analyzed word\u2019s instance mi, we extract from Coloc the set i of collocations that begin with mi. i is first ranked by decreasing order of its elements size, then each element in i is projected on the mi\u2018s local expression Ei of size i . If a collocation matches with a local expression, it is retained and\ninserted into the set Expres of identified collocations. If no collocation matches with the local expression of mi, mi is a simple word. If this simple word has an entry in WordNet, it will be inserted into the set Simples\u03be of simple words. Otherwise it will be added to the orphans set Orphel\u03be .\nThe terms identification algorithm is given in Table 4."}, {"heading": "4.2.1.2 Term Disambiguation", "text": "This step aims at assigning a meaning to each ambiguous index term based on the context in which it occurs. Collocations are almost unambiguous terms, hence the disambiguation process which mainly relies on WordNet will only involve the simple words which have entries in this lexical database. Therefore only the words of Simples\u03be are concerned.\nA word of Simples\u03be can have several entries (synsets) in WordNet which correspond to different\nsenses. The purpose of this step is to select the appropriate synset of the word based on its context.To disambiguate words, we propose an approach based on three disambiguation levels:\n\u2212 The first level is part of speech (POS) identification. This level aims at determining the POS of each word mi in the document using the Stanford POS Tagger. \u2212 The second level is domain disambiguation. This level aims at identifying the usage domain of a word in the context of a document (or query). Domain identification relies on the use of WordNetDomains. This disambiguation level will limit the number of senses to be discussed in the third level. \u2212 The third level is word sense disambiguation. It aims at selecting among the possible senses of the word in the selected domain the most appropriate sense in the document."}, {"heading": "4.2.1.2.1 Part of speech Identification", "text": "A word may have several instances in a document, each of which is characterized by its POS. The objective of this step is to identify the POS of each instance in a given material. For this aim, we simply rely on utilizing the Stanford POS Tagger. As the synsets associated with a word\u2019s instance are grouped in WordNet according to their part of speech, this step aims to limit the synsets to be examined in the next disambiguation steps to those having the same POS as the target instance."}, {"heading": "4.2.1.2.2. Domain Disambiguation", "text": "WordNet synsets are labeled in WordNetDomains by means of domain labels. An ambiguous word has several associated senses (synsets) in WordNet each of which can belong to one or more domains in WordNetDomains. The aim of this step is to select the correct domain of a word in the context of the document.\nAssuming that the appropriate domain of a word mi is likely to be highly related to the other domains of its (global) context i\u03b6 , we assign each domain Dj associated with each sense (synset) of mi a score based on its semantic relatedness with other domains Dk associated with the other terms tk ( { } )ExpresSimpleskt  \u222a\u2208 belonging to i\u03b6 .The domain that maximizes this score is then selected as the appropriate domain of the word mi in the document. Formally:\n( ) ( )    = \u2211 \u2211 \u2208 \u2208 kj t nkj j DDSimDScore ik ,maxarg ]..1[\nwhere ( )kj DDSim , denotes the semantic relatedness of domains Dj and Dk estimated through the Wu-Palmer [10] measure which we adapt to the WordNetDomains hierarchy as follows:\n( ) . )()( )(*2 ,\n*\nkj kj DdepthDdepth\nDdepth DDSim\n+ =\nWhere:\n- *D is the least common subsumer of Dj and Dk in the WordNetDomains hierarchy.\n-    Ddepth is the depth of D in the WordNetDomains hierarchy.\nRemark._ We work on the assumption that a domain is directly related to other domains in the WordNetDomains hierarchy. Therefore, it makes no sense to use the domain factotum for this disambiguation technique."}, {"heading": "4.2.1.2.3 Word sense disambiguation", "text": "At this stage, every word mi in Simples\u03be is associated with a single domain Di in its context. But it\nstill can be associated with more than one synset (sense) in this domain. In this case, it must be disambiguated. The aim is to select among all the synsets associated with mi in Di, the appropriate sense (meaning) of mi in its context.\nLet )( jiS be the set of all synsets associated with the word mi in the domain jD , and [ ]kS ji )( the kth\nelement of )( jiS . To disambiguate the word mi in its domain jD , we associate a score with each\nsynset [ ]kS ji )( based on its semantic relatedness with other synsets associated with the other terms of the context. The synset with the highest score is selected as the appropriate meaning of the word mi in its context. Formally:\n[ ] ( )\n[ ] [ ]( )   \n\n\n   \n = \u2211 \u2211\n\u2260 \u2208 \u2264\u2264 nSkSSimArgkS mlji il tl Snk ji il jl )()( 1 )( ,max \nWhere [ ] [ ]( )nSkSSim mlji )()( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ]kS ji )( and [ ]nS ml )( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] \u2026)"}, {"heading": "4.2.2. Concepts Weighting", "text": "The objective of this step is to assign weights to the identified concepts (synsets) which express their importance in the document.\nStarting from the idea that the more a concept is locally central in the document and globally central in the collection the more it is representative of the content of the document, we will weight concepts according to their local and global centralities defined as follows:\nThe local centrality of a concept Ci in a document d, denoted cc(Ci, d), is defined on the basis of its relevance in the document on the one hand and on its occurrence frequency on the other hand. The concept\u2019s relevance is measured through its semantic relatedness with the other concepts in the document. Its frequency is the cumulative frequency of all its representative terms in the document. Formally:\n( ) ( ) ( ) ( )\u2211 \u2260 \u2212+= li liii CCSimdCtfdCcc ,1,*, \nWhere:\n- \u03b1 is a weighting factor used to balance the frequency with respect to the relevance. This factor is determined experimentally,\n- Sim(Ci, Cl) measures the semantic similarity between concepts Ci and Cl, calculated on the basis of the Resnik measure [11],\n- tf(Ci, d) is the occurrence frequency of the concept Ci in the document.\nDefinition._ A concept Ci is (locally) central in a document d if its local centrality in d is greater than a fixed threshold s (ie. ( ) sdCcc i >, ).\nThe global centrality of a concept Ci defines its discrimination power in the collection of documents (that is its ability to discriminate between those documents that contain informative concepts and those that contain non-informative ones). The idea is that a concept that is central in too many documents is non-informative. On the other hand, a concept that is central in few\ndocuments is considered more informative. The ratio of documents in which the concept Ci is central defines the document centrality of Ci, noted dc(Ci). Formally:\n( ) N\nn Cdc i =\nWhere N is the total number of documents in the collection, and n is the number of documents in which Ci is a central concept.\nThe discrimination power (ie. the global centrality) of a concept is then seen as a measure of its inverted document centrality (denoted by idc). Formally:\n( ) ( )ii Cdc Cidc\n1=\nThe weight associated with a concept Ci in a document d is then defined as a combination of its local centrality and its global centrality. Formally:\n( ) ( ) ( )iidi CidcdCccCW *,, =\nThis proposed scheme, called cc-idc, allows weighting the concepts as well as the orphan keywords. In this latter case, only the tf factor of local centrality is considered (WordNet-based semantic similarity doesn\u2019t apply for orphan keywords, it is then set to zero)."}, {"heading": "5. EXPERIMENTAL EVALUATION", "text": "In this section, we present our experimental evaluation of the proposed approach. Our objective is twofold: (1) measure the effectiveness of our concept-based indexing approach over classical indexing approaches, and (2) study the performance of our semantic weighting scheme (cc-idc) over classical weighting schemes.\nIn the following, we first introduce the experimental setting (test collection and evaluation protocol), then we present and discuss evaluation results."}, {"heading": "5.1. Experimental Setting", "text": ""}, {"heading": "5.1.1 Test Collection", "text": "For our experiments, we used the Time5 test collection. This dataset contains 423 documents consisting of newspaper articles from the TIME magazine and a large number of queries (83). Human relevant judgements are associated with each query. We chose to use the 15 queries that provide the most significant results in classical search based on tf*idf weighting."}, {"heading": "5.1.2. Evaluation Protocol", "text": "The approach is implemented through a concept-based vector IR system. In this system, documents and queries are seen as vectors of weighted concepts, and compared through the classical cosine measure. Evaluation is performed according to TREC protocol. It is based on two\n5 ftp://ftp.cs.cornell.edu/pub/smart/time/\nmeasures: precision at cutoff x, P @ x (x = 5, 10, 20, 50, 100, 200, 500), and mean average precision (MAP).\nPrecision at cutoff x, P@x is the ratio of relevant documents among the top x returned documents. Formally, if RRx denote the number of relevant documents among the x first retrieved documents, then:\nx\nRR xP x=@\nMAP is the arithmetic mean over all queries of the average precision (calculated at the ranks in which relevant documents are retrieved) calculated for each query. Formally:\n( )\u2211 \u2211 = =         = N j Q i i j j docP QN MAP 1 1 11\nWhere:\n- Qj is the number of relevant documents for query j ,\n- N is the total number of queries,\n- P(doci) is the precision at the rank of the i th observed relevant document.\nPractically, for each query, the top 100 documents retrieved by the system are examined, and precisions P @ x (x = 1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 100) and MAP are calculated. The Trec_eval6 program is used for these calculations. We then compare the results from our semantic index to those returned by reference systems (or baselines). In our experiments, we consider two baselines:\n- The first (denoted Classic-TF_IDF) corresponds to a classical index based on tf*idf - weighted keywords,\n- The second (denoted Classic-BM25) is a classical index based on Okapi-BM25weighted keywords [34].\nOur objective through these experiments is twofold: first evaluate the impact of concept-based indexing over keyword-based indexing, and second evaluate the impact of the weighting scheme cc-idc on retrieval effectiveness."}, {"heading": "5.2. Concept-Based Indexing Evaluation", "text": "To evaluate the effectiveness of our concept-based indexing disregarding the proposed weighting scheme, we consider our semantic index firstly weighted by tf*idf (index Sem-TF-IDF) and secondly weighted by Okapi-BM25 (index Sem-BM25). Then, we compare the retrieval results from these indexes to those of Classic-TF-IDF and Classic-BM25 baselines respectively. This approach allows isolate concepts contribution from weights contribution.\nThe evaluation results obtained for these different models are presented in Figure 1.\n6 http://trec.nist.gov/trec_eval/\nAccording to these results, we noticed that:\n- The Sem-TF-IDF approach performs better than the Classic-TF-IDF baseline. Significant improvement rates (better than 25%) of 100 % for P@5, 103,33 % for P@10, 124 % for P@15, 80% for P@20, 57,27% for P@30, 27,27% for P@50, 44,44% for P@100, and 61,23 % for the MAP are observed.\n- Besides, the Sem-BM25 approach is better than the Classic-BM25 baseline. Whereas Sem-BM25 and Classic-BM25 performs identically for the P@5 precision, significant improvement rates of 36,67 % for P@10, 75 % for P@15, 40% for P@20, 42,99% for P@30, and 26,8 % for the MAP are observed. Improvement rates for P@50, P@100 although non-significant are positive of 8,33% and 15% respectively.\nFrom these results, it is clear that our semantic indexes (Sem-TF-IDF and Sem-BM25) are more effective than keyword-based indexes. At this evaluation stage, it is therefore clearly stated that our concept-based indexing approach is more efficient than keyword-based approaches.\nMoreover, as shown in Figure 2, the Sem-TF-IDF approach performs better than the Sem-BM25 approach. Improvement rates of 100 % for P5, 48,78 % for P10, 28,55 % for P15, 10% for P@30, 7,69% for P@50, 13,04 for P@100, and 23,60% for the MAP are observed."}, {"heading": "5.3. Concept Weighting Evaluation", "text": "The second series of experiments focuses on the evaluation of our concepts weighting scheme introduced in section 4.2.2. Practically, we aim at measuring the impact of the weighting scheme cc-idc on the retrieval effectiveness. For this aim, we compare retrieval results from our semantic index weighted by cc-idc (Sem-CC-IDC) to those of Sem-TF-IDF and Sem-BM25.\nAs the proposed weighting scheme cc-idc depends on the \u03b1 weighting parameter, preliminary experiments are necessary to fix the appropriate related value"}, {"heading": "5.3.1.  Selection", "text": "We conducted a series of experiments in order to fix the appropriate value for the \u03b1 weighting parameter. In these experiments, we vary \u03b1 values between 0 and 1, resulting in different weighting schemes which are successively used to weight our semantic index. The weighted indexes thus obtained are then evaluated through their retrieval results. Evaluation is performed according to the protocol introduced above (section 5.1.2); It is based on two measures: average precision MP@x and MAP, where MP@x is the arithmetic mean of P@x (x = 1,2,3,4,5,10,15,20,30,50,100) precisions\nTable 5 presents the results of this evaluation. These results show that the overall best retrieval performances (according to MP@x and MAP) are obtained for \u03b1 = 0.2."}, {"heading": "P @ 1 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333", "text": "In the following, we fix the value of \u03b1 to 0.2. This value favors concept relevance over concept frequency in the corresponding weighting scheme."}, {"heading": "5.3.2. Experimental Evaluation", "text": "The purpose of this step is to evaluate the impact on the retrieval effectiveness of the proposed semantic weighting scheme cc-idc over classical weighting schemes tf*idf and Okapi-BM25. For this aim, we compare retrieval results from our semantic index Sem-CC-IDC (for \u03b1 = 0.2) to\nthose of Sem-TF-IDF and Sem-BM25 respectively. The experimental results are represented in Figure 3.\nAccording to these results, we noticed that:\n- The Sem-CC-IDC index performs better than the Sem-TF-IDF index. Significant improvement rates (better than 25%) of 200% for P@5, 80,33% for P @10, 33,36% for P@15, 33,33% for P@20, 54.47% for P@30, 35,71% for P@50 and 218,34% for MAP are observed. Nevertheless, a non-significant decreasing rate of 7,69% is obtained at P@100 precision.\n- Moreover, Sem-CC-IDC performs better than Sem-BM25. Significant performance rates (better than 25%) of 500%, 168.29%, 71.42%, 71.42%, 69.91%, 46,15%, and 293% are observed respectively for P@5, P@10, P@15, P@20, P@ 30, P@50 and MAP. An increasing rate of about 4% is also observed for P@100 precision.\nFrom these results, it is clear that our cc-idc-weighted semantic index (Sem-CC-IDC) is more effective than classically-weighted semantic indexes (Sem-TF-IDF and Sem-BM25). At this evaluation stage, it is therefore clearly stated that our cc-idc-weighting scheme is more effective for concepts than classical tf*idf and BM25 schemes."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we have presented a novel approach to automatic concept-based document indexing. Our contribution is twofold: first, we have introduced a novel concept identification approach based on a novel domain-based word sense disambiguation framework that rely on the joint use of WordNet and WordNetDomains, and second we have defined a novel semantic weighting scheme that relies on concept centrality. In our proposal, the centrality of a concept is based on its apparent importance (measured across its frequency of occurrence) in the document on the one hand and on its latent importance (measured across its semantic relatedness to other concepts) in the document on the other hand. Our experimental results showed that the proposed concept-based indexing approach is more effective than classical keyword-based indexing ones; Moreover, our cc-idc-weighting approach (for the fixed value of the weighting parameter \u03b1) performs better than classical TF-IDF and BM25 weighting schemes. In future works, we plan first to check to what extent the weighting factor \u03b1 depends or not on the used document\ncollection, and second to propose a retrieval score for documents that takes into account semantic concepts weights."}], "references": [{"title": "Relevance feedback in information retrieval", "author": ["J.J. Rocchio"], "venue": "In The SMART Retrieval System, in Experiments in Automatic Document Processing. G.Salton editor,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1971}, {"title": "A fuzzy linguistic approach generalizing Boolean information retrieval: a model and its evaluation,", "author": ["G. Bordogna", "G. Pasi"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "A model for a weighted retrieval system,", "author": ["D.A. Buell", "D.H. Kraft"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1981}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G Salton", "C Buckley"], "venue": "Information Processing and Management", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Beyond the keyword bariier: knowledge-based information retrieval. Information services and use", "author": ["Mauldin M", "J. Carbonell", "R. Thomason"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1987}, {"title": "WordNet : A Lexical database for English", "author": ["G. Miller"], "venue": "Actes de ACM", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing", "author": ["Luisa Bentivogli", "Pamela Forner", "Bernardo Magnini", "Emanuele Pianta"], "venue": "COLING", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "ed.) WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["C. Leacock", "M. Chodorow"], "venue": "In C. Fellbaum Ed., MIT Press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Verb semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In 32nd. Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language", "author": ["P. Resnik"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "In Proceedings of 15th International Conference On Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["Jay J. Jiang", "David W. Conrath"], "venue": "In Proceedings of International Conference on Research", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "EvaluatingWordNet-based Measures of Lexical Semantic Relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Chinese WordNet Domains: Bootstrapping Chinese WordNet with Semantic Domain Labels", "author": ["Lung-Hao Lee", "Yu-Ting Yu", "Chu-Ren Huang"], "venue": "In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Concept-Based Information Retrieval using Explicit Semantic Analysis", "author": ["O. Egozi", "S. Markovitch", "E. Gabrilovich"], "venue": "ACM Transactions on Information Systems, Volume 29 Issue", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Document indexing: a concept-based approach to term weight estimation", "author": ["B.Y. Kang", "S.J. Lee"], "venue": "In Journal of Information Processing & Management. Volume 41, Issue", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "An exploratory analysis of phrases in text retrieval", "author": ["Jeremy Pickens", "W. Bruce Croft"], "venue": "RIAO", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Word sense disambiguation for free-text indexing using a massive semantic network", "author": ["M. Sussna"], "venue": "2nd International Conference on Information and Knowledge Management", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Disambiguating noun groupings with respect to WordNet senses", "author": ["P. Resnik"], "venue": "3thWorkshop on Very Large Corpora,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Using WordNet to disambiguate word senses for text retrieval. Association for Computing Machinery Special Interest Group on Information Retrieval", "author": ["E.M. Voorhees"], "venue": "Journal of Computer Science & Information Technology (IJCSIT)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A Conceptual Indexing Approach based on Document content Representation. Dans : CoLIS5 : Fifth International Conference on Conceptions of Libraries and Information Science, Glasgow, UK, 4 juin 8 juin", "author": ["M. Baziz", "M. Boughanem", "N. Aussenac-Gilles"], "venue": "Lecture Notes in Computer Science LNCS Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Using WordNet for Concept-based document indexing in information retrieval", "author": ["F. Boubekeur", "M. Boughanem", "L. Tamine", "M. Daoud"], "venue": "Fourth International Conference on Semantic Processing (SEMAPRO),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Retrieval effectiveness of an ontology-based model for information selection", "author": ["L.R. Khan", "D. Mc Leod", "E.Hovy"], "venue": "The VLDB Journal", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Word Sense Disambiguation using WordNetDomains", "author": ["S.G. Kolte", "S.G. Bhirud"], "venue": "In First International Conference on Emerging Trends in Engineering and Technology", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Indexation s\u00e9mantique de documents textuels\u00bb, 14e Colloque International sur le Document Electronique", "author": ["F. Boubekeur", "W. Azzoug", "S. Chiout", "M. Boughanem"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Vers une approche statistique pour l\u2019indexation s\u00e9mantique des documents multilingues ", "author": ["F. Harrathi", "C. Roussey", "L. Maisonnasse", "S. Calabretto"], "venue": "Actes du XXVIII\u00b0 congre\u0300s INFORSID,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A new factor for computing the relevance of a document to a query (regular paper)", "author": ["M. Boughanem", "I. Mallak", "H. Prade"], "venue": "Dans : IEEE World Congress on Computational Intelligence (WCCI", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Vers un mod\u00e8le d'indexation s\u00e9mantique adapt\u00e9 aux dossiers m\u00e9dicaux de patients", "author": ["D. Dinh", "L.Tamine"], "venue": "(short paper). Dans : Confe\u0301rence francophone en Recherche d'Information et Applications (CORIA", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from a nice cream cone", "author": ["M.E. Lesk"], "venue": "In Proceedings of the SIGDOC Conference. Toronto,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "The probability ranking principle in IR", "author": ["S.E. Robertson"], "venue": "Journal of Documentation", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "Representing the user\u2019s information need involves a one step or multi-step query formulation by means of prior terms expressed by the user and/or additive information driven by iterative query improvements like relevance feedback [1].", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "Weights are associated with document or query keywords [2], [3] to express their importance in the considered material.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Weights are associated with document or query keywords [2], [3] to express their importance in the considered material.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The weighting scheme is generally based on variations of the well known tf*idf formula [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "This leads to a \u201clexical focused\u201d relevance estimation which is less effective than a \u201csemantic focused\u201d one [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "To identify accurate concepts in the considered material, the approach relies on a two-steps word sense disambiguation (WSD) approach based on the joint use of WordNet [6] and its extension WordNetDomains [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "To identify accurate concepts in the considered material, the approach relies on a two-steps word sense disambiguation (WSD) approach based on the joint use of WordNet [6] and its extension WordNetDomains [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 5, "context": "WordNet [6] is an electronic lexical database which covers the majority of nouns, verbs, adjectives and adverbs of the English language, which it structured in a network of nodes and links.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "A synset's gloss may also contain comments and/or one or more examples of how the words in the synset are used [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Leackock and Chodrow measure [9] and Wu-Palmer measure [10] range from this category.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Leackock and Chodrow measure [9] and Wu-Palmer measure [10] range from this category.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "We refer the interested reader to [14] for an overview on the cited measures.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "WordNetDomains [7] is an extension of WordNet lexical database that results from the annotation of each synset with one or more domain label from a set of 176 domains hierarchically organized through the subsumption (specialization/generalization) relation (for example, Tennis is a more specific domain than Sport, and Architecture is a more general domain than Buildings).", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": ") which appeared frequently in different contexts [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Indeed, in such a model, documents could be retrieved even when the same concept is described by different terms in the query and the documents, thus alleviating the synonymy problem and increasing recall [17].", "startOffset": 205, "endOffset": 209}, {"referenceID": 16, "context": "These concepts describe the content of a given text to different extents [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": ") [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "To disambiguate an ambiguous word, Voorhees [22], classified each synset of this word based on the number of words collocated between a neighborhood of this synset in the WordNet is-a hierarchy and the local context (the sentence in which the word occurs) of the target word.", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "[27], proposed an approach based on the semantic closeness of concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23], assigned a score to each sense of each ambiguous word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In a more recent work, the authors in [28] proposed a domain-oriented disambiguation approach that relies on first identifying the correct domain of a word in the document based on WordNetDomains, and then disambiguating the word in the identified domain based on WordNet.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "Based on a similar principle, we proposed in [29] to disambiguate a domain on the basis of its semantic relatedness to other domains associated with other terms in the same context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 188, "endOffset": 192}, {"referenceID": 21, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "[30], Baziz et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] and Voorhees [22] rely on this principle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] and Voorhees [22] rely on this principle.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Based on the extended vector space model introduced in [9], in which every vector consists of a set of subvectors of various concept types (called ctypes), Voorhees [22] proposed to weight the concepts using a normalized classic tf*idf scheme.", "startOffset": 55, "endOffset": 58}, {"referenceID": 20, "context": "Based on the extended vector space model introduced in [9], in which every vector consists of a set of subvectors of various concept types (called ctypes), Voorhees [22] proposed to weight the concepts using a normalized classic tf*idf scheme.", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "[23], extends the tf*idf scheme to taking into account the compound terms (or multi-words).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [30], the proposed tf*ief weighting scheme is an adapted version of tf*idf to concepts weighting relatively to a given element of an XML document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [31], [32], this importance is estimated through the number of semantic relations between the target concept and other concepts in a document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [31], [32], this importance is estimated through the number of semantic relations between the target concept and other concepts in a document.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "These relations are also weighted in [18].", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "In [31], the number of relations a concept has with other concepts in the document defines its centrality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "The authors in [32] combine centrality and specificity to estimate the importance of a concept in a document.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In our semantic indexing approach proposed in [24], the importance of a concept in a document is expressed through its (cumulative) semantic relatedness to other concepts in the document.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "This has been combined with the statistical frequency measure in our proposal in [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "where ( ) k j D D Sim , denotes the semantic relatedness of domains Dj and Dk estimated through the Wu-Palmer [10] measure which we adapt to the WordNetDomains hierarchy as follows:", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 251, "endOffset": 255}, {"referenceID": 29, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 257, "endOffset": 261}, {"referenceID": 10, "context": "This factor is determined experimentally, - Sim(C, C) measures the semantic similarity between concepts C and C, calculated on the basis of the Resnik measure [11], - tf(C, d) is the occurrence frequency of the concept C in the document.", "startOffset": 159, "endOffset": 163}, {"referenceID": 30, "context": "In our experiments, we consider two baselines: - The first (denoted Classic-TF_IDF) corresponds to a classical index based on tf*idf weighted keywords, - The second (denoted Classic-BM25) is a classical index based on Okapi-BM25weighted keywords [34].", "startOffset": 246, "endOffset": 250}], "year": 2013, "abstractText": "Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexicalfocused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.", "creator": "Microsoft Office Word"}}}