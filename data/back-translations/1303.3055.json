{"id": "1303.3055", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions", "abstract": "We study the problem of learning Markov decision-making processes with finite state and action spaces, when transition probability distributions and loss functions are chosen contrary to each other and allowed to change over time. We introduce an algorithm whose regret grows in relation to each policy in a comparative class with the square root of the number of rounds of the game, provided the transition probabilities meet a uniform mixed condition. Our approach is efficient as long as the comparison class is polynomial and we can calculate expectations about sample paths for each policy. Developing an efficient algorithm without any regrets in the general case remains an open problem.", "histories": [["v1", "Tue, 12 Mar 2013 23:25:37 GMT  (13kb)", "http://arxiv.org/abs/1303.3055v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yasin abbasi-yadkori", "peter l bartlett", "varun kanade", "yevgeny seldin", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1303.3055"}, "pdf": {"name": "1303.3055.pdf", "metadata": {"source": "CRF", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions", "authors": ["Yasin Abbasi-Yadkori", "Peter L. Bartlett", "Csaba Szepesv\u00e1ri"], "emails": ["yasin.abbasiyadkori@qut.edu.au", "bartlett@eecs.berkeley.edu", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n30 55\nv1 [\ncs .L\nG ]\n1 2\nM ar\n2 01"}, {"heading": "1 Notation", "text": "Let X be a finite state space and A be a finite action space. Let \u2206S be the space of probability distributions over set S. Define a policy \u03c0 as a mapping from the state space to \u2206A, \u03c0 : X \u2192 \u2206A. We use \u03c0(a|x) to denote the probability of choosing action a in state x under policy \u03c0. A random action under policy \u03c0 is denoted by \u03c0(x). A transition probability kernel (or transition model) m is a mapping from the direct product of the state and action spaces to \u2206X : m : X \u00d7 A \u2192 \u2206X . Let P (\u03c0,m) be the transition probability matrix of policy \u03c0 under transition model m. A loss function is a bounded real-valued function over state and action spaces, \u2113 : X \u00d7 A \u2192 R. For a vector v, define \u2016v\u20161 = \u2211\ni |vi|. For a real-valued function f defined over X \u00d7 A, define \u2016f\u2016\u221e,1 = maxx\u2208X \u2211 a\u2208A |f(x, a)|. The inner product between two vectors v and w is denoted by \u3008v, w\u3009."}, {"heading": "2 Introduction", "text": "Consider the following game between a learner and an adversary: at round t, the learner chooses a policy \u03c0t from a policy class \u03a0. In response, the adversary chooses a transition model mt from a set of models M and a loss function \u2113t. The learner takes action at \u223c \u03c0t(.|xt), moves to state xt+1 \u223c mt(.|xt, at) and suffers loss \u2113t(xt, at). To simplify the discussion, we assume that the adversary is oblivious, i.e. its choices do not depend on the previous choices of the learner. We assume that \u2113t \u2208 [0, 1]. In this paper, we study the full-information version of the game, where the learner observes the transition model mt and the loss function \u2113t at the end of round t. The game is shown in Figure 1. The objective of the learner is to suffer low loss over a period of T rounds, while the performance of the learner is measured using its regret with respect to the total loss he would have achieved had he followed the stationary policy in the comparison class \u03a0 minimizing the total loss.\nEven-Dar et al. (2004) prove a hardness result for MDP problems with adversarially chosen transition models. Their proof, however, seems to have gaps as it assumes that the learner chooses a deterministic policy before observing the state at each round. Note that an online learning algorithm only needs to choose an action at the current state and does not need to construct a complete deterministic policy at each round. Their hardness result applies to deterministic transition models, while we make a mixing assumption in our analysis. Thus, it is still an open problem whether it is possible to obtain a computationally efficient algorithm with a sublinear regret.\nYu and Mannor (2009a,b) study the same setting, but obtain only a regret bound that scales with the amount of variation in the transition models. This regret bound can grow linearly with time.\nEven-Dar et al. (2009) prove regret bounds for MDP problems with a fixed and known transition model and adversarially chosen loss functions. In this paper, we prove regret bounds for MDP problems with adversarially chosen transition models and loss functions. We are not aware of any earlier regret bound for this setting. Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy.\nMDPs with changing transition kernels are good models for a wide range of problems, including dialogue systems, clinical trials, portfolio optimization, two player games such as poker, etc."}, {"heading": "3 Online MDP Problems", "text": "Let A be an online learning algorithm that generates a policy \u03c0t at round t. Let x A t be the state at round t if we have followed the policies generated by algorithm A. Similarly, x\u03c0t denotes the state if we have chosen the same policy \u03c0 up to time t. Let \u2113(x, \u03c0) = \u2113(x, \u03c0(x)). The regret of algorithm A up to round T with respect to any policy \u03c0 \u2208 \u03a0 is defined by\nRT (A, \u03c0) =\nT \u2211\nt=1\n\u2113t(x A t , at)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0 t , \u03c0) ,\nwhere at = \u03c0t(x A t ). Note that the regret with respect to \u03c0 is defined in terms of the sequence of states x\u03c0t that would have been visited under policy \u03c0. Our objective is to design an algorithm that achieves low regret with respect to any policy \u03c0.\nIn the absence of state variables, the problem reduces to a full information online learning problem (Cesa-Bianchi and Lugosi, 2006). The difficulty with MDP problems is that, unlike the full information online learning problems, the choice of policy at each round changes the future states and losses. The main idea behind the design and the analysis of our algorithm is the following regret decomposition:\nRT (A, \u03c0) =\nT \u2211\nt=1\n\u2113t(x A t , at)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t) +\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0 t , \u03c0) . (1)\nLet\nBT (A) =\nT \u2211\nt=1\n\u2113t(x A t , at)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t) ,\nCT (A, \u03c0) = T \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0 t , \u03c0) .\nNotice that the choice of policies has no influence over future losses in CT (A, \u03c0). Thus, CT (A, \u03c0) can be bounded by a specific reduction to full information online learning algorithms (to be specified later). Also, notice that the competitor policy \u03c0 does not appear in BT (A). In fact, BT (A) depends only on the algorithm A. We will show that if algorithm A and the class of models satisfy the following two \u201csmoothness\u201d assumptions, then BT (A) can be bounded by a sublinear term.\nAssumption A1 Rarely Changing Policies Let \u03b1t be the probability that algorithm A changes its policy at round t. There exists a constant D such that for any 1 \u2264 t \u2264 T , any sequence of models m1, . . . ,mt and loss functions \u21131, . . . , \u2113t, \u03b1t \u2264 D/ \u221a t.\nAssumption A2 Uniform Mixing There exists a constant \u03c4 > 0 such that for all distributions d and d\u2032 over the state space, any deterministic policy \u03c0, and any model m \u2208 M ,\n\u2016dP (\u03c0,m)\u2212 d\u2032P (\u03c0,m)\u20161 \u2264 e\u22121/\u03c4 \u2016d\u2212 d\u2032\u20161 .\nAs discussed by Neu et al. (2010), if Assumption A2 holds for deterministic policies, then it holds for all policies."}, {"heading": "3.1 Full Information Algorithms", "text": "We would like to have a full information online learning algorithm that rarely changes its policy. The first candidate that we consider is the well-known Exponentially Weighted Average (EWA) algorithm (Vovk, 1990, Littlestone and Warmuth, 1994) shown in Figure 2. In our MDP problem, the EWA algorithm chooses a policy \u03c0 \u2208 \u03a0 according to distribution\nqt(\u03c0) \u221d exp ( \u2212\u03bb t\u22121 \u2211\ns=1\nE [\u2113s(x \u03c0 s , \u03c0)]\n)\n, \u03bb > 0 , (2)\nThe policies that this EWA algorithm generates most likely are different in consecutive rounds and thus, the EWA algorithm might change its policy frequently. However, a variant of EWA, called Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, satisfies Assumption A1. Our algorithm, called SD-MDP, is based on the SD algorithm and is shown in Figure 4. Notice that the algorithm needs to know the number of rounds, T , in advance.\nConsider a basic full information problem with N experts. Let RT (SD, i) be the regret of the SD algorithm with respect to expert i up to time T . We have the following results for the SD algorithm.\nTheorem 1. For any expert i \u2208 {1, . . . , N},\nRT (SD, i) \u2264 4 \u221a T logN + logN ,\nand also for any 1 \u2264 t \u2264 T ,\nP (Switch at time t) \u2264 \u221a logN\nT .\nProof. The proof of the regret bound can be found in (Geulen et al., 2010, Theorem 3). The proof of the bound on the probability of switch is similar to the proof of Lemma 2 in (Geulen et al., 2010) and is as follows: As shown in (Geulen et al., 2010, Lemma 2), the probability of switch at time t is\n\u03b1t = Wt\u22121 \u2212Wt\nWt\u22121 .\nThus, Wt = (1\u2212 \u03b1t)Wt\u22121. Because the loss function is bounded in [0, 1], we have that\nWt =\nN \u2211\ni=1\nwi,t =\nN \u2211\ni=1\nwi,t\u22121(1 \u2212 \u03b7)ct(i) \u2265 N \u2211\ni=1\nwi,t\u22121(1\u2212 \u03b7) = (1\u2212 \u03b7)Wt\u22121 .\nThus, 1\u2212 \u03b1t \u2265 1\u2212 \u03b7, and thus,\n\u03b1t \u2264 \u03b7 \u2264 \u221a logN\nT ."}, {"heading": "3.2 Analysis of the SD-MDP Algorithm", "text": "The main result of this section is the following regret bound for the SD-MDP algorithm.\nTheorem 2. Let the loss functions selected by the adversary be bounded in [0, 1], and the transition models selected by the adversary satisfy Assumption A2. Then, for any policy \u03c0 \u2208 \u03a0,\nE [RT (SD-MDP, \u03c0)] \u2264 (4 + 2\u03c42) \u221a T log |\u03a0|+ log |\u03a0| . In the rest of this section, we write A to denote the SD-MDP algorithm. For the proof we use\nthe regret decomposition (1):\nRT (A, \u03c0) = BT (A) + CT (A, \u03c0) .\n3.2.1 Bounding E [CT (A, \u03c0)] Lemma 3. For any policy \u03c0 \u2208 \u03a0,\nE [CT (A, \u03c0)] = E\n[\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0 t , \u03c0)\n]\n\u2264 4 \u221a T log |\u03a0|+ log |\u03a0| .\nProof. Consider the following imaginary game between a learner and an adversary: we have a set of experts (policies) \u03a0 = {\u03c01, . . . , \u03c0|\u03a0|}. At round t, the adversary chooses a loss vector ct \u2208 [0, 1]\u03a0, whose ith element determines the loss of expert \u03c0i at this round. The learner chooses a distribution over experts qt (defined by the SD algorithm), from which it draws an expert \u03c0t. Next, the learner observes the loss function ct. From the regret bound for the SD algorithm (Theorem 1), it is guaranteed that for any expert \u03c0,\nT \u2211\nt=1\n\u3008ct, qt\u3009 \u2212 T \u2211\nt=1\nct(\u03c0) \u2264 4 \u221a T log |\u03a0|+ log |\u03a0| .\nNext, we determine how the adversary chooses the loss vector. At time t, the adversary chooses a\nloss function \u2113t and sets ct(\u03c0 i) = E\n[\n\u2113t(x \u03c0i t , \u03c0 i) ]\n. Noting that \u3008ct, qt\u3009 = E [\u2113t(x\u03c0tt , \u03c0t)] and ct(\u03c0) = E [\u2113t(x \u03c0 t , \u03c0)] finishes the proof.\n3.2.2 Bounding E [BT (A)]\nFirst, we prove the following two lemmas.\nLemma 4. For any state distribution d, any transition model m, and any policies \u03c0 and \u03c0\u2032,\n\u2016dP (\u03c0,m)\u2212 dP (\u03c0\u2032,m)\u20161 \u2264 \u2016\u03c0 \u2212 \u03c0\u2032\u2016\u221e,1 .\nProof. Proof is easy and can be found in (Even-Dar et al., 2009), Lemma 5.1.\nLemma 5. Let \u03b1t be the probability of a policy switch at time t. Then, \u03b1t \u2264 \u221a log |\u03a0|/T . Proof. Proof is identical to the proof of Theorem 1.\nLemma 6. We have that\nE [BT (A)] = E\n[\nT \u2211\nt=1\n\u2113t(x A t , at)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t)\n]\n\u2264 2\u03c42 \u221a log |\u03a0|T .\nProof. Let Ft = \u03c3(\u03c01, . . . , \u03c0t). Notice that the choice of policies are independent of the state variables. We can write\nE [BT (A)] = E\n[\nT \u2211\nt=1\n\u2113t(x A t , at)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c0t t , \u03c0t)\n]\n= E\n[\nT \u2211\nt=1\n\u2211\nx\u2208X\n(\nI{xA t =x} \u2212 I{x\u03c0t t =x}\n)\n\u2113t(x, \u03c0t(x))\n]\n= E\n[\nT \u2211\nt=1\n\u2211\nx\u2208X\nE\n[(\nI{xA t =x} \u2212 I{x\u03c0t t =x}\n) \u2113t(x, \u03c0t(x)) \u2223 \u2223 \u2223 FT ]\n]\n= E\n[\nT \u2211\nt=1\n\u2211\nx\u2208X\n\u2113t(x, \u03c0t(x))E [(\nI{xA t =x} \u2212 I{x\u03c0t t =x}\n) \u2223\n\u2223 \u2223 FT ]\n]\n\u2264 E [ T \u2211\nt=1\n\u2016\u2113t\u2016\u221e \u2225 \u2225 \u2225 E [( I{xA t =x} \u2212 I{x\u03c0t t =x} ) \u2223 \u2223 \u2223 FT ]\u2225 \u2225 \u2225\n1\n]\n= E\n[\nT \u2211\nt=1\n\u2016\u2113t\u2016\u221e \u2016ut \u2212 vt,t\u20161\n]\n\u2264 E [ T \u2211\nt=1\n\u2016ut \u2212 vt,t\u20161\n]\n, (3)\nwhere us = E [\nI{xA s =x}\n\u2223 \u2223FT ] is the distribution of xAs for s \u2264 t and vs,t = E [ I{x \u03c0t s =x}\n\u2223 \u2223 \u2223 FT ] is the\ndistribution of x\u03c0ts for s \u2264 t.1 Let Et be the event of a policy switch at time t. From inequality\n\u2016\u03c0t\u2212k \u2212 \u03c0t\u2016\u221e,1 \u2264 \u2016\u03c0t\u2212k \u2212 \u03c0t\u2212k+1\u2016\u221e,1 + \u00b7 \u00b7 \u00b7+ \u2016\u03c0t\u22121 \u2212 \u03c0t\u2016\u221e,1 \u2264 2 t \u2211\ns=t\u2212k+1\nI{Es} ,\nand Lemma 5, we get that\nE\n[ \u2016\u03c0t\u2212k \u2212 \u03c0t\u2016\u221e,1 ]\n\u2264 2 \u221a log |\u03a0| T k . (4)\nLet P \u03c0t = P (\u03c0,mt). We have that\nE [ \u2016ut \u2212 vt,t\u20161 ] = E [\u2225 \u2225ut\u22121P \u03c0t\u22121 t\u22121 \u2212 vt\u22121,tP \u03c0tt\u22121 \u2225 \u2225 1 ]\n= E [\u2225 \u2225ut\u22121P \u03c0t\u22121 t\u22121 \u2212 ut\u22121P \u03c0tt\u22121 + ut\u22121P \u03c0tt\u22121 \u2212 vt\u22121,tP \u03c0tt\u22121 \u2225 \u2225\n1\n]\n\u2264 E [\u2225 \u2225ut\u22121P \u03c0t\u22121 t\u22121 \u2212 ut\u22121P \u03c0tt\u22121 \u2225 \u2225 1 + \u2225 \u2225ut\u22121P \u03c0t t\u22121 \u2212 vt\u22121,tP \u03c0tt\u22121 \u2225 \u2225 1 ] \u2264 E [\n\u2016\u03c0t\u22121 \u2212 \u03c0t\u2016\u221e,1 + e\u22121/\u03c4 \u2016ut\u22121 \u2212 vt\u22121,t\u20161 ]\n\u2264 E [\n\u2016\u03c0t\u22121 \u2212 \u03c0t\u2016\u221e,1 + e\u22121/\u03c4 ( \u2225 \u2225ut\u22122P \u03c0t\u22122 t\u22122 \u2212 ut\u22122P \u03c0tt\u22122 \u2225 \u2225\n1\n+ \u2225 \u2225ut\u22122P \u03c0t t\u22122 \u2212 vt\u22122,tP \u03c0tt\u22122 \u2225 \u2225 1 ) ]\n\u2264 E [ \u2016\u03c0t\u22121 \u2212 \u03c0t\u2016\u221e,1 + e\u22121/\u03c4 \u2016\u03c0t\u22122 \u2212 \u03c0t\u2016\u221e,1 + e\u22122/\u03c4 \u2016ut\u22122 \u2212 vt\u22122,t\u20161 ]\n\u2264 . . .\n\u2264 t \u2211\nk=0\ne\u2212k/\u03c4E [ \u2016\u03c0t\u2212k \u2212 \u03c0t\u2016\u221e,1 ] + e\u2212t/\u03c4 \u2016u0 \u2212 v0,t\u20161\n\u2264 t \u2211\nk=0\n2e\u2212k/\u03c4 \u221a log |\u03a0| T k + 0 By (4)\n\u2264 2 \u221a log |\u03a0| T \u03c42 , (5)\nwhere we have used the fact that \u2016u0 \u2212 v0,t\u20161 = 0, because the initial distributions are identical. By (5) and (3), we get that\nE [BT (A)] \u2264 2\u03c42 T \u2211\nt=1\n\u221a\nlog |\u03a0| T = 2\u03c42 \u221a log |\u03a0|T .\nWhat makes the analysis possible is the fact that all policies mix no matter what transition model is played by the adversary.\nProof of Theorem 2. The result is obvious by Lemmas 3 and 6.\nThe next corollary extends the result of Theorem 2 to continuous policy spaces.\nCorollary 7. Let \u03a0 be an arbitrary policy space, N (\u01eb) be the \u01eb-covering number of space (\u03a0, \u2016.\u2016\u221e,1), and C(\u01eb) be an \u01eb-cover. Assume that we run the SD-MDP algorithm on C(\u01eb). Then, under the same assumptions as in Theorem 2, for any policy \u03c0 \u2208 \u03a0,\nE [RT (SD-MDP, \u03c0)] \u2264 (4 + 2\u03c42) \u221a T logN (\u01eb) + logN (\u01eb) + \u03c4T \u01eb .\n1Notice that FT contains only policies, which are independent of the state variables.\nProof. Let LT (\u03c0) = E [ \u2211T t=1 \u2113t(x \u03c0 t , \u03c0) ] be the value of policy \u03c0. Let u\u03c0,t(x) = P (x \u03c0 t = x). First, we prove that the value function is Lipschitz with Lipschitz constant \u03c4T . The argument is similar to the argument in the proof of Lemma 6. For any \u03c01 and \u03c02,\n|LT (\u03c01)\u2212 LT (\u03c02)| = \u2223 \u2223 \u2223 \u2223\n\u2223\nE\n[\nT \u2211\nt=1\n\u2113t(x \u03c01 t , \u03c01)\u2212\nT \u2211\nt=1\n\u2113t(x \u03c02 t , \u03c02)\n]\u2223\n\u2223 \u2223 \u2223 \u2223\n\u2264 2 \u2223 \u2223 \u2223\n\u2223 \u2223\nT \u2211\nt=1\n\u2016u\u03c01,t \u2212 u\u03c02,t\u20161 \u2016\u2113t\u2016\u221e\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 2 \u2223 \u2223 \u2223\n\u2223 \u2223\nT \u2211\nt=1\n\u2016u\u03c01,t \u2212 u\u03c02,t\u20161\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nWith an argument similar to the one in the proof of Lemma 6, we can show that\n\u2016u\u03c01,t \u2212 u\u03c02,t\u20161 \u2264 \u03c4 \u2016\u03c01 \u2212 \u03c02\u2016\u221e,1 .\nThus, |LT (\u03c01)\u2212 LT (\u03c02)| \u2264 \u03c4T \u2016\u03c01 \u2212 \u03c02\u2016\u221e,1 . Given this and the fact that for any policy \u03c0 \u2208 \u03a0, there is a policy \u03c0\u2032 \u2208 C(\u01eb) such that \u2016\u03c0 \u2212 \u03c0\u2032\u2016\u221e,1 \u2264 \u01eb, we get that\nE [RT (SD-MDP, \u03c0)] \u2264 (4 + 2\u03c42) \u221a T logN (\u01eb) + logN (\u01eb) + \u03c4T \u01eb .\nIn particular if \u03a0 is the space of all policies, N (\u01eb) \u2264 (|A|/\u01eb)|A||X |, so regret is no more than\nE [RT (SD-MDP, \u03c0)] \u2264 (4 + 2\u03c42) \u221a T |A||X | log |A| \u01eb + |A||X | log |A| \u01eb + \u03c4T \u01eb .\nBy the choice of \u01eb = 1T , we get that E [RT (SD-MDP, \u03c0)] = O(\u03c4 2 \u221a T |A| |X | log(|A|T ))."}], "references": [{"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Experts in a Markov decision process", "author": ["Eyal Even-Dar", "Sham M. Kakade", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Even.Dar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2004}, {"title": "Online Markov decision processes", "author": ["Eyal Even-Dar", "Sham M. Kakade", "Yishay Mansour"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "Regret minimization for online buffering problems using the weighted majority algorithm", "author": ["Sascha Geulen", "Berthold V\u00f6cking", "Melanie Winkler"], "venue": "In COLT,", "citeRegEx": "Geulen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Geulen et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online Markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andr\u00e1s Gy\u00f6rgy", "Andr\u00e1s Antos Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Aggregating strategies", "author": ["Vladimir Vovk"], "venue": "In COLT, pages 372\u2013383,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Arbitrarily modulated Markov decision processes", "author": ["Jia Yuan Yu", "Shie Mannor"], "venue": "In IEEE Conference on Decision and Control,", "citeRegEx": "Yu and Mannor.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor.", "year": 2009}, {"title": "Online learning in Markov decision processes with arbitrarily changing rewards and transitions", "author": ["Jia Yuan Yu", "Shie Mannor"], "venue": "In GameNets,", "citeRegEx": "Yu and Mannor.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Even-Dar et al. (2004) prove a hardness result for MDP problems with adversarially chosen transition models.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "|xt, at) Learner observes mt and lt end for Figure 1: Online Markov Decision Processes Even-Dar et al. (2009) prove regret bounds for MDP problems with a fixed and known transition model and adversarially chosen loss functions.", "startOffset": 87, "endOffset": 110}, {"referenceID": 0, "context": "In the absence of state variables, the problem reduces to a full information online learning problem (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 101, "endOffset": 132}, {"referenceID": 4, "context": "As discussed by Neu et al. (2010), if Assumption A2 holds for deterministic policies, then it holds for all policies.", "startOffset": 16, "endOffset": 34}, {"referenceID": 3, "context": "However, a variant of EWA, called Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, satisfies Assumption A1.", "startOffset": 59, "endOffset": 80}, {"referenceID": 3, "context": "The proof of the bound on the probability of switch is similar to the proof of Lemma 2 in (Geulen et al., 2010) and is as follows: As shown in (Geulen et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": "Proof is easy and can be found in (Even-Dar et al., 2009), Lemma 5.", "startOffset": 34, "endOffset": 57}], "year": 2013, "abstractText": "We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game, provided the transition probabilities satisfy a uniform mixing condition. Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy. Designing an efficient algorithm with small regret for the general case remains an open problem.", "creator": "LaTeX with hyperref package"}}}