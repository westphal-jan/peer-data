{"id": "1303.6977", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2013", "title": "ABC Reinforcement Learning", "abstract": "This paper provides a simple, general framework for probability-free Bayesian reinforcement learning through Approximate Bayesian Computation (ABC). The main advantage is that we only need a prior distribution across a class of simulators (generative models), which is useful in areas where an analytical probability model of the underlying process is too complex but detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique, even in this case. Furthermore, it can be seen as an extension of rollout algorithms to the case where we do not know which is the right model for rollouts. We will experimentally demonstrate the potential of this approach compared to LSPI. Finally, we will introduce a theorem that shows that ABC is basically a sound methodology, even if insufficient statistics are used.", "histories": [["v1", "Wed, 27 Mar 2013 20:51:33 GMT  (26kb)", "https://arxiv.org/abs/1303.6977v1", "16 pages, 4 figures"], ["v2", "Wed, 8 May 2013 12:54:53 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v2", "10 pages, 4 figures"], ["v3", "Tue, 18 Jun 2013 09:42:59 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v3", "10 pages, 4 figures"], ["v4", "Fri, 28 Jun 2013 11:18:26 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v4", "Corrected version of paper appearing in ICML 2013"]], "COMMENTS": "16 pages, 4 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["christos dimitrakakis", "nikolaos tziortziotis"], "accepted": true, "id": "1303.6977"}, "pdf": {"name": "1303.6977.pdf", "metadata": {"source": "CRF", "title": "ABC Reinforcement Learning", "authors": ["Christos Dimitrakakis"], "emails": ["christos.dimitrakakis@gmail.com", "ntziorzi@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n69 77\nv4 [\nst at\n.M L\n] 2\n8 Ju\nn 20"}, {"heading": "1. Introduction", "text": "Bayesian reinforcement learning (Strens, 2000; Vlassis et al., 2012) is the decision-theoretic approach (DeGroot, 1970) to solving the reinforcement learning problem. However, apart from the fact that calculating posterior distributions and the Bayesoptimal decision is frequently intractable (Duff, 2002; Ross et al., 2008), another major difficulty is the specification of the prior and model class. While there exist a number of non-parametric Bayesian model classes which can be brought to bear for estimation of the dynamics of an unknown process, it may not be a trivial matter to select the correct class and prior. On the other hand, it is frequently known that the process can be approximated well by a complex parametrised simulator. The question is how to take advantage of this knowledge when the best simulator\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nparameters are not known.\nWe propose a simple, general, reinforcement learning framework employing the principles of Approximate Bayesian Computation (ABC, see (Csille\u0301ry et al., 2010) for an overview) for performing Bayesian inference using simulation. In doing so, we extend rollout algorithms for reinforcement learning, such as those described in (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Lagoudakis & Parr, 2003a), to the case where we do not know what the correct model to draw rollouts from is.\nWe show how to use ABC to compute approximate posteriors over a set of environment models in the context of reinforcement learning. This includes a simple but general theoretical result on the quality of ABC posterior approximations. Finally, building on previous approaches to Bayesian reinforcement learning, we propose a strategy for selecting policies in this setting."}, {"heading": "1.1. The setting", "text": "In the reinforcement learning problem, an agent is acting in some unknown environment \u00b5, according to some policy \u03c0. The agent\u2019s policy is a procedure for selecting a sequence of actions, with the action at time t being at \u2208 A. The environment reacts to this sequence with a corresponding sequence of observations xt \u2208 X and rewards rt \u2208 R. This interaction may depend on the complete history1 h \u2208 H, where H , (X \u00d7 A \u00d7 R)\u2217 is the set of all state action reward sequences, as neither the agent or the environment are necessarily finite-order Markov. For example, the agent may learn, or the environment may be partially observable.\nIn this paper, we use a number of shorthands to simplify notation. Firstly, we denote the (random) prob-\n1A history may include multiple trajectories in episodic environments.\nability measure for the agent\u2019s action at time t by:\n\u03c0t(A) , P \u03c0(at \u2208 A | x t, rt, at\u22121), (1.1)\nwhere xt is a shorthand for the sequence (xi) t i=1; similarly, we use xtk for (xi) t i=k. We denote the environment\u2019s response at time t+1 given the history at time t by:\n\u00b5t(B) , P\u00b5((xt+1, rt+1) \u2208 B | x t, rt, at). (1.2)\nIn a further simplification, we shall also use \u03c0t(at) for the probability (or density) of the action actually taken by the policy at time t, and similarly, \u00b5t(xt) for the realised observation. Finally, we use P\u03c0\u00b5 to denote joint distributions on action, observation and reward sequences under the environment \u00b5 and policy \u03c0.\nThe agent\u2019s goal is determined through its utility:\nU ,\n\u221e\u2211\nt=1\n\u03b3t\u22121rt, (1.3)\nwhich is a discounted sum of the total instantaneous rewards obtained, with \u03b3 \u2208 [0, 1]. Without loss of generality, we assume that U \u2208 [0, Umax]. The optimal policy maximises the expected utility E\u03c0\u00b5 U . As in the reinforcement learning problem the environment \u00b5 is unknown, this maximisation is ill-posed. Intuitively, we can increase the expected utility by either: (i) Trying to better estimate \u00b5 in order to perform the maximisation later (exploration), or (ii) Use a best-guess estimate of \u00b5 to obtain high rewards (exploitation).\nIn order to solve this trade-off, we can adopt a Bayesian viewpoint (DeGroot, 1970; Savage, 1972), where we consider a (potentially infinite) set of environment models M. In particular, we select a prior probability measure \u03be onM. For an appropriate subset B \u2282 M, the quantity \u03be(B) describes our initial belief that the correct model lies in B. We can now formulate the alternative goal of maximising the expected utility with respect to our prior:\nE \u03c0 \u03be U =\n\u222b\nM\n(E\u03c0\u00b5 U) d\u03be(\u00b5). (1.4)\nWe can now formalise the problem as finding a policy \u03c0\u2217\u03be \u2208 argmax\u03c0 E \u03c0 \u03be U . Any such policy is Bayesoptimal, as it solves the exploration-exploitation problem with respect to our prior belief."}, {"heading": "1.2. Related work and our contribution", "text": "The first difficulty when adopting a Bayesian approach to sequential decision making is that finding the policy maximising (1.4) is hard (Duff, 2002)\neven in restricted classes of policies (Dimitrakakis, 2011). On the other hand, simple heuristics such as Thompson sampling (Strens, 2000; Thompson, 1933) provide an efficient trade-off (Agrawal & Goyal, 2012; Kaufmanna et al., 2012) between exploration and exploitation. Alghough other heuristics exist (Araya et al., 2012; Castro & Precup, 2007; Kolter & Ng, 2009; Poupart et al., 2006; Strens, 2000), in this paper we focus on an approximate version of Thompson sampling for reasons of simplicity. The second difficulty is that in many interesting problems, the exact posterior calculation may be intractable, mainly due to partial observability (Poupart & Vlassis, 2008; Ross et al., 2008). Interestingly, an ABC approach would not suffer from this problem for reasons that will be made clear in the sequel.\nThe most fundamental difficulty in a Bayesian framework is specifying a generative model class: it is not always clear what is the best model to use for an application. However, frequently we have access to a class of parametrised simulators for the problem. Therefore, one reasonable approach is to find a good policy for a simulator in the class, and then apply it to the actual problem. Methods for finding good policies using simulation have been extensively studied before (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Gabillon et al., 2011; Wu et al., 2010). However, in all those cases simulation was performed on a simulator with fixed parameters.\nApproximate Bayesian Computation (ABC) (see Csille\u0301ry et al., 2010; Marin et al., 2011, for an overview) is a general framework for likelihood-free Bayesian inference via simulation. It has been developed because of the existence of applications, such as econometric modelling (e.g. Geweke, 1999), where detailed simulators were available, but no useful analytical probabilistic models. While ABC methods have also been used for inference in dynamical systems (e.g Toni et al., 2009), they have not yet been applied to the reinforcement learning problem.\nThis paper proposes to perform Bayesian reinforcement learning through ABC on an arbitrary class of parametrised simulators. As ABC has been widely used in applications characterised by large amounts of data and complex simulations with many unknown parameters, it may also scale well in reinforcement learning applications. The proposed methodology is generally applicable to arbitrary problems, including partially observable environments, continuous state spaces, and stochastic Markov games.\nABC Reinforcement Learning generalises methods previously developed for simulation-based approximation of optimal policies to the Bayesian case. While in the standard framework covered by Bertsekas (1999), a particular simulator of the environment is assumed to exist, via ABC we can relax this assumption. We only need a class of parametrised simulators that contain one close to the real environment dynamics. Thus, the only remaining difficulty is computational complexity.\nFinally, we provide a simple but general bound for ABC posterior computation. This bounds the KL divergence of the approximate posterior computed via ABC and the complete posterior distribution. As far as we know, this is a new and widely applicable result, although some other theoretical results using similar assumptions appear in (Jasra et al., 2010) and in (Dean & Singh, 2011) for hidden Markov models.\nSection 2 introduces ABC inference for reinforcement learning, discusses its difference from standard Bayesian inference, and presents a theorem on the quality of the ABC approximation. Section 3 describes the ABC-RL framework and the ABC-LSPI algorithm for continuous state spaces. An experimental illustration is given in Sec. 4, followed by a discussion in Sec. 5. The appendix contains the collected proofs."}, {"heading": "2. Approximate Bayesian Computation", "text": "Approximate Bayesian Computation encompasses a number of likelihood-free techniques where only an approximate posterior is calculated via simulation. We first discuss how standard Bayesian inference in reinforcement learning differs from ABC inference. We then introduce a theorem on the quality of the ABC approximation."}, {"heading": "2.1. Bayesian inference for reinforcement learning", "text": "Imagine that the history h \u2208 H has been generated from a process \u00b5 \u2208 M controlled with a historydependent policy \u03c0, something which we denote as h \u223c P\u03c0\u00b5. Now consider a prior \u03be on M with the property that \u03be(\u00b7 | \u03c0) = \u03be(\u00b7), i.e. that the prior is independent of the policy used. Then the posterior probability, given a history h generated by a policy \u03c0, that \u00b5 \u2208 B can be written as: 2\n\u03be(B | h, \u03c0) =\n\u222b B P \u03c0 \u00b5(h) d\u03be(\u00b5)\u222b\nM P \u03c0 \u00b5(h) d\u03be(\u00b5)\n. (2.1)\n2For finite M, the posterior simplifies to \u03be(\u00b5 | h, \u03c0) = P \u03c0 \u00b5(h)\u03be(\u00b5)/ \u2211 \u00b5\u2032\u2208M P \u03c0 \u00b5\u2032(h)\u03be(\u00b5 \u2032)\nFortunately, the dependence on the policy can be removed, since the posterior is the same for all policies that put non-zero mass on the observed data:\nRemark 2.1. Let h \u223c P\u03c0\u00b5. Then \u2200\u03c0 \u2032 6= \u03c0 such that P \u03c0\u2032\n\u00b5 (h) > 0, \u03be(B | h, \u03c0) = \u03be(B | h, \u03c0 \u2032).\nConsequently, when calculating posteriors, the policy employed need not be considered, even when the process and policy depend on the complete history. In the ABC setting we do not have direct access to the probabilities \u00b5t, for the models \u00b5 in our model class M. However, we can always generate observations from any model: xt+1 \u223c \u00b5t. This idea is used by ABC to calculate approximate posterior distributions."}, {"heading": "2.2. ABC inference for reinforcement learning", "text": "The main idea of ABC is to approximate samples from the posterior distribution via simulation. We produce a sequence of sample models \u00b5(k) from the prior \u03be, and then generate data h(k) from each. If the generated data is \u201csufficiently close\u201d to the history h, then the k-th model is accepted as a sample from the posterior \u03be(\u00b5 | h). More specifically, ABC requires that we define an approximately sufficient statistic f : H \u2192 W on some normed vector space (W , \u2016 \u00b7 \u2016). If \u2016f(h)\u2212 f(h(k))\u2016 \u2264 \u03b5 then \u00b5(k) is accepted as a sample from the posterior. Algorithm 1 gives the sampling method in detail for reinforcement learning. An important difference with the standard ABC posterior approximation, as well as exact inference, is the dependency on \u03c0.\nNote that even though Remark 2.1 declares that the posterior is independent of the policy used, when using ABC this is no longer true. We must maintain the complete policy used until then to generate samples, otherwise there is no way to generate a sequence of observations.3 Intuitively, the algorithm can basically be seen as generating rollouts from a number of simulators, sampled from our prior distribution. The sampled set of simulators with a sufficient close statistic is then an approximate sample from our posterior distribution. The first question is what types of statistics we need.\nIn fact, just as in standard ABC, if the statistic is sufficient, then the samples will be generated according to the posterior.\nCorollary 2.1. If f is a sufficient statistic, then the set M\u0302 returned by Alg. 1 for \u01eb = 0 is a sample from the posterior.\n3For episodic problems, we must maintain the sequence of policies used.\nAlgorithm 1 ABC-RL-Sample\ninput Prior \u03be on M, history h \u2208 H, threshold \u03b5, statistic f : H \u2192 W , policy \u03c0, maximum number of samples Nsam, stopping condition \u03c4 . M\u0302 = \u2205. for k = 1, . . . , Nsam do \u00b5(k) \u223c \u03be. h(k) \u223c P \u03c0\n\u00b5(k) if \u2225\u2225f(h)\u2212 f(h(k)) \u2225\u2225 < \u03b5 then M\u0302 := M\u0302 \u222a { \u00b5(k) } . end if if \u03c4 then break end if\nend for return M\u0302\nThe (standard) proof is deferred to the appendix. Thus, for \u01eb = 0, when the statistic is sufficient, the sampling distribution and the posterior are identical. However, things are not so clear when \u01eb > 0.\nWe now provide a simple theorem which characterises the relation of the approximate posterior to the true posterior, when we use a (not necessarily sufficient) statistic with threshold \u01eb > 0. First, we remind the definition of the KL-divergence.\nDefinition 2.1. The KL-divergence D between two probability measures \u03be, \u03be\u2032 on M is\nD (\u03be \u2016 \u03be\u2032) ,\n\u222b\nM\nln d\u03be(\u00b5)\nd\u03be\u2032(\u00b5) d\u03be(\u00b5). (2.2)\nIn order to prove meaningful results, we need some additional assumptions on the likelihood function. In this particular case, we simply assume that it is smooth (Lipschitz) with respect to the statistical distance:\nAssumption 2.1. For a given policy \u03c0, for any \u00b5, and histories x, h \u2208 H, there exists L > 0 such that\u2223\u2223ln [ P \u03c0 \u00b5(h)/P \u03c0 \u00b5(x) ]\u2223\u2223 \u2264 L\u2016f(h)\u2212 f(x)\u2016.\nWe note in passing that this assumption is related to the notion of differential privacy (Dwork & Lei, 2009), from which it was inspired.\nWe now can state the following theorem, whose proof can be found in the appendix, which generalises the previous corollary.\nTheorem 2.1. Under a policy \u03c0 and statistic f satisfying Assumption 2.1, the approximate posterior distribution \u03be\u01eb(\u00b7 | h) satisfies:\nD (\u03be(\u00b7 | h) \u2016 \u03be\u01eb(\u00b7 | h)) \u2264 ln |A h \u01eb |+ 2L\u01eb, (2.3)\nwhere Ah\u01eb , { z \u2208 H | \u2016f(z)\u2212 f(h)\u2016 \u2264 \u01eb } is the \u01eb-ball around the observed history h with respect to the statistical distance and |Ah\u01eb | denotes its size.\nThe divergence depends on the statistic in the following ways. Firstly, it approaches 0 as \u01eb \u2192 0. Secondly, it is smaller for smoother likelihoods. However, because of the dependence on the size of the \u01eb-ball4 around the observed statistic, the statistic cannot be arbitrarily smooth. Nevertheless, it may be the case that a sufficient statistic is not required for good performance. Since in reinforcement learning we are mainly interested in the utility rather than in system identification, we may be able to get good results by using utility-related statistics.\nObservation-based statistics A simple idea is to select features on which to calculate statistics. Discounted cumulative feature expectation are especially interesting, due to their connection with value functions (e.g. ?, Sec. 6.9.2). The main drawback is that this adds yet another hyper-parameter to tune. In addition, unlike econometrics or bioinformatics, we may not be interested in model identification per se, but only in finding a good policy.\nUtility-based statistics Quantities related to the utility may be a good match for reinforcement learning. In the simplest case, it may be sufficient to only consider unconditional moments of the utility, which is the approach followed in this paper. However, these may only trivially satisfy Ass. 2.1 for arbitrary policies. Nevertheless, as we shall see, even a very simple such statistic has a reasonably good performance."}, {"heading": "2.3. A Hoeffding-based utility statistic", "text": "In particular, given a history h including Ndat trajectories in the environment, with the i-th trajectory obtaining utility U (i), we obtain a mean estimate E\u0302 Ndat\nU , 1 Ndat U (i). We then obtain a history h\u0302(k)\ncontaining Ntrj trajectories from the sampled environment \u00b5(k) and construct the mean estimate E\u0302 Ntrj\nk U . In order to test whether these are close enough, we use the Hoeffding inequality (Hoeffding, 1963). In fact, it is easy to see that, with probability at least 1 \u2212 \u03b4, |E\u03c0\u00b5 U \u2212 E \u03c0 \u00b5(k) U | is lower bounded by:\n|E\u0302 N datU\u2212E\u0302 Ntrj k U |\u2212Umax\n\u221a ln(2/\u03b4)(Ndat +Ntrj)\n2NdatNtrj , (2.4)\n4For discrete observations this is simply the counting measure of the ball. For more general cases it can be extended to an appropriate measure.\nwhere Umax is the range of the utility function. We then use (2.4) as the statistical distance\u2225\u2225f(h)\u2212 f(h(k))\n\u2225\u2225 between the observed history h and the sampled history h(k). The advantage of using this statistic is that the more data we have, it becomes harder to accept a sample.\nThis statistic has two parameters. Firstly, the error probability \u03b4, which does not need to be very small in practice, as the Hoeffding bound is only tight for high-variance distributions. The second parameter is Ntrj. This does not need to be very large, since it only makes a marginal difference in the bound when Ntrj \u226b Ndat. An illustration of the type of samples obtained with this statistic is given in Figure 1, which shows the dependency of the approximate posterior distribution on the threshold \u01eb when conditioned on a fixed amount Ndat of training trajectories."}, {"heading": "3. ABC reinforcement learning", "text": "We now present a simple algorithm for ABC reinforcement learning, based on the ideas explained in the previous section. For any given set of observations and policies, we draw a number of sample environments from the prior distribution. For each environment, we execute the relevant policy and calculate the appropriate statistics. If these are close enough to the observed statistic, the sample is accepted. The next step is to find a good policy for the sampled simulator. As we can draw an arbitrary number of rollouts in the simulator, any type of approximate dynamic programming algorithm can be used. In our experiments, we used LSPI (Lagoudakis & Parr, 2003b), which is simple to program and effective. The hope is that if the approximate posterior sampling is reasonable, then we can take advantage of our prior knowledge of the environment class, to learn a good policy with less data, at the expense of additional computation.\nAlgorithm 2 ABC-RL\nparameters M, \u03be, h, \u03c0, f \u03c4 = {|M\u0302 | = 1} \u00b5\u0302 = ABC-RL-Sample(M, \u03be, h, \u03c0, f, \u03c4) return \u03c0\u0302 \u2248 argmax\u03c0 E \u03c0 \u00b5\u0302 U\nA sketch of the algorithm is shown in Alg.2. This has a number of additional parameters that need to be discussed. The most important is the stopping condition \u03c4 . The simplest idea, which we use in this paper, is to stop when a single model \u00b5\u0302 has been generated by ABC-RL-Sample.\nThen an (approximate) optimal policy for the sam-\npled model \u00b5\u0302 can be found via an exact (or approximate) dynamic programming algorithm. This simplifies the optimisation step significantly, as otherwise it would be necessary to optimise over multiple models. This particular version of the algorithm can be seen as an ABC variant of Thompson sampling (Strens, 2000; Thompson, 1933).\nThe exact algorithm to use for the policy optimisation depends largely upon the class of simulators we have. In principle any type of environment can be handled, as long as a simulation-based approximation method can be used to discover a good policy. In extremis, direct policy search may be used. However, in the work presented in this paper, we limit ourselves to continuous-state Markov decision processes, for which numerous efficient ADP algorithms exist."}, {"heading": "3.1. ABC-LSPI", "text": "Let us consider the class of continuous-state, discreteaction Markov decision processes (MDPs). Then, a number of sample-based ADP algorithms can be used to find good policies, such as fitted Q-iteration (FQI) (Ernst et al., 2005) and least-square policy iteration (LSPI) (Lagoudakis & Parr, 2003b), which we use herein.\nSince we take an arbitrary number of trajectories from the sampled MDP, an important algorithmic parameter is the number of rollouts Nrol to draw. Higher values lead to better approximations, at the expense of additional computation. Finally, since LSPI uses a linear value function5 approximation, it is necessary to select an appropriate basis for the fit to be good.\nThe computational complexity of ABC-LSPI depends on the quality of approximation we wish to achieve and on the number of samples required to sample a model with statistics \u03b5-close to those of the data. To reduce computation, if Nsam models have been generated without one being accepted, we double \u01eb and call ABC-RL-Sample again."}, {"heading": "4. Experiments", "text": "We performed some experiments to investigate the viability of ABC-RL, with all algorithms implemented using (?). In these, we compared ABC-LSPI to LSPI. The intuition is that, if ABC can find a good simulator, then we can perform a much better estimation of the value function by drawing a large number of samples\n5The value function V (s) is simply the expected utility conditioned on the system state s. We omit details as this is not necessary to understand the framework proposed.\nfrom the simulator, rather than estimating the value function directly from the observations."}, {"heading": "4.1. Domains", "text": "We consider two domains to illustrate ABC-RL. In both of these domains, we have access to a set of parametrised simulators M = {\u00b5\u03b8 | \u03b8 \u2208 \u0398 } for the domains. However, we do not know the true parameters \u03b8\u2217 \u2208 \u0398 of the domains. For ABC, sampled parameters \u03b8(k) are drawn from a uniform distribution Unif (\u0398), with \u0398 = { \u03b8 \u2208 Rn \u2223\u2223 \u03b8i \u2208 [ 12\u03b8\u2217i , 32\u03b8\u2217i ] } .\nMountain car This is a generalised version of the mountain car domain described in Sutton & Barto (1998). The goal is to bring a car to the top of a hill. The problem has 7 parameters: upper and lower bounds on the horizontal position of the car, upper and lower bounds on the car\u2019s velocity, maximum acceleration, gravity, and finally the amount of uniform noise present. The real environment parameters are \u03b8\u2217 = (0.5,\u22121.2, 0.07,\u22120.07, 0.001, 0.0025, 0.2). In this problem, the goal is to reach the right-most horizontal position. The observation consists of the horizontal position and velocity and the reward is \u22121 at every step until the goal is reached.\nPendulum This is a generalised version of the pendulum domain (Sutton & Barto, 1998), but without boundaries. The goal of the agent in this environment is to maintain a pendulum upright, using a controller that can switch actions every 0.1s. The problem has 6 parameters: the pendulum mass, the cart mass, the pendulum length, the gravity, the amount of uniform noise, and the simulation time interval. In this envi-\nronment, the reward is +1 for every step where the pendulum is balanced. The actual environment parameters are \u03b8\u2217 = (2.0, 8.0, 0.5, 9.8, 0.01, 0.01)."}, {"heading": "4.2. Results", "text": "We compared the offline performance of LSPI and ABC-LSPI on the two domains. We first observe Ndat trajectories in the real environment drawn using a uniformly random policy. These trajectories are used by both ABC-LSPI and LSPI to estimate a policy. This policy is then evaluated over 103 trajectories. The experiment was repeated for 102 runs. Since LSPI requires a basis, in both cases we employed a uniform 4\u00d7 4 grid of RBFs, as well as an additional unit basis for the value function estimation.\nThe results of the experiment are shown in Fig. 2, where we plot the expected utility (with a discount factor \u03b3 = 0.99) of the policy found as the number of trajectories increase. Both LSPI and ABC-LSPI manage to find an improved policy with more data. However, the source of their improvement is different. In the case of LSPI, the additional data leads to better estimation of the value function. In ABC-LSPI, the additional data leads to a better sampled model. The value function is then estimated using a large number of rollouts in the sampled model. The CPU time taken by ABC ranges in 20 to 40s, versus 0.05 to 30s for pure LSPI, depending on the amount of training data. This is due to the additional overhead of sampling as well as the increased amount of rollouts used for ADP.\nIn general, the ABC approach quickly reaches a good performance, but then has little improvement. This effect is particularly prominent in the Mountain Car do-\nmain (Fig. 2(a)), where it is significantly worse asymptotically than LSPI. This can be attributed to the fact that even though more data is available, the number of samples drawn from the prior is not sufficient for a good model to be found. In fact, upon investigation we noticed that although most model parameters were reliably estimated, there was a difficulty in estimating the goal location from the given trajectories. This was probably the main reason why ABC didn\u2019t reach optimal performance in this case. However, it may be possible to improve upon this result with a more efficient sampling scheme, or a statistic that is closer to sufficiency than the simple utility-based statistic we used.\nOn the other hand, the performance is significantly better than LSPI in the pendulum environment (Fig. 2(b)). There are two possible reason for this. Firstly, ABC-LSPI not only uses more samples for the value function estimation, but also better distributed samples, as it estimates the value function by drawing trajectories starting from uniformly drawn states in the sampled environment. Secondly, and perhaps more importantly, that even for very differently parametrised pendulum problems the optimal policies on the pendulum domain are quite similar. Thus, even if ABC only samples a very approximate simulator, its optimal policy is going to be close to that of the real environment."}, {"heading": "5. Conclusion", "text": "We presented an extension of ABC, a likelihood-free method for approximate Bayesian computation, to controlled dynamical systems. This method is particularly interesting for domains where it is difficult to specify an appropriate probabilistic model, and where computation is significantly cheaper than data collection. It is in principle generally applicable to any type of reinforcement learning problem, including continuous, partially observable and multi-agent domains. We also introduce a general theorem for the quality of the approximate ABC posterior distribution, which can be used for further analysis of ABC methods.\nWe then applied ABC inference to reinforcement learning. This involves using simulation both to estimate approximate posterior distributions and to find good policies. Thus, ABC-RL can be simultaneously seen as an extension of ABC inference to control problems and an extension of approximate dynamic programming methods to likelihood-free approximate Bayesian inference. The main advantage is when have no reasonable probabilistic model, but we do have access to a parametrised set of simulators, which contain good approximations to the real environment. This is frequently the case in complex control problems. However, we see that ABC-RL (specifically ABC-LSPI) is competitive with pure LSPI even in problems with low dimensionality where LSPI is expected to perform quite well.\nABC-RL appears a viable approach, even with a very simple sampling scheme, and a utility-based statistic. In future work, we would like to investigate more elaborate ABC schemes such as Markov chain Monte Carlo, as well as statistics that are closer to sufficient, such as discounted feature expectations and conditional utilities. This would enable us to examine its performance in more complex problems where the\npractical advantages of ABC would be more evident. However, we believe that the results are extremely encouraging and that the ABC methodology has great potential in the field of reinforcement learning."}, {"heading": "A. Collected proofs", "text": "Proof of Remark 2.1. Let h = (xT+1, aT , rT ). Using induction,\nP \u03c0 \u00b5(h) =\nT\u220f\nt=0\n\u00b5t(xt+1)\u03c0t(at).\nReplacing in the posterior calculation (A.1) we obtain:\n\u03be(B | h, \u03c0) =\n\u222b B \u220fT t=0 \u00b5t(xt+1) d\u03be(\u00b5)\u222b\nM\n\u220fT t=0 \u00b5t(xt+1) d\u03be(\u00b5)\n(A.1)\nsince the \u220fT\nt=0 \u03c0t(at) terms can be taken out of the integrals and cancel out.\nProof of Corollary 2.1. By definition, a sufficient statistic f : H \u2192 W has the following property:\n\u2200\u00b5, \u03c0 : P\u03c0\u00b5(h) = P \u03c0 \u00b5(h \u2032) iff f(h) = f(h\u2032). (A.2)\nThe probability of drawing a model in B \u2282 M is:\n\u222b B \u2211 z\u2208H I {f(z) = f(h)}P\n\u03c0 \u00b5(z) d\u03be(\u00b5)\u222b\nM\n\u2211 z\u2208H I {f(z) = f(h)}P \u03c0 \u00b5(z) d\u03be(\u00b5)\n=\n\u222b B P \u03c0 \u00b5(h) d\u03be(\u00b5)\u222b\nM P \u03c0 \u00b5(h) d\u03be(\u00b5)\n= \u03be(B | h, \u03c0), (A.3)\ndue to (A.2).\nProof of Theorem 2.1. For notational simplicity, we introduce \u03c6(\u00b7) = \u222b M P \u03c0 \u00b5(\u00b7) d\u03be(\u00b5) for the marginal prior measure on H, also omitting the dependency on \u03c0. Then the ABC posterior \u03be\u01eb(B | h) equals:\n\u222b B \u2211 z\u2208H I {\u2016f(z)\u2212 f(h)\u2016 < \u01eb}P\n\u03c0 \u00b5(z) d\u03be(\u00b5)\u222b\nM\n\u2211 z\u2208H I {\u2016f(z)\u2212 f(h)\u2016 < \u01eb}P \u03c0 \u00b5(z) d\u03be(\u00b5)\n=\n\u222b B P \u03c0 \u00b5(A\nh \u01eb ) d\u03be(\u00b5)\u222b\nM P \u03c0 \u00b5(A h \u01eb ) d\u03be(\u00b5)\n=\n\u222b B P \u03c0 \u00b5(A h \u01eb ) d\u03be(\u00b5)\n\u03c6(Ah\u01eb ) . (A.4)\nFrom Definition 2.1:\nD (\u03be(\u00b7 | h) \u2016 \u03be\u01eb(\u00b7 | h)) =\n\u222b\nB\nln d\u03be(\u00b5 | h)\nd\u03be\u01eb(\u00b5 | h) d\u03be(\u00b5 | h)\n(a) =\n\u222b\nM\nln\n( P \u03c0 \u00b5(h)\nP \u03c0 \u00b5(A h \u01eb )\n\u00d7 \u03c6(Ah\u01eb )\n\u03c6(h)\n) d\u03be(\u00b5 | h)\n=\n\u222b\nM\n( ln P \u03c0 \u00b5(h)\nP \u03c0 \u00b5(A h \u01eb )\nd\u03be(\u00b5 | h) + ln \u03c6(Ah\u01eb )\n\u03c6(h)\n) d\u03be(\u00b5 | h)\n(b) \u2264\n\u222b\nM\n( ln\nP \u03c0 \u00b5(h)\nminz\u2208Ah \u01eb P \u03c0 \u00b5(z)\n+ ln \u03c6(Ah\u01eb )\n\u03c6(h)\n) d\u03be(\u00b5 | h)\n(c) \u2264\n\u222b\nM\n(\u2223\u2223\u2223\u2223\u2223ln P \u03c0 \u00b5(h)\nminz\u2208Ah \u01eb P \u03c0 \u00b5(z)\n\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223ln \u03c6(Ah\u01eb ) \u03c6(h) \u2223\u2223\u2223\u2223 ) d\u03be(\u00b5 | h)\n(d) \u2264 L\u01eb+ \u2223\u2223\u2223\u2223ln \u03c6(Ah\u01eb )\n\u03c6(h)\n\u2223\u2223\u2223\u2223 (e) \u2264 2L\u01eb+ ln |Ah\u01eb |.\nEquality (a) follows from equations (A.3) and (A.4). Inequality (b) follows from the fact that P\u03c0\u00b5(A h \u01eb ) =\u2211\nz\u2208Ah \u01eb P \u03c0 \u00b5(z) \u2265 minz\u2208Ah\u01eb P \u03c0 \u00b5(z), while (c) follows from |x| \u2265 x. For (d), first note that for any z \u2208 Ah\u01eb , by the definition of Ah\u01eb , | ln[P \u03c0 \u00b5(h)/P \u03c0 \u00b5(z)]| \u2264 L\u01eb, by Assumption 2.1, which can be taken out of the integral. The second | \u00b7 | term in the integral is independent of \u00b5 and so is also taken out. We can then bound the integral using \u222b M\n\u03be(\u00b5 | h) = \u03be(M | h) = 1. For (e), Assumption 2.1 gives that \u03c6(z) = \u222b M P \u03c0 \u00b5(z) d\u03be(\u00b5) \u2264 exp(L\u01eb)\u03c6(h) for any z \u2208 Ah\u01eb so, ln[\u03c6(A h \u01eb )/\u03c6(h)] \u2264 L\u01eb + ln |Ah\u01eb |. Finally, as h \u2208 A h \u01eb , \u03c6(A h \u01eb ) \u2265 \u03c6(h) by additivity of measures, so the | \u00b7 | can be removed, thus obtaining the final result."}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "Shipra", "Goyal", "Navi"], "venue": "COLT", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Near-optimal BRL using optimistic local transitions", "author": ["M. Araya", "V. Thomas", "O Buffet"], "venue": "In ICML,", "citeRegEx": "Araya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Araya et al\\.", "year": 2012}, {"title": "Rollout algorithms for constrained dynamic programming", "author": ["Bertsekas", "Dimitri P"], "venue": "Technical Report LIDS 2646, Dept. of Electrical Engineering and Computer Science,", "citeRegEx": "Bertsekas and P.,? \\Q2006\\E", "shortCiteRegEx": "Bertsekas and P.", "year": 2006}, {"title": "Using linear programming for Bayesian exploration in Markov decision processes", "author": ["Castro", "Pablo Samuel", "Precup", "Doina"], "venue": null, "citeRegEx": "Castro et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2007}, {"title": "Approximate Bayesian computation (ABC) in practice", "author": ["K. Csill\u00e9ry", "M.G.B. Blum", "O.E. Gaggiotti", "O Fran\u00e7ois"], "venue": "Trends in ecology & evolution,", "citeRegEx": "Csill\u00e9ry et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Csill\u00e9ry et al\\.", "year": 2010}, {"title": "Asymptotic behaviour of approximate bayesian estimators", "author": ["Dean", "Thomas A", "Singh", "Sumeetpal S"], "venue": "arXiv preprint arXiv:1105.3655,", "citeRegEx": "Dean et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2011}, {"title": "Optimal Statistical Decisions", "author": ["DeGroot", "Morris H"], "venue": null, "citeRegEx": "DeGroot and H.,? \\Q1970\\E", "shortCiteRegEx": "DeGroot and H.", "year": 1970}, {"title": "Robust bayesian reinforcement learning through tight lower bounds", "author": ["Dimitrakakis", "Christos"], "venue": "In European Workshop on Reinforcement Learning (EWRL 2011),", "citeRegEx": "Dimitrakakis and Christos.,? \\Q2011\\E", "shortCiteRegEx": "Dimitrakakis and Christos.", "year": 2011}, {"title": "Rollout sampling approximate policy iteration", "author": ["Dimitrakakis", "Christos", "Lagoudakis", "Michail G"], "venue": "Machine Learning,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2008}, {"title": "Optimal Learning Computational Procedures for Bayes-adaptive Markov Decision Processes", "author": ["Duff", "Michael O\u2019Gordon"], "venue": "PhD thesis, University of Massachusetts at Amherst,", "citeRegEx": "Duff and O.Gordon.,? \\Q2002\\E", "shortCiteRegEx": "Duff and O.Gordon.", "year": 2002}, {"title": "Differential privacy and robust statistics", "author": ["Dwork", "Cynthia", "Lei", "Jing"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Ernst", "Damien", "Geurts", "Pierre", "Wehenkel", "Louis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Classificationbased policy iteration with a critic", "author": ["Gabillon", "Victor", "Lazaric", "Alessandro", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "ICML", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Using simulation methods for Bayesian econometric models: inference, development, and communication", "author": ["J. Geweke"], "venue": "Econometric Reviews,", "citeRegEx": "Geweke,? \\Q1999\\E", "shortCiteRegEx": "Geweke", "year": 1999}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Filtering via approximate bayesian computation", "author": ["Jasra", "Ajay", "Singh", "Sumeetpal S", "Martin", "James S", "McCoy", "Emma"], "venue": "Stat. Comput,", "citeRegEx": "Jasra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jasra et al\\.", "year": 2010}, {"title": "Thompson sampling: An optimal finite time analysis", "author": ["Kaufmanna", "Emilie", "Korda", "Nathaniel", "Munos", "R\u00e9mi"], "venue": "In ALT-2012,", "citeRegEx": "Kaufmanna et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmanna et al\\.", "year": 2012}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "ICML", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In ICML, pp", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Approximate Bayesian computational methods", "author": ["J.M. Marin", "P. Pudlo", "C.P. Robert", "R.J. Ryder"], "venue": "Statistics and Computing,", "citeRegEx": "Marin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Marin et al\\.", "year": 2011}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "ICML", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Model-based Bayesian reinforcement learning in partially observable domains", "author": ["Poupart", "Pascal", "Vlassis", "Nikos"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "Poupart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2008}, {"title": "Bayes-adaptive POMDPs", "author": ["Ross", "Stephane", "Chaib-draa", "Brahim", "Pineau", "Joelle"], "venue": "Advances in Neural Information Processing Systems 20,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Strens", "Malcolm"], "venue": "ICML", "citeRegEx": "Strens and Malcolm.,? \\Q2000\\E", "shortCiteRegEx": "Strens and Malcolm.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of two Samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems", "author": ["T. Toni", "D. Welch", "N. Strelkowa", "A. Ipsen", "M.P.H. Stumpf"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Toni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Toni et al\\.", "year": 2009}, {"title": "Reinforcement Learning, chapter Bayesian Reinforcement Learning, pp. 359\u2013386", "author": ["N. Vlassis", "M. Ghavamzadeh", "S. Mannor", "P. Poupart"], "venue": null, "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Rollout sampling policy iteration for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Chen", "Xiaoping"], "venue": "In The 26th conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Bayesian reinforcement learning (Strens, 2000; Vlassis et al., 2012) is the decision-theoretic approach (DeGroot, 1970) to solving the reinforcement learning problem.", "startOffset": 32, "endOffset": 68}, {"referenceID": 23, "context": "However, apart from the fact that calculating posterior distributions and the Bayesoptimal decision is frequently intractable (Duff, 2002; Ross et al., 2008), another major difficulty is the specification of the prior and model class.", "startOffset": 126, "endOffset": 157}, {"referenceID": 4, "context": "We propose a simple, general, reinforcement learning framework employing the principles of Approximate Bayesian Computation (ABC, see (Csill\u00e9ry et al., 2010) for an overview) for performing Bayesian inference using simulation.", "startOffset": 134, "endOffset": 157}, {"referenceID": 26, "context": "On the other hand, simple heuristics such as Thompson sampling (Strens, 2000; Thompson, 1933) provide an efficient trade-off (Agrawal & Goyal, 2012; Kaufmanna et al.", "startOffset": 63, "endOffset": 93}, {"referenceID": 16, "context": "On the other hand, simple heuristics such as Thompson sampling (Strens, 2000; Thompson, 1933) provide an efficient trade-off (Agrawal & Goyal, 2012; Kaufmanna et al., 2012) between exploration and exploitation.", "startOffset": 125, "endOffset": 172}, {"referenceID": 1, "context": "Alghough other heuristics exist (Araya et al., 2012; Castro & Precup, 2007; Kolter & Ng, 2009; Poupart et al., 2006; Strens, 2000), in this paper we focus on an approximate version of Thompson sampling for reasons of simplicity.", "startOffset": 32, "endOffset": 130}, {"referenceID": 21, "context": "Alghough other heuristics exist (Araya et al., 2012; Castro & Precup, 2007; Kolter & Ng, 2009; Poupart et al., 2006; Strens, 2000), in this paper we focus on an approximate version of Thompson sampling for reasons of simplicity.", "startOffset": 32, "endOffset": 130}, {"referenceID": 23, "context": "The second difficulty is that in many interesting problems, the exact posterior calculation may be intractable, mainly due to partial observability (Poupart & Vlassis, 2008; Ross et al., 2008).", "startOffset": 148, "endOffset": 192}, {"referenceID": 12, "context": "Methods for finding good policies using simulation have been extensively studied before (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Gabillon et al., 2011; Wu et al., 2010).", "startOffset": 88, "endOffset": 208}, {"referenceID": 29, "context": "Methods for finding good policies using simulation have been extensively studied before (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Gabillon et al., 2011; Wu et al., 2010).", "startOffset": 88, "endOffset": 208}, {"referenceID": 15, "context": "As far as we know, this is a new and widely applicable result, although some other theoretical results using similar assumptions appear in (Jasra et al., 2010) and in (Dean & Singh, 2011) for hidden Markov models.", "startOffset": 139, "endOffset": 159}, {"referenceID": 26, "context": "This particular version of the algorithm can be seen as an ABC variant of Thompson sampling (Strens, 2000; Thompson, 1933).", "startOffset": 92, "endOffset": 122}, {"referenceID": 11, "context": "Then, a number of sample-based ADP algorithms can be used to find good policies, such as fitted Q-iteration (FQI) (Ernst et al., 2005) and least-square policy iteration (LSPI) (Lagoudakis & Parr, 2003b), which we use herein.", "startOffset": 114, "endOffset": 134}], "year": 2013, "abstractText": "We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.", "creator": "LaTeX with hyperref package"}}}