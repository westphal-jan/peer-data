{"id": "1305.1679", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2013", "title": "High Level Pattern Classification via Tourist Walks in Networks", "abstract": "Complex networks refer to large-format graphics with non-trivial connecting patterns. The prominent and interesting features that the complex network study offers in comparison to graph theory are the emphasis on the dynamic properties of the networks and the ability to naturally uncover the pattern formation of the vertices. In this paper, we present a technique for classifying hybrid data that combines a low and a high classifier. The low term can be equipped with any traditional classification techniques that realize the classification task by taking only physical characteristics (e.g. geometric or statistical characteristics) of the input data into account. However, the high term has the ability to recognize data patterns with semantic meanings. In this way, classification is realized by extracting the underlying network characteristics constructed from the input data. Consequently, the high-level classification process measures the coincidence of test patterns with the high-level dynamics we can already use to build the different pattern dynamics from.", "histories": [["v1", "Tue, 7 May 2013 23:40:08 GMT  (1727kb,D)", "http://arxiv.org/abs/1305.1679v1", "Submitted to the IEEE Transactions on Neural Networks and Learning Systems"]], "COMMENTS": "Submitted to the IEEE Transactions on Neural Networks and Learning Systems", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["thiago christiano silva", "liang zhao"], "accepted": false, "id": "1305.1679"}, "pdf": {"name": "1305.1679.pdf", "metadata": {"source": "CRF", "title": "High Level Pattern Classification via Tourist Walks in Networks", "authors": ["Thiago Christiano Silva"], "emails": ["zhao}@icmc.usp.br."], "sections": [{"heading": null, "text": "Index Terms\u2014High level classification, tourist walks, supervised learning, complex networks.\nI. INTRODUCTION\nSUPERVISED data classification aims at generating amap from the input data to the corresponding desired output, for a given training set. The constructed map, called a classifier, is used to predict new input instances. Many supervised data classification techniques have been developed [1]\u2013[3], such as k-nearest neighbors, Bayesian classifiers, neural networks, decision trees, committee machines, and so on. In essence, all these techniques train and, consequently, classify unlabeled data items according to the physical features\nThiago Christiano Silva and Liang Zhao are with the Department of Computer Sciences, Institute of Mathematics and Computer Science (ICMC), University of Sa\u0303o Paulo (USP), Av. Trabalhador Sa\u0303o-carlense, 400, 13560- 970, Sa\u0303o Carlos, SP, Brazil. E-mail: {thiagoch, zhao}@icmc.usp.br.\n(e.g., distance, similarity or distribution) of the input data. These techniques that predict class labels using only physical features are called low level classification techniques [4].\nUsually, the data items are not isolated points in the attribute space, but instead tend to form certain patterns. For example, in Fig. 1, the two test instances represented by the triangleshaped are most probably to be classified as pertaining to the square-shaped class if only physical features, such as distances among data instances, are considered. On the other hand, if we take into account the relationships among the data, we intuitively classify the triangle-shaped items as members of the circular-shaped class, since a clear pattern (lozenge) is formed. The human (animal) brain performs both low and high orders of learning and it has facility to identify patterns according to the semantic meaning of the input data. However, this kind of task, in general, is still hard to be performed by computers. Supervised data classification by considering not only physical attributes but also pattern formation is referred to as high level classification [4].\nTraditional classification techniques often share the same vision: dividing the data space into sub-spaces, each of which representing a class. They are short in reproducing complexformed or twisted classes. On the other hand, the salient feature of the proposed technique is that it provides two really distinct orthogonal visions: low level and high level visions for data classification. The former views the data\u2019s classes from the perspective of their physical features, while the latter captures pattern formations of the data, which, in turn, permits the classifier to reproduce complex-formed and (or) twisted classes, i.e., a test instance can be put into a class if it conforms with the pattern formed by that class no matter how far it is from the center of that class. In this sense, strongly related techniques are co-training [5] and tritraining [6], which attempt to consider the cooperation of various classification techniques (ensemble), each focusing on a theoretically different \u201cvision\u201d of the data. However,\nar X\niv :1\n30 5.\n16 79\nv1 [\ncs .A\nI] 7\nM ay\n2 01\n3\nthe involved classification techniques in co-training or tritraining are really only making statistically independent decisions, which still share, in essence, the same data vision. Another difficulty in these techniques resides in building these statistically independent visions. Since it is supposed that the data is generated from an unknown distribution, it is often hard to achieve such task. In our proposed work, the uncorrelated visions are naturally captured by the classifiers themselves, not by changing the content of the data items, but by \u201clooking\u201d at the data relationships in a different way (low and high levels).\nFollowing the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.\nFrom the viewpoint of high level classification, all the above mentioned approaches are quite restricted either to the types of semantic features to be extracted, such as Semantic Web, or to the types of data, such as the contextual classification, which is devoted to considering spatial relationship among pixels in image processing. To our knowledge, it is still lacking an explicit and general scheme to deal with high level classification in the literature, which is quite desirable for many applications, such as invariant pattern recognition. The current paper presents an endower to this direction.\nAs mentioned, low level classification usually presents difficulty in identifying the complex relationships among the data items. Consequently, these techniques are not suitable for uncovering semantically meaningful patterns formed by the data. This is because the data patterns are often not encountered with a fixed shape or distribution, instead, they are frequently determined by the local and/or global interactions among the data items. It is well know that the network representation can capture arbitrary levels of relationships or interactions of the input data. For this reason, we here show how the topological properties of the input data can help in identifying the pattern formation and, consequently, can be used for general high level classification. In this case, the topological properties are revealed by tourist walks. A tourist walk can be defined as follows: Given a set of cities, each time the tourist (walker) goes to the nearest city that has not been visited in the past \u00b5 time steps [24]. It has been shown that tourist walk is useful for data clustering [25] and image processing [26]. However, all these kinds of works are realized in regular lattices. Here, we study tourist walk in networks and we show that it has the ability of capturing the topological properties of the underlying network in a local to global fashion. Moreover, the tourist walks approach applied to a networked environment is a relatively new approach taken here. Additionally, its utilization for discovering patterns in a network is a totally novel scheme in the literature.\nIn this paper, we propose a technique that combines the low level and the high level supervised data classifications. The\nidea of this paper is built upon the general framework proposed by [4]. The low level classification can be implemented by any traditional classification technique, while the high level classification exploits the complex topological properties of the underlying network constructed from the input data. In the original work introduced in [4], the high level classification problem is treated using three existing network measures in a combined way (assortativity, clustering coefficient, and average degree). Thus, a serious open problem is how one may choose other network measures in an intuitive way and also how to define the inference weight that is given for each of them. In this paper, a novel measure for high level classification is introduced by using tourist walks in networks. Since they are a deterministic dynamical process, the local and global information of the underlying network can be detected by the exclusive use of such measure. Moreover, the use of tourist walks presents some nontrivial advantages over the previous approach, such as:\n\u2022 It is able to capture the organizational and complex features of the class component from a local to global fashion in a natural and intuitive way. For example, when the memory window of the tourist is low, it is able to extract local features of the graph component. As we increase the memory window, the dynamics of the walk compels the tourist to venture far away from its starting vertex. Hence, it is able to capture more global features of the graph component; \u2022 It occurs that the tourist walk method presents a critical memory length, where any values larger than this critical point make no change in the transient and cycle lengths of the graph component. This is an interesting phenomenon, which is observed when the memory length reaches a sufficient high value. We say that, when this happen, the walks have reached the \u201ccomplexity saturation\u201d of the class component. In this occasion, the global topological and organizational features of the network are said to be completely characterized in the sense of tourist walks; \u2022 In view of the intuitive dynamical properties displayed by a tourist walk, one can avoid the weight assignment among various network measures, which is a problem when static network measures are used, as occurs in [4]. This is because static measures provide partial or instant visions of the underlying network. Thus, one must artificially combine them to get a global vision.\nStill in this paper, we show how the proposed technique can be used to solve general invariant pattern recognition problems [27]\u2013[29], particularly when the pattern variances are nonlinear and there is not a closed form to describe the invariance.\nThe remainder of the paper is organized as follows. A detailed overview of the tourist walks is supplied in Section II. The proposed model is defined in Section III. Computer simulations are performed on synthetic and real-world data sets in Section IV. In Section V, we apply the proposed technique to manual digits recognition problem. Finally, Section VI concludes the paper."}, {"heading": "II. RELEVANT BACKGROUND: TOURIST WALKS", "text": "A tourist walk can be can conceptualized as a walker (tourist) aiming at visiting sites (data items) in a d-dimensional map, representing the data set. At each discrete time step, the tourist follows a simple deterministic rule: it visits the nearest site which has not been visited in the previous \u00b5 steps. In other words, the walker performs partially self-avoiding deterministic walks over the data set, where the self-avoiding factor is limited to the memory window \u00b5\u2212 1. This quantity can be understood as a repulsive force emanating from the sites in this memory window, which prevents the walker from visiting them in this interval (refractory time). Therefore, it is prohibited that a trajectory to intersect itself inside this memory window. In spite of being a simple rule, it has been shown that this movement dynamic possesses complex behavior when \u00b5 > 1 [24].\nThe tourist\u2019s behavior heavily depends on the data set\u2019s configuration and the starting site. In computational terms, the tourist\u2019s movements are entirely realized by means of a neighborhood table. This table is constructed by ordering all the data items in relation to a specific site. This procedure is performed for every site of the data set.\nEach tourist walk can be decomposed in two terms: (i) the initial transient part of length t and (ii) a cycle (attractor) with period c. Figure 2 shows an illustration of a tourist walk with \u00b5 = 1, i.e., the walker always goes to the nearest neighbor. In this case, one can see that the transient length is t = 3 and the cycle length c = 6.\nConsidering the attractor or cycle period as a walk section that begins and ends at the same site of the data set may lead one to think that, once the tourist visits a specific site, a new visit to it would configure an attractor. However, during a walk, a site may be re-visited without configuring an attractor. For instance, if we had chosen a \u00b5 = 6 for the walk in Fig. 2, the re-visit that the tourist performs on the site 4 would have not configured an attractor, since the site 5 would still be forbidden site to be visited again; hence, the tourist would be compelled to visit another site. This characteristic enables sophisticated trajectories over the data set, at cost of also increasing the difficulty of detecting an attractor.\nA note that is worth pointing out is that, in the majority of the works related to these walks [24], [30], [31], the tourist may visit any other site other than the ones contained in its memory window. As \u00b5 increases, there is a significant chance that the walker will begin performing large jumps in the data set, since the neighborhood is most likely to be already visited in its entirety within the time frame \u00b5. As we will show in this work, in the context of classification, this is an undesirable characteristic that can be simply avoided by using a graph representation of the input data. In this way, the walker is only permitted to visit vertices, represented now by the sites, that are in its connected neighborhood (link). With this modified mechanism, it is most probable that, for large values of \u00b5, depending on the network configuration, the walker will get trapped within a vertex, not being able to further visit other vertices of the neighborhood. In this scenario, we say that the walk only had a transient part and the cycle period is\nnull (c = 0). Therefore, the tourist walks approach applied to a networked environment is a relatively new approach taken here. Additionally, its utilization for discovering patterns in a network is a totally novel scheme in the literature."}, {"heading": "III. MODEL DESCRIPTION", "text": "In this section, a useful set of notations is presented, along with the premises of the underlying hybrid classification framework. Next, we give a quick overview on the particularities of the original hybrid classification framework [4]. Finally, the high level classification based on tourist walks is formally introduced, as well as an overview of its algorithm."}, {"heading": "A. Notations and Premises", "text": "The hybrid classifier is designed to work in a supervised learning environment. In the following, some mathematical notations and premises are discussed.\nConsider that Xtraining = {(x1, y1), . . . , (xl, yl)} \u2282 X \u00d7 L denotes the training set, which is composed of l labeled training instances. Each training instance, xi \u2208 X , is given a discrete label or target yi \u2208 L. Furthermore, each training instance is described by a d-dimensional vector, i.e., xi = (f1, . . . , fd), where each entry symbolizes a feature or descriptor of that item.\nThe goal here is to construct a hypothesis, in a way that the classifier maps x 7\u2192 y. Commonly, the constructed classifier is checked with regard to its prediction power by submitting it to a test set Xtest = {xl+1, . . . , xl+u}, in which labels are not provided. In this case, each data item is called test instance. For an unbiased learning, the training and test sets must be disjoint, i.e., Xtraining \u2229 Xtest = \u2205."}, {"heading": "B. Overview of the Hybrid Classification Framework", "text": "In this section, we review the hybrid classification framework [4]. Specifically, in the following sections, the particularities of the training and classification phases, and the general definition of the classification scheme are discussed.\n1) Training Phase: In this phase, the data in the training set are mapped into a graph G using a network formation technique g : Xtraining 7\u2192 G = \u3008V, E\u3009, where V = {1, . . . , V } is the set of vertices and E is the set of edges. Each vertex in V represents a training instance in Xtraining. As it will be described later, the pattern formation of the classes will be\nextracted by using the complex topological features of this networked representation. Therefore, the network construction is vital for the prediction produced by the high level classifier.\nIn this stage, we first construct a network component for each class of the vector-based training set. The strategy to create edges depends on the type of region in which each vertex is. When the region is sparse, we utilize a k-nearest neighbor (k-NN) approach, whereas, when it is dense, we employ the -radius technique. While the k-NN sets up an edge between the k most similar vertices and the reference vertex, the -radius method creates a link to whichever vertex that is within a predefined distance with radius . The way that we classify a region as dense or sparse is by checking whether, within a circular region centered in the reference vertex, there are more than k vertices of the same class as the reference vertex. If so, the region is classified as dense and the -radius is selected; otherwise, the k-NN is elected. During the network formation, we are only permitted to create edges between vertices of the same class. The reason why we employ a k-NN in sparse region is to prevent the appearance of multiple graph components representing the same class. Therefore, we expect that this process will form isolated class components, in a way that each class is guaranteed to have a single and unique component representing it.\n2) Classification Phase: In the classification phase, the unlabeled data items in the Xtest are presented to the classifier one by one. In contrast to the training phase, the class labels of the test instances are unknown. In view of that, we keep utilizing the edge formation strategy previously introduced but with slight changes: now, we do not consider the labels of the neighboring vertices. All the other technicalities remain the same. This prevents test instances from becoming singleton vertices, provided that k > 0.\nWith respect to the high order of learning, once the data item is inserted, each class analyzes, in isolation, the impact of the insertion of this data item on its respective class component by using a number of complex topological features. In the proposed high level model, each class retains an isolated graph component. Each of these components calculates the changes that occur in its pattern formation with the insertion of this test instance. If slight or no changes occur, then it is said that the test instance is in compliance with that class pattern. As a result, the high level classifier yields a great membership value for that test instance on that class. Conversely, if these changes dramatically modify the class pattern, then the high level classifier produces a small membership value on that class. These changes are quantified via network measures, each of which numerically translating the organization of the component from a local to global fashion. As we will see, we will extract information generated from the dynamical process of a tourist walker in a networked environment in an intuitive manner, such as to capture the pattern formation of the network in a local to global basis.\n3) General Hybrid Classification Framework: Tthe hybrid classification framework F consists of a convex combination of two orthogonal terms, where each of which renders an uncorrelated vision about the data items, as follows:\ni. A low level classifier, for instance, a decision tree, SVM,\nor a k-NN classifier. The vision that it stresses is the physical similarities among the data; ii. A high level classifier, which is responsible for classifying a test instance according to its organizational or semantic meaning with the data. The vision that it values most is the pattern compliance of new test instances with the existing structure built up in the training process.\nMathematically, the membership of the test instance xi \u2208 Xtest with respect to the class j \u2208 L yielded by the hybrid framework, here written as F (j)i , is given by:\nF (j) i = (1\u2212 \u03bb)L (j) i + \u03bbH (j) i , (1)\nwhere L(j)i , H (j) i \u2208 [0, 1] represent the membership of the test instance xi towards class j produced by the low and high level classifiers, respectively, and \u03bb \u2208 [0, 1] is the compliance term, which plays the role of counterbalancing the classification decisions supplied by both low and high level classifiers. Note that, when \u03c1 = 0, (1) reduces to a common low level classifier.\nA test instance xi receives the label of the class j \u2208 L that maximizes (1). Mathematically, the estimated label of xi, y\u0302xi , is decided according to the following expression:\ny\u0302xi = arg max j\u2208L\nF (j) i . (2)\nNote that the predictions produced by both classifiers are combined via a linear combination to derive the prediction of the high level framework (meta-learning). Once the test instance xi gets classified, it is either discarded or incorporated to the training set with the corresponding predicted label. In the second case, only the edges created between the test instance and the class that it belongs to are maintained. Note that, in any of the two situations, each class is still represented by a single graph component."}, {"heading": "C. Deriving the High Level Classification Technique Using Tourist Walks", "text": "Equation (1) supplies a general framework for the hybrid classification process, in the sense that various supervised data classification techniques can be brought into play. The first term of (1) is rather straightforward to implement, since it can be any traditional classification technique. The literature provides a myriad of supervised data classification techniques. Some of these include graph-based methods, decision trees, SVM and its variations, neural networks, Bayesian learning, among many others. However, to our knowledge, little has been done in the area of classifiers that take into account the patterns or organizational features inherently hidden into the relationships among the data items. Thus, we now proceed to a detailed analysis of the proposed high level classification term H .\nMotivated by the intrinsic ability of describing topological structures among the data items, we propose a networkbased (graph-based) technique for the high level classifier H . Specifically, the inference of pattern formation within the data is processed using the generated network. In order to do so,\nthe following structural constraints must be satisfied for any constructed network:\ni. Each class is an isolated subgraph (graph component); ii. Each class retains a representative and unique graph\ncomponent. As we have drawn attention to, the pattern formation of the data is quantified through a combination of network measures generated by a tourist walker in a networked environment. These measures are chosen in a way to cover relevant high level aspects of the class component. One can conceive these dynamical values as true network measures representing each class component. Having in mind the basic concepts revolving around tourist walks, the decision output of the high level classifier is given by:\nH (j) i =\n\u2211\u00b5c \u00b5=0 [ \u03b1t(1\u2212 T (j)i (\u00b5)) + \u03b1c(1\u2212 C (j) i (\u00b5)) ] \u2211 g\u2208L \u2211\u00b5c \u00b5=0 [ \u03b1t(1\u2212 T (g)i (\u00b5)) + \u03b1c(1\u2212 C (g) i (\u00b5))\n] (3)\nwhere \u00b5c is a critical value that indicates the maximum memory length of the tourist walks, \u03b1t, \u03b1c \u2208 [0, 1] are usercontrollable coefficients that indicate the influence of each network measure in the process of classification, T (j)i (\u00b5) and C\n(j) i (\u00b5) are functions that depend on the transient and cycle lengths, respectively, of the tourist walk applied to the ith data item with regard to the class j. These functions are responsible for providing an estimative whether or not the data item i under analysis possesses the same patterns of component j. The denominator in (3) has been introduced solely for normalization matters. Indeed, in order to (3) to be a valid convex combination of network measures, \u03b1t and \u03b1c must be chosen such as to satisfy \u03b1t + \u03b1c = 1.\nRegarding T (j)i (\u00b5) and C (j) i (\u00b5), they are given by the\nfollowing expressions:\nT (j) i (\u00b5) = \u2206t (j) i (\u00b5)p (j) C (j) i (\u00b5) = \u2206c (j) i (\u00b5)p (j) (4)\nwhere \u2206t(j)i (u),\u2206c (j) i (u) \u2208 [0, 1] are the variations of the transient and cycle lengths that occur on the component representing class j if i joins it and p(j) \u2208 [0, 1] is the proportion of data items pertaining to class j. Remembering that each class has a component representing itself, the strategy to check the pattern compliance of a test instance is to examine whether its insertion causes a great variation of the network measures representing the class component. In other words, if there is a small change in the network measures, the test instance is in compliance with all the other data items that comprise that class component, i.e., it follows the same pattern as the original members of that class. On the other hand, if its insertion is responsible for a significant variation of the component\u2019s network measures, then probably the test instance may not belong to that class. This is exactly the behavior that (3) together with (4) propose, since a small variation of f(u) causes a large membership value output by H; and vice versa.\nIn the following, we explain how to compute \u2206t(j)i (\u00b5) and \u2206c\n(j) i (\u00b5) that appear in (4). Firstly, we need to numerically quantify the transient and cycle lengths of a component. Since the tourist walks are strongly dependent on the starting vertices, for a fixed \u00b5, we perform tourist walks initiating from each one of the vertices that are members of a class component. The transient and cycle lengths of the jth component, \u3008t(j)i (\u00b5)\u3009 and \u3008c (j) i (\u00b5)\u3009, are simply given by the average transient and cycle lengths of all its vertices, respectively. In order to estimate the variation of the component\u2019s network measures, consider that xi \u2208 Xtest is a test instance. In relation to an arbitrary class j, we virtually insert xi into component j using the network formation technique that we have seen, and recalculate the new average transient and cycle lengths of this component. We denote these new values as \u3008t\u2032(j)i (\u00b5)\u3009 and \u3008c\u2032(j)i (\u00b5)\u3009, respectively. This procedure is performed for all classes j \u2208 L. It may occur that some classes u \u2208 L will not share any connections with the test instance xi. Using this approach, \u3008t(k)i (\u00b5)\u3009 = \u3008t\u2032 (k) i (\u00b5)\u3009 and \u3008c (k) i (\u00b5)\u3009 = \u3008c\u2032 (k) i (\u00b5)\u3009, which is undesirable, since this configuration would state that xi complies perfectly with class u. In order to overcome this problem, a simple post-processing is necessary: For all components u \u2208 L that do not share at least 1 link with xi, we deliberately set \u3008t\u2032(j)i (\u00b5)\u3009 and \u3008c\u2032 (j) i (\u00b5)\u3009 to a high value. This high value must be greater than the largest variation that occurs in a component which shares a link with the data item under analysis. One may interpret this post-processing as a way to state that xi does not share any pattern formation with class u, since it is not even connected to it.\nWith all this information at hand, we are able to calculate \u2206t\n(j) i (\u00b5) and \u2206c (j) i (\u00b5),\u2200j \u2208 L, as follows:\n\u2206t (j) i (\u00b5) = |\u3008t\u2032(j)i (\u00b5)\u3009 \u2212 \u3008t (j) i (\u00b5)\u3009|\u2211\nu\u2208L |\u3008t\u2032 (u) i (\u00b5)\u3009 \u2212 \u3008t (u) i (\u00b5)\u3009|\n\u2206c (j) i (\u00b5) = |\u3008c\u2032(j)i (\u00b5)\u3009 \u2212 \u3008c (j) i (\u00b5)\u3009|\u2211\nu\u2208L |\u3008c\u2032 (u) i (\u00b5)\u3009 \u2212 \u3008c (u) i (\u00b5)\u3009|\n(5)\nwhere the denominator is introduced only for normalization matters. According to (5), for insertions that result in a considerable variation of the component\u2019s transient and cycle lengths, \u2206t(j)i (\u00b5) and \u2206c (j) i (\u00b5) will be high. In view of (4), T (j) i (\u00b5) and C (j) i (\u00b5) are expected to be also high, yielding a low membership value predicted by the high level classifier H\n(j) i , as (3) reveals. On the other hand, for insertions that do not significantly interfere in the pattern formation of the data, \u2206t\n(j) i (\u00b5) and \u2206c (j) i (\u00b5) will be low, and, as a result, T (j) i (\u00b5) and C(j)i (\u00b5) are expected to be also low, producing a high membership value for the high level classifier H(j)i , as (3) exposes.\nThe network-based high level classifier quantifies the variations of the transient and cycle lengths of tourist walks with limited memory \u00b5 that occur in the class components when a test instance artificially joins each of them in isolation. According to (3), this procedure is performed for several values of the memory length \u00b5, ranging from 0 (memoryless) to a critical value \u00b5c. This is done in order to capture complex\npatterns of each of the representative class components in a local to global fashion. When \u00b5 is small, the walks tend to possess a small transient and cycle parts, so that the walker does not wander far away from the starting vertex. In this way, the walking mechanism is responsible for capturing the local structures of the class component. On the other hand, when \u00b5 increases, the walker is compelled to venture deep into the component, possibly very far away from its starting vertex. In this case, the walking process is responsible for capturing the global features of the component. In sum, the fundamental idea of the high level classifier is to make use of a mixture of local and global features of the class components by means of a combination of tourist walks with different values of \u00b5.\nFinally, we intuitively explain the role of p(j) \u2208 [0, 1] in (4), i.e., the relative size of each component in the graph. In realworld databases, unbalanced classes are usually encountered. In general, a database frequently encompasses several classes of different sizes. A great portion of the network measures is very sensitive to the size of the components. In an attempt to soften this problem, we introduce in (4) the term p(j), which is the proportion of vertices that class j has. Mathematically, it is given by:\np(j) = 1\nV V\u2211 u=1 1{yu=j}, (6)\nwhere V is the number of vertices and 1{.} is the indicator function that yields 1 if the argument is logically true, or 0, otherwise. In view of the introduction of this mechanism, we expect to obviate the effects of unbalanced classes in the classification process."}, {"heading": "D. Algorithm", "text": "For didactic purposes, Algorithm 1 enumerates the sequence of steps needed to perform a high level classification of a single test instance, according to the rules that we have described in this section.\nAlgorithm 1: An overview of the high level classification procedure based on tourist walks for a single test instance.\n1) Construct a set of network components G = {G1, . . . ,GL}, for each class from the vector-based training set by using the combined k-NN and rules from the training phase; 2) Calculate the average transient and cycle lengths for each network\u2019s component, representing a class of training data; 3) Insert a test instance xi into the formed graph by using the k-NN and rules from the classification phase; 4) Calculate the new average transient and cycle lengths for each network\u2019s component that received at least one link from the test instance; 5) Calculate the transient and cycle length variations by using Eq. (4); 6) Produce the high level decision value by using Eq. (3)."}, {"heading": "E. Linking the High Level Classifier based on Tourist Walks and the General Framework introduced in [4]", "text": "One may wonder how the high level classifier based on tourist walks given by (3) is plugged into the general framework for high level classification introduced in [4]. The following proposition links both approaches.\nProposition 1. The high level classifier based on tourist walks, whose decision equations are:\nH (j) i =\n\u2211\u00b5c \u00b5=0 [ \u03b1t(1\u2212 T (j)i (\u00b5)) + \u03b1c(1\u2212 C (j) i (\u00b5)) ] \u2211 g\u2208L \u2211\u00b5c \u00b5=0 [ \u03b1t(1\u2212 T (g)i (\u00b5)) + \u03b1c(1\u2212 C (g) i (\u00b5))\n] , (7)\nT (j) i (\u00b5) = \u2206t (j) i (\u00b5)p (j)\nC (j) i (\u00b5) = \u2206c (j) i (\u00b5)p\n(j), (8)\nis a particular form of the generic high level framework given in [4, Eqs. (5) and (7)]:\nH (j) i =\n\u2211m u=1 \u03b1(u) [ 1\u2212 f (j)i (u) ] \u2211 g\u2208L \u2211m u=1 \u03b1(u) [ 1\u2212 f (g)i (u)\n] , (9) f (j) i (u) = \u2206G (j) i (u)p (j). (10)\nProof: First, one can see that (7) can be written as:\nH (j) i =\n\u22112\u00b5c+2 \u00b5=1 v(\u00b5)(1\u2212 V\n(j) i (\u00b5))\u2211 g\u2208L \u22112\u00b5c+2 \u00b5=1 v(\u00b5)(1\u2212 V (g) i (\u00b5)) , (11)\nwhere:\nv(\u00b5) = { \u03b1t if \u00b5 \u2264 \u00b5c + 1 \u03b1c if \u00b5 > \u00b5c + 1 , (12)\nand\nV (j) i (\u00b5) =  T (j) i (\u00b5) if \u00b5 \u2264 \u00b5c + 1 C\n(j) i (\u00b5) if \u00b5 > \u00b5c + 1\n, (13)\nare piecewise functions defined over the domain {1, . . . , 2\u00b5c+ 2}. In view of [4, Eq. (6)], one must have that:\n2\u00b5c+2\u2211 \u00b5=1 v(\u00b5) = 1\u21d2 (\u00b5c + 1)(\u03b1t + \u03b1c) = 1. (14)\nMoreover, by equivalence, comparing the upper limits of the summations of (9) and (11), one has that:\nm = 2\u00b5c + 2. (15)\nUsing [4, Eq. (6)] on (14), one has:\n(\u00b5c + 1)(\u03b1t + \u03b1c) = 2\u00b5c+2\u2211 \u00b5=1 v(\u00b5) = 1 = m\u2211 u=1 \u03b1(u) = 2\u00b5c+2\u2211 u=1 \u03b1(u).\n(16)\nTherefore, by polynomial equivalence, one can take:\n\u03b1(u) = v(u) = { \u03b1t if u \u2264 \u00b5c + 1 \u03b1c if u > \u00b5c + 1 . (17)\nUsing the same reasoning, we have that:\nf (j) i (u) = V (j) i (u) =  T (j) i (u) if u \u2264 \u00b5c + 1 C\n(j) i (u) if u > \u00b5c + 1\n. (18)\nFinally, by looking at (8) and (10), we can infer that the tourist walk based classifier can be coupled into the hybrid framework by taking 2\u00b5c+ 2 network measures, such that the first \u00b5c + 1 are the transient lengths with increasing memory lengths, each of which weighted by \u03b1t, and the remaining \u00b5c+ 1 are cycle lengths with increasing memory lengths, each of which weighted by \u03b1c. In view of this, the high level classifier based on tourist walks is, in fact, a particular implementation of the generic high level classifier."}, {"heading": "IV. COMPUTER SIMULATIONS", "text": "In this section, we present computer simulation results in order to assess the effectiveness of the proposed hybrid classification model based on tourist walks. The error estimation method in these simulations is set to be the stratified 10-fold cross-validation."}, {"heading": "A. Motivating and Illustrative Examples", "text": "In this section, we provide simple examples with the goal of showing the mechanics of the proposed hybrid classification technique. For this end, we simplify the parameters selection procedure, as follows: the weights given for the transient and cycle lengths are the same, i.e., \u03b1t = \u03b1c = 0.5; and the critical tourist walk length \u00b5c is set as being the size of the smallest class in the problem. Here, we design particular situations in which the use of the high order of learning is welcomed, since the reliance on mere physical measures would probably deceive a low level classifier.\nAs an introductory example, consider the synthetic data set supplied in Fig. 1 in the Introduction section. Our goal is to classify the two triangle-shaped data items (Xtest). The items in Xtest are inserted one by one using = 0.05 and k = 2 to construct the network components before and after each insertion. For the purposes of this simulation, once a test instance is classified, it is discarded. With respect to the low level classifier, a fuzzy SVM classifier is utilized [32], which is equipped with optimization method criterion defined as the Karush-Kuhn-Tucker violation fixed at 10\u22123 (the same condition suggested by [33]). The following four kernels are used: (i) Linear: u \u00b7 v, (ii) RBF: exp(\u2212\u03b3 \u2016 u\u2212 v \u20162), (iii) Sigmoid: tanh(\u03b3u\u00b7v+c), and (iv) Polynomial: (c+u\u00b7v)d. Additionally,\nthe cost parameter C must be set in all simulations. For each low level classifier, we apply a fine-tuning parameter phase in order to choose a model with optimized classification results.\nThe networked data set exhibited in Fig. 1 has 2 classes, a red or circular-shaped class (16 vertices) and a blue or squareshaped class (58 vertices). Consequently, the proportions of vertices pertaining to each class are: p(red) = 21.64% and p(blue) = 78.36%. Clearly, while the circular-shaped (red) class carries a perceivable pattern, which, geometrically speaking, is a lozenge-shaped lattice disposed in a 2-dimensional space, the blue (\u201csquare\u201d) class does not indicate a strong pattern as the first class. In this particular situation, we will see that the results produced by the SVM by itself are not satisfactory. Even though the kernel is responsible for spawning the data items into a higher space via a nonlinear transformation, the test instances in question may only be distinguished by using semantic knowledge, i.e., the topological features of the classes. It is worth stressing that special kernels could be created in order to solve specific data distributions, but they would be totally peculiar for the problems at hand, making their usability constrained to a very few real situations. On the other side, the proposed high level technique captures pattern formation of the data in a general manner.\nTable I reports the classification results of the two existing test instances for all the 4 kernels. For each kernel, the final prediction is given for three different values of the compliance term \u03bb: \u03bb = 0 (pure SVM), \u03bb = 0.5 (equal weight for the decision of the SVM and the high level classifier), and \u03bb = 0.9 (a high weight for the high level classifier and a low weight for the SVM). We can clearly see that a pure SVM with wellknown kernels is not able to correctly classify the data items shown in Fig. 1. However, if we make our final decision based on a mixture of the SVM and the high level classifier, one can verify that correct results can be obtained, i.e., the two triangleshaped data items are classified to red or circular-shaped class.\nNow, consider the classification problem arranged in Fig. 3. Here, we are going to empirically calculate the minimum required compliance term \u03bbmin for which the data items from the test set are classified as members of the red or circular-\nshaped class. In the figure, one can see that there is a segment of line representing the red or circular-shaped class (9 vertices) and also a condensed rectangular class outlined by the blue or square-shaped class (1000 vertices). The network formation in the training and test phases uses k = 1 and = 0.07 (this radius covers, for any vertex in the straight line, 2 adjacent vertices, except for the vertices in each end). The fuzzy SVM technique with RBF kernel (C = 22 and \u03b3 = 2\u22121) is employed as the traditional low level classifier. The task is to classify the 14 test instances depicted by the big triangle-shape items from left to right. After a test instance is classified, it is incorporated to the training set with the corresponding predicted label. The graphic embedded in Fig. 3 shows the minimum required value of \u03bbmin for which the the triangle-shaped items are classified as members of the red or circular-shaped class. This graphic is constructed according to the triangle-shaped element that is exactly at the same position with respect to the x-axis in the scatter plot drawn above. For example, the first triangle-shaped data item can be correctly classified if one chooses \u03bb \u2265 \u03bbmin \u2248 0.37. The second and third data items would require at least \u03bbmin = 0.81 and \u03bbmin = 0.96, respectively, and so on. Specifically, as the straight line crosses the condensed region of the blue or square-shaped class, the compliance term approaches \u03bb\u2192 1, since one cannot establish its decision based on the low level classifier, because it would erroneously decide favorable to the blue or square-shaped class."}, {"heading": "B. Parameter Sensitivity Analysis", "text": "In this section, we will perform several simulations with the goal of better understanding the influence of each parameter in the model.\n1) Influence of the Compliance Term for Different High Level Classifiers: In this section, we study the influence of the compliance term \u03bb for three different types of high level classifiers, as follows: (i) one constructed solely using the component\u2019s cycle length; (ii) one built exclusively using the component\u2019s transient length; and (iii) the best weighted combination of these two measures. The optimization process is conducted by finding \u03b1t \u00d7 \u03b1c \u2208 {0, 0.1, . . . , 1} \u00d7 {0, 0.1, . . . , 1} (search space), subjected to \u03b1t+\u03b1c = 1, which results in the highest accuracy rate of the model. Here, we show that, in general, the transient and cycles lengths are not correlated. The objective of this section is to make this\npoint clear and demonstrate that, when they act together, they are able to produce better accuracy rates. In order to conduct these tests, classes with Gaussian distributions are used. The network in the classification phase is constructed with k = 1 and = 0.05. The same is employed in the classification phase. The similarity measure is simply given by the reciprocal of the Euclidean measure. For the traditional classifier, the fuzzy SVM with RBF kernel is employed with C = 24 and \u03b3 = 2\u22122. Finally, this process is repeated 100 times and the mean and the corresponding standard deviation for each value of \u03bb is reported.\nNow, we advance to our first experiment in which we consider a rather simple classification scenario, where the classes are completely separated from each other, as Fig. 4a exhibits. Figure 4b depicts the accuracy rate of the proposed classifier vs. \u03bb for the three types of high level classifiers discussed before. The best weighted combination of the cycle and transient lengths is \u03b1c = 0.6 and \u03b1t = 0.4. One can see that the high level classifier constructed by the combination of the transient and cycle lengths is able to outperform the other high level classifiers constructed using only one of these measures. In addition, when \u03bb = 0 (only the usage of the traditional classifier), almost no wrong labels are assigned. On the other hand, as \u03bb increases, the accuracy rate of the proposed technique starts to monotonically decrease. This is predictable, since the two classes are similar; hence, the network measures associated to each representative component are almost equivalent. Arguments such that there is no mixture between the classes and the spatial correlations of both classes are similar reinforce this phenomenon. Therefore, it is natural that the high level classifier will become confused in classifying these data items under such conditions. This example shows that only the high level classifier is insufficient to get good classification results.\nNext, let us examine the problem of classification illustrated in Fig. 4c, where the two classes slightly collide with each other. As a result of this phenomenon, a conflicting region is constituted. The data items which reside in this region are most likely to be misclassified by a pure traditional classifier. Similarly to the previous case, Fig. 4d which displays the accuracy rate of the model. In this case, the best weighted combination of transient and cycle lengths is \u03b1t = 0.7 and \u03b1c = 0.3. One can see that a mixture of low level and high level classifiers does supply a boost in the accuracy rate of\nthe model. Specifically, the highest accuracy rate occurs when \u03bb \u2208 {0.15, 0.2}. The region of coalescence of both classes influences in the network construction, in such a way that the representative components of each class become slightly different. These arguments explain that a little relevance to the high level classifier decision can cause the accuracy rate to increase, since the network measures describing each component are slightly distinct.\nWe now proceed to inspect the classification problem proposed in Fig. 4e. In this particular situation, the two classes heavily collide with each other. Due to that, the two classes become almost indistinguishable. In consequence of the network formation technique, two very distinct and representative components will arise from these two classes. In this special scenario, the two components that comprise the network are expected to be possess different network properties, i.e., some unique structural pattern for each class is hoped for emerging. Figure 4f depicts the accuracy rate reached by the classifier against various values of \u03bb. The best weighted combination here is \u03b1t = 0.2 and \u03b1c = 0.8. One can see that the accuracy\nrate keeps on increasing until a high value of the compliance term, namely \u03bb = 0.65, where it achieves 65%, against 54% with \u03bb = 0 for the high level classifier combining the transition and cycle lengths. This phenomenon is precisely explained by the complex structural patterns that each component exhibits, in which merely traditional classifiers would not be able to capture; therefore, a heavy weight on the high level classifier decision was responsible for this significant increase in the accuracy rate.\n2) Influence of the Critical Memory Length: As we have seen, the high level classifier makes its prediction by using combinations of several tourist walks with memory lengths {0, 1, . . . , \u00b5c}. A natural question that arises is: is it really necessary to perform the computation of tourist walks with \u00b5 ranging from 0 to a maximum feasible number, i.e., the number of vertices in a component? In this section, we will empirically show that, usually, it is not necessary to perform all these computations. For this end, we will reinforce this argument by verifying this behavior in a synthetic data set and two real-world data sets.\nWe start out by revisiting the synthetic data set given in Fig. 1 shown in the Introduction section. The behavior of the transient and cycle lengths are displayed in Figs. 5a and 5b, respectively. One can note that these two dynamical measures vary as \u00b5 increases up to a point where they reach a steady region in which no more oscillating is verified. When this happens, we say that the dynamics of the tourist walk have reached a saturation point, in the sense that further computations of tourist walks with larger memory lengths are redundant. Moreover, one can see that this saturation point is reached very quickly in relation to the graph components.\nContinuing our exploration of this interesting phenomenon, we now turn our attention to two well-known data sets from the UCI Machine Learning Repository [34]: Iris (balanced classes) and Wine (unbalanced classes). Consider Figs. 6a and 6b, where it is depicted the transient and cycle lengths for the classes of the Iris data set, and Figs. 7a and 7b, in which it is displayed the same information for the Wine data set. With respect to the transient length behavior, we can see that, for both data sets, as \u00b5 increases, the transient length also increases. However, when \u00b5 is sufficiently large, the components\u2019 transient lengths settle down in a flat region (like the previous case). On the other hand, for both data sets, the cycle length behavior is rather interesting, which can be\nroughly divided in three different regions: (i) for a small \u00b5, it is directly proportional to \u00b5; (ii) for intermediate values of \u00b5, it is inversely proportional to \u00b5; and (iii) for sufficiently large values of \u00b5, it also settles down in a steady region. One can interpret these results as follows: \u2022 When \u00b5 is small, it is very likely that the transient and\ncycle parts will also be small, because the memory of the tourist is very limited. We can conceive this as a walk with almost no restrictions; \u2022 When \u00b5 assumes an intermediate value, the transient length keeps increasing but the cycle length reaches a peak and starts to decrease afterwards. This peak characterizes the topological complexity of the component and varies from one to another. Hence, this is the most important region for capturing pattern formation of the class component by using the topological structure of the network. \u2022 When \u00b5 is large, the tourist has a greater chance of getting trapped in a vertex of the graph, once all the neighborhood of the visited vertex is contained in the memory window \u00b5. In this scenario, the transient length is expected to be very high and the cycle length, null. This phenomenon explains the steady regions in Figs. 6a and 6b. In this region, we can say that the tourist walks have already covered all the global aspects of the class component, and increasing the memory length \u00b5 will not capture any new topological features or pattern formation of the class. In this scenario, it is said that the tourist walks have completely described the topological com-\nplexity of the class component (saturation). In view of this, the calculation of tourist walks by further increasing \u00b5 is redundant.\nThis analysis suggests that the accuracy of the high level classifier may not change given that we choose a \u00b5c residing near these steady regions. This means that higher values for \u00b5c will only cause redundant computations and the accuracy will not be enhanced. In order to check this, Figs. 8a and 8b reveal the behavior of the hybrid classification framework for different values of \u00b5c. We have used three distinct compliance terms, namely \u03bb \u2208 {0, 0.05, 0.6} for the Iris data set and \u03bb \u2208 {0, 0.06, 0.7} for the Wine data set. When \u03bb = 0, only the low level classifier is used, in such a way that the value of \u00b5c is irrelevant, since the high level term is disabled. With respect to the other cases, when \u00b5c \u2265 20, the model provides the same accuracy rates for both data sets, confirming our prediction that, once it reaches the steady region where the transient and cycle lengths settle down, subsequent increases of \u00b5c do not change the accuracy of the model. In practical terms, according to our simulations, fixing the critical length \u00b5c as 10 \u2212 30% of the component\u2019s size is enough to get satisfactory results."}, {"heading": "C. Simulations on Real-World Data Sets", "text": "In this section, we will apply the proposed framework to several well-known UCI data sets. The most relevant metadata of each data set is given in Table II. For a detailed description, refer to [34]. Concerning the numerical attributes, the reciprocal of the Euclidean distance is employed. For categorical examples, the overlap similarity measure [35] is utilized. All data sets are submitted to a standardization pre-processing step.\nHere, the high level classifier is composed of the best weighted combination of transient and cycle lengths. The optimization process is done by encountering \u03b1t \u00d7 \u03b1c \u2208 {0, 0.1, . . . , 1} \u00d7 {0, 0.1, . . . , 1} (search space), subjected to \u03b1t + \u03b1c = 1, which result in the highest accuracy rate of the model. The critical memory length is fixed to \u00b5c = 0.3nmax, where nmax indicates the size of the largest component. The parameter optimization results are given in last two columns of Table II.\nHere, we will deal with two kinds of high level classifiers: (i) one in which the tourist walks are performed in a network constructed from the vector-based data set and (ii) one in which the tourist walks are realized in a lattice, i.e., the tourist is free to visit any other data site apart from the ones in the memory window \u00b5. In the latter case, it is expected that the walker will perform long jumps in the data set once the memory length \u00b5 assumes large values. It will be verified that such mechanism is not very welcomed in classification tasks. Furthermore, this serves as a strong argument for the employment and introduction of network-based high level classifiers.\nTable III reports the results obtained by the proposed technique on the data sets listed in Table II. For comparison purposes, we evaluate the performance of the framework against different low level classifiers: Bayesian networks [36], Weighted k-nearest neighbors [37], Multi-layer perceptrons (MLP) [38], Multi-class SVM (M-SVM) [32], [39]. The outcome of each algorithm is estimated by the average value over hundred runs of a stratified 10-fold cross-validation process. Also, for each result, three different types of results are indicated as follows: \u2022 \u201cPure\u201d row: only the low level classifier is utilized (\u03bb = 0).\nIn this case, inside the parentheses are indicated the best parameters obtained from the optimization process for each technique; \u2022 \u201cNetworkless\u201d row: a mixture of low and high level classifiers is employed. The value inside the parentheses indicates the best compliance term \u03bb. The high level classifier is constructed using the best weighted combination of transient and cycle lengths with the weights respecting Table II. Moreover, the tourist walks are performed in a networkless environment, i.e., the tourist can visit any other site (data item) apart from those contained in the memory window \u00b5. \u2022 \u201cNetwork\u201d row: the same setup as before, but the tourist walks are conducted on a networked environment. In this case, the tourist can only visit vertices (items) that are in the neighborhood and not in the memory window \u00b5. Here, the network in the training phase is built using k = 1 and = 0.03. The values inside the parentheses exhibit: the used in the classification phase and the best compliance term \u03bb, respectively. For the sake of clarity, take the first entry of Table III. The pure low level classifier Bayesian Networks achieved an accuracy rate of 57.8 \u00b1 2.6 (\u03bb = 0). However, if we use the\nproposed technique in a networkless environment, the accuracy rate is refined, achieving 58.6\u00b12.3 when \u03bb = 0.04. Now, when the proposed technique is used in a network environment, the accuracy rate reaches 66.3 \u00b1 2.6 when \u03bb = 0.28. In general, the proposed technique is able to boost the accuracy rates of the data sets under analysis. Furthermore, we can see that the networked high level classifier can outperform the networkless version."}, {"heading": "V. APPLICATION: HANDWRITTEN DIGITS RECOGNITION", "text": "In this section, we provide an application of the proposed technique to handwritten digits recognition. Specifically, in Subsect. V, we detail the experimental results obtained from the modified NIST set, a data sets composed of tens of thousands of real handwritten digits. Still in this section, we present two simple examples to show how the proposed technique can be used for invariant pattern recognition.\nWhile recognizing individual digits is only one of a myriad of problems that involves specific designing of practical recognition system, it still is, undoubtedly, an excellent benchmark for comparing shape recognition methods. The data set in which we will conduct our studies hereon is named Modified NIST set [40]. This data set provides a training set with 60 000 samples and a test set of 10 000 samples. Each image has 28 \u00d7 28 pixels. With respect to the high level classifier, the network in the training phase is constructed using k = 3 and = 0.01. In the classification phase, the same is used.\nFor comparison matters, we use the proposed high level classifier with 3 distinct low level classifiers. The high level classifier will remain constant as we have been using, i.e., with an underlying network. The techniques that will be exploited are listed below: \u2022 A linear classifier implemented as an 1-layer neural\nnetwork. We use the same experimental setup given in [40]; \u2022 A k-nearest neighbor classifier with an Euclidean distance measure between input images (k = 3); \u2022 A k-nearest neighbor classifier with the similarity function given by a set of weighted eigenvalue measures [4]. Specifically, we use \u03c6 = 4 eigenvalues and adopt the following \u03b2 function: \u03b2(x) = 16 exp(x3 ).\nFigure 9 shows the performance of these techniques acting together with the high level classifier in a networked environment. Our main goal here is to reveal that a mixture of traditional and high level classifier is able to increase the accuracy rate. For example, the linear neural network reached 88% of accuracy rate when only a traditional classifier is applied. A small increase in the compliance term is responsible for increasing the accuracy rate to 91% (\u03bb = 0.2). Regarding the k-nearest neighbor algorithm, for a pure traditional classifier, we have obtained 95% of accuracy rate, against 97.6% (\u03bb = 0.25). For the proposed weighted eigenvalue measure, we have obtained 98% of accuracy rate when \u03bb = 0, against 99.1% when \u03bb = 0.2. It is worth noting that the enhancement is significant. Even in the third case, the improvement is quite welcomed, because it is a hard task to increase an already very high accuracy rate. Moreover, one can see that maximum compliance term is intrinsic to the data set, since, for\nthree completely distinct low level classifiers, the maximum accuracy rate is reached in the surroundings of \u03bb = 0.225.\nFigures 10a and 10b illustrate how the digit classification is carried out by using simple networks containing a small number of digits \u20185\u2019 and \u20186\u2019, randomly drawn from the MNIST data set. Firstly, let us consider the Fig. 10a, where the digits \u20185\u2019 with brown boxes and the digits \u20186\u2019 with blue boxes represent the training set. The task now is to classify the test instance represented by the digit with a red box. If only low level classification is applied, the test digit is probably to be classified as a digit \u20186\u2019, because it has more neighbors of digit\n\u20186\u2019 than that of \u20185\u2019. On the other hand, if we also consider high level classification, the test digit can be correctly classified as a digit \u20185\u2019, because it complies more to the pattern formed by training digits \u20185\u2019 than to the one formed by digits \u20186\u2019. More specifically, if the test digit is put into the class \u20185\u2019, it just extends the already formed \u201cline\u201d pattern. As a consequence, the inclusion of the test digit to the class \u20185\u2019 generates small variations of the component measures. However, if the test digit is inserted into the class \u20186\u2019, larger variations of the component measures occur, since cycles are formed in the component (before insertion of the test digit, there is no cycle in the component). Figures 11a and 11b show the transient and cycle lengths as well as the corresponding variations as a function of \u00b5, when the test digit is inserted into the component of digit \u20185\u2019. We see that the variations are very small, indicating the strong compliance of the test digit with the pattern formed by training digits \u20185\u2019. On the other hand, Figs. 11c and 11d show the same information, when the test digit is put into the component of digit \u20186\u2019. Here, we see that larger variations occur, which means that the test digit does not conform to the pattern formed by the component of digits \u20186\u2019. As a result, the test instance is correctly classified as a digit \u20185\u2019. The same reasoning can be applied to the digit network shown by Fig. 10b. In this case, the transient and cycle lengths as well as the corresponding variations are shown in Figs. 11e - 11h, when the test digit is inserted into the component of digit \u20185\u2019 or \u20186\u2019, respectively. In this situation, the test instance\nis classified as a digit \u20186\u2019."}, {"heading": "VI. CONCLUSIONS", "text": "In this work, we have proposed an alternative and novel technique for data classification, which combines both low and high level characteristics of the data. The former classifies data instances by their physical features and the latter measures the compliance of the test instance with the pattern formation of the input data. To this end, tourist walks have been employed to capture the complex topological properties of the network constructed from the input data. A quite interesting feature of the proposed technique is that the high level term\u2019s influence has to be increased in order to get correct classification as the complexity of the class configuration increased. This\nmeans that the high level term is specially useful in complex situations of classification. Also, it is worth observing that the application of the tourist walks dynamics in the context of high level classifier is also a novel approach in the literature. We have shown that, even though such walk is constructed under very simple rules, it is still able to capture topological features of the underlying network in a local to global basis. In addition, we uncover a critical memory length, above which no change occurs in the transient and cycle lengths of the network component.\nWe hope our work can provide an alternative way to the understanding of high level semantic machine learning. As a future work, mechanisms for taking representative samples, rather than recalculating all the tourist walks for all the vertices\nin the network, are going to be considered."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the Sa\u0303o Paulo State Research Foundation (FAPESP) and by the Brazilian National Research Council (CNPq)."}], "references": [{"title": "Pattern Classification, 2nd ed", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Network-based high level data classification", "author": ["T.C. Silva", "L. Zhao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 6, pp. 954\u2013970, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "COLT: Proceedings of the Workshop on Computational Learning Theory, 1998, pp. 92\u2013100.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Tri-training: exploiting unlabeled data using three classifiers", "author": ["Z.-H. Zhou", "M. Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 11, pp. 1529\u20131541, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "The semantic web", "author": ["T. Berners-Lee", "J. Hendler", "O. Lassila"], "venue": "Scientific American, vol. 284, no. 5, pp. 34\u201343, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "The semantic web revisited", "author": ["N. Shadbolt", "W. Hall", "T. Berners-Lee"], "venue": "IEEE Intelligent Systems, vol. 6, pp. 96\u2013101, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "The semantic web in action", "author": ["L. Feigenbaum", "I. Herman", "T. Hongsermeier", "E. Neumann", "S. Stephens"], "venue": "Scientific American, vol. 297, no. 6, pp. 90\u201397, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Linear prediction models with graph regularization for web-page categorization.", "author": ["T. Zhang", "A. Popescul", "B. Dom"], "venue": "in KDD. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S.A. Macskassy", "F. Provost"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 935\u2013983, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Combining content and link for classification using matrix factorization.", "author": ["S. Zhu", "K. Yu", "Y. Chi", "Y. Gong"], "venue": "in SIGIR. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Using ghost edges for classification in sparsely labeled networks", "author": ["B. Gallagher", "H. Tong", "T. Eliassi-rad", "C. Faloutsos"], "venue": "Knowledge Discovery and Data Mining, 2008, pp. 256\u2013264.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Classifying networked entities with modularity kernels", "author": ["D. Zhang", "R. Mao"], "venue": "International Conference on Information and Knowledge Management, 2008, pp. 113\u2013122.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Sciences, University of Wisconsin-Madison, Tech. Rep. 1530, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Use of contextual constraints in recognition of contour-traced handprinted characters", "author": ["R.W. Donaldson", "G.T. Toussaint"], "venue": "IEEE Trans. Computers, pp. 1096\u20131099, 1970.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1970}, {"title": "A cognitive pyramid for contextual classification of remote sensing images", "author": ["E. Binaghi", "I. Gallo", "M. Pepe"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 41, no. 12, pp. 2906\u20132922, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning relevant image features with multiple-kernel classification", "author": ["D. Tuia", "G. Camps-Valls", "G. Matasci", "M. Kanevski"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 48, no. 10, pp. 3780\u20133791, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning contextual dependency network models for link-based classification", "author": ["Y. Tian", "Q. Yang", "T. Huang", "C.X. Ling", "W. Gao"], "venue": "IEEE Trans. Data and Knowledge Engineering, vol. 18, no. 11, pp. 1482\u20131496, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "survey of image classification methods and techniques for improving classification performance", "author": ["D. Lu", "Q. Weng"], "venue": "International Journal of Remote Sensing, vol. 28, no. 5, pp. 823\u2013870, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "On classification with incomplete data", "author": ["D. Williams", "X. Liao", "Y. Xue", "L. Carin", "B. Krishnapuram"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 3, pp. 427\u2013436, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural network for graphs: A contextual constructive approach", "author": ["A. Micheli"], "venue": "IEEE Trans. Neural Networks, vol. 20, no. 3, pp. 498\u2013511, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Deterministic walks in random media", "author": ["G.F. Lima", "A.S. Martinez", "O. Kinouchi"], "venue": "Phy. Rev. Lett., vol. 87, p. 010603, 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Deterministic walks as an algorithm of pattern recognition", "author": ["M.G. Campiteli", "P.D. Batista", "O. Kinouchi", "A.S. Martinez"], "venue": "Physical Review E, vol. 74, p. 026703, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Texture analysisandclassificationusingdeterministictouristwalk", "author": ["A.R. Backes", "W.N. Gon\u00e7alves", "A.S. Martinez", "O.M. Bruno"], "venue": "Pattern Recognition, vol. 43, pp. 685\u2013694, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Rotation invariant kernels and their application to shape analysis", "author": ["O.C. Hamsici", "A.M. Martinez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 11, pp. 1985\u20131999, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1985}, {"title": "Age-invariant face recognition", "author": ["U. Park", "Y. Tong", "A.K. Jain"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp. 947\u2013954, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 3, pp. 411\u2013426, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical physics - the salesman and the tourist", "author": ["H.E. Stanley", "S.V. Buldyrev"], "venue": "Nature, vol. 413, pp. 373\u2013374, 2001.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Deterministic walks in random networks: an application to thesaurus graphs", "author": ["O. Kinouchi", "A.S. Martinez", "G.F. Lima", "G.M. Louren\u00e7o", "S. Risau- Gusman"], "venue": "Physica A, vol. 315, pp. 665\u2013676, 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Fuzzy support vector machines", "author": ["C.-F. Lin", "S.-D. Wang"], "venue": "IEEE Transactions on Neural Networks, vol. 13, pp. 464\u2013471, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "A comparison of methods for multi-class support vector machines", "author": ["C.-W. Hsu", "C.-J. Lin"], "venue": "IEEE Transactions on Neural Networks, vol. 13, pp. 415\u2013425, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "UCI machine learning repository", "author": ["A. Frank", "A. Asuncion"], "venue": "2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity measures for categorical data: A comparative evaluation", "author": ["S. Boriah", "V. Chandola", "V. Kumar"], "venue": "SIAM Data Mining Conference, 2008, pp. 243\u2013254.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, second edition ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1988}, {"title": "Fuzzy support vector machines for multiclass problems", "author": ["S. Abe", "T. Inoue"], "venue": "in European Symposium on Artificial Neural Networks, 2002, 2002, pp. 113\u2013118.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 1, "context": "These techniques that predict class labels using only physical features are called low level classification techniques [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "Supervised data classification by considering not only physical attributes but also pattern formation is referred to as high level classification [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "In this sense, strongly related techniques are co-training [5] and tritraining [6], which attempt to consider the cooperation of various classification techniques (ensemble), each focusing on a theoretically different \u201cvision\u201d of the data.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "In this sense, strongly related techniques are co-training [5] and tritraining [6], which attempt to consider the cooperation of various classification techniques (ensemble), each focusing on a theoretically different \u201cvision\u201d of the data.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 144, "endOffset": 147}, {"referenceID": 6, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 283, "endOffset": 287}, {"referenceID": 11, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 288, "endOffset": 292}, {"referenceID": 12, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 332, "endOffset": 336}, {"referenceID": 13, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 385, "endOffset": 389}, {"referenceID": 19, "context": "Following the literature stream on such matter, there are several kinds of works related to high level classification, such as the Semantic Web [7]\u2013[9], which uses ontologies to describe the semantics of the data, statistical relational learning, which realizes collective inference [10]\u2013[14] or graph-based semisupervised learning [15], [16], and contextual classification techniques [17]\u2013[23], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.", "startOffset": 390, "endOffset": 394}, {"referenceID": 20, "context": "A tourist walk can be defined as follows: Given a set of cities, each time the tourist (walker) goes to the nearest city that has not been visited in the past \u03bc time steps [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "It has been shown that tourist walk is useful for data clustering [25] and image processing [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "It has been shown that tourist walk is useful for data clustering [25] and image processing [26].", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "The idea of this paper is built upon the general framework proposed by [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "In the original work introduced in [4], the high level classification problem is treated using three existing network measures in a combined way (assortativity, clustering coefficient, and average degree).", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "In this occasion, the global topological and organizational features of the network are said to be completely characterized in the sense of tourist walks; \u2022 In view of the intuitive dynamical properties displayed by a tourist walk, one can avoid the weight assignment among various network measures, which is a problem when static network measures are used, as occurs in [4].", "startOffset": 371, "endOffset": 374}, {"referenceID": 23, "context": "Still in this paper, we show how the proposed technique can be used to solve general invariant pattern recognition problems [27]\u2013[29], particularly when the pattern variances are nonlinear and there is not a closed form to describe the invariance.", "startOffset": 124, "endOffset": 128}, {"referenceID": 25, "context": "Still in this paper, we show how the proposed technique can be used to solve general invariant pattern recognition problems [27]\u2013[29], particularly when the pattern variances are nonlinear and there is not a closed form to describe the invariance.", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "In spite of being a simple rule, it has been shown that this movement dynamic possesses complex behavior when \u03bc > 1 [24].", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "A note that is worth pointing out is that, in the majority of the works related to these walks [24], [30], [31], the tourist may visit any other site other than the ones contained in its memory window.", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "A note that is worth pointing out is that, in the majority of the works related to these walks [24], [30], [31], the tourist may visit any other site other than the ones contained in its memory window.", "startOffset": 101, "endOffset": 105}, {"referenceID": 27, "context": "A note that is worth pointing out is that, in the majority of the works related to these walks [24], [30], [31], the tourist may visit any other site other than the ones contained in its memory window.", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Next, we give a quick overview on the particularities of the original hybrid classification framework [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "In this section, we review the hybrid classification framework [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "and the General Framework introduced in [4]", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "One may wonder how the high level classifier based on tourist walks given by (3) is plugged into the general framework for high level classification introduced in [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 28, "context": "With respect to the low level classifier, a fuzzy SVM classifier is utilized [32], which is equipped with optimization method criterion defined as the Karush-Kuhn-Tucker violation fixed at 10\u22123 (the same condition suggested by [33]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 29, "context": "With respect to the low level classifier, a fuzzy SVM classifier is utilized [32], which is equipped with optimization method criterion defined as the Karush-Kuhn-Tucker violation fixed at 10\u22123 (the same condition suggested by [33]).", "startOffset": 227, "endOffset": 231}, {"referenceID": 30, "context": "Continuing our exploration of this interesting phenomenon, we now turn our attention to two well-known data sets from the UCI Machine Learning Repository [34]: Iris (balanced classes) and Wine (unbalanced classes).", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "For a detailed description, refer to [34].", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "For categorical examples, the overlap similarity measure [35] is utilized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "For comparison purposes, we evaluate the performance of the framework against different low level classifiers: Bayesian networks [36], Weighted k-nearest neighbors [37], Multi-layer perceptrons (MLP) [38], Multi-class SVM (M-SVM) [32], [39].", "startOffset": 164, "endOffset": 168}, {"referenceID": 33, "context": "For comparison purposes, we evaluate the performance of the framework against different low level classifiers: Bayesian networks [36], Weighted k-nearest neighbors [37], Multi-layer perceptrons (MLP) [38], Multi-class SVM (M-SVM) [32], [39].", "startOffset": 200, "endOffset": 204}, {"referenceID": 28, "context": "For comparison purposes, we evaluate the performance of the framework against different low level classifiers: Bayesian networks [36], Weighted k-nearest neighbors [37], Multi-layer perceptrons (MLP) [38], Multi-class SVM (M-SVM) [32], [39].", "startOffset": 230, "endOffset": 234}, {"referenceID": 34, "context": "For comparison purposes, we evaluate the performance of the framework against different low level classifiers: Bayesian networks [36], Weighted k-nearest neighbors [37], Multi-layer perceptrons (MLP) [38], Multi-class SVM (M-SVM) [32], [39].", "startOffset": 236, "endOffset": 240}, {"referenceID": 35, "context": "The data set in which we will conduct our studies hereon is named Modified NIST set [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 35, "context": "We use the same experimental setup given in [40]; \u2022 A k-nearest neighbor classifier with an Euclidean distance measure between input images (k = 3); \u2022 A k-nearest neighbor classifier with the similarity function given by a set of weighted eigenvalue measures [4].", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "We use the same experimental setup given in [40]; \u2022 A k-nearest neighbor classifier with an Euclidean distance measure between input images (k = 3); \u2022 A k-nearest neighbor classifier with the similarity function given by a set of weighted eigenvalue measures [4].", "startOffset": 259, "endOffset": 262}], "year": 2013, "abstractText": "Complex networks refer to large-scale graphs with nontrivial connection patterns. The salient and interesting features that the complex network study offer in comparison to graph theory are the emphasis on the dynamical properties of the networks and the ability of inherently uncovering pattern formation of the vertices. In this paper, we present a hybrid data classification technique combining a low level and a high level classifier. The low level term can be equipped with any traditional classification techniques, which realize the classification task considering only physical features (e.g., geometrical or statistical features) of the input data. On the other hand, the high level term has the ability of detecting data patterns with semantic meanings. In this way, the classification is realized by means of the extraction of the underlying network\u2019s features constructed from the input data. As a result, the high level classification process measures the compliance of the test instances with the pattern formation of the training data. Out of various high level perspectives that can be utilized to capture semantic meaning, we utilize the dynamical features that are generated from a tourist walker in a networked environment. Specifically, a weighted combination of transient and cycle lengths generated by the tourist walk is employed for that end. Furthermore, we show that the proposed technique is able to capture the organizational and complex features of the class component from a local to global fashion in a natural and intuitive way by altering the memory size of the tourist walk. Still in this work, we uncover the existence of a critical memory length, we say complex saturation, where any values larger than this critical point make no change in the transient and cycle lengths of the network component. Interestingly, our study shows that the proposed technique is able to further improve the already optimized performance of traditional classification techniques. Finally, we apply the proposed technique to the recognition of handwritten digit images and promising results have been obtained.", "creator": "LaTeX with hyperref package"}}}