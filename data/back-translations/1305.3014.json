{"id": "1305.3014", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "abstract": "Online advertising has been introduced as one of the most efficient advertising methods in recent years. However, advertisers are concerned about the efficiency of their online advertising campaigns and therefore want to limit their advertising impressions to specific websites and / or specific audiences. These limitations, known as targeting criteria, limit accessibility for better performance. This trade-off between accessibility and performance highlights the need for a predictive system that can quickly predict / estimate this trade-off with good accuracy. The main idea is to remember a small representative sample across multiple machines and formulate the prediction problem as queries against the sample. The key challenge is to find the best layers over the past data, to perform multivariate stratified samples, while ensuring that the small results that are currently widely used across the industry are blurred.", "histories": [["v1", "Tue, 14 May 2013 03:48:09 GMT  (392kb,D)", "http://arxiv.org/abs/1305.3014v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["ali jalali", "santanu kolay", "peter foldes", "ali dasdan"], "accepted": false, "id": "1305.3014"}, "pdf": {"name": "1305.3014.pdf", "metadata": {"source": "CRF", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "authors": ["Ali Jalali", "Santanu Kolay", "Peter Foldes", "Ali Dasdan"], "emails": ["ajalali@turn.com", "skolay@turn.com", "pfoldes@turn.com", "adasdan@turn.com"], "sections": [{"heading": "1 Introduction", "text": "Forecasting is the problem of predicting the future of a time series considering the past. When it comes to big data, generating a fast and accurate report of the past is a challenge itself. Typically advertisers would like to frequently forecast on thousands of dimensions to have a good understanding of the market and spend their budget wisely. Running such queries against massive data takes a lot of resources and time making forecasting impractical. In practice, it is expected to return the forecasting result in few seconds so that the advertisers can adjust their audience and contextual targeting.\nOne way of generating fast and accurate reports is to take a relatively small sample from the original big data and run queries against that sample. At the end, the result can be scaled up to compensate for the sampling rate. In such a scheme, the accuracy of the report depends on how good a representative the sample is for the entire big dataset. Since the data lies in a high dimensional space, with thousands of dimensions, it is challenging to get a good representative of the data. In particular, uniform sampling performs poorly in this regime, despite its popular use in the industry.\nIn this paper, we cast the problem of generating fast and accurate reports as a sampling problem and introduce metrics to measure the goodness of the report. The sampling algorithm should be able to guarantee a good accuracy while it is practically feasible on big data. Also, since the samples are going to be used in a distributed system, the sampling algorithm should be able to produce reasonable results even if some parts of the system fail, i.e., some portion of the samples are not available at a certain point. We propose both a robust and accurate sampling algorithm as well as the architecture of the distributed system that uses those samples to generate the results.\nThe proposed sampling algorithm is a stratified sampling algorithm that has four steps. In the first step, the algorithm identifies the right set of strata in the high-dimensional data space to be sampled from. Second step involves with a fuzzy fall-back step that maps minor strata to (related) major strata to ensure the diversity and accuracy. The distributed stratified sampling itself is implemented in the third step. The last step is to distribute the samples across multiple machines to make report generation fast and the system\nar X\niv :1\n30 5.\n30 14\nv1 [\ncs .L\nG ]\n1 4\nM ay\n2 01\nfault tolerant. We explain each of these steps in separate \u00a7 3-5. Experimental results and discussion is provided in \u00a7 6; showing the effectiveness of our method in real application. The paper is finally concluded in section \u00a7 7."}, {"heading": "2 Background and Related Work", "text": "In this section, we first formalize the problem and explain the setup. Then, we explore the related work in the context of our problem. We cannot cover all of the literature; rather, we cite more recent works that are closer to our setting and formulation."}, {"heading": "2.1 Problem Setup and Motivation", "text": "Suppose our big data is in the form of a set D = {X1,. . .,XN} of K-feature vectors Xi = (x1, . . . , xK) for all i \u2208 {1, . . . , N}. Practically, we can assume all features are categorical and if not, we bucketize them. Without loss of generality, assume that each feature xj can take mj different values from {1, . . . ,mj}. We would like to find a good representative sample set Ds \u2282 D such that n = |Ds| << N , i.e., the size of the sample is much smaller than the size of the original dataset. We need to define a metric to compare different sample sets and prefer one to the other in a consistent way.\nConsider the set of all queries Q that want to count the number of K-feature vectors lie in a certain subspace. The cardinality of this set is 2 \u220f mj since there are \u220fK j=1mj different possibilities for each feature vector. Suppose we submit a queries qi \u2208 Q to both D and Ds and we get Ni and ni, respectively. We expect that Ni \u2248 Nn ni and hence, we can define the following error metric:\nerr(Ds) = max qi\u2208Q \u2223\u2223Ni \u2212 Nn ni\u2223\u2223 Ni = max qi\u2208Q \u2223\u2223\u2223\u22231\u2212 Nn niNi \u2223\u2223\u2223\u2223 . (1)\nThe smaller err(Ds) is, the better representative Ds is for our big data. Notice that although normalized on the true size of the query Ni, this error can exceed 1 simply if the over estimation is too large.\nBefore we proceed, it is essential to understand why uniform sampling cannot be a solution in this highdimensional regime. Imagine we build Ds by uniformly (with probability nN ) sampling feature vectors from D. For all possible query qi in the high-dimensional space, we require niNi \u2248 n N . It is easy to see that satisfying so many requirements at the same time is highly unlikely. To show this, we provide a toy example. Suppose our big data has N = 100 binary feature vectors and K = 2, i.e., each feature vector takes a value in {(0, 0), (0, 1), (1, 0), (1, 1)}. The population of each of these vector values is shown in Fig. 1(a), e.g., we have 20 feature vectors equal to (0, 0) in our data. We would like to take a representative sample with n = 10. Fig. 1(b) illustrates a good representative sample Ds of the data, i.e., it picks two feature vectors from (0, 0), one feature vector from (0, 1), etc. For any query on this Ds, if we multiply the result ni by N n = 10, we get the true result of that query Ni as if we were running that query on the original data. In this case, it is easy to see that err(Ds) = 0. However, with uniform sampling, we are not guaranteed to get such a nice sample even though (like this case), our sample size is big enough. With uniform sampling, we might end up with a sample set that looks like Fig. 1(c).\nIt is natural to ask with what probability the uniform sampling produces the minimum error sample. In the case of our toy example (see Fig. 1), this probability can be expressed as\nP =\n( 10 1 )( 20 2 )( 30 3 )( 40 4 )( 100 10\n) \u2248 0.04. (2) This probability is shockingly low and it becomes worse as the number of values (aka strata) that the feature vector can take increases. This increase is inevitable in high dimensions. More generally, if we have M strata, i.e., feature vectors take M different (vector) values, with sizes N1, . . . , NM with \u2211 iNi = N , and, our sampling size is n, the probability that uniform sampling achieves the lowest error has a hyper-geometric distribution as\nP \u2217 =\n N1[ n N N1 ] N2[ n N N2 ] . . .  NM[ n N NM ] ( N\nn ) . (3) Using the Normal approximation summarized in Theorem 2 of [22], we have\nP \u2217 \u2264 N ! ( e n )n(\u221a 2\u03c0 )M (n\u2212M + 1) . (4)\nThis probability vanishes as M increases; implying that uniform sampling cannot produce a good representative even with large enough sample size. This motivates consideration of stratified sampling; however, implementing stratified sampling in high dimensions is a challenge. Further, we will discuss that stratified sampling alone is not enough and introduce a fuzzy fall-back step."}, {"heading": "2.2 Related Work", "text": "There is large body of work in literature related to Approximate Query Processing (AQP). There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17]. Garofalakis et al. [11] provides a tutorial introduction to the subject. Our work is most closely related to the sampling based and histogram based methods.\nThe work whose approach is closest to our method is by Acharya et al. [1]. They solve the problem of approximating GROUP BY queries using a smaller sample. They propose to use a mixture of uniform (House) and biased (Senate) sampling techniques. For groups with high selectivity the uniform sampling produces the best result, whereas for smaller groups biased (stratified sampling) produces better results. They also propose an algorithm (Congressional sampling) to mix the sampling approach to produce a fixed size sample that provides good error bounds for all groups under the expected query load. In their biased sampling approach, they use all possible subsets of the grouping columns to find the strata, each strata basically represents one possible row of a GROUP BY query. However, it is not clear how this algorithm scales for tables with very large number of columns, as is the case in our problem. Another main drawback of this paper is the assumption about highly selective queries: They ignore cases where samples generated out of congressional sampling may not contain any row where GROUP BY keys has very log selectivity. In\nour approach we must account for it; otherwise, we are not able to forecast for some types of constraints entered by our users leading to bad user experience. Also many of our forecast related queries especially geographical based constraints are generally very selective in nature.\nChaudhuri et. al. [6] provide a stratified sampling based approach to approximate query processing under a workload distribution. The authors propose the STRAT algorithm, which first defines a set of strata called \u201cFundamental Regions\u201d. These fundamental regions are chosen based on an expected query load with the constraint that whenever a fundamental region is used all elements from that region must be selected. The authors show good result with this approach for SUM and COUNT type of queries based on GROUP BY constraints. However this approach does not fit our use case exactly, as we do not have an estimate of the expected query load beforehand. Also, the paper does not clearly mention the sample generation complexity for high dimensional dataset. However the authors provide a very good analysis of variance of different types of queries and show their method performs quite well under diverse set of query workload.\nAuthors of [10] introduce a novel way of maintaining dynamic samples for AQP. They use \u201cicicles\u201d, a type of self-tuning samples that produces smaller errors for more frequent query types. For each query they initially generate the result out of full dataset and then choose a uniform sample from the result set. As new queries arrive, the sample is augmented with other rows matching this query. Rows selected multiple times across multiple queries keeps accumulating higher weight. Over time the sample contains a subset of rows from the original dataset with each selected row having a weight proportion to the no of queries that row answers. The authors show that icicles provide significantly better results than uniform sampling; however, it is not clear how the method can control the overall sample size, as in our implementation we have limited space available across all strata and we are expected to provide reasonable accuracy on a set of unforeseen queries.\nIn [9], authors study the problem of building multidimensional histograms to answers queries approximately over very high dimensional data cubes. They propose LCS-Hist, which processes the data offline and builds multidimensional histogram to be used as pre-aggregates during queries later. Their method first partitions the n-dimensional cube into a set of partitions with the objective of minimizing within partition variance using a dynamic programming approach. Then they try to merge buckets having similar distribution based some distance metric. After several types of bucket merging, the number of buckets become lot more manageable. The authors report significant space reduction by maintaining the aggregates using the histograms compared to the full cube. Even though the range splitting and bucket merging has some similarity with our approach, our fundamental problem is to derive a set of user samples that can be used for arbitrary queries, not necessarily OLAP style [19] aggregates. Even though the authors provide very detailed analysis of approximation error compared to other histogram based methods, they do not provide any result on the computation complexity of their method. As our system has to generate a sample of millions of rows from an original data set of billions of rows, the runtime of our algorithm is of paramount importance.\nSome authors have proposed probabilistic methods for selectivity estimation [12]. They model the data set as joint distribution over the variables on which GROUP BY or JOIN queries are run. A naive approach to modeling the joint probability distribution will lead to exponential no of entries, so they propose to use conditional independence found in many real-life data sets. Based on the input query and the conditional independence among the various variables, a Bayesian Network is formulated and answering the query simply boils down to inference on the Bayesian Network. Our approach has some similarity to thier work in the sense that we also rely on the conditional independence property to reduce our computation. However the authors seem to focus more on the model complexity and do not provide much informaation on the runtime of the model building and inference. We propose to use a state of the art Markov Random Field method which is learned in distributed environment.\nThe problem of estimating the result of aggregation query with low selectivity has been tackled by other work related to deep Web [20], with the additional constraint of not having full access to the data set. This method proposes a stratification method based on a queriable auxiliary attribute based on which the breaking points in the strata are determined. The breaking points in the strata are computed based on a novel Bayesian Adaptive Harmony Search algorithm. The problem solves the sample allocation problem as a constrained optimization problem where the optimization metric consists of both sample variance as well as precision.\nTo the best of our knowledge, our problem setting is quite unique. Though our method bears some resemblance to the methods proposed in [1, 9], our data set is very high-dimensional (250K) and consists of categorical variables only. This allows us to pose the problem in terms of sets of objects and we can perform stratification based on set operations.\nC \u00a0B \u00a0A \u00a0\nMarkov Chain"}, {"heading": "3 Identifying Strata", "text": "The first step towards a better sampling is to identify subsets of similar feature vectors, known as strata. In high-dimensions, it is a challenge to partition the data into strata such that each stratum is not too big to include non-similar feature vectors and not too small to require a large sample size. In this section, we propose a method for identification of such strata.\nAlthough the feature vectors live in a high-dimensional space, not all of the features are independent. This is the key to dimensionality reduction. Suppose we find a subset of features, i.e., dimensions, that every other feature is highly correlated with this sets. Then, if we build our strata by reducing the dimension of the feature vectors to that subset, we can represent the entire population fairly well. To find such subset of features we use Markov Random Fields (MRFs) [18].\nConsidering each feature as a node in the graph, MRF assigns a matrix \u0398j,k \u2208 Rmj\u00d7mk to the edge intersecting features xj and xk such that\nP [ xj = t \u2223\u2223 x1, . . ., xj\u22121, xj+1, . . . , xK] \u221d exp\n\u2212\u2211 k 6=j mk\u2211 q=1 \u0398j,k(t, q)1{xk=q}  (5) where, \u0398j,k(t, q) is the (t, q) element of the matrix \u0398j,k and 1{\u00b7} is the indicator function. If we remove the edges with matrix \u0398j,k equal to zero, the remaining graph has the property that each feature conditioned on its neighbors in the graph is independent from the rest of the graph [18]. See Fig. 2. This is a generalization of the Markov Chain where each event given the immediate previous event is independent from the history of events. A distributed and fast algorithm to learn the MRF graph from data is detailed in [16].\nOnce the MRF graph among features is learned, we need to find a minimum vertex cover [8] of that graph. Basically, we need to find a minimum subset of vertices such that if we remove them, the rest of the graph becomes completely disconnected. Then, according to MRFs, those vertices are independent of each other given the vertices in our subset. As an example, suppose in the MRF shown in Fig. 2, if we select features {A,B2, C1, C2} as our features, the rest of the graph become conditionally independent.\nMinimum vertex cover is known to be an NP-hard problem; however, there are good approximations such as the algorithm proposed in [7, 23]. Without loss of generality, suppose we select features {1, . . . ,K\u2217} (out of {1, . . . ,K}). For each value that each feature can take, we generate a key-value tag, e.g., \u201cx1 = 1\u201d is a tag. There are the total of \u220fK\u2217 j=1mj different tags. Now, each feature vector can be tagged with the set of relevant tags, e.g, the feature vector (\u03b11, \u03b12, . . . , \u03b1K) will be tagged (only on the first K \u2217 features) by the set of tags {\u201cx1 = \u03b11\u201d, . . . ,\u201cxK\u2217 = \u03b1K\u2217\u201d}. From a different point of view, each tag can be seen as a set and each feature vector lies in the intersection of the sets corresponding to its tags. This view represents a partitioning on the feature space. We pick each of those partitions as a stratum. Another advantage of this tagging strategy is that if we have one (or some) missing data, we will only lose a tag and the feature vector will be moved to the intersection of the relevant K\u2217\u22121 sets. The only issue here is that the strata might be very small to sample from and hence, we introduce a fuzzy fall-back step in the next section."}, {"heading": "4 Fuzzy Fall-back", "text": "Consider the strata we found in the previous sections in the form of intersection of sets. Let Tv is the set of all feature vectors that are tagged with the tag v. As defined before, a tag v is of the form \u201cxi = m\u201d for the ith feature taking value m. Considering strata as the partitions defined by all possible intersections of sets Tv, some of these strata include more than N2n feature vectors and hence, they can potentially have at least one representative in our stratified sample. However, in reality, most of the such strata do not have enough population. That is why we introduce a fall-back step.\nLet \u03a51 be the set of all strata with more than N 2n in population size and \u03a52 to be the set of all strata that only belong to one tag set; e.g. Tv \u2212 \u22c3\nu6=v Tu is an example of such strata; and, finally let \u03a5 = \u03a51 \u222a\u03a52 be the set of stable strata. Next, we are going to map unstable strata \u03a5\u22a5 into \u03a5 so that the chance of having small population strata decreases.\nWe propose two algorithm for the mapping of unstable strata to stable ones. We call this step as fuzzy fall-back step since we change the membership of feature vectors across strata by falling back into their subsets. Our first proposed algorithm has lower computational complexity but slightly more inaccurate. The second algorithm provides a better stratification results at the cost of some computational cost.\n\u2022 Algorithm 1: For each unstable stratum P \u2208 \u03a5\u22a5, find the set of all stable strata Q1, . . . ,Qr \u2208 \u03a5 that are a subset of P with maximum number of tags. Among Qi\u2019s, pick Q that has the smallest population and consider the union of stratum P and Q as a new stratum.\n\u2022 Algorithm 2: This algorithm, at each step, starts with an unstable stratum P that is in the intersection of maximum number of sets. Then, it distributes the population of P among the subset strata (regardless whether they are stable or unstable) proportional to their size. After distribution, some of those subsets might become stable, i.e., get more than N2n in population size and hence get removed from the set of unstable strata. The process continues until there is no unstable strata or there is no unstable strata with a subset.\nAfter this step, we can simply sample from each strata according to their size. Notice that with this strategy, if Ds is our sample, for each strata, we have\nerr(Ds) \u2264 N\n2n . (6)\nThis error bound is far better than the uniform bound shown in (4). The only caveat is that when we want to make estimation in real system, we have queries against all features not just the ones we selected in Section 3. To compensate for that, we use the result of MRF summarized in (5). In practice, if the edge matrices are big to keep, one can store average (or a summary) of that matrix. Our experimental result shows the high performance of this method."}, {"heading": "5 Distributed Implementation", "text": "The stratified sampling provides the benefit of improved accuracy over the uniform sampling given the same sample size constraint. However the accuracy is still dependent on the sample size. In order to handle a larger sample size, we distribute the sample over a set of servers. As the queries are trivially parallelizable, the overall system latency is significantly reduced. We are also able to support many simultaneous queries over the same sample. We describe the requirements of the distributed system in \u00a7 5.1. The system\u2019s architecture is introduced and explained in \u00a7 5.2. \u00a7 5.3 contains a brief analysis on how the design fulfills the requirements."}, {"heading": "5.1 System requirements", "text": "The following non-functional requirements try to capture the most important necessities of the system:\n1. Near real-time: System response is expected to be in order of seconds.\n2. Progressive result update: The results are updated in realtime as the scanning of the sample progresses. The major advantage of this approach is that users are able to get a very quick estimate of the results before the full computation finishes. Our system generates partial query results in order to satisfy this requirement.\n3. Fault tolerance: In presence of network or hardware failures, the system should still produce results, at potentially reduced speed or accuracy.\n4. Scalability: We consider two main aspects:\n(a) Horizontal scalability: Adding more servers should allow the system to execute on the sample faster or execute on larger samples for higher accuracy.\n(b) Increased query handling: As the system usage increases due to users or automated software services, the system can handle the increased number of query requests without significant performance or accuracy degradation.\nThere are additional requirements that describe availability and reliability scenarios, such as the guarantee that generating and distributing a new sample will not affect currently running reports. Describing these requirements and the design decisions concerning these are outside the scope of this paper."}, {"heading": "5.2 System architecture", "text": "The distributed system consists of two kinds of nodes: counter nodes and aggregate nodes. The counter nodes are responsible for going over the part of the stratified sample present on that node and perform the necessary query. The aggregator nodes choose a set of counter nodes from a pool of counter nodes, distribute the query request to each selected counter node, receive partial results from counters and aggregate them into a single final result. This simple distinction of responsibilities allows the system to process increasing the amount of data with only adding a small constant overhead of network communication and the increased time of partial report aggregation as new servers are introduced. This approach has been evangelized by well known search engines [2] effectively.\nFigure 3 shows the flow of the system. Other services can make requests to any of the aggregator nodes with all the targeting information used in the forecasting. The aggregator node will select a sub-cluster of counters and distribute the request. The counters will add these requests to a queue that is used to store and update all awaiting forecasting requests, see section 5.2.2 for more details. The counters will also acquire a thread to push partial results frequently to the aggregator which distributed the request. Pushing the data instead of pulling on request reduces the amount of communication between the servers and can help in determining if a forecast prediction is already within a certain margin of error threshold. This support\nfor early termination helps in reducing the overall system load on counter nodes, thus enabling higher query throughput.\nMeanwhile, the stratified sample is generated periodically offline by a Map-Reduce process and stored on a Hadoop File System (HDFS) that can be accessed from all nodes. Distribution of the sample between the counter nodes and the notification when a new set of sample data is available is handled by a Zookeeper [15] cluster, a distributed lock service widely used in the industry. The sample itself is divided up into per server sub-samples, making sure the tuples in the per server sample is disjoint, and is further partitioned into smaller fragments. As each sub-sample has exactly similar strata present, the computation at each counter node can be performed independently and can be scaled up properly. When a counter node fails, the sub-sample is temporarily lost from the whole system until a new counter node takes over, which can potentially reduce the accuracy the forecast in that time interval. The per server sample is loaded up in memory, since the IO overhead can be significant for systems dealing with large data sets. Zookeeper is used to orchestrate loading the updated sample data, ensuring that not all the counters are unavailable at the time of loading. This flow is shown in Fig. 4."}, {"heading": "5.2.1 Sample management pipeline", "text": "Managing the servers and the sample is done through Zookeeper in the following way:\n1. Counter nodes acquire sub-samples through a leader selection process controlled by the Zookeeper service. This prevents the sub-sample from being loaded in multiple counter nodes. Having the subsample loaded in multiple counter nodes will lead to reduced overall accuracy. Given a fixed set of counter machines, it is always better to have each machine loading a separate sub-sample, as we handle any non-responding counter node gracefully. The Zookeeper service is also responsible for alerting when new offline sub-samples are generated.\n2. Aggregator selection is managed through a distributed semaphore, ensuring a fixed number of aggregator nodes among a pool of aggregator nodes to be present.\n3. Zookeeper also orchestrates the loading of the sample in a staggered fashion so that not all counetr nodes are non-responsive during the sub-sample reload time.\nNewly acquired sub-samples are loaded into counter node memory using multiple threads, both for performance reasons and to set up multiple threads for consuming the data. It is assumed that memory is scarce, at least scarce enough that two per server sample cannot be loaded into memory at the same time. This dictates our design decision of making sure that the counter node stops taking new requests from the aggregator, then\nwaits for the existing requests to be finished and then only start reloading the new sub-sample in memory. This counter node is ready for new request only after the reload has happened successfully."}, {"heading": "5.2.2 Request queue pipeline", "text": "Each forecasting request received by any of the forecasting nodes are wrapped in a collector and added to a queue. The collectors are responsible for gathering the set of partial results, whether it is done by processing the sample as in the case of counter nodes, or continuously receiving partial results from other nodes as in the case of aggregator nodes.\nIn case of counter nodes, the collector orchestrates a set of threads processing data that is fed by consumer threads iterating over the data. This strategy of consuming and processing data is useful in our use case, namely when processing data points require compressing and decompressing for memory optimization. This strategy further ensures that the system can scale with additional requests as, besides the extra overhead of evaluating the data points for the new request, there is no additional cost of optimizing the data. The data consuming and processing threads used are symmetric, meaning each consumer pushes data to a specific processing thread, which makes synchronization and locking unnecessary. Every time a partial result needs to be pushed, the results of the processing threads are merged and sent to the aggregator.\nThe aggregator collectors are responsible for keeping track of partial responses received for a request and making sure all information about the sample of the counters involved is at hand for accurate scaling of the results. Counter nodes push the partial results instead of a delta between pushes to ensure that losing a partial result during network communication is acceptable. This also means that the aggregator collector needs to merge all results from the counter nodes whenever a response is requested.\nCollectors are considered to be finished when either (a) the full set of the sample has been processed, (b) an acceptable threshold is reached in terms of prediction error, or (c) the result was not requested for a longer amount of time. The queue, in case of every node, runs a maintenance thread on itself to check if any of the collectors are done. Those that are done are removed from the queue and put into a cache for some time so the results remain available before removing it completely from the system. These results are currently not persisted."}, {"heading": "5.2.3 Handling information on the sample for multiple servers", "text": "It is not enough for the counter nodes to know about the sample they have acquired, it is also important that other nodes, especially aggregator nodes, know it too so that they can successfully scale the results. It might also be the case that a node did not load the full available sample into memory. This has the benefit to fine tune the system in terms of performance and accuracy. To ensure that the latest information is available, nodes automatically broadcast this to every other node. Whenever a node comes online it also both receives and sends this information to every other node.\nOn the other hand, ongoing forecasts should not be affected by these changes. Assuming a scenario where a counter finished producing a result, pushed it to the aggregator, and loaded up a new sample, the aggregator should still use the old sample information for scaling. For this reason every aggregator collector stores not only the partial results received for each collector, but also the summed sample information of all nodes participating in the forecast."}, {"heading": "5.3 Analysis of the design", "text": "Section 5.1 detailed the requirements of the system. The analysis below describes how these requirements are fulfilled by the above described system and the various tradeoffs."}, {"heading": "5.3.1 Performance", "text": "The performance of the system can be modelled as the overall load on the system defined by\ntd + ts + tc + tm = O(N + S\nN ), (7)\nwhere, td is the time to distribute the request among N counter nodes, ts is the time to scan the sample per node, tc is the communication latency overhead, tm is the time to merge the results on the aggregator node for a sample of size S. The above equation can be used to fine-tune the system. Lowering the number of querying a certain result increases the performance, but slows down the progressive report. Same is achieved by increasing the delay between the counter nodes reporting partial results. The greatest flexibility comes with tuning the sample size available per counter nodes, since tm is the most significant contribution to the overall system load. Greater representative sample may increase accuracy especially for more granular filtering, while small sample speeds up the report generation as counters finish faster. It is interesting to note that by adding more counters, ts for N nodes is approximately the same as ts for N+1 nodes since the nodes work in parallel and the communication overhead is significantly less that the cost of processing the sample.\nSince there are no database interactions and no disk IO operations, the system can be easily tuned to process a required size sample within seconds."}, {"heading": "5.3.2 User experience", "text": "Forecasting can potentially take multiple tens of seconds so it is important to reduce the perceived waiting time for users. There are multiple research showing that having incremental page loading capability reduces the perceived waiting time significantly [3]. In our system we strive to implement incremental result publishing so that users can have a significantly good estimate of the query result very quickly. This is very important during the initial advertising campaign setup when the user is trying to explore several what-if scenarios. Since aggregator nodes are continuously receiving partial results from counters that they can merge and scale using how much of the sample has been processed, partial results are available at any time."}, {"heading": "5.3.3 Availability", "text": "There are multiple ways the design ensures fault tolerance. Through Zookeeper, we can monitor how many nodes are active and alive, and can distribute responsibilities accordingly. A node failure can result in a temporary decrease in performance or accuracy until a new node is introduced. In case of aggregator nodes, it is possible that some report generation fail as the node fails. This is deemed acceptable, as the failure happens quickly; the failed forecasting reports are easy to resubmit. As long as another aggregator node is available, the request can be served.\nIt is possible that in extreme circumstances, too many nodes fail simultaneously. While the counter nodes are able to receive requests just by themselves and produce a fully usable result, the potential margin of error for just one machine might be too large for the report to be usable. However, we also provide a margin of error with all of our estimates and the user can choose to ignore the results if the error bounds are out of the expectation."}, {"heading": "5.3.4 Scalability", "text": "Scalability is defined in two different ways in requirements. The ability to add servers with ease to forecast faster or with greater accuracy is achieved by the division of responsibilities. Since adding new counter nodes introduces only a small overhead in communication, the sample size can be increased for greater accuracy. It is also possible to redistribute the current sample to include the newly added node, which decreases the time needed to process the sample per server.\nHandling increased amounts of forecast queries is achieved by considering two different factors: (1) Because new aggregators can be introduced to handle the outside communication and to make sure no request is lost, still providing the callers with the possibility of serving partial results, and (2) because the size of the subcluster used for forecasting can be tuned. It is possible to select a smaller sub-cluster of counter nodes so that the nodes are not exhausted, but potentially reducing the accuracy of the forecast."}, {"heading": "6 Experimental Results", "text": "We provide two sets of experimental results. One set of results show the improvement of our sampling strategy over uniform and simple stratified sampling, while the other set of results is concerned with the distributed system implementation."}, {"heading": "6.1 Sampling Performance", "text": "We illustrate the improvement of our sampling algorithm over uniform sampling and simple stratified sampling. The simple stratified sampling is our algorithm minus the fuzzy fall-back step. We plot the relative error of queries with respect to the size of the query. We make a sample of size n = 300K out of a total population of N = 1.5B feature vectors. Fig. 5 shows that our sampling error is less than 8% of the uniform sampling error and less than 18% of the simple stratified sampling error on a query of size 1M .\nObviously, the estimation error decreases as the size of the query increases but our relative (to other methods) error is also decreasing. This shows that our method not only performs better than the other two methods but also provides more advantages as the size of the query increases. The amount of our improvement is less comparing to the simple stratified scheme because the simple stratified scheme is using our reduced set of features for stratification."}, {"heading": "6.2 System Performance", "text": "In Eq. (7) some metrics are introduced as the describing factors of system performance. We run experiments to get a sense of how the system behaves by generating reports with different filtering criteria.\nTable 1 shows the messages communicated within the system. Since Initiate report, Sample information and Distribute request messages are rare compared to Partial result and potentially Fetch response, it is more important to focus on the performance of the latter two. Our measurements show that fetching the results take a significantly more time than pushing the partial results from counter nodes to the aggregator nodes. This is not due to the message size that is almost identical to the size of partial results but is due to the time it takes to merge the results.\nIn trying to see how much influence fetching results have on the overall system, Table 6 shows how the total report generation time changes depending on the interval the results are requests. The table shows the relative performance deviation from the baseline case where no fetching is done. According to the results, there is no significant impact on the overall performance of the system due to more frequent merging of partial results.\nOut of the terms defined in Eq. (7), ts has the greatest impact on the total runtime. Comparing the distributed system with three counter nodes against a simple machine solution that consumes the same sample without any communication overhead showed about 33% increase on average. Improving the performance and the parallelization is a topic for future work."}, {"heading": "7 Conclusions", "text": "Forecasting is key for advertisers to pinpoint their exact targeting constraints and budget spend so as to reach the best segments of the audience with the optimum valuation. As such, forecasting has to be accurate. Moreover, forecasting needs to be fast to allow advertisers to perform interactive exploration while finalizing their campaign parameters during campaign setup. Forecasting is also useful to evaluate the DSP partner of the advertiser to compare a before and after picture of what was forecasted and how the campaign returned. The same evaluation is also used by DSPs during debugging campaign delivery and performance issues.\nIn this paper, we propose a forecasting solution to solve for both accuracy and speed. Our Markov Random Fields based algorithm perform an accurate stratified sampling of the audience seen by the DSP. We offer a fuzzy fall-back mechanism to make the algorithm more practical for small-sized strata. Our distributed solution enables us to run forecasting queries in seconds and to tolerate faults with servers in the distributed cluster. Experimental results from the deployed system validate the efficiency and efficacy of the proposed solution.\nFuture work includes adding support for ad hoc querying the forecasting sample."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Changgull Song for testing the entire framework in the staging environment and Kuang-Chih Lee for his valuable comments."}], "references": [{"title": "Congressional samples for approximate answering of groupby queries", "author": ["S. Acharya", "P.B. Gibbons", "V. Poosala"], "venue": "ACM SIGMOD Record,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Web search for a planet: The google cluster architecture", "author": ["L.A. Barroso", "J. Dean", "U. Holzle"], "venue": "Micro, IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Integrating user-perceived quality into web server design", "author": ["N. Bhatti", "A. Bouch", "A. Kuchinsky"], "venue": "Computer Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Stholes: a multidimensional workload-aware histogram", "author": ["N. Bruno", "S. Chaudhuri", "L. Gravano"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Approximate query processing using wavelets", "author": ["K. Chakrabarti", "M. Garofalakis", "R. Rastogi", "K. Shim"], "venue": "The VLDB Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Optimized stratified sampling for approximate query processing", "author": ["S. Chaudhuri", "G. Das", "V. Narasayya"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Vertex cover: further observations and further improvements", "author": ["J. Chen", "I.A. Kanj", "W. Jia"], "venue": "Journal of Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Introduction to algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "MIT press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Lcs-hist: taming massive high-dimensional data cube compression", "author": ["A. Cuzzocrea", "P. Serafino"], "venue": "In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Icicles: Self-tuning samples for approximate query answering", "author": ["V. Ganti", "M.-L. Lee", "R. Ramakrishnan"], "venue": "In Proceedings of the 26th International Conference on Very Large Data Bases,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Approximate query processing: Taming the terabytes", "author": ["M. Garofalakis", "P.B. Gibbons"], "venue": "In Proc. of the 27th Intl. Conference on Very Large Data Bases,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Selectivity estimation using probabilistic models", "author": ["L. Getoor", "B. Taskar", "D. Koller"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "New sampling-based summary statistics for improving approximate query answers", "author": ["P.B. Gibbons", "Y. Matias"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Approximating multi-dimensional aggregate range queries over real attributes", "author": ["D. Gunopulos", "G. Kollios", "V.J. Tsotras", "C. Domeniconi"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Zookeeper: Wait-free coordination for internet-scale systems", "author": ["P. Hunt", "M. Konar", "F.P. Junqueira", "B. Reed"], "venue": "In USENIX ATC,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On learning discrete graphical models using greedy methods", "author": ["A. Jalali", "C.C. Johnson", "P. Ravikumar"], "venue": "In Neural Information Processing Conference (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Scalable approximate query processing with the dbo engine", "author": ["C. Jermaine", "S. Arumugam", "A. Pol", "A. Dobra"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Markov random fields and their applications", "author": ["R. Kindermann", "J.L. Snell"], "venue": "American Mathematical Society Providence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1980}, {"title": "High-dimensional olap: A minimal cubing approach", "author": ["X. Li", "J. Han", "H. Gonzalez"], "venue": "In Proceedings of the Thirtieth international conference on Very large data bases-Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Stratified sampling for data mining on the deep web", "author": ["T. Liu", "F. Wang", "G. Agrawal"], "venue": "Frontiers of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Wavelet-based histograms for selectivity estimation", "author": ["Y. Matias", "J.S. Vitter", "M. Wang"], "venue": "ACM SIGMOD Record,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "On the normal approximation to the hypergeometric distribution", "author": ["W. Nicholson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1956}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": "Dover publications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}], "referenceMentions": [{"referenceID": 21, "context": "Using the Normal approximation summarized in Theorem 2 of [22], we have", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 3, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 13, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 8, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 4, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 84, "endOffset": 91}, {"referenceID": 20, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 84, "endOffset": 91}, {"referenceID": 12, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 16, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "[11] provides a tutorial introduction to the subject.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] provide a stratified sampling based approach to approximate query processing under a workload distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Authors of [10] introduce a novel way of maintaining dynamic samples for AQP.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "In [9], authors study the problem of building multidimensional histograms to answers queries approximately over very high dimensional data cubes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "Even though the range splitting and bucket merging has some similarity with our approach, our fundamental problem is to derive a set of user samples that can be used for arbitrary queries, not necessarily OLAP style [19] aggregates.", "startOffset": 216, "endOffset": 220}, {"referenceID": 11, "context": "Some authors have proposed probabilistic methods for selectivity estimation [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "The problem of estimating the result of aggregation query with low selectivity has been tackled by other work related to deep Web [20], with the additional constraint of not having full access to the data set.", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "Though our method bears some resemblance to the methods proposed in [1, 9], our data set is very high-dimensional (250K) and consists of categorical variables only.", "startOffset": 68, "endOffset": 74}, {"referenceID": 8, "context": "Though our method bears some resemblance to the methods proposed in [1, 9], our data set is very high-dimensional (250K) and consists of categorical variables only.", "startOffset": 68, "endOffset": 74}, {"referenceID": 17, "context": "To find such subset of features we use Markov Random Fields (MRFs) [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "If we remove the edges with matrix \u0398j,k equal to zero, the remaining graph has the property that each feature conditioned on its neighbors in the graph is independent from the rest of the graph [18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 15, "context": "A distributed and fast algorithm to learn the MRF graph from data is detailed in [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Once the MRF graph among features is learned, we need to find a minimum vertex cover [8] of that graph.", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "Minimum vertex cover is known to be an NP-hard problem; however, there are good approximations such as the algorithm proposed in [7, 23].", "startOffset": 129, "endOffset": 136}, {"referenceID": 22, "context": "Minimum vertex cover is known to be an NP-hard problem; however, there are good approximations such as the algorithm proposed in [7, 23].", "startOffset": 129, "endOffset": 136}, {"referenceID": 1, "context": "This approach has been evangelized by well known search engines [2] effectively.", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "Distribution of the sample between the counter nodes and the notification when a new set of sample data is available is handled by a Zookeeper [15] cluster, a distributed lock service widely used in the industry.", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "There are multiple research showing that having incremental page loading capability reduces the perceived waiting time significantly [3].", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "Online advertising has been introduced as one of the most efficient methods of advertising throughout the recent years. Yet, advertisers are concerned about the efficiency of their online advertising campaigns and consequently, would like to restrict their ad impressions to certain websites and/or certain groups of audience. These restrictions, known as targeting criteria, limit the reachability for better performance. This trade-off between reachability and performance illustrates a need for a forecasting system that can quickly predict/estimate (with good accuracy) this trade-off. Designing such a system is challenging due to (a) the huge amount of data to process, and, (b) the need for fast and accurate estimates. In this paper, we propose a distributed fault tolerant system that can generate such estimates fast with good accuracy. The main idea is to keep a small representative sample in memory across multiple machines and formulate the forecasting problem as queries against the sample. The key challenge is to find the best strata across the past data, perform multivariate stratified sampling while ensuring fuzzy fall-back to cover the small minorities. Our results show a significant improvement over the uniform and simple stratified sampling strategies which are currently widely used in the industry.", "creator": "LaTeX with hyperref package"}}}