{"id": "1306.0186", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2013", "title": "RNADE: The real-valued neural autoregressive density-estimator", "abstract": "We introduce RNADE, a new model for estimating the common density of real vectors. Our model calculates the density of a data point as a product of one-dimensional conditions modelled using mixing density networks with common parameters. RNADE learns a distributed representation of the data while having a tractable expression for calculating densities. A tractable probability allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on multiple datasets of heterogeneous and perceptible data and find that it exceeds the mixing models in all but one case.", "histories": [["v1", "Sun, 2 Jun 2013 09:37:53 GMT  (315kb,D)", "https://arxiv.org/abs/1306.0186v1", "12 pages, 3 figures, 3 tables, 2 algorithms"], ["v2", "Thu, 9 Jan 2014 11:14:27 GMT  (316kb,D)", "http://arxiv.org/abs/1306.0186v2", "12 pages, 3 figures, 3 tables, 2 algorithms. Merges the published paper and supplementary material into one document"]], "COMMENTS": "12 pages, 3 figures, 3 tables, 2 algorithms", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["benigno uria", "iain murray", "hugo larochelle"], "accepted": true, "id": "1306.0186"}, "pdf": {"name": "1306.0186.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["b.uria@ed.ac.uk", "i.murray@ed.ac.uk", "hugo.larochelle@usherbrooke.ca"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic approaches to machine learning involve modeling the probability distributions over large collections of variables. The number of parameters required to describe a general discrete distribution grows exponentially in its dimensionality, so some structure or regularity must be imposed, often through graphical models [e.g. 1]. Graphical models are also used to describe probability densities over collections of real-valued variables.\nOften parts of a task-specific probabilistic model are hard to specify, and are learned from data using generic models. For example, the natural probabilistic approach to image restoration tasks (such as denoising, deblurring, inpainting) requires a multivariate distribution over uncorrupted patches of pixels. It has long been appreciated that large classes of densities can be estimated consistently by kernel density estimation [2], and a large mixture of Gaussians can closely represent any density. In practice, a parametric mixture of Gaussians seems to fit the distribution over patches of pixels and obtains state-of-the-art restorations [3]. It may not be possible to fit small image patches significantly better, but alternative models could further test this claim. Moreover, competitive alternatives to mixture models might improve performance in other applications that have insufficient training data to fit mixture models well.\nRestricted Boltzmann Machines (RBMs), which are undirected graphical models, fit samples of binary vectors from a range of sources better than mixture models [4, 5]. One explanation is that RBMs form a distributed representation: many hidden units are active when explaining an observation, which is a better match to most real data than a single mixture component. Another explanation is that RBMs are mixture models, but the number of components is exponential in the number of hidden units. Parameter tying among components allows these more flexible models to generalize better from small numbers of examples. There are two practical difficulties with RBMs: the likelihood of the model must be approximated, and samples can only be drawn from the model approximately by Gibbs sampling. The Neural Autoregressive Distribution Estimator (NADE) overcomes these difficulties [5]. NADE is a directed graphical model, or feed-forward neural network, initially derived as an approximation to an RBM, but then fitted as a model in its own right.\nar X\niv :1\n30 6.\n01 86\nv2 [\nst at\n.M L\n] 9\nJ an\nIn this work we introduce the Real-valued Autoregressive Density Estimator (RNADE), an extension of NADE. An autoregressive model expresses the density of a vector as an ordered product of one-dimensional distributions, each conditioned on the values of previous dimensions in the (perhaps arbitrary) ordering. We use the parameter sharing previously introduced by NADE, combined with mixture density networks [6], an existing flexible approach to modeling real-valued distributions with neural networks. By construction, the density of a test point under RNADE is cheap to compute, unlike RBM-based models. The neural network structure provides a flexible way to alter the mean and variance of a mixture component depending on context, potentially modeling non-linear or heteroscedastic data with fewer components than unconstrained mixture models."}, {"heading": "2 Background: Autoregressive models", "text": "Both NADE [5] and our RNADE model are based on the chain rule (or product rule), which factorizes any distribution over a vector of variables into a product of terms: p(x) = \u220fD d=1 p(xd | x<d), where x<d denotes all attributes preceding xd in a fixed arbitrary ordering of the attributes. This factorization corresponds to a Bayesian network where every variable is a parent of all variables after it. As this model assumes no conditional independences, it says nothing about the distribution in itself. However, the (perhaps arbitrary) ordering we choose will matter if the form of the conditionals is constrained. If we assume tractable parametric forms for each of the conditional distributions, then the joint distribution can be computed for any vector, and the parameters of the model can be locally fitted to a penalized maximum likelihood objective using any gradient-based optimizer.\nFor binary data, each conditional distribution can be modeled with logistic regression, which is called a fully visible sigmoid belief network (FVSBN) [7]. Neural networks can also be used for each binary prediction task [8]. The neural autoregressive distribution estimator (NADE) also uses neural networks for each conditional, but with parameter sharing inspired by a mean-field approximation to Restricted Boltzmann Machines [5]. In detail, each conditional is given by a feed-forward neural network with one hidden layer, hd \u2208 RH :\np(xd = 1 |x<d) = sigm ( v>d hd + bd ) where hd = sigm (W \u00b7,<dx<d + c) , (1)\nwhere vd \u2208 RH , bd \u2208 R, c \u2208 RH , and W \u2208 RH\u00d7(D\u22121) are neural network parameters, and sigm represents the logistic sigmoid function 1/(1 + e\u2212x).\nThe weights between the inputs and the hidden units for each neural network are tied: W \u00b7,<d is the first d\u22121 columns of a shared weight matrix W . This parameter sharing reduces the total number of parameters from quadratic in the number of input dimensions to linear, lessening the need for regularisation. Computing the probability of a datapoint can also be done in time linear in dimensionality, O(DH), by sharing the computation when calculating the hidden activation of each neural network (ad =W \u00b7,<dx<d + c):\na1 = c, ad+1 = ad + xdW \u00b7,d. (2)\nWhen approximating Restricted Boltzmann Machines, the output weights {vd} in (1) were originally tied to the input weightsW . Untying these weights gave better statistical performance on a range of tasks, with negligible extra computational cost [5].\nNADE has recently been extended to count data [9]. The possibility of extending generic neural autoregressive models to continuous data has been mentioned [8, 10], but has not been previously explored to our knowledge. An autoregressive mixture of experts with scale mixture model experts has been developed as part of a sophisticated multi-resolution model specifically for natural images [11]. In more general work, Gaussian processes have been used to model the conditional distributions of a fully visible Bayesian network [12]. However, these \u2018Gaussian process networks\u2019 cannot deal with multimodal conditional distributions or with large datasets (currently ' 104 points would require further approximation). In the next section we propose a more flexible and scalable approach."}, {"heading": "3 Real-valued neural autoregressive density estimators", "text": "The original derivation of NADE suggests deriving a real-valued version from a mean-field approximation to the conditionals of a Gaussian-RBM. However, we discarded this approach because the\nlimitations of the Gaussian-RBM are well documented [13, 14]: its isotropic conditional noise model does not give competitive density estimates. Approximating a more capable RBM model, such as the mean-covariance RBM [15] or the spike-and-slab RBM [16], might be a fruitful future direction.\nThe main characteristic of NADE is the tying of its input-to-hidden weights. The output layer was \u2018untied\u2019 from the approximation to the RBM to give the model greater flexibility. Taking this idea further, we add more parameters to NADE to represent each one-dimensional conditional distribution with a mixture of Gaussians instead of a Bernoulli distribution. That is, the outputs are mixture density networks [6], with a shared hidden layer, using the same parameter tying as NADE.\nThus, our Real-valued Neural Autoregressive Density-Estimator or RNADE model represents the probability density of a vector as:\np(x) = D\u220f d=1 p(xd |x<d) with p(xd |x<d) = pM(xd |\u03b8d), (3)\nwhere pM is a mixture of Gaussians with parameters \u03b8d. The mixture model parameters are calculated using a neural network with all of the preceding dimensions, x<d, as inputs. We now give the details.\nRNADE computes the same hidden unit activations, ad, as before using (2). As discussed by Bengio [10], as an RNADE (or a NADE) with sigmoidal units progresses across the input dimensions d \u2208 {1 . . . D}, its hidden units will tend to become more and more saturated, due to their input being a weighted sum of an increasing number of inputs. Bengio proposed alleviating this effect by rescaling the hidden units\u2019 activation by a free factor \u03c1d at each step, making the hidden unit values\nhd = sigm (\u03c1dad) . (4)\nLearning these extra rescaling parameters worked slightly better, and all of our experiments use them.\nPrevious work on neural networks with real-valued outputs has found that rectified linear units can work better than sigmoidal non-linearities [17]. The hidden values for rectified linear units are:\nhd = { \u03c1dad if \u03c1dad > 0 0 otherwise.\n(5)\nIn preliminary experiments we found that these hidden units worked better than sigmoidal units in RNADE, and used them throughout (except for an example result with sigmoidal units in Table 2).\nFinally, the mixture of Gaussians parameters for the d-th conditional, \u03b8d = {\u03b1d,\u00b5d,\u03c3d}, are set by: K mixing fractions, \u03b1d = softmax ( V \u03b1d >hd + b \u03b1 d ) (6)\nK component means, \u00b5d = V \u00b5 d > hd + b \u00b5 d (7) K component standard deviations, \u03c3d = exp ( V \u03c3d >hd + b \u03c3 d ) , (8)\nwhere free parameters V \u03b1d , V \u00b5 d , V \u03c3 d are H\u00d7K matrices, and b\u03b1d , b\u00b5d , b\u03c3d are vectors of size K. The softmax [18] ensures the mixing fractions are positive and sum to one, the exponential ensures the standard deviations are positive.\nFitting an RNADE can be done using gradient ascent on the model\u2019s likelihood given a training set of examples. We used minibatch stochastic gradient ascent in all our experiments. In those RNADE models with MoG conditionals, we multiplied the gradient of each component mean by its standard deviation (for a Gaussian, Newton\u2019s method multiplies the gradient by its variance, but empirically multiplying by the standard deviation worked better). This gradient scaling makes tight components move more slowly than broad ones, a heuristic that we found allows the use of higher learning rates.\nVariants: Using a mixture of Gaussians to represent the conditional distributions in RNADE is an arbitrary parametric choice. Given several components, the mixture model can represent a rich set of skewed and multimodal distributions with different tail behaviors. However, other choices could be appropriate in particular circumstances. For example, work on natural images often uses scale mixtures, where components share a common mean. Conditional distributions of perceptual data are often assumed to be Laplacian [e.g. 19]. We call our main variant with mixtures of Gaussians RNADE-MoG, but also experiment with mixtures of Laplacian outputs, RNADE-MoL."}, {"heading": "4 Experiments", "text": "We compared RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks [20, 21]. Given the known poor performance of discrete mixtures [4, 5], we limited our experiments to modeling continuous attributes. However it would be easy to include both discrete and continuous variables in a NADE-like architecture."}, {"heading": "4.1 Low-dimensional data", "text": "We first considered five UCI datasets [22], previously used to study the performance of other density estimators [23, 20]. These datasets have relatively low dimensionality, with between 10 and 32 attributes, but have hard thresholds and non-linear dependencies that may make it difficult to fit mixtures of Gaussians or factor analyzers.\nFollowing Tang et al. [20], we eliminated discrete-valued attributes and an attribute from every pair with a Pearson correlation coefficient greater than 0.98. Each dimension of the data was normalized by subtracting its training subset sample mean and dividing by its standard deviation. All results are reported on the normalized data.\nAs baselines we fitted full-covariance Gaussians and mixtures of factor analysers. To measure the performance of the different models, we calculated their log-likelihood on held-out test data. Because these datasets are small, we used 10-folds, with 90% of the data for training, and 10% for testing.\nWe chose the hyperparameter values for each model by doing per-fold cross-validation; using a ninth of the training data as validation data. Once the hyperparameter values had been chosen, we trained each model using all the training data (including the validation data) and measured its performance on the 10% of held-out testing data. In order to avoid overfitting, we stopped the training after reaching a training likelihood higher than the one obtained on the best validation-wise iteration of the corresponding validation run. Early stopping is crucial to avoid overfitting the RNADE models. It also improves the results of the MFAs, but to a lesser degree.\nThe MFA models were trained using the EM algorithm [24, 25], the number of components and factors were crossvalidated. The number of factors was chosen from even numbers from 2 . . . D, where selecting D gives a mixture of Gaussians. The number of components was chosen among all even numbers from 2 . . . 50 (crossvalidation always selected fewer than 50 components).\nRNADE-MoG and RNADE-MoL models were fitted using minibatch stochastic gradient descent, using minibatches of size 100, for 500 epochs, each epoch comprising 10 minibatches. For each experiment, the number of hidden units (50), the non-linear activation-function of the hidden units (RLU), and the form of the conditionals were fixed. Three hyperparameters were crossvalidated using grid-search: the number of components on each one-dimensional conditional was chosen from the set {2, 5, 10, 20}; the weight-decay (used only to regularize the input to hidden weights) from the set {2.0, 1.0, 0.1, 0.01, 0.001, 0}; and the learning rate from the set {0.1, 0.05, 0.025, 0.0125}. Learning-rates were decreased linearly to reach 0 after the last epoch.\nWe also trained fully-visible Bayesian networks (FVBN), an autoregressive model where each onedimensional conditional is modelled by a separate mixture density network using no parameter tying.\nThe same cross-validation procedure and hyperparameters as for RNADE training were used. The best validationwise MDN for each one-dimensional conditional was chosen.\nThe results are shown in Table 1. Autoregressive methods obtained statistical performances superior to mixture models on all datasets. An RNADE with mixture of Gaussian conditionals was among the statistically significant group of best models on all datasets. Unfortunately we could not reproduce the data-folds used by previous work, however, our improvements are larger than those demonstrated by a deep mixture of factor analyzers over standard MFA [20]."}, {"heading": "4.2 Natural image patches", "text": "We also measured the ability of RNADE to model small patches of natural images. Following the recent work of Zoran and Weiss [3], we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset [26] (Figure 1 gives examples).\nPixels in this dataset can take a finite number of brightness values ranging from 0 to 255. Modeling discretized data using a real-valued distribution can lead to arbitrarily high density values, by locating narrow high density spike on each of the possible discrete values. In order to avoid this \u2018cheating\u2019 solution, we added noise uniformly distributed between 0 and 1 to the value of each pixel. We then divided by 256, making each pixel take a value in the range [0, 1].\nIn previous experiments, Zoran and Weiss [3] subtracted the mean pixel value from each patch, reducing the dimensionality of the data by one: the value of any pixel could be perfectly predicted as minus the sum of all other pixel values. However, the original study still used a mixture of fullcovariance 64-dimensional Gaussians. Such a model could obtain arbitrarily high model likelihoods, so unfortunately the likelihoods reported in previous work on this dataset [3, 20] are difficult to interpret. In our preliminary experiment using RNADE, we observed that if we model the 64- dimensional data, the 64th pixel is always predicted by a very thin spike centered at its true value. The ability of RNADE to capture this spurious dependency is reassuring, but we wouldn\u2019t want our results to be dominated by it. Recent work by Zoran and Weiss [21], projects the data on the leading 63 eigenvectors of each component, when measuring the model likelihood [27]. For comparison amongst a range of methods, we advocate simply discarding the 64th (bottom-right) pixel.\nWe trained our model using patches drawn randomly from 180 images in the training subset of BSDS300. A validation dataset containing 1,000 random patches from the remaining 20 images in the training subset were used for early-stopping when training RNADE. We measured the performance of each model by measuring their log-likelihood on one million patches drawn randomly from the test subset, which is composed of 100 images not present in the training subset. Given the larger scale of this dataset, hyperparameters of the RNADE and MoG models were chosen manually using the performance of preliminary runs on the validation data, rather than by an extensive search.\nThe RNADE model had 512 rectified-linear hidden units and a mixture of 20 one-dimensional Gaussian components per output. Training was done by minibatch gradient descent, with 25 datapoints per minibatch, for a total of 200 epochs, each comprising 1,000 minibatches. The learning-rate was scheduled to start at 0.001 and linearly decreased to reach 0 after the last epoch. Gradient momentum with momentum factor 0.9 was used, but initiated at the beginning of the second epoch. A weight decay rate of 0.001 was applied to the input-to-hidden weight matrix only. Again, we found that multiplying the gradient of the mean output parameters by the standard deviation improves results. RNADE training was early stopped but didn\u2019t show signs of overfitting. We produced a further run\nwith 1024 hidden units for 400 epochs, with still no signs of overfitting; even larger models might perform better.\nThe MoG model was trained using minibatch EM, for 1,000 iterations. At each iteration 20,000 randomly sampled datapoints were used in an EM update. A step was taken from the previous mixture model towards the parameters resulting from the M-step: \u03b8t = (1 \u2212 \u03b7)\u03b8t\u22121 + \u03b7\u03b8EM , where the step size (\u03b7) was scheduled to start at 0.1 and linearly decreased to reach 0 after the last update. The training of the MoG was also early-stopped and also showed no signs of overfitting.\nThe results are shown in Table 2. We compare RNADE with a mixtures of Gaussians model trained on 63 pixels, and with a MoG trained by Zoran and Weiss (downloaded from Daniel Zoran\u2019s website) from which we removed the 64th row and column of each covariance matrix. The best RNADE test log-likelihood is, on average, 0.7 nats per patch lower than Zoran and Weiss\u2019s MoG, which had a different training procedure than our mixture of Gaussians.\nFigure 1 shows a few examples from the test set, and samples from the MoG and RNADE models. Some of the samples from RNADE are unnaturally noisy, with pixel values outside the legal range (see fourth sample from the right in Figure 1). If we constrain the pixels values to a unit range, by rejection sampling or otherwise, these artifacts go away. Limiting the output range of the model would also improve test likelihood scores slightly, but not by much: log-likelihood does not strongly penalize models for putting a small fraction of probability mass on \u2018junk\u2019 images.\nAll of the results in this section were obtained by fitting the pixels in a raster-scan order. Perhaps surprisingly, but consistent with previous results on NADE [5] and by Frey [28], randomizing the order of the pixels made little difference to these results. The difference in performance was comparable to the differences between multiple runs with the same pixel ordering."}, {"heading": "4.3 Speech acoustics", "text": "We also measured the ability of RNADE to model small patches of speech spectrograms, extracted from the TIMIT dataset [29]. The patches contained 11 frames of 20 filter-banks plus energy; totaling 231 dimensions per datapoint. These filter-bank encoding is common in speech-recognition, and better for visualization than the more frequently used MFCC features. A good generative model of speech could be used, for example, in denoising, or speech detection tasks.\nWe fitted the models using the standard TIMIT training subset, and compared RNADE with a MoG by measuring their log-likelihood on the complete TIMIT core-test dataset.\nThe RNADE model has 1024 rectified-linear hidden units and a mixture of 20 one-dimensional Gaussian components per output. Given the larger scale of this dataset hyperparameter choices were again made manually using validation data, and the same minibatch training procedures for RNADE and MoG were used as for natural image patches.\nThe results are shown in Table 3. RNADE obtained, on average, 10 nats more per test example than a mixture of Gaussians. In Figure 2 a few examples from the test set, and samples from the MoG and RNADE models are shown. In contrast with the log-likelihood measure, there are no marked differences between the samples from each model. Both set of samples look like blurred spectrograms, but RNADE seems to capture sharper formant structures (peaks of energy at the lower frequency bands characteristic of vowel sounds)."}, {"heading": "5 Discussion", "text": "Mixture Density Networks (MDNs) [6] are a flexible conditional model of probability densities, that can capture skewed, heavy-tailed, and multi-modal distributions. In principle, MDNs can be applied to multi-dimensional data. However, the number of parameters that the network has to output grows quadratically with the number of targets, unless the targets are assumed independent. RNADE exploits an autoregressive framework to apply practical, one-dimensional MDNs to unsupervised density estimation.\nTo specify an RNADE we needed to set the parametric form for the output distribution of each MDN. A sufficiently large mixture of Gaussians can closely represent any density, but it is hard to learn the conditional densities found in some problems with this representation. The marginal for the brightness of a pixel in natural image patches is heavy tailed, closer to a Laplace distribution\nthan Gaussian. Therefore, RNADE-MoG must fit predictions of the first pixel, p(x1), with several Gaussians of different widths, that coincidentally have zero mean. This solution can be difficult to fit, and RNADE with a mixture of Laplace outputs predicted the first pixel of image patches better than with a mixture of Gaussians (Figure 3b and c). However, later pixels were predicted better with Gaussian outputs (Figure 3f); the mixture of Laplace model is not suitable for predicting with large contexts. For image patches, a scale mixture can work well [11], and could be explored within our framework. However for general applications, scale mixtures within RNADE would be too restrictive (e.g., p(x1) would be zero-mean and unimodal). More flexible one-dimensional forms may aid RNADE to generalize better for different context sizes and across a range of applications.\nOne of the main drawbacks of RNADE, and of neural networks in general, is the need to decide the value of several training hyperparameters. The gradient descent learning rate can be adjusted automatically using, for example, the techniques developed by Schaul et al. [30]. Also, methods for choosing hyperparameters more efficiently than grid search have been recently developed [31, 32]. These, and several other recent improvements in the neural network field, like dropouts [33], should be directly applicable to RNADE, and possibly obtain even better performance than shown in this work. RNADE makes it relatively straight-forward to translate advances in the neural-network field into better density estimators, or at least into new estimators with different inductive biases.\nIn summary, we have presented RNADE, a novel \u2018black-box\u2019 density estimator. Both likelihood computation time and the number of parameters scale linearly with the dataset dimensionality. Generalization across a range of tasks, representing arbitrary feature vectors, image patches, and auditory spectrograms is excellent. Performance on image patches was close to a recently reported state-of-the-art mixture model [3], and RNADE outperformed mixture models on all other datasets considered."}, {"heading": "Acknowledgments", "text": "We thank John Bridle, Steve Renals, Amos Storkey, and Daniel Zoran for useful interactions.\nA Implementation details\nIn this appendix we provide pseudo-code for the calculation of densities and learning gradients. No new material is presented. A Python implementation of RNADE is available from http://www.benignouria.com/en/research/RNADE.\nAlgorithm 1 Computation of p(x) a\u2190 c p(x)\u2190 1 for d from 1 to D do \u03c8d \u2190 \u03c1da . Rescaling factors hd \u2190 \u03c8d 1\u03c8d>0 . Rectified linear units z\u03b1d \u2190 V \u03b1d >hd + b \u03b1 d\nz\u00b5d \u2190 V \u00b5d > hd + b \u00b5 d z\u03c3d \u2190 V \u03c3d >hd + b \u03c3 d \u03b1d \u2190 softmax(z\u03b1d ) . Enforce constraints \u00b5d \u2190 z\u00b5d \u03c3d \u2190 exp(z\u03c3d ) p(x)\u2190 p(x)pMoG(xd;\u03b1d,\u00b5d,\u03c3d) . pMoG is the density of a mixture of Gaussians a\u2190 a+ xdW \u00b7,d . Activations are calculated recursively, xd is a scalar\nend for return p(x)\nAlgorithm 2 Computation of the learning gradients for a datapoint x a\u2190 c for d from 1 to D do . Compute the activation of the last dimension a\u2190 a+ xdW \u00b7,d\nend for for d from D to 1 do . Backpropagate errors \u03c8 \u2190 \u03c1da . Rescaling factors h\u2190 \u03c8 1\u03c8>0 . Rectified linear units z\u03b1 \u2190 V \u03b1d>h+ b\u03b1d z\u00b5 \u2190 V \u00b5d > h+ b\u00b5d\nz\u03c3 \u2190 V \u03c3d>hd + b\u03c3d \u03b1\u2190 softmax(z\u03b1) . Enforce constraints \u00b5\u2190 z\u00b5 \u03c3 \u2190 exp(z\u03c3) \u03c6\u2190 12 (\u00b5\u2212xd)2 \u03c32 \u2212 log\u03c3 \u2212 12 log(2\u03c0) . Calculate gradients \u03c0 \u2190 \u03b1\u03c6\u2211K j=1 \u03b1j\u03c6j \u2202z\u03b1 \u2190 \u03c0 \u2212\u03b1 \u2202V \u03b1d \u2190 \u2202z\u03b1h \u2202b\u03b1d \u2190 \u2202z\u03b1 \u2202z\u00b5 \u2190 \u03c0(xd \u2212 \u00b5)/\u03c32 \u2202z\u00b5 \u2190 \u2202z\u00b5 \u2217 \u03c3 . Move tighter components slower, allows higher learning rates \u2202V \u00b5d \u2190 \u2202z\u00b5h \u2202b\u00b5d \u2190 \u2202z\u00b5 \u2202z\u03c3 \u2190 \u03c0{(xd \u2212 \u00b5)2/\u03c32 \u2212 1} \u2202V \u03c3d \u2190 \u2202z\u03c3h \u2202b\u03c3d \u2190 \u2202z\u03c3 \u2202h\u2190 \u2202z\u03b1V \u03b1d + \u2202z\u00b5V \u00b5d + \u2202z\u03c3V \u03c3d \u2202\u03c8 \u2190 \u2202h1\u03c8>0 . Second factor: indicator function with condition \u03c8 > 0 \u2202\u03c1d \u2190 \u2211 j \u2202\u03c8jaj \u2202a\u2190 \u2202a+ \u2202\u03c8\u03c1 \u2202W \u00b7,d \u2190 \u2202axd if d = 1 then\n\u2202c\u2190 \u2202a else a\u2190 a\u2212 xdW \u00b7,d\nend if end for\nreturn \u2202\u03c1, \u2202W , \u2202c, \u2202b\u03b1, \u2202V \u03b1, \u2202b\u00b5, \u2202V \u00b5, \u2202b\u03c3 , \u2202V \u03c3\nIn Algorithm 1 we detail the pseudocode for calculating the density of a datapoint under an RNADE with mixture of Gaussian conditionals. The model has parameters: \u03c1 \u2208 RD,W \u2208 RH\u00d7D\u22121, c \u2208 RH , b\u03b1 \u2208 RD\u00d7K , V \u03b1 \u2208 RD\u00d7H\u00d7K , b\u00b5 \u2208 RD\u00d7K , V \u00b5 \u2208 RD\u00d7H\u00d7K , b\u03c3 \u2208 RD\u00d7K , V \u03c3 \u2208 RD\u00d7H\u00d7K\nTraining of an RNADE model can be done using a gradient ascent algorithm on the log-likelihood of the model given the training data. Gradients can be calculated using automatic differentiation libraries (e.g. Theano [34]). However we found our manual implementation to work faster in practice, possibly due to our recomputation of the a terms in the second for loop in Algorithm 2, which is more cache-friendly than storing them during the first loop.\nHere we show the derivation of the gradients of each parameter of a NADE model with MoG conditionals. Following [6], we define \u03c6i(xd |x<d) as the density of xd under the i-th component of the conditional:\n\u03c6i(xd |x<d) = 1\u221a\n2\u03c0\u03c3d,i exp\n{ \u2212 (xd \u2212 \u00b5d,i) 2\n2\u03c32d,i\n} , (9)\nand \u03c0i(xd |x<d) as the \u201cresponsibility\u201d of the i-th component for xd:\n\u03c0i(xd |x<d) = \u03b1d,i\u03c6i(xd |x<d)\u2211K j=1\u03b1d,j\u03c6j(xd |x<d) . (10)\nIt is easy to find just by taking their derivatives that:\n\u2202p(x) \u2202z\u03b1d,i = \u03c0i(xd |x<d)\u2212\u03b1d,i (11)\n\u2202p(x) \u2202z\u00b5d,i = \u03c0i(xd |x<d) xd \u2212 \u00b5d,i \u03c32d,i\n(12)\n\u2202p(x) \u2202z\u03c3d,i = \u03c0i(xd |x<d)\n{ (xd \u2212 \u00b5d,i)2 \u03c32d,i \u2212 1 }\n(13)\nUsing the chain rule we can calculate the derivative of the parameters of the output layer parameters:\n\u2202p(x) \u2202V \u03b1d = \u2202p(x) \u2202z\u03b1d,i \u2202z\u03b1d,i V \u03b1d = \u2202p(x) \u2202z\u03b1d,i h (14)\n\u2202p(x) \u2202b\u03b1d = \u2202p(x) \u2202z\u03b1d,i \u2202z\u03b1d,i b\u03b1d = \u2202p(x) \u2202z\u03b1d,i (15)\n\u2202p(x) \u2202V \u00b5d = \u2202p(x) \u2202z\u00b5d,i \u2202z\u03b1d,i V \u00b5d = \u2202p(x) \u2202z\u00b5d,i h (16)\n\u2202p(x) \u2202b\u00b5d = \u2202p(x) \u2202z\u00b5d,i \u2202z\u03b1d,i b\u00b5d = \u2202p(x) \u2202z\u00b5d,i (17)\n\u2202p(x) \u2202V \u03c3d = \u2202p(x) \u2202z\u03c3d,i \u2202z\u03b1d,i V \u03c3d = \u2202p(x) \u2202z\u03c3d,i h (18)\n\u2202p(x) \u2202b\u03c3d = \u2202p(x) \u2202z\u03c3d,i \u2202z\u03b1d,i b\u03c3d = \u2202p(x) \u2202z\u03c3d,i (19)\nBy \u201cbackpropagating\u201d the we can calculate the partial derivatives with respect to the output of the hidden units:\n\u2202p(x) \u2202hd = \u2202p(x) \u2202z\u03b1d,i \u2202z\u03b1d,i \u2202hd + \u2202p(x) \u2202z\u00b5d,i \u2202z\u00b5d,i \u2202hd + \u2202p(x) \u2202z\u03c3d,i \u2202z\u03c3d,i \u2202hd\n(20)\n= \u2202p(x)\n\u2202z\u03b1d,i V \u03b1d +\n\u2202p(x) \u2202z\u00b5d,i V \u00b5d + \u2202p(x) \u2202z\u03c3d,i V \u03c3d (21)\nand calculate the partial derivatives with respect to all other parameters in RNADE:\n\u2202p(x) \u2202\u03c8d = \u2202p(x) \u2202hd 1\u03c8d>0 (22)\n\u2202p(x) \u2202\u03c1d = \u2211 j \u2202p(x) \u2202\u03c8d,j ad,j (23)\n\u2202p(x) \u2202ad = \u2202p(x) \u2202ad+1 + \u2202p(x) \u2202hd \u03c1d1\u03c8d>0 (24)\n\u2202p(x) \u2202W \u00b7,d = \u2202p(x) \u2202ad xd (25)\n\u2202p(x) \u2202c = \u2202p(x) \u2202a1 (26)\nNote that gradients are calculated recursively, due to (24), starting at d = D and progressing down to d = 1."}], "references": [{"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Estimation of a multivariate density", "author": ["T. Cacoullos"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1966}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Technical Report NCRG 4288,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Does the wake-sleep algorithm produce good density estimators", "author": ["B.J. Frey", "G.E. Hinton", "P. Dayan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Discussion of the neural autoregressive distribution estimator", "author": ["Y. Bengio"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations", "author": ["L. Theis", "R. Hosseini", "M. Bethge"], "venue": "PLoS ONE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Gaussian process networks", "author": ["N. Friedman", "I. Nachman"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "In all likelihood, deep belief is not enough", "author": ["L. Theis", "S. Gerwinn", "F. Sinz", "M. Bethge"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M.A. Ranzato", "G.E. Hinton"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A spike and slab restricted Boltzmann machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, W&CP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neuro-computing: algorithms, architectures and applications, pages 227\u2013236", "author": ["J.S. Bridle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "SHORTEN: simple lossless and near-lossless waveform compression", "author": ["T. Robinson"], "venue": "Technical Report CUED/F-INFENG/TR.156,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Deep mixtures of factor analysers", "author": ["Y. Tang", "R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Natural images, Gaussian mixtures and dead leaves", "author": ["D. Zoran", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Mixed cumulative distribution networks", "author": ["R. Silva", "C. Blundell", "Y.W. Teh"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "The EM algorithm for mixtures of factor analyzers", "author": ["Z. Ghahramani", "G.E. Hinton"], "venue": "Technical Report CRG-TR-96-1, University of Toronto,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Mixture of factor analyzers", "author": ["J. Verbeek"], "venue": "Matlab implementation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Graphical models for machine learning and digital communication", "author": ["B. Frey"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Timit acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren", "V. Zue"], "venue": "Linguistic Data Consortium,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1993}, {"title": "No More Pesky Learning Rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "In Proceedings of the 30th international conference on Machine learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Arxiv preprint arXiv:1207.0580,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "It has long been appreciated that large classes of densities can be estimated consistently by kernel density estimation [2], and a large mixture of Gaussians can closely represent any density.", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "In practice, a parametric mixture of Gaussians seems to fit the distribution over patches of pixels and obtains state-of-the-art restorations [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "Restricted Boltzmann Machines (RBMs), which are undirected graphical models, fit samples of binary vectors from a range of sources better than mixture models [4, 5].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "Restricted Boltzmann Machines (RBMs), which are undirected graphical models, fit samples of binary vectors from a range of sources better than mixture models [4, 5].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "The Neural Autoregressive Distribution Estimator (NADE) overcomes these difficulties [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "We use the parameter sharing previously introduced by NADE, combined with mixture density networks [6], an existing flexible approach to modeling real-valued distributions with neural networks.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "Both NADE [5] and our RNADE model are based on the chain rule (or product rule), which factorizes any distribution over a vector of variables into a product of terms: p(x) = \u220fD d=1 p(xd | x<d), where x<d denotes all attributes preceding xd in a fixed arbitrary ordering of the attributes.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "For binary data, each conditional distribution can be modeled with logistic regression, which is called a fully visible sigmoid belief network (FVSBN) [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 7, "context": "Neural networks can also be used for each binary prediction task [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "The neural autoregressive distribution estimator (NADE) also uses neural networks for each conditional, but with parameter sharing inspired by a mean-field approximation to Restricted Boltzmann Machines [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Untying these weights gave better statistical performance on a range of tasks, with negligible extra computational cost [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "NADE has recently been extended to count data [9].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "The possibility of extending generic neural autoregressive models to continuous data has been mentioned [8, 10], but has not been previously explored to our knowledge.", "startOffset": 104, "endOffset": 111}, {"referenceID": 9, "context": "The possibility of extending generic neural autoregressive models to continuous data has been mentioned [8, 10], but has not been previously explored to our knowledge.", "startOffset": 104, "endOffset": 111}, {"referenceID": 10, "context": "An autoregressive mixture of experts with scale mixture model experts has been developed as part of a sophisticated multi-resolution model specifically for natural images [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "In more general work, Gaussian processes have been used to model the conditional distributions of a fully visible Bayesian network [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "limitations of the Gaussian-RBM are well documented [13, 14]: its isotropic conditional noise model does not give competitive density estimates.", "startOffset": 52, "endOffset": 60}, {"referenceID": 13, "context": "limitations of the Gaussian-RBM are well documented [13, 14]: its isotropic conditional noise model does not give competitive density estimates.", "startOffset": 52, "endOffset": 60}, {"referenceID": 14, "context": "Approximating a more capable RBM model, such as the mean-covariance RBM [15] or the spike-and-slab RBM [16], might be a fruitful future direction.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Approximating a more capable RBM model, such as the mean-covariance RBM [15] or the spike-and-slab RBM [16], might be a fruitful future direction.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "That is, the outputs are mixture density networks [6], with a shared hidden layer, using the same parameter tying as NADE.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "As discussed by Bengio [10], as an RNADE (or a NADE) with sigmoidal units progresses across the input dimensions d \u2208 {1 .", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Previous work on neural networks with real-valued outputs has found that rectified linear units can work better than sigmoidal non-linearities [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "The softmax [18] ensures the mixing fractions are positive and sum to one, the exponential ensures the standard deviations are positive.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We compared RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 20, "context": "We compared RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 3, "context": "Given the known poor performance of discrete mixtures [4, 5], we limited our experiments to modeling continuous attributes.", "startOffset": 54, "endOffset": 60}, {"referenceID": 4, "context": "Given the known poor performance of discrete mixtures [4, 5], we limited our experiments to modeling continuous attributes.", "startOffset": 54, "endOffset": 60}, {"referenceID": 21, "context": "We first considered five UCI datasets [22], previously used to study the performance of other density estimators [23, 20].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "We first considered five UCI datasets [22], previously used to study the performance of other density estimators [23, 20].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "[20], we eliminated discrete-valued attributes and an attribute from every pair with a Pearson correlation coefficient greater than 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The MFA models were trained using the EM algorithm [24, 25], the number of components and factors were crossvalidated.", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "The MFA models were trained using the EM algorithm [24, 25], the number of components and factors were crossvalidated.", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Unfortunately we could not reproduce the data-folds used by previous work, however, our improvements are larger than those demonstrated by a deep mixture of factor analyzers over standard MFA [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "Following the recent work of Zoran and Weiss [3], we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset [26] (Figure 1 gives examples).", "startOffset": 45, "endOffset": 48}, {"referenceID": 24, "context": "Following the recent work of Zoran and Weiss [3], we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset [26] (Figure 1 gives examples).", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "We then divided by 256, making each pixel take a value in the range [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "In previous experiments, Zoran and Weiss [3] subtracted the mean pixel value from each patch, reducing the dimensionality of the data by one: the value of any pixel could be perfectly predicted as minus the sum of all other pixel values.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Such a model could obtain arbitrarily high model likelihoods, so unfortunately the likelihoods reported in previous work on this dataset [3, 20] are difficult to interpret.", "startOffset": 137, "endOffset": 144}, {"referenceID": 19, "context": "Such a model could obtain arbitrarily high model likelihoods, so unfortunately the likelihoods reported in previous work on this dataset [3, 20] are difficult to interpret.", "startOffset": 137, "endOffset": 144}, {"referenceID": 20, "context": "Recent work by Zoran and Weiss [21], projects the data on the leading 63 eigenvectors of each component, when measuring the model likelihood [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "Perhaps surprisingly, but consistent with previous results on NADE [5] and by Frey [28], randomizing the order of the pixels made little difference to these results.", "startOffset": 67, "endOffset": 70}, {"referenceID": 25, "context": "Perhaps surprisingly, but consistent with previous results on NADE [5] and by Frey [28], randomizing the order of the pixels made little difference to these results.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "We also measured the ability of RNADE to model small patches of speech spectrograms, extracted from the TIMIT dataset [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "Mixture Density Networks (MDNs) [6] are a flexible conditional model of probability densities, that can capture skewed, heavy-tailed, and multi-modal distributions.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "For image patches, a scale mixture can work well [11], and could be explored within our framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Also, methods for choosing hyperparameters more efficiently than grid search have been recently developed [31, 32].", "startOffset": 106, "endOffset": 114}, {"referenceID": 29, "context": "Also, methods for choosing hyperparameters more efficiently than grid search have been recently developed [31, 32].", "startOffset": 106, "endOffset": 114}, {"referenceID": 30, "context": "These, and several other recent improvements in the neural network field, like dropouts [33], should be directly applicable to RNADE, and possibly obtain even better performance than shown in this work.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "Performance on image patches was close to a recently reported state-of-the-art mixture model [3], and RNADE outperformed mixture models on all other datasets considered.", "startOffset": 93, "endOffset": 96}, {"referenceID": 31, "context": "Theano [34]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "Following [6], we define \u03c6i(xd |x<d) as the density of xd under the i-th component of the conditional:", "startOffset": 10, "endOffset": 13}], "year": 2014, "abstractText": "We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.", "creator": "LaTeX with hyperref package"}}}