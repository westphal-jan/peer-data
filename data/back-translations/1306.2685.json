{"id": "1306.2685", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2013", "title": "Flexible sampling of discrete data correlations without the marginal distributions", "abstract": "Learning the mutual dependence of discrete variables is a fundamental problem in machine learning, with many applications such as prediction, clustering, and dimensionality reduction. More recently, the framework of copula modelling has become increasingly popular due to its modular parameterization of joint distributions. Among other features, copula offer a recipe for combining flexible models for universal boundary distributions with parametric families that are suitable for potentially high-dimensional dependency structures. More radical is Hoff's (2007) extended precedence probability approach, which completely bypasses learning boundary models when this information is complementary to the present learning task, such as standard dimensionality problems or copula parameter estimates. The main idea is to present data based on observable precedence statistics and ignore other information from marginal areas.", "histories": [["v1", "Wed, 12 Jun 2013 01:13:46 GMT  (2997kb,AD)", "http://arxiv.org/abs/1306.2685v1", null], ["v2", "Thu, 8 Aug 2013 18:23:45 GMT  (3495kb,AD)", "http://arxiv.org/abs/1306.2685v2", "An overhauled version of the experimental section has been added in the supplementary material"], ["v3", "Thu, 14 Nov 2013 15:31:46 GMT  (3356kb,AD)", "http://arxiv.org/abs/1306.2685v3", "An overhauled version of the experimental section moved to the main paper. Old experimental section moved to supplementary material"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["alfredo a kalaitzis", "ricardo bezerra de andrade e silva"], "accepted": true, "id": "1306.2685"}, "pdf": {"name": "1306.2685.pdf", "metadata": {"source": "CRF", "title": "Flexible Sampling for the Gaussian Copula Extended Rank Likelihood Model", "authors": ["Alfredo Kalaitzis", "Ricardo Silva"], "emails": ["a.kalaitzis@ucl.ac.uk", "ricardo@stats.ucl.ac.uk"], "sections": [{"heading": "1 Contribution", "text": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6]. More recently, the idea of copula modeling [15] has been combined with such standard building blocks. Our contribution is a novel algorithm for efficient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [16].\nA copula is a continuous cumulative distribution function (CDF) with uniformly distributed univariate marginals in the unit interval [0, 1]. It complements graphical models and other formalisms that provide a modular parameterization of joint distributions. The core idea is simple and given by the following observation: suppose we are given a (say) bivariate CDF F (y1, y2) with marginals F1(y1) and F2(y2). This CDF can then be rewritten as F (F\u221211 (F1(y1)), F \u22121 2 (F2(y2))). The function C(\u00b7, \u00b7) given by F (F\u221211 (\u00b7), F \u22121 2 (\u00b7)) is a copula. For discrete distributions, this decomposition is not unique but still well-defined [15]. Copulas have found numerous applications in statistics and machine learning since they provide a way of constructing flexible multivariate distributions by mix-and-matching different copulas with different univariate marginals. For instance, one can combine flexible univariate marginals Fi(\u00b7) with useful but more constrained high-dimensional copulas. We will not further motivate the use of copula models, which has been discussed at length in recent machine learning publications and conference workshops, and for which comprehensive textbooks\nar X\niv :1\n30 6.\n26 85\nv1 [\nst at\n.M L\n] 1\n2 Ju\nn 20\nexist [e.g., 9]. For a recent discussion on the applications of copulas from a machine learning perspective, [4] provides an overview. [10] is an early reference in machine learning. The core idea dates back at least to the 1950s [15].\nIn the discrete case, copulas can be difficult to apply: transforming a copula CDF into a probability mass function (PMF) is computationally intractable in general. For the continuous case, a common trick goes as follows: transform variables by defining ai \u2261 F\u0302i(yi) for an estimate of Fi(\u00b7) and then fit a copula density c(\u00b7, . . . , \u00b7) to the resulting ai [e.g. 9]. It is not hard to check this breaks down in the discrete case [7]. An alternative is to represent the CDF to PMF transformation for each data point by a continuous integral on a bounded space. Sampling methods can then be used. This trick has allowed many applications of the Gaussian copula to discrete domains. Readers familiar with probit models will recognize the similarities to models where an underlying latent Gaussian field is discretized into observable integers as in Gaussian process classifiers and ordinal regression [17]. Such models can be indirectly interpreted as special cases of the Gaussian copula.\nIn what follows, we describe in Section 2 the Gaussian copula and the general framework for constructing Bayesian estimators of Gaussian copulas by [7], the extended rank likelihood framework. This framework entails computational issues which are discussed. A recent general approach for MCMC in constrained Gaussian fields by [16] can in principle be directly applied to this problem as a blackbox, but at a cost that scales quadratically in sample size and as such it is not practical in general. Our key contribution is given in Section 4. An illustrative experiment is performed in Section 5. Conclusions are discussed in the final section."}, {"heading": "2 Gaussian Copulas and the Extended Rank Likelihood", "text": "It is not hard to see that any multivariate Gaussian copula is fully defined by a correlation matrix C, since marginal distributions have no free parameters. In practice, the following equivalent generative model is used to define a sample U according to a Gaussian copula GC(C):\n1. Sample Z from a zero mean Gaussian with covariance matrix C 2. For each Zj , set Uj = \u03a6(zj), where \u03a6(\u00b7) is the CDF of the standard Gaussian\nIt is clear that each Uj follows a uniform distribution in [0, 1]. To obtain a model for variables {y1, y2, . . . , yp} with marginal distributions Fj(\u00b7) and copula GC(C), one can add the deterministic step yj = F\u22121j (uj). Now, given n samples of observed data Y \u2261 {y (1) 1 , . . . , y (1) p , y (2) 1 , . . . , y (n) p }, one is interested on inferring C via a Bayesian approach and the posterior distribution p(C, \u03b8F |Y) \u221d pGC(Y | C, \u03b8F )\u03c0(C, \u03b8F )\nwhere \u03c0(\u00b7) is a prior distribution, \u03b8F are marginal parameters for each Fj(\u00b7), which in general might need to be marginalized since they will be unknown, and pGC(\u00b7) is the PMF of a (here discrete) distribution with a Gaussian copula and marginals given by \u03b8F .\nLet Z be the underlying latent Gaussians of the corresponding copula for dataset Y. Although Y is a deterministic function of Z, this mapping is not invertible due to the discreteness of the distribution: each marginal Fj(\u00b7) has jumps. Instead, the reverse mapping only enforces the constraints where y (i1) j < y (i2) j implies z (i1) j < z (i2) j . Based on this observation, [7] considers the event Z \u2208 D, where D is the set of values of Z in Rn\u00d7p obeying those constraints, that is\nZ \u2208 Rn\u00d7p : max{z(k)j s.t. y (k) j < y (i) j } < z (i) j < min{z (k) j s.t. y (i) j < y (k) j }\nSince {Y = y} \u21d2 Z(y) \u2208 D, we have pGC(Y | C, \u03b8F ) = pGC(Z \u2208 D,Y | C, \u03b8F )\n= pN (Z \u2208 D | C)\u00d7 pGC(Y| Z \u2208 D,C, \u03b8F ), (1)\nthe first factor of the last line being that of a zero-mean a Gaussian density function marginalized over D.\nThe extended rank likelihood is defined by the first factor of (1). With this likelihood, inference for C is given simply by marginalizing\np(C,Z |Y) \u221d I(Z \u2208 D)pN (Z| C)\u03c0(C), (2)\nthe first factor of the right-hand side being the usual binary indicator function.\nStrictly speaking, this is not a fully Bayesian method since partial information on the marginals is ignored. Nevertheless, it is possible to show that under some mild conditions that there is information in the extended rank likelihood to consistently estimate C [13]. It has two important properties: first, in many applications where marginal distributions are completely ancillary, this sidesteps any major assumptions about the shape of {Fi(\u00b7)} \u2013 applications include learning the degree of dependence among variables (e.g., to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.g., [5]); second, MCMC inference in the extended rank likelihood is considerably simpler than with the joint likelihood, since dropping marginal models will remove complicated entanglements between C and \u03b8F . Therefore, even if \u03b8F is necessary (when, for instance, predicting missing values of Y), an estimate of C can be computed separately and will not depend on the choice of estimator for {Fi(\u00b7)}. The standard model with a full correlation matrix C can be further refined to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others.\nAn off-the-shelf algorithm for sampling from (2) is full Gibbs sampling: first, given Z, the (full or structured) correlation matrix C can be sampled by standard methods. More to the point, sampling Z is straightforward if for each variable j and data point i we sample Z(i)j conditioned on all other variables. The corresponding distribution is an univariate truncated Gaussian. This is the approached used originally by Hoff. However, mixing can be severely compromised by the sampling of Z, and that is where novel sampling methods can facilitate inference."}, {"heading": "3 Exact HMC for truncated Gaussian distributions", "text": "Hoff\u2019s algorithm modifies the positions of all Z(i)j associated with a particular discrete value of Yj , conditioned on the remaining points. As the number of data points increases, the spread of the hard boundaries on Z(i)j , given by data points of Zj associated with other levels of Yj , increases. This reduces the space in which variables Z(i)j can move at a time.\nTo improve the mixing, we aim to sample from the joint Gaussian distribution of all latent variables Z\n(i) j , i = 1 . . . n , conditioned on other columns of the data, such that the constraints between them are satisfied and thus the ordering in the observation level is conserved. Standard Gibbs approaches for sampling from truncated Gaussians reduce the problem to sampling from univariate truncated Gaussians. Even though these are efficient for covariances with high correlations1, mixing can be slow when artificial correlations are induced by very tight truncation bounds.\nIn the following, we briefly describe the methodology recently introduced by [16] that deals with the problem of sampling from log(x) \u221d \u2212 12x\n>Mx + r>x , where x, r \u2208 Rn and M is positive definite, with linear constraints of the form f>j x \u2264 gj , where fj \u2208 Rn, j = 1 . . .m, is the normal vector to some linear boundary in the sample space.\nLater in this section we shall describe how this framework can be applied to the Gaussian copula extended rank likelihood model. More importantly, the observed rank statistics impose only linear constraints of the form xi1 \u2264 xi2 . We shall describe how this special structure can be exploited to reduce the runtime complexity of the constrained sampler from O(n2) (in the number of observations) to O(n) in practice."}, {"heading": "3.1 Hamiltonian Monte Carlo (HMC) for the Gaussian Distribution", "text": "Hamiltonian Monte Carlo [14] is a MCMC method that extends the sampling space with auxiliary variables so that (ideally) deterministic moves in the joint space brings the sampler to potentially far places in the original variable space. Deterministic moves cannot in general be done, but this is possible in the Gaussian case.\n1First, samples are taken from an isotropic Gaussian. In the end, the Gibbs samples are transformed back to the original space.\nThe form of the Hamiltonian for the general d-dimensional Gaussian case with mean \u00b5 and precision matrix M is:\nH = 1 2 x>Mx\u2212 r>x + 1 2 s>M\u22121s , (3)\nwhere M is also known in the present context as the mass matrix, r = M\u00b5 and s is the velocity. Both x and s are Gaussian distributed so this Hamiltonian can be seen (up to a constant) as the negative log of the product of two independent Gaussian random variables 2.\nIn a system where this Hamiltonian function is constant, we can exactly compute its evolution through the pair of differential equations:\nx\u0307 = \u2207sH = M\u22121s , s\u0307 = \u2212\u2207xH = \u2212Mx + r . (4)\nThese are solved exactly by x(t) = \u00b5+a sin(t) +b cos(t) , where a and b can be identified at initial conditions (t = 0) :\na = x\u0307(0) = M\u22121s , b = x(0)\u2212 \u00b5 . (5)\nTherefore, the exact HMC algorithm can be summarised as follows:\n\u2022 Initialise the allowed travel time T and some initial position x0 . \u2022 Repeat for HMC samples k = 1 . . . N\n1. Sample sk \u223c N (0,M) 2. Use sk and xk to update a and b and store the new position at the end of the\ntrajectory xk+1 = x(T ) as an HMC sample.\nIt can be easily shown that the Markov chain of sampled positions has the desired equilibrium distribution N ( \u00b5,M\u22121 ) [16]."}, {"heading": "3.2 Sampling with linear constraints", "text": "The plot thickens when the target distribution is truncated by linear constraints of the form Fx \u2264 g . Here, F is a constraint matrix in Rm\u00d7n whose every row is the normal vector to a linear boundary in the sample space 3. In general, to remain within the boundaries of each wall, once a new velocity has been sampled one must compute all possible collision times with the walls. The smallest of all collision times signifies the wall that the particle should bounce from at that collision time. Figure 1 illustrates the concept with two simple examples on 2 and 3 dimensions.\n2A physical interpretation is that of a sum of potential and kinetic energy terms, where the total energy of the system is conserved.\n3Equivalent to sampling from a Gaussian that is confined in the convex polyhedron {x : Fx \u2264 g}, not necessarily bounded.\nThe collision times can be computed analytically and their equations can be found in the supplementary material. We also point the reader to [16] for a more detailed discussion of this implementation. Once the wall to be hit has been found, then position and velocity at impact time are computed and the velocity is reflected about the boundary normal4. The constrained HMC sampler is summarized follows:\n\u2022 Initialise the allowed travel time T and some initial position x0 .\n\u2022 Repeat for HMC samples k = 1 . . . N\n1. Sample sk \u223c N (0,M) 2. Use sk and xk to update a and b . 3. Reset remaining travel time Tleft \u2190 T . Until no travel time is left or no walls can be\nreached (no solutions exist), do: (a) Compute impact times with all walls and pick the smallest one, th (if a solution\nexists). (b) Compute v(th) and reflect it about the hyperplane fh . This is the updated\nvelocity after impact. The updated position is x(th) . (c) Tleft \u2190 Tleft \u2212 th\n4. Store the new position at the end of the trajectory xk+1 as an HMC sample.\nIn general, all walls are candidates for impact, so the runtime of the sampler is linear in m , the number of constraints. This means that the computational load is concentrated in step 3(a). Another consideration is that of the allocated travel time T . Depending on the shape of the bounding polyhedron and the number of walls, a very large travel time can induce many more bounces thus requiring more computations per sample. On the other hand, a very small travel time explores the distribution more locally so the mixing of the chain can suffer. What constitutes a given travel time \u201clarge\u201d or \u201csmall\u201d is relative to the dimensionality, the number of constraints and the structure of the constraints.\nAs we describe in the following section, due to the nature of our problem, the number of constraints is O(n2) . Clearly, this restricts any direct application of the HMC framework for Gaussian copula estimation to small-sample (n) datasets. More importantly, we show how to exploit the structure of the constraints to reduce the number of candidate walls (prior to each bounce) to O(n) ."}, {"heading": "4 HMC for the Gaussian Copula Extended Rank Likelihood Model", "text": "Given some discrete data Y \u2208 Rn\u00d7p , the task is to infer the correlation matrix of the underlying Gaussian copula. Hoff\u2019s sampling algorithm proceeds by alternating between sampling the continuous latent representation Z(i)j of each Y (i) j , for i = 1 . . . n, j = 1 . . . p , and sampling a covariance matrix from an inverse-Wishart distribution conditioned on the sampled matrix Z \u2208 Rn\u00d7p . 5, which is then renormalized as a correlation matrix.\nIn a similar vein to Hoff\u2019s sampling algorithm, we replace the successive sampling of each Zi,j conditioned on Zi,\\j (a conditional univariate truncated Gaussian) with the simultaneous sampling of Z:,j conditioned on Z:,\\j . This is done through an HMC step from a conditional multivariate truncated Gaussian.\nThe added benefit of this HMC step over the standard Gibbs approach, is that of a handle for regulating the trade-off between exploration and runtime via the allocated travel time T . Larger travel times potentially allow for larger moves in the sample space, but it comes at a cost as explained in the sequel.\n4Also equivalent to transforming the velocity with a Householder reflection matrix about the bounding hyperplane.\n5In the sequel, we switch to this matrix notation for referring to the samples, as opposed to the random variables, with Zi,j replacing Z (i) j and Z:,j being a column of data.\n4.1 The Hough Envelope algorithm\nThe special structure of constraints. Recall that the number of constraints is quadratic in the dimension of the distribution. This is because every Z sample must satisfy the conditions of the event Z \u2208 D of the extended rank likelihood (see Section 2). In other words, for any column Z:,j , all entries are organised into a partition L(j) of |L(j)| levels, the number of unique values observed for the discrete or ordinal variable Y (j) . Thereby, for any two adjacent levels lk, lk+1 \u2208 L(j) and any pair i1 \u2208 lk, i2 \u2208 lk+1, it must be true that Zli,j < Zli+1,j . Equivalently, a constraint f exists where fi1 = 1, fi2 = \u22121 and g = 0 . It is easy to see that O(n2) of such constraints are induced by the order statistics of the j-th variable. To deal with this boundary explosion, we developed the Hough Envelope algorithm to search efficiently, within all pairs in {Z:,j}, in practically linear time.\nRecall in HMC (section 3.2) that the trajectory of the particle, x(t), is decomposed as\nxi(t) = ai sin(t) + bi cos(t) + \u00b5i , (6)\nand there are n such functions, grouped into a partition of levels as described above. The Hough envelope6 is found for every pair of adjacent levels. We illustrate this with an example of 10 dimensions and two levels in Figure 2, without loss of generalization to any number of levels or dimensions. Assume we represent trajectories for points in level lk with blue curves, and points in lk+1 with red curves. Assuming we start with a valid state, at time t = 0 all red curves are above all blue curves. The goal is to find the smallest t where a blue curve meets a red curve. This will be our collision time where a bounce will be necessary.\n1. First we find the largest component bluemax of the blue level at t = 0. This takes O(n) time. Clearly, this will be the largest component until its sinusoid intersects that of any other component.\n2. To find the next largest component, compute the roots of xbluemax(t) \u2212 xi(t) = 0 for all components and pick the smallest (earliest) one (represented by a green dot). This also takes O(n) time.\n3. Repeat this procedure until a red sinusoid crosses the highest running blue sinusoid. When this happens, the time of earliest bounce and its constraint are found.\n6The name is inspired from the fact that each xi(t) is the sinusoid representation, in angle-distance space, of all lines that pass from the (ai, bi) point in a\u2212 b space. A representation known in image processing as the Hough transform [3].\nIn the worst-case scenario, n such repetition have to be made, but in practice we can safely assume an fixed upper bound h on the number of blue crossings before a inter-level crossing occurs. In experiments, we found h << n, no more than 10 repetitions in simulations with hundreds of thousands of curves. Thus, this search strategy takes O(n) time in practice to complete, mirroring the analysis of other output-sensitive algorithms such as the gift wrapping algorithm for computing convex hulls [8]. Our HMC sampling approach is summarized in Algorithm 1.\nAlgorithm 1 HMC for GCERL # Notation: TMN (\u00b5,C,F) is a truncated multivariate normal with mean \u00b5, covariance C and constraints encoded by F and g = 0 . # IW(df,V0) is an inverse-Wishart prior with degrees of freedom df and scale matrix V0 . Input: Y \u2208 Rn\u00d7p, allocated travel time T , a starting Z and variable covariance V \u2208 Rp\u00d7p , df = p+ 2, V0 = dfIp and chain size N . Generate constraints F(j) from Y:,j , for j = 1 . . . p . for samples k = 1 . . . N do\n# Resample Z as follows: for variables j = 1 . . . p do\nCompute parameters: \u03c32j = Vjj \u2212Vj,\\jV \u22121 \\j,\\jV\\j,j , \u00b5j = Z:,\\jV \u22121 \\j,\\jV\\j,j . Get one sample Z:,j \u223c TMN ( \u00b5j , \u03c3 2 j I, F (j) )\nefficiently by using the Hough Envelope algorithm, see section 4.1.\nend for Resample V \u223c IW(df + n,V0 + Z>Z) . Compute correlation matrix C, s.t. Ci,j = Vi,j/ \u221a Vi,iVj,j and store sample, C(k) \u2190 C .\nend for"}, {"heading": "5 Illustration", "text": "In theory, one can only expect mixing improvements over the algorithm outlined by Hoff, and we do not believe the computational cost of HMC with rank likelihood constraints can be reduced by more than a constant factor in the worst case. Empirically, however, it is not obvious how these theoretical advantages will translate into practice since the HMC method has a considerable overhead per iteration, nor it is clear in which way the better mixing truly pays-off. We perform a computational experiment in order to highlight the strengths and shortcomings of the proposed sampler against the simple but potentially effective Hoff algorithm. The desirable statistical properties of the extended rank likelihood are discussed elsewhere in detail in the references given, and as such we will focus solely on the computational aspects of inference."}, {"heading": "5.1 Setup", "text": "We generate synthetic data from a 10-dimensional multivariate binary distribution with a full, randomly sample, copula correlation matrix. A dataset of 10,000 samples is used as a detailed case study in the next section and provides a typical scenario. Posterior inference is done using a inverse Wishart prior with 12 degrees of freedom and a identity matrix scaled by 12, as in [7], and defining the implied prior over the copula correlation matrix by standardization of matrices given by this prior. We focus on binary models because this allows us to evaluate the effect of a single boundary and how just this one barrier affects Hoff\u2019s algorithm. We focus on full correlation matrices instead of sparse inverse matrices [2] or low-rank decompositions [13] because this allows us a minimal interference of the mixing properties of other parameters and latent variables on our evaluation."}, {"heading": "5.2 Illustrative Result and Analysis", "text": "To achieve a reasonable running time for our HMC implementation7 we reduced our travel time to the relatively small \u03c0/100. Figure 3 summarizes the main findings with some choices of copula\n7Written in unoptimized MATLAB code, which is believed to severely slow down the procedure due to long loops of bouncing steps that could be much better handled in a compiled language. Notice that in our implementation, the cost of each HMC step was roughly 50 times the cost of a Hoff step.\nV2xV9\nV1xV5\nV3xV10\ncorrelation coefficients out of the 10 dimensional case. The Supplementary Material has the full matrix. Color-coded depictions of copula correlations are shown, with sample-to-sample variability being relatively small for the HMC case after 30 iterations.\nIt is clear that the need for reducing travel time will not in general guarantee a high effective sample size. We observed increases of effective sample size at the order of a 1.5 to 5-fold improvement compared to the procedure by Hoff, which under our current MATLAB implementation does not seem to to justify the overhead. However, even at this modest level of travel time, the procedure does move the whole mass of underlying truncated Gaussian data points to its mode in far fewer iterations, when compared to Hoff\u2019s algorithm. This well within the expected behaviour, where the pure Gibbs procedure requires a large number of moves towards the right direction, since the whole data has to travel together based on changes at a single level at a time. Within this position, however, local exploration is somewhat similar for both methods. Our take-home message: with a sample size at the order of tens of thousands many bounces will be necessary, but even in this case there are good uses of an expensive HMC method that can bring a sampler efficiently to a proper location. While the plain full correlation model might have other simple ways of being initialized, this might not be clear with other extended rank likelihood models [13]. We provide a black-box approach that is by construction less sensitive to starting points, and has a built-in mechanism for trading-off the computational cost of its step against the quality of its mixing."}, {"heading": "6 Conclusion", "text": "Sampling large random vectors simultaneously in order to improve mixing is in general a very hard problem, and this is why clever methods such as HMC or elliptical slice sampling [12] are necessary. We expect that the method here developed is useful not only for those with data analysis problems within the large family of Gaussian copula extended rank likelihood models, but the method itself and its behaviour might provide some new insights on MCMC sampling in constrained spaces in general. Another direction of future work consists of exploring methods for elliptical copulas, and related possible extensions of general HMC for non-Gaussian copula models."}], "references": [{"title": "Discrete Multivariate Analysis: Theory and Practice", "author": ["Y. Bishop", "S. Fienberg", "P. Holland"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Copula Gaussian graphical models and their application to modeling functional disability data", "author": ["A. Dobra", "A. Lenkoski"], "venue": "Annals of Applied Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Use of the Hough transformation to detect lines and curves in pictures", "author": ["R.O. Duda", "P.E. Hart"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1972}, {"title": "Copulas and machine learning", "author": ["G. Elidan"], "venue": "Proceedings of the Copulae in Mathematical and Quantitative Finance workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Semiparametric principal component analysis", "author": ["F. Han", "H. Liu"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Extending the rank likelihood for semiparametric copula estimation", "author": ["P. Hoff"], "venue": "Annals of Applied Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "On the identification of the convex hull of a finite set of points in the plane", "author": ["R. Jarvis"], "venue": "Information Processing Letters,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1973}, {"title": "Multivariate Models and Dependence Concepts", "author": ["H. Joe"], "venue": "Chapman-Hall,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Learning with tree-averaged densities and distributions", "author": ["S. Kirshner"], "venue": "Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Elliptical slice sampling", "author": ["I. Murray", "R. Adams", "D. MacKay"], "venue": "JMLR Workshop and Conference Proceedings: AISTATS 2010,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Bayesian Gaussian copula factor models for mixed data", "author": ["J. Murray", "D. Dunson", "L. Carin", "J. Lucas"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "An Introduction to Copulas", "author": ["R. Nelsen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Exact Hamiltonian Monte Carlo for truncated multivariate Gaussians", "author": ["A. Pakman", "L. Paninski"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 188, "endOffset": 192}, {"referenceID": 5, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 14, "context": "More recently, the idea of copula modeling [15] has been combined with such standard building blocks.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Our contribution is a novel algorithm for efficient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [16].", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "Our contribution is a novel algorithm for efficient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [16].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "A copula is a continuous cumulative distribution function (CDF) with uniformly distributed univariate marginals in the unit interval [0, 1].", "startOffset": 133, "endOffset": 139}, {"referenceID": 14, "context": "For discrete distributions, this decomposition is not unique but still well-defined [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "For a recent discussion on the applications of copulas from a machine learning perspective, [4] provides an overview.", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "[10] is an early reference in machine learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The core idea dates back at least to the 1950s [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "It is not hard to check this breaks down in the discrete case [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 16, "context": "Readers familiar with probit models will recognize the similarities to models where an underlying latent Gaussian field is discretized into observable integers as in Gaussian process classifiers and ordinal regression [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "In what follows, we describe in Section 2 the Gaussian copula and the general framework for constructing Bayesian estimators of Gaussian copulas by [7], the extended rank likelihood framework.", "startOffset": 148, "endOffset": 151}, {"referenceID": 15, "context": "A recent general approach for MCMC in constrained Gaussian fields by [16] can in principle be directly applied to this problem as a blackbox, but at a cost that scales quadratically in sample size and as such it is not practical in general.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "It is clear that each Uj follows a uniform distribution in [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 6, "context": "Based on this observation, [7] considers the event Z \u2208 D, where D is the set of values of Z in Rn\u00d7p obeying those constraints, that is Z \u2208 Rn\u00d7p : max{z j s.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "Nevertheless, it is possible to show that under some mild conditions that there is information in the extended rank likelihood to consistently estimate C [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 6, "context": ", to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": ", to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": ", [5]); second, MCMC inference in the extended rank likelihood is considerably simpler than with the joint likelihood, since dropping marginal models will remove complicated entanglements between C and \u03b8F .", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "The standard model with a full correlation matrix C can be further refined to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others.", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "The standard model with a full correlation matrix C can be further refined to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others.", "startOffset": 224, "endOffset": 228}, {"referenceID": 15, "context": "In the following, we briefly describe the methodology recently introduced by [16] that deals with the problem of sampling from log(x) \u221d \u2212 12x >Mx + r>x , where x, r \u2208 R and M is positive definite, with linear constraints of the form f> j x \u2264 gj , where fj \u2208 R, j = 1 .", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Hamiltonian Monte Carlo [14] is a MCMC method that extends the sampling space with auxiliary variables so that (ideally) deterministic moves in the joint space brings the sampler to potentially far places in the original variable space.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "It can be easily shown that the Markov chain of sampled positions has the desired equilibrium distribution N ( \u03bc,M\u22121 ) [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "We also point the reader to [16] for a more detailed discussion of this implementation.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "A representation known in image processing as the Hough transform [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Thus, this search strategy takes O(n) time in practice to complete, mirroring the analysis of other output-sensitive algorithms such as the gift wrapping algorithm for computing convex hulls [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "Posterior inference is done using a inverse Wishart prior with 12 degrees of freedom and a identity matrix scaled by 12, as in [7], and defining the implied prior over the copula correlation matrix by standardization of matrices given by this prior.", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "We focus on full correlation matrices instead of sparse inverse matrices [2] or low-rank decompositions [13] because this allows us a minimal interference of the mixing properties of other parameters and latent variables on our evaluation.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "We focus on full correlation matrices instead of sparse inverse matrices [2] or low-rank decompositions [13] because this allows us a minimal interference of the mixing properties of other parameters and latent variables on our evaluation.", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "While the plain full correlation model might have other simple ways of being initialized, this might not be clear with other extended rank likelihood models [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 11, "context": "Sampling large random vectors simultaneously in order to improve mixing is in general a very hard problem, and this is why clever methods such as HMC or elliptical slice sampling [12] are necessary.", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parameterization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increase quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.", "creator": "LaTeX with hyperref package"}}}