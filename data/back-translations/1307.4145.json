{"id": "1307.4145", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jul-2013", "title": "A Safe Screening Rule for Sparse Logistic Regression", "abstract": "L1-regulated logistic regression (or sparse logistic regression) is a widely used method of simultaneous classification and feature selection. Although many recent efforts have been used to efficiently implement it, applying it to high-dimensional data still poses significant challenges. In this paper, we present a fast and effective rule on sparse logistic regression screening (Slores) to identify the 0 components in the solution vector, which can result in a significant reduction in the number of features that need to be entered for optimization. An attractive feature of Slores is that the dataset only needs to be scanned once to perform the screening, and its compression costs are negligible compared to those of solving the sparse logistic regression problem. Furthermore, Slores is independent of solvers for sparse logistic regression, so that Slores can be integrated with any existing solvent to improve efficiency.", "histories": [["v1", "Tue, 16 Jul 2013 02:03:51 GMT  (304kb,D)", "https://arxiv.org/abs/1307.4145v1", null], ["v2", "Thu, 18 Jul 2013 22:57:16 GMT  (304kb,D)", "http://arxiv.org/abs/1307.4145v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "jiayu zhou", "jun liu 0003", "peter wonka", "jieping ye"], "accepted": true, "id": "1307.4145"}, "pdf": {"name": "1307.4145.pdf", "metadata": {"source": "CRF", "title": "A Safe Screening Rule for Sparse Logistic Regression", "authors": ["Jie Wang", "Jiayu Zhou", "Jun Liu", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Logistic regression (LR) is a popular and well established classification method that has been widely used in many domains such as machine learning [5, 8], text mining [4, 9], image processing [10, 17], bioinformatics [1, 15, 22, 30, 31], medical and social sciences [2, 19] etc. When the number of feature variables is large compared to the number of training samples, logistic regression is prone to over-fitting. To reduce overfitting, regularization has been shown to be a promising approach. Typical examples include `2 and `1 regularization. Although `1 regularized LR is more challenging to solve compared to `2 regularized LR, it has received much attention in the last few years and the interest in it is growing [23, 27, 31] due to the increasing prevalence of high-dimensional data. The most appealing property of `1 regularized LR is the sparsity of the resulting models, which is equivalent to feature selection.\nIn the past few years, many algorithms have been proposed to efficiently solve the `1 regularized LR [6, 14, 13, 20]. However, for large-scale problems, solving the `1 regularized LR with higher accuracy remains challenging. One promising solution is by \u201cscreening\u201d, that is, we first identify the \u201cinactive\u201d features, which have 0 coefficients in the solution and then discard them from the optimization. This would result in a reduced feature matrix and substantial savings in computational cost and memory size. In [7], El Ghaoui et al. proposed novel screening rules, called \u201cSAFE\u201d, to accelerate the optimization for a class of `1 regularized problems, including LASSO [25], `1 regularized LR and `1 regularized support vector machines. Inspired by SAFE, Tibshirani et al. [24] proposed \u201cstrong rules\u201d for a large class of `1 regularized problems, including LASSO, elastic net, `1 regularized LR and more general convex problems. In [29, 28], Xiang et al. proposed \u201cDOME\u201d rules to further improve SAFE rules for LASSO based on the observation that SAFE rules can be understood as a special case of the general \u201csphere test\u201d. Although both strong rules and the sphere tests are more effective in discarding features than SAFE for solving\nar X\niv :1\n30 7.\n41 45\nv2 [\ncs .L\nG ]\n1 8\nJu l 2\n01 3\nLASSO, it is worthwhile to mention that strong rules may mistakenly discard features that have non-zero coefficients in the solution and the sphere tests are not easy to be generalized to handle the `1 regularized LR. To the best of our knowledge, the SAFE rule is the only screening test for the `1 regularized LR that is \u201csafe\u201d, that is, it only discards features that are guaranteed to be absent from the resulting models.\nIn this paper, we develop novel screening rules, called \u201cSlores\u201d, for the `1 regularized LR. The proposed screening tests detect inactive features by estimating an upper bound of the inner product between each feature vector and the \u201cdual optimal solution\u201d of the `1 regularized LR, which is unknown. The more accurate the estimation is, the more inactive features can be detected. An accurate estimation of such an upper bound turns out to be quite challenging. Indeed most of the key ideas/insights behind existing \u201csafe\u201d screening rules for LASSO heavily rely on the least square loss, which are not applicable for the `1 regularized LR case due to the presence of the logistic loss. To this end, we propose a novel framework to accurately estimate an upper bound. Our key technical contribution is to formulate the estimation of an upper bound of the inner product as a constrained convex optimization problem and show that it admits a closed form solution. Therefore, the\nestimation of the inner product can be computed efficiently. Our extensive experiments have shown that Slores discards far more features than SAFE yet requires much less computational efforts. In contrast with strong rules, Slores is \u201csafe\u201d, i.e., it never discards features which have non-zero coefficients in the solution. To illustrate the effectiveness of Slores, we compare Slores, strong rule and SAFE on a data set of prostate cancer along a sequence of 86 parameters equally spaced on the \u03bb/\u03bbmax scale from 0.1 to 0.95, where \u03bb is the parameter for the `1 penalty and \u03bbmax is the smallest tuning parameter [12] such that the solution of the `1 regularized LR is 0 [please refer to Eq. (1)]. The data matrix contains 132 patients with 15154 features. To measure the performance of different screening rules, we compute the rejection ratio which is the ratio between the number of features discarded by screening rules and the number of features with 0 coefficients in the solution. Therefore, the larger the rejection ratio is, the more effective the screening rule is. The results are shown in Fig. 1. Clearly, Slores discards far more features than SAFE especially when \u03bb/\u03bbmax is large while the strong rule is not applicable when \u03bb/\u03bbmax \u2264 0.5. We present more experimental results and discussions to demonstrate the effectiveness of Slores in Section 6."}, {"heading": "2 Basics and Motivations", "text": "In this section, we briefly review the basics of the `1 regularized LR and then motivate the general screening rules via the KKT conditions. Suppose we are given a set of training samples {xi}mi=1 and the associate labels b \u2208 <m, where xi \u2208 <p and bi \u2208 {1,\u22121} for all i \u2208 {1, . . . ,m}. The `1 regularized logistic regression is:\nmin \u03b2,c\n1\nm m\u2211 i=1 log(1 + exp(\u2212\u3008\u03b2, x\u0304i\u3009 \u2212 bic)) + \u03bb\u2016\u03b2\u20161, (LRP\u03bb)\nwhere \u03b2 \u2208 <p and c \u2208 < are the model parameters to be estimated, x\u0304i = bixi, and \u03bb > 0 is the tuning parameter. Let the data matrix be X \u2208 <m\u00d7p with the ith row being x\u0304i and the jth column being x\u0304j .\nLet C = {\u03b8 \u2208 <m : \u03b8i \u2208 (0, 1), i = 1, . . . ,m} and f(y) = y log(y) + (1 \u2212 y) log(1 \u2212 y) for y \u2208 (0, 1). The dual problem of (LRP\u03bb) (please refer to the supplement) is given by\nmin \u03b8\n{ g(\u03b8) = 1\nm m\u2211 i=1 f(\u03b8i) : \u2016X\u0304T \u03b8\u2016\u221e \u2264 m\u03bb, \u3008\u03b8,b\u3009 = 0, \u03b8 \u2208 C\n} . (LRD\u03bb)\nTo simplify notations, we denote the feasible set of problem (LRD\u03bb) as F\u03bb, and let (\u03b2\u2217\u03bb, c\u2217\u03bb) and \u03b8\u2217\u03bb be the optimal solutions of problems (LRP\u03bb) and (LRD\u03bb) respectively. In [12], the authors have shown that for some special choice of the tuning parameter \u03bb, both of (LRP\u03bb) and (LRD\u03bb) have closed form solutions. In fact, let P = {i : bi = 1}, N = {i : bi = \u22121}, and m+ and m\u2212 be the cardinalities of P and N respectively. We define\n\u03bbmax = 1 m\u2016X\u0304 T \u03b8\u2217\u03bbmax\u2016\u221e, (1)\nwhere\n[\u03b8\u2217\u03bbmax ]i =\n{ m\u2212\nm , if i \u2208 P, m+\nm , if i \u2208 N , i = 1, . . . ,m. (2)\n([\u00b7]i denotes the ith component of a vector.) Then, it is known [12] that \u03b2\u2217\u03bb = 0 and \u03b8\u2217\u03bb = \u03b8\u2217\u03bbmax whenever \u03bb \u2265 \u03bbmax. When \u03bb \u2208 (0, \u03bbmax], it is known that (LRD\u03bb) has a unique optimal solution. (For completeness, we include the proof in the supplement.) We can now write the KKT conditions of problems (LRP\u03bb) and (LRD\u03bb) as\n\u3008\u03b8\u2217\u03bb, x\u0304j\u3009 \u2208  m\u03bb, if [\u03b2\u2217\u03bb]j > 0,\n\u2212m\u03bb, if [\u03b2\u2217\u03bb]j < 0, [\u2212m\u03bb,m\u03bb], if [\u03b2\u2217\u03bb]j = 0. j = 1, . . . , p. (3)\nIn view of Eq. (3), we can see that\n|\u3008\u03b8\u2217\u03bb, x\u0304j\u3009| < m\u03bb\u21d2 [\u03b2\u2217\u03bb]j = 0. (R1)\nIn other words, if |\u3008\u03b8\u2217\u03bb, x\u0304j\u3009 < m\u03bb, then the KKT conditions imply that the coefficient of x\u0304j in the solution \u03b2\u2217\u03bb is 0 and thus the j\nth feature can be safely removed from the optimization of (LRP\u03bb). However, for the general case in which \u03bb < \u03bbmax, (R1) is not applicable since it assumes the knowledge of \u03b8 \u2217 \u03bb. Although it is unknown, we can still estimate a region A\u03bb which contains \u03b8\u2217\u03bb. As a result, if max\u03b8\u2208A |\u3008\u03b8, x\u0304j\u3009| < m\u03bb, we can also conclude that [\u03b2\u2217\u03bb]j = 0 by (R1). In other words, (R1) can be relaxed as\nT (\u03b8\u2217\u03bb, x\u0304 j) := max \u03b8\u2208A\u03bb |\u3008\u03b8, x\u0304j\u3009| < m\u03bb\u21d2 [\u03b2\u2217\u03bb]j = 0. (R1\u2032)\nIn this paper, (R1\u2032) serves as the foundation for constructing our screening rules, Slores. From (R1\u2032), it is easy to see that screening rules with smaller T (\u03b8\u2217\u03bb, x\u0304\nj) are more aggressive in discarding features. To give a tight estimation of T (\u03b8\u2217\u03bb, x\u0304\nj), we need to restrict the region A\u03bb which includes \u03b8\u2217\u03bb as small as possible. In Section 3, we show that the estimation of the upper bound T (\u03b8\u2217\u03bb, x\u0304\nj) can be obtained via solving a convex optimization problem. We show in Section 4 that the convex optimization problem admits a closed form solution and derive Slores in Section 5 based on (R1\u2032)."}, {"heading": "3 Estimating the Upper Bound via Solving a Convex Optimiza-", "text": "tion Problem\nIn this section, we present a novel framework to estimate an upper bound T (\u03b8\u2217\u03bb, x\u0304 j) of |\u3008\u03b8\u2217\u03bb, x\u0304j\u3009|. In the subsequent development, we assume a parameter \u03bb0 and the corresponding dual optimal \u03b8 \u2217 \u03bb0\nare given. In our Slores rule to be presented in Section 5, we set \u03bb0 and \u03b8 \u2217 \u03bb0 to be \u03bbmax and \u03b8 \u2217 \u03bbmax given in Eqs. (1) and (2). We formulate the estimation of T (\u03b8\u2217\u03bb, x\u0304 j) as a constrained convex optimization problem in this section, which will be shown to admit a closed form solution in Section 4. For the dual function g(\u03b8), it follows that [\u2207g(\u03b8)]i = 1m log( \u03b8i 1\u2212\u03b8i ), [\u2207 2g(\u03b8)]i,i = 1 m 1 \u03b8i(1\u2212\u03b8i) \u2265 4 m . Since \u22072g(\u03b8) is a diagonal matrix, it follows that \u22072g(\u03b8) 4mI, where I is the identity matrix. Thus, g(\u03b8) is strongly convex with modulus \u00b5 = 4m [18]. Rigorously, we have the following lemma.\nLemma 1. Let \u03bb > 0 and \u03b81, \u03b82 \u2208 F\u03bb, then\na). g(\u03b82)\u2212 g(\u03b81) \u2265 \u3008\u2207g(\u03b81), \u03b82 \u2212 \u03b81\u3009+ 2m\u2016\u03b82 \u2212 \u03b81\u2016 2 2. (4)\nb). If \u03b81 6= \u03b82, the inequality in (4) becomes a strict inequality, i.e., \u201c\u2265\u201d becomes \u201c>\u201d.\nGiven \u03bb \u2208 (0, \u03bb0], it is easy to see that both of \u03b8\u2217\u03bb and \u03b8\u2217\u03bb0 belong to F\u03bb0 . Therefore, Lemma 1 can be a useful tool to bound \u03b8\u2217\u03bb with the knowledge of \u03b8 \u2217 \u03bb0 . In fact, we have the following theorem.\nTheorem 2. Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0, then the following holds:\na). \u2016\u03b8\u2217\u03bb \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2264\nm\n2\n[ g ( \u03bb \u03bb0 \u03b8\u2217\u03bb0 ) \u2212 g(\u03b8\u2217\u03bb0) + ( 1\u2212 \u03bb\u03bb0 ) \u3008\u2207g(\u03b8\u2217\u03bb0), \u03b8 \u2217 \u03bb0\u3009 ]\n(5)\nb). If \u03b8\u2217\u03bb 6= \u03b8\u2217\u03bb0 , the inequality in (5) becomes a strict inequality, i.e., \u201c\u2264\u201d becomes \u201c<\u201d. Proof. a). It is easy to see that F\u03bb \u2286 F\u03bb0 , \u03b8\u2217\u03bb \u2208 F\u03bb and \u03b8\u2217\u03bb0 \u2208 F\u03bb0 . Therefore, both of \u03b8 \u2217 \u03bb0\nand \u03b8\u2217\u03bb belong to the set F\u03bb0 . By Lemma 1, we have\n\u2016\u03b8\u2217\u03bb \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2264 m2 [ g(\u03b8\u2217\u03bb)\u2212 g(\u03b8\u2217\u03bb0) + \u3008\u2207g(\u03b8 \u2217 \u03bb0), \u03b8 \u2217 \u03bb0 \u2212 \u03b8 \u2217 \u03bb\u3009 ] . (6)\nLet \u03b8\u03bb = \u03bb \u03bb0 \u03b8\u2217\u03bb0 . It is easy to see that\n\u03b8\u03bb \u2208 C, \u2016X\u0304T \u03b8\u03bb\u2016\u221e = \u03bb\u03bb0 \u2016X\u0304 T \u03b8\u2217\u03bb0\u2016\u221e \u2264 m\u03bb, \u3008\u03b8\u03bb,b\u3009 = \u03bb \u03bb0 \u3008\u03b8\u2217\u03bb0 ,b\u3009 = 0.\nTherefore, we can see that \u03b8\u03bb \u2208 F\u03bb and thus\ng(\u03b8\u2217\u03bb) = min \u03b8\u2208F\u03bb g(\u03b8) \u2264 g(\u03b8\u03bb) = g ( \u03bb \u03bb0 \u03b8\u2217\u03bb0 ) .\nThen the inequality in (6) becomes\n\u2016\u03b8\u2217\u03bb \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2264 m2 [ g ( \u03bb \u03bb0 \u03b8\u2217\u03bb0 ) \u2212 g(\u03b8\u2217\u03bb0) + \u3008\u2207g(\u03b8 \u2217 \u03bb0), \u03b8 \u2217 \u03bb0 \u2212 \u03b8 \u2217 \u03bb\u3009 ] . (7)\nOn the other hand, by noting that (LRD\u03bb) is feasible, we can see that the Slater\u2019s conditions holds and thus the KKT conditions [21] lead to:\n0 \u2208 \u2207g(\u03b8\u2217\u03bb) + p\u2211 j=1 \u03b7+j x\u0304 j + p\u2211 j\u2032=1 \u03b7\u2212i (\u2212x\u0304 i) + \u03b3b +NC(\u03b8 \u2217 \u03bb), (8)\nwhere \u03b7+, \u03b7\u2212 \u2208 <p+, \u03b3 \u2208 < and NC(\u03b8\u2217\u03bb) is the normal cone of C at \u03b8\u2217\u03bb [21]. Because \u03b8\u2217\u03bb \u2208 C and C is an open set, \u03b8\u2217\u03bb is an interior point of C and thus NC(\u03b8\u2217\u03bb) = \u2205 [21]. Therefore, Eq. (8) becomes:\n\u2207g(\u03b8\u2217\u03bb) + p\u2211 j=1 \u03b7+j x\u0304 j + p\u2211 j\u2032=1 \u03b7\u2212j\u2032 (\u2212x\u0304 j\u2032) + \u03b3b = 0. (9)\nLet I+\u03bb0 = {j : \u3008\u03b8 \u2217 \u03bb0 , x\u0304j\u3009 = m\u03bb0, j = 1, . . . , p}, I\u2212\u03bb0 = {j \u2032 : \u3008\u03b8\u2217\u03bb0 , x\u0304 j\u2032\u3009 = \u2212m\u03bb0, j = 1, . . . , p} and I\u03bb0 = I+\u03bb0 \u222a I \u2212 \u03bb0 . We can see that I+\u03bb0 \u2229 I \u2212 \u03bb0\n= \u2205. By the complementary slackness condition, if k /\u2208 I\u03bb0 , we have \u03b7+k = \u03b7 \u2212 k = 0. Therefore,\n\u3008\u2207g(\u03b8\u2217\u03bb0), \u03b8 \u2217 \u03bb0\u3009+ \u2211 j\u2208I+\u03bb0 \u03b7+j \u3008\u03b8 \u2217 \u03bb0 , x\u0304 j\u3009+ \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032 \u3008\u03b8 \u2217 \u03bb0 ,\u2212x\u0304 j\u2032\u3009+ \u03b3\u3008\u03b8\u2217\u03bb0 ,b\u3009 = 0\n\u21d4\u2212 1m\u03bb0 \u3008\u2207g(\u03b8 \u2217 \u03bb0), \u03b8 \u2217 \u03bb0\u3009 = \u2211 j\u2208I+\u03bb0 \u03b7+j + \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032\nSimilarly, we have\n\u3008\u2207g(\u03b8\u2217\u03bb0), \u03b8 \u2217 \u03bb\u3009+ \u2211 j\u2208I+\u03bb0 \u03b7+j \u3008\u03b8 \u2217 \u03bb, x\u0304 j\u3009+ \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032 \u3008\u03b8 \u2217 \u03bb,\u2212x\u0304j \u2032 \u3009+ \u03b3\u3008\u03b8\u2217\u03bb,b\u3009 = 0\n\u21d4\u2212 \u3008\u2207g(\u03b8\u2217\u03bb0), \u03b8 \u2217 \u03bb\u3009 = \u2211 j\u2208I+\u03bb0 \u03b7+j \u3008\u03b8 \u2217 \u03bb, x\u0304 j\u3009+ \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032 \u3008\u03b8 \u2217 \u03bb,\u2212x\u0304j \u2032 \u3009\n\u2264 \u2211 j\u2208I+\u03bb0 \u03b7+j |\u3008\u03b8 \u2217 \u03bb, x\u0304 j\u3009|+ \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032 |\u3008\u03b8 \u2217 \u03bb,\u2212x\u0304j \u2032 \u3009|\n\u2264 m\u03bb { \u2211 j\u2208I+\u03bb0 \u03b7+j + \u2211 j\u2032\u2208I\u2212\u03bb0 \u03b7\u2212j\u2032 } = \u2212 \u03bb\u03bb0 \u3008g(\u03b8 \u2217 \u03bb0), \u03b8 \u2217 \u03bb0\u3009\nRecall (7), the inequality in (5) follows. b). The proof is the same as part a) by noting part b) of Lemma 1.\nTheorem 2 implies that \u03b8\u2217\u03bb is inside a ball centred at \u03b8 \u2217 \u03bb0 with radius\nr = \u221a m 2 [ g ( \u03bb \u03bb0 \u03b8\u2217\u03bb0 ) \u2212 g(\u03b8\u2217\u03bb0) + (1\u2212 \u03bb \u03bb0 )\u3008\u2207g(\u03b8\u2217\u03bb0), \u03b8 \u2217 \u03bb0 \u3009 ] . (10)\nRecall that to make our screening rules more aggressive in discarding features, we need to get a tight upper bound T (\u03b8\u2217\u03bb, x\u0304\nj) of |\u3008\u03b8\u2217\u03bb, x\u0304j\u3009| [please see (R1\u2032)]. Thus, it is desirable to further restrict the possible region A\u03bb of \u03b8\u2217\u03bb. Clearly, we can see that\n\u3008\u03b8\u2217\u03bb,b\u3009 = 0 (11)\nsince \u03b8\u2217\u03bb is feasible for problem (LRD\u03bb). On the other hand, we call the set I\u03bb0 defined in the proof of Theorem 2 the \u201cactive set\u201d of \u03b8\u2217\u03bb0 . In fact, we have the following lemma for the active set. Lemma 3. Given the optimal solution \u03b8\u2217\u03bb of problem (LRD\u03bb), the active set I\u03bb = {j : |\u3008\u03b8\u2217\u03bb, x\u0304j\u3009| = m\u03bb, j = 1, . . . , p} is not empty if \u03bb \u2208 (0, \u03bbmax].\nSince \u03bb0 \u2208 (0, \u03bbmax], we can see that I\u03bb0 is not empty by Lemma 3. We pick j0 \u2208 I\u03bb0 and set\nx\u0304\u2217 = sign(\u3008\u03b8\u2217\u03bb0 , x\u0304 j0\u3009)x\u0304j0 . (12)\nIt follows that \u3008x\u0304\u2217, \u03b8\u2217\u03bb0\u3009 = m\u03bb0. Due to the feasibility of \u03b8 \u2217 \u03bb for problem (LRD\u03bb), \u03b8 \u2217 \u03bb satisfies\n\u3008\u03b8\u2217\u03bb, x\u0304\u2217\u3009 \u2264 m\u03bb. (13)\nAs a result, Theorem 2, Eq. (11) and (13) imply that \u03b8\u2217\u03bb is contained in the following set:\nA\u03bb\u03bb0 := {\u03b8 : \u2016\u03b8 \u2212 \u03b8 \u2217 \u03bb0\u2016 2 2 \u2264 r2, \u3008\u03b8,b\u3009 = 0, \u3008\u03b8, x\u0304\u2217\u3009 \u2264 m\u03bb}.\nSince \u03b8\u2217\u03bb \u2208 A\u03bb\u03bb0 , we can see that |\u3008\u03b8 \u2217 \u03bb, x\u0304 j\u3009| \u2264 max\u03b8\u2208A\u03bb\u03bb0 |\u3008\u03b8, x\u0304 j\u3009|. Therefore, (R1\u2032) implies that if\nT (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) := max \u03b8\u2208A\u03bb\u03bb0 |\u3008\u03b8, x\u0304j\u3009| (UBP)\nis smaller than m\u03bb, we can conclude that [\u03b2\u2217\u03bb]j = 0 and x\u0304 j can be discarded from the optimization of (LRP\u03bb). Notice that, we replace the notations A\u03bb and T (\u03b8\u2217\u03bb, x\u0304j) with T (\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) and A \u03bb \u03bb0 to emphasize their dependence on \u03b8\u2217\u03bb0 . Clearly, as long as we can solve for T (\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0), (R1 \u2032) would be an applicable screening rule to discard features which have 0 coefficients in \u03b2\u2217\u03bb. We give a closed form solution of problem (42) in the next section."}, {"heading": "4 Solving the Convex Optimization Problem (UBP)", "text": "In this section, we show how to solve the convex optimization problem (42) based on the standard Lagrangian multiplier method. We first transform problem (42) into a pair of convex minimization problem (UBP\u2032) via Eq. (15) and then show that the strong duality holds for (UBP\u2032) in Lemma 6. The strong duality guarantees the applicability of the Lagrangian multiplier method. We then give the closed form solution of (UBP\u2032) in Theorem 8. After we solve problem (UBP\u2032), it is straightforward to compute the solution of problem (42) via Eq. (15).\nBefore we solve (42) for the general case, it is worthwhile to mention a special case in which Px\u0304j = x\u0304j \u2212 \u3008x\u0304j ,b\u3009 \u2016b\u201622 b = 0. Clearly, P is the projection operator which projects a vector onto the orthogonal complement of the space spanned by b. In fact, we have the following theorem.\nTheorem 4. Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0, and assume \u03b8\u2217\u03bb0 is known. For j \u2208 {1, . . . , p}, if Px\u0304 j = 0, then T (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = 0.\nBecause of (R1\u2032), we immediately have the following corollary.\nCorollary 5. Let \u03bb \u2208 (0, \u03bbmax) and j \u2208 {1, . . . , p}. If Px\u0304j = 0, then [\u03b2\u2217\u03bb]j = 0.\nFor the general case in which Px\u0304j 6= 0, let\nT+(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) := max \u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8,+x\u0304j\u3009, T\u2212(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) := max \u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8,\u2212x\u0304j\u3009. (14)\nClearly, we have\nT (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = max{T+(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0), T\u2212(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0)}. (15)\nTherefore, we can solve problem (42) by solving the two sub-problems in (14). Let \u03be \u2208 {+1,\u22121}. Then problems in (14) can be written uniformly as\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = max \u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8, \u03bex\u0304j\u3009. (UBPs)\nTo make use of the standard Lagrangian multiplier method, we transform problem (UBPs) to the following minimization problem:\n\u2212T\u03be(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) = min \u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8,\u2212\u03bex\u0304j\u3009 (UBP\u2032)\nby noting that max\u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8, \u03bex\u0304j\u3009 = \u2212min\u03b8\u2208A\u03bb\u03bb0 \u3008\u03b8,\u2212\u03bex\u0304 j\u3009.\nLemma 6. Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0 and assume \u03b8\u2217\u03bb0 is known. The strong duality holds for problem (UBP \u2032). Moreover, problem (UBP\u2032) admits an optimal solution in A\u03bb\u03bb0 .\nBecause the strong duality holds for problem (UBP\u2032) by Lemma 6, the Lagrangian multiplier method is applicable for (UBP\u2032). In general, we need to first solve the dual problem and then recover the optimal solution of the primal problem via KKT conditions. Recall that r and x\u0304\u2217 are defined by Eq. (10) and (12) respectively. Lemma 7 derives the dual problems of (UBP\u2032) for different cases.\nLemma 7. Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0 and assume \u03b8\u2217\u03bb0 is known. For j \u2208 {1, . . . , p} and Px\u0304 j 6= 0, let x\u0304 = \u2212\u03bex\u0304j. Denote\nU1 = {(u1, u2) : u1 > 0, u2 \u2265 0} and U2 = { (u1, u2) : u1 = 0, u2 = \u2212 \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u2217\u201622\n} .\na). If \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 (\u22121, 1], the dual problem of (UBP \u2032) is equivalent to:\nmax (u1,u2)\u2208U1\ng\u0304(u1, u2) = \u2212 12u1 \u2016Px\u0304 + u2Px\u0304 \u2217\u201622 + u2m(\u03bb0 \u2212 \u03bb) + \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 \u2212 1 2u1r 2. (UBD\u2032)\nMoreover, g\u0304(u1, u2) attains its maximum in U1. b). If \u3008Px\u0304,Px\u0304\n\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121, the dual problem of (UBP \u2032) is equivalent to:\nmax (u1,u2)\u2208U1\u222aU2 g\u0304(u1, u2) = { g\u0304(u1, u2), if (u1, u2) \u2208 U1, \u2212 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162m\u03bb, if (u1, u2) \u2208 U2.\n(UBD\u2032\u2032)\nWe can now solve problem (UBP\u2032) in the following theorem.\nTheorem 8. Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0, d = m(\u03bb0\u2212\u03bb)r\u2016Px\u0304\u2217\u20162 and assume \u03b8 \u2217 \u03bb0 is known. For j \u2208 {1, . . . , p} and Px\u0304j 6= 0, let x\u0304 = \u2212\u03bex\u0304j.\na). If \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2265 d, then T\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = r\u2016Px\u0304\u20162 \u2212 \u3008\u03b8 \u2217 \u03bb0 , x\u0304\u3009; (16)\nb). If \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 < d, then\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = r\u2016Px\u0304 + u \u2217 2Px\u0304 \u2217\u20162 \u2212 u\u22172m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009, (17)\nwhere\nu\u22172 = \u2212a1 +\n\u221a \u2206\n2a2 , (18)\na2 = \u2016Px\u0304\u2217\u201642(1\u2212 d2), a1 = 2\u3008Px\u0304,Px\u0304\u2217\u3009\u2016Px\u0304\u2217\u201622(1\u2212 d2), a0 = \u3008Px\u0304,Px\u0304\u2217\u30092 \u2212 d2\u2016Px\u0304\u201622\u2016Px\u0304\u2217\u201622, \u2206 = a21 \u2212 4a2a0 = 4d2(1\u2212 d2)\u2016Px\u0304\u2217\u201642(\u2016Px\u0304\u201622\u2016Px\u0304\u2217\u201622 \u2212 \u3008Px\u0304,Px\u0304\u2217\u30092).\nNotice that, although the dual problems of (UBP\u2032) in Lemma 7 are different, the resulting upper bound T\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) can be given by Theorem 8 in a uniform way. The tricky part is how to deal with the extremal cases in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 {\u22121,+1}. To avoid the lengthy discussion of Theorem 8, we omit the proof in the main text and include the details in the supplement."}, {"heading": "5 The proposed Slores Rule for `1 Regularized Logistic Regression", "text": "Using (R1\u2032), we are now ready to construct the screening rules for the `1 Regularized Logistic Regression. By Corollary 5, we can see that the orthogonality between the jth feature and the response vector b implies the absence of x\u0304j from the resulting model. For the general case in which Px\u0304j 6= 0, (R1\u2032) implies that if T (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = max{T+(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0), T\u2212(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0)} < m\u03bb, then the j th feature can be discarded from the optimization of (LRP\u03bb). Notice that, letting \u03be = \u00b11, T+(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) and T\u2212(\u03b8 \u2217 \u03bb, x\u0304\nj ; \u03b8\u2217\u03bb0) have been solved by Theorem 8. Rigorously, we have the following theorem.\nTheorem 9 (Slores). Let \u03bb0 > \u03bb > 0 and assume \u03b8 \u2217 \u03bb0 is known.\n1. If \u03bb \u2265 \u03bbmax, then \u03b2\u2217\u03bb = 0;\n2. If \u03bbmax \u2265 \u03bb0 > \u03bb > 0 and either of the following holds:\n(a) Px\u0304j = 0,\n(b) max{T\u03be(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) : \u03be = \u00b11} < m\u03bb,\nthen [\u03b2\u2217\u03bb]j = 0.\nBased on Theorem 9, we construct the Slores rule as summarized below in Algorithm 1.\nAlgorithm 1 R = Slores(X,b, \u03bb, \u03bb0, \u03b8\u2217\u03bb0) Initialize R := {1, . . . , p}; if \u03bb \u2265 \u03bbmax then\nset R = \u2205; else\nfor j = 1 to p do\nif Px\u0304j = 0 then remove j from R; else if max{T\u03be(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) : \u03be = \u00b11} < m\u03bb then remove j from R;\nend if end for\nend if Return: R\nNotice that, the output R of Slores is the indices of the features that need to be entered to the optimization. As a result, suppose the output of Algorithm 1 is R = {j1, . . . , jk}, we can substitute the full matrix X in problem (LRP\u03bb) with the submatrix XR = (x\u0304\nj1 , . . . , x\u0304jk) and just solve for [\u03b2\u2217\u03bb]R and c\u2217\u03bb.\nOn the other hand, Algorithm 1 implies that Slores needs five inputs. Since X and b come with the data and \u03bb is chosen by the user, we only need to specify \u03b8\u2217\u03bb0 and \u03bb0. In other words, we need to provide Slores with an dual optimal solution of problem (LRD\u03bb) for an arbitrary parameter. A natural choice is by setting \u03bb0 = \u03bbmax and \u03b8 \u2217 \u03bb0\n= \u03b8\u2217\u03bbmax given in Eq. (1) and Eq. (2)."}, {"heading": "6 Experiments", "text": "We evaluate our screening rules using the newgroup data set [12] and Yahoo web pages data sets [26]. The newgroup data set is cultured from the data by Koh et al. [12]. The Yahoo data sets include 11 top-level categories, each of which is further divided into a set of subcategories. In our experiment we construct five balanced binary classification datasets from the topics of Computers, Education, Health, Recreation, and Science. For each topic, we choose samples from one subcategory as the positive class and randomly sample an equal number of samples from the rest of subcategories as the negative class. The statistics of the data sets are given in Table 1.\nWe compare the performance of Slores and the strong rule which achieves state-of-the-art performance for `1 regularized LR. We do not include SAFE because it is less effective in discarding features than strong rules and requires much higher computational time [24]. Fig. 1 has shown the performance of Slores, strong rule and SAFE. We compare the efficiency of the three screening rules on the same prostate cancer data set in Table 2. All of the screening rules are tested along a sequence of 86 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.1 to 0.95. We repeat the procedure 100 times and during each time we undersample 80% of the data. We report the total running time of the three screening rules over the 86 values of \u03bb/\u03bbmax in Table 2. For reference, we also report the total\nrunning time of the solver1. We observe that the running time of Slores and strong rule is negligible compared to that of the solver. However, SAFE takes much longer time even than the solver.\n1In this paper, the ground truth is computed by SLEP [16].\nIn Section 6.1, we evaluate the performance of Slores and strong rule. Recall that we use the rejection ratio, i.e., the ratio between the number of features discarded by the screening rules and the number of features with 0 coefficients in the solution, to measure the performance of screening rules. Note that, because no features with non-zero coefficients in the solution would be mistakenly discarded by Slores, its rejection ratio is no larger than one. We then compare the efficiency of Slores and strong rule in Section 6.2.\nThe experiment settings are as follows. For each data set, we undersample 80% of the date and run Slores and strong rules along a sequence of 86 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.1 to 0.95. We repeat the procedure 100 times and report the average performance and running time at each of the 86 values of \u03bb/\u03bbmax. Slores, strong rules and SAFE are all implemented in Matlab. All of the experiments are carried out on a Intel(R) (i7-2600) 3.4Ghz processor."}, {"heading": "6.1 Comparison of Performance", "text": "In this experiment, we evaluate the performance of the Slores and the strong rule via the rejection ratio. Fig. 2 shows the rejection ratio of Slores and strong rule on six real data sets. When \u03bb/\u03bbmax > 0.5, we can see that both Slores and strong rule are able to identify almost 100% of the inactive features, i.e., features with 0 coefficients in the solution vector. However, when \u03bb/\u03bbmax \u2264 0.5, strong rule can not detect the inactive features. In contrast, we observe that Slores exhibits much stronger capability in discarding inactive features for small \u03bb, even when \u03bb/\u03bbmax is close to 0.1. Taking the data point at which \u03bb/\u03bbmax = 0.1 for example, Slores discards about 99% inactive features for the newsgroup data set. For the other data sets, more than 80% inactive features are identified by Slores. Therefore, in terms of rejection ratio, Slores significantly outperforms the strong rule. It is also worthwhile to mention that the discarded features by Slores are guaranteed to have 0 coefficients in the solution. But strong rule may mistakenly discard features which have non-zero coefficients in the solution."}, {"heading": "6.2 Comparison of Efficiency", "text": "We compare efficiency of Slores and the strong rule in this experiment. The data sets for evaluating the rules are the same as Section 6.1. The running time of the screening rules reported in Fig. 3 includes the computational cost of the rules themselves and that of the solver after screening. We plot the running time of the screening rules against that of the solver without screening. As indicated by Fig. 2, when \u03bb/\u03bbmax > 0.5, Slores and strong rule discards almost 100% of the inactive features. As a result, the size of the feature matrix involved in the optimization of problem (LRP\u03bb) is greatly reduced. From Fig. 3, we can observe that the efficiency is improved by about one magnitude on average compared to that of the solver without screening. However, when \u03bb/\u03bbmax < 0.5, strong rule can not identify any inactive features and thus the running time is almost the same as that of the solver without screening. In contrast, Slores is still able to identify more than 80% of the inactive features for the data sets cultured from the Yahoo web pages data sets and thus the efficiency is improved by roughly 5 times. For the newgroup data set, about 99% inactive features are identified by Slores which leads to about 10 times savings in running time. These results demonstrate the power of the proposed Slores rule in improving the efficiency of solving the `1 regularized LR."}, {"heading": "7 Conclusions", "text": "In this paper, we propose novel screening rules to effectively discard features for `1 regularized LR. Extensive numerical experiments on real data demonstrate that Slores outperforms the existing state-of-the-art screening rules. We plan to extend the framework of Slores to more general sparse formulations, including convex ones, like group Lasso, fused Lasso, `1 regularized SVM, and non-convex ones, like `p regularized problems where 0 < p < 1."}, {"heading": "A Deviation of the Dual Problem of the Sparse Logistic Regres-", "text": "sion\nSuppose we are given a set of training samples {xi}mi=1 and the associate labels b \u2208 <m, where xi \u2208 <p and bi \u2208 {1,\u22121} for all i \u2208 {1, . . . ,m}. The logistic regression problem takes the form as follows:\nmin \u03b2,c\n1\nm m\u2211 i=1 log(1 + exp(\u2212\u3008\u03b2, x\u0304i\u3009 \u2212 bic)) + \u03bb\u2016\u03b2\u20161,\nwhere \u03b2 \u2208 <p and c \u2208 < are the model parameters to be estimated, x\u0304i = bixi and \u03bb > 0. Let X\u0304 denote the data matrix whose rows consist of x\u0304i. Denote the columns of X\u0304 as x\u0304 j , j \u2208 {1, . . . , p}.\nA.1 Dual Formulation\nBy introducing the slack variables qi = 1 m (\u2212\u3008\u03b2, x\u0304i\u3009 \u2212 bic) for all i \u2208 {1, . . . ,m}, problem (LRP\u03bb) can be formulated as:\nmin \u03b2,c\n1\nm m\u2211 i=1 log [ 1 + exp(mqi) ] + \u03bb\u2016\u03b2\u20161, (19)\ns.t. qi = 1\nm (\u2212\u3008\u03b2, x\u0304i\u3009 \u2212 bic), i \u2208 {1, . . . ,m}.\nThe Lagrangian is\nL(q, \u03b2, c; \u03b8) = 1\nm m\u2211 i=1 log [ 1 + exp(mqi) ] + \u03bb\u2016\u03b2\u20161 + m\u2211 i=1 \u03b8i [ 1 m (\u2212\u3008\u03b2, x\u0304i\u3009 \u2212 bic)\u2212 qi ]\n(20)\n= 1\nm m\u2211 i=1 log [ 1 + exp(mqi) ] \u2212 \u3008\u03b8,q\u3009+ \u03bb\u2016\u03b2\u20161 \u2212 1 m \u3008 m\u2211 i=1 \u03b8ix\u0304i, \u03b2\u3009 \u2212 c m \u3008\u03b8,b\u3009\nIn order to find the dual function, we need to solve the following subproblems:\nmin q f1(q) =\n1\nm m\u2211 i=1 log [ 1 + exp(mqi) ] \u2212 \u3008\u03b8,q\u3009, (21)\nmin \u03b2 f2(\u03b2) = \u03bb\u2016\u03b2\u20161 \u2212\n1 m \u3008 m\u2211 i=1 \u03b8ix\u0304i, \u03b2\u3009, (22)\nmin c f3(c) = \u2212\nc\nm \u3008\u03b8,b\u3009. (23)\nConsider f1(q). It is easy to see\n[\u2207f1(q)]i = exp(mqi)\n1 + exp(mqi) \u2212 \u03b8i.\nBy setting [\u2207f1(q)]i = 0, we get\nexp(mq\u2032i) = \u03b8i\n1\u2212 \u03b8i , q\u2032i =\n1 m log( \u03b8i 1\u2212 \u03b8i ).\nClearly, we can see that \u03b8i \u2208 (0, 1) for all i \u2208 {1, . . . ,m}. Therefore,\nmin q f1(q) = f1(q \u2032) = \u2212 1 m m\u2211 i=1 [ \u03b8i log(\u03b8i) + (1\u2212 \u03b8i) log(1\u2212 \u03b8i) ] .\nConsider f2(\u03b2) and let \u03b2 \u2032 = argmin\u03b2 f2(\u03b2). The optimality condition is\n0 \u2208 \u03bbv \u2212 1 m \u03b8ix\u0304i, \u2016v\u2016\u221e \u2264 1, \u3008v, \u03b2\u2032\u3009 = \u2016\u03b2\u2032\u20161.\nIt is easy to see that\n\u3008 1 m m\u2211 i=1 \u03b8ix\u0304i, \u03b2 \u2032\u3009 = \u2016\u03b2\u2032\u20161,\nand thus\n\u3008\u03b8, x\u0304j\u3009 \u2208  m\u03bb, if \u03b2\u2032j > 0,\n\u2212m\u03bb, if \u03b2\u2032j < 0, [\u2212m\u03bb,m\u03bb], if \u03b2\u2032j = 0.\nMoreover, it follows that\nmin \u03b2 f2(\u03b2) = f2(\u03b2\n\u2032) = 0.\nFor f3(c), we can see that\nf \u20323(c) = \u2212 1\nm \u3008\u03b8,b\u3009.\nTherefore, we have\n\u3008\u03b8,b\u3009 = 0,\nsince otherwise infc f3(c) = \u2212\u221e and the dual problem is infeasible. Clearly, minc f3(c) = 0. All together, the dual problem is\nmin \u03b8\ng(\u03b8) = 1\nm m\u2211 i=1 [ \u03b8i log(\u03b8i) + (1\u2212 \u03b8i) log(1\u2212 \u03b8i) ] , (LRD\u03bb)\ns.t. \u2016X\u0304T \u03b8\u2016\u221e \u2264 m\u03bb, \u3008\u03b8,b\u3009 = 0, \u03b8 \u2208 C,\nwhere C = {\u03b8 \u2208 <m : \u03b8i \u2208 (0, 1), i = 1, . . . ,m}."}, {"heading": "B Proof of the Existence of the Optimal Solution of (LRD\u03bb)", "text": "In this section, we prove that problem (LRD\u03bb) has a unique optimal solution for all \u03bb > 0. Therefore, in Lemma A, we first show that problem (LRD\u03bb) is feasible for all \u03bb > 0. Then Lemma B confirms the existence of the dual optimal solution \u03b8\u2217\u03bb.\nLemma 10. A 1. For \u03bb > 0, problem (LRD\u03bb) is feasible, i.e., F\u03bb 6= \u2205. 2. The Slater\u2019s condition holds for problem (LRD\u03bb) in which \u03bb > 0.\nProof. 1. When \u03bb \u2265 \u03bbmax, the feasibility of (LRD\u03bb) is trivial because of the existence of \u03b8\u2217\u03bbmax . We focus on the case in which \u03bb \u2208 (0, \u03bbmax] below. Recall that [12] \u03bbmax is the smallest tuning parameter such that \u03b2 \u2217 \u03bb = 0 and \u03b8 \u2217 \u03bb = \u03b8 \u2217 \u03bbmax\nwhenever \u03bb \u2265 \u03bbmax. For convenience, we rewrite the definition of \u03bbmax and \u03b8\u2217\u03bbmax as follows.\n\u03bbmax = max j\u2208{1,...,p}\n1 m |\u3008\u03b8\u2217\u03bbmax , x\u0304 j\u3009|,\n[\u03b8\u2217\u03bbmax ]i =  m\u2212 m , if i \u2208 P,\nm+ m , if i \u2208 N , i = 1, . . . ,m.\nClearly, we have \u03b8\u2217\u03bbmax \u2208 F\u03bbmax , i.e., \u03b8 \u2217 \u03bbmax \u2208 C, \u2016X\u0304T \u03b8\u2217\u03bbmax\u2016\u221e \u2264 m\u03bbmax and \u3008\u03b8 \u2217 \u03bbmax ,b\u3009 = 0. Let us define:\n\u03b8\u03bb = \u03bb\n\u03bbmax \u03b8\u2217\u03bbmax .\nSince 0 < \u03bb\u03bbmax \u2264 1, we can see that \u03b8\u03bb \u2208 C. Moreover, it is easy to see that\n\u2016X\u0304T \u03b8\u03bb\u2016\u221e = \u03bb\n\u03bbmax \u2016X\u0304T \u03b8\u2217\u03bbmax\u2016\u221e \u2264 m\u03bb,\nand\n\u3008\u03b8\u03bb,b\u3009 = \u03bb\n\u03bbmax \u3008\u03b8\u2217\u03bbmax ,b\u3009 = 0.\nTherefore, \u03b8\u03bb \u2208 F\u03bb, i.e., F\u03bb is not empty and thus (LRD\u03bb) is feasible.\n2. The constraints of (LRD\u03bb) are all affine. Therefore, the Slater\u2019s condition reduces to the feasibility of (LRD\u03bb) [3]. When \u03bb \u2265 \u03bbmax, \u03b8\u2217\u03bbmax is clearly a feasible solution of (LRD\u03bb). When \u03bb \u2208 (0, \u03bbmax], we have shown the feasibility of (LRD\u03bb) in part 1. Therefore, the Slater\u2019s condition always holds for (LRD\u03bb) in which \u03bb > 0.\nLemma 11. B Given \u03bb \u2208 (0, \u03bbmax], problem (LRD\u03bb) has a unique optimal solution, i.e., there exists a unique \u03b8\u2217\u03bb \u2208 F\u03bb such that g(\u03b8) achieves its minimum over F\u03bb at \u03b8\u2217\u03bb.\nProof. Let C\u0303 := {\u03b8 \u2208 <m : \u03b8i \u2208 [0, 1], i = 1, . . . ,m} and\nF\u0303\u03bb := {\u03b8 \u2208 <m : \u03b8 \u2208 C\u0303, \u2016X\u0304T \u03b8\u2016\u221e \u2264 m\u03bb, \u3008\u03b8,b\u3009 = 0}.\nClearly, F\u03bb \u2286 F\u0303\u03bb and F\u0303\u03bb is compact. We show that g(\u03b8) can be continuously extended to F\u0303\u03bb. For y \u2208 (0, 1), define\nf(y) = y log(y) + (1\u2212 y) log(1\u2212 y).\nBy L\u2032Ho\u0302pital\u2032s rule, it is easy to see that limy\u21930 f(y) = limy\u21911 f(y) = 0. Therefore, let\nf\u0303(y) = { f(y), y \u2208 (0, 1), 0, y \u2208 {0, 1},\nand\ng\u0303(\u03b8) = 1\nm m\u2211 i=1 f\u0303(\u03b8i).\nClearly, when \u03b8 \u2208 F\u03bb, we have g\u0303(\u03b8) = g(\u03b8), i.e., g(\u03b8) is the restriction of g\u0303(\u03b8) over F\u03bb. On the other hand, the definition of g\u0303(\u03b8) implies that g\u0303(\u03b8) is continuous over F\u0303\u03bb. Together with the fact that F\u0303\u03bb is compact, we can see that there exists \u03b8\u2217\u03bb \u2208 F\u0303\u03bb and g\u0303(\u03b8\u2217\u03bb) = min\u03b8\u2208F\u0303\u03bb g\u0303(\u03b8). Consider the optimization problem\nmin \u03b8\u2208F\u0303\u03bb\ng\u0303(\u03b8) (LRD\u2032\u03bb)\nBecause of Lemma A, we know that F\u03bb 6= \u2205 and thus F\u0303\u03bb 6= \u2205 either. Therefore, problem (LRD\u2032\u03bb) is feasible. By noting that the constraints of problem (LRD\u2032\u03bb) are all linear, the Slater\u2019s condition is satisfied. Hence, there exists a set of Lagrangian multipliers \u03b7+, \u03b7\u2212 \u2208 <p+, \u03be+, \u03be\u2212 \u2208 <m+ and \u03b3 \u2208 < such that\n\u2207g\u0303(\u03b8\u2217\u03bb) + m\u2211 i=1 \u03b7+i x\u0304 i + m\u2211 i=1 \u03b7\u2212i (\u2212x\u0304 i) + \u03be+ \u2212 \u03be\u2212 + \u03b3b = 0. (24)\nWe can see that if there is an i0 such that [\u03b8 \u2217 \u03bb]i0 \u2208 {0, 1}, i.e., \u03b8\u2217\u03bb /\u2208 F\u03bb, Eq. (24) does not hold since |[\u2207g\u0303(\u03b8\u2217\u03bb)]i0 | = \u221e. Therefore, we can conclude that \u03b8\u2217\u03bb \u2208 F\u03bb. Moreover, it is easy to see that \u03b8\u2217\u03bb = argmin\u03b8\u2208F\u0303\u03bb g\u0303(\u03b8) = argmin\u03b8\u2208F\u03bb g(\u03b8), i.e. \u03b8 \u2217 \u03bb is a minimum of g(\u03b8) over F\u03bb. The uniqueness of \u03b8\u2217\u03bb is due to the strict convexity of g(\u03b8) (strong convexity implies strict convexity), which completes the proof."}, {"heading": "C Proof of Lemma 1", "text": "Proof. Recall that the domain of g is C = {\u03b8 \u2208 <m : [\u03b8]i \u2208 (0, 1), i = 1, . . . ,m}.\na. It is easy to see that\n[\u2207g(\u03b8)]i = 1\nm log( [\u03b8]i 1\u2212 [\u03b8]i ), [\u22072g(\u03b8)]i,i = 1 m\n1 [\u03b8]i(1\u2212 [\u03b8]i) \u2265 4 m , [\u22072g(\u03b8)]i,j = 0, i 6= j.\nClearly, \u22072g(\u03b8) is a diagonal matrix and\n\u22072g(\u03b8) \u2265 4 m I. (25)\nTherefore, g is a strong convex function with convexity parameter \u00b5 = 4m [18]. The claim then follows directly from the definition of strong convex functions.\nb. If \u03b81 6= \u03b82, then there exists at least one i\u2032 \u2208 {1, . . . ,m} such that [\u03b81]i\u2032 6= [\u03b82]i\u2032 . Moreover, at most one of [\u03b81]i\u2032 and [\u03b82]i\u2032 can be 1 2 . Without loss of generality, assume [\u03b81]i\u2032 6= 1 2 .\nFor t \u2208 (0, 1), let \u03b8(t) = t\u03b82 + (1\u2212 t)\u03b81. Since [\u03b81]i\u2032 6= 12 , we can find t \u2032 \u2208 (0, 1) such that [\u03b8(t\u2032)]i\u2032 6= 12 . Therefore, we can see that\n\u3008\u22072g(\u03b8(t\u2032))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009 = m\u2211 i=1 [\u22072g(\u03b8(t\u2032))]i,i([\u03b82]i \u2212 [\u03b81]i)2 (26)\n= \u2211\ni\u2208{1,...,m}, i 6=i\u2032\n[\u22072g(\u03b8(t\u2032))]i,i([\u03b82]i \u2212 [\u03b81]i)2\n+ [\u22072g(\u03b8(t\u2032))]i\u2032,i\u2032([\u03b82]i\u2032 \u2212 [\u03b81]i\u2032)2.\nClearly, when i \u2208 {1, . . . ,m} \\ i\u2032, we have [\u22072g(\u03b8(t\u2032))]i,i \u2265 4m and thus\u2211 i\u2208{1,...,m},\ni 6=i\u2032\n[\u22072g(\u03b8(t\u2032))]i,i([\u03b82]i \u2212 [\u03b81]i)2 \u2265 \u2211\ni\u2208{1,...,m}, i 6=i\u2032\n4 m ([\u03b82]i \u2212 [\u03b81]i)2. (27)\nSince [\u03b8(t\u2032)]i\u2032 6= 12 , we have [\u2207 2g(\u03b8(t\u2032))]i,i > 4 m which results in\n[\u22072g(\u03b8(t\u2032))]i\u2032,i\u2032([\u03b82]i\u2032 \u2212 [\u03b81]i\u2032)2 > 4\nm ([\u03b82]i\u2032 \u2212 [\u03b81]i\u2032)2. (28)\nMoreover, because [\u03b81]i\u2032 6= [\u03b82]i\u2032 , we can see that\n([\u03b82]i\u2032 \u2212 [\u03b81]i\u2032)2 6= 0. (29)\nTherefore, Eq. (26) and the inequalities in (27), (28) and (29) imply that\n\u3008\u22072g(\u03b8(t\u2032))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009 > 4\nm \u2016\u03b82 \u2212 \u03b81\u201622. (30)\nDue to the continuity of\u22072g(\u03b8(t)), there exist a small number > 0 such that for all t \u2208 (t\u2032\u2212 12 , t \u2032+ 12 ), we have\n\u3008\u22072g(\u03b8(t))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009 > 4\nm \u2016\u03b82 \u2212 \u03b81\u201622, (31)\nIt follows that g(\u03b82)\u2212 g(\u03b81) = \u222b 1\n0\n\u2207g(\u03b8(t))(\u03b82 \u2212 \u03b81)dt (32)\n= \u222b 1 0 [ \u2207g(\u03b8(0)) + \u222b t 0 \u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81)d\u03c4 ] (\u03b82 \u2212 \u03b81)dt\n= \u3008\u2207g(\u03b81), \u03b82 \u2212 \u03b81\u3009+ \u222b 1\n0 \u222b t 0 \u3008\u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009d\u03c4dt.\nLet\n\u2206t2t1 := \u222b t2 t1 \u222b t 0 \u3008\u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009d\u03c4dt.\nClearly, the second term on the right hand side of Eq. (32) is \u220610, which can be written as\n\u220610 = \u2206 t\u2032\u2212 12 0 + \u2206 t\u2032+ 12\nt\u2032\u2212 12 + \u22061t\u2032+ 12 .\nDue to the inequality in (31), we can see that\n\u2206 t\u2032+ 12\nt\u2032\u2212 12 = \u222b t\u2032+ 12 t\u2032\u2212 12 \u222b t 0 \u3008\u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009d\u03c4dt (33)\n> \u222b t\u2032+ 12 t\u2032\u2212 12 4 m \u2016\u03b82 \u2212 \u03b81\u201622tdt\n= 4\nm \u2016\u03b82 \u2212 \u03b81\u201622 \u00b7 (t\u2032 ).\nDue to the inequality in (25), it is easy to see that\n\u3008\u22072g(\u03b8(t))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009 \u2265 4\nm \u2016\u03b82 \u2212 \u03b81\u201622 (34)\nfor all t \u2208 (0, 1). Therefore,\n\u2206 t\u2032\u2212 12 0 = \u222b t\u2032+ 12 t\u2032\u2212 12 \u222b t 0 \u3008\u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009d\u03c4dt (35)\n\u2265 \u222b t\u2032\u2212 12\n0\n4 m \u2016\u03b82 \u2212 \u03b81\u201622tdt\n= 4\nm \u2016\u03b82 \u2212 \u03b81\u201622 \u00b7\n[ 1\n2\n( t\u2032 \u2212 1\n2\n)2] .\nSimilarly,\n\u22061t\u2032+ 12 = \u222b 1 t\u2032+ 12 \u222b t 0 \u3008\u22072g(\u03b8(\u03c4))(\u03b82 \u2212 \u03b81), \u03b82 \u2212 \u03b81\u3009d\u03c4dt (36)\n\u2265 \u222b 1 t\u2032+ 12 4 m \u2016\u03b82 \u2212 \u03b81\u201622tdt\n= 4\nm \u2016\u03b82 \u2212 \u03b81\u201622 \u00b7\n1\n2\n[ 1\u2212 ( t\u2032 + 1\n2\n)2] .\nThe inequalites in (33), (35) and (36) imply that\n\u220610 > 2\nm \u2016\u03b82 \u2212 \u03b81\u201622. (37)\nTherefore, the inequality in (37) and Eq. (32) lead to the following strict inequality\ng(\u03b82)\u2212 g(\u03b81) > \u3008\u2207g(\u03b81), \u03b82 \u2212 \u03b81\u3009+ 2\nm \u2016\u03b82 \u2212 \u03b81\u201622, (38)\nwhich completes the proof."}, {"heading": "D Proof of Lemma 3", "text": "Proof. According to the definition of \u03bbmax in Eq. (1), there must be j0 \u2208 {1, . . . , p} such that \u03bbmax = 1 m |\u3008\u03b8 \u2217 \u03bbmax\n, x\u0304j0\u3009|. Clearly, j0 \u2208 I\u03bbmax and thus I\u03bbmax is not empty. For \u03bb \u2208 (0, \u03bbmax), we prove the statement by contradiction. Suppose I\u03bb is empty, then the KKT condition\nfor (LRD\u03bb) at \u03b8 \u2217 \u03bb can be written as:\n0 \u2208 \u2207g(\u03b8\u2217\u03bb) + p\u2211 j=1 \u03b7+j x\u0304 j + p\u2211 j\u2032=1 \u03b7\u2212i (\u2212x\u0304 i) + \u03b3b +NC(\u03b8 \u2217 \u03bb),\nwhere \u03b7+, \u03b7\u2212 \u2208 <p+, \u03b3 \u2208 < and NC(\u03b8\u2217\u03bb) is the normal cone of C at \u03b8\u2217\u03bb. Because \u03b8\u2217\u03bb \u2208 C and C is an open set, \u03b8\u2217\u03bb is an interior point of C and thus NC(\u03b8\u2217\u03bb) = \u2205. Therefore, the above equation becomes:\n\u2207g(\u03b8\u2217\u03bb) + p\u2211 j=1 \u03b7+j x\u0304 j + p\u2211 j\u2032=1 \u03b7\u2212j\u2032 (\u2212x\u0304 j\u2032) + \u03b3b = 0. (39)\nMoreover, by the complementary slackness condition [3], we have \u03b7+j = \u03b7 \u2212 j = 0 for j = 1, . . . , p since I\u03bb is empty. Then, Eq. (39) becomes:\n\u2207g(\u03b8\u2217\u03bb) + \u03b3b = 0. (40) By the similar argument, the KKT condition for (LRD\u03bbmax) at \u03b8 \u2217 \u03bbmax is:\n\u2207g(\u03b8\u2217\u03bbmax) + p\u2211 j=1 \u03b7\u0304+j x\u0304 j + p\u2211 j\u2032=1 \u03b7\u0304\u2212j\u2032 (\u2212x\u0304 j\u2032) + \u03b3\u2032b = 0, (41)\nwhere \u03b7\u0304+, \u03b7\u0304\u2212 \u2208 <p+ and \u03b3\u2032 \u2208 <. Since I\u03bb = \u2205, we can see that |\u3008\u03b8\u2217\u03bb, x\u0304j\u3009| < m\u03bb < m\u03bbmax for all j \u2208 {1, . . . ,m}. Therefore, \u03b8\u2217\u03bb also satisfies Eq. (41) by setting \u03b7+ = \u03b7\u0304+, \u03b7\u2212 = \u03b7\u0304\u2212 and \u03b3 = \u03b3\u2032 without violating the complementary slackness conditions. As a result, \u03b8\u2217\u03bb is an optimal solution of problem (LRD\u03bbmax) as well.\nMoreover, it is easy to see that \u03b8\u2217\u03bb 6= \u03b8\u2217\u03bbmax because |\u3008\u03b8 \u2217 \u03bb, x\u0304 j0\u3009| < m\u03bb < m\u03bbmax = |\u3008\u03b8\u2217\u03bbmax , x\u0304 j0\u3009|. Consequently, (LRD\u03bbmax) has at least two distinct optimal solutions, which contradicts with Lemma B. Therefore, I\u03bb must be an nonempty set. Because \u03bb is arbitrary in (0, \u03bbmax), the proof is complete."}, {"heading": "E Proof of Theorem 4", "text": "Proof. Recall that we need to solve the following optimization proble:\nT (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) := max\u03b8 { |\u3008\u03b8, x\u0304j\u3009| : \u2016\u03b8 \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2264 r2, \u3008\u03b8,b\u3009 = 0, \u3008\u03b8, x\u0304\u2217\u3009 \u2264 m\u03bb } . (42)\nThe feasibility of \u03b8 implies that \u3008\u03b8,b\u3009 = 0,\ni.e., \u03b8 belongs to the orthogonal complement of the space spanned by b. As a result, \u03b8 = P\u03b8 and\n|\u3008\u03b8, x\u0304j\u3009| = |\u3008P\u03b8, x\u0304j\u3009| = |\u3008\u03b8,Px\u0304j\u3009| = 0.\nTherefore, we can see that T (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = 0, which completes the proof."}, {"heading": "F Proof of Corollary 5", "text": "Proof. We set \u03bb0 = \u03bbmax and \u03b8 \u2217 \u03bb0 = \u03b8\u2217\u03bbmax . Clearly, we have \u03bb0 > \u03bb > 0 and \u03b8 \u2217 \u03bb0 is known. Therefore, the assumptions in Theorem 4 are satisfied and thus T (\u03b8\u2217\u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = 0.\nBy the rule in (R1\u2032), it is straightforward to see that [\u03b2\u2217\u03bb]j = 0, which completes the proof."}, {"heading": "G Proof of Lemma 6", "text": "Proof. We show that the Slater\u2019s condition holds for (UBP\u2032). Recall that the feasible set of problem (UBP\u2032) is\nA\u03bb\u03bb0 = { \u03b8 : \u2016\u03b8 \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2264 r2, \u3008\u03b8,b\u3009 = 0, \u3008\u03b8, x\u0304\u2217\u3009 \u2264 m\u03bb } .\nTo show that the Slater\u2019s condition holds, we need to seek a point \u03b8\u2032 such that\n\u2016\u03b8\u2032 \u2212 \u03b8\u2217\u03bb0\u2016 2 2 < r 2, \u3008\u03b8\u2032,b\u3009 = 0, \u3008\u03b8\u2032, x\u0304\u2217\u3009 \u2264 m\u03bb.\nConsider \u03b8\u2217\u03bb, since \u03b8 \u2217 \u03bb \u2208 A\u03bb\u03bb0 , the last two constraints of A \u03bb \u03bb0 are satisfied by \u03b8\u2217\u03bb automatically. On the other hand, because \u03bb0 \u2208 (0, \u03bbmax], Lemma 3 leads to the fact that the active set I\u03bb0 is not empty. Let j0 \u2208 I\u03bb0 , we can see that |\u3008\u03b8\u2217\u03bb0 , x\u0304 j0\u3009| = m\u03bb0. However, because \u03b8\u2217\u03bb \u2208 F\u03bb, we have |X\u0304T \u03b8\u2217\u03bb|\u221e \u2264 m\u03bb,\nand thus |\u3008\u03b8\u2217\u03bb, x\u0304j0\u3009| \u2264 m\u03bb < m\u03bb0. Therefore, we can see that \u03b8\u2217\u03bb 6= \u03b8\u2217\u03bb0 . By part b of Theorem 2, it follows that\n\u2016\u03b8\u2217\u03bb \u2212 \u03b8\u2217\u03bb0\u2016 2 2 < r 2.\nAs a result, the Slater\u2019s condition holds for (UBP\u2032) which implies the strong duality of (UBP\u2032). Moreover, it is easy to see that problem (UBP\u2032) admits optimal solution in A\u03bb\u03bb0 because the objective function of (UBP\u2032) is continuous and A\u03bb\u03bb0 is compact."}, {"heading": "H Proof of Lemma 7", "text": "Proof. The Lagrangian of (UBP\u2032) is\nL(\u03b8;u1, u2, v) = \u3008\u03b8, x\u0304\u3009+ u1 2 ( \u2016\u03b8 \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2212 r2 ) + u2(\u3008\u03b8, x\u0304\u2217\u3009 \u2212m\u03bb2) + v\u3008\u03b8,b\u3009 (43)\n= \u3008\u03b8, x\u0304 + u2x\u0304\u2217 + vb\u3009+ u1 2 ( \u2016\u03b8 \u2212 \u03b8\u2217\u03bb0\u2016 2 2 \u2212 r2 ) \u2212 u2m\u03bb2.\nwhere u1, u2 \u2265 0 and v \u2208 < are the Lagrangian multipliers. To derive the dual function g\u0302(u1, u2, v) = min\u03b8 L(\u03b8;u1, u2, v), we can simply set \u2207\u03b8L(\u03b8;u1, u2, v) = 0, i.e.,\n\u2207\u03b8L(\u03b8;u1, u2, v) = x\u0304 + u1(\u03b8 \u2212 \u03b8\u2217\u03bb1) + u2x\u0304 \u2217 + vb = 0.\nWhen u1 6= 0, we set\n\u03b8 = \u2212 1 u1 (x\u0304 + u2x\u0304 \u2217 + vb) + \u03b8\u2217\u03bb0 (44)\nto minimize L(\u03b8;u, v) with (u, v) fixed. By plugging Eq. (44) to Eq. (43), we can see that the dual function g\u0302(u1, u2, v) is:\ng\u0302(u1, u2, v) =\u2212 1\n2u1 \u2016x\u0304 + u2x\u0304\u2217 + vb\u201622 + \u3008\u03b8\u2217\u03bb0 , x\u0304 + u2x\u0304 \u2217 + vb\u3009 \u2212 1 2 u1r 2 \u2212 u2m\u03bb2 (45)\n=\u2212 1 2u1 \u2016x\u0304 + u2x\u0304\u2217 + vb\u201622 + u2m(\u03bb1 \u2212 \u03bb2) + \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 \u2212 1 2 u1r 2.\nThe second line of (45) is due to the fact that \u3008\u03b8\u2217\u03bb0 , x\u0304 \u2217\u3009 = m\u03bb1 and \u3008\u03b8\u2217\u03bb0 ,b\u3009 = 0.\nWhen u1 = 0, it is not straightforward to derive the dual function. The difficulty comes from the fact that x\u0304 + u2x\u0304 \u2217 + vb might be 0 for some u\u20322 \u2265 0 and v\u2032.\na). Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 (\u22121, 1], we show that x\u0304 + u2x\u0304 \u2217 + vb 6= 0 for all u2 \u2265 0 and v.\nSuppose for contradiction that x\u0304 + u\u20322x\u0304 \u2217 + v\u2032b = 0, in which u\u20322 \u2265 0 and v\u2032. Then we can see that\nP (x\u0304 + u\u20322x\u0304 \u2217 + v\u2032b) = 0.\nSince Pb = 0, it follows that Px\u0304 + u\u20322Px\u0304 \u2217 = 0, (46)\nand thus\nu\u20322 = \u2212 \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u2217\u201622 . (47)\nClearly, Eq. (46) implies that Px\u0304 and Px\u0304\u2217 are collinear, i.e., \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 {\u22121, 1}. Moreover, by Eq. (47), we know that \u3008Px\u0304,Px\u0304\u2217\u3009 \u2264 0. Therefore, it is easy to see that \u3008Px\u0304,Px\u0304\n\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121, which\ncontradicts the assumption. Hence, x\u0304 + u2x\u0304 \u2217 + vb 6= 0 for all u2 \u2265 0 and v.\nWe then show that g\u0302(u1, u2, v) = \u2212\u221e when u1 = 0 for all u2 \u2265 0 and v. Since g\u0302(u1, u2, v) = min\u03b8 L(\u03b8;u1, u2, v), for u1 = 0, u2 \u2265 0 and v, we have\ng\u0302(0, u2, v) = min \u03b8 L(\u03b8; 0, u2, v) (48)\n= min \u03b8 \u3008\u03b8, x\u0304 + u2x\u0304\u2217 + vb\u3009 \u2212 u2m\u03bb2.\nLet \u03b8 = \u2212t(x\u0304 + u2x\u0304\u2217 + vb), it follows that\n\u3008\u03b8, x\u0304 + u2x\u0304\u2217 + vb\u3009 \u2212 u2m\u03bb2 = \u2212t\u2016x\u0304 + u2x\u0304\u2217 + vb\u201622 \u2212 u2m\u03bb2, (49)\nand thus\nlim t\u2192\u221e\n\u2212t\u2016x\u0304 + u2x\u0304\u2217 + vb\u201622 \u2212 u2m\u03bb2 = \u2212\u221e. (50)\nTherefore, g\u0302(u1, u2, v) = min\u03b8 L(\u03b8;u1, u2, v), for u1 = 0, u2 \u2265 0 and v. All together, the dual function is\ng\u0302(u1, u2, v) =  \u2212 12u1 \u2016x\u0304 + u2x\u0304 \u2217 + vb\u201622 + u2m(\u03bb1 \u2212 \u03bb2) + \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 \u2212 1 2u1r 2, if u1 > 0, u2 \u2265 0,\n\u2212\u221e, if u1 = 0, u2 \u2265 0.\n(51)\nBecause the dual problem is to maximize the dual function, we can write the dual problem as:\nmax u1>0,u2\u22650,v\ng\u0302(u1, u2, v) = \u2212 1\n2u1 \u2016x\u0304 + u2x\u0304\u2217 + vb\u201622 + u2m(\u03bb1 \u2212 \u03bb2) + \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 \u2212\n1 2 u1r 2. (52)\nMoreover, it is easy to see that problem (52) is an unconstrained optimization problem with respect to v. Therefore, we set\n\u2202g\u0302(u1, u2, v) \u2202v = \u2212 1 u1 \u3008x\u0304 + u2x\u0304\u2217 + vb,b\u3009 = 0,\nand thus\nv = \u2212\u3008x\u0304 + u2x\u0304 \u2217,b\u3009\n\u2016b\u201622 . (53)\nBy plugging (53) into g\u0302(u1, u2, v) and noting that U1 = {(u1, u2) : u1 > 0, u2 \u2265 0}, problem (52) is equivalent to\nmax (u1,u2)\u2208U1\ng\u0304(u1, u2) = \u2212 1\n2u1 \u2016Px\u0304 + u2Px\u0304\u2217\u201622 + u2m(\u03bb1 \u2212 \u03bb2) + \u3008\u03b8\u2217\u03bb1 , x\u0304\u3009 \u2212\n1 2 u1r 2. (UBD\u2032)\nBy Lemma 6, we know that the Slater\u2019s conditions holds for (UBP\u2032). Therefore, the strong duality holds for (UBP\u2032) and (UBD\u2032). By the strong duality theorem [11], there exists u\u22171 \u2265 0, u\u22172 \u2265 0 and v\u2217 such that g\u0302(u\u22171, u \u2217 2, v \u2217) = maxu1\u22650,u2\u22650,v g\u0302(u1, u2, v). By Eq. (51), it is easy to see that u \u2217 1 > 0. Therefore, g\u0304(u1, u2) attains its maximum in U1.\nb). Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121, it is easy to see that\nx\u0304 + u2x\u0304 \u2217 + vb = 0\u21d4 u2 = \u2212 \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u2217\u201622 and v = \u2212\u3008x\u0304 + u2x\u0304 \u2217,b\u3009 \u2016b\u201622 .\nLet u\u20322 = \u2212 \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u2217\u201622 and v\u2032 = \u2212 \u3008x\u0304+u2x\u0304 \u2217,b\u3009 \u2016b\u201622 , we have\nL(\u03b8; 0, u\u20322, v \u2032) = \u2212u\u20322m\u03bb2 = \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u2217\u201622 m\u03bb = \u2212 \u2016Px\u0304\u20162 \u2016Px\u0304\u2217\u20162 m\u03bb,\nand thus\ng\u0302(0, u\u20322, v \u2032) = min \u03b8 L(\u03b8; 0, u\u20322, v \u2032) = \u2212 \u2016Px\u0304\u20162 \u2016Px\u0304\u2217\u20162 m\u03bb.\nBy a similar argument as in the proof of part 1, we can see that the dual function is\ng\u0302(u1, u2, v) =  \u2212 12u1 \u2016x\u0304 + u2x\u0304 \u2217 + vb\u201622 + u2m(\u03bb1 \u2212 \u03bb2) + \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 \u2212 1 2u1r 2, if u1 > 0, u2 \u2265 0, \u2212\u221e, if u1 = 0, u2 \u2265 0, u2 6= u\u20322, v 6= v\u2032,\n\u2212 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162m\u03bb, if u1 = 0, u2 = u \u2032 2, v = v \u2032,\n(54)\nand the dual problem is equivalent to\nmax (u1,u2)\u2208U1\u222aU2 g\u0304(u1, u2) = { g\u0304(u1, u2), if (u1, u2) \u2208 U1, \u2212 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162m\u03bb, if (u1, u2) \u2208 U2.\n(UBD\u2032\u2032)"}, {"heading": "I Proof of Theorem 8", "text": "I.1 Proof for the Non-Collinear Case In this section, we show that the results in Theorem 8 holds for the case in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 (\u22121, 1), i.e., Px\u0304 and Px\u0304\u2217 are not collinear.\nProof. As shown by part a) of Lemma 7, the dual problem of (UBP\u2032) is equivalent to (UBD\u2032). Since the strong duality holds by Lemma 6, we have\n\u2212 T\u03be(\u03b8\u2217\u03bb, x\u0304j ; \u03b8\u2217\u03bb0) = maxu1>0,u2\u22650 g\u0304(u1, u2). (55)\nClearly, problem (UBD\u2032) can be solved via the following minimization problem\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = minu1>0,u2\u22650 \u2212g\u0304(u1, u2). (56)\nLet u\u22171 and u \u2217 2 be the optimal solution of problem (56). By part a) of Lemma 7, the existence of u \u2217 1 > 0 and u\u22172 \u2265 0 is guaranteed. By introducing the slack variables s1 \u2265 0 and s2 \u2265 0, the KKT conditions of problem (56) can be written as follows:\n\u2212 12(u\u22171)2 \u2016Px\u0304 + u \u2217 2Px\u0304\n\u2217\u201622 + 1\n2 r2 \u2212 s1 = 0, (57)\n1 u\u22171 \u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2212m(\u03bb0 \u2212 \u03bb)\u2212 s2 = 0, (58)\ns1u \u2217 1 = 0, s2u \u2217 2 = 0. (59)\nSince u\u22171 > 0, then s1 = 0 and Eq. (64) results in\nu\u22171 = \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162/r. (60)\nBy plugging u\u22171 into (65), we have\n\u03d5(u\u22172) := \u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162\u2016Px\u0304\u2217\u20162 = m(\u03bb0 \u2212 \u03bb) + s2 r\u2016Px\u0304\u2217\u20162 = d+ s2 r\u2016Px\u0304\u2217\u20162 . (61)\nIt is easy to observe that\nm1). since \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 (\u22121, 1), i.e., Px\u0304 and Px\u0304 \u2217 are not collinear, \u03d5(u\u22172) monotonically increases when\nu\u22172 \u2192\u221e and limu2\u2192\u221e \u03d5(u\u22172) = 1;\nm2). d \u2208 (0, 1] due to the fact that \u03bb0 > \u03bb and Eq. (68) (note s2 \u2265 0).\na): Assume \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2265 d. We divide the proof into two cases.\na1) Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 > d, Eq. (68) and the monotonicity of \u03d5 imply that\nd+ s2\nr\u2016Px\u0304\u2217\u20162 = \u03d5(u\u22172) \u2265 \u03d5(0) = \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 > d.\nTherefore, we can see that s2 > 0 and thus u \u2217 2 = 0 due to the complementary slackness condition. By plugging u\u22172 = 0 into Eq. (67), we can get u \u2217 1 = \u2016Px\u0304\u20162 r . The result in part a) of Theorem 8 follows by noting that T\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212g\u0304(u \u2217 1, u \u2217 2).\na2) Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = d. If u \u2217 2 > 0, then s2 = 0 by the complementary slackness condition. In view\nof Eq. (68) and m1), we can see that\nd = \u03d5(u\u22172) > \u03d5(0) = \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = d,\nwhich leads to a contradiction. Therefore u\u22172 = 0 and the result in part a) of Theorem 8 follows by a similar argument as in the proof of a1).\nb): Assume \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 < d. If u \u2217 2 = 0, Eq. (68) results in\nd+ s2\nr\u2016Px\u0304\u2217\u20162 = \u03d5(0) = \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 < d,\nwhich implies that s2 < 0, a contradiction. Thus, we have u \u2217 2 > 0 and s2 = 0 by the complementary slackness condition. Eq. (68) becomes:\n\u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162\u2016Px\u0304\u2217\u20162 = m(\u03bb0 \u2212 \u03bb) r\u2016Px\u0304\u2217\u20162 = d. (62)\nExpanding the terms in Eq. (62) yields the following quadratic equation:\na2(u \u2217 2) 2 + a1u \u2217 2 + a0 = 0, (63)\nwhere a0, a1 and a2 are given by Eq. (86).\nOn the other hand, Eq. (62) implies that d \u2264 1. In view of the assumptions \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 < d and \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 6= \u22121, we can see that Px\u0304 \u2217 and Px\u0304 are not collinear. Therefore, m1) and Eq. (62) imply that d < 1. Moreover, the assumption \u03bb0 > \u03bb leads to d > 0. As a result, we have (1\u2212 d2) > 0 and thus\na2a0 < 0,\u2206 = a 2 1 \u2212 4a2a0 = 4d2(1\u2212 d2)\u2016Px\u0304\u2217\u201642(\u2016Px\u0304\u201622\u2016Px\u0304\u2217\u201622 \u2212 \u3008Px\u0304,Px\u0304\u2217\u30092) > 0.\nConsequently, (63) has only one positive solution which can be computed by the formula in Eq. (86). The result in Eq. (85) follows by a similar argument as in the proof of a1).\nI.2 Proof for the Collinear and Positive Correlated Case We prove for the case in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = 1.\nProof. Because \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = 1, by part a) of Lemma 7, the dual problem of (UBP \u2032) is given by (UBD\u2032).\nTherefore, the following KKT conditions hold as well:\n\u2212 12(u\u22171)2 \u2016Px\u0304 + u \u2217 2Px\u0304\n\u2217\u201622 + 1\n2 r2 \u2212 s1 = 0, (64)\n1 u\u22171 \u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2212m(\u03bb1 \u2212 \u03bb2)\u2212 s2 = 0, (65)\ns1u \u2217 1 = 0, s2u \u2217 2 = 0 (66)\nwhere u\u22171 > 0 and u \u2217 2 \u2265 0 are the optimal solution of (UBD\u2032), and s1, s2 \u2265 0 are the slack variables. Since u\u22171 > 0, then s1 = 0 and Eq. (64) results in\nu\u22171 = \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162/r. (67)\nBy plugging u\u22171 into (65), we have\n\u03d5(u\u22172) := \u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162\u2016Px\u0304\u2217\u20162 = m(\u03bb0 \u2212 \u03bb) + s2 r\u2016Px\u0304\u2217\u20162 = d+ s2 r\u2016Px\u0304\u2217\u20162 , (68)\nwhich implies d \u2264 1. (69) Moreover, we can see that the assumption \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = 1 actually implies Px\u0304 and Px\u0304 \u2217 are collinear.\nTherefore, there exists \u03b1 > 0 such that Px\u0304 = \u03b1Px\u0304\u2217. (70)\nBy plugging Eq. (70) into Eq. (68), we have\n\u03d5(u\u22172) = \u3008Px\u0304 + u\u22172Px\u0304\u2217,Px\u0304\u2217\u3009 \u2016Px\u0304 + u\u22172Px\u0304\u2217\u20162\u2016Px\u0304\u2217\u20162 = (\u03b1+ u\u22172)\u3008Px\u0304\u2217,Px\u0304\u2217\u3009 (\u03b1+ u\u22172)\u2016Px\u0304\u2217\u20162\u2016Px\u0304\u2217\u20162 = 1, (71)\ni.e., \u03d5(u\u22172) is a constant and does not depend on the value of u \u2217 2.\nAs a result, we can see that \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = 1 \u2265 d. Therefore, we need to show that the result in part a) of Theorem 8 holds. We divide the proof into two cases.\nCase 1. Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 > d, i.e., d < 1, Eq. (68) and Eq. (71) imply that\nd+ s2\nr\u2016Px\u0304\u2217\u20162 = \u03d5(u\u22172) = 1 > d.\nTherefore, we can see that s2 > 0 and thus u \u2217 2 = 0 due to the complementary slackness condition. By plugging u\u22172 = 0 into Eq. (67), we can get u \u2217 1 = \u2016Px\u0304\u20162 r . The result in part a) of Theorem 8 follows by noting that T\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212g\u0304(u \u2217 1, u \u2217 2).\nCase 2. Suppose \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = d = 1. On the other hand, in view of Eq. (68) and Eq. (71), we can observe\nthat s2 = 0 because otherwise\n\u03d5(u\u22172) = 1 = d+ s2\nr\u2016Px\u0304\u2217\u20162 > 1.\nBy plugging 70 into Eq. (67), we have u\u22171 = (\u03b1+ u \u2217 2)\u2016Px\u0304\u2217\u20162/r and thus\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212g\u0304(u \u2217 1, u \u2217 2) = r(\u03b1+ u \u2217 2)\u2016Px\u0304\u2217\u20162 \u2212 u\u22172m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009. (72)\nBy noting d = 1, it follows that m(\u03bb0 \u2212 \u03bb) = r\u2016Px\u0304\u2217\u20162. (73)\nTherefore, in view of Eq. (70) and Eq. (73), Eq. (72) becomes\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = r(\u03b1+ u \u2217 2)\u2016Px\u0304\u2217\u20162 \u2212 u\u22172r\u2016Px\u0304\u2217\u20162 \u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 = r\u2016Px\u0304\u20162 \u2212 \u3008\u03b8 \u2217 \u03bb0 , x\u0304\u3009,\nwhich is the same as the result in part a) of Theorem 8.\nI.3 Proof for the Collinear and Negative Correlated Case\nBefore we proceed to prove Theorem 8 for the case in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121, it is worthwhile to noting the following lemma.\nLemma 12. C Let \u03bbmax \u2265 \u03bb0 > \u03bb > 0, d = m(\u03bb0\u2212\u03bb)r\u2016Px\u0304\u2217\u20162 and assume \u03b8 \u2217 \u03bb0 is known. Then d \u2208 (0, 1]. Proof. Let x\u0304 := \u2212\u03be(\u2212x\u0304\u2217). Clearly, we can see that\n\u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = 1.\nTherefore, the results in Section I.2 apply. As a result, in view of the inequality in (69), Notice that, d is independent of x\u0304. In other words, as long as there exists a j0 \u2208 {1, . . . , p} such that Px\u0304j0 6= 0 and \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 \u2208 (\u22121, 1), one can see that\nd \u2264 1.\nMoreover, since \u03bb0 > \u03bb, it is easy to see that d > 0, which completes the proof.\nWe prove Theorem 8 for the case in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121.\nProof. Consider the case in which \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121. Clearly, Px\u0304 and Px\u0304 \u2217 are collinear and there exists\n\u03b1 = \u2212 \u3008Px\u0304,Px\u0304 \u2217\u3009\n\u2016Px\u0304\u2217\u201622 such that\nPx\u0304 = \u2212\u03b1Px\u0304\u2217. (74) As shown by part b) of Lemma 7, the dual problem is given by (UBD\u2032\u2032). Therefore, to find\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = min(u1,u2)\u2208U1\u222aU2 \u2212g\u0304(u1, u2), (75)\nwe can first compute T \u2032\u03be(\u03b8 \u2217 \u03bb, x\u0304\nj ; \u03b8\u2217\u03bb0) = inf(u1,u2)\u2208U1 \u2212g\u0304(u1, u2) (76)\nand then compare T \u2032\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) with \u2212g\u0304(u1, u2)|(u1,u2)\u2208U2 = \u2016Px\u0304\u20162 Px\u0304\u2217 m\u03bb. Clearly, we have\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = min { T \u2032\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0), \u2016Px\u0304\u20162 Px\u0304\u2217 m\u03bb } (77)\nLet us consider the problem in (76). From problem (UBD\u2032\u2032), we observe that\n\u2212 g\u0304(u1, u2)|(u1,u2)\u2208U1 = \u2212g\u0304(u1, u2). (78)\nBy noting Eq. (74), we have\n\u2212 g\u0304(u1, u2)|(u1,u2)\u2208U1 = 1 2u1 (u2 \u2212 \u03b1)2\u2016Px\u0304\u2217\u201622 \u2212 u2m(\u03bb0 \u2212 \u03bb) + 1 2 u1r 2 \u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009. (79)\nSuppose u1 is fixed, it is easy to see that\nu\u22172(u1) = argmin u2\u22650 \u2212g\u0304(u1, u2)|(u1,u2)\u2208U1 = u1m(\u03bb0 \u2212 \u03bb) \u2016Px\u0304\u2217\u201622 + \u03b1 (80)\nand\nh(u1) := min u2 \u2212g\u0304(u1, u2)|(u1,u2)\u2208U1 = \u2212g\u0304(u1, u \u2217 2(u1)) (81)\n= 1\n2 u1r\n2(1\u2212 d2)\u2212 \u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009.\nClearly, the domain of h(u1) is u1 > 0 and T \u2032 \u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = infu1>0 h(u1). By Lemma C, one can see that that d \u2208 (0, 1]. If d = 1, then\nmin u1>0\nh(u1) = \u2212\u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009.\nOtherwise, if d \u2208 (0, 1), it is easy to see that\ninf u1>0 h(u1) = lim u1\u21930\nh(u1) = \u2212\u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009.\nAll together, we have T \u2032\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212\u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8 \u2217 \u03bb0 , x\u0304\u3009. (82)\nMoreover, we observe that\n\u3008\u03b8\u2217\u03bb0 , x\u0304\u3009 = \u3008P\u03b8 \u2217 \u03bb0 , x\u0304\u3009 = \u3008\u03b8 \u2217 \u03bb0 ,Px\u0304\u3009 = \u3008\u03b8 \u2217 \u03bb0 ,\u2212\u03b1Px\u0304 \u2217\u3009 = \u2212\u03b1m\u03bb0.\nTherefore, Eq. (82) can simplified as:\nT \u2032\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212\u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8 \u2217 \u03bb0 , x\u0304\u3009 = \u03b1m\u03bb = \u2212 \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u2217\u201622 m\u03bb = \u2016Px\u0304\u20162 \u2016Px\u0304\u2217\u20162 m\u03bb. (83)\nBy Eq. (77), we can see that\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2016Px\u0304\u20162 \u2016Px\u0304\u2217\u20162 m\u03bb. (84)\nSince \u3008Px\u0304,Px\u0304 \u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 = \u22121 and d \u2208 (0, 1] by Lemma C, one can see that \u3008Px\u0304,Px\u0304\u2217\u3009 \u2016Px\u0304\u20162\u2016Px\u0304\u2217\u20162 < d. Therefore, we\nonly need to show that part b) of Theorem 8 yields the same solution as Eq. (84) . Recall that part b) of Theorem 8 says:\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = r\u2016Px\u0304 + u \u2217 2Px\u0304 \u2217\u20162 \u2212 u\u22172m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8\u2217\u03bb0 , x\u0304\u3009, (85)\nwhere\nu\u22172 = \u2212a1+\n\u221a \u2206\n2a2 ,\na2 = \u2016Px\u0304\u2217\u201642(1\u2212 d2), a1 = 2\u3008Px\u0304,Px\u0304\u2217\u3009\u2016Px\u0304\u2217\u201622(1\u2212 d2), a0 = \u3008Px\u0304,Px\u0304\u2217\u30092 \u2212 d2\u2016Px\u0304\u201622\u2016Px\u0304\u2217\u201622, \u2206 = a21 \u2212 4a2a0 = 4d2(1\u2212 d2)\u2016Px\u0304\u2217\u201642(\u2016Px\u0304\u201622\u2016Px\u0304\u2217\u201622 \u2212 \u3008Px\u0304,Px\u0304\u2217\u30092).\n(86)\nIn fact, if we plugging Eq. (74) into Eq. (86), we have\n\u2206 = 0,\nu\u22172 = \u2212a1 2a2 = \u2212\u3008Px\u0304,Px\u0304 \u2217\u3009 \u2016Px\u0304\u2217\u201622 .\nTherefore, we can see that u\u22172 = \u03b1, Px\u0304 + u \u2217 2Px\u0304 \u2217 = 0 and thus Eq. (85) results in\nT\u03be(\u03b8 \u2217 \u03bb, x\u0304 j ; \u03b8\u2217\u03bb0) = \u2212\u03b1m(\u03bb0 \u2212 \u03bb)\u2212 \u3008\u03b8 \u2217 \u03bb0 , x\u0304\u3009 = \u03b1m\u03bb = \u2016Px\u0304\u20162 \u2016Px\u0304\u2217\u20162 m\u03bb. (87)\nClearly, Eq. (84) and Eq. (87) give the same result, which completes the proof."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>The `1-regularized logistic regression (or sparse logistic regression) is a widely used method for si-<lb>multaneous classification and feature selection. Although many recent efforts have been devoted to its<lb>efficient implementation, its application to high dimensional data still poses significant challenges. In<lb>this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify<lb>the \u201c0\u201d components in the solution vector, which may lead to a substantial reduction in the number<lb>of features to be entered to the optimization. An appealing feature of Slores is that the data set needs<lb>to be scanned only once to run the screening and its computational cost is negligible compared to that<lb>of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse<lb>logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We<lb>have evaluated Slores using high-dimensional data sets from different applications. Extensive experi-<lb>mental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the<lb>efficiency of solving sparse logistic regression is improved by one magnitude in general.", "creator": "LaTeX with hyperref package"}}}