{"id": "1307.4564", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2013", "title": "From Bandits to Experts: A Tale of Domination and Independence", "abstract": "We consider the partial observation model for multi-armed bandits introduced by Mannor and Shamir. Our main result is a characterization of regret in the directed observation model with respect to the dominant and independent numbers of the observation graph. We also show that in the undirected case the learner can achieve an optimal regret without even being able to access the observation graph before selecting an action. Both results are shown using variants of the Exp3 algorithm, which saves time on the observation graph.", "histories": [["v1", "Wed, 17 Jul 2013 10:24:00 GMT  (24kb)", "http://arxiv.org/abs/1307.4564v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["noga alon", "nicol\u00f2 cesa-bianchi", "claudio gentile", "yishay mansour"], "accepted": true, "id": "1307.4564"}, "pdf": {"name": "1307.4564.pdf", "metadata": {"source": "CRF", "title": "From Bandits to Experts: A Tale of Domination and Independence", "authors": ["Noga Alon"], "emails": ["nogaa@tau.ac.il", "nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "mansour@tau.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n45 64\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "Prediction with expert advice \u2014see, e.g., [10, 13, 5, 8, 6]\u2014 is a general abstract framework for studying sequential prediction problems, formulated as repeated games between a player and an adversary. A well studied example of prediction game is the following: In each round, the adversary privately assigns a loss value to each action in a fixed set. Then the player chooses an action (possibly using randomization) and incurs the corresponding loss. The goal of the player is to control regret, which is defined as the excess loss incurred by the player as compared to the best fixed action over a sequence of rounds. Two important variants of this game have been studied in the past: the expert setting, where at the end of each round the player observes the loss assigned to each action for that round, and the bandit setting, where the player only observes the loss of the chosen action, but not that of other actions.\nLet K be the number of available actions, and T be the number of prediction rounds. The best possible regret for the expert setting is of order \u221a T logK. This optimal rate is achieved by the Hedge algorithm [8] or the Follow the Perturbed Leader algorithm [9]. In the bandit setting, the optimal regret is of order \u221a TK, achieved by the INF algorithm [2]. A bandit variant of Hedge, called Exp3 [3], achieves a regret with a slightly worse bound of order \u221a TK logK.\nRecently, Mannor and Shamir [11] introduced an elegant way for defining intermediate observability models between the expert setting (full observability) and the bandit setting (single observability). An intuitive way of representing an observability model is through a directed graph\nover actions: an arc from action i to action j implies that when playing action i we get information also about the loss of action j. Thus, the expert setting is obtained by choosing a complete graph over actions (playing any action reveals all losses), and the bandit setting is obtained by choosing an empty edge set (playing an action only reveals the loss of that action).\nThe main result of [11] concerns undirected observability graphs. The regret is characterized in terms of the independence number \u03b1 of the undirected observability graph. Specifically, they prove that \u221a T\u03b1 logK is the optimal regret (up to logarithmic factors) and show that a variant of Exp3, called ELP, achieves this bound when the graph is known ahead of time, where \u03b1 \u2208 {1, . . . ,K} interpolates between full observability (\u03b1 = 1 for the clique) and single observability (\u03b1 = K for the graph with no edges). Given the observability graph, ELP runs a linear program to compute the desired distribution over actions. In the case when the graph changes over time, and at each time\nstep ELP observes the current observability graph before prediction, a bound of \u221a\u2211T\nt=1 \u03b1t logK is shown, where \u03b1t is the independence number of the graph at time t. A major problem left open in [11] was the characterization of regret for directed observability graphs, a setting for which they only proved partial results.\nOur main result is a full characterization (to within logarithmic factors) of regret in the case of directed and dynamic observability graphs. Our upper bounds are proven using a new algorithm, called Exp3-DOM. This algorithm is efficient to run even in the dynamic case: it just needs to compute a small dominating set of the current observability graph (which must be given as side information) before prediction.1 As in the undirected case, the regret for the directed case is characterized in terms of the independence numbers of the observability graphs (computed ignoring edge directions). We arrive at this result by showing that a key quantity emerging in the analysis of Exp3-DOM can be bounded in terms of the independence numbers of the graphs. This bound (Lemma 13 in the appendix) is based on a combinatorial construction which might be of independent interest.\nWe also explore the possibility of the learning algorithm receiving the observability graph only after prediction, and not before. For this setting, we introduce a new variant of Exp3, called Exp3-SET, which achieves the same regret as ELP for undirected graphs, but without the need of accessing the current observability graph before each prediction. We show that in some random directed graph models Exp3-SET has also a good performance. In general, we can upper bound the regret of Exp3-SET as a function of the maximum acyclic subgraph of the observability graph, but this upper bound may not be tight. Yet, Exp3-SET is much simpler and computationally less demanding than ELP, which needs to solve a linear program in each round.\nThere are a variety of real-world settings where partial observability models corresponding to directed and undirected graphs are applicable. One of them is route selection. We are given a graph of possible routes connecting cities: when we select a route r connecting two cities, we observe the cost (say, driving time or fuel consumption) of the \u201cedges\u201d along that route and, in addition, we have complete information on any sub-route r\u2032 of r, but not vice versa. We abstract this in our model by having an observability graph over routes r, and an arc from r to any of its sub-routes r\u2032.\nSequential prediction problems with partial observability models also arise in the context of recommendation systems. For example, an online retailer, which advertises products to users, knows that users buying certain products are often interested in a set of related products. This knowledge can be represented as a graph over the set of products, where two products are joined\n1 Computing an approximately minimum dominating set can be done by running a standard greedy set cover algorithm, see Section 2.\nby an edge if and only if users who buy any one of the two are likely to buy the other as well. In certain cases, however, edges have a preferred orientation. For instance, a person buying a video game console might also buy a high-def cable to connect it to the TV set. Vice versa, interest in high-def cables need not indicate an interest in game consoles.\nSuch observability models may also arise in the case when a recommendation system operates in a network of users. For example, consider the problem of recommending a sequence of products, or contents, to users in a group. Suppose the recommendation system is hosted on an online social network, on which users can befriend each other. In this case, it has been observed that social relationships reveal similarities in tastes and interests [12]. However, social links can also be asymmetric (e.g., followers of celebrities). In such cases, followers might be more likely to shape their preferences after the person they follow, than the other way around. Hence, a product liked by a celebrity is probably also liked by his/her followers, whereas a preference expressed by a follower is more often specific to that person."}, {"heading": "2 Learning protocol, notation, and preliminaries", "text": "As stated in the introduction, we consider an adversarial multi-armed bandit setting with a finite action set V = {1, . . . ,K}. At each time t = 1, 2, . . . , a player (the \u201clearning algorithm\u201d) picks some action It \u2208 V and incurs a bounded loss \u2113It,t \u2208 [0, 1]. Unlike the standard adversarial bandit problem [3, 6], where only the played action It reveals its loss \u2113It,t, here we assume all the losses in a subset SIt,t \u2286 V of actions are revealed after It is played. More formally, the player observes the pairs (i, \u2113i,t) for each i \u2208 SIt,t. We also assume i \u2208 Si,t for any i and t, that is, any action reveals its own loss when played. Note that the bandit setting (Si,t = {i}) and the expert setting (Si,t = V ) are both special cases of this framework. We call Si,t the observation set of action i at time t, and write i t\u2212\u2192 j when at time t playing action i also reveals the loss of action j. Hence, Si,t = {j \u2208 V : i t\u2212\u2192 j}. The family of observation sets {Si,t}i\u2208V we collectively call the observation system at time t.\nThe adversaries we consider are nonoblivious. Namely, each loss \u2113i,t at time t can be an arbitrary function of the past player\u2019s actions I1, . . . , It\u22121. The performance of a player A is measured through the regret\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] ,\nwhere LA,T = \u2113I1,1 + \u00b7 \u00b7 \u00b7+ \u2113IT ,T and Lk,T = \u2113k,1 + \u00b7 \u00b7 \u00b7+ \u2113k,T are the cumulative losses of the player and of action k, respectively. The expectation is taken with respect to the player\u2019s internal randomization (since losses are allowed to depend on the player\u2019s past random actions, also Lk,t may be random).2 The observation system {Si,t}i\u2208V is either adversarially generated (in which case, each Si,t can be an arbitrary function of past player\u2019s actions, just like losses are), or randomly generated \u2014see Section 3. In this respect, we distinguish between adversarial and random observation systems.\nMoreover, whereas some algorithms need to know the observation system at the beginning of each step t, others need not. From this viewpoint, we shall consider two online learning settings. In the first setting, called the informed setting, the whole observation system {Si,t}i\u2208V selected by the adversary is made available to the learner before making its choice It. This is essentially the\n2 Although we defined the problem in terms of losses, our analysis can be applied to the case when actions return rewards gi,t \u2208 [0, 1] via the transformation \u2113i,t = 1\u2212 gi,t.\n\u201cside-information\u201d framework first considered in [11] In the second setting, called the uninformed setting, no information whatsoever regarding the time-t observation system is given to the learner prior to prediction.\nWe find it convenient to adopt the same graph-theoretic interpretation of observation systems as in [11]. At each time step t = 1, 2, . . . , the observation system {Si,t}i\u2208V defines a directed graph Gt = (V,Dt), where V is the set of actions, and Dt is the set of arcs, i.e., ordered pairs of nodes. For j 6= i, arc (i, j) \u2208 Dt if and only if i t\u2212\u2192 j (the self-loops created by i t\u2212\u2192 i are intentionally ignored). Hence, we can equivalently define {Si,t}i\u2208V in terms of Gt. Observe that the outdegree d+i of any i \u2208 V equals |Si,t| \u2212 1. Similarly, the indegree d\u2212i of i is the number of action j 6= i such that i \u2208 Sj,t (i.e., such that j t\u2212\u2192 i). A notable special case of the above is when the observation system is symmetric over time: j \u2208 Si,t if and only if i \u2208 Sj,t for all i, j and t. In words, playing i at time t reveals the loss of j if and only if playing j at time t reveals the loss of i. A symmetric observation system is equivalent to Gt being an undirected graph or, more precisely, to a directed graph having, for every pair of nodes i, j \u2208 V , either no arcs or length-two directed cycles. Thus, from the point of view of the symmetry of the observation system, we also distinguish between the directed case (Gt is a general directed graph) and the symmetric case (Gt is an undirected graph for all t). For instance, combining the terminology introduced so far, the adversarial, informed, and directed setting is when Gt is an adversarially-generated directed graph disclosed to the algorithm in round t before prediction, while the random, uninformed, and directed setting is when Gt is a randomly generated directed graph which is not given to the algorithm before prediction.\nThe analysis of our algorithms depends on certain properties of the sequence of graphs Gt. Two graph-theoretic notions playing an important role here are those of independent sets and dominating sets. Given an undirected graph G = (V,E), an independent set of G is any subset T \u2286 V such that no two i, j \u2208 T are connected by an edge in E. An independent set is maximal if no proper superset thereof is itself an independent set. The size of a largest (maximal) independent set is the independence number of G, denoted by \u03b1(G). If G is directed, we can still associate with it an independence number: we simply view G as undirected by ignoring arc orientation. If G = (V,D) is a directed graph, then a subset R \u2286 V is a dominating set for G if for all j 6\u2208 R there exists some i \u2208 R such that arc (i, j) \u2208 D. In our bandit setting, a time-t dominating set Rt is a subset of actions with the property that the loss of any remaining action in round t can be observed by playing some action in Rt. A dominating set is minimal if no proper subset thereof is itself a dominating set. The domination number of directed graph G, denoted by \u03b3(G), is the size of a smallest (minimal) dominating set of G.\nComputing a minimum dominating set for an arbitrary directed graph Gt is equivalent to solving a minimum set cover problem on the associated observation system {Si,t}i\u2208V . Although minimum set cover is NP-hard, the well-known Greedy Set Cover algorithm [7], which repeatedly selects from {Si,t}i\u2208V the set containing the largest number of uncovered elements so far, computes a dominating set Rt such that |Rt| \u2264 \u03b3(Gt) (1 + lnK).\nFinally, we can also lift the independence number of an undirected graph to directed graphs through the notion of maximum acyclic subgraphs: Given a directed graph G = (V,D), an acyclic subgraph of G is any graph G\u2032 = (V \u2032,D\u2032) such that V \u2032 \u2286 V , and D\u2032 = D \u2229 ( V \u2032 \u00d7 V \u2032 ) , with no (directed) cycles. We denote by mas(G) = |V \u2032| the maximum size of such V \u2032. Note that when G is undirected (more precisely, as above, when G is a directed graph having for every pair of nodes i, j \u2208 V either no arcs or length-two cycles), then mas(G) = \u03b1(G), otherwise mas(G) \u2265 \u03b1(G). In particular, when G is itself a directed acyclic graph, then mas(G) = |V |.\nAlgorithm 1: Exp3-SET: Algorithm for the uninformed setting\nParameter: \u03b7 \u2208 [0, 1]; Initialize: wi,1 = 1 for all i \u2208 V = {1, . . . ,K}; For t = 1, 2, . . . :\n1. Observation system {Si,t}i\u2208V is generated (but not disclosed) ;\n2. Set pi,t = wi,t Wi,t\nfor each i \u2208 V , where Wt = \u2211\nj\u2208V\nwj,t ;\n3. Play action It drawn according to distribution pt = (p1,t, . . . , pK,t) ;\n4. Observe pairs (i, \u2113i,t) for all i \u2208 SIt,t;\n5. Observation system {Si,t}i\u2208V is disclosed ; 6. For any i \u2208 V set wi,t+1 = wi,t exp ( \u2212\u03b7 \u2113\u0302i,t ) , where\n\u2113\u0302i,t = \u2113i,t qi,t\nI{i \u2208 SIt,t} and qi,t = \u2211\nj : j t\u2212\u2192i\npj,t ."}, {"heading": "3 Algorithms without Explicit Exploration: The Uninformed Set-", "text": "ting\nIn this section, we show that a simple variant of the Exp3 algorithm [3] obtains optimal regret (to within logarithmic factors) in two variants of the uninformed setting: (1) adversarial and symmetric, (2) random and directed. We then show that even the harder adversarial and directed setting lends itself to an analysis, though with a weaker regret bound.\nExp3-SET (Algorithm 1) runs Exp3 without mixing with the uniform distribution. Similar to Exp3, Exp3-SET uses loss estimates \u2113\u0302i,t that divide each observed loss \u2113i,t by the probability qi,t of observing it. This probability qi,t is simply the sum of all pj,t such that j t\u2212\u2192 i (the sum includes pi,t). Next, we bound the regret of Exp3-SET in terms of the key quantity\nQt = \u2211\ni\u2208V\npi,t qi,t\n= \u2211\ni\u2208V pi,t\u2211 j : j t\u2212\u2192i pj,t . (1)\nEach term pi,t/qi,t can be viewed as the probability of drawing i from pt conditioned on the event that i was observed. Similar to [11], a key aspect to our analysis is the ability to deterministically (and nonvacuously)3 upper bound Qt in terms of certain quantities defined on {Si,t}i\u2208V . We shall do so in two ways, either irrespective of how small each pi,t may be (this section) or depending on suitable lower bounds on the probabilities pi,t (Section 4). In fact, forcing lower bounds on pi,t is equivalent to adding exploration terms to the algorithm, which can be done only when knowing {Si,t}i\u2208V before each prediction \u2014an information available only in the informed setting.\n3 An obvious upper bound on Qt is K.\nThe following simple result is the building block for all subsequent results in the uninformed setting.4\nTheorem 1 In the adversarial case, the regret of Exp3-SET satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nE[Qt] .\nAs we said, in the adversarial and symmetric case the observation system at time t can be described by an undirected graph Gt = (V,Et). This is essentially the problem of [11], which they studied in the easier informed setting, where the same quantity Qt above arises in the analysis of their ELP algorithm. In their Lemma 3, they show that Qt \u2264 \u03b1(Gt), irrespective of the choice of the probabilities pt. When applied to Exp3-SET, this immediately gives the following result.\nCorollary 2 In the adversarial and symmetric case, the regret of Exp3-SET satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nE[\u03b1(Gt)] .\nIn particular, if for constants \u03b11, . . . , \u03b1T we have \u03b1(Gt) \u2264 \u03b1t, t = 1, . . . , T , then setting \u03b7 =\u221a (2 lnK) /\u2211T t=1 \u03b1t, gives\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 \u221a\u221a\u221a\u221a2(lnK) T\u2211\nt=1\n\u03b1t .\nAs shown in [11], the knowledge of \u2211T\nt=1 \u03b1(Gt) for tuning \u03b7 can be dispensed with (at the cost of extra log factors in the bound) by binning the values of \u03b7 and running Exp3 on top of a pool of instances of Exp-SET, one for each bin. The bounds proven in Corollary 2 are equivalent to those proven in [11] (Theorem 2 therein) for the ELP algorithm. Yet, our analysis is much simpler and, more importantly, our algorithm is simpler and more efficient than ELP, which requires solving a linear program at each step. Moreover, unlike ELP, Exp-SET does not require prior knowledge of the observation system {Si,t}i\u2208V at the beginning of each step.\nWe now turn to the directed setting. We first treat the random case, and then the harder adversarial case.\nThe Erdo\u030bs-Renyi model is a standard model for random directed graphs G = (V,D), where we are given a density parameter r \u2208 [0, 1] and, for any pair i, j \u2208 V , arc (i, j) \u2208 D with independent probability r.5 We have the following result.\nCorollary 3 Let Gt be generated according to the Erdo\u030bs-Renyi model with parameter r \u2208 [0, 1]. Then the regret of Exp3-SET satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 lnK\n\u03b7 +\n\u03b7 T\n2r\n( 1\u2212 (1\u2212 r)K ) .\n4 All proofs are given in the appendix. 5 Self loops, i.e., arcs (i, i) are included by default here."}, {"heading": "In the above, the expectations E[\u00b7] are w.r.t. both the algorithm\u2019s randomization and the random", "text": "generation of Gt occurring at each round. In particular, setting \u03b7 = \u221a 2r lnK\nT (1\u2212(1\u2212r)K ) , gives\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264\n\u221a 2(lnK)T (1\u2212 (1\u2212 r)K)\nr .\nNote that as r ranges in [0, 1] we interpolate between the bandit (r = 0)6 and the expert (r = 1) regret bounds.\nIn the adversarial setting, we have the following result.\nCorollary 4 In the adversarial and directed case, the regret of Exp3-SET satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nE[mas(Gt)] .\nIn particular, if for constants m1, . . . ,mT we have mas(Gt) \u2264 mt, t = 1, . . . , T , then setting \u03b7 =\u221a (2 lnK) /\u2211T t=1 mt, gives\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264 \u221a\u221a\u221a\u221a2(lnK) T\u2211\nt=1\nmt .\nObserve that Corollary 4 is a strict generalization of Corollary 2 because, as we pointed out in Section 2, mas(Gt) \u2265 \u03b1(Gt), with equality holding when Gt is an undirected graph.\nAs far as lower bounds are concerned, in the symmetric setting, the authors of [11] derive a lower bound of \u2126 (\u221a \u03b1(G)T ) in the case when Gt = G for all t. We remark that similar to the symmetric\nsetting, we can derive a lower bound of \u2126 (\u221a \u03b1(G)T ) . The simple observation is that given a directed graph G, we can define a new graph G\u2032 which is made undirected just by reciprocating arcs; namely, if there is an arc (i, j) in G we add arcs (i, j) and (j, i) in G\u2032. Note that \u03b1(G) = \u03b1(G\u2032). Since in G\u2032 the learner can only receive more information than in G, any lower bound on G also applies to G\u2032. Therefore we derive the following corollary to the lower bound of [11] (Theorem 4 therein).\nCorollary 5 Fix a directed graph G, and suppose Gt = G for all t. Then there exists a (randomized) adversarial strategy such that for any T = \u2126 ( \u03b1(G)3 ) and for any learning strategy, the expected regret of the learner is \u2126 (\u221a \u03b1(G)T ) .\nOne may wonder whether a sharper lower bound argument exists which applies to the general directed setting and involves the larger quantity mas(G). Unfortunately, the above measure does not seem to be related to the optimal regret: Using Claim 1 in the appendix (see proof of Theorem 3) one can exhibit a sequence of graphs each having a large acyclic subgraph, on which the regret of Exp3-SET is still small.\nThe lack of a lower boundmatching the upper bound provided by Corollary 4 is a good indication that something more sophisticated has to be done in order to upper bound Qt in (1). This leads us to consider more refined ways of allocating probabilities pi,t to nodes. However, this allocation will require prior knowledge of the graphs Gt.\n6 Observe that limr\u21920+ 1\u2212(1\u2212r)K r = K."}, {"heading": "4 Algorithms with Explicit Exploration: The Informed Setting", "text": "We are still in the general scenario where graphs Gt are arbitrary and directed, but now Gt is made available before prediction. We start by showing a simple example where our analysis of Exp3-SET inherently fails. This is due to the fact that, when the graph induced by the observation system is directed, the key quantity Qt defined in (1) cannot be nonvacuously upper bounded independent of the choice of probabilities pi,t. A way round it is to introduce a new algorithm, called Exp3DOM, which controls probabilities pi,t by adding an exploration term to the distribution pt. This exploration term is supported on a dominating set of the current graph Gt. For this reason, Exp3DOM requires prior access to a dominating set Rt at each time step t which, in turn, requires prior knowledge of the entire observation system {Si,t}i\u2208V .\nAs announced, the next result shows that, even for simple directed graphs, there exist distributions pt on the vertices such that Qt is linear in the number of nodes while the independence number is 1.7 Hence, nontrivial bounds on Qt can be found only by imposing conditions on distribution pt.\nFact 6 Let G = (V,D) be a total order on V = {1, . . . ,K}, i.e., such that for all i \u2208 V , arc (j, i) \u2208 D for all j = i+ 1, . . . ,K. Let p = (p1, . . . , pK) be a distribution on V such that pi = 2\u2212i, for i < K and pk = 2 \u2212K+1. Then\nQ = K\u2211\ni=1\npi pi + \u2211 j : j\u2212\u2192i pj\n= K\u2211\ni=1 pi\u2211K j=i pj = K + 1 2 .\nWe are now ready to introduce and analyze the new algorithm Exp3-DOM for the adversarial, informed and directed setting. Exp3-DOM (see Algorithm 2) runs O(logK) variants of Exp3 indexed by b = 0, 1, . . . , \u230alog2K\u230b. At time t the algorithm is given observation system {Si,t}i\u2208V , and computes a dominating set Rt of the directed graph Gt induced by {Si,t}i\u2208V . Based on the size |Rt| of Rt, the algorithm uses instance bt = \u230alog2 |Rt|\u230b to pick action It. We use a superscript b to denote the quantities relevant to the variant of Exp3 indexed by b. Similarly to the analysis of Exp3-SET, the key quantities are\nq (b) i,t =\n\u2211\nj : i\u2208Sj,t\np (b) j,t =\n\u2211\nj : j t\u2212\u2192i\np (b) j,t and Q (b) t =\n\u2211\ni\u2208V\np (b) i,t q (b) i,t , b = 0, 1, . . . , \u230alog2K\u230b .\nLet T (b) = { t = 1, . . . , T : |Rt| \u2208 [2b, 2b+1 \u2212 1] } . Clearly, the sets T (b) are a partition of the time steps {1, . . . , T}, so that \u2211b |T (b)| = T . Since the adversary adaptively chooses the dominating sets Rt, the sets T\n(b) are random. This causes a problem in tuning the parameters \u03b3(b). For this reason, we do not prove a regret bound for Exp3-DOM, where each instance uses a fixed \u03b3(b), but for a slight variant (described in the proof of Theorem 7 \u2014see the appendix) where each \u03b3(b) is set through a doubling trick.\nTheorem 7 In the adversarial and directed case, the regret of Exp3-DOM satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] \u2264\n\u230alog2 K\u230b\u2211\nb=0\n 2 b lnK\n\u03b3(b) + \u03b3(b)E\n  \u2211\nt\u2208T (b)\n( 1 + Q (b) t\n2b+1\n)    . (2)\n7 In this specific example, the maximum acyclic subgraph has size K, which confirms the looseness of Corollary 4.\nAlgorithm 2: Exp3-DOM\nInput: Exploration parameters \u03b3(b) \u2208 (0, 1] for b \u2208 { 0, 1, . . . , \u230alog2 K\u230b } ; Initialization: w (b) i,1 = 1 for all i \u2208 V and b \u2208 { 0, 1, . . . , \u230alog2 K\u230b } ; For t = 1, 2, . . . :\n1. Observation system {Si,t}i\u2208V is generated and disclosed ;\n2. Compute a dominating set Rt \u2286 V for Gt associated with {Si,t}i\u2208V ; 3. Let bt be such that |Rt| \u2208 [ 2bt , 2bt+1 \u2212 1 ] ;\n4. Set W (bt) t = \u2211 i\u2208V w (bt) i,t ;\n5. Set p (bt) i,t =\n( 1\u2212 \u03b3(bt) ) w(bt)i,t W\n(bt) t\n+ \u03b3(bt)\n|Rt| I{i \u2208 Rt};\n6. Play action It drawn according to distribution p (bt) t = ( p (bt) 1,t , . . . , p (bt) V,t ) ;\n7. Observe pairs (i, \u2113i,t) for all i \u2208 SIt,t;\n8. For any i \u2208 V set w(bt)i,t+1 = w (bt) i,t exp ( \u2212\u03b3(bt) \u2113\u0302(bt)i,t /2bt ) , where\n\u2113\u0302 (bt) i,t =\n\u2113i,t\nq (bt) i,t\nI{i \u2208 SIt,t} and q(bt)i,t = \u2211\nj : j t\u2212\u2192i\np (bt) j,t .\nMoreover, if we use a doubling trick to choose \u03b3(b) for each b = 0, . . . , \u230alog2 K\u230b, then\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] = O  (lnK)E   \u221a\u221a\u221a\u221a T\u2211\nt=1\n( 4|Rt|+Q(bt)t )  + (lnK) ln(KT )   . (3)\nImportantly, the next result shows how bound (3) of Theorem 7 can be expressed in terms of the sequence \u03b1(Gt) of independence numbers of graphs Gt whenever the Greedy Set Cover algorithm [7] (see Section 2) is used to compute the dominating set Rt of the observation system at time t.\nCorollary 8 If Step 2 of Exp3-DOM uses the Greedy Set Cover algorithm to compute the dominating sets Rt, then the regret of Exp-DOM with doubling trick satisfies\nmax k\u2208V\nE [ LA,T \u2212 Lk,T ] = O  ln(K) \u221a\u221a\u221a\u221aln(KT ) T\u2211\nt=1\n\u03b1(Gt) + ln(K) ln(KT )\n  ,\nwhere, for each t, \u03b1(Gt) is the independence number of the graph Gt induced by observation system {Si,t}i\u2208V ."}, {"heading": "5 Conclusions and work in progress", "text": "We have investigated online prediction problems in partial information regimes that interpolate between the classical bandit and expert settings. We have shown a number of results characterizing prediction performance in terms of: the structure of the observation system, the amount of information available before prediction, the nature (adversarial or fully random) of the process generating the observation system. Our results are substantial improvements over the paper [11] that initiated this interesting line of research. Our improvements are diverse, and range from considering both informed and uninformed settings to delivering more refined graph-theoretic characterizations, from providing more efficient algorithmic solutions to relying on simpler (and often more general) analytical tools.\nSome research directions we are currently pursuing are the following.\n1. We are currently investigating the extent to which our results could be applied to the case when the observation system {Si,t}i\u2208V may depend on the loss \u2113It,t of player\u2019s action It. Notice that this would prevent a direct construction of an unbiased estimator for unobserved losses, which many worst-case bandit algorithms (including ours \u2014see the appendix) hinge upon.\n2. The upper bound contained in Corollary 4 and expressed in terms of mas(\u00b7) is almost certainly suboptimal, even in the uninformed setting, and we are trying to see if more adequate graph complexity measures can be used instead.\n3. Our lower bound (Corollary 5) heavily relies on the corresponding lower bound in [11] which, in turn, refers to a constant graph sequence. We would like to provide a more complete charecterization applying to sequences of adversarially-generated graphs G1, G2, . . . , GT in terms of sequences of their corresponding independence numbers \u03b1(G1), \u03b1(G2), . . . , \u03b1(GT ) (or variants thereof), in both the uninformed and the informed settings."}, {"heading": "Acknowledgments", "text": "The first author was supported in part by an ERC advanced grant, by a USA-Israeli BSF grant, and by the Israeli I-CORE program. The second author acknowledges partial support by MIUR (project ARS TechnoMedia, PRIN 2010-2011, grant no. 2010N5K7EB 003). The fourth author was supported in part by a grant from the Israel Science Foundation, a grant from the United StatesIsrael Binational Science Foundation (BSF), a grant by Israel Ministry of Science and Technology and the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11)."}, {"heading": "A Technical lemmas and proofs", "text": "This section contains the proofs of all technical results occurring in the main text, along with ancillary graph-theoretic lemmas. Throughout this appendix, Et[\u00b7] is a shorthand for E [ \u00b7 | I1, . . . , It\u22121 ] .\nProof of Theorem 1. Following the proof of Exp3 [3], we have\nWt+1 Wt\n= \u2211\ni\u2208V\nwi,t+1 Wt\n= \u2211\ni\u2208V\nwi,t exp(\u2212\u03b7 \u2113\u0302i,t) Wt\n= \u2211\ni\u2208V\npi,t exp(\u2212\u03b7 \u2113\u0302i,t)\n\u2264 \u2211\ni\u2208V\npi,t ( 1\u2212 \u03b7\u2113\u0302i,t + 1\n2 \u03b72(\u2113\u0302i,t) 2\n) using e\u2212x \u2264 1\u2212 x+ x2/2 for all x \u2265 0\n\u2264 1\u2212 \u03b7 \u2211\ni\u2208V\npi,t\u2113\u0302i,t + \u03b72\n2\n\u2211\ni\u2208V\npi,t(\u2113\u0302i,t) 2 .\nTaking logs, using ln(1\u2212 x) \u2264 \u2212x for all x \u2265 0, and summing over t = 1, . . . , T yields\nln WT+1 W1\n\u2264 \u2212\u03b7 T\u2211\nt=1\n\u2211\ni\u2208V\npi,t\u2113\u0302i,t + \u03b72\n2\nT\u2211\nt=1\n\u2211\ni\u2208V\npi,t(\u2113\u0302i,t) 2 .\nMoreover, for any fixed comparison action k, we also have\nln WT+1 W1 \u2265 ln wk,T+1 W1\n= \u2212\u03b7 T\u2211\nt=1\n\u2113\u0302k,t \u2212 lnK .\nPutting together and rearranging gives\nT\u2211\nt=1\n\u2211\ni\u2208V\npi,t\u2113\u0302i,t \u2264 T\u2211\nt=1\n\u2113\u0302k,t + lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\n\u2211\ni\u2208V\npi,t(\u2113\u0302i,t) 2 . (4)\nNote that, for all i \u2208 V ,\nEt[\u2113\u0302i,t] = \u2211\nj : i\u2208Sj,t\npj,t \u2113i,t qi,t\n= \u2211\nj : j t\u2212\u2192i\npj,t \u2113i,t qi,t = \u2113i,t qi,t\n\u2211\nj : j t\u2212\u2192i\npj,t = \u2113i,t .\nMoreover,\nEt [ (\u2113\u0302i,t) 2 ] = \u2211\nj : i\u2208Sj,t\npj,t \u21132i,t q2i,t = \u21132i,t q2i,t\n\u2211\nj : j t\u2212\u2192i\npj,t \u2264 1\nq2i,t\n\u2211\nj : j t\u2212\u2192i\npj,t = 1\nqi,t .\nHence, taking expectations Et on both sides of (4), and recalling the definition of Qt, we can write\nT\u2211\nt=1\n\u2211\ni\u2208V\npi,t \u2113i,t \u2264 T\u2211\nt=1\n\u2113k,t + lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nQt . (5)\nFinally, taking expectations to remove conditioning gives\nE [ LA,T \u2212 Lk,T ] \u2264 lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nE[Qt] ,\nas claimed.\nProof of Corollary 3. Fix round t, and let G = (V,D) be the Erdo\u030bs-Renyi random graph generated at time t, N\u2212i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) \u2208 D, and denote by d\u2212i the indegree of i.\nClaim 1 Let p1, . . . , pK be an arbitrary probability distribution defined over V , f : V \u2192 V be an arbitrary permutation of V , and Ef denote the expectation w.r.t. permutation f when f is drawn uniformly at random. Then, for any i \u2208 V , we have\nEf   pf(i) pf(i) + \u2211 j : f(j)\u2208N\u2212\nf(i) pf(j)\n  = 1\n1 + d\u2212i .\nProof of Claim 1. Consider selecting a subset S \u2282 V of 1 + d\u2212i nodes. We shall consider the contribution to the expectation when S = N\u2212f(i)\u222a{f(i)}. Since there are K(K\u22121) \u00b7 \u00b7 \u00b7 (K\u2212d \u2212 i +1) terms (out of K!) contributing to the expectation, we can write\nEf   pf(i) pf(i) + \u2211 j : f(j)\u2208N\u2212\nf(i) pf(j)\n  = 1(K\nd\u2212i\n) \u2211\nS\u2282V,|S|=d\u2212i\n1\n1 + d\u2212i\n\u2211\ni\u2208S\npi pi + \u2211 j\u2208S,j 6=i pj\n= 1(K d\u2212i\n) \u2211\nS\u2282V,|S|=d\u2212i\n1\n1 + d\u2212i\n= 1\n1 + d\u2212i .\nClaim 2 Let p1, . . . , pK be an arbitrary probability distribution defined over V , and E denote the expectation w.r.t. the Erdo\u030bs-Renyi random draw of arcs at time t. Then, for any fixed i \u2208 V , we have\nE   pi pi +\n\u2211 j : j t\u2212\u2192i pj\n  = 1\nrK\n( 1\u2212 (1\u2212 r)K ) .\nProof of Claim 2. For the given i \u2208 V and time t, consider the Bernoulli random variables Xj , j \u2208\nV \\ {i}, and denote by Ej : j 6=i the expectation w.r.t. all of them. We symmetrize E [\npi pi+ \u2211\nj : j t\u2212\u2192i pj\n]\nby means of a random permutation f , as in Claim 1. We can write\nE   pi pi +\n\u2211 j : j t\u2212\u2192i pj\n  = Ej : j 6=i [ pi\npi + \u2211 j : j 6=iXjpj\n]\n= Ej : j 6=iEf\n[ pf(i)\npf(i) + \u2211 j : j 6=iXf(j)pf(j)\n] (by symmetry)\n= Ej : j 6=i\n[ 1\n1 + \u2211\nj : j 6=iXj\n] (from Claim 1)\n=\nK\u22121\u2211\ni=0\n( K \u2212 1\ni\n) ri(1\u2212 r)K\u22121\u2212i 1\ni+ 1\n= 1\nrK\nK\u22121\u2211\ni=0\n( K\ni+ 1\n) ri+1(1\u2212 r)K\u22121\u2212i\n= 1\nrK\n( 1\u2212 (1\u2212 r)K ) .\nAt this point, we follow the proof of Theorem 1 up until (5). We take an expectation EG1,...,GT w.r.t. the randomness in generating the sequence of graphs G1, . . . , GT . This yields\nT\u2211\nt=1\nEG1,...,GT\n[ \u2211\ni\u2208V\npi,t \u2113i,t\n] \u2264 T\u2211\nt=1\n\u2113k,t + lnK\n\u03b7 +\n\u03b7\n2\nT\u2211\nt=1\nEG1,...,GT [Qt] .\nWe use Claim 2 to upper bound EG1,...,GT [Qt] by 1 r\n( 1\u2212 (1\u2212 r)K ) , and take the outer expectation\nto remove conditioning, as in the proof of Theorem 1. This concludes the proof.\nThe following lemma can be seen as a generalization of Lemma 3 in [11].\nLemma 9 Let G = (V,D) be a directed graph with vertex set V = {1, . . . ,K}, and arc set D. Let N\u2212i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) \u2208 D. Then\nK\u2211\ni=1\npi pi + \u2211 j\u2208N\u2212i pj \u2264 mas(G) .\nProof. We will show that there is a subset of vertices V \u2032 such that the induced graph is acyclic and |V \u2032| \u2265 \u2211Ki=1 pipi+\u2211\nj\u2208N \u2212\ni\npj .\nWe prove the lemma by growing set V \u2032 starting off from V \u2032 = \u2205. Let\n\u03a60 =\nK\u2211\ni=1\npi pi + \u2211 j\u2208N\u2212i pj ,\nand i1 be the vertex which minimizes pi + \u2211\nj\u2208N\u2212i pj over i \u2208 V . We are going to delete i1 from\nthe graph, along with all its incoming neighbors (set N\u2212i1 ), and all edges which are incident (both\ndeparting and incoming) to these nodes, and then iterating on the remaining graph. Let us denote the in-neighborhoods of the shrunken graph from the first step by N\u2212i,1.\nThe contribution of all the deleted vertices to \u03a60 is\n\u2211\nr\u2208N\u2212i1 \u222a{i1}\npr pr + \u2211 j\u2208N\u2212r pj \u2264\n\u2211\nr\u2208N\u2212i1 \u222a{i1}\npr pi1 + \u2211 j\u2208N\u2212i1 pj = 1 ,\nwhere the inequality follows from the minimality of i1. Let V \u2032 \u2190 V \u2032 \u222a {i1}, and V1 = V \u2212 (N\u2212i1 \u222a {i1}). Then from the first step we have\n\u03a61 = \u2211\ni\u2208V1\npi pi + \u2211 j\u2208N\u2212i,1 pj \u2265\n\u2211\ni\u2208V1\npi pi + \u2211 j\u2208N\u2212i pj \u2265 \u03a60 \u2212 1 .\nWe apply the very same argument to \u03a61 with node i2 (minimizing pi + \u2211\nj\u2208N\u2212i,1 pj over i \u2208 V1),\nto \u03a62 with node i3, . . . , to \u03a6s\u22121 with node is, up until \u03a6s = 0, i.e., up until no nodes are left in the shrunken graph. This gives \u03a60 \u2264 s = |V \u2032|, where V \u2032 = {i1, i2, . . . , is}. Moreover, since in each step r = 1, . . . , s we remore all remaining arcs incoming to ir, the graph induced by set V\n\u2032 cannot contain cycles.\nProof of Corollary 4. The claim follows from a direct combination of Theorem 1 with Lemma 9.\nProof of Fact 6. Using standard properties of geometric sums, one can immediately see that\nK\u2211\ni=1 pi\u2211K j=i pj =\nK\u22121\u2211\ni=1\n2\u2212i\n2\u2212i+1 +\n2\u2212K+1 2\u2212K+1 = K \u2212 1 2 + 1 = K + 1 2 ,\nhence the claimed result.\nThe following graph-theoretic lemma turns out to be fairly useful for analyzing directed settings. It is a directed-graph counterpart to a well-known result [4, 14] holding for undirected graphs.\nLemma 10 Let G = (V,D) be a directed graph, with V = {1, . . . ,K}. Let d\u2212i be the indegree of node i, and \u03b1 = \u03b1(G) be the independence number of G. Then\nK\u2211\ni=1\n1\n1 + d\u2212i \u2264 2\u03b1 ln\n( 1 + K\n\u03b1\n) .\nProof. We will proceed by induction, starting off from the original K-node graph G = GK with indegrees {d\u2212i }Ki=1 = {d\u2212i,K}Ki=1, and independence number \u03b1 = \u03b1K , and then progressively shrink G by eliminating nodes and incident (both departing and incoming) arcs, thereby obtaining a sequence of smaller and smaller graphs GK , GK\u22121, GK\u22122, . . ., and associated indegrees {d\u2212i,K}Ki=1, {d\u2212i,K\u22121}K\u22121i=1 , {d\u2212i,K\u22122}K\u22122i=1 , . . . , and independence numbers \u03b1K , \u03b1K\u22121, \u03b1K\u22122, . . .. Specifically, in step s we sort nodes i = 1, . . . , s of Gs in nonincreasing value of d \u2212 i,s, and obtain Gs\u22121 from Gs by eliminating node 1 (i.e., one having the largest indegree among the nodes of Gs), along with its incident arcs. On all such graphs, we will use the classical Turan\u2019s theorem (e.g., [1]) stating that\nany undirected graph with ns nodes and ms edges has an independent set of size at least ns\n2ms ns\n+1 .\nThis implies that if Gs = (Vs,Ds), then \u03b1s satisfies 8\n|Ds| |Vs| \u2265 |Vs| 2\u03b1s \u2212 1 2 . (6)\nWe then start from GK . We can write\nd\u22121,K = maxi=1...K d\u2212i,K \u2265\n1\nK\nK\u2211\ni=1\nd\u2212i,K = |DK | |VK | \u2265 |VK | 2\u03b1K \u2212 1 2 .\nHence,\nK\u2211\ni=1\n1\n1 + d\u2212i,K =\n1\n1 + d\u22121,K +\nK\u2211\ni=2\n1\n1 + d\u2212i,K\n\u2264 2\u03b1K \u03b1K +K +\nK\u2211\ni=2\n1\n1 + d\u2212i,K\n\u2264 2\u03b1K \u03b1K +K +\nK\u22121\u2211\ni=1\n1\n1 + d\u2212i,K\u22121 ,\nwhere the last inequality follows from d\u2212i+1,K \u2265 d\u2212i,K\u22121, i = 1, . . . K \u2212 1, due to the arc elimination turning GK into GK\u22121. Recursively applying the very same argument to GK\u22121 (i.e., to the sum\u2211K\u22121\ni=1 1\n1+d\u2212 i,K\u22121\n), and then iterating all the way to G1 yields the upper bound\nK\u2211\ni=1\n1\n1 + d\u2212i,K \u2264\nK\u2211\ni=1\n2\u03b1i \u03b1i + i .\nCombining with \u03b1i \u2264 \u03b1K = \u03b1, and \u2211K i=1 1 \u03b1+i \u2264 ln ( 1 + K\u03b1 ) concludes the proof.\nThe next lemma relates the size |Rt| of the dominating set Rt computed by the Greedy Set Cover algorithm of [7] operating on the time-t observation system {Si,t}i\u2208V to the independence number \u03b1(Gt) and the domination number \u03b3(Gt) of Gt.\nLemma 11 Let {Si}i\u2208V be an observation system, and G = (V,D) be the induced directed graph, with vertex set V = {1, . . . ,K}, independence number \u03b1 = \u03b1(G), and domination number \u03b3 = \u03b3(G). Then the dominating set R constructed by the Greedy Set Cover algorithm (see Section 2) satisfies\n|R| \u2264 min { \u03b3(1 + lnK), \u23082\u03b1 lnK\u2309+ 1 } .\nProof. As recalled in Section 2, the Greedy Set Cover algorithm of [7] achieves |R| \u2264 \u03b3(1 + lnK). In order to prove the other bound, consider the sequence of graphs G = G1, G2, . . . , where each Gs+1 = (Vs+1,Ds+1) is obtained by removing from Gs the vertex is selected by the Greedy Set\n8 Notice that |Ds| is at least as large as the number of edges of the undirected version of Gs which the independence number \u03b1s actually refers to.\nCover algorithm, together with all the vertices in Gs that are dominated by is, and all arcs incident to these vertices. By definition of the algorithm, the outdegree d+s of is in Gs is largest in Gs. Hence,\nd+s \u2265 |Ds| |Vs| \u2265 |Vs| 2\u03b1s \u2212 1 2 \u2265 |Vs| 2\u03b1 \u2212 1 2\nby Turan\u2019s theorem (e.g., [1]), where \u03b1s is the independence number of Gs and \u03b1 \u2265 \u03b1s. This shows that\n|Vs+1| = |Vs| \u2212 d+s \u2212 1 \u2264 |Vs| ( 1\u2212 1\n2\u03b1\n) \u2264 |Vs|e\u22121/(2\u03b1) .\nIterating, we obtain |Vs| \u2264 K e\u2212s/(2\u03b1). Choosing s = \u23082\u03b1 lnK\u2309+ 1 gives |Vs| < 1, thereby covering all nodes. Hence the dominating set R = {i1, . . . , is} so constructed satisfies |R| \u2264 \u23082\u03b1 lnK\u2309 + 1.\nLemma 12 If a, b \u2265 0, and a+ b \u2265 B > A > 0, then\na a+ b\u2212A \u2264 a a+ b + A B \u2212A .\nProof. a\na+ b\u2212A \u2212 a a+ b =\naA (a+ b)(a+ b\u2212A) \u2264 A a+ b\u2212A \u2264 A B \u2212A .\nWe now lift Lemma 10 to a more general statement.\nLemma 13 Let G = (V,D) be a directed graph, with vertex set V = {1, . . . ,K}, and arc set D. Let N\u2212i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) \u2208 D. Let \u03b1 be the independence number of G, R \u2286 V be a dominating set for G of size r = |R|, and p1, . . . , pK be a probability distribution defined over V , such that pi \u2265 \u03b2 > 0, for i \u2208 R. Then\nK\u2211\ni=1\npi pi + \u2211 j\u2208N\u2212i pj \u2264 2\u03b1 ln\n( 1 +\n\u2308K2r\u03b2 \u2309+K \u03b1\n) + 2r .\nProof. The idea is to appropriately discretize the probability values pi, and then upper bound the discretized counterpart of \u2211K i=1 pi pi+ \u2211 j\u2208N \u2212\ni\npj by reducing to an expression that can be handled\nby Lemma 10. In order to make this discretization effective, we need to single out the terms pi pi+ \u2211\nj\u2208N \u2212\ni\npj corresponding to nodes i \u2208 R. We first write\nK\u2211\ni=1\npi pi + \u2211 j\u2208N\u2212i pj =\n\u2211\ni\u2208R\npi pi + \u2211 j\u2208N\u2212i pj + \u2211 i/\u2208R pi pi + \u2211 j\u2208N\u2212i pj\n\u2264 r + \u2211\ni/\u2208R\npi pi + \u2211 j\u2208N\u2212i pj , (7)\nand then focus on (7).\nLet us discretize the unit interval9 (0, 1] into subintervals ( j\u22121M , j M ], j = 1, . . . ,M , where M =\n\u2308K2r\u03b2 \u2309. Let p\u0302i = j/M be the discretized version of pi, being j the unique integer such that\np\u0302i \u2212 1/M < pi \u2264 p\u0302i .\nLet us focus on a single node i /\u2208 R with indegree d\u2212i = |N\u2212i |, and introduce the shorthand notation Pi =\n\u2211 j\u2208N\u2212i pj, and P\u0302i = \u2211 j\u2208N\u2212i p\u0302j. We have that P\u0302i \u2265 Pi \u2265 \u03b2, since i is dominated by some\nnode j \u2208 R\u2229N\u2212i such that pj \u2265 \u03b2. Moreover, Pi > P\u0302i\u2212 d\u2212i M \u2265 \u03b2\u2212 d\u2212i M > 0, and p\u0302i+ P\u0302i \u2265 \u03b2. Hence, for any fixed node i /\u2208 R, we can write pi\npi + Pi \u2264 p\u0302i p\u0302i + Pi\n< p\u0302i\np\u0302i + P\u0302i \u2212 d \u2212 i\nM\n\u2264 p\u0302i p\u0302i + P\u0302i\n+ d\u2212i /M\n\u03b2 \u2212 d\u2212i /M\n= p\u0302i\np\u0302i + P\u0302i + d\u2212i \u03b2M \u2212 d\u2212i\n< p\u0302i\np\u0302i + P\u0302i +\nr\nK \u2212 r ,\nwhere in the second-last inequality we used Lemma 12 with a = p\u0302i, b = P\u0302i, A = d \u2212 i /M , and B = \u03b2 > d\u2212i /M . Recalling (7), and summing over i then gives\nK\u2211\ni=1\npi pi + Pi\n\u2264 r + \u2211\ni/\u2208R\np\u0302i\np\u0302i + P\u0302i + r =\n\u2211\ni/\u2208R\np\u0302i\np\u0302i + P\u0302i + 2r . (8)\nTherefore, we continue by bounding from above the right-hand side of (8). We first observe that\n\u2211\ni/\u2208R\np\u0302i\np\u0302i + P\u0302i =\n\u2211\ni/\u2208R\ns\u0302i\ns\u0302i + S\u0302i , S\u0302i =\n\u2211\nj\u2208N\u2212i\ns\u0302j , (9)\nwhere s\u0302i = Mp\u0302i, i = 1, . . . ,K, are integers. Based on the original graph G, we construct a new graph G\u0302 made up of connected cliques. In particular:\n\u2022 Each node i of G is replaced in G\u0302 by a clique Ci of size s\u0302i; nodes within Ci are connected by length-two cycles.\n\u2022 If arc (i, j) is in G, then for each node of Ci draw an arc towards each node of Cj.\nWe would like to apply Lemma 10 to G\u0302. Notice that, by the above construction:\n\u2022 The independence number of G\u0302 is the same as that of G;\n\u2022 The indegree d\u0302\u2212k of each node k in clique Ci satisfies d\u0302\u2212k = s\u0302i \u2212 1 + S\u0302i. 9 The zero value won\u2019t be of our concern here, because if pi = 0, the corresponding term in (7) can be disregarded.\n\u2022 The total number of nodes of G\u0302 is K\u2211\ni=1\ns\u0302i = M\nK\u2211\ni=1\np\u0302i < M\nK\u2211\ni=1\n( pi + 1\nM\n) = M +K .\nHence, we are in a position to apply Lemma 10 to G\u0302 with indegrees d\u0302\u2212k , revealing that\n\u2211\ni/\u2208R\ns\u0302i\ns\u0302i + S\u0302i =\n\u2211\ni/\u2208R\n\u2211\nk\u2208Ci\n1\n1 + d\u0302\u2212k \u2264\nK\u2211\ni=1\n\u2211\nk\u2208Ci\n1\n1 + d\u0302\u2212k \u2264 2\u03b1 ln\n( 1 + M +K\n\u03b1\n) .\nPutting together as in (8) and (9), and recalling the value of M gives the claimed result.\nProof of Theorem 7. We start to bound the contribution to the overall regret of an instance indexed by b. When clear from the context, we remove the superscript b from \u03b3(b), w (b) i,t , p (b) i,t , and other related quantities. For any t \u2208 T (b) we have\nWt+1 Wt\n= \u2211\ni\u2208V\nwi,t+1 Wt\n= \u2211\ni\u2208V\nwi,t Wt\nexp ( \u2212(\u03b3/2b) \u2113\u0302i,t )\n= \u2211\ni\u2208Rt\npi,t \u2212 \u03b3/|Rt| 1\u2212 \u03b3 exp ( \u2212(\u03b3/2b) \u2113\u0302i,t ) + \u2211\ni 6\u2208Rt\npi,t 1\u2212 \u03b3 exp\n( \u2212(\u03b3/2b) \u2113\u0302i,t )\n\u2264 \u2211\ni\u2208Rt\npi,t \u2212 \u03b3/|Rt| 1\u2212 \u03b3\n( 1\u2212 \u03b3\n2b \u2113\u0302i,t +\n1\n2 ( \u03b3 2b \u2113\u0302i,t )2) + \u2211\ni 6\u2208Rt\npi,t 1\u2212 \u03b3\n( 1\u2212 \u03b3\n2b \u2113\u0302i,t +\n1\n2 ( \u03b3 2b \u2113\u0302i,t\n)2)\n(using e\u2212x \u2264 1\u2212 x+ x2/2 for all x \u2265 0)\n\u2264 1\u2212 \u03b3/2 b 1\u2212 \u03b3 \u2211\ni\u2208V\npi,t\u2113\u0302i,t + \u03b32/2b 1\u2212 \u03b3 \u2211\ni\u2208Rt\n\u2113\u0302i,t |Rt| + 1 2\n(\u03b3/2b)2 1\u2212 \u03b3 \u2211\ni\u2208V\npi,t ( \u2113\u0302i,t )2 .\nTaking logs, upper bounding, and summing over t \u2208 T (b) yields\nln W|T (b)|+1 W1 \u2264 \u2212 \u03b3/2 b 1\u2212 \u03b3 \u2211\nt\u2208T (b)\n\u2211\ni\u2208V\npi,t\u2113\u0302i,t + \u03b32/2b 1\u2212 \u03b3 \u2211\nt\u2208T (b)\n\u2211\ni\u2208Rt\n\u2113\u0302i,t |Rt| + 1 2\n(\u03b3/2b)2 1\u2212 \u03b3 \u2211\nt\u2208T (b)\n\u2211\ni\u2208V\npi,t ( \u2113\u0302i,t )2 .\nMoreover, for any fixed comparison action k, we also have\nln W|T (b)|+1\nW1 \u2265 ln\nwk,|T (b)|+1\nW1 = \u2212 \u03b3 2b\n\u2211\nt\u2208T (b)\n\u2113\u0302k,t \u2212 lnK .\nPutting together, rearranging, and using 1\u2212 \u03b3 \u2264 1 gives\n\u2211\nt\u2208T (b)\n\u2211\ni\u2208V\npi,t\u2113\u0302i,t \u2264 \u2211\nt\u2208T (b)\n\u2113\u0302k,t + 2b lnK\n\u03b3 + \u03b3\n\u2211\nt\u2208T (b)\n\u2211\ni\u2208Rt\n\u2113\u0302i,t |Rt| + \u03b3 2b+1\n\u2211\nt\u2208T (b)\n\u2211\ni\u2208V\npi,t ( \u2113\u0302i,t )2 .\nReintroducing the notation \u03b3(b) and summing over b = 0, 1, . . . , \u230alog2K\u230b gives\nT\u2211\nt=1\n( \u2211\ni\u2208V\np (bt) i,t \u2113\u0302 (bt) i,t \u2212 \u2113\u0302k,t\n) \u2264 \u230alog2 K\u230b\u2211\nb=0\n2b lnK\n\u03b3(b) +\nT\u2211\nt=1\n\u2211\ni\u2208Rt\n\u03b3(bt)\u2113\u0302 (bt) i,t\n|Rt| +\nT\u2211\nt=1\n\u03b3(bt) 2bt+1\n\u2211\ni\u2208V\np (bt) i,t ( \u2113\u0302 (bt) i,t )2 . (10)\nNow, similarly to the proof of Theorem 1, we have that, for any i and t, Et [ \u2113\u0302 (bt) i,t ] = \u2113i,t and\nEt\n[ (\u2113\u0302\n(bt) i,t )\n2 ] \u2264 1\nq (bt) i,t\n. Hence, taking expectations Et on both sides of (10) and recalling the definition\nof Q (b) t gives\nT\u2211\nt=1\n( \u2211\ni\u2208V\np (bt) i,t \u2113i,t \u2212 \u2113k,t\n) \u2264 \u230alog2 K\u230b\u2211\nb=0\n2b lnK\n\u03b3(b) +\nT\u2211\nt=1\n\u2211\ni\u2208Rt\n\u03b3(bt)\u2113i,t |Rt| +\nT\u2211\nt=1\n\u03b3(bt) 2bt+1 Q (bt) t . (11)\nMoreover, T\u2211\nt=1\n\u2211\ni\u2208Rt\n\u03b3(bt)\u2113i,t |Rt| \u2264 T\u2211\nt=1\n\u2211\ni\u2208Rt\n\u03b3(bt) |Rt| =\nT\u2211\nt=1\n\u03b3(bt) =\n\u230alog2 K\u230b\u2211\nb=0\n\u03b3(b)|T (b)|\nand T\u2211\nt=1\n\u03b3(bt) 2bt+1 Q (bt) t =\n\u230alog2 K\u230b\u2211\nb=0\n\u03b3(b) 2b+1\n\u2211\nt\u2208T (b)\nQ (b) t .\nHence, plugging back into (11), taking outer expectations on both sides and recalling that T (b) is random (since the adversary adaptively decides which steps t fall into T (b)), we get\nE [ LA,T \u2212 Lk,T ] \u2264\n\u230alog2 K\u230b\u2211\nb=0\nE\n 2 b lnK\n\u03b3(b) + \u03b3(b)|T (b)|+ \u03b3\n(b)\n2b+1\n\u2211\nt\u2208T (b)\nQ (b) t\n \n=\n\u230alog2 K\u230b\u2211\nb=0\n 2 b lnK\n\u03b3(b) + \u03b3(b)E\n  \u2211\nt\u2208T (b)\n( 1 + Q (b) t\n2b+1\n)    . (12)\nThis establishes (2). In order to prove inequality (3), we need to tune each \u03b3(b) separately. However, a good choice of \u03b3(b) depends on the unknown random quantity\nQ (b) = \u2211\nt\u2208T (b)\n( 1 + Q (b) t\n2b+1\n) .\nTo overcome this problem, we slightly modify Exp3-DOM by applying a doubling trick10 to guess Q (b) for each b. Specifically, for each b = 0, 1, . . . , \u230alog2K\u230b, we use a sequence \u03b3(b)r = \u221a (2b lnK)/2r, for r = 0, 1, . . . . We initially run the algorithm with \u03b3 (b) 0 . Whenever the algorithm is running with\n10 The pseudo-code for the variant of Exp3-DOM using such a doubling trick is not displayed in this extended abstract.\n\u03b3 (b) r and observes that \u2211 sQ (b) s > 2 r, where the sum is over all s so far in T (b),11 then we restart the algorithm with \u03b3 (b) r+1. Because the contribution of instance b to (12) is\n2b lnK\n\u03b3(b) + \u03b3(b)\n\u2211\nt\u2208T (b)\n( 1 + Q (b) t\n2b+1\n) ,\nthe regret we pay when using any \u03b3 (b) r is at most 2 \u221a (2b lnK)2r. The largest r we need is \u2308 log2 Q (b)\u2309 and \u2308log2 Q (b) \u2309\u2211\nr=0\n2r/2 < 5 \u221a Q (b) .\nSince we pay regret at most 1 for each restart, we get\nE [ LA,T \u2212 Lk,T ] \u2264 c\n\u230alog2 K\u230b\u2211\nb=0\nE   \u221a\u221a\u221a\u221a\u221a(lnK)  2b|T (b)|+ 1\n2\n\u2211\nt\u2208T (b)\nQ (b) t\n + \u2308 log2Q (b)\u2309   .\nfor some positive constant c. Taking into account that\n\u230alog2 K\u230b\u2211\nb=0\n2b|T (b)| \u2264 2 T\u2211\nt=1\n|Rt|\n\u230alog2 K\u230b\u2211\nb=0\n\u2211\nt\u2208T (b)\nQ (b) t =\nT\u2211\nt=1\nQ (bt) t\n\u230alog2 K\u230b\u2211\nb=0\n\u2308 log2 Q (b)\u2309 = O ( (lnK) ln(KT ) ) ,\nwe obtain\nE [ LA,T \u2212 Lk,T ] \u2264 c\n\u230alog2 K\u230b\u2211\nb=0\nE   \u221a\u221a\u221a\u221a\u221a(lnK)  2b|T (b)|+ 1\n2\n\u2211\nt\u2208T (b)\nQ (b) t\n   +O ( (lnK) ln(KT ) )\n\u2264 c \u230alog2 K\u230bE\n  \u221a\u221a\u221a\u221a lnK \u230alog2 K\u230b T\u2211\nt=1\n( 2|Rt|+ 1\n2 Q\n(bt) t\n) +O ( (lnK) ln(KT ) )\n= O  (lnK)E   \u221a\u221a\u221a\u221a T\u2211\nt=1\n( 4|Rt|+Q(bt)t )  + (lnK) ln(KT )  \nas desired.\n11 Notice that \u2211\ns Q\n(b) s is an observable quantity.\nProof of Corollary 8. We start off from the upper bound (3) in the statement of Theorem 7. We want to bound the quantities |Rt| and Q(bt)t occurring therein at any step t in which a restart does not occur \u2014the regret for the time steps when a restart occurs is already accounted for by the term O ( (lnK) ln(KT ) ) in (3). Now, Lemma 11 gives\n|Rt| = O ( \u03b1(Gt) lnK ) .\nIf \u03b3t = \u03b3 (bt) t for any time t when a restart does not occur, it is not hard to see that \u03b3t =\n\u2126 (\u221a (lnK)/(KT ) ) . Moreover, Lemma 13 states that\nQt = O ( \u03b1(Gt) ln(K 2/\u03b3t) + |Rt| ) = O ( \u03b1(Gt) ln(K/\u03b3t) ) .\nHence, Qt = O ( \u03b1(Gt) ln(KT ) ) .\nPutting together as in (3) gives the desired result."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We consider the partial observability model for multi-armed bandits, introduced by Mannor<lb>and Shamir [11]. Our main result is a characterization of regret in the directed observability<lb>model in terms of the dominating and independence numbers of the observability graph. We also<lb>show that in the undirected case, the learner can achieve optimal regret without even accessing<lb>the observability graph before selecting an action. Both results are shown using variants of the<lb>Exp3 algorithm operating on the observability graph in a time-efficient manner.", "creator": "LaTeX with hyperref package"}}}