{"id": "1307.4653", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2013", "title": "A New Convex Relaxation for Tensor Completion", "abstract": "We study the problem of learning a tensor from a series of linear measurements. A prominent methodology for this problem is based on a generalization of the regularization of trace standards, which has been widely used for learning low-ranking matrices up to tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. Subsequently, we describe a technique for solving the related regularization problem based on the method of inversion multipliers. Experiments with one synthetic dataset and two real datasets suggest that the proposed method significantly improves the regularization of the tensor track in terms of estimation errors, while remaining computationally comprehensible.", "histories": [["v1", "Wed, 17 Jul 2013 14:38:47 GMT  (26kb)", "http://arxiv.org/abs/1307.4653v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["bernardino romera-paredes", "massimiliano pontil"], "accepted": true, "id": "1307.4653"}, "pdf": {"name": "1307.4653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bernardino.paredes.09@ucl.ac.uk", "m.pontil@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n46 53\nv1 [\ncs .L"}, {"heading": "1 Introduction", "text": "During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein. This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others. In this paper, we propose a new method to tensor completion, which is based on a convex regularizer which encourages low rank tensors and develop an algorithm for solving the associated regularization problem.\nArguably the most widely used convex approach to tensor completion is based upon the extension of trace norm regularization [23] to that context. This involves computing the average of the trace norm of each matricization of the tensor [15]. A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball [7]. Unfortunately, the extension of this methodology to the more general\ntensor setting presents some difficulties. In particular, we shall prove in this paper that the tensor trace norm is not a tight convex relaxation of the tensor rank.\nThe above negative result stems from the fact that the spectral norm, used to compute the convex relaxation for the trace norm, is not an invariant property of the matricization of a tensor. This observation leads us to take a different route and study afresh the convex relaxation of tensor rank on the Euclidean ball. We show that this relaxation is tighter than the tensor trace norm, and we describe a technique to solve the associated regularization problem. This method builds upon the alternating direction method of multipliers and a subgradient method to compute the proximity operator of the proposed regularizer. Furthermore, we present numerical experiments on one synthetic dataset and two real-life datasets, which indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.\nThe paper is organized in the following manner. In Section 2, we describe the tensor completion framework. In Section 3, we highlight some limitations of the tensor trace norm regularizer and present an alternative convex relaxation for the tensor rank. In Section 4, we describe a method to solve the associated regularization problem. In Section 5, we report on our numerical experience with the proposed method. Finally, in Section 6, we summarize the main contributions of this paper and discuss future directions of research."}, {"heading": "2 Preliminaries", "text": "In this section, we begin by introducing some notation and then proceed to describe the learning problem. We denote by N the set of natural numbers and, for every k \u2208 N, we define [k] = {1, . . . , k}. Let N \u2208 N and let1 p1, . . . , pN \u2265 2. An N-order tensor W \u2208 Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN , is a collection of real numbers (Wi1,...,iN : in \u2208 [pn], n \u2208 [N ]). Boldface Euler scripts, e.g. W , will be used to denote tensors of order higher than two. Vectors are 1-order tensors and will be denoted by lower case letters, e.g. x or a; matrices are 2-order tensors and will be denoted by upper case letters, e.g. W . If x \u2208 Rd then for every r \u2264 s \u2264 d, we define xr:s := (xi : r \u2264 i \u2264 s). We also use the notation pmin = min{p1, . . . , pN} and pmax = max{p1, . . . , pN}. A mode-n fiber of a tensor W is a vector composed of the elements of W obtained by fixing all indices but one, corresponding to the n-th mode. This notion is a higher order analogue of columns (mode-1 fibers) and rows (mode-2 fibers) for matrices. The mode-n matricization (or unfolding) of W , denoted by W(n), is a matrix obtained by arranging the mode-n fibers of W so that each of them is a column of W(n) \u2208 Rpn\u00d7Jn , where Jn := \u220f\nk 6=n pk. Note that the order of the columns is not important as long as it is consistent.\nWe are now ready to describe the learning problem. We choose a linear operator I : Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN \u2192 Rm, representing a set of linear measurements obtained from a target tensor W0 as y = I(W0)+\u03be, where \u03be is some disturbance noise. In this paper, we mainly focus on tensor completion, in which case the operator I measures elements of the tensor. That is, we have I(W0) = (W0i1(j),...,iN (j) : j \u2208 [m]), where, for every j \u2208 [m] and n \u2208 [N ], the index in(j) is a prescribed integer in the set [pn]. Our aim\n1For simplicity we assume that pn \u2265 2 for every n \u2208 [N ], otherwise we simply reduce the order of the tensor without loss of information.\nis to recover the tensor W0 from the data (I, y). To this end, we solve the regularization problem\nmin { \u2016y \u2212 I(W)\u201622 + \u03b3R(W) : W \u2208 Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN }\n(1)\nwhere \u03b3 is a positive parameter which may be chosen by cross validation. The role of the regularizer R is to encourage tensors W which have a simple structure in the sense that they involve a small number of \u201cdegrees of freedom\u201d. A natural choice is to consider the average of the rank of the tensor\u2019s matricizations. Specifically, we consider the combinatorial regularizer\nR(W) = 1\nN\nN \u2211\nn=1\nrank(W(n)). (2)\nFinding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22]. They all agree to use the trace norm for tensors as a convex proxy of R. This is defined as the average of the trace norm of each matricization of W , that is,\n\u2016W\u2016tr = 1\nN\nN \u2211\nn=1\n\u2016W(n)\u2016tr (3)\nwhere \u2016W(n)\u2016tr is the trace (or nuclear) norm of matrix W(n), namely the \u21131-norm of the vector of singular values of matrix W(n) (see, e.g. [13]). Note that in the particular case of 2-order tensors, functions (2) and (3) coincide with the usual notion of rank and trace norm of a matrix, respectively.\nA rational behind the regularizer (3) is that the trace norm is the tightest convex lower bound to the rank of a matrix on the spectral unit ball, see [7, Thm. 1]. This lower bound is given by the convex envelope of the function\n\u03a8(W ) =\n{\nrank(W ), if \u2016W\u2016\u221e \u2264 1 +\u221e, otherwise (4)\nwhere \u2016 \u00b7 \u2016\u221e is the spectral norm, namely the largest singular value of W . The convex envelope can be derived by computing the double conjugate of \u03a8. This is defined as\n\u03a8\u2217\u2217(W ) = sup { \u3008W,S\u3009 \u2212\u03a8\u2217(W ) : S \u2208 Rp1\u00d7p2 }\n(5)\nwhere \u03a8\u2217 is the conjugate of \u03a8, namely \u03a8\u2217(S) = sup {\u3008W,S\u3009 \u2212\u03a8(W ) : W \u2208 Rp1\u00d7p2}. Note that \u03a8 is a spectral function, that is, \u03a8(W ) = \u03c8(\u03c3(W )) where \u03c8 : Rd+ \u2192 R denotes the associated symmetric gauge function. Using von Neumann\u2019s trace theorem (see e.g. [13]) it is easily seen that \u03a8\u2217(S) is also a spectral function. That is, \u03a8\u2217(S) = \u03c8\u2217(\u03c3(S)), where\n\u03c8\u2217(\u03c3) = sup { \u3008\u03c3, w\u3009 \u2212 \u03c8(w) : w \u2208 Rd+ } , with d := min(p1, p2).\nWe refer to [7] for a detailed discussion of these ideas. We will use this equivalence between spectral and gauge functions repeatedly in the paper."}, {"heading": "3 Alternative Convex Relaxation", "text": "In this section, we show that the tensor trace norm is not a tight convex relaxation of the tensor rank R in equation (2). We then propose an alternative convex relaxation for this function.\nNote that due to the composite nature of the function R, computing its convex envelope is a challenging task and one needs to resort to approximations. In [21], the authors note that the tensor trace norm \u2016 \u00b7 \u2016tr in equation (3) is a convex lower bound to R on the set\nG\u221e := { W \u2208 Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN : \u2225 \u2225W(n) \u2225 \u2225 \u221e \u2264 1, \u2200n \u2208 [N ] } .\nThe key insight behind this observation is summarized in Lemma 4, which we report in Appendix A. However, the authors of [21] leave open the question of whether the tensor trace norm is the convex envelope of R on the set G\u221e. In the following, we will prove that this question has a negative answer by showing that there exists a convex function \u2126 6= \u2016 \u00b7 \u2016tr which underestimates the function R on G\u221e and such that for some tensor W \u2208 G\u221e it holds that \u2126(W) > \u2016W\u2016tr. To describe our observation we introduce the set\nG2 := { W \u2208 Rp1\u00d7...\u00d7pN : \u2016W\u20162 \u2264 1 }\nwhere \u2016 \u00b7 \u20162 is the Euclidean norm for tensors, that is,\n\u2016W\u201622 := p1 \u2211\ni1=1\n\u00b7 \u00b7 \u00b7 pN \u2211\niN=1\n(Wi1,...,iN )2.\nWe will choose\n\u2126(W) = \u2126\u03b1(W) := 1\nN\nN \u2211\nn=1\n\u03c9\u2217\u2217\u03b1 ( \u03c3 ( W(n) ))\n(6)\nwhere \u03c9\u2217\u2217\u03b1 is the convex envelope of the cardinality of a vector on the \u21132-ball of radius \u03b1 and we will choose \u03b1 = \u221a pmin. Note, by Lemma 4 stated in Appendix A, that, for every \u03b1 > 0, function \u2126\u03b1 is a convex lower bound of function R on the set \u03b1G2. Below, for every vector s \u2208 Rd we denote by s\u2193 the vector obtained by reordering the components of s so that they are non increasing in absolute value, that is, |s\u21931| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |s\u2193d|.\nLemma 1. Let \u03c9\u2217\u2217\u03b1 be the convex envelope of the cardinality function on the \u21132-ball of radius \u03b1. Then, for every x \u2208 Rd such that \u2016x\u20162 = \u03b1, it holds that \u03c9\u2217\u2217\u03b1 (x) = card (x).\nProof. First, we note that the conjugate of the function card on the \u21132 ball of radius \u03b1 is given by the formula\n\u03c9\u2217\u03b1 (s) = sup \u2016y\u20162\u2264\u03b1 {\u3008s, y\u3009 \u2212 card (y)} = max r\u2208{0,...,d} {\u03b1\u2016s\u21931:r\u20162 \u2212 r}. (7)\nHence, by the definition of the double conjugate, we have, for every s \u2208 Rd that\n\u03c9\u2217\u2217\u03b1 (x) \u2265 \u3008s, x\u3009 \u2212 max r\u2208{0,...,d} {\u03b1\u2016s\u21931:r\u20162 \u2212 r}.\nIn particular, if s = kx for some k > 0 this inequality becomes\n\u03c9\u2217\u2217\u03b1 (x) \u2265 k\u2016x\u201622 \u2212 max r\u2208{0,...,d} (\u03b1k\u2016x\u21931:r\u20162 \u2212 r).\nIf k is large enough, the maximum is attained at r = card(x). Consequently,\n\u03c9\u2217\u2217\u03b1 (x) \u2265 k\u03b12 \u2212 k\u03b12 + card(x) = card(x). By the definition of the convex envelope, it also holds that \u03c9\u2217\u2217\u03b1 (x) \u2264 card(x). The result follows.\nThe next lemma provides, together with Lemma 1, a sufficient condition for the existence of a tensor W \u2208 G\u221e at which the proposed regularizer is strictly larger than the tensor trace norm. Lemma 2. If N \u2265 3 and p1, . . . , pN are not all equal to each other, then there exists W \u2208 Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN such that: (a) \u2016W\u20162 = \u221apmin, (b) W \u2208 G\u221e, (c) min\nn\u2208[N ] rank(W(n)) < max n\u2208[N ] rank(W(n)).\nProof. Without loss of generality we assume that p1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 pN . By hypothesis p1 < pN . First we consider the special case p1 = \u00b7 \u00b7 \u00b7 = pN\u22121, and pN = p1 + 1. (8) We define a class of tensors W by choosing a singular value decomposition for their mode-N matricization,\nWi1,i2,...,iN = pN \u2211\nk=1\n\u03c3ku k iN vki1,...,iN\u22121 (9)\nwhere \u03c31 = \u00b7 \u00b7 \u00b7 = \u03c3pN = \u221a p1/(p1 + 1), the vectors uk \u2208 RpN , \u2200k \u2208 [pN ] are orthonormal and the vectors vk \u2208 Rp1p2\u00b7\u00b7\u00b7pN\u22121, \u2200k \u2208 [pN ] are orthonormal as well. Moreover, we choose vk as\nvki1,...,iN\u22121 =\n\n\n\n1 if i1 = \u00b7 \u00b7 \u00b7 = iN\u22121 = k, k < pN 1\u221a p1\nif i2 = \u00b7 \u00b7 \u00b7 = iN\u22121 = module(i1, p1) + 1, k = pN 0 otherwise.\n(10)\nBy construction the matrix W(N) has rank equal to pN and Frobenius norm equal to \u221a p1. Thus properties (a) and (c) hold true. It remains to show that W satisfies property (b). To this end, we will show, for every n \u2208 [N ] and every x \u2208 Rpn , that\n\u2016W\u22a4(n)x\u20162 \u2264 \u2016x\u20162. The case n = N is immediate. If n = 1 we have\n\u2016W\u22a4(1)x\u201622 = \u2211\ni2,...,iN\n(\n\u2211\nk\n\u03c3k \u2211\ni1\nukiNv k i1,...,iN\u22121 xi1\n)2\n= \u2211\ni2,...,iN\n\u2211\nk,\u2113\n\u2211\ni1,j1\nxi1xj1\u03c3k\u03c3\u2113u k iN u\u2113iNv k i1,i2,...,iN\u22121 v\u2113j1,i2,...,iN\u22121\n= \u2211\nk\n\u03c32k \u2211\ni1,j1\nxi1xj1\n\n\n\u2211\ni2,...,iN\u22121\nvki1,i2,...,iN\u22121v k j1,i2,...,iN\u22121\n\n\n= \u2211\nk\n\u03c32kx 2 k + \u03c32pN p1 \u2211\nk\nx2k = \u2016x\u201622\nwhere we used \u2211 iN ukiNu \u2113 iN = \u03b4k,\u2113 in the third equality, equation (10) and a direct computation in the fourth equality, and the definition of \u03c3k in the last equality. All other cases, namely n = 2, . . . , N \u2212 1, are conceptually identical, so we only discuss the case n = 2. We have\n\u2016W\u22a4(2)x\u201622 = \u2211\ni1,i3,...,iN\n(\n\u2211\nk\n\u03c3k \u2211\ni2\nukiNv k i2,...,iN\u22121 xi2\n)2\n= \u2211\ni1,i3,...,iN\n\u2211\nk,\u2113\n\u2211\ni2,j2\nxi2xj2\u03c3k\u03c3\u2113u k iN u\u2113iNv k i1,i2,...,iN\u22121 v\u2113i1,j2,...,iN\u22121\n= \u2211\nk\n\u03c32k \u2211\ni2,j2\n(\nxi2xj2 \u2211\ni1,i3,...,iN=1\nvki1,i2,...,iN\u22121v k i1,j2,...,iN\u22121\n)\n= \u2211\nk\n\u03c32kx 2 k + \u03c32pN p1 \u2211\nk\nx2k = \u2016x\u201622\nwhere again we used \u2211 iN ukiNu \u2113 iN\n= \u03b4k,\u2113 in the third equality, equation (10) and a direct computation in the fourth equality, and the definition of \u03c3k in the last equality. Finally, if assumption (8) is not true we set Wi1,...,iN = 0 if in \u2265 p1 + 1, for some n \u2264 N \u2212 1 or iN > p1 + 1. We then proceed as in the case p1 = \u00b7 \u00b7 \u00b7 = pN\u22121 and pN = p1 + 1.\nWe are now ready to present the main result of this section.\nProposition 3. Let p1, . . . , pN \u2208 N, let \u2016 \u00b7 \u2016tr be the tensor trace norm in equation (3) and let \u2126\u03b1 be the function in equation (6) for \u03b1 = \u221a pmin. If pmin < pmax, then there are infinitely many tensors W \u2208 G\u221e such that \u2126\u03b1(W) > \u2016W\u2016tr. Moreover, for every W \u2208 G2, it holds that \u21261(W) \u2265 \u2016W\u2016tr.\nProof. By construction \u2126\u03b1(W) \u2264 R(W) for every W \u2208 \u03b1G2. Since G\u221e \u2282 \u03b1G2 then \u2126\u03b1 is a convex lower bound for the tensor rank R on the set G\u221e as well. The first claim now follows by Lemmas 1 and 2. Indeed, all tensors obtained following the process described in Lemma 2 have the property that\n\u2016W\u2016tr = 1\nN\nN \u2211\nn=1\n\u2016\u03c3(W(n))\u20161\n= 1\nN\n(\npmin(N \u2212 1) + \u221a p2min + pmin\n)\n< 1\nN (pmin(N \u2212 1) + pmin + 1) = \u2126(W) = R(W).\nFurthermore there are infinitely many such tensors which satisfy this claim since the left singular vectors can be arbitrarily chosen in equation (9). To prove the second claim, we note that since \u03c9\u2217\u22171 is the convex envelope of the cardinality card on the Euclidean unit ball, then it holds that \u03c9\u2217\u22171 (\u03c3) \u2265 \u2016\u03c3\u20161 for every vector \u03c3 such that \u2016\u03c3\u20162 \u2264 1. Consequently,\n\u21261(W) = 1\nN\nN \u2211\nn=1\n\u03c9\u2217\u22171 ( \u03c3 ( W(n) )) \u2265 1 N\nN \u2211\nn=1\n\u2016\u03c3(W(n))\u20161 = \u2016W\u2016tr.\nThe above result stems from the fact that the spectral norm is not an invariant property of the matricization of a tensor, whereas the Euclidean (Frobenius) norm is. This observation leads us to further study the function \u2126\u03b1."}, {"heading": "4 Optimization Method", "text": "In this section, we explain how to solve the regularization problem associated with the proposed regularizer (6). For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21]."}, {"heading": "4.1 Alternating Direction Method of Multipliers (ADMM)", "text": "To explain ADMM we consider a more general problem comprising both tensor trace norm regularization and the regularizer we propose,\nmin W\n{\nE (W) + \u03b3 N \u2211\nn=1\n\u03a8 ( W(n) )\n}\n(11)\nwhere E(W) is an error term such as \u2016y\u2212I(W)\u201622 and \u03a8 is a convex spectral function. It is defined, for every matrix A, as\n\u03a8(A) = \u03c8(\u03c3(A))\nwhere \u03c8 is a gauge function, namely a function which is symmetric and invariant under permutations. In particular, if \u03c8 is the \u21131 norm then problem (11) corresponds to tensor trace norm regularization, whereas if \u03c8 = \u03c9\u2217\u2217\u03b1 it implements the proposed regularizer.\nProblem (11) poses some difficulties because the terms under the summation are interdependent, that is, the different matricizations of W have the same elements rearranged in a different way. In order to overcome this difficulty, the authors of [8, 21] proposed to use ADMM as a natural way to decouple the regularization term appearing in problem (11). This strategy is based on the introduction of N auxiliary tensors, B1, . . . ,BN \u2208 Rp1\u00d7\u00b7\u00b7\u00b7\u00d7pN , so that problem (11) can be reformulated as2\nmin W ,B1,...,BN\n{\n1 \u03b3 E (W) +\nN \u2211\nn=1\n\u03a8 ( Bn(n) ) : Bn = W, n \u2208 [N ] }\n(12)\nThe corresponding augmented Lagrangian (see e.g. [3, 4]) is given by\nL (W ,B,A) = 1 \u03b3 E (W) +\nN \u2211\nn=1\n(\n\u03a8 ( Bn(n) ) \u2212 \u3008An,W \u2212Bn\u3009+ \u03b2\n2 \u2016W \u2212 Bn\u201622\n)\n, (13)\n2The somewhat cumbersome notationB n(n) denotes the mode-n matricization of tensor Bn, that is, Bn(n) = (Bn)(n).\nwhere \u3008\u00b7, \u00b7\u3009 denotes the scalar product between tensors, \u03b2 is a positive parameter and A1, . . .AN \u2208 R p1\u00d7\u00b7\u00b7\u00b7\u00d7pN are the set of Lagrange multipliers associated with the constraints in problem (12).\nADMM is based on the following iterative scheme\nW [i+1] \u2190 argmin\nW\nL ( W ,B[i],A[i] )\n(14)\nB [i+1] n \u2190 argmin\nBn\nL ( W [i+1],B,A[i] )\n(15)\nA [i+1] n \u2190 A[i]n \u2212\n( \u03b2W [i+1] \u2212 B[i+1]n ) . (16)\nStep (16) is straightforward, whereas step (14) is described in [8]. Here we focus on the step (15) since this is the only problem which involves function \u03a8. We restate it with more explanatory notations as\nargmin Bn(n)\n{\n\u03a8 ( Bn(n) ) \u2212 \u2329 An(n),W(n) \u2212Bn(n) \u232a + \u03b2\n2\n\u2225 \u2225W(n) \u2212Bn(n) \u2225 \u2225 2\n2\n}\n.\nBy completing the square in the right hand side, the solution of this problem is given by\nB\u0302n(n) = prox 1 \u03b2 \u03a8 (X) := argmin\nBn(n)\n{\n1 \u03b2 \u03a8 ( Bn(n) ) + 1 2 \u2225 \u2225Bn(n) \u2212X \u2225 \u2225 2 2\n}\nwhere X = W(n) \u2212 1\u03b2An(n). By using properties of proximity operators (see e.g. [1, Prop. 3.1]) we know that if \u03c8 is a gauge function then\nprox 1 \u03b2 \u03a8 (X) = UXdiag\n(\nprox 1 \u03b2 \u03c8 (\u03c3(X))\n)\nV \u22a4X\nwhere UX and VX are the orthogonal matrices formed by the left and right singular vectors of X , respectively.\nIf we choose \u03c8 = \u2016\u00b7\u20161 the associated proximity operator is the well-known soft thresholding operator, that is, prox 1\n\u03b2 \u2016\u00b7\u20161 (\u03c3) = v, where the vector v has components\nvi = sign (\u03c3i)\n(\n|\u03c3i| \u2212 1\n\u03b2\n)\n.\nOn the other hand, if we choose \u03c8 = \u03c9\u2217\u2217\u03b1 , we need to compute prox 1 \u03b2 \u03c9\u2217\u2217\u03b1\n. In the next section, we describe a method to accomplish this task."}, {"heading": "4.2 Computation of the Proximity Operator", "text": "In order to compute the proximity operator of the function 1 \u03b2 \u03c9\u2217\u2217\u03b1 we will use several properties of proximity calculus. First, we use the formula (see e.g. [6]) proxg\u2217 (x) = x\u2212proxg (x) for g\u2217 = 1\u03b2\u03c9\u2217\u2217\u03b1 . Next we use a property of conjugate functions from [20, 12], which states that g(\u00b7) = 1\n\u03b2 \u03c9\u2217\u03b1(\u03b2\u00b7).\nFinally, by the scaling property of proximity operators [6], we have that proxg (x) = 1 \u03b2 prox\u03b2\u03c9\u2217\u03b1 (\u03b2x).\nAlgorithm 1 Computation of prox\u03b2\u03c9\u2217\u03b1(y)\nInput: y \u2208 Rd, \u03b1, \u03b2 > 0. Output: w\u0302 \u2208 Rd. Initialization: initial step \u03c40 = 12 , initial and best found solution w\n0 = w\u0302 = PS(y) \u2208 Rd. for t = 1, 2, . . . do \u03c4 \u2190 \u03c40\u221a\nt\nFind k such that k \u2208 argmax { \u03b1\u2016wt\u221211:r \u20162 \u2212 r : 0 \u2264 r \u2264 d } w\u03031:k \u2190 wt\u221211:k \u2212 \u03c4 ( wt\u221211:k (\n1 + \u03b1\u03b2\u2016wt\u221211:k \u20162\n) \u2212 y1:k )\nw\u0303k+1:d \u2190 wt\u22121k+1:d \u2212 \u03c4 ( wt\u22121k+1:d \u2212 yk+1:d ) wt \u2190 P\u0303S (w\u0303) If h(wt) < h(w\u0302) then w\u0302 \u2190 wt If \u201cStopping Condition = True\u201d then terminate.\nend for\nIt remains to compute the proximity operator of a multiple of the function \u03c9\u2217\u03b1 in equation (7), that is, for any \u03b2 > 0, y \u2208 S, we wish to compute\nprox\u03b2\u03c9\u2217\u03b1 (y) = argmin w\n{h (w) : w \u2208 S}\nwhere we have defined S := {w \u2208 Rd : w1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 wd \u2265 0} and\nh (w) = 1\n2 \u2016w \u2212 y\u201622 + \u03b2 d max r=0 {\u03b1 \u2016w1:r\u20162 \u2212 r} .\nIn order to solve this problem we employ the projected subgradient method, see e.g. [5]. It consists in applying two steps at each iteration. First, it advances along a negative subgradient of the current solution; second, it projects the resultant point onto the feasible set S. In fact, according to [5], it is sufficient to compute an approximate projection, a step which we describe in Appendix B. To compute a subgradient of h at w, we first find any integer k such that k \u2208 dargmax r=0\n{\u03b1 \u2016w1:r\u20162 \u2212 r}. Then, we calculate a subgradient g of the function h at w by the formula\ngi =\n{ (\n1 + \u03b1\u03b2\u2016w1:k\u20162\n)\nwi \u2212 yi, if i \u2264 k, wi \u2212 yi, otherwise.\nNow we have all the ingredients to apply the projected subgradient method, which is summarized in Algorithm 1. In our implementation we stop the algorithm when an update of w\u0302 is not made for more than 103 iterations."}, {"heading": "5 Experiments", "text": "We have conducted a set of experiments to assess whether there is any advantage of using the proposed regularizer over the tensor trace norm for tensor completion. First, we have designed a synthetic\nexperiment to evaluate the performance of both approaches under controlled conditions. Then, we have tried both methods on two tensor completion real data problems. In all cases, we have used a validation procedure to tune the hyper-parameter \u03b3, present in both approaches, among the values {10j : j = \u22127,\u22126, . . . , 0}. In our proposed approach there is one further hyper-parameter, \u03b1, to be specified. It should take the value of the Frobenius norm of any matricization of the underlying tensor. Since this is unknown, we propose to use the estimate\n\u03b1\u0302 =\n\u221a \u221a \u221a\n\u221a\u2016w\u201622 + (mean(w)2 + var(w)) ( N \u220f\ni=1\npi \u2212m ) ,\nwhere m if the number of known entries and w \u2208 Rm contains their values. This estimator assumes that each value in w is sampled from N (mean(w), var(w)), where mean(w) and var(w) are the average and the variance of the elements in w."}, {"heading": "5.1 Synthetic Dataset", "text": "We have generated a 3-order tensor W0 \u2208 R40\u00d720\u00d710 by the following procedure. First we generated a tensor W with ranks (12, 6, 3) using Tucker decomposition (see e.g. [15])\nWi1,i2,i3 = 12 \u2211\nj1=1\n6 \u2211\nj2=1\n3 \u2211\nj3=1\nCj1,j2,j3M (1)i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) \u2208 [40]\u00d7 [20]\u00d7 [10]\nwhere each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1). We then created the ground truth tensor W0 by the equation\nW0i1,i2,i3 = Wi1,i2,i3 \u2212mean(W)\nstd(W) + \u03bei1,i2,i3\nwhere mean(W) and std(W) are the mean and standard deviation of the elements of W and the \u03bei1,i2,i3 are i.i.d. Gaussian random variables with zero mean and variance \u03c3\n2. We have randomly sampled 10% of the elements of the tensor to compose the training set, 45% for the validation set, and the remaining 45% for the test set. After repeating this process 20 times, we report the average results in Figure 1 (Left). Having conducted a paired t-test for each value of \u03c32, we conclude that the visible differences in the performances are highly significant, obtaining always p-values less than 0.01 for \u03c32 \u2264 10\u22122. Furthermore, we have conducted an experiment to test the running time of both approaches. We have generated tensors W0 \u2208 Rp\u00d7p\u00d7p for different values of p \u2208 {20, 40, . . . , 200}, following the same procedure as outlined above. The results are reported in Figure 1 (Right). For low values of p, the ratio between the running time of our approach and that of trace norm regularization is quite high. For example in the lowest value tried for p in this experiment, p = 20, this ratio is 22.661. However, as the volume of the tensor increases, the ratio quickly decreases. For example, for p = 200, the running time ratio is 1.9113. These outcomes are expected since when p is low, the most demanding routine in our method is the one described in Algorithm 1, where each iteration is of order O (p) and O (p2) in the\nbest and worst case, respectively. However, as p increases the singular value decomposition routine, which is common to both methods, becomes the most demanding because it has a time complexity O (p3) [9]. Therefore, we can conclude that even though our approach is slower than the trace norm based method, this difference becomes much smaller as the size of the tensor increases."}, {"heading": "5.2 School Dataset", "text": "The first real dataset we have tried is the Inner London Education Authority (ILEA) dataset3 . It is composed of examination marks ranging from 0 to 70, of 15362 students which are described by a set of attributes such as school and ethnic group. Most of these attributes are categorical, thereby we can think of exam mark prediction as a tensor completion problem where each of the modes corresponds to a categorical attribute. In particular, we have used the following attributes: school (139), gender (2), VR-band (3), ethnic (11), and year (3), leading to a 5-order tensor W \u2208 R139\u00d72\u00d73\u00d711\u00d73. We have selected randomly 5% of the instances to make the test set and another 5% of the instances for the validation set. From the remaining instances, we have randomly chosen m of them for several values of m. This procedure has been repeated 20 times and the average performance is presented in Figure 2 (Left). There is a distinguishable improvement of our approach with respect to tensor trace norm regularization. To check whether this gap is significant, we have conducted a set of paired t-tests for each value of m. In all cases we obtained a p-value below 0.01."}, {"heading": "5.3 Video Completion", "text": "In the second real-data experiment we have performed a video completion test. Any video can be treated as a 4-order tensor: \u201cwidth\u201d \u00d7 \u201cheight\u201d \u00d7 \u201cRGB\u201d \u00d7 \u201cvideo length\u201d, so we can use tensor\n3Available at http://www.bristol.ac.uk/cmm/learning/support/datasets/ilea567.zip.\ncompletion algorithms to rebuild a video from a few inputs, a procedure that can be useful for compression purposes. In our case, we have used the Ocean video, available at [16]. This video sequence can be treated as a tensor W \u2208 R160\u00d7112\u00d73\u00d732. We have randomly sampled m tensors elements as training data, 5% of them as validation data, and the remaining ones composed the test set. After repeating this procedure 10 times, we present the average results in Figure 2 (Right). The proposed approach is noticeably better than the tensor trace norm in this experiment. This apparent outcome is strongly supported by the paired t-tests which we run for each value of m, obtaining always p-values below 0.01, and for the cases m > 5\u00d7 104, we obtained p-values below 10\u22126."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a convex relaxation for the average of the rank of the matricizations of a tensor. We compared this relaxation to a commonly used convex relaxation used in the context of tensor completion, which is based on the trace norm. We proved that this second relaxation is not tight and argued that the proposed convex regularizer may be advantageous. Empirical comparisons indicate that our method consistently improves in terms of estimation error over tensor trace norm regularization, while being computationally comparable on the range of problems we considered. In the future it would be interesting to study methods to speed up the computation of the proximity operator of our regularizer and investigate its utility in tensor learning problem beyond tensor completion such as multilinear multitask learning [19]."}, {"heading": "A A Useful Lemma", "text": "Lemma 4. Let C1, . . . , CN be convex subsets of a Euclidean space and let D = \u22c2N n=1 Cn 6= \u2205. Let g : \u220fN\nn=1 Cn \u2192 R and let h : D \u2192 R be the function defined, for every x \u2208 D, as h(x) = g(x, . . . , x). Then, for every x \u2208 D, it holds that\nh\u2217\u2217(x) \u2265 g\u2217\u2217(x1, . . . , xN) \u2223 \u2223 xn=x, \u2200n\u2208[N ] .\nProof. Since the restriction of g on DN \u2286 \u220fNn=1 Cn equals to h, the convex envelope of g when evaluated on the smaller set DN cannot be larger than the convex envelope of h on D.\nUsing this result it is immediately possible to derive a convex lower bound for the function R in equation (2). Since the convex envelope of the rank function on the unit ball of the spectral norm is the trace norm, using Lemma 4 with Cn = {W : \u2016W(n)\u2016\u221e \u2264 1} and\ng(W1, . . . ,WN) = 1\nN\nN \u2211\nn=1\nrank((Wn)(n)),\nwe conclude that the convex envelope of the function R on the set G\u221e is bounded from below by 1 N \u2211N\nn=1 \u2016W(n)\u2016tr. Likewise the convex envelope of R on the set \u03b1G2 is lower bounded by the function \u2126\u03b1 in equation (6)."}, {"heading": "B Computation of an Approximated Projection", "text": "Here, we address the issue of computing an approximate Euclidean projection onto the set\nS = {v \u2208 Rd : v1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 vd \u2265 0}.\nThat is, for every v, we shall find a point P\u0303S(v) \u2208 S such that \u2225\n\u2225 \u2225 P\u0303S (v)\u2212 z\n\u2225 \u2225 \u2225\n2 \u2264 \u2016v \u2212 z\u20162 , \u2200z \u2208 S. (17)\nAs noted in [5], in order to build P\u0303S such that this property holds true, it is useful to express the set of interest as the smallest one in a series of nested sets. In our problem, we can express S as\nS = Sd \u2286 Sd\u22121 \u2286 . . . \u2286 S1,\nwhere Si := { v \u2208 Rd : v1 \u2265 v2 \u2265 . . . \u2265 vi, v \u2265 0 }\n. This property allows us to sequentially compute an approximate projection on the set S using the formula\nP\u0303S (v) = PSd ( PSd\u22121 \u00b7 \u00b7 \u00b7 (PS1 (v)) )\n(18)\nwhere, for every close convex set C, we let PC be the associated projection operator. Indeed, following [5], we can argue by induction on i that P\u0303S (v) verifies condition (17). The base case is \u2016PS1 (v)\u2212 z\u20162 = \u2016v \u2212 z\u20162, which is obvious. Now, if for a given 1 \u2264 i \u2264 d\u2212 1 it holds that\n\u2016PSi (\u00b7 \u00b7 \u00b7PS1 (v))\u2212 z\u20162 \u2264 \u2016v \u2212 z\u20162\nthen\n\u2225 \u2225PSi+1 (PSi (\u00b7 \u00b7 \u00b7PS1 (v)))\u2212 z \u2225 \u2225 2 \u2264 \u2016PSi (\u00b7 \u00b7 \u00b7PS1 (v))\u2212 z\u20162 \u2264 \u2016v \u2212 z\u20162,\nsince z is also contained in Si+1. Note that to evaluate the right hand side of equation (18) we do not require full knowledge of PSi , we only need to compute PSi+1(v) for v \u2208 Si. The next proposition describes a recursive formula to achieve this step.\nAlgorithm 2 Computing an approximated projection onto the set S = {v \u2208 Rd : v1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 vd \u2265 0}. Input: y \u2208 Rd+. Output: v \u2208 S. Initialization: v \u2190 y. for i = 1, 2, . . . , d do\nwhile vi < vi+1 do j \u2190 argmax{\u2113 : \u2113 \u2208 [i], vi = vi\u2212\u2113+1} if vi + vi+1\u2212vi j+1 then\nv1:i+1 \u2190 [ v1:i\u2212j, ( vi + vi+1\u2212vi j+1 ) 1 j+1 ]\nelse v1:i+1 \u2190 [ v1:i\u2212j , vi\u2212j1 j, vi+1 \u2212 (vi\u2212j \u2212 vi) j ]\nend if end while\nend for\nProposition 5. For any v \u2208 Si, we express its first i elements as v1:i = [ v1:i\u2212j , vi1 j ]\n, where the last j \u2208 [i] is the largest integer such that vi\u2212j+1 = vi\u2212j+2 = \u00b7 \u00b7 \u00b7 = vi. It holds that\nPSi+1(v) =\n\n  \n  \nv if vi \u2265 vi+1 [\nv1:i\u2212j, ( vi + vi+1\u2212vi j+1 ) 1 j+1, vi+2:d ] if vi < vi+1 and vi\u2212j \u2265 vi+ vi+1\u2212vij+1 PSi+1 ([ v1:i\u2212j, vi\u2212j1 j , vi+1\u2212 (vi\u2212j \u2212 vi) j, vi+2:d ]) otherwise,\nwhere 1d \u2208 Rd denotes the vector containing 1 in all its elements.\nProof. The first case is straightforward. In the following we prove the remaining two. In both cases it will be useful to recall that the projection operator PC on any convex set C is characterized as\nx = PC (y) \u21d0\u21d2 \u3008y \u2212 x, z \u2212 x\u3009 \u2264 0, \u2200z \u2208 C. (19)\nTo prove the second case, we use property (19) and apply simple algebraic transformations to obtain, for all z \u2208 Si+1, that\n\u2329 v \u2212 PSi+1 (v) , z \u2212 PSi+1 (v) \u232a = vi+1 \u2212 vi j + 1 ( jzi+1 \u2212 \u2016zi\u2212j+1:i\u20161 ) \u2264 0.\nFinally we prove the third case. We want to show that if x = PSi+1(v) then\nx = PSi+1 ([ v1:i\u2212j , vi\u2212j1 j, vi+1 \u2212 (vi\u2212j \u2212 vi) j, vi+2:d ]) .\nBy using property (19), the last equation is equivalent to the statement that if\n\u3008v \u2212 x, z \u2212 x\u3009 \u2264 0, \u2200z \u2208 Si+1 then (20) \u2329[\nv1:i\u2212j , vi\u2212j1 j, vi+1 \u2212 (vi\u2212j \u2212 vi) j, vi+2:d\n] \u2212 x, z \u2212 x \u232a \u2264 0, \u2200z \u2208 Si+1. (21)\nA way to show that it holds true is to prove that the term in the left hand side of (21) is upper bounded by the corresponding term in (20). That is, for every z \u2208 Si+1, we want to show that\n\u2329[\nv1:i\u2212j, vi\u2212j1 j , vi+1 \u2212 (vi\u2212j \u2212 vi) j, vi+2:d\n] \u2212 v, z \u2212 x \u232a \u2264 0.\nA direct computation yields the equivalent inequality\n(vi\u2212j \u2212 vi) ( jxi+1 \u2212 \u2016xi\u2212j+1:i\u20161 + \u2016zi\u2212j+1:i\u20161 \u2212 jzi+1 ) \u2264 0. (22)\nSince x = PSi+1 (v), vi\u2212j+1 = vi\u2212j+2 = \u00b7 \u00b7 \u00b7 = vi and vi+1 > vi, then xi\u2212j+1 = xi\u2212j+2 = \u00b7 \u00b7 \u00b7 = xi+1. Consequently, the left hand side of inequality (22) is equivalent to\n(vi\u2212j \u2212 vi) ( \u2016zi\u2212j+1:i\u20161 \u2212 jzi+1 ) \u2264 0.\nNote that the first factor is negative and the second is positive because z and v are in Si+1. The result follows.\nAlgorithm 2 summarizes our method to compute the approximated projection operator onto the set S, based on Proposition 5."}], "references": [{"title": "Efficient first order methods for linear composite regularizers", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "L. Shen", "Y. Xu"], "venue": "arXiv:1104.1436", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix Analysis", "author": ["R. Bhatia"], "venue": "Springer Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Prentice-Hall", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, 3(1):1\u2013122", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Subgradient methods", "author": ["S. Boyd", "L. Xiao", "A. Mutapcic"], "venue": "Stanford University", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed- Point Algorithms for Inverse Problems in Science and Engineering (H. H. Bauschke et al. Eds), pages 185\u2013212, Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "Proc. American Control Conference, Vol. 6, pages 4734\u20134739", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems, 27(2)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "3rd Edition. Johns Hopkins University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition (CVPR), pages 3386\u20133393", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Part I. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Part II. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Topics in Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge University Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiverse recommendation: ndimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "Proc. 4th ACM Conference on Recommender Systems, pages 79\u201386", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bade"], "venue": "SIAM Review, 51(3):455\u2013 500", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "Proc. 12th International Conference on Computer Vision (ICCV), pages 2114\u20132121", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient methods for minimizing composite objective functions", "author": ["Y. Nesterov"], "venue": "ECORE Discussion Paper, 2007/96", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "Journal of Machine Learning Research, 12:3413\u20133430", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "Proc. 30th International Conference on Machine Learning (ICML), pages 1444\u20131452", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimization Methods for Non-differentiable Functions", "author": ["N.Z. Shor"], "venue": "Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1985}, {"title": "R", "author": ["M. Signoretto"], "venue": "Van de Plas, B. De Moor, J.A.K. Suykens. Tensor versus matrix completion: a comparison with application to spectral data. IEEE Signal Processing Letters, 18(7):403\u2013406", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems (NIPS) 17, pages 1329\u20131336", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima", "J.S.T. Presto"], "venue": "arXiv:1010.0789", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex tensor decomposition via structured Schatten norm regularization", "author": ["R. Tomioka", "T. Suzuki"], "venue": "arXiv:1303.6370", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical performance of convex tensor decomposition", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "Advances in Neural Information Processing Systems (NIPS) 24, pages 972\u2013980", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 15, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 20, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 22, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 23, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 24, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 13, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 189, "endOffset": 192}, {"referenceID": 21, "context": "Arguably the most widely used convex approach to tensor completion is based upon the extension of trace norm regularization [23] to that context.", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "This involves computing the average of the trace norm of each matricization of the tensor [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 7, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 15, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 20, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 12, "context": "[13]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]) it is easily seen that \u03a8\u2217(S) is also a spectral function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We refer to [7] for a detailed discussion of these ideas.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].", "startOffset": 164, "endOffset": 171}, {"referenceID": 7, "context": "In order to overcome this difficulty, the authors of [8, 21] proposed to use ADMM as a natural way to decouple the regularization term appearing in problem (11).", "startOffset": 53, "endOffset": 60}, {"referenceID": 2, "context": "[3, 4]) is given by L (W ,B,A) = 1 \u03b3 E (W) + N \u2211", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4]) is given by L (W ,B,A) = 1 \u03b3 E (W) + N \u2211", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "(16) Step (16) is straightforward, whereas step (14) is described in [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "[6]) proxg\u2217 (x) = x\u2212proxg (x) for g\u2217 = 1 \u03b2\u03c9 \u03b1 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Next we use a property of conjugate functions from [20, 12], which states that g(\u00b7) = 1 \u03b2 \u03c9\u2217 \u03b1(\u03b2\u00b7).", "startOffset": 51, "endOffset": 59}, {"referenceID": 11, "context": "Next we use a property of conjugate functions from [20, 12], which states that g(\u00b7) = 1 \u03b2 \u03c9\u2217 \u03b1(\u03b2\u00b7).", "startOffset": 51, "endOffset": 59}, {"referenceID": 5, "context": "Finally, by the scaling property of proximity operators [6], we have that proxg (x) = 1 \u03b2 prox\u03b2\u03c9\u2217 \u03b1 (\u03b2x).", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In fact, according to [5], it is sufficient to compute an approximate projection, a step which we describe in Appendix B.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "[15]) Wi1,i2,i3 = 12 \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) \u2208 [40]\u00d7 [20]\u00d7 [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) \u2208 [40]\u00d7 [20]\u00d7 [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "However, as p increases the singular value decomposition routine, which is common to both methods, becomes the most demanding because it has a time complexity O (p) [9].", "startOffset": 165, "endOffset": 168}], "year": 2013, "abstractText": "We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.", "creator": "LaTeX with hyperref package"}}}