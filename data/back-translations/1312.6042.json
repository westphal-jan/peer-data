{"id": "1312.6042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Learning States Representations in POMDP", "abstract": "We propose to deal with sequential processes in which only partial observations are available by learning a latent space of representation where strategies can be precisely learned.", "histories": [["v1", "Fri, 20 Dec 2013 17:03:50 GMT  (61kb,D)", "https://arxiv.org/abs/1312.6042v1", null], ["v2", "Sat, 8 Feb 2014 10:10:53 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v2", "4 pages; added references in section 4"], ["v3", "Tue, 11 Mar 2014 14:12:36 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v3", "4 pages"], ["v4", "Tue, 17 Jun 2014 10:24:51 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v4", "4 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gabriella contardo", "ludovic denoyer", "thierry artieres", "patrick gallinari"], "accepted": false, "id": "1312.6042"}, "pdf": {"name": "1312.6042.pdf", "metadata": {"source": "META", "title": "Learning States Representations in POMDP", "authors": ["Gabriella Contardo", "Ludovic Denoyer"], "emails": ["gabriella.contardo@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lip6.fr", "patrick.gallinari@lip6.fr"], "sections": [{"heading": "1. Introduction", "text": "We consider a Markov decision process (MDP) defined by possible states s \u2208 S, possible actions a \u2208 A, transition between states P (s\u2032|s, a) and reward function r(s, a). Standard reinforcement learning (RL) approaches make the assumption that the input provided to the model (i.e. the state of the system) contains enough information to learn an optimal policy (i.e a function \u03c0(s) \u2208 A that chooses which action to take in a state in order to maximize the expected discounted reward). When facing approximated reinforcement learning problems, the input consists in a feature vector - called observation - which is assumed to fully characterize the current state of the process, thus allowing for an optimal action choice. However, this assumption is unrealistic in real-life applications where the observation is only a partial view of the current state provided by limited sensors1. For example, it is the case in visual reinforcement learning problems where the input is a camera-based picture of the environment, which can not provide second-order informations such as the moving speed of the different elements of the system. Without this information, a policy learned from such an observation will probably give low quality results.\nWe propose to address this problem by switching from\n1This more general case is also known as partially observable Markov decision processes or POMDP\nthe original observation space to a latent representation space, which we expect to be more informative, and then learn policies in this new space. The model operates in two steps: (i) First, it learns how to find good representations on a set of randomly collected trajectories. This unsupervised operation is used to learn the system only once, and may be used to tackle different tasks sharing the same dynamical process. (ii) The model then infers new representations for any new trajectory, these representations being then used for discovering an optimal policy for a particular reward function.\nOur approach is transductive in the sense that whenever a new observation occurs, the system has to recompute all the previous representations so that it best matches the whole observation sequence. Although this process could be expensive in terms of computation, we show how to perform fast Monte-Carlo simulations in the representation space resulting in a highspeed algorithm. In comparison to common representation learning algorithms which directly compute the representation given the observation, our approach does the opposite, considering that a good representation is a representation from which the observation can be computed."}, {"heading": "2. Model", "text": "Let us denote st the state of the process at step t and ot the observation corresponding to this state. An observation is a feature vector of size m: ot \u2208 Rm. Note that the learning agent only accesses the observation and does not know in which exact state the process is. The latent representation of a state st will be denoted zt \u2208 Rn. Finding an optimal policy at the representation level, \u03c0(zt), can be made using standard Reinforcement Learning techniques.\nOur model is based on two ideas: (i) The latent repre-\nar X\niv :1\n31 2.\n60 42\nv4 [\ncs .L\nG ]\n1 7\nJu n\n20 14\nsentation zt of a state st should contain enough information to compute the corresponding observation ot. We thus consider a decoder function d\u03b8 : Rn \u2192 Rm that aims at computing the observation given the representation of the current state. (ii) The representation zt of a state st should contain information about the dynamics of the system, allowing to compute the representation of the next state. This is handled through the use of a dynamical function m\u03b3 : Rn \u00d7A \u2192 Rn such that m\u03b3(zt, at) aims at computing zt+1.\nWe address now the two steps of this approach: unsupervised learning from randomly sampled trajectories which will consist in learning the decoder and the dynamical function, and inferring representations on new states which will consist in finding the latent representation sequence from a new observation sequence."}, {"heading": "2.1. Unsupervised Learning", "text": "Given a sequence of observations and actions (o1, a1, ..., ot, at), we define the following loss function:\nL(z, \u03b8, \u03b3) = \u2211 t \u2206dec(d\u03b8(zt), ot)\n+ \u2211 t \u2206dyn(m\u03b3(zt, at), zt+1) (1)\nwhere \u2206dec measures the quality of the decoder, \u2206dyn measures the quality of the dynamical model and z is the sequence of latent representations. The value of this loss function directly reflects the ability of z, \u03b8 and \u03b3 to explain the observations. Given a set of Q trajectories, learning resumes to:\nz\u2217, \u03b8\u2217, \u03b3\u2217 = argmin \u2211\nq\u2208[1;Q]\nL(zq, \u03b8, \u03b3) (2)\nwhere zq is the sequence of representations computed for trajectory number q.\nLearning produces both the optimal decoder d\u03b8\u2217 and the dynamical function m\u03b3\u2217 , together with the representation sequences corresponding to the Q trajectories."}, {"heading": "2.2. Inferring new representations", "text": "Knowing d\u03b8\u2217 and m\u03b3\u2217 , the next question is to compute representations for new trajectories. Consider that at time t, given a sequence of observations (o1, a1, ..., ot, at), the t first representations z1 to zt have already been computed. We propose two methods to compute the representation zt+1:"}, {"heading": "PO FLat 2 0.86", "text": "Exact Inference Given a new observation ot+1, the first method consists in solving:\nz\u2217 = argminz1,..,zt+1L(z, \u03b8 \u2217, \u03b3\u2217) (3)\nThis produces the optimal representation zt+1 while revising previously computed representations z1 to zt. This characteristic of our model can be intepreted as a thinking process since it means that any new information gathered by the system will make it revise the whole representation sequence. The drawback of such an inference schema is its high complexity : finding a new representation may be slow and the optimization must be performed at each step.\nFast Inference The second method consists in using the dynamical function to directly compute the next representation through zt+1 = m\u03b3\u2217(zt, at). In that case, the new representation can be produced directly from zt without requiring the observation ot+1. The advantages are twofold. First the computation of m\u03b3\u2217(zt, at) is fast, which makes this inference method particularly adapted for processes where data acquisition is slow, such as moving robots for example. Moreover, the dynamical function can be used as a learned simulator which allows one to compute Monte-Carlo simulations directly in the latent representation space, and thus to discover an optimal policy without needing to compute new real-world trajectories."}, {"heading": "3. Experiments", "text": "We present preliminary results obtained on a classical toy example of the domain: mountain car (Sutton & Barto, 1998). This problem has been studied in many different articles with many different variants. In this article, we consider that: (i) A state s = (x, x\u0307) is defined by the position of the car x and its speed x\u0307. (ii) The initial state s1 is generated by uniformly sampling x and x\u0307 and following a random policy during 5 steps.\n(iii) Each trajectory has a limited size T - in these exeriments T = 100. (iv) When the car reaches the goal, or when the maximum trajectory size is reached, the episode stops. The reward function measures the average number of sucessful trajectories. We consider two settings: The full observation (FO) setting where the entire knowledge of the current state is given to the system i.e ot = (xt, x\u0307t) and the partial observation (PO) setting where observations only contain the position of the car at time step t i.e ot = (xt).\nIn order to learn an optimal policy, we use the RCPI algorithm (Dimitrakakis & Lagoudakis, 2008) using a linear classification model with a hinge-loss. At each iteration of RCPI, we sample 1000 states and simulate the current policy using only 1 trajectory per state. The base model we are using is a combination of an L2 regularized linear decoder with a linear+hyperbolictangent dynamical model. The \u2206 losses are L1 norms since using an L2 norms gives lower performances.\nWe consider four models: The From observation model (FObs.) directly considers that the representation zt of a state st is the observation i.e zt = ot. This corresponds to the classical Approximated Reinforcement Learning context. The From latent model (FLat.) computes the zt value by minimizing the objective function for each new state. The From dynamical model (FDyn.) computes the representation of the initial state z1 by minimizing the loss, using the 5-sized trajectory that generated s1, and then use the fast inference method. At last, the partial (FPar.) model computes the representation of a state zt by randomly choosing between FDyn and FLat.\nExperimental results are illustrated in Table 1 which shows the performance - the expected reward - obtained by the policy found after 10 iterations of RCPI (averaged on 5 runs). The baseline corresponds to the first line where the full observation of the state is provided to the system (i.e speed and position). In that case, almost 95% of the trajectories are sucessful. With our method, the best performance is obtained with the FLat model in a latent space of size 5 - 91% of success. Note that using alternative inference methods (FDyn and FPar) allows one to obtain good performance. Particularly, the FDyn results show the ability of the model to directly learn from the dynamical model, without acquiring observations and thus at a very high speed. An illustration of the learned latent space is given in Figure 1. As may be seen, states corresponding to different speeds are projected in different areas of the latent space, meaning the missing information has been recovered."}, {"heading": "4. Related work", "text": "Efficient approaches have been proposed to extract high-level representations using deep-learning (Bengio, 2009) but few studies have proposed extension to deal with sequential processes. A formal analysis has been proposed in (Ryabko, 2013). Models concerning partially observable sequential processes have been proposed in the context of controlling tasks problems. For example, (Schmidhuber, 1990) and (Cuccu et al., 2011) present models using recurrent neural networks (RNN) to learn a controller for a given task. In these approaches, informative representations are constructed by the RNN, but these representations are driven by the task to solve. Some unsupervised approaches have been recently proposed. In that case, a representation learning model is learned over the observations, without needing to define a reward function. The policy is learned afterward using these representations, by usually using classical RL algorithms. For instance, (Gissle\u0301n et al., 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al., 2012) present an extension of RNN for unsupervised learning. In comparison to these models, our transductive approach is simultaenously based on unsupervised trajectories, and also allows us to choose which action to take even if observations are missing, by learning a dynamic model in the latent space."}, {"heading": "5. Conclusion", "text": "We proposed a novel approach to learn representations on sequential processes when only partial observations are given. The model is unsupervised and transductive. It can be used for both inferring new representations, but also as a simulator, to predict what can happen in the future. Experiments on more realistic\ndomains are currently under investigation."}, {"heading": "Acknowledgements", "text": "This work was performed within the Labex SMART supported by French state funds managed by the ANR within the Investissements d\u2019Avenir programme under reference ANR-11-LABX-65 and by the Lampada project ANR-09-EMER-007."}], "references": [{"title": "Intrinsically motivated neuroevolution for vision-based reinforcement learning", "author": ["Cuccu", "Giuseppe", "Luciw", "Matthew", "Schmidhuber", "J\u00fcrgen", "Gomez", "Faustino"], "venue": "In Development and Learning (ICDL),", "citeRegEx": "Cuccu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cuccu et al\\.", "year": 2011}, {"title": "Rollout sampling approximate policy iteration", "author": ["Dimitrakakis", "Christos", "Lagoudakis", "Michail G"], "venue": "Machine Learning,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2008}, {"title": "Solving partially observable reinforcement learning problems with recurrent neural networks", "author": ["Duell", "Siegmund", "Udluft", "Steffen", "Sterzing", "Volkmar"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Duell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duell et al\\.", "year": 2012}, {"title": "Sequential constant size compressors for reinforcement learning", "author": ["Gissl\u00e9n", "Linus", "Luciw", "Matt", "Graziano", "Vincent", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Gissl\u00e9n et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gissl\u00e9n et al\\.", "year": 2011}, {"title": "Unsupervised model-free representation learning", "author": ["Ryabko", "Daniil"], "venue": "arXiv preprint arXiv:1304.4806,", "citeRegEx": "Ryabko and Daniil.,? \\Q2013\\E", "shortCiteRegEx": "Ryabko and Daniil.", "year": 2013}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "For example, (Schmidhuber, 1990) and (Cuccu et al., 2011) present models using recurrent neural networks (RNN) to learn a controller for a given task.", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "For instance, (Gissl\u00e9n et al., 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 2, "context": ", 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al., 2012) present an extension of RNN for unsupervised learning.", "startOffset": 108, "endOffset": 128}], "year": 2014, "abstractText": "We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned.", "creator": "LaTeX with hyperref package"}}}