{"id": "1312.7658", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2013", "title": "Response-Based Approachability and its Application to Generalized No-Regret Algorithms", "abstract": "The approach theory introduced by Blackwell (1956) provides basic results in repeated games with vector-weighted payouts and has since been useful in the theory of learning in games and learning algorithms in the opposing online constellation. In the face of repeated play with vector payouts, a target rate of $S $is achievable for a particular player (the agent) if he can ensure that the average payout vector corresponds to that rate, regardless of what his opponent does. Blackwell provided two equivalent sets of conditions for a convex amount that must be achievable; the first (primary) condition is a geometric separation condition, while the second (dual) condition requires that the amount {\\ em is not exclusive}, namely that for each mixed action of the opponent there is a mixed action of the agent (a {em response}), so that the resulting payout vector $is conditional on a specific projection of each projector's projection based on an initial algorithm.", "histories": [["v1", "Mon, 30 Dec 2013 09:15:03 GMT  (37kb)", "http://arxiv.org/abs/1312.7658v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["andrey bernstein", "nahum shimkin"], "accepted": false, "id": "1312.7658"}, "pdf": {"name": "1312.7658.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["aberenstein@gmail.com", "shimkin@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n76 58\nv1 [\ncs .L\nG ]"}, {"heading": "1. Introduction", "text": "Consider a repeated matrix game with vector-valued rewards that is played by two players, the agent and the adversary or opponent. In a learning context the agent may represent the learning algorithm, while the adversary stands for an arbitrary or unpredictable learning environment. For each pair of simultaneous actions a and z (of the agent and the opponent, respectively) in the one-stage game, a reward vector r(a, z) \u2208 R\u2113, \u2113 \u2265 1, is obtained. In Blackwell\u2019s approachability problem (Blackwell, 1956), the agent\u2019s goal is to ensure that the long-term average reward vector approaches a given target set S, namely converges to S\nalmost surely in the point-to-set distance. If that convergence can be ensured irrespectively of the opponent\u2019s strategy, the set S is said to be approachable, and a strategy of the agent that satisfies this property is an approaching strategy (or algorithm) for S.\nBlackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment.\nStandard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set.\nThe idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies. An explicit approachability algorithm which is based on computing the response to calibrated forecasts of the opponent\u2019s actions has been proposed in Perchet (2009), and further analyzed in Bernstein et al. (2013). However, the algorithms in these papers are essentially based on the computation of calibrated forecasts of the opponent\u2019s actions, a task which is known to be computationally hard (Hazan and Kakade, 2012). In contrast, the algorithm proposed in the present paper operates strictly in the payoff space, similarly to Blackwell\u2019s approachability algorithm.\nThe main motivation for the proposed algorithm comes from certain generalizations of the basic no-regret problem, where the set to be approached is complex so that computing the projection direction may be hard, while the response map is explicit by construction. These generalizations include the constrained regret minimization problem (Mannor et al., 2009), regret minimization with global cost functions (Even-Dar et al., 2009), regret minimization in variable duration repeated games (Mannor and Shimkin, 2008), and regret\nminimization in stochastic game models (Mannor and Shimkin, 2003). In these cases, the computation of a response reduces to computing a best-response in the underlying regret minimization problem, and hence can be carried out efficiently. The application of our algorithm to some of these problems is discussed in Section 5 of this paper.\nThe paper proceeds as follows. In Section 2 we review the approachability problem and existing approachability algorithms, and illustrate the formulation of standard noregret problems as approachability problems. Section 3 presents our basic algorithm and establishes its approachability properties. In Section 4, we provide an interpretation of certain aspects of the proposed algorithm, and propose some variants and extensions to the basic algorithm. Section 5 applies the proposed algorithms to generalized no-regret problems, including constrained regret minimization and online learning with global cost functions. We conclude the paper in Section 6."}, {"heading": "2. Review of Approachability and Related No-Regret Algorithms", "text": "In this Section, we present the approachability problem and review Blackwell\u2019s approachability conditions. We further discuss existing approachability algorithms, and illustrate the application of the approachability framework to classical regret minimization problems."}, {"heading": "2.1 Approachability Theory", "text": "Consider a repeated two-person matrix game, played between an agent and an arbitrary opponent. The agent chooses its actions from a finite set A, while the opponent chooses its actions from a finite set Z. At each time instance n = 1, 2, ..., the agent selects its action an \u2208 A, observes the action zn \u2208 Z chosen by the opponent, and obtains a vector-valued reward Rn = r(an, zn) \u2208 R\u2113, where \u2113 \u2265 1, and r : A\u00d7 Z \u2192 R\u2113 is a given reward function. The average reward vector obtained by the agent up to time n is then R\u0304n = n \u22121\u2211n k=1Rk. A mixed action of the agent is a probability vector p \u2208 \u2206(A), where p(a) specifies the probability of choosing action a \u2208 A, and \u2206(A) denotes the set of probability vectors over A . Similarly, q \u2208 \u2206(Z) denotes a mixed action of the opponent. Let q\u0304n \u2208 \u2206(Z) denote the empirical distribution of the opponent\u2019s actions at time n, namely\nq\u0304n(z) , 1\nn\nn \u2211\nk=1\nI {zn = z} , z \u2208 Z,\nwhere I is indicator function. Further define the Euclidean span of the reward vector as\n\u03c1 , max a,z,a\u2032,z\u2032\n\u2225 \u2225r(a, z)\u2212 r(a\u2032, z\u2032) \u2225 \u2225 , (1)\nwhere \u2016\u00b7\u2016 is the Euclidean norm. The inner product between two vectors v \u2208 R\u2113 and w \u2208 R\u2113 is denoted by v \u00b7 w.\nIn what follows, we find it convenient to use the notation\nr(p, q) , \u2211\na\u2208A,z\u2208Z p(a)q(z)r(a, z)\nfor the expected reward under mixed actions p \u2208 \u2206(A) and q \u2208 \u2206(Z); the distinction between r(a, z) and r(p, q) should be clear from their arguments. Occasionally, we will use\nr(p, z) = \u2211 a\u2208A p(a)r(a, z) for the expected reward under mixed action p \u2208 \u2206(A) and pure action z \u2208 Z. The notation r(a, q) is to be interpreted similarly.\nLet\nhn , {a1, z1, ..., an, zn} \u2208 (A\u00d7Z)n\ndenote the history of the game up to (and including) time n. A strategy \u03c0 = (\u03c0n) of the agent is a collection of decision rules \u03c0n : (A\u00d7Z)n\u22121 \u2192 \u2206(A), n \u2265 1, where each mapping \u03c0n specifies a mixed action pn = \u03c0n(hn\u22121) for the agent at time n. The agent\u2019s pure action an is sampled from pn. Similarly, the opponent\u2019s strategy is denoted by \u03c3 = (sigman), with \u03c3n : (A\u00d7Z)n\u22121 \u2192 \u2206(Z). Let P\u03c0,\u03c3 denote the probability measure on (A\u00d7Z)\u221e induced by the strategy pair (\u03c0, \u03c3).\nLet S be a given target set. Below is the classical definition of an approachable set from Blackwell (1956).\nDefinition 1 (Approachable Set) A closed set S \u2286 R\u2113 is approachable by the agent\u2019s strategy \u03c0 if the average reward R\u0304n = n \u22121\u2211n k=1Rk converges to S in the Euclidian pointto-set distance d(\u00b7, S), almost surely for every strategy \u03c3 of the opponent, at a uniform rate over all strategies \u03c3 of the opponent. That is, for every \u01eb > 0 there is an integer N such that, for every strategy \u03c3 of the opponent,\nP \u03c0,\u03c3\n{ d ( R\u0304n, S ) \u2265 \u01eb for some n \u2265 N } < \u01eb.\nThe set S is approachable if there exists such a strategy for the agent.\nIn the sequel, we find it convenient to state most of our results in terms of the expected average reward, where expectation is applied only to the agent\u2019s mixed actions:\nr\u0304n = 1\nn\nn \u2211\nk=1\nrk , 1\nn\nn \u2211\nk=1\nr(pk, zk).\nWith this modified reward, the stated convergence results will be shown to hold pathwise, for any possible sequence of the opponent\u2019s actions. See, e.g., Theorem 6, where we show that d(r\u0304n, S) \u2264 \u03c1\u221an for all n. The corresponding almost sure convergence for the actual average reward R\u0304n can be easily deduced using martingale convergence theory. Indeed, note that\nd ( R\u0304n, S ) \u2264 \u2225 \u2225R\u0304n \u2212 r\u0304n \u2225 \u2225+ d (r\u0304n, S) .\nBut the first term is the norm of the mean of the vector martingale difference sequence Dk = r(ak, zk)\u2212 r(pk, zk). This can be easily shown to converge to zero at a uniform rate of O (1/ \u221a n), using standard results (e.g., from Shiryaev (1995)); see for instance Shimkin and Shwartz (1993), Proposition 4.1. In particular, it can be shown that there exists a finite constant K so that for each \u03b4 > 0\n\u2225 \u2225R\u0304n \u2212 r\u0304n \u2225 \u2225 \u2264 K log(1/\u03b4)\u221a n\nwith probability at least 1\u2212 \u03b4.\nNext, we present a formulation of Blackwell\u2019s results (Blackwell, 1956) which provides us with conditions for approachability of general and convex sets. To this end, for any x /\u2208 S, let c(x) \u2208 S denote a closest point in S to x. Also, for any p \u2208 \u2206(A) let T (p) , {r(p, q) : q \u2208 \u2206(Z)}, which coincides with the convex hull of the vectors {r(p, z)}z\u2208Z .\nDefinition 2\n(i) B-sets: A closed set S \u2286 R\u2113 will be called a B-set (where B stands for Blackwell) if for every x /\u2208 S there exists a mixed action p\u2217 = p\u2217(x) \u2208 \u2206(A) such that the hyperplane through y = c(x) perpendicular to the line segment xy, separates x from T (p\u2217).\n(ii) D-sets: A closed set S \u2286 R\u2113 will be called a D-set (where D stands for Dual) if for every q \u2208 \u2206(Z) there exists a mixed action p \u2208 \u2206(A) so that r(p, q) \u2208 S. We shall refer to such p as an S-response (or just response) of the agent to q.\nTheorem 3\n(i) Primal Condition and Algorithm. A B-set is approachable, by using at time n the mixed action p\u2217(r\u0304n\u22121) whenever r\u0304n\u22121 /\u2208 S. If r\u0304n\u22121 \u2208 S, an arbitrary action can be used.\n(ii) Dual Condition. A closed set S is approachable only if it is a D-set.\n(iii) Convex Sets. Let S be a closed convex set. Then, the following statements are equivalent: (a) S is approachable, (b) S is a B-set, (c) S is a D-set.\nWe note that the approachability algorithm in Theorem 3(i) is valid also if r\u0304n in the primal condition is replaced with R\u0304n. In addition, Theorem 3 has the following Corollary.\nCorollary 4 The convex hull of a D-set is approachable (and is also a B-set).\nProof The convex hull of a D-set is a convex D-set. The claim then follows by Theorem 3.\nSince Blackwell\u2019s original construction, some other approachability algorithms that are based on similar geometric ideas have been proposed in the literature. Hart and MasColell (2001) proposed a class of approachability algorithms that use a general steering direction with separation properties. As shown there, this is essentially equivalent to the computation of the projection to the target set in some norm. When Euclidean norm is used, the resulting algorithm is equivalent to Blackwell\u2019s original scheme. Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set.\nWe mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets. In Shimkin and Shwartz (1993) and Milman (2006), approachability was extended\nto stochastic (Markov) game models. An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory.\nRecently, Mannor et al. (2011) proposed a robust approachability algorithm for repeated games with partial monitoring and applied it to the corresponding regret minimization problem.\nIn all these papers, at each time step, either the computation of the projection to the target set, or that of a steering direction with separation properties is required."}, {"heading": "2.2 Approachability and No-Regret Algorithms", "text": "We next present the problem of regret minimization in repeated matrix games, and show how these problems can be formulated in terms of approachability with an appropriately defined reward vector and target set. We start with Blackwell\u2019s original formulation, and proceed to the alternative one by Hart and Mas-Colell (2001). In the final subsection, we consider briefly the more elaborate problem of internal regret minimization. We will mainly emphasize the role of the dual condition and the simple computation of the response for these problems, and refer to the respective references for details of the (primal) resulting algorithms.\nConsider, as before, the agent that faces an arbitrarily varying environment (the opponent). The repeated game model is the same as above, except that the vector reward function r is replaced by a scalar reward (or utility) function u : A \u00d7 Z \u2192 R. Let U\u0304n , n \u22121\u2211n k=1 Uk denote the average reward by time n, and let\nU\u2217n(z1, ..., zn) , 1\nn max a\u2208A\nn \u2211\nk=1\nu(a, zk)\ndenote the best reward-in-hindsight of the agent after observing z1, ..., zn. That is, U \u2217 n is the maximal average reward the agent could obtain at time n if he knew the opponent\u2019s actions beforehand and used a single fixed action. It is not hard to see that the best rewardin-hindsight can be defined as a convex function u\u2217 of the empirical distribution q\u0304n of the opponent\u2019s actions:\nU\u2217n(z1, ..., zn) = max a\u2208A u(a, q\u0304n) , u \u2217(q\u0304n). (2)\nThis motivates the definition of the average regret as (u\u2217(q\u0304n) \u2212 U\u0304n), and the following definition of a no-regret algorithm:\nDefinition 5 (No-Regret Algorithm) We say that a strategy of the agent is a no-regret algorithm (also termed a Hannan Consistent strategy) if\nlim sup n\u2192\u221e\n( u\u2217(q\u0304n)\u2212 U\u0304n ) \u2264 0,\nalmost surely, for any strategy of the opponent."}, {"heading": "2.2.1 Blackwell\u2019s Formulation", "text": "Following Hannan\u2019s seminal paper, Blackwell (1954) used approachability theory in order to elegantly show the existence of regret minimizing algorithms. Define the vector-valued\nrewards Rn , (Un,1(zn)) \u2208 R \u00d7 \u2206(Z), where 1(z) is the probability vector in \u2206(Z) supported on z. The corresponding average reward is then R\u0304n , n \u22121\u2211n k=1Rk = ( U\u0304n, q\u0304n )\n. Finally, define the target set\nS = {(u, q) \u2208 R\u00d7\u2206(Z) : u \u2265 u\u2217(q)} .\nIt is easily verified that this set is a D-set: by construction, for each q there exists an S-response p \u2208 argmaxp\u2208\u2206(Z) u(p, q) so that r(p, q) = (u(p, q), q) \u2208 S, namely u(p, q) \u2265 u\u2217(q). Also, S is a convex set by the convexity of u\u2217(q) in q. Hence, by Theorem 3, S is approachable, and by the continuity of u\u2217(q), an algorithm that approaches S also minimizes the regret in the sense of Definition 5. Application of Blackwell\u2019s approachability strategy to the set S therefore results in a no-regret algorithm. We note that the required projection of the average reward vector onto S is somewhat implicit in this formulation."}, {"heading": "2.2.2 Regret Matching", "text": "An alternative formulation, proposed in Hart and Mas-Colell (2001), leads to a a simple and explicit no-regret algorithm for this problem. Let\nLn(a \u2032) ,\n1\nn\nn \u2211\nk=1\n( u(a\u2032, zk)\u2212 u(ak, zk) )\n(3)\ndenote the regret accrued due to not using action a\u2032 constantly up to time n. The no-regret requirement in Definition 5 is then equivalent to\nlim sup n\u2192\u221e\nLn(a) \u2264 0 a \u2208 A (4)\nalmost surely, for any strategy of the opponent. In turn, this goal is equivalent to the approachability of the the non-positive orthant S = RA\u2212 in the game with vector payoff r = (ra\u2032) \u2208 RA, defined as ra\u2032(a, z) = u(a\u2032, z)\u2212 u(a, z).\nTo verify the dual condition, observe that ra\u2032(p, q) = u(a \u2032, q) \u2212 u(p, q). Choosing p \u2208 argmaxp u(p, q) clearly ensures r(p, q) \u2208 S, hence is an S-response to q (in the sense of Definition 2(ii)), and S is a D-set. Note that the response here can always be taken as a pure action.\nIt was shown in Hart and Mas-Colell (2001) that application of Blackwell\u2019s approachability strategy in this formulation leads to the so-called regret matching algorithm, where the probability of action a at time step n is given by:\npn(a) = [Ln\u22121(a)]+ \u2211\na\u2032\u2208A [Ln\u22121(a \u2032)]+\n. (5)\nHere, [xa]+ , max{xa, 0}. In fact, using their generalization of Blackwell\u2019s approachability strategies, the authors of that paper obtained a whole class of no-regret algorithms with different weighting of the components of Ln."}, {"heading": "2.2.3 Internal Regret", "text": "We close this section with another application of approachability to the stronger notion of internal regret. Given a pair of different actions a, a\u2032 \u2208 A, suppose the agent were to replace action a with a\u2032 every time a was played in the past. His reward at time k = 1, ..., n would become:\nWk(a, a \u2032) ,\n{\nu(a\u2032, zk), if ak = a,\nu(ak, zk), otherwise.\nThe internal average regret of the agent for not playing a\u2032 instead of a is then given by\nIn(a, a \u2032) ,\n1\nn\nn \u2211\nk=1\n( Wk(a, a \u2032)\u2212 Uk ) . (6)\nA no-internal-regret strategy must ensure that\nlim sup n\u2192\u221e max a,a\u2032\u2208A\nIn(a, a \u2032) \u2264 0. (7)\nTo show existence of such strategies, define the vector-valued reward function r(a, z) \u2208 R A\u00d7A by setting its (a1, a2) coordinate to\nra1,a2(a, z) ,\n{\nu(a2, z)\u2212 u(a1, z), if a = a1, 0, otherwise.\nInternal no-regret is then equivalent to approachability of the negative quadrant S0 = {r \u2264 0}. It is easy to verify that S0 is a D-set, by pointing out the response map: Given a mixed action q of the opponent, choosing a\u2217 \u2208 argmaxa\u2208A u(a, q) clearly results in r(a\u2217, q) \u2264 0. Therefore, By Theorem 3(iii), the set S0 is approachable.\nThe formulation of internal-no-regret as the approachability problem above, along with explicit approaching strategies, in due to Hart and Mas-Colell (2000). The importance of internal regret in game theory stems from the fact that if each player in a repeated N -player game uses such a no-internal regret strategy, then the empirical distribution of the players\u2019 actions convergence to the set of correlated equilibria. Some interesting relations between internal and external (Hannan\u2019s) regret are discussed in Blum and Mansour (2007)."}, {"heading": "3. Response-Based Approachability", "text": "In this section, we present our basic algorithm and establish its approachability properties.\nThroughout the paper, we consider a target set S that satisfies the following assumption.\nAssumption 1 The set S is a convex and approachable set. Hence, by Theorem 3, S is a D-set: For all q \u2208 \u2206(Z) there exists an S-response p \u2208 \u2206(A) such that r(p, q) \u2208 S.\nUnder this assumption, we may define a response map pS : \u2206(Z) \u2192 \u2206(A) that assigns to each mixed action q a response pS(q) so that r(pS(q), q) \u2208 S.\nWe note that in some cases of interest, including those discussed in Section 5, the target S may itself be defined through an appropriate response map. Suppose that for each\nq \u2208 \u2206(Z), we are given a response p\u2217(q) \u2208 \u2206(A), devised so that r(p\u2217(q), q) satisfies some desired properties. Then the set S = conv{r(p\u2217(q), q), q \u2208 \u2206(Z)} is, by construction, a convex D-set, hence approachable.\nWe next present our main results and the basic form of the related approachability algorithm. The general idea is the following. By resorting to the response map, we create a specific sequence of target points (r\u2217k) with r \u2217 k \u2208 S. Letting\nr\u0304\u2217n = 1\nn\nn \u2211\nk=1\nr\u2217k\ndenote the n-step average target point, it follows that r\u0304\u2217n \u2208 S by convexity of S. Finally, the agents actions are chosen so that the difference r\u0304\u2217n\u2212 r\u0304n converges to zero, implying that r\u0304n converges to S.\nLet \u03bbn , r\u0304 \u2217 n \u2212 r\u0304n\ndenote the difference between the average target vector and the average reward vector.\nTheorem 6 Let \u03bb0 = 0. Suppose that at each time step n \u2265 1, the agent chooses its mixed action pn (from which an is sampled) and two additional mixed actions q \u2217 n \u2208 \u2206(Z) and p\u2217n \u2208 \u2206(A) as follows:\n(i) pn and q \u2217 n are equilibrium strategies in the zero-sum game with payoff matrix defined\nby r(a, z) projected in the direction \u03bbn\u22121, namely,\npn \u2208 argmax p\u2208\u2206(A) min q\u2208\u2206(Z) \u03bbn\u22121 \u00b7 r(p, q), (8)\nq\u2217n \u2208 argmin q\u2208\u2206(Z) max p\u2208\u2206(A) \u03bbn\u22121 \u00b7 r(p, q), (9)\n(ii) p\u2217n is chosen as an S-response to q \u2217 n, so that r(p \u2217 n, q \u2217 n) \u2208 S; set r\u2217n = r(p\u2217n, q\u2217n).\nThen d (r\u0304n, S) \u2264 \u2016\u03bbn\u2016 \u2264\n\u03c1\u221a n , n \u2265 1, (10)\nfor any strategy of the opponent.\nObserve that the required choice of p\u2217n as an S-response to q \u2217 n is possible due to our standing Assumption 1. The conclusion of this theorem clearly implies that the set S is approached by the specified strategy, and provides an explicit bound on the rate of convergence. The approachability algorithm implied by Theorem 6 is summarized in Algorithm 1.\nThe computational requirements Algorithm 1 are as follows. The algorithm has two major computations at each time step n:\n1. The computation of the (pn, q \u2217 n) \u2013 the equilibrium strategies in the zero-sum matrix\ngame with the reward function \u03bbn\u22121 \u00b7 r(p, q). This boils down to the solution of the related primal and dual linear programs, and hence can be done efficiently. Note that, given the vector \u03bbn\u22121, this computation does not involve the target set S.\n2. The computation of the target point r\u2217n = r(p \u2217 n, q \u2217 n), which is problem dependent. For\nexample, in the constrained regret minimization problem this reduces to the computation of a best-response action to q\u2217n. This problem is further discussed in Section 5.\nThe proof of the last Theorem follows from the next result, which also provides less specific conditions on the required choice of (pn, q \u2217 n).\nProposition 7\n(i) Suppose that at each time step n \u2265 1, the agent chooses the triple (pn, q\u2217n, p\u2217n) so that\n\u03bbn\u22121 \u00b7 (r(pn, z)\u2212 r(p\u2217n, q\u2217n)) \u2265 0, \u2200z \u2208 Z, (11)\nand sets r\u2217n = r(p \u2217 n, q \u2217 n). Then it holds that\n\u2016\u03bbn\u2016 \u2264 \u03c1\u221a n \u2200n \u2265 1.\n(ii) If, in addition, p\u2217n is chosen as an S-response to q \u2217 n, so that r \u2217 n = r(p \u2217 n, q \u2217 n) \u2208 S, then\nd (r\u0304n, S) \u2264 \u2016\u03bbn\u2016 \u2264 \u03c1\u221a n , n \u2265 1, (12)\nThe specific choice of (pn, q \u2217 n) in equations (8)-(9) satisfies the requirement in (11), as argued below. Indeed, the latter requirement is less restrictive, and can replace (8)-(9) in the definition of the basic algorithm. However, the former choice is convenient as it ensures that (11) holds for any choice of p\u2217n.\nWe proceed to the proof of Proposition 7 and Theorem 6. We first establish a useful recursive relation for \u2016\u03bbn\u20162.\nLemma 8 For any n \u2265 1, we have that\nn2 \u2016\u03bbn\u20162 \u2264 (n \u2212 1)2 \u2016\u03bbn\u22121\u20162 + 2(n\u2212 1)\u03bbn\u22121 \u00b7 (r\u2217n \u2212 rn) + \u03c12.\nwhere \u03c1 is the span of the reward function (1).\nProof We have that\n\u2016r\u0304\u2217n \u2212 r\u0304n\u20162 = \u2225 \u2225 \u2225\n\u2225 n\u2212 1 n ( r\u0304\u2217n\u22121 \u2212 r\u0304n\u22121 ) + 1 n (r\u2217n \u2212 rn)\n\u2225 \u2225 \u2225 \u2225 2\n=\n(\nn\u2212 1 n )2 \u2225 \u2225r\u0304\u2217n\u22121 \u2212 r\u0304n\u22121 \u2225 \u2225 2 + 1 n2 \u2016r\u2217n \u2212 rn\u20162\n+2 n\u2212 1 n2 ( r\u0304\u2217n\u22121 \u2212 r\u0304n\u22121 ) \u00b7 (r\u2217n \u2212 rn)\n\u2264 ( n\u2212 1 n )2 \u2225 \u2225r\u0304\u2217n\u22121 \u2212 r\u0304n\u22121 \u2225 \u2225 2 + \u03c12 n2\n+2 n\u2212 1 n2 ( r\u0304\u2217n\u22121 \u2212 r\u0304n\u22121 ) \u00b7 (r\u2217n \u2212 rn) ,\nAlgorithm 1 Response-Based Approachability Initialization: At time step n = 1, use arbitrary mixed action p1 and set an arbitrary target point r\u22171 \u2208 S. At time step n = 2, 3, ...:\n1. Set an approachability direction\n\u03bbn\u22121 = r\u0304 \u2217 n\u22121 \u2212 r\u0304n\u22121,\nwhere\nr\u0304n\u22121 = 1\nn\u2212 1\nn\u22121 \u2211\nk=1\nr(pk, zk), r\u0304 \u2217 n\u22121 =\n1\nn\u2212 1\nn\u22121 \u2211\nk=1\nr\u2217k\nare, respectively, the average (smoothed) reward vector and the average target point.\n2. Solve a zero-sum matrix game with the scalar reward function \u03bbn\u22121 \u00b7 r(p, q). In particular, find the optimal mixed action pn and q \u2217 n that satisfy\npn \u2208 argmax p\u2208\u2206(A) min q\u2208\u2206(Z) \u03bbn\u22121 \u00b7 r(p, q),\nq\u2217n \u2208 argmin q\u2208\u2206(Z) max p\u2208\u2206(A) \u03bbn\u22121 \u00b7 r(p, q).\n3. Choose action an according to pn.\n4. Pick p\u2217n so that r(p \u2217 n, q \u2217 n) \u2208 S, and set the target point\nr\u2217n = r(p \u2217 n, q \u2217 n).\nwhere \u03c1 is the reward bound defined in (1). The proof is concluded by multiplying both sides of the inequality by n2.\nProof [Proof of Proposition 7.] Under condition (11), we have for all n that\n\u03bbn\u22121 \u00b7 (r\u2217n \u2212 rn) = \u03bbn\u22121 \u00b7 (r(p\u2217n, q\u2217n)\u2212 r(pn, zn)) \u2264 0.\nHence, by Lemma 8,\nn2 \u2016\u03bbn\u20162 \u2264 (n\u2212 1)2 \u2016\u03bbn\u22121\u20162 + \u03c12, n \u2265 1.\nApplying this inequality recursively, we obtain that\nn2 \u2016\u03bbn\u20162 \u2264 n\u03c12, n \u2265 1\nor\n\u2016\u03bbn\u20162 \u2264 \u03c12/n, n \u2265 1,\nas claimed in part (i). Part (ii) now follows since r\u2217n \u2208 S (for all n) implies that r\u0304\u2217n \u2208 S (recall that S is a convex set), hence\nd (r\u0304n, S) \u2264 \u2016r\u0304n \u2212 r\u0304\u2217n\u2016 = \u2016\u03bbn\u2016 .\nProof [Proof of Theorem 6.] It only remains to show that the choice of (pn, q \u2217 n) in equations (8)-(9) implies the required inequality in (11). Indeed, under (8) and (9) we have that\n\u03bbn\u22121 \u00b7 r(pn, zn) \u2265 max p\u2208\u2206(A) min q\u2208\u2206(Z) \u03bbn\u22121 \u00b7 r(p, q)\n= min q\u2208\u2206(Z) max p\u2208\u2206(A)\n\u03bbn\u22121 \u00b7 r(p, q)\n, max p\u2208\u2206(A)\n\u03bbn\u22121 \u00b7 r(p, q\u2217n)\n\u2265 \u03bbn\u22121 \u00b7 r(p\u2217n, q\u2217n),\nwhere the equality follows by the minimax theorem for matrix games. Therefore, condition (11) is satisfied for any p\u2217n, and in particular for the one satisfying r(p \u2217 n, q \u2217 n) \u2208 S. This concludes the proof of the Theorem."}, {"heading": "4. Interpretation and Extensions", "text": "We open this section with an illuminating interpretation of the proposed algorithm in terms of a certain approachability problem in an auxiliary game, and proceed to present several variants and extensions to the basic algorithm. While each of these variants is presented separately, they may also be combined when appropriate."}, {"heading": "4.1 An Auxiliary Game Interpretation", "text": "A central part of Algorithm 1 is the choice of the pair (pn, q \u2217 n) so that r\u0304n tracks r\u0304 \u2217 n, namely \u03bbn = r\u0304 \u2217 n \u2212 r\u0304n \u2192 0 (see Equations (8)-(9) and Proposition 7). If fact, the choice of (pn, q\u2217n) in (8)-(9) can be interpreted as Blackwell\u2019s strategy for a specific approachability problem in an auxiliary game, which we define next.\nSuppose that at time n, the agent chooses a pair of actions (a, z\u2217) \u2208 A \u00d7 Z and the opponent chooses a pair of actions (a\u2217, z) \u2208 A\u00d7Z. The vector payoff function, now denoted by v, is given by v((a, z\u2217), (a\u2217, z)) = r(a\u2217, z\u2217)\u2212 r(a, z), so that\nVn = r(a \u2217 n, z \u2217 n)\u2212Rn.\nConsider the single-point target set S0 = {0} \u2282 R\u2113. This set is clearly convex, and we next show that it is a D-set in the auxiliary game. We need to show that for any \u03b7 \u2208 \u2206(A\u00d7Z) there exists \u00b5 \u2208 \u2206(A\u00d7Z) so that v(\u00b5, \u03b7) \u2208 S0, namely v(\u00b5, \u03b7) = 0. That that end, observe that\nv(\u00b5, \u03b7) = r(p\u2217, q\u2217)\u2212 r(p, q)\nwhere p and q\u2217 are the marginal distributions of \u00b5 on A and Z, respectively, while p\u2217 and q are the respective marginal distributions of \u03b7. Therefore we obtain v(\u00b5, \u03b7) = 0 by choosing \u00b5 with the same marginals as \u03b7, for example {\u00b5(a, z) = p(a)q\u2217(z)} with p = p\u2217 and q\u2217 = q. Thus, by Theorem 3, S0 is approachable.\nWe may now apply Blackwell\u2019s approachability strategy to this auxiliary game. Since S0 is the origin, the direction from S0 to the average reward V\u0304n\u22121 is just the average reward vector itself. Therefore, the primal (geometric separation) condition here is equivalent to\nV\u0304n\u22121 \u00b7 v(\u00b5, \u03b7) \u2264 0, \u2200 \u03b7 \u2208 \u2206(A\u00d7Z)\nor\nV\u0304n\u22121 \u00b7 (r(p\u2217, q\u2217)\u2212 r(p, q)) \u2264 0, \u2200 p\u2217 \u2208 \u2206(A), q \u2208 \u2206(Z).\nNow, a pair (p, q\u2217) that satisfies this inequality is any pair of equilibrium strategies in the zero-sum game with reward v projected in the direction of V\u0304n\u22121. That is, for\np \u2208 argmax p\u2208\u2206(A) min q\u2208\u2206(Z) V\u0304n\u22121 \u00b7 r(p, q), (13)\nq\u2217 \u2208 argmin q\u2208\u2206(Z) max p\u2208\u2206(A) V\u0304n\u22121 \u00b7 r(p, q), (14)\nit is easily verified that\nV\u0304n\u22121 \u00b7 r(p\u2217, q\u2217) \u2265 V\u0304n\u22121 \u00b7 r(p, q), \u2200 p\u2217 \u2208 \u2206(A), q \u2208 \u2206(Z)\nas required.\nThe choice of (pn, q \u2217 n) in Equations (8)-(9) follows (13)-(14), with \u03bbn\u22121 replacing V\u0304n\u22121.\nWe note that the two are not identical, as V\u0304n is the temporal average of Vn = r(a \u2217 n, z \u2217 n) \u2212 r(an, zn) while \u03bbn is the average the smoothed difference r(p \u2217 n, q \u2217 n)\u2212 r(pn, zn); however this does not change the approachability result above, and in fact either can be used. More generally, any approachability algorithm in the auxiliary game can be used to choose the pair (pn, q \u2217 n) in Algorithm 1.\nWe note that in our original problem, the mixed action q\u2217n is not chosen by an \u201copponent\u201d but rather specified as part of Algorithm 1. But since the approachability result above holds for an arbitrary choice of q\u2217n, it also holds for this particular one.\nWe proceed to present some additional variants of our algorithm."}, {"heading": "4.2 Idling when Inside S", "text": "Recall that in the original approachability algorithm of Blackwell, an arbitrary action an can be chosen by the agent whenever r\u0304n\u22121 \u2208 S. This may reduce the computational burden of the algorithm, and adds another degree of freedom that may be used to optimize other criteria.\nSuch arbitrary choice of an (or pn) when the average reward is in S is also possible in our algorithm. However, some care is required in the setting of the average target point r\u0304\u2217n over these time instances, as otherwise the two terms of the difference \u03bbn = r\u0304 \u2217 n \u2212 r\u0304n may drift apart. As it turns out, what is required is simply to shift the average target point r\u0304\u2217n\nto r\u0304n at these time instances, and use the modified point in the computation of the steering direction \u03bbn. In recursive form, we obtain the following modified recursion:\n\u03bb\u03030 = 0,\n\u03bb\u0303n =\n{\nn\u22121 n \u03bb\u0303n\u22121 + 1 n(r \u2217 n \u2212 rn), if r\u0304n /\u2208 S\n0, if r\u0304n \u2208 S, n \u2265 1. (15)\nIt may be seen that the steering direction \u03bb\u0303n is reset to 0 whenever the average reward is in S. With this modified definition, we are able to maintain the same convergence properties of the algorithm.\nProposition 9 Let Assumption 1 hold. Suppose that the agent uses Algorithm 1 with the following modifications:\n1. The steering direction \u03bbn\u22121 is replaced by the modifed direction \u03bb\u0303n\u22121 defined recursively in (15);\n2. Whenever r\u0304n\u22121 \u2208 S, an arbitrary action an is chosen. Then, it holds that\nd (r\u0304n, S) \u2264 \u03c1\u221a n , n \u2265 1,\nfor any strategy of the opponent.\nProof We establish the claim in two steps. We first show that \u2016\u03bb\u0303n\u2016 bounds the Euclidean distance of r\u0304n from S. We then show that \u2016\u03bb\u0303n\u2016 satisfies an analogue of Lemma 8, and therefore the analysis of the previous section holds.\nTo see that d(r\u0304n, S) \u2264 \u2016\u03bb\u0303n\u2016 for all n, observe that if r\u0304n \u2208 S, then trivially d(r\u0304n, S) = \u2016\u03bb\u0303n\u2016 = 0. Assume next that r\u0304n /\u2208 S. Let n0 < n be the last instant n such that r\u0304n0 \u2208 S. Using the abbreviate notation\nr\u0304m:n = 1\nn\u2212m+ 1\nn \u2211\nk=m\nrk ,\nand similarly for r\u0304\u2217m:n, we obtain\n\u03bb\u0303n = 1\nn\n\n\nn \u2211\nk=n0+1\nr\u2217k \u2212 n \u2211\nk=n0+1\nrk\n\n\n= n\u2212 n0\nn\n( r\u0304n0+1:n \u2212 r\u0304\u2217n0+1:n ) .\nOn the other hand,\nd(r\u0304n, S) = d( n0 n r\u0304n0 + n\u2212 n0 n r\u0304n0+1:n, S)\n\u2264 n0 n d(r\u0304n0 , S) + n\u2212 n0 n d (r\u0304n0+1:n, S) \u2264 0 + n\u2212 n0 n \u2225 \u2225r\u0304n0+1:n \u2212 r\u0304\u2217n0+1:n \u2225 \u2225 = \u2225 \u2225\n\u2225\u03bb\u0303n\n\u2225 \u2225 \u2225 ,\nwhere the first inequality follows by the convexity of the point-to-set Euclidean distance to a convex set, and the second inequality holds since\nr\u0304\u2217n0+1:n = 1\nn\u2212 n0\nn \u2211\nk=n0+1\nr\u2217k \u2208 S.\nFor the second step, note that the recursive definition (15) of \u03bb\u0303n implies, similarly to the proof of Lemma 8, that\nn2 \u2225 \u2225\n\u2225\u03bb\u0303n\n\u2225 \u2225 \u2225 2 \u2264 (n \u2212 1)2 \u2225 \u2225 \u2225\u03bb\u0303n\u22121 \u2225 \u2225 \u2225 2 + 2(n\u2212 1)\u03bb\u0303n\u22121 \u00b7 (r\u2217n \u2212 rn) + \u03c12.\nHence, when r\u0304n\u22121 \u2208 S, we have that \u03bb\u0303n\u22121 = 0, and arbitrary an and r\u2217n can be chosen. Also, similarly to the analysis in Section 3, whenever r\u0304n\u22121 /\u2208 S, the solution (pn, q\u2217n) of the zero-sum game in the direction \u03bb\u0303n\u22121 ensures that\n\u03bb\u0303n\u22121 \u00b7 (r(p\u2217n, q\u2217n)\u2212 r(pn, zn)) \u2264 0,\nand thus the convergence of \u2225 \u2225\n\u2225\u03bb\u0303n\n\u2225 \u2225 \u2225 to zero is implied."}, {"heading": "4.3 Directionally Unbounded Target Sets", "text": "In some applications of interest, the target set S may be unbounded in certain directions. Indeed, this is the case in the approachability formulation of the no-regret problem, where the goal is essentially to make the average reward as large as possible. In particular, in Blackwell\u2019s formulation (Section 2.2.1), the set S = {(u, q) : u \u2265 u\u2217(q)} is unbounded in the direction of the first coordinate u. In Hart and Mas-Collel\u2019s formulation (Section 2.2.2), the set S = {L \u2264 0} is unbounded in the negative direction of all the coordinates of L.\nIn such cases, the requirement that \u03bbn = r\u0304 \u2217 n \u2212 r\u0304n \u2192 0, which is a property of our basic algorithm, may be too strong, and may even be counter-productive. For example, in Blackwell\u2019s no-regret formulation mentioned above, we would like to increase the first coordinate of r\u0304n as much as possible, hence allowing negative values of \u03bbn makes sense (rather than steering that coordinate to 0 by reducing r\u0304n). We propose next a modification of our algorithm that addresses this issue.\nGiven the (closed and convex) target set S \u2282 R\u2113, let DS be the set of vectors d \u2208 R\u2113 such that d + S \u2282 S. It may be seen that DS is a closed and convex cone, which trivially equals {0} if (and only if) S is bounded. We refer to the unit vectors in DS as directions in which S is unbounded.\nReferring to the auxiliary game interpretation of our algorithm in Section 4.1, we may now relax the requirement that \u03bbn approaches {0} to the requirement that \u03bbn approaches \u2212DS . Indeed, if we maintain r\u0304\u2217n \u2208 S as before, then \u03bbn \u2208 \u2212DS suffices to verify that r\u0304n = r\u0304 \u2217 n \u2212 \u03bbn \u2208 S.\nWe may now apply Blackwell\u2019s approachability strategy to the cone DS in place of the origin. The required modification to the algorithm is simple: replace the steering direction \u03bbn in (8)-(9) or (11) with the direction from the closest point in \u2212DS to \u03bbn:\n\u03bb\u0303n = \u03bbn \u2212 Proj\u2212DS(\u03bbn)\nThat projection is particularly simple in case S is unbounded along primary coordinates, so that the cone DS is a quadrant, generated by a collection ej , j \u2208 J of orthogonal unit vectors. In that case, clearly,\nProj\u2212DS (\u03bb) = \u2212 \u2211 j\u2208J (ej \u00b7 \u03bb)\u2212 .\nThus, the negative components of \u03bbn in directions (ej) are nullified.\nThe modified algorithm admits analogous bounds to those of the basic algorithm, with (10) or (12) replaced by\nd (r\u0304n, S) \u2264 d(\u03bbn,\u2212DS) \u2264 \u03c1\u221a n , n \u2265 1.\nThe proof is similar and will thus be omitted."}, {"heading": "4.4 Using the Non-smoothed Rewards", "text": "In the basic algorithm of Section 3, the definition of the steering direction \u03bbn employs the smoothed rewards r(pk, zk) rather than the actual ones, namely Rk = r(ak, zk). We consider here the case where the latter are used. This is essential in case that the opponent\u2019s action zk is not observed, so that r(pk, zk) cannot be computed, but rather the reward vector Rk is observed directly. It also makes sense since the quantity we are actually interested in is the average reward R\u0304n, and not its smoothed version r\u0304n.\nThus, we replaced \u03bbn\u22121 with\n\u03bb\u0303n\u22121 = r\u0304 \u2217 n\u22121 \u2212 R\u0304n\u22121.\nThe rest of the algorithm is the same as Algorithm 1. We have the following result for this variant.\nTheorem 10 Suppose that Assumption 1 holds. Then, if the agent uses Algorithm 1, with \u03bbn\u22121 replaced by\n\u03bb\u0303n\u22121 = r\u0304 \u2217 n\u22121 \u2212 R\u0304n\u22121.\nit holds that\nlim n\u2192\u221e\n\u2016\u03bb\u0303n\u2016 = 0,\nalmost surely, for any strategy of the opponent, at a uniform rate of O(1/ \u221a n) over all strategies of the opponent. More precisely, for every \u03b4 > 0, we have that\nP\n{\nmax k\u2265n\n\u2016\u03bb\u0303k\u2016 \u2264 \u221a 6\u03c12\n\u03b4n\n}\n\u2265 1\u2212 \u03b4. (16)\nProof First observe that Lemma 8 still holds if rn = r(pn, zn) is replaced with Rn = r(an, zn) throughout. Namely,\nn2\u2016\u03bb\u0303n\u20162 \u2264 (n\u2212 1)2\u2016\u03bb\u0303n\u22121\u20162 + 2(n\u2212 1)\u03bb\u0303n\u22121 \u00b7 (r\u2217n \u2212 r(an, zn)) + \u03c12, n \u2265 1.\nLet {Fn} denote the filtration induced by the history. We have that\nE\n[ n2\u2016\u03bb\u0303n\u20162 \u2223 \u2223 \u2223 Fn\u22121 ] \u2264 (n\u2212 1)2\u2016\u03bb\u0303n\u22121\u20162 + 2(n \u2212 1)\u03bb\u0303n\u22121 \u00b7 E [(r\u2217n \u2212 r(an, zn)) | Fn\u22121] + \u03c12\n= (n\u2212 1)2\u2016\u03bb\u0303n\u22121\u20162 + 2(n \u2212 1)\u03bb\u0303n\u22121 \u00b7 (r\u2217n \u2212 E [r(an, zn) | Fn\u22121]) + \u03c12 \u2264 (n\u2212 1)2\u2016\u03bb\u0303n\u22121\u20162 + \u03c12, (17)\nwhere the equality follows since q\u2217n and p \u2217 n are determined by the history up to time n \u2212 1 and hence so does r\u2217n = r(p \u2217 n, q \u2217 n), and the last inequality holds since\n\u03bb\u0303n\u22121 \u00b7 (r\u2217n \u2212 E [r(an, zn) | Fn\u22121]) \u2264 0,\nsimilarly to the proof of Theorem 6. Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.1 in Shimkin and Shwartz (1993) to deduce that a sequence \u2016\u03bb\u0303n\u2016 satisfying (17) converges to zero almost surely at a uniform rate that depends only on \u03c1. In particular, using the proof of Proposition 4.1 in Shimkin and Shwartz (1993) we know that for every \u01eb > 0 and \u03b4 > 0, there exists N = N(\u01eb, \u03b4, \u03c1) so that\nP\n{ \u2203n \u2265 N : \u2016\u03bb\u0303n\u2016 \u2265 \u01eb } \u2264 \u03b4,\nwhere N can be chosen to be any constant greater than 6\u03c1 2\n\u03b4\u01eb2 . This completes the proof of the Theorem."}, {"heading": "5. Generalized No-Regret Algorithms", "text": "The proposed approachability algorithms can be usefully applied to several generalized regret minimization problems, in which the computation of a projection onto the target set is involved, but a response is readily obtainable. We start by providing a generic description of the problem using a general set-valued goal function, and then specialize the discussion to some specific goal functions that have been considered in the recent literature. We do not consider convergence rates in these section, but rather focus on asymptotic convergence results. Convergence rates can readily be derived by referring to our bounds in the previous sections; see, e.g., (10) or (16).\nConsider a repeated matrix game as before, where the vector-valued reward r(a, z) is replaced with v(a, z) \u2208 RK . Suppose that for each mixed action q of the opponent, the agent has a satisficing1 payoff set V \u2217(q) \u2282 RK , and at least one mixed action p = p\u2217(q) that satisfies v(p, q) \u2208 V \u2217(q). We refer to any such action as a response of the agent to q. Let V \u2217 : q \u2208 \u2206(Z) 7\u2192 V \u2217(q) denote the corresponding set-valued goal function. As before, let Vn = v(an, zn) and V\u0304n = 1 n \u2211n k=1 Vk. A generalized no-regret strategy for this model may be defined as strategy for the agent that ensures\nlim n\u2192\u221e\nd(V\u0304n, V \u2217(q\u0304n)) = 0 (a. s.)\n1. Borrowing from H. Simon\u2019s terminology for achieving satisfying (or good-enough) results in decision making.\nfor any strategy of the opponent. If such a strategy exists, we say that the goal function V \u2217 is attainable by the agent.\nThe classical no-regret problem is obtained as a special case, with scalar reward v(a, z) = u(a, z) and satisficing payoff set V \u2217(q) = {v \u2208 R : v \u2265 v\u2217(q)}, where v\u2217(q) , maxp v(p, q). As shown in Section 2.2.1, this problem can be formulated as a particular case of approachability to the set S = {(v, q) : v \u2208 V \u2217(q)}, and existence of approaching strategies relies on convexity of the function v\u2217(q), which implies convexity of S.\nA similar line of reasoning may be pursued for the generalized no-regret problem described above. The no-regret property is clearly equivalent to approachability of the set S = {(v, q) : v \u2208 V \u2217(q)}, in the game with reward vector r(p, q) = (v(p, q), q). As convexity of S, hence of V \u2217, plays an important role, we recall the following definition for set-valued functions.\nDefinition 11 (Convex hull) A set valued function V : q \u2208 \u2206(Z) 7\u2192 V (q) \u2282 RK is convex if \u03b1V (q1) + (1\u2212 \u03b1)V (q2) \u2282 V (\u03b1q1 + (1\u2212 \u03b1)q2) for any q1, q2 and \u03b1 \u2208 [0, 1] (where the first plus sign stands for the Minkowski sum). The convex hull V c of V is the minimal set-valued function which is convex and contains V , in the sense that V (q) \u2282 V c(q) for each q. Note that a minimal member (in the sense of set inclusion) exists, as the required property is invariant under intersections.\nThe following claims follow easily from the definition of S.\nProposition 12\n(i) The set S = {(v, q) : v \u2208 V \u2217(q)} is a D-set. Hence, its convex hull conv(S) is approachable.\n(ii) If the set-valued goal function V \u2217 is convex, then it is attainable by the agent. In general, the convex hull V c of V \u2217 is attainable by the agent.\nProof To see that S is a D-set, note that by its definition, for any q there exists p such that v(p, q) \u2208 V \u2217(q), hence (v(p, q), q) \u2208 S. Therefore conv(S) is a convex D-set, which is approachable by Theorem 3. Claim (ii) now follows by verifying that conv(S) = {(v, q) : v \u2208 V c(q)}.\nIt follows that any convex goal function V \u2217 is attainable. When V \u2217 is not convex, which is often the case, one may need to resort to a relaxed goal function, namely the convex hull V c. The computation of V c and its suitability as a (relaxed) goal function need to be examined for each specific problem.\nAs a consequence of Proposition 12, V c (or V \u2217 itself when convex) can be attained by any approachability algorithm applied to the convex set conv(S) = {(v, q) : v \u2208 V c(q)}. However, the required projection unto that set may be hard to compute. This is especially true when V \u2217 is non-convex, as V c my be hard to compute explicitly. In such cases, the Response-Based Approachability algorithm developed in this paper offers a convenient alternative, as it only requires to compute at each stage a response of the agent to a certain mixed action of the opponent, relative to the original goal function V \u2217. As seen below, this\ncomputation typically requires the solution of an optimization problem, which is inherent in the definition of V \u2217.\nWe next specialize the discussion to certain concrete models of interest.\nAlgorithm 2 Generalized No-Regret Algorithm Input: Desired reward sets, represented by the multifunction V \u2217 : \u2206(Z) \u2192 RK . Initialization: At time step n = 1, use arbitrary mixed action p1, and set arbitrary values v\u22171 \u2208 RK , q\u22171 \u2208 \u2206(Z). At time step n = 2, 3, ...:\n1. Set \u03bbv,n\u22121 = v\u0304 \u2217 n\u22121 \u2212 v\u0304n\u22121, \u03bbq,n\u22121 = q\u0304\u2217n\u22121 \u2212 q\u0304n\u22121,\nwhere\nv\u0304\u2217n\u22121 = 1\nn\u2212 1\nn\u22121 \u2211\nk=1\nv\u2217k, q\u0304 \u2217 n\u22121 =\n1\nn\u2212 1\nn\u22121 \u2211\nk=1\nq\u2217k.\n2. Solve the following zero-sum matrix game:\npn \u2208 argmax p\u2208\u2206(A) min q\u2208\u2206(Z) (\u03bbv,n\u22121 \u00b7 v(p, q) + \u03bbq,n\u22121 \u00b7 q) ,\nq\u2217n \u2208 argmin q\u2208\u2206(Z) max p\u2208\u2206(A) (\u03bbv,n\u22121 \u00b7 v(p, q) + \u03bbq,n\u22121 \u00b7 q) .\n3. Choose action an according to pn.\n4. Pick p\u2217n such that v (p \u2217 n, q \u2217 n) \u2208 V \u2217(q\u2217n), and set\nv\u2217n = v(p \u2217 n, q \u2217 n)."}, {"heading": "5.1 Global Cost Functions", "text": "The following problem of regret minimization with global cost functions was introduced in Even-Dar et al. (2009). Suppose that the goal of the agent is to minimize a general (i.e., non-linear) function of the average reward vector V\u0304n. In particular, we are given a continuous function G : RK \u2192 R, and the goal is to minimize G(V\u0304n). For example, G may be some norm of V\u0304n. We define the best-cost-in-hindsight, given a mixed action q of the opponent, as\nG\u2217(q) , min p\u2208\u2206(A) G(v(p, q)),\nso that the satisficing payoff set may be defined as\nV \u2217(q) = {v \u2208 V0 : G(v) \u2264 G\u2217(q)} ,\nwhere V0 = conv{v(a, z)}a\u2208A,z\u2208Z is the set of feasible reward vectors. Clearly, the agent\u2019s response to q is any mixed action that minimizes G(v(p, q)), namely\np\u2217(q) \u2208 argmin p\u2208\u2206(A) G(v(p, q)). (18)\nBy Proposition 12, the convex hull V c of V \u2217 is attainable by the agent. The relation between V c and V \u2217 can be seen depend on convexity properties of G and G\u2217. In particular, we have the following immediate result (a slight extension of Even-Dar et al. (2009)).\nProposition 13\n(i) For q \u2208 \u2206(Z),\nV c(q) \u2282 {v \u2208 V0 : conv(G)(v) \u2264 conc(G\u2217)(q)} ,\nwhere conv(G) and conc(G\u2217) are the lower convex hull of G and the upper concave hull of G\u2217, respectively.\n(ii) Consequently, if G(v) is a convex function over v \u2208 V0, then the relaxed goal function conc(G\u2217)(q) is attainable.\n(iii) If, furthermore, G\u2217(q) is a concave function of q, then V c = V \u2217, and the goal function G\u2217(q) is attainable.\nClearly, if G\u2217(q) is not concave, the attainable goal function is weaker than the original one. Still, this relaxed goal is meaningful, at least in cases where G(v) is convex (case (ii) above), so that conc(G\u2217)(q) is attainable. Noting that G\u2217(q) \u2264 maxq\u2032 minpG(v(p, q\u2032)), it follows that\nconc(G\u2217)(q) \u2264 max q\u2032\u2208\u2206(Z) min p\u2208\u2206(A) G(v(p, q\u2032)) \u2264 min p\u2208\u2206(A) max q\u2032\u2208\u2206(Z) G(v(p, q\u2032)) . (19)\nThe latter min-max bound is just the security level of the agent in the repeated game, namely the minimal value of G(V\u0304n) that can be secured (as n \u2192 \u221e) by playing a fixed (non-adaptive) mixed action q\u2032. Note that the second inequality in Equation (19) will be strict except for special cases where the min-max theorem holds for G(v(p, q)) (which is hardly expected if G\u2217(q) is non-concave).\nConvexity of G(v) depends directly on its definition, and will hold for cases of interest such as norm functions. Concavity of G\u2217(q), on the other hand, is more demanding and will hold only in special cases. We give below two examples, one in which G(v) is convex but G\u2217(q) is not necessarily concave, and one in which both properties are satisfied. In the next subsection we discuss an example where neither is true.\nExample 1 (Absolute Value) Let v : A \u00d7 Z \u2192 R be a scalar reward function, and suppose that we wish to minimize the deviation of the average reward V\u0304n from a certain set value, say 0. Define then G(v) = |v|, and note that G is a convex function. Now,\nG\u2217(q) , min p\u2208\u2206(A) |v(p, q)| =\n\n\n mina\u2208A v(a, q) : v(a, q) > 0, a \u2208 A mina\u2208A(\u2212v(a, q)) : v(a, q) < 0, a \u2208 A 0 : otherwise\nThe response p\u2217(q) of the agent is obvious from these relations. We can observe two special case in this example:\n(i) The problem reduces to the classical no-regret problem if the rewards v(a, z) all have the same sign (positive or negative), as the absolute value can be removed. Indeed, in this case G\u2217(q) is concave, as a minimum of linear functions.\n(ii) If the set {v(a, q), a \u2208 A} includes elements of opposite signs (0 included) for each q, then G\u2217 = 0, and the point v = 0 becomes attainable.\nIn general, however, |v(p, q)| may be a strictly convex function of q for a fixed p, and the minimization above need not lead to a concave function. In that case, we can ensure only the attainability of conc(G\u2217)(q).\nWe note that the computation of conc(G\u2217) may be fairly complicated in general, which implies the same for computing the projection onto the associated goal set S = {(v, q) : |v| \u2264 conc(G\u2217)(q)}. However, these computations are not needed in the proposed ResponseBased Approachability algorithm, where the required computation of the agent\u2019s response p\u2217(q) is straightforward.\nExample 2 (Load Balancing) The following model was considered in Even-Dar et al. (2009), motivated by load balancing and job scheduling problems. Consider a scalar loss function \u2113 : A\u00d7Z \u2192 R, with \u2113(a, z) \u2265 0, and define a corresponding vector-valued reward function v : A\u00d7Z \u2192 R|A|, where v(a, z) has \u2113(a, z) at entry a and 0 otherwise:\nv(a, z)[a\u2032] =\n{\n\u2113(a, z), a = a\u2032\n0, otherwise.\nThe average reward vector then represents the the average loss of the agent on different actions. Namely,\nV\u0304n[a] = 1\nn\nn \u2211\nk=1\nI {a = ak} \u2113(a, zk).\nAlso, note that v(p, q) = {p(a)\u2113(a, q)}a\u2208A , p\u2299 \u2113(\u00b7, q),\nand G\u2217(q) = min\np\u2208\u2206(A) G(v(p, q)) = min p\u2208\u2206(A) G (p\u2299 \u2113(\u00b7, q)) .\nEven-Dar et al. (2009) analyzed the case where G is either the d-norm with d > 1, or the infinity norm (the makespan). Clearly G is convex here. Furthermore, it was shown that the function\nF \u2217(\u2113) , min p\u2208\u2206(A) G (p\u2299 \u2113)\nis concave in \u2113. Now, since G\u2217(q) = F \u2217(\u2113(\u00b7, q)) and \u2113(\u00b7, q) is linear in q, then G\u2217(q) is also concave in q.\nThe agent\u2019s response is easily computed for this problem: The response p = p\u2217(q) is generally mixed, with pa proportional to \u2113(a, q)\n\u2212d/(d\u22121) for d < \u221e, and to \u2113(a, q)\u22121 for the infinity norm."}, {"heading": "5.2 Reward-to-Cost Maximization", "text": "Consider the repeated game model as before, where the goal of the agent is to maximize the ratio U\u0304n/C\u0304n. Here, U\u0304n is the average of a scalar reward function u(a, z) and C\u0304n is the average of a scalar positive cost function c(a, z). This problem is mathematically equivalent to the problem of regret minimization in repeated games with variable stage duration considered in Mannor and Shimkin (2008) (in that paper, the cost was specifically taken as the stage duration). Observe that this problem is a particular case of the global cost function problem presented in Section 5.1, with vector-valued payoff function v(a, z) = (u(a, z), c(a, z)) and G(v) = \u2212u/c. However, here G(v) is not convex in v. We will therefore need to apply specific analysis in order to obtain similar bounds to those of Proposition 13(ii).\nWe mention that similar bounds to the ones established below were obtained in Mannor and Shimkin (2008). The algorithm there was based on playing a best-response to calibrated forecasts of the opponent\u2019s mixed actions. As mentioned in the introduction, obtaining these forecasts is computationally hard, and the present formulation offers a considerably less demanding alternative.\nDenote\n\u03c1(a, q) , u(a, q)\nc(a, q) , \u03c1(p, q) ,\nu(p, q) c(p, q) .\nand let\nval(\u03c1) , max p\u2208\u2206(A) min q\u2208\u2206(Z) \u03c1(p, q) = min q\u2208\u2206(Z) max p\u2208\u2206(A) \u03c1(p, q)\n(the last equality is proved in the above-mentioned paper; note that \u03c1(p, q) is not generally concave-convex). It may be seen that val(\u03c1) is the value of the zero-sum repeated game with payoffs U\u0304n/C\u0304n, hence serves as a security level for the agent. A natural goal for the agent would be to improve on val(\u03c1) whenever the opponent\u2019s actions deviate (in terms of their empirical mean) from the minimax optimal strategy.\nLet\n\u03c1\u2217(q) , max p\u2208\u2206(A) \u03c1(p, q)\ndenote the best ratio-in-hindsight. We apply Algorithm 2, with v = (u, c) and the satisficing payoff set\nV \u2217(q) = { v = (u, c) : u c \u2265 \u03c1\u2217(q) }\n(observe that both \u03c1\u2217(q) and V \u2217(q) are non-convex functions in general). The agent\u2019s response is given by any mixed action\np\u2217(q) \u2208 P \u2217(q) , argmax p\u2208\u2206(A) \u03c1(p, q).\nIt is easily verified that the maximum can always be obtained here in pure actions (Mannor and Shimkin (2008); see also the proof of Prop. 15 below). Denote\nA\u2217(q) , argmax a\u2208A \u03c1(a, q),\nand define the following relaxation of \u03c1\u2217(q):\n\u03c11(q) , inf\n\n\n\n\u2211J j=1 u(aj , qj) \u2211J j=1 c(aj , qj) : 1 \u2264 J < \u221e, qj \u2208 \u2206(Z), 1 J J \u2211 j=1 qj = q, aj \u2208 A\u2217(qj)\n\n\n\n. (20)\nClearly, \u03c11(q) \u2264 \u03c1\u2217(q). We will show below that \u03c11 is attained by applying Algorithm 2 to this problem. First, however, we compare \u03c11 to the security level val(\u03c1).\nLemma 14\n(i) \u03c11(q) \u2265 val(\u03c1) for all q \u2208 \u2206(Z).\n(ii) \u03c11(q) > val(\u03c1) whenever \u03c1 \u2217(q) > val(\u03c1).\n(iii) If q corresponds to a pure action z, namely q = \u03b4z, then \u03c11(q) = \u03c1 \u2217(q).\n(iv) \u03c11(q) is a continuous function of q.\nProof To prove this Lemma, we first derive a more convenient expression for \u03c11(q). For a \u2208 A, let\nQa , {q \u2208 \u2206(Z) : a \u2208 A\u2217(q)}\ndenote the (closed) set of mixed actions to which a is a best-response action. Observe that for given J , q1, ..., qJ and aj \u2208 A\u2217(qj), we have\n\u2211J j=1 u(aj , qj) \u2211J j=1 c(aj , qj) = \u2211 a\u2208ANau(a, q\u0304a) \u2211 a\u2208A Nac(a, q\u0304a) ,\nwhere\nNa =\nJ \u2211\nj=1\nI {aj = a} , q\u0304a = 1\nNa\nJ \u2211\nj=1\nI {aj = a} qj .\nNote that q\u0304a \u2208 conv(Qa) as it is a convex combination of qj \u2208 Qa. Therefore, the definition in (20) is equivalent to\n\u03c11(q) = min\n{\n\u2211\na\u2208A \u03b1au(a, qa) \u2211\na\u2208A \u03b1ac(a, qa) : \u03b1 \u2208 \u2206(A), qa \u2208 conv(Qa),\n\u2211 a\u2208A \u03b1aqa = q\n}\n.\nNow, this is exactly the definition of the so-called calibration envelope in Mannor and Shimkin (2008), and the claims of the lemma follow by Lemma 6.1 and Proposition 6.4 there.\nIt may be seen that \u03c11(q) does not fall below the security level val(q), and is strictly above it when q is not a minimax action with respect to \u03c1(p, q). Furthermore, at the vertices vertices of \u2206(Z), it actually coincides with the best ratio-in-hindsight \u03c1\u2217(q).\nWe proceed to the following result that proves the attainability of \u03c11(q).\nProposition 15 Consider Algorithm 2 applied to the model of the present Subsection. Suppose that the response action to q\u2217n is chosen as any action p \u2217 n \u2208 P \u2217(q\u2217n) and consequently the target point is set to v\u2217n = (u(p \u2217 n, q \u2217 n), c(p \u2217 n, q \u2217 n)). Then,\nlim inf n\u2192\u221e\n(\nU\u0304n C\u0304n \u2212 \u03c11(q\u0304n) ) \u2265 0 (a.s.)\nfor any strategy of the opponent.\nProof Algorithm 2 ensures that, with probability 1,\n\u2016q\u0304n \u2212 q\u0304\u2217n\u2016 \u2192 0, (21) \u2223 \u2223 \u2223 \u2223 \u2223 U\u0304n \u2212 1 n n \u2211\nk=1\nu(p\u2217k, q \u2217 k)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2192 0, \u2223 \u2223 \u2223 \u2223 \u2223 C\u0304n \u2212 1 n n \u2211\nk=1\nc(p\u2217k, q \u2217 k)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2192 0; (22)\nsee Theorem 6 and recall the asymptotic equivalence of smoothed and non-smoothed averages. Noting that the cost c is positive and bounded away zero, (22) implies that\nlim n\u2192\u221e U\u0304n C\u0304n = lim n\u2192\u221e\n\u2211n k=1 r(p \u2217 k, q \u2217 k) \u2211n k=1 c(p \u2217 k, q \u2217 k) . (23)\nLet\n\u03c12(q) , inf\n\n\n\n\u2211J j=1 u(pj , qj) \u2211J j=1 c(pj, qj) : 1 \u2264 J < \u221e, qj \u2208 \u2206(Z), 1 J J \u2211 j=1 qj = q, pj \u2208 P \u2217(qj)\n\n\n\n. (24)\nClearly, \u2211n\nk=1 r(p \u2217 k, q \u2217 k)\n\u2211n k=1 c(p \u2217 k, q \u2217 k)\n\u2265 \u03c12(q\u0304\u2217n). (25)\nAlso, it may be verified that the infimum in (24) is obtained in pure actions aj \u2208 A\u2217(qj), implying that\n\u03c12(q) = \u03c11(q). (26)\nIndeed, note that \u2211J\nj=1 u(pj , qj) \u2211J\nj=1 c(pj, qj) \u2264 K\nis equivalent to J \u2211\nj=1\nu(pj , qj)\u2212K J \u2211\nj=1\nc(pj , qj) \u2264 0.\nNow, consider minimizing the left-hand-side over pj \u2208 P \u2217(qj). Due to the linearity in pj and the fact that P \u2217(qj) is just the mixture of actions in A\u2217(qj), the optimal actions are pure (that is, in A\u2217(qj)).\nCombining (23), (25), and (26), we obtain\nlim inf n\u2192\u221e\n(\nU\u0304n C\u0304n \u2212 \u03c11(q\u0304\u2217n) ) \u2265 0.\nThe proof is concluded by using (21) and the continuity of \u03c11 (see Lemma 14)."}, {"heading": "5.3 Constrained Regret Minimization", "text": "We finally address the constrained regret minimization problem, introduced in Mannor et al. (2009). Here, in addition to the scalar reward function u, we are given a vectorvalued cost function c : A \u00d7 Z \u2192 Rs. We are also given a closed and convex set \u0393 \u2286 Rs, the constraint set, which specifies the allowed values for the long-term average cost. A specific common case is that of a linear constraint on each cost component, that is \u0393 = {c \u2208 Rs : ci \u2264 \u03b3i, i = 1, ..., s} for some given vector \u03b3 \u2208 Rs. The constraint set is assumed to be feasible (or not excludable), in the sense that for every q \u2208 \u2206(Z), there exists p \u2208 \u2206(A) such that c(p, q) \u2208 \u0393.\nLet C\u0304n , n \u22121\u2211n\nk=1 ck denote, as before, the average cost by time n. The agent is required to satisfy the cost constraints, in the sense that limn\u2192\u221e d(C\u0304n,\u0393) = 0 must hold, irrespectively of the opponent\u2019s play. Subject to these constraints, the agent wishes to maximize its average reward U\u0304n.\nWe observe that a concrete learning application for the constrained regret minimization problem was proposed in Bernstein et al. (2010). There, we considered the on-line problem of merging the output of multiple binary classifiers, with the goal of maximizing the truepositive rate, while keeping the false-positive rate under a given threshold 0 < \u03b3 < 1. As shown in that paper, this problem may be formulated as a constrained regret minimization problem.\nA natural extension of the best-reward-in-hindsight u\u2217(q) in (2) to the constrained setting is given by\nu\u2217\u0393(q) , max p\u2208\u2206(A) {u(p, q) : c(p, q) \u2208 \u0393} . (27)\nWe can now define the satisficing payoff set of the pairs v = (u, c) \u2208 R1+s in terms of u\u2217\u0393(q) and \u0393:\nV \u2217(q) , { v = (u, c) \u2208 R1+s : u \u2265 u\u2217\u0393(q), c \u2208 \u0393 } .\nNote that u\u2217\u0393(q) is not convex in general, and consequently V \u2217(q) is not convex as well. Indeed, it was shown in Mannor et al. (2009) that V \u2217(q) is not approachable in general. The convex hull of V \u2217(q) may be written as\nV c(q) = { (u, c) \u2208 Rs+1 : u \u2265 conv (u\u2217\u0393) (q), c \u2208 \u0393 } , (28)\nwhere the function conv (u\u2217\u0393) is the lower convex hull of u \u2217 \u0393.\nTwo algorithms were proposed in Mannor et al. (2009) for attaining V c(q). The first is a standard (Blackwell\u2019s) approachability algorithm for S = {(v, q) : v \u2208 V c(q)}, which requires the demanding calculation of projection directions to S. The second algorithm employs a best-response to calibrated forecasts of the opponent\u2019s mixed actions. As mentioned in the introduction, obtaining these forecasts is computationally hard. In contrast, our algorithm only requires the computation of the response p\u2217(q) as any maximizing action in (27). Similarly to the case of global cost functions, step 4 of Algorithm 2 boils down to solving the optimization problem in (27) for q = q\u2217n. Note that p \u2217 n can be efficiently computed for a given q\u2217n since (27) is a convex program in general, while it is a linear program whenever the constraints set is a polyhedron.\nRemark 16 Note that since V c(q) is unbounded in the direction of its first coordinate u, the algorithm variant presented in Subsection 4.3 can be applied. In this case, the first\ncoordinate of the steering direction \u03bbn can be set to zero in \u03bb\u0303n whenever it is negative, which corresponds to u\u0304n\u22121 \u2265 u\u0304\u2217n\u22121, thereby avoiding an unnecessary reduction in u\u0304n\u22121. Similarly, for linear constraint sets of the form {ci \u2264 \u03b3i}, the ci-coordinate of \u03bbn may be nullified whenever [c\u0304n\u22121]i \u2264 [c\u0304\u2217n\u22121]i.\nA similar modification can be applied also in the reward-to-cost problem of Section 5.2. That is, the u-coordinate of \u03bbn can be set to zero whenever u\u0304n\u22121 \u2265 u\u0304\u2217n\u22121, while the ccoordinate of \u03bbn may be nullified whenever c\u0304n\u22121 \u2264 c\u0304\u2217n\u22121."}, {"heading": "6. Conclusion", "text": "We have introduced in this paper a class of approachability algorithms that are based on Blackwell\u2019s dual, rather than primal, approachability condition. The proposed algorithms rely directly on the availability of a response function, rather than projection onto the goal set (or related geometric quantities), and are therefore convenient in certain problems where the latter may be hard to compute. At the same time, the additional computational requirements are generally comparable to those of the standard Blackwell algorithm and its variants.\nThe proposed algorithms were applied to a class of generalized no-regret problems, that includes reward-to-cost maximization, and reward maximization subject to averagecost constraints. The resulting algorithms are apparently the first computationally efficient algorithms in this generalized setting.\nIn this paper we have focused on a repeated matrix game model, where the action sets of the agent and the adversary in the stage game are both discrete. It is worth pointing out that the essential results of this paper also apply directly to models with convex action sets are convex, say x \u2208 X and y \u2208 Y , and the vector reward function r(x, y) is bilinear in its arguments. In that case the (observed) actions x and y simply take the place of the mixed actions p and q, leading to similar algorithms and convergence results. The continuousaction model is of course relevant to linear classification and regression problems.\nOther extensions of possible interest for the response-based algorithms suggested in this paper include stochastic game models, problems of partial monitoring, and possibly nonlinear (concave-convex) reward functions. These are left for future work."}, {"heading": "Acknowledgements", "text": "We wish to thank Shie Mannor for useful discussions, and for pointing out the application to regret minimization with global cost functions. We further thank Elad Hazan for helpful comments on the Appendix. This research was supported by the Israel Science Foundation grant No. 1319/11."}, {"heading": "Appendix A", "text": "We outline in this Appendix a somewhat more direct version of the no-regret based approachability algorithms proposed in Abernethy et al. (2012). This version avoids the lifting procedure used in that paper, that treats general (convex) target sets by lifting them to convex cones in higher dimension. This brief outline is meant to highlight the geometric nature and requirements of this class of algorithms.\nLet S \u2286 R\u2113 be the convex and closed target set to be approached. Let hS denote the support function of S:\nhS(\u03b8) , sup r\u2208S\n(\u03b8 \u00b7 r), \u03b8 \u2208 R\u2113.\nNote that hS is a convex function. The Euclidean distance from a point r to S may be expressed as\nd(r, S) = max \u03b8\u2208B2(1)\n{\u03b8 \u00b7 r \u2212 hS(\u03b8)} , (29)\nwhere B2(1) is the Euclidean unit ball, B2(1) = {\u03b8 \u2208 R\u2113 : \u03b8 \u00b7\u03b8 \u2264 1} (see Rockafellar (1970), Section 16; this equality can also be verified directly using the minimax theroem).\nBlackwell\u2019s (primal) separation condition can now be written as follows2:\n\u2022 For each \u03b8 \u2208 B2(1) there exist p \u2208 \u2206(Z) so that, for every q \u2208 \u2206(Z),\n\u03b8 \u00b7 r(p, q) \u2264 sup r\u2208S \u03b8 \u00b7 r \u2261 hS(\u03b8) ,\nthat is,\n\u03b8 \u00b7 r(p, q)\u2212 hS(\u03b8) \u2264 0. (30)\nAn approachability algorithm can be devised as follows. Observe that the function fr(\u03b8) = \u03b8 \u00b7 r \u2212 hS(\u03b8) is concave in \u03b8 (for each r). Hence, an online concave programming algorithm applied to the sequence of functions (\u03b8 \u00b7 rn\u2212hS(\u03b8)), with rn arbitrary (bounded) vectors, will produce a sequence of steering directions {\u03b8n} in B2(1) so that\n1\nn\nn \u2211\nk=1\n(\u03b8k \u00b7 rk \u2212 hS(\u03b8k)) \u2265 max \u03b8\u2208B2(1) {\u03b8 \u00b7 r\u0304n \u2212 hS(\u03b8)} \u2212 o(1). (31)\nNow, observing (30), one can choose each pn so that rn = r(pn, zn) satisfies \u03b8n \u00b7rn\u2212hS(\u03b8n) \u2264 0. Substituting in (31) we obtain\nmax \u03b8\u2208B2(1)\n{\u03b8 \u00b7 r\u0304n \u2212 hS(\u03b8)} \u2264 o(1).\nHence, by (29), d(r\u0304n, S) \u2264 o(1). Observe that the above scheme applies an online concave programming algorithm to the functions fr(\u03b8), that are defined through the support function hS(\u03b8). Thus, it essentially\n2. We use here the notations and formulation of the present paper, where p and q are mixed actions in their respective simplices. However, the following observations are valid also for the case where p and q are (observed) actions in bounded convex sets and r(p, q) a bilinear function thereof, as considered in Abernethy et al. (2012).\nrequires computing the support function hS (or its derivative) at some point in each stage of the game.\nTo be specific, let us apply the gradient ascent algorithm of Zinkevich (2003) to the problem. The resulting approachability algorithm proceeds as follows.\n1. At stage n, we start with \u03b8n\u22121, pn\u22121, rn\u22121, zn\u22121 from the previous stage.\n2. Let\n\u03b8n = Proj(\u03b8n\u22121 \u2212 \u03b7n\u2207\u03b8(\u03b8n\u22121 \u00b7 rn\u22121 \u2212 hS(\u03b8n\u22121)) = Proj(\u03b8n\u22121 \u2212 \u03b7n(rn\u22121 \u2212\u2207hS(\u03b8n\u22121)) (32)\nwhere Proj is then projection onto the unit ball.\n3. Choose pn according to (30), so that \u03b8n \u00b7 r(pn, z)\u2212 hS(\u03b8n) \u2264 0 for all z \u2208 Z.\n4. Observe the opponent\u2019s action zn, and set rn = r(pn, zn)."}], "references": [{"title": "Blackwell approachability and low-regret learning are equivalent", "author": ["J. Abernethy", "P.L. Bartlett", "E. Hazan"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Repeated Games with Incomplete Information", "author": ["R.J. Aumann", "M. Maschler"], "venue": null, "citeRegEx": "Aumann and Maschler.,? \\Q1995\\E", "shortCiteRegEx": "Aumann and Maschler.", "year": 1995}, {"title": "Online classification with specificity", "author": ["A. Bernstein", "S. Mannor", "N. Shimkin"], "venue": "Proceedings of the Conference on Neural Information Processing Systems (NIPS", "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Opportunistic approachability and generalized no-regret problems", "author": ["A. Bernstein", "S. Mannor", "N. Shimkin"], "venue": "To appear in Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2013}, {"title": "Controlled random walks", "author": ["D. Blackwell"], "venue": "In Proceedings of the International Congress of Mathematicians,", "citeRegEx": "Blackwell.,? \\Q1954\\E", "shortCiteRegEx": "Blackwell.", "year": 1954}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Blackwell.,? \\Q1956\\E", "shortCiteRegEx": "Blackwell.", "year": 1956}, {"title": "From external to internal regret", "author": ["A. Blum", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Online learning with global cost functions", "author": ["E. Even-Dar", "R. Kleinberg", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Even.Dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "A proof of calibration via blackwell\u2019s approachability theorem", "author": ["D. Foster"], "venue": "Games and Economic Behavior,", "citeRegEx": "Foster.,? \\Q1999\\E", "shortCiteRegEx": "Foster.", "year": 1999}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D.K. Levine"], "venue": null, "citeRegEx": "Fudenberg and Levine.,? \\Q1998\\E", "shortCiteRegEx": "Fudenberg and Levine.", "year": 1998}, {"title": "Approximation to Bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "A simple adaptive procedure leading to correlated", "author": ["S. Hart", "A. Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2000\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2000}, {"title": "A general class of adaptive strategies", "author": ["S. Hart", "A. Mas-Colell"], "venue": "Journal of Economic Theory,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2001\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2001}, {"title": "Calibration is computationally hard", "author": ["E. Hazan", "S. Kakade"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT", "citeRegEx": "Hazan and Kakade.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kakade.", "year": 2012}, {"title": "Approachability in infinite dimensional spaces", "author": ["E. Lehrer"], "venue": "International Journal of Game Theory,", "citeRegEx": "Lehrer.,? \\Q2002\\E", "shortCiteRegEx": "Lehrer.", "year": 2002}, {"title": "Learning to play", "author": ["E. Lehrer", "E. Solan"], "venue": "partially-specified equilibrium. Manuscript,", "citeRegEx": "Lehrer and Solan.,? \\Q2007\\E", "shortCiteRegEx": "Lehrer and Solan.", "year": 2007}, {"title": "Approachability with bounded memory", "author": ["E. Lehrer", "E. Solan"], "venue": "Games and Economic Behavior,", "citeRegEx": "Lehrer and Solan.,? \\Q2009\\E", "shortCiteRegEx": "Lehrer and Solan.", "year": 2009}, {"title": "A general internal regret-free strategy", "author": ["Bernstein", "Shimkin E. Lehrer", "E. Solan"], "venue": null, "citeRegEx": "Bernstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2013}, {"title": "A geometric approach to multi-criterion reinforcement learning", "author": ["S. Mannor", "N. Shimkin"], "venue": null, "citeRegEx": "Mannor and Shimkin.,? \\Q2003\\E", "shortCiteRegEx": "Mannor and Shimkin.", "year": 2003}, {"title": "Calibration and internal no-regret with partial monitoring", "author": ["V. Perchet"], "venue": null, "citeRegEx": "Perchet.,? \\Q2006\\E", "shortCiteRegEx": "Perchet.", "year": 2006}, {"title": "Strategic Learning and Its Limits", "author": ["H.P. Young"], "venue": null, "citeRegEx": "1992", "shortCiteRegEx": "1992", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "Abstract Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup.", "startOffset": 47, "endOffset": 64}, {"referenceID": 5, "context": "In Blackwell\u2019s approachability problem (Blackwell, 1956), the agent\u2019s goal is to ensure that the long-term average reward vector approaches a given target set S, namely converges to S", "startOffset": 39, "endOffset": 56}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 178, "endOffset": 205}, {"referenceID": 9, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 230, "endOffset": 244}, {"referenceID": 12, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 287, "endOffset": 314}, {"referenceID": 14, "context": "However, the algorithms in these papers are essentially based on the computation of calibrated forecasts of the opponent\u2019s actions, a task which is known to be computationally hard (Hazan and Kakade, 2012).", "startOffset": 181, "endOffset": 205}, {"referenceID": 8, "context": ", 2009), regret minimization with global cost functions (Even-Dar et al., 2009), regret minimization in variable duration repeated games (Mannor and Shimkin, 2008), and regret", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies.", "startOffset": 179, "endOffset": 458}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies.", "startOffset": 179, "endOffset": 534}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2).", "startOffset": 179, "endOffset": 734}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2).", "startOffset": 179, "endOffset": 842}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006).", "startOffset": 179, "endOffset": 1073}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006).", "startOffset": 179, "endOffset": 1087}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms.", "startOffset": 179, "endOffset": 1123}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment.", "startOffset": 179, "endOffset": 1338}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies.", "startOffset": 179, "endOffset": 2478}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies.", "startOffset": 179, "endOffset": 2514}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies. An explicit approachability algorithm which is based on computing the response to calibrated forecasts of the opponent\u2019s actions has been proposed in Perchet (2009), and further analyzed in Bernstein et al.", "startOffset": 179, "endOffset": 2730}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies. An explicit approachability algorithm which is based on computing the response to calibrated forecasts of the opponent\u2019s actions has been proposed in Perchet (2009), and further analyzed in Bernstein et al. (2013). However, the algorithms in these papers are essentially based on the computation of calibrated forecasts of the opponent\u2019s actions, a task which is known to be computationally hard (Hazan and Kakade, 2012).", "startOffset": 179, "endOffset": 2779}, {"referenceID": 19, "context": "minimization in stochastic game models (Mannor and Shimkin, 2003).", "startOffset": 39, "endOffset": 65}, {"referenceID": 4, "context": "Below is the classical definition of an approachable set from Blackwell (1956). Definition 1 (Approachable Set) A closed set S \u2286 Rl is approachable by the agent\u2019s strategy \u03c0 if the average reward R\u0304n = n \u22121\u2211n k=1Rk converges to S in the Euclidian pointto-set distance d(\u00b7, S), almost surely for every strategy \u03c3 of the opponent, at a uniform rate over all strategies \u03c3 of the opponent.", "startOffset": 62, "endOffset": 79}, {"referenceID": 5, "context": "Next, we present a formulation of Blackwell\u2019s results (Blackwell, 1956) which provides us with conditions for approachability of general and convex sets.", "startOffset": 54, "endOffset": 71}, {"referenceID": 3, "context": "Since Blackwell\u2019s original construction, some other approachability algorithms that are based on similar geometric ideas have been proposed in the literature. Hart and MasColell (2001) proposed a class of approachability algorithms that use a general steering direction with separation properties.", "startOffset": 6, "endOffset": 185}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework).", "startOffset": 10, "endOffset": 34}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets.", "startOffset": 10, "endOffset": 469}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets.", "startOffset": 10, "endOffset": 627}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets. In Shimkin and Shwartz (1993) and Milman (2006), approachability was extended", "startOffset": 10, "endOffset": 767}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets. In Shimkin and Shwartz (1993) and Milman (2006), approachability was extended", "startOffset": 10, "endOffset": 785}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory.", "startOffset": 96, "endOffset": 110}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory.", "startOffset": 96, "endOffset": 141}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory. Recently, Mannor et al. (2011) proposed a robust approachability algorithm for repeated games with partial monitoring and applied it to the corresponding regret minimization problem.", "startOffset": 96, "endOffset": 231}, {"referenceID": 4, "context": "We start with Blackwell\u2019s original formulation, and proceed to the alternative one by Hart and Mas-Colell (2001). In the final subsection, we consider briefly the more elaborate problem of internal regret minimization.", "startOffset": 14, "endOffset": 113}, {"referenceID": 4, "context": "1 Blackwell\u2019s Formulation Following Hannan\u2019s seminal paper, Blackwell (1954) used approachability theory in order to elegantly show the existence of regret minimizing algorithms.", "startOffset": 2, "endOffset": 77}, {"referenceID": 12, "context": "2 Regret Matching An alternative formulation, proposed in Hart and Mas-Colell (2001), leads to a a simple and explicit no-regret algorithm for this problem.", "startOffset": 58, "endOffset": 85}, {"referenceID": 10, "context": "It was shown in Hart and Mas-Colell (2001) that application of Blackwell\u2019s approachability strategy in this formulation leads to the so-called regret matching algorithm, where the probability of action a at time step n is given by:", "startOffset": 16, "endOffset": 43}, {"referenceID": 10, "context": "The formulation of internal-no-regret as the approachability problem above, along with explicit approaching strategies, in due to Hart and Mas-Colell (2000). The importance of internal regret in game theory stems from the fact that if each player in a repeated N -player game uses such a no-internal regret strategy, then the empirical distribution of the players\u2019 actions convergence to the set of correlated equilibria.", "startOffset": 130, "endOffset": 157}, {"referenceID": 6, "context": "Some interesting relations between internal and external (Hannan\u2019s) regret are discussed in Blum and Mansour (2007).", "startOffset": 92, "endOffset": 116}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.", "startOffset": 48, "endOffset": 86}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.1 in Shimkin and Shwartz (1993) to deduce that a sequence \u2016\u03bb\u0303n\u2016 satisfying (17) converges to zero almost surely at a uniform rate that depends only on \u03c1.", "startOffset": 48, "endOffset": 151}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.1 in Shimkin and Shwartz (1993) to deduce that a sequence \u2016\u03bb\u0303n\u2016 satisfying (17) converges to zero almost surely at a uniform rate that depends only on \u03c1. In particular, using the proof of Proposition 4.1 in Shimkin and Shwartz (1993) we know that for every \u01eb > 0 and \u03b4 > 0, there exists N = N(\u01eb, \u03b4, \u03c1) so that P {", "startOffset": 48, "endOffset": 353}, {"referenceID": 8, "context": "1 Global Cost Functions The following problem of regret minimization with global cost functions was introduced in Even-Dar et al. (2009). Suppose that the goal of the agent is to minimize a general (i.", "startOffset": 114, "endOffset": 137}, {"referenceID": 8, "context": "In particular, we have the following immediate result (a slight extension of Even-Dar et al. (2009)).", "startOffset": 77, "endOffset": 100}, {"referenceID": 8, "context": "Example 2 (Load Balancing) The following model was considered in Even-Dar et al. (2009), motivated by load balancing and job scheduling problems.", "startOffset": 65, "endOffset": 88}, {"referenceID": 8, "context": "Even-Dar et al. (2009) analyzed the case where G is either the d-norm with d > 1, or the infinity norm (the makespan).", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "This problem is mathematically equivalent to the problem of regret minimization in repeated games with variable stage duration considered in Mannor and Shimkin (2008) (in that paper, the cost was specifically taken as the stage duration).", "startOffset": 141, "endOffset": 167}, {"referenceID": 19, "context": "This problem is mathematically equivalent to the problem of regret minimization in repeated games with variable stage duration considered in Mannor and Shimkin (2008) (in that paper, the cost was specifically taken as the stage duration). Observe that this problem is a particular case of the global cost function problem presented in Section 5.1, with vector-valued payoff function v(a, z) = (u(a, z), c(a, z)) and G(v) = \u2212u/c. However, here G(v) is not convex in v. We will therefore need to apply specific analysis in order to obtain similar bounds to those of Proposition 13(ii). We mention that similar bounds to the ones established below were obtained in Mannor and Shimkin (2008). The algorithm there was based on playing a best-response to calibrated forecasts of the opponent\u2019s mixed actions.", "startOffset": 141, "endOffset": 688}, {"referenceID": 19, "context": "It is easily verified that the maximum can always be obtained here in pure actions (Mannor and Shimkin (2008); see also the proof of Prop.", "startOffset": 84, "endOffset": 110}, {"referenceID": 19, "context": "Now, this is exactly the definition of the so-called calibration envelope in Mannor and Shimkin (2008), and the claims of the lemma follow by Lemma 6.", "startOffset": 77, "endOffset": 103}, {"referenceID": 2, "context": "We observe that a concrete learning application for the constrained regret minimization problem was proposed in Bernstein et al. (2010). There, we considered the on-line problem of merging the output of multiple binary classifiers, with the goal of maximizing the truepositive rate, while keeping the false-positive rate under a given threshold 0 < \u03b3 < 1.", "startOffset": 112, "endOffset": 136}], "year": 2013, "abstractText": "Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup. Given a repeated game with vector payoffs, a target set S is approachable by a certain player (the agent) if he can ensure that the average payoff vector converges to that set no matter what his adversary opponent does. Blackwell provided two equivalent sets of conditions for a convex set to be approachable. The first (primary) condition is a geometric separation condition, while the second (dual) condition requires that the set be non-excludable, namely that for every mixed action of the opponent there exists a mixed action of the agent (a response) such that the resulting payoff vector belongs to S. Existing approachability algorithms rely on the primal condition and essentially require to compute at each stage a projection direction from a given point to S. In this paper, we introduce an approachability algorithm that relies on Blackwell\u2019s dual condition. Thus, rather than projection, the algorithm relies on computation of the response to a certain action of the opponent at each stage. The utility of the proposed algorithm is demonstrated by applying it to certain generalizations of the classical regret minimization problem, which include regret minimization with side constraints and regret minimization for global cost functions. In these problems, computation of the required projections is generally complex but a response is readily obtainable.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}