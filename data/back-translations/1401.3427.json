{"id": "1401.3427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning", "abstract": "This paper defines the concept of analogous dissimilarity between four objects, with particular emphasis on objects structured as sequences. First, it examines the case in which the four objects exhibit zero analogous dissimilarity, i.e. are in analogous proportion. Second, if one of these objects is unknown, it gives algorithms for its calculation. Third, it addresses the problem of the definition of analogous dissimilarity, which is a measure of how far four objects are from their analogous relationship. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences.", "histories": [["v1", "Wed, 15 Jan 2014 04:42:13 GMT  (343kb)", "http://arxiv.org/abs/1401.3427v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["laurent miclet", "sabri bayoudh", "arnaud delhay"], "accepted": false, "id": "1401.3427"}, "pdf": {"name": "1401.3427.pdf", "metadata": {"source": "CRF", "title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning", "authors": ["Laurent Miclet", "Sabri Bayoudh", "Arnaud Delhay"], "emails": ["LAURENT.MICLET@UNIV-RENNES1.FR", "SABRI.BAYOUDH@UNIV-ST-ETIENNE.FR", "ARNAUD.DELHAY@UNIV-RENNES1.FR"], "sections": [{"heading": null, "text": "focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer."}, {"heading": "1. Introduction", "text": "Analogy is a way of reasoning that has been studied throughout the history of philosophy and has been widely used in Artificial Intelligence and Linguistics. We focus in this paper on a restricted concept of analogy called \u2018analogical proportion\u2019."}, {"heading": "1.1 Analogical Proportion between Four Elements", "text": "An analogical proportion between four elements A, B, C and D in the same universe is usually expressed as follows: \u201cA is to B as C is to D\u201d. Depending on the elements, analogical proportions1 can have very different meanings. For example, natural language analogical proportions could be: \u201ca crow is to a raven as a merlin is to a peregrine\u201d or \u201cvinegar is to wine as a sloe is to a cherry\u201d. They are based on the semantics of the words. By contrast, in the formal universe of sequences, analogical proportions such as \u201cabcd is to abc as abbd is to abb\u201d or \u201cg is to gt as gg is to ggt\u201d are morphological.\nWhether morphological or not, the examples above show the intrinsic ambiguity in defining an analogical proportion. We could as well accept, for other good reasons: \u201cg is to gt as gg is to ggtt\u201d or \u201cvinegar is to wine as vulgar is to wul\u201d. Obviously, such ambiguities are inherent in semantic analogies, since they are related to the meaning of words (the concepts are expressed through natural language). Hence, it seems important, as a first step, to focus on formal morphological properties. Moreover, solving such analogies in sequences is an\n1. When there is no ambiguity, we may use \u2018analogy\u2019 for short instead of \u2018analogical proportion\u2019.\nc\u00a92008 AI Access Foundation. All rights reserved.\noperational problem in several fields of linguistics, such as morphology and syntax, and provides a basis to learning and data mining by analogy in the universe of sequences.\nIn this paper, we will firstly consider analogical proportions in sets of objects and we will secondly present how they may be transferred to sequences of elements of these sets."}, {"heading": "1.2 Solving Analogical Equations", "text": "When one of the four elements is unknown, an analogical proportion turns into an equation. For instance, on sequences of letters, the analogical proportion \u201cwolf is to leaf as wolves is to x\u201d corresponds to the equation S = {x | wolf is to leaf as wolves is to x}. Resolving this equation consists in computing the (possibly empty) set S of sequences x which satisfy the analogy. The sequence leaves is an exact semantic and morphological solution. We shall see that, however, it is not straightforward to design an algorithm able to solve this kind of equation, in particular when looking for an approximate solution if necessary.\nSolving analogical equations on sequences is useful for linguistic analysis tasks and has been applied (with empirical resolution techniques, or in simple cases) mainly to lexical analysis tasks. For example, Yvon (1999) presents an analogical approach to the grapheme-to-phoneme conversion, for text-to-speech synthesis purposes. More generally, the resolution of analogical equations can also be seen as a basic component of learning by analogy systems, which are part of the lazy learning techniques (Daelemans, 1996)."}, {"heading": "1.3 Using Analogical Proportions in Machine Learning", "text": "Let S = {(x, u(x))} be a finite set of training examples, where x is the description of an example (x may be a sequence or a vector in Rn, for instance) and u(x) its label in a finite set. Given the description y of a new pattern, we would like to assign to y a label u(y), based only from the knowledge of S. This is the problem of inductive learning of a classification rule from examples, which consists in finding the value of u at point y (Mitchell, 1997). The nearest neighbor method, which is the most popular lazy learning technique, simply finds in S the description x\u22c6 which minimizes some distance to y and hypothesizes u(x\u22c6), the label of x\u22c6, for the label of y.\nMoving one step further, learning from analogical proportions consists in searching in S for a triple (x\u22c6, z\u22c6, t\u22c6) such that \u201cx\u22c6 is to z\u22c6 as t\u22c6 is to y\u201d and predicts for y the label u\u0302(y) which is solution of the equation \u201cu(x\u22c6) is to u(z\u22c6) as u(t\u22c6) is to u\u0302(y)\u201d. If more than one triple is found, a voting procedure can be used. Such a learning technique is based on the resolution of analogical equations. Pirrelli and Yvon (1999) discuss the relevance of such a learning procedure for various linguistic analysis tasks. It is important to notice that y and u(y) are in different domains: for example, in the simple case of learning a classification rule, y may be a sequence whereas u is a class label.\nThe next step in learning by analogical proportions is, given y, to find a triple (x\u22c6, z\u22c6, t\u22c6) in S such that \u201cx\u22c6 is to z\u22c6 as t\u22c6 is to y\u201d holds almost true, or, when a closeness measure is defined, the triple which is the closest to y in term of analogical proportion. We study in this article how to quantify this measure, in order to provide a more flexible method of learning by analogy."}, {"heading": "1.4 Related Work", "text": "This paper is related with several domains of artificial intelligence. Obviously, the first one is that of reasoning by analogy. Much work has been done on this subject from a cognitive science point of view, which had led to computational models of reasoning by analogy: see for example, the classical paper (Falkenhainer, Forbus, & Gentner, 1989), the book (Gentner, Holyoak, & Kokinov, 2001) and the recent survey (Holyoak, 2005). Usually, these works use the notion of transfer, which is not within the scope of this article. It means that some knowledge on solving a problem in a domain is transported to another domain. Since we work on four objects that are in the same space, we implicitly ignore the notion of transfer between different domains. Technically speaking, this restriction allows us to use an axiom called \u2018exchange of the means\u2019 to define an analogical proportion (see Definition 2.1). However, we share with these works the following idea: there may be a similar relation between two couples of structured objects even if the objects are apparently quite different. We are interested in giving a formal and algorithmic definition of such a relation.\nOur work also aims to define some supervised machine learning process (Mitchell, 1997; Cornu\u00e9jols & Miclet, 2002), in the spirit of lazy learning. We do not seek to extract a model from the learning data, but merely conclude what is the class, or more generally the supervision, of a new object by inspecting (a part of) the learning data. Usually, lazy learning, like the k-nearest neighbors technique, makes use of unstructured objects, such as vectors. Since distance measures can be also defined on strings, trees and even graphs, this technique has also been used on structured objects, in the framework of structural pattern recognition (see for example the work of Bunke & Caelli, 2004; Blin & Miclet, 2000; Basu, Bunke, & Del Bimbo, 2005). We extend here the search of the nearest neighbor in the learning set to that of the best triple (when combined with the new object, it is the closest to make an analogical proportion). This requires defining what is an analogical proportion on structured objects, like sequences, but also to give a definition of how far a 4-tuple of objects is from being in analogy (that we call analogical dissimilarity).\nLearning by analogy on sequences has already being studied, in a more restricted manner, on linguistic data (Yvon, 1997, 1999; Itkonen & Haukioja, 1997). Reasoning and learning by analogy has proven useful in tasks like grapheme to phoneme conversion, morphology and translation. Sequences of letters and/or of phonemes are a natural application to our work, but we are also interested in other type of data, structured as sequences or trees, such as prosodic representations for speech synthesis, biochemical sequences, online handwriting recognition, etc.\nAnalogical proportions between four structured objects of the same universe, mainly strings, have been studied with a mathematical and algorithmic approach, like ours, by Mitchell (1993) and Hofstadter et al. (1994), Dastani et al. (2003), Schmid et al. (2003). To the best of our knowledge our proposition is original: to give a formal definition of what can be an analogical dissimilarity between four objects, in particular between sequences, and to produce algorithms that enable the efficient use of this concept in machine learning practical problems. We have already discussed how to compute exact analogical proportions between sequences in the paper by Yvon et al. (2004) and given a preliminary attempt to compute analogical dissimilarity between sequences in the paper by Delhay and Miclet (2004). Excerpts of the present article have been presented in conferences (Bayoudh, Miclet, & Delhay, 2007a; Bayoudh, Mouch\u00e8re, Miclet, & Anquetil, 2007b).\nTo connect with another field of A.I., let us quote Aamodt and Plaza (1994) about the use of the term \u2018analogy\u2019 in Case-Based Reasoning (CBR): \u2019Analogy-based reasoning: This term is sometimes used, as a synonym to case-based reasoning, to describe the typical case-based approach.\nHowever, it is also often used to characterize methods that solve new problems based on past cases from a different domain, while typical case-based methods focus on indexing and matching strategies for single-domain cases.\u2019 According to these authors, who use the word \u2018analogy\u2019 in its broader meaning, typical CBR deals with single domain problems, as analogical proportions also do. In that sense, our study could be seen as a particular case of CBR, as applied in this paper to supervised learning of classification rules."}, {"heading": "1.5 Organization of the Paper", "text": "This paper is organized in six sections. After this introduction, we present in section 2 the general principles which govern the definition of an analogical proportion between four objects in the same set and we define what is an analogical equation in a set. We apply these definitions in Rn and {0, 1}n. Finally, this section defines analogical proportion between four sequences on an alphabet in which an analogy is defined, using an optimal alignment method between the four sequences.\nSections 3 introduces the new concept of analogical dissimilarity (AD) between four objects, by measuring in some way how much these objects are in analogy. In particular, it must be equivalent to say that four objects are in analogy and that their analogical dissimilarity is null. Then we extend it to sequences. The end of this section gives two algorithms: SEQUANA4 computes the value of AD between four sequences and SOLVANA solves analogical equations in a generalized manner: it can produce approximate solutions (i.e. of strictly positive AD).\nSection 4 begins to explore the use of the concept of analogical dissimilarity in supervised machine learning. We give an algorithm (FADANA) for the fast search of the k-best analogical 3-tuples in the learning set.\nSection 5 presents two applications of these concepts and algorithms on real problems. We firstly apply FADANA to objects described by binary and nominal features. Experiments are conducted on classical benchmarks and favorably compared with standard classification techniques. Secondly, we make use of SOLVANA to produce new examples in a handwritten recognition system. This allows training a classifier from a very small number of learning patterns.\nThe last section presents work to be done, particularly in discussing more real world application of learning by analogy, especially in the universe of sequences."}, {"heading": "2. Analogical Proportions and Equations", "text": "In this section, we give a formal definition of the analogical proportion between four objects and explain what is to solve an analogical equation. Instanciations of the general definitions are given when the objects are either finite sets (or equivalently binary vectors), or vectors of real numbers or sequences on finite alphabets."}, {"heading": "2.1 The Axioms of Analogical Proportion", "text": "The meaning of an analogical proportion A : B :: C : D between four objects in a set X depends on the nature of X , in which the \u2018is to\u2019 and the \u2018as\u2019 relations have to be defined. However, general properties can be required, according to the usual meaning of the word \u2019analogy\u2019 in philosophy and linguistics. According to Lepage (2003) three basic axioms can be given:\nDefinition 2.1 (Analogical proportion) An analogical proportion on a set X is a relation on X4, i.e. a subset A \u2282 X4. When (A,B,C,D) \u2208 A, the four elements A, B, C and D are said to be\nin analogical proportion, and we write: \u2018the analogical proportion A : B :: C : D holds true\u2019, or simply A : B :: C : D , which reads \u2018A is to B as C is to D\u2019. For every 4-tuple in analogical proportion, the following equivalences must hold true:\nSymmetry of the \u2018as\u2019 relation: A : B :: C : D \u21d4 C : D :: A : B Exchange of the means: A : B :: C : D \u21d4 A : C :: B : D\nThe third axiom (determinism) requires that one of the two following implications holds true (the other being a consequence):\nA : A :: B : x \u21d2 x = B A : B :: A : x \u21d2 x = B\nAccording to the first two axioms, five other formulations are equivalent to the canonical form A : B :: C : D :\nB : A :: D : C D : B :: C : A C : A :: D : B D : C :: B : A and B : D :: A : C\nConsequently, there are only three different possible analogical proportions between four objects, with the canonical forms:\nA : B :: C : D A : C :: D : B A : D :: B : C"}, {"heading": "2.2 Analogical Equations", "text": "To solve an analogical equation consists in finding the fourth term of an analogical proportion, the first three being known.\nDefinition 2.2 (Analogical equation) D is a solution of the analogical equation A : B :: C : x if and only if A : B :: C : D .\nWe already know from previous sections that, depending on the nature of the objects and the definition of analogy, an analogical equation may have either no solution or a unique solution or several solutions. We study in the sequel how to solve analogical equations in different sets."}, {"heading": "2.3 Analogical Proportion between Finite Sets and Binary Objects", "text": "When the \u2018as\u2019 relation is the equality between sets, Lepage has given a definition of an analogical proportion between sets coherent with the axioms. This will be useful in section 2.3.2 in which objects are described by sets of binary features."}, {"heading": "2.3.1 AN ANALOGICAL PROPORTION IN FINITE SETS", "text": "Definition 2.3 (Analogical proportion between finite sets) Four sets A, B, C and D are in analogical proportion A : B :: C : D if and only if A can be transformed into B, and C into D, by adding and subtracting the same elements to A and C.\nThis is the case, for example, of the four sets: A = {t1, t2, t3, t4, }, B = {t1, t2, t3, t5} and C = {t1, t4, t6, t7}, D = {t1, t5, t6, t7}, where t4 has been taken off from, and t5 has been added to A and C, giving B and D."}, {"heading": "2.3.2 SOLVING ANALOGICAL EQUATIONS IN FINITE SETS", "text": "Considering analogy in sets, Lepage (2003) has shown the following theorem, with respect to the axioms of analogy (section 2.1):\nTheorem 2.4 (Solution of an analogical equation in sets) Let A, B and C be three sets. The analogical equation A : B :: C : D where D is the unknown has a solution if and only if the following conditions hold true:"}, {"heading": "A \u2286 B \u222a C and A \u2287 B \u2229 C", "text": "The solution is then unique, given by:\nD = ((B \u222a C)\\A) \u222a (B \u2229 C)\n2.3.3 ANALOGICAL PROPORTIONS IN {0, 1}n\nLet now X be the set {0, 1}n. For each x \u2208 X and each i \u2208 [1, n], fi(x) = 1 (resp. fi(x) = 0) means that the binary feature fi takes the value TRUE (resp. FALSE) on the object x.\nLet A : B :: C : D be an analogical equation. For each feature fi, there are only eight different possibilities of values on A, B and C. We can derive the solutions from the definition and properties of analogy on sets, with the two following principles:\n\u2022 Each feature fi(D) can be computed independently.\n\u2022 The following table gives the solution fi(D):\nfi(A) 0 0 0 0 1 1 1 1 fi(B) 0 0 1 1 0 0 1 1 fi(C) 0 1 0 1 0 1 0 1 fi(D) 0 1 1 ? ? 0 0 1\nIn two cases among the eight, fi(D) does not exists. This derives from the defining of X by binary features, which is equivalent to defining X as a finite set. Theorem 2.4 imposes conditions on the resolution of analogical equations on finite sets, which results in the fact that two binary analogical equations have no solution."}, {"heading": "2.4 Analogical Proportion in Rn", "text": ""}, {"heading": "2.4.1 DEFINITION", "text": "Let O be the origin of Rn. Let a = (a1, a2, . . . , an)\u22ba be a vector of Rn, as defined by its n coordinates. Let a, b, c and d be four vectors of Rn. The interpretation of an analogical proportion a : b :: c : d is usually that a, b, c, d are the corners of a parallelogram, a and d being opposite corners (see Figure 1).\nDefinition 2.5 (Analogical proportion in Rn) Four elements of Rn are in the analogical proportion (a : b :: c : d) if and only if they form a parallelogram, that is when \u2212\u2192 Oa+ \u2212\u2192 Od = \u2212\u2192 Ob+ \u2212\u2192 Oc or equivalently \u2212\u2192 ab = \u2212\u2192 cd or equivalently \u2212\u2192ac = \u2212\u2192 bd\nIt is straightforward that the axioms of analogy, given in section 2.1 are verified by this definition.\n2.4.2 SOLVING ANALOGICAL EQUATIONS IN Rn\nSolving the analogical equation a : b :: c : x , where a, b and c are vectors of Rn and x is the unknown derives directly from the definition of analogy in vector spaces: the four vectors must form a parallelogram. There is always one and only one solution given by the equation:\n\u2212\u2192 Ox = \u2212\u2192 Ob+ \u2212\u2192 Oc\u2212 \u2212\u2192 Oa"}, {"heading": "2.5 Analogical Proportion between Sequences", "text": ""}, {"heading": "2.5.1 NOTATIONS", "text": "A sequence2 is a finite series of symbols from a finite alphabet \u03a3. The set of all sequences is denoted \u03a3\u22c6. For x, y in \u03a3\u22c6, xy denotes the concatenation of x and y. We also denote |x | = n the length of x, and we write x as x = x1 . . . x|x| or x = x[1] . . . x[n], with xi or x[i] \u2208 \u03a3. We denote \u01eb the empty word, of null length, and \u03a3+ = \u03a3\u22c6\\{\u01eb}.\nA factor (or subword) f of a sequence x is a sequence in \u03a3\u22c6 such that there exists two sequences u and v in \u03a3\u22c6 with: x = ufv. For example, abb and bbac are factors of abbacbbaba.\nA subsequence of a sequence x = x1 . . . x|x| is composed of the letters of x with the indices i1 . . . ik, such that i1 < i2 . . . < ik. For example, ca and aaa are two subsequences of abbacbaba."}, {"heading": "2.5.2 DEFINITION", "text": "Let \u03a3 be an alphabet. We add a new letter to \u03a3, that we denote \u223d, giving the augmented alphabet \u03a3\u2032. The interpretation of this new letter is simply that of an \u2018empty\u2019 symbol, that we will need in subsequent sections.\nDefinition 2.6 (Semantic equivalence) Let x be a sequence of \u03a3\u22c6 and y a sequence of \u03a3\u2032\u22c6. x and y are semantically equivalent if the subsequence of y composed of letters of \u03a3 is x. We denote this relation by \u2261.\nFor example, ab \u223d a \u223d a \u2261 abaa. Let us assume that there is an analogy in \u03a3\u2032, i.e. that for every 4-tuple a, b, c, d of letters of \u03a3\u2032, the relation a : b :: c : d is defined as being either TRUE or FALSE.\nDefinition 2.7 (Alignment between two sequences) An alignment between two sequences x, y \u2208 \u03a3\u22c6, of lengths m and n, is a word z on the alphabet (\u03a3\u2032) \u00d7 (\u03a3\u2032) {(\u223d,\u223d)} whose first projection is semantically equivalent to x and whose second projection is semantically equivalent to y.\n2. More classically in language theory, a word or a sentence.\nInformally, an alignment represents a one-to-one letter matching between the two sequences, in which some letters \u223d may be inserted. The matching (\u223d,\u223d) is not permitted. An alignment can be presented as an array of two rows, one for x and one for y, each word completed with some \u223d, resulting in two words of \u03a3\u2032 having the same length.\nFor instance, here is an alignment between x = abgef and y = acde :\nx\u2032\ny\u2032\n=\n=\na | a\nb | c\n\u223d\n| d\ng | \u223d\ne | e\nf | \u223d\nWe can extend this definition to that of an alignment between four sequences.\nDefinition 2.8 (Alignment between four sequences) An alignment between four sequences u, v, w, x \u2208 \u03a3\u2217, is a word z on the alphabet (\u03a3 \u222a {\u223d})4 {(\u223d,\u223d,\u223d,\u223d)} whose projection on the first, the second, the third and the fourth component is respectively semantically equivalent to u, v, w and x.\nThe following definition uses alignments between four sequences.\nDefinition 2.9 (Analogical proportion between sequences) Let u, v, w and x be four sequences on \u03a3\u22c6, on which an analogy is defined. We say that u, v, w and x are in analogical proportion in \u03a3\u22c6 if there exists four sequences u\u2032, v\u2032, w\u2032 and x\u2032 of same length n in \u03a3\u2032, with the following properties:\n1. u\u2032 \u2261 u, v\u2032 \u2261 v, w\u2032 \u2261 w and x\u2032 \u2261 x.\n2. \u2200i \u2208 [1, n] the analogies u\u2032i : v \u2032 i :: w \u2032 i : x \u2032 i hold true in \u03a3 \u2032.\nOne has to note that Lepage (2001) and Stroppa and Yvon (2004) have already proposed a definition of an analogical proportion between sequences with applications to linguistic data. Basically, the difference is that they accept only trivial analogies in the alphabet (such as a : b :: a : b or a :\u223d:: a :\u223d).\nFor example, let \u03a3\u2032 = {a, b, \u03b1, \u03b2,B,C,\u223d} with the non trivial analogies a : b :: A : B , a : \u03b1 :: b : \u03b2 and A : \u03b1 :: B : \u03b2 . The following alignment between the four sequences aBA, \u03b1bBA, ba and \u03b2ba is an analogical proportion on \u03a3\u22c6:\na \u03b1 b \u03b2\n\u223d\nb \u223d\nb\nB B a a\nA A \u223d\n\u223d"}, {"heading": "3. Analogical Dissimilarity", "text": ""}, {"heading": "3.1 Motivation", "text": "In this section, we are interested in defining what could be a relaxed analogy, which linguistic expression would be \u2018a is to b almost as c is to d\u2019. To remain coherent with our previous definitions, we measure the term \u2018almost\u2019 by some positive real value, equal to 0 when the analogy stands true, and increasing when the four objects are less likely to be in analogy. We also want this value, that we call \u2018analogical dissimilarity\u2019 (AD), to have good properties with respect to the analogy. We want it\nto be symmetrical, to stay unchanged when we permute the mean terms of the analogy and finally to respect some triangle inequality. These requirements will allow us, in section 4, to generalize a classical fast nearest neighbor search algorithm and to exhibit an algorithmic learning process which principle is to extract, from a learning set, the 3-tuple of objects that has the least AD when combined with another unknown object. This lazy learning technique is a therefore a generalization of the nearest neighbor method.\nWe firstly study the definition of the analogical dissimilarity on the same structured sets as in the previous sections, and secondly extend it to sequences.\n3.2 A Definition in {0, 1}n\nDefinition 3.1 (Analogical dissimilarity in {0, 1}) The analogical dissimilarity between four binary values is given by the following table:\nu 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 v 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 w 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 x 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\nAD(u, v, w, t) 0 1 1 0 1 0 2 1 1 2 0 1 0 1 1 0\nIn other words, the AD between four binary values is the minimal number of bits that have to be switched in order to produce an analogical proportion. It can be seen as an extension of the edit distance in four dimensions which supports the coherence with analogy.\nDefinition 3.2 (Analogical dissimilarity in {0, 1}n) The analogical dissimilarity AD(u, v, w, t) between four objects u, v, w and t of a finite set X defined by binary features is the sum of the values of the analogical dissimilarities between the features."}, {"heading": "3.2.1 PROPERTIES", "text": "With this definition, the analogical dissimilarity has the following properties:\nProperty 3.1 (Properties of AD in {0, 1}n)\nCoherence with analogy. (AD(u, v, w, x) = 0) \u21d4 u : v :: w : x\nSymmetry for \u2018as\u2019. AD(u, v, w, x) = AD(w, x, u, v)\nExchange of medians. AD(u, v, w, x) = AD(u,w, v, x)\nTriangle inequality. AD(u, v, z, t) \u2264 AD(u, v, w, x) +AD(w, x, z, t)\nAsymmetry for \u2018is to\u2019. In general: AD(u, v, w, x) 6= AD(v, u, w, x)\nThe first properties are quite straightforward from the definition. The demonstration of the third one is simple as well. If the property\nAD(fi(u), fi(v), fi(z), fi(t)) \u2264 AD(fi(u), fi(v), fi(w), fi(x))\n+ AD(fi(w), fi(x), fi(z), fi(t))\nholds true for every 6-tuple of elements and every feature fi, then property (4) is true. The demonstration being done by examining all possible cases: it is impossible to find 6 binary features a, b, c, d, e, f such that AD(a, b, e, f) = 2 and AD(a, b, c, d) + AD(c, d, e, f) < 2. More precisely, if AD(a, b, e, f) = 2, AD(a, b, c, d) + AD(c, d, e, f) is also equal to 2 for all the four values that (c, d) can take."}, {"heading": "3.3 Analogical Dissimilarity in Rn", "text": "The analogical dissimilarity between four vectors must reflect in some way how far they are from constructing a parallelogram. Four vectors u, v, w and x are in analogical proportion (i.e., form a parallelogram) with opposite sides \u2212\u2192uv and \u2212\u2192wx if and only if \u2212\u2192 Ou+ \u2212\u2192 Ox = \u2212\u2192 Ov+ \u2212\u2192 Ow, or equivalently u+ x = v + w, we have chosen the following definition (see Figure 2):\nDefinition 3.3 (Analogical dissimilarity between vectors) The analogical dissimilarity between four vectors u, v, w and x of Rn in which is defined the norm \u2016 \u2016p and the corresponding distance \u03b4p is given by the real positive value AD(u, v, w, x) = \u03b4p(u+ x, v +w) = \u2016(u+ x)\u2212 (v +w)\u2016p. It is also equal to \u03b4p(t, x), where t is the solution of the analogical equation u : v :: w : t.\nProperty 3.2 (Properties of AD between vectors) This definition of analogical dissimilarity in Rn guarantees that the following properties hold true: coherence with analogy, symmetry for \u2018as\u2019, exchange of medians, triangle inequality and asymmetry for \u2018is to\u2019.\nThe first two properties are quite straightforward from the definition. Since \u2016 \u2016p is a norm, it respects the triangle inequality which involves the third property:\nAD(u, v, z, t) \u2264 AD(u, v, w, x) +AD(w, x, z, t)"}, {"heading": "3.4 Analogical Dissimilarity between Sequences", "text": "We present in the following a definition and two algorithms. Firstly, we extend the notion of analogical dissimilarity to sequences. The first algorithm, called SEQUANA4, computes the analogical dissimilarity between four sequences of \u03a3\u22c6. The second one, called SOLVANA, given an analogical equation on sequences, produces the Directed Acyclic Graph (DAG) of all the solutions. If there is no solution, it gives the DAG of all the sentences that have the least analogical dissimilarity when associated with the three known sentences of the equation.\nThese two algorithms are quite general, since they make no particular assumption on the alphabet of the sequences. This alphabet \u03a3 is simply augmented to \u03a3\u2032 = \u03a3 \u222a {\u223d} to produce alignments as described in section 2.5. The analogical dissimilarity on \u03a3\u2032 must be such that: AD(\u223d,\u223d, a, a) = 0, and AD(\u223d, a, b, c) > 0 for every a, b, c \u2208 \u03a3, but no more constraint is required."}, {"heading": "3.4.1 DEFINITION", "text": "Let \u03a3 be a set on which is defined an analogical dissimilarity AD. We augment it to \u03a3\u2032 by adding the special symbol \u223d. We assume now that there is an analogical dissimilarity AD on \u03a3\u2032.\nDefinition 3.4 (Analogical dissimilarity between four sequences) The cost of an alignment between four sequences is the sum of the analogical dissimilarities between the 4-tuples of letters given by the alignment.\nThe analogical dissimilarity AD(u, v, w, x) between four sequences in \u03a3\u22c6 is the cost of an alignment of minimal cost of the four sequences.\nThis definition ensures that the following properties hold true: coherence with analogy, symmetry for \u2018as\u2019, exchange of medians and asymmetry for \u2018is to\u20193.\nDepending on what are we looking for, many methods have been developed for multiples alignment in bio-informatics (Needleman & Wunsch, 1970; Smith & Waterman, 1981) :\n1. For structure or functional similarity like in protein modelization, pattern identification or structure prediction in DNA, methods using simultaneous alignment like MSA (Wang & Jiang, 1994) or DCA (Dress, F\u00fcllen, & Perrey, 1995), or iterative alignment like MUSCLE (Edgar, 2004) are the best.\n2. For Evolutionary similarity like in phylogenic classification, methods using progressive alignment and tree structure, like ClustalW (Thompson, Higgins, & Gibson, 1994), are the most fitted.\nHowever, all of these alignment methods (global or local) are heuristic algorithms to overcome the problem of time and space complexity introduced first by the length of sequences and second by the number of the sequences to align. In our generation problem neither the sequence length which is around 30 characters nor the number of sequences to align which is always four in analogy need a heuristic alignment to speed up the algorithm. But techniques used in bio-informatics to compute automatically the substitution matrix could be very helpful and interesting in handwritten characters recognition. Introducing Gap (Gep, Gop) penalties like in DNA or protein sequences should also be an interesting idea to explore."}, {"heading": "3.5 Computing the Analogical Dissimilarity between Four Sequences: the SEQUANA4 Algorithm", "text": "We compute AD(u, v, w, x) with a dynamic programming algorithm, called SEQUANA4, that progresses in synchronicity in the four sequences to build an optimal alignment.\n3. With this definition of AD, the \u2018triangle inequality\u2019 property is not always true on sequences.\nThe input of this algorithm is the augmented alphabet \u03a3\u2032 on which there an analogical dissimilarity AD(a, b, c, d). The output is the analogical dissimilarity between four sentences of \u03a3\u22c6, namely AD(u, v, w, x).\nWe give below the basics formulas of the recurrence. When implementing the computation, one has to check the correct progression of the indexes i, j, k and l.\nInitialisation\nCu0v0w0x0 \u2190 0 ; for i = 1, |u| do Cuiv0w0x0 \u2190 C ui\u22121v0 w0x0 +AD(ui,\u223d,\u223d,\u223d) done ; for j = 1, |v| do Cu0vjw0x0 \u2190 C u0vj\u22121 w0x0 +AD(\u223d, vj ,\u223d,\u223d) done ;\nfor k = 1, |w| do Cu0v0wkx0 \u2190 C u0v0 wk\u22121x0 +AD(\u223d,\u223d, wk,\u223d) done ; for l = 1, |x| do Cu0v0w0xl \u2190 C u0v0 w0xl\u22121 +AD(\u223d,\u223d,\u223d, xl) done ;\nRecurrence\nC uivj wkxl = Min\n \n\nC ui\u22121vj\u22121 wk\u22121xl\u22121 +AD(ui, vj , wk, xl)\n[ i \u2190 i+ 1; j \u2190 j + 1; k \u2190 k + 1; l \u2190 l + 1 ]\nC ui\u22121vj\u22121 wk\u22121xl +AD(ui, vj , wk,\u223d)\n[ i \u2190 i+ 1; j \u2190 j + 1; k \u2190 k + 1 ]\nC ui\u22121vj\u22121 wkxl\u22121 +AD(ui, vj ,\u223d, xl)\n[ i \u2190 i+ 1; j \u2190 j + 1; l \u2190 l + 1 ]\nC ui\u22121vj\u22121 wkxl +AD(ui, vj ,\u223d,\u223d)\n[ i \u2190 i+ 1; j \u2190 j + 1 ]\nC uivj\u22121 wk\u22121xl\u22121 +AD(\u223d, vj , wk, xl)\n[ j \u2190 j + 1; k \u2190 k + 1; l \u2190 l + 1 ]\nC uivj\u22121 wkxl\u22121 +AD(\u223d, vj ,\u223d, xl)\n[ j \u2190 j + 1; l \u2190 l + 1 ]\nC uivj\u22121 wk\u22121xl +AD(\u223d, vj , wk,\u223d)\n[ i \u2190 i+ 1; k \u2190 k + 1 ]\nC uivj\u22121 wkxl +AD(\u223d, vj ,\u223d,\u223d)\n[ j \u2190 j + 1 ]\nC ui\u22121vj wk\u22121xl\u22121 +AD(ui,\u223d, wk, xl)\n[ i \u2190 i+ 1; j \u2190 j + 1; l \u2190 l + 1 ]\nC ui\u22121vj wkxl\u22121 +AD(ui,\u223d,\u223d, xl)\n[ i \u2190 i+ 1; l \u2190 l + 1 ]\nC ui\u22121vj wk\u22121xl +AD(ui,\u223d, wk,\u223d)\n[ i \u2190 i+ 1; k \u2190 k + 1 ]\nC ui\u22121vj wkxl +AD(ui,\u223d,\u223d,\u223d)\n[ i \u2190 i+ 1 ]\nC uivj wk\u22121xl\u22121 +AD(\u223d,\u223d, wk, xl)\n[ k \u2190 k + 1; l \u2190 l + 1 ]\nC uivj wkxl\u22121 +AD(\u223d,\u223d,\u223d, xl)\n[ l \u2190 l + 1 ]\nC uivj wk\u22121xl +AD(\u223d,\u223d, wk,\u223d)\n[ k \u2190 k + 1 ]\nEnd When i = |u| and j = |v| and k = |w| and l = |x|. Result C\nu|u|v|v| w|w|x|x| is AD(u, v, w, x) in \u03a3\n\u22c6. Complexity This algorithms runs in a time complexity in O ( |u|.|v|.|w|.|x| ) . Correctness The correctness of this algorithm is demonstrated by recurrence, since it uses the dynamic programming principles. It requires only the analogical dissimilarity in \u03a3\u2032 to have the properties that we have called: coherence with analogy, symmetry for \u2018as\u2019 and exchange of medians. The triangle inequality property is not necessary."}, {"heading": "3.6 Generalized Resolution of Analogical Equations in Sequences: the SOLVANA Algorithm", "text": ""}, {"heading": "3.6.1 APPROXIMATE SOLUTIONS TO AN ANALOGICAL EQUATION", "text": "Up to now, we have considered that an analogical equation has either one (or several) exact solutions, or no solution. In the latter case, the concept of analogical dissimilarity is useful to define an approximate solution.\nDefinition 3.5 (Best approximate solution to an analogical equation) Let X be a set on which is defined an analogy and an analogical dissimilarity AD. Let a : b :: c : x be an analogical equation in X . The set of best approximate solutions to this equation is given by:\n{ y : argmin\ny\u2208X AD(a, b, c, y)\n}\nIn other words, the best approximate solutions are the objects y \u2208 X that are the closest to be in analogical proportion with a, b and c. Obviously, this definition generalizes that of a solution to an analogical equation given at section 2.2. Since we have defined AD with good properties on several alphabets and on sequences on these alphabets, we can compute an approximate solution to analogical equations in all these domains.\nWe can easily enlarge this concept and define the set of the k-best solutions to the analogical equation a : b :: c : x . Informally, it is any subset of k elements of X which have a minimal AD when associated in fourth position with a, b and c.\nIn Rn and {0, 1}n, there is only one best approximate solution to an analogical equation, which can be easily computed (see sections 3.2 and 3.3). Finding the set of the k-best solutions is also a simple problem.\nLet us turn now to an algorithm which finds the set of the best approximate solutions to the equation u : v :: w : x when the objects are sequences on an alphabet on which an AD has been defined. We will also make some comments to extend its capacity to find the set of the k-best solutions."}, {"heading": "3.6.2 THE SOLVANA ALGORITHM", "text": "This algorithm uses dynamic programming to construct a 3-dimensional array. When the construction is finished, a backtracking is performed to produce the DAG of all the best solutions.\nAn alignment of four sequences of different lengths is realized by inserting letters \u223d so that all the four sequences have the same length. Once this is done, we consider in each column of the alignment the analogical dissimilarity in the augmented alphabet.\nWe construct a three dimensional n1 \u00d7 n2 \u00d7 n3 matrix M (respectively the length of the first, second and third sequences A, B and C of the analogical equation \u2018A is to B as C is to x\u2019). To find the fourth sequence, we fill up M with the following recurrence:\nM [i, j, k] 1\u2264i,j,k\u2264n1,n2,n3 = Min\n \n\nM [i\u2212 1, j \u2212 1, k \u2212 1] +Min x\u2208\u03a3\u2032 AD(ai, bj , ck, x) M [i, j \u2212 1, k \u2212 1] +Min x\u2208\u03a3\u2032 AD(\u223d, bj , ck, x) M [i, j, k \u2212 1] +Min x\u2208\u03a3\u2032 AD(\u223d,\u223d, ck, x) M [i, j \u2212 1, k] +Min x\u2208\u03a3\u2032 AD(\u223d, bj ,\u223d, x) M [i\u2212 1, j, k \u2212 1] +Min x\u2208\u03a3\u2032 AD(ai,\u223d, ck, x) M [i\u2212 1, j \u2212 1, k] +Min x\u2208\u03a3\u2032 AD(ai, bj ,\u223d, x) M [i\u2212 1, j, k] +Min x\u2208\u03a3\u2032 AD(ai,\u223d,\u223d, x)\nai is the ith object of the sequence A. \u03a3\u2032 = \u03a3 \u222a {\u223d}.\nAt each step, we save in the cell M [i, j, k] not only the cost but also the letter(s) found by analogical resolution along the optimal way of progression. When M is completed, a backward propagation gives us all the optimal generated sequences with the same optimal analogical dissimilarity, strucured as a DAG. The computational complexity of this algorithm is O(m \u2217 n3), where m = Card(\u03a3\u2032) and n is the maximum length of sequences"}, {"heading": "3.6.3 EXAMPLE", "text": "Let \u03a3 = {a, b, c, A,B,C} be an alphabet defined by 5 binary features, as follows:\nf1 f2 f3 f4 f5 a 1 0 0 1 0 b 0 1 0 1 0 c 0 0 1 1 0 A 1 0 0 0 1 B 0 1 0 0 1 C 0 0 1 0 1 \u223d 0 0 0 0 0\nThe first three features indicates what is the letter (for example, f1 is true on a and A only) and the last two indicate the case of the letter (f4 holds true for lower case letters, f5 for upper case letters).\nFor example, let ab : Bc :: Bc : x be an analogical equation. There is no exact solution, but six best approximate solutions y such that AD(ab,Bc,Bc, y) = 4, for example y = BB or y = Cc. Figure 3 displays the DAG of the results produced by SOLVANA on this example."}, {"heading": "4. Analogical Dissimilarity and Machine Learning", "text": ""}, {"heading": "4.1 Motivation", "text": "We assume here that there exists an analogy defined on the set X and an analogical dissimilarity AD with the following properties: coherence with analogy, symmetry for \u2018as\u2019, triangle inequality, exchange of medians and asymmetry for \u2018is to\u2019.\nLet S be a set of elements of X , which is of cardinality m, and let y be another element of X with y 6\u2208 S. The problem that we tackle in this section is to find the triple of objects (u, v, w) in S\nsuch that:\nAD(u, v, w, y) = argmin t1,t2,t3\u2208S AD(t1, t2, t3, y)\nThis will directly lead us to use the notion of AD in supervised machine learning, e.g. of a classification rule."}, {"heading": "4.2 The Brute Force Solution", "text": "An obvious solution is to examine all the triples in S. This brute force method requires m3 calls to a procedure computing the analogical dissimilarity between four objects of X . According to the properties of analogical dissimilarity, this number can actually be divided by 8, but it does not change the theoretical and practical complexity of the search.\nThe situation is similar to that of the search for the nearest neighbor in Machine Learning, for which the naive algorithm requires m distance computations. Many proposals have been made to decrease this complexity (see for example the work of Ch\u00e1vez, Navarro, Baeza-Yates, & Marroqu\u00edn, 2001). We have chosen to focus on an extension of the AESA algorithm, based on the property of triangle inequality for distances (Mic\u00f3, Oncina, & Vidal, 1994). Since we have defined the concept of analogical dissimilarity with a similar property, it is natural to explore how to extend this algorithm."}, {"heading": "4.3 \u2018FADANA\u2019: FAst search of the least Dissimilar ANAlogy", "text": "This section describes a fast algorithm to find, given a set of objects S of cardinalty m and an object y, the three objects (z\u22c6, t\u22c6, x\u22c6) in S such that the analogical dissimilarity AD(z\u22c6, t\u22c6, x\u22c6, y) is minimal. It is based on the AESA technique, which can be extended to analogical dissimilarity. Thanks to its properties, an analogical dissimilarity AD(z, t, x, y) can be seen as a distance between the two couples (z, t) and (x, y), and consequently we will basically work on couples of objects. We use equivalently in this paragraph the terms \u2018(analogical) distance between the two couples (u, v) and (w, x)\u2019 and \u2018(analogical) dissimilarity between the four elements u, v, w and x\u2019 to describe AD(u, v, w, x)."}, {"heading": "4.3.1 PRELIMINARY COMPUTATION", "text": "In this part, which is done off line, we have to compute the analogical dissimilarity between every four objects in the data base. This step has a complexity in time and space of O(m4), where m is the size of S. We will come back to this point in section 4.4, where we will progress from an AESA-like to a LAESA-like technique and reduce the computational complexity."}, {"heading": "4.3.2 PRINCIPLE OF THE ALGORITHM", "text": "The basic operation is to compose a couple of objects by adding to y an object xi \u2208 S where i = 1,m. The goal is now to find the couple of objects in S having the lowest distance with (xi, y), then to change xi into xi+1. Looping m times on an AESA-like select and eliminate technique insures to finally find the triple in S having the lowest analogical dissimilarity when associated with y."}, {"heading": "4.3.3 NOTATIONS", "text": "Let us denote:\n\u2022 C the set of couples (u, v) which distance to (xi, y) has already been computed.\n\u2022 \u03b4 = argmin (z,t)\u2208U (AD(z, t, xi, y))\n\u2022 \u03b4i = argmin (z,t)\u2208U ,1\u2264j\u2264i (AD(z, t, xi, y))\n\u2022 Dist = {AD(z, t, xi, y), (z, t) \u2208 C}\n\u2022 Dist(j) the jth element of Dist\n\u2022 QuadU = {(z, t, xi, y), (z, t) \u2208 C}\n\u2022 QuadU (j) the jth element of QuadU\nThe algorithm is constructed in the three following phases:"}, {"heading": "4.3.4 INITIALIZATION", "text": "Each time that xi changes (when i is increased by 1), the set U is refilled with all the possible couples of objects \u2208 S.\nThe set C and Dist which contain respectively the couples and the distances to (xi, y) that have been measured during one loop, are initialized as empty sets.\nThe local minimum Min, containing the minimum of analogical dissimilarities of one loop is set to infinity.\nk = Card(C) represents the number of couples where the distance have been computed with (xi, y) in the current loop. k is initialized to zero.\nAlgorithm 1 Algorithm FADANA: initialization. begin U \u2190 {(xi, xj), i = 1,m and j = 1,m}; C \u2190 \u2205; Min \u2190 +\u221e; Dist \u2190 \u2205; k \u2190 0; end"}, {"heading": "4.3.5 SELECTION", "text": "The goal of this function is to extract from the set U the couple (zz, tt) that is the more promising in terms of the minimum analogical dissimilarity with (xi, y), using the criterion:\n(zz, tt) = argmin (u,v)\u2208U Max (z,t)\u2208C\n\u2223 \u2223 AD(u, v, z, t)\u2212AD(z, t, xi, y) \u2223 \u2223\nAlgorithm 2 Algorithm FADANA: selection of the most promising couple. selection(U , C, (xi, y),Dist) begin s \u2190 0 for i = 1, Card(U) do\nif s \u2264 \u2211\nj\u2208C |AD(zj , tj , ui, vi)\u2212Dist(j)| then s \u2190 \u2211\nj\u2208C |AD(zj , tj , ui, vi)\u2212Dist(j)|; argmin \u2190 i;\nend if end for Return (uarg min, varg min); end"}, {"heading": "4.3.6 ELIMINATION", "text": "During this section all the couples (u, v) \u2208 U where the analogical distance with (xi, y) can not be less than what we already found are eliminated thanks to the two criteria below:\nAD(u, v, z, t) \u2264 AD(z, t, y, xi)\u2212 \u03b4 \u21d2 AD(u, v, xi, y) \u2265 \u03b4\nand AD(u, v, z, t) \u2265 AD(z, t, y, xi) + \u03b4 \u21d2 AD(u, v, xi, y) \u2265 \u03b4\nwhere \u03b4 = AD(z\u22c6, t\u22c6, x\u22c6, y) represents the minimum analogical dissimilarity found until now (see figure 4). Note that \u03b4 is updated during the whole algorithm and is never reinitialized when i is increased.\nAlgorithm 3 Algorithm FADANA: elimination of the useless couples. eliminate(U , C, (xi, y), \u03b4, k) (zk, tk) is the kth element of QuadU begin for i = 1, Card(U) do\nif AD(zk, tk, ui, vi) \u2264 Dist(k) + \u03b4 then U \u2190 U \u2212 {(ui, vi)}; C \u2190 C \u222a {(ui, vi)}; else if AD(zk, tk, ui, vi) \u2265 Dist(k)\u2212 \u03b4 then U \u2190 U \u2212 {(ui, vi)}; C \u2190 C \u222a {(ui, vi)};\nend if end for end\nAlgorithm 4 Algorithm FADANA: main procedure. begin S \u2190 {xi, i = 1,m}; AD\u22c6 \u2190 +\u221e; for i = Card(S) do Initialize; while U 6= \u2205 do\n(z, t) \u2190 selection(U , C, (xi, y),Dist); Dist(k) \u2190 AD(z, t, xi, y); k = k + 1; U \u2190 U \u2212 {(z, t)}; C \u2190 C \u222a {(z, t)}; if Dist(k) \u2265 Min then eliminate(U , C, (xi, y), \u03b4, k) else Min \u2190 Dist(k); if Dist(k) < AD\u22c6 then\nAD\u22c6 \u2190 Dist(k); z\u22c6 \u2190 z, t\u22c6 \u2190 t, x\u22c6 \u2190 xi;\nend if for k = 1, Card(C) do eliminate(U , C, (xi, y), \u03b4, k)\nend for end if\nend while end for The best triple in S is (z\u22c6, t\u22c6, x\u22c6) ; The least analogical dissimilarity is AD\u22c6 = AD(z\u22c6, t\u22c6, x\u22c6, y) ; end"}, {"heading": "4.4 Selection of Base Prototypes in FADANA", "text": "So far, FADANA has the drawback of requiring a precomputing time and storage in O(m4), which is in practice impossible to handle for m > 100.\nTo go further, we have devised an ameliorated version of the FADANA algorithm, in which the preliminary computation and storage is limited to N.m2, where N is a certain number of couples of objects. The principle is similar to that of LAESA (Mic\u00f3 et al., 1994). N base prototypes couples are selected among the m2 possibilities through a greedy process, the first one being chosen at random, the second one being as far as possible from the first one, and so on. The distance between couples of objects is, according to the definition of the analogical dissimilarity:\n\u03b4 ( (x, y), (z, t) ) = AD(z, t, x, y)"}, {"heading": "4.5 Efficiency of FADANA", "text": "We have conducted some experiments to measure the efficiency of FADANA. We have tested this algorithm on four databases from the UCI Repository (Newman, Hettich, Blake, & Merz, 1998), by noting each time the percentage of AD computed in-line for different numbers of base prototypes compared to those made by the naive method (see Figure 5, the scales are logarithmic). The number of base prototypes is expressed as percentage on the learning set. Obviously, if the learning set contains m elements, the number of possible 3-tuples that can be built is m3. This point explains why the percentage of base prototypes compared to the size of the learning set can rise above 100%. The number of in-line computations of the AD is the mean over the test set.\nWe observe in these results that the optimal number of base prototypes is between 10% and 20% if we aim to optimize the computation time performance."}, {"heading": "5. Two Applications in Machine Learning Problems", "text": ""}, {"heading": "5.1 Classification of Objects Described by Binary and Nominal Features", "text": "The purpose of this first experiment is to measure the benefit of analogical dissimilarity applied to a basic problem of classification, compared to standard classifiers such k-nearest neighbors, neural networks, and decision trees. In this benchmarking, we are not yet interested in classifying sequences, but merely to investigate what the basic concept of analogical dissimilarity can bring to the learning of a classification rule for symbolic objects."}, {"heading": "5.1.1 METHOD DESCRIPTION", "text": "Let S = {( oi, h(oi) ) | 1 \u2264 i \u2264 m } be a learning set, where h(oi) is the class of the object oi. The objects are defined by binary attributes. Let x be an object not in S. The learning problem is to find the class of a new object x, using the learning set S. To do this, we define a learning rule based on the concept of analogical dissimilarity depending on an integer k, which could be called the k least dissimilar 3-tuple rule.\nThe basic principle is the following: among all the 3-tuples (a, b, c) in S3, we consider the subset of those which produce the least analogical dissimilarity when associated with x (the FADANA algorithm is used here). For a part of them, the analogical equation h(a) : h(b) :: h(c) : g has an exact solution in the finite set of the classes. We keep only these 3-tuples and we choose the class which takes the majority among these values g as the class for x.\nMore precisely, the procedure is as follows:\n1. Compute the analogical dissimilarity between x and all the n 3-tuples in S which produce a solution for the class of x.\n2. Sort these n 3-tuples by the increasing value of their AD when associated with x.\n3. If the k-th object has the value p, then let k\u2032 be the greatest integer such that the k\u2032-th object has the same value p.\n4. Solve the k\u2032 analogical equations on the label of the class. Take the winner of the votes among the k\u2032 results.\nTo explain, we firstly consider the case where there are only two classes \u03c90 and \u03c91. An example with 3 classes will follow.\nPoint 1 means that we retain only the 3-tuples which have one of the four4 configurations for their class displayed in Table 1. We ignore the 3-tuples that do not lead to an equation with a trivial solution on classes:\n4. There are actually two more, each one equivalent to one of the four (by exchange of the means objects).\nh(a) : h(b) :: h(c) : h(x) \u03c90 : \u03c91 :: \u03c91 : ? \u03c91 : \u03c90 :: \u03c90 : ?\nExample Let S = {(a, \u03c90), (b, \u03c90), (c, \u03c91), (d, \u03c91), (e, \u03c92)} be a set of five labelled objects and let x 6\u2208 S be some object to be classified. According to the analogical proportion axioms, there is only 75 (= (Card(S)3 + Card(S)2)/2) non-equivalent analogical equations among 125(= Card(S)3) equations that can be formed between three objects from S and x. Table (2) shows only the first 14 lines after sorting with regard to some arbitrarily analogical dissimilarity. The following table gives the classification of an object x according to k:\nk 1 2 3 4 5 6 7 k\u2032 1 3 3 5 5 7 7\nclassification of x 1 1 1 ? ? 2 2"}, {"heading": "5.1.2 WEIGHTING THE ATTRIBUTES", "text": "The basic idea in weighting the attributes is that they do not have the same importance in the classification, and that more importance has to be given to the most discriminative. The idea of selecting or enhancing interesting attributes is classical in Machine Learning, and not quite new in the framework of analogy. In a paper of Turney (2005), a discrimination is done by keeping the most frequent patterns in words. Therefore, a greater importance is given to the attributes that are actually discriminant. However, in an analogical classification system, there are several ways to find the class of the unknown element. Let us take again the preceding two class problem example (see table 1) to focus on this point.\nWe notice that there are two ways to decide between the class \u03c90 and the class \u03c91 (there is also a third possible configuration which is equivalent to the second by exchange of the means). We therefore have to take into account the equation used to find the class. This is why we define a set of weights for each attribute, depending on the number of classes. These sets are stored in what we call an analogical weighting matrix.\nDefinition 5.1 An analogical weighting matrix (W ) is a three dimensional array. The first dimension is for the attributes, the second one is for the class of the first element in an analogical proportion and the third one is for the class of the last element in an analogical proportion. The analogical proportion weighting matrix is a d\u00d7 C \u00d7 C matrix, where d is the number of attributes and C is the number of classes.\nFor a given attribute ak of rank k, the element Wkij of the matrix indicates which weight must be given to ak when encountered in an analogical proportion on classes whose first element is \u03c9i, and for which \u03c9j is computed as the solution.\nHence, for the attribute ak:\nLast element (decision)\nFirst element class \u03c9i class \u03c9j class \u03c9i Wkii Wkij class \u03c9j Wkji Wkjj\nSince we only take into account the 3-tuples that give a solution on the class decision, all the possible situations are of one of the three patterns:\nPossible patterns First Decision element class \u03c9i : \u03c9i :: \u03c9j : \u03c9j \u03c9i \u03c9j \u03c9i : \u03c9j :: \u03c9i : \u03c9j \u03c9i \u03c9j \u03c9i : \u03c9i :: \u03c9i : \u03c9i \u03c9i \u03c9i\nThis observation gives us a way to compute the values Wkij from the learning set."}, {"heading": "5.1.3 LEARNING THE WEIGHTING MATRIX FROM THE TRAINING SAMPLE", "text": "The goal is now to fill the three dimensional analogical weighting matrix using the learning set. We estimate Wkij by the frequency that the attribute k is in an analogical proportion with the first element class \u03c9i, and solves in class \u03c9j .\nFirstly, we tabulate the splitting of every attribute ak on the classes \u03c9i:\n. . . class \u03c9i . . . ak = 0 . . . n0i . . . ak = 1 . . . n1i . . .\nwhere ak is the attribute k and n0i (resp. n1i) is the number of objects in the class i that have the value 0 (resp. 1) for the binary attribute k. Hence,\n\u22111 k=0 \u2211C i=1 nki = m (the number of objects\nin the training set). Secondly, we compute Wkij by estimating the probability to find a correct analogical proportion on attribute k with first element class \u03c9i which solves in class \u03c9j . In the following table we show all the possible ways of having an analogical proportion on the binary attribute k. 0i (resp. 1i) is the 0 (resp. 1) value of the attribute k that has class \u03c9i.\n1st 0i : 0i :: 0j : 0j 4th 1i : 1i :: 1j : 1j 2sd 0i : 1i :: 0j : 1j 5th 1i : 0i :: 1j : 0j 3rd 0i : 0i :: 1j : 1j 6th 1i : 1i :: 0j : 0j\nPk(1 st) estimates the probability that the first analogical proportion in the table above occurs.\nPk(1 st) = n0in0in0jn0j/m 4\n...\nFrom Wkij = Pk(1st) + \u00b7 \u00b7 \u00b7+ Pk(6th), we compute\nWkij = ( (n20i + n 2 1i)(n 2 0j + n 2 1j) + 2 \u2217 n0in0jn1in1j ) /(6 \u2217m4)\nThe decision algorithm of section 5.1.1 is only modified at point 1, which turns into Weighted Analogical Proportion Classifier (WAPC):\n\u2022 Given x, find all the n 3-tuples in S which can produce a solution for the class of x. For every 3-tuple among these n, say (a, b, c), consider the class \u03c9i of the first element a and the class \u03c9j of the solution. Compute the analogical dissimilarity between x and this 3-tuple with the weighted AD:\nAD(a, b, c, x) =\nd\u2211\nk=1\nWkijAD(ak, bk, ck, xk)\nOtherwise, if point 1 is not modified, the method is called Analogical Proportion Classifier (APC)."}, {"heading": "5.1.4 EXPERIMENTS AND RESULTS", "text": "We have applied the weighted analogical proportion classifier (WAPC) to eight classical data bases, with binary and nominal attributes, of the UCI Repository.\nMONK 1,2 and 3 Problems (MO.1, MO.2 and MO.3), MONK3 problem has noise added. SPECT heart data (SP.). Balance-Scale (B.S) and Hayes Roth (H.R) database, both multiclass database. Breast-W (Br.) and Mushroom (Mu.), both data sets contain missing values. kr-vs-kp Kasparov vs Karpov (k.k.).\nIn order to measure the efficiency of WAPC, we have applied some standard classifiers to the same databases, and we have also applied APC to point out the contribution of the weighting matrix (Sect.5.1.2). We give here the parameters used for the comparison method in Table 3:\n\u2022 Decision Table: the number of non improving decision tables to consider before abandoning the search is 5.\n\u2022 Id3: unpruned decision tree, no missing values allowed.\n\u2022 Part: partial C4.5 decision tree in each iteration and turns the \u2018best\u2019 leaf into a rule, One-pervalue encoding.\n\u2022 Multi layer Perceptron: back propagation training, One-per-value encoding, one hidden layer with (# classes + # attributes)/2 nodes.\n\u2022 LMT (\u2019logistic model trees\u2019): classification trees with logistic regression functions at the leaves, One-per-value encoding.\n\u2022 IB1: Nearest-neighbor classifier with normalized Euclidean distance, which have better results than IB10.\n\u2022 JRip: propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), optimized version of IREP. .\nWe have worked with the WEKA package (Witten & Frank, 2005), choosing 6 different classification rules on the same data. Some are well fit to binary data, like ID3, PART, Decision Table. Others, like IB1 or Multilayer Perceptron, are more adapted to numerical and noisy data.\nThe results are given in Table 3. We have arbitrarily taken k = 100 for our two rules. The value k is not very sensitive in the case of nominal and binary data and on small databases such as the ones that are used in the experiments (see Figure 6). However, it is possible to set k using a validation set.\nWe draw the following conclusions from this study: firstly, according to the good classification rate of WAPC in Br. and Mu. databases, we can say that WAPC handles the missing values well. Secondly, WAPC seems to belong to the best classifiers for the B.S and H.R databases,\nwhich confirms that WAPC deals well with multiclass problems. Thirdly, as shown by the good classification rate of WAPC in the MO.3 problem, WAPC handles well noisy data. Finally, the results on MO. and B.S database are exactly the same with the weighted decision rule WAPC than with APC. This is due to the fact that all AD that are computed up to k = 100 are of null value. But on the other data bases, the weighting is quite effective. Unfortunatly, the last database show that WAPC have a poor recognition rate on some databases, which means that analogy do not fit all classification problems."}, {"heading": "5.2 Handwritten Character Recognition: Generation of New Examples", "text": ""}, {"heading": "5.2.1 INTRODUCTION", "text": "In a number of Pattern Recognition systems, the acquisition of labeled data is expensive or user unfriendly process. For example, when buying a smartphone equipped with a handwritten recognition system, the customer is not likely to write dozens of examples of every letter and digit in order to provide the system with a consequent learning sample. However, to be efficient, any statistical classification system has to be retrained to the new personal writing style or the new patterns with as many examples as possible, or at least a sufficient number of well chosen examples.\nTo overcome this paradox, and hence to make possible the learning of a classifier with very few examples, a straightforward idea is to generate new examples by randomly adding noise to the elements of a small learning sample. In his recent book, Bishop (2007) gives no theoretical coverage of such a procedure, but rather draws a pragmatic conclusion: \u2018 . . . the addition of random noise to the inputs . . . has been shown to improve generalization in appropriate circumstances\u2019.\nAs far as character recognition is concerned, generating synthetic data for the learning of a recognition system has mainly be used with offline systems (which process an image of the char-\nacter). For offline character recognition, several image distortions have however been used (Cano, P\u00e9rez-Cortes, Arlandis, & Llobet, 2002): slanting, shrinking, ink erosion and ink dilatation. For online character recognition, several online distortions have been used, such as speed variation and angular variation (Mouch\u00e8re, Anquetil, & Ragot, 2007).\nWe therefore are interested in the quick tuning of a handwritten character recognition to a new user, and we consider that only a very small set of examples of each character (typically 2 or 3) can be required from the new user. As we learn a writer-dependent system, the synthetic data have to keep the same handwriting style as the original data."}, {"heading": "5.2.2 ANALOGY BASED GENERATION", "text": "In this second experiment, we are interested in handwritten characters, which are captured online. They are represented by a sequence of letters of \u03a3, where \u03a3 = {1, 2, ..., 16, 0, C, ..., N} is the alphabet of the Freeman symbols code augmented of symbols for anchorage points. These anchorage points come from an analysis of the stable handwriting properties, as defined in (Mouch\u00e8re et al., 2007): pen-up/down, y-extrema, angular points and in-loop y-extrema.\nHaving a learning set that contains a few examples of each letter, we generate synthetic examples by analogical proportion as described in section 3.6 (see Figure 7). Hence, by generating artificial examples of the letter f by analogical proportion using only three instances we augment the learning set with new and different examples as shown in the following pictures.\n\ufe38 \ufe37\ufe37 \ufe38\nOriginal letters\n=\u21d2 \ufe38 \ufe37\ufe37 \ufe38\nAnalogy based generated letters"}, {"heading": "5.2.3 EXPERIMENTS", "text": "In this section we show that our generation strategies improves the recognition rate of three classical classifiers learned with few data.\nExperimental Protocol In the data base that we use (Mouch\u00e8re et al., 2007), twelve different writers have written 40 times the 26 lowercase letters (1040 characters) on a PDA. We use a 4-fold\nstratified cross validation. The experiments are composed of two phases in which three writerdependent recognition systems are learned: a Radial Basis Function Network (RBFN), a K-Nearest Neighbor (K-NN) and a one-against-all Support Vector Machine (SVM).\nFirstly, we compute two Reference recognition Rates without data generation: RR10 which is the recognition rate achievable with 10 original characters without character generation and RR30 gives an idea of achievable recognition rates with more original data. Practically speaking, in the context of on the fly learning phase we should not ask the user to input more than 10 characters per class.\nSecondly the artificial character generation strategies are tested. For a given writer, one to ten characters per class are randomly chosen. Then 300 synthetic characters per class are generated to make a synthetic learning database. This experiment is done 3 times per cross validation split and per writer (12 times per user). The mean and the standard deviation of these 12 performance rates are computed. Finally the means of these measurements are computed to give a writer dependent mean recognition rate and the associated standard deviation.\nWe study three different strategies for the generation of synthetic learning databases. The strategy \u2019Image Distortions\u2019 chooses randomly for each generation one among several image distortions. In the same way the strategy \u2019Online and Image Distortions\u2019 chooses randomly one distortion among the image distortions and online distortions. The \u2019Analogy and Distortions\u2019 strategy generates two-thirds of the base with the previous strategy and the remaining third with AP generation.\nResults Figure 8 compares the recognition rates achieved by the three generation strategies for the three classifiers. Firstly we can note that the global behavior is the same for the three classifiers. Thus the following conclusions do not depend on the classifier type. Secondly the three generation strategies are complementary because using \u2019Online and Image Distortions\u2019 is better than \u2019Image Distortions\u2019 alone and \u2019Analogy and Distortions\u2019 is better than using distortions. Furthermore using only four original character with the complete generation strategy is better than the RR10. The RR30 is achieved by using 9 or 10 original characters. Thus we can conclude that using our generation strategies learns classifier with very few original data as efficiently as using original data from a long input phase : we need about three times fewer original data to achieve the same recognition rate.\nComparing \u2018Image Distortions\u2019, \u2018Online Distortions\u2019 and \u2018Analogy\u2019 alone shows that \u2018Analogy\u2019 is less efficient than the ad-hoc methods. Nevertheless, generating sequences by approximate analogical proportion is meaningful and somewhat independant of classical distorsions. In other words, the analogy of character shapes, which is used in \u2018natural intelligence\u2019, has been somehow captured by our definition and algorithms.\nOur aim here is to know if the difference of the average of the three methods is significant. We have performed two methods of validation to evaluate the difference between two stategies. The first method is parametric: the T-TEST (Gillick & Cox, 1989). The second method is non-parametric: the SIGN TEST (Hull, 1993). In both methods, the comparaison is between the first and the second strategy then between the second and the third strategy on each number of original characters.\nThe T-TEST compares the value of the difference between the two generation methods regarding to the variation between the differences. The assumption is that the errors are in a normal distribution and that the errors are independent. If the mean difference is large comparing to the standard deviation, the two strategies are statistically different. In our case, the probability that our results are a random artefact is less than 10\u221212.\nThe SIGN TEST is non-parametric comparison method. Its benefit is to avoid assumptions on the normal distribution of the observations and the errors. This test replaces each difference by the sign of this difference. The sum of these occurrences is compared to the value of the hypothesis H0 (H0: the difference between the methods is not significant). Thus if a strategy is frequently better than the expected mean, then this strategy is significantly better. In our case, the probability that the hypothesis H0 is true is less than 10\u221230. Hence, the difference is significantly better."}, {"heading": "6. Conclusions and Future Work", "text": "In this article, we have investigated a formal notion of analogy between four objects in the same universe. We have given definitions of analogy, formulas and algorithms for solving analogical equations in some particular sets. We have given a special focus on objects structured as sequences, with an original definition of analogy based on optimal alignments. We also have introduced, in a coherent manner, the new notion of analogical dissimilarity, which quantifies how far four objects are from being in analogy. This notion is useful for lazy supervised learning: we have shown how the time consuming brute force algorithm could be ameliorated by generalizing a fast nearest neighbor search algorithm, and given a few preliminary experiments. However, much is left to be done, and we want especially to explore further the following questions:\n\u2022 What sort of data are particularly suited for lazy learning by analogy? We know from the bibliography that linguistic data have been successfully processed with learning by analogy techniques, in fields such as grapheme to phoneme transcription, morphology, translation. We are currently working on experiments on phoneme to grapheme transcription, which can be useful in some special cases in speech recognition (for proper names, for example). We also are interested on other sequential real data, such as biosequences, in which the analogical reasoning technique is (rather unformally) presently already used. The selection of the data and of the supervision are equally important, since both the search of the less dissemblant analogic triple and the labelling process are based on the same concept of analogy.\n\u2022 What sort of structured data can be processed? Sequences can naturally be extended to ordered trees, in which several generalizations of alignments have already been defined. This could be useful, for example, in extending the nearest neighbor technique in learning prosodic trees for speech synthesis (Blin & Miclet, 2000). We could also imagine that sequences models, like Hidden Markov Models (HMM) could be combined through an analogical construction.\n\u2022 What sort of algorithms can be devised to let large amount of data be processed by such techniques? We have given a first answer with the FADANA algorithm, and we believe that the quality of the results can be still increased. More experiments remain to be done with this type of algorithm. We have to notice also that not all the properties of analogical dissimilarity have been used so far. We believe that an algorithm with a precomputing and a storage in O(m) can be devised, and we are currently working on it.\nIn conclusion, we are confident in the fact that the new notion of analogical dissimilarity and the lazy learning technique that we have associated with it can be extended to more real data, other structures of data and larger problems."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous referrees for their constructive and detailed comments on the first version of this article."}], "references": [{"title": "Case-based reasoning: Foundational issues, methodological variations, and system approaches", "author": ["A. Aamodt", "E. Plaza"], "venue": "Artificial Intelligence Communications,", "citeRegEx": "Aamodt and Plaza,? \\Q1994\\E", "shortCiteRegEx": "Aamodt and Plaza", "year": 1994}, {"title": "Learning by analogy : a classification rule for binary and nominal data", "author": ["S. Bayoudh", "L. Miclet", "A. Delhay"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bayoudh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bayoudh et al\\.", "year": 2007}, {"title": "Learning a classifier with very few examples: analogy based and knowledge based generation of new examples for character", "author": ["S. Bayoudh", "H. Mouch\u00e8re", "L. Miclet", "E. Anquetil"], "venue": null, "citeRegEx": "Bayoudh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bayoudh et al\\.", "year": 2007}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Springer.", "citeRegEx": "Bishop,? 2007", "shortCiteRegEx": "Bishop", "year": 2007}, {"title": "Generating synthetic speech prosody with lazy learning in tree structures", "author": ["L. Blin", "L. Miclet"], "venue": "In Proceedings of CoNLL-2000 : 4th Conference on Computational Natural Language Learning,", "citeRegEx": "Blin and Miclet,? \\Q2000\\E", "shortCiteRegEx": "Blin and Miclet", "year": 2000}, {"title": "Graph Matching in Pattern Recognition and Machine Vision, Special Issue of International Journal of Pattern Recognition and Artificial Intelligence", "author": ["H. Bunke", "T. Caelli"], "venue": "World Scientific", "citeRegEx": "Bunke and Caelli,? \\Q2004\\E", "shortCiteRegEx": "Bunke and Caelli", "year": 2004}, {"title": "Training set expansion in handwritten character recognition", "author": ["J. Cano", "J. P\u00e9rez-Cortes", "J. Arlandis", "R. Llobet"], "venue": "In 9th Int. Workshop on Structural and Syntactic Pattern Recognition,", "citeRegEx": "Cano et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cano et al\\.", "year": 2002}, {"title": "Searching in metric spaces", "author": ["E. Ch\u00e1vez", "G. Navarro", "R. Baeza-Yates", "Marroqu\u00edn", "J.-L"], "venue": "ACM Comput. Surv.,", "citeRegEx": "Ch\u00e1vez et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ch\u00e1vez et al\\.", "year": 2001}, {"title": "Apprentissage artificiel : concepts et algorithmes", "author": ["A. Cornu\u00e9jols", "L. Miclet"], "venue": null, "citeRegEx": "Cornu\u00e9jols and Miclet,? \\Q2002\\E", "shortCiteRegEx": "Cornu\u00e9jols and Miclet", "year": 2002}, {"title": "Abstraction considered harmful: lazy learning of language processing", "author": ["W. Daelemans"], "venue": "den Herik, H. J. V., & Weijters, A. (Eds.), Proceedings of the sixth Belgian-Dutch Conference on Machine Learning, pp. 3\u201312, Maastricht, The Nederlands.", "citeRegEx": "Daelemans,? 1996", "shortCiteRegEx": "Daelemans", "year": 1996}, {"title": "Analogical projection in pattern perception", "author": ["M. Dastani", "B. Indurkhya", "R. Scha"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence,", "citeRegEx": "Dastani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dastani et al\\.", "year": 2003}, {"title": "Analogical equations in sequences : Definition and resolution", "author": ["A. Delhay", "L. Miclet"], "venue": "In International Colloquium on Grammatical Induction,", "citeRegEx": "Delhay and Miclet,? \\Q2004\\E", "shortCiteRegEx": "Delhay and Miclet", "year": 2004}, {"title": "A divide and conquer approach to multiple alignment", "author": ["A.W.M. Dress", "G. F\u00fcllen", "S. Perrey"], "venue": "In ISMB,", "citeRegEx": "Dress et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dress et al\\.", "year": 1995}, {"title": "Muscle: a multiple sequence alignment method with reduced time and space complexity", "author": ["R. Edgar"], "venue": "BMC Bioinformatics, 5(1), 113.", "citeRegEx": "Edgar,? 2004", "shortCiteRegEx": "Edgar", "year": 2004}, {"title": "The structure-mapping engine: Algorithm and examples", "author": ["B. Falkenhainer", "K. Forbus", "D. Gentner"], "venue": "Artificial Intelligence,", "citeRegEx": "Falkenhainer et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Falkenhainer et al\\.", "year": 1989}, {"title": "The analogical mind: Perspectives from cognitive science", "author": ["D. Gentner", "K.J. Holyoak", "B. Kokinov"], "venue": null, "citeRegEx": "Gentner et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gentner et al\\.", "year": 2001}, {"title": "Some statistical issues in the comparison of speech recognition algorithms", "author": ["L. Gillick", "S. Cox"], "venue": "In IEEE Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gillick and Cox,? \\Q1989\\E", "shortCiteRegEx": "Gillick and Cox", "year": 1989}, {"title": "Fluid Concepts and Creative Analogies", "author": ["D. Hofstadter"], "venue": "Fluid Analogies Research Group", "citeRegEx": "Hofstadter,? \\Q1994\\E", "shortCiteRegEx": "Hofstadter", "year": 1994}, {"title": "Analogy", "author": ["K. Holyoak"], "venue": "The Cambridge Handbook of Thinking and Reasoning, chap. 6. Cambridge University Press.", "citeRegEx": "Holyoak,? 2005", "shortCiteRegEx": "Holyoak", "year": 2005}, {"title": "Using statistical testing in the evaluation of retrieval experiments", "author": ["D. Hull"], "venue": "Research and Development in Information Retrieval, pp. 329\u2013338.", "citeRegEx": "Hull,? 1993", "shortCiteRegEx": "Hull", "year": 1993}, {"title": "A rehabilitation of analogy in syntax (and elsewhere)", "author": ["E. Itkonen", "J. Haukioja"], "venue": null, "citeRegEx": "Itkonen and Haukioja,? \\Q1997\\E", "shortCiteRegEx": "Itkonen and Haukioja", "year": 1997}, {"title": "Apparatus and method for producing analogically similar word based on pseudodistances between words", "author": ["Y. Lepage"], "venue": null, "citeRegEx": "Lepage,? \\Q2001\\E", "shortCiteRegEx": "Lepage", "year": 2001}, {"title": "De l\u2019analogie rendant compte de la commutation en linguistique", "author": ["Y. Lepage"], "venue": "Universit\u00e9 Joseph Fourier, Grenoble. Habilitation \u00e0 diriger les recherches.", "citeRegEx": "Lepage,? 2003", "shortCiteRegEx": "Lepage", "year": 2003}, {"title": "A new version of the nearest-neighbour approximating and eliminating search algorithm aesa with linear preprocessing-time and memory requirements", "author": ["L. Mic\u00f3", "J. Oncina", "E. Vidal"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Mic\u00f3 et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Mic\u00f3 et al\\.", "year": 1994}, {"title": "Analogy-Making as Perception", "author": ["M. Mitchell"], "venue": "MIT Press.", "citeRegEx": "Mitchell,? 1993", "shortCiteRegEx": "Mitchell", "year": 1993}, {"title": "Machine Learning", "author": ["T. Mitchell"], "venue": "McGraw-Hill.", "citeRegEx": "Mitchell,? 1997", "shortCiteRegEx": "Mitchell", "year": 1997}, {"title": "Writer style adaptation in on-line handwriting recognizers by a fuzzy mechanism approach: The adapt method", "author": ["H. Mouch\u00e8re", "E. Anquetil", "N. Ragot"], "venue": "Int. Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Mouch\u00e8re et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mouch\u00e8re et al\\.", "year": 2007}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": "J Mol Biol,", "citeRegEx": "Needleman and Wunsch,? \\Q1970\\E", "shortCiteRegEx": "Needleman and Wunsch", "year": 1970}, {"title": "UCI repository of machine learning databases", "author": ["D. Newman", "S. Hettich", "C. Blake", "C. Merz"], "venue": null, "citeRegEx": "Newman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Newman et al\\.", "year": 1998}, {"title": "Analogy in the lexicon: a probe into analogy-based machine learning of language", "author": ["V. Pirrelli", "F. Yvon"], "venue": "In Proceedings of the 6th International Symposium on Human Communication,", "citeRegEx": "Pirrelli and Yvon,? \\Q1999\\E", "shortCiteRegEx": "Pirrelli and Yvon", "year": 1999}, {"title": "An algebraic framework for solving proportional and predictive analogies", "author": ["U. Schmid", "H. Gust", "K\u00fchnberger", "K.-U", "J. Burghardt"], "venue": "Proceedings of the European Conference on Cognitive Science (EuroCogSci", "citeRegEx": "Schmid et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Schmid et al\\.", "year": 2003}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S. Waterman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Smith and Waterman,? \\Q1981\\E", "shortCiteRegEx": "Smith and Waterman", "year": 1981}, {"title": "Analogie dans les s\u00e9quences : un solveur \u00e0 \u00e9tats finis", "author": ["N. Stroppa", "F. Yvon"], "venue": "TALN", "citeRegEx": "Stroppa and Yvon,? \\Q2004\\E", "shortCiteRegEx": "Stroppa and Yvon", "year": 2004}, {"title": "Improved sensitivity of profile searches through the use of sequence weights and gap excision", "author": ["J.D. Thompson", "D.G. Higgins", "T.J. Gibson"], "venue": "Computer Applications in the Biosciences,", "citeRegEx": "Thompson et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Thompson et al\\.", "year": 1994}, {"title": "Measuring semantic similarity by latent relational analysis", "author": ["P.D. Turney"], "venue": "Proceedings Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05), 05, 1136.", "citeRegEx": "Turney,? 2005", "shortCiteRegEx": "Turney", "year": 2005}, {"title": "On the complexity of multiple sequence alignment", "author": ["L. Wang", "T. Jiang"], "venue": "Journal of Computational Biology,", "citeRegEx": "Wang and Jiang,? \\Q1994\\E", "shortCiteRegEx": "Wang and Jiang", "year": 1994}, {"title": "Data Mining: Practical machine learning tools and techniques, 2nd Edition", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "Witten and Frank,? \\Q2005\\E", "shortCiteRegEx": "Witten and Frank", "year": 2005}, {"title": "Paradigmatic cascades: a linguistically sound model of pronunciation by analogy", "author": ["F. Yvon"], "venue": "Proceedings of the 35th annual meeting of the Association for Computational Linguistics (ACL), Madrid, Spain.", "citeRegEx": "Yvon,? 1997", "shortCiteRegEx": "Yvon", "year": 1997}, {"title": "Pronouncing unknown words using multi-dimensional analogies", "author": ["F. Yvon"], "venue": "Proceeding of the European conference on Speech Application and Technology (Eurospeech), Vol. 1, pp. 199\u2013202, Budapest, Hungary.", "citeRegEx": "Yvon,? 1999", "shortCiteRegEx": "Yvon", "year": 1999}, {"title": "Solving analogical equations on words", "author": ["F. Yvon", "N. Stroppa", "A. Delhay", "L. Miclet"], "venue": "Tech. rep. ENST2004D005, E\u0301cole Nationale Supe\u0301rieure des Te\u0301le\u0301communications", "citeRegEx": "Yvon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yvon et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "More generally, the resolution of analogical equations can also be seen as a basic component of learning by analogy systems, which are part of the lazy learning techniques (Daelemans, 1996).", "startOffset": 172, "endOffset": 189}, {"referenceID": 36, "context": "For example, Yvon (1999) presents an analogical approach to the grapheme-to-phoneme conversion, for text-to-speech synthesis purposes.", "startOffset": 13, "endOffset": 25}, {"referenceID": 25, "context": "This is the problem of inductive learning of a classification rule from examples, which consists in finding the value of u at point y (Mitchell, 1997).", "startOffset": 134, "endOffset": 150}, {"referenceID": 24, "context": "This is the problem of inductive learning of a classification rule from examples, which consists in finding the value of u at point y (Mitchell, 1997). The nearest neighbor method, which is the most popular lazy learning technique, simply finds in S the description x which minimizes some distance to y and hypothesizes u(x), the label of x, for the label of y. Moving one step further, learning from analogical proportions consists in searching in S for a triple (x, z, t) such that \u201cx is to z as t is to y\u201d and predicts for y the label \u00fb(y) which is solution of the equation \u201cu(x) is to u(z) as u(t) is to \u00fb(y)\u201d. If more than one triple is found, a voting procedure can be used. Such a learning technique is based on the resolution of analogical equations. Pirrelli and Yvon (1999) discuss the relevance of such a learning procedure for various linguistic analysis tasks.", "startOffset": 135, "endOffset": 784}, {"referenceID": 18, "context": "Much work has been done on this subject from a cognitive science point of view, which had led to computational models of reasoning by analogy: see for example, the classical paper (Falkenhainer, Forbus, & Gentner, 1989), the book (Gentner, Holyoak, & Kokinov, 2001) and the recent survey (Holyoak, 2005).", "startOffset": 288, "endOffset": 303}, {"referenceID": 25, "context": "Our work also aims to define some supervised machine learning process (Mitchell, 1997; Cornu\u00e9jols & Miclet, 2002), in the spirit of lazy learning.", "startOffset": 70, "endOffset": 113}, {"referenceID": 14, "context": "Much work has been done on this subject from a cognitive science point of view, which had led to computational models of reasoning by analogy: see for example, the classical paper (Falkenhainer, Forbus, & Gentner, 1989), the book (Gentner, Holyoak, & Kokinov, 2001) and the recent survey (Holyoak, 2005). Usually, these works use the notion of transfer, which is not within the scope of this article. It means that some knowledge on solving a problem in a domain is transported to another domain. Since we work on four objects that are in the same space, we implicitly ignore the notion of transfer between different domains. Technically speaking, this restriction allows us to use an axiom called \u2018exchange of the means\u2019 to define an analogical proportion (see Definition 2.1). However, we share with these works the following idea: there may be a similar relation between two couples of structured objects even if the objects are apparently quite different. We are interested in giving a formal and algorithmic definition of such a relation. Our work also aims to define some supervised machine learning process (Mitchell, 1997; Cornu\u00e9jols & Miclet, 2002), in the spirit of lazy learning. We do not seek to extract a model from the learning data, but merely conclude what is the class, or more generally the supervision, of a new object by inspecting (a part of) the learning data. Usually, lazy learning, like the k-nearest neighbors technique, makes use of unstructured objects, such as vectors. Since distance measures can be also defined on strings, trees and even graphs, this technique has also been used on structured objects, in the framework of structural pattern recognition (see for example the work of Bunke & Caelli, 2004; Blin & Miclet, 2000; Basu, Bunke, & Del Bimbo, 2005). We extend here the search of the nearest neighbor in the learning set to that of the best triple (when combined with the new object, it is the closest to make an analogical proportion). This requires defining what is an analogical proportion on structured objects, like sequences, but also to give a definition of how far a 4-tuple of objects is from being in analogy (that we call analogical dissimilarity). Learning by analogy on sequences has already being studied, in a more restricted manner, on linguistic data (Yvon, 1997, 1999; Itkonen & Haukioja, 1997). Reasoning and learning by analogy has proven useful in tasks like grapheme to phoneme conversion, morphology and translation. Sequences of letters and/or of phonemes are a natural application to our work, but we are also interested in other type of data, structured as sequences or trees, such as prosodic representations for speech synthesis, biochemical sequences, online handwriting recognition, etc. Analogical proportions between four structured objects of the same universe, mainly strings, have been studied with a mathematical and algorithmic approach, like ours, by Mitchell (1993) and Hofstadter et al.", "startOffset": 240, "endOffset": 2946}, {"referenceID": 14, "context": "Analogical proportions between four structured objects of the same universe, mainly strings, have been studied with a mathematical and algorithmic approach, like ours, by Mitchell (1993) and Hofstadter et al. (1994), Dastani et al.", "startOffset": 191, "endOffset": 216}, {"referenceID": 9, "context": "(1994), Dastani et al. (2003), Schmid et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "(1994), Dastani et al. (2003), Schmid et al. (2003). To the best of our knowledge our proposition is original: to give a formal definition of what can be an analogical dissimilarity between four objects, in particular between sequences, and to produce algorithms that enable the efficient use of this concept in machine learning practical problems.", "startOffset": 8, "endOffset": 52}, {"referenceID": 9, "context": "(1994), Dastani et al. (2003), Schmid et al. (2003). To the best of our knowledge our proposition is original: to give a formal definition of what can be an analogical dissimilarity between four objects, in particular between sequences, and to produce algorithms that enable the efficient use of this concept in machine learning practical problems. We have already discussed how to compute exact analogical proportions between sequences in the paper by Yvon et al. (2004) and given a preliminary attempt to compute analogical dissimilarity between sequences in the paper by Delhay and Miclet (2004).", "startOffset": 8, "endOffset": 472}, {"referenceID": 9, "context": "(1994), Dastani et al. (2003), Schmid et al. (2003). To the best of our knowledge our proposition is original: to give a formal definition of what can be an analogical dissimilarity between four objects, in particular between sequences, and to produce algorithms that enable the efficient use of this concept in machine learning practical problems. We have already discussed how to compute exact analogical proportions between sequences in the paper by Yvon et al. (2004) and given a preliminary attempt to compute analogical dissimilarity between sequences in the paper by Delhay and Miclet (2004). Excerpts of the present article have been presented in conferences (Bayoudh, Miclet, & Delhay, 2007a; Bayoudh, Mouch\u00e8re, Miclet, & Anquetil, 2007b).", "startOffset": 8, "endOffset": 599}, {"referenceID": 0, "context": ", let us quote Aamodt and Plaza (1994) about the use of the term \u2018analogy\u2019 in Case-Based Reasoning (CBR): \u2019Analogy-based reasoning: This term is sometimes used, as a synonym to case-based reasoning, to describe the typical case-based approach.", "startOffset": 15, "endOffset": 39}, {"referenceID": 21, "context": "According to Lepage (2003) three basic axioms can be given: Definition 2.", "startOffset": 13, "endOffset": 27}, {"referenceID": 21, "context": "2 SOLVING ANALOGICAL EQUATIONS IN FINITE SETS Considering analogy in sets, Lepage (2003) has shown the following theorem, with respect to the axioms of analogy (section 2.", "startOffset": 75, "endOffset": 89}, {"referenceID": 21, "context": "One has to note that Lepage (2001) and Stroppa and Yvon (2004) have already proposed a definition of an analogical proportion between sequences with applications to linguistic data.", "startOffset": 21, "endOffset": 35}, {"referenceID": 21, "context": "One has to note that Lepage (2001) and Stroppa and Yvon (2004) have already proposed a definition of an analogical proportion between sequences with applications to linguistic data.", "startOffset": 21, "endOffset": 63}, {"referenceID": 13, "context": "For structure or functional similarity like in protein modelization, pattern identification or structure prediction in DNA, methods using simultaneous alignment like MSA (Wang & Jiang, 1994) or DCA (Dress, F\u00fcllen, & Perrey, 1995), or iterative alignment like MUSCLE (Edgar, 2004) are the best.", "startOffset": 266, "endOffset": 279}, {"referenceID": 23, "context": "The principle is similar to that of LAESA (Mic\u00f3 et al., 1994).", "startOffset": 42, "endOffset": 61}, {"referenceID": 34, "context": "In a paper of Turney (2005), a discrimination is done by keeping the most frequent patterns in words.", "startOffset": 14, "endOffset": 28}, {"referenceID": 3, "context": "In his recent book, Bishop (2007) gives no theoretical coverage of such a procedure, but rather draws a pragmatic conclusion: \u2018 .", "startOffset": 20, "endOffset": 34}, {"referenceID": 26, "context": "These anchorage points come from an analysis of the stable handwriting properties, as defined in (Mouch\u00e8re et al., 2007): pen-up/down, y-extrema, angular points and in-loop y-extrema.", "startOffset": 97, "endOffset": 120}, {"referenceID": 26, "context": "Experimental Protocol In the data base that we use (Mouch\u00e8re et al., 2007), twelve different writers have written 40 times the 26 lowercase letters (1040 characters) on a PDA.", "startOffset": 51, "endOffset": 74}, {"referenceID": 19, "context": "The second method is non-parametric: the SIGN TEST (Hull, 1993).", "startOffset": 51, "endOffset": 63}], "year": 2008, "abstractText": "This paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer.", "creator": null}}}