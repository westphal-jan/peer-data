{"id": "1401.3437", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Learning Partially Observable Deterministic Action Models", "abstract": "We present precise algorithms for identifying deterministic action effects and prerequisites in dynamic, partially observable domains. They are used when the action model (the way actions affect the world) of a domain is not known and must be learned from partial observations over time. Such scenarios are widely used in real-world applications and pose a challenge to AI tasks, as traditional domain structures underlying traceability (e.g. conditional independence) fail there (e.g. world characteristics are correlated). Our work differs from traditional assumptions about partial observations and action models. In particular, it focuses on problems where actions are deterministically simple logical structures and where observation models exhibit all characteristics observed with a certain frequency.", "histories": [["v1", "Wed, 15 Jan 2014 04:52:56 GMT  (469kb)", "http://arxiv.org/abs/1401.3437v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eyal amir", "allen chang"], "accepted": false, "id": "1401.3437"}, "pdf": {"name": "1401.3437.pdf", "metadata": {"source": "CRF", "title": "Learning Partially Observable Deterministic Action Models", "authors": ["Eyal Amir", "Allen Chang"], "emails": ["EYAL@ILLINOIS.EDU", "ALLENC256@YAHOO.COM"], "sections": [{"heading": null, "text": "Our algorithms take sequences of partial observations over time as input, and output deterministic action models that could have lead to those observations. The algorithms output all or one of those models (depending on our choice), and are exact in that no model is misclassified given the observations. Our algorithms take polynomial time in the number of time steps and state features for some traditional action classes examined in the AI-planning literature, e.g., STRIPS actions. In contrast, traditional approaches for HMMs and Reinforcement Learning are inexact and exponentially intractable for such domains. Our experiments verify the theoretical tractability guarantees, and show that we identify action models exactly. Several applications in planning, autonomous exploration, and adventure-game playing already use these results. They are also promising for probabilistic settings, partially observable reinforcement learning, and diagnosis."}, {"heading": "1. Introduction", "text": "Partially observable domains are common in the real world. They involve situations in which one cannot observe the entire state of the world. Many examples of such situations are available from all walks of life, e.g., the physical worlds (we do not observe the position of items in other rooms), the Internet (we do not observe more than a few web pages at a time), and inter-personal communications (we do not observe the state of mind of our partners).\nAutonomous agents\u2019 actions involve a special kind of partial observability in such domains. When agents explore a new domain (e.g., one goes into a building or meets a new person), they have limited knowledge about their action models (actions\u2019 preconditions and effects). These action models do not change with time, but they may depend on state features. Such agents can act intelligently, if they learn how their actions affect the world and use this knowledge to respond to their goals.\nc\u00a92008 AI Access Foundation. All rights reserved.\nLearning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from Reinforcement Learning. It enables reasoning about actions instead of expensive trials in the world.\nLearning actions\u2019 effects and preconditions is difficult in partially observable domains. The difficulty stems from the absence of useful conditional independence structures in such domains. Most fully observable domains include such structures, e.g., the Markov property (independence of the state at time t + 1 from the state at time t \u2212 1, given the (observed) state at time t). These are fundamental to tractable solutions of learning and decision making.\nIn partially observable domains those structures fail (e.g., the state of the world at time t + 1 depends on the state at time t \u2212 1 because we do not observe the state at time t), and complex approximate approaches are the only feasible path. For these reasons, much work so far has been limited to fully observable domains (e.g., Wang, 1995; Pasula, Zettlemoyer, & Kaelbling, 2004), hill-climbing (EM) approaches that have unbounded error in deterministic domains (e.g., Ghahramani, 2001; Boyen, Friedman, & Koller, 1999), and approximate action models (Dawsey, Minsker, & Amir, 2007; Hill, Minsker, & Amir, 2007; Kuffner. & LaValle, 2000; Thrun, 2003).\nThis paper examines the application of an old-new structure to learning in partially observable domains, namely, determinism and logical formulation. It focuses on some such deterministic domains in which tractable learning is feasible, and shows that a traditional assumption about the form of determinism (the STRIPS assumption, generalized to ADL, Pednault, 1989) leads to tractable learning and state estimation. Learning in such domains has immediate applications (e.g., exploration by planning, Shahaf, Chang, & Amir, 2006; Chang & Amir, 2006) and it can also serve as the basis for learning in stochastic domains. Thus, a fundamental advance in the application of such a structure is important for opening the field to new approaches of broader applicability. The following details the technical aspects of our advance.\nThe main contribution of this paper is an approach called SLAF (Simultaneous Learning and Filtering) for exact learning of actions models in partially observable deterministic domains. This approach determines the set of possible transition relations, given an execution sequence of actions and partial observations. For example, the input could come from watching another agent act or from watching the results of our own actions\u2019 execution. The approach is online, and updates a propositional logical formula called Transition Belief Formula. This formula represents the possible transition relations and world states at every time step. In this way, it is similar in spirit to Bayesian learning of HMMs (e.g., Ghahramani, 2001) and Logical Filtering (Amir & Russell, 2003).\nThe algorithms that we present differ in their range of applicability and their computational complexity. First, we present a deduction-based algorithm that is applicable to any nondeterministic learning problem, but that takes time that is worst-case exponential in the number of domain fluents. Then, we present algorithms that update a logical encoding of all consistent transition relations in polynomial time per step, but that are limited in applicability to special classes of deterministic actions.\nOne set of polynomial-time algorithms that we present applies to action-learning scenarios in which actions are ADL (Pednault, 1989) (with no conditional effects) and one of the following holds: (a) the action model already has preconditions known, and we observe action failures (e.g., when we perform actions in the domain), or (b) actions execution always succeeds (e.g., when an expert or tutor performs actions).\nOur algorithms output a transition belief formula that represents the possible transition relations and states after partial observations of the state and actions. They do so by updating each component of the formula separately in linear time. Thus, updating the transition belief formula with every action execution and observation takes linear time in the size of the input formula.\nProcessing a sequence of T action executions and observations takes time O(T 2 \u00b7 n) for case (b). The main reason for this is a linear growth in the representation size of the transition belief formula: at time t, the iterative process that updates this formula would process a formula that has size linear in t.\nFor case (a) processing a sequence of length T takes polynomial time O(T \u00b7 nk)), only if we observe every feature in the domain every\u2264 k steps in expectation, for some fixed k. The reason for this is that the transition belief formula can be kept in k-CNF (k Conjunctive Normal Form), thus of size O(nk). (Recall that a propositional formula is in k-CNF, if it is of the form \u2227 i\u2264m \u2228 j\u2264k li,j , with every li,j a propositional variable or its negation.) Case (b) takes time O(T \u00b7n) under the same assumption.\nAnother set of polynomial-time algorithms that we present takes linear time in the representation size. In this case actions are known to be injective, i.e., map states 1:1. There, we can bound the computation time for T steps with O(T \u00b7 nk), if we approximate the transition-belief formula representation with a k-CNF formula.\nIn contrast, work on learning in Dynamic Bayesian Networks (e.g., Boyen et al., 1999), reinforcement learning in POMDPs (e.g., Littman, 1996), and Inductive Logic Programming (ILP) (e.g., Wang, 1995) either approximate the solution with unbounded error for deterministic domains, or take time \u2126(22n), and are inapplicable in domains larger than 10 features. Our algorithms are better in this respect, and scale polynomially and practically to domains of 100\u2019s of features and more. Section 8 provides a comparison with these and other works.\nWe conduct a set of experiments that verify these theoretical results. These experiments show that our algorithms are faster and better qualitatively than related approaches. For example, we can learn some ADL actions\u2019 effects in domains of > 100 features exactly and efficiently.\nAn important distinction must be made between learning action models and traditional creation of AI-Planning operators. From the perspective of AI Planning, action models are the result of explicit modeling, taking into account modeling decisions. In contrast, learning action models is deducing all possible transition relations that are compatible with a set of partially observed execution trajectories.\nIn particular, action preconditions are typically used by the knowledge engineer to control the granularity of the action model so as to leave aside from specification unwanted cases. For example, if driving a truck with insufficient fuel from one site to another might generate unexpected situations that the modeller does not want to consider, then a simple precondition can be used to avoid considering that case. The intention in this paper is not to mimic this modeling perspective, but instead find action models that generate sound states when starting from a sound state. Sound state is any state in which the system can be in practice, namely, ones that our observations of real executions can reflect.\nOur technical advance for deterministic domains is important for many applications such as automatic software interfaces, internet agents, virtual worlds, and games. Other applications, such as robotics, human-computer interfaces, and program and machine diagnosis can use deterministic action models as approximations. Finally, understanding the deterministic case better can help us\ndevelop better results for stochastic domains, e.g., using approaches such as those by Boutilier, Reiter, and Price (2001), Hajishirzi and Amir (2007).\nIn the following, Section 2 defines SLAF precisely, Section 3 provides a deduction-based exact SLAF algorithm, Section 4 presents tractable action-model-update algorithms, Section 5 gives sufficient conditions and algorithms for keeping the action-model representation compact (thus, overall polynomial time), and Section 7 presents experimental results."}, {"heading": "2. Simultaneous Learning and Filtering (SLAF)", "text": "Simultaneous Learning and Filtering (SLAF) is the problem of tracking a dynamic system from a sequence of time steps and partial observations, when we do not have the system\u2019s complete dynamics initially. A solution for SLAF is a representation of all combinations of action models that could possibly have given rise to the observations in the input, and a representation of all the corresponding states in which the system may now be (after the sequence of time steps that were given in the input occurs).\nComputing (the solution for) SLAF can be done in a recursive fashion by dynamic programming in which we determine SLAF for time step t+1 from our solution of SLAF for time t. In this section we define SLAF formally in such a recursive fashion.\nIgnoring stochastic information or assumptions, SLAF involves determining the set of possible ways in which actions can change the world (the possible transition models, defined formally below) and the set of states the system might be in. Any transition model determines a set of possible states, so a solution to SLAF is a transition model and its associated possible states.\nWe define SLAF with the following formal tools, borrowing intuitions from work on Bayesian learning of Hidden Markov Models (HMMs) (Ghahramani, 2001) and Logical Filtering (Amir & Russell, 2003).\nDefinition 2.1 A transition system is a tuple \u3008P,S,A, R\u3009, where \u2022 P is a finite set of propositional fluents; \u2022 S \u2286 Pow(P) is the set of world states. \u2022 A is a finite set of actions; \u2022 R \u2286 S \u00d7A\u00d7 S is the transition relation (transition model).\nThus, a world state, s \u2208 S , is a subset of P that contains propositions true in this state (omitted propositions are false in that state), and R(s, a, s\u2032) means that state s\u2032 is a possible result of action a in state s. Our goal in this paper is to find R, given known P , S , A, and a sequence of actions and partial observations (logical sentences on any subset of P).\nAnother, equivalent, representation for S that we will also use in this paper is the following. A literal is a proposition, p \u2208 P , or its negation, \u00acp. A complete term over P is a conjunction of literals from P such that every fluent appears exactly once. Every state corresponds to a complete term of P and vice versa. For that reason, we sometime identify a state s \u2208 S with this term. E.g., for states s1, s2, s1\u2228s2 is the disjunction of the complete terms corresponding to s1, s2, respectively.\nA transition belief state is a set of tuples \u3008s,R\u3009 where s is a state and R a transition relation. Let R = Pow(S \u00d7 A \u00d7 S) be the set of all possible transition relations on S,A. Let S = S \u00d7 R. When we hold a transition belief state \u03c1 \u2286 S we consider every tuple \u3008s,R\u3009 \u2208 \u03c1 possible.\nExample 2.2 Consider a domain where an agent is in a room with a locked door (see Figure 1)."}, {"heading": "In its possession are three different keys, and suppose the agent cannot tell from observation only which key opens the door. The goal of the agent is to unlock the door.", "text": "This domain can be represented as follows: let the set of variables defining the state space be P = {locked} where locked is true if and only if the door is locked. Let the set of states be S = {s1, s2} where s1 = {locked} (the state in which the door is locked) and s2 = {} (here the door is unlocked). Let A = {unlock1,unlock2,unlock3} be the three actions wherein the agent tries unlocking the door using each of the three keys.\nLet R1 = {\u3008s1,unlock1, s2\u3009, \u3008s1,unlock2, s1\u3009, \u3008s1,unlock3, s1\u3009} represent a transition relation in which key 1 unlocks the door and the other keys do not. Define R2 and R3 in a similar fashion (e.g., with R2 key 2 unlocks the door and keys 1 and 3 do not). A transition belief state represents the set of possibilities that we consider consistent with our observations so far. Consider a transition belief state given by \u03c1 = {\u3008s1, R1\u3009, \u3008s1, R2\u3009, \u3008s1, R3\u3009}, i.e., the state of the world is fully known but the action model is only partially known.\nWe would like the agent to be able to open the door despite not knowing which key opens it. To do this, the agent will learn the actual action model (i.e., which key opens the door). In general, not only will learning an action model be useful in achieving an immediate goal, but such knowledge will be useful as the agent attempts to perform other tasks in the same domain.2\nDefinition 2.3 (SLAF Semantics) Let \u03c1 \u2286 S be a transition belief state. The SLAF of \u03c1 with actions and observations \u3008aj , oj\u30091\u2264j\u2264t is defined by\n1. SLAF [a](\u03c1) = {\u3008s\u2032, R\u3009 | \u3008s, a, s\u2032\u3009 \u2208 R, \u3008s,R\u3009 \u2208 \u03c1};\n2. SLAF [o](\u03c1) = {\u3008s,R\u3009 \u2208 \u03c1 | o is true in s}; 3. SLAF [\u3008aj , oj\u3009i\u2264j\u2264t](\u03c1) = SLAF [\u3008aj , oj\u3009i+1\u2264j\u2264t](SLAF [oi](SLAF [ai](\u03c1))).\nStep 1 is progression with a, and Step 2 filtering with o.\nExample 2.4 Consider the domain from Example 2.2. The progression of \u03c1 on the action unlock1 is given by SLAF [unlock1](\u03c1) = {\u3008s2, R1\u3009, \u3008s1, R2\u3009, \u3008s1, R3\u3009}. Likewise, the filtering of \u03c1 on the observation \u00aclocked (the door became unlocked) is given by SLAF [\u00aclocked](\u03c1) = {\u3008s2, R1\u3009}.2\nExample 2.5 A slightly more involved example is the following situation presented in Figure 2. There, we have two rooms, a light bulb, a switch, an action of flipping the switch, and an observation, E (we are in the east room). The real states of the world before and after the action, s2, s2\u2032, respectively (shown in the top part), are not known to us.\nThe bottom of Figure 2 demonstrates how knowledge evolves after performing the action sw-on. There, \u03c11 = {\u3008s1, R1\u3009, \u3008s2, R2\u3009, \u3008s3, R3\u3009} for some s1, R1, s3, R3, s2 = {E}, and R2 that includes \u3008s2, sw-on, s\u20322\u3009 (the identity and full details of R1, R2, R3 are irrelevant here, so we omit them). \u03c12 is the resulting transition belief state after action sw-on and observation E: \u03c12 = SLAF [sw-on, E](\u03c11). 2\nWe assume that observations (and an observation model relating observations to state fluents) are given to us as logical sentences over fluents after performing an action. They are denoted with o.\nThis approach to transition belief states generalizes Version Spaces of action models (e.g., Wang, 1995) as follows: If the current state, s, is known, then the version space\u2019s lattice contains the set of transition relations \u03c1s = {R | \u3008s,R\u3009 \u2208 \u03c1}. Thus, from the perspective of version spaces, SLAF semantics is equivalent to a set of version spaces, one for each state in which we might be.\nThis semantics also generalizes belief states: If the transition relation, R, is known, then the belief state (set of possible states) is \u03c1R = {s | \u3008s,R\u3009 \u2208 \u03c1} (read \u03c1 restricted to R), and Logical Filtering (Amir & Russell, 2003) of belief state \u03c3 and action a is equal to (thus, we can define it as)\nFilter[a](\u03c3) = (SLAF [a]({\u3008s,R\u3009 | s \u2208 \u03c3}))R.\nThus, SLAF semantics is equivalent to holding a set of belief states, each conditioned on a transition relation, similar to saying \u201cif the transition relation is R, then the belief state (set of states) is \u03c3R\u201d."}, {"heading": "3. Learning by Logical Inference", "text": "Learning transition models using Definition 2.3 directly is intractable \u2013 it requires space \u2126(22|P|) in many cases. The reason for that is the explicit representation of the very large set of possible transition-state pairs. Instead, in this section and the rest of this paper we represent transition belief states more compactly using propositional logic. In many scenarios there is some amount of structure that can be exploited to make a propositional representation compact.\nA combinatorial argument implies that no encoding is compact for all sets. Nonetheless, we are motivated by the success of propositional (logical) approaches for logical filtering (Amir & Russell, 2003; Shahaf & Amir, 2007) and logical-database regression (Reiter, 1991, 2001), and observe that propositional logic represents compactly some natural sets of exponential size.\nIn this section we re-define SLAF as an operation on propositional logical formulas with a propositional formula as output. SLAF\u2019s input is a propositional formula that represents a transition belief state, and SLAF computes a new transition belief formula from that input and a sequence of actions and observations.\nWe want to find algorithms for SLAF that manipulate an input formula and produce a correct output. We use general-purpose logical inference for this task in this section. In later sections we sidestep expensive general-purpose inference, and make assumptions that lead to tractable algorithms. For the rest of this paper we focus on deterministic transition relations, namely, transition relations that are partial functions (every action has at most one outcome state for every state)."}, {"heading": "3.1 Representing Transition Relations in Logic", "text": "Our initial algorithm for solving SLAF (to be presented momentarily) does so with a compact representation of transition belief states. We present this logical encoding of transition belief states first, and define a deduction-based algorithm in the next section.\nWe use the following general terminology for propositional logical languages (all the terminological conventions apply with or without subscripts and superscripts). L denotes a vocabulary, i.e., a set of propositional variables that we use in the present context. L denotes a language, i.e., a set of propositional sentences. \u03d5, \u03c8, and other script Greek letters stand for propositional formulas in the language of the present context. F,G also stand for such formulas, but in a restricted context (see below). L(\u03d5) denotes the vocabulary of \u03d5. L(L) denotes the language built from propositions in L using the standard propositional connectives (\u00ac, \u2228, \u2227,...). L(\u03d5) is a shorthand for L(L(\u03d5)).\nWe represent deterministic transition relations with a propositional vocabulary, LA, whose propositions are of the form aFG, for a \u2208 A, F a literal over P , and G a logical formula. F is the effect of aFG, and G is the precondition of aFG. When proposition aFG takes the truth value TRUE, this has the intended meaning that \u201cIf G holds in the present state, then F holds in the state that results from executing a\u201d.\nWe let F \u2286 P \u222a {\u00acp | p \u2208 P} be the set of all effects, F , that we consider. We let G be the set of all preconditions, G, that we consider. In the rest of this section and Section 4 we assume that G represents a single state in S . Recall that we identify a state with a complete term which is the conjunction of literals that hold in that state. We use this representation of states and write aFs instead of aFG. Later we build on this definition and consider G\u2019s that are more general formulas.\nFrom our assumption (G \u2261 S for now, as stated above) we conclude that LA has O(2|P| \u00b7 2|P| \u00b7 |A|) propositional variables. We prove fundamental results for this language and a set of axioms, disregarding the size of the language for a moment. Section 5 focuses on decreasing the language size for computational efficiency.\nOur semantics for vocabulary LA lets every interpretation (truth assignment), M , for LA correspond with a transition relation, RM . Every transition relation has at least one (possibly more) interpretation that corresponds to it, so this correspondence is surjective (onto) but not injective (1-to-1). Every propositional sentence \u03d5 \u2208 L(LA) specifies a set of transition models as follows:\nThe set of models1 (satisfying interpretations) of \u03d5, I[\u03d5] = {M interpretation to LA | M |= \u03d5}, specifies the corresponding set of transition relations, {RM |M \u2208 I[\u03d5]}.\nInformally, assume that propositions aF1s , ...aFks \u2208 LA take the value TRUE in M , and that all other propositions with precondition s take the value FALSE. Then, RM (with action a) takes the state s to a state s\u2032 that satisfies \u2227 i\u2264k Fi, and is identical to s otherwise. If no such s\u2032 exists (e.g., Fi = \u00acFj , for some i, j \u2264 k), then RM takes s to no s\u2032 (thus, a is not executable in s according to RM ).\nThe following paragraphs show how interpretations over LA correspond to transition relations. They culminate with a precise definition of the correspondence between formulas in L(LA\u222aP) and transition belief states \u03c1 \u2286 S.\nEVERY INTERPRETATION OF LA CORRESPONDS TO A UNIQUE TRANSITION RELATION\nEvery interpretations of LA defines a unique transition relation RM as follows. Let M be an interpretation of LA. For every state s \u2208 S and an action a \u2208 A we either define a unique state s\u2032 such that \u3008s, a, s\u2032\u3009 \u2208 RM or decide that there is no s\u2032 for which \u3008s, a, s\u2032\u3009 \u2208 RM .\nM gives an interpretation for every proposition aFs , for F a fluent or its negation. If for any fluent p \u2208 P , M [aps] = M [a\u00acps ] = TRUE (M [\u03d5] is the truth value of \u03d5 according to interpretation M ), we decide that there is no s\u2032 such that \u3008s, a, s\u2032\u3009 \u2208 RM . Otherwise, define\ns\u2032 = {p \u2208 P |M |= aps} \u222a {p \u2208 s |M |= \u00aca \u00acp s }\nIn the left-hand side of \u222a we consider the cases of p \u2208 P for which M [aps] 6= M [a\u00acps ], and on the right-hand side of \u222a we treat the cases of p \u2208 P for which M [aps] = M [a\u00acps ] = FALSE (this is called inertia because p keeps its previous value for lack of other specifications). Put another way, s\u2032[p] = M [aps]\u2228 (s[p]\u2227\u00acM [a \u00acp s ]), if we view s\u2032 as an interpretation of P . RM is well defined, i.e., there is only one RM for every M .\nEVERY TRANSITION RELATION HAS AT LEAST ONE CORRESPONDING INTERPRETATION OF LA\nIt is possible that RM = RM \u2032 for M 6= M \u2032. This occurs in two circumstances: (a) cases in which there is no \u3008s, a, s\u2032\u3009 \u2208 RM for some s, a and (b) when M [aps] = M [a\u00acps ] = FALSE (inertia) and M \u2032[aps] = s[p], M \u2032[a\u00acps ] = s[\u00acp] (not inertia).\nFor an example of the first circumstance, let p be a fluent, let M be an interpretation such that M [aps] = M [a \u00acp s ] for some G. Define M \u2032 an interpretation that is identical to M on all propositions besides aps, a\u00acps as follows. Define M \u2032[aps] to have the opposite truth assignment to M [aps] (FALSE instead of TRUE, and TRUE instead of FALSE). Define M \u2032[a\u00acps ] = M [a\u00acps ].\nThen, RM = RM \u2032 because they map all pairs s, a in the same way. In particular, for state s that corresponds to G, there is no \u3008s, a, s\u2032\u3009 \u2208 RM and similarly there is no \u3008s, a, s\u2032\u3009 \u2208 RM \u2032 .\nFinally, every transition relation R has at least one interpretation M such that R = RM . To see this, define MR for every \u3008s, a, s\u2032\u3009 \u2208 R the interpretation to aps (p any fluent) MR[aps] = TRUE iff p \u2208 s\u2032. Also, for the same \u3008s, a, s\u2032\u3009 define MR[a\u00acps ] = FALSE iff p /\u2208 s\u2032. Finally, for all s, a for which there is no such s\u2032, define MR[aps] = MR[a\u00acps ] = TRUE. Then, R = RMR .\n1. We overload the word model for multiple related meanings. model refers to a satisfying interpretation of a logical formula. Transition model is defined in Definition 2.1 to be a transition relation in a transition system. Action model is define in the Introduction section to be any well-defined specification of actions\u2019 preconditions and effects.\nEVERY TRANSITION RELATION DEFINES A FORMULA OVER LA Every deterministic transition relation R defines a logical formula whose set of models all map to R. There are many such possible formulas, and we define the most general one (up to logical equivalence) that does not make use of inertia.\nDefine Th(R) as follows.\nTh0(R) = {a F s ,\u00aca \u00acF s | a F s \u2208 LA, \u3008s, a, s \u2032\u3009 \u2208 R, s\u2032 |= F} Th1(R) = {a p s \u2228 a \u00acp s | p \u2208 P, s \u2208 S}\nTh2(R) = { \u2228 p\u2208P(a p s \u2227 a \u00acp s ) | \u00ac\u2203s\u2032, \u3008s, a, s\u2032\u3009 \u2208 R} Th(R) = Th0(R) \u222a Th1(R) \u222a Th2\nTh0 addresses fluent changes, Th1 addresses fluent innertia (effectively disallowing innertia in our definition), and Th2 addresses conditions in which actions are not executable. Thus, Th(R) includes as a model M every interpretation that satisfies RM = R and that requires no inertia for its definition of RM . It represents R in that each of its models M satisfies RM = R.\nIt is illuminating to see how our modeling decisions (above and throughout this section) lead to the last definition. On the one hand, we choose to have every interpretation of LA correspond to a transition relation (we do this to simplify later arguments about logical entailment). Consequently, we associate interpretations M with M [aFs ] = M [a\u00acFs ] = FALSE with transition relations R(s, a, s\u2032) that keep the value of F fixed between s, s\u2032 (this is inertia for F in a, s). On the other hand, when we define Th(R) above, we choose axioms that exclude such models (thus, we avoid models that include inertia) because it simplifies our later discussion of learning algorithms.\nIn summary, we consider every interpretation of LA as representing exactly one transition relation, and we consider the set of axioms definingR as those that define it directly, i.e., without inertia (without M [aFs ] = M [a\u00acFs ] = FALSE).\nTRANSITION BELIEF STATES CORRESPOND TO FORMULAS OVER LA \u222a P\nThus, for every transition belief state \u03c1 we can define a formula in L(LA \u222a P) that corresponds to it: Th(\u03c1) = \u2228 \u3008s,R\u3009\u2208\u03c1(s\u2227 Th(R)). Other formulas exist that would characterize \u03c1 in a similar way, and they are not all equivalent. This is so because there are stronger formulas \u03d5 \u2208 L(LA) such that \u03d5 |= Th(R) and Th(R) 6|= \u03d5 and every model, M , of \u03d5 satisfies RM = R.\nSimilarly, for every formula \u03d5 \u2208 L(LA \u222aP) we define a transition belief state \u03c1(\u03d5) = {\u3008M \u00b9P , RM \u3009 | M |= \u03d5, }, i.e., all the state-transition pairs that satisfy \u03d5 (M \u00b9P is M restricted to P , viewed as a complete term over P). We say that formula \u03d5 is a transition belief formula, if Th(\u03c1(\u03d5)) \u2261 \u03d5 (note: \u03c1(Th(\u03c1)) = \u03c1 always holds)."}, {"heading": "3.2 Transition-Formula Filtering", "text": "In this section, we show that computing the transition belief formula for SLAF [a](\u03d5) for successful action a and transition belief formula \u03d5 is equivalent to a logical consequence finding operation. This characterization of SLAF as consequence-finding permits using consequence finding as an algorithm for SLAF, and is important later in this paper for proving the correctness of our more tractable, specialized algorithms.\nLet CnL(\u03d5) denote the set of logical consequences of \u03d5 restricted to vocabulary L. That is, CnL(\u03d5) contains the set of prime implicates of \u03d5 that contain only propositions from the set L.\nRecall that an implicate \u03b1 of a formula \u03d5 is a clause entailed by \u03d5 (\u03d5 |= \u03b1). Recall that a prime implicate \u03b1 of a formula \u03d5 is an implicate of \u03d5 that is not subsumed (entailed) by any other implicates of \u03d5.\nConsequence finding is any process that computesCnL(\u03d5) for an input, \u03d5. For example, propositional resolution (Davis & Putnam, 1960; Chang & Lee, 1973) is an efficient consequence finder when used properly (Lee, 1967; del Val, 1999) (Marquis, 2000, surveys results about prime implicates and consequence finding algorithms). Thus, CnL(\u03d5) \u2261 {\u03c8 \u2208 L(L)| \u03d5 |= \u03c8}.\nFor a set of propositions P, let P \u2032 represent the same set of propositions but with every proposition primed (i.e., each proposition f is annotated to become f \u2032). Typically, we will use a primed fluent to denote the value of the unprimed fluent one step into the future after taking an action. Let \u03d5[P \u2032/P] denote the same formula as \u03d5, but with all primed fluents replaced by their unprimed counterparts. For example, the formula (a \u2228 b\u2032)[P \u2032/P] is equal to a \u2228 b when b \u2208 P. (See Section 8 for a discussion and comparison with relevant formal verification techniques.)\nThe following lemma shows the logical equivalence of existential quantification of quantified boolean formulas and consequence finding restricted to a vocabulary. Recall that quantified boolean formulas (QBF) are propositional formulas with the addition of existential and universal quantifiers over propositions. Informally, the QBF \u2203x.\u03d5 is true for a given interpretation if and only if there exists some true/false valuation of x that makes \u03d5 true under the assignment. The lemma will prove useful for showing the equivalence between SLAF and consequence-finding.\nLemma 3.1 \u2203x.\u03d5 \u2261 CnL(\u03d5)\\{x}(\u03d5), for any propositional logic formula \u03d5 and propositional variable x.\nPROOF See Section B.1. 2 The lemma extends easily to the case of multiple variables:\nCorollary 3.2 For any formula \u03d5 and set of propositional variables X , \u2203X.\u03d5 \u2261 CnL(\u03d5)\\X(\u03d5).\nWe present an algorithm for updating transition belief formulas whose output is equivalent to that of SLAF (when SLAF is applied to the equivalent transition belief state). Our algorithm applies consequence finding to the input transition belief formula together with a set of axioms that define transitions between time steps. We present this set of axioms first.\nFor a deterministic (possibly conditional) action, a, the action model of a (for time t) is axiomatized as\nTeff(a) = \u2227 l\u2208F ,G\u2208G((a l G \u2227G)\u21d2 l\n\u2032) \u2227\u2227 l\u2208F (l \u2032 \u21d2 ( \u2228 G\u2208G(a l G \u2227G)))\n(1)\nThe first part of (1) says that assuming a executes at time t, and it causes l when G holds, and G holds at time t, then l holds at time t + 1. The second part says that if l holds after a\u2019s execution, then it must be that alG holds and also G holds in the current state. These two parts are very similar to (in fact, somewhat generalize) effect axioms and explanation closure axioms used in the Situation Calculus (see McCarthy & Hayes, 1969; Reiter, 2001).\nNow, we are ready to describe our zeroth-level algorithm (SLAF0) for SLAF of a transition belief formula. Let L\u2032 = P \u2032\u222aLA be the vocabulary that includes only fluents of time t+1 and effect propositions from LA. Recall (Definition 2.3) that SLAF has two operations: progression (with an action) and filtering (with an observation). At time t we apply progression for the given action at and current transition belief formula, \u03d5t, and then apply filtering with the current observations:\n\u03d5t+1 = SLAF0[at, ot](\u03d5t) = (Cn L\u2032(\u03d5t \u2227 Teff(at)))[P \u2032/P] \u2227 ot (2)\nThis is identical to Definition 2.3 (SLAF semantics), with the above replacing 1 and 2. As stated above, for SLAF0 we can implement CnL \u2032 (\u03d5) using consequence finding algorithms such as resolution and some of its variants (e.g., Simon & del Val, 2001; McIlraith & Amir, 2001; Lee, 1967; Iwanuma & Inoue, 2002). The following theorem shows that this formula-SLAF algorithm is correct and exact.\nTheorem 3.3 (Representation) For \u03d5 transition belief formula, a action,\nSLAF [a]({\u3008s,R\u3009 \u2208 S | \u3008s,R\u3009 satisfies \u03d5}) = {\u3008s,R\u3009 \u2208 S | \u3008s,R\u3009 satisfies SLAF0[a](\u03d5)}\nPROOF See Section B.2. 2 This theorem allows us to identify SLAF0 with SLAF , and we do so throughout the rest of the paper. In particular, we show that polynomial-time algorithms for SLAF in special cases are correct by showing that their output is logically equivalent to that of SLAF0.\nUSING THE OUTPUT OF SLAF0 The output of any algorithm for SLAF of a transition belief formula is a logical formula. The way to use this formula for answering questions about SLAF depends on the query and the form of the output formula. When we wish to find if a transition model and state are possible, we wish to see if M |= \u03d5, for M an interpretation of L = P \u222a LA and \u03d5 the output of SLAF0.\nThe answer can be found by a simple model-checking algorithm2. For example, to check that an interpretation satisfies a logical formula we assign the truth values of the variables in the interpretation into the formula; computing the truth value of the formula can be done in linear time.\nThus, this type of query from SLAF takes linear time in the size of the output formula from SLAF because the final query is about a propositional interpretation and a propositional formula.\nWhen we wish to find if a transition model is possible or if a state is possible, we can do so with propositional satisfiability (SAT) solver algorithms (e.g., Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). Similarly, when we wish to answer whether all the possible models satisfy a property we can use a SAT solver.\nExample 3.4 Recall Example 2.4 in which we discuss a locked door and three combinations. Let \u03d50 = locked, and let \u03d51 = SLAF0[unlock2, locked](\u03d50). We wish to find if \u03d51 implies that trying to unlock the door with key 2 fails to open it. This is equivalent to asking if all models consistent with \u03d51 give the value TRUE to unlock2lockedlocked.\nWe can answer such a query by taking the SLAF0 output formula, \u03d51, and checking if \u03d51 \u2227 \u00acunlock2lockedlocked is SAT (has a model). (Follows from the Deduction Theorem for propositional logic: \u03d5 |= \u03c8 iff \u03d5 \u2227 \u00ac\u03c8 is not SAT.)\nOne example application of this approach is the goal achievement algorithm of Chang and Amir (2006). It relies on SAT algorithms to find potential plans given partial knowledge encoded as a transition belief formula.\n2. This is not model checking in the sense used in the Formal Verification literature. There, the model is a transition model, and checking is done by updating a formula in OBDD with some transformations\nOur zeroth-level algorithm may enable more compact representation, but it does not guarantee it, nor does it guarantee tractable computation. In fact, no algorithm can maintain compact representation or tractable computation in general. Deciding if a clause is true as a result of SLAF is coNP-hard because the similar decision problem for Logical Filtering is coNP-hard (Eiter & Gottlob, 1992; Amir & Russell, 2003) even for deterministic actions. (The input representation for both problems includes an initial belief state formula in CNF. The input representation for Filtering includes further a propositional encoding in CNF of the (known) transition relation.)\nAlso, any representation of transition belief states that uses poly(|P|) propositions grows exponentially (in the number of time steps and |P|) for some starting transition belief states and action sequences, when actions are allowed to be nondeterministic3. The question of whether such exponential growth must happen with deterministic actions and flat formula representations (e.g., CNF, DNF, etc.; see Darwiche & Marquis, 2002) is open (logical circuits are known to give a solution for deterministic actions, when a representation is given in terms of fluents at time 0, Shahaf et al., 2006)."}, {"heading": "4. Factored Formula Update", "text": "Update of any representation is hard when it must consider the set of interactions between all parts of the representation. Operations like those used in SLAF0 consider such interactions, manipulate them, and add many more interactions as a result. When processing can be broken into independent pieces, computation scales up linearly with the number of pieces (i.e., computation time is the total of times it takes for each piece separately). So, it is important to find decompositions that enable such independent pieces and computation. Hereforth we examine one type of decomposition, namely, one that follows logical connectives.\nLearning world models is easier when SLAF distributes over logical connectives. A function, f , distributes over a logical connective, \u00b7 \u2208 {\u2228,\u2227, ...}, if f(\u03d5 \u00b7 \u03c8) \u2261 f(\u03d5) \u00b7 f(\u03c8). Computation of SLAF becomes tractable, when it distributes over \u2227,\u2228. The bottleneck of computation in that case becomes computing SLAF for each part separately.\nIn this section we examine conditions that guarantee such distribution, and present a linear-time algorithm that gives an exact solution in those cases. We will also show that the same algorithm gives a weaker transition belief formula when such distribution is not possible.\nDistribution properties that always hold for SLAF follow from set theoretical considerations and Theorem 3.3:\nTheorem 4.1 For \u03d5,\u03c8 transition belief formulas, a action,\nSLAF [a](\u03d5 \u2228 \u03c8) \u2261 SLAF [a](\u03d5) \u2228 SLAF [a](\u03c8) |= SLAF [a](\u03d5 \u2227 \u03c8)\u21d2 SLAF [a](\u03d5) \u2227 SLAF [a](\u03c8)\nPROOF See Appendix B.3. 2 Stronger distribution properties hold for SLAF whenever they hold for Logical Filtering.\nTheorem 4.2 Let \u03c11, \u03c12 be transition belief states.\nSLAF [a](\u03c11 \u2229 \u03c12) = SLAF [a](\u03c11) \u2229 SLAF [a](\u03c12)\n3. This follows from a theorem about filtering by Amir and Russell (2003), even if we provide a proper axiomatization (note that our axiomatization above is for deterministic actions only).\niff for every R Filter[a](\u03c1R1 \u2229 \u03c1 R 2 ) = Filter[a](\u03c1 R 1 ) \u2229 Filter[a](\u03c1 R 2 ).\nWe conclude the following corollary from Theorems 3.3, 4.2 and theorems by Amir and Russell (2003). Corollary 4.3 For \u03d5,\u03c8 transition belief formulas, a action, SLAF [a](\u03d5 \u2227 \u03c8) \u2261 SLAF [a](\u03d5) \u2227 SLAF [a](\u03c8) if for every relation R in \u03c1\u03d5,\u03c1\u03c8 one of the following holds:\n1. a in R maps states 1:1 2. a in R has no conditional effects, \u03d5\u2227\u03c8 includes all its prime implicates, and we observe if a\nfails 3. The state is known for R: for at most one s, \u3008s,R\u3009 \u2208 \u03c1\u03d5 \u222a \u03c1\u03c8.\nCondition 2 combines semantics and syntax. It is particularly useful in the correct computation of SLAF in later sections. It states when \u03d5 \u2227 \u03c8 has a particular syntactic form (namely, together they include their joint prime implicates), and a is simple enough (but not necessarily 1:1), then computation of SLAF can be broken into separate SLAF of \u03d5 and \u03c8.\nFigure 3 presents Procedure Factored-SLAF, which computes SLAF exactly when the conditions of Corollary 4.3 hold. Consequently, Factored-SLAF returns an exact solution whenever our actions are known to be 1:1. If our actions have no conditional effects and their success/failure is observed, then a modified Factored-SLAF can solve this problem exactly too (see Section 5).\nIf we pre-compute (and cache) the 2n possible responses of Literal-SLAF, then every time step t in this procedure requires linear time in the representation size of \u03d5, our transition belief formula at that time step. This is a significant improvement over the (super exponential) time taken by a straightforward algorithm, and over the (potentially exponential) time taken by general-purpose consequence finding used in our zeroth-level SLAF procedure above.\nTheorem 4.4 Step-SLAF(a, o, \u03d5) returns a formula \u03d5\u2032 such that SLAF [a, o](\u03d5) |= \u03d5\u2032. If every run of Literal-SLAF takes time c, then Step-SLAF takes time O(|\u03d5|c). (recall that |\u03d5| is the syntactic, representation size of \u03d5.) Finally, if we assume one of the assumptions of Corollary 4.3, then \u03d5\u2032 \u2261 SLAF [a, o](\u03d5).\nA belief-state formula is a transition belief formula that has no effect propositions, i.e., it includes only fluent variables and no propositions of the form aFG. This is identical to the traditional use of the term belief-state formula, e.g., (Amir & Russell, 2003). We can give a closed-form solution for the SLAF of a belief-state formula (procedure Literal-SLAF in Figure 3). This makes procedure Literal-SLAF tractable, avoiding general-purpose inference in filtering a single literal, and also allows us to examine the structure of belief state formulas in more detail.\nTheorem 4.5 For belief-state formula \u03d5 \u2208 L(P), action a, Ca = \u2227 G\u2208G,l\u2208F (a l G \u2228 a \u00acl G ), and G1, ..., Gm \u2208 G all the terms in G such that Gi |= \u03d5,\nSLAF [a](\u03d5) \u2261 \u2227\nl1,...,lm\u2208F\nm\u2228\ni=1\n(li \u2228 a \u00acli Gi ) \u2227 Ca\nHere, \u2227 l1,...,lm\u2208F\nmeans a conjunction over all possible (combinations of) selections of m literals from F .\nPROOF See Appendix B.4. 2 This theorem is significant in that it says that we can can write down the result of SLAF in a prescribed form. While this form is still potentially of exponential size, it boils down to simple computations. Its proof follows by a straightforward (though a little long) derivation of the possible prime implicates of SLAF [a](\u03d5t).\nA consequence of Theorem 4.5 is that we can implement Procedure Literal-SLAF using the equivalence\nSLAF [a](l) \u2261 { Theorem 4.5 L(l) \u2286 P l \u2227 SLAF [a](TRUE) otherwise }\nNotice that the computation in Theorem 4.5 for \u03d5 = l a literal is simple because G1, ..., Gm are all the complete terms in L(P) that include l. This computation does not require a general-purpose consequence finder, and instead we need to answer 2n+1 queries at the initialization phase, namely, storing in a table the values of SLAF [a](l) for all l = p and l = \u00acp for p \u2208 P and also for l = TRUE.\nIn general,m could be as high as 2|P|, the number of complete terms in G, and findingG1, ..., Gm may take time exponential in |P|. Still, the simplicity of this computation and formula provide the basic ingredients needed for efficient computations in the following sections for restricted cases. It should also give guidelines on future developments of SLAF algorithms."}, {"heading": "5. Compact Model Representation", "text": "Previous sections presented algorithms that are potentially intractable for long sequences of actions and observations. For example, in Theorem 4.5, m could be as high as 2|P|, the number of complete terms in G. Consequently, clauses may have exponential length (in n = |P|) and there may be a super-exponential number of clauses in this result.\nIn this section we focus on learning action models efficiently in the presence of action preconditions and failures. This is important for agents that have only partial domain knowledge and are therefore likely to attempt some inexecutable actions.\nWe restrict our attention to deterministic actions with no conditional effects, and provide an overall polynomial bound on the growth of our representation, its size after many steps, and the\ntime taken to compute the resulting model. This class of actions generalizes STRIPS (Fikes, Hart, & Nilsson, 1972), so our results apply to a large part of the AI-planning literature.\nWe give an efficient algorithm for learning both non-conditional deterministic action effects and preconditions, as well as an efficient algorithm for learning such actions\u2019 effects in the presence of action failures."}, {"heading": "5.1 Actions of Limited Effect", "text": "In many domains we can assume that every action a affects at most k fluents, for some small k > 0. It is also common to assume that our actions are STRIPS, and that they may fail without us knowing, leaving the world unchanged. Those assumptions together allow us to progress SLAF with a limited (polynomial factor) growth in the formula size.\nWe use a language that is similar to the one in Section 3, but which uses only action propositions alG with G being a fluent term of size k (instead of a fluent term of size n in G). Semantically, all1\u2227...\u2227lk \u2261 \u2227 lk+1,...,ln all1\u2227...\u2227ln .\nTheorem 5.1 Let \u03d5 \u2208 L(P) be a belief-state formula, and a a STRIPS action with \u2264 k fluents affected or in the precondition term. Let Gk be the set of all terms of k fluents in L(P) that are consistent with \u03d5. Then,\nSLAF [a](\u03d5) \u2261 \u2227\nG1, ..., Gk \u2208 G k G1 \u2227 ... \u2227Gk |= \u03d5 l1, ..., lk \u2208 F\nk\u2228\ni=1\n(li \u2228 a \u00acli Gi ) \u2227 Ca\nHere, \u2227\n... refers to a conjunction over all possible (combinations of) selections of m literals from F and G1, ..., Gk from Gk such that G1 \u2227 ... \u2227Gk |= \u03d5.\nPROOF See Section B.5. 2 The main practical difference between this theorem and Theorem 4.5 is the smaller number of terms that need to be checked for practical computation. The limited language enables and entails a limited number of terms that are at play here. Specifically, when a has at most k literals in its preconditions, we need to check all combinations of k terms G1, ..., Gk \u2208 Gk, computation that is bounded by O(exp(k)) iterations.\nThe proof uses two insights. First, if a has only one case in which change occurs, then every clause in Theorem 4.5 is subsumed by a clause that is entailed by SLAF [a](\u03d5t), has at most one aliGi per literal li (i.e., li 6= lj for i 6= j) and Gi is a fluent term (has no disjunctions). Second, every alG with G term is equivalent to a formula on alGi with Gi terms of length k, if a affects only k fluents.\nThus, we can encode all of the clauses in the conjunction using a subset of the (extended) action effect propositions, alG, with G being a term of size k. There are O(nk) such terms, and O(nk+1) such propositions. Every clause is of length 2k, with the identity of the clause determined by the first half (the set of action effect propositions). Consequently, SLAF [a](\u03d5t) takes O(nk\n2+k \u00b7 k2) space to represent using O(nk2+k) clauses of length \u2264 2k."}, {"heading": "5.2 Actions of No Conditional Effects: Revised Language", "text": "In this section we reformulate the representation that we presented above in Section 3.1. Let L0f = \u22c3 a\u2208A{a\nf , af\u25e6, a\u00acf , a[f ], a[\u00acf ]} for every f \u2208 P . Let the vocabulary for the formulas representing transition belief states be defined as L = P \u222a L0f . The intuition behind propositions in this vocabulary is as follows: \u2022 al \u2013 \u201ca causes l\u201d for literal l. Formally, al \u2261 \u2227 s\u2208S a l s.\n\u2022 af\u25e6 \u2013 \u201ca keeps f\u201d. Formally, af\u25e6 \u2261 \u2227 s\u2208S((s\u21d2 f)\u21d2 a f s ) \u2227 ((s\u21d2 \u00acf)\u21d2 a \u00acf s ).\n\u2022 a[l] \u2013 \u201ca causes FALSE if \u00acl\u201d. Formally, a[l] \u2261 \u2227 s\u2208S(s \u21d2 \u00acl) \u21d2 (a l s \u2227 a \u00acl s ). (Thus, l is a\nprecondition for executing a, and it must hold when a executes.) For each model of a transition belief formula overL, the valuation of the fluents fromP defines a state. The valuation of the propositions from all L0f defines an unconditional deterministic transition relation as follows: action proposition af (a\u00acf ) is true if and only if action a in the transition relation causes f (\u00acf ) to hold after a is executed. Action proposition af\u25e6 is true if and only if action a does not affect fluent f . Action proposition a[f ] (a[\u00acf ]) is true if and only if f (\u00acf ) is in the precondition of a. We assume the existence of logical axioms that disallow \u201cinconsistent\u201d or \u201cimpossible\u201d models. These axioms are:\n1. af \u2228 a\u00acf \u2228 af\u25e6\n2. \u00ac(af \u2227 a\u00acf ) \u2227 \u00ac(a\u00acf \u2227 af\u25e6) \u2227 \u00ac(af \u2227 af\u25e6)\n3. \u00ac ( a[f ] \u2227 a[\u00acf ] )\nfor all possible a \u2208 A and f \u2208 P. The first two axioms state that in every action model, exactly one of af , a\u00acf , or af\u25e6 must hold (thus, a causes f , its negation, or keeps f unchanged). The last axiom disallows interpretations where both a[f ] and a[\u00acf ] hold. We state these axioms so that we do not need to represent these constraints explicitly in the transition belief formula itself.\nWe will use the set theoretic and propositional logic notations for transition belief states interchangeably. Note that the vocabulary we have defined is sufficient for describing any unconditional STRIPS action model, but not any deterministic action model in general.\nExample 5.2 Consider the domain from Example 2.2. The transition belief state \u03d5 can be represented by the transition belief formula:\nlocked \u2227\n((unlock\u00aclocked1 \u2227 unlock locked\u25e6 2 \u2227 unlock locked\u25e6 3 ) \u2228 (unlocklocked\u25e61 \u2227 unlock \u00aclocked 2 \u2227 unlock locked\u25e6 3 ) \u2228 (unlocklocked\u25e61 \u2227 unlock locked\u25e6 2 \u2227 unlock \u00aclocked 3 )).\n2\nWe provide an axiomatization that is equivalent to SLAF and is a special case of Teff (1) with our notation above. We do this over P and P \u2032. Recall that we intend primed fluents to represent the value of the fluent immediately after action a is taken.\n\u03c4eff(a) \u2261 \u2227\nf\u2208P\nPrea,f \u2227 Effa,f\nPrea,f \u2261 \u2227\nl\u2208{f,\u00acf}\n(a[l] \u21d2 l)\nEffa,f \u2261 \u2227\nl\u2208{f,\u00acf}\n((al \u2228 (af\u25e6 \u2227 l))\u21d2 l\u2032) \u2227 (l\u2032 \u21d2 (al \u2228 (af\u25e6 \u2227 l))).\nHere Prea,f describes the precondition of action a. It states that if literal l occurs in the precondition of a then literal l must have held in the state before taking a. The formula Effa,f describes the effects of action a. It states that the fluents before and after taking the action must be consistent according to the action model defined by the propositions af , a\u00acf , and af\u25e6.\nNow we can show that the revised axiomatization of action models, \u03c4eff, leads to an equivalent definition to SLAF within our restricted action models.\nTheorem 5.3 For any successful action a, SLAF [a](\u03d5) \u2261 CnL\u222aP \u2032 (\u03d5 \u2227 \u03c4eff(a))[P \u2032/P].\nPROOF See Appendix B.6. 2"}, {"heading": "5.3 Always-Successful Non-Conditional Actions", "text": "We are now ready to present an algorithm that learns effects for actions that have no conditional effects. The algorithm allows actions that have preconditions that are not fully known. Still, it assumes that the filtered actions all executed successfully (without failures), so it cannot effectively learn those preconditions (e.g., it would know only some more than it knew originally about those preconditions after seeing a sequence of events). Such a sequence of actions might, for example, be generated by an expert agent whose execution traces can be observed.\nThe algorithm maintains transition belief formulas in a special fluent-factored form, defined below. By maintaining formulas in this special form, we will show that certain logical consequence finding operations can be performed very efficiently. A formula is fluent-factored, if it is the conjunction of formulas \u03d5f such that each \u03d5f concerns only one fluent, f , and action propositions.\nAlso, for every fluent, f , \u03d5f is conjunction of a positive element, a negative element, and a neutral one \u03d5f \u2261 (\u00acf \u2228 explf ) \u2227 (f \u2228 expl\u00acf ) \u2227 Af , with explf , expl\u00acf , Af formulae over action propositions af , a\u00acf , a[f ], a[\u00acf ], and af\u25e6 (possibly multiple different actions). The intuition here is that explf and expl\u00acf are all the possible explanations for f being true and false, respectively. Also, Af holds knowledge about actions\u2019 effects and preconditions on f , knowledge that does not depend on f \u2019s current value. Note that any formula in L(Lf ) can be represented as a fluentfactored formula. Nonetheless, a translation sometimes leads to a space blowup, so we maintain our representation in this form by construction.\nThe new learning algorithm, AS-STRIPS-SLAF4, is shown in Figure 4. To simplify exposition, it is described for the case of a single action-observation pair, though it should be obvious how to apply the algorithm to sequences of actions and observations. Whenever an action is taken, first the subformulas Af , explf , and expl\u00acf for each \u03d5f are updated according to steps 1.(a)-(c). Then,\n4. AS-STRIPS-SLAF extends AE-STRIPS-SLAF (Amir, 2005) by allowing preconditions for actions\nwhen an observation is received, each \u03d5f is updated according to the observation according to steps 1.(d)-(e). Step 2 merely indicates that in most implementations, it is likely that some simplification procedure will be used on the formula such as subsumption elimination. However, the use of any such simplification procedure is not strictly necessary in order for the theoretical guarantees of our algorithm to hold.\nFor example, if we know nothing about actions that affect f (e.g., when we start our exploration), then \u03d5f = (\u00acf \u2228TRUE)\u2227 (f \u2228TRUE)\u2227TRUE. With this representation, SLAF [a](\u03d5f ) is the conjunction (\u00acf\u2228explf )\u2227(f\u2228expl\u00acf )\u2227Af as computed by step 1 of Procedure AS-STRIPS-SLAF. A similar formula holds for observations.\nThe following theorem shows the correctness of the algorithm. It shows that the steps taken by the algorithm produce a result equivalent to the logical consequence-finding characterization of SLAF of Theorem 5.3.\nTheorem 5.4 SLAF[\u3008a, o\u3009](\u03d5) \u2261 AS-STRIPS-SLAF[\u3008a, o\u3009](\u03d5) for any fluent-factored formula \u03d5, successfully executed action a, and observation term o.\nPROOF See Appendix B.7 2 The time and space complexity of procedure AS-STRIPS-SLAF are given in the following theorem. As a time guarantee, it is shown that the procedure takes linear time in the size of the input formula. Under the condition that the algorithm receives observations often enough\u2014specifically\nthat every fluent is observed at least once every k calls to the procedure\u2014it is possible to show that the transition belief formula remains in k-CNF indefinitely (recall that \u03d5 is in k-CNF for some fixed k, if \u03d5 is a conjunction of clauses each of size \u2264 k). Thus, regardless of the length of actionobservation input sequence, the output of AS-STRIPS-SLAF and the value of \u03d5 throughout its computation is in k-CNF. This amounts to a space guarantee on the size of the formula.\nTheorem 5.5 The following are true of AS-STRIPS-SLAF:\n1. The procedure takes linear time in the size of the input formula for a single action, observation pair input.\n2. If for every fluent and every k steps there is an observation of that fluent in one of those steps, and the input formula is in k-CNF, them the resulting formula (after an arbitrary number of steps) is in k-CNF.\n3. If the input of AS-STRIPS-SLAF is fluent-factored, then so is its output.\nPROOF See Appendix B.8 2 The following corollary follows immediately from the above.\nCorollary 5.6 In order to process T steps of actions and observations, AS-STRIPS-SLAF requires O (T \u00b7 |P|) time. Additionally, if every fluent is observed every at most k steps, then the resulting\nformula always has size that is O ( |P| \u00b7 |A|k ) .\nThis Corollary holds because Theorem 5.5(2) guarantees a bound on the size of our belief-state formula at any point in the algorithm."}, {"heading": "5.4 Learning Actions Which May Fail", "text": "In many partially observable domains, a decision-making agent cannot know beforehand whether each action it decides to take will fail or succeed. In this section we consider possible action failure, and assume that the agent knows whether each action it attempts fails or succeeds after trying the action.\nMore precisely, we assume that there is an additional fluent OK observed by the agent such that OK is true if and only if the action succeeded. A failed action, in this case, may be viewed as an extra \u201cobservation\u201d by the agent that the preconditions for the action were not met. That is, an action failure is equivalent to the observation\n\u00ac \u2227\nf\u2208P\n(a[f ] \u21d2 f) \u2227 (a[\u00acf ] \u21d2 \u00acf). (3)\nAction failures make performing the SLAF operation considerably more difficult. In particular, observations of the form (3) cause interactions between fluents where the value of a particular fluent might no longer depend on only the action propositions for that fluent, but on the action propositions for other fluents as well. Transition belief states can no longer be represented by convenient fluentfactored formulas in such cases, and it becomes more difficult to devise algorithms which give useful time and space performance guarantees.\nAs we shall demonstrate, action failures can be dealt with tractably if we assume that the action preconditions are known by the agent. That is, the agent must learn the effects of the actions it can take, but does not need to learn the preconditions of these actions. In particular, this means that for each action a, the algorithm is given access to a formula (more precisely, logical term) Pa describing the precondition of action a. Clearly, because the algorithm does not need to learn the preconditions of its actions, we can restrict the action proposition vocabulary used to describe belief states to the ones of the forms af , a\u00acf , and af\u25e6, as we no longer need action propositions of the forms a[f ] or a[\u00acf ].\nWe present procedure PRE-STRIPS-SLAF5 (Figure 5) that performs SLAF on transition belief formulas in the presence of action failures for actions of non-conditional effects. It maintains transition belief formulas as conjunctions of disjunctions of fluent-factored formulas (formulas of the form \u03d5 = \u2227 i \u2228 j \u03d5i,j where each \u03d5i,j is fluent factored). Naturally, such formulas are a superset of all fluent-factored formulas.\n5. PRE-STRIPS-SLAF is essentially identical to CNF-SLAF (Shahaf et al., 2006)\nThe algorithm operates as follows: When an action executes successfully (and an ensuing observation is received), each of the component fluent-factored formulas \u03d5i,j is filtered separately according to the AS-STRIPS-SLAF procedure on the action-observation pair (Step 2). On the other hand, when an action fails, a disjunction of fluent-factored formulas is appended to the transition belief formula (Step 1). Each component of the disjunction corresponds to one of the possible reasons the action failed (i.e., to one of the literals occurring in the action\u2019s precondition). Finally, as observations are accumulated by the learning algorithm, it collapses disjunctions of fluent-factored formulas occurring in the belief formula together (Step 3) or simplifies them generally (Step 4), decreasing the total size of the formula. As with the case of AS-STRIPS-SLAF, these simplification steps are not necessary in order for our time and space guarantees to hold.\nThe proof of correctness of Algorithm PRE-STRIPS-SLAF relies on our distribution results from Section 4, Theorem 4.1 and Corollary 4.3.\nWe proceed to show the correctness of PRE-STRIPS-SLAF. The following theorem shows that the procedure always returns a filtered transition belief formula that is logically weaker than the exact result, so it always produces a safe approximation. Additionally, the theorem shows that under the conditions of Corollary 4.3, the filtered transition belief formula is an exact result.\nTheorem 5.7 The following are true:\n1. SLAF[a, o](\u03d5) |= PRE-STRIPS-SLAF[a, o](\u03d5)\n2. PRE-STRIPS-SLAF[a, o](\u03d5) \u2261 SLAF[a, o](\u03d5) if Corollary 4.3 holds.\nPROOF See Appendix B.9. 2 Now we consider the time and space complexity of the algorithm. The following theorem shows that (1) the procedure is time efficient, and (2) given frequent enough observations (as in Theorem 5.5), the algorithm is space efficient because the transition belief formula stays indefinitely compact.\nTheorem 5.8 The following are true of PRE-STRIPS-SLAF:\n1. The procedure takes time linear in the size of the formula for a single action, observation pair input.\n2. If every fluent is observed every at most k steps and the input formula is in m \u00b7 k-CNF, then the filtered formula is in m \u00b7k-CNF, where m is the maximum number of literals in any action precondition.\nPROOF See Appendix B.10 2 Therefore, we get the following corollary:\nCorollary 5.9 In order to process T steps of actions and observations, PRE-STRIPS-SLAF requires O (T \u00b7 |P|) time. If every fluent is observed at least as frequently as every k steps, then the resulting\nformula always has size that is O ( |P| \u00b7 |A|mk ) ."}, {"heading": "6. Building on Our Results", "text": "In this section we describe briefly how one might extend our approach to include an elaborate observation model, bias, and parametrized actions."}, {"heading": "6.1 Expressive Observation Model", "text": "The observation model that we use throughout this paper is very simple: at every state, if a fluent is observed to have value v, then this is its value in the current state. We can consider an observation model that is more general.\nAn observation model, O, is a set of logical sentences that relates propositions in a set Obs with fluents in P . Obs includes propositions that do not appear in P , and which are independent of the previous and following state (times t\u2212 1 and t+ 1) given the fluents at time t.\nSLAF with o is the result of conjoining \u03d5t withCnLt(o\u2227O), i.e., finding the prime implicates of o\u2227O and conjoining those with \u03d5t. We can embed this extension into our SLAF algorithms above, if we can maintain the same structures that those algorithms use. If O is in k-CNF and at every step we observe all but (at most) 1 variable, then finding the prime implicates is easy. Embedding this into the transition-belief formula is done by conjunction of these prime implicates with the formula, and removal of subsumed clauses. The resulting formula is still fluent factored, if the input was fluent factored. Then, the algorithms above remain applicable with the same time complexity, by replacing ot with the prime implicates of ot \u2227Ot.\nUsing the Model The algorithms we described above provide an exact solution to SLAF, and all the tuples \u3008s,R\u3009 that are in this solution are consistent with our observations. They compute a solution to SLAF that is represented as a logical formula. We can use a SAT solver (e.g., Moskewicz et al., 2001) to answer queries over this formula, such as checking if it entails af , for action a and fluent f . This would show if in all consistent models action a makes f have the value TRUE.\nThe number of variables in the result formula is always independent of T , and is linear in |P| for some of our algorithms. Therefore, we can use current SAT solvers to treat domains of 1000 features and more.\nPreference and Probabilistic Bias Many times we have information that leads us to prefer some possible action models over others. For example, sometimes we can assume that actions change only few fluents, or we suspect that an action (e.g., open-door) does not affect some features (e.g., position) normally. We can represent such bias using a preference model (e.g., McCarthy, 1986; Ginsberg, 1987) or a probabilistic prior over transition relations (e.g., Robert, Celeux, & Diebolt, 1993).\nWe can add this bias at the end of our SLAF computation, and get an exact solution if we can compute the effect of this bias together with a logical formula efficiently. Preferential biases were studied before and fit easily with the result of our algorithms (e.g., we can use implementations of Doherty, Lukaszewicz, & Szalas, 1997, for inference with such bias).\nAlso, algorithms for inference with probabilistic bias and logical sentences are now emerging and can be used here too (Hajishirzi & Amir, 2007). There, the challenge is to not enumerate tentative models explicitly, a challenge that is overcome with some success in the work of Hajishirzi and Amir (2007) for the similar task of filtering. We can use such algorithms to apply probabilistic bias to the resulting logical formula.\nFor example, given a probabilistic graphical model (e.g., Bayesian Networks) and a set of propositional logical sentences, we can consider the logical sentences as observations. With this approach, a logical sentence \u03d5 gives rise to a characteristic function \u03b4\u03d5(\u2212\u2192x ) which is 1 when \u2212\u2192x satisfies \u03d5 and 0 otherwise. For a conjunction of clauses we get a set of such functions (one per clause). Thus, inference in the combined probabilistic-logical system is a probabilistic inference. For example,\none can consider variable elimination (e.g., Dechter, 1999) in which there are additional potential functions.\nParametrized Actions In many systems and situations it is natural to use parametrized actions. These are action schemas whose effect depend on their parameters, but their definition applies identically to all instantiations.\nFor example, move(b, x, y) can be an action which moves b from position x to position y, with b, x, y as parameters of the action. These are very common in planning systems (e.g., STRIPS, PDDL, Situation Calculus). A complete treatment of parameterized actions is outside the scope of this paper, but we give guidelines for a generalization of our current approach for such actions.\nConsider a domain with a set of fluent predicates and a universe of named objects. The propositional fluents that are defined in this domain are all ground instantiations of predicate fluents. SLAF can work on this set of propositional fluents and instantiated actions in the same manner as for the rest of this paper. We have action propositions a(\u2212\u2192X )lG instantiated for every vector of object names\u2212\u2192 X .\nThe different treatment comes in additional axioms that say that \u2200\u2212\u2192x ,\u2212\u2192y .a(\u2212\u2192x )lG \u21d0\u21d2 a( \u2212\u2192y )lG. Inference over a transition belief state with these axioms will be able to join information collected about different instantiations of those actions. We expect that a more thorough treatment will be able to provide more efficient algorithms whose time complexity depend on the number of action schemas instead of the number of instantiated actions.\nSeveral approaches already start to address this problem, including the work of Nance, Vogel, and Amir (2006) for filtering and the work of Shahaf and Amir (2006) for SLAF."}, {"heading": "7. Experimental Evaluation", "text": "Previous sections discussed the problem settings that we consider and algorithms for their solutions. They showed that modifying traditional settings for learning in dynamic partially observable domains is important. Determinism alone does not lead to tractability, but our additional assumptions of simple, logical action structure and bounded from 0 frequency of observations for all fluents do. Specifically, so far we showed that the time and space for computing SLAF of a length-T time sequence over n fluents are polynomial in T and n.\nThis section considers the practical considerations involved in using our SLAF procedures. In particular, it examines the following questions: \u2022 How much time and space do SLAF computations take in practice? \u2022 How much time is required to extract a model from the logical formula in the result of our\nSLAF procedures? \u2022 What is the quality of the learned model (taking an arbitrary consistent model)? How far is it\nfrom the true (generating) model? \u2022 Do the conditions for the algorithms\u2019 correctness hold in practice? \u2022 Can the learned model be used for successful planning and execution? How do the learning\nprocedures fit with planning and execution? We implemented our algorithms and ran experiments with AS-STRIPS-SLAF over the following domains taken from the 3rd International Planning Competition (IPC): Drivelog, Zenotravel, Blocksworld, and Depots (details of the domains and the learning results appear in Appendix C).\nEach such experiment involves running a chosen algorithm over a sequence of randomly generated action-observation sequences of 5000 steps. Information was recorded every 200 steps.\nThe random-sequence generator receives the correct description of the domain, specified in PDDL (Ghallab, Howe, Knoblock, McDermott, Ram, Veloso, Weld, & Wilkins, 1998; Fox & Long, 2002) (a plannig-domain description language), the size of the domain, and a starting state. (The size of the domain is the number of propositional fluents in it. It is set by a specification of the number of objects in the domain and the number and arity of predicates in the domain.) It generates a valid sequence of actions and observations for this domain and starting state, i.e., a sequence that is consistent with the input PDDL to that generator but in which actions may fail (action failure is consistent with the PDDL if the action is attempted in a state in which it canot execute).\nFor our experiments, we chose to have observations as follows: In every time step we select 10 fluents uniformly at random to observe. We applied no additional restrictions (such as making sure each fluent was observed every fixed k steps).\nOur SLAF algorithm receives only such a sequences of actions and observations, and no domain information otherwise (e.g., it does not receive the size of the domain, the fluents, the starting state, or the PDDL). The starting knowledge for the algorithm is the empty knowledge, TRUE.\nFor each domain we ran the algorithm over different numbers of propositional fluents (19 to 250 fluents). We collected the time and space taken for each SLAF computation and plotted them as a function of the input-sequence length (dividing by t the total computation time for t steps). The time and space results are shown in Figures 6, 7, 8, and 9. The graphs are broken into the different domains and compare the time and space taken for different domain sizes. The time is SLAF-time without CNF simplification (e.g. we do not remove subsumed clauses)\nHow much time and space do SLAF computations take in practice? We can answer the first question now. We can observe in these figures that the time per step remains relatively constant throughout execution. Consequently, the time taken to perform SLAF of different domains grows linearly with the number of time steps. Also, we see that the time for SLAF grows with the domain size, but scales easily for moderate domain sizes (1ms per step of SLAF for domains of 200 fluents).\nHow much time is required to extract a model from the logical formula in the result of our SLAF procedures? Our SLAF procedures return a logical formula for each sequence of actions and observations. We need to apply further work to extract a candidate (consistent) model from that formula. This computation is done with a SAT solver for CNF formulas.\nWhat is the quality of the learned model (taking an arbitrary consistent model)? How far is it from the true (generating) model? There are sometimes many possible models, and with little further bias we must consider each of them possible and as likely. We decided to introduce one such bias, namely, that actions are instances of actions schemas. Thus, the actions are assumed to have the same effect on their parameters (or objects), given properties of those parameters. Thus, actions\u2019 effects are assumed independent of the identity of those parameter.\nSo, with the vanilla implementation, there are propositions which look like\n((STACK E G) CAUSES (ON E G)) ((STACK E G) CAUSES (ON G E)) ((STACK A B) CAUSES (ON A B)) ((STACK A A) CAUSES (ON A A)) etc.\nInstead, we replace ground propositions like those above with schematized propositions:\n((STACK ?X ?Y) CAUSES (ON ?X ?Y)) ((STACK ?X ?Y) CAUSES (ON ?Y ?X)) ((STACK ?X ?Y) CAUSES (ON ?X ?Y)) etc.\nThus, the belief-state formula looks something like:\n(AND (AND (OR (ON E G) (OR ((STACK ?X ?Y) CAUSES (NOT (ON ?X ?Y))) (AND ((STACK ?X ?Y) KEEPS (ON ?X ?Y)) (NOT ((STACK ?X ?Y) NEEDS (ON ?X ?Y))))))\n...\nAn example fragment of a model (the complete output is given in Appendix C) that is consistent with our training data is\nBlocksworld domain: converting to CNF * 209 fluents clause count: 235492 * 1000 randomly selected actions variable count: 187 * 10 fluents observed per step adding clauses * \"schematized\" learning calling zchaff\n* 1:1 precondition heuristics parsing result SLAF time: 2.203 Inference time: 42.312 Learned model:\n(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB))) (UNSTACK NEEDS (CLEAR ?OB)) (UNSTACK NEEDS (ARM-EMPTY))\n(UNSTACK NEEDS (NOT (HOLDING ?OB))) (UNSTACK NEEDS (ON ?OB ?UNDEROB)) (UNSTACK CAUSES (CLEAR ?UNDEROB)) (UNSTACK CAUSES (NOT (CLEAR ?OB))) (UNSTACK CAUSES (NOT (ARM-EMPTY))) (UNSTACK CAUSES (HOLDING ?OB)) (UNSTACK CAUSES (NOT (ON ?OB ?UNDEROB))) (UNSTACK KEEPS (ON-TABLE ?UNDEROB)) (UNSTACK KEEPS (ON-TABLE ?OB)) (UNSTACK KEEPS (HOLDING ?UNDEROB)) (UNSTACK KEEPS (ON ?UNDEROB ?UNDEROB)) (UNSTACK KEEPS (ON ?OB ?OB)) (UNSTACK KEEPS (ON ?UNDEROB ?OB)) ...\nSometimes, there are multiple possible schematized propositions that correspond to a ground action proposition, in which case we disjoin the propositions together when doing the replacement (i.e., a single ground propositional symbol gets replaced by a disjunction of schema propositions).\nThis replacement is a simple procedure, but one that is effective in both deriving more information for fewer steps and also in speeding up model finding from the SLAF formula. We implemented it to run while SLAF runs. One could do this during the SAT-solving portion of the algorithm, with an equivalent result.\nRegarding the latter, we ran into scaling issues with the SAT solver (ZChaff , Moskewicz et al., 2001; Tang, Yinlei Yu, & Malik, 2004) and the common lisp compiler in large experiments. We got around these issues by applying the replacement scheme above, thus reducing greatly the number of variables that the SAT solver handles.\nAnother issue that we ran into is that the SAT solver tended to choose models with blank preconditions (the sequences that we used in these experiments include no action failure, so \u2019no preconditions\u2019 is never eliminated by our algorithm). To add some \u201dbias\u201d to the extracted action model, we added axioms of the following form:\n(or (not (,a causes ,f)) (,a needs (not ,f))) (or (not (,a causes (not ,f))) (,a needs ,f))\nThese axioms state that if an action a causes a fluent f to hold, then a requires f to not hold in the precondition (similarly, there is an analagous axiom for \u00acf ). Intuitively, these axioms cause the sat solver to favor 1:1 action models. We got the idea for this heuristic from the work of Wu, Yang, and Jiang (2007), which uses a somewhat similar set of axioms to bias their results in terms of learning preconditions. Clearly, these axioms don\u2019t always hold, and in the results, one can see that the learned preconditions are often inaccurate.\nSome other inaccuracies in the learned action models are reasonable. For example, if a fluent never changes over the course of an action sequence, the algorithm may infer that an arbitrary action causes that fluent to hold.\nDo the conditions for the algorithms\u2019 correctness hold in practice? In all of those scenarios that we report here, the conditions that guarantee correctness for our algorithms hold. In our experiments we assumed that the main conditions of the algorithms hold, namely, that actions are\ndeterministic and of few preconditions. We did not enforce having observations for every fluent every (fixed) k steps. The latter condition is not necessery for correctness of the algorithm, but is necessary to guarantee polynomial-time computation. Our experiments verify that this is not necessary in practice, and it indeed be the case that the algorithms can have a polynomial-time guarantee for our modified observation\nAn earlier work of Hlubocky and Amir (2004) has included a modified version of AS-STRIPSSLAF in their architecture and tested it on a suite of adventure-game-like virtual environments that are generated at random. These include arbitrary numbers of places, objects of various kinds, and configurations and settings of those. There, an agent\u2019s task is to exit a house, starting with no knowledge about the state space, available actions and their effects, or characteristics of objects. Their experiments show that the agent learns the effects of its actions very efficiently. This agent makes decisions using the learned knowledge, and inference with the resulting representation is fast (a fraction of a second per SAT problem in domains including more than 30 object, modes, and locations).\nCan the learned model be used for successful planning and execution? How do the learning procedures fit with planning and execution? The learned model can be used for planning by translating it to PDDL. However, that model is not always the correct one for the domain, so the plan may not be feasible or may not lead to the required goal. In those cases, we can interleave planning, execution, and learning, as was described in the work of Chang and Amir (2006). There, one finds a short plan with a consistent action model, executes that plan, collects observations, and applies SLAF to those. When plan failure can be detected (e.g., that the goal was not achieved), the results of Chang and Amir (2006) guarantee that the joint planning-execution-learning procedure would reach the goal in a bounded amount of time. That bounded time is in fact linear in the length of the longest plan needed for reaching the goal, and is exponential in the complexity of the action model that we need to learn."}, {"heading": "8. Comparison with Related Work", "text": "HMMs (Boyen & Koller, 1999; Boyen et al., 1999; Murphy, 2002; Ghahramani, 2001) can be used to estimate a stochastic transition model from observations. Initially, we expected to compare our work with the HMM implementation of Murphy (2002), which uses EM (a hill-climbing approach). Unfortunately, HMMs require an explicit representation of the state space, and our smallest domain (31 features) requires a transition matrix of (231)2 entries. This prevents initializing HMMs procedures on any current computer.\nStructure learning approaches in Dynamic Bayes Nets (DBNs) (e.g., Ghahramani & Jordan, 1997; Friedman, Murphy, & Russell, 1998; Boyen et al., 1999) use EM and additional approximations (e.g., using factoring, variation, or sampling), and are more tractable. However, they are still limited to small domains (e.g., 10 features , Ghahramani & Jordan, 1997; Boyen et al., 1999), and also have unbounded errors in discrete deterministic domains, so are not usable in our settings.\nA simple approach for learning transition models was devised in the work of Holmes and Charles Lee Isbell (2006) for deterministic POMDPs. There, the transition and observation models are deterministic. This approach is close to ours in that it represents the hidden state and possible models using a finite structure, a Looping Prediction Suffix Tree. This structure can be seen as related to our representation in that both grow models that relate action histories to possible transition models. In our work here such interactions are realized in the recursive structure of the\ntransition-belief formula built by AS-STRIPS-SLAF, e.g.,\n(af \u2228 (af\u25e6 \u2227 \u00aca[\u00acf ] \u2227 explf ))\nwhere explf refers to a similar formula to this that was created in the previous time step. The main difference that we should draw between our work and that of Holmes and Charles Lee Isbell (2006) is that the latter refers to states explicitly, whereas our work refers to features. Consequently, our representation is (provably) more compact and our procedures scale to larger domains both theoretically and in practice. Furthermore, our procedure provably maintains a reference to all the possible models when data is insufficient to determine a single model, whereas the work of Holmes and Charles Lee Isbell (2006) focuses only on the limit case of enough information for determining a single consistent model. On the down-side, our procedure does not consider stochasticity in the belief state, and this remains an area for further development.\nA similar relationship holds between our work and that of Littman, Sutton, and Singh (2002). In that work, a model representation is given with a size linear in the number of states. This model, predictive state representation (PSR), is based on action/observation histories and predicts behavior based on those histories. That work prefers a low-dimensional vector basis instead a featurebased representation of states (one of the traditional hallmarks of the Knowledge Representation approach). There is no necessary correspondence between the basis vectors and some intuitive features in the real world necessarily. This enables a representation of the world that is more closely based on behavior.\nLearning PSRs (James & Singh, 2004) is nontrivial because one needs to find a good lowdimensional vector basis (the stage called discovery of tests). That stage of learning PSRs requires matrices of size \u2126(n2), for states spaces of size n.\nOur work advances over that line of work in providing correct results in time that is polylogarithmic in the number of states. Specifically, our work learns (deterministic) transition models in polynomial time in the state features, thus taking time O(poly(log n)).\nReinforcement Learning (RL) approaches (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996) compute a mapping between world states and preferred actions. They are highly intractable in partially observable domains (Kaelbling, Littman, & Cassandra, 1998), and approximation (e.g., Kearns, Mansour, & Ng, 2000; Meuleau, Peshkin, Kim, & Kaelbling, 1999; Even-Dar, Kakade, & Mansour, 2005; McCallum, 1995) is practical only for small domains (e.g., 10-20 features) with small horizon time T .\nIn contrast to HMMs, DBNs, and RL, our algorithms are exact and tractable in large domains (> 300 features). We take advantages of properties common to discrete domains, such as determinism, limited effects of actions, and observed failure.\nPrevious work on learning deterministic action models in the AI-Planning literature assumes fully observable deterministic domains. These learn parametrized STRIPS actions using, e.g., version spaces (Gil, 1994; Wang, 1995), general classifiers (Oates & Cohen, 1996), or hill-climbing ILP (Benson, 1995). Recently, the work of Pasula et al. (2004) gave an algorithm that learns stochastic actions with no conditional effects. The work of Schmill, Oates, and Cohen (2000) approximates partial observability by assuming that the world is fully observable. We can apply these to partially observable learning problems (sometimes) by using the space of belief states instead of world states, but this increases the problem size to exponentially, so this is not practical for our problem.\nFinally, recent research on learning action models in partially observable domains includes the works of Yang, Wu, and Jiang (2005) and of Shahaf and Amir (2006). In the works of Yang et al.\n(2005), example plan traces are encoded as a weighted maximum SAT problem, from which a candidate STRIPS action model is extracted. In general, there may be many possible action models for any given set of example traces, and therefore the approach is by nature approximate (in contrast to ours, which always identifies the exact set of possible action models). The work also introduces additional approximations in the form of heuristic rules meant to rule out unlikely action models.\nThe work of Shahaf and Amir (2006) presents an approach for solving SLAF using logicalcircuit encodings of transition belief states. This approach performs tractable SLAF for more general deterministic models than those that we present in Section 5, but it also requires SAT solvers for logical circuits. Such SAT solvers are not optimized nowadays in comparison with CNF SAT solvers, so their overall performance for answering questions from SLAF is lower than ours. Most importantly, the representation given by Shahaf and Amir (2006) grows (linearly) with the number of time steps, and this can still hinder long sequences of actions and observations. In comparison, our transition belief formula has bounded size that is independent of the number of time steps that we track.\nOur encoding language, LA is typical in methods for software and hardware verification and testing. Relevant books and methods (e.g., Clarke, Grumberg, & Peled, 1999) are closely related to this representation, and results that we achieve here are applicable there and vice versa. The main distinction we should draw between our work and that done in Formal Methods (e.g., Model Checking and Bounded Model Checking) is that we are able to conclude size bounds for logical formulas involved in the computation. While OBDDs are used with some success in Model Checking, and CNF representations are used with some success in Bounded Model Checking, they have little bounds for the sizes of formulas in theory or in practice. The conditions available in AI applications are used in the current manuscript to deliver such bounds and yield tractability and scalability results that are both theoretical and of practical significance.\nInterestingly, methods that use Linear Temporal Logics (LTL) cannot distinguish between what can happen and what actually happens (Calvanese, Giacomo, & Vardi, 2002). Thus, they cannot consider what causes an occurence. Our method is similar in that it does not consider alternate futures for a state explicitly. However, we use an extended language, namely LA, that makes those alternatives explicit. This allows us to forego the limitations of LTL and produce the needed result."}, {"heading": "9. Conclusions", "text": "We presented a framework for learning the effects and preconditions of deterministic actions in partially observable domains. This approach differs from earlier methods in that it focuses on determining the exact set of consistent action models (earlier methods do not). We showed that in several common situations this can be done exactly in time that is polynomial (sometime linear) in the number of time steps and features. We can add bias and compute an exact solution for large domains (hundreds of features), in many cases. Furthermore, we show that the number of action-observation traces that must be seen before convergence is polynomial in the number of features of the domain. These positive results contrast with the difficulty encountered by many approaches to learning of dynamic models and reinforcement learning in partially observable domains.\nThe results that we presented are promising for many applications, including reinforcement learning, agents in virtual domains, and HMMs. Already, this work is being applied to autonomous agents in adventure games, and exploration is guided by the transition belief state that we compute\nand information gain criteria. In the future we plan to extend these results to stochastic domains, and domains with continuous features."}, {"heading": "Acknowledgments", "text": "We wish to thank Megan Nance for providing the code and samples for the sequence generator, and also wish to thank Dafna Shahaf for an encouraging collaboration that enhanced our development and understanding of these results. The first author also wishes to acknowledge stimulating discussion with Brian Hlubocky on related topics. We wish to acknowledge support from DAF Air Force Research Laboratory Award FA8750-04-2-0222 (DARPA REAL program). The second author wishes to acknowledge support from a University of Illinois at Urbana-Champaign, College of Engineering fellowship. Finally, the first author acknowledges support from a joint Fellowship (2007-2008) with the Center for Advanced Studies and the Beckman Institute at the University of Illinois Urbana-Champaign.\nEarlier versions of the results in this manuscript appeared in conference proceedings (Amir, 2005; Shahaf et al., 2006)."}, {"heading": "Appendix A. Our Representation and Domain Descriptions", "text": "The transition relation RM for interpretation M in LA is defined in Section 3.1 in a way that is similar to the interpretation of Domain Descriptions. Domain Descriptions are a common method for specifying structured deterministic domains (Fagin, Ullman, & Vardi, 1983; Lifschitz, 1990; Pednault, 1989; Gelfond & Lifschitz, 1998). Other methods that are equivalent in our contexts include Successor-State Axioms (Reiter, 2001) and the Fluent Calculus (Thielscher, 1998). Their relevance and influence on our work merit a separate exposition and relationship to our work.\nA domain description D is a finite set of effect rules of the form \u201ca causes F if G\u201d that describe the effects of actions, for F and G being state formulas (propositional combinations of fluent names). We say that F is the head or effect and G is the precondition for each such rule. We write \u201ca causes F \u201d, for \u201ca causes F if TRUE\u201d. We denote byED(a) the set of effect rules inD for action a \u2208 A. For an effect rule e, let Ge be its precondition and Fe its effect. Rule e is active in state s, if s |= Ge (s taken here as a interpretation in P).\nEvery domain description D defines a unique transition relationRD(s, a, s\u2032) as follows. \u2022 Let F (a, s) be the conjunction of effects of rules that are active in s for a, i.e., \u2227 {Fe | e \u2208\nED(a), s |= Ge}. We set F (a, s) = TRUE when no rule of a is active in s. \u2022 Let I(a, s) be the set of fluents that are not affected by a in s, i.e., I(a, s) = {f \u2208 PD | \u2200e \u2208 ED(a) (s |= Ge)\u21d2 (f /\u2208 L(Fe))}. \u2022 Define (recalling that world states are sets of fluents)\nRD =\n{ \u3008s, a, s\u2032\u3009 \u2223\u2223\u2223\u2223 (s\u2032 \u2229 I(a, s)) = (s \u2229 I(a, s)) and s\u2032 |= F (a, s) } (4)\nThus, if action a has an effect of FALSE in s, then it cannot execute in s. This definition applies inertia (a fluent keeps its value) to all fluents that appear in no active rule. In some contexts it is useful to specify this inertia explicitly by extra effect rules of the form \u201ca keeps f if G\u201d, for a fluent f \u2208 P . It is a shorthand for writing the two rules \u201ca causes f if f \u2227G\u201d and \u201ca causes \u00acf if \u00acf \u2227G\u201d. When D includes all its inertia (keeps) statements, we say that D is a complete domain description.\nExample A.1 Consider the scenario of Figure 2 and assume that actions and observations occur as in Figure 10. Actions are assumed deterministic, with no conditional effects, and their preconditions must holds before they execute successfully. Then, every action affects every fluent either negatively, positively, or not at all. Thus, every transition relation has a complete domain description that includes only rules of the form \u201ca causes l\u201d or \u201ca keeps l\u201d, where l is a fluent literal (a fluent or its negation).\nConsequently, every transition relation R is completely defined by some domain description D such that (viewing a tuple as a set of its elements)\nR \u2208 \u220f\na\u2208\n8\n> <\n> : go-W go-E sw-on\n9\n> =\n> ;\n   a causes E, a causes \u00acE a keeps E   \u00d7    a causes sw, a causes \u00acsw a keeps sw   \u00d7    a causes lit, a causes \u00aclit a keeps lit   \nSay that initially we know the effects of go-E, go-W, but do not know what sw-on does. Then, transition filtering starts with the product set of R (of 27 possible relations) and all possible 23\nstates. Also, at time step 4 we know that the world state is exactly {E,\u00aclit,\u00acsw}. We try sw-on and get that Filter[sw-on](\u03c14) includes the same set of transition relations but with each of those transition relations projecting the state {E,\u00aclit,\u00acsw} to an appropriate choice from S . When we receive the observations o5 = \u00acE \u2227 \u00acsw of time step 5, \u03c15 = Filter[o5](Filter[sw-on](\u03c14)) removes from the transition belief state all the relations that gave rise to \u00acE or to \u00acsw. We are left with transition relations satisfying one of the tuples in\n{ sw-on causes E, sw-on keeps E } \u00d7 { sw-on causes sw } \u00d7    sw-on causes lit sw-on causes \u00aclit sw-on keeps lit   \nFinally, when we perform action go-W, again we update the set of states associated with every transition relation in the set of pairs \u03c15. When we receive the observations of time step 6, we conclude \u03c16 = Filter[o6](Filter[go-W](\u03c15)) =\n  \u2329  \u00acE lit sw    ,    sw-on causes E, sw-on causes sw, sw-on causes lit, go-E...    \u232a , \u2329  \u00acE lit sw    ,    sw-on keeps E, sw-on causes sw, sw-on causes lit, go-E...    \u232a   \n(5)\n2"}, {"heading": "Appendix B. Proofs", "text": ""}, {"heading": "B.1 Proof of Lemma 3.1: Consequence Finding and Existential Quantification", "text": "PROOF Consider some CNF form of \u03d5. Suppose the clauses containing the literal x are x \u2228 \u03b11, . . . , x \u2228 \u03b1a where \u03b11, . . . , \u03b1a are clauses. Suppose the clauses containing the literal \u00acx are \u00acx \u2228 \u03b21, . . . ,\u00acx \u2228 \u03b2b. Suppose the clauses not containing x or \u00acx are \u03b31, . . . , \u03b3c. Then note that CnL(\u03d5)\\{x}(\u03d5) \u2261 ( \u2227 1\u2264i\u2264c \u03b3i) \u2227 ( \u2227 1\u2264i\u2264a,1\u2264j\u2264b \u03b1i \u2228 \u03b2j) (the formula produced by adding all resolvents over variable x and then removing clauses not belonging to L(L(\u03d5)\\{x})), since resolution is complete for consequence finding.\nNecessity. (\u2203x.\u03d5 |= CnL(\u03d5)\\{x}(\u03d5)) Consider any model m of \u2203x.\u03d5. By definition, m can be extended to L(\u03d5) (i.e., by assigning some value to m(x)) such that m(\u03d5) = 1. Extend m such that this is the case. Now suppose for contradiction m is not a model of CnL(\u03d5)\\{x}(\u03d5). It cannot be the case that m(\u03b3k) = 0 for any k, because then m(\u03d5) = 0, contradiction. Then, it must be the case that m(\u03b1i \u2228 \u03b2j) = 0 for some 1 \u2264 i \u2264 a and 1 \u2264 j \u2264 b. Therefore, m(\u03b1i) = 0 and m(\u03b2j) = 0. But m is a model of \u03d5, so both m(x \u2228 \u03b1i) = 1 or m(\u00acx \u2228 \u03b2j) = 1. Thus either m(\u03b1i) = 1 or m(\u03b2j) = 1, contradiction.\nSufficiency. (CnL(\u03d5)\\{x}(\u03d5) |= \u2203x.\u03d5) Consider any model m of CnL(\u03d5)\\{x}(\u03d5). Suppose for contradiction m(\u2203x.\u03d5) = 0. That is, if m is extended to L(\u03d5), then m(\u03d5) = 0. Now, extend m to L(\u03d5) such that m(x) = 0. It cannot be the case that m(\u03b3k) = 0 for some k since m models CnL(\u03d5)\\{x}(\u03d5). Because m(\u00acx) = 1, it cannot be the case that m(\u00acx \u2228 \u03b2j) = 0 for any j. Therefore m(x\u2228\u03b1i) = 0 for some 1 \u2264 i \u2264 a. Therefore, m(\u03b1i) = 0. Because m(\u03b1i \u2228 \u03b2j) = 1 for all 1 \u2264 j \u2264 n, we must have that m(\u03b2j) = 1 for all j. But now if we alter m such that m(x) = 1, then m satisfies \u03d5, contradiction. 2"}, {"heading": "B.2 Proof of Theorem 3.3", "text": "PROOF Both sides of the equality relation are sets of state-transition-relation pairs. We show that the two sets have the same elements. We show first that the left-hand side of the equality is contained in the right-hand side.\nTake \u3008s\u2032, R\u3009 \u2208 SLAF [a]({\u3008s,R\u3009 \u2208 S | \u3008s,R\u3009 satisfies \u03d5}). From Definition 2.3 there is s \u2208 S such that s \u2208 {s \u2208 S | \u3008s,R\u3009 satisfies \u03d5} such that \u3008s, a, s\u2032\u3009 \u2208 R. In other words, there is s \u2208 S such that \u3008s,R\u3009 satisfies \u03d5 and \u3008s, a, s\u2032\u3009 \u2208 R.\nTo prove that \u3008s\u2032, R\u3009 satisfies SLAF0[a](\u03d5) we need to show that \u03d5 \u2227 Teff(at) has a model M such that RM = R and s\u2032 interprets P \u2032. Let M be such that s interprets P , s\u2032 interprets P \u2032, and MR interpreting LA as in the previous section. This interpretation does not satisfy this formula only if one of the conjuncts of \u03d5\u2227 \u2227 l\u2208F ,G\u2208G((a l G \u2227G)\u21d2 l \u2032)\u2227 \u2227 l\u2208F (l \u2032 \u21d2 ( \u2228 G\u2208G(a l G \u2227G))) is falsified. This cannot be the case for \u03d5 by our choice of s. Assume by contradiction that (alG \u2227G)\u21d2 l\u2032) fails for some l. Then, (alG \u2227G) hold in M and l\u2032 is FALSE. The portion of M that interprets LA is built according to MR, for our R. Since \u3008s,R, s\u2032\u3009 we know that s\u2032 satisfies l, by the construction of MR. This is a contradicts l\u2032 being FALSE in M (M interprets P \u2032 according to s\u2032), and therefore we conclude that (alG \u2227G)\u21d2 l\u2032) for every l\u2032.\nSimilarly, assume by contradiction that (l\u2032 \u21d2 ( \u2228 G\u2208G(a l G \u2227G))) fails for some l. Then, l\u2032 holds in s\u2032, and als fails. Again, from the way we constructed MR it must be that als in M takes the value that corresponds to the l\u2032\u2019s truth value in s\u2032. Thus, it must be that als takes the value TRUE in M , and we are done with the first direction.\nFor the opposite direction (showing the right-hand side is contained in the left-hand side), take \u3008s\u2032, R\u3009 \u2208 S that satisfies SLAF [a](\u03d5). We show that\n\u3008s\u2032, R\u3009 \u2208 SLAF [a]({\u3008s,R\u3009 \u2208 S | \u3008s,R\u3009 satisfies \u03d5}).\n\u3008s\u2032, R\u3009 |= SLAF [a](\u03d5) implies (Corollary 3.2) that there is s \u2208 S such that \u3008s\u2032, R, s\u3009 |= \u03d5\u2227Teff(a) (s\u2032, R, s interpreting P \u2032, LA,P , respectively). A similar argument to the one we give for the first part shows that \u3008s, a, s\u2032\u3009 \u2208 R, and \u3008s\u2032, R\u3009 \u2208 SLAF [a]({\u3008s,R\u3009 \u2208 S | \u3008s,R\u3009 satisfies \u03d5}). 2"}, {"heading": "B.3 Proof of Theorem 4.1: Distribution of SLAF Over Connectives", "text": "For the first part, we will show that the sets of models of SLAF [a](\u03d5 \u2228 \u03c8) and SLAF [a](\u03d5) \u2228 SLAF [a](\u03c8) are identical.\nTake a model M of SLAF [a](\u03d5\u2228\u03c8). Let M \u2032 be a model of \u03d5\u2228\u03c8 for which SLAF [a](M \u2032) = M . Then, M \u2032 is a model of \u03d5 or M is a model of \u03c8. Without loss of generalization, assume M is a model of \u03d5. Thus, M |= SLAF [a](\u03d5), and it follows that M is a model of SLAF [a](\u03d5) \u2228 SLAF [a](\u03c8).\nFor the other direction, take M a model of SLAF [a](\u03d5) \u2228 SLAF [a](\u03c8). Then, M a model of SLAF [a](\u03d5) or M is a model of SLAF [a](\u03c8). Without loss of generalization assume that M is a model of SLAF [a](\u03d5). Take M \u2032 a model of \u03d5 such that M = SLAF [a](M \u2032). So, M \u2032 |= \u03d5\u2228\u03c8. It follows that M |= SLAF [a](\u03d5 \u2228 \u03c8).\nA similar argument follows for the \u2227 case. Take a model M of SLAF [a](\u03d5 \u2227 \u03c8). Let M \u2032 be a model of \u03d5 \u2227 \u03c8 for which SLAF [a](M \u2032) = M . Then, M \u2032 is a model of \u03d5 and \u03c8. Thus, M |= SLAF [a](\u03d5) andM |= SLAF [a](\u03c8). It follows thatM is a model of SLAF [a](\u03d5)\u2227SLAF [a](\u03c8). 2\nB.4 Proof of Theorem 4.5: Closed form for SLAF of a belief state formula\nPROOF SKETCH We follow the characterization offered by Theorem 3.3 and Formula (1). We take \u03d5t\u2227at\u2227Teff(a, t) and resolve out the literals of time t. This resolution is guaranteed to generate a set of consequences that is equivalent to CnLt+1(\u03d5t \u2227 at \u2227 Teff(a, t)).\nAssuming\u03d5t\u2227at, Teff(a, t) is logically equivalent to Teff(a, t)|\u03d5 = \u2227 l\u2208F ,G\u2208G,G|=\u03d5((a l G\u2227Gt)\u21d2\nlt+1) \u2227 \u2227 l\u2208F ,G\u2208G,G|=\u03d5(lt+1 \u21d2 (Gt \u21d2 a l G)). This follows from two observations. First, notice that \u03d5t implies that for any G \u2208 G such that G 6|= \u03d5 we get Gt \u21d2 alG and (alG \u2227 Gt) \u21d2 lt+1 (the antecedent does not hold, so the formula is true). Second, notice that, in the second part of the original Teff(a, t), ( \u2228 G\u2208G,G|=\u03d5(a l G \u2227Gt)) is equivalent (assuming \u03d5) to ( \u2227 G\u2208G,G|=\u03d5(Gt \u21d2 a l G)).\nNow, resolving out the literals of time t from \u03d5t \u2227 at \u2227 Teff(a, t)|\u03d5 should consider all the resolutions of clauses (Gt is a term) of the form alG \u2227 Gt \u21d2 lt+1 and all the clauses of the form lt+1 \u21d2 (Gt \u21d2 a l G) with each other. This yields the equivalent to\n\u2227\nG1, ..., Gm \u2208 G \u03d5 |= W\ni\u2264m Gi l1, ..., lm \u2208 F\n[ m\u2228\ni=1\nlt+1i \u2228 m\u2228\ni=1\n(a\u00acliGi \u2227 \u00aca li Gi )]\nbecause to eliminate all the literals of time t we have to resolve together sets of clauses with matching Gi\u2019s such that \u03d5 |= \u2228 i\u2264nGi. The formula above encodes all the resulting clauses for a chosen\nset of G1, ..., Gm and a chosen set of literals l1, ..., lm. The reason for including (a\u00acliGi \u2227 \u00aca li Gi ) is that we can always choose a clause with Gi, li of a specific type (either one that includes \u00acalG or one that produces a\u00aclG .\nFinally, we get the formula in our theorem because aFG \u2261 \u00aca\u00acFG (G characterizes exactly one state in S), and the fact that there is one set of G1, ..., Gm that is stronger than all the rest (it entails all the rest) because G1, ..., Gm are complete terms. This set is the complete fluent assignments Gi that satisfy \u03d5. 2\nB.5 Proof of Theorem 5.1: Closed form when k affected fluents\nPROOF SKETCH For literal l and clause C in the conjunction in Theorem 4.5, we aggregate all the action propositions to a single action proposition alGl , with G the disjunction of all the complete preconditions for l in C (notice that there cannot be l\u2019s negation because then the C is a tautology). Lemma B.2 shows that Gl is equivalent to a fluent term. First we prove the more restricted Lemma B.1. Lemma B.1 Let C = \u2228m i=1(l t+1 i \u2228 a \u00acli Gi\n) be a clause of the formula in Theorem 4.5, and let Gl =\u2228 {Gi | li = l, Gi |= \u00acli}, for any literal l6. Assume that a\u2019s effect is deterministic with only one case with a term precondition (if that case does not hold, then nothing changes). Then, Gl is equivalent to a term.\nPROOF Gl is a disjunction of complete state terms Gi, so it represents the set of states that corresponds to those Gi\u2019s. The intuition that we apply is that\nC \u2261 \u2228m i=1 l t+1 i \u2228 \u00aca li Gi \u2261\n( \u2227m i=1 a li Gi )\u21d2 ( \u2228m i=1 l t+1 i ) \u2261 al Gl \u21d2 l \u2228 C \u2032\nfor C \u2032 the part of C that does not affect l. The reason is that for complete terms Gi we know that \u00aca\u00acliGi \u2261 a li Gi\n. Thus, our choice of Gl is such that it includes all the conditions under which l changes, if we assume the precondition al\nGl .\nWe now compute the action model of a for l by updating a copy of Gl. Let li be a fluent literal, and set Glt = Gl.\n1. If there is no Gl2 \u2208 Glt such that Gl2 |= li, then all of the terms in Glt include \u00acli. Thus, Glt \u2261 G l t \u2227 \u00acli, and alGl \u2261 a l Gl\u2227\u00acli . Thus, add \u00acli as a conjunct to Glt.\n2. Otherwise, if there is no Gl2 \u2208 Glt such that Gl2 |= \u00acli, then all of the terms in Glt include li. Thus, Glt \u2261 Glt \u2227 li, and alGl \u2261 a l Gl\u2227li . Thus, add li as a conjunct to Glt.\n3. Otherwise, we know that under both li and \u00acli the value of the fluent of l changes into l = TRUE. Since we assume that the value of l changes under a single case of its preconditions (i.e., a has no conditional effects - it either succeeds with the change, or fails with the change), then li cannot be part of those preconditions, i.e., for every term Gi in Glt we can replace li with \u00acli and vice versa, and the precondition would still entail l after the action. Thus, al Glt \u2261 al Glt\\{li} , for Glt \\ {li} the result of replacing both li and \u00acli by TRUE.\n6. Not related to the literal l in the proof above.\nThe result of these replacements is a term Glt that is consistent with \u03d5t (it is consistent with the original Gl) and satisfies \u201ca has 1 case\u201d |= al\nGlt \u21d0\u21d2 al Gl . 2\nLemma B.2 Let C = \u2228m i=1 l t+1 i \u2228 a \u00acli Gi\nbe a clause of the formula in Theorem 4.5, and let G\u0302l =\u2228 {Gi | li = l}, for any literal l. Assume that a\u2019s effect is deterministic with only one case with a\nterm precondition (if that case does not hold, then nothing changes). Then, either G\u0302l is equivalent\nto a term, or C is subsumed by another clause that is entailed from that formula such that there G\u0302l\nis equivalent to a term.\nPROOF Consider Gl from Lemma B.1, and let Gl1 a complete fluent term in G\u0302l and not in Gl. Thus, Gl1 |= l. Let Glt the term that is equivalent to Gl according to Lemma B.1.\nClause C is equivalent to al Glt \u21d2 a\u00acl Gl 1 \u2228 .... However, al Glt already asserts change from \u00acl to l in the result of action a, and a\u00acl\nGl 1 asserts a change under a different condition from l to \u00acl. Thus, \u201ca has 1 case\u201d |= al\nGlt \u21d2 \u00aca\u00acl Gl 1 . We get a subsuming clause C \u2032 = C \\ a\u00acl Gl 1 . In this way we can remove from C all the literals alGi with Gi not in G\nl. After such a process we are left with a clause C that has Gl \u2261 G\u0302l. However, clause C is now not of the form of Theorem 5.1 because some of the Gi\u2019s are missing. How do we represent that in the theorem? We must allow some of the Gi\u2019s to be missing.\n2\nProof of theorem continues Thus, the representation of C \u2032 (the new clause) takes space O(n2) (each propositional symbol is encoded with O(n) space, assuming constant encoding for every fluent p \u2208 P).\nHowever, the number of propositional symbols is still large (there areO(2n) fluent terms, so this encoding still requires O(2nn) (all preconditions to all effects) new propositional symbols. Now we notice that we can limit our attention to clauses C that have at most k literals l whose action proposition alGl satisfies |= Gl \u21d2 l. This is because if we have more than k such propositions in C, say {aliGli}i\u2264k\u2032 , then C \u2261 ( \u2227k i=1 \u00aca li Gli ) \u21d2 a lk+1 Glk+1 \u2228 ... \u2228 i\u2264m l t+1 i , which is always subsumed by ( \u2227k i=1 \u00aca\nli Gli )\u21d2 a lk+1 Glk+1 . The latter is a sentence that is always true, if we assume that a can change\nat most k fluents (alk+1Glk+1 asserts that lk+1 remains the same, and \u00aca li Gli asserts that li changes in one of the conditions satisfied by Gli).\nUsing clauses of the form ( \u2227k i=1 \u00aca\nli Gli )\u21d2 a lk+1 Glk+1 (which state that we have at most k effects)\nwe can resolve away aljGlj for j > k in every clause C of the original conjunction. Thus, we are left with clauses of the form C \u2261 ( \u2227k i=1 \u00aca\nli Gli\n) \u21d2 \u2228 i\u2264m l t+1 i . Now, since our choice of literals\nother than l1, ..., lk is independent of the rest of the clause, and we can do so for every polarity of those fluents, we get that all of these clauses resolve (on these combinations of literals) into (and are subsumed by)\nC \u2261 ( k\u2227\ni=1\n\u00acaliGli )\u21d2\n\u2228\ni\u2264k\nlt+1i (6)\nThus, we get a conjunction of clauses of the form (6), with Gli (i \u2264 k) being a fluent term. So, the conjunction of clauses in Theorem 4.5 is equivalent to a conjunction of clauses such that each clause has at most 2k literals. Thus, the space required to represent each clause is 2kn.\nFinally, we use the fact that every action is dependent on at most k other fluents. Every proposition that asserts no-change in li is equivalent to a conjunction of literals stating that none of the possible k preconditions that are consistent with it affect li. For example alil1\u2227...\u2227lk\u2227lu implies that alil1\u2227...\u2227lk \u2227 a li l1\u2227...\u2227lk\u22121\u2227lu\n\u2227 .... Similarly, each one the elements in the conjunction implies alil1\u2227...\u2227lk\u2227lu . 2"}, {"heading": "B.6 Proof of Theorem 5.3: Equivalent Restricted Definition of Action Axioms", "text": "PROOF Let\u03d5\u2032 = \u2203P.(\u03d5\u2227\u03c4eff(a)). Now we claim that for any successful action a, SLAF [a](\u03d5) \u2261 \u03d5\u2032[P \u2032/P]. To see this, consider any model of \u03d5 \u2032. The valuation to the fluents in \u22c3 f\u2208P L 0 f define a transition relationR and the valuation to the fluents inP \u2032 define a state s\u2032 \u2208 S such that \u3008s\u2032, R\u3009 \u2208 \u03d5\u2032. By the definition of \u03d5\u2032, \u3008s\u2032, R\u3009 \u2208 \u03d5\u2032 if and only if there exists \u3008s,R\u3009 \u2208 \u03d5 such that \u03c4eff(a) is satisfied. Finally note that \u03c4eff(a) is satisfied if and only if the preconditions of action a are met and s\u2032 is consistent with the effects of action a applied to s. That is, \u03c4eff(a) is satisfied if and only if \u3008s, a, s\u2032\u3009 \u2208 R. Together, these observations and Corollary 3.2 yield the theorem. 2"}, {"heading": "B.7 Proof of Theorem 5.4: AS-STRIPS-SLAF is Correct", "text": "PROOF Let the shorthand notation C(\u03c8) denote C(\u03c8) \u2261 CnL\u2032(\u03c8)[P \u2032/P]. By definition, SLAF [\u3008a, o\u3009](\u03d5) \u2261 SLAF [o](SLAF [a](\u03d5)). From Theorem 5.3, we have SLAF [a](\u03d5) \u2261 C(\u03d5 \u2227 \u03c4eff(a)). A formula equivalent to C(\u03d5 \u2227 \u03c4eff(a)) may be generated by resolving out all fluents fromP (by following the procedure from the proof of Lemma 3.1). Suppose \u03d5 = \u2227 f\u2208P \u03d5f is in fluent-factored form. Then we may rewrite C(\u03d5 \u2227 \u03c4eff(a)) as:\nSLAF [a](\u03d5) \u2261\n \u2227\nf\u2208P\nC(Prea,f ) \u2227 C(Effa,f ) \u2227 C(\u03d5f )   \u2227 (7)\n \u2227\nf\u2208P\nC(Prea,f \u2227 Effa,f )   \u2227\n \u2227\nf\u2208P\nC(\u03d5f \u2227 Prea,f )   \u2227\n \u2227\nf\u2208P\nC(\u03d5f \u2227 Effa,f )\n \nThis equivalence holds because all resolvents generated by resolving out literals from P in C(\u03d5 \u2227 \u03c4eff(a)) will still be generated in the above formula. That is, each pair of clauses that can be possibly resolved together (where a fluent from P is \u201cresolved out\u201d) in \u03d5 \u2227 \u03c4eff(a) to generate a new consequence in C (\u03d5 \u2227 \u03c4eff(a)) will appear together in one of the C (\u00b7) components of (7). Because\nevery clause in \u03d5\u2227\u03c4eff(a) contains at most one literal from P, we see that all possible consequences will be generated.\nNow we note that Effa,f may be rewritten as follows:\nEffa,f \u2261 \u2227\nl\u2208{f,\u00acf}\n((al \u2228 (af\u25e6 \u2227 l))\u21d2 l\u2032) \u2227 (l\u2032 \u21d2 (al \u2228 (af\u25e6 \u2227 l))) (8)\n\u2261 \u2227\nl\u2208{f,\u00acf}\n(l\u21d2 (l\u2032 \u2228 a\u00acl)) \u2227 (l\u2032 \u21d2 (al \u2228 af\u25e6))\nIt is straightforward to verify the equivalence of the rewritten formula to the original formula. Note that in performing this rewriting, we may discard clauses of the form af \u2228 a\u00acf \u2228 af\u25e6, as they must be true in every consistent model (given the axioms described in Section 5.2).\nNow consider the consequences that are generated by each component of (7). We may compute these consequences by performing resolution. We have that C(Prea,f ) \u2261 \u00aca[f ] \u2228 \u00aca[\u00acf ], but we may discard these clauses because only inconsistent action models will violate this clause. By the definition of fluent-factored formulas, C(\u03d5f ) \u2261 Af . Next, the remaining components can be computed straightforwardly:\nC(Effa,f ) \u2261 \u2227\nl\u2208{f,\u00acf}\n(\u00acl\u2032 \u2228 al \u2228 af\u25e6)\nC(\u03d5f \u2227 Prea,f ) \u2261 C(\u03d5f ) \u2227 C(Prea,f ) \u2227 \u2227\nl\u2208{f,\u00acf}\n(\u00aca[l] \u2228 expll)\nC(Prea,f \u2227 Effa,f ) \u2261 C(Prea,f ) \u2227 C(Effa,f ) \u2227 \u2227\nl\u2208{f,\u00acf}\n(\u00acl\u2032 \u2228 al \u2228 \u00aca[\u00acl])\nC(\u03d5f \u2227 Effa,f ) \u2261 C(\u03d5f ) \u2227 C(Effa,f ) \u2227 \u2227\nl\u2208{f,\u00acf}\n(\u00acl\u2032 \u2228 al \u2228 expll)\nFinally, it is not difficult to see that in steps (a)-(c) the procedure sets \u03d5 to the following formula (in fluent-factored form):\nSLAF [a](\u03d5) \u2261 \u2227\nf\u2208P\nAf \u2227 \u2227\nl\u2208{f,\u00acf}\n(\u00aca[l] \u2228 expll) \u2227 (\u00acl \u2032 \u2228 al \u2228 (af\u25e6 \u2227 \u00aca[\u00acl] \u2227 expll)\nNow, note that SLAF [o](SLAF [a](\u03d5)) \u2261 SLAF [a](\u03d5) \u2227 o. Note that because o is a term, then SLAF [a](\u03d5) \u2227 o can be made fluent-factored by performing unit resolution. This is exactly what steps 1.(d)-(e) do. 2"}, {"heading": "B.8 Proof of Theorem 5.5: AS-STRIPS-SLAF Complexity", "text": "PROOF Consider the size of the formula returned by AS-STRIPS-SLAF. Overview: We note that if a formula is in i-CNF, for any integer i > 0, then the filtered formula after one step is in (i+ 1)-CNF. Then, we note that every observation of fluent f resets the f -part of the belief state formula to 1-CNF (thus, to i = 1).\nFurther Details: For the first part, this is because each call to the procedure appends at most one literal to any existing clauses of the formula, and no new clauses of length more than k + 1 are\ngenerated. Additionally, if every fluent is observed every at most k steps, then the transition belief formula stays in k-CNF (i.e., indefinitely compact). This is because existing clauses may only grow in length (1 literal per timestep) when augmented in steps 1.(a)-(c), but when the appropriate fluent is observed in steps 1.(d)-(e), the clauses stop growing. Finally, it is easy to see that each of the steps 1.(a)-(e) can be performed in polynomial time. 2"}, {"heading": "B.9 Proof of Theorem 5.7: PRE-STRIPS-SLAF is Correct", "text": "PROOF Consider the semantics of SLAF when filtering on a STRIPS action with a known precondition. In the case of an action failure, a world is in the filtered transition belief state if and only if the world did not meet the action precondition (and satisfies the observation). Clearly, step 1 of the algorithm performs this filtering by conjoining the belief formula with the negation of the action precondition (converted into a logically equivalent disjunction of fluent-factored formulas).\nIn the case of an action success, filtering can be performed by first removing worlds which do not satisfy the action precondition (so in all remaining worlds, the action is executable) and then filtering the remaining worlds using algorithm AS-STRIPS-SLAF. Moreover, by Theorem 4.1 and Corollary 4.3 it follows that filtering the formula \u03d5 can be performed by filtering each of the subformulas \u03d5i,j separately. Furthermore, SLAF[\u3008a, o\u3009](\u03d5) |= PRE-STRIPS-SLAF[\u3008a, o\u3009](\u03d5), and PRE-STRIPS-SLAF[\u3008a, o\u3009](\u03d5) \u2261 SLAF[\u3008a, o\u3009](\u03d5) if any of the conditions of Corollary 4.3. The filtering of each subformula is performed by steps 2 and 3 of the algorithm.\nFinally, note that steps 3 and 4 serve only to simplify the belief formula and produce a logically equivalent formula. 2"}, {"heading": "B.10 Proof of Theorem 5.8: PRE-STRIPS-SLAF Complexity", "text": "PROOF Note that each call to AS-STRIPS-SLAF on a subformula takes time linear in the size of the subformula, and the steps not involving AS-STRIPS-SLAF can be performed in linear time. Thus the the total time complexity is linear. Additionally, note that if every fluent is observed every at most k steps, then every fluent-factored subformula \u03d5i,j of the belief formula is in k-CNF, by a theorem of Amir (2005). If action preconditions contain at most m literals, then each disjunction of the form \u2228 j \u03d5i,j contains at most m disjuncts. Therefore, the entire belief formula stays in m \u00b7 k-CNF, indefinitely. 2"}, {"heading": "Appendix C. Experiments And Their Outputs", "text": "Our experiments (Section 7) examine properties of our algorithms for learning action models. They show that learning is tractable and exact. In this Appendix section we bring the generating models and the learned models for more detailed comparison by the reader. Recall that our algorithms output a representation for the set of models that are possible given the input. Below we bring only one satisfying model from the learned formula.\nOur experiments include the following domains from the International Planning Competition (IPC): Drivelog, Zenotravel, Blocksworld, and Depots."}, {"heading": "C.1 Driverlog Domain", "text": "The Driverlog domain has the following generating PDDL: (define (domain driverlog)\n(:requirements :typing) (:types location locatable - object driver truck obj - locatable ) (:predicates\n(at ?obj - locatable ?loc - location) (in ?obj1 - obj ?obj - truck) (driving ?d - driver ?v - truck) (path ?x ?y - location) (empty ?v - truck) )\n(:action LOAD-TRUCK :parameters (?obj - obj ?truck - truck ?loc - location) :precondition (and (at ?truck ?loc) (at ?obj ?loc)) :effect (and (not (at ?obj ?loc)) (in ?obj ?truck)))\n(:action UNLOAD-TRUCK :parameters (?obj - obj ?truck - truck ?loc - location) :precondition (and (at ?truck ?loc) (in ?obj ?truck)) :effect (and (not (in ?obj ?truck)) (at ?obj ?loc)))\n(:action BOARD-TRUCK :parameters (?driver - driver ?truck - truck ?loc - location) :precondition (and (at ?truck ?loc) (at ?driver ?loc) (empty ?truck)) :effect (and (not (at ?driver ?loc)) (driving ?driver ?truck)\n(not (empty ?truck))))\n(:action DISEMBARK-TRUCK :parameters (?driver - driver ?truck - truck ?loc - location) :precondition (and (at ?truck ?loc) (driving ?driver ?truck)) :effect (and (not (driving ?driver ?truck)) (at ?driver ?loc)\n(empty ?truck)))\n(:action DRIVE-TRUCK :parameters (?truck - truck ?loc-from - location ?loc-to - location ?driver - driver)\n:precondition (and (at ?truck ?loc-from) (driving ?driver ?truck) (path ?loc-from ?loc-to)) :effect (and (not (at ?truck ?loc-from)) (at ?truck ?loc-to)))\n(:action WALK :parameters (?driver - driver ?loc-from - location ?loc-to - location) :precondition (and (at ?driver ?loc-from) (path ?loc-from ?loc-to)) :effect (and (not (at ?driver ?loc-from)) (at ?driver ?loc-to))) )\nOne learned model (one possible satisfying model of our formula) from our random-sequence input in this Driverlog domain is the following (brought together with the experimental parameters).\nDriverlog domain: * IPC3 problem 99 * 231 fluents * 1000 randomly selected actions * 10 fluents observed per step\n* \"schematized\" learning * 1:1 precondition heuristics * Action distribution: ((BOARD-TRUCK . 52) (DRIVE-TRUCK . 86) (DISEMBARK-TRUCK . 52) (WALK . 529) (UNLOAD-TRUCK . 139) (LOAD-TRUCK . 142))\nconverting to CNF clause count: 82338 variable count: 210 adding clauses calling zchaff parsing result SLAF time: 2.469 Inference time: 8.406 Learned model:\n(WALK NEEDS (AT ?DRIVER ?LOC-FROM)) (WALK NEEDS (NOT (AT ?DRIVER ?LOC-TO))) (WALK CAUSES (AT ?DRIVER ?LOC-TO)) (WALK CAUSES (NOT (AT ?DRIVER ?LOC-FROM))) (WALK KEEPS (PATH ?LOC-FROM ?LOC-FROM)) (WALK KEEPS (PATH ?LOC-TO ?LOC-TO)) (WALK KEEPS (PATH ?LOC-TO ?LOC-FROM)) (WALK KEEPS (PATH ?LOC-FROM ?LOC-TO)) (DRIVE-TRUCK NEEDS (NOT (AT ?TRUCK ?LOC-TO))) (DRIVE-TRUCK NEEDS (AT ?TRUCK ?LOC-FROM)) (DRIVE-TRUCK CAUSES (AT ?TRUCK ?LOC-TO)) (DRIVE-TRUCK CAUSES (NOT (AT ?TRUCK ?LOC-FROM))) (DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-TO)) (DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-FROM)) (DRIVE-TRUCK KEEPS (DRIVING ?DRIVER ?TRUCK)) (DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-TO)) (DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-FROM)) (DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-TO)) (DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-FROM)) (DRIVE-TRUCK KEEPS (EMPTY ?TRUCK)) (DISEMBARK-TRUCK NEEDS (NOT (AT ?DRIVER ?LOC))) (DISEMBARK-TRUCK NEEDS (DRIVING ?DRIVER ?TRUCK)) (DISEMBARK-TRUCK NEEDS (NOT (EMPTY ?TRUCK))) (DISEMBARK-TRUCK CAUSES (AT ?DRIVER ?LOC)) (DISEMBARK-TRUCK CAUSES (NOT (DRIVING ?DRIVER ?TRUCK))) (DISEMBARK-TRUCK CAUSES (EMPTY ?TRUCK)) (DISEMBARK-TRUCK KEEPS (AT ?TRUCK ?LOC)) (DISEMBARK-TRUCK KEEPS (PATH ?LOC ?LOC)) (BOARD-TRUCK NEEDS (AT ?DRIVER ?LOC)) (BOARD-TRUCK NEEDS (NOT (DRIVING ?DRIVER ?TRUCK))) (BOARD-TRUCK NEEDS (EMPTY ?TRUCK)) (BOARD-TRUCK CAUSES (NOT (AT ?DRIVER ?LOC))) (BOARD-TRUCK CAUSES (DRIVING ?DRIVER ?TRUCK)) (BOARD-TRUCK CAUSES (NOT (EMPTY ?TRUCK))) (BOARD-TRUCK KEEPS (AT ?TRUCK ?LOC)) (BOARD-TRUCK KEEPS (PATH ?LOC ?LOC)) (UNLOAD-TRUCK NEEDS (NOT (AT ?OBJ ?LOC))) (UNLOAD-TRUCK NEEDS (IN ?OBJ ?TRUCK)) (UNLOAD-TRUCK CAUSES (AT ?OBJ ?LOC))\n(UNLOAD-TRUCK CAUSES (NOT (IN ?OBJ ?TRUCK))) (UNLOAD-TRUCK KEEPS (AT ?TRUCK ?LOC)) (UNLOAD-TRUCK KEEPS (PATH ?LOC ?LOC)) (UNLOAD-TRUCK KEEPS (EMPTY ?TRUCK)) (LOAD-TRUCK NEEDS (AT ?OBJ ?LOC)) (LOAD-TRUCK NEEDS (NOT (IN ?OBJ ?TRUCK))) (LOAD-TRUCK CAUSES (NOT (AT ?OBJ ?LOC))) (LOAD-TRUCK CAUSES (IN ?OBJ ?TRUCK)) (LOAD-TRUCK KEEPS (AT ?TRUCK ?LOC)) (LOAD-TRUCK KEEPS (PATH ?LOC ?LOC)) (LOAD-TRUCK KEEPS (EMPTY ?TRUCK))"}, {"heading": "C.2 Zeno-Travel Domain", "text": "The Zeno-Travel domain has the following generating PDDL: (define (domain zeno-travel) (:requirements :typing) (:types aircraft person city flevel - object) (:predicates (at ?x - (either person aircraft) ?c - city)\n(in ?p - person ?a - aircraft) (fuel-level ?a - aircraft ?l - flevel) (next ?l1 ?l2 - flevel))\n(:action board :parameters (?p - person ?a - aircraft ?c - city) :precondition (and (at ?p ?c) (at ?a ?c)) :effect (and (not (at ?p ?c)) (in ?p ?a)))\n(:action debark :parameters (?p - person ?a - aircraft ?c - city) :precondition (and (in ?p ?a) (at ?a ?c)) :effect (and (not (in ?p ?a)) (at ?p ?c)))\n(:action fly :parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 - flevel) :precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)) :effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))\n(fuel-level ?a ?l2)))\n(:action zoom :parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 ?l3 - flevel) :precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)\n(next ?l3 ?l2) ) :effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))\n(fuel-level ?a ?l3) ))\n(:action refuel :parameters (?a - aircraft ?c - city ?l - flevel ?l1 - flevel) :precondition (and (fuel-level ?a ?l) (next ?l ?l1) (at ?a ?c)) :effect (and (fuel-level ?a ?l1) (not (fuel-level ?a ?l)))))\nOne learned model (one possible satisfying model of our formula) from our random-sequence input in this Zeno-Travel domain is the following (brought together with the experimental parameters). Zenotravel domain:\n* IPC3 problem 9 * 91 fluents, 21000 possible unique actions * 1000 actions in learned action sequence * 5 observed fluents per step * \"schematized\" learning * 1:1 precondition heuristics * Action distribution: ((ZOOM . 27) (FLY . 216) (REFUEL . 264) (BOARD . 249) (DEBARK . 244))\nconverting to CNF clause count: 71119 variable count: 138 adding clauses calling zchaff parsing result SLAF time: 1.109 Inference time: 11.015 Learned model:\n(REFUEL NEEDS (FUEL-LEVEL ?A ?L)) (REFUEL NEEDS (NOT (FUEL-LEVEL ?A ?L1))) (REFUEL CAUSES (NOT (FUEL-LEVEL ?A ?L))) (REFUEL CAUSES (FUEL-LEVEL ?A ?L1)) (REFUEL KEEPS (NEXT ?L ?L)) (REFUEL KEEPS (NEXT ?L ?L1)) (REFUEL KEEPS (NEXT ?L1 ?L)) (REFUEL KEEPS (NEXT ?L1 ?L1)) (ZOOM NEEDS (NOT (FUEL-LEVEL ?A ?L3))) (ZOOM NEEDS (FUEL-LEVEL ?A ?L1)) (ZOOM CAUSES (FUEL-LEVEL ?A ?L3)) (ZOOM CAUSES (NOT (FUEL-LEVEL ?A ?L1))) (ZOOM KEEPS (FUEL-LEVEL ?A ?L2)) (ZOOM KEEPS (NEXT ?L3 ?L3)) (ZOOM KEEPS (NEXT ?L3 ?L2)) (ZOOM KEEPS (NEXT ?L3 ?L1)) (ZOOM KEEPS (NEXT ?L2 ?L3)) (ZOOM KEEPS (NEXT ?L2 ?L2)) (ZOOM KEEPS (NEXT ?L2 ?L1)) (ZOOM KEEPS (NEXT ?L1 ?L3)) (ZOOM KEEPS (NEXT ?L1 ?L2)) (ZOOM KEEPS (NEXT ?L1 ?L1)) (FLY NEEDS (NOT (FUEL-LEVEL ?A ?L2))) (FLY NEEDS (FUEL-LEVEL ?A ?L1)) (FLY CAUSES (FUEL-LEVEL ?A ?L2)) (FLY CAUSES (NOT (FUEL-LEVEL ?A ?L1))) (FLY KEEPS (NEXT ?L2 ?L2)) (FLY KEEPS (NEXT ?L2 ?L1)) (FLY KEEPS (NEXT ?L1 ?L2)) (FLY KEEPS (NEXT ?L1 ?L1)) (DEBARK NEEDS (IN ?P ?A)) (DEBARK CAUSES (NOT (IN ?P ?A))) (BOARD NEEDS (NOT (IN ?P ?A))) (BOARD CAUSES (IN ?P ?A))"}, {"heading": "C.3 Blocks-World Domain", "text": "The Blocksworld domain has the following generating PDDL:\n(define (domain blocksworld) (:requirements :strips) (:predicates (clear ?x - object)\n(on-table ?x - object) (arm-empty) (holding ?x - object) (on ?x ?y - object))\n(:action pickup :parameters (?ob - object) :precondition (and (clear ?ob) (on-table ?ob) (arm-empty)) :effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))\n(not (arm-empty))))\n(:action putdown :parameters (?ob - object) :precondition (holding ?ob) :effect (and (clear ?ob) (arm-empty) (on-table ?ob)\n(not (holding ?ob))))\n(:action stack :parameters (?ob - object\n?underob - object) :precondition (and (clear ?underob) (holding ?ob)) :effect (and (arm-empty) (clear ?ob) (on ?ob ?underob)\n(not (clear ?underob)) (not (holding ?ob))))\n(:action unstack :parameters (?ob - object\n?underob - object) :precondition (and (on ?ob ?underob) (clear ?ob) (arm-empty)) :effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob))\n(not (clear ?ob)) (not (arm-empty)))))\nOne learned model (one possible satisfying model of our formula) from our random-sequence input in this Blocksworld domain is the following (brought together with the experimental parameters).\nBlocksworld domain: * 209 fluents * 1000 randomly selected actions * 10 fluents observed per step * \"schematized\" learning * 1:1 precondition heuristics\nconverting to CNF clause count: 235492 variable count: 187 adding clauses calling zchaff parsing result SLAF time: 2.203 Inference time: 42.312\nLearned model:\n(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB))) (UNSTACK NEEDS (CLEAR ?OB)) (UNSTACK NEEDS (ARM-EMPTY)) (UNSTACK NEEDS (NOT (HOLDING ?OB))) (UNSTACK NEEDS (ON ?OB ?UNDEROB)) (UNSTACK CAUSES (CLEAR ?UNDEROB)) (UNSTACK CAUSES (NOT (CLEAR ?OB))) (UNSTACK CAUSES (NOT (ARM-EMPTY))) (UNSTACK CAUSES (HOLDING ?OB)) (UNSTACK CAUSES (NOT (ON ?OB ?UNDEROB))) (UNSTACK KEEPS (ON-TABLE ?UNDEROB)) (UNSTACK KEEPS (ON-TABLE ?OB)) (UNSTACK KEEPS (HOLDING ?UNDEROB)) (UNSTACK KEEPS (ON ?UNDEROB ?UNDEROB)) (UNSTACK KEEPS (ON ?OB ?OB)) (UNSTACK KEEPS (ON ?UNDEROB ?OB)) (STACK NEEDS (CLEAR ?UNDEROB)) (STACK NEEDS (NOT (CLEAR ?OB))) (STACK NEEDS (NOT (ARM-EMPTY))) (STACK NEEDS (HOLDING ?OB)) (STACK NEEDS (NOT (ON ?OB ?UNDEROB))) (STACK CAUSES (NOT (CLEAR ?UNDEROB))) (STACK CAUSES (CLEAR ?OB)) (STACK CAUSES (ARM-EMPTY)) (STACK CAUSES (NOT (HOLDING ?OB))) (STACK CAUSES (ON ?OB ?UNDEROB)) (STACK KEEPS (ON-TABLE ?UNDEROB)) (STACK KEEPS (ON-TABLE ?OB)) (STACK KEEPS (HOLDING ?UNDEROB)) (STACK KEEPS (ON ?UNDEROB ?UNDEROB)) (STACK KEEPS (ON ?OB ?OB)) (STACK KEEPS (ON ?UNDEROB ?OB)) (PUTDOWN NEEDS (NOT (CLEAR ?OB))) (PUTDOWN NEEDS (NOT (ON-TABLE ?OB))) (PUTDOWN NEEDS (NOT (ARM-EMPTY))) (PUTDOWN NEEDS (HOLDING ?OB)) (PUTDOWN CAUSES (CLEAR ?OB)) (PUTDOWN CAUSES (ON-TABLE ?OB)) (PUTDOWN CAUSES (ARM-EMPTY)) (PUTDOWN CAUSES (NOT (HOLDING ?OB))) (PUTDOWN KEEPS (ON ?OB ?OB)) (PICKUP NEEDS (CLEAR ?OB)) (PICKUP NEEDS (ON-TABLE ?OB)) (PICKUP NEEDS (ARM-EMPTY)) (PICKUP NEEDS (NOT (HOLDING ?OB))) (PICKUP CAUSES (NOT (CLEAR ?OB))) (PICKUP CAUSES (NOT (ON-TABLE ?OB))) (PICKUP CAUSES (NOT (ARM-EMPTY))) (PICKUP CAUSES (HOLDING ?OB)) (PICKUP KEEPS (ON ?OB ?OB))"}, {"heading": "C.4 Depot Domain", "text": "The Depot domain has the following generating PDDL:\n(define (domain Depot) (:requirements :typing) (:types place locatable - object\ndepot distributor - place truck hoist surface - locatable pallet crate - surface)\n(:predicates (at ?x - locatable ?y - place) (on ?x - crate ?y - surface) (in ?x - crate ?y - truck) (lifting ?x - hoist ?y - crate) (available ?x - hoist) (clear ?x - surface))\n(:action Drive :parameters (?x - truck ?y - place ?z - place) :precondition (and (at ?x ?y)) :effect (and (not (at ?x ?y)) (at ?x ?z)))\n(:action Lift :parameters (?x - hoist ?y - crate ?z - surface ?p - place) :precondition (and (at ?x ?p) (available ?x) (at ?y ?p) (on ?y ?z) (clear ?y)) :effect (and (not (at ?y ?p)) (lifting ?x ?y) (not (clear ?y))\n(not (available ?x)) (clear ?z) (not (on ?y ?z))))\n(:action Drop :parameters (?x - hoist ?y - crate ?z - surface ?p - place) :precondition (and (at ?x ?p) (at ?z ?p) (clear ?z) (lifting ?x ?y)) :effect (and (available ?x) (not (lifting ?x ?y)) (at ?y ?p)\n(not (clear ?z)) (clear ?y) (on ?y ?z)))\n(:action Load :parameters (?x - hoist ?y - crate ?z - truck ?p - place) :precondition (and (at ?x ?p) (at ?z ?p) (lifting ?x ?y)) :effect (and (not (lifting ?x ?y)) (in ?y ?z) (available ?x)))\n(:action Unload :parameters (?x - hoist ?y - crate ?z - truck ?p - place) :precondition (and (at ?x ?p) (at ?z ?p) (available ?x) (in ?y ?z)) :effect (and (not (in ?y ?z)) (not (available ?x)) (lifting ?x ?y))) )\nOne learned model (one possible satisfying model of our formula) from our random-sequence input in this Depot domain is the following (brought together with the experimental parameters).\nDepots domain: * IPC3 problem 5 * 250 fluents * 1000 randomly selected actions * 10 fluents observed per step * \"schematized\" learning * 1:1 precondition heuristics\nconverting to CNF\nclause count: 85359 variable count: 236 adding clauses calling zchaff parsing result SLAF time: 2.797 Inference time: 8.062 Learned model:\n(UNLOAD NEEDS (IN ?Y ?Z)) (UNLOAD NEEDS (NOT (LIFTING ?X ?Y))) (UNLOAD NEEDS (AVAILABLE ?X)) (UNLOAD CAUSES (NOT (IN ?Y ?Z))) (UNLOAD CAUSES (LIFTING ?X ?Y)) (UNLOAD CAUSES (NOT (AVAILABLE ?X))) (UNLOAD KEEPS (AT ?Z ?P)) (UNLOAD KEEPS (AT ?Y ?P)) (UNLOAD KEEPS (AT ?X ?P)) (UNLOAD KEEPS (ON ?Y ?Y)) (UNLOAD KEEPS (CLEAR ?Y)) (LOAD NEEDS (NOT (IN ?Y ?Z))) (LOAD NEEDS (LIFTING ?X ?Y)) (LOAD NEEDS (NOT (AVAILABLE ?X))) (LOAD CAUSES (IN ?Y ?Z)) (LOAD CAUSES (NOT (LIFTING ?X ?Y))) (LOAD CAUSES (AVAILABLE ?X)) (LOAD KEEPS (AT ?Z ?P)) (LOAD KEEPS (AT ?Y ?P)) (LOAD KEEPS (AT ?X ?P)) (LOAD KEEPS (ON ?Y ?Y)) (LOAD KEEPS (CLEAR ?Y)) (DROP NEEDS (NOT (AT ?Y ?P))) (DROP NEEDS (NOT (ON ?Y ?Z))) (DROP NEEDS (LIFTING ?X ?Y)) (DROP NEEDS (NOT (AVAILABLE ?X))) (DROP NEEDS (CLEAR ?Z)) (DROP NEEDS (NOT (CLEAR ?Y))) (DROP CAUSES (AT ?Y ?P)) (DROP CAUSES (ON ?Z ?Z)) (DROP CAUSES (NOT (ON ?Z ?Z))) (DROP CAUSES (ON ?Z ?Y)) (DROP CAUSES (NOT (ON ?Z ?Y))) (DROP CAUSES (ON ?Y ?Z)) (DROP CAUSES (NOT (LIFTING ?X ?Y))) (DROP CAUSES (LIFTING ?X ?Z)) (DROP CAUSES (NOT (LIFTING ?X ?Z))) (DROP CAUSES (AVAILABLE ?X)) (DROP CAUSES (NOT (CLEAR ?Z))) (DROP CAUSES (CLEAR ?Y)) (DROP KEEPS (AT ?Z ?P)) (DROP KEEPS (AT ?X ?P)) (DROP KEEPS (ON ?Z ?Z)) (DROP KEEPS (ON ?Z ?Y)) (DROP KEEPS (ON ?Y ?Y)) (DROP KEEPS (LIFTING ?X ?Z))\n(LIFT NEEDS (AT ?Y ?P)) (LIFT NEEDS (ON ?Y ?Z)) (LIFT NEEDS (NOT (LIFTING ?X ?Y))) (LIFT NEEDS (AVAILABLE ?X)) (LIFT NEEDS (NOT (CLEAR ?Z))) (LIFT NEEDS (CLEAR ?Y)) (LIFT CAUSES (NOT (AT ?Y ?P))) (LIFT CAUSES (NOT (ON ?Y ?Z))) (LIFT CAUSES (ON ?Z ?Z)) (LIFT CAUSES (NOT (ON ?Z ?Z))) (LIFT CAUSES (ON ?Z ?Y)) (LIFT CAUSES (NOT (ON ?Z ?Y))) (LIFT CAUSES (LIFTING ?X ?Y)) (LIFT CAUSES (LIFTING ?X ?Z)) (LIFT CAUSES (NOT (LIFTING ?X ?Z))) (LIFT CAUSES (NOT (AVAILABLE ?X))) (LIFT CAUSES (CLEAR ?Z)) (LIFT CAUSES (NOT (CLEAR ?Y))) (LIFT KEEPS (AT ?Z ?P)) (LIFT KEEPS (AT ?X ?P)) (LIFT KEEPS (ON ?Y ?Y)) (LIFT KEEPS (ON ?Z ?Z)) (LIFT KEEPS (ON ?Z ?Y)) (LIFT KEEPS (LIFTING ?X ?Z)) (DRIVE NEEDS (AT ?X ?Y)) (DRIVE NEEDS (NOT (AT ?X ?Z))) (DRIVE CAUSES (NOT (AT ?X ?Y))) (DRIVE CAUSES (AT ?X ?Z))"}], "references": [], "referenceMentions": [], "year": 0, "abstractText": "We present exact algorithms for identifying deterministic-actions\u2019 effects and preconditions in<lb>dynamic partially observable domains. They apply when one does not know the action model (the<lb>way actions affect the world) of a domain and must learn it from partial observations over time.<lb>Such scenarios are common in real world applications. They are challenging for AI tasks because<lb>traditional domain structures that underly tractability (e.g., conditional independence) fail there<lb>(e.g., world features become correlated). Our work departs from traditional assumptions about<lb>partial observations and action models. In particular, it focuses on problems in which actions are<lb>deterministic of simple logical structure and observation models have all features observed with<lb>some frequency. We yield tractable algorithms for the modified problem for such domains.<lb>Our algorithms take sequences of partial observations over time as input, and output determin-<lb>istic action models that could have lead to those observations. The algorithms output all or one of<lb>those models (depending on our choice), and are exact in that no model is misclassified given the<lb>observations. Our algorithms take polynomial time in the number of time steps and state features<lb>for some traditional action classes examined in the AI-planning literature, e.g., STRIPS actions. In<lb>contrast, traditional approaches for HMMs and Reinforcement Learning are inexact and exponen-<lb>tially intractable for such domains. Our experiments verify the theoretical tractability guarantees,<lb>and show that we identify action models exactly. Several applications in planning, autonomous<lb>exploration, and adventure-game playing already use these results. They are also promising for<lb>probabilistic settings, partially observable reinforcement learning, and diagnosis.", "creator": null}}}