{"id": "1401.3457", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "abstract": "This paper presents a new method of concluding the semantic properties of documents by using free-text keyphrase annotations, which are becoming increasingly common due to the recent dramatic growth of semi-structured, user-generated online content. A particularly relevant area is product reviews, which are often commented on by their authors with pro / contra keyphrases, such as a real bargain or good value. These annotations represent the underlying semantic properties, but unlike expert comments, they are loud: laymen may use different labels to name the same property, and some labels may be missing. To learn with such loud annotations, we find a hidden paraphrase structure that clusters the keyphrases. The paraphrase structure is linked to a latent theme model of review texts, allowing the system to predict the properties of uncommented documents and effectively aggregate the semantic properties of multiple keyphrases.", "histories": [["v1", "Wed, 15 Jan 2014 05:14:31 GMT  (513kb)", "http://arxiv.org/abs/1401.3457v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["s r k branavan", "harr chen", "jacob eisenstein", "regina barzilay"], "accepted": true, "id": "1401.3457"}, "pdf": {"name": "1401.3457.pdf", "metadata": {"source": "CRF", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "authors": ["S.R.K. Branavan", "Harr Chen", "Jacob Eisenstein", "Regina Barzilay"], "emails": ["BRANAVAN@CSAIL.MIT.EDU", "HARR@CSAIL.MIT.EDU", "JACOBE@CSAIL.MIT.EDU", "REGINA@CSAIL.MIT.EDU"], "sections": [{"heading": null, "text": "aging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \u201ca real bargain\u201d or \u201cgood value.\u201d These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases."}, {"heading": "1. Introduction", "text": "Identifying the document-level semantic properties implied by a text is a core problem in natural language understanding. For example, given the text of a restaurant review, it would be useful to extract a semantic-level characterization of the author\u2019s reaction to specific aspects of the restaurant, such as food and service quality (see Figure 1). Learning-based approaches have dramatically increased the scope and robustness of such semantic processing, but they are typically dependent on large expert-annotated datasets, which are costly to produce (Zaenen, 2006).\nWe propose to use an alternative source of annotations for learning: free-text keyphrases produced by novice users. As an example, consider the lists of pros and cons that often accompany reviews of products and services. Such end-user annotations are increasingly prevalent online, and they grow organically to keep pace with subjects of interest and socio-cultural trends. Beyond such pragmatic considerations, free-text annotations are appealing from a linguistic standpoint because they capture the intuitive semantic judgments of non-specialist language users. In many real-world datasets, these annotations are created by the document\u2019s original author, providing a direct window into the semantic judgments that motivated the document text.\nc\u00a92009 AI Access Foundation. All rights reserved.\nThe major obstacle to the computational use of such free-text annotations is that they are inherently noisy \u2014 there is no fixed vocabulary, no explicit relationship between annotation keyphrases, and no guarantee that all relevant semantic properties of a document will be annotated. For example, in the pros/cons annotations accompanying the restaurant reviews in Figure 1, the same underlying semantic idea is expressed in different ways through the keyphrases \u201cgreat nutritional value\u201d and \u201chealthy.\u201d Additionally, the first review discusses quality of service, but is not annotated as such. In contrast, expert annotations would replace synonymous keyphrases with a single canonical label, and would fully label all semantic properties described in the text. Such expert annotations are typically used in supervised learning methods. As we will demonstrate in the paper, traditional supervised approaches perform poorly when free-text annotations are used instead of clean, expert annotations.\nThis paper demonstrates a new approach for handling free-text annotation in the context of a hidden-topic analysis of the document text. We show that regularities in the text can clarify noise in the annotations \u2014 for example, although \u201cgreat nutritional value\u201d and \u201chealthy\u201d have different surface forms, the text in documents that are annotated by these two keyphrases will likely be similar. By modeling the relationship between document text and annotations over a large dataset, it is possible to induce a clustering over the annotation keyphrases that can help to overcome the problem of inconsistency. Our model also addresses the problem of incompleteness \u2014 when novice annotators fail to label relevant semantic topics \u2014 by estimating which topics are predicted by the document text alone.\nCentral to this approach is the idea that both document text and the associated annotations reflect a single underlying set of semantic properties. In the text, the semantic properties correspond to the induced hidden topics \u2014 this is similar to the growing body of work on latent topic models, such as latent Dirichlet allocation (LDA; Blei, Ng, & Jordan, 2003). However, unlike existing work on topic modeling, we tie hidden topics in the text with clusters of observed keyphrases. This connection is motivated by the idea that both the text and its associated annotations are grounded in a shared set of semantic properties. By modeling these properties directly, we ensure that the inferred hidden topics are semantically meaningful, and that the clustering over free-text annotations is robust to noise.\nOur approach takes the form of a hierarchical Bayesian framework, and includes an LDA-style component in which each word in the text is generated from a mixture of multinomials. In addition, we also incorporate a similarity matrix across the universe of annotation keyphrases, which is\nconstructed based on the orthographic and distributional features of the keyphrases. We model this matrix as being generated from an underlying clustering over the keyphrases, such that keyphrases that are clustered together are likely to produce high similarity scores. To generate the words in each document, we model two distributions over semantic properties \u2014 one governed by the annotation keyphrases and their clusters, and a background distribution to cover properties not mentioned in the annotations. The latent topic for each word is drawn from a mixture of these two distributions. After learning model parameters from a noisily-labeled training set, we can apply the model to unlabeled data.\nWe build a system that extracts semantic properties from reviews of products and services. This system uses as training corpus that includes user-created free-text annotations of the pros and cons in each review. Training yields two outputs: a clustering of keyphrases into semantic properties, and a topic model that is capable of inducing the semantic properties of unlabeled text. The clustering of annotation keyphrases is relevant for applications such as content-based information retrieval, allowing users to retrieve documents with semantically relevant annotations even if their surface forms differ from the query term. The topic model can be used to infer the semantic properties of unlabeled text.\nThe topic model can also be used to perform multi-document summarization, capturing the key semantic properties of multiple reviews. Unlike traditional extraction-based approaches to multidocument summarization, our induced topic model abstracts the text of each review into a representation capturing the relevant semantic properties. This enables comparison between reviews even when they use superficially different terminology to describe the same set of semantic properties. This idea is implemented in a review aggregation system that extracts the majority sentiment of multiple reviewers for each product or service. An example of the output produced by this system is shown in Figure 6. This system is applied to reviews in 480 product categories, allowing users to navigate the semantic properties of 49,490 products based on a total of 522,879 reviews. The effectiveness of our approach is confirmed by several evaluations.\nFor the summarization of both single and multiple documents, we compare the properties inferred by our model with expert annotations. Our approach yields substantially better results than alternatives from the research literature; in particular, we find that learning a clustering of free-text annotation keyphrases is essential to extracting meaningful semantic properties from our dataset. In addition, we compare the induced clustering with a gold standard clustering produced by expert annotators. The comparison shows that tying the clustering to the hidden topic model substantially improves its quality, and that the clustering induced by our system coheres well with the clustering produced by expert annotators.\nThe remainder of the paper is structured as follows. Section 2 compares our approach with previous work on topic modeling, semantic property extraction, and multi-document summarization. Section 3 describes the properties of free-text annotations that motivate our approach. The model itself is described in Section 4, and a method for parameter estimation is presented in Section 5. Section 6 describes the implementation and evaluation of single-document and multi-document summarization systems using these techniques. We summarize our contributions and consider directions for future work in Section 7. The code, datasets and expert annotations used in this paper are available online at http://groups.csail.mit.edu/rbg/code/precis/."}, {"heading": "2. Related Work", "text": "The material presented in this section covers three lines of related work. First, we discuss work on Bayesian topic modeling that is related to our technique for learning from free-text annotations. Next, we discuss state-of-the-art methods for identifying and analyzing product properties from the review text. Finally, we situate our summarization work in the landscape of prior research on multi-document summarization."}, {"heading": "2.1 Bayesian Topic Modeling", "text": "Recent work in the topic modeling literature has demonstrated that semantically salient topics can be inferred in an unsupervised fashion by constructing a generative Bayesian model of the document text. One notable example of this line of research is Latent Dirichlet Allocation (LDA; Blei et al., 2003). In the LDA framework, semantic topics are equated to latent distributions of words in a text; thus, each document is modeled as a mixture of topics. This class of models has been used for a variety of language processing tasks including topic segmentation (Purver, Ko\u0308rding, Griffiths, & Tenenbaum, 2006), named-entity resolution (Bhattacharya & Getoor, 2006), sentiment ranking (Titov & McDonald, 2008b), and word sense disambiguation (Boyd-Graber, Blei, & Zhu, 2007).\nOur method is similar to LDA in that it assigns latent topic indicators to each word in the dataset, and models documents as mixtures of topics. However, the LDA model is unsupervised, and does not provide a method for linking the latent topics to external observed representations of the properties of interest. In contrast, our model exploits the free-text annotations in our dataset to ensure that the induced topics correspond to semantically meaningful properties.\nCombining topics induced by LDA with external supervision was first considered by Blei and McAuliffe (2008) in their supervised Latent Dirichlet Allocation (sLDA) model. The induction of the hidden topics is driven by annotated examples provided during the training stage. From the perspective of supervised learning, this approach succeeds because the hidden topics mediate between document annotations and lexical features. Blei and McAuliffe describe a variational expectationmaximization procedure for approximate maximum-likelihood estimation of the model\u2019s parameters. When tested on two polarity assessment tasks, sLDA shows improvement over a model in which topics where induced by an unsupervised model and then added as features to a supervised model.\nThe key difference between our model and sLDA is that we do not assume access to clean supervision data during training. Since the annotations provided to our algorithm are free-text in nature, they are incomplete and fraught with inconsistency. This substantial difference in input structure motivates the need for a model that simultaneously induces the hidden structure in freetext annotations and learns to predict properties from text."}, {"heading": "2.2 Property Assessment for Review Analysis", "text": "Our model is applied to the task of review analysis. Traditionally, the task of identifying the properties of a product from review texts has been cast as an extraction problem (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005; Popescu, Nguyen, & Etzioni, 2005). For example, Hu and Liu (2004) employ association mining to identify noun phrases that express key portions of product reviews. The polarity of the extracted phrases is determined using a seed set of adjectives expanded via WordNet\nrelations. A summary of a review is produced by extracting all property phrases present verbatim in the document.\nProperty extraction was further refined in OPINE (Popescu et al., 2005), another system for review analysis. OPINE employs a novel information extraction method to identify noun phrases that could potentially express the salient properties of reviewed products; these candidates are then pruned using WordNet and morphological cues. Opinion phrases are identified using a set of handcrafted rules applied to syntactic dependencies extracted from the input document. The semantic orientation of properties is computed using a relaxation labeling method that finds the optimal assignment of polarity labels given a set of local constraints. Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words.\nThese two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al. (2005), opinion phrases are extracted via handcrafted rules. An alternative approach is to learn the rules for feature extraction from annotated data. To this end, property identification can be modeled in a classification framework (Kim & Hovy, 2006). A classifier is trained using a corpus in which free-text pro and con keyphrases are specified by the review authors. These keyphrases are compared against sentences in the review text; sentences that exhibit high word overlap with previously identified phrases are marked as pros or cons according to the phrase polarity. The rest of the sentences are marked as negative examples.\nClearly, the accuracy of the resulting classifier depends on the quality of the automatically induced annotations. Our analysis of free-text annotations in several domains shows that automatically mapping from even manually-extracted annotation keyphrases to a document text is a difficult task, due to variability in keyphrase surface realizations (see Section 3). As we argue in the rest of this paper, it is beneficial to explicitly address the difficulties inherent in free-text annotations. To this end, our work is distinguished in two significant ways from the property extraction methods described above. First, we are able to predict properties beyond those that appear verbatim in the text. Second, our approach also learns the semantic relationships between different keyphrases, allowing us to draw direct comparisons between reviews even when the semantic ideas are expressed using different surface forms.\nWorking in the related domain of web opinion mining, Lu and Zhai (2008) describe a system that generates integrated opinion summaries, which incorporate expert-written articles (e.g., a review from an online magazine) and user-generated \u201cordinary\u201d opinion snippets (e.g., mentions in blogs). Specifically, the expert article is assumed to be structured into segments, and a collection of representative ordinary opinions is aligned to each segment. Probabilistic Latent Semantic Analysis (PLSA) is used to induce a clustering of opinion snippets, where each cluster is attached to one of the expert article segments. Some clusters may also be unaligned to any segment, indicating opinions that are entirely unexpressed in the expert article. Ultimately, the integrated opinion summary is this combination of a single expert article with multiple user-generated opinion snippets that confirm or supplement specific segments of the review.\nOur work\u2019s final goal is different \u2014 we aim to provide a highly compact summary of a multitude of user opinions by identifying the underlying semantic properties, rather than supplementing a single expert article with user opinions. We specifically leverage annotations that users already provide in their reviews, thus obviating the need for an expert article as a template for opinion inte-\ngration. Consequently, our approach is more suitable for the goal of producing concise keyphrase summarizations of user reviews, particularly when no review can be taken as authoritative.\nThe work closest in methodology to our approach is a review summarizer developed by Titov and McDonald (2008a). Their method summarizes a review by selecting a list of phrases that express writers\u2019 opinions in a set of predefined properties (e.g.,, food and ambiance for restaurant reviews). The system has access to numerical ratings in the same set of properties, but there is no training set providing examples of appropriate keyphrases to extract. Similar to sLDA, their method uses the numerical ratings to bias the hidden topics towards the desired semantic properties. Phrases that are strongly associated with properties via hidden topics are extracted as part of a summary.\nThere are several important differences between our work and the summarization method of Titov and McDonald. Their method assumes a predefined set of properties and thus cannot capture properties outside of that set. Moreover, consistent numerical annotations are required for training, while our method emphasizes the use of free-text annotations. Finally, since Titov and McDonald\u2019s algorithm is extractive, it does not facilitate property comparison across multiple reviews."}, {"heading": "2.3 Multidocument Summarization", "text": "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev & McKeown, 1998; Carbonell & Goldstein, 1998; Mani & Bloedorn, 1997; Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters.\nIdentification of repeated information is equally central in our approach \u2014 our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors. Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999; Marsi & Krahmer, 2005). Both string- and tree-based comparison algorithms are augmented with lexico-semantic knowledge using resources such as WordNet.\nThe approach described in this paper does not perform comparisons at the sentence level. Instead, we first abstract reviews into a set of properties and then compare property overlap across different documents. This approach relates to domain-dependent approaches for text summarization (Radev & McKeown, 1998; White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001; Elhadad & McKeown, 2001). These methods identify the relations between documents by comparing their abstract representations. In these cases, the abstract representation is constructed using off-the-shelf information extraction tools. A template specifying what types of information to select is crafted manually for a domain of interest. Moreover, the training of information extraction systems requires a corpus manually annotated with the relations of interest. In contrast, our method does not require\nmanual template specification or corpora annotated by experts. While the abstract representations that we induce are not as linguistically rich as extraction templates, they nevertheless enable us to perform in-depth comparisons across different reviews."}, {"heading": "3. Analysis of Free-Text Keyphrase Annotations", "text": "In this section, we explore the characteristics of free-text annotations, aiming to quantify the degree of noise observed in this data. The results of this analysis motivate the development of the learning algorithm described in Section 4.\nWe perform this investigation in the domain of online restaurant reviews using documents downloaded from the popular Epinions1 website. Users of this website evaluate products by providing both a textual description of their opinion, as well as concise lists of keyphrases (pros and cons) summarizing the review. Pros/cons keyphrases are an appealing source of annotations for online review texts. However, they are contributed independently by multiple users and are thus unlikely to be as clean as expert annotations. In our analysis, we focus on two features of free-text annotations: incompleteness and inconsistency. The measure of incompleteness quantifies the degree of label omission in free-text annotations, while inconsistency reflects the variance of the keyphrase vocabulary used by various annotators.\nTo test the quality of these user-generated annotations, we compare them against \u201cexpert\u201d annotations produced in a more systematic fashion. This annotation effort focused on six properties that were commonly mentioned by the review authors, specifically those shown in Table 1. Given a review and a property, the task is to assess whether the review\u2019s text supports the property. These annotations were produced by two judges guided by a standardized set of instructions. In contrast to author annotations from the website, the judges conferred during a training session to ensure consistency and completeness. The two judges collectively annotated 170 reviews, with 30 annotated\n1. http://www.epinions.com/\nby both. Cohen\u2019s Kappa, a measure of inter-annotator agreement that ranges from zero to one, is 0.78 on this joint set, indicating high agreement (Cohen, 1960). On average, each review text was annotated with 2.56 properties.\nSeparately, one of the judges also standardized the free-text pros/cons annotations for the same 170 reviews. Each review\u2019s keyphrases were matched to the same six properties. This standardization allows for direct comparison between the properties judged to be supported by a review\u2019s text and the properties described in the same review\u2019s free-text annotations. We find that many semantic properties that were judged to be present in the text were not user annotated \u2014 on average, the keyphrases expressed 1.66 relevant semantic properties per document, while the text expressed 2.56 properties. This gap demonstrates the frequency with which authors omitted relevant semantic properties from their review annotations."}, {"heading": "3.1 Incompleteness", "text": "To measure incompleteness, we compare the properties stated by review authors in the form of pros and cons against those stated only in the review text, as judged by expert annotators. This comparison is performed using precision, recall and F-score. In this setting, recall is the proportion of semantic properties in the text for which the review author also provided at least one annotation keyphrase; precision is the proportion of keyphrases that conveyed properties judged to be supported by the text; and F-score is their harmonic mean. The results of the comparison are summarized in the left half of Table 1.\nThese incompleteness results demonstrate the significant discrepancy between user and expert annotations. As expected, recall is quite low; more than 40% of property occurrences are stated in the review text without being explicitly mentioned in the annotations. The precision scores indicate that the converse is also true, though to a lesser extent \u2014 some keyphrases will express properties not mentioned in text.\nInterestingly, precision and recall vary greatly depending on the specific property. They are highest for good food, matching the intuitive notion that high food quality would be a key salient property of a restaurant, and thus more likely to be mentioned in both text and annotations. Conversely, the recall for good service is lower \u2014 for most users, high quality of service is apparently not a key point when summarizing a review with keyphrases."}, {"heading": "3.2 Inconsistency", "text": "The lack of a unified annotation scheme in the restaurant review dataset is apparent \u2014 across all reviewers, the annotations feature 26,801 unique keyphrase surface forms over a set of 49,310 total keyphrase occurrences. Clearly, many unique keyphrases express the same semantic property \u2014 in Figure 2, good price is expressed in ten different ways. To quantify this phenomenon, the judges\nmanually clustered a subset of the keyphrases associated with the six previously mentioned properties. Specifically, 121 keyphrases associated with the six major properties were chosen, accounting for 10.8% of all keyphrase occurrences.\nWe use these manually clustered annotations to examine the distributional pattern of keyphrases that describe the same underlying property, using two different statistics. First, the number of different keyphrases for each property gives a lower bound on the number of possible paraphrases. Second, we measure how often the most common keyphrase is used to annotate each property, i.e., the coverage of that keyphrase. This metric gives a sense of how diffuse the keyphrases within a property are, and specifically whether one single keyphrase dominates occurrences of the property. Note that this value is an overestimate of the true coverage, since we are only considering a tenth of all keyphrase occurrences.\nThe right half of Table 1 summarizes the variability of property paraphrases. Observe that each property is associated with numerous paraphrases, all of which were found multiple times in the actual keyphrase set. Most importantly, the most frequent keyphrase accounted for only about a third of all property occurrences, strongly suggesting that targeting only these labels for learning is a very limited approach. To further illustrate this last point, consider the property of good service, whose keyphrase realizations\u2019 distributional histogram appears in Figure 3. The cumulative percentage frequencies of the most frequent keyphrases associated with this property are plotted. The top four keyphrases here account for only three quarters of all property occurrences, even within the limited set of keyphrases we consider in this analysis, motivating the need for aggregate consideration of keyphrases.\nIn the next section, we introduce a model that induces a clustering among keyphrases while relating keyphrase clusters to the text, directly addressing these characteristics of the data."}, {"heading": "4. Model Description", "text": "We present a generative Bayesian model for documents annotated with free-text keyphrases. Our model assumes that each annotated document is generated from a set of underlying semantic topics. Semantic topics generate the document text by indexing a language model; in our approach, they are also associated with clusters of keyphrases. In this way, the model can be viewed as an extension of Latent Dirichlet Allocation (Blei et al., 2003), where the latent topics are additionally biased toward the keyphrases that appear in the training data. However, this coupling is flexible, as some words are permitted to be drawn from topics that are not represented by the keyphrase annotations. This permits the model to learn effectively in the presence of incomplete annotations, while still encouraging the keyphrase clustering to cohere with the topics supported by the document text.\nAnother critical aspect of our model is that we desire the ability to use arbitrary comparisons between keyphrases, in addition to information about their surface forms. To accommodate this goal, we do not treat the keyphrase surface forms as generated from the model. Rather, we acquire a real-valued similarity matrix across the universe of possible keyphrases, and treat this matrix as generated from the keyphrase clustering. This representation permits the use of surface and distributional features for keyphrase similarity, as described in Section 4.1.\nAn advantage of hierarchical Bayesian models is that it is easy to change which parts of the model are observed and which parts are hidden. During training, the keyphrase annotations are observed, so that the hidden semantic topics are coupled with clusters of keyphrases. To account for words not related to semantic topics, some topics may not have any associated keyphrases. At test time, the model is presented with documents for which the keyphrase annotations are hidden. The model is evaluated on its ability to determine which keyphrases are applicable, based on the hidden topics present in the document text.\nThe judgment of whether a topic applies to a given unannotated document is based on the probability mass assigned to that topic in the document\u2019s background topic distribution. Because there are no annotations, the background topic distribution should capture the entirety of the document\u2019s topics. For the task involving reviews of products and services, multiple topics may accompany each document. In this case, each topic whose probability is above a threshold (tuned on the development set) is predicted as being supported."}, {"heading": "4.1 Keyphrase Clustering", "text": "To handle the hidden paraphrase structure of the keyphrases, one component of the model estimates a clustering over keyphrases. The goal is to obtain clusters where each cluster correspond to a welldefined semantic topic \u2014 e.g., both \u201chealthy\u201d and \u201cgood nutrition\u201d should be grouped into a single cluster. Because our overall joint model is generative, a generative model for clustering could easily be integrated into the larger framework. Such an approach would treat all of the keyphrases in each cluster as being generated from a parametric distribution. However, this representation would not permit many powerful features for assessing the similarity of pairs of keyphrases, such as string overlap or keyphrase co-occurrence in a corpus (McCallum, Bellare, & Pereira, 2005).\nFor this reason, we represent each keyphrase as a real-valued vector rather than as its surface form. The vector for a given keyphrase includes the similarity scores with respect to every other observed keyphrase (the similarity scores are represented by s in Figure 4). We model these similarity scores as generated by the cluster memberships (represented by x in Figure 4). If two keyphrases\nare clustered together, their similarity score is generated from a distribution encouraging high similarity; otherwise, a distribution encouraging low similarity is used.2\nThe features used for producing the similarity matrix are given in Table 2, encompassing lexical and distributional similarity measures. Our implemented system takes a linear combination of these two data sources, weighting both sources equally. The resulting similarity matrix for keyphrases from the restaurant domain is shown in Figure 5.\nAs described in the next section, when clustering keyphrases, our model takes advantage of the topic structure of documents annotated with those keyphrases, in addition to information about the individual keyphrases themselves. In this sense, it differs from traditional approaches for paraphrase identification (Barzilay & McKeown, 2001; Lin & Pantel, 2001)."}, {"heading": "4.2 Document Topic Modeling", "text": "Our analysis of the document text is based on probabilistic topic models such as LDA (Blei et al., 2003). In the LDA framework, each word is generated from a language model that is indexed by the word\u2019s topic assignment. Thus, rather than identifying a single topic for a document, LDA identifies a distribution over topics. High probability topic assignments will identify compact, low-entropy language models, so that the probability mass of the language model for each topic is divided among a relatively small vocabulary.\nOur model operates in a similar manner, identifying a topic for each word, denoted by z in Figure 4. However, where LDA learns a distribution over topics for each document, we deterministically construct a document-specific topic distribution from the clusters represented by the document\u2019s keyphrases \u2014 this is \u03b7 in the figure. \u03b7 assigns equal probability to all topics that are represented in the keyphrase annotations, and very small probability to other topics. Generating the word topics in this way ties together the clustering and language models.\nAs noted above, sometimes the keyphrase annotation does not represent all of the semantic topics that are expressed in the text. For this reason, we also construct another \u201cbackground\u201d distribution \u03c6 over topics. The auxiliary variable c indicates whether a given word\u2019s topic is drawn from the distribution derived from annotations, or from the background model. Representing c as a hidden variable allows us to stochastically interpolate between the two language models \u03c6 and \u03b7. In addition, any given document will most likely also discuss topics that are not covered by any keyphrase. To account for this, the model is allowed to leave some of the clusters empty, thus leaving some of the topics to be independent of all the keyphrases."}, {"heading": "4.3 Generative Process", "text": "Our model assumes that all observed data is generated through a stochastic process involving hidden parameters. In this section, we formally specify this generative process. This specification guides inference of the hidden parameters based on observed data, which are the following:\n\u2022 For each of the L keyphrases, a vector s` of length L denoting a pairwise similarity score in the interval [0, 1] to every other keyphrase.\n\u2022 For each document d, its bag of words wd of length Nd. The nth word of d is wd,n.\n2. Note that while we model each similarity score as an independent draw; clearly this assumption is too strong, due to symmetry and transitivity. Models making similar assumptions about the independence of related hidden variables have previously been shown to be successful (for example, Toutanova & Johnson, 2008).\n\u2022 For each document d, a set of keyphrase annotations hd, which includes index ` if the document was annotated with keyphrase `.\n\u2022 The number of clusters K, which should be large enough to encompass topics with actual clusters of keyphrases, as well as word-only topics.\nThese observed variables are generated according to the following process:\n1. Draw a multinomial distribution \u03c8 over the K keyphrase clusters from a symmetric Dirichlet prior with parameter \u03c80.3\n2. For ` = 1 . . . L:\n(a) Draw the `th keyphrase\u2019s cluster assignment x` from Multinomial(\u03c8).\n3. For (`, `\u2032) = (1 . . . L, 1 . . . L):\n(a) If x` = x`\u2032 , draw s`,`\u2032 from Beta(\u03b1=) \u2261 Beta(2, 1), encouraging scores to be biased toward values close to one.\n(b) If x` 6= x`\u2032 , draw s`,`\u2032 from Beta(\u03b1 6=) \u2261 Beta(1, 2), encouraging scores to be biased toward values close to zero.\n4. For k = 1 . . .K:\n(a) Draw language model \u03b8k from a symmetric Dirichlet prior with parameter \u03b80.\n5. For d = 1 . . . D:\n(a) Draw a background topic model \u03c6d from a symmetric Dirichlet prior with parameter \u03c60.\n(b) Deterministically construct an annotation topic model \u03b7d, based on keyphrase cluster assignments x and observed document annotations hd. Specifically, let H be the set of topics represented by phrases in hd. Distribution \u03b7d assigns equal probability to each element of H, and a very small probability mass to other topics.4\n(c) Draw a weighted coin \u03bbd from Beta(\u03bb0), which will determine the balance between annotation \u03b7d and background topic models \u03c6d.\n(d) For n = 1 . . . Nd: i. Draw a binary auxiliary variable cd,n from Bernoulli(\u03bbd), which determines whether\nthe topic of the word wd,n is drawn from the annotation topic model \u03b7d or the background model \u03c6d.\nii. Draw a topic assignment zd,n from the appropriate multinomial as indicated by cd,n.\niii. Draw word wd,n from Multinomial(\u03b8zd,n), that is, the language model indexed by the word\u2019s topic.\n3. Variables subscripted with zero are fixed hyperparameters. 4. Making a hard assignment of zero probability to the other topics creates problems for parameter estimation. A\nprobability of 10\u22124 was assigned to all topics not represented by the keyphrase cluster memberships."}, {"heading": "5. Parameter Estimation", "text": "To make predictions on unseen data, we need to estimate the parameters of the model. In Bayesian inference, we estimate the distribution for each parameter, conditioned on the observed data and hyperparameters. Such inference is intractable in the general case, but sampling approaches allow us to approximately construct distributions for each parameter of interest.\nGibbs sampling is perhaps the most generic and straightforward sampling technique. Conditional distributions are computed for each hidden variable, given all the other variables in the model. By repeatedly sampling from these distributions in turn, it is possible to construct a Markov chain whose stationary distribution is the posterior of the model parameters (Gelman, Carlin, Stern, & Rubin, 2004). The use of sampling techniques in natural language processing has been previously investigated by many researchers, including Finkel, Grenager, and Manning (2005) and Goldwater, Griffiths, and Johnson (2006).\nWe now present sampling equations for each of the hidden variables in Figure 4. The prior over keyphrase clusters \u03c8 is sampled based on the hyperprior \u03c80 and the keyphrase cluster assignments x. We write p(\u03c8 | . . .) to mean the probability conditioned on all the other variables.\np(\u03c8 | . . .) \u221d p(\u03c8 | \u03c80)p(x | \u03c8), = p(\u03c8 | \u03c80) \u220f ` p(x` | \u03c8)\n= Dirichlet(\u03c8;\u03c80) \u220f ` Multinomial(x`;\u03c8) = Dirichlet(\u03c8;\u03c8\u2032),\nwhere \u03c8\u2032i is \u03c80 + count(x` = i). This conditional distribution is derived based on the conjugacy of the multinomial to the Dirichlet distribution. The first line follows from Bayes\u2019 rule, and the second line from the conditional independence of cluster assignments x given keyphrase distribution \u03c8.\nResampling equations for \u03c6d and \u03b8k can be derived in a similar manner:\np(\u03c6d | . . .) \u221d Dirichlet(\u03c6d;\u03c6\u2032d), p(\u03b8k | . . .) \u221d Dirichlet(\u03b8k; \u03b8\u2032k),\nwhere \u03c6\u2032d,i = \u03c60 + count(zn,d = i \u2227 cn,d = 0) and \u03b8\u2032k,i = \u03b80 + \u2211\nd count(wn,d = i \u2227 zn,d = k). In building the counts for \u03c6\u2032i, we consider only cases in which cn,d = 0, indicating that the topic zn,d is indeed drawn from the background topic model \u03c6d. Similarly, when building the counts for \u03b8\u2032k, we consider only cases in which the word wd,n is drawn from topic k.\nTo resample \u03bb, we employ the conjugacy of the Beta prior to the Bernoulli observation likelihoods, adding counts of c to the prior \u03bb0.\np(\u03bbd | . . .) \u221d Beta(\u03bbd;\u03bb\u2032d),\nwhere \u03bb\u2032d = \u03bb0 + [ \u2211\nn count(cd,n = 1)\u2211 n count(cd,n = 0)\n] .\nThe keyphrase cluster assignments are represented by x, whose sampling distribution depends on \u03c8, s, and z, via \u03b7:\np(x` | . . .) \u221d p(x` | \u03c8)p(s | x`,x\u2212`, \u03b1)p(z | \u03b7, \u03c8, c)\n\u221d p(x` | \u03c8) \u220f `\u2032 6=` p(s`,`\u2032 | x`, x`\u2032 , \u03b1)  D\u220f d \u220f cd,n=1 p(zd,n | \u03b7d)  = Multinomial(x`;\u03c8)\n\u220f `\u2032 6=` Beta(s`,`\u2032 ;\u03b1x`,x`\u2032 )  D\u220f d \u220f cd,n=1 Multinomial(zd,n; \u03b7d)  . The leftmost term of the above equation is the prior on x`. The next term encodes the dependence of the similarity matrix s on the cluster assignments; with slight abuse of notation, we write \u03b1x`,x`\u2032 to denote \u03b1= if x` = x`\u2032 , and \u03b1 6= otherwise. The third term is the dependence of the word topics zd,n on the topic distribution \u03b7d. We compute the final result of this probability expression for each possible setting of x`, and then sample from the normalized multinomial.\nThe word topics z are sampled according to the topic distribution \u03b7d, the background distribution \u03c6d, the observed words w, and the auxiliary variable c:\np(zd,n | . . .) \u221d p(zd,n | \u03c6, \u03b7d, cd,n)p(wd,n | zd,n, \u03b8)\n= { Multinomial(zd,n; \u03b7d)Multinomial(wd,n; \u03b8zd,n) if cd,n = 1, Multinomial(zd,n;\u03c6d)Multinomial(wd,n; \u03b8zd,n) otherwise.\nAs with x, each zd,n is sampled by computing the conditional likelihood of each possible setting within a constant of proportionality, and then sampling from the normalized multinomial.\nFinally, we sample the auxiliary variable cd,n, which indicates whether the hidden topic zd,n is drawn from \u03b7d or \u03c6d. c depends on its prior \u03bb and the hidden topic assignments z:\np(cd,n | . . .) \u221d p(cd,n | \u03bbd)p(zd,n | \u03b7d, \u03c6d, cd,n)\n= { Bernoulli(cd,n;\u03bbd)Multinomial(zd,n; \u03b7d) if cd,n = 1, Bernoulli(cd,n;\u03bbd)Multinomial(zd,n;\u03c6d) otherwise.\nAgain, we compute the likelihood of cd,n = 0 and cd,n = 1 within a constant of proportionality, and then sample from the normalized Bernoulli distribution.\nFinally, our model requires values for fixed hyperparameters \u03b80, \u03bb0, \u03c80, and \u03c60, which are tuned in the standard way based on development set performance. Appendix C lists the hyperparameters values used for each domain in our experiments.\nOne of the main applications of our model is to predict the properties supported by documents that are not annotated with keyphrases. At test time, we would like to compute a posterior estimate of \u03c6d for an unannotated test document d. Since annotations are not present, property prediction is based only on the text component of the model. For this estimate, we use the same Gibbs sampling procedure, restricted to zd,n and \u03c6d, with the stipulation that cd,n is fixed at zero so that zd,n is always drawn from \u03c6d. In particular, we treat the language models as known; to more accurately integrate over all possible language models, we use the final 1000 samples of the language models from training as opposed to using a point estimate. For each topic, if its probability in \u03c6d exceeds a certain threshold, that topic is predicted. This threshold is tuned independently for each topic on a development set. The empirical results we present in Section 6 are obtained in this manner."}, {"heading": "6. Evaluation of Summarization Quality", "text": "Our model for document analysis is implemented in PRE\u0301CIS,5 a system that performs single- and multi-document review summarization. The goal of PRE\u0301CIS is to provide users with effective access to review data via mobile devices. PRE\u0301CIS contains information about 49,490 products and services ranging from childcare products to restaurants and movies. For each of these products, the system contains a collection of reviews downloaded from consumer websites such as Epinions, CNET, and Amazon. PRE\u0301CIS compresses data for each product into a short list of pros and cons that are supported by the majority of reviews. An example of a summary of 27 reviews for the movie Pirates of the Caribbean: At World\u2019s End is shown in Figure 6. In contrast to traditional multidocument summarizers, the output of the system is not a sequence of sentences, but rather a list of phrases indicative of product properties. This summarization format follows the format of pros/cons summaries that individual reviewers provide on multiple consumer websites. Moreover, the brevity of the summary is particularly suitable for presenting on small screens such as those of mobile devices.\nTo automatically generate the combined pros/cons list for a product or service, we first apply our model to each review. The model is trained independently for each product domain (e.g., movies) using a corresponding subset of reviews with free-text annotations. These annotations also provide a set of keyphrases that contribute to the clusters associated with product properties. Once the\n5. PRE\u0301CIS is accessible at http://groups.csail.mit.edu/rbg/projects/precis/.\nmodel is trained, it labels each review with a set of properties. Since the set of possible properties is the same for all reviews of a product, the comparison among reviews is straightforward \u2014 for each property, we count the number of reviews that support it, and select the property as part of a summary if it is supported by the majority of the reviews. The set of semantic properties is converted into a pros/cons list by presenting the most common keyphrase for each property.\nThis aggregation technology is applicable in two scenarios. The system can be applied to unannotated reviews, inducing semantic properties from the document text; this conforms to the traditional way in which learning-based systems are applied to unlabeled data. However, our model is valuable even when individual reviews do include pros/cons keyphrase annotations. Due to the high degree of paraphrasing, direct comparison of keyphrases is challenging (see Section 3). By inferring a clustering over keyphrases, our model permits comparison of keyphrase annotations on a more semantic level.\nThe remainder of this section provides a set of evaluations of our model\u2019s ability to capture the semantic content of document text and keyphrase annotations. Section 6.1 describes an evaluation of our system\u2019s ability to extract meaningful semantic summaries from individual documents, and also assesses the quality of the paraphrase structure induced by our model. Section 6.2 extends this evaluation to our system\u2019s ability to summarize multiple review documents."}, {"heading": "6.1 Single-Document Evaluation", "text": "First, we evaluate our model with respect to its ability to reproduce the annotations present in individual documents, based on the document text. We compare against a wide variety of baselines and variations of our model, demonstrating the appropriateness of our approach to this task. In addition, we explicitly evaluate the quality of the paraphrase structure induced by our model by comparing against a gold standard clustering of keyphrases provided by expert annotators."}, {"heading": "6.1.1 EXPERIMENTAL SETUP", "text": "In this section, we describe the datasets and evaluation techniques used for experiments with our system and other automatic methods. We also comment on how hyperparameters are tuned for our model, and how sampling is initialized.\nData Sets We evaluate our system on reviews from three domains: restaurants, cell phones, and digital cameras. These reviews were downloaded from the Epinions website; we used user-authored pros and cons associated with reviews as keyphrases (see Section 3). Statistics for the datasets are provided in Table 3. For each of the domains, we selected 50% of the documents for training.\nWe consider two strategies for constructing test data. First, we consider evaluating the semantic properties inferred by our system against expert annotations of the semantic properties present in each document. To this end, we use the expert annotations originally described in Section 3 as a test\nset;6 to reiterate, these were annotations of 170 reviews in the restaurant domain, of which we now hold out 50 as a development set. The review texts were annotated with six properties according to standardized guidelines. This strategy enforces consistency and completeness in the ground truth annotations, differentiating them from free-text annotations.\nUnfortunately, our ability to evaluate against expert annotations is limited by the cost of producing such annotations. To expand evaluation to other domains, we use the author-written keyphrase annotations that are present in the original reviews. Such annotations are noisy \u2014 while the presence of a property annotation on a document is strong evidence that the document supports the property, the inverse is not necessarily true. That is, the lack of an annotation does not necessarily imply that its respective property does not hold \u2014 e.g., a review with no good service-related keyphrase may still praise the service in the body of the document.\nFor experiments using free-text annotations, we overcome this pitfall by restricting the evaluation of predictions of individual properties to only those documents that are annotated with that property or its antonym. For instance, when evaluating the prediction of the good service property, we will only select documents which are either annotated with good service or bad service-related keyphrases.7 For this reason, each semantic property is evaluated against a unique subset of documents. The details of these development and test sets are presented in Appendix A.\nTo ensure that free-text annotations can be reliably used for evaluation, we compare with the results produced on expert annotations whenever possible. As shown in Section 6.1.2, the free-text evaluations produce results that cohere well with those obtained on expert annotations, suggesting that such labels can be used as a reasonable proxy for expert annotation evaluations.\nEvaluation Methods Our first evaluation leverages the expert annotations described in Section 3. One complication is that expert annotations are marked on the level of semantic properties, while the model makes predictions about the appropriateness of individual keyphrases. We address this by representing each expert annotation with the most commonly-observed keyphrase from the manually-annotated cluster of keyphrases associated with the semantic property. For example, an annotation of the semantic property good food is represented with its most common keyphrase realization, \u201cgreat food.\u201d Our evaluation then checks whether this keyphrase is within any of the clusters of keyphrases predicted by the model.\nThe evaluation against author free-text annotations is similar to the evaluation against expert annotations. In this case, the annotation takes the form of individual keyphrases rather than semantic properties. As noted, author-generated keyphrases suffer from inconsistency. We obtain a consistent evaluation by mapping the author-generated keyphrase to a cluster of keyphrases as a determined by the expert annotator, and then again selecting the most common keyphrase realization of the cluster. For example, the author may use the keyphrase \u201ctasty,\u201d which maps to the semantic cluster good food; we then select the most common keyphrase realization, \u201cgreat food.\u201d As in the expert evaluation, we check whether this keyphrase is within any of the clusters predicted by the model.\nModel performance is quantified using recall, precision, and F-score. These are computed in the standard manner, based on the model\u2019s representative keyphrase predictions compared against the corresponding references. Approximate randomization (Yeh, 2000; Noreen, 1989) is used for statistical significance testing. This test repeatedly performs random swaps of individual results\n6. The expert annotations are available at http://groups.csail.mit.edu/rbg/code/precis/. 7. This determination is made by mapping author keyphrases to properties using an expert-generated gold standard\nclustering of keyphrases. It is much cheaper to produce an expert clustering of keyphrases than to obtain expert annotations of the semantic properties in every document.\nfrom each candidate system, and checks whether the resulting performance gap remains at least as large. We use this test because it is valid for comparing nonlinear functions of random variables, such as F-scores, unlike other common methods such as the sign test. Previous work that used this test include evaluations at the Message Understanding Conference (Chinchor, Lewis, & Hirschman, 1993; Chinchor, 1995); more recently, Riezler and Maxwell (2005) advocated for its use in evaluating machine translation systems.\nParameter Tuning and Initialization To improve the model\u2019s convergence rate, we perform two initialization steps for the Gibbs sampler. First, sampling is done only on the keyphrase clustering component of the model, ignoring document text. Second, we fix this clustering and sample the remaining model parameters. These two steps are run for 5,000 iterations each. The full joint model is then sampled for 100,000 iterations. Inspection of the parameter estimates confirms model convergence. On a 2GHz dual-core desktop machine, a multithreaded C++ implementation of model training takes about two hours for each dataset.\nOur model needs to be provided with the number of clusters K.8 We set K large enough for the model to learn effectively on the development set. For the restaurant data we set K to 20. For cell phones and digital cameras,K was set to 30 and 40, respectively. These values were tuned using the development set. However, we found that as long as K was large enough to accommodate a significant number of keyphrase clusters, and a few additional to account for topics with no keyphrases, the specific value of K does not affect the model\u2019s performance. All other hyperparameters were adjusted based on development set performance, though tuning was not extensive.\nAs previously mentioned, we obtain document properties by examining the probability mass of the topic distribution assigned to each property. A probability threshold is set for each property via the development set, optimizing for maximum F-score."}, {"heading": "6.1.2 RESULTS", "text": "In this section, we report the performance of our model, comparing it with an array of increasingly sophisticated baselines and model variations. We first demonstrate that learning a clustering of annotation keyphrases is crucial for accurate semantic prediction. Next, we investigate the impact of paraphrasing quality on model accuracy by considering the expert-generated gold standard clustering of keyphrases as another comparison point; we also consider alternative automatically computed sources of paraphrase information.\nFor ease of comparison, the results of all the experiments are shown in Table 5 and Table 6, with a summary of the baselines and model variations in Table 4.\nComparison against Simple Baselines Our first evaluation compares our model to four na\u0131\u0308ve baselines. All four treat keyphrases as independent, ignoring their latent paraphrase structure.\n\u2022 Random: Each keyphrase is supported by a document with probability of one half. The results of this baseline are computed in expectation, rather than actually run. This baseline is expected to have a recall of 0.5, because in expectation it will select half of the correct keyphrases. Its precision is the average proportion of annotations in the test set against the number of possible annotations. That is, in a test set of size n with m properties, if property\n8. This requirement could conceivably be removed by modeling the cluster indices as being drawn from a Dirichlet process prior.\ni appears ni times, then expected precision is \u2211m i=1 ni mn . For instance, for the restaurants gold standard evaluation, the six tested properties appeared a total of 249 times over 120 documents, yielding an expected precision of 0.346.\n\u2022 Keyphrase in text: A keyphrase is supported by a document if it appears verbatim in the text. Precision should be high while recall will be low, because the model is unable to detect paraphrases of the keyphrase in the text. For instance, for the first review from Figure 1, \u201ccleanliness\u201d would be supported because it appears in the text; however, \u201chealthy\u201d would not be supported, even though the synonymous \u201cgreat nutrition\u201d does appear.\n\u2022 Keyphrase classifier:9 A separate discriminative classifier is trained for each keyphrase. Positive examples are documents that are labeled by the author with the keyphrase; all other documents are considered to be negative examples. Consequently, for any particular keyphrase, documents labeled with synonymous keyphrases would be among the negative examples. A keyphrase is supported by a document if that keyphrase\u2019s classifier returns a positive prediction.\nWe use support vector machines, built using SVMlight (Joachims, 1999) with the same features as our model, i.e.,word counts.10 To partially circumvent the imbalanced positive/negative data problem, we tuned prediction thresholds on a development set to maximize F-score, in the same manner that we tuned thresholds for our model.\n\u2022 Heuristic keyphrase classifier: This baseline is similar to keyphrase classifier above, but attempts to mitigate some of the noise inherent in the training data. Specifically, any given positive example document may contain text unrelated to the given keyphrase. We attempt to reduce this noise by removing from the positive examples all sentences that have no word overlap with the given keyphrase. A keyphrase is supported by a document if that keyphrase\u2019s classifier returns a positive prediction.11\nLines 2-5 of Tables 5 and 6 present these results, using both gold annotations and the original authors\u2019 annotations for testing. Our model outperforms these three baselines in all evaluations with strong statistical significance.\nThe keyphrase in text baseline fares poorly: its F-score is below the random baseline in three of the four evaluations. As expected, the recall of this baseline is usually low because it requires keyphrases to appear verbatim in the text. The precision is somewhat better, but the presence of a significant number of false positives indicates that the presence of a keyphrase in the text is not necessarily a reliable indicator of the associated semantic property.\nInterestingly, one domain in which keyphrase in text does perform well is digital cameras. We believe that this is because of the prevalence of specific technical terms in the keyphrases used in this domain, such as \u201czoom\u201d and \u201cbattery life.\u201d Such technical terms are also frequently used in the review text, making the recall of keyphrase in text substantially higher in this domain than in the other evaluations.\n9. Note that the classifier results reported in the initial publication (Branavan, Chen, Eisenstein, & Barzilay, 2008) were obtained using the default parameters of a maximum entropy classifier. Tuning the classifier\u2019s parameters allowed us to significantly improve performance of all classifier baselines. 10. In general, SVMs have the additional advantage of being able to incorporate arbitrary features, but for the sake of comparison we restrict ourselves to using the same features across all methods. 11. We thank a reviewer for suggesting this baseline.\nThe keyphrase classifier baseline outperforms the random and keyphrase in text baselines, but still achieves consistently lower performance than our model in all four evaluations. Notably, the performance of heuristic keyphrase classifier is worse than keyphrase classifier except in one case. This alludes to the difficulty of removing the noise inherent in the document text.\nOverall, these results indicate that methods which learn and predict keyphrases without accounting for their intrinsic hidden structure are insufficient for optimal property prediction. This leads us toward extending the present baselines with clustering information.\nIt is important to assess the consistency of the evaluation based on free-text annotations (Table 6) with the evaluation that uses expert annotations (Table 5). While the absolute scores on the expert annotations dataset are lower than the scores with free-text annotations, the ordering of performance between the various automatic methods is the same across the two evaluation scenarios. This consistency is maintained in the rest of our experiments as well, indicating that for the purpose of relative comparison between the different automatic methods, our method of evaluating with free-text annotations is a reasonable proxy for evaluation on expert-generated annotations.\nComparison against Clustering-based Approaches The previous section demonstrates that our model outperforms baselines that do not account for the paraphrase structure of keyphrases. We now ask whether it is possible to enhance the baselines\u2019 performance by augmenting them with the keyphrase clustering induced by our model. Specifically, we introduce three more systems, none of which are \u201ctrue\u201d baselines, since they all use information inferred by our model.\n\u2022 Model cluster in text: A keyphrase is supported by a document if it or any of its paraphrases appears in the text. Paraphrasing is based on our model\u2019s clustering of the keyphrases. The use of paraphrasing information enhances recall at the potential cost of precision, depending on the quality of the clustering. For example, assuming \u201chealthy\u201d and \u201cgreat nutrition\u201d are clustered together, the presence of \u201chealthy\u201d in the text would also indicate support for \u201cgreat nutrition,\u201d and vice versa.\n\u2022 Model cluster classifier: A separate discriminative classifier is trained for each cluster of keyphrases. Positive examples are documents that are labeled by the author with any keyphrase from the cluster; all other documents are negative examples. All keyphrases of a cluster are supported by a document if that cluster\u2019s classifier returns a positive prediction. Keyphrase clustering is based on our model. As with keyphrase classifier, we use support vector machines trained on word count features, and we tune the prediction thresholds for each individual cluster on a development set.\nAnother perspective on model cluster classifier is that it augments the simplistic text modeling portion of our model with a discriminative classifier. Discriminative training is often considered to be more powerful than equivalent generative approaches (McCallum et al., 2005), leading us to expect a high level of performance from this system.\n\u2022 Heuristic model cluster classifier: This method is similar to model cluster classifier above, but with additional heuristics used to reduce the noise inherent in the training data. Positive example documents may contain text unrelated to the given cluster. To reduce this noise, sentences that have no word overlap with any of the cluster\u2019s keyphrases are removed. All keyphrases of a cluster are supported by a document if that cluster\u2019s classifier returns a positive prediction. Keyphrase clustering is based on our model.\nLines 6-8 of Tables 5 and 6 present results for these methods. As expected, using a clustering of keyphrases with the baseline methods substantially improves their recall, with low impact on precision. Model cluster in text invariably outperforms keyphrase in text \u2014 the recall of keyphrase in text is improved by the addition of clustering information, though precision is worse in some cases. This phenomenon holds even in the cameras domain, where keyphrase in text already performs well. However, our model still significantly outperforms model cluster in text in all evaluations.\nAdding clustering information to the classifier baseline results in performance that is sometimes better than our model\u2019s. This result is not surprising, because model cluster classifier gains the benefit of our model\u2019s robust clustering while learning a more sophisticated classifier for assigning properties to texts. The resulting combined system is more complex than our model by itself, but has the potential to yield better performance. On the other hand, using a simple heuristic to reduce the noise present in the training data consistently hurts the performance of the classifier, possibly due to the reduction in the amount of training data.\nOverall, the enhanced performance of these methods, in contrast to the keyphrase baselines, is aligned with previous observations in entailment research (Dagan, Glickman, & Magnini, 2006), confirming that paraphrasing information contributes greatly to improved performance in semantic inference tasks.\nThe Impact of Paraphrasing Quality The previous section demonstrates one of the central claims of this paper: accounting for paraphrase structure yields substantial improvements in semantic inference when using noisy keyphrase annotations. A second key aspect of our research is the idea that clustering quality benefits from tying the clusters to hidden topics in the document text. We evaluate this claim by comparing our model\u2019s clustering against an independent clustering baseline. We also compare against a \u201cgold standard\u201d clustering produced by expert human annotators. To test the impact of these clustering methods, we substitute the model\u2019s inferred clustering with each alternative and examine how the resulting semantic inferences change. This comparison is performed for the semantic inference mechanism of our model, as well as for the model cluster in text, model cluster classifier and heuristic model cluster classifier baselines.\nTo add a \u201cgold standard\u201d clustering to our model, we replace the hidden variables that correspond to keyphrase clusters with observed values that are set according to the gold standard clustering.12 The only parameters that are trained are those for modeling text. This model variation, gold cluster model, predicts properties using the same inference mechanism as the original model. The baseline variations gold cluster in text, gold cluster classifier and heuristic gold cluster classifier are likewise derived by substituting the automatically computed clustering with gold standard clusters.\nAn additional clustering is obtained using only the keyphrase similarity information. Specifically, we modify our original model so that it learns the keyphrase clustering in isolation from the text, and only then learns the property language models. In this framework, the keyphrase clustering is entirely independent of the review text, because the text modeling is learned with the keyphrase clustering fixed. We refer to this modification of the model as independent cluster model. Because our model treats the document text as a mixture of latent topics, this is reminiscent of models such as supervised latent Dirichlet allocation (sLDA; Blei & McAuliffe, 2008), with the labels acquired by performing a clustering across keyphrases as a preprocessing step. As in the previous experiment, we introduce three new baseline variations \u2014 independent cluster in text, independent cluster classifier and heuristic independent cluster classifier.\n12. The gold standard clustering was created as part of the evaluation procedure described in Section 6.1.1.\nLines 9-16 of Tables 5 and 6 present the results of these experiments. The gold cluster model produces F-scores comparable to our original model, providing strong evidence that the clustering induced by our model is of sufficient quality for semantic inference. The application of the expertgenerated clustering to the baselines (lines 10, 11 and 12) yields less consistent results, but overall this evaluation provides little reason to believe that performance would be substantially improved by obtaining a clustering that was closer to the gold standard.\nThe independent cluster model consistently reduces performance with respect to the full joint model, supporting our hypothesis that joint learning gives rise to better prediction. The independent clustering baselines, independent cluster in text, independent cluster classifier and heuristic independent cluster classifier (lines 14 to 16), are also worse than their counterparts that use the model clustering (lines 6 to 8). This observation leads us to conclude that while the expert-annotated clustering does not always improve results, the independent clustering always degrades them. This supports our view that joint learning of clustering and text models is an important prerequisite for better property prediction.\nAnother way of assessing the quality of each automatically-obtained keyphrase clustering is to quantify its similarity to the clustering produced by the expert annotators. For this purpose we use the Rand Index (Rand, 1971), a measure of cluster similarity. This measure varies from zero to one, with higher scores indicating greater similarity. Table 7 shows the Rand Index scores for our model\u2019s full joint clustering, as well as the clustering obtained from independent cluster model. In every domain, joint inference produces an overall clustering that improves upon the keyphrasesimilarity-only approach. These scores again confirm that joint inference across keyphrases and document text produces a better clustering than considering features of the keyphrases alone."}, {"heading": "6.2 Summarizing Multiple Reviews", "text": "Our last experiment examines the multi-document summarization capability of our system. We study our model\u2019s ability to aggregate properties across a set of reviews, compared to baselines that aggregate by directly using the free-text annotations."}, {"heading": "6.2.1 DATA AND EVALUATION", "text": "We selected 50 restaurants, with five user-written reviews for each restaurant. Ten annotators were asked to annotate the reviews for five restaurants each, comprising 25 reviews per annotator. They used the same six salient properties and the same annotation guidelines as in the previous restaurant annotation experiment (see Section 3). In constructing the ground truth, we label properties that are supported in at least three of the five reviews.\nWe make property predictions on the same set of reviews with our model and the baselines presented below. For the automatic methods, we register a prediction if the system judges the property to be supported on at least two of the five reviews.13 The recall, precision, and F-score are computed over these aggregate predictions, against the six salient properties marked by annotators."}, {"heading": "6.2.2 AGGREGATION APPROACHES", "text": "In this evaluation, we run the trained version of our model as described in Section 6.1.1. Note that keyphrases are not provided to our model, though they are provided to the baselines.\nThe most obvious baseline for summarizing multiple reviews would be to directly aggregate their free-text keyphrases. These annotations are presumably representative of the review\u2019s semantic properties, and unlike the review text, keyphrases can be matched directly with each other. Our first baseline applies this notion directly:\n\u2022 Keyphrase aggregation: A keyphrase is supported for a restaurant if at least two out of its five reviews are annotated verbatim with that keyphrase.\nThis simple aggregation approach has the obvious downside of requiring very strict matching between independently authored reviews. For that reason, we consider extensions to this aggregation approach that allow for annotation paraphrasing:\n\u2022 Model cluster aggregation: A keyphrase is supported for a restaurant if at least two out of its five reviews are annotated with that keyphrase or one of its paraphrases. Paraphrasing is according to our model\u2019s inferred clustering.\n\u2022 Gold cluster aggregation: Same as model cluster aggregation, but using the expert-generated clustering for paraphrasing.\n\u2022 Independent cluster aggregation: Same as model cluster aggregation, but using the clustering learned only from keyphrase similarity for paraphrasing.\n13. When three corroborating reviews are required, the baseline systems produce very few positive predictions, leading to poor recall. Results for this setting are presented in Appendix B."}, {"heading": "6.2.3 RESULTS", "text": "Table 8 compares the baselines against our model. Our model outperforms all of the annotationbased baselines, despite not having access to the keyphrase annotations. Notably, keyphrase aggregation performs very poorly, because it makes very few predictions, as a result of its requirement of exact keyphrase string match. As before, the inclusion of keyphrase clusters improves the performance of the baseline models. However, the incompleteness of the keyphrase annotations (see Section 3) explains why the recall scores are still low compared to our model. By incorporating document text, our model obtains dramatically improved recall, at the cost of reduced precision, ultimately yielding a significantly improved F-score.\nThese results demonstrate that review summarization benefits greatly from our joint model of the review text and keyphrases. Na\u0131\u0308ve approaches that consider only keyphrases yield inferior results, even when augmented with paraphrase information."}, {"heading": "7. Conclusions and Future Work", "text": "In this paper, we have shown how free-text keyphrase annotations provided by novice users can be leveraged as a training set for document-level semantic inference. Free-text annotations have the potential to vastly expand the set of training data available to developers of semantic inference systems; however, as we have shown, they suffer from lack of consistency and completeness. We overcome these problems by inducing a hidden structure of semantic properties, which correspond both to clusters of keyphrases and hidden topics in the text. Our approach takes the form of a hierarchical Bayesian model, which addresses both the text and keyphrases jointly.\nOur model is implemented in a system that successfully extracts semantic properties of unannotated restaurant, cell phone, and camera reviews, empirically validating our approach. Our experiments demonstrate the necessity of handling the paraphrase structure of free-text keyphrase annotations; moreover, they show that a better paraphrase structure is learned in a joint framework that also models the document text. Our approach outperforms competitive baselines for semantic property extraction from both single and multiple documents. It also permits aggregation across multiple keyphrases with different surface forms for multi-document summarization.\nThis work extends an actively growing literature on document topic modeling. Both topic modeling and paraphrasing posit a hidden layer that captures the relationship between disparate surface forms: in topic modeling, there is a set of latent distributions over lexical items, while paraphrasing is represented by a latent clustering over phrases. We show these two latent structures can be linked, resulting in increased robustness and semantic coherence.\nWe see several avenues of future work. First, our model draws substantial power from features that measure keyphrase similarity. This ability to use arbitrary similarity metrics is desirable; however, representing individual similarity scores as random variables is a compromise, as they are clearly not independent. We believe that this problem could be avoided by modeling the generation of the entire similarity matrix jointly.\nA related approach would be to treat the similarity matrix across keyphrases as an indicator of covariance structure. In such a model, we would learn separate language models for each keyphrase, but keyphrases that are rated as highly similar would be constrained to induce similar language models. Such an approach might be possible in a Gaussian process framework (Rasmussen & Williams, 2006).\nCurrently the focus of our model is to identify the semantic properties expressed in a given document, which allows us to produce a summary of those properties. However, as mentioned in Section 3, human authors do not give equal importance to all properties when producing a summary of pros and cons. One possible extension of this work would be to explicitly model the likelihood of each topic being annotated in a document. We might then avoid the current post-processing step that uses property-specific thresholds to compute final predictions from the model output.\nFinally, we have assumed that the semantic properties themselves are unstructured. In reality, properties are related in interesting ways. Trivially, in the domain of reviews it would be desirable to model antonyms explicitly, e.g., no restaurant review should be simultaneously labeled as having good and bad food. Other relationships between properties, such as hierarchical structures, could also be considered. This suggests possible connections to the correlated topic model of Blei and Lafferty (2006).\nBibliographic Note\nPortions of this work were previously presented in a conference publication (Branavan et al., 2008). The current article extends this work in several ways, most notably: the development and evaluation of a multi-document review summarization system that uses semantic properties induced by our method (Section 6.2); a detailed analysis of the distributional properties of free-text annotations (Section 3); and an expansion of the evaluation to include an additional domain and sets of baselines not considered in the original paper (Section 6.1.1)."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of National Science Foundation (NSF) CAREER grant IIS0448168, the Microsoft Research New Faculty Fellowship, the U.S. Office of Naval Research (ONR), Quanta Computer, and Nokia Corporation. Harr Chen is supported by the National Defense Science and Engineering and NSF Graduate Fellowships. Thanks to Michael Collins, Zoran Dzunic, Amir Globerson, Aria Haghighi, Dina Katabi, Kristian Kersting, Terry Koo, Yoong Keok Lee, Brian Milch, Tahira Naseem, Dan Roy, Christina Sauper, Benjamin Snyder, Luke Zettlemoyer, and the journal reviewers for helpful comments and suggestions. We also thank Marcia Davidson and members of the NLP group at MIT for help with expert annotations. Any opinions, findings, conclusions or recommendations expressed in this article are those of the authors, and do not necessarily reflect the views of NSF, Microsoft, ONR, Quanta, or Nokia."}, {"heading": "Appendix A. Development and Test Set Statistics", "text": "Table 9 lists the semantic properties for each domain and the number of documents that are used for evaluating each of these properties. As noted in Section 6.1.1, the gold standard evaluation is complete, testing every property with each document. Conversely, the free-text evaluations for each property only use documents that are annotated with the property or its antonym \u2014 this is why the number of documents differs for each semantic property."}, {"heading": "Appendix B. Additional Multiple Review Summarization Results", "text": "Table 10 lists results of the multi-document experiment, with a variation on the aggregation \u2014 we require each automatic method to predict a property for three of five reviews to predict that property for the product, rather than two as presented in Section 6.2. For the baseline systems, this change causes a precipitous drop in recall, leading to F-score results that are substantially worse than those presented in Section 6.2.3. In contrast, the F-score for our model is consistent across both evaluations."}, {"heading": "Appendix C. Hyperparameter Settings", "text": "Table 11 lists the values of hyperparameters \u03b80, \u03c80, and \u03c60 used in all experiments for each domain. These values were arrived at through tuning on the development set. In all cases, \u03bb0 was set to (1, 1), making Beta(\u03bb0) the uniform distribution."}], "references": [{"title": "Information fusion in the context of multidocument summarization", "author": ["R. Barzilay", "K. McKeown", "M. Elhadad"], "venue": "In Proceedings of ACL,", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["R. Barzilay", "K.R. McKeown"], "venue": "In Proceedings of ACL,", "citeRegEx": "Barzilay and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Barzilay and McKeown", "year": 2001}, {"title": "A latent Dirichlet model for unsupervised entity resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In Proceedings of the SIAM International Conference on Data Mining", "citeRegEx": "Bhattacharya and Getoor,? \\Q2006\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2006}, {"title": "Correlated Topic Models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "In Advances in NIPS,", "citeRegEx": "Blei and Lafferty,? \\Q2006\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2006}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J. McAuliffe"], "venue": "In Advances in NIPS,", "citeRegEx": "Blei and McAuliffe,? \\Q2008\\E", "shortCiteRegEx": "Blei and McAuliffe", "year": 2008}, {"title": "A topic model for word sense disambiguation", "author": ["J. Boyd-Graber", "D. Blei", "X. Zhu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2007}, {"title": "Learning document-level semantic properties from free-text annotations", "author": ["S.R.K. Branavan", "H. Chen", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of ACL,", "citeRegEx": "Branavan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2008}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "In Proceedings of ACM SIGIR,", "citeRegEx": "Carbonell and Goldstein,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein", "year": 1998}, {"title": "Statistical significance of MUC-6 results", "author": ["N. Chinchor"], "venue": "In Proceedings of the 6th Conference on Message Understanding,", "citeRegEx": "Chinchor,? \\Q1995\\E", "shortCiteRegEx": "Chinchor", "year": 1995}, {"title": "Evaluating message understanding systems: An analysis of the third message understanding conference (MUC-3)", "author": ["N. Chinchor", "D.D. Lewis", "L. Hirschman"], "venue": "Computational Linguistics,", "citeRegEx": "Chinchor et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Chinchor et al\\.", "year": 1993}, {"title": "A coefficient of agreement for nominal scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen,? \\Q1960\\E", "shortCiteRegEx": "Cohen", "year": 1960}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Towards generating patient specific summaries of medical articles", "author": ["N. Elhadad", "K.R. McKeown"], "venue": "In Proceedings of NAACL Workshop on Automatic Summarization,", "citeRegEx": "Elhadad and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Elhadad and McKeown", "year": 2001}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Bayesian Data Analysis (2nd edition). Texts in Statistical Science", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Contextual dependencies in unsupervised word segmentation", "author": ["S. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "In Proceedings of ACL,", "citeRegEx": "Goldwater et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Making Large-Scale Support Vector Machine Learning Practical, pp. 169\u2013184", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Automatic identification of pro and con reasons in online reviews", "author": ["Kim", "S.-M", "E. Hovy"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "Kim et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2006}, {"title": "Discovery of inference rules for question-answering", "author": ["D. Lin", "P. Pantel"], "venue": "Natural Language Engineering,", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Opinion observer: Analyzing and comparing opinions on the web", "author": ["B. Liu", "M. Hu", "J. Cheng"], "venue": "In Proceedings of WWW,", "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Opinion integration through semi-supervised topic modeling", "author": ["Y. Lu", "C. Zhai"], "venue": "In Proceedings of WWW,", "citeRegEx": "Lu and Zhai,? \\Q2008\\E", "shortCiteRegEx": "Lu and Zhai", "year": 2008}, {"title": "Multi-document summarization by graph search and matching", "author": ["I. Mani", "E. Bloedorn"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Mani and Bloedorn,? \\Q1997\\E", "shortCiteRegEx": "Mani and Bloedorn", "year": 1997}, {"title": "Explorations in sentence fusion", "author": ["E. Marsi", "E. Krahmer"], "venue": "In Proceedings of the European Workshop on Natural Language Generation,", "citeRegEx": "Marsi and Krahmer,? \\Q2005\\E", "shortCiteRegEx": "Marsi and Krahmer", "year": 2005}, {"title": "A conditional random field for discriminativelytrained finite-state string edit distance", "author": ["A. McCallum", "K. Bellare", "F. Pereira"], "venue": "In Proceedings of UAI,", "citeRegEx": "McCallum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2005}, {"title": "A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization", "author": ["A. Nenkova", "L. Vanderwende", "K. McKeown"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Nenkova et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2006}, {"title": "Computer-Intensive Methods for Testing Hypotheses: An Introduction", "author": ["E. Noreen"], "venue": null, "citeRegEx": "Noreen,? \\Q1989\\E", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "OPINE: Extracting product features and opinions from reviews", "author": ["Popescu", "A.-M", "B. Nguyen", "O. Etzioni"], "venue": "In Proceedings of HLT/EMNLP,", "citeRegEx": "Popescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2005}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["M. Purver", "K.P. K\u00f6rding", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation and user studies", "author": ["D. Radev", "H. Jing", "M. Budzikowska"], "venue": "In Proceedings of ANLP/NAACL Summarization Workshop", "citeRegEx": "Radev et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2000}, {"title": "Generating natural language summaries from multiple on-line sources", "author": ["D. Radev", "K. McKeown"], "venue": "Computational Linguistics,", "citeRegEx": "Radev and McKeown,? \\Q1998\\E", "shortCiteRegEx": "Radev and McKeown", "year": 1998}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand,? \\Q1971\\E", "shortCiteRegEx": "Rand", "year": 1971}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "On some pitfalls in automatic evaluation and significance testing for MT", "author": ["S. Riezler", "J.T. Maxwell"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,", "citeRegEx": "Riezler and Maxwell,? \\Q2005\\E", "shortCiteRegEx": "Riezler and Maxwell", "year": 2005}, {"title": "Multiple aspect ranking using the good grief algorithm", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of NAACL/HLT,", "citeRegEx": "Snyder and Barzilay,? \\Q2007\\E", "shortCiteRegEx": "Snyder and Barzilay", "year": 2007}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of ACL,", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of WWW,", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Advances in NIPS,", "citeRegEx": "Toutanova and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2008}, {"title": "Multi-document summarization via information extraction", "author": ["M. White", "T. Korelsky", "C. Cardie", "V. Ng", "D. Pierce", "K. Wagstaff"], "venue": "In Proceedings of HLT,", "citeRegEx": "White et al\\.,? \\Q2001\\E", "shortCiteRegEx": "White et al\\.", "year": 2001}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "In Proceedings of COLING,", "citeRegEx": "Yeh,? \\Q2000\\E", "shortCiteRegEx": "Yeh", "year": 2000}, {"title": "Mark-up barking up the wrong tree", "author": ["A. Zaenen"], "venue": "Computational Linguistics,", "citeRegEx": "Zaenen,? \\Q2006\\E", "shortCiteRegEx": "Zaenen", "year": 2006}], "referenceMentions": [{"referenceID": 40, "context": "Learning-based approaches have dramatically increased the scope and robustness of such semantic processing, but they are typically dependent on large expert-annotated datasets, which are costly to produce (Zaenen, 2006).", "startOffset": 205, "endOffset": 219}, {"referenceID": 4, "context": "Combining topics induced by LDA with external supervision was first considered by Blei and McAuliffe (2008) in their supervised Latent Dirichlet Allocation (sLDA) model.", "startOffset": 82, "endOffset": 108}, {"referenceID": 16, "context": "For example, Hu and Liu (2004) employ association mining to identify noun phrases that express key portions of product reviews.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "Property extraction was further refined in OPINE (Popescu et al., 2005), another system for review analysis.", "startOffset": 49, "endOffset": 71}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al.", "startOffset": 53, "endOffset": 300}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al. (2005), opinion phrases are extracted via handcrafted rules.", "startOffset": 53, "endOffset": 393}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al. (2005), opinion phrases are extracted via handcrafted rules. An alternative approach is to learn the rules for feature extraction from annotated data. To this end, property identification can be modeled in a classification framework (Kim & Hovy, 2006). A classifier is trained using a corpus in which free-text pro and con keyphrases are specified by the review authors. These keyphrases are compared against sentences in the review text; sentences that exhibit high word overlap with previously identified phrases are marked as pros or cons according to the phrase polarity. The rest of the sentences are marked as negative examples. Clearly, the accuracy of the resulting classifier depends on the quality of the automatically induced annotations. Our analysis of free-text annotations in several domains shows that automatically mapping from even manually-extracted annotation keyphrases to a document text is a difficult task, due to variability in keyphrase surface realizations (see Section 3). As we argue in the rest of this paper, it is beneficial to explicitly address the difficulties inherent in free-text annotations. To this end, our work is distinguished in two significant ways from the property extraction methods described above. First, we are able to predict properties beyond those that appear verbatim in the text. Second, our approach also learns the semantic relationships between different keyphrases, allowing us to draw direct comparisons between reviews even when the semantic ideas are expressed using different surface forms. Working in the related domain of web opinion mining, Lu and Zhai (2008) describe a system that generates integrated opinion summaries, which incorporate expert-written articles (e.", "startOffset": 53, "endOffset": 2013}, {"referenceID": 35, "context": "The work closest in methodology to our approach is a review summarizer developed by Titov and McDonald (2008a). Their method summarizes a review by selecting a list of phrases that express writers\u2019 opinions in a set of predefined properties (e.", "startOffset": 84, "endOffset": 111}, {"referenceID": 0, "context": "In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova, Vanderwende, & McKeown, 2006).", "startOffset": 119, "endOffset": 215}, {"referenceID": 0, "context": "Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999; Marsi & Krahmer, 2005).", "startOffset": 85, "endOffset": 131}, {"referenceID": 0, "context": "In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters. Identification of repeated information is equally central in our approach \u2014 our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors.", "startOffset": 120, "endOffset": 936}, {"referenceID": 10, "context": "78 on this joint set, indicating high agreement (Cohen, 1960).", "startOffset": 48, "endOffset": 61}, {"referenceID": 34, "context": "The generation of numerical ratings is based on the algorithm described in Snyder and Barzilay (2007).", "startOffset": 75, "endOffset": 102}, {"referenceID": 39, "context": "Approximate randomization (Yeh, 2000; Noreen, 1989) is used for statistical significance testing.", "startOffset": 26, "endOffset": 51}, {"referenceID": 26, "context": "Approximate randomization (Yeh, 2000; Noreen, 1989) is used for statistical significance testing.", "startOffset": 26, "endOffset": 51}, {"referenceID": 8, "context": "Previous work that used this test include evaluations at the Message Understanding Conference (Chinchor, Lewis, & Hirschman, 1993; Chinchor, 1995); more recently, Riezler and Maxwell (2005) advocated for its use in evaluating machine translation systems.", "startOffset": 94, "endOffset": 146}, {"referenceID": 8, "context": "Previous work that used this test include evaluations at the Message Understanding Conference (Chinchor, Lewis, & Hirschman, 1993; Chinchor, 1995); more recently, Riezler and Maxwell (2005) advocated for its use in evaluating machine translation systems.", "startOffset": 95, "endOffset": 190}, {"referenceID": 17, "context": "We use support vector machines, built using SVMlight (Joachims, 1999) with the same features as our model, i.", "startOffset": 53, "endOffset": 69}, {"referenceID": 24, "context": "Discriminative training is often considered to be more powerful than equivalent generative approaches (McCallum et al., 2005), leading us to expect a high level of performance from this system.", "startOffset": 102, "endOffset": 125}, {"referenceID": 31, "context": "For this purpose we use the Rand Index (Rand, 1971), a measure of cluster similarity.", "startOffset": 39, "endOffset": 51}, {"referenceID": 3, "context": "This suggests possible connections to the correlated topic model of Blei and Lafferty (2006).", "startOffset": 68, "endOffset": 93}, {"referenceID": 6, "context": "Portions of this work were previously presented in a conference publication (Branavan et al., 2008).", "startOffset": 76, "endOffset": 99}], "year": 2009, "abstractText": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \u201ca real bargain\u201d or \u201cgood value.\u201d These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.", "creator": "TeX"}}}