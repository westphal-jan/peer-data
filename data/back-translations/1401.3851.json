{"id": "1401.3851", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Intrusion Detection using Continuous Time Bayesian Networks", "abstract": "Intrusion Detection Systems (IDSs) fall into two high-level categories: Network-based Systems (NIDS), which monitor network behavior, and Host-based Systems (HIDS), which monitor system calls. In this paper, we present a general technique for both systems. To model such systems efficiently, we use Continuous Time Bayesian Networks (CTBNs) and avoid setting a fixed update interval, which is common for discrete time models. We build generative models from normal training data, and anomalous behaviors are marked based on their probability under this standard. For NIDS, we construct a hierarchical CTBN model for network packet tracking and use Rao-blackwellized parameters to learn the parameters.", "histories": [["v1", "Thu, 16 Jan 2014 04:59:06 GMT  (466kb)", "http://arxiv.org/abs/1401.3851v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CR", "authors": ["jing xu", "christian r shelton"], "accepted": false, "id": "1401.3851"}, "pdf": {"name": "1401.3851.pdf", "metadata": {"source": "CRF", "title": "Intrusion Detection using Continuous Time Bayesian Networks", "authors": ["Jing Xu", "Christian R. Shelton"], "emails": ["JINGXU@CS.UCR.EDU", "CSHELTON@CS.UCR.EDU"], "sections": [{"heading": null, "text": "(NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls. In this work, we present a general technique for both systems. We use anomaly detection, which identifies patterns not conforming to a historic norm. In both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid specifying a fixed update interval common to discrete-time models. We build generative models from the normal training data, and abnormal behaviors are flagged based on their likelihood under this norm. For NIDS, we construct a hierarchical CTBN model for the network packet traces and use Rao-Blackwellized particle filtering to learn the parameters. We illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. We demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset."}, {"heading": "1. Introduction", "text": "Misuse or abuse of computer systems is a critical issue for system administrators. Our goal is to detect these attacks that attempt to compromise the performance quality of a particular host machine.\nIt is time-consuming and error-prone to acquire labeled data that contains both good and bad behaviors from which to build a classifier. Additionally, the frequency with which attacks are developed can make maintaining a database of all previously seen attacks inefficient or even infeasible. Anomaly detection can identify new attacks even if the attack type was unknown beforehand. Unsupervised learning allows the anomaly detector to adapt to changing environments, thereby extending its domain of usefulness. By modeling normal behavior from historic clean data, we can identify abnormal activity without a direct prior model of the attack by simply comparing its deviation from the learned norm.\nIn a network-based intrusion detection system (NIDS), the network packet traces are monitored. Network traffic traces collect information from a network\u2019s data stream and provide an external view of the network behavior. In a host-based intrusion detection system (HIDS), the internal state of a computing system is analyzed. System call logs are a convenient way of monitoring executing programs\u2019 behavior through their operating system calls.\nBoth systems are composed of activities that happen at dramatically different time granularity. Users alternate between busily using their computer and resting. During the busy period, a burst of action may cause a peak of network traffic flow or operating system usage. However, during the\nc\u00a92010 AI Access Foundation. All rights reserved.\nresting period, the computer just maintains its regular running pattern, and network or system activities are much less intense, e.g. automatically checking email every few minutes. Even within each of these global modes there are variations. Therefore, a dynamic model that requires discretizing the time is not efficient. We develop intrusion detection techniques using continuous time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002) for both data types. Although the two data are of completely different formats and semantic meaning, we demonstrate the flexibility of a continuous time generative model (such as a CTBN) to describe either.\nOur first effort is to detect anomalies from network traffic traces (NIDS). Abnormal traffic must differ in some way from the normal traffic patterns. While this difference may be very subtle and difficult to detect, the more subtle the attack, the longer the attack will take and the more it will stress the patience of the attacker. Looking at summarized information like flow statistics is not helpful, especially for stealthy worms which can mingle well with normal traffic by sacrificing their spreading speed and scale. We, therefore, feel that looking for abnormalities in the detailed network traffic flow level is a utile method for finding attacks. A network flow for a given host machine is a sequence of continuous-time asynchronous events. Furthermore, these events form a complex structured system, where statistical dependencies relate network activities like packet emissions and connections. We employ CTBNs to reason about these structured stochastic network processes.\nOur CTBN model contains a number of observed network events (packet emissions and concurrent port connections changes). To allow our model to be more descriptive, we also add latent variables that tie the activity variables together. Exact inference in this method is no longer feasible. Therefore, we use Rao-Blackwellized particle filtering (RBPF) to estimate the parameters.\nOur second effort is to detect intrusions using system call logs (HIDS). A system log file contains an ordered list of calls made to a computer\u2019s operating system by an executing program. We focus on analyzing the ordering and the context of the sequence, rather than simply counting the overall statistics. A CTBN is a natural way of modeling such sequential data. Because of the finite resolution of computer clock, all the system calls issued within a clock tick are assigned a same time stamp. Therefore the data stream consists of long periods of time with no activity, followed by sequences of calls in which the order is correctly recorded, but the exact timing information is lost. This poses a new challenge for CTBN reasoning. We present here a learning method for such type of data without resorting to time discretization.\nWe validate our NIDS technique on the MAWI dataset and the LBNL dataset, and our HIDS technique on the DARPA 1998 BSM dataset. Both applications give good results when compared with other method.\nIn Section 2 we discuss the related work in intrusion detection. In Section 3 we review continuoustimeMarkov processes and continuous time Bayesian networks. In Section 4 we describe the CTBN model and the RBPF inference algorithms for the NIDS problem. In Section 5 we describe the CTBN model and the parameter estimation algorithm for HIDS, including how to deal with imprecise timing measurements. In Section 6 we show our experimental results for both of the applications."}, {"heading": "2. Related Work", "text": "Much of the previous work in intrusion detection focuses on one area only \u2014 either detecting the network traffic or mining the system call logs. The work of Eskin, Arnold, Prerau, Portnoy, and Stolfo (2002) is similar to our approach in that they apply their method to both of these kinds of\ndata. They map data elements to a feature space and detect anomalies by determining which points lie in sparse regions using cluster-based estimation, K-nearest neighbors and one-class SVM. They use a data-dependent normalization feature map for network traffic data and a spectrum kernel for system call traces."}, {"heading": "2.1 NIDS", "text": "For network traffic data, we build upon our previous work (Xu & Shelton, 2008). There we made the assumption that network activities are independent across different ports. This allowed us to factorize the model into port-level submodels and standard exact inference techniques could be used for parameter learning. In this paper, we remove this restriction. There is no application-specific reason that traffic should be independent by ports. By tying the traffic together, our model describes more complicated structural dependencies among variables. We derive a Rao-Blackwellized particle filtering algorithm to estimate the parameters for our model. Our work also differs in that we are not only interested in the intrusion detection problem, but host identity recognition as well.\nAs a signature-based detection algorithm, we share many of the assumptions of Karagiannis, Papagiannaki, and Faloutsos (2005). In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution.\nMany learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context.\nLakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally. Rieck and Laskov (2007) model the language features like n-grams and words from connection payloads. Xu, Zhang, and Bhattacharyya (2005) also use unsupervised methods, but they concentrate on clustering traffic across a whole network. Similarly, Soule, Salamatian, and Taft (2005) build an anomaly detector based on Markov models, but it is for the network traffic patterns as a whole and does not function at the host level.\nThe work of Soule et al. (2004) is very similar in statistical flavor to our work. They also fit a distribution (in their case, a histogram modeled as a Dirichlet distribution) to network data. However, they model flow-level statistics, whereas we work at the level of individual connections. Additionally, they are attempting network-wide clustering of flows instead of anomaly detection.\nThe work of Moore and Zuev (2005), like our approach, models traffic with graphical models, in particular, Naive Bayes networks. But their goal is to categorize network traffic instead of detecting attacks. Kruegel, Mutz, Robertson, and Valeur (2003) present a Bayesian approach to the detecting problem as an event classification task while we only care about whether the host is under attack during an interval.\nThe work of Lazarevic, Ertoz, Kumar, Ozgur, and Srivastava (2003) is also similar to our work. It is one of the few papers to attempt to find attacks at the host level. They employ nearest neighbor, a Mahalanobis distance approach, and a density-based local outliers method, each using 23 features of the connections. Although their methods make the standard i.i.d. assumption about the data (and therefore miss the temporal context of the connection) and use 23 features (compared to our few features), we compare our results to theirs in Section 6, as the closest prior work. Agosta, Duik-Wasser, Chandrashekar, and Livadas (2007) present an adaptive detector whose threshold is time-varying. It is similar to our work in that they also rely on model-based algorithms. But they employ the host internal states like CPU loads which are not available to us.\nWhile there has been a great variety of previous work, our work is novel in that it detects anomalies at the host level using only the timing features of network activities. We do not consider each connection (or packet) in isolation, but rather in a complex context. We capture the statistical dynamic dependencies between packets and connections to find sequences of network traffic that are anomalous as a group."}, {"heading": "2.2 HIDS", "text": "Previous work on detecting intrusions in system call logs can be roughly grouped into two categories: sequence-based and feature-based. Sequence-based methods focus on the sequential order of the events while feature-based methods treat system calls as independent data elements. Our method belongs to the former category since we use a CTBN to model the dynamics of the sequences.\nTime-delay embedding (tide) and sequence time-delay embedding (stide) are two examples of sequence based methods (Forrest, A.Hofmeyr, Somayaji, & A.Longstaff, 1996; A.Hofmeyr, Forrest, & Somayaji, 1998). They generalize the data by building a database storing previously seen system call sub-sequences, and test by looking up subsequences in the database. These methods are straightforward and often achieve good results. We compare with them in our experiments. Tandon and Chan (2005) look at a richer set of attributes like return value and arguments associated with a system call while we only make use of the system call names.\nFeature based methods like those of Hu, Liao, and Vemuri (2003) use the same dataset we use, the DARPA 1998 BSM dataset, but their training data is noisy and they try to find a classification hyperplane using robust support vector machines (RSVMs) to separate normal system call profiles from intrusive ones. Eskin (2000) also works on noisy data. They make the assumption that their training data contains a large portion of normal elements and few anomalies. They present a mixture of distribution over normal and abnormal data and calculate the likelihood change if a data point is moved from normal part to abnormal part to get the optimum data partition.\nYeung and Ding (2002) try to use both techniques. They provide both dynamic and static behavioral models for system call data. For the dynamic method, a hidden Markov model (HMM) is used to model the normal system events and a likelihood is calculated for each testing sequence and compared against a certain threshold. Our work for the system call traces problem is very close\nto their framework since we also build a dynamic model for the sequential data and compute the likelihood of a testing example as a score. But we are different in that our CTBN models the continuous time dynamics rather than time-sliced behaviors. For the static method, they represent the normal behavior by a command occurrence frequency distribution and measure the distance from the testing example to this norm by cross entropy. The dataset they use is KDD archive dataset."}, {"heading": "2.3 Other Work", "text": "Simma et al. (2008) also use a continuous-time model to reason about network traffic. They apply their method to find dependences in exterprise-level services. Their model is non-Markovian, but also deals with network events as the basic observational unit.\nTo estimate the parameters of the large network we build for the network traffic data, we use Rao-Blackwellized particle filters (RBPFs). Doucet, de Freitas, Murphy, and Russel (2000) propose a RBPF algorithm for dynamic Bayesian networks that works in discrete time fashion by exploiting the structure of the DBN. Ng, Pfeffer, and Dearden (2005) extend the RBPF to continuous time dynamic systems and apply the method to the K-9 experimental Mars rover at NASA Ames Research Center. Their model is a hybrid system containing both discrete and continuous variables. They use particle filters for the discrete variables and unscented filters for the continuous variables. Our work are similar to theirs in that we apply a RBPF to a CTBN. But our model only contains discrete variables and our evidence is over continuous time (as opposed to only \u201csnapshots\u201d of the system state)."}, {"heading": "3. Continuous Time Bayesian Networks", "text": "We begin by briefly reviewing the definition of Markov processes and continuous time Bayesian networks (CTBNs)."}, {"heading": "3.1 Homogeneous Markov Process", "text": "A finite-state, continuous-time, homogeneous Markov process Xt is described by an initial distribution P 0X and, given a state space V al(X) = {x1, ..., xn}, an n\u00d7n matrix of transition intensities:\nQX =  \u2212qx1 qx1x2 . . . qx1xn qx2x1 \u2212qx2 . . . qx2xn ... ... . . .\n... qxnx1 qxnx2 . . . \u2212qxn  . qxixj is the intensity (or rate) of transition from state xi to state xj and qxi = \u2211 j 6=i qxixj .\nThe transient behavior of Xt can be described as follows. Variable X stays in state x for time exponentially distributed with parameter qx. The probability density function f for Xt remaining at x for duration t is fx(q, t) = qx exp(\u2212qxt) for t \u2265 0. The expected time to the next transition given the state is currently x is 1/qx. Upon transitioning, X shifts to state x\u2032 with probability \u03b8xx\u2032 = qxx\u2032/qx. Note that given qx, \u03b8xx\u2032 and qxx\u2032 are iosmorphic. We will sometime gives formulae in terms of \u03b8xx\u2032 where it simplifies the expression.\nThe distribution over the state of the process X at some future time t, Px(t), can be computed directly from QX. If P 0X is the distribution over X at time 0 (represented as a vector), then, letting\nexp be the matrix exponential, PX(t) = P 0X exp(QX \u00b7 t) ."}, {"heading": "3.2 Complete Data", "text": "Complete data for an HMP are represented by a set of trajectoriesD = {\u03c41, ...\u03c4n}. Each trajectory \u03c4i is a complete set of state transitions: d = {(xd, td, x\u2032d)}, meaning that X stayed in state xd for a duration of td, and then transitioned to state x\u2032d. Therefore we know the exact state of the variable X at any time 0 \u2264 t \u2264 T ."}, {"heading": "3.3 Sufficient Statistics and Likelihood", "text": "Given an HMP and its full dataD, the likelihood of a single state transition d = {(xd, td, x\u2032d)} \u2208 D is\nLX(q, \u03b8 : d) = (qxd exp(\u2212qxdtd))(\u03b8xdx\u2032d) . The likelihood function forD can be decomposed by transition:\nLX(q, \u03b8 : D) = ( \u220f d\u2208D LX(q : d))( \u220f d\u2208D LX(\u03b8 : d))\n= ( \u220f x qM [x]x exp(\u2212qxT [x]))( \u220f x \u220f x\u2032 6=x \u03b8 M [x,x\u2032] xx\u2032 ) .\nIf we take the log of the above function, we get the log likelihood:\nlX(q, \u03b8 : D) = lX(q : D) + lX(\u03b8 : D) = \u2211 x (M [x] ln(qx)\u2212 qxT [x] + \u2211 x\u2032 6=x M [x, x\u2032] ln(\u03b8xx\u2032)) .\nHere M [x, x\u2032] and T [x] are the sufficient statistics of the HMP model. M [x, x\u2032] is the number of timesX transitions from the state x to x\u2032. We denoteM [x] = \u2211 x\u2032 M [x, x\n\u2032], the total number of times the system leaves state x. T [x] is the total duration that X stays in the state x."}, {"heading": "3.4 Learning from Complete Data", "text": "To estimate the parameters of the transition intensity matrix Q, we maximize the above log likelihood function. This yields the maximum likelihood estimates:\nq\u0302x = M [x] T [x] , \u03b8\u0302xx\u2032 = M [x, x\u2032] M [x] ."}, {"heading": "3.5 Incomplete Data", "text": "Incomplete data from an HMP are composed partially observed trajectoriesD = {\u03c4\u22121 , ...\u03c4\u2212n }. Each trajectory \u03c4\u2212i consists of a set of d = {(Sd, td, dt)} observations, where Sd is a subsystem (a nonempty subset of the states of X) of the process. Each of the triplets specifies an \u201cinterval evidence.\u201d It states that the variableX is in the subsystem Sd from time td to time td+ dt. Some of the observations may be duration-free. i.e., we only observeX \u2208 Sd at time t, but do not know how long it stayed there. This is called a \u201cpoint evidence\u201d and can be generalized using the same triplet notation described above by setting the duration to be 0. For a partially observed trajectory, we only observe sequences of subsystems, and do not observe the state transitions within the subsystems."}, {"heading": "3.6 Expected Sufficient Statistics and Expected Likelihood", "text": "We can consider possible completions of a partially observed trajectory that specify the transitions that are consistent with the partial trajectory. By combining the partial trajectory and its completion, we get a full trajectory. We defineD+ = {\u03c4+1 , ..., \u03c4+n } to be completions of all the partial trajectories inD. Given a model, we have a distriubtion overD+, givenD.\nFor dataD+, the expected sufficient statistics with respect to the probability density over possible completions of the data are T\u0304 [x], M\u0304 [x, x\u2032] and M\u0304 [x]. The expected log likelihood is\nE[lX(q, \u03b8 : D+)] = E[lX(q : D+)] + E[lX(\u03b8 : D+)] = \u2211 x (M\u0304 [x] ln(qx)\u2212 qxT\u0304 [x] + \u2211 x\u2032 6=x M\u0304 [x, x\u2032] ln(\u03b8xx\u2032)) ."}, {"heading": "3.7 Learning from Incomplete Data", "text": "The expectation maximization (EM) algorithm can be used to find a local maximum of the likelihood from partial trajectory. The EM algorithm iterates over the following E step and M step until the convergence on the derived likelihood function.\nE step: Given the current HMP parameters, compute the expected sufficient statistics: T\u0304 [x], M\u0304 [x, x\u2032] and M\u0304 [x] for the data set D. This is the most complex part of the algorithm. We give further details below.\nM step: From the computed expected sufficient statistics, update the new model parameters for the next EM iteration:\nqx = M\u0304 [x] T\u0304 [x] , \u03b8xx\u2032 = M\u0304 [x, x\u2032] M\u0304 [x] .\nNow we show how to calculate the expected sufficient statistics using the forward-backward message passing method.\nA trajectory \u03c4 \u2208 D can be devided into N intervals where each of the interval is separated by adjacent event changes. Assume the trajectory spans the time interval [0, T ), and let \u03c4 [v, w] be the observed evidence between time v and w, including events on the time stamp v and w, and let \u03c4(v, w) be the same set of evidence but excluding v and w. Let S be the subsystem the states are restricted on this interval.\nWe define \u03b1t = P (Xt, \u03c4 [0, t]), \u03b2t = P (\u03c4 [t, T ] | Xt)\nto be vectors (indexed by possible assignments to Xt). Similarly, we define the corresponding distribution that excludes certain point evidence as follows.\n\u03b1\u2212t = P (Xt, \u03c4 [0, t)), \u03b2 + t = P (\u03c4(t, T ] | Xt) .\nDenote \u03b4j to be a vector of all 0\u2019s except for its j-th position being 1, and denote\u2206ij be a matrix of all 0\u2019s except that the element on i-th row and j-th column is 1.\nWe are now able to show the derived expected sufficient statistics. For time, E[T [x]] = \u222b T\n0 P (Xt | \u03c4 [0, T ])\u03b4x dt\n= 1\nP (\u03c4 [0, T ]) N\u22121\u2211 i=0 \u222b ti+1 ti P (Xt, \u03c4 [0, T ])\u03b4x dt .\nThe constant fraction at the beginning of the last line serves to make the total expected time over all j sum to \u03c4 . The integral on each interval can be further expressed as\u222b w\nv P (Xt, \u03c4 [0, T ])\u03b4x dt = \u222b w v \u03b1v exp(QS(t\u2212 v))\u2206xx exp(QS(w \u2212 t))\u03b2w dt ,\nwhere QS is the same as QX except all elements that correspond to transitions to or from S are set to 0.\nThe equation for expected transition counts can similarly be defined:\nE[M [x, x\u2032]] = qx,x\u2032 P (\u03c4 [0, T ]) [ N\u22121\u2211 i=1 \u03b1\u2212ti\u2206x,x\u2032\u03b2 + t+i\n+ N\u22121\u2211 i=0 \u222b ti+1 ti \u03b1ti exp(QS(t\u2212 ti))\u2206x,x\u2032 exp(QS(ti+1 \u2212 t))\u03b2ti+1 dt] .\nThe integrals appearing in E[T ] and E[M ] can be computed via a standard ODE solver, like the Runge-Kutta method (Press, Teukolsky, Vetterling, & Flannery, 1992). Such a method uses an adaptive step size to move quickly through times of few expected changes and more slowly through times of rapid transitions.\nNow the only remaining problem is to calculate \u03b1 and \u03b2. LetQSS\u2032 be the transitioning intensity matrix of the HMP from one subsystem S to another S\u2032. This matrix is the same as QX , but only elements corresponding to transitions from S to S\u2032 are non-zero.\n\u03b1\u2212ti = \u03b1ti\u22121 exp(QSi\u22121(ti \u2212 ti\u22121)) , \u03b1ti = \u03b1 \u2212 ti QSi\u22121Si , \u03b2\u2212ti = exp(QSi(ti+1 \u2212 ti))\u03b2ti+1 , \u03b2ti = QSi\u22121Si\u03b2 \u2212 ti .\nDuring this forward-backward calculation, it is also trivial to answer queries such as\nP (Xt = x | \u03c4 [0, T ]) = 1\nP (\u03c4) \u03b1\u2212t \u2206xx\u03b2t ."}, {"heading": "3.8 Continuous Time Bayesian Networks", "text": "While HMPs are good for modeling many dynamic systems, they have their limitations when the systems have multiple components because the state space grows exponentially in the number of variables. An HMP does not model the variable independencies and therefore it has to use a unified state X to represent the joint behavior of all the involving components in the system. In the this section, we show how a continuous time Bayesian network can be used to address this issue.\nNodelman et al. (2002) extend the theory of HMPs and present continuous time Bayesian networks (CTBNs), which model the joint dynamics of several local variables by allowing the transition model of each local variable X to be a Markov process whose parametrization depends on some subset of other variables U ."}, {"heading": "3.9 Definition", "text": "We first give an definition of an inhomogeneous Markov process called a conditional Markov process. It is a critical concept for us to formally introduce the CTBN framework.\nDefinition 1 (Nodelman, Shelton, & Koller, 2003) A conditional Markov process X is an inhomogeneous Markov process whose intensity matrix varies as a function of the current values of a set of discrete conditioning variables U . It is parametrized using a conditional intensity matrix (CIM) QX|U \u2013 a set of homogeneous intensity matrices QX|u, one for each instantiation of values u to U .\nWe call U the parents ofX . When the set of U is empty, the CIM is simply a standard intensity matrix.\nCIMs provide a way to model the temporal behavior of one variable conditioned on some other variables. By putting these local models together, we have a joint structured model \u2014 a continuous time Bayesian network.\nDefinition 2 (Nodelman et al., 2003) A continuous time Bayesian networkN over a set of stochastic processes X consists of two components: an initial distribution P 0X , specified as a Bayesian network B over a set of random variables X, and a continuous transition model, specified using a directed (possibly cyclic) graph G whose nodes areX \u2208 X; UX denotes the parents ofX in G. Each variable X \u2208 X is associated with a conditional intensity matrix, QX|UX .\nThe dynamics of a CTBN are quantitatively defined by a graph. The instantaneous evolution of a variable depends only on the current value of its parents in the graph. The quantitative description of a variable\u2019s dynamics is given by a set of intensity matrices, one for each value of its parents. That means the transition behavior of the variable is controlled by the current values of its parents.\nThe standard notion of d-separation from Bayesian networks carries over to CTBNs. Because graphs are cyclic and variables represent processes (not single random variables), the implications are a little different. A variable (process) is still independent of its non-descendants given its parents, and it is still independent of everything given its Markov blanket (any variable that is either a parent, a child, or a parent of a child). Cycles can cause parents to also be children, but provided they are considered as both, the above definitions still hold. More importantly, the notion of \u201cgiven\u201d works only if the full trajectory for the variable in question is known. Therefore, X and its grandchildren are not independent givenX\u2019s children\u2019s values at a single instant. Rather, they are only independent given X\u2019s children\u2019s full trajectories from time 0 until the last time of interest.\nIf we amalgamate all the variables in the CTBN together, we get a single homogeneous Markov process over the joint state space. In the joint state intensity matrix, a rate of 0 is assigned to any transition that involves changing more than one variable\u2019s value at the exact same time. All other intensities can be found by looking up the value in the corresponding conditional intensity matrix for the variable that changes. The diagonal elements are the negative row sums.\nForward sampling can be done quickly in a CTBN without generating the full joint intensity matrix. We keep track of the \u201cnext event time\u201d for each variable (sampled from the relevant exponential distribution given the current values of itself and its parent). We then select the earliest event time and change that variable (sampling from the multinomial distribution implied by the row of that variable\u2019s relevant intensity matrix). The next event time for the variable that just changed and all of its children must be resampled, but no other variable\u2019s time must be resampled due to the memoriless property of the exponential distribution. In this way a sequence of events (a trajectory) can be sampled."}, {"heading": "3.10 Learning", "text": "In the context of CTBNs, the model parameters consist of the CTBN structure G, the initial distribution P0 parameterized by a regular Bayesian network, and the conditional intensity matrices (CIMs) of each variable in the network. In this section, we assume the CTBN structure is known to us, so we only focus on the parameter learning. We also assume the model is irreducible. So the initial distribution P0 becomes less important in the context of CTBN inference and learning, especially when the time range becomes significantly large. Therefore, parameter learning in our context is to estimate the conditional intensity matrices QXi|Ui for each variable Xi, where Ui is the set of parent variables of Xi."}, {"heading": "3.10.1 LEARNING FROM COMPLETE DATA", "text": "Nodelman et al. (2003) presented an efficient way to learn a CTBN model from fully observed trajectories. With complete data, we know full instantiations to all the variables for the whole trajectory. So we know which CIM is governing the transition dynamics of each variable at any time. The sufficient statistics areM [x, x\u2032|u] \u2014 the number of times X transitions from the state x to x\u2032 given its parent instantiation u \u2014 and T [x|u] \u2014 the total duration that X stays in the state x given its parent instantiation u. We denoteM [x|u] = \u2211 x\u2032 M [x, x\n\u2032|u]. The likelihood function forD can be decomposed as\nLN (q, \u03b8 : D) = \u220f Xi\u2208X LXi(qXi|Ui : D)LXi(\u03b8Xi|Ui : D)) (1)\nwhere\nLX(qX|U : D) = \u220f u \u220f x q M [x|u] x|u exp(\u2212qx|uT [x|u]) (2)\nand\nLX(\u03b8 : D) = \u220f u \u220f x \u220f x\u2032 6=x \u03b8Mxx\u2032|u[x, x \u2032|u] . (3)\nIf we put the above functions together and take the log, we get the log likelihood component for a single variable X:\nlX(q, \u03b8 : D) = lX(q : D) + lX(\u03b8 : D) = \u2211 u \u2211 x M [x|u] ln(qx|u)\u2212 q[x|u]T [x|u]\n+ \u2211 u \u2211 x \u2211 x\u2032 6=x M [x, x\u2032|u] ln(\u03b8xx\u2032|u)). (4)\nBy maximizing the above log likelihood function, the model parameters can be estimated as\nq\u0302x|u = M [x|u] T [x|u] , \u03b8\u0302xx\u2032|u = M [x, x\u2032|u] M [x|u] . (5)"}, {"heading": "3.10.2 LEARNING FROM INCOMPLETE DATA", "text": "Nodelman, Shelton, and Koller (2005) present the expectation maximization (EM) algorithm to learn a CTBN model from partially observed trajectories D. The expected sufficient statistics are M\u0304 [x, x\u2032|u], the expected number of times that X transitions from state x to x\u2032 when its parent set U takes the values u, and T\u0304 [x|u], the expected amount of time thatX stays in the state x under the parent instantiation u. We denote M\u0304 [x|u] to be \u2211 x\u2032 M\u0304 [x, x\n\u2032|u]. The expected log likelihood can be decomposed in the same way as in Equation 4, except that the sufficient statisticsM [x, x\u2032|u], T [x|u] andM [x|u] are now replaced with expected sufficient statistics M\u0304 [x, x\u2032|u], T\u0304 [x|u] and M\u0304 [x|u].\nThe EM algorithm for a CTBN works essentially in the same way as for an HMP. The expectation step is to calculate the expected sufficient statistics using inference method (will be described in Section 3.11). The maximization step is to update the model parameters:\nq\u0302x|u = M\u0304 [x|u] T\u0304 [x|u] , \u03b8\u0302xx\u2032|u = M\u0304 [x, x\u2032|u] M\u0304 [x|u] ."}, {"heading": "3.11 Inference", "text": "Now given a CTBN model and some (partially) observed data, we would like to query the model. For example, we may wish to calculate the expected sufficient statistics for the above EM algorithm."}, {"heading": "3.11.1 EXACT INFERENCE", "text": "Nodelman et al. (2005) provide an exact inference algorithm using expectation maximization to reason and learn the parameters from partially observed data. This exact inference algorithm requires flattening all the variables into a single Markov process and performing inference as in an HMP. It has the problem that it makes the state space grow exponentially large. Therefore, the exact inference method is only feasible for problems with very small state spaces."}, {"heading": "3.11.2 APPROXIMATE INFERENCE", "text": "Because of the issue addressed below, much work has been done on CTBN approximate inference. Nodelman, Koller, and Shelton (2005) present an expectation propagation algorithm. Saria, Nodelman, and Koller (2007) give another message passing algorithm that adapts the time granularity. Cohn, El-Hay, Friedman, and Kupferman (2009) provide a mean field variational approach. El-Hay, Friedman, and Kupferman (2008) show a Gibbs sampling method approach using Monte Carlo expectation maximization. Fan and Shelton (2008) give another sampling based approach that uses importance sampling. El-Hay, Cohn, Friedman, and Kupferman (2010) describe a different expectation propagation approach.\nTo estimate the parameters of the models we build for the two applications (NIDS and HIDS), we employ inference algorithms including exact inference and a Rao-Blackwellized particle filtering (RBPF) algorithm, depending on the model size. Ng et al. (2005) extended RBPF to CTBNs. Their model was a hybrid system containing both discrete and continuous variable. They used particle filters for the discrete variables and unscented filters for the continuous variable. Our work are similar to this work in the method of applying RBPF to CTBNs, but our model contains only discrete variables and our evidence is over continuous intervals."}, {"heading": "3.12 CTBN Applications", "text": "Although inference and learning algorithms have been well developed for CTBNs, there have been only a few applications to real world problems. Nodelman and Horvitz (2003) used CTBNs to reason about users\u2019 presence and availability over time. Ng et al. (2005) used CTBNs to monitor a mobile robot. Nodelman et al. (2005) used CTBNs to model life event history. Fan and Shelton (2009) modeled social networks via CTBNs. Our previous work (Xu & Shelton, 2008) presented an NIDS for host machine using CTBNs, but did not include HIDS."}, {"heading": "4. Anomaly Detection Using Network Traffic", "text": "In this section, we present an algorithm to detect anomalies in network traffic data using CTBNs. We only focus on a single host on the network. The sequence and timing of events (e.g. packet transimission and connection establishment) are very important in network traffic flow. It matters not just how many connections were initiated in the past minute, but also their timing: if they were evenly spaced the trace is probably normal, but if they all came in a quick burst it is more suspicious. Similarly, the sequence is important. If the connections were made to sequentially increasing ports it is more likely to be a scanning virus, whereas the same set of ports in random order is more likely to be normal traffic. These are merely simple examples. We would like to detect more complex patterns.\nA typical machine in the network may have diverse activities with various service types (e.g. HTTP, SMTP). The destination port number roughly describes the type of service to which a particular network activity belongs. Some worms propagate malicious traffic toward certain well known ports to affect the quality of the associated services. By looking at traffic associated with different ports we are more sensitive to subtle variations that do not appear if we aggregate trace information across ports. Figure 1 shows the most popular ports ranked by their frequencies in the network traffic on the datasets we use (described in more depth later). These services are, to some extent, independent of each other. We therefore model each port\u2019s traffic with its own CTBN submodel. We denote \u03c4 as the whole observed traffic sequences on the particular host, and \u03c4j as the traffic associated with port j."}, {"heading": "4.1 A CTBN Model for Network Traffic", "text": "We use the same port-level submodel as our previous work (Xu & Shelton, 2008). We have a latent variable H and four fully observed toggle variables: Pin, Pout, Cinc, Cdec.\nThe nodes packet-in, Pin, and packet-out, Pout, represent the transmission of a packet to or from the host. They have no intrinsic state: the transmission of a packet is an essentially instantaneous event. Therefore they have events (or \u201ctransitions\u201d) without having state. This is modeled using a toggle variable in which an event is evidence of a change in the state of the variable and the rate of transition associated with each state is required to be the same.\nThe nodes connection-increase Cin and connection-decrease Cdec together describe the status of the number of concurrent connections C active on the host. Notice that C can only increase or decrease by one at any given event (the beginning or ending time of a connection). We assume that the arrival of a new connection and the termination of an existing connection are both independent of the number of other connections. Thus the intensity with which some connection starts (or stops) is same as any other connections. Therefore, these are also modeled as toggle variables.\nNodeH has 8 states that represent different abstract attributes about the machine\u2019s internal state. The toggle variables (Pin, Pout, Cinc and Cdec) are each allowed to change only for 2 of the states of H and they are required to have the same rate for both of these states. 2 hidden states per toggle variable was chosen as a balance between expressive power and model efficiency.\nIn previous work, we assumed that the traffic associated with different ports are independent of each other, so the port-level submodels are isolated. Here we remove this restriction by introducing another latent variable G that ties the port submodels together. The full model is shown in Figure 2."}, {"heading": "4.2 Parameter Learning Using RBPF", "text": "To calculate the expected sufficient statistics in the E-step of EM for parameter learning, the exact inference algorithm of Nodelman et al. (2002) flattens all the variables into a joint intensity matrix and reasons about the resulting homogeneous Markov process. The time complexity is exponential in the number of variables. For example, if there are 9 port models, the network contains 46 variables in total. Approximate inference techniques like the clique tree algorithm (Nodelman et al., 2002), message passing algorithms (Nodelman et al., 2005; Saria et al., 2007), importance sampling (Fan\n& Shelton, 2008) and Gibbs sampling (El-Hay et al., 2008) overcome this problem by sacrificing accuracy.\nWe notice that our model has a nice tree structure which makes Rao-Blackwellized particle filtering (RBPF) a perfect fit. RBPF uses a particle filter to sample a portion of the variables and analytically integrates out the rest. It decomposes the model structure efficiently and thus reduces the sampling space.\nIf we denote the N port-level hidden variables as H1, ...,HN , the posterior distribution of the whole model can be factorized as P (G,H1, ...,HN | \u03c4) = P (G | \u03c4) \u220fN i=1 P (Hi | G, \u03c4). Note that G and Hi are processes, so this probability is a density over complete trajectories. We use a particle filter to estimate G\u2019s conditional distribution P (G | \u03c4) as a set of sampled trajectories of G. It is difficult to sample directly from the posterior distribution, so we use an importance sampler to sample a particle from a proposal distribution and the particles are weighted by the ratio of its likelihood under the posterior distribution to the likelihood under the proposal distribution (Doucet et al., 2000). Since the variable G is latent and has no parents, we can use forward sampling to sample the particles from P (G) and the weight of each particle is simply the likelihood of \u03c4 conditioned on this trajectory for G (Fan & Shelton, 2008). Each port-level submodel is then dseparated from the rest of the network, given full trajectory of G (see Section 3.9 for d-separation in CTBNs). Since each is small (only 8 hidden states), they can be marginalized out exactly. That is, we can calculate P (\u03c4i | G) (where \u03c4i is the portion of the trajectory for submodel i) exactly, marginalizing out Hi with the \u03b1-\u03b2 recursions from Section 3.7.\nThe expected sufficient statistics (ESS) for any variable X in a CTBN are T\u0304X|U[x|u], the expected amount of time X stays at state x given its parent instantiation u, and M\u0304X|U[x, x\u2032|u], the expected number of transitions from state x to x\u2032 given X\u2019s parent instantiation u. Let gi \u223c P (G), i = 1, . . . ,M be the particles. We define their likelihood weights to be wi = P (gi|\u03c4) P (gi)\nand let W = \u2211 iwi be the sum of the weights. Then general importance sampling allows that an expected sufficient statistic can be estimated in the following way, where SS is any sufficient statistic:\nE(g,h1,...,hN )\u223cP (G,H1,...,HN |\u03c4)[SS(g, h1, . . . , hN )]\n= Eg\u223cP (G|\u03c4)Eh1,...,hN\u223cP (H1,...,HN |g,\u03c4)[SS(g, h1, . . . , hN )] \u2248 1 W \u2211 i wiEh1,...,hN\u223cP (H1,...,HN |gi,\u03c4)[SS(g i, h1, . . . , hN )] .\nThe expected sufficient statistics of the whole model are in two categories: those that depend only on g, ESS(g), and those that depend on a port model k, ESS(g, hk, \u03c4k). ESS(g) is simply the summation of counts (the amount of time G stays at some state, or the number of times G transitions from one state to another) from the particles, weighted by the particle weights:\nEg\u223cP (G|\u03c4)[SS(g)] \u2248 1 W \u2211 i wiSS(gi) . (6)\nESS(g, hk, \u03c4k) can be calculated for each submodel independently:\nEg,h1,...,hN\u223cP (G,H1,...,HN |\u03c4)[SS(g, hk, \u03c4k)]\n\u2248 1 W \u2211 i wi \u222b hk P (hk|gi, \u03c4k)SS(gi, hk, \u03c4k) dhk\n= 1 W \u2211 i \u220f j P (\u03c4j |gi) P (\u03c4) \u222b hk P (hk|gi, \u03c4k)SS(gi, hk, \u03c4k) dhk \u221d 1 W \u2211 i \u220f j 6=k P (\u03c4j |gi) \u222b hk P (hk, \u03c4k|gi)SS(gi, hk, \u03c4k) dhk . (7)\nThe integrals are over all possible trajectories for the hidden process Hk. The first line holds by d-separation (we need only average over the submodel k, given an assignment to G). The second line expands the weight. The last line combines the weight term for submodel k with the terms in the integral to get the likelihood of hk and the submodel data. The constant of proportionality will cancel in the subsequent maximization, or it can be reconstructed by noting that \u2211 x T\u0304X|U[x|u] should be the total time of the interval. This last integral, \u222b hk P (hk, \u03c4k|gi)SS(gi, hk, \u03c4k) dhk, and P (\u03c4j |gi) can be calculated using the technique described by Nodelman et al. (2005), for exact ESS calculation. The calculations are very similar the integrals of Section 3.7, except they intensity matrices can change from interval to interval (they are a function of the sampled trajectory gi).\nThe full E-step algorithm is shown in Figure 3 (sk represents all of the variables in submodel k). Function Submodel Estep calculates the expected sufficient statistics and the likelihood for a subnet model (Equation 7). Function CountGSS counts the empirical time and transition statistics from the sampled trajectory of G (Equation 6).\nIn EM, we use the ESS as if they were the true sufficient statistics to maximize the likelihood with respect to the parameters. For a \u201cregular\u201d CTBN variable X (such as our hidden variable G and H), Equation 5 performs the maximization. For our toggle variables, e.g. Pi, the likelihood\ncomponent for the toggle variable is\u220f u Q MPi Pi|u exp(\u2212QPi|uT [U = u])\nwhich can be found by setting qx|u to be the same value (QPi|u) for all x (tieing the parameters) and simplifying the product over x in Equation 2. Thus the maximum likelihood parameter estimate is\nQPi|u = MPi\nT [U = u]\nwhere MPi is the number of events for variable Pi and QPi|u is the only parameter: the rate of switching.\nWe synchronize the particles at the end of each \u201cwindow\u201d (see Section 6.1) and resample as normal for a particle filter at those points. That is, we propagate the particles forward, but stop them all at the end of the window, resample based on the weights, and then continue with the new set of particles. In general, the particles are not aligned by time, except at these resampling points."}, {"heading": "4.3 Online Testing Using Likelihood", "text": "Once the CTBNmodel has been fitted to historic data, we detect attacks by computing the likelihood of a window of the data (see Section 6.1) under the model. If the likelihood falls below a threshold, we flag the window as anomalous. Otherwise, we mark it as normal.\nIn our experiments, we fix the window to be of a fixed time length, Tw. Therefore, if the window of interest starts at time T , we wish to calculate p(\u03c4 [T, T + Tw] | \u03c4 [0, T ]) where \u03c4 [s, t] represents the observed connections and packets from time s to time t. Again, we use a RBPF to estimate this probability. The samples at time T represent the prior distribution P (G | \u03c4 [0, T ]). Propagating them forward across the window of length Tw produces a set of trajectories for G, gi. Each submodel k can evalute P (\u03c4k[T, T + Tw] | gi) by exact marginalization (the sum of the vector \u03b1T+Tw , the forward message). The weighted average (over samples g\nk) of the product of the submodel probabilities is our estimate of P (\u03c4 [T, T + Tw] | \u03c4 [0, T ])."}, {"heading": "5. Anomaly Detection Using System Calls", "text": "Now we turn to the problem of detecting anomalies using system call logs."}, {"heading": "5.1 A CTBN Model for System Calls", "text": "System call logs monitor the kernel activities of machines. They record detailed information of the sequence of system calls to operating system. Many malicious attacks on the host can be revealed directly from the internal logs.\nWe analyze the audit log format of SUN\u2019s Solaris Basic Security Module (BSM) praudit audit logs. Each user-level and kernel event record has at least three tokens: header, subject, and return. An event begins with a \u201cheader\u201d in the format of: header, record length in bytes, audit record version number, event description, event description modifier, time and date. The \u201csubject\u201d line consists of: subject, user audit ID, effective user ID, effective group ID, real user ID, real group ID, process ID, session ID, and terminal ID consisting of a device and machine name. A \u201creturn\u201d with a return value indicating the success of the event closes the record.\nWe construct a CTBN model similar to our port-level network model. Individual system calls S1, ..., SN , which are the event description fields in the header token, are transiently observed: they happen instantaneously with no duration. We treat them as toggle variables like packets in the network model. We also introduce a hidden variable H as a parent of the system calls variables to allow correlations among them. This hidden variable is designed to model the internal state of the machine, although such a semantic meaning is not imposed by our method. Put together, our system call model looks like Figure 4.\nIf the state space of the hidden variable H is of sizem, the transition rate matrix of H is\nQH =  \u2212qh1 qh1h2 . . . qh1hm qh2h1 \u2212qh2 . . . qh2hm ... ... . . .\n... qhmh1 qhmh2 . . . \u2212qhm\n .\nand the transition intensity rate of the toggle variable s \u2208 S given the current value of its parent H is qs|hi , i = 1, ...,m.\nTo estimate the CTBN model parameters, we again use the expectation maximization (EM) algorithm. The expected sufficient statistics we need to calculate for our model are\n\u2022 M\u0304hihj , the expected number of times H transitions from state i to j; \u2022 T\u0304hi , the expected amount of time H stays in state i; and \u2022 M\u0304s|hi , the expected number of times system call s is evoked when H is in state i.\nThe maximum likelihood parameters are\nqhihj = M\u0304hihj T\u0304hi\nqs|hi = M\u0304s|hi T\u0304hi ."}, {"heading": "5.2 Parameter Estimation with Finite Resolution Clocks", "text": "Because of the finite resolution of computer clocks, multiple instantaneous events (system calls) occur within a single clock tick. Therefore in the audit logs, a batch of system calls may be recorded as being executed at a same time point, rather than their real time stamp, as a result of this finite time accuracy. However, the correct order of the events is kept in the logs. That is, we know exactly that system call S2 follows S1 if they are recorded in this order in the audit logs. Thus all the system call timings are only partially observed. This type of partial observation has not previously been considered in CTBN inference. A typical trajectory \u03c4 over [0, T ] of system call data is shown in Figure 5: a batch of system calls are evoked at some time after ti but before the next clock tick, followed by a quiet period of arbitrary length, and yet another bunch of events at some time after ti+1 and so on.\nLet \u03c4t1:t2 denote the evidence over interval [t1, t2), \u03c4t1:t2+ denote the evidence over [t1, t2], and \u03c4t1\u2212:t2 denote the evidence over (t1, t2). We define the vectors\n\u03b1\u2212ti = p(Ht\u2212i , \u03c40:ti) \u03b2+ti = p(\u03c4t+i :T |Ht+i )\nwhere Ht\u2212i is the value of H just prior to the transition at ti, and Ht+i is value just afterward. We also define the vectors\n\u03b1ti = p(Hti , \u03c40:t+i ) \u03b2ti = p(\u03c4ti:T |Hti)\nwhere the evidence at the transition time ti is included. We follow the forward-backward algorithm to compute \u03b1ti and \u03b2ti for all ti at which there is an event. To do this, we split any interval [ti, ti+1) into a \u201cspike\u201d period [ti, ti + \u03b4t) (\u03b4t is one resolution clock), during which there is a batch of system calls, and a \u201cquite\u201d period [ti+ \u03b4t, ti+1) over which no events exist, and do the propagations separately.\nFor a \u201cspike\u201d period [ti, ti+ \u03b4t), if the observed event sequence is s1, s2, ..., sk, we construct an artificial Markov process X with the following intensity matrix.\nQX =  Q\u0302H Q1 0 . . . 0 0 Q\u0302H Q2 . . . 0 ... ... . . . ... ...\n0 0 . . . Q\u0302H Qk 0 0 . . . 0 Q\u0302H\n .\nwhere\nQ\u0302H =  \u2212qh1 \u2212 \u2211 s\u2208S qs|h1 qh1h2 . . . qh1hm qh2h1 \u2212qh2 \u2212 \u2211 s\u2208S qs|h2 . . . qh2hm ... ... . . .\n... qhmh1 qhmh2 . . . \u2212qhm \u2212 \u2211 s\u2208S qs|hm  and\nQi =  qsi|h1 0 . . . 0 0 qsi|h2 . . . 0 ... ... . . .\n... 0 0 0 qsi|hm  X tracks the evidence sequence s1 \u2192 s2 \u2192 ...\u2192 sk. QX is a square block matrix of dimension m \u00d7 (k + 1). Each block is an m \u00d7m matrix. The subsystem X has k + 1 blocks of states. The first block represents the state ofH before any events. The second block representsH after exactly one event, s1, happens. The third block represents H after s1 followed by s2 happens, and so on. The last block represents H after all the events finish executing in order. The subsystem has zero transition intensities everywhere except along the sequence pass. The diagonal of Q\u0302H is the same matrix as that of QH except that the transition intensities of all the system call variables are subtracted. This is because the full system includes transitions that were not observed. While those transition rates were set to zero (to force the system to agree with the evidence), such conditioning does not change the diagonal elements of the rate matrix (Nodelman et al., 2002). Within each of the k + 1 states of a block, H can freely change its value. Therefore, the non-diagonal elements of Q\u0302H have the same intensities as QH. Upon transitioning, X can only transit from some state to another according to the event sequence. Therefore, most of the blocks are 0 matrices except those to the immediate right of the diagonal blocks. The transition behavior is described by the matrix Qi. Qi has 0 intensities on non-diagonal entries because H and S can not change simultaneously. The diagonal element Qi(h, h) is the intensities of event si happening, given the current value of the hidden state is h.\nWe take the forward pass as an example to describe the propagation; the backward pass can be performed similarly. Right before ti, \u03b1\u2212ti hasm dimensions. We expand it tom(k + 1) dimensions to form \u03b1ti which only has non-zero probabilities in the first m states. \u03b1ti now describes the distribution over the subsystem X . \u03b1tie\nQX\u03b4t represents the probability distribution at time ti + \u03b4t, given that some prefix of the observed sequence occurred. We take only the lastm state probabilities to condition on the entire sequence happening, thus resulting in anm-dimensional vector, \u03b1ti+\u03b4t .\nFor a \u201cquiet\u201d period [ti + \u03b4t, ti+1), no evidence is observed. Therefore \u03b1ti+\u03b4t is propagated to \u03b1ti+1 using Q\u0302H, the rate matrix conditioned on only H events occuring:\n\u03b1ti+1 = \u03b1ti+\u03b4t exp(Q\u0302H(ti+1 \u2212 ti \u2212 \u03b4t)) .\nWhen we are done with the full forward-backward pass over the whole trajectory, we can calculate the expected sufficient statistics M\u0304hihj , T\u0304hi and M\u0304s|hi . Again, we refer to the work of Nodelman et al. (2005) for the algorithm."}, {"heading": "5.3 Testing Using Likelihood", "text": "Once we have learned the model from the normal process in the system call logs, we calculate the log-likelihood of a future process under the model. The log-likelihood is then compared to a predefined threshold. If it is below the threshold, a possible anomaly is indicated. With only a single hidden variable, these calculations can be done exactly."}, {"heading": "6. Evaluation", "text": "To evaluate our methodology, we constructed experiments on two different types of data: network traffic traces and system call logs. In the following sections, we show the experiment results on both tasks.\nA dynamic Bayesian network (DBN) is another popular technique for graphical modeling of temporal data. Because they slice time, events without state changes (instantaneous events) are difficult to model. Any reasonable time resolution will result with multiple events for the same variable over one time period. There is no standard way of encoding this in a DBN. If we use a toggle variable, it only records the parity of the number of events over the time interval. Furthermore, for the NIDS, events are very bursty. During active times, multiple packets are emited per second. During inactive times, there may be no activity for hours. Finding a suitable sampling rate that maintains the efficency of the model is difficult. For the HIDS, the problem is more acute. We do not know of any way of modeling timing ambiguity in a DBN without throwing away all timing information or adding a mathematical framework that essentially turns the DBN into the CTBN described here. In general, we could not find a suitable way to apply a DBN to these problems without essentially turning the DBN into a CTBN by very finely slicing time and then applying numeric tricks to speed up inference that amount to converting the stochastic matrices into rate matrices and using numeric integration for the matrix exponential.\nWe have compared against current adaptive methods for each problem individually. These include nearest neighbor, support vector machines, and sequence time-delaying embedding. We give further details on these methods below."}, {"heading": "6.1 Experiment Results on Network Traffic", "text": "In this section, we present our experiment results on NIDS."}, {"heading": "6.1.1 DATASETS", "text": "We verify our approach on two publicly available real network traffic trace repositories: the MAWI working group backbone traffic MAWI and the LBNL/ICSI internal enterprise traffic LBNL.\nThe MAWI backbone traffic is part of the WIDE project which has collected raw daily packet header traces since 2001. It records the network traffic through the inter-Pacific tunnel between Japan and the USA. The dataset uses tcpdump and IP anonymizing tools to record 15-minute traces every day, and consists mostly of traffic from or to Japanese universities. In our experiment, we use the traces from January 1st to 4th of 2008, with 36,592,148 connections over a total time of one hour.\nThe LBNL traces are recorded from a medium-sized site, with emphasis on characterizing internal enterprise traffic. Publicly released in an anonymized form, the LBNL data collects more than\n100 hours network traces from thousands of internal hosts. From what is publicly released, we take one hour traces from January 7th, 2005 (the latest date available), with 3,665,018 total connections."}, {"heading": "6.1.2 WORM DETECTION", "text": "We start with the problem of worm detection. We split traffic traces for each host: half for training and half for testing. We learn a CTBN model from the training data for each of the hosts. Since the network data available are clean traffic with no known intrusions, we inject real attack traces into the testing data. In particular, we inject IP Scanner, W32.Mydoom, and Slammer. We then slide a fixed-time window over the testing traces, report a single log-likelihood value for each sliding window, and compare it with a predefined threshold. If it is below the threshold, we predict it as an abnormal time period. We define the ground truth for a window to be abnormal if any attack traffic exists in the interval, and normal otherwise. The window size we use is 50 seconds. We only consider windows that contain at least one network event.\nWe compare our method employing RBPF with our previous factored CTBN model (Xu & Shelton, 2008), connection counting, nearest neighbor, Parzen-window detector (Yeung & Chow, 2002), and one-class SVM with a spectrum string kernel (Leslie, Eskin, & Noble, 2002).\nThe connection counting method is straightforward. We score a window by the number of initiated connections in the window. As most worms aggregate many connections in a short time, this method captures this particular anomaly well.\nTo make nearest neighbor competitive, we try to extract a reasonable set of features. We follow the feature selection of the work of Lazarevic et al. (2003), who use a total of 23 features. Not all of their features are available in our data. Those available are shown in Figure 6. Notice that these features are associated with each connection record. To apply the nearest neighbor method to our window based testing framework, we first calculate the nearest distance of each connection inside the window to the training set (which is composed of normal traffic only), and assign the maximum among them as the score for the window. Similarly, for the Parzen window approach, we apply the same feature set and assign the maximum density among all the connections inside a window to be the score of that window.\nBesides the above feature-based algorithms, we would also like to see how sequence-based approaches compare against our methods. These algorithms are widely used in network anomaly detection. Like our approach, they treat the traffic traces as stream data so that sequential contexts\ncan be explored. One-class SVM with spectrum string kernel was chosen for comparison. We implemented a spectrum kernel in the LIBSVM library (Chang & Lin, 2001). We give the network activities (such as a connection starting or ending, or a packet emmision or receipt) inside each portlevel submodel a distinct symbol. The sequence of these symbols are fed to the algorithm as inputs. A decision surface is trained from normal training traffic. In testing, for each sliding window, the distance from this window string to the decision hyperplane is reported as the window score. We also tried experiments using the edit distance kernel, but their results are dominated by the spectrum kernel, so we do not report them here.\nWhen injecting the attack traffic, we randomly pick a starting point somewhere in the first half of the test trace and insert worm traffic for a duration equal to \u03b1 times the length of the full testing trace. The shorter \u03b1 is, the harder it is to detect the anomaly. We choose \u03b1 to be 0.02% for all the experiments in this work to challenge the detection tasks. We also scaled back the rates of the worms. When running at full speed, a worm is easy to detect for any method. When it slows down (and thus blends into the background traffic better), it becomes more difficult to detect. We let \u03b2 be the scaling rate (e.g. 0.1 indicates a worm running at one-tenth of its normal speed).\nFor our method, we set the state space of variable G to be 4 and variable H to be 8. We use 100 samples for particle filtering, and resample the particles after every 50 seconds. For the SVM spectrum kernel method, we choose the sub-sequence length to be 5 and the parameter \u03bd to be 0.8.\nWe show the ROC curves of all the methods in Figure 7. The curves show the overall performance on the 10 most active hosts for each dataset. Each point on the curves corresponds to a\ndifferent threshold of the algorithm. Our CTBN method out-performs the other algorithms except in the single case of the Mydoom attack against a background of the LBNL traffic. In many cases, the advantages of the CTBN approach are pronounced.\nFor all of the MAWI data, the factored and non-factored CTBN models perform comparably. We believe this is because the data only captures connections that traverse a trans-Pacific link. Therefore, not all of the connections in or out of a machine are represented. This makes reasoning about the global pattern of interaction for a machine difficult. For the LBNL data, one attack (IP scanning) shows no advantage to a non-factored model. One attack (Mydoom) shows a distinct advantage. And one attack (Slammer) indicates some advantage, depending on the desired false positive rate. This demonstrate some advantage to jointly modeling the traffic across all ports, although it is clear this advantage is not uniform over traffic patterns and attack types.\nWe also show how the ROC curves shift as we scale back the worm running speed \u03b2 in Figure 8. As firewalls are built to be more sensitive to block malicious traffic, worms have to act more stealthy to sneak through. We demonstrate the robustness of our method compared to the best competitor (connection counts) to the speed of the worm\u2019s attack."}, {"heading": "6.1.3 HOST IDENTIFICATION", "text": "Identifying individual hosts based on their network traffic patterns is another useful application of our model. For instance, a household usually installs a network router. Each family member\u2019s computer is connected to this router. To the outside Internet, the network traffic going out of the router behaves as if it is coming from one peer, but it is actually coming from different people. Dad will possibly read sports news while kids surf on social networks. It is interesting as well as useful to tell which family member is contributing the current network traffic. Host identification can also be used to combat identity theft. When a network identity is abused by the attacker, host identification techniques can help the network administrator tell whether the current network traffic of this host is consistent with its usual pattern or not.\nThe first set of experiments we construct is a host model fitting competition. The same 10 hosts picked for the worm detection tasks from LBNL dataset compose our testing pool. We learn the coupled CTBN model for each host. We split the test traces (clean) of a particular host into segments with lengths of 15 seconds. For each of the segments, we compute the log-likelihood of the segment under the learned model from all the hosts (including its own), and label the segment with the host that achieves the highest value. We compute a confusion matrix C whose element Cij equals the fraction of test traces of host i for which model j has highest log-likelihood. We expect to see the highest hit rates fall on the diagonals because ideally a host should be best described by its own model. Table 9 shows our results on the dataset of LBNL. The vast majority of traffic windows are assigned to the correct host. With the exception of host 1, the diagonals are distinctly higher than other elements in the same row. For comparison, we performed the same experiment using SVM spectrum kernel method. Again, we selected the sub-sequence length to be 5 and the parameter \u03bd to be 0.8. We tried multiple methods for normalization (of the distance to the hyperplane) and variations of parameters. All produced very poor results with almost all of the windows assigned to a single host. We omit the table of results.\nOur second experiment is a host traffic differentiation task. We mingle the network traffic from another host with the analyzed host. We expect the detection method to successfully tell apart the two. To verify this idea, we pick one host among the 10 we choose above from LBNL dataset and split its traffic evenly into training and testing. We again learn the model from training data. For testing data, we randomly choose a period and inject another host\u2019s traffic as if it were a worm. Our goal is to identify the period as abnormal since the host\u2019s traffic is no longer its own behavior. Figure 10 displays the results from two such combination tests. The parameters for injecting the traffic as a worm are \u03b1 = 0.02, \u03b2 = 0.001. In the left graph, the nearest neighbor and Parzen window curve overlap, and both CTBN curves overlap. In the right graph, the coupled CTBN curve substantially outperforms all the other curves."}, {"heading": "6.2 Experiment Results on System Call Logs", "text": "In this section, we present our experiment results on HIDS."}, {"heading": "6.2.1 DATASET", "text": "The dataset we used is the 1998 DARPA Intrusion Detection Evaluation Data Set fromMIT Lincoln Laboratory. Seven weeks of training data that contain labeled network-based attacks in the midst of normal background data are publicly available at the DARPA website. The Solaris Basic Security Module (BSM) praudit audit data on system call logs are provided for research analysis. We follow Kang, Fuller, and Honavar (2005) to cross-index the BSM logs and produce a labeled list file that labels individual processes. The resulting statistics are shown on the left table of Figure 11. The frequency of all the system calls appearing in the dataset is summarized in descending order on the right of Figure 11."}, {"heading": "6.2.2 ANOMALY DETECTION", "text": "Our experimental goal is to detect anomalous processes. We train our CTBN model on normal processes only and test on a mixture of both normal and attack processes. The state space of the\nhidden variable H is set to 2. The log-likelihood of a whole process under the learned model represents the score of this process. We compare to the score with a predefined threshold to classify the process as a normal one or a system abuse.\nWe implement sequence time-delaying embedding (stide) and stide with frequency threshold (t-stide) for comparison (Warrender, Forrest, & Pearlmutter, 1999). These two algorithms build a database of all previously seen normal sequences of system calls and compare the testing sequences with it. They are straightforward and perform very well empirically on most of the system call log datasets. We choose the parameter k, the sequence length to be 5, and h, the locality frame length, to be 50. The results for t-stide are not shown in the following resulting graphs since they overlapped with stide in almost all cases.\nOther approaches we compare against are nearest neighbor and one-class SVM with spectrum string kernel and edit distance kernel. We follow Hu et al. (2003) and transform a process into a feature vector, consisting of the occurrence numbers of each system call in the process. The nearest distance between a testing process and the training set of processes is assigned as the score. For one-class SVM, processes are composed of strings of system calls. Normal processes are used for learning the bounding surface and the signed distance to it is assigned as the score. We set the subsequence length to be 5 and the parameter \u03bd to be 0.5. Again, since the edit distance kernel results are dominated by the spectrum kernel, we do not show them.\nFigure 12 displays the results from two experiment settings. In the left graph, we train the model on the normal processes from week 1 and test it on all the processes from weeks 2 to 7. In the right graph, we train on normal processes from week 3 and test it on all the processes from week 4, the richest in attack processes volume. Because attacks are relatively rare compared to normal traffic, we are most interested in the region of the ROC curves with small false positive rates. So we only show the curves in the area where the false positive rate falls in the region [0, 0.05]. Our CTBN method beats nearest neighbor and SVM with spectrum kernel in both experiments. stide performs slightly better than our method in the combined test, but achieves the same accuracy in\nthe experiment using only week 3 and testing on week 4. The advantage to the CTBN model over stide is that it can be easily combined with other prior knowledge and other data sources (such as the network data from NIDS). We demonstrate that there is no loss of performance from such flexibility."}, {"heading": "7. Conclusions", "text": "In the realm of temporal reasoning, we have introduced two additions to the CTBN literature. First, we demonstrated a Rao-Blackwellized particle filter with continuous evidence. Second, we demonstrated that we can learn and reason about data that contains imprecise timings, while still refraining from discretizing time.\nIn the realm of intrusion detection, we have demonstrated a framework that performs well on two related tasks with very different data types. By concentrating purely on event timing, without the consideration of complex features, we were able to out-perform existing methods. The continuoustime nature of our model aided greatly in modeling the bursty event sequences that occur in systems logs and network traffic. We did not have to resort to time slicing, either producing rapid slices that are inefficient for quite periods, or lengthy slices that miss the timing of bursty events.\nA combination of the two sources of information (system calls and network events) would be straight-forward with the model we have produced. We believe it would result in more accurate detection. The collection of such data is difficult, however; we leave it as an interesting next step."}, {"heading": "Acknowledgments", "text": "This project was supported by Intel Research and UC MICRO, by the Air Force Office of Scientific Research (FA9550-07-1-0076), and by the Defense Advanced Research Project Agency (HR001109-1-0030)."}], "references": [{"title": "An adaptive anomaly detector for worm detection", "author": ["J.M. Agosta", "C. Duik-Wasser", "J. Chandrashekar", "C. Livadas"], "venue": "In Workshop on Tackling Computer Systems Problems with Machine Learning Techniques", "citeRegEx": "Agosta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Agosta et al\\.", "year": 2007}, {"title": "Intrusion detection using sequences of system calls", "author": ["S.A.Hofmeyr", "S. Forrest", "A. Somayaji"], "venue": "Journal of Computer Security,", "citeRegEx": "A.Hofmeyr et al\\.,? \\Q1998\\E", "shortCiteRegEx": "A.Hofmeyr et al\\.", "year": 1998}, {"title": "Host anomaly detection performance analysis based on system call of neuro-fuzzy using soundex algorithm and n-gram technique", "author": ["B. Cha"], "venue": "In Systems Communications (ICW)", "citeRegEx": "Cha,? \\Q2005\\E", "shortCiteRegEx": "Cha", "year": 2005}, {"title": "LIBSVM: a library for support vector machines. http:// www.csie.ntu.edu.tw/ \u0303cjlin/libsvm", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Mean field variational approximation for continous-time Bayesian networks", "author": ["I. Cohn", "T. El-Hay", "N. Friedman", "R. Kupferman"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Cohn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Extracting hidden anomalies using sketch and non Gaussian multiresulotion statistical detection procedures", "author": ["G. Dewaele", "K. Fukuda", "P. Borgnat"], "venue": "In ACM SIGCOMM", "citeRegEx": "Dewaele et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dewaele et al\\.", "year": 2007}, {"title": "Rao-Blackwellised particle filtering for dynamic Bayesian networks", "author": ["A. Doucet", "N. de Freitas", "K. Murphy", "S. Russel"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "Continuous-time belief propagation", "author": ["T. El-Hay", "I. Cohn", "N. Friedman", "R. Kupferman"], "venue": "In Proceedings of the Twenty-Seventh International Conference on Machine Learning", "citeRegEx": "El.Hay et al\\.,? \\Q2010\\E", "shortCiteRegEx": "El.Hay et al\\.", "year": 2010}, {"title": "Gibbs sampling in factorized continous-time Markov processes", "author": ["T. El-Hay", "N. Friedman", "R. Kupferman"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "El.Hay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "El.Hay et al\\.", "year": 2008}, {"title": "Anomaly detection over noisy data using learned probability distributions", "author": ["E. Eskin"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Eskin,? \\Q2000\\E", "shortCiteRegEx": "Eskin", "year": 2000}, {"title": "A geometric framework for unsupervised anomaly detection: Detecting intrusions in unlabeled data", "author": ["E. Eskin", "A. Arnold", "M. Prerau", "L. Portnoy", "S. Stolfo"], "venue": "Applications of Data Mining in Computer Security. Kluwer", "citeRegEx": "Eskin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eskin et al\\.", "year": 2002}, {"title": "Sampling for approximate inference in continuous time Bayesian networks", "author": ["Y. Fan", "C.R. Shelton"], "venue": "In Symposium on Artificial Intelligence and Mathematics", "citeRegEx": "Fan and Shelton,? \\Q2008\\E", "shortCiteRegEx": "Fan and Shelton", "year": 2008}, {"title": "Learning continuous-time social network dynamics", "author": ["Y. Fan", "C.R. Shelton"], "venue": "In Proceedings of the Twenty-Fifth International Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Fan and Shelton,? \\Q2009\\E", "shortCiteRegEx": "Fan and Shelton", "year": 2009}, {"title": "A sense of self for unix processes", "author": ["S. Forrest", "S.A.Hofmeyr", "A. Somayaji", "T.A.Longstaff"], "venue": "In IEEE Symposium on Security and Privacy,", "citeRegEx": "Forrest et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Forrest et al\\.", "year": 1996}, {"title": "Robust support vector machines for anomaly detection in computer security", "author": ["W. Hu", "Y. Liao", "V. Vemuri"], "venue": "In International Conference on Machine Learning and Applications", "citeRegEx": "Hu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2003}, {"title": "Learning classifiers for misuse detetction using a bag of system calls representation", "author": ["Kang", "D.-K", "D. Fuller", "V. Honavar"], "venue": "In IEEE International Conferences on Intelligence and Security Informatics", "citeRegEx": "Kang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2005}, {"title": "BLINC: Multilevel traffic classification in the dark", "author": ["T. Karagiannis", "K. Papagiannaki", "M. Faloutsos"], "venue": "In ACM SIGCOMM", "citeRegEx": "Karagiannis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Karagiannis et al\\.", "year": 2005}, {"title": "Bayesian event classification for intrusion detection", "author": ["C. Kruegel", "D. Mutz", "W. Robertson", "F. Valeur"], "venue": "In Annual Computer Security Applications Conference", "citeRegEx": "Kruegel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kruegel et al\\.", "year": 2003}, {"title": "Mining anomalies using traffic feature distributions", "author": ["A. Lakhina", "M. Crovella", "C. Diot"], "venue": "In ACM SIGCOMM,", "citeRegEx": "Lakhina et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lakhina et al\\.", "year": 2005}, {"title": "A compare study of anomaly detection schemes in network intrusion detection", "author": ["A. Lazarevic", "L. Ertoz", "V. Kumar", "A. Ozgur", "J. Srivastava"], "venue": "In SIAM International Conference on Data Mining", "citeRegEx": "Lazarevic et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lazarevic et al\\.", "year": 2003}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "W.S. Noble"], "venue": "In Pacific Symposium on Biocomputing", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "Host-based detection of worms through peer to peer cooperation", "author": ["D.J. Malan", "M.D. Smith"], "venue": "In Workshop on Rapid Malcode", "citeRegEx": "Malan and Smith,? \\Q2005\\E", "shortCiteRegEx": "Malan and Smith", "year": 2005}, {"title": "Internet traffic classification using Bayesian analysis techniques", "author": ["A.W. Moore", "D. Zuev"], "venue": "In ACM SIGMETRICS", "citeRegEx": "Moore and Zuev,? \\Q2005\\E", "shortCiteRegEx": "Moore and Zuev", "year": 2005}, {"title": "Continuous time particle filtering", "author": ["B. Ng", "A. Pfeffer", "R. Dearden"], "venue": "InNational Conference on Artificial Intelligence,", "citeRegEx": "Ng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2005}, {"title": "Continuous time Bayesian networks for inferring users\u2019 presence and activities with extensions for modeling and evaluation", "author": ["U. Nodelman", "E. Horvitz"], "venue": "Tech. rep. MSR-TR-2003-97, Microsoft Research", "citeRegEx": "Nodelman and Horvitz,? \\Q2003\\E", "shortCiteRegEx": "Nodelman and Horvitz", "year": 2003}, {"title": "Expectation propagation for continuous time Bayesian networks", "author": ["U. Nodelman", "D. Koller", "C.R. Shelton"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2005}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2002}, {"title": "Learning continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2003}, {"title": "Expectation maximization and complex duration distributions for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2005}, {"title": "Numerical Recipes in C (Second edition)", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": null, "citeRegEx": "Press et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Press et al\\.", "year": 1992}, {"title": "Attack plan recognition and prediction using causal networks", "author": ["X. Qin", "W. Lee"], "venue": "In Annual Computer Security Application Conference,", "citeRegEx": "Qin and Lee,? \\Q2004\\E", "shortCiteRegEx": "Qin and Lee", "year": 2004}, {"title": "Language models for detection of unknown attacks in network traffic", "author": ["K. Rieck", "P. Laskov"], "venue": "In Journal in Computer Virology", "citeRegEx": "Rieck and Laskov,? \\Q2007\\E", "shortCiteRegEx": "Rieck and Laskov", "year": 2007}, {"title": "Reasoning at the right time granularity", "author": ["S. Saria", "U. Nodelman", "D. Koller"], "venue": "InUncertainty in Artificial Intelligence", "citeRegEx": "Saria et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Saria et al\\.", "year": 2007}, {"title": "CT-NOR: Representing and reasoning about events in continuous time", "author": ["A. Simma", "M. Goldszmidt", "J. MacCormick", "P. Barham", "R. Black", "R. Isaacs", "R. Mortier"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Simma et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Simma et al\\.", "year": 2008}, {"title": "Flow classification by histogram", "author": ["A. Soule", "L. Salamatian", "N. Taft", "R. Emilion", "K. Papagiannali"], "venue": "In ACM SIGMETRICS", "citeRegEx": "Soule et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Soule et al\\.", "year": 2004}, {"title": "Combining filtering and statistical methods for anomaly detection", "author": ["A. Soule", "K. Salamatian", "N. Taft"], "venue": "In Internet Measurement Conference,", "citeRegEx": "Soule et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Soule et al\\.", "year": 2005}, {"title": "Learning useful system call attributes for anomaly detection", "author": ["G. Tandon", "P.K. Chan"], "venue": "In The Florida Artificial Intelligence Research Society Conference,", "citeRegEx": "Tandon and Chan,? \\Q2005\\E", "shortCiteRegEx": "Tandon and Chan", "year": 2005}, {"title": "Detecting intrusions using system calls: Alternative data models", "author": ["C. Warrender", "S. Forrest", "B. Pearlmutter"], "venue": "In IEEE Symposium on Security and Privacy,", "citeRegEx": "Warrender et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Warrender et al\\.", "year": 1999}, {"title": "Continuous time Bayesian networks for host level network intrusion detection", "author": ["J. Xu", "C.R. Shelton"], "venue": "In European Conference on Machine Learning", "citeRegEx": "Xu and Shelton,? \\Q2008\\E", "shortCiteRegEx": "Xu and Shelton", "year": 2008}, {"title": "Profiling internet backbone traffic: Behavior models and applications", "author": ["K. Xu", "Zhang", "Z.-L", "S. Bhattacharyya"], "venue": "In ACM SIGCOMM", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Multivariate statistical analysis of audit trails for host-based intrusion detection", "author": ["N. Ye", "S.M. Emran", "Q. Chen", "S. Vilbert"], "venue": "IEEE Transactions of Computers,", "citeRegEx": "Ye et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2002}, {"title": "Parzen-window network intrusion detectors", "author": ["Yeung", "D.-Y", "C. Chow"], "venue": "In International Conference on Pattern Recognition", "citeRegEx": "Yeung et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Yeung et al\\.", "year": 2002}, {"title": "User profiling for intrusion detection using dynamic and static", "author": ["XU", "SHELTON Yeung", "D.-Y", "Y. Ding"], "venue": null, "citeRegEx": "XU et al\\.,? \\Q2002\\E", "shortCiteRegEx": "XU et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "The work of Eskin, Arnold, Prerau, Portnoy, and Stolfo (2002) is similar to our approach in that they apply their method to both of these kinds of", "startOffset": 12, "endOffset": 62}, {"referenceID": 18, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al.", "startOffset": 145, "endOffset": 168}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al.", "startOffset": 169, "endOffset": 180}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al.", "startOffset": 169, "endOffset": 200}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information.", "startOffset": 169, "endOffset": 225}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data.", "startOffset": 169, "endOffset": 760}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data.", "startOffset": 169, "endOffset": 822}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels.", "startOffset": 169, "endOffset": 935}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification).", "startOffset": 169, "endOffset": 1521}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally.", "startOffset": 169, "endOffset": 1781}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally. Rieck and Laskov (2007) model the language features like n-grams and words from connection payloads.", "startOffset": 169, "endOffset": 1936}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally. Rieck and Laskov (2007) model the language features like n-grams and words from connection payloads. Xu, Zhang, and Bhattacharyya (2005) also use unsupervised methods, but they concentrate on clustering traffic across a whole network.", "startOffset": 169, "endOffset": 2049}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally. Rieck and Laskov (2007) model the language features like n-grams and words from connection payloads. Xu, Zhang, and Bhattacharyya (2005) also use unsupervised methods, but they concentrate on clustering traffic across a whole network. Similarly, Soule, Salamatian, and Taft (2005) build an anomaly detector based on Markov models, but it is for the network traffic patterns as a whole and does not function at the host level.", "startOffset": 169, "endOffset": 2193}, {"referenceID": 2, "context": "In particular, we also assume that we do not have access to the internals of the machines on the networks, which rules out methods like those of Malan and Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in that our approach does not rely on preset values, require human intervention and interpretation, nor assume that we have access to network-wide traffic information. Network-wide data and human intervention have advantages, but they can also lead to difficulties (data collation in the face of an attack and increased human effort), so we chose to leave them out of our solution. Many learning, or adaptive, methods have been proposed for network data. Some of these \u2014 for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004) \u2014 approach the problem as a classification task which requires labeled data. Dewaele, Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian marginal distribution to extract anomalies at different aggregation levels. The goal of such papers is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack detection, they would risk missing new types of attacks. Furthermore, they frequently treat each network activity separately, instead of considering their temporal context. Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that look at anomaly detection (instead of classification). They use an entropy-based method for the entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert (2002), use either statistical tests or subspace methods that assume the features of the connections or packets are distributed normally. Rieck and Laskov (2007) model the language features like n-grams and words from connection payloads. Xu, Zhang, and Bhattacharyya (2005) also use unsupervised methods, but they concentrate on clustering traffic across a whole network. Similarly, Soule, Salamatian, and Taft (2005) build an anomaly detector based on Markov models, but it is for the network traffic patterns as a whole and does not function at the host level. The work of Soule et al. (2004) is very similar in statistical flavor to our work.", "startOffset": 169, "endOffset": 2370}, {"referenceID": 21, "context": "The work of Moore and Zuev (2005), like our approach, models traffic with graphical models, in particular, Naive Bayes networks.", "startOffset": 12, "endOffset": 34}, {"referenceID": 21, "context": "The work of Moore and Zuev (2005), like our approach, models traffic with graphical models, in particular, Naive Bayes networks. But their goal is to categorize network traffic instead of detecting attacks. Kruegel, Mutz, Robertson, and Valeur (2003) present a Bayesian approach to the detecting problem as an event classification task while we only care about whether the host is under attack during an interval.", "startOffset": 12, "endOffset": 251}, {"referenceID": 21, "context": "The work of Moore and Zuev (2005), like our approach, models traffic with graphical models, in particular, Naive Bayes networks. But their goal is to categorize network traffic instead of detecting attacks. Kruegel, Mutz, Robertson, and Valeur (2003) present a Bayesian approach to the detecting problem as an event classification task while we only care about whether the host is under attack during an interval. The work of Lazarevic, Ertoz, Kumar, Ozgur, and Srivastava (2003) is also similar to our work.", "startOffset": 12, "endOffset": 480}, {"referenceID": 2, "context": "Agosta, Duik-Wasser, Chandrashekar, and Livadas (2007) present an adaptive detector whose threshold is time-varying.", "startOffset": 21, "endOffset": 55}, {"referenceID": 2, "context": "Tandon and Chan (2005) look at a richer set of attributes like return value and arguments associated with a system call while we only make use of the system call names.", "startOffset": 11, "endOffset": 23}, {"referenceID": 2, "context": "Tandon and Chan (2005) look at a richer set of attributes like return value and arguments associated with a system call while we only make use of the system call names. Feature based methods like those of Hu, Liao, and Vemuri (2003) use the same dataset we use, the DARPA 1998 BSM dataset, but their training data is noisy and they try to find a classification hyperplane using robust support vector machines (RSVMs) to separate normal system call profiles from intrusive ones.", "startOffset": 11, "endOffset": 233}, {"referenceID": 2, "context": "Tandon and Chan (2005) look at a richer set of attributes like return value and arguments associated with a system call while we only make use of the system call names. Feature based methods like those of Hu, Liao, and Vemuri (2003) use the same dataset we use, the DARPA 1998 BSM dataset, but their training data is noisy and they try to find a classification hyperplane using robust support vector machines (RSVMs) to separate normal system call profiles from intrusive ones. Eskin (2000) also works on noisy data.", "startOffset": 11, "endOffset": 491}, {"referenceID": 2, "context": "Tandon and Chan (2005) look at a richer set of attributes like return value and arguments associated with a system call while we only make use of the system call names. Feature based methods like those of Hu, Liao, and Vemuri (2003) use the same dataset we use, the DARPA 1998 BSM dataset, but their training data is noisy and they try to find a classification hyperplane using robust support vector machines (RSVMs) to separate normal system call profiles from intrusive ones. Eskin (2000) also works on noisy data. They make the assumption that their training data contains a large portion of normal elements and few anomalies. They present a mixture of distribution over normal and abnormal data and calculate the likelihood change if a data point is moved from normal part to abnormal part to get the optimum data partition. Yeung and Ding (2002) try to use both techniques.", "startOffset": 11, "endOffset": 851}, {"referenceID": 25, "context": "Nodelman et al. (2002) extend the theory of HMPs and present continuous time Bayesian networks (CTBNs), which model the joint dynamics of several local variables by allowing the transition model of each local variable X to be a Markov process whose parametrization depends on some subset of other variables U .", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "Definition 2 (Nodelman et al., 2003) A continuous time Bayesian networkN over a set of stochastic processes X consists of two components: an initial distribution P 0 X , specified as a Bayesian network B over a set of random variables X, and a continuous transition model, specified using a directed (possibly cyclic) graph G whose nodes areX \u2208 X; UX denotes the parents ofX in G.", "startOffset": 13, "endOffset": 36}, {"referenceID": 11, "context": "Fan and Shelton (2008) give another sampling based approach that uses importance sampling.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Fan and Shelton (2008) give another sampling based approach that uses importance sampling. El-Hay, Cohn, Friedman, and Kupferman (2010) describe a different expectation propagation approach.", "startOffset": 0, "endOffset": 136}, {"referenceID": 11, "context": "Fan and Shelton (2008) give another sampling based approach that uses importance sampling. El-Hay, Cohn, Friedman, and Kupferman (2010) describe a different expectation propagation approach. To estimate the parameters of the models we build for the two applications (NIDS and HIDS), we employ inference algorithms including exact inference and a Rao-Blackwellized particle filtering (RBPF) algorithm, depending on the model size. Ng et al. (2005) extended RBPF to CTBNs.", "startOffset": 0, "endOffset": 447}, {"referenceID": 21, "context": "Nodelman and Horvitz (2003) used CTBNs to reason about users\u2019 presence and availability over time.", "startOffset": 0, "endOffset": 28}, {"referenceID": 21, "context": "Ng et al. (2005) used CTBNs to monitor a mobile robot.", "startOffset": 0, "endOffset": 17}, {"referenceID": 21, "context": "Ng et al. (2005) used CTBNs to monitor a mobile robot. Nodelman et al. (2005) used CTBNs to model life event history.", "startOffset": 0, "endOffset": 78}, {"referenceID": 11, "context": "Fan and Shelton (2009) modeled social networks via CTBNs.", "startOffset": 0, "endOffset": 23}, {"referenceID": 26, "context": "Approximate inference techniques like the clique tree algorithm (Nodelman et al., 2002), message passing algorithms (Nodelman et al.", "startOffset": 64, "endOffset": 87}, {"referenceID": 25, "context": ", 2002), message passing algorithms (Nodelman et al., 2005; Saria et al., 2007), importance sampling (Fan", "startOffset": 36, "endOffset": 79}, {"referenceID": 32, "context": ", 2002), message passing algorithms (Nodelman et al., 2005; Saria et al., 2007), importance sampling (Fan", "startOffset": 36, "endOffset": 79}, {"referenceID": 25, "context": "To calculate the expected sufficient statistics in the E-step of EM for parameter learning, the exact inference algorithm of Nodelman et al. (2002) flattens all the variables into a joint intensity matrix and reasons about the resulting homogeneous Markov process.", "startOffset": 125, "endOffset": 148}, {"referenceID": 8, "context": "& Shelton, 2008) and Gibbs sampling (El-Hay et al., 2008) overcome this problem by sacrificing accuracy.", "startOffset": 36, "endOffset": 57}, {"referenceID": 6, "context": "It is difficult to sample directly from the posterior distribution, so we use an importance sampler to sample a particle from a proposal distribution and the particles are weighted by the ratio of its likelihood under the posterior distribution to the likelihood under the proposal distribution (Doucet et al., 2000).", "startOffset": 295, "endOffset": 316}, {"referenceID": 25, "context": "This last integral, \u222b hk P (hk, \u03c4k|g)SS(g, hk, \u03c4k) dhk, and P (\u03c4j |gi) can be calculated using the technique described by Nodelman et al. (2005), for exact ESS calculation.", "startOffset": 122, "endOffset": 145}, {"referenceID": 26, "context": "While those transition rates were set to zero (to force the system to agree with the evidence), such conditioning does not change the diagonal elements of the rate matrix (Nodelman et al., 2002).", "startOffset": 171, "endOffset": 194}, {"referenceID": 25, "context": "Again, we refer to the work of Nodelman et al. (2005) for the algorithm.", "startOffset": 31, "endOffset": 54}, {"referenceID": 19, "context": "Figure 6: Features for nearest neighbor approach from the work of (Lazarevic et al., 2003).", "startOffset": 66, "endOffset": 90}, {"referenceID": 9, "context": "We compare our method employing RBPF with our previous factored CTBN model (Xu & Shelton, 2008), connection counting, nearest neighbor, Parzen-window detector (Yeung & Chow, 2002), and one-class SVM with a spectrum string kernel (Leslie, Eskin, & Noble, 2002). The connection counting method is straightforward. We score a window by the number of initiated connections in the window. As most worms aggregate many connections in a short time, this method captures this particular anomaly well. To make nearest neighbor competitive, we try to extract a reasonable set of features. We follow the feature selection of the work of Lazarevic et al. (2003), who use a total of 23 features.", "startOffset": 238, "endOffset": 650}, {"referenceID": 14, "context": "We follow Hu et al. (2003) and transform a process into a feature vector, consisting of the occurrence numbers of each system call in the process.", "startOffset": 10, "endOffset": 27}], "year": 2010, "abstractText": "Intrusion detection systems (IDSs) fall into two high-level categories: network-based systems (NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls. In this work, we present a general technique for both systems. We use anomaly detection, which identifies patterns not conforming to a historic norm. In both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid specifying a fixed update interval common to discrete-time models. We build generative models from the normal training data, and abnormal behaviors are flagged based on their likelihood under this norm. For NIDS, we construct a hierarchical CTBN model for the network packet traces and use Rao-Blackwellized particle filtering to learn the parameters. We illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. We demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.", "creator": "TeX"}}}