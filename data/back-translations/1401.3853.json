{"id": "1401.3853", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Implicit Abstraction Heuristics", "abstract": "However, this heuristics is inherently limited, as the size of the abstract space must be limited by some, albeit very large, constants. To address this shortcoming, we introduce the concept of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning, and then introduce a concrete definition of this framework, called fork decomposition, which is based on two novel fragments of tractable cost-optimal planning, and the resulting permissible heuristics are then formally and empirically investigated. This study demonstrates the accuracy of fork decomposition of heuristics, but our empirical evaluation also emphasizes the compromise between their accuracy and the duration-complexity of their calculation.", "histories": [["v1", "Thu, 16 Jan 2014 04:59:55 GMT  (1431kb)", "http://arxiv.org/abs/1401.3853v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael katz", "carmel domshlak"], "accepted": false, "id": "1401.3853"}, "pdf": {"name": "1401.3853.pdf", "metadata": {"source": "CRF", "title": "Implicit Abstraction Heuristics", "authors": ["Michael Katz", "Carmel Domshlak"], "emails": ["dugi@tx.technion.ac.il", "dcarmel@ie.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Heuristic search, either through progression in the space of world states or through regression in the space of subgoals, is a common and successful approach to classical planning. It is probably the most popular approach to cost-optimal planning, that is, finding a plan with a minimal total cost of its actions. The difference between various heuristic-search algorithms for optimal planning is mainly in the admissible heuristic functions they employ. In state-space search, such a heuristic estimates the cost of achieving the goal from a given state and guarantees not to overestimate that cost.\nA useful heuristic function must be accurate as well as efficiently computable. Improving the accuracy of a heuristic function without substantially worsening the time complexity of computing it usually translates into faster search for optimal solutions. During the last decade, numerous computational ideas evolved into new admissible heuristics for classical planning; these include the delete-relaxing max heuristic hmax (Bonet & Geffner, 2001), critical path heuristics hm (Haslum & Geffner, 2000), landmark heuristics hL, hLA (Karpas & Domshlak, 2009) and hLM-cut (Helmert & Domshlak, 2009), and abstraction heuristics such\nc\u00a92010 AI Access Foundation. All rights reserved.\nas pattern database heuristics (Edelkamp, 2001) and merge-and-shrink heuristics (Helmert, Haslum, & Hoffmann, 2007). Our focus in this work is on the abstraction heuristics.\nGenerally speaking, an abstraction of a planning task is given by a mapping \u03b1 : S \u2192 S\u03b1 from the states of the planning task\u2019s transition system to the states of some \u201cabstract transition system\u201d such that, for all states s, s\u2032 \u2208 S, the cost from \u03b1(s) to \u03b1(s\u2032) is upperbounded by the cost from s to s\u2032. The abstraction heuristic value h\u03b1(s) is then the cost from \u03b1(s) to the closest goal state of the abstract transition system. Perhaps the most well-known abstraction heuristics are pattern database (PDB) heuristics, which are based on projecting the planning task onto a subset of its state variables and then explicitly searching for optimal plans in the abstract space. Over the years, PDB heuristics have been shown to be very effective in several hard search problems, including cost-optimal planning (Culberson & Schaeffer, 1998; Edelkamp, 2001; Felner, Korf, & Hanan, 2004; Haslum, Botea, Helmert, Bonet, & Koenig, 2007). The conceptual limitation of these heuristics, however, is that the size of the abstract space and its dimensionality must be fixed.1 The more recent merge-andshrink abstractions generalize PDB heuristics to overcome the latter limitation (Helmert et al., 2007). Instead of perfectly reflecting just a few state variables, merge-and-shrink abstractions allow for imperfectly reflecting all variables. As demonstrated by the formal and empirical analysis of Helmert et al., this flexibility often makes the merge-and-shrink abstractions much more effective than PDBs. However, the merge-and-shrink abstract spaces are still searched explicitly, and thus they still have to be of fixed size. While quality heuristics estimates can still be obtained for many problems, this limitation is a critical obstacle for many others.\nOur goal in this paper is to push the envelope of abstraction heuristics beyond explicit abstractions. We introduce a principled way to obtain abstraction heuristics that limit neither the dimensionality nor the size of the abstract spaces. The basic idea behind what we call implicit abstractions is simple and intuitive: instead of relying on abstract problems that are easy to solve because they are small, we can rely on abstract problems belonging to provably tractable fragments of optimal planning. The key point is that, at least theoretically, moving to implicit abstractions removes the requirement on the abstractions size to be small. Our contribution, however, is in showing that implicit abstractions are far from being of theoretical interest only. Specifically,\n1. We specify acyclic causal-graph decompositions, a general framework for additive implicit abstractions that is based on decomposing the problem at hand along its causal graph. We then introduce a concrete family of such abstractions, called fork decompositions, that are based on two novel fragments of tractable cost-optimal planning. Following the type of analysis suggested by Helmert and Mattmu\u0308ller (2008), we formally analyze the asymptotic performance ratio of the fork-decomposition heuristics and prove that their worst-case accuracy on selected domains is comparable with that of (even parametric) state-of-the-art admissible heuristics. We then empirically evaluate the accuracy of the fork-decomposition heuristics on a large set of domains from recent planning competitions and show that their accuracy is competitive with the state of the art.\n1. This does not necessarily apply to symbolic PDBs which, on some tasks, may exponentially reduce the PDB\u2019s representation (Edelkamp, 2002).\n2. The key attraction of explicit abstractions is that state-to-goal costs in the abstract space can be precomputed and stored in memory in a preprocessing phase so that heuristic evaluation during search can be done by a simple lookup. A necessary condition for this would seem to be the small size of the abstract space. However, we show that an equivalent of the PDB and merge-and-shrink\u2019s notion of \u201cdatabase\u201d exists for the fork-decomposition abstractions as well, despite the exponential-size abstract spaces of the latter. These databased implicit abstractions are based on a proper partitioning of the heuristic computation into parts that can be shared between search states and parts that must be computed online per state. Our empirical evaluation shows that A\u2217 equipped with the \u201cdatabased\u201d fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.\nThis work is a revision and extension of the formulation and results presented by Katz and Domshlak (2008, 2009), which in turn is based on ideas first sketched also by Katz and Domshlak (2007a)."}, {"heading": "2. Preliminaries", "text": "We consider classical planning tasks corresponding to state models with a single initial state and only deterministic actions. Specifically, we consider state models captured by the sas+\nformalism (Ba\u0308ckstro\u0308m & Nebel, 1995) with nonnegative action costs. Such a planning task is given by a quintuple \u03a0 = \u3008V,A, I,G, cost\u3009, where:\n\u2022 V is a set of state variables, with each v \u2208 V being associated with a finite domain D(v). For a subset of variables V \u2032 \u2286 V , we denote the set of assignments to V \u2032 by D(V \u2032) = \u00d7v\u2208V \u2032D(v). Each complete assignment to V is called a state, and S = D(V ) is the state space of \u03a0. I is an initial state. The goal G is a partial assignment to V ; a state s is a goal state iff G \u2286 s.\n\u2022 A is a finite set of actions. Each action a is a pair \u3008pre(a), eff(a)\u3009 of partial assignments to V called preconditions and effects, respectively. By Av \u2286 A we denote the actions affecting the value of v. cost : A \u2192 R0+ is a real-valued, nonnegative action cost function.\nFor a variable v and a value \u03d1 \u2208 D(v), instantiation of v by \u03d1 is denoted by v : \u03d1. For a partial assignment p, V(p) \u2286 V denotes the subset of state variables instantiated by p. In turn, for any V \u2032 \u2286 V(p), by p[V \u2032] we denote the value of V \u2032 in p; if V \u2032 = {v} is a singleton, we use p[v] for p[V \u2032]. For any sequence of actions \u03c1 and variable v \u2208 V , by \u03c1\u2193v we denote the restriction of \u03c1 to actions changing the value of v; that is, \u03c1\u2193v is the maximal subsequence of \u03c1 consisting only of actions in Av.\nAn action a is applicable in a state s iff s[v] = pre(a)[v] for all v \u2208 V(pre(a)). Applying a changes the value of v \u2208 V(eff(a)) to eff(a)[v]. The resulting state is denoted by sJaK; by sJ\u3008a1, . . . , ak\u3009K we denote the state obtained from sequential application of the (respectively applicable) actions a1, . . . , ak starting at state s. Such an action sequence is an s-plan if G \u2286 sJ\u3008a1, . . . , ak\u3009K, and it is a cost-optimal (or, in what follows, optimal) s-plan if the sum of its action costs is minimal among all s-plans. The purpose of (optimal) planning is finding an (optimal) I-plan. For a pair of states s1, s2 \u2208 S, by cost(s1, s2) we refer to the\ngoal is to deliver p1 from C to G and p2 from F to E using the cars c1, c2, c3 and truck t, making sure that c3 ends up at F . The cars may only use city roads (thin edges); the truck may only use the highway (thick edge). Figures (b), (c), and (d) depict, respectively, the causal graph of the problem, the domain transition graphs (labels omitted) of c1 and c2 (left), t (center), and c3 (right), and the identical domain transition graphs of of p1 and p2.\ncost of a cost-optimal plan from s1 to s2; h \u2217(s) = mins\u2032\u2287G cost(s, s \u2032) is the custom notation for the cost of the optimal s-plan in \u03a0. Finally, important roles in what follows are played by a pair of standard graphical structures induced by planning tasks.\n\u2022 The causal graph CG(\u03a0) of \u03a0 is a digraph over nodes V . An arc (v, v\u2032) is in CG(\u03a0) iff v 6= v\u2032 and there exists an action a \u2208 A such that (v, v\u2032) \u2208 V(eff(a)) \u222a V(pre(a))\u00d7 V(eff(a)). In this case, we say that (v, v\u2032) is induced by a. By succ(v) and pred(v) we respectively denote the sets of immediate successors and predecessors of v in CG(\u03a0).\n\u2022 The domain transition graph DTG(v,\u03a0) of a variable v \u2208 V is an arc-labeled digraph over the nodes D(v) such that an arc (\u03d1, \u03d1\u2032) labeled with pre(a)[V \\ {v}] and cost(a) exists in the graph iff both eff(a)[v] = \u03d1\u2032, and either pre(a)[v] = \u03d1 or v 6\u2208 V(pre(a)).\nTo illustrate various constructs, we use a slight variation of a Logistics-style example from Helmert (2006). This example is depicted in Figure 1a, and in sas+ it has\nV = {p1, p2, c1, c2, c3, t} D(p1) = D(p2) = {A,B,C,D,E, F,G, c1, c2, c3, t} D(c1) = D(c2) = {A,B,C,D} D(c3) = {E,F,G} D(t) = {D,E}\nI = {p1 :C, p2 :F, t :E, c1 :A, c2 :B, c3 :G} G = {p1 :G, p2 :E, c3 :F},\nand actions corresponding to all possible loads and unloads, as well as single-segment movements of the vehicles. For instance, if action a captures loading p1 into c1 at C, then pre(a) = {p1 :C, c1 :C}, and eff(a) = {p1 : c1}. All actions in the example have unit cost. The causal graph of this example, as well as the domain transition graphs of the state variables, are depicted in Figures 1b-1d.\nHeuristic functions are used by informed-search procedures to estimate the cost (of the cheapest path) from a search node to the nearest goal node. Our focus here is on statedependent, admissible abstraction heuristics. A heuristic function h is state-dependent if its estimate for a search node depends only on the problem state associated with that node, that is, h : S \u2192 R0+ \u222a {\u221e}. Most heuristics in use these days are state-dependent (though see, e.g., Richter, Helmert, & Westphal, 2008 and Karpas & Domshlak, 2009 for a different case). A heuristic h is admissible if h(s) \u2264 h\u2217(s) for all states s. If h1 and h2 are two admissible heuristics, and h2(s) \u2264 h1(s) for all states s, we say that h1 dominates h2.\nFor any set of admissible heuristics h1, . . . , hm, their pointwise maximum is always an admissible heuristic, dominating each individual heuristic in the set. For some sets of admissible heuristics, their pointwise sum is also admissible and dominates their pointwise maximum. Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009). In particular, Katz and Domshlak (2007a, 2008) and Yang et al. (2007, 2008) independently introduced a general criterion for admissible additive ensembles of heuristics, called in the former work action cost partitioning. This criterion can be formalized as follows. Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task and {costi : A \u2192 R0+}mi=1 a family of cost functions such that \u2211m i=1 costi(a) \u2264 cost(a) for all actions a \u2208 A. If {hi}mi=1 is a set of arbitrary admissible heuristic functions for \u03a0i = \u3008V,A, I,G, costi\u3009, respectively, then \u2211m i=1 hi is also an admissible heuristic for \u03a0. The set of cost functions {costi}mi=1 can be seen as a partition of the action costs cost."}, {"heading": "3. Abstractions and Abstraction Heuristics", "text": "The semantics of any planning task \u03a0 is given by its induced state-transition model, often called the transition graph of \u03a0.\nDefinition 1 A transition graph is a tuple T = (S,L, Tr, s0, S?, $) where S is a finite set of states, L is a finite set of transition labels, Tr \u2286 S \u00d7 L \u00d7 S is a set of (labeled) transitions, s0 \u2208 S is an initial state, S? \u2286 S is a set of goal states, and $ : L\u2192 R0+ is a transition cost function.\n\u2022 For a state s \u2208 S and a subset of states S\u2032 \u2286 S in T, cost(s, S\u2032) is the cost (of a cheapest with respect to $ path) from s to a state in S\u2032 along the transitions of T; if no state in S\u2032 is reachable from s, then we have cost(s, S\u2032) =\u221e.\n\u2022 Any path from s0 to S? is a plan for T, and cheapest such plans are called optimal.\nThe states of the transition graph T(\u03a0) induced by a planning task \u03a0 = \u3008V,A, I,G, cost\u3009 are the states of \u03a0. The transition labels of T(\u03a0) are the actions A; there is a transition (s, a, sJaK) \u2208 Tr iff a is applicable in s; the initial state s0 = I; the set of goal states S? = {s \u2208 S | s \u2287 G}; and the transition cost function $ = cost. We now proceed with formally specifying the notion of abstraction. Our definition of abstraction resembles that of Prieditis (1993), and right from the beginning we specify a more general notion of additive abstraction. Informally, by additive abstraction we refer to a set of abstractions interconstrained by a requirement to jointly not overestimate the transition-path costs in the abstracted transition graph.\nDefinition 2 An additive abstraction of a transition graph T = (S,L, Tr, s0, S?, $) is a set of pairs {\u3008Ti, \u03b1i\u3009}mi=1 where, for 1 \u2264 i \u2264 m,\n\u2022 Ti = (Si, Li, Tri, s0i , S?i , $i) is a transition graph,\n\u2022 \u03b1i : S \u2192 Si is a function, called abstraction mapping, such that\n\u2013 \u03b1i(s 0) = s0i , \u03b1i(s) \u2208 S?i for all s \u2208 S?, and, \u2013 for all pairs of states s, s\u2032 \u2208 S holds m\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264 cost(s, s\u2032). (1)\nA few words on why we use this particular notion of abstraction. The term \u201cabstraction\u201d is usually associated with simplifying the original system, reducing and factoring out details less crucial in the given context. Which details can be reduced and which should better be preserved depends, of course, on the context. For instance, in the context of formal verification, the abstract transition graphs are required not to decrease the reachability between the states; that is, if there is a path from s to s\u2032 in the original transition graph, then there should be a path from \u03b1(s) to \u03b1(s\u2032) in the abstract transition graph (Clarke, Grumberg, & Peled, 1999). In addition, the reachability should also be increased as little as possible. Beyond that, the precise relationship between the path costs in the original and abstract transition graphs is only of secondary importance. In contrast, when abstractions are designed to induce admissible heuristic functions for heuristic search, the relationship between the path costs as captured by Eq. 1 is what must be obeyed. However, requirements above and beyond the general requirement of Eq. 1 not to overestimate the distances between\nthe states are unnecessary. Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (\u03b1(s), l, \u03b1(s\u2032)) if (s, l, s\u2032), with a weaker condition stated in Eq. 1. The reader, of course, may well ask whether the generality of the condition in Eq. 1 beyond the condition of Helmert et al. (2007) really delivers any practical gain, and later we show that the answer to this question is affirmative. For now, we proceed with adding further requirements essential to making abstraction usable as a basis for heuristic functions.\nDefinition 3 Let \u03a0 be a planning task over states S, and let {\u3008Ti, \u03b1i\u3009}mi=1 be an additive abstraction of the transition graph T(\u03a0). If m = O(poly(||\u03a0||)) and, for all states s \u2208 S and all 1 \u2264 i \u2264 m, the cost cost(\u03b1i(s), S?i ) in Ti is computable in time O(poly(||\u03a0||)), then hA(s) = \u2211m i=1 cost(\u03b1i(s), S ? i ) is an abstraction heuristic function for \u03a0.\nNote that admissibility of hA is implied by the cost conservation condition of Eq. 1. To further illustrate the connection between abstractions and admissible heuristics, consider three well-known mechanisms for devising admissible planning heuristics: delete relaxation (Bonet & Geffner, 2001), critical-path relaxation (Haslum & Geffner, 2000),2 and pattern database heuristics (Edelkamp, 2001).\nFirst, while typically not considered this way, the delete relaxation of a planning task \u03a0 = \u3008V,A, I,G, cost\u3009 does correspond to an abstraction \u3008T+ = (S+, L+, Tr+, s0+, S?+, $+), \u03b1+\u3009 of the transition graph T(\u03a0). Assuming unique naming of the variable values in \u03a0 and denoting D+ = \u22c3 v\u2208V D(v), we have the abstract states S+ being the power-set of D+, and the labels L+ = {a, a+ | a \u2208 A}. The transitions come from two sources: for each abstract state s+ \u2208 S+ and each original action a \u2208 A applicable in s+, we have both (s+, a, s+JaK) \u2208 Tr+ and (s+, a+, s+ \u222a eff(a)) \u2208 Tr+. With a minor abuse of notation, the initial state and the goal states of the abstraction are s0+ = I and S ? + = {s+ \u2208 S+ | s+ \u2287 G}, and the abstraction mapping \u03b1+ is simply the identity function. It is easy to show that, for any state s of our planning task \u03a0, we have cost(\u03b1+(s), S ? +) = h\n+(s), where h+(s) is the delete-relaxation estimate of the cost from s to the goal. As an aside, we note that this \u201cdelete-relaxation abstraction\u201d \u3008T+, \u03b1+\u3009 in particular exemplifies that nothing in Definition 2 requires the size of the abstract state space to be limited by the size of the original state space. In any event, however, the abstraction \u3008T+, \u03b1+\u3009 does not induce a heuristic in terms of Definition 3 because computing h+(s) is known to be NP-hard (Bylander, 1994).\nThe situation for critical-path relaxation is exactly the opposite. While computing the corresponding family of admissible estimates hm is polynomial-time for any fixed m, this computation is not based on computing the shortest paths in an abstraction of the planning task. The state graph over which hm is computed is an AND/OR-graph (and not an OR-graph such as transition graphs), and the actual computation of hm corresponds to computing a critical tree (and not a shortest path) to the goal. To the best of our knowledge, the precise relation between critical path and abstraction heuristics is currently an open question (Helmert & Domshlak, 2009).\nOverall, the only abstraction heuristics in the toolbox of planning these days appear to be the explicit homomorphism abstractions, whose best-known representative is probably\n2. We assume the reader is familiar with these two relaxations. If not, their discussion here can be safely skipped.\nthe pattern database (PDB) heuristics. Given a planning task \u03a0 over state variables V , a PDB heuristic is based on projecting \u03a0 onto a subset of its variables V \u03b1 \u2286 V . Such a homomorphism abstraction \u03b1 maps two states s1, s2 \u2208 S into the same abstract state iff s1[V \u03b1] = s2[V \u03b1]. Inspired by the (similarly named) domain-specific heuristics for search problems such as (n2 \u2212 1)-puzzles or Rubik\u2019s Cube (Culberson & Schaeffer, 1998; Hernadvo\u0308lgyi & Holte, 1999; Felner et al., 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space S\u03b1 and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in S\u03b1 by exhaustive search. Moreover, an O(1) bound on |S\u03b1| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |S\u03b1| implies a fixed-dimensionality constraint |V \u03b1| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {\u03a0i}\u221ei=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in \u03a0i can be transported by some \u0398(i) vehicles, then starting from some i, h\u03b1 will not account at all for movements of vehicles essential for solving \u03a0i (Helmert & Mattmu\u0308ller, 2008).\nAiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dra\u0308ger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning. MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of states to be contracted. The problem\u2019s state space is viewed as the synchronized product of its projections onto the single state variables. Starting with all such \u201catomic\u201d abstractions, this product can be computed by iteratively composing two abstract spaces, replacing them with their product. While in a PDB the size of the abstract space S\u03b1 is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites. Helmert et al. (2007) have proposed a concrete strategy for this interleaved abstraction/refinement scheme and empirically demonstrated the power of the merge-and-shrink abstraction heuristics. Like PDBs, however, MS abstractions are explicit abstractions, and thus computing their heuristic values is also based on explicitly searching for optimal plans in the abstract spaces. Hence, while merge-and-shrink abstractions escape the fixed-dimensionality constraint of PDBs, the constraint on the abstract space to be of a fixed size still holds."}, {"heading": "4. Implicit Abstractions", "text": "Focusing on the O(1) bound posted by explicit abstractions on the size of the abstract space, our first observation is that explicit abstractions are not necessarily the only way to proceed with abstraction heuristics. Given a planning task \u03a0 over states S, suppose we can transform it into a different planning task \u03a0\u03b1 such that\n1. the transformation induces an abstraction mapping \u03b1 : S \u2192 S\u03b1 where S\u03b1 is the state space of \u03a0\u03b1, and\n2. both the transformation of \u03a0 to \u03a0\u03b1, as well as computing \u03b1 for any state in S, can be done in time polynomial in ||\u03a0||.\nHaving such planning-task-to-planning-task transformations in mind, we define what we call (additive) implicit abstractions.\nDefinition 4 An additive implicit abstraction of a planning task \u03a0 is a set of pairs A = {\u3008\u03a0i, \u03b1i\u3009}mi=1 such that {\u03a0i}mi=1 are some planning tasks and {\u3008T(\u03a0i), \u03b1i\u3009}mi=1 is an additive abstraction of T(\u03a0).\nLet us now examine the notion of implicit abstractions more closely. First, implicit abstractions allow for a natural additive combination of admissible heuristics for the abstract tasks. This composition is formulated below by Theorem 1, extending the original criterion for admissibility of additive heuristics described in Section 2. Second, as formulated by Theorem 2, implicit abstractions can be composed via the functional composition of their abstraction mappings. These two easy-to-prove properties of implicit abstractions allow us then to take the desired step from implicit abstractions to implicit abstraction heuristics.\nTheorem 1 (Admissibility) Let \u03a0 be a planning task and A = {\u3008\u03a0i, \u03b1i\u3009}mi=1 be an additive implicit abstraction of \u03a0. If, for each 1 \u2264 i \u2264 m, hi is an admissible heuristic for \u03a0i, then the function h(s) = \u2211m i=1 hi(\u03b1i(s)) is an admissible heuristic for \u03a0.\nProof: The proof is straightforward. Let T = (S,L, Tr, s0, S?, $) be the transition graph of \u03a0, and let s be some state in S. For each 1 \u2264 i \u2264 m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of \u03a0i.\nFirst, if hi is an admissible heuristic for \u03a0i, then for all si \u2208 S?i ,\nhi(\u03b1i(s)) \u2264 cost(\u03b1i(s), si).\nNow, for each state s\u2032 \u2208 S?, from Definition 2 we have \u03b1i(s\u2032) \u2208 S?i , and from Eq. 1 we have m\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264 cost(s, s\u2032),\nand thus\nh(s) = m\u2211 i=1 hi(\u03b1i(s)) \u2264 m\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264 cost(s, s\u2032),\ngiving us an admissible estimate for h\u2217(s).\nTheorem 2 (Composition) Let \u03a0 be a planning task and A = {\u3008\u03a0i, \u03b1i\u3009}mi=1 be an additive implicit abstraction of \u03a0. If, for each 1 \u2264 i \u2264 m, Ai = {\u3008\u03a0i,j , \u03b1i,j\u3009}mij=1 is an additive implicit abstraction of \u03a0i, then A\u2032 = \u22c3m i=1{\u3008\u03a0i,j , \u03b1i,j \u25e6 \u03b1i\u3009}mij=1 is an additive implicit abstraction of \u03a0.\nProof: Let T = (S,L, Tr, s0, S?, $) be the transition graph of \u03a0. For each 1 \u2264 i \u2264 m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of \u03a0i, and for each 1 \u2264 j \u2264 mi, let Ti,j = (Si,j , Li,j , Tri,j , s0i,j , S?i,j , $i,j) be the transition graph of \u03a0i,j . We need to show that \u03b1i,j \u25e6 \u03b1i is an abstraction mapping as in Definition 2. From \u03b1i and \u03b1i,j being abstraction mappings, we have\n\u2022 s0i,j = \u03b1i,j(s0i ) = \u03b1i,j(\u03b1i(s0)) = \u03b1i,j \u25e6 \u03b1i(s0),\n\u2022 for all s \u2208 S? we have \u03b1i(s) \u2208 S?i and thus \u03b1i,j(\u03b1i(s)) = \u03b1i,j \u25e6 \u03b1i(s) \u2208 S?i,j , and \u2022 for all si, s\u2032i \u2208 Si, cost(si, s\u2032i) \u2265 \u2211mi j=1 cost(\u03b1i,j(si), \u03b1i,j(s \u2032 i)), and thus for all s, s \u2032 \u2208 S,\ncost(s, s\u2032) \u2265 m\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2265 m\u2211 i=1 mi\u2211 j=1 cost(\u03b1i,j(\u03b1i(s)), \u03b1i,j(\u03b1i(s \u2032)))\n= m\u2211 i=1 mi\u2211 j=1 cost(\u03b1i,j \u25e6 \u03b1i(s), \u03b1i,j \u25e6 \u03b1i(s\u2032)).\nTogether, Theorems 1 and 2 suggest the following scheme for deriving abstraction heuristics. Given an additive implicit abstraction A = {\u3008\u03a0i, \u03b1i\u3009}mi=1, if all its individual abstract tasks belong to some tractable fragments of optimal planning, then we can use in practice the (sum of the) true costs in all \u03a0i as the admissible estimates for the costs in \u03a0. Otherwise, if optimal planning for some abstract tasks \u03a0i in A cannot be proven polynomial-time solvable, then we can further abstract these tasks, obtaining admissible estimates for the true costs in \u03a0i.\nDefinition 5 Let \u03a0 be a planning task over states S, and let A = {\u3008\u03a0i, \u03b1i\u3009}mi=1 be an additive implicit abstraction of \u03a0. If m = O(poly(||\u03a0||)), and, for all states s \u2208 S and all 1 \u2264 i \u2264 m, h\u2217(\u03b1i(s)) is polynomial-time computable, then hA(s) = \u2211m i=1 h\n\u2217(\u03b1i(s)) is an implicit abstraction heuristic function for \u03a0.\nCompared to explicit abstraction heuristics such as PDB heuristics and merge-andshrink heuristics, the direction of implicit abstraction heuristics is, at least in principle, appealing because neither the dimensionality nor even the size of the state spaces induced by implicit abstractions are required to be bounded by something restrictive, if at all. The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (Ba\u0308ckstro\u0308m & Nebel, 1995; Bylander, 1994; Jonsson & Ba\u0308ckstro\u0308m, 1998; Jonsson, 2007; Katz & Domshlak, 2007b). In fact, none so far has appeared to us very convenient for automatically devising useful problem transformations as above. Fortunately, we show next that the boundaries of tractability can be expanded in the right way, allowing us to successfully materialize the idea of implicit abstraction heuristics.\nIn the following, a key role is played by the causal graphs induced by the planning tasks. Informally, the basic idea behind what we call causal-graph decompositions is to abstract the given planning task \u03a0 along a subgraph of \u03a0\u2019s causal graph, with the goal of obtaining abstract problems of specific structure. Naturally, there are numerous possibilities for obtaining such structure-oriented abstractions. We now present one such decomposition that is tailored to abstractions around acyclic subgraphs. Informally, this decomposition can be seen as a sequential application of two kinds of task transformations: dropping preconditions (Pearl, 1984) and (certain form of) breaking actions with conjunctive effects into single-effect actions.\nDefinition 6 Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task, and let G = (VG , EG) be an acyclic subgraph of the causal graph CG(\u03a0). A planning task \u03a0G = \u3008VG , AG , IG , GG , costG\u3009 is an acyclic causal-graph decomposition of \u03a0 with respect to G if\n1. IG = I[VG ], GG = G[VG ], 2. AG = \u22c3 a\u2208AAG(a) where each AG(a) = {a1, . . . , al(a)} is a set of actions over VG\nsuch that, for a topological with respect to G ordering of the variables {v1, . . . , vl(a)} = V(eff(a)) \u2229 VG, and 1 \u2264 i \u2264 l(a),\neff(ai)[v] =\n{ eff(a)[v], v = vi\nunspecified, otherwise\npre(ai)[v] =  pre(a)[v], (v, vi) \u2208 EG \u2227 v 6\u2208 V(eff(a)) or v = vi eff(a)[v], (v, vi) \u2208 EG \u2227 v \u2208 V(eff(a)) unspecified, otherwise\n(2)\n3. For each action a \u2208 A, \u2211 a\u2032\u2208AG(a) costG(a \u2032) \u2264 cost(a). (3)\nIt is not hard to verify from Definition 6 that for any planning task \u03a0 and any acyclic causal-graph decomposition \u03a0G of \u03a0, the causal graph CG(\u03a0G) is exactly the subgraph G underlying the decomposition. To illustrate the notion of acyclic causal-graph decomposition, we consider a planning task \u03a0 = \u3008V,A, I,G, cost\u3009 over five state variables V = {u, v, x, y, z}, two unit-cost actions A = {a1, a2} as in Figure 2a, initial state I = {u :0, v :0, x :0, y :0, z :0}, and goal G = {u : 1, v : 1, x : 0, y : 1, z : 1}. The causal graph CG(\u03a0) is depicted in Figure 2a. Figures 2b-c show two subgraphs G1 and G2 of CG(\u03a0), respectively, as well as the action sets AG1(a1) = {a11, a21, a31} and AG1(a2) = {a12, a22, a32} in Figure 2(b), and the action sets AG2(a1) = {a11, a21, a31} and AG2(a2) = {a12, a22, a32} in Figure 2(c). For i \u2208 {1, 2}, let \u03a0i = \u3008V,Ai, I, G, costi\u3009 be the planning task with Ai = AGi(a1)\u222aAGi(a2) and costi(a) = 1/3 for all a \u2208 Ai. These two planning tasks \u03a0i (individually) satisfy the conditions of Definition 6 with respect to \u03a0 and Gi, and thus they are acyclic causal-graph decompositions of \u03a0 with respect to Gi.\nWe now proceed with specifying implicit abstractions defined via acyclic causal-graph decompositions.\nDefinition 7 Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task over states S, and let G = {Gi = (VGi , EGi)}mi=1 be a set of acyclic subgraphs of the causal graph CG(\u03a0). A = {\u3008\u03a0Gi , \u03b1i\u3009}mi=1 is an acyclic causal-graph abstraction of \u03a0 over G if, for some set of cost functions {costi : A\u2192 R0+}mi=1 satisfying\n\u2200a \u2208 A : m\u2211 i=1 costi(a) \u2264 cost(a), (4)\nwe have, for 1 \u2264 i \u2264 m,\n\u2022 \u03a0Gi = \u3008VGi , AGi , IGi , GGi , costGi\u3009 is an acyclic causal-graph decomposition of \u03a0i = \u3008V,A, I,G, costi\u3009 with respect to Gi, and\n\u2022 the abstraction mapping \u03b1i : S \u2192 Si is the projection mapping \u03b1i(s) = s[VGi ].\nTheorem 3 Acyclic causal-graph abstractions of the planning tasks are additive implicit abstractions of these tasks.\nProof: Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task, and let A = {\u3008\u03a0Gi , \u03b1i\u3009}mi=1 be an acyclic causal-graph abstraction of \u03a0 over a set of subgraphs G = {Gi = (VGi , EGi)}mi=1. Let T = (S,L, Tr, s0, S?, $) be the transition graph of \u03a0, and, for 1 \u2264 i \u2264 m, Ti = (Si, Li, Tri, s 0 i , S ? i , $i) be the transition graph of \u03a0Gi . We need to show that \u03b1i is an abstraction mapping as in Definition 2. First, from Definitions 6 and 7, we have\n\u2022 s0i = IGi = I[VGi ] = s0[VGi ] = \u03b1i(s0), and\n\u2022 for all s \u2208 S? we have s \u2287 G and thus \u03b1i(s) = s[VGi ] \u2287 G[VGi ] = GGi , providing us with \u03b1i(s) \u2208 S?i .\nNow, if s is a state of \u03a0 and a \u2208 A is an action with pre(a) \u2286 s, then \u03b1i(s) is a state of \u03a0Gi and pre(a)[VGi ] \u2286 \u03b1i(s). Let the action sequence \u03c1 = \u3008a1, a2, . . . , al(a)\u3009 be constructed from a as in Eq. 2. We inductively prove that \u03c1 is applicable in \u03b1i(s). First, for each v \u2208 VGi , either pre(a1)[v] = pre(a)[v], or pre(a1)[v] is unspecified, and thus \u03c11 = \u3008a1\u3009 is applicable in \u03b1i(s). The inductive hypothesis is now that \u03c1j = \u3008a1, a2, . . . , aj\u3009 is applicable in \u03b1i(s), and let s\u2032 = \u03b1i(s)J\u03c1jK. From Eq. 2, for each 1 \u2264 j\u2032 \u2264 j, aj\u2032 changes the value of vj\u2032 to eff(a)[vj\u2032 ],\nand that is the only change of vj\u2032 along \u03c1j . Likewise, since all the actions constructed as in Eq. 2 are unary-effect, {v1, . . . , vj} are the only variables in VGi affected along \u03c1j . Hence, for all v \u2208 VGi , if v = vj\u2032 , 1 \u2264 j\u2032 \u2264 j, then s\u2032[v] = eff(a)[v] = pre(aj+1)[v], and otherwise, s\u2032[v] = \u03b1i(s)[v], and if pre(a\nj+1)[v] is specified, then pre(aj+1)[v] = pre(a)[v] = \u03b1i(s)[v]. This implies that aj+1 is applicable in s\u2032 and, as a result, \u03c1j+1 = \u3008a1, a2, . . . , aj+1\u3009 is applicable in \u03b1i(s), finalizing the inductive proof. Likewise, exactly the same arguments on the affect of {aj}l(a)j=1 on \u03b1i(s) immediately imply that, if \u03c1 = \u3008a1, a2, . . . , al(a)\u3009, then \u03b1i(sJaK) = \u03b1i(s)J\u03c1K. Next, for each a \u2208 A, from Eqs. 3 and 4 we have\nm\u2211 i=1 \u2211 a\u2032\u2208AGi (a) costGi(a \u2032) \u2264 m\u2211 i=1 costi(a) \u2264 cost(a). (5)\nNow, let s, s\u2032 \u2208 S be a pair of original states such that cost(s, s\u2032) < \u221e, and let % = \u3008a1, . . . , ak\u3009 be the sequence of labels along a cheapest path from s to s\u2032 in T. From that, cost(s, s\u2032) = cost(%) = \u2211k j=1 cost(aj). The decomposition of such a path to the sequences of actions as in Eq. 2 is a (not neccesarily cheapest) path from \u03b1i(s) to \u03b1i(s \u2032) in Ti, and\nthus cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264\u2211kj=1\u2211a\u2032\u2208AGi (aj) costGi(a\u2032), providing us with\nm\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264 m\u2211 i=1 k\u2211 j=1 \u2211 a\u2032\u2208AGi (aj) costGi(a \u2032) = k\u2211 j=1 m\u2211 i=1 \u2211 a\u2032\u2208AGi (aj) costGi(a \u2032)\n(5) \u2264 k\u2211 j=1 cost(aj) = cost(s, s \u2032).\nThus, if we can decompose the given task \u03a0 into a set of tractable acyclic causalgraph decompositions \u03a0 = {\u03a0G1 , . . . ,\u03a0Gm}, then we can solve all these tasks in polynomial time, and derive an additive admissible heuristic for \u03a0. Before we proceed with considering concrete acyclic causal-graph decomposition, note that Definition 2 leaves the decision about the actual partition of the action costs rather open. In what follows we adopt the most straightforward, uniform action cost partition in which the cost of each action a is equally split among all the non-redundant representatives of a in \u22c3m i=1AGi(a). However, a better choice of action cost partition can sometimes be made. In fact, sometimes it can even be optimized (Katz & Domshlak, 2010)"}, {"heading": "5. Fork Decompositions", "text": "We now proceed with introducing two concrete acyclic causal-graph decompositions that, when combined with certain variable domain abstractions, provide us with implicit abstraction heuristics. These so called fork-decomposition heuristics are based on two novel fragments of tractable cost-optimal planning for tasks with fork and inverted-fork structured causal graphs.\nDefinition 8 For a planning task \u03a0 over variables V , and a variable v \u2208 V ,\n(1) v-fork of \u03a0 is the subgraph Gfv of CG(\u03a0) over nodes VGfv = {v} \u222a succ(v) and edges EGfv = {(v, u) | u \u2208 succ(v)}, and\n(2) v-ifork (short for inverted fork) of \u03a0 is a subgraph G iv of CG(\u03a0) over nodes VG iv = {v} \u222a pred(v) and edges EG iv = {(u, v) | u \u2208 pred(v)}.\nThe sets of all v-forks and all v-iforks of \u03a0 are denoted by GF = {Gfv}v\u2208V and GI = {G iv}v\u2208V , respectively.\nFor any planning task and each of its state variables v, both v-fork and v-ifork are acyclic digraphs, allowing us to define our three implicit abstractions as follows.\nDefinition 9 For any planning task \u03a0 = \u3008V,A, I,G, cost\u3009, (1) any acyclic causal-graph abstraction AF = {\u3008\u03a0fv, \u03b1fv\u3009}v\u2208V of \u03a0 over GF is called\nF-abstraction, and the set of abstract planning tasks \u03a0F = {\u03a0fv}v\u2208V is called F-decomposition of \u03a0;\n(2) any acyclic causal-graph abstraction AI = {\u3008\u03a0iv, \u03b1iv\u3009}v\u2208V of \u03a0 over GI is called I-abstraction, and the set of abstract planning tasks \u03a0I = {\u03a0iv}v\u2208V is called I-decomposition of \u03a0;\n(3) any acyclic causal-graph abstraction AFI = {\u3008\u03a0fv, \u03b1fv\u3009, \u3008\u03a0iv, \u03b1iv\u3009}v\u2208V of \u03a0 over GFI = GF \u222aGI is called FI-abstraction, and the set of abstract planning tasks \u03a0FI = {\u03a0fv,\u03a0iv}v\u2208V is called FI-decomposition of \u03a0. Definition 9 can be better understood by considering the FI-abstraction of the problem\n\u03a0 from our Logistics example; Figure 3 schematically illustrates the process. To simplify the example, here we as if eliminate from GFI all the single-node subgraphs, obtaining\nAFI = {\u3008\u03a0fc1 , \u03b1fc1\u3009, {\u3008\u03a0fc2 , \u03b1fc2\u3009, {\u3008\u03a0fc3 , \u03b1fc3\u3009, {\u3008\u03a0ft, \u03b1ft\u3009, {\u3008\u03a0ip1 , \u03b1ip1\u3009, {\u3008\u03a0ip2 , \u03b1ip2\u3009}.\nConsidering the action sets of the problems in \u03a0FI = {\u03a0fc1 ,\u03a0fc2 ,\u03a0fc3 ,\u03a0ft,\u03a0ip1 ,\u03a0ip2}, we see that each original driving action has one nonredundant (that is, \u201cchanging some variable\u201d) representative in three of the abstract planning tasks, while each load/unload action has one nonredundant representative in five of these tasks. For instance, the action drive-c1from-A-to-D has one nonredundant representative in each of the tasks {\u03a0fc1 ,\u03a0ip1 ,\u03a0ip2}, and the action load-p1-into-c1-at-A has one nonredundant representative in each of the tasks {\u03a0fc1 ,\u03a0fc2 ,\u03a0fc3 ,\u03a0ft,\u03a0ip1}. Since we assume a uniform partition of the action costs, the cost of each driving and load/unload action in each relevant abstract planning task is thus set to 1/3 and 1/5, respectively. From Theorem 3 we have AFI being an additive implicit abstraction of \u03a0, and from Theorem 1 we then have\nhFI = \u2211 v\u2208V ( h\u2217\u03a0fv + h\u2217\u03a0iv ) , (6)\nbeing an admissible estimate of h\u2217 in \u03a0. The question now is how good this estimate is. The optimal cost of solving our running example is 19. Taking as a reference the well-known admissible heuristics hmax (Bonet & Geffner, 2001) and h\n2 (Haslum & Geffner, 2000), we have hmax(I) = 8 and h\n2(I) = 13. Considering our FI-abstraction, the optimal plans for the tasks in \u03a0FI are as follows.\n\u03a0fc1 : load-p1-into-c2-at-C, unload-p1-from-c2-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\n\u03a0fc2 : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\n\u03a0fc3 : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, drive-c3-from-G-to-E, load-p1-into-c3-at-E, drive-c3-from-E-to-G, unload-p1-from-c3-at-G, drive-c3-from-G-to-E, drive-c3-from-E-to-F, load-p2-into-c3-at-F, drive-c3-from-F-to-E, unload-p2-from-c3-at-E, drive-c3-from-E-to-F.\n\u03a0ft : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, drive-t-from-E-to-D, load-p1-into-t-at-D, drive-t-from-D-to-E, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\n\u03a0ip1 : drive-c1-from-A-to-D, drive-c1-from-D-to-C, load-p1-into-c1-at-C, drive-c1-from-C-to-D, unload-p1-from-c1-at-D, drive-t-from-E-to-D, load-p1-into-t-at-D, drive-t-from-D-to-E, unload-p1-from-t-at-E, drive-c3-from-G-to-E, load-p1-into-c3-at-E, drive-c3-from-E-to-G, unload-p1-from-c3-at-G, drive-c3-from-G-to-E, drive-c3-from-E-to-F.\n\u03a0ip2 : drive-c3-from-G-to-E, drive-c3-from-E-to-F, load-p2-into-c3-at-F, drive-c3-from-F-to-E, unload-p2-from-c3-at-E, drive-c3-from-E-to-F.\nHence, we have\nhFI = h\u2217 \u03a0fc1 + h\u2217 \u03a0fc2 + h\u2217 \u03a0fc3 + h\u2217 \u03a0ft\n+ h\u2217 \u03a0ip1 + h\u2217 \u03a0fp2\n= 85 + 8 5 + 8 5 + 6 3 + 8 5 + 2 3 + 6 5 + 9 3 + 2 5 + 4 3 = 15,\n(7)\nand so hFI appears at least promising.\nUnfortunately, despite the seeming simplicity of the planning tasks in \u03a0FI, it turns out that implicit fork-decomposition abstractions as in Definitions 9 do not fit the requirements of implicit abstraction heuristics as in Definition 5. The causal graphs of the planning tasks in \u03a0F and \u03a0I form directed forks and directed inverted forks, respectively, and, in general, the number of variables in each such planning task can be as large as \u0398(|V |). The problem is that even satisficing planning for sas+ fragments with fork and inverted fork causal graphs is NP-complete (Domshlak & Dinitz, 2001). In fact, recent results by Chen and Gimenez (2008) show that planning for any sas+ fragment characterized by any nontrivial form of causal graph is NP-hard. Moreover, even if the domain transition graphs of all the state variables are strongly connected (as in our example), optimal planning for fork and inverted fork structured problems remain NP-hard (see Helmert 2003, and 2004 for the respective results). Next, however, we show that this is not the end of the story for fork decompositions.\nWhile the hardness of optimal planning for problems with fork and inverted fork causal graphs casts a shadow on the relevance of fork decompositions, a closer look at the proofs of the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003, 2004) reveals that they in particular rely on root variables having large domains. Exploiting this observation, we now show that this reliance is not incidental and characterize two substantial islands of tractability within the structural fragments of sas+.\nTheorem 4 (Tractable Forks) Given a planning task \u03a0 = \u3008V,A, I,G, cost\u3009 with a fork causal graph rooted at r \u2208 V , if |D(r)| = 2, the time complexity of the cost-optimal planning for \u03a0 is polynomial in ||\u03a0||.\nProof: Observe that, for any planning task \u03a0 as in the theorem, the fork structure of the causal graph CG(\u03a0) implies that all the actions in \u03a0 are unary-effect, and each leaf variable v \u2208 succ(r) preconditions only the actions affecting v itself. The algorithm below is based on the following three properties satisfied by the optimal plans \u03c1 for \u03a0.\n(i) For any leaf variable v \u2208 succ(r), the path \u03c1\u2193v from I[v] to G[v] induced by \u03c1 in DTG(v,\u03a0) is either cycle-free or contains only zero-cost cycles. This is the case because otherwise all the nonzero-cost cycles can be eliminated from \u03c1\u2193v while preserving its validity, violating the assumed optimality of \u03c1. Without loss of generality, in what follows we assume that this path \u03c1\u2193v in DTG(v,\u03a0) is cycle-free; in the case of fork causal graphs, we can always select an optimal \u03c1 that satisfies this requirement for all v \u2208 succ(r). Thus, we have |\u03c1\u2193v | \u2264 |D(v)| \u2212 1.\n(ii) Having fixed a sequence of value changes of r, the fork\u2019s leaves become mutually independent; that is, our ability to change the value of one of them does not affect our ability to change the value of all the others.\n(iii) Because r is binary-valued, if v \u2208 V \\ {r} is the \u201cmost demanding\u201d leaf variable in terms of the number of value changes required from r by the action preconditions along \u03c1\u2193v , then these are the only value changes of r along \u03c1, except for, possibly, a final value change to G[r]. Thus, in particular, we have |\u03c1\u2193r | \u2264 maxv\u2208succ(r) |D(v)|.\nWe begin with introducing some auxiliary notations. With |D(r)| = 2, let D(r) = {0, 1} with I[r] = 0. Let \u03c3(r) be an alternating 0/1 sequence starting with 0, and having 0 in all odd and 1 in all even positions. This sequence \u03c3(r) is such that |\u03c3(r)| = 1 if no action in A can change r\u2019s value to 1, |\u03c3(r)| = 2 if some action can change r\u2019s value to 1 but no action can then restore it to value 0, and otherwise, |\u03c3(r)| = 1 + maxv\u2208succ(r) |D(v)|. Let [\u03c3(r)] be the set of all nonempty prefixes of \u03c3(r) if G[r] is unspecified; otherwise, let it be the set of all nonempty prefixes of \u03c3(r) ending with G[r]. Note that, if [\u03c3(r)] = \u2205, then the problem is trivially unsolvable; in what follows we assume this is not the case. For each v \u2208 succ(r), let DTG0v and DTG1v be the subgraphs of the domain transition graphs DTG(v,\u03a0), obtained by removing from DTG(v,\u03a0) all the arcs labeled with r : 1 and r : 0, respectively.\nThe algorithm below incrementally constructs a set R of valid plans for \u03a0, starting with R = \u2205.\n(1) For each v \u2208 succ(r), and each pair of v\u2019s values x, y \u2208 D(v), compute the cheapest (that is, cost-minimal) paths \u03c00v(x, y) and \u03c0 1 v(x, y) from x to y in DTG 0 v and DTG 1 v,\nrespectively. For some pairs of values x, y, one or even both these paths may, of course, not exist.\n(2) For each sequence \u03c3 \u2208 [\u03c3(r)], and each v \u2208 succ(r), construct a layered digraph Lv(\u03c3) with |\u03c3|+ 1 node layers L0, . . . , L|\u03c3|, where L0 consists of only I[v], and for 1 \u2264 i \u2264 |\u03c3|, Li consists of all nodes y \u2208 D(v) for which a path \u03c0\u03c3[i]v (x, y) from some node x \u2208 Li\u22121 has been constructed in step (1). For each x \u2208 Li\u22121, y \u2208 Li, Lv(\u03c3) contains an arc (x, y) weighted with cost(\u03c0 \u03c3[i] v (x, y)).\n(3) For each \u03c3 \u2208 [\u03c3(r)], let k = |\u03c3|. A candidate plan \u03c1\u03c3 for \u03a0 is constructed as follows.\n(a) For each v \u2208 succ(r), find a cost-minimal path from I[v] to G[v] in Lv(\u03c3). If no such path exists, then proceed with the next prefix in [\u03c3(r)]. Otherwise, note that the i-th edge on this path (taking us from some x \u2208 Li\u22121 to some y \u2208 Li) corresponds to the cost-minimal path \u03c0 \u03c3[i] v (x, y) from x to y. Let us denote this path from x to\ny by Siv.\n(b) SetR = R\u222a{\u03c1\u03c3}, where \u03c1\u03c3 = S1 \u00b7a\u03c3[2] \u00b7S2 \u00b7. . .\u00b7a\u03c3[k] \u00b7Sk, each sequence Si is obtained by an arbitrary merge of the sequences {Siv}v\u2208succ(r), and a\u03d1 is the cheapest action changing the value of r to value \u03d1.\n(4) If R = \u2205, then fail, otherwise return \u03c1 = argmin\u03c1\u03c3\u2208R cost(\u03c1\u03c3).\nIt is straightforward to verify that the complexity of the above procedure is polynomial in the description size of \u03a0. To prove correctness, we show that the procedure returns a plan for any solvable task \u03a0, and that the returned plan \u03c1\u2032 satisfies cost(\u03c1\u2032) \u2264 cost(\u03c1) for any optimal plan \u03c1 for \u03a0.\nGiven a solvable task \u03a0, let \u03c1 be an optimal plan for \u03a0 with all \u03c1\u2193v for the leaf variables v being cycle-free. Let \u03c1\u2193r = \u3008a2 . . . , ak\u3009; the numbering of actions along \u03c1\u2193r starts with a2 to simplify indexing later on. For each v \u2208 succ(r), the actions of \u03c1\u2193r divide \u03c1\u2193v into subsequences of v-changing actions \u03c1\u2193v = \u03c1 1 v \u00b7 . . . \u00b7 \u03c1kv , separated by the value changes required from r. That is, for each 1 \u2264 i \u2264 k, all actions in \u03c1iv are preconditioned by the same value of r, if any, and if two actions a \u2208 \u03c1iv and a\u2032 \u2208 \u03c1i+1v are preconditioned by r, then pre(a)[r] 6= pre(a\u2032)[r]. Let \u03c3 \u2208 [\u03c3(r)] be a value sequence such that |\u03c3| = k = |\u03c1\u2193r |+1. For each v \u2208 succ(r), \u03c1\u2193v is a path from I[v] to G[v] in Lv(\u03c3), and therefore some \u03c1\u03c3 is added into R by the algorithm, meaning that the algorithm finds a solution. Now, if \u03c1\u03c3 \u2208 R, then, for each v \u2208 succ(r), let S1v \u00b7 S2v \u00b7 . . . \u00b7 Skv be a cost-minimal path from I[v] to G[v] in Lv(\u03c3) such that Siv is the sequence of actions changing the value of v and preconditioned either by r :0 or nothing for odd i, and by r :1 or nothing for even i. Thus,\ncost(S1v \u00b7 S2v \u00b7 . . . \u00b7 Skv ) = k\u2211 i=1 cost(Siv) \u2264 cost(\u03c1\u2193v).\nBecause sequence Si is obtained by an arbitrary merge of the sequences {Siv}v\u2208succ(r), and a\u03d1 is the cheapest action changing the value of r to \u03d1, then \u03c1\u03c3 = S\n1 \u00b7 a\u03c3[2] \u00b7S2 \u00b7 . . . \u00b7 a\u03c3[k] \u00b7Sk is an applicable sequence of actions that achieves the goal values for each v \u2208 succ(r) as well as for r, and\ncost(\u03c1\u03c3) = cost(S 1 \u00b7 a\u03c3[2] \u00b7 S2 \u00b7 . . . \u00b7 a\u03c3[k] \u00b7 Sk) = k\u2211 i=2 cost(a\u03c3[i]) + k\u2211 i=1 cost(Si) \u2264\n\u2264cost(\u03c1\u2193r) + \u2211\nv\u2208succ(r)\ncost(\u03c1\u2193v) = cost(\u03c1).\nHence, if \u03a0 is solvable, then the algorithm returns a plan for \u03a0, and this plan must be optimal. Finally, if \u03a0 is not solvable, then R necessarily remains empty, and thus the algorithm fails.\nWhile Theorem 4 concerns the tractability tasks with fork-structured causal graphs and roots with binary domains, in our earlier work we also reported an additional tractability result for fork-structured causal graphs with the domains of all variables being of a fixed size, though not necessarily binary-valued (Katz & Domshlak, 2008). We do not discuss this result here in detail because, at least so far, we have not found it very helpful in the context of devising effective abstraction heuristics.\nTheorem 5 (Tractable Inverted Forks) Given a planning task \u03a0 = \u3008V,A, I,G, cost\u3009 with an inverted fork causal graph with sink r \u2208 V , if |D(r)| = O(1), the time complexity of the cost-optimal planning for \u03a0 is polynomial in ||\u03a0||.\nProof: Let |D(r)| = d. Observe that the inverted-fork structure of the causal graph CG(\u03a0) implies all the actions in \u03a0 are unary-effect, and that the sink r preconditions only the actions affecting r itself. Hence, in what follows we assume that G[r] is specified; otherwise\nGiven a path \u3008a1, . . . , am\u3009 from I[r] to G[r] in DTG(r,\u03a0): \u03c1 := \u3008\u3009 am+1 := \u3008G[pred(r)], \u2205\u3009 foreach v \u2208 pred(r) do xv := I[v] for i := 1 to m+ 1 do\nforeach v \u2208 pred(r) do if pre(ai)[v] is specified and pre(ai)[v] 6= xv then\nif pre(ai)[v] is not reachable from xv in DTG(v,\u03a0) then fail append to \u03c1 the actions induced by some cost-minimal path\nfrom pre(ai)[v] to xv in DTG(v,\u03a0) xv := pre(ai)[v]\nif i < m+ 1 then append to \u03c1 the action ai return \u03c1\nFigure 4: Detailed outline of step (3) of the planning algorithm for inverted-fork structured\ntask.\n\u03a0 breaks down to a set of trivial planning problems over a single variable each. Likewise, from the above properties of \u03a0 it follows that, if \u03c1 is an optimal plan for \u03a0, then the path \u03c1\u2193r from I[r] to G[r] induced by \u03c1 in DTG(r,\u03a0) is either cycle-free or contains only zerocost cycles. The latter can be safely eliminated from \u03c1, and thus we can assume that \u03c1\u2193r is cycle-free. Given that, a simple algorithm that finds a cost-optimal plan for \u03a0 in time \u0398(||\u03a0||d + ||\u03a0||3) is as follows.\n(1) Create all \u0398(|Ar|d\u22121) cycle-free paths from I[r] to G[r] in DTG(r,\u03a0).\n(2) For each variable v \u2208 pred(r), and each pair of v\u2019s values x, y \u2208 D(v), compute the cost-minimal path from x to y in DTG(v,\u03a0). The whole set of such cost-minimal paths can be computed using \u0398(d|V |) applications of the Floyd-Warshall algorithm on the domain transition graphs of the sink\u2019s parents pred(r).\n(3) For each I[r]-to-G[r] path in DTG(r,\u03a0) generated in step (1), construct a plan for \u03a0 based on that path for r, and the cheapest paths computed in (2). This simple construction, depicted in Figure 4, is possible because the values of each parent variable can be changed independently of the values of all other variables in the inverted fork.\n(4) Take the cheapest plan among those constructed in (3). If no plan was constructed in step (3), then \u03a0 is unsolvable.\nWe have already observed that, for each cost-optimal plan \u03c1, \u03c1\u2193r is one of the I[r]-to-G[r] paths generated in step (1). For each v \u2208 pred(r), let Sv denote the sequence of values from D(v) that is required by the preconditions of the actions along \u03c1\u2193r . For each v \u2208 pred(r), we have \u03c1\u2193v corresponding to a (possibly cyclic) path from I[v] to G[v] in DTG(v,\u03a0), traversing the values (= nodes) from Sv in the order required by Sv. In turn, the plan for \u03a0 generated in (3) consists of cost-minimal such paths for all v \u2208 pred(r). Therefore, at least one of the\nplans generated in (3) must be cost-optimal for \u03a0, and the minimization step (4) will select one of them.\nTheorems 4 and 5 clarify the gap between fork decompositions and implicit abstraction heuristics, and now we can bridge this gap by further abstracting each task in the given fork decomposition of \u03a0. We do that by abstracting domains of the fork roots and inverted-fork sinks to meet the requirements of the tractable fragments. We note that, in itself, the idea of domain decomposition is not very new in general (Hernadvo\u0308lgyi & Holte, 1999) and in domain-independent planning in particular (Domshlak, Hoffmann, & Sabharwal, 2009). In fact, the shrinking step of the algorithm for building the merge-and-shrink abstractions is precisely a variable domain abstraction for meta-variables constructed in the merging steps (Helmert et al., 2007).\nDefinition 10 Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task over states S, v \u2208 V be a state variable, and \u03a6 = {\u03c61, . . . , \u03c6m} be a set of mappings from D(v) to some sets \u03931, . . . ,\u0393m, respectively. A = {\u3008\u03a0\u03c6i , \u03b1i\u3009}mi=1 is a domain abstraction of \u03a0 over \u03a6 if, for some set of cost functions {costi : A\u2192 R0+}mi=1 satisfying\n\u2200a \u2208 A : m\u2211 i=1 costi(a) \u2264 cost(a), (8)\nwe have, for 1 \u2264 i \u2264 m,\n\u2022 the abstraction mapping \u03b1i of states S is\n\u2200u \u2208 V : \u03b1i(s)[u] = { \u03c6i(s[u]), u = v\ns[u], u 6= v ,\nand, extending \u03b1i to partial assignments on V \u2032 \u2286 V as \u03b1i(s[V \u2032]) = \u03b1i(s)[V \u2032],\n\u2022 \u03a0\u03c6i = \u3008V,A\u03c6i , I\u03c6i , G\u03c6i , cost\u03c6i\u3009 is a planning task with\n1. I\u03c6i = \u03b1i(I), G\u03c6i = \u03b1i(G),\n2. A\u03c6i = {a\u03c6i = \u3008\u03b1i(pre(a)), \u03b1i(eff(a))\u3009 | a \u2208 A}, and 3. for each action a \u2208 A,\ncost\u03c6i(a\u03c6i) = costi(a). (9)\nWe say that \u03a0\u03c6i is a domain decomposition of \u03a0i = \u3008V,A, I,G, costi\u3009 with respect to \u03c6i.\nTheorem 6 Domain abstractions of the planning tasks are additive implicit abstractions of these tasks.\nProof: Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task and A = {\u3008\u03a0\u03c6i , \u03b1i\u3009}mi=1 be a domain abstraction of \u03a0 over \u03a6 = {\u03c61, . . . , \u03c6m}. Let T = (S,L, Tr, s0, S?, $) be the transition graph of \u03a0. For each 1 \u2264 i \u2264 m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of \u03a0\u03c6i . We need to show that \u03b1i is an abstraction mapping as in Definition 2.\nFirst, from Definition 10 we have\n\u2022 s0i = I\u03c6i = \u03b1i(I) = \u03b1i(s0), and\n\u2022 for all s \u2208 S? we have s \u2287 G and thus \u03b1i(s) \u2287 \u03b1i(G) = G\u03c6i , providing us with \u03b1i(s) \u2208 S?i .\nNow, if s is a state of \u03a0 and a \u2208 A is an action with pre(a) \u2286 s, then \u03b1i(s) is a state of \u03a0\u03c6i and pre(a\u03c6i) = \u03b1i(pre(a)) \u2286 \u03b1i(s). Thus, a\u03c6i is applicable in \u03b1i(s), and now we show that applying a\u03c6i in \u03b1i(s) results in \u03b1i(s)Ja\u03c6iK = \u03b1i(sJaK).\n1. For the effect variables v \u2208 V(eff(a)) = V(eff(a\u03c6i)), we have eff(a\u03c6i) \u2286 \u03b1i(s)Ja\u03c6iK and eff(a\u03c6i) = \u03b1i(eff(a)) \u2286 \u03b1i(sJaK).\n2. For all other variables v 6\u2208 V(eff(a)), we have sJaK[v] = s[v] and \u03b1i(s)Ja\u03c6iK[v] = \u03b1i(s)[v], and thus\n\u03b1i(s)Ja\u03c6iK[v] = \u03b1i(s)[v] = \u03b1i(s[v]) = \u03b1i(sJaK[v]) = \u03b1i(sJaK)[v].\nNext, for each a \u2208 A, from Eqs. 8 and 9 we have m\u2211 i=1 cost\u03c6i(a\u03c6i) = m\u2211 i=1 costi(a) \u2264 cost(a). (10)\nNow, let s, s\u2032 \u2208 S be a pair of states such that cost(s, s\u2032) \u2264 \u221e, and let % = \u3008a1, . . . , al\u3009 be the sequence of labels along a cheapest path from s to s\u2032 in T. From that, cost(s, s\u2032) = cost(%) =\u2211l\nj=1 cost(a j). The decomposition of such a path to the actions as in Definition 10 is a\n(not neccesarily cheapest) path from \u03b1i(s) to \u03b1i(s \u2032) in Ti, and thus cost(\u03b1i(s), \u03b1i(s\u2032)) \u2264\u2211l\nj=1 costi(a j), providing us with\nm\u2211 i=1 cost(\u03b1i(s), \u03b1i(s \u2032)) \u2264 m\u2211 i=1 l\u2211 j=1 cost\u03c6i(a j \u03c6i ) = l\u2211 j=1 m\u2211 i=1 cost\u03c6i(a j \u03c6i ) (10) \u2264 l\u2211 j=1 cost(aj) = cost(s, s\u2032).\nHaving put the notion of domain abstraction in the framework of implicit abstractions, we are now ready to connect fork decompositions and implicit abstraction heuristics. Given a FI-abstraction AFI = {\u3008\u03a0fv, \u03b1fv\u3009, \u3008\u03a0iv, \u03b1iv\u3009}v\u2208V of a planning task \u03a0 = \u3008V,A, I,G, cost\u3009,\n\u2022 for each \u03a0fv \u2208 \u03a0FI, we associate the root v of CG(\u03a0fv) with mappings \u03a6fv = {\u03c6fv,1, . . . , \u03c6fv,kv} such that kv = O(poly(||\u03a0||)) and all \u03c6fv,i : D(v)\u2192 {0, 1}, and then abstract \u03a0fv with Afv = {\u3008\u03a0fv,i, \u03b1fv,i\u3009}kvi=1, and\n\u2022 for each \u03a0iv \u2208 \u03a0FI, we associate the sink v of CG(\u03a0iv) with mappings \u03a6iv = {\u03c6iv,1, . . . , \u03c6iv,k\u2032v} such that k\u2032v = O(poly(||\u03a0||)) and all \u03c6iv,i : D(v) \u2192 {0, 1, . . . , bv,i}, bv,i = O(1), and then abstract \u03a0iv with Aiv = {\u3008\u03a0iv,i, \u03b1iv,i\u3009} k\u2032v i=1.\nFrom Theorem 3, Theorem 6, and the composition Theorem 2, we then immediately have\nAFI = \u22c3 v\u2208V  kv\u22c3 i=1 {\u3008\u03a0fv,i, \u03b1fv,i \u25e6 \u03b1fv\u3009} \u222a k\u2032v\u22c3 i=1 {\u3008\u03a0iv,i, \u03b1iv,i \u25e6 \u03b1iv\u3009}  (11) being an additive implicit abstraction of \u03a0. Hence, from Theorem 1,\nhFI = \u2211 v\u2208V  kv\u2211 i=1 h\u2217 \u03a0fv,i + k\u2032v\u2211 i=1 h\u2217 \u03a0iv,i  (12) is an admissible estimate of h\u2217 for \u03a0, and, from Theorems 4 and 5, hFI is also computable in time O(poly(||\u03a0||)).\nThis finalizes our construction of a concrete family of implicit abstraction heuristics. To illustrate the mixture of acyclic causal-graph and domain abstractions as above, we again use our running Logistics example. One bothersome question is to what extent further abstracting fork decompositions using domain abstractions affects the informativeness of the heuristic estimate. Though generally a degradation here is unavoidable, below we show that the answer to this question can sometimes be somewhat surprising.\nTo begin with an extreme setting, let all the domain abstractions for roots of forks and sinks of inverted forks be to binary-valued domains. Among multiple options for choosing the mapping sets {\u03a6fv} and {\u03a6iv}, here we use a simple choice of distinguishing between different values of each variable v on the basis of their cost from I[v] in DTG(v,\u03a0). Specifically, for each v \u2208 V , we set \u03a6fv = \u03a6iv, and, for each value \u03d1 \u2208 D(v) and each 1 \u2264 i \u2264 max\u03d1\u2032\u2208D(v) d(I[v], \u03d1\u2032),\n\u03c6fv,i(\u03d1) = \u03c6 i v,i(\u03d1) =\n{ 0, d(I[v], \u03d1) < i\n1, otherwise (13)\nFor example, the problem \u03a0fc1 is decomposed (see the domain transition graph of c1 on the left in Figure 1c) into two problems, \u03a0fc1,1 and \u03a0 f c1,2\n, with the binary abstract domains of c1 corresponding to the partitions {{A}, {B,C,D}} and {{A,D}, {B,C}} of D(c1), respectively. As yet another example, the problem \u03a0ip1 is decomposed (see the domain transition graph of p1 in Figure 1d) into six problems \u03a0 i p1,1\n, . . . ,\u03a0ip1,6 along the abstractions of D(p1) depicted in Figure 5a. Now, given the FI-decomposition of \u03a0 and mappings {\u03a6fv,\u03a6iv}v\u2208V as above, consider the problem \u03a0ip1,1, obtained from abstracting \u03a0 along the inverted fork of p1 and then abstracting D(p1) using\n\u03c6ip1,1(\u03d1) = { 0, \u03d1 \u2208 {C} 1, \u03d1 \u2208 {A,B,D,E, F,G, c1, c2, c3, t} .\nIt is not hard to verify that, from the original actions affecting p1, we are left in \u03a0 i p1,1 with only actions conditioned by c1 and c2. If so, then no information is lost 3 if we remove from \u03a0ip1,1 both variables c3 and t, as well as the actions changing (only) these variables,\n3. No information is lost here because we still keep either fork or inverted fork for each variable of \u03a0.\n(b) Ternary-valued domain abstractions: values that are mapped to the same abstract value are shown as nodes with the same color and borderline.\nand redistribute the cost of the removed actions between all other representatives of their originals in \u03a0. The latter revision of the action cost partition can be obtained directly by replacing the cost-partitioning steps corresponding to Eqs. 3-4 and 8-9 by a single, joint action cost partitioning applied over the final additive implicit abstraction AFI as in Eq. 11 and satisfying\ncost(a) \u2265 \u2211 v\u2208V  kv\u2211 i=1 \u2211 a\u2032\u2208AGfv (a) costfv,i(\u03c6 f v,i(a \u2032)) + k\u2032v\u2211 i=1 \u2211 a\u2032\u2208AGiv (a) costiv,i(\u03c6 i v,i(a \u2032))  . (14) In what follows, by uniform action cost partition we refer to a partition in which the cost of each action is equally split among all its nonredundant representatives in the final additive implicit abstraction.\nOverall, computing hFI as in Eq. 12 under our \u201call binary-valued domain abstractions\u201d and such a uniform action cost partition provides us with hFI(I) = 12 715 , and knowing that the original costs are all integers we can safely adjust it to hFI(I) = 13. Hence, even under the most severe domain abstractions as above, the estimate of hFI in our example task is not lower than that of h2.\nLet us now slightly refine our domain abstractions for the sinks of the inverted forks to be to a ternary range {0, 1, 2}. While mappings {\u03a6fv} remain unchanged, {\u03a6iv} are set to\n\u2200\u03d1 \u2208 D(v) : \u03c6iv,i(\u03d1) =  0, d(I[v], \u03d1) < 2i\u2212 1 1, d(I[v], \u03d1) = 2i\u2212 1 2, d(I[v], \u03d1) > 2i\u2212 1 . (15)\nFor example, the problem \u03a0ip1 is now decomposed into \u03a0 i p1,1 , . . . ,\u03a0ip1,3 along the abstractions of D(p1) depicted in Figure 5b. Applying now the same computation of hFI as in Eq. 12 over the new set of domain abstractions gives hFI(I) = 1512 , which, again, can be safely adjusted to hFI(I) = 16. Note that this value is higher than hFI = 15 obtained using the (generally intractable) \u201cpure\u201d fork-decomposition abstractions as in Eq. 6. At first view, this outcome may seem counterintuitive as the domain abstractions are applied over the fork decomposition, and one would expect a coarser abstraction to provide less precise estimates. This, however, is not necessarily the case when the employed action cost partition is ad hoc. For instance, domain abstraction for the sink of an inverted fork may create independence between the sink and its parent variables, and exploiting such domain-abstraction specific independence relations leads to more targeted action cost partition via Eq. 14.\nTo see why this surprising \u201cestimate improvement\u201d has been obtained, note that before the domain abstraction in Eq. 15 is applied on our example, the truck-moving actions drive-t-from-D-to-E and drive-t-from-E-to-D appear in three abstractions \u03a0ft, \u03a0 i p1 and \u03a0 i p2 , while after domain abstraction they appear in five abstractions \u03a0ft,1, \u03a0 i p1,1 , \u03a0ip1,2, \u03a0 i p1,3 and \u03a0ip2,1. However, a closer look at the action sets of these five abstractions reveals that the dependencies of p1 in CG(\u03a0 i p1,1 ) and CG(\u03a0ip1,3), and of p2 in CG(\u03a0 i p2,1\n) on t are redundant, and thus keeping representatives of move-D-E and move-E-D in the corresponding abstract tasks is entirely unnecessary. Hence, after all, the two truck-moving actions appear only in two post-domain-abstraction tasks. Moreover, in both these abstractions the truck-moving actions are fully counted, in contrast to the predomain-abstraction tasks where the portion of the cost of these actions allocated to \u03a0ip2 simply gets lost."}, {"heading": "6. Experimental Evaluation: Take I", "text": "To evaluate the practical attractiveness of the fork-decomposition heuristics, we have conducted an empirical study on a wide sample of planning domains from the International Planning Competitions (IPC) 1998-2006, plus a non-IPC Schedule-STRIPS domain.4\nThe domains were selected to allow a comparative evaluation with other, both baseline and state-of-the-art, approaches/planners, not all of which supported all the PDDL features at the time of our evaluation.\nLater we formally prove that, under ad hoc action cost partitions such as our uniform partition, none of the three fork decompositions as in Definition 9 is dominated by the other two. Hence, we have implemented three additive fork-decomposition heuristics, hF, hI, and hFI, within the standard heuristic forward search framework of the Fast Downward planner (Helmert, 2006) using the A\u2217 algorithm with full duplicate elimination. The hF\nheuristic corresponds to the ensemble of all (not clearly redundant) fork subgraphs of the\n4. Schedule-STRIPS appears in the domains\u2019 distribution of IPC-2000. Later we became aware of the fact that this domain was excluded from the competition because its encoding generated problems for various planners.\ntasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from \u201csolved by some planners\u201d (%S). The last row summarize the number of solved instances.\ncausal graph, with the domains of the roots being abstracted using the \u201cleave-one-value-out\u201d binary-valued domain decompositions as follows:\n\u2200\u03d1i \u2208 D(v) : \u03c6fv,i(\u03d1) = { 0, \u03d1 = \u03d1i\n1, otherwise . (16)\nThe hI heuristic is the same but for the inverted fork subgraphs, with the domains of the sinks being abstracted using the \u201cdistance-to-goal-value\u201d ternary-valued domain decompositions5 as in Eq. 17.\n\u2200\u03d1 \u2208 D(v) : \u03c6iv,i(\u03d1) =  0, d(\u03d1,G[v]) < 2i\u2212 1 1, d(\u03d1,G[v]) = 2i\u2212 1 2, d(\u03d1,G[v]) > 2i\u2212 1 . (17)\nThe ensemble of the hFI heuristic is the union of these for hF and hI. The action cost partition in all three heuristics was what we call \u201cuniform.\u201d\nWe make a comparison with two baseline approaches, namely \u201cblind A\u2217\u201d with heuristic value 0 for goal states and 1 otherwise, and A\u2217 with the hmax heuristic (Bonet & Geffner, 2001), as well as with state-of-the-art abstraction heuristics, represented by the mergeand-shrink abstractions of Helmert et al. (2007). The latter were constructed under the\n5. While \u201cdistance-from-initial-value\u201d is reasonable for the evaluation of just the initial state, \u201cleave-onevalue-out\u201d for fork roots and \u201cdistance-to-goal-value\u201d for inverted-fork sinks should typically be much more attractive for the evaluation of all the states examined by A\u2217.\nlinear, f -preserving abstraction strategy proposed by these authors, and this under two fixed bounds on the size of the abstract state spaces, notably |S\u03b1| < 104 and |S\u03b1| < 105. These four (baseline and merge-and-shrink) heuristics were implemented by Helmert et al. (2007) within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison. We also compare to the Gamer (Edelkamp & Kissmann, 2009) and HSP\u2217F (Haslum, 2008) planners, the winner and the runner-up at the sequential optimization track of IPC-2008. On the algorithmic side, Gamer is based on a bidirectional blind search using sophisticated symbolic-search techniques, and HSP\u2217F uses A\n\u2217 with an additive critical-path heuristic. The experiments were conducted on a 3GHz Intel E8400 CPU with 2 GB memory, using 1.5 GB memory limit and 30 minute timeout. The only exception was Gamer, for which we used similar machines but with 4 GB memory and 2 GB memory limit; this was done to provide Gamer with the environment for which it was configured.\nTable 1 summarizes our experimental results in terms of the number of tasks solved by each planner. Our impression of fork-decomposition heuristics from Table 1 is somewhat mixed. On the one hand, the performance of all three fork-decomposition based planners was comparable to one of the settings of the merge-and-shrink heuristic, and this clearly testifies for that the framework of implicit abstractions is not of theoretical interest only. On the other hand, all the planners, except for A\u2217 with the merge-and-shrink heuristic with |S\u03b1| < 104, failed to outperform A\u2217 with the baseline hmax heuristic. More important for us is that, unfortunately, all three fork-decomposition based planners failed to outperform even the basic blind search.\nThis, however, is not the end of the story for the fork-decomposition heuristics. Some hope can be found in the detailed results in Tables 9-14 in the appendix. As it appears from Table 10, on, e.g., the Logistics-ipc2 domain, hF almost consistently leads to expanding fewer search nodes than the (better between the two merge-and-shrink heuristics on this domain) MS -105, with the difference hitting four orders of magnitude. However, the time complexity of hF per search node is substantially higher than that of MS -105, with the two expanding at a rate of approximately 40 and 100000 nodes per second, respectively. The outcome is simple: while with no time limits (and only memory limit of 1.5 GB) hF\nsolves more tasks in Logistics-ipc2 than MS -105 (task 12-1 is solved with hF in 2519.01 seconds), this is not so with a standard time limit of half an hour used for Table 10. In what follows we examine the possibility of exploiting the informativeness of fork-decomposition heuristics while not falling into the trap of costly per-node heuristic evaluation.\n7. Back to Theory: h-Partitions and Databased Implicit Abstraction\nAccuracy and low time complexity are both desired yet competing properties of heuristic functions. For many powerful heuristics, and abstraction heuristics in particular, computing h(s) for each state s in isolation is impractical: while computing h(s) is polynomial in the description size of \u03a0, it is often not efficient enough to be performed at each search node. However, for some costly heuristics this obstacle can be largely overcome by sharing most of the computation between the evaluations of h on different states. If that is possible, the shared parts of computing h for all problem states are precomputed and memorized before the search, and then reused during the search by the evaluations of h on different\nstates. Such a mixed offline/online heuristic computation is henceforth called h-partition, and we define the time complexity of an h-partition as the complexity of computing h for a set of states. Given a subset of k problem states S\u2032 \u2286 S, the h-partition\u2019s time complexity of computing {h(s) | s \u2208 S\u2032} is expressed as O(X+kY ), where O(X) and O(Y ) are, respectively, the complexity of the (offline) pre-search and (online) per-node parts of computing h(s).\nThese days h-partitions are being adopted by various optimal planners using criticalpath heuristics hm for m > 1 (Haslum et al., 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007). Without effective h-partitions, optimal search with these heuristics would not scale up well, while with such h-partitions it constitutes the state of the art of cost-optimal planning. For instance, a very attractive property of PDB abstractions is the complexity of their natural h\u03b1-partition. Instead of computing h\u03b1(s) = h\u2217(\u03b1(s)) from scratch for each evaluated state s (impractical for all but tiny projections), the practice is to precompute and store h\u2217(s\u2032) for all abstract states s\u2032 \u2208 S\u03b1, after which the per-node computation of h\u03b1(s) boils down to a hash-table lookup for h\u2217(\u03b1(s)) with a perfect hash function. In our terms, the time and space complexity of that PDB h\u03b1-partition for a set of k states is O(|S\u03b1|(log(|S\u03b1|) + |A|) + k) and O(|S\u03b1|), respectively. This is precisely what makes PDB heuristics so attractive in practice. In that respect, the picture with mergeand-shrink abstractions is very much similar. While the order in which composites are formed and the choice of abstract states to contract are crucial to the complexity of their natural h\u03b1-partitions, the time and space complexity for the concrete linear abstraction strategy of Helmert et al. are respectively O(|V ||S\u03b1|(log(|S\u03b1|) + |A|) + k \u00b7 |V |) and O(|S\u03b1|). Similarly to PDB abstractions, the per-node computation of h\u03b1(s) with a merge-and-shrink abstraction \u03b1 is just a lookup in a data structure storing h\u2217(\u03b1(s)) for all abstract states \u03b1(s) \u2208 S\u03b1. Hence, while the pre-search computation with MS abstractions can be more costly than with PDBs, the online part of computing heuristic values is still extremely efficient. This per-node efficiency provides the merge-and-shrink heuristics with impressive practical effectiveness on numerous IPC domains (Helmert et al., 2007).\nTo sum up, we can say that the fixed size of abstract spaces induced by explicit abstractions such as PDBs and merge-and-shrink is not only a limitation but also a key to obtaining effective h-partitions. In contrast, escaping that limitation with implicit abstractions might trap us into having to pay a high price for each search-node evaluation. We now show, however, that the time-per-node complexity bottleneck of fork-decomposition heuristics can be successfully overcome. Specifically, we show that an equivalent of PDB\u2019s and mergeand-shrink notion of \u201cdatabase\u201d exists for fork-decomposition abstractions as well, despite their exponential-size abstract spaces. Of course, unlike with PDB and merge-and-shrink abstractions, the databased fork-decomposition heuristics do not (and cannot) provide us with a purely lookup online computation of h\u03b1(s). The online part of the h\u03b1-partition has to be nontrivial in the sense that its complexity cannot be O(1). In what comes next we prove the existence of such effective h-partitions for fork and inverted fork abstractions. In Section 8 we then empirically show that these h-partitions lead to fast pre-search and per-node computations, allowing the informativeness of the fork-decomposition heuristics to be successfully exploited in practice.\nTheorem 7 Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task with a fork causal graph rooted at a binary-valued variable r. There exists an h\u2217-partition for \u03a0 such that, for any set of k states, the time and space complexity of that h\u2217-partition is, respectively, O(d3|V |+ |Ar|+ kd|V |) and O(d2|V |), where d = maxv D(v).\nProof: The proof is by a modification of the polynomial-time algorithm for computing h\u2217(s) for a state s of such a task \u03a0 used in the proof of Theorem 4 (Tractable Forks). Given a state s, let D(r) = {0, 1}, where s[r] = 0. In what follows, for each of the two root\u2019s values \u03d1 \u2208 D(r), \u00ac\u03d1 denotes the opposite value 1\u2212 \u03d1; \u03c3(r), [\u03c3(r)], DTG0v and DTG1v are defined exactly as in the proof of Theorem 4.\n(1) For each of the two values \u03d1r \u2208 D(r) of the root variable, each leaf variable v \u2208 V \\{r}, and each pair of values \u03d1, \u03d1\u2032 \u2208 D(v), let p\u03d1,\u03d1\u2032;\u03d1r be the cost of the cheapest sequence of actions changing v from \u03d1 to \u03d1\u2032 provided r :\u03d1r. The whole set {p\u03d1,\u03d1\u2032;\u03d1r} for all the leaves v \u2208 V \\{r} can be computed by a straightforward variant of the all-pairs-shortest-paths, Floyd-Warshall algorithm on DTG\u03d1rv in time O(d 3|V |).\n(2) For each leaf variable v \u2208 V \\ {r}, 1 \u2264 i \u2264 d + 1, and \u03d1 \u2208 D(v), let g\u03d1;i be the cost of the cheapest sequence of actions changing s[v] to \u03d1 provided a sequence \u03c3 \u2208 [\u03c3(r)], |\u03c3| = i, of value changes of r. Having the values {p\u03d1,\u03d1\u2032;\u03d1r} from step (1), the set {g\u03d1;i} is given by the solution of the recursive equation\ng\u03d1;i =  ps[v],\u03d1;s[r], i = 1 min \u03d1\u2032 [ g\u03d1\u2032;i\u22121 + p\u03d1\u2032,\u03d1;s[r] ] , 1 < i \u2264 \u03b4\u03d1, i is odd min \u03d1\u2032 [ g\u03d1\u2032;i\u22121 + p\u03d1\u2032,\u03d1;\u00acs[r] ] , 1 < i \u2264 \u03b4\u03d1, i is even\ng\u03d1;i\u22121, \u03b4\u03d1 < i \u2264 d+ 1\n,\nwhere \u03b4\u03d1 = |D(v)|+ 1. Given that, we have\nh\u2217(s) = min \u03c3\u2208 [\u03c3(r)]  cost(\u03c3) + \u2211 v\u2208V \\{r} gG[v];|\u03c3|  , with cost(\u03c3) = \u2211|\u03c3| i=2 cost(a\u03c3[i]), where a\u03c3[i] \u2208 A is the cheapest action changing the\nvalue of r from \u03c3[i\u2212 1] to \u03c3[i].\nNote that step (1) is already state-independent, but the heavy step (2) is not. However, the state dependence of step (2) can mostly be overcome as follows. For each v \u2208 V \\ {r}, \u03d1 \u2208 D(v), 1 \u2264 i \u2264 d+ 1, and \u03d1r \u2208 D(r), let g\u0303\u03d1;i(\u03d1r) be the cost of the cheapest sequence of actions changing \u03d1 to G[v] provided the value changes of r induce a 0/1 sequence of length i starting with \u03d1r. The set {g\u0303\u03d1;i(\u03d1r)} is given by the solution of the recursive equation\ng\u0303\u03d1;i(\u03d1r) =  p\u03d1,G[v];\u03d1r , i = 1 min \u03d1\u2032 [ g\u0303\u03d1\u2032;i\u22121(\u00ac\u03d1r) + p\u03d1,\u03d1\u2032;\u03d1r ] , 1 < i \u2264 \u03b4\u03d1\ng\u0303\u03d1;i\u22121(\u03d1r), \u03b4\u03d1 < i \u2264 d+ 1 , (18)\nand two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5. (a) depicts the domain transition graphs of r (top), v (middle), and u (bottom); the numbers above and below each edge are the precondition on r and the cost of the respective action. (b) depicts the database created by the algorithm. For instance, the entry in row r :0\u2227 |\u03c3|=5 and column v :0 captures the value of g\u0303v:0;5(r :0) computed as in Eq. 18. The shaded entries are those examined during the online computation of h\u2217(r :0, v :0, u :0).\nwhich can be solved in time O(d3|V |). Note that this equation is now independent of the evaluated state s, and yet {g\u0303\u03d1;i(\u03d1r)} allow for computing h\u2217(s) for a given state s via\nh\u2217(s) = min \u03c3\u2208 [\u03c3(r|s[r])]  cost(\u03c3) + \u2211 v\u2208V \\{r} g\u0303s[v];|\u03c3|(s[r])  (19) where \u03c3(r|\u03d1r) is defined similarly to \u03c3(r) but with respect to the initial value \u03d1r of r.\nWith the new formulation, the only computation that has to be performed online, per search node, is the final minimization over [\u03c3(r|s[r])] in Eq. 19, and this is the lightest part of the whole algorithm anyway. The major computations, notably those of {p\u03d1,\u03d1\u2032;\u03d1r} and {g\u0303\u03d1;i(\u03d1r)}, can now be performed offline and shared between the evaluated states. The space required to store this information is O(d2|V |) as it contains only a fixed amount of information per pair of values of each variable. The time complexity of the offline computation is O(d3|V | + |Ar|); the |Ar| component stems from precomputing the costs cost(\u03c3). The time complexity of the online computation per state is O(d|V |); |V | comes from the internal summation and d comes from the size of [\u03c3(r|s[r])].\nFigure 6b shows the database created for a fork-structured problem with a binary-valued root r, two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5; the domain transition\ngraphs of v and u are depicted in Figure 6(a). Online computation of h\u2217(s) as in Eq. 19 for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each of the four rows having such entries, and minimizes over the resulting four sums, with the minimum being obtained in the row r :0 \u2227 |\u03c3|=5.\nTheorem 8 Let \u03a0 = \u3008V,A, I,G, cost\u3009 be a planning task with an inverted fork causal graph with sink r and |D(r)| = b = O(1). There exists an h\u2217-partition for \u03a0 such that, for any set of k states, the time and space complexity of that h\u2217-partition is O(b|V ||Ar|b\u22121 + d3|V | + k|V ||Ar|b\u22121) and O(|V ||Ar|b\u22121 + d2|V |), respectively, where d = maxv D(v). Proof: Like the proof of Theorem 7, the proof of Theorem 8 is based on a modification of the polynomial-time algorithm for computing h\u2217(s) used for the proof of Theorem 5 (Tractable Inverted Forks).\n(1) For each parent variable v \u2208 V \\ {r}, and each pair of its values \u03d1, \u03d1\u2032 \u2208 D(v), let p\u03d1,\u03d1\u2032 be the cost of the cheapest sequence of actions changing \u03d1 to \u03d1\u2032. The whole set {p\u03d1,\u03d1\u2032} can be computed using the Floyd-Warshall algorithm on the domain transition graph of v in time O(d3|V |).\n(2) Given a state s, for each cycle-free path \u03c0 = \u3008a1, . . . , am\u3009 from s[r] to G[r] in DTG(v,\u03a0), let g\u03c0 be the cost of the cheapest plan from s in \u03a0 based on \u03c0, and the cheapest paths {p\u03d1,\u03d1\u2032} computed in step (1). Each g\u03c0 can be computed as\ng\u03c0 = m\u2211 i=1 cost(ai) + m\u2211 i=0 \u2211 v\u2208V \\{r} pprei[v],prei+1[v],\nwhere pre0, . . . , prem+1 are the values required from the parents of r along the path \u03c0. That is, for each v \u2208 V \\ {r}, and 0 \u2264 i \u2264 m+ 1,\nprei[v] =  s[v], i = 0 G[v], i = m+ 1, and G[v] is specified\npre(ai)[v], 1 \u2264 i \u2264 m, and pre(ai)[v] is specified prei\u22121[v] otherwise\n.\nFrom that, we have h\u2217(s) = min\u03c0 g\u03c0. Note that step (1) is state-independent, but step (2) is not. However, the dependence\nof step (2) on the evaluated state can be substantially relaxed. As there are only O(1) different values of r, it is possible to consider cycle-free paths to G[r] from all values of r. For each such path \u03c0, and each parent variable v \u2208 V \\ {r}, we know what the first value of v required by \u03c0 would be. Given that, we can precompute the cost-optimal plans induced by each \u03c0 assuming the parents start at their first required values. The remainder of the computation of h\u2217(s) is delegated to online, and the modified step (2) is as follows.\nFor each \u03d1r \u2208 D(r) and each cycle-free path \u03c0 = \u3008a1, . . . , am\u3009 from \u03d1r to G[r] in DTG(r,\u03a0), let a \u201cproxy\u201d state s\u03c0 be\ns\u03c0[v] =  \u03d1r, v = r\nG[v], \u22001 \u2264 i \u2264 m : pre(ai)[v] is unspecified pre(ai)[v], i = argminj {pre(aj)[v] is specified} ,\nthat is, the nontrivial part of s\u03c0 captures the first values of V \\{r} required along \u03c0.6 Given that, let g\u03c0 be the cost of the cheapest plan from s\u03c0 in \u03a0 based on \u03c0, and the cheapest paths {p\u03d1,\u03d1\u2032} computed in (1). Each g\u03c0 can be computed as\ng\u03c0 = m\u2211 i=1 cost(ai) + \u2211 v\u2208V \\{r} pprei[v],prei+1[v]  , where, for each v \u2208 V \\ {r}, and 1 \u2264 i \u2264 m+ 1,\nprei[v] =  s\u03c0[v], i = 1 G[v], i = m+ 1, and G[v] is specified\npre(ai)[v], 2 \u2264 i \u2264 m, and pre(ai)[v] is specified prei\u22121[v], otherwise\n.\nStoring the pairs (g\u03c0, s\u03c0) accomplishes the offline part of the computation. Now, given a search state s, we can compute\nh\u2217(s) = min \u03c0 s.t.\ns\u03c0 [r]=s[r] g\u03c0 + \u2211 v\u2208V \\{r} ps[v],s\u03c0 [v] . (20) The number of cycle-free paths to G[r] in DTG(r,\u03a0) is \u0398(|Ar|b\u22121), and g\u03c0 for each such path \u03c0 can be computed in time O(b|V |). Hence, the overall offline time complexity is O(b|V ||Ar|b\u22121 +d3|V |), and the space complexity (including the storage of the proxy states s\u03c0) is O(|V ||Ar|b\u22121 + d2|V |). The time complexity of the online computation per state via Eq. 20 is O(|V ||Ar|b\u22121); |V | comes from the internal summation and |Ar|b\u22121 from the upper bound on the number of cycle-free paths from s[r] to G[r].\nFigure 7(b) shows the database created for an inverted fork structured problem with a ternary-valued sink variable r, two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2. The domain transition graphs of u and v are depicted at the top of Figure 7(a); the actual identities of actions affecting these two parents are not important here. The actions affecting the sink r are\na1 = \u3008{u :1, r :0}, {r :1}\u3009 a2 = \u3008{v :1, r :0}, {r :1}\u3009 a3 = \u3008{u :2, r :1}, {r :2}\u3009 a4 = \u3008{v :1, r :1}, {r :2}\u3009.\nThe domain transition graph of r is depicted at the bottom of Figure 7(a). Online computation of h\u2217(s) as in Eq. 20 for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each of the four rows having such entries, and minimizes over the resulting four sums, with the minimum being obtained in the lowest such row.\nvariable r and two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2. (a) depicts the domain transition graphs of u (top left), v (top right), and r (bottom); the numbers above and below each edge are the preconditions and the cost of the respective action, respectively. (b) depicts the database created by the algorithm. The shaded entries are those examined during the online computation of h\u2217(r :0, u :0, v :0)."}, {"heading": "8. Experimental Evaluation: Take II", "text": "To evaluate the practical attractiveness of the databased fork-decomposition heuristics, we have repeated our empirical evaluation as in Section 6, but now for the databased versions of the heuristics. The detailed results of this evaluation are relegated to Tables 15-20 in the appendix, but they are summarized here in Table 2. For each domain, the S column captures the number of tasks in that domain that were solved by at least one planner in the suite. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from \u201csolved by some planners\u201d (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the performance of the planners via three measures. The first is the number of tasks solved in all the 23 domains; this is basically the performance evaluation measure used in the optimization track at IPC-2008. As domains are not equally challenging and do not equally discriminate between the planners\u2019 performance, the second is a \u201cdomain-normalized\u201d performance measure\ns\u0302(p) = \u2211\ndomain D\n#tasks in D solved by planner p #tasks in D solved by some planners .\nFinally, the third measure corresponds to the number of domains w in which the planner in question solved at least as many tasks as any other planner.\nOverall, Table 2 clearly suggests that heuristic search with \u201cdatabased\u201d fork-decomposition heuristics favorably competes with the state of the art of optimal planning. In particular,\n6. For ease of presentation, we omit here the case where v is required neither along \u03c0, nor by the goal; such variables should be simply ignored when accounting for the cost of \u03c0.\ndecomposition heuristics. Per domain, S denotes the number of tasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from \u201csolved by some planners\u201d (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the number of solved instances, the domain-normalized measure of solved instances (s\u0302), and the number of domains in which the planners achieved superior performance (w).\nA\u2217 with the \u201conly forks\u201d heuristic hF exhibited the best overall performance according to all three measures. In terms of the absolute number of solved instances, A\u2217 with all three fork-decomposition heuristics outperformed all other planners in the suite. The contribution of databasing to the success of the fork-decomposition heuristics was dramatic. Looking back at the results with \u201cfully online\u201d heuristic computation depicted in Table 1, note that the total number of solved instances for the fork-decomposition heuristics hF, hI, and hFI\nincreased by 74, 55, and 76, respectively, and this made the whole difference.\nWe have also performed a comparative evaluation on the planning domains from the recent IPC-2008. The IPC-2008 domains differ from the previous domains in that actions had various costs, and, more importantly, many actions had zero cost. The latter is an issue for heuristic-search planners because heuristic functions cannot differentiate between subplans that have the same cost of zero, but differ in length. In any case, the comparative side of our evaluation on the IPC-2008 domains differ on several points from the previous one. First, neither for merge-and-shrink nor for hmax heuristics, we had implementation supporting arbitrary action costs. Hence, our comparison here is only with Gamer, HSP\u2217F, and blind search. Second, to ensure admissibility of the blind search, the latter has been modified to return on non-goal states the cost of the cheapest applicable action. Finally, all the planners were run on a 3GHz Intel E8400 CPU with 4 GB memory, using 2 GB memory\ntasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from \u201csolved by some planners\u201d (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the number of solved instances, the domain-normalized measure of solved instances (s\u0302), and the number of domains in which the planners achieved superior performance (w).\nlimit and 30 minute timeout. The results of this evaluation are summarized in Table 3; for the detailed results we refer the reader to Tables 21-22 in the appendix. Overall, these results show that A\u2217 with the fork-decomposition heuristics are very much competitive on the IPC-2008 domains as well."}, {"heading": "9. Formal Analysis: Asymptotic Performance Ratios", "text": "Empirical evaluation on a concrete set of benchmark tasks is a standard and important methodology for assessing the effectiveness of heuristic estimates: it allows us to study the tradeoff between the accuracy of the heuristics and the complexity of computing them. However, as rightfully noted by Helmert and Mattmu\u0308ller (2008), such evaluations almost never lead to absolute statements of the type \u201cHeuristic h is well-suited for solving problems from benchmark suite X,\u201d but only to relative statements of the type \u201cHeuristic h expands fewer nodes than heuristic h\u2032 on benchmark suite X.\u201d Moreover, one would probably like to obtain formal evidence of the effectiveness of a heuristic before proceeding with its implementation, especially for very complicated heuristic procedures such as those underlying the proofs of Theorems 7 and 8. Our formal analysis of the effectiveness of the fork-decomposition heuristics using the methodology suggested and exploited by Helmert and Mattmu\u0308ller was motivated primarily by this desire for formal evidence.\nGiven a planning domain D and heuristic h, Helmert and Mattmu\u0308ller (2008) consider the asymptotic performance ratio of h in D. The goal is to find a value \u03b1(h,D) \u2208 [0, 1] such that\n(1) for all states s in all problems \u03a0 \u2208 D, h(s) \u2265 \u03b1(h,D) \u00b7 h\u2217(s) + o(h\u2217(s)), and\n(2) there is a family of problems {\u03a0n}n\u2208N \u2286 D and solvable, non-goal states {sn}n\u2208N such that sn \u2208 \u03a0n, limn\u2192\u221e h\u2217(sn) =\u221e, and h(sn) \u2264 \u03b1(h,D) \u00b7 h\u2217(sn) + o(h\u2217(sn)).\nIn other words, h is never worse than \u03b1(, domain, \u00b7)h\u2217 (plus a sublinear term), and it can become as bad as \u03b1(h,D) \u00b7 h\u2217 (plus a sublinear term) for arbitrarily large inputs; note that both the existence and uniqueness of \u03b1(h,D) are guaranteed for any h and D.\nHelmert and Mattmu\u0308ller (2008) study the asymptotic performance ratio of some standard admissible heuristics on a set of well-known benchmark domains from the first four IPCs. Their results for Gripper, Logistics, Blocksworld, Miconic, and Satellite are shown in the first four columns of Table 4.\n\u2022 The h+ estimate corresponds to the optimal cost of solving the well-known \u201cdelete relaxation\u201d of the original planning task, which is generally NP-hard to compute (Bylander, 1994).\n\u2022 The hk, k \u2208 N+, family of heuristics is based on a relaxation where the cost of achieving a partial assignment is approximated by the highest cost of achieving its sub-assignment of size k (Haslum & Geffner, 2000); computing hk is exponential only in k.\n\u2022 The hPDB and hPDBadd heuristics are regular (maximized over) and additive pattern database heuristics where the size of each pattern is assumed to be O(log(n)) where n = |V |, and, importantly, the choice of the patterns is assumed to be optimal.\nThese results provide us with a baseline for evaluating our fork-decomposition heuristics hF, hI, and hFI. First, however, Theorem 9 shows that these three heuristics are worth analyzing because each alone can be strictly more informative than the other two, depending on the planning task and/or the state being evaluated.7\nTheorem 9 (Undominance) Under uniform action cost partition, none of the heuristic functions hF, hI, and hFI dominates another.\nProof: The proof is by example of two tasks, \u03a01 and \u03a02, which illustrate the following two cases: hF(I) > hFI(I) > hI(I) and hF(I) < hFI(I) < hI(I). These two tasks are defined over the same set of binary-valued variables V = {v1, v2, v3, u1, u2, u3}, have the same initial state I = {v1 : 0, v2 : 0, v3 : 0, u1 : 0, u2 : 0, u3 : 0}, and have the same goal 7. Theorem 9 is formulated and proven under the uniform action cost partition that we use throughout the\npaper, including the experiments. For per-step optimal action cost partitions (Katz & Domshlak, 2010), it is trivial to show that hFI dominates both hF and hI for all planning tasks.\nin each abstract problem along these subgraphs. Considering for example the first row of table (c), the action a1 in \u03a0\n1 has a single representative in each of the three fork abstractions, as well as a representative in the inverted-fork abstraction \u03a01G iv1 . Hence, the cost of each of its representatives in F-decomposition is 1/3, while the cost of its sole representative in I-decomposition is 1.\nG = {v1 : 1, v2 : 1, v3 : 1}. The difference between \u03a01 and \u03a02 is in the action sets, listed in Figure 8c-d, with all the actions being unit-cost actions. The two tasks induce identical causal graphs, depicted in Figure 8a. Hence, the collections of v-forks and v-iforks of both tasks are also identical; these are depicted in Figure 8b. The fractional costs of the tasks\u2019 action representatives in the corresponding abstract problems are given in Figure 8c-d.\nFigure 9 shows the optimal plans for all the abstract problems in F-decompositions \u03a01F = {\u03a01Gfu1 ,\u03a0 1 Gfu2 ,\u03a01Gfu3 } and \u03a02F = {\u03a02Gfu1 ,\u03a0 2 Gfu2 ,\u03a02Gfu3 }, I-decompositions \u03a01I = {\u03a01G iv1 ,\u03a0 1 G iv2 ,\u03a01G iv3\n} and \u03a02I = {\u03a02G iv1 ,\u03a0 2 G iv2 ,\u03a02G iv3\n}, and FI-decompositions \u03a01FI = \u03a01F \u222a\u03a01I and \u03a02FI = \u03a02F \u222a\u03a02I . The last column in both tables captures the estimates of the three heuristics for the initial states of \u03a01 and \u03a02, respectively. Together, these two cases show that none of the forkdecomposition heuristic functions hF, hI, and hFI dominates any other, and, since all the\nvariables above are binary-valued, the claim holds in conjunction with arbitrary variable domain abstractions.\nOne conclusion from Theorem 9 is that it is worth studying the asymptotic performance ratios for all three heuristics. The last three columns of Table 4 present our results for hF, hI, and hFI for the Gripper, Logistics, Blocksworld, Miconic, and Satellite domains. We also studied the performance ratios of max{hF, hI, hFI}, and in these five domains they appear to be identical to those of hF. (Note that \u201cratio of max\u201d should not necessarily be identical to \u201cmax of ratios,\u201d and thus this analysis is worthwhile.) Taking a conservative position, the performance ratios for the fork-decomposition heuristics in Table 4 are \u201cworst-case\u201d in the sense that\n(i) here we neither optimize the action cost partition (setting it to uniform as in the rest of the paper) nor eliminate clearly redundant abstractions, and\n(ii) we use domain abstractions to (up to) ternary-valued abstract domains only.\nThe domains of the fork roots are all abstracted using the \u201cleave-one-out\u201d binary-valued domain decompositions as in Eq. 16 while the domains of the inverted-fork sinks are all abstracted using the \u201cdistance-from-initial-value\u201d ternary-valued domain decompositions as in Eq. 15.\nOverall, the results for fork-decomposition heuristics in Table 4 are gratifying. First, note that the performance ratios for hk and hPDB are all 0. This is because every subgoal set of size k (for hk) and size log(n) (for hPDB) can be reached in the number of steps that only depends on k (respectively, log(n)), and not n, while h\u2217(sn) grows linearly in n in all the five domains. This leaves us with hPDBadd being the only state-of-the-art (tractable\nand) admissible heuristic to compare with. Table 4 shows that the asymptotic performance ratio of hF heuristic is at least as good as that of hPDBadd in all five domains, while h\nF is superior to hPDBadd in Miconic, getting here quite close to h\n+. When comparing hPDBadd and fork-decomposition heuristics, it is crucial to recall that the ratios devised by Helmert and Mattmu\u0308ller for hPDBadd are with respect to optimal, manually-selected set of patterns. By contrast, the selection of variable subsets for fork-decomposition heuristics is completely nonparametric, and thus requires no tuning of the abstraction-selection process.\nIn the rest of the section we prove these asymptotic performance ratios of hF, hI, and hFI in Table 4 for the five domains. We begin with a very brief outline of how the results are obtained. Some familiarity with the domains is assumed. Next, each domain is addressed in detail: we provide an informal domain description as well as its sas+ representation, and then prove lower and upper bounds on the ratios for all three heuristics.\nGripper Assuming n > 0 balls should be moved from one room to another, all three heuristics hF, hI, hFI account for all the required pickup and drop actions, and only for O(1)-portion of move actions. However, the former actions are responsible for 2/3 of the optimal-plan length (= cost). Now, with the basic uniform action-cost partition, hF, hI, and hFI account for the whole, O(1/n), and 2/3 of the total pickup/drop actions cost, respectively, providing the ratios in Table 4.8\nLogistics An optimal plan contains at least as many load/unload actions as move actions, and all three heuristics hF, hI, hFI fully account for the former, providing a lower bound of 1/2. An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t1, t2, no airplanes, one city, and n packages such that the initial and goal locations of all the packages and trucks are all pair-wise different.\nBlocksworld Arguments similar to those of Helmert and Mattmu\u0308ller (2008) for hPDBadd .\nMiconic All three heuristics fully account for all the loads/unload actions. In addition, hF\naccounts for the full cost of all the move actions to the passengers\u2019 initial locations, and for half of the cost of all the other move actions. This provides us with lower bounds of 1/2 and 5/6, respectively. Tightness of 1/2 for hI and hFI is shown on a task consisting of n passengers, 2n + 1 floors, and all the initial and goal locations being pair-wise different. Tightness of 5/6 for hF is shown on a task consisting of n passengers, n+ 1 floors, the elevator and all the passengers are initially at floor n+ 1, and each passenger i wishes to get to floor i.\nSatellite The length of an optimal plan for a problem with n images to be taken and k satellites to be moved to some end-positions is \u2264 6n + k. All three heuristics fully account for all the image-taking actions and one satellite-moving action per satellite as above, providing a lower bound of 16 . Tightness of 1/6 for all three heuristics is shown on a task as follows: Two satellites with instruments {i}li=1 and {i}2li=l+1, respectively, where l = n \u2212 \u221an. Each pair of instruments {i, l + i} can take images in modes {m0,mi}. There is a set of directions {dj}nj=0 and a set of image objectives\n8. We note that a very slight modification of the uniform action-cost partition results in a ratio of 2/3 for all three heuristics. Such optimizations, however, are outside of our scope here.\nv-iforks\n{oi}ni=1 such that, for 1 \u2264 i \u2264 l, oi = (d0,mi) and, for l < i \u2264 n, oi = (di,m0). Finally, the calibration direction for each pair of instruments {i, l + i} is di."}, {"heading": "9.1 Gripper", "text": "The domain consists of one robot robot with two arms Arms = {right, left}, two rooms Rooms = {r1, r2}, and a set Balls of n balls. The robot can pick up a ball with an arm arm \u2208 Arms if arm is empty, release a ball b \u2208 Balls from the arm arm if arm currently holds b, and move from one room to another. All balls and the robot are initially in room r1, both arms are empty, and the goal is to move all the balls to room r2. A natural description of this planning task in sas+ is as follows.\n\u2022 Variables V = {robot}\u22c3Arms\u22c3Balls with domains D(robot) = Rooms\nD(left) = D(right) = Balls \u222a {empty} \u2200b \u2208 Balls : D(b) = Rooms \u222a {robot}.\n\u2022 Initial state I = {b : r1 | b \u2208 Balls} \u222a {robot : r1, right :empty, left :empty}.\n\u2022 Goal G = {b : r2 | b \u2208 Balls}.\n\u2022 Actions A ={Move(r, r\u2032) | {r, r\u2032} \u2286 Rooms} \u22c3\n{Pickup(b, arm, r), Drop(b, arm, r) | b \u2208 Balls, arm \u2208 Arms, r \u2208 Rooms},\nwhere\n\u2013 move robot: Move(r, r\u2032) = \u3008{robot : r}, {robot : r\u2032}\u3009, \u2013 pickup ball: Pickup(b, arm, r) = \u3008{b : r, arm :empty, robot : r}, {b : robot, arm :b}\u3009, and\n\u2013 drop ball: Drop(b, arm, r) = \u3008{b : robot, arm :b, robot : r}, {b : r, arm :empty}\u3009.\nThe (parametric in n) causal graph of this task is depicted in Figure 10a.\nas well as the partition of the action costs between these representatives\ntasks"}, {"heading": "9.1.1 Fork Decomposition", "text": "Since the variables robot, right, and left have no goal value, the collection of v-forks and v-iforks is as in Figure 10b. The domains of inverted fork sinks are ternary valued. The domains of fork roots are abstracted as in Eq. 16 (\u201cleave one out\u201d), and thus\n\u03a0F = {\u03a0frobot} \u222a {\u03a0fright,\u03d1,\u03a0fleft,\u03d1 | \u03d1 \u2208 {empty} \u222aBalls}, \u03a0I = {\u03a0ib | b \u2208 Balls},\n\u03a0FI = {\u03a0frobot} \u222a {\u03a0fright,\u03d1,\u03a0fleft,\u03d1 | \u03d1 \u2208 {empty} \u222aBalls} \u222a {\u03a0ib | b \u2208 Balls}.\nFor each original action, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are listed in Table 5. Table 6 illustrates derivation of these numbers via decomposition of an example action Pickup(b, right, r1) in each of the fork decomposition abstractions. That action has one nonredundant representative in \u03a0frobot, two such representatives in each of \u03a0 f right,empty and \u03a0 f right,b, one representative in each \u03a0fright,b\u2032 for b \u2032 \u2208 Balls\\{b}, one representative in each \u03a0fleft,\u03d1 for \u03d1 \u2208 Balls\u222a{empty}, two representatives in \u03a0ib, and one representative in each \u03a0 i b\u2032 for b\n\u2032 \u2208 Balls \\ {b}. This results in cost 12n+5 for each representative in \u03a0F, 1 n+1 for each representative in \u03a0I, and\n1 3n+6 for each representative in \u03a0FI.\nGiven that, the optimal plans for the abstract tasks are as follows.\nh task optimal plan cost # h(I)\nhF\n\u03a0frobot \u3008Pickup(b1, right, r1), . . . , P ickup(bn, right, r1), 4n+5 2n+5\n1\n2n\u2212 2n\u22125 2n+5 ,Move(r1, r2), Drop(b1, right, r2), . . . , Drop(bn, right, r2)\u3009 \u03a0fright,\u03d1 \u3008Pickup(b1, left, r1), . . . , P ickup(bn, left, r1), 2n 2n+5 n + 1\n, Drop(b1, left, r2), . . . , Drop(bn, left, r2)\u3009 \u03a0fleft,\u03d1 \u3008Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),\n2n 2n+5\nn + 1\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)\u3009 hI \u03a0ib \u3008Pickup(b, right, r1) 1, P ickup(b, right, r1)2,Move(r1, r2), Drop(b, left, r2)2\u3009 3 n+1 + 1 n\nn 4n+1 n+1\nhFI\n\u03a0frobot \u3008Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),Move(r1, r2), 2n 3n+6 + 1 n+1 1\n4n 3 + 4n+6 3n+6\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)\u3009 \u03a0fright,\u03d1 \u3008Pickup(b1, left, r1), . . . , P ickup(bn, left, r1),\n2n 3n+6\nn + 1\n, Drop(b1, left, r2), . . . , Drop(bn, left, r2)\u3009 \u03a0fleft,\u03d1 \u3008Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),\n2n 3n+6\nn + 1\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)\u3009 \u03a0ib \u3008Pickup(b, right, r1) 1, P ickup(b, right, r1)2,Move(r1, r2), Drop(b, left, r2)2\u3009 3 3n+6 + 1 n+1 n\nAssuming n > 0 balls should be moved from one room to another, the cost of the optimal plan for the original task is 3n \u2212 1 when n is even, and 3n when n is odd. Therefore, the asymptotic performance ratios for the heuristics hF, hI, hFI on Gripper are 2/3, 0, and 4/9, respectively."}, {"heading": "9.2 Logistics", "text": "Each Logistics task consists of some k cities, x airplanes, y trucks and n packages. Each city i is associated with a set Li = {l1i . . . , l\u03b1ii } of locations within that city; the union of the locations of all the cities is denoted by L = \u22c3k i=1 Li. In addition, precisely one location in each city is an airport, and the set of airports is LA = {l11 . . . , l1k} \u2286 L. Each truck can move only within the city in which it is located, and airplanes can fly between airports. The airplanes are denoted by U = {u1, . . . , ux}, the trucks by T = {t1, . . . , ty}, and the packages by P = {p1, . . . , pn}. Let Ti = {t \u2208 T | I[t] \u2208 Li} denote the trucks of city i, and P = P1 \u222a P2 \u222a P3 \u222a P4 \u222a P5 denote a partition of the packages as follows:\n\u2022 each package in P1 = {p \u2208 P | I[p], G[p] \u2208 LA} is both initially at an airport and needs to be moved to another airport,\n\u2022 each package in P2 = {p \u2208 P | I[p] \u2208 LA \u2229 Li, G[p] \u2208 Lj \\ LA, i 6= j} is initially at an airport and needs to be moved to a non-airport location in another city,\n\u2022 each package in P3 = {p \u2208 P | I[p] \u2208 Li, G[p] \u2208 Li} needs to be moved within one city,\n\u2022 each package in P4 = {p \u2208 P | I[p] \u2208 Li \\LA, G[p] \u2208 LA \\Li} needs to be moved from a non-airport location in one city to the airport of some other city, and\n\u2022 each package in P5 = {p \u2208 P | I[p] \u2208 Li \\LA, G[p] \u2208 Lj \\LA, i 6= j} needs to be moved from a non-airport location in one city to a non-airport location in another city.\nA natural Logistics task description in sas+ is as follows.\n\u2022 Variables V = U \u222a T \u222a P with domains\n\u2200u \u2208 U : D(u) = LA, \u22001 \u2264 i \u2264 k, \u2200t \u2208 Ti : D(t) = Li,\n\u2200p \u2208 P : D(p) = L \u222a U \u222a T.\nu1 \u00b7 \u00b7 \u00b7 ux t1 \u00b7 \u00b7 \u00b7 ty\np1 \u00b7 \u00b7 \u00b7 pi \u00b7 \u00b7 \u00b7 pn\nu\np1 . . . pn\nt\np1 . . . pn p\nu1 . . . ux t1 . . . ty\nGfu, u \u2208 U Gft , t \u2208 T G ip, p \u2208 P (a) (b)\nThe (parametrized in n, x, and y) causal graph of Logistics tasks is depicted in Figure 11a."}, {"heading": "9.2.1 Fork Decomposition", "text": "Since the variables u \u2208 U and t \u2208 T have no goal value, the collection of v-forks and viforks is as in Figure 11b. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (\u201cdistance-from-initial-value\u201d), while the domains of the fork roots are abstracted\ntask, as well as the partition of the action costs between these representatives; tables (a) and (b) capture the move and load/unload actions, respectively\nas in Eq. 16 (\u201cleave-one-out\u201d). Thus, we have\n\u03a0F = \u22c3 u\u2208U \u22c3 l\u2208LA {\u03a0fu,l} \u222a k\u22c3 i=1 \u22c3 t\u2208Ti \u22c3 l\u2208Li {\u03a0ft,l},\n\u03a0I = \u22c3 p\u2208P {\u03a0ip,1} \u222a \u22c3 p\u2208P2\u222aP4\u222aP5 {\u03a0ip,2} \u222a \u22c3 p\u2208P5 {\u03a0ip,3},\n\u03a0FI = \u22c3 u\u2208U \u22c3 l\u2208LA {\u03a0fu,l} \u222a k\u22c3 i=1 \u22c3 t\u2208Ti \u22c3 l\u2208Li {\u03a0ft,l} \u222a \u22c3 p\u2208P {\u03a0ip,1} \u222a \u22c3 p\u2208P2\u222aP4\u222aP5 {\u03a0ip,2} \u222a \u22c3 p\u2208P5 {\u03a0ip,3}.\nThe total number of forks is nf = |\u03a0F| = |U | \u00b7 |LA|+ \u2211k\ni=1 |Ti| \u00b7 |Li|, and the total number of inverted forks is ni = |\u03a0I| = |P1| + 2 \u00b7 |P2| + |P3| + 2 \u00b7 |P4| + 3 \u00b7 |P5|. For each action a \u2208 A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Figure 12. Each row in the tables of Figure 12 corresponds to a certain Logistics action, each column (except for the last three) represents an abstract task, and each entry captures the number of representatives an action has in the corresponding task. The last three columns show the portion of the total cost that is given to an action representative in each task, in each of the three heuristics in question."}, {"heading": "9.2.2 Lower Bound", "text": "Note that any optimal plan for a Logistics task contains at least as many load/unload actions as move actions. Thus, the following lemma provides us with the lower bound of 1/2 for all three heuristics in question.\nLemma 1 For any Logistics task, hF, hI, and hFI account for the full cost of the load/unload actions required by any optimal plan for that task.\nProof: For any Logistics task, all the optimal plans for that task contain the same amount of load/unload actions for each package p \u2208 P as follows. p \u2208 P1: 2 actions \u2014 one load onto an airplane, and one unload from that airplane, p \u2208 P2: 4 actions \u2014 one load onto an airplane, one unload from that airplane, one load\nonto a truck, and one unload from that truck,\np \u2208 P3: 2 actions \u2014 one load onto a truck, and one unload from that truck, p \u2208 P4: 4 actions \u2014 one load onto a truck, one unload from that truck, one load onto an\nairplane, and one unload from that airplane, and\np \u2208 P5: 6 actions \u2014 two loads onto some trucks, two unloads from these trucks, one load onto an airplane, and one unload from that airplane.\nConsider the fork-decomposition \u03a0F. Any optimal plan for each of the abstract tasks will contain the number of load/unload actions exactly as above (the effects of these actions remain unchanged in these tasks). The cost of each representative of each load/unload action is 1\nnf , and there are nf abstract tasks. Therefore, the heuristic hF fully accounts for\nthe cost of the required load/unload actions. Now consider the fork-decomposition \u03a0I. With m being the domain-decomposition index of the abstraction, any optimal plan for the abstract task \u03a0ip,m will include one load and one unload actions as follows.\np \u2208 P1: one load onto an airplane and one unload from that airplane, p \u2208 P2,m=1: one load onto an airplane and one unload from that airplane, p \u2208 P2,m=2: one load onto a truck and one unload from that truck, p \u2208 P3: one load onto a truck and one unload from that truck, p \u2208 P4,m=1: one load onto a truck and one unload from that truck, p \u2208 P4,m=2: one load onto an airplane, and one unload from that airplane, p \u2208 P5,m=1: one load onto a truck and one unload from that truck, p \u2208 P5,m=2: one load onto an airplane and one unload from that airplane, and p \u2208 P5,m=3: one load onto a truck and one unload from that truck. The cost of each representative of load/unload actions is 1, and thus the heuristic hI fully accounts for the cost of the required load/unload actions.\nFinally, consider the fork-decomposition \u03a0FI. Any optimal plan for each of the forkstructured abstract tasks will contain the same number of load/unload actions as for \u03a0F. The cost of each representative of load/unload actions is 1\nnf+1 and there are nf such abstract\ntasks. In addition, each of these load/unload actions will also appear in exactly one inverted fork-structured abstract task. Therefore the heuristic hFI also fully accounts for the cost of the required load/unload actions.\nthe upper bound of 1/2"}, {"heading": "9.2.3 Upper Bound", "text": "An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t1, t2, no airplanes, one city, and n packages such that the initial and goal locations of all the packages are all pairwise different, and both trucks are initially located at yet another location. More formally, if L = {li}2ni=0, and T = {t1, t2}, then the sas+ encoding for this Logistics task is as follows.\n\u2022 Variables V = {t1, t2, p1, . . . , pn} with domains\n\u2200t \u2208 T : D(t) = L, \u2200p \u2208 P : D(p) = L \u222a T.\n\u2022 Initial state I = {t1 : l0, t2 : l0, p1 : l1, . . . , pn : ln}.\n\u2022 Goal G = {p1 : ln+1, . . . , pn : l2n}.\n\u2022 Actions A = {Lt(p, t, l), Ut(p, t, l) | l \u2208 L, t \u2208 T, p \u2208 P} \u222a {Mt(t, l, l\u2032) | t \u2208 T, {l, l\u2032} \u2286 L}.\nThe collection of v-forks and v-iforks for this task is depicted in Figure 13. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (\u201cdistance-from-initial-value\u201d), while the domains of the fork roots are abstracted as in Eq. 16 (\u201cleave-one-out\u201d), and therefore we have\n\u03a0F = {\u03a0ft1,l\u03a0ft2,l | l \u2208 L}, \u03a0I = {\u03a0ip,1 | p \u2208 P},\n\u03a0FI = {\u03a0ft1,l\u03a0ft2,l | l \u2208 L} \u222a {\u03a0ip,1 | p \u2208 P}.\nThe total number of forks is thus nf = 4n + 2 and the total number of inverted forks is ni = n. The partition of the action costs for Logistics tasks is described in Figure 12. Here we have P = P3 and thus the action cost partition is as follows.\nAction \u03a0ft,l \u03a0 f t,l\u2032 \u03a0 f t,l\u2032\u2032 \u03a0 f t\u2032,l\u2217 \u03a0 i p,1 \u03a0 i p\u2032,1 \u03a0F \u03a0I \u03a0FI Mt(t, l, l\u2032) 1 1 0 0 1 0 1 2 1 n 1 n+2 Lt(p, t, l) 1 1 1 1 1 0 1 4n+2 1 1 4n+3 Ut(p, t, l) 1 1 1 1 1 0 1 4n+2 1 1 4n+3\nGiven that, the optimal plans for the abstract task are\nh task optimal plan cost # h(I)\nhF \u03a0ft1,l \u3008Lt(p1, t2, l1), . . . , Lt(pn, t2, ln), Ut(p1, t2, ln+1), . . . , Ut(pn, t2, l2n)\u3009\n2n 4n+2\n2n+ 1 2n\n\u03a0ft2,l \u3008Lt(p1, t1, l1), . . . , Lt(pn, t1, ln), Ut(p1, t1, ln+1), . . . , Ut(pn, t1, l2n)\u3009 2n 4n+2 2n+ 1\nhI \u03a0ipi,1 \u3008Mt(t1, l0, li), Lt(pi, t1, li),Mt(t1, li, ln+i), Ut(pi, t1, ln+i)\u3009 2 n + 2 n 2n+ 2 hFI \u03a0ft1,l \u3008Lt(p1, t2, l1), . . . , Lt(pn, t2, ln), Ut(p1, t2, ln+1), . . . , Ut(pn, t2, l2n)\u3009 2n 4n+3 2n+ 1 2n+ 2n\nn+2\u03a0 f t2,l \u3008Lt(p1, t1, l1), . . . , Lt(pn, t1, ln), Ut(p1, t1, ln+1), . . . , Ut(pn, t1, l2n)\u3009 2n4n+3 2n+ 1 \u03a0ipi,1 \u3008Mt(t1, l0, li), Lt(pi, t1, li),Mt(t1, li, ln+i), Ut(pi, t1, ln+i)\u3009 2 n+2 + 2 4n+3 n\nwhile an optimal plan for the original task, e.g., \u3008Mt(t1, l0, l1), Lt(p1, t1, l1),Mt(t1, l1, l2), Lt(p2, t1, l2), Mt(t1, l2, l3), . . . , Lt(pn, t1, ln),Mt(t1, ln, ln+1), Ut(p1, t1, ln+1),Mt(t1, ln+1, ln+2), Ut(p2, t1, ln+2),\nMt(t1, ln+2, ln+3), . . . , Ut(pn, t1, l2n)\u3009, has the cost of 4n, providing us with the upper bound of 1/2 for all three heuristics. Putting our lower and upper bounds together, the asymptotic ratio of all three heuristics in question is 1/2."}, {"heading": "9.3 Blocksworld", "text": "Each Blocksworld task consists of a table table, a crane c, and n + 1 blocks B = {b1, . . . , bn+1}. Each block can be either on the table, or on top of some other block, or held by the crane. The crane can pick up a block if it currently holds nothing, and that block has no other block on top of it. The crane can drop the held block on the table or on top of some other block.\nConsider now a Blocksworld task as follows. The blocks initially form a tower b1, . . . , bn, bn+1 with bn+1 being on the table, and the goal is to move them to form a tower b1, . . . , bn\u22121, bn+1, bn with bn being on the table. That is, the goal is to swap the lowest two blocks of the tower. A natural description of this task in sas+ is as follows.\n\u2022 Variables V = {b, clearb | b \u2208 B} \u222a {c} with domains\nD(c) = {empty} \u222aB, \u2200b \u2208 B : D(b) = {table, c} \u222aB \\ {b}, D(clearb) = {yes, no}.\n\u2022 Initial state\nI = {c :empty, bn+1 : table, clearb1 :yes} \u22c3\n{bi :bi+1 | 1 \u2264 i \u2264 n} \u22c3\n{clearb :no | b \u2208 B \\ {b1}} .\n\u2022 Goal G = {bn : table, bn+1 :bn, bn\u22121 :bn+1} \u222a {bi :bi+1 | 1 \u2264 i \u2264 n\u2212 2}.\n\u2022 Actions A = {PT (b), DT (b) | b \u2208 B} \u222a {P (b, b\u2032), D(b, b\u2032) | {b, b\u2032} \u2286 B} where\n\u2013 pick block b from the table: PT (b) = \u3008{c :empty, b : table, clearb :yes}, {cb, b :c}\u3009, \u2013 pick block b from block b\u2032: P (b, b\u2032) = \u3008{c :empty, b :b\u2032, clearb :yes, clearb\u2032 :no}, {c :b, b :c, clearb\u2032 :yes}\u3009,\nthe Blocksworld task used in the proof\n\u2013 drop block b on the table: DT (b) = \u3008{c :b, b :c}, {c :empty, b : table}\u3009, and \u2013 drop block b on block b\u2032: D(b, b\u2032) = \u3008{c :b, b :c, clearb\u2032 :yes}, {c :empty, b :b\u2032, clearb\u2032 :no}\u3009.\nA schematic version of the causal graph of this task is depicted in Figure 14a. Since only the variables bn\u22121, bn, bn+1 have goal values that are different from their values in the initial state, the collection of v-forks and v-iforks is as in Figure 14b. After the (\u201cleave-one-out,\u201d Eq. 16) domain abstraction of the variable c, c-fork Gfc breaks down into n + 2 abstract tasks. The sinks of v-iforks G ibn\u22121 , G ibn , and G ibn+1 also go through the process of domain decomposition (\u201cdistance-from-initial-value,\u201d Eq. 15). However, due to the structure of the domain transition graphs of the block variables, domain decomposition here results in only a single abstract task for each of the v-iforks. Thus we have\n\u03a0F ={\u03a0fc,empty} \u222a {\u03a0fc,b | b \u2208 B} \u222a {\u03a0fclearb | b \u2208 B}, \u03a0I ={\u03a0ibn\u22121,1,\u03a0ibn,1,\u03a0ibn+1,1},\n\u03a0FI ={\u03a0fc,empty} \u222a {\u03a0fc,b | b \u2208 B} \u222a {\u03a0fclearb | b \u2208 B} \u222a\u03a0 i bn\u22121,1,\u03a0 i bn,1,\u03a0 i bn+1,1}.\nIt is technically straightforward to verify that, for each abstract task in \u03a0F, \u03a0I, and \u03a0FI, there exists a plan that (i) involves only the representatives of the actions\n{P (bn\u22121, bn), DT (bn\u22121), P (bn, bn+1), DT (bn), PT (bn+1), D(bn+1, bn), PT (bn\u22121), D(bn\u22121, bn+1)} , (21)\nand (ii) involves each representative of each original action at most once. Even if together these plans account for the total cost of all eight actions in Eq. 21, the total cost of all these plans (and thus the estimates of all the three heuristics) is upper-bounded by 8, while an optimal plan for the original task, e.g., \u3008P (b1, b2), DT (b1), P (b2, b3), DT (b2), . . . , P (bn, bn+1), DT (bn), PT (bn+1), D(bn+1, bn), PT (bn\u22121), D(bn\u22121, bn+1), PT (bn\u22122), D(bn\u22122, bn\u22121), . . . , PT (b1), D(b1, b2)\u3009, has a cost\nv-iforks\nof 4n. Hence, the asymptotic performance ratio of all three heuristics on the Blocksworld domain is 0."}, {"heading": "9.4 Miconic", "text": "Each Miconic task consists of one elevator e, a set of floors F , and the passengers P . The elevator can move between |F | floors and on each floor it can load and/or unload passengers. A natural sas+ description of a Miconic task is as follows.\n\u2022 Variables V = {e} \u222a P with domains D(e) = F,\n\u2200p \u2208 P : D(p) = F \u222a {e}.\n\u2022 Initial state I = {e :fe} \u222a {p :fp | p \u2208 P} \u2208 (F )|P |+1. \u2022 Goal G = {p :f \u2032p | p \u2208 P} \u2208 (F )|P |. \u2022 Actions A = {In(p, f), Out(p, f) | f \u2208 F, p \u2208 P} \u222a {Move(f, f \u2032) | {f, f \u2032} \u2286 F}, where\n\u2013 load passenger p into e on floor f : In(p, f) = \u3008{e :f, p :f}, {p :e}\u3009, \u2013 unload passenger p from e to floor f : Out(p, f) = \u3008{e :f, p :e}, {p :f}\u3009, and \u2013 move elevator from floor f to floor f \u2032: Move(f, f \u2032) = \u3008{e :f}, {e :f \u2032}\u3009.\nThe (parametrized in n) causal graph of Miconic tasks is depicted in Figure 15a, and Figure 15b depicts the corresponding collection of v-forks and v-iforks. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (\u201cdistance-from-initial-value\u201d), and the domains of the fork roots are abstracted as in Eq. 16 (\u201cleave-one-out\u201d). Thus, we have\n\u03a0F = {\u03a0fe,f | f \u2208 F}, \u03a0I = {\u03a0ip,1 | p \u2208 P},\n\u03a0FI = {\u03a0fe,f | f \u2208 F} \u222a {\u03a0ip,1 | p \u2208 P}. The total number of the fork-structured abstract tasks is thus nf = |\u03a0F| = |F | and the total number of the inverted fork structured abstract tasks is ni = |\u03a0I| = |P |. For each action a \u2208 A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Table 7.\nas well as the partition of the action costs among these representatives"}, {"heading": "9.4.1 Lower Bounds", "text": "First, as Miconic is a special case of the Logistics domain, Lemma 1 applies here analogously, with each package in P3 corresponding to a passenger. Thus, for each p \u2208 P , all three heuristics account for the full cost of the load/unload actions required by any optimal plan for that task.\nLet us now focus on the abstract tasks \u03a0F = {\u03a0fe,f | f \u2208 F}. Recall that the task \u03a0fe,f is induced by an e-fork and, in terms of domain decomposition, distinguishes between being at floor f and being somewhere else. Without loss of generality, the set of floors F can be restricted to the initial and the goal values of the variables, and this because no optimal plan will move the elevator to or from a floor f that is neither an initial nor a goal location of a passenger or the elevator. Let FI = {I[p] | p \u2208 P} and FG = {G[p] | p \u2208 P}. The costs of the optimal plans for each abstract task \u03a0fe,f are as follows.\nf \u2208 FI \u2229 FG : Let p, p\u2032 \u2208 P be a pair of passengers with initial and goal locations in f , respectively; that is, I[p] = G[p\u2032] = f . If f = I[e], then any plan for \u03a0fe,f has to move the elevator from f in order to load passenger p\u2032, and then move the elevator back to f in order to unload passenger p\u2032. Therefore the cost of any plan for \u03a0fe,f is at\nleast 2|P ||F | + 1, where (see the last three columns of Table 7) the first component of the\nsummation comes from summing the costs of the representatives of the load/unload actions for all the passengers, and the second component is the sum of the costs of representatives of the two respective move actions. Similarly, if f 6= I[e], then any plan for \u03a0fe,f has to move the elevator to f in order to load passenger p, and then move the elevator from f in order to unload p. Therefore, here as well, the cost of any plan for \u03a0fe,f is at least 2|P | |F | + 1.\nf \u2208 FI \\ FG : Let p \u2208 P be a passenger initially at f , that is, I[p] = f . If f = I[e], then any plan for \u03a0fe,f has to move the elevator from f in order to unload p, and thus the\ncost of any plan for \u03a0fe,f is at least 2|P | |F | + 1 2 . Otherwise, if f 6= I[e], then any plan for \u03a0fe,f has to move the elevator to f in order to load p, and then move the elevator from f in order to unload p. Hence, in this case, the cost of any plan for \u03a0fe,f is at least 2|P ||F | + 1.\nf \u2208 FG \\ FI : Let p \u2208 P be a passenger who must arrive at floor f , that is, G[p] = f . If f = I[e], then any plan for \u03a0fe,f has to move the elevator from f in order to load p, and then move the elevator back to f in order to unload p. Hence, here as well, the cost of any plan for \u03a0fe,f is at least 2|P | |F | + 1. Otherwise, if f 6= I[e], then any plan for\n\u03a0fe,f has to move the elevator to f in order to unload p, and thus the cost of any plan for \u03a0fe,f is at least 2|P | |F | + 1 2 .\nf 6\u2208 FG \u222a FI : If f = I[e], then any plan for \u03a0fe,f has to include a move from f in order to load/unload the passengers, and thus the cost of any plan for \u03a0fe,f is at least 2|P | |F | + 1 2 .\nOtherwise, if f 6= I[e], the elevator is initially \u201cin the set of all other locations,\u201d and thus the cost of any plan for \u03a0fe,f is at least 2|P | |F | .\nPutting this case-by-case analysis together, we have\nhF(I) \u2265  2|P |+ |FI \u2229 FG|+ |FI \\ FG|+ |FG\\FI |2 , I[e] \u2208 FI \u2229 FG 2|P |+ |FI \u2229 FG|+ |FI \\ FG| \u2212 1 + 12 + |FG\\FI | 2 , I[e] \u2208 FI \\ FG\n2|P |+ |FI \u2229 FG|+ |FI \\ FG|+ 1 + |FG\\FI |\u221212 , I[e] \u2208 FG \\ FI 2|P |+ |FI \u2229 FG|+ |FI \\ FG|+ |FG\\FI |\u221212 + 12 , I[e] 6\u2208 FG \u222a FI\n.\nNote that the value in the second case is the lowest. This gives us a lower bound on the hF\nestimate as in Eq. 22.\nhF(I) \u2265 2|P |+ |FI \\ FG|+ |FG \\ FI |\n2 + |FI \u2229 FG| \u2212\n1 2 . (22)\nNow, let us provide an upper bound on the length (= cost) of the optimal plan for a Miconic task. First, let P \u2032 \u2286 P denote the set of passengers with both initial and goal locations in FI \u2229 FG. Let m(P \u2032, FI \u2229 FG) denote the length of the optimal traversal of the floors FI \u2229FG such that, for each passenger p \u2208 P \u2032, a visit of I[p] comes before some visit of G[p]. Given that, on a case-by-case basis, a (not necessarily optimal) plan for the Miconic task at hand is as follows.\nI[e] \u2208 FI \u2229 FG : Collect all the passengers at I[e] if any, then traverse all the floors in FI \\ FG and collect passengers from these floors, then move the elevator to the first floor f on the optimal path \u03c0 traversing the floors FI \u2229 FG, drop off the passengers whose destination is f , collect the new passengers if any, keep moving along \u03c0 while collecting and dropping off passengers at their initial and target floors, and then traverse FG \\ FI , dropping off the remaining passengers at their destinations. The cost of such a plan (and thus of the optimal plan) is upper-bounded as in Eq. 23 below.\nh\u2217(I) \u2264 2|P |+ |FI \\ FG|+m(P \u2032, FI \u2229 FG) + |FG \\ FI |. (23)\nI[e] \u2208 FI \\ FG : Collect all the passengers at I[e] if any, then traverse all the floors in FI \\FG and collect passengers from these floors while making sure that this traversal ends up at the first floor f of the optimal path \u03c0 traversing the floors FI \u2229 FG, then follow \u03c0 while collecting and dropping passengers off at their initial and target floors, and then traverse FG\\FI , dropping the remaining passengers off at their destinations. As in the first case, the cost of such a plan is upper-bounded as in Eq. 23.\nI[e] 6\u2208 FI : Traverse the floors FI \\FG and collect all the passengers from these floors, then move along the optimal path \u03c0 traversing the floors FI \u2229 FG while collecting and dropping off passengers at their initial and target floors, and then traverse the floors FG \\FI , dropping the remaining passengers off at their destinations. Here as well, the cost of such a plan is upper-bounded by the expression in Eq. 23.\nLemma 2 For any Miconic task with passengers P , we have h F(I) h\u2217(I) \u2265 5|P |\u22121 6|P | .\nProof: Recall that P \u2032 \u2286 P is the set of all passengers with both initial and goal locations in FI \u2229 FG. First we give two upper bounds on the length of the optimal traversal of the floors FI \u2229 FG such that, for each passenger p \u2208 P \u2032, a visit of I[p] comes before some visit of G[p]. From Theorem 5.3.3 of Helmert (2008) we have\nm(P \u2032, FI \u2229 FG) = |FI \u2229 FG|+m\u2217(G\u2032), (24)\nwhere m\u2217(G\u2032) is the size of the minimum feedback vertex set of the directed graph G\u2032 = (V \u2032, E \u2032), with V \u2032 = FI \u2229 FG and E \u2032 containing an arc from f to f \u2032 if and only if a passenger p \u2208 P \u2032 is initially at floor f and should arrive at floor f \u2032.\nNote that m\u2217(G\u2032) is trivially bounded by the number of graph nodes V \u2032. In addition, observe that, for any order of the nodes V \u2032, the arcs E \u2032 can be partitioned into \u201cforward\u201d and \u201cbackward\u201d arcs, and one of these subsets must contain no more than |E\n\u2032| 2 arcs. Removing\nfrom G\u2032 all the nodes that are origins of the arcs in that smaller subset of E \u2032 results in a directed acyclic graph. Hence, the set of removed nodes is a (not necessarily minimum) feedback vertex set of G\u2032, and the size of this set is no larger than |E \u2032|2 . Putting these two bounds on m\u2217(G\u2032) together with Eq. 24 we obtain\nm(P \u2032, FI \u2229 FG) \u2264 min {\n2|FI \u2229 FG|, |FI \u2229 FG|+ |P \u2032| 2\n} . (25)\nFrom the disjointness of FG \\ FI and FI \u2229 FG, and the fact that the goal of all the passengers in P \u2032 is in FI , we have |FG \\ FI | \u2264 |P | \u2212 |P \u2032|. From Eqs. 22 and 23 we have\nhF h\u2217 \u2265 2|P |+ |FI \\ FG|+ |FG\\FI | 2 + |FI \u2229 FG| \u2212 12 2|P |+ |FI \\ FG|+ |FG \\ FI |+m(P \u2032, FI \u2229 FG) . (26)\nAs we are interested in a lower bound on the ratio h F\nh\u2217 , the right-hand side of the inequality should be minimized, and thus we can safely set |FI \\ FG| = 0 and |FG \\ FI | = |P | \u2212 |P \u2032|, obtaining\nhF h\u2217 \u2265 2|P |+\n|P |\u2212|P \u2032| 2 + |FI \u2229 FG| \u2212 12\n2|P |+ |P | \u2212 |P \u2032|+m(P \u2032, FI \u2229 FG) = 5|P | \u2212 |P \u2032|+ 2|FI \u2229 FG| \u2212 1 6|P | \u2212 2|P \u2032|+ 2m(P \u2032, FI \u2229 FG) . (27)\nLet us examine the right-most expression in Eq. 27 with respect to the two upper bounds on m(P \u2032, FI \u2229 FG) as in Eq. 25. \u2022 If the minimum is obtained on 2|FI \u2229 FG|, then m(P \u2032, FI \u2229 FG) \u2264 2|FI \u2229 FG| \u2264 |FI \u2229 FG|+ |P \u2032| 2 , where the last inequality can be reformulated as\n2|FI \u2229 FG| \u2212 |P \u2032| \u2264 0.\nThis allows us to provide a lower bound on the right-most expression in Eq. 27, and thus on h F\nh\u2217 as\nhF h\u2217 \u2265 5|P | \u2212 |P \u2032|+ 2|FI \u2229 FG| \u2212 1 6|P | \u2212 2|P \u2032|+ 2m(P \u2032, FI \u2229 FG) \u2265 5|P |+ (2|FI \u2229 FG| \u2212 |P \u2032|)\u2212 1 6|P |+ 2(2|FI \u2229 FG| \u2212 |P \u2032|) \u2265 5|P | \u2212 1\n6|P | . (28)\n\u2022 If the minimum is obtained on |FI\u2229FG|+ |P \u2032| 2 , then m(P \u2032, FI\u2229FG) \u2264 |FI\u2229FG|+ |P \u2032| 2 <\n2|FI \u2229 FG|, where the last inequality can be reformulated as\n2|FI \u2229 FG| \u2212 |P \u2032| > 0.\nThis again allows us to provide a lower bound on h F\nh\u2217 via Eq. 27 as\nhF h\u2217 \u2265 5|P | \u2212 |P \u2032|+ 2|FI \u2229 FG| \u2212 1 6|P | \u2212 2|P \u2032|+ 2m(P \u2032, FI \u2229 FG) \u2265 5|P |+ (2|FI \u2229 FG| \u2212 |P \u2032|)\u2212 1 6|P |+ (2|FI \u2229 FG| \u2212 |P \u2032|) \u2265 5|P | \u2212 1\n6|P | . (29)\nNote that both lower bounds on h F\nh\u2217 in Eq. 28 and Eq. 29 are as required by the claim of the lemma."}, {"heading": "9.4.2 Upper Bounds", "text": "A Miconic task on which the heuristic hF achieves the performance ratio of exactly 5/6 consists of an elevator e, floors F = {fi}ni=0, passengers P = {pi}ni=1, all the passengers and the elevator being initially at f0, and the target floors of the passengers all being pairwise disjoint. The sas+ encoding for the Miconic task is as follows.\n\u2022 Variables V = {e} \u222a P with the domains D(e) = F and \u2200p \u2208 P : D(p) = F \u222a {e}.\n\u2022 Initial state I = {e :f0, p1 :f0, . . . , pn :f0}.\n\u2022 Goal G = {p1 :f1, . . . , pn :fn}.\n\u2022 Actions A = {In(p, f), Out(p, f) | f \u2208 F, p \u2208 P} \u222a {Move(f, f \u2032) | {f, f \u2032} \u2286 F}.\nThe causal graph of this task and the corresponding collection of v-forks (consisting of only one e-fork) are depicted in Figure 15. The domain of e is abstracted as in Eq. 16 (\u201cleave-one-out\u201d), providing us with\n\u03a0F = {\u03a0fe,f0 ,\u03a0fe,f1 , . . . ,\u03a0fe,fn}.\nThe costs of the action representatives in these abstract tasks are given in Table 7 with nf = n+ 1. The optimal plans for the abstract tasks in \u03a0F are\ntask optimal plan cost # hF(I)\n\u03a0fe,f0 \u3008In(p1, f0), . . . , In(pn, f0),Move(f0, f1), Out(p1, f1), . . . , Out(pn, fn)\u3009 1 2 + 2n n+1 n+ 1 5n+1 2\u03a0\nf e,f1 \u3008In(p1, f0), . . . , In(pn, f0), Out(p2, f2), . . . , Out(pn, fn),Move(f0, f1), Out(p1, f1)\u3009 12 + 2n n+1\n\u03a0fe,fn \u3008In(p1, f0), . . . , In(pn, f0), Out(p1, f1), . . . , Out(pn\u22121, fn\u22121),Move(f0, fn), Out(pn, fn)\u3009 1 2 + 2n n+1\nwhile an optimal plan for the original task, \u3008In(p1, f0), . . . , In(pn, f0),Move(f0, f1), Out(p1, f1), Move(f1, f2), Out(p2, f2),Move(f2, f3), . . . , Out(pn, fn)\u3009, has a cost of 3n, providing us with the upper bound of 5/6 for the hF heuristic in Miconic. Putting this upper bound together with the previously obtained lower bound of 5/6, we conclude that the asymptotic performance ratio of hF in Miconic is 5/6.\nA Miconic task on which the heuristics hI and hFI achieve exactly 1/2 consists of an elevator e, floors F = {fi}2ni=0, passengers P = {pi}ni=1, and the initial and target floors for all the passengers and the elevator being pairwise disjoint. The task description in sas+ is as follows.\n\u2022 Variables V = {e} \u222a P with the domains D(e) = F and \u2200p \u2208 P : D(p) = F \u222a {e}.\n\u2022 Initial state I = {e :f0, p1 :f1, . . . , pn :fn}.\n\u2022 Goal G = {p1 :fn+1, . . . , pn :f2n}.\n\u2022 Actions A = {In(p, f), Out(p, f) | f \u2208 F, p \u2208 P} \u222a {Move(f, f \u2032) | {f, f \u2032} \u2286 F}.\nThe causal graph of this task and the corresponding collection of v-forks and v-iforks are depicted in Figure 15. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (\u201cdistance-from-initial-value\u201d), and the domains of the fork roots are all abstracted as in Eq. 16 (\u201cleave-one-out\u201d). This provides us with\n\u03a0I = {\u03a0ip1,1, . . . ,\u03a0ipn,1}, \u03a0FI = {\u03a0fe,f0 ,\u03a0fe,f1 , . . . ,\u03a0fe,fn ,\u03a0fe,fn+1 , . . . ,\u03a0fe,f2n ,\u03a0ip1,1, . . . ,\u03a0ipn,1}.\nThe costs of the action representatives in these abstract tasks are given in Table 7 with nf = 2n+ 1 and ni = n. The optimal plans for the abstract tasks in \u03a0I and \u03a0FI are\nh task optimal plan cost # h(I)\nhI \u03a0ipi,1 \u3008Move(f0, fi), In(pi, fi),Move(fi, fn+i), Out(pi, fn+i)\u3009 2 n + 2 n 2n+ 2 hFI \u03a0fe,f0 \u3008Move(f0, f1), In(p1, f1), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn, f2n)\u3009 1 n+2 + 2n 2n+2 1 2n+ 5n+1 n+2\n\u03a0fe,f1 \u3008Move(f0, f1), In(p1, f1),Move(f1, f2), In(p2, f2), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn, f2n)\u3009\n2 n+2 + 2n 2n+2 n\n\u03a0fe,fn \u3008Move(f0, fn), In(pn, fn),Move(fn, f1), In(p1, f1), . . . , In(pn\u22121, fn\u22121), Out(p1, fn+1), . . . , Out(pn, f2n)\u3009\n2 n+2 + 2n 2n+2\n\u03a0fe,fn+1 \u3008In(p1, f1), . . . , In(pn, fn), Out(p2, fn+2), . . . , Out(pn, f2n), Move(f0, fn+1), Out(p1, fn+1)\u3009\n1 n+2 + 2n 2n+2 n\n\u03a0fe,f2n \u3008In(p1, f1), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn\u22121, f2n\u22121), Move(f0, f2n), Out(pn, f2n)\u3009\n1 n+2 + 2n 2n+2\n\u03a0ipi,1 \u3008Move(f0, fi), In(pi, fi),Move(fi, fn+i), Out(pi, fn+i)\u3009 2 n+2 + 2 2n+2 n\nwhile an optimal plan for the original task, \u3008Move(f0, f1), In(p1, f1),Move(f1, f2), In(p2, f2), Move(f2, f3), . . . , In(pn, fn),Move(fn, fn+1), Out(p1, fn+1),Move(fn+1, fn+2), Out(p2, fn+2),\nMove(fn+2, fn+3), . . . , Out(pn, f2n)\u3009, has the cost of 4n, providing us with the upper bound of 1/2 for the hI and hFI heuristics in Miconic. Putting this upper bound together with the previously obtained lower bound of 1/2, we conclude that the asymptotic performance ratio of hI and hFI in Miconic is 1/2."}, {"heading": "9.5 Satellite", "text": "The Satellite domain is quite complex. A Satellite tasks consists of some satellites S, each s \u2208 S with a finite set of instruments Is onboard, I = \u22c3 s\u2208S Is. There is a set of image modes M, and for each mode m \u2208 M, there is a set Im \u2286 I of instruments supporting mode m. Likewise, there is a set of directions L, image objectives O \u2286 L\u00d7M, and functions cal : I 7\u2192 L, p0 : S 7\u2192 L, and p\u2217 : S0 7\u2192 L with S0 \u2286 S, where cal is the calibration target direction function, p0 is the initial direction function, and p\u2217 is the goal pointing direction function.\nLet us denote by Oi = {o = (d,m) \u2208 O | i \u2208 Im} the subset of all images that can be taken by instrument i, by Os = \u22c3i\u2208Is Oi the subset of all images that can be taken by instruments on satellite s, and by Sm = {s | Is \u2229 Im 6= \u2205} the subset of all satellites that can take images in mode m. The problem description in sas+ is as follows.\n\u2022 Variables V = S \u222a {Oni,Ci | i \u2208 I} \u222a O with domains\n\u2200s \u2208 S : D(s) = L, \u2200i \u2208 I : D(Oni) = D(Ci) = {0, 1}, \u2200o \u2208 O : D(o) = {0, 1}.\n\u2022 Initial state I = {s :p0(s) | s \u2208 S} \u222a {Oni :0,Ci :0 | i \u2208 I} \u222a {o :0 | o \u2208 O}.\n\u2022 Goal G = {s :p\u2217(s) | s \u2208 S0} \u222a {o :1 | o \u2208 O}.\n\u2022 Actions A = \u22c3 s\u2208S ( {Turn(s, d, d\u2032) | {d, d\u2032} \u2286 L} \u222a {SwOn(i, s), Cal(i, s), SwOff(i) | i \u2208 Is} ) \u222a\n{TakeIm(o, d, s, i) | o = (d,m) \u2208 O, s \u2208 Sm, i \u2208 Im \u2229 Is},\nwhere\n\u2013 turn satellite: Turn(s, d, d\u2032) = \u3008{s :d}, {s :d\u2032}\u3009, \u2013 power on instrument: SwOn(i, s) = \u3008{Oni\u2032 :0 | i\u2032 \u2208 Is}, {Oni :1}\u3009, \u2013 power off instrument: SwOff(i) = \u3008{Oni :1}, {Oni :0,Ci :0}\u3009, \u2013 calibrate instrument: Cal(i, s) = \u3008{Ci :0, Oni :1, s :cal(i)}, {Ci :1}\u3009, and \u2013 take an image: TakeIm(o, d, s, i) = \u3008{o :0,Ci :1, s :d}, {o :1}\u3009."}, {"heading": "9.5.1 Fork Decomposition", "text": "The causal graph of an example Satellite task and a representative subset of the collection of v-forks and v-iforks are depicted in Figure 16. Since the variables {Oni,Ci | i \u2208 I}\u222aS\\S0 have no goal value, the collection of v-forks and v-iforks will be as follows in the general case.\n\u2022 For each satellite s \u2208 S, an s-fork with the leaves Os.\ncollection of v-forks and v-iforks\n\u2022 For each instrument i \u2208 I, a Ci-fork with the leaves Oi.\n\u2022 For each image objective o = (d,m) \u2208 O, a o-ifork with the parents {Ci | i \u2208 Im}\u222aSm.\nThe root domains of all forks rooted at instruments i \u2208 I and of all the inverted-fork sinks are binary in the first place, and the root domains of the forks rooted at satellites s \u2208 S are abstracted as in Eq. 16 (\u201cleave-one-out\u201d). This provides us with\n\u03a0F = {\u03a0fs,d | s \u2208 S, d \u2208 L} \u222a {\u03a0fCi | i \u2208 I}, \u03a0I = {\u03a0io | o \u2208 O},\n\u03a0FI = {\u03a0fs,d | s \u2208 S, d \u2208 L} \u222a {\u03a0fCi | i \u2208 I} \u222a {\u03a0io | o \u2208 O}.\nThe total number of forks is thus nf = |S| \u00b7 |L| + |I| and the total number of inverted forks is ni = |O|. For each action a \u2208 A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Figure 17."}, {"heading": "9.5.2 Lower Bounds", "text": "First, note that any optimal plan for a Satellite task contains at most 6 actions per image objective o \u2208 O and one action per satellite s \u2208 S0 such that I[s] 6= G[s]. Now we show that each of the three heuristics fully account for the cost of at least one action per image objective o \u2208 O and one action per such a satellite. This will provide us with the lower bound of 1/6 on the asymptotic performance ratios of our three heuristics.\nLemma 3 For any Satellite task, hF, hI, and hFI fully account for the cost of at least one Take Image action TakeIm(o, d, s, i) for each image objective o \u2208 O.\nProof: For an image objective o = (d,m) \u2208 O, some actions TakeIm(o, d, s, i) = \u3008{o : 0,Ci : 1, s : d}, {o : 1}\u3009 will appear in optimal plans for |Sm| \u00b7 |L| fork abstract tasks rooted\ntask, as well as the partition of the action costs between these representatives; table (a) shows Turn, Switch On, Switch Off, and Calibrate actions, and table (b) shows Take Image actions\nin satellites, |Im| fork abstract tasks rooted in instrument calibration status variables Ci, and one inverted-fork abstract task with sink o. Together with the costs of the action representatives in the abstract problems (see Figure 17), we have\nhF : cost of each representative is 1|Sm|\u00b7|L|+|Im| and there are |Sm| \u00b7 |L|+ |Im| fork abstract tasks,\nhI : cost of each representative is 1 and there is one inverted fork abstract task, and\nhFI : cost of each representative is 1|Sm|\u00b7|L|+|Im|+1 and there are |Sm| \u00b7 |L|+ |Im|+ 1 abstract tasks.\nTherefore, for each o \u2208 O, the cost of one TakeIm(o, d, s, i) action will be fully accounted for by each of the three heuristics.\nLemma 4 For any Satellite task, hF, hI, and hFI fully account for the cost of at least one Turn action Turn(s, d, d\u2032) for each s \u2208 S0 such that I[s] 6= G[s].\nProof: If s \u2208 S0 is a satellite with I[s] 6= G[s], then an action Turn(s, I[s], d\u2032) will appear in any optimal plan for \u03a0fs,I[s], an action Turn(s, d,G[s]) will appear in any optimal plan for \u03a0fs,G[s], and for each o \u2208 Os, an action Turn(s, d,G[s]) will appear in any optimal plan for \u03a0io. Together with the costs of the action representatives in the abstract problems (see Figure 17) we have\nhF : cost of each representative is 12 and there are 2 fork abstract tasks,\nhI : cost of each representative is 1|Os| and there are |Os| inverted fork abstract tasks, and\nhFI : cost of each representative is 1|Os|+2 and there are |Os|+ 2 abstract tasks.\nTherefore, for each s \u2208 S0 such that I[s] 6= G[s], the cost of one Turn(s, d, d\u2032) action will be fully accounted for by each of the three heuristics.\nTogether, Lemmas 3 and 4 imply that, for h \u2208 {hF, hI, hFI}, on Satellite we have h h\u2217 \u2265 1/6."}, {"heading": "9.5.3 Upper Bound", "text": "A Satellite task on which all three heuristics achieve the ratio of exactly 1/6 consists of two identical satellites S = {s, s\u2032} with l instruments each, I = Is \u222a Is\u2032 = {1, . . . , l} \u222a {l + 1, . . . , 2l}, such that instruments {i, l+i} have two modes each: m0 and mi. There is a set of n+ 1 directions L = {dI , d1, . . . , dn} and a set of n image objectives O = {o1, . . . , on}, oi = (dI ,mi) for 1 \u2264 i \u2264 l and oi = (di,m0) for l < i \u2264 n. The calibration direction of instruments {i, l + i} is di. The sas+ encoding for this planning task is as follows.\n\u2022 Variables V = S \u222a O \u222a {Oni,Ci | i \u2208 I}.\n\u2022 Initial state I = {s :dI | s \u2208 S} \u222a {Oni :0,Ci :0 | i \u2208 I} \u222a {o :0 | o \u2208 O}.\n\u2022 Goal G = {o :1 | o \u2208 O}.\n\u2022 Actions A = \u22c3 s\u2208S ( {Turn(s, d, d\u2032) | {d, d\u2032} \u2286 L} \u222a {SwOn(i, s), Cal(i, s), SwOff(i) | i \u2208 Is} ) \u222a\n\u22c3 s\u2208S {TakeIm((dI ,mi), dI , s, i) | i \u2208 Is} \u222a n\u22c3 j=l+1 {TakeIm((dj ,m0), dj , s, i) | i \u2208 Is}  . The causal graph of this task is depicted in Figure 18a. The state variables {Oni,Ci |\ni \u2208 I} \u222a S have no goal value, and thus the collection of v-forks and v-iforks for this task is as in Figure 18b. The domains of the inverted-fork sinks are binary, and the domains of the fork roots are abstracted as in Eq. 16 (\u201cleave-one-out\u201d). This provides us with\n\u03a0F = {\u03a0fs,d,\u03a0fs\u2032,d | d \u2208 L} \u222a {\u03a0fCi | i \u2208 I}, \u03a0I = {\u03a0io | o \u2208 O},\n\u03a0FI = {\u03a0fs,d,\u03a0fs\u2032,d | d \u2208 L} \u222a {\u03a0fCi | i \u2208 I} \u222a {\u03a0io | o \u2208 O}.\nThe total number of forks in this task is nf = 2n+ 2l+ 2 and the total number of inverted forks is ni = n. The costs of the action representatives in each abstract task are given in Figure 17, where |Os| = |Os\u2032 | = |O| = n, |Oi| = n\u2212 l + 1, |Sm| = 2, |Im0 | = 2l, |Imi | = 2, and |L| = n+ 1.\nThe optimal plans per abstract task are depicted in Table 8, while an optimal plan for the original problem, \u3008SwOn(1, s), Turn(s, dI , d1), Cal(1, s), Turn(s, d1, dI), TakeIm(o1, dI , s, 1),\nthe Satellite task used in the proof of the upper bound of 1/6\nSatellite task used in the proof of the upper bound of 1/6\nSwOff(1), . . . SwOn(l \u2212 1, s), Turn(s, dI , dl\u22121), Cal(l \u2212 1, s), Turn(s, dl\u22121, dI), TakeIm(ol\u22121, dI , s, l \u2212 1), SwOff(l \u2212 1), SwOn(l, s), Turn(s, dI , dl), Cal(l, s), Turn(s, dl, dI), TakeIm(ol, dI , s, l), Turn(s, dI , dl+1), TakeIm(ol+1, dl+1, s, l), . . . , Turn(s, dn\u22121, dn), TakeIm(on, dn, s, l)\u3009, has the cost of 4l + 2n \u2212 1. For\nl = n \u2212 \u221an, this provides us with the asymptotic performance ratio of 1/6 for all three heuristics."}, {"heading": "10. Summary", "text": "We considered heuristic search for cost-optimal planning and introduced a domain-independent framework for devising admissible heuristics using additive implicit abstractions. Each such implicit abstraction corresponds to abstracting the planning task at hand by an instance of a tractable fragment of optimal planning. The key motivation for our investigation was to escape the restriction of explicit abstractions, such as pattern-database and merge-and-shrink abstractions, to abstract spaces of a fixed size. We presented a concrete scheme for additive implicit abstractions by decomposing the planning task along its causal graph and suggested a concrete realization of this idea, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. We then studied the induced admissible heuristics both formally and empirically, and showed that they favorably compete in informativeness with the state-of-the-art admissible heuristics both in theory and in practice. Our empirical evaluation stressed the tradeoff between the accuracy of the heuristics and runtime complexity of computing them. To alleviate the problem of expensive per-search-node runtime complexity of fork-decomposition heuristics, we showed that an equivalent of the explicit abstractions\u2019 notion of \u201cdatabase\u201d exists also for the fork-decomposition abstractions, and this despite their exponential-size abstract spaces. Our subsequent empirical evaluation of heuristic search with such databases for the fork-decomposition heuristics showed that it favorably competes with the state of the art of cost-optimal planning.\nThe basic principles of the implicit abstraction framework motivate further research in numerous directions, most importantly in (i) discovering new islands of tractability of optimal planning, and (ii) abstracting the general planning tasks into such islands. Likewise, there is promise in combining implicit abstractions with other techniques for deriving admissible heuristic estimates. A first step towards combining implicit abstractions with polynomial-time discoverable landmarks of the planning tasks has recently been taken by Domshlak, Katz, and Lefler (2010). We believe that various combinations of such techniques might well improve the informativeness of the heuristics, and this without substantially increasing their runtime complexity."}, {"heading": "Acknowledgments", "text": "The work of both authors was partly supported by Israel Science Foundation grants 670/07 and 1101/07."}, {"heading": "Appendix A. Detailed Results of Empirical Evaluation", "text": "Blocksworld, Depots, and Grid domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are computed fully online. Column task denotes problem instance, column h\u2217 denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\nLogistics-ipc2, and Mprime domains.\nPipesworld-Tankage, TPP, and Trucks domains.\nBlocksworld, Depots, and Driverlog domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h\u2217 denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\nLogistics-ipc2, and Mprime domains.\nNoTankage, Pipesworld-Tankage, Rovers, and Satellite domains.\nOpenstacks-strips-08, Parcprinter, and Scanalyzer domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h\u2217 denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\ning domains."}], "references": [{"title": "Complexity results for SAS+ planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence,", "citeRegEx": "B\u00e4ckstr\u00f6m and Nebel,? \\Q1995\\E", "shortCiteRegEx": "B\u00e4ckstr\u00f6m and Nebel", "year": 1995}, {"title": "Planning as heuristic search", "author": ["B. Bonet", "H. Geffner"], "venue": "Artificial Intelligence,", "citeRegEx": "Bonet and Geffner,? \\Q2001\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2001}, {"title": "The computational complexity of propositional STRIPS planning", "author": ["T. Bylander"], "venue": "Artificial Intelligence,", "citeRegEx": "Bylander,? \\Q1994\\E", "shortCiteRegEx": "Bylander", "year": 1994}, {"title": "Causal graphs and structurally restricted planning", "author": ["H. Chen", "O. Gimenez"], "venue": "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Chen and Gimenez,? \\Q2008\\E", "shortCiteRegEx": "Chen and Gimenez", "year": 2008}, {"title": "Additive-disjunctive heuristics for optimal planning", "author": ["A.I. Coles", "M. Fox", "D. Long", "A.J. Smith"], "venue": "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Coles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2008}, {"title": "Multi-agent off-line coordination: Structure and complexity", "author": ["C. Domshlak", "Y. Dinitz"], "venue": "In Proceedings of Sixth European Conference on Planning (ECP),", "citeRegEx": "Domshlak and Dinitz,? \\Q2001\\E", "shortCiteRegEx": "Domshlak and Dinitz", "year": 2001}, {"title": "Friends or foes? On planning as satisfiability and abstract CNF encodings", "author": ["C. Domshlak", "J. Hoffmann", "A. Sabharwal"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Domshlak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Domshlak et al\\.", "year": 2009}, {"title": "When abstractions met landmarks", "author": ["C. Domshlak", "M. Katz", "S. Lefler"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Domshlak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Domshlak et al\\.", "year": 2010}, {"title": "Directed model checking with distancepreserving abstractions", "author": ["K. Dr\u00e4ger", "B. Finkbeiner", "A. Podelski"], "venue": "Proceedings of the 13th International SPIN Workshop on Model Checking Software,", "citeRegEx": "Dr\u00e4ger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dr\u00e4ger et al\\.", "year": 2006}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "In Proceedings of the European Conference on Planning (ECP),", "citeRegEx": "Edelkamp,? \\Q2001\\E", "shortCiteRegEx": "Edelkamp", "year": 2001}, {"title": "Symbolic pattern databases in heuristic search planning", "author": ["S. Edelkamp"], "venue": "In Proceedings of the International Conference on AI Planning and Scheduling (AIPS),", "citeRegEx": "Edelkamp,? \\Q2002\\E", "shortCiteRegEx": "Edelkamp", "year": 2002}, {"title": "Automated creation of pattern database search heuristics", "author": ["S. Edelkamp"], "venue": "In Proceedings of the 4th Workshop on Model Checking and Artificial Intelligence (MoChArt)", "citeRegEx": "Edelkamp,? \\Q2006\\E", "shortCiteRegEx": "Edelkamp", "year": 2006}, {"title": "Optimal symbolic planning with action costs and preferences", "author": ["S. Edelkamp", "P. Kissmann"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Edelkamp and Kissmann,? \\Q2009\\E", "shortCiteRegEx": "Edelkamp and Kissmann", "year": 2009}, {"title": "Additive pattern database heuristics", "author": ["A. Felner", "R.E. Korf", "S. Hanan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Felner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Felner et al\\.", "year": 2004}, {"title": "Additive and reversed relaxed reachability heuristics revisited", "author": ["P. Haslum"], "venue": "In Proceedings of the 6th International Planning Competition", "citeRegEx": "Haslum,? \\Q2008\\E", "shortCiteRegEx": "Haslum", "year": 2008}, {"title": "New admissible heuristics for domainindependent planning", "author": ["P. Haslum", "B. Bonet", "H. Geffner"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Haslum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Haslum et al\\.", "year": 2005}, {"title": "Domain-independent construction of pattern database heuristics for cost-optimal planning", "author": ["P. Haslum", "A. Botea", "M. Helmert", "B. Bonet", "S. Koenig"], "venue": "In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Haslum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Haslum et al\\.", "year": 2007}, {"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "In Proceedings of the Fifth International Conference on Artificial Intelligence Planning Systems (ICAPS),", "citeRegEx": "Haslum and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Haslum and Geffner", "year": 2000}, {"title": "Complexity results for standard benchmark domains in planning", "author": ["M. Helmert"], "venue": "Artificial Intelligence,", "citeRegEx": "Helmert,? \\Q2003\\E", "shortCiteRegEx": "Helmert", "year": 2003}, {"title": "A planning heuristic based on causal graph analysis", "author": ["M. Helmert"], "venue": "In Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Helmert,? \\Q2004\\E", "shortCiteRegEx": "Helmert", "year": 2004}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Helmert,? \\Q2006\\E", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Landmarks, critical paths and abstractions: What\u2019s the difference anyway", "author": ["M. Helmert", "C. Domshlak"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Helmert and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Helmert and Domshlak", "year": 2009}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "In Proceedings of the 17th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Helmert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "Accuracy of admissible heuristic functions in selected planning domains", "author": ["M. Helmert", "R. Mattm\u00fcller"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,", "citeRegEx": "Helmert and Mattm\u00fcller,? \\Q2008\\E", "shortCiteRegEx": "Helmert and Mattm\u00fcller", "year": 2008}, {"title": "Understanding Planning Tasks: Domain Complexity and Heuristic Decomposition, Vol. 4929 of Lecture Notes in Computer", "author": ["M. Helmert"], "venue": null, "citeRegEx": "Helmert,? \\Q2008\\E", "shortCiteRegEx": "Helmert", "year": 2008}, {"title": "PSVN: A vector representation for production systems", "author": ["I. Hernadv\u00f6lgyi", "R. Holte"], "venue": "Tech. rep. 1999-07,", "citeRegEx": "Hernadv\u00f6lgyi and Holte,? \\Q1999\\E", "shortCiteRegEx": "Hernadv\u00f6lgyi and Holte", "year": 1999}, {"title": "The role of macros in tractable planning over causal graphs", "author": ["A. Jonsson"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Jonsson,? \\Q2007\\E", "shortCiteRegEx": "Jonsson", "year": 2007}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence,", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m,? \\Q1998\\E", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m", "year": 1998}, {"title": "Cost-optimal planning with landmarks", "author": ["E. Karpas", "C. Domshlak"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Karpas and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Karpas and Domshlak", "year": 2009}, {"title": "Structural patterns heuristics. In ICAPS-07 Workshop on Heuristics for Domain-independent Planning: Progress, Ideas, Limitations", "author": ["M. Katz", "C. Domshlak"], "venue": null, "citeRegEx": "Katz and Domshlak,? \\Q2007\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2007}, {"title": "Structural patterns of tractable sequentially-optimal planning", "author": ["M. Katz", "C. Domshlak"], "venue": "In Proceedings of the 17th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Katz and Domshlak,? \\Q2007\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2007}, {"title": "Structural patterns heuristics via fork decomposition", "author": ["M. Katz", "C. Domshlak"], "venue": "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Katz and Domshlak,? \\Q2008\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2008}, {"title": "Structural-pattern databases", "author": ["M. Katz", "C. Domshlak"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Katz and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2009}, {"title": "Optimal admissible composition of abstraction heuristics", "author": ["M. Katz", "C. Domshlak"], "venue": "Artificial Intelligence,", "citeRegEx": "Katz and Domshlak,? \\Q2010\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2010}, {"title": "Heuristics - Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Machine discovery of effective admissible heuristics", "author": ["A. Prieditis"], "venue": "Machine Learning,", "citeRegEx": "Prieditis,? \\Q1993\\E", "shortCiteRegEx": "Prieditis", "year": 1993}, {"title": "Landmarks revisited", "author": ["S. Richter", "M. Helmert", "M. Westphal"], "venue": "In Proceedings of the Twenty-Third National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Richter et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2008}, {"title": "A general additive search abstraction", "author": ["F. Yang", "J. Culberson", "R. Holte"], "venue": "Tech. rep. TR07-06,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "A general theory of additive state space abstractions", "author": ["F. Yang", "J. Culberson", "R. Holte", "U. Zahavi", "A. Felner"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "as pattern database heuristics (Edelkamp, 2001) and merge-and-shrink heuristics (Helmert, Haslum, & Hoffmann, 2007).", "startOffset": 31, "endOffset": 47}, {"referenceID": 9, "context": "Over the years, PDB heuristics have been shown to be very effective in several hard search problems, including cost-optimal planning (Culberson & Schaeffer, 1998; Edelkamp, 2001; Felner, Korf, & Hanan, 2004; Haslum, Botea, Helmert, Bonet, & Koenig, 2007).", "startOffset": 133, "endOffset": 254}, {"referenceID": 22, "context": "1 The more recent merge-andshrink abstractions generalize PDB heuristics to overcome the latter limitation (Helmert et al., 2007).", "startOffset": 107, "endOffset": 129}, {"referenceID": 18, "context": "Following the type of analysis suggested by Helmert and Mattm\u00fcller (2008), we formally analyze the asymptotic performance ratio of the fork-decomposition heuristics and prove that their worst-case accuracy on selected domains is comparable with that of (even parametric) state-of-the-art admissible heuristics.", "startOffset": 44, "endOffset": 74}, {"referenceID": 10, "context": "This does not necessarily apply to symbolic PDBs which, on some tasks, may exponentially reduce the PDB\u2019s representation (Edelkamp, 2002).", "startOffset": 121, "endOffset": 137}, {"referenceID": 29, "context": "This work is a revision and extension of the formulation and results presented by Katz and Domshlak (2008, 2009), which in turn is based on ideas first sketched also by Katz and Domshlak (2007a).", "startOffset": 82, "endOffset": 195}, {"referenceID": 18, "context": "Figure 1: Logistics-style example adapted from Helmert (2006) and illustrated in (a).", "startOffset": 47, "endOffset": 62}, {"referenceID": 18, "context": "To illustrate various constructs, we use a slight variation of a Logistics-style example from Helmert (2006). This example is depicted in Figure 1a, and in sas+ it has", "startOffset": 94, "endOffset": 109}, {"referenceID": 9, "context": "Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009).", "startOffset": 239, "endOffset": 276}, {"referenceID": 16, "context": "Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009).", "startOffset": 239, "endOffset": 276}, {"referenceID": 35, "context": "Our definition of abstraction resembles that of Prieditis (1993), and right from the beginning we specify a more general notion of additive abstraction.", "startOffset": 48, "endOffset": 65}, {"referenceID": 18, "context": "Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (\u03b1(s), l, \u03b1(s\u2032)) if (s, l, s\u2032), with a weaker condition stated in Eq.", "startOffset": 76, "endOffset": 98}, {"referenceID": 18, "context": "Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (\u03b1(s), l, \u03b1(s\u2032)) if (s, l, s\u2032), with a weaker condition stated in Eq. 1. The reader, of course, may well ask whether the generality of the condition in Eq. 1 beyond the condition of Helmert et al. (2007) really delivers any practical gain, and later we show that the answer to this question is affirmative.", "startOffset": 76, "endOffset": 393}, {"referenceID": 9, "context": "To further illustrate the connection between abstractions and admissible heuristics, consider three well-known mechanisms for devising admissible planning heuristics: delete relaxation (Bonet & Geffner, 2001), critical-path relaxation (Haslum & Geffner, 2000),2 and pattern database heuristics (Edelkamp, 2001).", "startOffset": 294, "endOffset": 310}, {"referenceID": 2, "context": "In any event, however, the abstraction \u3008T+, \u03b1+\u3009 does not induce a heuristic in terms of Definition 3 because computing h+(s) is known to be NP-hard (Bylander, 1994).", "startOffset": 148, "endOffset": 164}, {"referenceID": 13, "context": "Inspired by the (similarly named) domain-specific heuristics for search problems such as (n2 \u2212 1)-puzzles or Rubik\u2019s Cube (Culberson & Schaeffer, 1998; Hernadv\u00f6lgyi & Holte, 1999; Felner et al., 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al.", "startOffset": 122, "endOffset": 200}, {"referenceID": 16, "context": ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007).", "startOffset": 96, "endOffset": 139}, {"referenceID": 11, "context": "The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007).", "startOffset": 92, "endOffset": 129}, {"referenceID": 16, "context": "The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007).", "startOffset": 92, "endOffset": 129}, {"referenceID": 9, "context": ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space S\u03b1 and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in S\u03b1 by exhaustive search. Moreover, an O(1) bound on |S\u03b1| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |S\u03b1| implies a fixed-dimensionality constraint |V \u03b1| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {\u03a0i}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in \u03a0i can be transported by some \u0398(i) vehicles, then starting from some i, h\u03b1 will not account at all for movements of vehicles essential for solving \u03a0i (Helmert & Mattm\u00fcller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dr\u00e4ger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning.", "startOffset": 97, "endOffset": 1344}, {"referenceID": 9, "context": ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space S\u03b1 and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in S\u03b1 by exhaustive search. Moreover, an O(1) bound on |S\u03b1| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |S\u03b1| implies a fixed-dimensionality constraint |V \u03b1| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {\u03a0i}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in \u03a0i can be transported by some \u0398(i) vehicles, then starting from some i, h\u03b1 will not account at all for movements of vehicles essential for solving \u03a0i (Helmert & Mattm\u00fcller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dr\u00e4ger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning.", "startOffset": 97, "endOffset": 1420}, {"referenceID": 9, "context": ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space S\u03b1 and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in S\u03b1 by exhaustive search. Moreover, an O(1) bound on |S\u03b1| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |S\u03b1| implies a fixed-dimensionality constraint |V \u03b1| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {\u03a0i}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in \u03a0i can be transported by some \u0398(i) vehicles, then starting from some i, h\u03b1 will not account at all for movements of vehicles essential for solving \u03a0i (Helmert & Mattm\u00fcller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dr\u00e4ger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning. MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of states to be contracted. The problem\u2019s state space is viewed as the synchronized product of its projections onto the single state variables. Starting with all such \u201catomic\u201d abstractions, this product can be computed by iteratively composing two abstract spaces, replacing them with their product. While in a PDB the size of the abstract space S\u03b1 is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites. Helmert et al. (2007) have proposed a concrete strategy for this interleaved abstraction/refinement scheme and empirically demonstrated the power of the merge-and-shrink abstraction heuristics.", "startOffset": 97, "endOffset": 2190}, {"referenceID": 2, "context": "The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (B\u00e4ckstr\u00f6m & Nebel, 1995; Bylander, 1994; Jonsson & B\u00e4ckstr\u00f6m, 1998; Jonsson, 2007; Katz & Domshlak, 2007b).", "startOffset": 178, "endOffset": 285}, {"referenceID": 26, "context": "The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (B\u00e4ckstr\u00f6m & Nebel, 1995; Bylander, 1994; Jonsson & B\u00e4ckstr\u00f6m, 1998; Jonsson, 2007; Katz & Domshlak, 2007b).", "startOffset": 178, "endOffset": 285}, {"referenceID": 34, "context": "Informally, this decomposition can be seen as a sequential application of two kinds of task transformations: dropping preconditions (Pearl, 1984) and (certain form of) breaking actions with conjunctive effects into single-effect actions.", "startOffset": 132, "endOffset": 145}, {"referenceID": 3, "context": "In fact, recent results by Chen and Gimenez (2008) show that planning for any sas+ fragment characterized by any nontrivial form of causal graph is NP-hard.", "startOffset": 27, "endOffset": 51}, {"referenceID": 5, "context": "While the hardness of optimal planning for problems with fork and inverted fork causal graphs casts a shadow on the relevance of fork decompositions, a closer look at the proofs of the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003, 2004) reveals that they in particular rely on root variables having large domains.", "startOffset": 219, "endOffset": 246}, {"referenceID": 22, "context": "In fact, the shrinking step of the algorithm for building the merge-and-shrink abstractions is precisely a variable domain abstraction for meta-variables constructed in the merging steps (Helmert et al., 2007).", "startOffset": 187, "endOffset": 209}, {"referenceID": 20, "context": "Hence, we have implemented three additive fork-decomposition heuristics, h, h, and h, within the standard heuristic forward search framework of the Fast Downward planner (Helmert, 2006) using the A\u2217 algorithm with full duplicate elimination.", "startOffset": 170, "endOffset": 185}, {"referenceID": 18, "context": "\u201d We make a comparison with two baseline approaches, namely \u201cblind A\u2217\u201d with heuristic value 0 for goal states and 1 otherwise, and A\u2217 with the hmax heuristic (Bonet & Geffner, 2001), as well as with state-of-the-art abstraction heuristics, represented by the mergeand-shrink abstractions of Helmert et al. (2007). The latter were constructed under the", "startOffset": 291, "endOffset": 313}, {"referenceID": 14, "context": "We also compare to the Gamer (Edelkamp & Kissmann, 2009) and HSPF (Haslum, 2008) planners, the winner and the runner-up at the sequential optimization track of IPC-2008.", "startOffset": 66, "endOffset": 80}, {"referenceID": 14, "context": "These four (baseline and merge-and-shrink) heuristics were implemented by Helmert et al. (2007) within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison.", "startOffset": 74, "endOffset": 96}, {"referenceID": 15, "context": "These days h-partitions are being adopted by various optimal planners using criticalpath heuristics hm for m > 1 (Haslum et al., 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al.", "startOffset": 113, "endOffset": 134}, {"referenceID": 9, "context": ", 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007).", "startOffset": 119, "endOffset": 157}, {"referenceID": 22, "context": ", 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007).", "startOffset": 119, "endOffset": 157}, {"referenceID": 22, "context": "This per-node efficiency provides the merge-and-shrink heuristics with impressive practical effectiveness on numerous IPC domains (Helmert et al., 2007).", "startOffset": 130, "endOffset": 152}, {"referenceID": 18, "context": "However, as rightfully noted by Helmert and Mattm\u00fcller (2008), such evaluations almost never lead to absolute statements of the type \u201cHeuristic h is well-suited for solving problems from benchmark suite X,\u201d but only to relative statements of the type \u201cHeuristic h expands fewer nodes than heuristic h\u2032 on benchmark suite X.", "startOffset": 32, "endOffset": 62}, {"referenceID": 18, "context": "However, as rightfully noted by Helmert and Mattm\u00fcller (2008), such evaluations almost never lead to absolute statements of the type \u201cHeuristic h is well-suited for solving problems from benchmark suite X,\u201d but only to relative statements of the type \u201cHeuristic h expands fewer nodes than heuristic h\u2032 on benchmark suite X.\u201d Moreover, one would probably like to obtain formal evidence of the effectiveness of a heuristic before proceeding with its implementation, especially for very complicated heuristic procedures such as those underlying the proofs of Theorems 7 and 8. Our formal analysis of the effectiveness of the fork-decomposition heuristics using the methodology suggested and exploited by Helmert and Mattm\u00fcller was motivated primarily by this desire for formal evidence. Given a planning domain D and heuristic h, Helmert and Mattm\u00fcller (2008) consider the asymptotic performance ratio of h in D.", "startOffset": 32, "endOffset": 857}, {"referenceID": 18, "context": "Table 4: Performance ratios of multiple heuristics in selected planning domains; ratios for h+, hk, hPDB, hPDB add are by Helmert and Mattm\u00fcller (2008).", "startOffset": 122, "endOffset": 152}, {"referenceID": 18, "context": "Helmert and Mattm\u00fcller (2008) study the asymptotic performance ratio of some standard admissible heuristics on a set of well-known benchmark domains from the first four IPCs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 2, "context": "\u2022 The h+ estimate corresponds to the optimal cost of solving the well-known \u201cdelete relaxation\u201d of the original planning task, which is generally NP-hard to compute (Bylander, 1994).", "startOffset": 165, "endOffset": 181}, {"referenceID": 18, "context": "Blocksworld Arguments similar to those of Helmert and Mattm\u00fcller (2008) for hPDB add .", "startOffset": 42, "endOffset": 72}, {"referenceID": 18, "context": "3 of Helmert (2008) we have", "startOffset": 5, "endOffset": 20}], "year": 2010, "abstractText": "State-space search with explicit abstraction heuristics is at the state of the art of costoptimal planning. These heuristics are inherently limited, nonetheless, because the size of the abstract space must be bounded by some, even if a very large, constant. Targeting this shortcoming, we introduce the notion of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning. We then introduce a concrete setting of this framework, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. The induced admissible heuristics are then studied formally and empirically. This study testifies for the accuracy of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them. Indeed, some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining h(s) for each evaluated state s by a very fast lookup in a \u201cdatabase.\u201d By contrast, while fork-decomposition heuristics can be calculated in polynomial time, computing them is far from being fast. To address this problem, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. We demonstrate that an equivalent of the explicit abstraction notion of a \u201cdatabase\u201d exists for the fork-decomposition abstractions as well, despite their exponential-size abstract spaces. We then verify empirically that heuristic search with the \u201cdatabased\u201d fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.", "creator": "TeX"}}}