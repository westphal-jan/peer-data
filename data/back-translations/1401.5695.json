{"id": "1401.5695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "abstract": "We show the effectiveness of multilingual learning for unattended parts of the language tagging. The central assumption of our work is that by combining keywords from multiple languages, the structure of each language becomes more obvious. We look at two ways to apply this intuition to the problem of unattended part-of-speech tagging: a model that merges the keyword structures for a language pair directly into a single sequence, and a second model that incorporates the multilingual context instead using latent variables. Both approaches are formulated as hierarchical Bayesian models, using sampling techniques from Markov Chain Monte Carlo. Our results show that by incorporating multilingual evidence, we can achieve impressive performance improvements in a number of scenarios.", "histories": [["v1", "Wed, 15 Jan 2014 05:39:01 GMT  (1372kb)", "http://arxiv.org/abs/1401.5695v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tahira naseem", "benjamin snyder", "jacob eisenstein", "regina barzilay"], "accepted": false, "id": "1401.5695"}, "pdf": {"name": "1401.5695.pdf", "metadata": {"source": "CRF", "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "authors": ["Tahira Naseem", "Benjamin Snyder", "Jacob Eisenstein", "Regina Barzilay"], "emails": ["TAHIRA@CSAIL.MIT.EDU", "BSNYDER@CSAIL.MIT.EDU", "JACOBE@CSAIL.MIT.EDU", "REGINA@CSAIL.MIT.EDU"], "sections": [{"heading": null, "text": "ging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases."}, {"heading": "1. Introduction", "text": "In this paper, we explore the application of multilingual learning to part-of-speech tagging when no annotation is available.1 The fundamental idea upon which our work is based is that the patterns of ambiguity inherent in part-of-speech tag assignments differ across languages. At the lexical level, a word with part-of-speech tag ambiguity in one language may correspond to an unambiguous word in the other language. For example, the word \u201ccan\u201d in English may function as an auxiliary verb, a noun, or a regular verb. However, many other languages are likely to express these different senses with three distinct lexemes. Languages also differ in their patterns of structural ambiguity. For example, the presence of an article in English greatly reduces the ambiguity of the succeeding tag. In languages without articles, however, this constraint is obviously absent. The key idea of multilingual learning is that by combining natural cues from multiple languages, the structure of each becomes more apparent.\nEven in expressing the same meaning, languages take different syntactic routes, leading to cross-lingual variation in part-of-speech patterns. Therefore, an effective multilingual model must accurately represent common linguistic structure, yet remain flexible to the idiosyncrasies of each language. This tension only becomes stronger as additional languages are added to the mix. Thus, a key challenge of multilingual learning is to capture cross-lingual correlations while preserving individual language tagsets, tag selections, and tag orderings.\n1. Code, data sets, and the raw outputs of our experiments are available at http://groups.csail.mit.edu/rbg/code/multiling pos.\nc\u00a92009 AI Access Foundation. All rights reserved.\nIn this paper, we explore two different approaches for modeling cross-lingual correlations. The first approach directly merges pairs of tag sequences into a single bilingual sequence, employing joint distributions over aligned tag-pairs; for unaligned tags, language-specific distributions are still used. The second approach models multilingual context using latent variables instead of explicit node merging. For a group of aligned words, the multilingual context is encapsulated in the value of a corresponding latent variable. Conditioned on the latent variable, the tagging decisions for each language remain independent. In contrast to the first model, the architecture of the hidden variable model allows it to scale gracefully as the number of languages increases.\nBoth approaches are formulated as hierarchical Bayesian models with an underlying trigram HMM substructure for each language. The first model operates as a simple directed graphical model with only one additional coupling parameter beyond the transition and emission parameters used in monolingual HMMs. The latent variable model, on the other hand, is formulated as a non-parametric model; it can be viewed as performing multilingual clustering on aligned sets of tag variables. Each latent variable value indexes a separate distribution on tags for each language, appropriate to the given context. For both models, we perform inference using Markov Chain Monte Carlo sampling techniques.\nWe evaluate our models on a parallel corpus of eight languages: Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Serbian, and Slovene. We consider a range of scenarios that vary from combinations of bilingual models to a single model that is jointly trained on all eight languages. Our results show consistent and robust improvements over a monolingual baseline for almost all combinations of languages. When a complete tag lexicon is available and the latent variable model is trained using eight languages, average performance increases from 91.1% accuracy to 95%, more than halving the gap between unsupervised and supervised performance. In more realistic cases, where the lexicon is restricted to only frequently occurring words, we see even larger gaps between monolingual and multilingual performance. In one such scenario, average multilingual performance increases to 82.8% from a monolingual baseline of 74.8%. For some language pairs, the improvement is especially noteworthy; for instance, in complete lexicon scenario, Serbian improves from 84.5% to 94.5% when paired with English.\nWe find that in most scenarios the latent variable model achieves higher performance than the merged structure model, even when it too is restricted to pairs of languages. Moreover the hidden variable model can effectively accommodate large numbers of languages which makes it a more desirable framework for multilingual learning. However, we observe that the latent variable model is somewhat sensitive to lexicon coverage. The performance of the merged structure model, on the other hand, is more robust in this respect. In the case of the drastically reduced lexicon (with 100 words only), its performance is clearly better than the hidden variable model. This indicates that the merged structure model might be a better choice for the languages that lack lexicon resources.\nA surprising discovery of our experiments is the marked variation in the level of improvement across language pairs. If the best pairing for each language is chosen by an oracle, average bilingual performance reaches 95.4%, compared to average performance of 93.1% across all pairs. Our experiments demonstrate that this variability is influenced by cross-lingual links between languages as well as by the model under consideration. We identify several factors that contribute to the success of language pairings, but none of them can uniquely predict which supplementary language is most helpful. These results suggest that when multi-parallel corpora are available, a model that simultaneously exploits all the languages \u2013 such as the latent variable model proposed here \u2013 is\npreferable to a strategy that selects one of the bilingual models. We found that performance tends to improves steadily as the number of available languages increases.\nIn realistic scenarios, tagging resources for some number of languages may already be available. Our models can easily exploit any amount of tagged data in any subset of available languages. As our experiments show, as annotation is added, performance increases even for those languages lacking resources.\nThe remainder of the paper is structured as follows. Section 2 compares our approach with previous work on multilingual learning and unsupervised part-of-speech tagging. Section 3 presents two approaches for modeling multilingual tag sequences, along with their inference procedures and implementation details. Section 4 describes corpora used in the experiments, preprocessing steps and various evaluation scenarios. The results of the experiments and their analysis are given in Sections 5, and 6. We summarize our contributions and consider directions for future work in Section 7."}, {"heading": "2. Related Work", "text": "We identify two broad areas of related work: multilingual learning and inducing part-of-speech tags without labeled data. Our discussion of multilingual learning focuses on unsupervised approaches that incorporate two or more languages. We then describe related work on unsupervised and semisupervised models for part-of-speech tagging."}, {"heading": "2.1 Multilingual Learning", "text": "The potential of multilingual data as a rich source of linguistic knowledge has been recognized since the early days of empirical natural language processing. Because patterns of ambiguity vary greatly across languages, unannotated multilingual data can serve as a learning signal in an unsupervised setting. We are especially interested in methods to leverage more than two languages jointly, and compare our approach with relevant prior work.\nMultilingual learning may also be applied in a semi-supervised setting, typically by projecting annotations across a parallel corpus to another language where such resources do not exist (e.g., Yarowsky, Ngai, & Wicentowski, 2000; Diab & Resnik, 2002; Pado\u0301 & Lapata, 2006; Xi & Hwa, 2005). As our primary focus is on the unsupervised induction of cross-linguistic structures, we do not address this area."}, {"heading": "2.1.1 BILINGUAL LEARNING", "text": "Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991). Lexical ambiguity differs across languages \u2013 each sense of a polysemous word in one language may translate to a distinct counterpart in another language. This makes it possible to use aligned foreign-language words as a source of noisy supervision. Bilingual data has been leveraged in this way in a variety of WSD models (Brown et al., 1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002; Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual data closely approximates that of manual annotation (Ng et al., 2003). Polysemy is one source of ambiguity for part-of-speech tagging; thus our model implicitly leverages multilingual WSD in the context of a higher-level syntactic analysis.\nMultilingual learning has previously been applied to syntactic analysis; a pioneering effort was the inversion transduction grammar of Wu (1995). This method is trained on an unannotated parallel corpus using a probabilistic bilingual lexicon and deterministic constraints on bilingual tree structures. The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified bilingual grammar. These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).\nOne characteristic of this family of methods is that they were designed for inherently multilingual tasks such as machine translation and lexicon induction. While we share the goal of learning from multilingual data, we seek to induce monolingual syntactic structures that can be applied even when multilingual data is unavailable.\nIn this respect, our approach is closer to the unsupervised multilingual grammar induction work of Kuhn (2004). Starting from the hypothesis that trees induced over parallel sentences should exhibit cross-lingual structural similarities, Kuhn uses word-level alignments to constrain the set of plausible syntactic constituents. These constraints are implemented through hand-crafted deterministic rules, and are incorporated in expectation-maximization grammar induction to assign zero likelihood to illegal bracketings. The probabilities of the productions are then estimated separately for each language, and can be applied to monolingual data directly. Kuhn shows that this form of multilingual training yields better monolingual parsing performance.\nOur methods incorporate cross-lingual information in a fundamentally different manner. Rather than using hand-crafted deterministic rules \u2013 which may require modification for each language pair \u2013 we estimate probabilistic multilingual patterns directly from data. Moreover, the estimation of multilingual patterns is incorporated directly into the tagging model itself.\nFinally, multilingual learning has recently been applied to unsupervised morphological segmentation (Snyder & Barzilay, 2008). This research is related, but moving from morphological to syntactic analysis imposes new challenges. One key difference is that Snyder & Barzilay model morphemes as unigrams, ignoring the transitions between morphemes. In syntactic analysis, transition information provides a crucial constraint, requiring a fundamentally different model structure."}, {"heading": "2.1.2 BEYOND BILINGUAL LEARNING", "text": "While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. This work first induces bilingual models for each pair of languages and then combines them. We take a different approach by simultaneously learning from all languages, rather than combining bilingual results.\nA related thread of research is multi-source machine translation (Och & Ney, 2001; Utiyama & Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico, & Cattoni, 2008) where the goal is to translate from multiple source languages to a single target language. By using multi-source corpora, these systems alleviate sparseness and increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation systems build separate bilingual models and then select a final translation from their output. For instance, a method developed by Och and Ney (2001) generates several alternative translations from source sentences expressed in different languages and selects the most likely candidate. Cohn and Lapata (2007) consider a different generative model: rather than combining alternative sentence translations in a post-processing step, their model estimates the target phrase translation distribu-\ntion by marginalizing over multiple translations from various source languages. While their model combines multilingual information at the phrase level, at its core are estimates for phrase tables that are obtained using bilingual models.\nIn contrast, we present an approach for unsupervised multilingual learning that builds a single joint model across all languages. This makes maximal use of unlabeled data and sidesteps the difficult problem of combining the output of multiple bilingual systems without supervision."}, {"heading": "2.2 Unsupervised Part-of-Speech Tagging", "text": "Unsupervised part-of-speech tagging involves predicting the tags for words, without annotations of the correct tags for any word tokens. Generally speaking, the unsupervised setting does permit the use of declarative knowledge about the relationship between tags and word types, in the form of a dictionary of the permissible tags for the most common words. This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage.\nSince the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities. Such Bayesian priors permit integration over parameter settings, yielding models that perform well across a range of settings. This is particularly important in the case of small datasets, where many of the counts used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work.\nSeveral recent papers have explored the development of alternative training procedures and model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number of overlapping features in a conditional log-linear formulation. Contrastive estimation is used to provide a training criterion which maximizes the probability of the observed sentences compared to a set of similar sentences created by perturbing word order. The use of a large set of features and a discriminative training procedure led to strong performance gains.\nToutanova and Johnson (2008) propose an LDA-style model for unsupervised part-of-speech tagging, grouping words through a latent layer of ambiguity classes. Each ambiguity class corresponds to a set of permissible tags; in many languages this set is tightly constrained by morphological features, thus allowing an incomplete tagging lexicon to be expanded. Haghighi and Klein (2006) also use a variety of morphological features, learning in an undirected Markov Random Field that permits overlapping features. They propagate information from a small number of labeled \u201cprototype\u201d examples using the distributional similarity between prototype and non-prototype words.\nOur focus is to effectively incorporate multilingual evidence, and we require a simple model that can easily be applied to multiple languages with widely varying structural properties. We view this direction as orthogonal to refining monolingual tagging models for any particular language.\n2. In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995; Mihalcea, 2004)"}, {"heading": "3. Models", "text": "The motivating hypothesis of this work is that patterns of ambiguity at the part-of-speech level differ across languages in systematic ways. By considering multiple languages simultaneously, the total inherent ambiguity can be reduced in each language. But with the potential advantages of leveraging multilingual information comes the challenge of respecting language-specific characteristics such as tag inventory, selection and order. To this end, we develop models that jointly tag parallel streams of text in multiple languages, while maintaining language-specific tag sets and parameters over transitions and emissions.\nPart-of-speech tags reflect the syntactic and semantic function of the tagged words. Across languages, pairs of word tokens that are known to share semantic or syntactic function should have tags that are related in systematic ways. The word alignment task in machine translation is to identify just such pairs of words in parallel sentences. Aligned word pairs serve as the cross-lingual anchors of our model, allowing information to be shared via joint tagging decisions. Research in machine translation has produced robust tools for identifying word alignments; we use such a tool as a black box and treat its output as a fixed, observed property of the parallel data.\nGiven a set of parallel sentences, we posit a hidden Markov model (HMM) for each language, where the hidden states represent the tags and the emissions are the words. In the unsupervised monolingual setting, inference on the part-of-speech tags is performed jointly with estimation of parameters governing the relationship between tags and words (the emission probabilities) and between consecutive tags (the transition probabilities). Our multilingual models are built upon this same structural foundation, so that the emission and transition parameters retain an identical interpretation as in the monolingual setting. Thus, these parameters can be learned on parallel text and later applied to monolingual data.\nWe consider two alternative approaches for incorporating cross-lingual information. In the first model, the tags for aligned words are merged into single bi-tag nodes; in the second, latent variable model, an additional layer of hidden superlingual tags instead exerts influence on the tags of clusters of aligned words. The first model is primarily designed for bilingual data, while the second model operates over any number of languages. Figure 1 provides a graphical model representation of the monolingual, merged node, and latent variable models instantiated over a single parallel sentence.\nBoth the merged node and latent variable approaches are formalized as hierarchical Bayesian models. This provides a principled probabilistic framework for integrating multiple sources of information, and offers well-studied inference techniques. Table 1 summarizes the mathematical notation used throughout this section. We now describe each model in depth."}, {"heading": "3.1 Bilingual Unsupervised Tagging: A Merged Node Model", "text": "In the bilingual merged node model, cross-lingual context is incorporated by creating joint bi-tag nodes for aligned words. It would be too strong to insist that aligned words have an identical tag; indeed, it may not even be appropriate to assume that two languages share identical tag sets. However, when two words are aligned, we do want to choose their tags jointly. To enable this, we allow the values of the bi-tag nodes to range over all possible tag pairs \u3008t, t\u2032\u3009 \u2208 T \u00d7 T \u2032, where T and T \u2032 represent the tagsets for each language.\nThe tags t and t\u2032 need not be identical, but we do believe that they are systematically related. This is modeled using a coupling distribution \u03c9, which is multinomial over all tag pairs. The parameter \u03c9 is combined with the standard transition distribution \u03c6 in a product-of-experts model. Thus, the aligned tag pair \u3008yi, y\u2032j\u3009 is conditioned on the predecessors yi\u22121 and y \u2032 j\u22121, as well as the coupling parameter \u03c9(yi, y\u2032j). 3 The coupled bi-tag nodes serve as bilingual \u201canchors\u201d \u2013 due to the Markov dependency structure, even unaligned words may benefit from cross-lingual information that propagates from these nodes.\n3. While describing the merged node model, we consider only the two languages \u2113 and \u2113\u2032, and use a simplified notation in which we write \u3008y, y\u2032\u3009 to mean \u3008y\u2113, y\u2113 \u2032 \u3009. Similar abbreviations are used for the language-indexed parameters.\nWe now present a generative account of how the words in each sentence and the parameters of the model are produced. This generative story forms the basis of our sampling-based inference procedure."}, {"heading": "3.1.1 MERGED NODE MODEL: GENERATIVE STORY", "text": "Our generative story assumes the existence of two tagsets, T and T \u2032, and two vocabularies W and W \u2032 \u2013 one of each for each language. For ease of exposition, we formulate our model with bigram tag dependencies. However, in our experiments we used a trigram model (without any bigram interpolation), which is a trivial extension of the described model.\n1. Transition and Emission Parameters. For each tag t \u2208 T , draw a transition distribution \u03c6t over tags T , and an emission distribution \u03b8t over words W . Both the transition and emission distributions are multinomial, so they are drawn from their conjugate prior, the Dirichlet (Gelman, Carlin, Stern, & Rubin, 2004). We use symmetric Dirichlet priors, which encode only an expectation about the uniformity of the induced multinomials, but not do encode preferences for specific words or tags.\nFor each tag t \u2208 T \u2032, draw a transition distribution \u03c6\u2032t over tags T \u2032, and an emission distribution \u03b8\u2032t over words W \u2032, both from symmetric Dirichlet priors.\n2. Coupling Parameter. Draw a bilingual coupling distribution \u03c9 over tag pairs pairs T \u00d7 T \u2032. This distribution is multinomial with dimension |T | \u00b7 |T \u2032|, and is drawn from a symmetric Dirichlet prior \u03c90 over all tag pairs.\n3. Data. For each bilingual parallel sentence:\n(a) Draw an alignment a from a bilingual alignment distribution Ab. The following paragraph defines a and Ab more formally.\n(b) Draw a bilingual sequence of part-of-speech tags (y1, ..., ym), (y\u20321, ..., y \u2032 n) according to:\nP ((y1, ..., ym), (y \u2032 1, ..., y \u2032 n)|a, \u03c6, \u03c6 \u2032, \u03c9).4 This joint distribution thus conditions on the alignment structure, the transition probabilities for both languages, and the coupling distribution; a formal definition is given in Formula 1.\n(c) For each part-of-speech tag yi in the first language, emit a word from the vocabulary W : xi \u223c \u03b8yi , (d) For each part-of-speech tag y\u2032j in the second language, emit a word from the vocabulary W \u2032: x\u2032j \u223c \u03b8\n\u2032 y\u2032j .\nThis completes the outline of the generative story. We now provide more detail on how alignments are handled, and on the distribution over coupled part-of-speech tag sequences.\nAlignments An alignment a defines a bipartite graph between the words x and x\u2032 in two parallel sentences . In particular, we represent a as a set of integer pairs, indicating the word indices. Crossing edges are not permitted, as these would lead to cycles in the resulting graphical model; thus, the existence of an edge (i, j) precludes any additional edges (i + a, j \u2212 b) or (i \u2212 a, j + b),\n4. We use a special end state, rather than explicitly modeling sentence length. Thus the values of m and n are determined stochastically.\nfor a, b \u2265 0. From a linguistic perspective, we assume that the edge (i, j) indicates that the words xi and x\u2032j share some syntactic and/or semantic role in the bilingual parallel sentences.\nFrom the perspective of the generative story, alignments are treated as draws from a distribution Ab. Since the alignments are always observed, we can remain agnostic about the distribution Ab, except to require that it assign zero probability to alignments which either: (i) align a single index in one language to multiple indices in the other language or (ii) contain crossing edges. The resulting alignments are thus one-to-one, contain no crossing edges, and may be sparse or even possibly empty. Our technique for obtaining alignments that display these properties is described in Section 4.2.\nGenerating Tag Sequences In a standard hidden Markov model for part-of-speech tagging, the tags are drawn as a Markov process from the transition distribution. This permits the probability of a tag sequence to factor across the time steps. Our model employs a similar factorization: the tags for unaligned words are drawn from their predecessor\u2019s transition distribution, while joined tag nodes are drawn from a product involving the coupling parameter and the transition distributions for both languages.\nMore formally, given an alignment a and sets of transition parameters \u03c6 and \u03c6\u2032, we factor the conditional probability of a bilingual tag sequence (y1, ..., ym), (y\u20321, ..., y \u2032 n) into transition probabilities for unaligned tags, and joint probabilities over aligned tag pairs:\nP ((y1, ..., ym), (y \u2032 1, ..., y \u2032 n)|a, \u03c6, \u03c6\n\u2032, \u03c9) = \u220f\nunaligned i\n\u03c6yi\u22121(yi) \u220f\nunaligned j\n\u03c6\u2032y\u2032j\u22121 (y\u2032j)\n\u220f\n(i,j)\u2208a\nP (yi, y \u2032 j |yi\u22121, y \u2032 j\u22121, \u03c6, \u03c6 \u2032, \u03c9). (1)\nBecause the alignment contains no crossing edges, we can still model the tags as generated sequentially by a stochastic process. We define the distribution over aligned tag pairs to be a product of each language\u2019s transition probability and the coupling probability:\nP (yi, y \u2032 j |yi\u22121, y \u2032 j\u22121, \u03c6, \u03c6\n\u2032, \u03c9) = \u03c6yi\u22121(yi) \u03c6 \u2032 y\u2032j\u22121 (y\u2032j)\u03c9(yi, y \u2032 j)\nZ . (2)\nThe normalization constant here is defined as:\nZ = \u2211\ny,y\u2032\n\u03c6yi\u22121(y) \u03c6 \u2032 y\u2032j\u22121 (y\u2032) \u03c9(y, y\u2032).\nThis factorization allows the language-specific transition probabilities to be shared across aligned and unaligned tags.\nAnother way to view this probability distribution is as a product of three experts: the two transition parameters and the coupling parameter. Product-of-expert models (Hinton, 1999) allow each information source to exercise very strong negative influence on the probability of tags that they consider to be inappropriate, as compared with additive models. This is ideal for our setting, as it prevents the coupling distribution from causing the model to generate a tag that is unacceptable from the perspective of the monolingual transition distribution. In preliminary experiments we found that a multiplicative approach was strongly preferable to additive models."}, {"heading": "3.1.2 MERGED NODE MODEL: INFERENCE", "text": "The goal of our inference procedure is to obtain transition and emission parameters \u03b8 and \u03c6 that can be applied to monolingual test data. Ideally we would choose the parameters that have the highest marginal probability, conditioned on the observed words x and alignments a,\n\u03b8\u0302, \u03c6\u0302 = argmax \u03b8,\u03c6\n\u222b\nP (\u03b8, \u03c6,y, \u03c9|x,a, \u03b80, \u03c60, \u03c90)dyd\u03c9.\nWhile the structure of our model permits us to decompose the joint probability, it is not possible to analytically marginalize all of the hidden variables. We resort to standard Monte Carlo approximation, in which marginalization is performed through sampling. By repeatedly sampling individual hidden variables according to the appropriate distributions, we obtain a Markov chain that is guaranteed to converge to a stationary distribution centered on the desired posterior. Thus, after an initial burn-in phase, we can use the samples to approximate a marginal distribution over any desired parameter (Gilks, Richardson, & Spiegelhalter, 1996).\nThe core element of our inference procedure is Gibbs sampling (Geman & Geman, 1984). Gibbs sampling begins by randomly initializing all unobserved random variables; at each iteration, each random variable ui is then sampled from the conditional distribution P (ui|u\u2212i), where u\u2212i refers to all variables other than ui. Eventually, the distribution over samples drawn from this process will converge to the unconditional joint distribution P (u) of the unobserved variables. When possible, we avoid explicitly sampling variables which are not of direct interest, but rather integrate over them. This technique is known as collapsed sampling; it is guaranteed never to increase sampling variance, and will often reduce it (Liu, 1994).\nIn the merged node model, we need sample only the part-of-speech tags and the priors. We are able to exactly marginalize the emission parameters \u03b8 and approximately marginalize the transition and coupling parameters \u03c6 and \u03c9 (the approximations are required due to the re-normalized product of experts \u2014 see below for more details). We draw repeated samples of the part-of-speech tags, and construct a sample-based estimate of the underlying tag sequence. After sampling, we construct maximum a posteriori estimates of the parameters of interest for each language, \u03b8 and \u03c6.\nSampling Unaligned Tags For unaligned part-of-speech tags, the conditional sampling equations are identical to the monolingual Bayesian hidden Markov model. The probability of each tag decomposes into three factors:\nP (yi|y\u2212i,y \u2032,x,x\u2032, \u03b80, \u03c60) \u221d P (xi|yi,y\u2212i,x\u2212i, \u03b80)P (yi|yi\u22121,y\u2212i, \u03c60)P (yi+1|yi,y\u2212i, \u03c60), (3)\nwhich follows from the chain rule and the conditional independencies in the model. The first factor is for the emission xi and the remaining two are for the transitions. We now derive the form of each factor, marginalizing the parameters \u03b8 and \u03c6.\nFor the emission factor, we can exactly marginalize the emission distribution \u03b8, whose prior is Dirichlet with hyperparameter \u03b80. The resulting distribution is a ratio of counts, where the prior acts as a pseudo-count:\nP (xi|y,x\u2212i, \u03b80) =\n\u222b\n\u03b8yi\n\u03b8yi(xi)P (\u03b8yi |y,x\u2212i, \u03b80)d\u03b8yi = n(yi, xi) + \u03b80 n(yi) + |Wyi |\u03b80 . (4)\nHere, n(yi) is the number of occurrences of the tag yi in y\u2212i, n(yi, xi) is the number of occurrences of the tag-word pair (yi, xi) in (y\u2212i,x\u2212i), and Wyi is the set of word types in the vocabulary\nW that can take tag yi. The integral is tractable due to Dirichlet-multinomial conjugacy, and an identical marginalization was applied in the monolingual Bayesian HMM of Goldwater and Griffiths (2007).\nFor unaligned tags, it is also possible to exactly marginalize the parameter \u03c6 governing transitions. For the transition from i\u2212 1 to i,\nP (yi|yi\u22121,y\u2212i, \u03c60) =\n\u222b\n\u03c6yi\u22121\n\u03c6yi\u22121(yi)P (\u03c6yi |y\u2212i, \u03c60)d\u03c6yi\u22121 = n(yi\u22121, yi) + \u03c60 n(yi\u22121) + |T |\u03c60 . (5)\nThe factors here are similar to the emission probability: n(yi) is the number of occurrences of the tag yi in y\u2212i, n(yi\u22121, yi) is the number of occurrences of the tag sequence (yi\u22121, yi), and T is the tagset. The probability for the transition from i to i+ 1 is analogous.\nJointly Sampling Aligned Tags The situation for tags of aligned words is more complex. We sample these tags jointly, considering all |T \u00d7 T \u2032| possibilities. We begin by decomposing the probability into three factors:\nP (yi, y \u2032 j |y\u2212i, y \u2032 \u2212j , x, x \u2032,a, \u03b80, \u03b8 \u2032 0, \u03c6, \u03c6 \u2032, \u03c9) \u221d P (xi|y, x\u2212i, \u03b80)P (x\u2032j |y \u2032, x\u2032\u2212j , \u03b8 \u2032 0)P (yi, y \u2032 j |y\u2212i, y \u2032 \u2212j ,a, \u03c6, \u03c6 \u2032, \u03c9).\nThe first two factors are emissions, and are handled identically to the unaligned case (Formula 4). The expansion of the final, joint factor depends on the alignment of the succeeding tags. If neither of the successors (in either language) are aligned, we have a product of the bilingual coupling probability and four transition probabilities:\nP (yi, y \u2032 j |y\u2212i,y \u2032 \u2212j , \u03c6, \u03c6 \u2032, \u03c9) \u221d \u03c9(yi, y \u2032 j)\u03c6yi\u22121(yi)\u03c6yi(yi+1)\u03c6 \u2032 y\u2032j\u22121 (y\u2032j)\u03c6 \u2032 y\u2032j (y\u2032j+1).\nWhenever one or more of the succeeding words is aligned, the sampling formulas must account for the effect of the sampled tag on the joint probability of the succeeding tags, which is no longer a simple multinomial transition probability. We give the formula for one such case\u2014when we are sampling a joint tag pair (yi, y\u2032j), whose succeeding words (xi+1, x \u2032 j+1) are also aligned to one another:\nP (yi, yj |y\u2212i, y \u2032 \u2212j ,a, \u03c6, \u03c6 \u2032, \u03c9) \u221d \u03c9(yi, y \u2032 j)\u03c6yi\u22121(yi)\u03c6 \u2032 y\u2032j\u22121 (y\u2032j)\n[\n\u03c6yi(yi+1)\u03c6 \u2032 y\u2032j (y\u2032j+1)\n\u2211\nt,t\u2032 \u03c6yi(t)\u03c6 \u2032 y\u2032j (t\u2032)\u03c9(t, t\u2032)\n]\n. (6)\nIntuitively, if \u03c9 puts all of its probability mass on a single assignment yi+1 = t, y\u2032j+1 = t \u2032, then the transitions from i to i + 1 and j to j + 1 are irrelevant, and the final factor goes to one. Conversely, if \u03c9 is indifferent and assigns equal probability to all pairs \u3008t, t\u2032\u3009, then the final factor becomes proportional to \u03c6yi(yi+1)\u03c6\n\u2032 y\u2032j (y\u2032j+1), which is the same as if xi+1 and xj+1 were not\naligned. In general, as the entropy of \u03c9 increases, the transition to the succeeding nodes exerts a greater influence on yi and y\u2032j . Similar equations can be derived for cases where the succeeding tags are not aligned to each other, but one of them is aligned to another tag, e.g., xi+1 is aligned to x\u2032j+2.\nAs before, we would like to marginalize the parameters \u03c6, \u03c6\u2032, and \u03c9. Because these parameters interact as a product-of-experts model, these marginalizations are approximations. The form of the marginalizations for \u03c6 and \u03c6\u2032 are identical to Formula 5. For the coupling distribution,\nP\u03c9(yi, y \u2032 j |y\u2212i,y \u2032 \u2212j , \u03c90) \u2248\nn(yi, y \u2032 j) + \u03c90\nN(a) + |T \u00d7 T \u2032|\u03c90 , (7)\nwhere n(yi, y\u2032j) is the number of times tags yi and y \u2032 j were aligned, excluding i and j, and N(a) is the total number of alignments. As above, the prior \u03c90 appears as a smoothing factor; in the denominator it is multiplied by the dimensionality of \u03c9, which is the size of the cross-product of the two tagsets. Intuitively, this approximation would be exactly correct if each aligned tag had been generated twice \u2014 once by the transition parameter and once by the coupling parameter \u2014 instead of a single time by the product of experts.\nThe alternative to approximately marginalizing these parameters would be to sample them using a Metropolis-Hastings scheme as in the work by Snyder, Naseem, Eisenstein, and Barzilay (2008). The use of approximate marginalizations represents a bias-variance tradeoff, where the decreased sampling variance justifies the bias introduced by the approximations, for practical numbers of samples."}, {"heading": "3.2 Multilingual Unsupervised Tagging: A Latent Variable Model", "text": "The model described in the previous section is designed for bilingual aligned data; as we will see in Section 5, it exploits such data very effectively. However, many resources contain more than two languages: for example, Europarl contains eleven, and the Multext-East corpus contains eight. This raises the question of how best to exploit all available resources when multi-aligned data is available.\nOne possibility would be to train separate bilingual models and then combine their output at test time, either by voting or some other heuristic. However, we believe that cross-lingual information reduces ambiguity at training time, so it would be preferable to learn from multiple languages jointly during training. Indeed, the results in Section 5 demonstrate that joint training outperforms such a voting scheme.\nAnother alternative would be to try to extend the bilingual model developed in the previous section. While such an extension is possible in principle, the merged node model does not scale well in the case of multi-aligned data across more than two languages. Recall that we use merged nodes to represent both tags for aligned words; the state space of such nodes grows as |T |L, exponential in the number of languages L. Similarly, the coupling parameter \u03c9 has the same dimension, so that the counts required for estimation become too sparse as the number of languages increases. Moreover, the bi-tag model required removing crossing edges in the word-alignment, so as to avoid cycles. This is unproblematic for pairs of aligned sentences, usually requiring the removal of less than 5% of all edges (see Table 16 in Appendix A). However, as the number of languages grows, an increasing number of alignments will have to be discarded.\nInstead, we propose a new architecture specifically designed for the multilingual setting. As before, we maintain HMM substructures for each language, so that the learned parameters can easily be applied to monolingual data. However, rather than merging tag nodes for aligned words, we introduce a layer of superlingual tags. The role of these latent nodes is to capture cross-lingual patterns. Essentially they perform a non-parametric clustering over sets of aligned tags, encouraging multilingual patterns that occur elsewhere in the corpus.\nMore concretely, for every set of aligned words, we add a superlingual tag with outgoing edges to the relevant part-of-speech nodes. An example configuration is shown in Figure 1c. The superlingual tags are each generated independently, and they influence the selection of the part-of-speech tags to which they are connected. As before, we use a product-of-experts model to combine these cross-lingual cues with the standard HMM transition model.\nThis setup scales well. Crossing and many-to-many alignments may be used without creating cycles, as all cross-lingual information emanates from the hidden superlingual tags. Furthermore, the size of the model and its parameter space scale linearly with the number of languages. We now describe the role of the superlingual tags in more detail."}, {"heading": "3.2.1 PROPAGATING CROSS-LINGUAL PATTERNS WITH SUPERLINGUAL TAGS", "text": "Each superlingual tag specifies a set of distributions \u2014 one for each language\u2019s part-of-speech tagset. In order to learn repeated cross-lingual patterns, we need to constrain the number of values that the superlingual tags can take and thus the number of distributions they provide. For example, we might allow the superlingual tags to take on integer values from 1 to K, with each integer value indexing a separate set of tag distributions. Each set of distributions should correspond to a discovered cross-lingual pattern in the data. For example, one set of distributions might favor nouns in each language and another might favor verbs, though heterogenous distributions (e.g., favoring determiners in one language and prepositions in others) are also possible.\nRather than fixing the number of superlingual tag values to an arbitrary size K, we leave it unbounded, using a non-parametric Bayesian model. To encourage the desired multilingual clustering behavior, we use a Dirichlet process prior (Ferguson, 1973). Under this prior, high posterior probability is obtained only when a small number of values are used repeatedly. The actual number of sampled values will thus be dictated by the data.\nWe draw an infinite sequence of distribution sets \u03a81,\u03a82, . . . from some base distribution G0. Each \u03a8i is a set of distributions over tags, with one distribution per language, written \u03c8 (\u2113) i . To weight these sets of distributions, we draw an infinite sequence of mixture weights \u03c01, \u03c02, . . . from a stick-breaking process, which defines a distribution over the integers with most probability mass placed on some initial set of values. The pair of sequences \u03c01, \u03c02, . . . and \u03a81,\u03a82, . . . now define the distribution over superlingual tags and their associated distributions on parts-of-speech. Each superlingual tag z \u2208 N is drawn with probability \u03c0z , and is associated with the set of multinomials \u3008\u03c8\u2113z, \u03c8 \u2113\u2032\nz , . . .\u3009. As in the merged node model, the distribution over aligned part-of-speech tags is governed by a product of experts. In this case, the incoming edges are from the superlingual tags (if any) and the predecessor tag. We combine these distributions via their normalized product. Assuming tag position i of language \u2113 is connected to M superlingual tags, the part-of-speech tag yi is drawn according to,\nyi \u223c \u03c6yi\u22121(yi)\n\u220fM m=1 \u03c8 \u2113 zm (yi)\nZ , (8)\nwhere \u03c6yi\u22121 indicates the transition distribution, zm is the value of the m th connected superlingual tag, and \u03c8\u2113zm(yi) indicates the tag distribution for language \u2113 given by \u03a8zm . The normalization Z is obtained by summing this product over all possible values of yi.\nThis parameterization allows for a relatively simple parameter space. It also leads to a desirable property: for a tag to have high probability, each of the incoming distributions must allow it. That is, any expert can \u201cveto\u201d a potential tag by assigning it low probability, generally leading to consensus decisions.\nWe now formalize this description by giving the stochastic generative process for the observed data (raw parallel text and alignments), according to the multilingual model."}, {"heading": "3.2.2 LATENT VARIABLE MODEL: GENERATIVE STORY", "text": "For n languages, we assume the existence of n tagsets T 1, . . . , Tn and vocabularies, W 1, . . . ,Wn, one for each language. Table 1 summarizes all relevant parameters. For clarity the generative process is described using only bigram transition dependencies, but our experiments use a trigram model, without any bigram interpolations.\n1. Transition and Emission Parameters. For each language \u2113 = 1, ..., n and for each tag t \u2208 T \u2113, draw a transition distribution \u03c6\u2113t over tags T\u2113 and an emission distribution \u03b8 \u2113 t over\nwords W \u2113, all from symmetric Dirichlet priors of appropriate dimension.\n2. Superlingual Tag Parameters. Draw an infinite sequence of sets of distributions over tags \u03a81,\u03a82, . . ., where each \u03a8i is a set of n multinomials \u3008\u03c81i , \u03c8 2 i , . . . \u03c8 n i \u3009, one for each of n\nlanguages. Each multinomial \u03c8\u2113i is a distribution over the tagset T \u2113, and is drawn from a symmetric Dirichlet prior; these priors together comprise the base distribution G0, from which each \u03a8i is drawn.\nAt the same time, draw an infinite sequence of mixture weights \u03c0 \u223c GEM(\u03b1), where GEM(\u03b1) indicates the stick-breaking distribution (Sethuraman, 1994) with concentration parameter \u03b1 = 1. These parameters define a distribution over superlingual tags, or equivalently over the part-of-speech distributions that they index:\nz \u223c \u2211\u221e\nk \u03c0k\u03b4k=z (9)\n\u03a8 \u223c \u2211\u221e\nk \u03c0k\u03b4\u03a8=\u03a8k (10)\nwhere \u03b4\u03a8=\u03a8k is defined as one when \u03a8 = \u03a8k and zero otherwise. From Formula 10, we can say that the set of multinomials \u03a8 is drawn from a Dirichlet process, conventionally written DP (\u03b1,G0).\n3. Data. For each multilingual parallel sentence:\n(a) Draw an alignment a from multilingual alignment distribution Am. The alignment a specifies sets of aligned indices across languages; each such set may consist of indices in any subset of the languages.\n(b) For each set of indices in a, draw a superlingual tag value z according to Formula 9.\n(c) For each language \u2113, for i = 1, . . . (until end-tag reached):\ni. Draw a part-of-speech tag yi \u2208 T \u2113 according to Formula 8.\nii. Draw a word wi \u2208 W \u2113 according to the emission distribution \u03b8yi .\nOne important difference from the merged node model generative story is that the distribution over multilingual alignments Am is unconstrained: we can generate crossing and many-to-one alignments as needed. To perform Bayesian inference under this model we again use Gibbs sampling, marginalizing parameters whenever possible."}, {"heading": "3.2.3 LATENT VARIABLE MODEL: INFERENCE", "text": "As in section 3.1.2, we employ a sampling-based inference procedure. Again, standard closed forms are used to analytically marginalize the emission parameters \u03b8, and approximate marginalizations are applied to transition parameters \u03c6, and superlingual tag distributions \u03c8\u2113i ; similar techniques are used to marginalize the superlingual tag mixture weights \u03c0. As before, these approximations would be exact if each of the parameters in the numerator of Formula 8 were solely responsible for other sampled tags.\nWe still must sample the part-of-speech tags y and superlingual tags z. The remainder of the section describes the sampling equations for these variables.\nSampling Part-of-speech Tags To sample the part-of-speech tag for language \u2113 at position i we draw from:\nP (y\u2113i |y\u2212(\u2113,i),x,a, z) \u221d P (x \u2113 i |x \u2113 \u2212i,y \u2113)P (y\u2113i+1|y \u2113 i ,y\u2212(\u2113,i),a, z)P (y \u2113 i |y\u2212(\u2113,i),a, z) (11)\nwhere y\u2212(\u2113,i) refers to all tags except y \u2113 i . The first factor handles the emissions, and the latter two factors are the generative probabilities of (i) the current tag given the previous tag and superlingual tags, and (ii) the next tag given the current tag and superlingual tags. These two quantities are similar to equation 8, except here we integrate over the transition parameter \u03c6yi\u22121 and the superlingual tag parameters \u03c9\u2113z . We end up with a product of integrals, each of which we compute in closed form.\nTerms involving the transition distributions \u03c6 and the emission distributions \u03b8 are identical to the bilingual case, as described in Section 3.1.2. The closed form for integrating over the parameter of a superlingual tag with value z is given by:\n\u222b\n\u03c8\u2113z(yi)P (\u03c8 \u2113 z|\u03c8 \u2113 0)d\u03c8 \u2113 z =\nn(z, yi, \u2113) + \u03c8 \u2113 0 n(z, \u2113) + T \u2113\u03c8\u21130\nwhere n(z, yi, \u2113) is the number of times that tag yi is observed together with superlingual tag z in language \u2113, n(z, \u2113) is the total number of times that superlingual tag z appears with an edge into language \u2113, and \u03c8\u21130 is a symmetric Dirichlet prior over tags for language \u2113.\nSampling Superlingual Tags For each set of aligned words in the observed alignment a we need to sample a superlingual tag z. Recall that z is an index into an infinite sequence\n\u3008\u03c8\u211311 , . . . , \u03c8 \u2113n 1 \u3009, \u3008\u03c8 \u21131 2 , . . . , \u03c8 \u2113n 2 \u3009, . . . ,\nwhere each \u03c8\u2113z is a distribution over the tagset T \u2113. The generative distribution over z is given by Formula 9. In our sampling scheme, however, we integrate over all possible settings of the mixture weights \u03c0 using the standard Chinese Restaurant Process closed form (Escobar & West, 1995):\nP ( zi \u2223 \u2223z\u2212i,y ) \u221d \u220f\n\u2113\nP ( y\u2113i \u2223 \u2223zi, z\u2212i,y\u2212(\u2113,i) ) \u00b7\n{\n1 k+\u03b1n(zi) if zi \u2208 z\u2212i \u03b1\nk+\u03b1 otherwise (12)\nThe first group of factors is the product of closed form probabilities for all tags connected to the superlingual tag, conditioned on zi. Each of these factors is calculated in the same manner as equation 11 above. The final factor is the standard Chinese Restaurant Process closed form for posterior sampling from a Dirichlet process prior. In this factor, k is the total number of sampled superlingual tags, n(zi) is the total number of times the value zi occurs in the sampled superlingual tags, and \u03b1 is the Dirichlet process concentration parameter (see Step 2 in Section 3.2.2)."}, {"heading": "3.3 Implementation", "text": "This section describes implementation details that are necessary to reproduce our experiments. We present details for the merged node and latent variable models, as well as our monolingual baseline."}, {"heading": "3.3.1 INITIALIZATION", "text": "An initialization phase is required to generate initial settings for the word tags and hyperparameters, and for the superlingual tags in the latent variable model. The initialization is as follows:\n\u2022 Monolingual Model\n\u2013 Tags: Random, with uniform probability among tag dictionary entries for the emitted word.\n\u2013 Hyperparameters \u03b80, \u03c60: Initialized to 1.0\n\u2022 Merged Node Model\n\u2013 Tags: Random, with uniform probability among tag dictionary entries for the emitted word. For joined tag nodes, each slot is selected from the tag dictionary of the emitted word in the appropriate language.\n\u2013 Hyperparameters \u03b80, \u03c60, \u03c90: Initialized to 1.0\n\u2022 Latent Variable Model\n\u2013 Tags: Set to the final estimate from the monolingual model.\n\u2013 Superlingual Tags: Initially a set of 14 superlingual tag values is assumed \u2014 each value corresponds to one part-of-speech tag. Each alignment is assigned one of these 14 values based on the most common initial part-of-speech tag of the words in the alignment.\n\u2013 Hyperparameters \u03b8\u21130, \u03c6 \u2113 0: Initialized to 1.0 \u2013 Base Distribution G\u21130: Set to a symmetric Dirichlet distribution with parameter value fixed to 1.0\n\u2013 Concentration Parameter \u03b1: Set to 1.0 and remains fixed throughout."}, {"heading": "3.3.2 HYPERPARAMETER ESTIMATION", "text": "Both models have symmetric Dirichlet priors \u03b80 and \u03c60, for the emission and transition distributions respectively. The merged node model also has symmetric Dirichlet prior \u03c90 on the coupling parameter. We re-estimate these priors during inference, based on non-informative hyperpriors.\nHyperparameter re-estimation applies the Metropolis-Hastings algorithm after each full epoch of sampling the tags. In addition, we run an initial 200 iterations to speed convergence. MetropolisHastings is a sampling technique that draws a new value u from a proposal distribution, and makes a stochastic decision about whether to accept the new sample (Gelman et al., 2004). This decision is based on the proposal distribution and on the joint probability of u with the observed and sampled variables x\u2113 and y\u2113.\nWe assume an improper prior P (u) that assigns uniform probability mass over the positive reals, and use a Gaussian proposal distribution with the mean set to the previous value of the parameter and\nvariance set to one-tenth of the mean.5 For non-pathological proposal distributions, the MetropolisHastings algorithm is guaranteed to converge in the limit to a stationary Markov chain centered on the desired joint distribution. We observe an acceptance rate of approximately 1/6, which is in line with standard recommendations for rapid convergence (Gelman et al., 2004)."}, {"heading": "3.3.3 FINAL PARAMETER ESTIMATES", "text": "The ultimate goal of training is to learn models that can be applied to unaligned monolingual data. Thus, we need to construct estimates for the transition and emission parameters \u03c6 and \u03b8. Our sampling procedure focuses on the tags y. We construct maximum a posteriori estimates y\u0302, indicating the most likely tag sequences for the aligned training corpus. The predicted tags y\u0302 are then combined with priors \u03c60 and \u03b80 to construct maximum a posteriori estimates of the transition and emission parameters. These learned parameters are then applied to the monolingual test data to find the highest probability tag sequences using the Viterbi algorithm.\nFor the monolingual and merged node models, we perform 200 iterations of sampling, and select the modal tag settings in each slot. Further sampling was not found to produce different results. For the latent variable model, we perform 1000 iterations of sampling, and select the modal tag values from the last 100 samples."}, {"heading": "4. Experimental Setup", "text": "We perform a series of empirical evaluations to quantify the contribution of bilingual and multilingual information for unsupervised part-of-speech tagging. Our first evaluation follows the standard procedures established for unsupervised part-of-speech tagging: given a tag dictionary (i.e., a set of possible tags for each word type), the model selects the appropriate tag for each token occurring in a text. We also evaluate tagger performance when the available dictionaries are incomplete (Smith & Eisner, 2005; Goldwater & Griffiths, 2007). In all scenarios, the model is trained using only untagged text.\nIn this section, we first describe the parallel data and part-of-speech annotations used for system evaluation. Next we describe a monolingual baseline and the inference procedure used for testing."}, {"heading": "4.1 Data", "text": "As a source of parallel data, we use Orwell\u2019s novel \u201cNineteen Eighty Four\u201d in the original English as well as its translation to seven languages \u2014 Bulgarian, Czech, Estonian, Hungarian, Slovene, Serbian and Romanian.6 Each translation was produced by a different translator and published in print separately by different publishers.\nThis dataset has representatives from four language families \u2014 Slavic, Romance, Ugric and Germanic. This data is distributed as part of the publicly available Multext-East corpus, Version 3 (Erjavec, 2004). The corpus provides detailed morphological annotation at the token level, including part-of-speech tags. In addition, a lexicon for each language is provided.\n5. This proposal is identical to the parameter re-estimation applied for emission and transition priors by Goldwater and Griffiths (2007). 6. In our initial publication (Snyder et al., 2008), we used a subset of this data, only including sentences that have one-to-one alignments between all four languages considered in that paper. The current set-up makes use of all the sentences available in the corpus.\nThe corpus consists of 118,426 English words in 6,736 sentences (see Table 3). Of these sentences, the first 75% are used for training, taking advantage of the multilingual alignments. The remaining 25% are used for evaluation. In the evaluation, only monolingual information is made available to the model, to simulate performance on non-parallel data."}, {"heading": "4.2 Alignments", "text": "In our experiments we use sentence-level alignments provided in the Multext-East corpus. Wordlevel alignments are computed for each language pair using GIZA++ (Och & Ney, 2003). The procedures for handling these alignments are different for the merged node and latent variable models."}, {"heading": "4.2.1 MERGED NODE MODEL", "text": "We obtain 28 parallel bilingual corpora by considering all pairings of the eight languages. To generate one-to-one alignments at the word level, we intersect the one-to-many alignments going in each direction. This process results in alignment of about half the tokens in each bilingual parallel corpus. We further automatically remove crossing alignment edges, as these would induce cycles in the graphical model. We employ a simple heuristic: crossing alignment edges are removed based on the order in which they appear from left to right; this step eliminates on average 3.62% of the edges. Table 2 shows the number of aligned words for each language pair after removing crossing edges. More detailed statistics about the total number of alignments are provided in Appendix A."}, {"heading": "4.2.2 LATENT VARIABLE MODEL", "text": "As in the previous setting, we run GIZA++ on all 28 pairings of the 8 languages, taking the intersection of alignments in each direction. Since we want each latent superlingual variable to span as many languages as possible, we aggregate pairwise lexical alignments into larger sets of densely aligned words and assign a single latent superlingual variable to each such set. Specifically, for each word token, we consider the set of the word itself and all word tokens to which it is aligned. If pairwise alignments occur between 2/3 of all token pairs in this set, then it is considered densely\nconnected and is admitted as an alignment set. Otherwise, increasingly smaller subsets are considered until one that is densely connected is found. This procedure is repeated for all word tokens in the corpus that have at least one alignment. Finally, the alignment sets are pruned by removing those which are subsets of larger alignment sets. Each of the remaining sets is considered the site of a latent superlingual variable.\nThis process can be illustrated by an example. The sentence \u201cI know you, the eyes seemed to say, I see through you,\u201d appears in the original English version of the corpus. The English word token seemed is aligned to word tokens in Serbian (c\u030cinilo), Estonian (na\u0308is), and Slovenian (zdelo). The Estonian and Slovenian tokens are aligned to each other. Finally, the Serbian token is aligned to a Hungarian word token (mintha), which is itself not aligned to any other tokens. This configuration is shown in Figure 2, with the nodes labeled by the two-letter language abbreviations.\nWe now construct alignment sets for these words.\n\u2022 For the Hungarian word, there is only one other aligned word, in Serbian, so the alignment set consists only of this pair (C1 in the figure).\n\u2022 The Serbian word has aligned partners in both Hungarian and English; overall this set has two pairwise alignments out of a possible three, as the English and Hungarian words are not aligned. Still, since 2/3 of the possible alignments are present, an alignment set (C2) is formed. C1 is subsumed by C2, so it is eliminated.\n\u2022 The English word is aligned to tokens in Serbian, Estonian, and Slovenian; four of six possible links are present, so an alignment set (C3) is formed. Note that if the Estonian and Slovenian words were not aligned to each other then we would have only three of six links, so the set\nwould not be densely connected by our definition; we would then remove a member of the alignment set.\n\u2022 The Estonian token is aligned to words in Slovenian and English; all three pairwise alignments are present, so an alignment set (C4) is formed. An identical alignment set is formed by starting with the Slovenian word, but only one superlingual tag is created.\nThus, for these five word tokens, a total of three overlapping alignment sets are created. Over the entire corpus, this process results in 284,581 alignment sets, covering 76% of all word tokens. Of these tokens, 61% occur in exactly one alignment set, 29% occur in exactly two alignment sets, and the remaining 10% occur in three or more alignment sets. Of all alignment sets, 32% include words in just two languages, 26% include words in exactly three languages, and the remaining 42% include words in four or more languages. The sets remain fixed during sampling and are treated by the model as observed data."}, {"heading": "4.3 Tagset", "text": "The Multext-East corpus is manually annotated with detailed morphosyntactic information. In our experiments, we focus on the main syntactic category encoded as the first letter of the provided labels. The annotation distinguishes between 14 parts-of-speech, of which 11 are common for all languages in our experiments. Appendix B lists the tag repository for each of the eight languages.\nIn our first experiment, we assume that a complete tag lexicon is available, so that the set of possible parts-of-speech for each word is known in advance. We use the tag dictionaries provided in the Multext-East corpus. The average number of possible tags per token is 1.39. We also experimented with incomplete tag dictionaries, where entries are only available for words appearing more than five or ten times in the corpus. For other words, the entire tagset of 14 tags is considered. In these two scenarios, the average per-token tag ambiguity is 4.65 and 5.58, respectively. Finally we also considered the case when lexicon entries are available for only the 100 most frequent words. In this case the average tags per token ambiguity is 7.54. Table 3 shows the specific tag/token ratio for each language for all scenarios.\nIn the Multext-East corpus, punctuation marks are not annotated with part-of-speech tags. We expand the tag repository by defining a separate tag for all punctuation marks. This allows the model to make use of any transition or coupling patterns involving punctuation marks. However, we do not consider punctuation tokens when computing model accuracy."}, {"heading": "4.4 Monolingual Comparisons", "text": "As our monolingual baseline we use the unsupervised Bayesian hidden Markov model (HMM) of Goldwater and Griffiths (2007). This model, which they call BHMM1, modifies the standard HMM by adding priors and by performing Bayesian inference. Its performance is on par with state-ofthe-art unsupervised models. The Bayesian HMM is a particularly informative baseline because our model reduces to this baseline when there are no alignments in the data. This implies that any performance gain over the baseline can only be attributed to the multilingual aspect of our model. We used our own implementation after verifying that its performance on the Penn Treebank corpus was identical to that reported by Goldwater and Griffiths.\nTo provide an additional point of comparison, we use a supervised hidden Markov model trained using the annotated corpus. We apply the standard maximum-likelihood estimation and perform inference using Viterbi decoding with pseudo-count smoothing for unknown words (Rabiner, 1989). In Appendix C we also report supervised results using the \u201cStanford Tagger\u201d, version 1.67. Although the results are slightly lower than our own supervised HMM implementation, we note that this system is not directly comparable to our set-up, as it does not allow the use of a tag dictionary to constrain part-of-speech selections."}, {"heading": "4.5 Test Set Inference", "text": "We use the same procedure to apply all the models (the monolingual model, the bilingual merged node model, and the latent variable model) to test data. After training, trigram transition and word emission probabilities are computed, using the counts of tags assigned in the final training iteration. Similarly, the final sampled values of the hyperparameters are selected as smoothing parameters. We then apply Viterbi decoding to identify the highest probability tag sequences for each monolingual test set. We report results for multilingual and monolingual experiments averaged over five runs and for bilingual experiments averaged over three runs. The average standard-deviation of accuracy over multiple runs is less than 0.25 except when the lexicon is limited to the 100 most frequent words. In that case the standard deviation is 1.11 for monolingual model, 0.85 for merged node model and 1.40 for latent variable model."}, {"heading": "5. Results", "text": "In this section, we first report the performance for the two models on the full and reduced lexicon cases. Next, we report results for a semi-supervised experiment, where a subset of the languages have annotated text at training time. Finally, we investigate the sensitivity of both models to hyperparameter values and provide run time statistics for the latent variable model for increasing numbers of languages.\n7. http://nlp.stanford.edu/software/tagger.shtml"}, {"heading": "5.1 Full Lexicon Experiments", "text": "Our experiments show that both the merged node and latent variable models substantially improve tagging accuracy. Since the merged node model is restricted to pairs of languages, we provide average results over all possible pairings. In addition, we also consider two methods for combining predictions from multiple bilingual pairings: one using a voting scheme and the other employing an oracle to select the best pairings (see below for additional details).\nAs shown in Line 4 of Table 4, the merged node model achieves, on average, 93.2% accuracy, a two percentage point improvement over the monolingual baseline.8 The latent variable model \u2014 trained once on all eight languages \u2014 achieves 95% accuracy, nearly two percentage points higher than the bilingual merged node model. These two results correspond to error reductions of 23% and 43% respectively, and reduce the gap between unsupervised and supervised performance by over 30% and 60%.\nAs mentioned above, we also employ a voting scheme to combine information from multiple languages using the merged node model. Under this scheme, we train bilingual merged node models for each language pair. Then, when making tag predictions for a particular language \u2014 e.g., Romanian \u2014 we consider the preferences of each bilingual model trained with Romanian and a second language. The tag preferred by a plurality of models is selected. The results for this method are shown in line 6 of Table 4, and do not differ significantly from the average bilingual performance. Thus, this simple method of combining information from multiple language does not measure up to the joint multilingual model performance.\n8. The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by Goldwater and Griffiths (2007) on the WSJ corpus. We attribute this discrepancy to the differences in tag inventory used in our data-set. For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our tag inventory and corpus), the performance of Goldwater\u2019s model on WSJ is similar to what we report here.\nWe use the sign test to assess whether there are statistically significant differences in the accuracy of the tag predictions made by the monolingual baseline (line 2 of Table 4), the latent variable model (line 4), and the voting-based merged node model (line 6). All differences in these rows are found to be statistically significant at p < 0.05. Note that we cannot use the sign test to compare the average performance of the bilingual model (line 3), since this result is an aggregate over accuracies for every language pair."}, {"heading": "5.2 Reduced Lexicon Experiments", "text": "In realistic application scenarios, we may not have a tag dictionary with coverage across the entire lexicon. We consider three reduced lexicons: removing all words with counts of five or less; removing all words with counts of ten or less; and keeping only the top 100 most frequent words. Words that are removed from the lexicon can take any tag, increasing the overall difficulty of the task. These results are shown in Table 5 and graphically summarized in Figure 3. In all cases, the monolingual model is less robust to reduction in lexicon coverage than the multilingual models. In the case of the 100 word lexicon, the latent variable model achieves accuracy of 57.9%, compared to 53.8% for the monolingual baseline. The merged node model, on the other hand, achieves a slightly higher average performance of 59.5%. In the two other scenarios, the latent variable model trained on all eight languages outperforms the bilingual merged node model, even when an oracle selects the best bilingual pairing for each target language. For example, using the lexicon with words that appear greater than five times, the monolingual baseline achieves 74.7% accuracy, the merged node model using the best possible pairings achieves 81.7% accuracy, and the full latent variable model achieves an accuracy of 82.8%.\nNext we consider the performance of the bilingual merged node model when the lexicon is reduced for only one of the two languages. This condition may occur when dealing with two languages with asymmetric resources, in terms of unannotated text. As shown in Table 6, the merged models on average scores 5.7 points higher than the monolingual model when both tag dictionaries are reduced, but 14.3 points higher when the partner language has a full tag dictionary. This suggests that the bilingual models effectively transfer the additional lexical information available for the resource-rich language to the resource-poor language, yielding substantial performance improvements.\nPerhaps the most surprising result is that the resource-rich language gains as much on average from pairing with the resource-poor partner language as it would have gained from pairing with a language with a full lexicon. In both cases, an average accuracy of 93.2% is achieved, compared to the 91.1% monolingual baseline."}, {"heading": "5.3 Indirect Supervision", "text": "Although the main focus of this paper is unsupervised learning, we also provide some results indicating that multilingual learning can be applied to scenarios with varying amounts of annotated data. These scenarios are in fact quite realistic, as previously trained and highly accurate taggers will usually be available for at least some of the languages in a parallel corpus. We apply our latent variable model to these scenarios by simply treating the tags of annotated data (in any subset of languages) as fixed and observed throughout the sampling procedure. From a strictly probabilistic perspective this is the correct approach. However, we note that, in practice, heuristics and objective functions which place greater emphasis on the supervised portion of the data may yield better results. We do not explore that possibility here.\nTable 7 gives results for two scenarios of indirect supervision: where only one of the eight languages has annotated data, and where all but one of the languages has annotated data. In both cases, the unsupervised languages are provided with a 100 word lexicon, and all eight languages are trained together. When only one of the eight languages is supervised, the results vary depending on the choice of supervised language. When one of Bulgarian, Hungarian, or Romanian is supervised, no improvement is seen, on average, for the other seven languages. However, when Slovene is supervised, the improvement seen for the other languages is fairly substantial, with average accuracy rising to 64.8%, from 57.9% for the unsupervised latent variable model and 53.8% for the monolingual baseline. Perhaps unsurprisingly, the results are more impressive when all but one of the languages is supervised. In this case, the average accuracy of the lone unsupervised language rises to 74.4%. Taken together, these results indicate that any mixture of supervised resources may be added to the mix in a very simple and straightforward way, often yielding substantial improvements for the other languages."}, {"heading": "5.4 Hyperarameter Sensitivity and Runtime Statistics", "text": "Both models employ hyperparameters for the emission and transition distribution priors (\u03b80 and \u03c60 respectively) and the merged node model employs an additional hyperparameter for the coupling distribution prior (\u03c90). These hyperparameters are all updated throughout the inference procedure. The latent variable model uses two additional hyperparameters that remained fixed: the concentration parameter of the Dirichlet process (\u03b1) and the parameter of the base distribution for superlingual tags (\u03c80). For the experiments described above we used the initialization values listed in Section 3.3.1. Here we investigate the sensitivity of the models to different initializations of \u03b80, \u03c60, and \u03c90, and to different fixed values of \u03b1 and \u03c80. Tables 8 and 9 show the results obtained for the merged node and latent variable models, respectively, using a full lexicon. We observe that across a wide range of values, both models yield very similar results. In addition, we note that the final sampled hyperparameter values for transition and emission distributions always fall below one, indicating that sparse priors are preferred.\nAs mentioned in Section 3.2 one of the key theoretical benefits of the latent variable approach is that the size of the model and its parameter space scale linearly with the number of languages. Here we provide empirical confirmation by running the latent variable model on all possible subsets of the eight languages, recording the time elapsed for each run9. Figure 4 shows the average running time as the number of languages is increased (averaged over all subsets of each size). We see that the model running time indeed scales linearly as languages are added, and that the per-language running time increases very slowly: when all eight languages are included, the time taken is roughly double that for eight monolingual models run serially. Both of our models scale well with tagset size and the number of examples. The time dependence on the former is cubic, as we use trigram models and employ Viterbi decoding to find optimal sequences at test-time. During the training time, however, the time scales linearly with the tagset size for the latent variable model and quadratically for the merged node model. This is due to the use of Gibbs sampling that isolates the individual sampling decision on tags (for the latent variable model) and tag-pairs (for the merged node model). The dependence on the number of training examples is also linear for the same reason.\n9. All experiments were single-threaded and run using an Intel Xeon 3.0 GHz processor"}, {"heading": "6. Analysis", "text": "In this section we provide further analysis of: (i) factors that influence the effectiveness of language pairings in bilingual models, (ii) the incremental value of adding more languages in the latent vari-\nable model, (iii) the superlingual tags and their corresponding cross-lingual patterns as learned by the latent variable model, and (iv) whether multilingual data is more helpful than additional monolingual data. We focus here on the full lexicon scenario, though we expect our analysis to extend to the various reduced lexicon cases considered above as well."}, {"heading": "6.1 Predicting Effective Language Pairings", "text": "We first analyze the cross-lingual variation in performance for different bilingual language pairings. As shown in Table 10, the performance of the merged node model for each target language varies substantially across pairings. In addition, the identity of the optimally helpful language pairing also depends heavily on the target language in question. For instance, Slovene, achieves a large improvement when paired with Serbian (+7.4), a closely related Slavic language, but only a minor improvement when coupled with English (+1.8). On the other hand, for Bulgarian, the best performance is achieved when coupling with English (+6) rather than with closely related Slavic languages (+2.4 and +0). Thus, optimal pairings do not correspond simply to language relatedness. We note that when applying multilingual learning to morphological segmentation the best results were obtained for related languages, but only after incorporating declarative knowledge about their lower-level phonological relations using a prior which encourages phonologically close aligned morphemes (Snyder & Barzilay, 2008). Here too, a more complex model which models lower-level morphological relatedness (such as case) may yield better outcomes for closely related languages.\nAs an upper bound on the merged node model performance, line 7 of Table 10 shows the results when selecting (with the help of an oracle) the best partner for each language. The average accuracy using this oracle is 95.4%, substantially higher than the average bilingual pairing accuracy of 93.2%, and even somewhat higher than the latent variable model performance of 95%. This gap in\nperformance motivates a closer examination of the relationship between languages that constitute effective pairings."}, {"heading": "6.1.1 CROSS-LINGUAL ENTROPY", "text": "In a previous publication (Snyder et al., 2008) we proposed using cross-lingual entropy as a posthoc explanation for variation in coupling performance. This measure calculates the entropy of a tagging decision in one language given the identity of an aligned tag in the other language. While cross-lingual entropy seemed to correlate well with relative performance for the four languages considered in that publication, we find that it does not correlate as strongly for all eight languages considered here. We computed the Pearson correlation coefficient (Myers & Well, 2002) between the relative bilingual performance and cross-lingual entropy. For each target language, we rank the remaining seven languages based on two measures: how well the paired language contributes to improved performance of the target, and the cross-lingual entropy of the target language given the coupled language. We compute the Pearson correlation coefficient between these two rankings to assess their degree of overlap. See Table 19 in the Appendix for a complete list of results. On average, the coefficient was 0.29, indicating a weak positive correlation between relative bilingual performance and cross-lingual entropy."}, {"heading": "6.1.2 ALIGNMENT DENSITY", "text": "We note that even if cross-lingual entropy had exhibited higher correlation with performance, it would be of little practical utility in an unsupervised scenario since its estimation requires a tagged corpus. Next we consider the density of pairwise lexical alignments between language pairs as a predictive measure of their coupled performance. Since alignments constitute the multilingual anchors of our models, as a practical matter greater alignment density should yield greater opportunities for cross-lingual transfer. From the linguistic viewpoint, this measure may also indirectly\ncapture the correspondence between two languages. Moreover, this measure has the benefit of being computable from an untagged corpus, using automatically obtained GIZA++ alignments. As before, for each target language, we rank the other languages by relative bilingual performance, as well as by the percentage of words in the target language to which they provide alignments. Here we find an average Pearson coefficient of 0.42, indicating mild positive correlation. In fact, if we use alignment density as a criterion for selecting optimal pairing decisions for each target language, we obtain an average accuracy of 94.67% \u2014 higher than average bilingual performance, but still somewhat below the performance of the multilingual model."}, {"heading": "6.1.3 MODEL CHOICE", "text": "The choice of model may also contribute to the patterns of variability we observe across language pairs. To test this hypothesis, we ran our latent variable model on all pairs of languages. The results of this experiment are shown in Table 11. As in the case of the merged node model, the performance of each target language depends heavily on the choice of partner. However, the exact patterns of variability differ in this case from those observed for the merged node model. To measure this variability, we compare the pairing preferences for each language under each of the two models. More specifically, for each target language we rank the remaining seven languages by their contribution under each of our two models, and compute the Pearson coefficient between these two rankings. As seen in the last column of Table 19 in the Appendix, we find a coefficient of 0.49 between the two rankings, indicating positive, though far from perfect, correlation."}, {"heading": "6.1.4 UTILITY OF EACH LANGUAGE AS A BILINGUAL PARTNER", "text": "We also analyze the overall helpfulness of each language. As before, for each target language, we rank the remaining seven languages by the degree to which they contribute to increased target language performance when paired in a bilingual model. We can then ask whether the helpfulness\nrankings provided by each of the eight languages are correlated with one another \u2014 in other words, whether languages tend to be universally helpful (or unhelpful) or whether helpfulness depends heavily on the identity of the target language. We consider all pairs of target languages, and compute the Pearson rank correlation between their rankings of the six supplementary languages that they have in common (excluding the two target languages themselves). When we average these pairwise rank correlations we obtain a coefficient of 0.20 for the merged node model and 0.21 for the latent variable model. These low correlations indicate that language helpfulness depends crucially on the target language in question. Nevertheless, we can still compute the average helpfulness of each language (across all target languages) to obtain something like a \u201cuniversal\u201d helpfulness ranking. See Table 20 in the appendix for this ranking. We can then ask whether this ranking correlates with language properties which might be predictive of general helpfulness. We compare the universal helpfulness rankings10 to language rankings induced by tag-per-token ambiguity (the average number of tags allowed by the dictionary per token in the corpus) as well as trigram entropy (the entropy of the tag distribution given the previous two tags). In both cases we assign the highest rank to the language with lowest value, as we expect lower entropy and ambiguity to correlate with greater helpfulness. Contrary to expectations, the ranking induced by tag-per-token ambiguity actually correlates negatively with both universal helpfulness rankings by very small amounts (-0.28 for the merged node model and -0.23 for the latent variable model). For both models, Hungarian, which has the lowest tag-per-token ambiguity of all eight languages, had the worst universal helpfulness ranking. The correlations with trigram entropy were only a little more predictable. In the case of the latent variable model, there was no correlation at all between trigram entropy and universal helpfulness (-0.01). In the case of the merged node model, however, there was moderate positive correlation (0.43)."}, {"heading": "6.2 Adding Languages in the Latent Variable Model", "text": "While bilingual performance depends heavily on the choice of language pair, the latent variable model can easily incorporate all available languages, obviating the need for any choice. To test performance as the number of languages increases, we ran the latent variable model with all possible subsets of the eight languages in the full lexicon as well as all three reduced lexicon scenarios. Figures 5, 6, 7, and 8 plot the average accuracy as the number of available languages varies for all four lexicon scenarios (in decreasing order of the lexicon size). For comparison, the monolingual and average bilingual baseline results are given. In all scenarios, our latent variable model steadily gains in accuracy as the number of available languages increases, and in most scenarios sees an appreciable uptick when going from seven to eight languages. In the full lexicon case, the gap between supervised and unsupervised performance is cut by nearly two thirds under the unsupervised latent variable model with all eight languages.\nInterestingly, as the lexicon is reduced in size, the performance of the bilingual merged node model gains relative to the latent variable model on pairs. In the full lexicon case, the latent variable model is clearly superior, whereas in the two moderately reduced lexicon cases, the performance on pairs is more or less the same for the two models. In the case of the drastically reduced lexicon\n10. We note that the universal helpfulness rankings obtained from each of the two multilingual models match each other only roughly: their correlation coefficient with one another is 0.50. In addition, \u201cuniversal\u201d in this context refers only to the eight languages under consideration and the rankings could very well change in a wider multilingual context.\n(100 words), the merged node model is the clear winner. Thus, it seems that of the two models, the performance gains of the latent variable model are more sensitive to the size of the lexicon.\nThe same four figures (5, 6, 7, and 8) also show the multilingual performance broken down by language. All languages except for English tend to increase in accuracy as additional languages are added to the mix. Indeed, in the two cases of moderately reduced lexicons (Figures 6 and 7) all languages except for English show steady large gains which actually increase in size when going from seven to the full set of eight languages. In the full lexicon case (Figure 5), Estonian, Romanian, and Slovene display steady increases until the very end. Hungarian peaks at two languages, Bulgarian at three languages, and Czech and Serbian at seven languages. In the more drastic reduced lexicon case (Figure 8), the performance across languages is less consistent and the gains when languages are added are less stable. All languages report gains when going from one to two languages, but only half of them increase steadily up to eight languages. Two languages seem to trend downward after two or three languages, and the other two show mixed behavior.\nIn the full lexicon case (Figure 5), English is the only language which fails to improve. In the other scenarios, English gains initially but these gains are partially eroded when more languages are added. It is possible that English is an outlier since it has significantly lower tag transition entropy than any of the other languages (see Table 3). Thus it may be that internal tag transitions are simply more informative for English than any information that can be gleaned from multilingual context."}, {"heading": "6.3 Analysis of the Superlingual Tag Values", "text": "In this section we analyze the superlingual tags and their corresponding part-of-speech distributions, as learned by the latent variable model. Recall that each superlingual tag intuitively represents a discovered multilingual context and that it is through these tags that multilingual information is propagated. More formally, each superlingual tag provides a complete distribution over partsof-speech for each language, allowing the encoding of both primary and secondary preferences separately for each language. These preferences then interact with the language-specific context (i.e. the surrounding parts-of-speech and the corresponding word). We place a Dirichlet process prior on the superlingual tags, so the number of sampled values is dictated by the complexity of the data. In fact, as shown in Table 12, the number of sampled superlingual tags steadily increases with the number of languages. As multilingual contexts becomes more complex and diverse, additional superlingual tags are needed.\nNext we analyze the part-of-speech tag distributions associated with superlingual tag values. Most superlingual tag values correspond to low entropy tag distributions, with a single dominant part-of-speech tag across all languages. See, for example, the distributions associated with superlin-\ngual tag value 6 in Table 13, all of which favor nouns by large margins. Similar sets of distributions occur favoring verbs, adjectives, and the other primary part-of-speech categories. In fact, among the seventeen sampled superlingual tag values, nine belong to this type, and they cover 80% of actual superlingual tag instances. The remaining superlingual tags correspond to more complex crosslingual patterns. The associated tag distributions in those cases favor different part-of-speech tags in various languages and tend to have higher entropy, with the probability mass spread more evenly over two or three tags. One such example is the set of distributions associated with the superlingual tag value 14 in Table 13, which seems to be a mixed noun/verb class. In six out of eight languages the most favored tag is verb, while a strong secondary choice in these cases is noun. However, for Estonian and Hungarian, this preference is reversed, with nouns being given higher probability. This superlingual tag may have captured the phenomenon of \u201clight verbs,\u201d whereby verbs in one language correspond to a combination of a noun and verb in another language. For example the English verb whisper/V, when translated into Urdu, becomes the collocation whisper/N do/V. In these cases, verbs and nouns will often be aligned to one another, requiring a more complex superlingual tag. The analysis of these examples shows that the superlingual tags effectively learns both simple and complex cross-lingual patterns"}, {"heading": "6.3.1 PERFORMANCE WITH REDUCED DATA", "text": "One potential objection to the claims made in this section is that the improved results may be due merely to the addition of more data, so that the multilingual aspect of the model may be irrelevant. We test this idea by evaluating the monolingual, merged node, and latent variable systems on training sets in which the number of examples is reduced by half. The multilingual models in this setting have access to exactly half as much data as the monolingual model in the original experiment. As shown in Table 14, both the monolingual baseline and our models are quite insensitive to this drop in data. In fact, both of our models, when trained on half of the corpus, still outperform the monolingual model trained on the entire corpus. This indicates that the performance gains demonstrated by multilingual learning cannot be explained merely by the addition of more data."}, {"heading": "7. Conclusions", "text": "The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We considered two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables.\nOur results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. When a full lexicon is available, our two models cut the gap between unsupervised and supervised performance by nearly one third (merged node model, averaged over all pairs) and two thirds (latent variable model, using all eight languages). For all but one language, we observe performance gains as additional languages are added. The sole exception is English, which only gains from additional languages in reduced lexicon settings.\nIn most scenarios, the latent variable model achieves better performance than the merged node model, and has the additional advantage of scaling gracefully with the number of languages. These observations suggest that the non-parametric latent variable structure provides a more flexible paradigm for incorporating multilingual cues. However, the benefit of the latent variable model relative to the merged node model (even when running both models on pairs of languages) seems to decrease with the size of the lexicon. Thus, in practical scenarios where only a small lexicon or no lexicon is available, the merged node model may represent a better choice.\nOur experiments have shown that performance can vary greatly depending on the choice of additional languages. It is difficult to predict a priori which languages constitute good combinations. In particular, language relatedness itself cannot be used as a consistent predictor as sometimes closely related languages constitute beneficial couplings and sometimes unrelated languages are more helpful. We identify a number of features which correlate with bilingual performance, though we observe that these features interact in complex ways. Fortunately, our latent variable model allows us to bypass this question by simply using all available languages.\nIn both of our models, lexical alignments play a crucial role as they determine the typology of the model for each sentence. In fact, we observed a positive correlation between alignment density and bilingual performance, indicating the importance of high quality alignments. In our experiments, we considered the alignment structure an observed variable, produced by standard MT\ntools which operate over pairs of languages. An interesting alternative would be to incorporate alignment structure into the model itself, to find alignments best tuned for tagging accuracy based on the evidence of multiple languages rather than pairs.\nAnother limitation of the two models is that they only consider one-to-one lexical alignments. When pairing isolating and synthetic languages11 it would be beneficial to align short analytical phrases consisting of multiple words to single morpheme-rich words in the other language. To do so would involve flexibly aligning and chunking the parallel sentences throughout the learning process.\nAn important direction for future work is to incorporate even more sources of multilingual information, such as additional languages and declarative knowledge of their typological properties (Comrie, 1989). In this paper we showed that performance improves as the number of languages increases. We were limited by our corpus to eight languages, but we envision future work on massively parallel corpora involving dozens of languages as well as learning from languages with non-parallel data.\nBibliographic Note\nPortions of this work were previously presented in two conference publications (Snyder, Naseem, Eisenstein, & Barzilay, 2008, 2009). The current article extends this work in several ways, most notably: we present a new inference procedure for the merged node model which yields improved results (Section 3.1.2) and we conduct extensive new empirical analyses of the multilingual results. More specifically, we analyze properties of language combinations that contribute to successful multilingual learning, we show that adding multilingual data provides much greater benefit than increasing the quantity of monolingual data, we investigate additional scenarios of lexicon reduction, we investigate scalability as a function of the number of languages, and we experiment with semi-supervised settings (Sections 5 and 6)."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168 and grants IIS-0835445, IIS-0904684) and the Microsoft Research Faculty Fellowship. Thanks to Michael Collins, Tommi Jaakkola, Amir Globerson, Fernando Pereira, Lillian Lee, Yoong Keok Lee, Maria Polinsky and the anonymous reviewers for helpful comments and suggestions. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.\n11. Isolating languages are those with a morpheme to word ratio close to one, and synthetic languages are those which allow multiple morphemes to be easily combined into single words. English is an example of an isolating language, whereas Hungarian is a synthetic language."}, {"heading": "Appendix A. Alignment Statistics", "text": ""}, {"heading": "Appendix C. Stanford Tagger Performance", "text": ""}, {"heading": "Appendix B. Tag Repository", "text": ""}, {"heading": "Appendix D. Rank Correlation", "text": ""}, {"heading": "Appendix E. Universal Helpfulness", "text": ""}], "references": [{"title": "Trainable grammars for speech recognition", "author": ["J. Baker"], "venue": "In Proceedings of the Acoustical Society of America", "citeRegEx": "Baker,? \\Q1979\\E", "shortCiteRegEx": "Baker", "year": 1979}, {"title": "Part-of-speech tagging in context", "author": ["M. Banko", "R.C. Moore"], "venue": "In Proceedings of the COLING,", "citeRegEx": "Banko and Moore,? \\Q2004\\E", "shortCiteRegEx": "Banko and Moore", "year": 2004}, {"title": "Phrase-based statistical machine translation with pivot languages", "author": ["N. Bertoldi", "M. Barbaiani", "M. Federico", "R. Cattoni"], "venue": "In International Workshop on Spoken Language Translation Evaluation Campaign on Spoken Language Translation (IWSLT),", "citeRegEx": "Bertoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2008}, {"title": "Unsupervised sense disambiguation using bilingual probabilistic models", "author": ["I. Bhattacharya", "L. Getoor", "Y. Bengio"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bhattacharya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bhattacharya et al\\.", "year": 2004}, {"title": "Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging", "author": ["E. Brill"], "venue": "Computational Linguistics,", "citeRegEx": "Brill,? \\Q1995\\E", "shortCiteRegEx": "Brill", "year": 1995}, {"title": "Word-sense disambiguation using statistical methods", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Brown et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1991}, {"title": "Improving statistical machine translation efficiency by triangulation", "author": ["Y. Chen", "A. Eisele", "M. Kay"], "venue": "In Proceedings of LREC", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Chiang,? \\Q2005\\E", "shortCiteRegEx": "Chiang", "year": 2005}, {"title": "Machine translation by triangulation: Making effective use of multiparallel corpora", "author": ["T. Cohn", "M. Lapata"], "venue": "In Proceedings of ACL", "citeRegEx": "Cohn and Lapata,? \\Q2007\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2007}, {"title": "Language universals and linguistic typology: Syntax and morphology", "author": ["B. Comrie"], "venue": null, "citeRegEx": "Comrie,? \\Q1989\\E", "shortCiteRegEx": "Comrie", "year": 1989}, {"title": "Two languages are more informative than one", "author": ["I. Dagan", "A. Itai", "U. Schwall"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Dagan et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1991}, {"title": "An unsupervised method for word sense tagging using parallel corpora", "author": ["M. Diab", "P. Resnik"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Diab and Resnik,? \\Q2002\\E", "shortCiteRegEx": "Diab and Resnik", "year": 2002}, {"title": "MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora", "author": ["T. Erjavec"], "venue": "In Fourth International Conference on Language Resources and Evaluation, LREC,", "citeRegEx": "Erjavec,? \\Q2004\\E", "shortCiteRegEx": "Erjavec", "year": 2004}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M. Escobar", "M. West"], "venue": "Journal of the american statistical association,", "citeRegEx": "Escobar and West,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West", "year": 1995}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Bayesian data analysis", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Inducing a multilingual dictionary from a parallel multitext in related languages", "author": ["D. Genzel"], "venue": "In Proceedings of HLT/EMNLP,", "citeRegEx": "Genzel,? \\Q2005\\E", "shortCiteRegEx": "Genzel", "year": 2005}, {"title": "Markov chain Monte Carlo in practice", "author": ["W. Gilks", "S. Richardson", "D. Spiegelhalter"], "venue": null, "citeRegEx": "Gilks et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gilks et al\\.", "year": 1996}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S. Goldwater", "T.L. Griffiths"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Goldwater and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Goldwater and Griffiths", "year": 2007}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "In Proceedings of the Ninth International Conference on Artificial Neural Networks,", "citeRegEx": "Hinton,? \\Q1999\\E", "shortCiteRegEx": "Hinton", "year": 1999}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In Proceedings of EMNLP/CoNLL,", "citeRegEx": "Johnson,? \\Q2007\\E", "shortCiteRegEx": "Johnson", "year": 2007}, {"title": "Experiments in parallel-text based grammar induction", "author": ["J. Kuhn"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Kuhn,? \\Q2004\\E", "shortCiteRegEx": "Kuhn", "year": 2004}, {"title": "Word translation disambiguation using bilingual bootstrapping", "author": ["C. Li", "H. Li"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Li and Li,? \\Q2002\\E", "shortCiteRegEx": "Li and Li", "year": 2002}, {"title": "The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem", "author": ["J.S. Liu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Liu,? \\Q1994\\E", "shortCiteRegEx": "Liu", "year": 1994}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo,? \\Q1994\\E", "shortCiteRegEx": "Merialdo", "year": 1994}, {"title": "Current Issues in Linguistic Theory: Recent Advances in Natural Language Processing, chap. Unsupervised Natural Language Disambiguation Using Non-Ambiguous Words", "author": ["R. Mihalcea"], "venue": null, "citeRegEx": "Mihalcea,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea", "year": 2004}, {"title": "Research Design and Statistical Analysis (2nd edition)", "author": ["J.L. Myers", "A.D. Well"], "venue": null, "citeRegEx": "Myers and Well,? \\Q2002\\E", "shortCiteRegEx": "Myers and Well", "year": 2002}, {"title": "Exploiting parallel texts for word sense disambiguation: an empirical study", "author": ["H.T. Ng", "B. Wang", "Y.S. Chan"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Ng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Statistical multi-source translation", "author": ["F.J. Och", "H. Ney"], "venue": "In MT Summit", "citeRegEx": "Och and Ney,? \\Q2001\\E", "shortCiteRegEx": "Och and Ney", "year": 2001}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "Optimal constituent alignment with edge covers for semantic projection", "author": ["S. Pad\u00f3", "M. Lapata"], "venue": "In Proceedings of ACL, pp", "citeRegEx": "Pad\u00f3 and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3 and Lapata", "year": 2006}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "A perspective on word sense disambiguation methods and their evaluation", "author": ["P. Resnik", "D. Yarowsky"], "venue": "In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What,", "citeRegEx": "Resnik and Yarowsky,? \\Q1997\\E", "shortCiteRegEx": "Resnik and Yarowsky", "year": 1997}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N.A. Smith", "J. Eisner"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Smith and Eisner,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of the ACL/HLT,", "citeRegEx": "Snyder and Barzilay,? \\Q2008\\E", "shortCiteRegEx": "Snyder and Barzilay", "year": 2008}, {"title": "Unsupervised multilingual learning for pos tagging", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "Adding more languages improves unsupervised multilingual part-of-speech tagging: A bayesian non-parametric approach", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Snyder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2009}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Toutanova and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2008}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of NAACL/HLT,", "citeRegEx": "Utiyama and Isahara,? \\Q2006\\E", "shortCiteRegEx": "Utiyama and Isahara", "year": 2006}, {"title": "Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora", "author": ["D. Wu"], "venue": "In IJCAI,", "citeRegEx": "Wu,? \\Q1995\\E", "shortCiteRegEx": "Wu", "year": 1995}, {"title": "Machine translation with a stochastic grammatical channel", "author": ["D. Wu", "H. Wong"], "venue": "In Proceedings of the ACL/COLING,", "citeRegEx": "Wu and Wong,? \\Q1998\\E", "shortCiteRegEx": "Wu and Wong", "year": 1998}, {"title": "A backoff model for bootstrapping resources for non-English languages", "author": ["C. Xi", "R. Hwa"], "venue": "In Proceedings of EMNLP, pp", "citeRegEx": "Xi and Hwa,? \\Q2005\\E", "shortCiteRegEx": "Xi and Hwa", "year": 2005}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["D. Yarowsky", "G. Ngai", "R. Wicentowski"], "venue": "In Proceedings of HLT,", "citeRegEx": "Yarowsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 10, "context": "Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991).", "startOffset": 111, "endOffset": 151}, {"referenceID": 5, "context": "Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991).", "startOffset": 111, "endOffset": 151}, {"referenceID": 5, "context": "Bilingual data has been leveraged in this way in a variety of WSD models (Brown et al., 1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002; Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual data closely approximates that of manual annotation (Ng et al.", "startOffset": 73, "endOffset": 216}, {"referenceID": 29, "context": ", 1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002; Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual data closely approximates that of manual annotation (Ng et al., 2003).", "startOffset": 240, "endOffset": 257}, {"referenceID": 0, "context": "The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified bilingual grammar.", "startOffset": 29, "endOffset": 42}, {"referenceID": 7, "context": "These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).", "startOffset": 120, "endOffset": 151}, {"referenceID": 39, "context": "Multilingual learning has previously been applied to syntactic analysis; a pioneering effort was the inversion transduction grammar of Wu (1995). This method is trained on an unannotated parallel corpus using a probabilistic bilingual lexicon and deterministic constraints on bilingual tree structures.", "startOffset": 135, "endOffset": 145}, {"referenceID": 0, "context": "The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified bilingual grammar. These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005). One characteristic of this family of methods is that they were designed for inherently multilingual tasks such as machine translation and lexicon induction. While we share the goal of learning from multilingual data, we seek to induce monolingual syntactic structures that can be applied even when multilingual data is unavailable. In this respect, our approach is closer to the unsupervised multilingual grammar induction work of Kuhn (2004). Starting from the hypothesis that trees induced over parallel sentences should exhibit cross-lingual structural similarities, Kuhn uses word-level alignments to constrain the set of plausible syntactic constituents.", "startOffset": 30, "endOffset": 709}, {"referenceID": 16, "context": "For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages.", "startOffset": 14, "endOffset": 28}, {"referenceID": 16, "context": "For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. This work first induces bilingual models for each pair of languages and then combines them. We take a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och & Ney, 2001; Utiyama & Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico, & Cattoni, 2008) where the goal is to translate from multiple source languages to a single target language. By using multi-source corpora, these systems alleviate sparseness and increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation systems build separate bilingual models and then select a final translation from their output. For instance, a method developed by Och and Ney (2001) generates several alternative translations from source sentences expressed in different languages and selects the most likely candidate.", "startOffset": 14, "endOffset": 957}, {"referenceID": 8, "context": "Cohn and Lapata (2007) consider a different generative model: rather than combining alternative sentence translations in a post-processing step, their model estimates the target phrase translation distribu-", "startOffset": 0, "endOffset": 23}, {"referenceID": 22, "context": "Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities.", "startOffset": 56, "endOffset": 100}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.", "startOffset": 63, "endOffset": 78}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004).", "startOffset": 63, "endOffset": 290}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities. Such Bayesian priors permit integration over parameter settings, yielding models that perform well across a range of settings. This is particularly important in the case of small datasets, where many of the counts used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work. Several recent papers have explored the development of alternative training procedures and model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number of overlapping features in a conditional log-linear formulation.", "startOffset": 63, "endOffset": 1346}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities. Such Bayesian priors permit integration over parameter settings, yielding models that perform well across a range of settings. This is particularly important in the case of small datasets, where many of the counts used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work. Several recent papers have explored the development of alternative training procedures and model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number of overlapping features in a conditional log-linear formulation. Contrastive estimation is used to provide a training criterion which maximizes the probability of the observed sentences compared to a set of similar sentences created by perturbing word order. The use of a large set of features and a discriminative training procedure led to strong performance gains. Toutanova and Johnson (2008) propose an LDA-style model for unsupervised part-of-speech tagging, grouping words through a latent layer of ambiguity classes.", "startOffset": 63, "endOffset": 1801}, {"referenceID": 20, "context": "Haghighi and Klein (2006) also use a variety of morphological features, learning in an undirected Markov Random Field that permits overlapping features.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995; Mihalcea, 2004)", "startOffset": 102, "endOffset": 131}, {"referenceID": 27, "context": "In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995; Mihalcea, 2004)", "startOffset": 102, "endOffset": 131}, {"referenceID": 21, "context": "Product-of-expert models (Hinton, 1999) allow each information source to exercise very strong negative influence on the probability of tags that they consider to be inappropriate, as compared with additive models.", "startOffset": 25, "endOffset": 39}, {"referenceID": 25, "context": "This technique is known as collapsed sampling; it is guaranteed never to increase sampling variance, and will often reduce it (Liu, 1994).", "startOffset": 126, "endOffset": 137}, {"referenceID": 19, "context": "The integral is tractable due to Dirichlet-multinomial conjugacy, and an identical marginalization was applied in the monolingual Bayesian HMM of Goldwater and Griffiths (2007). For unaligned tags, it is also possible to exactly marginalize the parameter \u03c6 governing transitions.", "startOffset": 146, "endOffset": 177}, {"referenceID": 14, "context": "To encourage the desired multilingual clustering behavior, we use a Dirichlet process prior (Ferguson, 1973).", "startOffset": 92, "endOffset": 108}, {"referenceID": 35, "context": "At the same time, draw an infinite sequence of mixture weights \u03c0 \u223c GEM(\u03b1), where GEM(\u03b1) indicates the stick-breaking distribution (Sethuraman, 1994) with concentration parameter \u03b1 = 1.", "startOffset": 130, "endOffset": 148}, {"referenceID": 15, "context": "MetropolisHastings is a sampling technique that draws a new value u from a proposal distribution, and makes a stochastic decision about whether to accept the new sample (Gelman et al., 2004).", "startOffset": 169, "endOffset": 190}, {"referenceID": 15, "context": "We observe an acceptance rate of approximately 1/6, which is in line with standard recommendations for rapid convergence (Gelman et al., 2004).", "startOffset": 121, "endOffset": 142}, {"referenceID": 12, "context": "This data is distributed as part of the publicly available Multext-East corpus, Version 3 (Erjavec, 2004).", "startOffset": 90, "endOffset": 105}, {"referenceID": 38, "context": "In our initial publication (Snyder et al., 2008), we used a subset of this data, only including sentences that have one-to-one alignments between all four languages considered in that paper.", "startOffset": 27, "endOffset": 48}, {"referenceID": 19, "context": "This proposal is identical to the parameter re-estimation applied for emission and transition priors by Goldwater and Griffiths (2007). 6.", "startOffset": 104, "endOffset": 135}, {"referenceID": 33, "context": "We apply the standard maximum-likelihood estimation and perform inference using Viterbi decoding with pseudo-count smoothing for unknown words (Rabiner, 1989).", "startOffset": 143, "endOffset": 158}, {"referenceID": 19, "context": "As our monolingual baseline we use the unsupervised Bayesian hidden Markov model (HMM) of Goldwater and Griffiths (2007). This model, which they call BHMM1, modifies the standard HMM by adding priors and by performing Bayesian inference.", "startOffset": 90, "endOffset": 121}, {"referenceID": 19, "context": "The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by Goldwater and Griffiths (2007) on the WSJ corpus.", "startOffset": 98, "endOffset": 129}, {"referenceID": 38, "context": "In a previous publication (Snyder et al., 2008) we proposed using cross-lingual entropy as a posthoc explanation for variation in coupling performance.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "An important direction for future work is to incorporate even more sources of multilingual information, such as additional languages and declarative knowledge of their typological properties (Comrie, 1989).", "startOffset": 191, "endOffset": 205}], "year": 2009, "abstractText": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.", "creator": null}}}