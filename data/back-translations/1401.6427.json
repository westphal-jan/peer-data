{"id": "1401.6427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Towards Unsupervised Learning of Temporal Relations between Events", "abstract": "Since most existing methods are monitored and require large corpora, which do not exist for many languages, we have focused our efforts on reducing the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly monitored machine learning approach for classifying temporal relationships between events. In the first stage, the algorithm learns a general classifier from a annotated corpus. Then, inspired by the hypothesis of \"some kind of temporal relationship per discourse,\" it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier, an expected time relation between events.", "histories": [["v1", "Thu, 23 Jan 2014 02:50:50 GMT  (795kb)", "http://arxiv.org/abs/1401.6427v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["seyed abolghasem mirroshandel", "gholamreza ghassem-sani"], "accepted": false, "id": "1401.6427"}, "pdf": {"name": "1401.6427.pdf", "metadata": {"source": "CRF", "title": "Towards Unsupervised Learning of Temporal Relations between Events", "authors": ["Seyed Abolghasem Mirroshandel", "Gholamreza Ghassem-Sani"], "emails": ["mirroshandel@ce.sharif.edu", "sani@sharif.edu"], "sections": [{"heading": "1. Introduction", "text": "Much progress has been made in natural language processing (NLP) in recent years. Combining statistical and symbolic methods has played a significant role in these advances. As a result, tasks such as part-of-speech tagging (S\u00f8gaard, 2011), parsing (Petrov & Klein, 2007), and named entity recognition (Mikheev, Grover, & Moens, 1998) have been addressed with satisfactory results. However, in some other tasks such as temporal information processing, which need a deeper analysis of meaning, the achieved results have not yet been as satisfactory.\nTemporal information is encoded in the textual description of events. Lately, the increasing attention to practical NLP applications such as question answering, summarization, and information extraction have resulted in a growing demand of temporal information processing (Tatu & Srikanth, 2008). In question answering, one may expect the system to answer questions such as \u201cwhen an event occurred\u201d, or \u201cwhat is the chronological order be-\nc\u00a92012 AI Access Foundation. All rights reserved.\ntween some desired events\u201d. In text summarization, especially in the multi-document type, knowing the order of events is a useful source for correctly merging related information.\nConstruction of the TimeBank corpus in 2003 (Pustejovsky et al., 2003), provided the opportunity of applying different machine learning methods to the task of temporal relation extraction. However, it has been realized that even a six-class classification of temporal relations is a very complicated task, even for human annotators (Mani, Verhagen, Wellner, Lee, & Pustejovsky, 2006).\nThis paper presents two different approaches in which the need for annotated data in temporal relation learning is reduced. The first approach is a weakly supervised machine learning algorithm for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of \u201cone type of temporal relation per discourse\u201d, it extracts useful information from a cluster of topically related documents for retraining of the model. By combining the global information of such a cluster with local decisions of a general classifier, we propose a novel bootstrapping cross-document classifier to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is at least 7% higher than that of the state-of-the-art of statistical methods (Chambers, Wang, & Jurafsky, 2007).\nThe second introduced approach is a novel usage of expectation maximization (EM) algorithm for temporal relation learning. This algorithm also employs Allen\u2019s interval algebra (Allen, 1984) for correction of predicted relations. For applying interval algebra, we utilize two different approaches: 1) a heuristic search method and 2) integer linear programming (ILP). We think that the experimental results of this EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.\nThe remainder of this paper is organized as follows: section 2 is about previous approaches to the temporal relation learning. Section 3 explains our first proposed method, which is evaluated in section 4. The second algorithm is explained in section 5, and evaluated in section 6. Finally, section 7 includes our conclusions and some possible future work."}, {"heading": "2. Temporal Relation Learning", "text": "Assuming that we have access to the texts in which events and time expressions have been appropriately tagged, two different tasks pertaining to temporal relation learning can be distinguished: 1) detecting whether there exist any relation between a given pair of events/time expressions; 2) identifying the relation type for positive cases of the first task. The first task is very hard to evaluate, because the annotators may ignore many plausible existing relations while tagging the corpora (Mani et al., 2006). Accordingly, in this paper like other existing research, we have only addressed the second task, which can be more specifically defined as follows: \u201cFor a given ordered pair of components (x1, x2), where x1 and x2 are annotated events and/or time expressions, a temporal relation classifier identifies the type of relation ri that temporally links x1 to x2\u201d. As it is shown in Figure 1, each temporal relation can be one of the fourteen types proposed in TimeML (Pustejovsky et al., 2003). For example, in \u201cPowerful political pressures (event1) may convince (event2) the Conservative government to keep (event3) its so-called golden share, which limits any individual holding to\n15%, until the restriction (event4) expires (event5) on Dec. 31, 1990 (time1)\u201d. (taken from document wsj 0745 of TimeBank, see Pustejovsky et al., 2003). The task is to automatically tag the relations between pairs (event1, event2), (event3, event5), (event5, event4), and (event5, time1) with BEFORE, ENDED BY, ENDS, and IS INCLUDED, respectively (see Figure 2). Since automatic extraction of just \u201cevent-event\u201d relations is itself a difficult task, in this paper, we have focused on this particular task, and left the detection of other type of temporal relations such as \u201cevent-time\u201d or \u201ctime-time\u201d to future work.\nThere are many ongoing research focusing on temporal relation learning. Additionally, there have been two important shared tasks on temporal information extraction: TempEval 2007 (Verhagen et al., 2007) and TempEval 2010 (Verhagen, Sauri, Caselli, & Pustejovsky, 2010). In TempEval 2007, there were three different tasks regarding temporal relations classification between A) events and times within the same sentence; B) creation time of a document and its events; and C) main (verb) events in adjacent sentences.\nIn TempEval 2010, there were six different tasks including A, B) Determining the time expressions and events of input texts and specified features; and temporal relation classification between C) events and times within the same sentence; D) creation time of a document and its events; E) main events in consecutive sentences; and F) two events where one event syntactically dominates the other event.\nDue to focusing on temporal relations between event pairs, task C of TempEval 2007 plus tasks E and F of TempEval 2010 are similar to the task that we tackle in this paper; however, these tasks can be considered as special cases of ours. For instance, in task E of TempEval 2010, only the event pairs from consecutive sentences are considered; whereas, in our task the event pairs can be either from the same sentence or from any other two sentences of the input text.\nThe research on temporal relation learning can be divided into different categories. In this paper, we divide these efforts into three groups: 1) Statistical; 2) Rule-based, and 3)\nHybrid, which are explained in the following sections.\nconvince (event2) the Conservative government to keep (event3) its so-called golden share, which limits any individual holding to 15%, until the restriction (event4) expires (event5) on Dec. 31, 1990 (time1)\u201d. Bold arrows show relations between event pairs."}, {"heading": "2.1 Statistical Methods", "text": "In all statistical methods, a classification (or clustering) algorithm is employed over a number of tagged and/or extracted features of an input corpus. Maximum Entropy (Mani, Wellner, Verhagen, & Pustejovsky, 2007; Derczynski & Gaizauskas, 2010), Support Vector Machines (Chambers et al., 2007; Bethard & Martin, 2007; Hepple, Setzer, & Gaizauskas, 2007; Cheng, Asahara, & Matsumoto, 2007; Mirroshandel, Ghassem-Sani, & Khayyamian, 2009a, 2009b, 2011), Conditional Random Fields (Llorens, Saquete, & Navarro, 2010; Kolya, Ekbal, & Bandyopadhyay, 2010), and Markov Logic Networks (UzZaman & Allen, 2010; Ha, Baikadi, Licata, & Lester, 2010) are some of the statistical techniques that have been applied to this problem.\nMaxEnt is one of the first approaches to the temporal relation learning, which uses maximum entropy classification algorithm (Mani et al., 2007). In this method, the classifier assigns one of six different temporal relation types to each event-event or event-time pair. The classifier relies on a number of features including modality, polarity, tense, aspect, and the event class, which have been hand-tagged in the corpus. In addition to these features, it also relies on pairwise agreement of two additional features: tense and aspect. We later propose a new technique to improve MaxEnt. The results of comparing the proposed method and MaxEnt are given in section 4.\nUSFD2 (Derczynski & Gaizauskas, 2010) is another method that employs maximum entropy in solving tasks C and F of TempEval 2010. This method uses the same features as MaxEnt plus some features related to the so-called signals of the text. USFD2 achieved the second highest score in task C of TempEval 2010. However, its results on task F were not satisfactory enough.\nThe state-of-the-art of the statistical methods is analogous to MaxEnt (Chambers et al., 2007). It works in two consecutive stages and employs some event-event features in addition to those used by MaxEnt. In this work, Support Vector Machines (SVM) are used for classification. Similar results were reported for using a Naive Bayes classier instead of SVM. Section 4 also includes the results of comparing this work with our proposed algorithm.\nSVMs have been also used as a classification algorithm in several other research. CUTMP (Bethard & Martin, 2007) applied SVM for solving all three tasks of TempEval 2007. It also used gold-standard TimeBank features for event and time expressions plus parts of derived parse trees from the input text. CU-TMP first solves task B and then uses its results to tackle tasks A and C.\nUSFD (Hepple et al., 2007) and NAIST-Japan (Cheng et al., 2007) were two other participants of TempEval 2007 that used SVM for classification. In NAIST-Japan, the task was defined as a sequence labeling model. The task is approached by using HMMSVM, relying on features from dependency-parsed trees and standard attributes of target events/time expressions. The result of this system was slightly more than average in tasks A and B, but less than average in task C. It was shown that extracted features from dependency parsed trees were not so effective for task C, in contrast with tasks A and B. In USFD, temporal relation learning is treated as a simple classification task (Hepple et al., 2007). They used different classification algorithm from WEKA machine learning workbench (Hall et al., 2009). In task C, SVMs have gained the best result among the participants of the shared task.\nIn another work, a corpus of parallel temporal and causal relations was employed, and SVMs were used to extract both types of relations (Bethard & Martin, 2008). Since existing corpora provide no parallel temporal or causal annotations, 1000 conjoined event pairs were annotated (Bethard & Martin, 2008; Bethard, 2007). It was shown that causal relation information could be helpful in temporal relation extraction, too. It was also shown that temporal relation information has mutually positive effects in causal relation extraction (Bethard & Martin, 2008).\nBethard and his colleagues (2007a, 2007b) have applied SVM to classify event pairs in which the first event is a verb and the second one is the head of a clausal argument of that verb. They have used a combination of a number of event based features (e.g., tense and aspect) and some syntactic features (e.g., a specific path through the parse tree). Their reported results have shown a high accuracy for these specific event pairs.\nThere are also other algorithms which utilize grammatical information in SVM using convolution tree kernels (Mirroshandel et al., 2009a, 2009b, 2011). It was shown that grammatical aspects of the input text are rich sources of information for temporal relation classification. Argument Ancestor Path Distance (AAPD) convolution tree kernel is the most successful tree kernel that has been used in SVM classification. This kernel is similar to the CollinsDuffy tree kernel (Collins & Duffy, 2001). The CollinsDuffy kernel effectively counts the number of common subtrees between two comparing parse trees. In this kernel,\nall subtrees have the same importance, whereas in AAPD, different weighting functions are used to compute the kernel value. Furthermore, in AAPD, the significance of subtrees are measured using the distance from a so-called argument ancestor path (AAP). An AAP is the ancestor nodes of an argument (event). An example of a node (NN) and the distance between the node and an AAP is shown in Figure 3. In AAPD, the closer a node is to the path, the less it is decayed by the weighting function. In other words, the nodes which are located nearer to the path are more important than those farther away.\nTo improve the accuracy of AAPD, it was combined with some other kernels, which were either linear or polynomial (Zhang, Zhang, Su, & Zhou, 2006). However, polynomial composite kernels have shown superior results (Mirroshandel et al., 2009b). Section 4 includes the results of comparing AAPD and AAPD Polynomial kernels.\nMarkov Logic Networks (MLN) is another classification algorithm, which have been used by two participants of TempEval 2010: TRIPS & TRIOS (UzZaman & Allen, 2010) and NCSU (Ha et al., 2010). TRIPS and TRIOS use a number of features produced by a deep semantic parser, plus a few features extracted from target pairs (i.e., event/time expression). In contrast with other participants, TRIPS and TRIOS operate on raw texts. In other words, these systems do not use any tagged events/time expressions. They both outperformed all other teams on two tasks (C and E). TRIOS also gained the second best results on the four remaining tasks. NCSU is another participant of TempEval 2010 that uses MLN for classification. It relies on basic annotated features, syntactic features extracted from generated parse trees, and lexical semantic features from two external resources (Ver-\nbOcean and WordNet) (Ha et al., 2010). NCSU was applied to tasks C, D, E, and F in two different settings: NCSU-indi and NCSU-joint. In NCSU-indi, an independent MLN was trained for each task. On the other hand, a set of global formulae was also added to NCSU-joint to ensure the consistency among classification decisions from four local MLNs (one for each task). NCSU-indi achieved the best result in task F and the second best result on task C.\nOne of the most successful participants of TempEval 2010 was TIPSem that is based on Conditional Random Field (CRF) models for classification purpose (Llorens et al., 2010). TIPSem employs different morphological, syntactic, and semantic features for building CRF models. In Spanish, it achieved the best results in all tasks. In English, TIPSem achieved the best results in Tasks B and D; and was one of the best systems in all other tasks. JU CSE TEMP was another participant of TempEval 2010 that utilized CRF models for temporal relation learning tasks (Kolya et al., 2010). The system needs only the goldstandard features of TimeBank for time expressions and/or events. In comparison with TIPSem, JU CSE TEMP achieved weaker results, which shows the importance of feature engineering in temporal relation learning.\nThere is another approach that applies different machine learning techniques to detect intra-sentential events, and builds a corpus of sentences with two or more events in which at least one event is triggered by a key time word (e.g., after, before, etc.). The classifier is based on a number of syntactic and clausal ordering features (Lapata & Lascarides, 2006; Bramsen, Deshpande, Lee, & Barzilay, 2006).\nThere exist a comprehensive study about statistical methods, which compares three different interval based algebras in terms of classification accuracy, performance, and expressiveness power (Denis & Muller, 2010). There are also a few algorithms that exclusively work on temporal relation classification between events and time expressions. One of such algorithms employs cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data (Boguraev & Ando, 2005).\nThere is a group of statistical methods that rely on information of argument fillers (called anchors) of every event expression as a valuable clue for recognizing temporal relations. In these methods, by looking at a set of event expressions whose argument fillers have a similar distribution, analogous event expressions are recognized. Algorithms such as DIRT (Lin & Pantel, 2001), TE/ASE (Szpektor, Tanev, Dagan, & Coppola, 2004), and that of the Pekar\u2019s system (2006) are examples of this type of statistical method.\nDIRT is an unsupervised method based on an extended version of the so-called distributional hypothesis (Lin & Pantel, 2001). According to this hypothesis, words that occur in the same contexts are usually similar. Here, instead of words, the algorithm applies the distributional hypothesis to certain paths of the dependency trees of a parsed corpus.\nTE/ASE, too, is an unsupervised algorithm, which has two major phases. In the first phase (called Anchor Set Extraction), the algorithm extracts similar anchors. Then, in the second phase (called Template Extraction), the system extracts templates from the resulting anchor sets. In the final part of the algorithm, some post-processing transformations are applied to the extracted templates to remove inappropriate templates (Szpektor et al., 2004).\nIn Pekar\u2019s approach (2006), co-occurrence of two verbs inside a locally coherent text is used to extract some useful information. This method has three major steps. First, based on the local discourse, it identifies several pairs of clauses as being related. Next, based on those related clauses, it tries to create a number of templates of verb pairs by using information such as syntactic behavior. In the last step, the algorithm scores and employs those templates for relation extraction."}, {"heading": "2.2 Rule-Based Methods", "text": "The common idea behind rule-based methods is to find some general patterns for classifying temporal relations. In most of these works, rules (patterns) are manually defined.\nPerhaps the simplest rule-based method is the one that was developed using a knowledge resource called VerbOcean (Chklovski & Pantel, 2005). VerbOcean has a small number of manually designed generic rules. The style of rules is in the form of \u201c* <Verb-X> * <VerbY> *\u201d. For example, there are rules such as \u201cto Verb-X and then Verb-Y\u201d, \u201cto Verb-X and eventually Verb-Y\u201d, or \u201cto Verb-X and later Verb-Y\u201d for the \u201chappens-before\u201d relation type; and also there are rules such as \u201cVerb-X even Verb-Y\u201d or \u201cVerb-Y or at least Verb-X\u201d for the so-called \u201cstrength\u201d relation type. After manually creating these rules, a number of semantic relations (e.g., strength, antonymy, happens-before, etc.) between events can be detected. Several heuristics were also employed to filter inappropriate relations (Chklovski & Pantel, 2005).\nThere is another rule-based method for temporal relation learning focused on biomedical texts (Mulkar-Mehta, Hobbs, Liu, & Zhou, 2009). It was shown that existing methods for temporal relation learning were not effective for such texts. In this work, some specific axioms (rules) were used to predict the temporal and causal relations. A pattern extraction algorithm was employed to create the system rules in a semi-automatic manner.\nXRCE-T, a participant of TempEval 2007, is a rule-based system that relies on syntactic and semantic features (e.g., deep syntactic analysis and determination of thematic roles) (Hage\u0300ge & Tannier, 2007). XRCE-T was in fact used as a post-processing module of a general purpose linguistic analyzer.\nIn another study, rules of temporal transitivity were used to increase the training set. The test accuracy on this enlarged corpus showed some improvements (Mani et al., 2007).\nReasoning with pre-determined rules is another approach to the rules\u2019 usage. In the work of Tatu and Srikanth (2008), a rich set of axioms (rules) was created and used by a first order logic based theorem prover to find a proof for each temporal relation by refutation.\nA set of discourse rules was used in the algorithm of Muller and Tannier (2004) to establish the possible relations between every two consecutive events of the input text. These rules were based on tenses of the event verbs. Then a classical path-consistency algorithm (Allen, 1984) was applied to the extracted relations of the first step."}, {"heading": "2.3 Hybrid Methods", "text": "It has been shown that one can increase the accuracy of temporal relation classifiers by merging some of discussed methods. For example in the work of Chambers and Jurafsky (2008), local decisions generated by a statistical method were combined with two types of implicit global rule-based properties. These properties included the transitivity rule (e.g., A\nbefore B and B before C implies A before C), and time expressions normalization (e.g., last month is before yesterday). The constraints were used to create a more densely-connected network of events, and then a global state of consistency was enforced by incorporating the constraints into an integer linear programming framework (Chambers & Jurafsky, 2008).\nInteger linear programming with local classifiers was shown to be appropriate only for cases in which the number of possible relations between events is restricted (Denis & Muller, 2011). It was suggested that a translation of constraints from temporal intervals to their endpoints can be used to handle a significantly smaller set of constraints. During translation, temporal relations are preserved. This method was shown to have a rather high accuracy. They also proposed a graph decomposition technique that can further improve the accuracy.\nIn another algorithm, which was spiritually similar to that of (Chambers & Jurafsky, 2008), instead of applying global constraints using integer linear programming, the so-called Markov logic (ML) was used (Yoshikawa, Riedel, Asahara, & Matsumoto, 2009). Global constraints can be easily captured through adding some weighted first order logic formulas. It was shown that the problem can be solved by ML more easily and accurately than by ILP.\nWVALI is another hybrid system, which has an enhanced classification process by using some rules from a particular knowledge base (Puscasu, 2007). In this system, different heuristics and temporal reasoning mechanism have been combined with statistical data extracted from the training corpus. WVALI achieved the best results in all tasks of TempEval 2007.\nLCC-TE was another hybrid system of TempEval 2007. It combined different machine learning models with human rules for temporal relation learning (Min, Srikanth, & Fowler, 2007). LCC-TE uses gold-standard features available in TimeBank, as well as a number of derived and extended features such as grammatical and semantic features. The evaluations on LCC-TE have shown acceptable results in all three tasks."}, {"heading": "3. Bootstrapped Cross-Document Classification (BCDC)", "text": "In this section, a new method of extracting temporal relations between events is introduced. We call this method Bootstrapped Cross-Document Classification (BCDC). The results of experiments with BCDC show a significant improvement over previous work in terms of accuracy (see Tables 6 and 7). We have used SVM with three different kernels in the learning process. There are two novelties in our bootstrapping (self-training) method: 1) it is an information retrieval based approach that extracts useful information exclusively from related documents. 2) It builds a specific model for each test document. Before describing BCDC, our motivation is briefly explained in the next section."}, {"heading": "3.1 Motivation", "text": "In a regular corpus with heterogeneous documents, verbs, which often act as event triggers, may have different senses in different documents. For example, event \u201cfiring\u201d may have a sense of \u201cshooting\u201d a gun in a document about army, whereas it may also have a sense of \u201cending\u201d someone\u2019s job in a different document about a company. However, for a cluster of topically-related documents, the distribution should be much less divergent. This motivated us to apply the so-called \u201cone sense per discourse\u201d hypothesis (Yarowsky, 1995) to the\nproblem of temporal relation classification, and extend the scope of discourse from a single document to a cluster of topically related documents. Also inspired by another work that proposed assumptions of one event trigger sense and one event argument role per discourse (Ji & Grishman, 2008), we based our work on an analogous assumption, which we called \u201cone type of temporal relation per discourse\u201d. In other words, we assume that similar event pairs in different places of topically related documents are very likely to have the same temporal relations. Although, as it is later explained, we have not explicitly employed this assumption in our proposed algorithm, we have tried to verify the assumption by considering temporal relations of the Opinion corpus (Mani et al., 2006). In this corpus, documents are located in four different directories each having a specific topic. In our verification, we have considered all documents within the same directory as being related. In other words, two documents are considered as \u201crelated documents\u201d if they are in the same directory (i.e., they have the same topic). To verify the assumption, we selected those event pairs that have appeared more than once. In Opinion corpus, there are a total number of 2666 temporally related event pairs (i.e., TLinks), out of which, only 994 pairs appeared more than once1. Table 1 shows the results of our verification. Supporting samples are those event pairs that have appeared in two or more related documents with exact the same temporal relation. Even if event pairs having different relations are from unrelated documents, they are also regarded as the supporting samples. On the contrary, if event pairs having different relations are from related documents, they are considered as contradictory samples. As it is shown in Table 1, more than 95% of the samples have supported our assumption (i.e., \u201cone temporal relation per discourse\u201d).\nAs an example, in the following sentences, which have been taken from different documents of the same topic (i.e., Kenya Tanzania Embassy bombings), the event pair (blast and kill) has IBEFORE temporal relation in all sentences:\n\u201cReports reaching here said a massive blast damaged the U.S. embassy in Nairobi , killing 40 people while wounding at least 1,000 people.\u201d\n\u201cMore than 100 people have been killed and more than 1,000 others wounded in the blasts next to the U.S. embassies in Kenya and Tanzania on Friday.\u201d\n\u201cIn Dar es Salaam , she laid a wreath next to the crater left by the embassy blast that killed 10 people.\u201d\n1. Before counting the number of event pairs, we applied a lemmatizer to event words."}, {"heading": "3.2 Feature Engineering", "text": "In BCDC, two types of features are used: basic and extra event-event features. Basic features are simple features related to individual events and extra event-event features are those extracted from two related events. In the next two sections, these features are explained in more detail."}, {"heading": "3.2.1 Basic Features", "text": "These are simple features extracted from events. For each event, there are five temporal attributes, which are tagged in standard corpora: 1) tense; 2) grammatical aspect; 3) modality; 4) polarity, and 5) event class. Tense and aspect define temporal location and event structure; thus, they are necessary in any method of temporal relation extraction. Modality and polarity specify non-occurring or hypothetical situations. The event class shows the type of event. The range of values for these attributes is based on the work of Pustejovsky et al. (2003), and is shown in Table 2. These attributes are either annotated in the input corpus or can be automatically extracted by existing tools.\nIn addition to the five mentioned attributes, BCDC also employs the string of words that constitute each event, their part of speech tags as well as a number of contextual features including pairwise agreement of tenses and aspects. Part of speech tags of events are again either annotated in the corpora or can be determined by existing POS taggers. For example, in sentence \u201cHe succeeds James A. Taylor, who ...\u201d, \u201csucceeds\u201d is an event with the following features:\n[tense: present], [aspect: none], [modality: none], [polarity: positive], [event class: aspectual], [word: succeeds], [pos: verb]"}, {"heading": "3.2.2 Extra Event-Event Features", "text": "Extra event-event features are based on two related events and are automatically extracted from the input text. In our case, there are three types of these features, defined as follows:\nEvent-Event parse tree: if both events are in the same sentence, the algorithm can use the parse tree of the sentence to learn some useful syntactic properties such as domination. In a parse tree, event A dominates event B, if A is an ancestor of B. These properties are not explicit features, but rather implicit properties which can be extracted and learned by the SVM using appropriate tree kernels such as those proposed in the work of Mirroshan-\ndel et al. (2009b). Parse trees can be extracted by a statistical parser and there is no need for any Treebank.\nPrepositional phrase: a preposition head is often an indicator of a temporal class. Thus, we can use a new feature that indicates if an event is a part of a prepositional phrase. This information can also be extracted from the parse trees. For example, in sentence \u201cI saw him before the earthquake\u201d, the relation between events \u201csaw\u201d and \u201cearthquake\u201d can be easily determined by the word \u201cbefore\u201d in the prepositional phrase \u201cbefore the earthquake\u201d.\nEvent-Event distance: it is based on the idea that the strength of the relationship between two events is inversely related to the textual distance of those events. It means that the relationship becomes weaker as the distance increases (and vice versa). Accordingly, intra- and inter-sentential events should be treated differently. We train two separate models: one for the intra-sentential events and one for the inter-sentential ones."}, {"heading": "3.3 Proposed Algorithm", "text": "BCDC applies a novel usage of bootstrapping to the classification of temporal relations between events. It works in two main stages. In the first stage, using a standard corpus, a general model is learned. Then in stage two, the general model is retrained for each test document based on some related information. Figure 4 shows the flowchart of the proposed algorithm, which is described in more detail in the following sections."}, {"heading": "3.3.1 Stage One", "text": "In stage one, BCDC employs the discussed features extracted from a standard corpus to train a general model for classification using SVM. At the end of this stage, we will have a model for temporal relation classification. However, such models, which have also been proposed before by other researchers, all have the problem of being too general. In other words, such a model does not have any specific information about the particular domain under consideration. To better deal with this problem, BCDC has an extra bootstrapping phase of training."}, {"heading": "3.3.2 Stage Two", "text": "In stage two, we retrain the general model produced in stage one, for each test document with some related information. In order to achieve this goal, the bootstrapping phase of BCDC proceeds according to the following steps:\nStep 1: first of all, an unprocessed test document is randomly selected.\nStep 2: then BCDC finds the top N documents that are topically related to the selected test document from a large unannotated corpus. The choice of related documents can be made by the INDRI retrieval system (Strohman, Metzler, Turtle, & Croft, 2005). Note that the mentioned large unannotated corpus is different from the training and test corpora.\nStep 3: in this step, we extract events and required features from the related documents found by INDRI. The events and specified features of section 3.2 can be automatically annotated by EVITA (Saur\u0301\u0131, Knippen, Verhagen, & Pustejovsky, 2005). Although some of the events and/or features extracted by EVITA may be incorrect, our experimental results show that they can still be very helpful. The extra event-event features and other required features can be extracted by a POS tagger and a statistical parser.\nStep 4: then by using the existing model, the temporal relations between only intrasentential event pairs of the related documents are predicted. Besides, a normalized measure of confidence is computed for each relation. We have used SVM for our classification purpose. Therefore, we have designed a confidence measure using SVM, as it is explained below.\nIn SVM binary classification, positive and negative instances are linearly partitioned by a hyper-plane (with maximum marginal distance to instances) in the original or a higher dimensional feature space. In order to classify a new instance X, its distance to the hyperplane is computed and X is assigned to the class that corresponds to the sign of the computed distance. The distance between instance X and hyper-plane H, which can be either a positive or a negative value, is supported by the support vectors X1 . . . Xl and computed by equation 1 (Han & Kambert, 2006):\nd(X,H) = l\u2211\ni=1\nyi \u03b1i Xi X T + b0 (1)\nwhere yi is the class label of support vector Xk; \u03b1k and b0 are numeric parameters that are automatically determined.\nWe have used one-versus-one case of multi-class classification with m classes, in which a set of m \u2217 (m\u2212 1) / 2 hyper-planes (i.e., one hyper-plane for every class pair) denoted by H is defined. The hyper-plane that separates class i and j is referred to as Hi,j . Hi is used to denote a subset of m \u2212 1 hyper-planes of H that separates class i from the others. In order to classify a new instance X, its distance to each hyper-plane Hi,j is computed. Then X is assigned to class i or j. At the end of this process, for every instance X, each class i has accumulated a certain number of votes, represented as Vi(X), which is the number of times that the classifier has assigned instance X to class i. The final class of X, denoted by C(X), will be the one with the highest number of votes.\nIn the process described above, it is easy to compute the confidence values based on the distance measures of equation 1 (i.e., the closer a case is to the support vectors, the less it is confident). More precisely, in the multi-class classification, we define the confidence of instance X as the sum of its distances to all its class-separating hyper-planes:\n\u03d5(X) = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nH\u2208HC(x)\nd(X,H) \u2223\u2223\u2223\u2223\u2223\u2223 (2) Based on equation 2, the larger value of \u03d5(X) shows that X is more confident, and vice\nversa.\nStep 5: in this step, BCDC chooses the K most confident temporal relations of those detected in step 4.\nStep 6: we then retrain the SVM by injecting the temporal relations selected in step 5. It should be noted that for each test document, the original model is retrained by the most confident relations from the documents related to only that test document and not any other test documents.\nthe model is trained on only the original training data plus the most confident predicted relations from the \u201crelevant\u201d documents for the current test document and not any of predicted relations for other test documents.\nSteps 4-6 are repeated until one of the following two termination conditions will be satisfied: 1) there will be no more unselected temporal relation, or 2) a predefined number of iterations will be reached.\nStep 7: when the retraining phase of the general model for a selected test document is finished, the temporal relations of the test document are classified based on the new specifically retrained model.\nThen, if there are still some unprocessed test documents, BCDC will start from step 1 again; otherwise the algorithm will terminate.\nThe fundamental idea of the second stage of BCDC is to obtain some document- and cluster-wide statistics about the temporal relations between different types of events, and then using this information to improve temporal relation identification.\nAs it was explained above, a specific model is learned for each test document, using a number of unannotated text documents which are topically related to that test document (i.e., bootstrapping phase). However, if some test documents are themselves topically related, their corresponding retrained models will be very similar. For the sake of efficiency, we can run the bootstrapping phase for just one of such test documents, and then use the same retrained model for the rest. In other words, we run steps 2 to 6 of BCDC just for one member of a set of similar test documents, and for other members, we solely apply step 7.\nAs it was explained in the section 3.1, we do not explicitly use the assumption of \u201cone type of temporal relation per discourse\u201d in any part of BCDC. However, in bootstrapping, we somehow implicitly benefit from this assumption by seeking only topically related documents, which are more likely to include similar event pairs with identical temporal relations."}, {"heading": "4. Experimental Results of BCDC", "text": "In this section, the specification of the employed corpora is briefly explained. Then, the accuracy of BCDC is analyzed."}, {"heading": "4.1 Characteristic of Corpora", "text": "We have used two standard corpora (i.e., TimeBank (v 1.2) and Opinion, see Mani et al., 2006) in our experiments. TimeBank has 183 newswire documents with 64, 077 tokens, and\nOpinion has 73 documents with 38, 709 tokens. These two datasets have been annotated based on the TimeML standard (Pustejovsky et al., 2003). As mentioned before, there are fourteen temporal relation types (SIMULTANEOUS, IDENTITY, BEFORE, AFTER, IBEFORE, IAFTER, INCLUDES, IS INCLUDED, DURING, DURING INV, BEGINS, BEGUN BY, ENDS, ENDED BY) in the TLink class of TimeML. For the sake of reducing the data sparseness problem, as many others (Mani et al., 2006; Tatu & Srikanth, 2008; Mani et al., 2007; Chambers et al., 2007), we have used a normalized version of these relation types including only six following relations:\nSIMULTANEOUS ENDS BEGINS BEFORE IBEFORE INCLUDES\nFor normalizing, the inverse relations are merged. These conversions are shown in the Table 3. In the first six conversions, relations can be easily converted by swapping their arguments. Relations IDENTITY and SIMULTAENOUS are collapsed, since IDENTITY is a subtype of SIMULTANEOUS (i.e., two events are IDENTITY if they are SIMULTANEOUS and coreferential). Similarly, relations DURING INV and INCLUDES are also collapsed because DURING INV is a subtype of INCLUDES (i.e., identical to the Allen\u2019s CONTAINS) based on the Allen\u2019s interval algebra (Allen, 1984). It should be clear that by using these conversions, no information is lost.\nIn our experiments, like some previous work (Mani et al., 2006; Chambers et al., 2007; Chambers & Jurafsky, 2008), TimeBank and Opinion corpora have been merged into a single corpus called Opinion TimeBank Corpus (OTC). Table 4 shows the normalized TLink class distribution (only for Event-Event relations) over TimeBank and OTC. As it is shown, relation \u201cBEFORE\u201d is the most frequent relation; thus it forms the majority class, and can be used as a baseline of the experiments.\nFor comparison with some other methods, we also used the English part of the TempEval2 corpus. This part is based on TimeBank (Verhagen et al., 2010; Pustejovsky et al., 2003; Boguraev, Pustejovsky, Ando, & Verhagen, 2007). However, all the TimeBank annotations have been reviewed based on the guidelines of TempEval 2010 and the temporal relations have been modified according to the specific types of the shared task.\nThere are two parts in this corpus: 1) the training part including 163 documents and 53, 450 tokens; and 2) the test part with 21 documents and 4, 848 tokens. There are six different temporal relation types: BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER, and VAGUE. Among six different tasks of TempEval 2010, we just focused on tasks E and F, which are similar to the problem that we have tackled in this paper. Tasks E and F are the only tasks which consider exclusively the relations between two events. The distribution of temporal relation types for these tasks over the training and test parts of the corpus is shown in Table 5. The majority classes have been underlined in the table.\nAs it was discussed in section 3.3.2, for each text, we retrieve a number of topically related texts using a public domain software called INDRI. In our experiments, these related texts have been retrieved from the English part of TDT5 multilingual news text corpus2. In total, TDT5 consists of 407, 505 text documents in English (278, 109 documents), Mandarin Chinese (56, 486 documents), and modern standard Arabic (72, 910 documents). It also has 250 different topics. Unlike previous TDT corpora, TDT5 does not contain any broadcast news data; all sources are newswires.\n2. TDT 2004: Annotation Manual, Available at http://www.ldc.upenn.edu/Projects/TDT2004."}, {"heading": "4.2 Experiments", "text": "We have used the LIBSVM java source for the SVM classification (Chang & Lin, 2011). The EVITA system (Saur\u0301\u0131 et al., 2005) has been used for event extraction. EVITA works based on both linguistic and statistical information. In addition to event extraction, event attributes (which were described in Table 2) can also be extracted by EVITA. We have also used the Stanford NLP package3 for tokenization, sentence segmentation, part of speech tagging, and parsing. The INDRI retrieval system (Strohman et al., 2005) has been employed to obtain related documents. INDRI is a language model based search engine that provides a state-of-the-art text search engine. The English part of TDT5 has been indexed by INDRI, and by using this search engine, the texts that are highly related to some specified documents can be retrieved.\nAs it was mentioned earlier, we applied our algorithm to TimeBank, OTC, and TempEval 2010 Corpora. We randomly selected 20 documents (almost 10 percent of total documents) of TimeBank as our development set. Based on several experiments on this development set and with different number of extracted related documents in step 2 of BCDC (i.e., N), and number of most confident relations chosen in step 5 (i.e., K), we have set N to 25 and K to 40.\nOn TimeBank and OTC, the results were evaluated by first excluding the 20 documents of the development set and then measuring accuracy using the five-fold cross validation method. However, for the corpus of TempEval-2, there was no need for cross validation, because the training and test sets are predetermined, and we just reported the accuracy of BCDC on the test set.\nTable 6 shows the results of three different settings of the proposed algorithm against several others over TimeBank and OTC. In this table, the baseline is the majority class for event-event relations (i.e., the BEFORE relation) of the evaluated corpora. Mani\u2019s method is regarded as a successful statistical approach to temporal relation identification, which exclusively uses gold standard features of events (Mani et al., 2007). Methods proposed by Chambers and Mani are similar except that Chambers has also used a number of extra features in a two step algorithm. His method is currently regarded as the state-of-the-art of statistical approaches over TimeBank and OTC. To achieve a higher accuracy, he has also used some extra resources such as WordNet (Chambers et al., 2007).\nArgument ancestor path distance (AAPD) is an accurate convolution tree kernel which only uses parse trees of event-event sentences for temporal relation classification (Mirroshandel et al., 2009b). AAPD polynomial is a composite kernel that combines a simple event kernel and AAPD (Mirroshandel et al., 2009b). The mentioned simple event kernel is a linear kernel that exclusively uses the same features as that of Mani\u2019s method (Mirroshandel et al., 2009b). AAPD and AAPD polynomial kernels were designed to be applied only to the event pairs that are within the same sentence. Accordingly, the relations of TimeBank and OTC were split into two parts: 1) relations between intra-sentential event pairs, and 2) relations between inter-sentential event pairs. Then these kernels were applied only to the first part and for the second part, we just used simple event kernel (i.e., Mani\u2019s kernel). In Table 6, the results reported for AAPD and AAPD polynomial kernel are in fact the outcome of merging the partial results from these two parts.\n3. Available at http://nlp.stanford.edu/software/index.shtml\n\u201cBCDC + Event Kernel + Basic Features\u201d is our bootstrapped algorithm, which only uses basic features, mentioned in section 3.2.1, by applying a simple event kernel (Mirroshandel et al., 2011). In \u201cBCDC + AAPD Kernel + Extra Event-Event Features\u201d, we utilized extra event-event features in AAPD kernel. Third setting (\u201cBCDC + AAPD Polynomial Kernel + Basic Features +Extra Event-Event Features\u201d) uses AAPD Polynomial kernel to combine all basic and extra event-event features.\nFor better comparison, we also applied a classic bootstrapping method with the same SVM kernels and features as that of BCDC. For reporting these results, the initial model was trained on a standard corpus (i.e., like stage one of BCDC). Then, in an iterative manner, most confident samples of all documents (rather than just related documents) were used to retrain the model. Note that in this case, there is no need to the process of retrieving related documents, and only one model is learned for all test documents. In order to find the best value for K (i.e., number of most confident samples) in the classic bootstrapping method, we performed several different experiments on the mentioned development set. Incidentally, our experiments showed that here, too, K should be set to 40.\nAs Table 6 indicates, \u201cBCDC + AAPD Kernel + Extra Event-Event Features\u201d and \u201cBCDC + AAPD Polynomial Kernel + Basic Features + Extra Event-Event Features\u201d both show a significant improvement over the state-of-the-art method (i.e., Chambers\u2019 method). Comparison between BCDC and classical bootstrapping shows the effectiveness of the proposed idea of extracting the retraining samples only from related documents.\nThe improvement over TimeBank is more considerable than that of OTC. It seems that different distributions of temporal relations in the two corpora has caused the difference\nbetween these improvements. As it is shown in Table 4, in OTC, the majority class (i.e., BEFORE relation) has a larger part of the whole corpus. This causes the learning algorithm to become biased towards the BEFORE relation, and thus the correct prediction of other relations becomes harder. On the contrary, in TimeBank, the distribution is less biased and thus BCDC has shown more improvement on this corpus.\nFor testing statistical significance, we applied a type of \u201cstratified shuffling\u201d, which is a kind of \u201ccompute-intensive randomized test\u201d. The null hypothesis (i.e., the two models that produced the observed results are the same) was tested by randomly shuffling the generated output for each event pair between the two models and then re-computing the evaluation metrics (i.e., accuracy in this case). If the difference in a particular metric after a shuffling is equal to or greater than the original observed difference in that metric, then a counter (nc) for that metric is incremented. Ideally, we should perform all 2n possible shuffles, where n shows the number of test cases (i.e., event pairs). But, in our case, this is impractical because n is a rather large number. Therefore, as many others, we have tried only 10, 000\niterations (nt). After finishing all iterations, the p-value (likelihood of incorrectly rejecting the null hypothesis) is simply calculated by (nc+1)/(nt+1) (Yeh, 2000). Table 7 shows the result of the significance test on our proposed methods. In this test, each proposed method was compared with its relevant method.\nAs it is shown in Table 7, majority of the methods passed the test (the p-value is less than 0.05). There are only four exceptions in which the p-value is slightly greater than 0.05.\nTable 8 shows the accuracy of BCDC on the English part of the corpus used in TempEval 2010 for tasks E and F. JU-CSE, NCSU-indi, NCSU-joint, TIPSem, TIPSem-B, TRIOS, and TRIPS are participants of the TempEval 2010 shared task (Verhagen et al., 2010). The other methods are the same as in Table 6. AAPD kernel can only be applied to event pairs within the same sentence. Therefore, we are unable to apply it to task E. For inter-sentential event pairs, AAPD Polynomial kernel is almost similar to simple event kernel, because it cannot use the syntactic parse trees, which are appropriate sources of information.\nAs it can be seen in Table 8, although BCDC has shown some improvement in the accuracy of temporal relation identification (i.e., in comparison with our base methods), it is generally weaker than almost all the participants of TempEval 2010. We think this weakness is due to the restricted feature set that have been used in BCDC. In other words, majority of participants of TempEval 2010 have used richer feature sets of different levels (e.g., lexical, syntactic, and semantic), while we have just used simple event features (plus a few syntactic features only for task F). We think but have not verified yet that with a richer set of features, BCDC will produce more successful results on TempEval\u2019s tasks, too. Besides, we think replacing our base method for a more successful method such as TipSem\nor TRIPS, can make BCDC competitive with participants of TempEval 2010. However, to show this, we first need to find an appropriate confidence measure for step 4 of BCDC4, which requires further investigation and is one of our directions in future research."}, {"heading": "4.3 Analysis", "text": "As it can be seen in Tables 6 and 8, BCDC has shown a substantial improvement over several different methods in terms of accuracy and without using any extra annotated data. Bootstrapping by using a number of related documents have the following positive effects:\n1) By knowing the relation between events, we can better predict the relation types between analogous events, which may appear in related documents.\n2) In related documents, the number of sentences with similar patterns will increase, and the tree kernels can extract more confident information from the parse trees. Thus in\n4. It should be noted that our proposed confidence measure is just useful for SVM classification technique.\nthis way, SVM can be more informative.\n3) The used corpora are rather small with few examples for each relation. This data sparseness problem can affect the performance of any temporal relation identification method. In BCDC, retrieving related documents and extraction of new temporal relations of these documents can increase the number of relations and improve its performance by alleviating the data sparseness problem.\nOne remaining question is \u201cwhat is the impact of choosing related documents?\u201d. In other words, what if we randomly choose a number of unrelated documents in the bootstrapping phase of BCDC. To show the effectiveness of the idea of using related documents, we have repeated our experiments with N = 25 (i.e., the same as original BCDC) randomly selected documents. The results of these experiments are shown in Figure 5. As it is shown, although randomly selected documents have slightly improved the base methods, however, the improvement is not comparable with that of using related documents."}, {"heading": "5. Using EM for Temporal Relation Learning (EMTRL)", "text": "Since supervised and even semi-supervised methods need annotated corpora, which for many languages and/or domains do not exist, here, we propose an unsupervised algorithm for the temporal relation learning problem. Due to the encouraging results of the expectation maximization (EM) algorithm in other unsupervised tasks of natural language processing such as unsupervised grammar induction (Klein, 2005), unsupervised anaphora resolution (Cherry & Bergsma, 2005; Charniak & Elsner, 2009), and unsupervised coreference resolution (Ng, 2008), we decided to evaluate EM in unsupervised temporal relation extraction. Currently, there is no reported work in temporal relation extraction based on EM. In fact, there has not yet been any attempt towards an unsupervised approach to temporal relation extraction. Here, we explain how EM can be successfully applied to the task of temporal relation extraction and show that the performance of EM is encouraging in this task. Before that, we first introduce the definitions and notations that will be later used in subsequent sections."}, {"heading": "5.1 The EM Algorithm", "text": "EM is a general algorithm for maximum likelihood estimation (MLE) (Dempster, Laird, & Rubin, 1977). This algorithm can be used when we deal with incomplete information. As it was mentioned before, in temporal relation learning, the task is to determine the type of temporal relation r that is between two events e1 and e2. In this algorithm, context means the sentence (or sentences) containing a pair of events."}, {"heading": "5.2 The Proposed Model", "text": "Let us call the new proposed algorithm EMTRL, which stands for EM based temporal relation learning. EMTRL operates at the corpus level, inducing valid temporal clustering for all event pairs of a given corpus. More specifically, EMTRL induces a probability distribution to maximize P (corpus) (the probability of the corpus). To easily incorporate\nlinguistic constraints, corpus is represented by its event pairs (ei ej). We assume event pairs are independent:\nP (corpus) = \u220f\nei ej \u2208 corpus P (ei ej) (3)\nWe can rewrite P (ei ej) so that it uses a hidden variable TCi j (temporal class for event pair ei ej) that influences the observed variables (ei ej):\nP (ei ej) = \u2211\nTCi j \u2208 possible temporal classes P (ei ej , TCi j) (4)\nThe probability P (ei ej , TCi j) can be rewritten as:\nP (ei ej , TCi j) = P (ei ej | TCi j)P (TCi j) (5)\nFor inducing temporal relations, EMTRL runs EM on this model. We use a uniform distribution over P (TCi j). It is clear that if we could choose a more informative prior distribution P (TCi j), it would have some benefits like having a better handle on the skewness of the distribution. In some other applications of EM, there are settings for this prior distribution. However, in the problem of temporal relation learning, there cannot be any other prior distribution except uniform distribution; because here, all temporal relation types would seem equal to the learner.\nIf we expand equation 5, each pair ei ej can be represented by its features, which can be potentially used for determining the temporal relation type between events ei and ej . Therefore, P (ei ej | TCi j) can be rewritten using equation 6:\nP (ei ej | TCi j) = P (ei e1j , ei e2j , ... ei ekj | TCi j) (6)\nwhere ei e l j is the value of the l th feature of ei ej . These features, which are similar to those mentioned in the work of Chambers and Jurafsky (2008), are listed in Table 9.\nTo reduce the data sparseness problem and improve the probability estimation, the conditional independence is assumed for these features\u2019 value generation. We only assume that tense and aspect are dependent (i.e., tensei and aspecti), because tense and aspect define temporal location and event structure, and thus considering these features together can be a rich source of information in any temporal relation extraction system. By conditional independence assumption, the value of P (ei ej | TCi j) can be rewritten as:\nP (ei ej | TCi j) = \u220f\nall features l\nP (ei e l j | TCi j) (7)\nThese probabilities (i.e. P (ei e l j | TCi j) ) are regarded as the parameters of our proposed model. Because using them, the likelihood of different temporal classes can be determined. Based on the features in Table 9 and different temporal classes, P (ei e l j | TCi j) can be defined. Four examples of such probabilities are shown below:\n\u2022 P (class(ei) = \u201cOCCURRENCE\u201d AND class(ej) = \u201cPERCEPTION\u201d | TCi j = \u201cBEFORE\u201d)\n\u2022 P (ei dominates ej | TCi j = \u201cAFTER\u201d )\n\u2022 P (Tense(ei) = \u201cPAST\u201d AND Tense(ej) = \u201cPAST\u201d AND Aspect(ei) = \u201cNONE\u201d AND Aspect(ej) = \u201cPROGRESSIV E\u201d | TCi j = \u201cOV ERLAP\u201d)\n\u2022 P (POS of ei = \u201cV \u201d AND POS of ej = \u201cN\u201d | TCi j = \u201cAFTER\u201d )"}, {"heading": "5.3 The Induction Algorithm", "text": "To induce a temporal clustering on a corpus, EM was applied to our proposed model. In EMTRL, the corpus (i.e., event pairs) and the temporal clustering TC are respectively the observed and unobserved (the hidden) random variables. The EM algorithm includes two main steps of expectation (E) and maximization (M), which in our task can be defined in the following way to iteratively estimate the parameters of the model (i.e., P (ei e l j | TCi j)):\nE-step: Fix current parameters of the model, and assign a probability, P (TCi j | ei ej), to each possible temporal class for event pairs (ei ej) of the corpus. This probability can be computed by following equation:\nP (TCi j | ei ej) = P (ei ej , TCi j)\nP (ei ej) (8)\nWe can rewrite equation 8 by using equations 4, 5, 7:\nP (TCi j | ei ej) = P (TCi j)\n\u220f all features l P (ei e\nl j | TCi j)\u2211\nTC\u2032i j \u2208 possible temporal classes P (TC \u2032i j) \u220f all features l P (ei e l j | TC \u2032i j)\n(9) Using equation 9, for each event pair (eiej), the temporal relation type (temporal class) with the highest probability is selected. These relations will be later used in the M-step to update the parameters of the model.\nM-step: By fixing determined temporal relations in E-step, the parameters of the model, P (ei e l j | TCi j), are updated in this step. For achieving this goal, different optimization algorithms such as conjugate gradient can be used. However, these algorithms are slow and costly. In addition, it is difficult to smooth these methods in a desired manner. Therefore, we have used the relative frequency method for re-estimation of the parameters, using equation 10:\nP (ei e l j | TCi j) =\nN(ei e l j , TCi j)\nN(TCi j) (10)\nwhere N(\u2217) counts the number of times that given items or joint items have appeared in the corpus. For example, updating probability P (ei dominates ej | TCi j = \u201cAFTER\u201d) can be done by dividing N(ei dominates ej , TCi j = \u201cAFTER\u201d) (i.e., number of times that ei dominates ej and the temporal relation between ei and ej is AFTER) by N(TCi j = \u201cAFTER\u201d) (i.e., number of times that relation between event pairs in the corpus is AFTER).\nSteps E and M are repeated until one of the following termination conditions will be satisfied: 1) a predefined number of iterations will be reached, or 2) there will be no more changes in P (ei e l j | TCi j). In practice, EMTRL is usually stopped after 30 predefined iterations, while final behavior had been apparent after 15\u2212 22 iterations. After finishing the training phase, the temporal relation ( \u02c6TCi j) for requested event pairs ei ej can be determined using the following equation:\n\u02c6TCi j = arg maxTC\u2032 \u2208 possible temporal classesP (TC \u2032 | ei ej) (11)\nNow, the EM algorithm can begin at either the E-Step or the M-step. We start the induction algorithm at the M-step. It is clear that parameters of the model are not available in the first iteration of EM. Instead, an initial distribution over temporal clustering can be used. There is an important question: how one should initialize this distribution?\nInitialization is an important task in EM, because EM only guarantees to find a local maximum of likelihood. The quality of such a local maxima is highly dependent on the initial starting point. We tested three different ways of initialization:\n1) Random Initialization: a uniform distribution over all temporal clustering was used; therefore, all temporal clustering in the first step had equal probability.\n2) 10% Supervised Initialization: we used a small part of a labeled corpus (10% of each relation type) for this task. Relations were selected randomly.\n3) Rule-based Initialization: we used specific rules for initial estimation of temporal relation types and used this initial estimation for computing parameters of the model. These rules were the combination of the so-called GTag rules (Mani et al., 2006), VerbOcean (Chklovski & Pantel, 2005), and rules derived from certain signal words (e.g., \u201con\u201d, \u201cduring\u201d, \u201cwhen\u201d, and \u201cif\u201d) of the text. GTag contains 187 syntactic and lexical rules for inferring and labeling temporal relations between event, document time, and time expressions. Out of these rules, 169 are between event pairs, which were utilized in EMTRL. These 169 rules are either between event pairs of the same sentence or between two main events of two consecutive sentences. An example of a GTag rule is shown below; other rules are accessible from the Blinker part of the TARSQI toolkit5.\nif conjBetweenEvents = Y ES &&\nisTheSameSentence = TRUE && event1.class = (OCCURRENCE|PERCEPTION |ASPECTUAL|I ACTION) && event2.class = STATE && event1.tense = PAST && event2.tense = PAST && event1.aspect = NONE && event2.aspect = PERFECT && event1.pos = V ERB && event2.pos = V ERB\n5. Available at http://www.timeml.org/site/tarsqi/index.html\nThen\nrelation(event1, event2) = AFTER\nVerbOcean contains lexical rules between two verbs, which can be mined using some lexical and syntactic patterns. The relation between verb pairs can be one of different semantic relations such as strength, enablement, antonymy, similarity, and happens-before. We extracted 4, 205 happens-before rules from VerbOcean. Two examples of these rules are shown below:\nannounce [happens-before] postpone :: 12.844086 review [happens-before] recommend :: 9.049530\nEach rule contains two verbs, their relation, and the strength value of the relationship. For example, the second rule shows relation happens-before between \u201creview\u201d and \u201crecommend\u201d with strength of 9.049530. We also designed 23 other rules based on some signal words such as \u201cbefore\u201d, \u201con\u201d, \u201cwhen\u201d. These rules are in the GTag format. An example of this group of rules is given below:\nif isTheSameSentence = True &&\nsignal = before && signalBetweenTwoEvents = True\nThen\nrelation(event1, event2) = before\nLike many other statistical NLP tasks, smoothing is vital here to alleviate the problem of data sparseness. In particular, in the first few iterations, much more smoothing is required than in later iterations. In our experiments, we used simply the add-1 smoothing technique in computing equation 10."}, {"heading": "6. Experimental Results of EMTRL", "text": "Like our experiments with BCDC, TimeBank and OTC were also used in the experiments with EMTRL. However, in order to simplify the task, we used a different normalized version of these corpora, which contained only the three following temporal relations:\nBEFORE AFTER OVERLAP\nThe main reason for this simplification in EMTRL was that reducing the level of supervision in the task of temporal relation learning makes it an even more difficult task, which is itself already considered to be a hard one (Mani et al., 2006). To normalize these corpora and reduce the number of relation types to three, we adopted the same normalization approach like some previous work (Bethard et al., 2007b), BEFORE and IBEFORE relations were merged into only BEFORE relations. Similarly, the AFTER and IAFTER relations should also be merged into AFTER relations. All the remaining ten relation types were collapsed in OVERLAP relations. Table 10 shows the converted TLink class distribution over TimeBank and OTC.\nBeside TimeBank and OTC, the performance of EMTRL has been also evaluated on tasks E and F of TempEval-2 corpus. The tasks and relations distribution are the same as those shown in Table 5."}, {"heading": "6.1 Results and Discussions", "text": "In our experiments, the baselines were the majority class of event pair relations in the employed corpora (i.e., OV ERLAP in both corpora). Note that the Mani\u2019s method is in fact supervised, which exclusively uses gold-standard features (Mani et al., 2007). The Chambers\u2019 method is similar to Mani\u2019s, except that it also uses some external resources such as WordNet (Chambers et al., 2007). Here, the result of our implementation of Mani and Chambers methods are different from their reported results, because, as it was explained before, we only considered three temporal relation types while in their reported experiments, there were six relation types.\nIn Table 11, in addition to the results of employing EMTRL with three different initializations, we have also reported the results of these initializations as stand-alone classifiers. For Random Initialization and EMTRL + Random Initialization, a question that may arise is how these methods can determine the label of different classes. In fact, these methods can only distinguish three different classes (Class1, Class2, and Class3). Among different possible ways that these unlabeled classes can be mapped to BEFORE, AFTER, or OV ERLAP , we choose the mapping in which the similarity between predicted and annotated temporal relations is maximized.\nConsidering the unsupervised nature of EMTRL, the results of Table 11 can be encouraging. As it is shown in the table, TimeBank\u2019s baseline is well above that of OTC. That is because TimeBank is highly biased towards OVERLAP. Accordingly, it is more difficult for learning methods to pass the baseline of TimeBank. The performance of the Mani\u2019s method, which is a fully supervised approach, is only slightly over this baseline. In this case, EMTRL\u2019s accuracy is considerably below the baseline. However, in the case of OTC, its performance has passed the baseline.\nAs it is shown in Table 11, EMTRL\u2019s accuracy in all three different initializations, have been respectively superior to that of the stand-alone counterparts. The statistical significance of all those results in this table that shows the superiority of EMTRL over the baseline (i.e., in the case of OTC) or the stand-alone initializations (i.e., both corpora) have been verified by the stratified shuffling test with significance level \u03b1 = 0.05.\nTable 11 shows that the best accuracy belongs to the Chambers\u2019 method. However, it should be noted that this method currently has the best-reported results over TimeBank and OTC among all supervised temporal relation extraction methods.\nTable 11 also shows that EMTRL + Randomized Initialization has not been efficient in either corpora. It may be due to the fact that randomized initialization in this very hard problem causes some divergence in the probability distribution. On the other hand, two other initializations have shown satisfactory results in tackling the problem. This implies that initialization is a critical factor in EMTRL, and even little source of supervision can be crucial for achieving satisfactory results.\nTable 12 shows the results of applying EMTRL to the corpus of TempEval 2010. In comparison with the accuracy of kernels in Table 8, EMTRL could achieve encouraging results. In this case, EMTRL\u2019s accuracy in all three different initializations, have also been respectively superior to that of the stand-alone counterparts. TRIPS and NCSU-indi are the most successful supervised systems in tasks E and F of TempEval 2010, respectively (Verhagen et al., 2010)."}, {"heading": "6.2 Inconsistency Removal", "text": "Since in a pair-wise relation learning system, the relation between each pair of events is predicted without considering its impact on the relations of other event pairs, system may encounter some inconsistencies among predicted relations. This may happen after selecting temporal relations by equation 9 in E-step. It can also happen in finding final class labels by equation 11. Figure 6 shows an example of an inconsistent relation between events A, B, and C:\nThere are several ways of eliminating such inconsistencies (Mani et al., 2007; Tatu & Srikanth, 2008; Chambers & Jurafsky, 2008). In this work, we have used two different approaches: a greedy best-first search strategy and an Integer Linear Programming (ILP) based method. More details about both approaches are given next."}, {"heading": "6.2.1 Greedy Best-First Search Strategy", "text": "In order to detect possible inconsistencies between predicted relations, we first build a graph for each text, where each node corresponds to an event, and an edge represents a temporal relation between corresponding events. Then, any existing contradiction among connected nodes of each graph can be discovered by applying a set of rules (i.e., 640 rules) based on the Allen\u2019s interval algebra (Allen, 1984). As an example, consider the following three rules:\n\u2022 before(x, y) && before(y, z) \u2212\u2192 before(x, z) \u2022 after(x, y) && before(z, y) \u2212\u2192 after(x, z) \u2022 after(x, y) && includes(y, z) \u2212\u2192 after(x, z)\nThe inconsistent relations of each graph is stored in a sorted list named SL, based on a computed confidence score (i.e., P (TCi j | ei ej) of equation 9). Thus, in SL, the first and the last elements are the most and the least confident relations, respectively.\nThe algorithm starts from the first relation of SL, and pops off this relation and adds it to another list named FL. After adding a new relation to FL, the algorithm verifies the consistency among relations of FL. If the new relation introduces an inconsistency, it will be\nreplaced by the next confident relation between its corresponding events. This replacement may be repeated until that the new relation will be consistent with other relations existing in FL. When there are no more contradictions in FL, the algorithm will move the next element of SL to FL. These operations are iterated until there will remain no more relations in SL. The resultant consistent relations in FL can then be used in subsequent M-step or in the final result of EM."}, {"heading": "6.2.2 The Integer Linear Programming (ILP)", "text": "In our second approach, we cast the task of finding most probable temporal relations as an optimization problem. In contrast with the previous method, this approach, which is based on an integer linear programming (ILP) framework, finds an optimal solution based on the parameters of the model, P (ei e l j | TCi j). This method is similar to that of Chambers and Jurafsky (2008). In this ILP framework, for each event pair (ei , ej), there is a relation type M from ei to ej denoted by TRi j\u2212M . The objective function of the framework is defined as follows:\nmax \u2211 i \u2211 j > i (\u2211 M (Pi j\u2212M TRi j\u2212M + Pj i\u2212M TRj i\u2212M ) ) (12)\nwhere (Pi j\u2212M (i.e., P (M | ei ej) of equation 8) is the probability of the temporal relation of type M from ei to ej . This objective function maximizes the sum of probabilities of all temporal relations between event pairs of the input text. There are also three following constraints (i.e., 13, 14, 15) on this objective function:\n\u2200i \u2200j \u2200M, i > j : TRi j\u2212M , TRj i\u2212M \u2208 {0, 1} (13)\nConstraint 13 implies that each TRi j\u2212M variable is either zero or one.\n\u2200i \u2200j, i > j : \u2211 M (TRi j\u2212M + TRj i\u2212M ) = 1 (14)\nConstraint 14 ensures that between each pair of events (ei and ej), only one TRi j\u2212M variable is set to one, and the rest are set to zero. In other words, it is impossible for a pair of events to have two (or more) relations.\nTRi j\u2212M1 + TRj k\u2212M2 \u2212 TRi k\u2212M3 \u2264 1 (15)\nConstraint 15 guarantees the transitivity conditions among event pairs, wherever relations TRi j\u2212M1 and TRj k\u2212M2 entail relation TRi k\u2212M3. It is obvious that the transitivity constraint is effective only when the event pairs are connected to one another. In a disconnected graph, this constraint has little effect. For example, in Figure 6, by considering this constraint and relations TRA B\u2212After and TRB C\u2212After, TRA C\u2212After is the only possible relation between events A and C.\nAfter generating the set of all constraints for each document, we can use an ILP solver (SCIP6) to solve the problem. One important issue about ILP is that this technique is more\n6. This ILP solver which is the fastest existing noncommercial mixed integer programming solver. Available at http://scip.zib.de/\neffective on dense temporal graphs than in sparse ones.\nAfter removing contradictions in the temporal relations, generated (consistent) relations can be easily used in updating the probabilities of the model in the M-step. The results of Tables 11 and 12 are without applying the greedy best-first search or ILP. The accuracy results with the greedy and ILP algorithms over TimeBank and OTC are shown in Table 13. Table 14 shows the accuracy results for tasks E and F over the corpus of TempEval 2010. One question that may arise is how we can enforce the transitivity constraints in EM, when we have only labels Class1, Class2, and Class3, rather than BEFORE, AFTER, and OVERLAP. This problem only happens for the case of EMTRL + Random Initialization, for which we have used a prior assignment of Class1 = AFTER, Class2 = BEFORE, and Class3 = OV ERLAP . For the other two initializations (i.e., 10% Supervised and Rule-base), this problem does not occur, because our algorithm starts with the actual class labels BEFORE, AFTER, and OVERLAP.\nTables 13 and 14 show the impact of utilizing the greedy best first search and ILP approaches in EMTRL against the base method. By using these strategies, some of the inconsistencies that may exist among predicted temporal relations, are removed (in step E of EMTRL) to make the predicted relations more reliable. As a result, in step M, the parameters of the model will be updated more accurately and thus the accuracy of the whole algorithm will iteratively increase.\nThe significance of the results depicted in Tables 13 and 14 have been verified by the stratified shuffling with significance level \u03b1 = 0.05. As we had expected, the results of these approaches on EMTRL + Random Initialization was not statistically significant. On the other hand, in majority of tests on EMTRL + 10% Supervised Initialization and EMTRL + Rule-based Initialization, where we compared the output of the greedy and ILP algorithms with that of base method, the statistical significance of the results was verified."}, {"heading": "7. Conclusion and Future Work", "text": "In this paper, we have addressed the problem of temporal relation learning between events, which has been a topic of interest since early days of statistical natural language processing. We have concentrated our efforts to reduce the need to annotated corpora as much as possible. Accordingly, in this paper, two new algorithms, a weakly supervised and an unsupervised, were presented.\nThe first algorithm was a two-stage weakly supervised approach for classification of temporal relations. In the first stage of the algorithm, a SVM based classifier was trained to learn temporal relations of the corpus. Then, in the second stage of the algorithm, a cross-document bootstrapping technique was employed to iteratively improve the model produced in the first stage. By the idea of bootstrapping, which has been inspired by the hypothesis that we have called \u201cone type of temporal relation between events per discourse\u201d, for each test document, some global evidences from a cluster of topically related documents refined local decisions made by the initial model. The results of experiment with this new technique showed a significant improvement in terms of accuracy over related work including the state-of-the-art of the statistical methods.\nThe second proposed algorithm was a novel model that used the EM algorithm with interval algebra reasoning for temporal relation learning. We compared this work with some of the successful fully supervised methods. Our experiments showed encouraging results, considering the low level of supervision that was provided for the algorithm.\nCurrently, we are working on finding ways of further improvement of our algorithms, and at the same time trying to reduce the supervision level. In BCDC, by extracting semantic features from related documents, we may be able to improve its performance. Inconsistency removal (i.e. ILP and greedy best first search) algorithms can be also employed in BCDC. Besides, employing the hypothesis of \u201cone type of temporal relation between events per discourse\u201d as an explicit constraint can be other possible direction for further research. In EMTRL, one can use other sources of information like narrative information, relations between events and document times, and relations between events and time expressions to build a denser temporal graph. This increases the effectiveness of the greedy best first search and integer linear programming algorithms. We also think, but have not verified yet, that using a richer feature set may further improve the accuracy of EMTRL."}, {"heading": "Acknowledgments", "text": "The authors wish to thank the associate editor and the anonymous reviewers for their valuable comments."}], "references": [{"title": "Towards a general theory of action and time", "author": ["J. Allen"], "venue": "Artificial intelligence,", "citeRegEx": "Allen,? \\Q1984\\E", "shortCiteRegEx": "Allen", "year": 1984}, {"title": "Cu-tmp: Temporal relation classification using syntactic and semantic features", "author": ["S. Bethard", "J. Martin"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Bethard and Martin,? \\Q2007\\E", "shortCiteRegEx": "Bethard and Martin", "year": 2007}, {"title": "Learning semantic links from a corpus of parallel temporal and causal relations", "author": ["S. Bethard", "J. Martin"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,", "citeRegEx": "Bethard and Martin,? \\Q2008\\E", "shortCiteRegEx": "Bethard and Martin", "year": 2008}, {"title": "Finding temporal structure in text: Machine learning of syntactic temporal relations", "author": ["S. Bethard", "J. Martin", "S. Klingenstein"], "venue": "International Journal of Semantic Computing,", "citeRegEx": "Bethard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2007}, {"title": "Timelines from text: Identification of syntactic temporal relations", "author": ["S. Bethard", "J. Martin", "S. Klingenstein"], "venue": "In Semantic Computing,", "citeRegEx": "Bethard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2007}, {"title": "Finding event, temporal and causal structure in text: A machine learning approach", "author": ["S. Bethard"], "venue": "Ph.D. thesis,", "citeRegEx": "Bethard,? \\Q2007\\E", "shortCiteRegEx": "Bethard", "year": 2007}, {"title": "Timeml-compliant text analysis for temporal reasoning", "author": ["B. Boguraev", "R. Ando"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Boguraev and Ando,? \\Q2005\\E", "shortCiteRegEx": "Boguraev and Ando", "year": 2005}, {"title": "Timebank evolution as a community resource for timeml parsing", "author": ["B. Boguraev", "J. Pustejovsky", "R. Ando", "M. Verhagen"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Boguraev et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Boguraev et al\\.", "year": 2007}, {"title": "Inducing temporal graphs", "author": ["P. Bramsen", "P. Deshpande", "Y. Lee", "R. Barzilay"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bramsen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bramsen et al\\.", "year": 2006}, {"title": "Jointly combining implicit constraints improves temporal ordering", "author": ["N. Chambers", "D. Jurafsky"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chambers and Jurafsky,? \\Q2008\\E", "shortCiteRegEx": "Chambers and Jurafsky", "year": 2008}, {"title": "Classifying temporal relations between events", "author": ["N. Chambers", "S. Wang", "D. Jurafsky"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "Chambers et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2007}, {"title": "Libsvm: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Em works for pronoun anaphora resolution", "author": ["E. Charniak", "M. Elsner"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Charniak and Elsner,? \\Q2009\\E", "shortCiteRegEx": "Charniak and Elsner", "year": 2009}, {"title": "Naist. japan: Temporal relation identification using dependency parsed tree", "author": ["Y. Cheng", "M. Asahara", "Y. Matsumoto"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Cheng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2007}, {"title": "An expectation maximization approach to pronoun resolution", "author": ["C. Cherry", "S. Bergsma"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Cherry and Bergsma,? \\Q2005\\E", "shortCiteRegEx": "Cherry and Bergsma", "year": 2005}, {"title": "Global path-based refinement of noisy graphs applied to verb semantics", "author": ["T. Chklovski", "P. Pantel"], "venue": "In Natural Language Processing\u2013IJCNLP", "citeRegEx": "Chklovski and Pantel,? \\Q2005\\E", "shortCiteRegEx": "Chklovski and Pantel", "year": 2005}, {"title": "Convolution kernels for natural language", "author": ["M. Collins", "N. Duffy"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Collins and Duffy,? \\Q2001\\E", "shortCiteRegEx": "Collins and Duffy", "year": 2001}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Comparison of different algebras for inducing the temporal structure of texts", "author": ["P. Denis", "P. Muller"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Denis and Muller,? \\Q2010\\E", "shortCiteRegEx": "Denis and Muller", "year": 2010}, {"title": "Predicting globally-coherent temporal structures from texts via endpoint inference and graph decomposition", "author": ["P. Denis", "P. Muller"], "venue": "In Twenty-Second International Joint Conference on Artificial Intelligence", "citeRegEx": "Denis and Muller,? \\Q2011\\E", "shortCiteRegEx": "Denis and Muller", "year": 2011}, {"title": "Usfd2: Annotating temporal expresions and tlinks for tempeval-2", "author": ["L. Derczynski", "R. Gaizauskas"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Derczynski and Gaizauskas,? \\Q2010\\E", "shortCiteRegEx": "Derczynski and Gaizauskas", "year": 2010}, {"title": "Ncsu: Modeling temporal relations with markov logic and lexical ontology", "author": ["E. Ha", "A. Baikadi", "C. Licata", "J. Lester"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Ha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2010}, {"title": "Xrce-t: Xip temporal module for tempeval campaign", "author": ["C. Hag\u00e8ge", "X. Tannier"], "venue": "In Proceedings of the fourth international workshop on semantic evaluations", "citeRegEx": "Hag\u00e8ge and Tannier,? \\Q2007\\E", "shortCiteRegEx": "Hag\u00e8ge and Tannier", "year": 2007}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Data Mining: Concepts and Techniques (second edition)", "author": ["J. Han", "M. Kambert"], "venue": null, "citeRegEx": "Han and Kambert,? \\Q2006\\E", "shortCiteRegEx": "Han and Kambert", "year": 2006}, {"title": "Usfd: preliminary exploration of features and classifiers for the tempeval-2007 tasks", "author": ["M. Hepple", "A. Setzer", "R. Gaizauskas"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Hepple et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hepple et al\\.", "year": 2007}, {"title": "Refining event extraction through cross-document inference", "author": ["H. Ji", "R. Grishman"], "venue": "In Proceedings of the Joint Conference of the 46th Annual Meeting of the ACL,", "citeRegEx": "Ji and Grishman,? \\Q2008\\E", "shortCiteRegEx": "Ji and Grishman", "year": 2008}, {"title": "The Unsupervised Learning of Natural Language Structure", "author": ["D. Klein"], "venue": "Ph.D. thesis,", "citeRegEx": "Klein,? \\Q2005\\E", "shortCiteRegEx": "Klein", "year": 2005}, {"title": "Learning sentence-internal temporal relations", "author": ["M. Lapata", "A. Lascarides"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Lapata and Lascarides,? \\Q2006\\E", "shortCiteRegEx": "Lapata and Lascarides", "year": 2006}, {"title": "Dirt: discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Tipsem (english and spanish): Evaluating crfs and semantic roles in tempeval-2", "author": ["H. Llorens", "E. Saquete", "B. Navarro"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Llorens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Llorens et al\\.", "year": 2010}, {"title": "Machine learning of temporal relations", "author": ["I. Mani", "M. Verhagen", "B. Wellner", "C. Lee", "J. Pustejovsky"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Mani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2006}, {"title": "Three approaches to learning tlinks in timeml", "author": ["I. Mani", "B. Wellner", "M. Verhagen", "J. Pustejovsky"], "venue": null, "citeRegEx": "Mani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2007}, {"title": "Description of the ltg system used for muc-7", "author": ["A. Mikheev", "C. Grover", "M. Moens"], "venue": "In Proceedings of 7th Message Understanding Conference (MUC-7)", "citeRegEx": "Mikheev et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Mikheev et al\\.", "year": 1998}, {"title": "Lcc-te: a hybrid approach to temporal relation identification in news text", "author": ["C. Min", "M. Srikanth", "A. Fowler"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Min et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Min et al\\.", "year": 2007}, {"title": "Event-time temporal relation classification using syntactic tree kernels", "author": ["S. Mirroshandel", "G. Ghassem-Sani", "M. Khayyamian"], "venue": "In Proceeding of the 4th Language and Technology Conference,", "citeRegEx": "Mirroshandel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mirroshandel et al\\.", "year": 2009}, {"title": "Using tree kernels for classifying temporal relations between events", "author": ["S. Mirroshandel", "G. Ghassem-Sani", "M. Khayyamian"], "venue": "In Proceedings of the 23th Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "Mirroshandel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mirroshandel et al\\.", "year": 2009}, {"title": "Using syntactic-based kernels for classifying temporal relations", "author": ["S. Mirroshandel", "G. Ghassem-Sani", "M. Khayyamian"], "venue": "Journal of Computer Science and Technology,", "citeRegEx": "Mirroshandel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mirroshandel et al\\.", "year": 2011}, {"title": "Discovering causal and temporal relations in biomedical texts recognizing causal and temporal relations", "author": ["R. Mulkar-Mehta", "J. Hobbs", "C. Liu", "X. Zhou"], "venue": "In Proceedings of the AAAI Spring Symposium, Stanford CA", "citeRegEx": "Mulkar.Mehta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mulkar.Mehta et al\\.", "year": 2009}, {"title": "Annotating and measuring temporal relations in texts", "author": ["P. Muller", "X. Tannier"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Muller and Tannier,? \\Q2004\\E", "shortCiteRegEx": "Muller and Tannier", "year": 2004}, {"title": "Unsupervised models for coreference resolution", "author": ["V. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ng,? \\Q2008\\E", "shortCiteRegEx": "Ng", "year": 2008}, {"title": "Acquisition of verb entailment from text", "author": ["V. Pekar"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Pekar,? \\Q2006\\E", "shortCiteRegEx": "Pekar", "year": 2006}, {"title": "Improved inference for unlexicalized parsing", "author": ["S. Petrov", "D. Klein"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Petrov and Klein,? \\Q2007\\E", "shortCiteRegEx": "Petrov and Klein", "year": 2007}, {"title": "Wvali: Temporal relation identification by syntactico-semantic analysis", "author": ["G. Puscasu"], "venue": "In Proceedings of the 4th International Workshop on SemEval,", "citeRegEx": "Puscasu,? \\Q2007\\E", "shortCiteRegEx": "Puscasu", "year": 2007}, {"title": "The timebank corpus", "author": ["J. Pustejovsky", "P. Hanks", "R. Sauri", "A. See", "R. Gaizauskas", "A. Setzer", "D. Radev", "B. Sundheim", "D. Day", "L. Ferro", "M. Lazo"], "venue": "In Corpus Linguistics,", "citeRegEx": "Pustejovsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Evita: a robust event recognizer for qa systems", "author": ["R. Sau\u0155\u0131", "R. Knippen", "M. Verhagen", "J. Pustejovsky"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Sau\u0155\u0131 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sau\u0155\u0131 et al\\.", "year": 2005}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["A. S\u00f8gaard"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers,", "citeRegEx": "S\u00f8gaard,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard", "year": 2011}, {"title": "Indri: A language model-based search engine for complex queries", "author": ["T. Strohman", "D. Metzler", "H. Turtle", "W. Croft"], "venue": "In Proceedings of the International Conference on Intelligent Analysis", "citeRegEx": "Strohman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Strohman et al\\.", "year": 2005}, {"title": "Scaling web-based acquisition of entailment relations", "author": ["I. Szpektor", "H. Tanev", "I. Dagan", "B. Coppola"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Szpektor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Szpektor et al\\.", "year": 2004}, {"title": "Experiments with reasoning for temporal relations between events", "author": ["M. Tatu", "M. Srikanth"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Tatu and Srikanth,? \\Q2008\\E", "shortCiteRegEx": "Tatu and Srikanth", "year": 2008}, {"title": "Trips and trios system for tempeval-2: Extracting temporal information from text", "author": ["N. UzZaman", "J. Allen"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "UzZaman and Allen,? \\Q2010\\E", "shortCiteRegEx": "UzZaman and Allen", "year": 2010}, {"title": "Semeval-2007 task 15: Tempeval temporal relation identification", "author": ["M. Verhagen", "R. Gaizauskas", "F. Schilder", "M. Hepple", "G. Katz", "J. Pustejovsky"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Verhagen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Verhagen et al\\.", "year": 2007}, {"title": "Semeval-2010 task 13: Tempeval-2", "author": ["M. Verhagen", "R. Sauri", "T. Caselli", "J. Pustejovsky"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Verhagen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Verhagen et al\\.", "year": 2010}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Yarowsky,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky", "year": 1995}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "In Proceedings of the 18th conference on Computational linguistics-Volume", "citeRegEx": "Yeh,? \\Q2000\\E", "shortCiteRegEx": "Yeh", "year": 2000}, {"title": "Jointly identifying temporal relations with markov logic", "author": ["K. Yoshikawa", "S. Riedel", "M. Asahara", "Y. Matsumoto"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "Yoshikawa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yoshikawa et al\\.", "year": 2009}, {"title": "A composite kernel to extract relations between entities with both flat and structured features", "author": ["M. Zhang", "J. Zhang", "J. Su", "G. Zhou"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 46, "context": "As a result, tasks such as part-of-speech tagging (S\u00f8gaard, 2011), parsing (Petrov & Klein, 2007), and named entity recognition (Mikheev, Grover, & Moens, 1998) have been addressed with satisfactory results.", "startOffset": 50, "endOffset": 65}, {"referenceID": 44, "context": "Construction of the TimeBank corpus in 2003 (Pustejovsky et al., 2003), provided the opportunity of applying different machine learning methods to the task of temporal relation extraction.", "startOffset": 44, "endOffset": 70}, {"referenceID": 0, "context": "This algorithm also employs Allen\u2019s interval algebra (Allen, 1984) for correction of predicted relations.", "startOffset": 53, "endOffset": 66}, {"referenceID": 31, "context": "The first task is very hard to evaluate, because the annotators may ignore many plausible existing relations while tagging the corpora (Mani et al., 2006).", "startOffset": 135, "endOffset": 154}, {"referenceID": 44, "context": "As it is shown in Figure 1, each temporal relation can be one of the fourteen types proposed in TimeML (Pustejovsky et al., 2003).", "startOffset": 103, "endOffset": 129}, {"referenceID": 51, "context": "Additionally, there have been two important shared tasks on temporal information extraction: TempEval 2007 (Verhagen et al., 2007) and TempEval 2010 (Verhagen, Sauri, Caselli, & Pustejovsky, 2010).", "startOffset": 107, "endOffset": 130}, {"referenceID": 10, "context": "Maximum Entropy (Mani, Wellner, Verhagen, & Pustejovsky, 2007; Derczynski & Gaizauskas, 2010), Support Vector Machines (Chambers et al., 2007; Bethard & Martin, 2007; Hepple, Setzer, & Gaizauskas, 2007; Cheng, Asahara, & Matsumoto, 2007; Mirroshandel, Ghassem-Sani, & Khayyamian, 2009a, 2009b, 2011), Conditional Random Fields (Llorens, Saquete, & Navarro, 2010; Kolya, Ekbal, & Bandyopadhyay, 2010), and Markov Logic Networks (UzZaman & Allen, 2010; Ha, Baikadi, Licata, & Lester, 2010) are some of the statistical techniques that have been applied to this problem.", "startOffset": 119, "endOffset": 299}, {"referenceID": 32, "context": "MaxEnt is one of the first approaches to the temporal relation learning, which uses maximum entropy classification algorithm (Mani et al., 2007).", "startOffset": 125, "endOffset": 144}, {"referenceID": 10, "context": "The state-of-the-art of the statistical methods is analogous to MaxEnt (Chambers et al., 2007).", "startOffset": 71, "endOffset": 94}, {"referenceID": 25, "context": "USFD (Hepple et al., 2007) and NAIST-Japan (Cheng et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 13, "context": ", 2007) and NAIST-Japan (Cheng et al., 2007) were two other participants of TempEval 2007 that used SVM for classification.", "startOffset": 24, "endOffset": 44}, {"referenceID": 25, "context": "In USFD, temporal relation learning is treated as a simple classification task (Hepple et al., 2007).", "startOffset": 79, "endOffset": 100}, {"referenceID": 23, "context": "They used different classification algorithm from WEKA machine learning workbench (Hall et al., 2009).", "startOffset": 82, "endOffset": 101}, {"referenceID": 5, "context": "Since existing corpora provide no parallel temporal or causal annotations, 1000 conjoined event pairs were annotated (Bethard & Martin, 2008; Bethard, 2007).", "startOffset": 117, "endOffset": 156}, {"referenceID": 21, "context": "Markov Logic Networks (MLN) is another classification algorithm, which have been used by two participants of TempEval 2010: TRIPS & TRIOS (UzZaman & Allen, 2010) and NCSU (Ha et al., 2010).", "startOffset": 171, "endOffset": 188}, {"referenceID": 21, "context": "bOcean and WordNet) (Ha et al., 2010).", "startOffset": 20, "endOffset": 37}, {"referenceID": 30, "context": "One of the most successful participants of TempEval 2010 was TIPSem that is based on Conditional Random Field (CRF) models for classification purpose (Llorens et al., 2010).", "startOffset": 150, "endOffset": 172}, {"referenceID": 48, "context": "In the final part of the algorithm, some post-processing transformations are applied to the extracted templates to remove inappropriate templates (Szpektor et al., 2004).", "startOffset": 146, "endOffset": 169}, {"referenceID": 21, "context": "bOcean and WordNet) (Ha et al., 2010). NCSU was applied to tasks C, D, E, and F in two different settings: NCSU-indi and NCSU-joint. In NCSU-indi, an independent MLN was trained for each task. On the other hand, a set of global formulae was also added to NCSU-joint to ensure the consistency among classification decisions from four local MLNs (one for each task). NCSU-indi achieved the best result in task F and the second best result on task C. One of the most successful participants of TempEval 2010 was TIPSem that is based on Conditional Random Field (CRF) models for classification purpose (Llorens et al., 2010). TIPSem employs different morphological, syntactic, and semantic features for building CRF models. In Spanish, it achieved the best results in all tasks. In English, TIPSem achieved the best results in Tasks B and D; and was one of the best systems in all other tasks. JU CSE TEMP was another participant of TempEval 2010 that utilized CRF models for temporal relation learning tasks (Kolya et al., 2010). The system needs only the goldstandard features of TimeBank for time expressions and/or events. In comparison with TIPSem, JU CSE TEMP achieved weaker results, which shows the importance of feature engineering in temporal relation learning. There is another approach that applies different machine learning techniques to detect intra-sentential events, and builds a corpus of sentences with two or more events in which at least one event is triggered by a key time word (e.g., after, before, etc.). The classifier is based on a number of syntactic and clausal ordering features (Lapata & Lascarides, 2006; Bramsen, Deshpande, Lee, & Barzilay, 2006). There exist a comprehensive study about statistical methods, which compares three different interval based algebras in terms of classification accuracy, performance, and expressiveness power (Denis & Muller, 2010). There are also a few algorithms that exclusively work on temporal relation classification between events and time expressions. One of such algorithms employs cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data (Boguraev & Ando, 2005). There is a group of statistical methods that rely on information of argument fillers (called anchors) of every event expression as a valuable clue for recognizing temporal relations. In these methods, by looking at a set of event expressions whose argument fillers have a similar distribution, analogous event expressions are recognized. Algorithms such as DIRT (Lin & Pantel, 2001), TE/ASE (Szpektor, Tanev, Dagan, & Coppola, 2004), and that of the Pekar\u2019s system (2006) are examples of this type of statistical method.", "startOffset": 21, "endOffset": 2770}, {"referenceID": 41, "context": "In Pekar\u2019s approach (2006), co-occurrence of two verbs inside a locally coherent text is used to extract some useful information.", "startOffset": 3, "endOffset": 27}, {"referenceID": 32, "context": "The test accuracy on this enlarged corpus showed some improvements (Mani et al., 2007).", "startOffset": 67, "endOffset": 86}, {"referenceID": 0, "context": "Then a classical path-consistency algorithm (Allen, 1984) was applied to the extracted relations of the first step.", "startOffset": 44, "endOffset": 57}, {"referenceID": 30, "context": "The test accuracy on this enlarged corpus showed some improvements (Mani et al., 2007). Reasoning with pre-determined rules is another approach to the rules\u2019 usage. In the work of Tatu and Srikanth (2008), a rich set of axioms (rules) was created and used by a first order logic based theorem prover to find a proof for each temporal relation by refutation.", "startOffset": 68, "endOffset": 205}, {"referenceID": 30, "context": "The test accuracy on this enlarged corpus showed some improvements (Mani et al., 2007). Reasoning with pre-determined rules is another approach to the rules\u2019 usage. In the work of Tatu and Srikanth (2008), a rich set of axioms (rules) was created and used by a first order logic based theorem prover to find a proof for each temporal relation by refutation. A set of discourse rules was used in the algorithm of Muller and Tannier (2004) to establish the possible relations between every two consecutive events of the input text.", "startOffset": 68, "endOffset": 438}, {"referenceID": 9, "context": "For example in the work of Chambers and Jurafsky (2008), local decisions generated by a statistical method were combined with two types of implicit global rule-based properties.", "startOffset": 27, "endOffset": 56}, {"referenceID": 43, "context": "WVALI is another hybrid system, which has an enhanced classification process by using some rules from a particular knowledge base (Puscasu, 2007).", "startOffset": 130, "endOffset": 145}, {"referenceID": 53, "context": "This motivated us to apply the so-called \u201cone sense per discourse\u201d hypothesis (Yarowsky, 1995) to the", "startOffset": 78, "endOffset": 94}, {"referenceID": 31, "context": "Although, as it is later explained, we have not explicitly employed this assumption in our proposed algorithm, we have tried to verify the assumption by considering temporal relations of the Opinion corpus (Mani et al., 2006).", "startOffset": 206, "endOffset": 225}, {"referenceID": 44, "context": "The range of values for these attributes is based on the work of Pustejovsky et al. (2003), and is shown in Table 2.", "startOffset": 65, "endOffset": 91}, {"referenceID": 44, "context": "These two datasets have been annotated based on the TimeML standard (Pustejovsky et al., 2003).", "startOffset": 68, "endOffset": 94}, {"referenceID": 31, "context": "For the sake of reducing the data sparseness problem, as many others (Mani et al., 2006; Tatu & Srikanth, 2008; Mani et al., 2007; Chambers et al., 2007), we have used a normalized version of these relation types including only six following relations:", "startOffset": 69, "endOffset": 153}, {"referenceID": 32, "context": "For the sake of reducing the data sparseness problem, as many others (Mani et al., 2006; Tatu & Srikanth, 2008; Mani et al., 2007; Chambers et al., 2007), we have used a normalized version of these relation types including only six following relations:", "startOffset": 69, "endOffset": 153}, {"referenceID": 10, "context": "For the sake of reducing the data sparseness problem, as many others (Mani et al., 2006; Tatu & Srikanth, 2008; Mani et al., 2007; Chambers et al., 2007), we have used a normalized version of these relation types including only six following relations:", "startOffset": 69, "endOffset": 153}, {"referenceID": 0, "context": ", identical to the Allen\u2019s CONTAINS) based on the Allen\u2019s interval algebra (Allen, 1984).", "startOffset": 75, "endOffset": 88}, {"referenceID": 31, "context": "In our experiments, like some previous work (Mani et al., 2006; Chambers et al., 2007; Chambers & Jurafsky, 2008), TimeBank and Opinion corpora have been merged into a single corpus called Opinion TimeBank Corpus (OTC).", "startOffset": 44, "endOffset": 113}, {"referenceID": 10, "context": "In our experiments, like some previous work (Mani et al., 2006; Chambers et al., 2007; Chambers & Jurafsky, 2008), TimeBank and Opinion corpora have been merged into a single corpus called Opinion TimeBank Corpus (OTC).", "startOffset": 44, "endOffset": 113}, {"referenceID": 52, "context": "This part is based on TimeBank (Verhagen et al., 2010; Pustejovsky et al., 2003; Boguraev, Pustejovsky, Ando, & Verhagen, 2007).", "startOffset": 31, "endOffset": 127}, {"referenceID": 44, "context": "This part is based on TimeBank (Verhagen et al., 2010; Pustejovsky et al., 2003; Boguraev, Pustejovsky, Ando, & Verhagen, 2007).", "startOffset": 31, "endOffset": 127}, {"referenceID": 45, "context": "The EVITA system (Sau\u0155\u0131 et al., 2005) has been used for event extraction.", "startOffset": 17, "endOffset": 37}, {"referenceID": 47, "context": "The INDRI retrieval system (Strohman et al., 2005) has been employed to obtain related documents.", "startOffset": 27, "endOffset": 50}, {"referenceID": 32, "context": "Mani\u2019s method is regarded as a successful statistical approach to temporal relation identification, which exclusively uses gold standard features of events (Mani et al., 2007).", "startOffset": 156, "endOffset": 175}, {"referenceID": 10, "context": "To achieve a higher accuracy, he has also used some extra resources such as WordNet (Chambers et al., 2007).", "startOffset": 84, "endOffset": 107}, {"referenceID": 37, "context": "1, by applying a simple event kernel (Mirroshandel et al., 2011).", "startOffset": 37, "endOffset": 64}, {"referenceID": 54, "context": "After finishing all iterations, the p-value (likelihood of incorrectly rejecting the null hypothesis) is simply calculated by (nc+1)/(nt+1) (Yeh, 2000).", "startOffset": 140, "endOffset": 151}, {"referenceID": 52, "context": "JU-CSE, NCSU-indi, NCSU-joint, TIPSem, TIPSem-B, TRIOS, and TRIPS are participants of the TempEval 2010 shared task (Verhagen et al., 2010).", "startOffset": 116, "endOffset": 139}, {"referenceID": 27, "context": "Due to the encouraging results of the expectation maximization (EM) algorithm in other unsupervised tasks of natural language processing such as unsupervised grammar induction (Klein, 2005), unsupervised anaphora resolution (Cherry & Bergsma, 2005; Charniak & Elsner, 2009), and unsupervised coreference resolution (Ng, 2008), we decided to evaluate EM in unsupervised temporal relation extraction.", "startOffset": 176, "endOffset": 189}, {"referenceID": 40, "context": "Due to the encouraging results of the expectation maximization (EM) algorithm in other unsupervised tasks of natural language processing such as unsupervised grammar induction (Klein, 2005), unsupervised anaphora resolution (Cherry & Bergsma, 2005; Charniak & Elsner, 2009), and unsupervised coreference resolution (Ng, 2008), we decided to evaluate EM in unsupervised temporal relation extraction.", "startOffset": 315, "endOffset": 325}, {"referenceID": 9, "context": "These features, which are similar to those mentioned in the work of Chambers and Jurafsky (2008), are listed in Table 9.", "startOffset": 68, "endOffset": 97}, {"referenceID": 31, "context": "These rules were the combination of the so-called GTag rules (Mani et al., 2006), VerbOcean (Chklovski & Pantel, 2005), and rules derived from certain signal words (e.", "startOffset": 61, "endOffset": 80}, {"referenceID": 31, "context": "The main reason for this simplification in EMTRL was that reducing the level of supervision in the task of temporal relation learning makes it an even more difficult task, which is itself already considered to be a hard one (Mani et al., 2006).", "startOffset": 224, "endOffset": 243}, {"referenceID": 32, "context": "Note that the Mani\u2019s method is in fact supervised, which exclusively uses gold-standard features (Mani et al., 2007).", "startOffset": 97, "endOffset": 116}, {"referenceID": 10, "context": "The Chambers\u2019 method is similar to Mani\u2019s, except that it also uses some external resources such as WordNet (Chambers et al., 2007).", "startOffset": 108, "endOffset": 131}, {"referenceID": 52, "context": "TRIPS and NCSU-indi are the most successful supervised systems in tasks E and F of TempEval 2010, respectively (Verhagen et al., 2010).", "startOffset": 111, "endOffset": 134}, {"referenceID": 32, "context": "There are several ways of eliminating such inconsistencies (Mani et al., 2007; Tatu & Srikanth, 2008; Chambers & Jurafsky, 2008).", "startOffset": 59, "endOffset": 128}, {"referenceID": 0, "context": ", 640 rules) based on the Allen\u2019s interval algebra (Allen, 1984).", "startOffset": 51, "endOffset": 64}, {"referenceID": 9, "context": "This method is similar to that of Chambers and Jurafsky (2008). In this ILP framework, for each event pair (ei , ej), there is a relation type M from ei to ej denoted by TRi j\u2212M .", "startOffset": 34, "endOffset": 63}], "year": 2012, "abstractText": "Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of \u201cone type of temporal relation per discourse\u201d, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.", "creator": "TeX"}}}