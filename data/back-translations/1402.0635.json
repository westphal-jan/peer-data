{"id": "1402.0635", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Generalization and Exploration via Randomized Value Functions", "abstract": "We propose an approach to exploration based on randomized value functions and an algorithm -- Randomized Least Squares Value Iteration (RLSVI) -- that embodies this approach. We explain why versions of least squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient and present computational results that show dramatic efficiencies that RLSVI enjoys. Our experiments focus on learning about episodes of Markov's finite horizon decision-making process and use a version of RLSVI designed for this task, but we also propose a version of RLSVI that is dedicated to continuous learning in an infinite horizon with a discounted Markov decision-making process.", "histories": [["v1", "Tue, 4 Feb 2014 06:41:59 GMT  (47kb,D)", "https://arxiv.org/abs/1402.0635v1", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v2", "Tue, 7 Jul 2015 23:11:02 GMT  (176kb,D)", "http://arxiv.org/abs/1402.0635v2", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v3", "Mon, 15 Feb 2016 10:20:11 GMT  (3057kb,D)", "http://arxiv.org/abs/1402.0635v3", "arXiv admin note: text overlap witharXiv:1307.4847"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1307.4847", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG cs.SY", "authors": ["ian osband", "benjamin van roy", "zheng wen"], "accepted": true, "id": "1402.0635"}, "pdf": {"name": "1402.0635.pdf", "metadata": {"source": "META", "title": "Generalization and Exploration via Randomized Value Functions", "authors": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "emails": ["IOSBAND@STANFORD.EDU", "BVR@STANFORD.EDU", "ZHENGWEN207@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "The design of reinforcement learning (RL) algorithms that explore intractably large state-action spaces efficiently remains an important challenge. In this paper, we propose randomized least-squares value iteration (RLSVI), which generalizes using a linearly parameterized value function. Prior RL algorithms that generalize in this way require, in the worst case, learning times exponential in the number of model parameters and/or the planning horizon. RLSVI aims to overcome these inefficiencies.\nRLSVI operates in a manner similar to least-squares value iteration (LSVI) and also shares much of the spirit of other closely related approaches such as TD, LSTD, and SARSA (see, e.g., (Sutton & Barto, 1998; Szepesva\u0301ri, 2010)). What fundamentally distinguishes RLSVI is that the algorithm explores through randomly sampling statistically plausible value functions, whereas the aforementioned alternatives are typically applied in conjunction with action-dithering schemes such as Boltzmann or -greedy exploration, which lead to highly inefficient learning. The concept of explor-\ning by sampling statistically plausible value functions is broader than any specific algorithm, and beyond our proposal and study of RLSVI. We view an important role of this paper is to establish this broad concept as a promising approach to tackling a critical challenge in RL: synthesizing efficient exploration and effective generalization.\nWe will present computational results comparing RLSVI to LSVI with action-dithering schemes. In our case studies, these algorithms generalize using identical linearly parameterized value functions but are distinguished by how they explore. The results demonstrate that RLSVI enjoys dramatic efficiency gains. Further, we establish a bound on the expected regret for an episodic tabula rasa learning context. Our bound is O\u0303( \u221a H3SAT ), where S and A denote the cardinalities of the state and action spaces, T denotes time elapsed, and H denotes the episode duration. This matches the worst case lower bound for this problem up to logarithmic factors (Jaksch et al., 2010). It is interesting to contrast this against known O\u0303( \u221a H3S2AT ) bounds for other provably efficient tabula rasa RL algorithms (e.g., UCRL2 (Jaksch et al., 2010)) adapted to this context. To our knowledge, our results establish RLSVI as the first RL algorithm that is provably efficient in a tabula rasa context and also demonstrates efficiency when generalizing via linearly parameterized value functions.\nThere is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006). The literature on RL algorithms that generalize and explore in a provably efficient manner is sparser. There is work on model-based RL algorithms (Abbasi-Yadkori & Szepesva\u0301ri, 2011; Osband & Van Roy, 2014a;b), which apply to specific model classes and are computationally intractable. Value function generalization approaches have the potential to overcome those computational challenges and offer practical means for synthesizing efficient exploration and effective generalization. A relevant line of work establishes that efficient RL with value function generalization reduces to efficient KWIK online ar X\niv :1\n40 2.\n06 35\nv3 [\nst at\n.M L\n] 1\n5 Fe\nb 20\n16\nregression (Li & Littman, 2010; Li et al., 2008). However, it is not known whether the KWIK online regression problem can be solved efficiently. In terms of concrete algorithms, there is optimistic constraint propagation (OCP) (Wen & Van Roy, 2013), a provably efficient RL algorithm for exploration and value function generalization in deterministic systems, and C-PACE (Pazis & Parr, 2013), a provably efficient RL algorithm that generalizes using interpolative representations. These contributions represent important developments, but OCP is not suitable for stochastic systems and is highly sensitive to model mis-specification, and generalizing effectively in high-dimensional state spaces calls for methods that extrapolate. RLSVI advances this research agenda, leveraging randomized value functions to explore efficiently with linearly parameterized value functions. The only other work we know of involving exploration through random sampling of value functions is (Dearden et al., 1998). That work proposed an algorithm for tabula rasa learning; the algorithm does not generalize over the state-action space."}, {"heading": "2. Episodic reinforcement learning", "text": "A finite-horizon MDPM=(S,A,H,P,R,\u03c0), where S is a finite state space,A is a finite action space,H is the number of periods, P encodes transition probabilities, R encodes reward distributions, and \u03c0 is a state distribution. In each episode, the initial state s0 is sampled from \u03c0, and, in period h=0,1,\u00b7\u00b7\u00b7 ,H\u22121, if the state is sh and an action ah is selected then a next state sh+1 is sampled from Ph(\u00b7|sh,ah) and a reward rh is sampled from Rh(\u00b7|sh,ah,sh+1). The episode terminates when state sH is reached and a terminal reward is sampled from RH (\u00b7|sH).\nTo represent the history of actions and observations over multiple episodes, we will often index variables by both episode and period. For example, slh, alh and rlh respectively denote the state, action, and reward observed during period h in episode l.\nA policy \u00b5 = (\u00b50, \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5H\u22121) is a sequence of functions, each mapping S to A. For each policy \u00b5, we define a value function for h = 0, ..,H:\nV \u00b5h (s):=EM [\u2211H \u03c4=hr\u03c4 \u2223\u2223\u2223sh=s,a\u03c4=\u00b5\u03c4 (s\u03c4 ) for \u03c4=h,..,H\u22121] The optimal value function is defined by V \u2217h (s) =\nsup\u00b5 V \u00b5 h (s). A policy \u00b5 \u2217 is said to be optimal if V \u00b5 \u2217\n= V \u2217. It is also useful to define a state-action optimal value function for h = 0, ..,H \u2212 1:\nQ\u2217h(s, a) := EM [ rh + V \u2217 h+1(sh+1) \u2223\u2223sh = s, ah = a]"}, {"heading": "A policy \u00b5\u2217 is optimal\u21d0\u21d2 \u00b5\u2217h(s)\u2208argmax\u03b1\u2208AQ\u2217h(s,\u03b1), \u2200s,h.", "text": "A reinforcement learning algorithm generates each action alh based on observations made up to period h of episode l. Over\neach episode, the algorithm realizes reward \u2211H h=0rlh. One way to quantify the performance of a reinforcement learning algorithm is in terms of the expected cumulative regret over L episodes, or time T=LH , defined by\nRegret(T,M) = \u2211T/H\u22121 l=0 EM [ V \u22170 (sl0)\u2212 \u2211H h=0 rlh ] .\nConsider a scenario in which the agent models that, for each h, Q\u2217h \u2208 span [\u03a6h] for some \u03a6h \u2208 RSA\u00d7K . With some abuse of notation, we use S and A to denote the cardinalities of the state and action spaces. We refer this matrix \u03a6h as a generalization matrix and use \u03a6h(s, a) to denote the row of matrix \u03a6h associated with state-action pair (s, a). For k = 1, 2, \u00b7 \u00b7 \u00b7 ,K, we write the kth column of \u03a6h as \u03c6hk and refer to \u03c6hk as a basis function. We refer to contexts where the agent\u2019s belief is correct as coherent learning, and refer the alternative as agnostic learning."}, {"heading": "3. The problem with dithering for exploration", "text": "LSVI can be applied at each episode to estimate the optimal value function Q\u2217 from data gathered over previous episodes. To form an RL algorithm based on LSVI, we must specify how the agent selects actions. The most common scheme is to selectively take actions at random, we call this approach dithering. Appendix A presents RL algorithms resulting from combining LSVI with the most common schemes of -greedy or Boltzmann exploration.\nThe literature on efficient RL shows that these dithering schemes can lead to regret that grows exponentially in H and/or S (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Kakade, 2003). Provably efficient exploration schemes in RL require that exploration is directed towards potentially informative state-action pairs and consistent over multiple timesteps. This literature provides several more intelligent exploration schemes that are provably efficient, but most only apply to tabula rasa RL, where little prior information is available and learning is considered efficient even if the time required scales with the cardinality of the state-action space. In a sense, RLSVI represents a synthesis of ideas from efficient tabula rasa reinforcement learning and value function generalization methods.\nTo motivate some of the benefits of RLSVI, in Figure 1 we provide a simple example that highlights the failings of dithering methods. In this setting LSVI with Boltzmann or -greedy exploration requires exponentially many episodes to learn an optimal policy, even in a coherent learning context and even with a small number of basis functions.\nThis environment is made up of a long chain of states S = {1, .., N}. Each step the agent can transition left or right. Actions left are deterministic, but actions right only succeed with probability 1 \u2212 1/N , otherwise they go left. All states have zero reward except for the far rightN which\ngives a reward of 1. Each episode is of length H = N \u2212 1 and the agent will begin each episode at state 1. The optimal policy is to go right at every step to receive an expected reward of p\u2217 = (1\u2212 1N )\nN\u22121 each episode, all other policies give no reward. Example 1 establishes that, for any choice of basis function, LSVI with any -greedy or Boltzmann exploration will lead to regret that grows exponentially in S. A similar result holds for policy gradient algorithms.\nExample 1. Let l\u2217 be the first episode during which state N is visited. It is easy to see that \u03b8lh = 0 for all h and all l < l\u2217. Furthermore, with either -greedy or Boltzmann exploration, actions are sampled uniformly at random over episodes l < l\u2217. Thus, in any episode l < l\u2217, the red node will be reached with probability p\u22172\u2212(S\u22121) = p\u22172\u2212H . It follows that E[l\u2217] \u2265 2S\u22121 \u2212 1 and lim infT\u2192\u221eRegret(T,M) \u2265 2S\u22121 \u2212 1."}, {"heading": "4. Randomized value functions", "text": "We now consider an alternative approach to exploration that involves randomly sampling value functions rather than actions. As a specific scheme of this kind, we propose randomized least-squares value iteration (RLSVI), which we present as Algorithm 1.1 To obtain an RL algorithm, we simply select greedy actions in each episode, as specified in Algorithm 2.\nThe manner in which RLSVI explores is inspired by Thompson sampling (Thompson, 1933), which has been shown to explore efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014). In Thompson sampling, the agent samples from a posterior distribution over models, and selects the action that optimizes the sampled model. RLSVI similarly samples from a distribution over plausible value functions and selects actions that optimize resulting samples. This distribution can be thought of as an approximation to a posterior distribution over value functions. RLSVI bears a close connection to PSRL (Osband et al., 2013), which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL. PSRL satisfies regret bounds that scale with the dimensionality, rather than the cardinality, of the underlying MDP (Osband & Van Roy, 2014b;a). However, PSRL does not accommodate value function generalization without MDP planning, a feature that we expect to be of great practical importance.\n1Note that when l = 0, both A and b are empty, hence, we set \u03b8\u0303l0 = \u03b8\u0303l1 = \u00b7 \u00b7 \u00b7 = \u03b8\u0303l,H\u22121 = 0.\nAlgorithm 1 Randomized Least-Squares Value Iteration Input: Data \u03a60(si0,ai0),ri0,..,\u03a6H\u22121(siH\u22121,aiH\u22121),riH : i<L, Parameters \u03bb>0, \u03c3>0 Output: \u03b8\u0303l0,..,\u03b8\u0303l,H\u22121\n1: for h=H\u22121,..,1,0 do 2: Generate regression problem A\u2208Rl\u00d7K , b\u2208Rl:\nA\u2190  \u03a6h(s0h,a0h)... \u03a6h(sl\u22121,h,al\u22121,h)  bi\u2190 { rih+max\u03b1 ( \u03a6h+1\u03b8\u0303l,h+1 ) (si,h+1,\u03b1) if h<H\u22121\nrih+ri,h+1 if h=H\u22121\n3: Bayesian linear regression for the value function\n\u03b8lh\u2190 1\n\u03c32\n( 1\n\u03c32 A>A+\u03bbI\n)\u22121 A>b\n\u03a3lh\u2190 ( 1\n\u03c32 A>A+\u03bbI )\u22121 4: Sample \u03b8\u0303lh\u223cN(\u03b8lh,\u03a3lh) from Gaussian posterior 5: end for\nAlgorithm 2 RLSVI with greedy action Input: Features \u03a60,..,\u03a6H\u22121; \u03c3>0, \u03bb>0\n1: for l=0,1,.. do 2: Compute \u03b8\u0303l0,..,\u03b8\u0303l,H\u22121 using Algorithm 1 3: Observe sl0 4: for h=0,..,H\u22121 do 5: Sample alh\u2208argmax\u03b1\u2208A ( \u03a6h\u03b8\u0303lh ) (slh,\u03b1) 6: Observe rlh and sl,h+1 7: end for 8: Observe rlH 9: end for"}, {"heading": "5. Provably efficient tabular learning", "text": "RLSVI is an algorithm designed for efficient exploration in large MDPs with linear value function generalization. So far, there are no algorithms with analytical regret bounds in this setting. In fact, most common methods are provably inefficient, as demonstrated in Example 1, regardless of the choice of basis function. In this section we will establish an expected regret bound for RLSVI in a tabular setting without generalization where the basis functions \u03a6h = I .\nThe bound is on an expectation with respect to a probability space (\u2126,F ,P). We define the MDP M = (S,A, H, P,R, \u03c0) and all other random variables we will consider with respect to this probability space. We assume that S, A, H , and \u03c0, are deterministic and that R and P are drawn from a prior distribution. We will assume that\nrewards R(s, a, h) are drawn from independent Dirichlet \u03b1R(s, a, h) \u2208 R2+ with values on {\u22121, 0} and transitions Dirichlet \u03b1P (s, a, h) \u2208 RS+. Analytical techniques exist to extend similar results to general bounded distributions; see, for example (Agrawal & Goyal, 2012).\nTheorem 1. If Algorithm 1 is executed with \u03a6h=I for h= 0,..,H\u22121, \u03bb\u2265max(s,a,h) ( 1T\u03b1R(s,a,h)+1T\u03b1P (s,a,h) ) and \u03c3\u2265 \u221a H2+1, then:\nE [Regret(T,M)] \u2264 O\u0303 (\u221a H3SAT )\n(1)\nSurprisingly, these scalings better state of the art optimistic algorithms specifically designed for efficient analysis which would admit O\u0303( \u221a H3S2AT ) regret (Jaksch et al., 2010). This is an important result since it demonstrates that RLSVI can be provably-efficient, in contrast to popular dithering approaches such as -greedy which are provably inefficient."}, {"heading": "5.1. Preliminaries", "text": "Central to our analysis is the notion of stochastic optimism, which induces a partial ordering among random variables.\nDefinition 1. For any X and Y real-valued random variables we say thatX is stochastically optimistic for Y if and only if for any u:R\u2192R convex and increasing\nE[u(X)] \u2265 E[u(Y )].\nWe will use the notation X <so Y to express this relation. It is worth noting that stochastic optimism is closely connected with second-order stochastic dominance: X <so Y if and only if \u2212Y second-order stochastically dominates \u2212X (Hadar & Russell, 1969). We repoduce the following result which establishes such a relation involving Gaussian and Dirichlet random variables in Appendix G.\nLemma 1. For all V \u2208 [0, 1]N and \u03b1 \u2208 [0,\u221e)N with \u03b1T1 \u2265 2, if X \u223c N(\u03b1>V/\u03b1>1, 1/\u03b1>1) and Y = PTV for P \u223c Dirichlet(\u03b1) then X <so Y ."}, {"heading": "5.2. Proof sketch", "text": "Let Q\u0303lh = \u03a6h\u03b8\u0303lh and \u00b5\u0303l denote the value function and policy generated by RLSVI for episode l and let V\u0303 lh(s) = maxa Q\u0303 l h(s, a). We can decompose the per-episode regret\nV \u22170 (sl0)\u2212 V \u00b5\u0303l 0 (sl0) = V\u0303 l 0 (sl0)\u2212V \u00b5\u0303l0 (sl0)\ufe38 \ufe37\ufe37 \ufe38\n\u2206concl\n+ V \u22170 (sl0)\u2212V\u0303 l0 (sl0)\ufe38 \ufe37\ufe37 \ufe38 \u2206optl .\nWe will bound this regret by first showing that RLSVI generates optimistic estimates of V \u2217, so that \u2206optl has nonpositive expectation for any history Hl available prior to episode l. The remaining term \u2206concl vanishes as estimates generated by RLSVI concentrate around V \u2217.\nLemma 2. Conditional on any data H, the Q-values generated by RLSVI are stochastically optimistic for the true Q-values Q\u0303lh(s, a) <so Q \u2217 h(s, a) for all s, a, h.\nProof. Fix any data Hl available and use backwards induction on h = H \u2212 1, .., 1. For any (s, a, h) we write n(s, a, h) for the amount of visits to that datapoint in Hl. We will write R\u0302(s, a, h), P\u0302 (s, a, h) for the empirical mean reward and mean transitions based upon the data Hl. We can now write the posterior mean rewards and transitions:\nR(s, a, h)|Hl = \u22121\u00d7 \u03b1R1 (s, a, h) + n(s, a, h)R\u0302(s, a, h)\n1T\u03b1R(s, a, h) + n(s, a, h)\nP (s, a, h)|Hl = \u03b1P (s, a, h) + n(s, a, h)P\u0302 (s, a, h)\n1T\u03b1P (s, a, h) + n(s, a, h)\nNow, using \u03a6h = I for all (s, a, h) we can write the RLSVI updates in similar form. Note that, \u03a3lh is diagonal with each diagonal entry equal to \u03c32/(n(s, a, h) + \u03bb\u03c32). In the case of h = H \u2212 1\n\u03b8 l H\u22121(s, a) = n(s, a,H \u2212 1)R\u0302(s, a,H \u2212 1)\nn(s, a,H \u2212 1) + \u03bb\u03c32\nUsing the relation that R\u0302 \u2265 R Lemma 1 means that\nN(\u03b8 l H\u22121(s, a), 1\nn(s, a, h) + 1T\u03b1R(s, a, h) ) <so RH\u22121|Hl.\nTherefore, choosing \u03bb > maxs,a,h 1T\u03b1R(s, a, h) and \u03c3 > 1, we must satisfy the lemma for all s, a and h = H \u2212 1.\nFor the inductive step we assume that the result holds for all s, a and j > h, we now want to prove the result for all (s, a) at timestep h. Once again, we can express \u03b8 l\nh(s, a) in closed form.\n\u03b8 l h(s, a) = n(s, a, h)\n( R\u0302(s, a, h) + P\u0302 (s, a, h)T V\u0303 lh+1 ) n(s, a, h) + \u03bb\u03c32\nTo simplify notation we omit the arguments (s, a, h) where they should be obvious from context. The posterior mean estimate for the next step value V \u2217h , conditional onHl:\nE[Q\u2217h(s, a)|Hl] = R+ P T V \u2217h+1 \u2264\nn(R\u0302+ P\u0302TV \u2217h+1)\nn+ \u03bb\u03c32 .\nAs long as \u03bb > 1T\u03b1R + 1T (\u03b1P ) and \u03c32 > H2. By our induction process V\u0303 lh+1 <so V \u2217 h+1 so that\nE[Q\u2217h(s, a)|Hl] \u2264 E\n[ n(R\u0302+ P\u0302T V\u0303 lh+1)\nn+ \u03bb\u03c32 | Hl\n] .\nWe can conclude by Lemma 1 and noting that the noise from rewards is dominated by N(0, 1) and the noise from transitions is dominated by N(0, H2). This requires that \u03c32 \u2265 H2 + 1.\nLemma 2 means RLSVI generates stochastically optimistic Q-values for any history Hl. All that remains is to prove the remaining estimates E[\u2206concl |Hl] concentrate around the true values with data. Intuitively this should be clear, since the size of the Gaussian perturbations decreases as more data is gathered. In the remainder of this section we will sketch this result.\nThe concentration error \u2206concl = V\u0303 l 0 (sl0) \u2212 V \u00b5\u0303l 0 (sl0). We decompose the value estimate V\u0303 l0 explicitly:\nV\u0303 l0 (sl0) = n(R\u0302+ P\u0302T V\u0303 lh+1)\nn+ \u03bb\u03c32 + w\u03c3\n= R+ P T V\u0303 lh+1 + b R + bP + w\u03c30\nwhere w\u03c3 is the Gaussian noise from RLSVI and bR = bR(sl0, al00), b\nP = bP (sl0, al00) are optimistic bias terms for RLSVI. These terms emerge since RLSVI shrinks estimates towards zero rather than the Dirichlet prior for rewards and transitions.\nNext we note that, conditional on Hl we can rewrite P T V\u0303 lh+1 = V\u0303 l h+1(s\n\u2032) + dh where s\u2032 \u223c P \u2217(s, a, h) and dh is some martingale difference. This allows us to decompose the error in our policy to the estimation error of the states and actions we actually visit. We also note that, conditional on the data Hl the true MDP is independent of the sampling process of RLSVI. This means that:\nE[V \u00b5\u0303l0 (sl0)|Hl] = R+ P T V \u00b5\u0303lh+1.\nOnce again, we can replace this transition term with a single sample s\u2032 \u223c P \u2217(s, a, h) and a martingale difference. Combining these observations allows us to reduce the concentration error\nE[V\u0303 l0 (sl0)\u2212 V \u00b5\u0303l 0 (sl0)|Hl] =\nH\u22121\u2211 h=0 { bR(slh, alh, h) + b P (slh, alh, h) + w \u03c3 h } .\nWe can even write explicit expressions for bR, bP and w\u03c3 .\nbR(s, a, h) = nR\u0302 n+ \u03bb\u03c32 \u2212 nR\u0302\u2212 \u03b1 R 1 n+ 1T\u03b1R\nbP (s, a, h) = nP\u0302T V\u0303 lh+1 n+ \u03bb\u03c32 \u2212 (nP\u0302 + \u03b1P )T V\u0303 lh+1 n+ 1T\u03b1P\nw\u03c3h \u223c N ( 0, \u03c32\nn+ \u03bb\u03c32 ) The final details for this proof are technical but the argument is simple. We let \u03bb=1T\u03b1R+1T\u03b1P and \u03c3=\u221a H2+1. Up to O\u0303 notation bR' \u03b1 R 1\nn+1T\u03b1P , bP' H1\nT\u03b1P n+1T\u03b1P\nand w\u03c3h' H\u221an+H21T\u03b1R+1T\u03b1P . Summing using a pigeonhole principle for \u2211 s,a,hn(s,a,h)=T gives us an\nupper bound on the regret. We write K(s,a,h):=( \u03b1R1 (s,a,h)+H1 T\u03b1P (s,a,h) ) to bound the effects of the\nprior mistmatch in RLSVI arising from the bias terms bR, bP . The constraint \u03b1T1 \u2265 2 can only be violated twice for each s, a, h. Therefore up to O(\u00b7) notation:\nE [\u2211T/H\u22121\nl=0 E[\u2206 conc l |Hl] ] \u2264 2SAH+\u2211\ns,a,hK(s,a,h)log(T+K(s,a,h))+H \u221a SAHT log(T )\nThis completes the proof of Theorem 1."}, {"heading": "6. Experiments", "text": "Our analysis in Section 5 shows that RLSVI with tabular basis functions acts as an effective Gaussian approximation to PSRL. This demonstrates a clear distinction between exploration via randomized value functions and dithering strategies such as Example 1. However, the motivating for RLSVI is not for tabular environments, where several provably efficient RL algorithms already exist, but instead for large systems that require generalization.\nWe believe that, under some conditions, it may be possible to establish polynomial regret bounds for RLSVI with value function generalization. To stimulate thinking on this topic we present a conjecture of result what may be possible in Appendix B. For now, we will present a series of experiments designed to test the applicability and scalability of RLSVI for exploration with generalization.\nOur experiments are divided into three sections. First, we present a series of didactic chain environments similar to Figure 1. We show that RLSVI can effectively synthesize exploration with generalization with both coherent and agnostic value functions that are intractable under any dithering scheme. Next, we apply our Algorithm to learning to play Tetris. We demonstrate that RLSVI leads to faster learning, improved stability and a superior learned policy in a large-scale video game. Finally, we consider a business application with a simple model for a recommendation system. We show that an RL algorithm can improve upon even the optimal myopic bandit strategy. RLSVI learns this optimal strategy when dithering strategies do not."}, {"heading": "6.1. Testing for efficient exploration", "text": "We now consider a series of environments modelled on Example 1, where dithering strategies for exploration are provably inefficient. Importantly, and unlike the tabular setting of Section 5, our algorithm will only interact with the MDP but through a set of basis function \u03a6 which generalize across states. We examine the empirical performance of RLSVI and find that it does efficiently balance exploration and generalization in this didactic example."}, {"heading": "6.1.1. COHERENT LEARNING", "text": "In our first experiments, we generate a random set ofK basis functions. This basis is coherent but the individual basis functions are not otherwise informative. We form a ran-\ndom linear subspace VhK spanned by (1,Q\u2217h,w\u03031,..,w\u0303k\u22122). Here wi and w\u0303i are IID Gaussian \u223cN(0,I)\u2208RSA. We then form \u03a6h by projecting (1,w1,..,wk\u22121) onto VhK and renormalize each component to have equal 2-norm2. Figure 2 presents the empirical regret for RLSVI withK=10,N= 50,\u03c3=0.1,\u03bb=1 and an -greedy agent over 5 seeds3.\nFigure 1 shows that RLSVI consistently learns the optimal policy in roughly 500 episodes. Any dithering strategy would take at least 1015 episodes for this result. The state of the art upper bounds for the efficient optimistic algorithm UCRL given by appendix C.5 in (Dann & Brunskill, 2015) for H = 15,S= 6,A= 2, = 1,\u03b4= 1 only kick in after more than 1010 suboptimal episodes. RLSVI is able to effectively exploit the generalization and prior structure from the basis functions to learn much faster.\nWe now examine how learning scales as we change the chain length N and number of basis functions K. We observe that RLSVI essentially maintains the optimal policy once it discovers the rewarding state. We use the number of episodes until 10 rewards as a proxy for learning time. We report the average of five random seeds.\nFigure 3 examines the time to learn as we vary the chain length N with fixed K=10 basis functions. We include the dithering lower bound 2N\u22121 as a dashed line and a lower bound scaling 110H\n2SA for tabular learning algorithms as a solid line (Dann & Brunskill, 2015). ForN=100, 2N\u22121> 1028 and H2SA>106. RLSVI demonstrates scalable generalization and exploration to outperform these bounds.\nFigure 4 examines the time to learn as we vary the basis functions K in a fixed N=50 length chain. Learning time\n2For more details on this experiment see Appendix C. 3In this setting any choice of or Boltzmann \u03b7 is equivalent.\nscales gracefully with K. Further, the marginal effect of K decrease as dim(VhK)=K approaches dim(RSA)=100. We include a local polynomial regression in blue to highlight this trend. Importantly, even for large K the performance is far superior to the dithering and tabular bounds4.\nFigure 5 examines these same scalings on a logarithmic scale. We find the data for these experiments is consistent with polynomial learning as hypothesized in Appendix B. These results are remarkably robust over several orders of magnitude in both \u03c3 and \u03bb. We present more detailed analysis of these sensitivies in Appendix C."}, {"heading": "6.1.2. AGNOSTIC LEARNING", "text": "Unlike the example above, practical RL problems will typically be agnostic. The true value function Q\u2217h will not lie within VhK . To examine RLSVI in this setting we generate basis functions by adding Gaussian noise to the true value function \u03c6hk \u223c N(Q\u2217h, \u03c1I). The parameter \u03c1 determines the scale of this noise. For \u03c1 = 0 this problem is coherent but for \u03c1 > 0 this will typically not be the case. We fix N = 20,K = 20, \u03c3 = 0.1 and \u03bb = 1.\nFor i=0,..,1000 we run RLSVI for 10,000 episodes with \u03c1=i/1000 and a random seed. Figure 6 presents the number of episodes until 10 rewards for each value of \u03c1. For large values of \u03c1, and an extremely misspecified basis, RLSVI is not effective. However, there is some region 0 < \u03c1 < \u03c1\u2217 where learning remains remarkably stable5.\nThis simple example gives us some hope that RLSVI can be\n4For chain N=50, the bounds 2N\u22121>1014 and H2SA>105. 5Note Q\u2217h(s,a)\u2208{0,1} so \u03c1=0.5 represents significant noise.\nuseful in the agnostic setting. In our remaining experiments we will demonstrate that RLSVI can acheive state of the art results in more practical problems with agnostic features."}, {"heading": "6.2. Tetris", "text": "We now turn our attention to learning to play the iconic video game Tetris. In this game, random blocks fall sequentially on a 2D grid with 20 rows and 10 columns. At each step the agent can move and rotate the object subject to the constraints of the grid. The game starts with an empty grid and ends when a square in the top row becomes full. However, when a row becomes full it is removed and all bricks above it move downward. The objective is to maximize the score attained (total number of rows removed) before the end of the game.\nTetris has been something of a benchmark problem for RL and approximate dynamic programming, with several papers on this topic (Gabillon et al., 2013). Our focus is not so much to learn a high-scoring Tetris player, but instead to demonstrate the RLSVI offers benefits over other forms of exploration with LSVI. Tetris is challenging for RL with a huge state space with more than 2200 states. In order to tackle this problem efficiently we use 22 benchmark features. These featurs give the height of each column, the absolute difference in height of each column, the maximum height of a column, the number of \u201choles\u201d and a constant. It is well known that you can find far superior linear basis functions, but we use these to mirror their approach.\nIn order to apply RLSVI to Tetris, which does not have fixed episode length, we made a few natural modifications to the algorithm. First, we approximate a timehomogeneous value function. We also only the keep most recent N=105 transitions to cap the linear growth in memory and computational requirements, similar to (Mnih, 2015). Details are provided in Appendix D. In Figure 7 we present learning curves for RLSVI \u03bb=1,\u03c3=1 and LSVI with a tuned -greedy exploration schedule6 averaged over 5 seeds. The results are significant in several ways.\nFirst, both RLSVI and LSVI make significant improvements over the previous approach of LSPI with the same\n6We found that we could not acheive good performance for any fixed . We used an annealing exploration schedule that was tuned to give good performance. See Appendix D\nbasis functions (Bertsekas & Ioffe, 1996). Both algorithms reach higher final performance (' 3500 and 4500 respectively) than the best level for LSPI (3183). They also reach this performance after many fewer games and, unlike LSPI do not \u201ccollapse\u201d after finding their peak performance. We believe that these improvements are mostly due to the memory replay buffer, which stores a bank of recent past transitions, rather than LSPI which is purely online.\nSecond, both RLSVI and LSVI learn from scratch where LSPI required a scoring initial policy to begin learning. We believe this is due to improved exploration schemes, LSPI is completely greedy so struggles to learn without an initial policy. LSVI with a tuned schedule is much better. However, we do see a significant improvement through exploration via RLSVI even when compared to the tuned scheme. More details are available in Appendix D."}, {"heading": "6.3. A recommendation engine", "text": "We will now show that efficient exploration and generalization can be helpful in a simple model of customer interaction. Consider an agent which recommends J \u2264 N products from Z = {1, 2, . . . , N} sequentially to a customer. The conditional probability that the customer likes a product depends on the product, some items are better than others. However it also depends on what the user has observed, what she liked and what she disliked. We represent the products the customer has seen by Z\u0303 \u2286 Z . For each product n \u2208 Z\u0303 we will indicate xn \u2208 {\u22121,+1} for her preferences {dislike, like} respectively. If the customer has not observed the product n /\u2208 Z\u0303 we will write xn = 0. We model the probability that the customer will like a new product a /\u2208 Z\u0303 by a logistic transformation linear in x:\nP(a|x) = 1/ (1 + exp (\u2212 [\u03b2a + \u2211 n \u03b3anxn])) . (2)\nImportantly, this model reflects that the customers\u2019 preferences may evolve as their experiences change. For example, a customer may be much more likely to watch the second season of the TV show \u201cBreaking Bad\u201d if they have watched the first season and liked it.\nThe agent in this setting is the recommendation system,\nwhose goal is to maximize the cumulative amount of items liked through time for each customer. The agent does not know p(a|x) initially, but can learn to estimate the parameters \u03b2, \u03b3 through interactions across different customers. Each customer is modeled as an episode with horizon length H = J with a \u201ccold start\u201d and no previous observed products Z\u0303 = \u2205. For our simulations we set \u03b2a = 0 \u2200a and sample a random problem instance by sampling \u03b3an \u223c N(0, c2) independently for each a and n.\nAlthough this setting is simple, the number of possible states |S| = |{\u22121, 0,+1}|H = 3J is exponential in J . To learn in time less than |S| it is crucial that we can exploit generalization between states as per equation (2). For this problem we constuct the following simple basis functions: \u22001 \u2264 n,m, a \u2264 N , let \u03c6m(x, a) = 1{a = m} and \u03c6mn(x, a) = xn1{a = m}. In each period h form \u03a6h = ((\u03c6n)n, (\u03c6m)m). The dimension of our function class K = N2 +N is exponentially smaller than the number of states. However, barring a freak event, this simple basis will lead to an agnostic learning problem.\nFigure 8 and 9 show the performance of RLSVI compared to several benchmark methods. In Figure 8 we plot the cumulative regret of RLSVI when compared against LSVI with Boltzmann exploration and identical basis features. We see that RLSVI explores much more efficiently than Boltzmann exploration over a wide range of temperatures.\nIn Figure 9 we show that, using this efficient exploration method, the reinforcement learning policy is able to outperform not only benchmark bandit algorithms but even\nthe optimal myopic policy7. Bernoulli Thompson sampling does not learn much even after 1200 episodes, since the algorithm does not take context into account. The linear contextual bandit outperforms RLSVI at first. This is not surprising, since learning a myopic policy is simpler than a multi-period policy. However as more data is gathered RLSVI eventually learns a richer policy which outperforms the myopic policy.\nAppendix E provides pseudocode for this computational study. We set N = 10, H = J = 5, c = 2 and L = 1200. Note that such problems have |S| = 4521 states; this allows us to solve each MDP exactly so that we can compute regret. Each result is averaged over 100 problem instances and for each problem instance, we repeat simulations 10 times. The cumulative regret for both RLSVI (with \u03bb = 0.2 and \u03c32 = 10\u22123) and LSVI with Boltzmann exploration (with \u03bb = 0.2 and a variety of \u201ctemperature\u201d settings \u03b7) are plotted in Figure 8. RLSVI clearly outperforms LSVI with Boltzmann exploration.\nOur simulations use an extremely simplified model. Nevertheless, they highlight the potential value of RL over multiarmed bandit approaches in recommendation systems and other customer interactions. An RL algorithm may outperform even even an optimal myopic system, particularly where large amounts of data are available. In some settings, efficient generalization and exploration can be crucial."}, {"heading": "7. Closing remarks", "text": "We have established a regret bound that affirms efficiency of RLSVI in a tabula rasa learning context. However the real promise of RLSVI lies in its potential as an efficient method for exploration in large-scale environments with generalization. RLSVI is simple, practical and explores efficiently in several environments where state of the art approaches are ineffective.\nWe believe that this approach to exploration via randomized value functions represents an important concept beyond our specific implementation of RLSVI. RLSVI is designed for generalization with linear value functions, but many of the great successes in RL have come with highly nonlinear \u201cdeep\u201d neural networks from Backgammon (Tesauro, 1995) to Atari8 (Mnih, 2015). The insights and approach gained from RLSVI may still be useful in this nonlinear setting. For example, we might adapt RLSVI to instead take approximate posterior samples from a nonlinear value function via a nonparametric bootstrap (Osband & Van Roy, 2015).\n7The optimal myopic policy knows the true model defined in Equation 2, but does not plan over multiple timesteps.\n8Interestingly, recent work has been able to reproduce similar performance using linear value functions (Liang et al., 2015)."}, {"heading": "APPENDICES", "text": ""}, {"heading": "A. LSVI with Boltzmann exploration/ -greedy exploration", "text": "The LSVI algorithm iterates backwards over time periods in the planning horizon, in each iteration fitting a value function to the sum of immediate rewards and value estimates of the next period. Each value function is fitted via least-squares: note that vectors \u03b8lh satisfy\n\u03b8lh \u2208 arg min \u03b6\u2208RK\n( \u2016A\u03b6 \u2212 b\u20162 + \u03bb\u2016\u03b6\u20162 ) . (3)\nNotice that in Algorithm 3, when l = 0, matrix A and vector b are empty. In this case, we simply set \u03b8l0 = \u03b8l1 = \u00b7 \u00b7 \u00b7 = \u03b8l,H\u22121 = 0.\nAlgorithm 3 Least-Squares Value Iteration Input: Data \u03a6(si0,ai0),ri0,..,\u03a6(siH\u22121,aiH\u22121),riH : i<L . Parameter \u03bb>0 Output: \u03b8l0,...,\u03b8l,H\u22121\n1: \u03b8lH\u21900, \u03a6H\u21900 2: for h=H\u22121,...,1,0 do 3: Generate regression problem A\u2208Rl\u00d7K , b\u2208Rl:\nA\u2190  \u03a6h(s0h,a0h)... \u03a6h(sl\u22121,h,al\u22121,h)  bi\u2190 { rih+max\u03b1 ( \u03a6h+1\u03b8\u0303l,h+1 ) (si,h+1,\u03b1) if h<H\u22121\nrih+ri,h+1 if h=H\u22121\n4: Linear regression for value function\n\u03b8lh\u2190(A>A+\u03bbI)\u22121A>b\n5: end for\nRL algorithms produced by synthesizing Boltzmann exploration or -greedy exploration with LSVI are presented as Algorithms 4 and 5. In these algorithms the \u201ctemperature\u201d parameters \u03b7 in Boltzmann exploration and in -greedy exploration control the degree to which random perturbations distort greedy actions.\nAlgorithm 4 LSVI with Boltzmann exploration Input: Features \u03a60,..,\u03a6H\u22121; \u03b7>0, \u03bb>0\n1: for l=0,1,\u00b7\u00b7\u00b7 do 2: Compute \u03b8l0,...,\u03b8l,H\u22121 based on Algorithm 3 3: Observe xl0 4: for h=0,1,...,H\u22121 do 5: Sample alh\u223cE[(\u03a6h\u03b8lh)(xlh,a)/\u03b7] 6: Observe rlh and xl,h+1 7: end for 8: end for\nAlgorithm 5 LSVI with -greedy exploration Input: Features \u03a60,..,\u03a6H\u22121; >0, \u03bb>0\n1: for l=0,1,... do 2: Compute \u03b8l0,...,\u03b8l,H\u22121 using Algorithm 3 3: Observe xl0 4: for h=0,1,\u00b7\u00b7\u00b7,H\u22121 do 5: Sample \u03be\u223cBernoulli( ) 6: if \u03be=1 then 7: Sample alh\u223cunif(A) 8: else 9: Sample alh\u2208argmax\u03b1\u2208A(\u03a6h\u03b8lh)(xlh,\u03b1)\n10: end if 11: Observe rlh and xl,h+1 12: end for 13: end for"}, {"heading": "B. Efficient exploration with generalization", "text": "Our computational results suggest that, when coupled with generalization, RLSVI enjoys levels of efficiency far beyond what can be achieved by Boltzmann or -greedy exploration. We leave as an open problem establishing efficiency guarantees in such contexts. To stimulate thinking on this topic, we put forth a conjecture.\nConjecture 1. For all M = (S,A, H, P,R, \u03c0), \u03a60, . . . ,\u03a6H\u22121, \u03c3, and \u03bb, if reward distributions R have support [\u2212\u03c3, \u03c3], there is a unique (\u03b80, . . . , \u03b8H\u22121) \u2208 RK\u00d7H satisfying Q\u2217h = \u03a6h\u03b8h for h = 0, . . . ,H \u2212 1, and\u2211H\u22121 h=0 \u2016\u03b8h\u20162 \u2264 KH \u03bb , then there exists a polynomial poly such that\nRegret(T,M) \u2264 \u221a T poly ( K,H,max\nh,x,a \u2016\u03a6h(x, a)\u2016, \u03c3, 1/\u03bb\n) .\nAs one would hope for from an RL algorithm that generalizes, this bound does not depend on the number of states or actions. Instead, there is a dependence on the number of basis functions. In Appendix C we present empirical results that are consistent with this conjecture."}, {"heading": "C. Chain experiments", "text": "C.1. Generating a random coherent basis\nWe present full details for Algorithm 6, which generates the random coherent basis functions \u03a6h \u2208 RSA\u00d7K for h = 1, ..,H . In this algorithm we use some standard notation for indexing vector elements. For any A \u2208 Rm\u00d7n we will write A[i, j] for the element in the ith row and jth column. We will use the placeholder \u00b7 to repesent the entire axis so that, for example, A[\u00b7, 1] \u2208 Rn is the first column of A.\nAlgorithm 6 Generating a random coherent basis Input: S,A,H,K \u2208 N, Q\u2217h \u2208 RSA for h = 1, ..,H Output: \u03a6h \u2208 RSA\u00d7K for h = 1, ..,H\n1: Sample \u03a8 \u223c N(0, I) \u2208 RHSA\u00d7K 2: Set \u03a8[\u00b7, 1]\u2190 1 3: Stack Q\u2217 \u2190 (Q\u22171, .., Q\u2217h) \u2208 RHSA 4: Set \u03a8[\u00b7, 2]\u2190 Q\u2217 5: Form projection P \u2190 \u03a8(\u03a8T\u03a8)\u22121\u03a8T 6: Sample W \u223c N(0, I) \u2208 RHSA\u00d7K 7: Set W [\u00b7, 1]\u2190 1 8: Project WP \u2190 PW \u2208 RHSA\u00d7K 9: Scale WP [\u00b7, k]\u2190 WP [\u00b7,k]\u2016WP [\u00b7,k]\u20162HSA for k = 1, ..,K\n10: Reshape \u03a6\u2190 reshape(WP ) \u2208 RH\u00d7SA\u00d7K 11: Return \u03a6[h, \u00b7, \u00b7] \u2208 RSA\u00d7K for h = 1, ..,H The reason we rescale the value function in step (9) of Algorithm 6 is so that the resulting random basis functions are on a similar scale to Q\u2217. This is a completely arbitrary choice as any scaling in \u03a6 can be exactly replicated by similar rescalings in \u03bb and \u03c3."}, {"heading": "C.2. Robustness to \u03bb, \u03c3", "text": "In Figures 10 and 11 we present the cumulative regret for N = 50,K = 10 over the first 10000 episodes for several orders of magnitude for \u03c3 and \u03bb. For most combinations of parameters the learning remains remarkably stable.\nWe find that large values of \u03c3 lead to slowers learning, since the Bayesian posterior concentrates only very slowly with new data. However, in stochastic domains we found that choosing a \u03c3 which is too small might cause the RLSVI posterior to concentrate too quickly and so fail to sufficiently explore. This is a similar insight to previous analyses of Thompson sampling (Agrawal & Goyal, 2012) and matches the flavour of Theorem 1.\nC.3. Scaling with number of bases K In Figure 4 we demonstrated that RLSVI seems to scale gracefully with the number of basis features on a chain of length N = 50. In Figure 13 we reproduce these reults for chains of several different lengths. To highlight the overall trend we present a local polynomial regression for each chain length.\nRoughly speaking, for low numbers of featuresK the number of episodes required until learning appears to increase linearly with the number of basis features. However, the marginal increase from a new basis features seems to decrease and almost plateau once the number of features reaches the maximum dimension for the problemK \u2265 SA.\nC.4. Approximate polynomial learning Our simulation results empirically demonstrate learning which appears to be polynomial in both N and K. Inspired by the results in Figure 5, we present the learning times for different N and K together with a quadratic regression fit separately for each K."}, {"heading": "D. Tetris experiments", "text": ""}, {"heading": "D.1. Algorithm specification", "text": "In Algorithm 7 we present a natural adaptation to RLSVI without known episode length, but still a regular episodic structure. This is the algorithm we use for our experiments in Tetris. The LSVI algorithms are formed in the same way.\nAlgorithm 7 Stationary RLSVI Input: Data \u03a6(s1,a1),r1,..,\u03a6(sT ,aT ) . Previous estimate \u03b8\u0303\u2212l \u2261 \u03b8\u0303l\u22121 . Parameters \u03bb>0, \u03c3>0, \u03b3\u2208 [0,1] Output: \u03b8\u0303l\n1: Generate regression problem A\u2208RT\u00d7K , b\u2208RT :\nA\u2190  \u03a6h(s1,a1)... \u03a6h(sT ,aT )  bi\u2190 { ri+\u03b3max\u03b1 ( \u03a6\u03b8\u0303\u2212l ) (si+1,\u03b1) if si not terminal\nri if si is terminal\n2: Bayesian linear regression for the value function\n\u03b8l\u2190 1\n\u03c32\n( 1\n\u03c32 A>A+\u03bbI\n)\u22121 A>b\n\u03a3l\u2190 ( 1\n\u03c32 A>A+\u03bbI )\u22121 3: Sample \u03b8\u0303l\u223cN(\u03b8l,\u03a3l) from Gaussian posterior\nAlgorithm 8 RLSVI with greedy action Input: Features \u03a6; \u03bb>0, \u03c3>0, \u03b3\u2208 [0,1]\n1: \u03b8\u22120 \u21900; t\u21900 2: for Episode l=0,1,.. do 3: Compute \u03b8\u0303l using Algorithm 7 4: Observe st 5: while TRUE do 6: Update t\u2190 t+1 7: Sample at\u2208argmax\u03b1\u2208A ( \u03a6\u03b8\u0303 ) (st,\u03b1) 8: Observe rt and st+1 9: if st+1 is terminal then\n10: BREAK 11: end if 12: end while 13: end for\nThis algorithm simply approximates a time-homogenous value function using Bayesian linear regression. We found that a discount rate of \u03b3 = 0.99 was helpful for stability in both RLSVI and LSVI.\nIn order to avoid growing computational and memory cost as LSVI collects more data we used a very simple strategy\nto only store the most recent N transitions. For our experiments we set N = 105. Computation for RLSVI and LSVI remained negligible compared to the cost of running the Tetris simulator for our implementations.\nTo see how small this memory requirement is note that, apart from the number of holes, every feature and reward is a positive integer between 0 and 20 inclusive. The number of holes is a positive integer between 0 and 199. We could store the information 105 transitions for every possible action using less than 10mb of memory."}, {"heading": "D.2. Effective improvements", "text": "We present the results for RLSVI with fixed \u03c3 = 1 and \u03bb = 1. This corresponds to a Bayesian linear regression with a known noise variance in Algorithm 7. We actually found slightly better performance using a Bayesian linear regression with an inverse gamma prior over an unknown variance. This is the conjugate prior for Gaussian regression with known variance. Since the improvements were minor and it slightly complicates the algorithm we omit these results. However, we believe that using a wider prior over the variance will be more robust in application, rather than picking a specific \u03c3 and \u03bb."}, {"heading": "D.3. Mini-tetris", "text": "In Figure 7 we show that RLSVI outperforms LSVI even with a highly tuned annealing scheme for . However, these results are much more extreme on a didactic version of mini-tetris. We make a tetris board with only 4 rows and only S, Z pieces. This problem is much more difficult and highlights the need for efficient exploration in a more extreme way.\nIn Figure 14 we present the results for this mini-tetris environment. As expected, this example highlights the benefits of RLSVI over LSVI with dithering. RLSVI greatly outperforms LSVI even with a tuned schedule. RLSVI learns faster and reaches a higher convergent policy."}, {"heading": "E. Recommendation system experiments", "text": ""}, {"heading": "E.1. Experiment Setup", "text": "For the recommendation system experiments, the experiment setup is specified in Algorithm 9. We set N = 10, J = H = 5, c = 2 and L = 1200.\nAlgorithm 9 Recommendation System Experiments: Experiment Setup Input: N \u2208 Z++, J = H \u2208 Z++, c > 0, L \u2208 Z++ Output: \u2206\u0302(0), . . . , \u2206\u0302(L\u2212 1)\nfor i = 1, . . . , 100 do Sample a problem instance \u03b3an \u223c N(0, c2) Run the Bernoulli bandit algorithm 100 times Run the linear contextual bandit algorithm 100 times for for each \u03b7 \u2208 {10\u22124, 10\u22123, 10\u22122, 10\u22121, 1, 10} do\nRun LSVI-Boltzmann with \u03bb = 0.2 and \u03b7 10 times end for Run RLSVI with \u03bb = 0.2 and \u03c32 = 10\u22123 10 times\nend for Compute the average regret for each algorithm\nThe myopic policy is defined as follows: for all episode l = 0, 1, \u00b7 \u00b7 \u00b7 and for all step h = 0, \u00b7 \u00b7 \u00b7 , H \u2212 1, choose alh \u2208 arg maxa P (a|xlh), where alh and xlh are respectively the action and the state at step h of episode l.\nE.2. Bernoulli bandit algorithm\nThe Bernoulli bandit algorithm is described in Algorithm 10, which is a Thompson sampling algorithm with uniform prior. Obviously, this algorithm aims to learn the myopic policy.\nAlgorithm 10 Bernoulli bandit algorithm"}, {"heading": "Input: N \u2208 N, J \u2208 N, L \u2208 N", "text": "Initialization: Set \u03b1n = \u03b2n = 1, \u2200n = 1, 2, . . . , N for l = 0, . . . , L\u2212 1 do\nRandomly sample p\u0302ln \u223c beta (\u03b1n, \u03b2n), \u2200n = 1, . . . , N Sort p\u0302ln\u2019s in the descending order, and recommend the first J products in order to the customer for n = 1, . . . , N do\nif product n is recommended in episode l then if customer likes product then \u03b1n \u2190 \u03b1n + 1\nelse \u03b2n \u2190 \u03b2n + 1\nend if end if\nend for end for\nE.3. Linear contextual bandit algorithm\nIn this subsection, we describe the linear contextual bandit algorithm. The linear contextual bandit algorithm is similar to RLSVI, but without backward value propagation, a key feature of RLSVI. It is straightforward to see that the linear contextual bandit algorithm aims to learn the myopic policy. This algorithm is specified in Algorithm 11 and 12. Notice that this algorithm can be implemented incrementally, hence, it is computationally efficient. In this computational study, we use the same basis functions as RLSVI, and the same algorithm parameters (i.e. \u03bb = 0.2 and \u03c32 = 10\u22123).\nAlgorithm 11 Randomized exploration in linear contextual bandits Input: Data \u03a6(si0,ai0),ri0,..,\u03a6(siH\u22121,aiH\u22121),riH : i<L . Parameters \u03bb>0, \u03c3>0 Output: \u03b8\u0302l0,...,\u03b8\u0302l,H\u22121\n1: \u03b8\u0302lH\u21900, \u03a6H\u21900 2: for h=H\u22121,...,1,0 do 3: Generate regression matrix and vector\nA\u2190  \u03a6h(s0h,a0h)... \u03a6h(sl\u22121,h,al\u22121,h)  b\u2190\n r0,h... rl\u22121,h  4: Estimate value function\n\u03b8lh\u2190 1\n\u03c32\n( 1\n\u03c32 A>A+\u03bb\u03c32I\n)\u22121 A>b\n\u03a3lh\u2190 ( 1\n\u03c32 A>A+\u03bbI )\u22121 5: Sample \u03b8\u0302lh\u223cN(\u03b8lh,\u03a3lh) 6: end for\nAlgorithm 12 Linear contextual bandit algorithm Input: Features \u03a60,..,\u03a6H , \u03c3>0,\u03bb>0\n1: for l=0,1,\u00b7\u00b7\u00b7 do 2: Compute \u03b8\u0302l0,...,\u03b8\u0302l,H\u22121 using Algorithm 11 3: Observe xl0 4: for h=0,\u00b7\u00b7\u00b7,H\u22121 do 5: Sample alh\u223cunif ( argmax\u03b1\u2208A ( \u03a6h\u03b8\u0302lh ) (xlh,\u03b1)\n) 6: Observe rlh and xl,h+1 7: end for 8: end for"}, {"heading": "F. Extensions", "text": "We now briefly discuss a couple possible extensions of the version of RLSVI proposed in Algorithm 1 and 8. One is an incremental version which is computationally more efficient. The other addresses continual learning in an infinite horizon discounted Markov decision process. In the same sense that RLSVI shares much with LSVI but is distinguished by its new approach to exploration, these extensions share much with least-squares Q-learning (Lagoudakis et al., 2002).\nF.1. Incremental learning\nNote that Algorithm 1 is a batch learning algorithm, in the sense that, in each episode l, though \u03a3lh\u2019s can be computed incrementally, it needs all past observations to compute \u03b8\u0304lh\u2019s. Thus, its per-episode compute time grows with l, which is undesirable if the algorithm is applied over many episodes.\nOne way to fix this problem is to derive an incremental RLSVI that updates \u03b8\u0304lh\u2019s and \u03a3lh\u2019s using summary statistics of past data and new observations made over the most recent episode. One approach is to do this by computing\n\u03a3\u22121l+1,h \u2190 (1\u2212 \u03bdl)\u03a3 \u22121 lh +\n1\n\u03c32 \u03a6h (xlh, alh)\n> \u03a6h (xlh, alh)\nyl+1,h \u2190 (1\u2212 \u03bdl)ylh + 1\n\u03c32\n[ rlh + max\n\u03b1\u2208A\n( \u03a6h+1\u03b8\u0303l,h+1 ) (xl,h+1, \u03b1) ] \u03a6h (xlh, alh) > , (4)\nand setting \u03b8\u0304l+1,h = \u03a3\u22121l+1,hyl+1,h. Note that we sample \u03b8\u0303lh \u223c N(\u03b8\u0304lh,\u03a3lh), and initialize y0h = 0, \u03a3 \u22121 0h = \u03bbI , \u2200h. The step size \u03bdl controls the influence of past observations on \u03a3lh and \u03b8\u0304lh. Once \u03b8\u0303lh\u2019s are computed, the actions are chosen based on Algorithm 8. Another approach would be simply to approximate the solution for \u03b8lh numerically via random sampling and stochastic gradient descent similar to other works with non-linear architectures (Mnih, 2015). The per-episode compute time of these incremental algorithms are episode-independent, which allows for deployment at large scale. On the other hand, we expect the batch version of RLSVI to be more data efficient and thus incur lower regret."}, {"heading": "F.2. Continual learning", "text": "Finally, we propose a version of RLSVI for RL in infinite-horizon time-invariant discounted MDPs. A discounted MDP is identified by a sextupleM = (S,A, \u03b3, P,R, \u03c0), where \u03b3 \u2208 (0, 1) is the discount factor. S,A, P,R, \u03c0 are defined similarly with the finite horizon case. Specifically, in each time t = 0, 1, . . ., if the state is xt and an action at is selected then a subsequent state xt+1 is sampled from P (\u00b7|xt, at) and a reward rt is sampled from R (\u00b7|xt, at, xt+1). We also use V \u2217 to denote the optimal state value function, and Q\u2217 to denote the optimal action-contingent value function. Note that V \u2217 and Q\u2217 do not depend on t in this case.\nAlgorithm 13 Continual RLSVI Input: \u03b8\u0303t \u2208 RK , wt \u2208 RK , \u03a6 \u2208 R|S||A|\u00d7K , \u03c3 > 0, \u03bb > 0, \u03b3 \u2208 (0, 1), {(x\u03c4 , a\u03c4 , r\u03c4 ) : \u03c4 \u2264 t}, xt+1 Output: \u03b8\u0303t+1 \u2208 RK , wt+1 \u2208 RK\n1: Generate regression matrix and vector\nA\u2190  \u03a6(x0, a0)... \u03a6(xt, at)  b\u2190  r0 + \u03b3max\u03b1\u2208A ( \u03a6\u03b8\u0303t ) (x1, \u03b1) ...\nrt + \u03b3max\u03b1\u2208A ( \u03a6\u03b8\u0303t ) (xt+1, \u03b1)  2: Estimate value function\n\u03b8t+1 \u2190 1\n\u03c32\n( 1\n\u03c32 A>A+ \u03bbI\n)\u22121 A>b \u03a3t+1 \u2190 ( 1\n\u03c32 A>A+ \u03bbI )\u22121 3: Sample wt+1 \u223c N( \u221a 1\u2212 \u03b32wt, \u03b32\u03a3t+1) 4: Set \u03b8\u0303t+1 = \u03b8t+1 + wt+1\nSimilarly with the episodic case, an RL algorithm generates each action at based on observations made up to time t,\nincluding all states, actions, and rewards observed in previous time steps, as well as the state space S, action space A, discount factor \u03b3, and possible prior information. We consider a scenario in which the agent has prior knowledge that Q\u2217 lies within a linear space spanned by a generalization matrix \u03a6 \u2208 R|S||A|\u00d7K .\nA version of RLSVI for continual learning is presented in Algorithm 13. Note that \u03b8\u0303t and wt are values computed by the algorithm in the previous time period. We initialize \u03b8\u03030 = 0 and w0 = 0. Similarly to Algorithm 1, Algorithm 13 randomly perturbs value estimates in directions of significant uncertainty to incentivize exploration. Note that the random perturbation vectors wt+1 \u223c N( \u221a 1\u2212 \u03b32wt, \u03b32\u03a3t+1) are sampled to ensure autocorrelation and that marginal covariance matrices of consecutive perturbations differ only slightly. In each period t, once \u03b8\u0303t is computed, a greedy action is selected. Avoiding frequent abrupt changes in the perturbation vector is important as this allows the agent to execute on multi-period plans to reach poorly understood state-action pairs."}, {"heading": "G. Gaussian vs Dirichlet optimism", "text": "The goal of this subsection is to prove Lemma 1, reproduced below:\nFor all v \u2208 [0, 1]N and \u03b1 \u2208 [1,\u221e)N with \u03b1T1 \u2265 2, if x \u223c N(\u03b1>v/\u03b1>1, 1/\u03b1>1) and y = pT v for p \u223c Dirichlet(\u03b1) then x <so y.\nWe begin with a lemma recapping some basic equivalences of stochastic optimism.\nLemma 3 (Optimism equivalence). The following are equivalent to X <so Y :\n1. For any random variable Z independent of X and Y , E[max(X,Z)] \u2265 E[max(Y,Z)] 2. For any \u03b1 \u2208 R, \u222b\u221e \u03b1 {P(X \u2265 s)\u2212 P(Y \u2265 s)} ds \u2265 0. 3. X =D Y +A+W for A \u2265 0 and E [W |Y +A] = 0 for all values y + a. 4. For any u : R\u2192 R convex and increasing E[u(X)] \u2265 E[u(Y )]\nThese properties are well known from the theory of second order stochastic dominance (Levy, 1992; Hadar & Russell, 1969) but can be re-derived using only elementary integration by parts. X <so Y if and only if \u2212Y is second order stochastic dominant for \u2212X ."}, {"heading": "G.1. Beta vs. Dirichlet", "text": "In order to prove Lemma 1 we will first prove an intermediate result that shows a particular Beta distribution y\u0303 is optimistic for y. Before we can prove this result we first state a more basic result that we will use on Gamma distributions.\nLemma 4. For independent random variables \u03b31 \u223c Gamma(k1, \u03b8) and \u03b32 \u223c Gamma(k2, \u03b8),\nE[\u03b31|\u03b31 + \u03b32] = k1\nk1 + k2 (\u03b31 + \u03b32) and E[\u03b32|\u03b31 + \u03b32] = k2 k1 + k2 (\u03b31 + \u03b32).\nWe can now present our optimistic lemma for Beta versus Dirichlet.\nLemma 5. Let y = p>v for some random variable p \u223c Dirichlet(\u03b1) and constants v \u2208 <d and \u03b1 \u2208 Nd. Without loss of generality, assume v1 \u2264 v2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 vd. Let \u03b1\u0303 = \u2211d i=1 \u03b1i(vi \u2212 v1)/(vd \u2212 v1) and \u03b2\u0303 = \u2211d i=1 \u03b1i(vd \u2212 vi)/(vd \u2212 v1). Then, there exists a random variable p\u0303 \u223c Beta(\u03b1\u0303, \u03b2\u0303) such that, for y\u0303 = p\u0303vd + (1\u2212 p\u0303)v1, E[y\u0303|y] = E[y]. Proof. Let \u03b3i = Gamma(\u03b1, 1), with \u03b31, . . . , \u03b3d independent, and let \u03b3 = \u2211d i=1 \u03b3i, so that\np \u2261D \u03b3/\u03b3.\nLet \u03b10i = \u03b1i(vi \u2212 v1)/(vd \u2212 v1) and \u03b11i = \u03b1i(vd \u2212 vi)/(vd \u2212 v1) so that\n\u03b1 = \u03b10 + \u03b11.\nDefine independent random variables \u03b30 \u223c Gamma(\u03b10i , 1) and \u03b31 \u223c Gamma(\u03b11i , 1) so that\n\u03b3 \u2261D \u03b30 + \u03b31.\nTake \u03b30 and \u03b31 to be independent, and couple these variables with \u03b3 so that \u03b3 = \u03b30 + \u03b31. Note that \u03b2\u0303 = \u2211d i=1 \u03b1 0 i and\n\u03b1\u0303 = \u2211d i=1 \u03b1 1 i . Let \u03b3 0 = \u2211d i=1 \u03b3 0 i and \u03b3 1 = \u2211d i=1 \u03b3 1 i , so that\n1\u2212 p\u0303 \u2261D \u03b30/\u03b3 and p\u0303 \u2261D \u03b31/\u03b3.\nCouple these variables so that 1\u2212 p\u0303 = \u03b30/\u03b3 and p\u0303 = \u03b31/\u03b3. We then have E[y\u0303|y] = E[(1\u2212 p\u0303)v1 + p\u0303vd|y] = E [ v1\u03b3 0\n\u03b3 + vd\u03b3\n1\n\u03b3 \u2223\u2223\u2223y] = E [E [v1\u03b30 + vd\u03b31 \u03b3 \u2223\u2223\u2223\u03b3, y] \u2223\u2223\u2223y] = E [ v1E[\u03b3\n0|\u03b3] + vdE[\u03b31|\u03b3] \u03b3 \u2223\u2223\u2223y] = E[v1\u2211di=1E[\u03b30i |\u03b3i] + vd\u2211di=1E[\u03b31i |\u03b3i] \u03b3 \u2223\u2223\u2223y] (a) = E [ v1 \u2211d i=1 \u03b3i\u03b1 0 i /\u03b1i + vd \u2211d i=1 \u03b3i\u03b1 1 i /\u03b1i\n\u03b3\n\u2223\u2223\u2223y]\n= E\n[ v1 \u2211d i=1 \u03b3i(vi \u2212 v1) + vd \u2211d i=1 \u03b3i(vd \u2212 vi)\n\u03b3(vd \u2212 v1)\n\u2223\u2223\u2223y]\n= E [\u2211d i=1 \u03b3ivi \u03b3 \u2223\u2223\u2223y] = E[ d\u2211 i=1 pivi \u2223\u2223\u2223y] = y, where (a) follows from Lemma 4."}, {"heading": "G.2. Gaussian vs Beta", "text": "In the previous section we showed that a matched Beta distribution y\u0303 would be optimistic for the Dirichlet y. We will now show that the Normal random variable x is optimistic for y\u0303 and so complete the proof of Lemma 1, x <so y\u0303 <so y.\nUnfortunately, unlike the case of Beta vs Dirichlet it is quite difficult to show this optimism relationship between Gaussian x and Beta y\u0303 directly. Instead we make an appeal to the stronger dominance relationship of single-crossing CDFs.\nDefinition 2 (Single crossing dominance). Let X and Y be real-valued random variables with CDFs FX and FY respectively. We say that X single-crossing dominates Y if E[X] \u2265 E[Y ] and there a crossing point a \u2208 R such that:\nFX(s) \u2265 FY (s) \u21d0\u21d2 s \u2264 a. (5)\nNote that single crossing dominance implies stochastic optimism. The remainder of this section is devoted to proving that the following lemma:\nLemma 6. Let y\u0303 \u223c Beta(\u03b1, \u03b2) for any \u03b1 > 0, \u03b2 > 0 and x \u223c N ( \u00b5 = \u03b1\u03b1+\u03b2 , \u03c3 2 = 1\u03b1+\u03b2 ) . Then, x single crossing dominates y\u0303.\nTrivially, these two distributions will always have equal means so it is enough to show that their CDFs can cross at most once on (0, 1)."}, {"heading": "G.3. Double crossing PDFs", "text": "By repeated application of the mean value theorem, if we want to prove that the CDFs cross at most once on (0, 1) then it is sufficient to prove that the PDFs cross at most twice on the same interval. Our strategy will be to show via mechanical calculus that for the known densities of x and y\u0303 the PDFs cross at most twice on (0, 1). We lament that the proof as it stands is so laborious, but our attempts at a more elegant solution has so far been unsucessful. The remainder of this appendix is devoted to proving this \u201cdouble-crossing\u201d property via manipulation of the PDFs for different values of \u03b1, \u03b2.\nWe write fN for the density of the Normal x and fB for the density of the Beta y\u0303 respectively. We know that at the boundary fN (0\u2212) > fB(0\u2212) and fN (1+) > fB(1+) where the \u00b1 represents the left and right limits respectively. Since the densities are postive over the interval, we can consider the log PDFs instead.\nlB(x) = (\u03b1\u2212 1) log(x) + (\u03b2 \u2212 1) log(1\u2212 x) +KB\nlN (x) = \u2212 1\n2 (\u03b1+ \u03b2)\n( x\u2212 \u03b1\n\u03b1+ \u03b2\n)2 +KN\nSince log(x) is injective and increasing, if we could show that lN (x)\u2212 lB(x) = 0 has at most two solutions on the interval we would be done.\nInstead we will attempt to prove an even stronger condition, that l\u2032N (x)\u2212l\u2032B(x) = 0 has at most one solution in the interval. This is not necessary for what we actually want to show, but it is sufficient and easier to deal with since we can ignore the annoying constants.\nl\u2032B(x) = \u03b1\u2212 1 x \u2212 \u03b2 \u2212 1\n1\u2212 x l\u2032N (x) = \u03b1\u2212 (\u03b1+ \u03b2)x\nFinally we will consider an even stronger condition, if l\u2032\u2032N (x) \u2212 l\u2032\u2032B(x) = 0 has no solution then l\u2032B(x) \u2212 l\u2032N (x) must be monotone over the region and so it can have at most one root.\nl\u2032\u2032B(x) = \u2212 \u03b1\u2212 1 x2 \u2212 \u03b2 \u2212 1 (1\u2212 x)2\nl\u2032\u2032N (x) = \u2212(\u03b1+ \u03b2)\nSo now let us define:\nh(x) := l\u2032\u2032N (x)\u2212 l\u2032\u2032B(x) = \u03b1\u2212 1 x2 + \u03b2 \u2212 1 (1\u2212 x)2 \u2212 (\u03b1+ \u03b2) (6)\nOur goal now is to show that h(x) = 0 does not have any solutions for x \u2208 [0, 1].\nOnce again, we will look at the derivatives and analyse them for different values of \u03b1, \u03b2 > 0. h\u2032(x) = \u22122 ( \u03b1\u2212 1 x3 \u2212 \u03b2 \u2212 1 (1\u2212 x)3 )\nh\u2032\u2032(x) = 6 ( \u03b1\u2212 1 x4 + \u03b2 \u2212 1 (1\u2212 x)4 ) G.3.1. SPECIAL CASE \u03b1 > 1, \u03b2 \u2264 1\nIn this region we want to show that actually g(x) = l\u2032N (x) \u2212 l\u2032B(x) has no solutions. We follow a very similar line of argument and write A = \u03b1\u2212 1 > 0 and B = \u03b2 \u2212 1 \u2264 0 as before.\ng(x) = \u03b1\u2212 (\u03b1+ \u03b2)x+ \u03b2 \u2212 1 1\u2212 x \u2212 \u03b1\u2212 1 x\ng\u2032(x) = h(x) = A\nx2 +\nB\n(1\u2212 x)2 \u2212 (\u03b1+ \u03b2)\ng\u2032\u2032(x) = h\u2032(x) = \u22122 ( A\nx3 \u2212 B (1\u2212 x)3 ) Now since B \u2264 0 we note that g\u2032\u2032(x) \u2264 0 and so g(x) is a concave function. If we can show that the maximum of g lies below 0 then we know that there can be no roots.\nWe now attempt to solve g\u2032(x) = 0:\ng\u2032(x) = A\nx2 +\nB\n(1\u2212 x)2 = 0 =\u21d2 \u2212A/B = ( x\n1\u2212 x )2 =\u21d2 x = K\n1 +K \u2208 (0, 1)\nWhere here we write K = \u221a \u2212A/B > 0. We\u2019re ignoring the case of B = 0 as this is even easier to show separately. We now evaluate the function g at its minimum xK = K1+K and write C = \u2212B \u2265 0.\ng(xK) = (A+ 1)\u2212 (A+B + 2) K 1 +K +B(1 +K)\u2212A1 +K K\n= \u2212AK2 \u2212AK \u2212A+BK3 +BK2 +BK \u2212K2 +K = \u2212AK2 \u2212AK \u2212A\u2212 CK3 \u2212 CK2 \u2212 CK \u2212K2 +K = \u2212A(A/C)\u2212A(A/C)1/2 \u2212A\u2212 C(A/C)3/2 \u2212 C(A/C)\u2212 C(A/C)1/2 \u2212A/C + (A/C)1/2 = \u2212A2C\u22121 \u2212A3/2C\u22121/2 \u2212A\u2212A3/2C\u22121/2 \u2212A\u2212A1/2C1/2 \u2212AC\u22121 +A1/2C1/2 = \u2212A2C\u22121 \u2212 2A3/2C\u22121/2 \u2212 2A\u2212AC\u22121 \u2264 0\nTherefore we are done with this sub proof. The case of \u03b1 \u2264 1, \u03b2 > 1 can be dealt with similarly.\nG.3.2. CONVEX FUNCTION \u03b1 > 1, \u03b2 > 1, (\u03b1\u2212 1)(\u03b2 \u2212 1) \u2265 19 In the case of \u03b1, \u03b2 > 1 we know that h(x) is a convex function on (0, 1). So now if we solve h\u2032(x\u2217) = 0 and h(x\u2217) > 0 then we have proved our statement. We will write A = \u03b1\u2212 1, B = \u03b2 \u2212 1 for convenience.\nWe now attempt to solve h\u2032(x) = 0\nh\u2032(x) = A x3 \u2212 B (1\u2212 x)3 = 0\n=\u21d2 A/B = ( x\n1\u2212 x )3 =\u21d2 x = K\n1 +K \u2208 (0, 1)\nWhere for convenience we have written K = (A/B)1/3 > 0. We now evaluate the function h at its minimum xK = K1+K .\nh(xK) = A (K + 1)2\nK2 +B(K + 1)2 \u2212 (A+B + 2)\n= A(2/K + 1/K2) +B(K2 + 2K)\u2212 2 = 3(A2/3B1/3 +A1/3B2/3)\u2212 2\nSo as long as h(xK) > 0 we have shown that the CDFs are single crossing. We note a simpler characterization of A,B that guarantees this condition:\nA,B \u2265 1/3 =\u21d2 AB \u2265 1/9 =\u21d2 (A2/3B1/3 +A1/3B2/3) \u2265 2/3\nAnd so we have shown that somehow for \u03b1, \u03b2 large enough away from 1 we are OK. Certianly we have proved the result for \u03b1, \u03b2 \u2265 4/3.\nG.3.3. FINAL REGION {\u03b1 > 1, \u03b2 > 1, (\u03b1\u2212 1)(\u03b2 \u2212 1) \u2264 19}\nWe now produce a final argument that even in this remaining region the two PDFs are at most double crossing. The argument is really no different than before, the only difficulty is that it is not enough to only look at the derivatives of the log likelihoods, we need to use some bound on the normalizing constants to get our bounds. By symmetry in the problem, it will suffice to consider only the case \u03b1 > \u03b2, the other result follows similarly.\nIn this region of interest, we know that \u03b2 \u2208 (1, 43 ) and so we will make use of an upper bound to the normalizing constant of the Beta distribution, the Beta function.\nB(\u03b1, \u03b2) = \u222b 1 x=0 x\u03b1\u22121(1\u2212 x)\u03b2\u22121dx\n\u2264 \u222b 1 x=0 x\u03b1\u22121dx = 1 \u03b1 (7)\nOur thinking is that, because in B the value of \u03b2 \u2212 1 is relatively small, this approximation will not be too bad. Therefore, we can explicitly bound the log likelihood of the Beta distribution:\nlB(x) \u2265 l\u0303B(x) := (\u03b1\u2212 1) log(x) + (\u03b2 \u2212 1) log(1\u2212 x) + log(\u03b1)\nWe will now make use of a calculus argument as in the previous sections of the proof. We want to find two points x1 < x2 for which h(xi) = l\u2032\u2032N (x)\u2212 l\u2032\u2032B(x) > 0. Since \u03b1, \u03b2 > 1 we know that h is convex and so for all x /\u2208 [x1, x2] then h > 0. If we can also show that the gap of the Beta over the maximum of the normal log likelihood\nGap : lB(xi)\u2212 lN (xi) \u2265 f(xi) := l\u0303B(xi)\u2212max x lN (x) > 0 (8)\nis positive then it must mean there are no crossings over the region [x1, x2], since l\u0303B is concave and therefore totally above the maximum of lN over the whole region [x1, x2].\nNow consider the regions x \u2208 [0, x1), we know by consideration of the tails that if there is more than one root in this segment then there must be at least three crossings. If there are three crossings, then the second derivative of their difference h must have at least one root on this region. However we know that h is convex, so if we can show that h(xi) > 0 this cannot be possible. We use a similar argument for x \u2208 (x2, 1]. We will now complete this proof by lengthy amounts of calculus.\nLet\u2019s remind ourselves of the definition:\nh(x) := l\u2032\u2032N (x)\u2212 l\u2032\u2032B(x) = \u03b1\u2212 1 x2 + \u03b2 \u2212 1 (1\u2212 x)2 \u2212 (\u03b1+ \u03b2)\nFor ease of notation we will write A = \u03b1\u2212 1, B = \u03b2 \u2212 1. We note that:\nh(x) \u2265 h1(x) = A\nx2 \u2212 (A+B + 2), h(x) \u2265 h2(x) =\nB\n(1\u2212 x)2 \u2212 (A+B + 2)\nand we solve for h1(x1) = 0, h2(x2) = 0. This means that\nx1 =\n\u221a A\nA+B + 2 , x2 = 1\u2212\n\u221a B\nA+B + 2\nand clearly h(x1) > 0, h(x2) > 0. Now, if we can show that, for all possible values of A,B in this region f(xi) = lB(xi)\u2212maxx lN (x) > 0, our proof will be complete.\nWe will now write f(xi) = fi(A,B) to make the dependence on A,B more clear.\nf1(A,B) = log(1 +A) +A log\n(\u221a A\nA+B + 2\n) +B log ( 1\u2212 \u221a A\nA+B + 2\n) + 1\n2 log(2\u03c0)\u2212 1 2 log(A+B + 2)\nf2(A,B) = log(1 +A) +A log\n( 1\u2212 \u221a B\nA+B + 2\n) +B log (\u221a B\nA+B + 2\n) + 1\n2 log(2\u03c0)\u2212 1 2 log(A+B + 2)\nWe will now show that \u2202fi\u2202B \u2264 0 for all of the values in our region A > B > 0.\n\u2202f1 \u2202B = \u2212 A 2(A+B + 2) + log\n( 1\u2212 \u221a A\nA+B + 2\n) +\nB \u221a A\n2(A+B + 2)3/2 ( 1\u2212 \u221a\nA A+B+2\n) \u2212 1 2(A+B + 2)\n= 1\n2(A+B + 2)  B\u221aA\u221a A+B + 2 ( 1\u2212 \u221a A\nA+B+2\n) \u2212A\u2212 1 + log(1\u2212\u221a A\nA+B + 2\n)\n= 1\n2(A+B + 2)\n( B \u221a A\n\u221a A+B + 2\u2212 \u221a A \u2212A\u2212 1\n) + log ( 1\u2212 \u221a A\nA+B + 2\n)\n\u2264 1 2(A+B + 2)\n( \u221a B/3\n\u221a A+B + 2\u2212 \u221a A \u2212A\u2212 1\n) \u2212 \u221a\nA\nA+B + 2\n\u2264 1 2(A+B + 2)\n( 1\n3\n\u221a B\nB + 2 \u2212A\u2212 1\n) \u2212 \u221a\nA\nA+B + 2\n\u2264 \u2212 A 2(A+B + 2)\n\u2212 \u221a\nA\nA+B + 2\n\u2264 0\nand similarly,\n\u2202f2 \u2202B = \u2212A\n \u221a B A+B+2\n2B +\n1\n2(A+B + 2) + log(\u221a B A+B + 2 ) +B ( A+ 2 2B(A+B + 2) ) \u2212 1 2(A+B + 2)\n= 1\n2(A+B + 2)\n( A+ 2\u2212A\u2212 1\u2212A \u221a A+B + 2\nB\n) + log (\u221a B\nA+B + 2\n)\n= 1\n2(A+B + 2)\n( 1\u2212A \u221a A+B + 2\nB\n) + 1\n2 log\n( B\nA+B + 2\n)\nNow we can look at each term to observe that \u2202 2f2\n\u2202A\u2202B < 0. Therefore this expression \u2202f2 \u2202B is maximized over A for A = 0.\nWe now examine this expression:\n\u2202f2 \u2202B \u2223\u2223 A=0 = 1 2(B + 2) + 1 2 log\n( B\nB + 2\n) \u2264 1\n2\n( 1\nB + 2 +\nB B + 2 \u2212 1 ) \u2264 0\nTherefore, the expressions fi are minimized at at the largest possible B = 19A for any given A over our region. We will now write gi(A) := fi(A, 19A ) for this evalutation at the extremal boundary. If we can show that gi(A) \u2265 0 for all A \u2265 1 3 and i = 1, 2 we will be done.\nWe will perform a similar argument to show that gi is monotone increasing, g\u2032i(A) \u2265 0 for all A \u2265 13 .\ng1(A) = log(1 +A) +A log\n(\u221a A\nA+ 19A + 2\n) + 1\n9A log\n( 1\u2212 \u221a A\nA+ 19A + 2\n)\n+ 1 2 log(2\u03c0)\u2212 1 2 log(A+ 1 9A + 2)\n= log(1 +A) + A 2 log(A)\u2212 1 2 (1 +A) log(A+ 1 9A + 2)\n+ 1\n9A log\n( 1\u2212 \u221a A\nA+ 19A + 2\n) + 1\n2 log(2\u03c0)\nNote that the function p(A) = A + 19A is increasing in A for A \u2265 1 3 . We can conservatively bound g from below noting\n1 9A \u2264 1 in our region.\ng1(A) \u2265 = log(1 +A) + A 2 log(A)\u2212 1 2 (1 +A) log(A+ 3) + 1 9A log\n( 1\u2212 \u221a A\nA+ 2\n) + 1\n2 log(2\u03c0)\n\u2265 log(1 +A) + A 2 log(A)\u2212 1 2 (1 +A) log(A+ 3)\u2212 1 9A\n\u221a A+ 1\n2 log(2\u03c0) =: g\u03031(A)\nNow we can use calculus to say that:\ng\u0303\u20321(A) = 1\nA+ 1 +\n1\nA+ 3 +\nlog(A)\n2 +\n1 18A3/2 \u2212 1 2 log(A+ 3)\n\u2265 1 A+ 1 + 1 A+ 3 +\n1\n18A3/2 +\n1 2 log( A A+ 3 )\nThis expression is monotone decreasing in A and with a limit \u2265 0 and so we can say that g\u03031(A) is monotone increasing. Therefore g1(A) \u2265 g\u03031(A) \u2265 g\u03031(1/3) for all A. We can explicitly evaluate this numerically and g\u03031(1/3) > 0.01 so we are done.\nThe final piece of this proof is to do a similar argument for g2(A)\ng2(A) = log(1 +A) +A log\n( 1\u2212 \u221a 1 9A\nA+ 19A + 2\n) + 1\n9A log\n(\u221a 1\n9A\nA+ 19A + 2\n)\n+ 1 2 log(2\u03c0)\u2212 1 2 log(A+ 1 9A + 2)\n= log(1 +A) +A log ( 1\u2212 \u221a 1\n9A2 + 18A+ 1\n) + 1\n2\n( 1\n9A log\n( 1\n9A )) \u22121\n2\n( 1\n9A + 1\n) log ( A+ 1\n9A + 2\n) + 1\n2 log(2\u03c0) \u2265 log(1 +A) +A ( \u2212 1\u221a\n9A2\n) + 1\n2\n( 1\n9A log\n( 1\n9A\n)) \u2212 1\n2\n( 1\n3 + 1\n) log ( A+ 1\n3 + 2\n) + 1\n2 log(2\u03c0)\n\u2265 log(1 +A)\u2212 1 3 \u2212 1 2e \u2212 2 3 log(A+ 7 3 ) + 1 2 log(2\u03c0) =: g\u03032(A)\nNow, once again we can see that g\u03032 is monotone increasing:\ng\u0303\u20322(A) = 1 1 +A \u2212 2/3 A+ 7/3\n= A+ 5\n(A+ 1)(3A+ 7) \u2265 0\nWe complete the argument by noting g2(A) \u2265 g\u03032(A) \u2265 g\u03032(1/3) > 0.01, which concludes our proof of the PDF double crossing in this region."}, {"heading": "G.4. Recap", "text": "Using the results of the previous sections we complete the proof of Lemma 6 for Gaussian vs Beta dominance for all possible \u03b1, \u03b2 > 0 such that \u03b1 + \u03b2 \u2265 1. Piecing together Lemma 5 with Lemma 6 completes our proof of Lemma 1. We imagine that there is a much more elegant and general proof method available for future work."}], "references": [{"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Abbasi-Yadkori", "Yasin", "Szepesv\u00e1ri", "Csaba"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Further optimal regret bounds for Thompson sampling", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "arXiv preprint arXiv:1209.3353,", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Temporal differences-based policy iteration and applications in neuro-dynamic programming", "author": ["Bertsekas", "Dimitri P", "Ioffe", "Sergey"], "venue": "Lab. for Info. and Decision Systems Report LIDS-P-2349,", "citeRegEx": "Bertsekas et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1996}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Dann", "Christoph", "Brunskill", "Emma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2015}, {"title": "Approximate dynamic programming finally performs well in the game of tetris", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Rules for ordering uncertain prospects", "author": ["Hadar", "Josef", "Russell", "William R"], "venue": "The American Economic Review, pp", "citeRegEx": "Hadar et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Hadar et al\\.", "year": 1969}, {"title": "Nearoptimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Kakade", "Sham"], "venue": "PhD thesis,", "citeRegEx": "Kakade and Sham.,? \\Q2003\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2003}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["Kearns", "Michael J", "Koller", "Daphne"], "venue": "In IJCAI, pp", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Least-squares methods in reinforcement learning for control", "author": ["Lagoudakis", "Michail", "Parr", "Ronald", "Littman", "Michael L"], "venue": "In Second Hellenic Conference on Artificial Intelligence (SETN-02),", "citeRegEx": "Lagoudakis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2002}, {"title": "The sample-complexity of general reinforcement learning", "author": ["Lattimore", "Tor", "Hutter", "Marcus", "Sunehag", "Peter"], "venue": "In ICML,", "citeRegEx": "Lattimore et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2013}, {"title": "Stochastic dominance and expected utility: survey and analysis", "author": ["Levy", "Haim"], "venue": "Management Science,", "citeRegEx": "Levy and Haim.,? \\Q1992\\E", "shortCiteRegEx": "Levy and Haim.", "year": 1992}, {"title": "Reducing reinforcement learning to KWIK online regression", "author": ["Li", "Lihong", "Littman", "Michael"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J"], "venue": "In ICML, pp", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "State of the art control of atari games using shallow reinforcement learning", "author": ["Liang", "Yitao", "Machado", "Marlos C", "Talvitie", "Erik", "Bowling", "Michael H"], "venue": "CoRR, abs/1512.01563,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ortner", "Ronald", "Ryabko", "Daniil"], "venue": "In NIPS,", "citeRegEx": "Ortner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ortner et al\\.", "year": 2012}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Near-optimal reinforcement learning in factored MDPs", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapped thompson sampling and deep exploration", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1507.00300,", "citeRegEx": "Osband et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2015}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Osband", "Ian", "Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "PAC optimal exploration in continuous space Markov decision processes", "author": ["Pazis", "Jason", "Parr", "Ronald"], "venue": "In AAAI. Citeseer,", "citeRegEx": "Pazis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pazis et al\\.", "year": 2013}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Russo", "Dan", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Russo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "PAC model-free reinforcement learning", "author": ["Strehl", "Alexander L", "Li", "Lihong", "Wiewiora", "Eric", "Langford", "John", "Littman", "Michael L"], "venue": "In ICML, pp", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard", "Barto", "Andrew"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In NIPS, pp", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "This matches the worst case lower bound for this problem up to logarithmic factors (Jaksch et al., 2010).", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": ", UCRL2 (Jaksch et al., 2010)) adapted to this context.", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 22, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 26, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 15, "context": "regression (Li & Littman, 2010; Li et al., 2008).", "startOffset": 11, "endOffset": 48}, {"referenceID": 30, "context": "The manner in which RLSVI explores is inspired by Thompson sampling (Thompson, 1933), which has been shown to explore efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014).", "startOffset": 68, "endOffset": 84}, {"referenceID": 22, "context": "RLSVI bears a close connection to PSRL (Osband et al., 2013), which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL.", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "Surprisingly, these scalings better state of the art optimistic algorithms specifically designed for efficient analysis which would admit \u00d5( \u221a H3S2AT ) regret (Jaksch et al., 2010).", "startOffset": 159, "endOffset": 180}, {"referenceID": 5, "context": "Tetris has been something of a benchmark problem for RL and approximate dynamic programming, with several papers on this topic (Gabillon et al., 2013).", "startOffset": 127, "endOffset": 150}, {"referenceID": 16, "context": "Interestingly, recent work has been able to reproduce similar performance using linear value functions (Liang et al., 2015).", "startOffset": 103, "endOffset": 123}], "year": 2016, "abstractText": "We propose randomized least-squares value iteration (RLSVI) \u2013 a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or -greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates nearoptimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.", "creator": "LaTeX with hyperref package"}}}