{"id": "1402.1389", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models", "abstract": "The recently developed Bayesian latent variable model (GPLVM) is a powerful generative model for the discovery of low-dimensional embedding in linear time complexity. However, modern datasets are so large that they are difficult to master even with linear time models. We present a novel repair of variation conclusions for the GPLVM and the sparse GP model, enabling an efficient distributed inference algorithm.", "histories": [["v1", "Thu, 6 Feb 2014 16:08:40 GMT  (210kb,D)", "http://arxiv.org/abs/1402.1389v1", "9 pages, 8 figures"], ["v2", "Mon, 29 Sep 2014 21:16:47 GMT  (172kb,D)", "http://arxiv.org/abs/1402.1389v2", "9 pages, 8 figures"]], "COMMENTS": "9 pages, 8 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yarin gal", "mark van der wilk", "carl e rasmussen"], "accepted": true, "id": "1402.1389"}, "pdf": {"name": "1402.1389.pdf", "metadata": {"source": "META", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models", "authors": ["Yarin Gal", "Mark van der Wilk", "Carl E. Rasmussen"], "emails": ["YG279@CAM.AC.UK", "MV310@CAM.AC.UK", "CER54@CAM.AC.UK"], "sections": [{"heading": null, "text": "We present a unifying derivation for both models, analytically deriving the optimal variational distribution over the inducing points. We then assess the suggested inference on datasets of different sizes, showing that it scales well with both data and computational resources. We furthermore demonstrate its practicality in real-world settings using datasets with up to 100 thousand points, comparing the inference to sequential implementations, assessing the distribution of the load among the different nodes, and testing its robustness to network failures."}, {"heading": "1. Introduction", "text": "The Bayesian Gaussian process latent variable model (GPLVM, Titsias & Lawrence (2010)) forms an important component in the Bayesian non-parametric arsenal. Originating as an extension of sparse Gaussian process regression (Titsias, 2009), it can be used to perform non-linear dimensionality reductions in linear time complexity. However, the use of the model with big datasets such as the ones used in continuous-space natural language disambiguation is quite cumbersome and challenging, and thus the model has largely been ignored in such communities. Many other Bayesian non-parametric tools share this limitation: the Dirichlet process, for example, is slow to perform inference in, and often weeks pass between development cycles\n\u2217Joint first author.\nwhen working on large datasets (Gal & Blunsom, 2013).\nIt is desirable to scale the model up to be able to handle large amounts of data. One approach is to distribute computation across many nodes in a parallel implementation. Many have reasoned about the requirements such distributed inference procedures should satisfy (Brockwell, 2006; Wilkinson, 2005; Asuncion et al., 2008). The inference procedure should:\n1. distribute the computational load evenly across cores, 2. scale favourably with the number of nodes, 3. and have low overhead in the global steps.\nIn this paper we introduce a novel distributed inference algorithm for sparse GPs and the GPLVM that satisfies the requirements above. We derive an exact unifying reparametrisation of the bounds derived by Titsias (2009) and Titsias & Lawrence (2010) which allows us to perform inference using the original guarantees without the need for weaker lower bounds, and using the optimal variational distribution over the inducing points. This is achieved by the fact that conditioned on the inducing points, the data decouples and the variational parameters can be updated independently on different nodes, with the only communication between nodes requiring constant time. This also allows the optimisation of the embeddings in the GPLVM to be done by parallel scaled conjugate gradient (SCG).\nWe present an extensive set of experiments showing that inference running time scales inversely with computational power. We also compare the results of our reparameterisation to those obtained from GPy (Titsias & Lawrence, 2010). We further demonstrate the practicality of the inference, inspecting the distribution of the load over the different nodes and comparing run-times to sequential implementations. We test the robustness of the inference by dropping out nodes at random, and measuring the resulting log-marginal likelihood. Finally, we test the performance of the GPLVM on datasets of sizes not commonly handled in the GP community. We perform dimensionality reduction on datasets with up to 100 thousand points and demonstrate results on the USPS dataset.\nar X\niv :1\n40 2.\n13 89\nv1 [\nst at\n.M L\n] 6\nF eb\n2 01\n4\nThe main contributions of this paper can be summarised as follows. We scale the GPLVM and sparse GPs, presenting the first unifying parallel inference algorithm for them able to process datasets with hundreds of thousands of points. An extensive set of experiments demonstrates the properties of this suggested inference. The proposed inference was implemented in Python using the Map-Reduce framework (Dean & Ghemawat, 2008) to work on multicore architectures, and is available as an open-source package2. The full derivation of the re-parametrisation of the inference is given in the supplementary material. The open source software package contains an extensively documented implementation of the derivations, with references to the equations presented in the supplementary material for explanation.\nThis paper is structured as follows. Is \u00a72 we quickly review the Gaussian process latent variable model and sparse GP regression. In \u00a73 we develop parallel inference in a unifying setting, and in \u00a74 we present an experimental evaluation of the inference. We expand on this in \u00a75 where we demonstrate the practicality of the inference in real-world settings comparing it to sequential implementations and assessing its distribution of the load among the different nodes, and review related work in \u00a76. Finally, we present the conclusions in \u00a77."}, {"heading": "2. The Gaussian Process Latent Variable Model and Sparse GP Regression", "text": "Next we quickly review the sparse Gaussian process regression model and the Gaussian process latent variable model (GPLVM). We will review the model structure and the approximations developed (Titsias & Lawrence, 2010; Titsias, 2009) to make the inference efficient."}, {"heading": "2.1. Sparse Gaussian Process Regression", "text": "Given a training dataset consisting of n inputs {X1, . . . , Xn} and their corresponding outputs {F1, . . . , Fn} we would like to find the posterior distribution over the functions mapping the inputs to the outputs. The functions are assumed to be d dimensional while the inputs are q dimensional. This data is often written in matrix form for convenience:\nX \u2208 Rn\u00d7q\nF \u2208 Rn\u00d7d\nFi = g(Xi)\nHere we will adopt the convention that capitals denote matrices of data, while subscripted vectors of the same letter will denote a row, i.e. a single data point. For example, Fi\n2see github.com/markvdw/GParML\ndenotes the i\u2019th function value, while F denotes the matrix of all given function values.\nIn the regression setting we place a Gaussian process prior over the space of functions. This implies a joint Gaussian distribution over all the function values3. For multivariate functions, each dimension is be modelled by a separate GP.\ngi \u223c GP(\u00b5(x), k(x,x\u2032)) Kab = k(xa,xb)\np(F |X) = N (F ;\u00b5(X),K)\n= exp\n( \u2212 12Tr [ (F \u2212 \u00b5(X))TK\u22121(F \u2212 \u00b5(X)) ]) (2\u03c0)nd/2|K|d/2\nWe are often given a noisy evaluations of the function. For this we introduce a new dataset Y containing the noisy observations, making the function values F latent. We assume that the noise on each observation is i.i.d, with noise precision \u03b2,\np(Y |F ) = exp\n( \u2212\u03b22Tr [ (Y \u2212 F )T (Y \u2212 F ) ]) (2\u03c0\u03b2\u22121)nd/2 .\nEvaluating p(Y |X) directly is an expensive operation that involves the inversion of the n by nmatrixK \u2013 thus requiring O(n3) time complexity. Instead, Snelson & Ghahramani (2006) suggested the use of a collection of m \u201cinducing points\u201d \u2013 a set of points lying in the same input space with corresponding values in the output space. These inducing points aim to summarise the characteristics of the function using less points than the training data.\nGiven the locations of the inducing points Z, an m by q matrix form inducing points, and the inferred values of the points u, an m by d matrix, prediction corresponds to taking the GP posterior using only the inducing points instead of the whole training set, which requires only O(m3) time complexity,\np(F \u2217|X\u2217, Y ) \u2248\u222b N ( F \u2217; k\u2217mK \u22121 mmu, k\u2217\u2217 \u2212 k\u2217mK\u22121mmkm\u2217 ) p(u|Y,X)du\nwhere Kmm is the covariance between the m inducing points, and likewise for the other subscripts.\nLearning the distribution over the values of the inducing points requires a simplifying approximation to be made on p(F |X,u, Z), i.e. how the training data relates to the inducing points. One example is assuming the deterministic relationship F = KnmK\u22121mmu, giving a computational complexity ofO(nm2). Quin\u0303onero-Candela & Rasmussen\n3We follow the definition of matrix normal distribution (Arnold, 1981). For a full treatment of Gaussian Processes, see Rasmussen & Williams (2006).\n(2005) view this procedure as changing the prior to make inference more tractable, with Z as hyperparameters which can be tuned using optimisation. On the other hand, Titsias (2009) relates this approximation to a variational approximation, with Z as variational parameters. This gives the marginal likelihood above an alternative interpretation as a lower bound on the exact marginal likelihood. Again, the Z values can be optimised over to tighten the lower bound. A detailed derivation is given in section 3 of the supplementary material."}, {"heading": "2.2. Gaussian Process Latent Variable Models", "text": "The GPLVM model is the unsupervised equivalent of the regression problem above. This model can be viewed as a non-linear generalisation of PCA (Lawrence, 2005). The model set-up is identical to the regression case, only we assume a prior over the now latent variable X and attempt to infer both the mapping from X to Y and the distribution over X at the same time.\nXi \u223c N (Xi; 0, I) F (Xi) \u223c GP(0, k(X,X))\nYi \u223c N (Fi, \u03b2\u22121I)\nA Variational Bayes approximation for this model has been developed by Titsias & Lawrence (2010) using similar techniques as for variational sparse GPs. In fact, the sparse GP can be seen as a special case of the GPLVM where the inputs are given zero variance.\nThe main task in deriving approximate inference revolves around finding a variational lower bound to:\np(Y ) = \u222b p(Y |F )p(F |X)p(X)d(F,X)\nWhich leads to a Gaussian approximation to the posterior q(X) \u2248 p(X|Y ), explained in detail in section 4 of the supplementary material. In the next section we derive a parallel inference scheme for both models following a reparametrisation of the derivations of Titsias (2009) which allows us to decouple the distribution over data points."}, {"heading": "3. Parallel inference", "text": "We now exploit the conditional independence of the data given the inducing points to derive a parallel inference scheme for both the sparse GP model and the GPLVM, which will allow us to easily scale these models to large datasets. The key equations are given below, with an indepth explanation given in sections sections 3 and 4 of the supplementary material. We present a unifying derivation of the inference procedures for both the regression case and the latent variable modelling (LVM) case, by identifying\nthat the explicit inputs in the regression case are identical to the latent inputs in the LVM case when their mean is set to the observed inputs and used with variance 0 (i.e. the latent inputs are fixed and not optimised).\nWe start with the general expression for the log marginal likelihood of the sparse GP regression model, after introducing the inducing points,\nlog p(Y |X) = log \u222b p(Y |F )p(F |X,u)p(u)d(u, F ).\nThe LVM derivation encapsulates this expression by multiplying with the prior over X and then marginalising over X:\nlog p(Y ) = log \u222b p(Y |F )p(F |X,u)p(u)p(X)d(u, F,X).\nWe then introduce a free-form variational distribution q(u) over the inducing points, and another over X (where in the regression case, p(X)\u2019s and q(X)\u2019s variance is set to 0 and their mean set to X). Using Jensen\u2019s inequality we get the following lower bound:\nlog p(Y |X) \u2265 \u222b p(F |X,u)q(u) log p(Y |F )p(u)\nq(u) d(u, F )\n= \u222b q(u) (\u222b p(F |X,u) log p(Y |F )d(F ) + log p(u)\nq(u)\n) d(u)\n(3.1)\nall distributions that involve u also depend on Z which we have omitted for brevity. Next we integrate p(Y ) overX to be able to use 3.1,\nlog p(Y ) = log \u222b q(X)\np(Y |X)p(X) q(X) d(X)\n\u2265 \u222b q(X) ( log p(Y |X) + log p(X)\nq(X)\n) d(X)\n(3.2)\nand obtain a bound which can be used for both models. Up to here the derivation is identical to the two derivations given in (Titsias & Lawrence, 2010; Titsias, 2009). However, now we exploit the conditional independence when conditioned on u to break the inference into small independent components."}, {"heading": "3.1. Decoupling the data conditioned on the inducing points", "text": "The introduction of the inducing points decouples the function values from each other in the following sense. If we represent Y as the individual data points (Y1;Y2; ...;Yn) with Yi \u2208 R1\u00d7d and similarly for F , we can write the lower\nbound as a sum over the data points, since Yi are independent of Fj for j 6= i:\u222b\np(F |X,u) log p(Y |F )d(F )\n= \u222b p(F |X,u) n\u2211 i=1 log p(Yi|Fi)d(F )\n= n\u2211 i=1 \u222b p(Fi|Xi,u) log p(Yi|Fi)d(Fi)\nSimplifying this expression and integrating over X we get that each term is given by\n\u2212 d 2 log(2\u03c0\u03b2\u22121)\u2212 \u03b2 2\n( YiY T i\n\u2212 2 \u3008Fi\u3009p(Fi|Xi,u)q(Xi) Y T i +\n\u2329 FiF T i \u232a p(Fi|Xi,u)q(Xi) ) )\nwhere we use triangular brackets \u3008F \u3009p(F ) to denote the expectation of F with respect to the distribution p(F ).\nNow, using calculus of variations we can find optimal q(u) analytically. Plugging the optimal distribution into eq. 3.1 and using further algebraic manipulations we obtain the following lower bound:\nlog p(Y ) \u2265\n\u2212 nd 2 log 2\u03c0 + dn 2 log \u03b2 + d 2 log |Kmm| \u2212 d 2 log |Kmm + \u03b2D| \u2212 \u03b2 2 A\u2212 \u03b2d 2 B + \u03b2d 2 Tr(K\u22121mmD) + \u03b22\n2 Tr(CT \u00b7 (Kmm + \u03b2D)\u22121 \u00b7 C)\u2212KL (3.3)\nwhere\nA = n\u2211 i=1 YiY T i\nC = n\u2211 i=1 \u3008Kmi\u3009q(Xi) Yi\nB = n\u2211 i=1 \u3008Kii\u3009q(Xi)\nD = n\u2211 i=1 \u3008KmiKim\u3009q(Xi)\nand\nKL = n\u2211 i=1 KL(q(Xi)||p(Xi))\nwhen the inputs are latent or set to 0 when they are observed.\nNotice that the obtained unifying bound is identical to the ones derived in (Titsias, 2009) for the regression case and (Titsias & Lawrence, 2010) for the LVM case since \u3008Kmi\u3009q(Xi) = Kmi for q(Xi) with variance 0 and mean Xi. However, the terms are re-parametrised as independent sums over the input points \u2013 sums that can be computed on different nodes in a network without inter-communication. An in-depth explanation of the different transitions is given in the supplementary material sections 3 and 4."}, {"heading": "3.2. The parallel inference", "text": "A parallel inference algorithm can be easily derived based on this factorisation. Using the Map-Reduce framework (Dean & Ghemawat, 2008) we can maintain different subsets of the inputs and their corresponding outputs on each node in a parallel implementation and distribute the global parameters (such as the kernel hyper-parameters and the locations of the inducing points) to the nodes, collecting only the partial terms calculated on each node.\nWe denote by G the set of global parameters over which we need to perform optimisation. These include Z (the locations of the inducing points), \u03b2 (the observation noise), and k (the set of kernel hyper-parameters). Additionally we denote by Lk the set of local parameters on each node k that need to be optimised. These include the mean and variance for each output point for the LVM model. First, we send to all end-point nodes the global parameters G for them to calculate the partial terms \u3008Kmi\u3009q(Xi) Yi, \u3008KmiKim\u3009q(Xi), \u3008Kii\u3009q(Xi), YiY T i , and KL(q(Xi)||p(Xi)). The calculation of these terms is explained in more detail in the supplementary material section 4. The end-point nodes return these partial terms to the central node (these are m\u00d7m\u00d7 q matrices \u2013 constant space complexity for fixed m). The central node then sends the accumulated terms and partial derivatives back to the nodes and performs global optimisation over G. For the LVM task the nodes then perform at the same time local optimisation on Lk, the embedding posterior parameters. In total, we have two Map-Reduce steps between the central node and the end-point nodes to follow:\n1. The central node distributes G,\n2. Each end-point node k returns a partial sum of the terms A,B,C,D and KL based on Lk,\n3. The central node calculates F , \u2202F (m\u00d7m\u00d7 q matrices) and distributes to the end-point nodes,\n4. The central node optimises G; at the same time the end-point nodes optimise Lk.\nfor the regression task the third step is not required as well as the second part of the forth step. The appendices of the supplementary material contain the derivations of the partial derivatives with respect to the global variables as well as the local ones.\nOptimisation of the global parameters can be done using any procedure that utilises the calculated partial derivative (such as scaled conjugate gradient (M\u00f8ller, 1993)), and the optimisation of the local variables can be carried out by parallelising SCG or using local gradient descent. We now explore the developed inference empirically and evaluate its properties on a range of tasks.\nTime scaling with cores"}, {"heading": "4. Experimental Evaluation", "text": "We assessed the inference on a wide set of experiments evaluating its scalability with computation power as well as with data, and explored the numerical stability of the inference. We further explored the distribution of the load over the different nodes and compared the inference to sequential implementations such as GPy (Titsias & Lawrence, 2010). Finally, we tested the robustness of our parallel inference procedure by dropping-out nodes at random and measure the resulting log-marginal likelihood. We tested the performance of the GPLVM on large datasets not commonly handled in the GP community performing dimensionality reduction and density estimation on the full USPS dataset (4K data points) and datasets with hundreds of thousands of points."}, {"heading": "4.1. Implementation & Setup", "text": "In the following experiments we used an SE ARD kernel over the latent space in order to automatically determine the intrinsic dimensionality of the latent space, as in (Titsias & Lawrence, 2010). We initialise our latent points using PCA and our inducing points using k-means with added noise. We optimise using scaled conjugate gradient (M\u00f8ller, 1993) following the original implementation by (Titsias & Lawrence, 2010).\nOur experiments were run on a 4 processor, 64-core Opteron 6276 machine. One caveat concerning this processor is that there is only one floating point unit (FPU) per two cores. Often a single thread does not utilise the FPU fully, so clever instruction scheduling allows two threads to use one FPU without much performance degradation. Throughout these experiments we assume a 64-core machine, but the FPU sharing could account for some of the diminishing returns observed when using a large number of threads.\nDistributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\n3210123\nFirst principal latent dimension\n3\n2\n1\n0\n1\n2\n3\n4\nS e c o n d p rin c ip a l la te n t d im e n s io n\nParallel GPLVM\n3210123\nFirst principal latent dimension\n4\n2\n0\n2\n4\nS e c o n d p rin c ip a l la te n t d im e n s io n\nGPy\nFigure 4. Latent space produced by the parallel inference (left) and GPy (right) using the oilflow dataset (Titsias & Lawrence, 2010)."}, {"heading": "4.2. Scaling with Computation Power", "text": "We investigate how much inference on a given dataset can be sped up using our parallel implementation, given more computational resources. In the ideal case we would see a halving of time given double the resources, but due to overheads in distributing the computation it is usually only possible to get close.\nWe assess the improvement of the running time of the algorithm on a simple synthetic dataset of which large amounts of data could easily be generated. The dataset was obtained by simulating a 1D latent space and transforming this into 3D observations through linear functions with sines superimposed (see figure 1). 100k points were generated and the algorithm was run using an increasing number of cores and a 2 dimensional latent space. We measured both the total running time of the algorithm, including initialisation and threading overheads, and the amount of time spent only in the two Map-Reduce functions. This allows us to assess the impact of our new inference on the running time as a whole, and the effectiveness on just the parts which were parallelised.\nFigure 2 shows the improvement of run-times as a function of available cores. When initialisation and threading overheads are not considered, we obtain a relation very close to the ideal t \u221d c \u00b7 (cores)\u22121. When doubling the number of cores from 5 to 10 we achieve a factor 1.99 decrease in computation time \u2013 very close to ideal. There is some hint of diminishing returns when we scale up from 30 to 60 cores, where we decrease running time by a factor of 1.644. When initialisation and overheads are considered, we speed up inference by a factor of 1.96 for a change from 5 to 10 cores, and by a factor of 1.54 for a change from 30 to 60 cores.\nFigure 2 also indicates that there is a substantial overhead in our implementation. This is not due to fixed costs such as the PCA initialisation, but more due to the large costs associated with creating the worker threads in Python that\nrun the map functions in parallel. There is still much performance to be gained from optimising this."}, {"heading": "4.3. Scaling with Data", "text": "Using the same setup, we assessed the scaling of the running time as we increased both the dataset size and computational resources equally. This answers the question of how large a dataset we can handle given an increasing amount of resources.\nFor a doubling of data, we doubled the number of available CPUs. In the ideal case of no constant costs and no threading overheads, computation time should be constant. Again, we measure the total running time of the algorithm, and the time spent only in the Map-Reduce functions.\nFigure 3 shows that we are able to effectively utilise the extra computational resources. Our total running time, including overheads, takes 67% longer for a dataset scaled by 60 times. The Map-Reduce calculations only take 35% longer."}, {"heading": "4.4. Comparison to GPy", "text": "We also compare the computation time of our inference procedure to the single threaded, but highly optimised GPy implementation (see figure 3). We show that we significantly outperform GPy given more computational resources. Our parallel inference allows us to run the GPLVM on datasets which would simply take too long to run with a single threaded implementation. However, for small datasets GPy is significantly faster. This is partly due to the large overheads discussed in the previous experiment and partly due to optimisations in GPy. GPy performs certain calculations in native C++, while our implementation is written fully in Python.\nIn addition to scaling experiments, we compare our latent space to GPy, which we use as a reference implementation. Just like in the original paper (Titsias & Lawrence, 2010), we used the oil-flow dataset. Both algorithms were run un-\ntil no significant improvement in the marginal likelihood was found.\nThe two latent spaces are shown in figure 4. The latent spaces are qualitatively similar, but differ due to a slightly different implementation of the optimiser. Like the results in (Titsias & Lawrence, 2010) all but one of the ARD parameters decrease to zero, giving an effectively 1D latent space."}, {"heading": "4.5. USPS Data", "text": "As a practical test, we ran our inference scheme on the USPS digits dataset, as used in (Rasmussen & Williams, 2006). We trained a GPLVM over the complete dataset of 4649 examples containing all digits from 0-9, using 150 inducing points. The training was run overnight and was complete the following day. We managed to reconstruct digits with 34% of their pixels missing (see figure 6). We also ran the same experiment using only 1000 digits to assess the utility of using more data in the GPLVM. We compared the average mean reconstruction error and found that the larger dataset gives a 5.9% improvement.\nAs this dataset can now be run overnight, a greater number of development cycles can be used for fine tuning of the model."}, {"heading": "5. Practicality of the Inference", "text": ""}, {"heading": "5.1. Distribution of the Load", "text": "One of our stated requirements for a practical parallel inference algorithm is an approximately equal distribution of the load on the nodes. This is especially relevant in a MapReduce framework, where the reduce step can only happen after all map computations have finished, so the maximum execution time of one of the threads is the rate limiting step. Figure 5 shows the minimum, maximum and average execution time of all threads for a range of iterations of a run. On average there is a 3.7% difference between the mean and maximum run-time of a thread, suggesting an even distribution of the load."}, {"heading": "5.2. Robustness to Node Failure", "text": "One other desirable characteristic of a parallel inference scheme is robustness to failure of nodes. One way of dealing with this would be to load the data to a different node and restart the calculation. However, since the speed of one iteration is limited by the slowest calculation on one of the nodes, this could slow down the algorithm by the time it takes to load the intermediate data onto the new node. An alternative strategy would be to drop the partial term from the calculation and use a slightly noisy gradient calculation in the optimisation for one iteration. Here we investigate the robustness of our inference to this procedure.\nWe ran our parallel inference on the oil-flow dataset using the same setting as above for 500 iterations accumulating the log marginal likelihood as a function of the iteration. We used 10 nodes and simulated failure frequencies of 0%, 1% and 2% per iteration. The experiment was repeated 10 times and the log marginal likelihood averaged. Even a failure rate of 1% per iteration for 500 iterations translates to a high number of 1 out the 10 nodes failing on average every 10 iterations.\nAs we observe in figure 7 a node failure frequency of 1% hurts total performance by decreasing the log marginal likelihood from -1500 to -5000 on average. It seems that a higher failure frequency leads to convergence to worse local optima or a failure of the optimiser, possibly because of the finite differences approximation to the function curvature used by SCG, which might suffer from noisy gradient estimations. It is also interesting to note that the embeddings discovered are less pronounced than the ones shown in figure 4 but still have only one major latent dimension. For 0% failure rate the ARD parameters are 0.02 for all but one dimension (0.15), for 1% failure rate the ARD parameters are 0.10 for all but one dimension (0.17), and for 2% failure rate the ARD parameters are 0.29 for all but one dimension (0.34)."}, {"heading": "6. Related Work", "text": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al. (2013)) for the problem of scaling up sparse Gaussian process regression. In their research, the variational distribution over the inducing points was explicitly represented and optimisation was performed over the variational distribution itself instead of using the optimal analytical solution, a necessity of the SVI setting that cannot be averted. Hensman et al. (2013) proposed future directions for research which include the derivation of SVI for GPLVMs, and suggested that the proposed SVI for sparse GP regression could be carried out in parallel.\nHowever, in order to perform SVI a weaker lower bound on the log marginal likelihood than the one proposed by Titsias (2009) has to be used, and many parameters have to\nbe optimised in addition to the kernel hyper-parameters. In addition to that, since the model uses mini-batches, the locations of the inducing points cannot be inferred easily and have to be fixed in advance. This is due to the strong correlation between the locations and values of the inducing points. Figure 8 demonstrates that a negative log marginal likelihood local minimum for the location of an inducing point when fixing u is not necessarily a minimum as a function of u. Furthermore, in the SVI setting many additional optimiser parameters have to be introduced and fine-tuned by hand (Hensman et al., 2013) to control the step-length of different quantities such as the gradient. Additionally, many heuristics are used to decide what quantities to update when. For example, the step-lengths do not change for the first epoch, and then change differently for different parameters. These difficulties make SVI rather hard to work with."}, {"heading": "7. Conclusions", "text": "We have scaled the GPLVM and sparse GPs model presenting the first unifying parallel inference algorithm which is able to process datasets with hundreds of thousands of data points. An extensive set of experiments studying the properties of the suggested inference was presented. The inference was implemented for a multi-core architecture and is available as an open-source package, containing an extensively documented implementation of the derivations, with references to the equations presented in the supplementary material for explanation."}], "references": [{"title": "The theory of linear models and multivariate analysis. Wiley series in probability and mathematical statistics: Probability and mathematical statistics", "author": ["S.F. Arnold"], "venue": null, "citeRegEx": "Arnold,? \\Q1981\\E", "shortCiteRegEx": "Arnold", "year": 1981}, {"title": "Asynchronous distributed learning of topic models", "author": ["Asuncion", "Arthur U", "Smyth", "Padhraic", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "Parallel Markov Chain Monte Carlo simulation by Pre-Fetching", "author": ["A.E. Brockwell"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Brockwell,? \\Q2006\\E", "shortCiteRegEx": "Brockwell", "year": 2006}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["Dean", "Jeffrey", "Ghemawat", "Sanjay"], "venue": "Commun. ACM,", "citeRegEx": "Dean et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2008}, {"title": "A systematic Bayesian treatment of the IBM alignment models", "author": ["Gal", "Yarin", "Blunsom", "Phil"], "venue": "In Proceedings of NAACL-HLT, pp", "citeRegEx": "Gal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2013}, {"title": "Gaussian processes for big data", "author": ["Hensman", "James", "Fusi", "Nicolo", "Lawrence", "Neil D"], "venue": null, "citeRegEx": "Hensman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Stochastic Variational Inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Probabilistic non-linear principal component analysis with gaussian process latent variable models", "author": ["Lawrence", "Neil"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence and Neil.,? \\Q2005\\E", "shortCiteRegEx": "Lawrence and Neil.", "year": 2005}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning", "author": ["M\u00f8ller", "Martin Fodslette"], "venue": "Neural networks,", "citeRegEx": "M\u00f8ller and Fodslette.,? \\Q1993\\E", "shortCiteRegEx": "M\u00f8ller and Fodslette.", "year": 1993}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["Qui\u00f1onero-Candela", "Joaquin", "Rasmussen", "Carl Edward"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2006}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M.K. Titsias"], "venue": "Technical report, Technical Report,", "citeRegEx": "Titsias,? \\Q2009\\E", "shortCiteRegEx": "Titsias", "year": 2009}, {"title": "Bayesian gaussian process latent variable model", "author": ["Titsias", "Michalis", "Lawrence", "Neil"], "venue": null, "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Parallel Bayesian computation", "author": ["Wilkinson", "Darren J"], "venue": "Handbook of Parallel Computing and Statistics,", "citeRegEx": "Wilkinson and J.,? \\Q2005\\E", "shortCiteRegEx": "Wilkinson and J.", "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "Originating as an extension of sparse Gaussian process regression (Titsias, 2009), it can be used to perform non-linear dimensionality reductions in linear time complexity.", "startOffset": 66, "endOffset": 81}, {"referenceID": 12, "context": "The Bayesian Gaussian process latent variable model (GPLVM, Titsias & Lawrence (2010)) forms an important component in the Bayesian non-parametric arsenal.", "startOffset": 60, "endOffset": 86}, {"referenceID": 2, "context": "Many have reasoned about the requirements such distributed inference procedures should satisfy (Brockwell, 2006; Wilkinson, 2005; Asuncion et al., 2008).", "startOffset": 95, "endOffset": 152}, {"referenceID": 1, "context": "Many have reasoned about the requirements such distributed inference procedures should satisfy (Brockwell, 2006; Wilkinson, 2005; Asuncion et al., 2008).", "startOffset": 95, "endOffset": 152}, {"referenceID": 12, "context": "We derive an exact unifying reparametrisation of the bounds derived by Titsias (2009) and Titsias & Lawrence (2010) which allows us to perform inference using the original guarantees without the need for weaker lower bounds, and using the optimal variational distribution over the inducing points.", "startOffset": 71, "endOffset": 86}, {"referenceID": 12, "context": "We derive an exact unifying reparametrisation of the bounds derived by Titsias (2009) and Titsias & Lawrence (2010) which allows us to perform inference using the original guarantees without the need for weaker lower bounds, and using the optimal variational distribution over the inducing points.", "startOffset": 71, "endOffset": 116}, {"referenceID": 12, "context": "We will review the model structure and the approximations developed (Titsias & Lawrence, 2010; Titsias, 2009) to make the inference efficient.", "startOffset": 68, "endOffset": 109}, {"referenceID": 0, "context": "Qui\u00f1onero-Candela & Rasmussen We follow the definition of matrix normal distribution (Arnold, 1981).", "startOffset": 85, "endOffset": 99}, {"referenceID": 0, "context": "Qui\u00f1onero-Candela & Rasmussen We follow the definition of matrix normal distribution (Arnold, 1981). For a full treatment of Gaussian Processes, see Rasmussen & Williams (2006).", "startOffset": 86, "endOffset": 177}, {"referenceID": 12, "context": "On the other hand, Titsias (2009) relates this approximation to a variational approximation, with Z as variational parameters.", "startOffset": 19, "endOffset": 34}, {"referenceID": 12, "context": "A Variational Bayes approximation for this model has been developed by Titsias & Lawrence (2010) using similar techniques as for variational sparse GPs.", "startOffset": 71, "endOffset": 97}, {"referenceID": 12, "context": "In the next section we derive a parallel inference scheme for both models following a reparametrisation of the derivations of Titsias (2009) which allows us to decouple the distribution over data points.", "startOffset": 126, "endOffset": 141}, {"referenceID": 12, "context": "Up to here the derivation is identical to the two derivations given in (Titsias & Lawrence, 2010; Titsias, 2009).", "startOffset": 71, "endOffset": 112}, {"referenceID": 12, "context": "Notice that the obtained unifying bound is identical to the ones derived in (Titsias, 2009) for the regression case and (Titsias & Lawrence, 2010) for the LVM case since \u3008Kmi\u3009q(Xi) = Kmi for q(Xi) with variance 0 and mean Xi.", "startOffset": 76, "endOffset": 91}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al. (2013)) for the problem of scaling up sparse Gaussian process regression.", "startOffset": 31, "endOffset": 123}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al. (2013)) for the problem of scaling up sparse Gaussian process regression. In their research, the variational distribution over the inducing points was explicitly represented and optimisation was performed over the variational distribution itself instead of using the optimal analytical solution, a necessity of the SVI setting that cannot be averted. Hensman et al. (2013) proposed future directions for research which include the derivation of SVI for GPLVMs, and suggested that the proposed SVI for sparse GP regression could be carried out in parallel.", "startOffset": 31, "endOffset": 489}, {"referenceID": 5, "context": "Furthermore, in the SVI setting many additional optimiser parameters have to be introduced and fine-tuned by hand (Hensman et al., 2013) to control the step-length of different quantities such as the gradient.", "startOffset": 114, "endOffset": 136}, {"referenceID": 11, "context": "However, in order to perform SVI a weaker lower bound on the log marginal likelihood than the one proposed by Titsias (2009) has to be used, and many parameters have to be optimised in addition to the kernel hyper-parameters.", "startOffset": 110, "endOffset": 125}], "year": 2017, "abstractText": "The recently developed Bayesian Gaussian process latent variable model (GPLVM) is a powerful generative model for discovering low dimensional embeddings in linear time complexity. However, modern datasets are so large that even linear-time models find them difficult to cope with. We introduce a novel re-parametrisation of variational inference for the GPLVM and sparse GP model that allows for an efficient distributed inference algorithm. We present a unifying derivation for both models, analytically deriving the optimal variational distribution over the inducing points. We then assess the suggested inference on datasets of different sizes, showing that it scales well with both data and computational resources. We furthermore demonstrate its practicality in real-world settings using datasets with up to 100 thousand points, comparing the inference to sequential implementations, assessing the distribution of the load among the different nodes, and testing its robustness to network failures.", "creator": "LaTeX with hyperref package"}}}