{"id": "1402.1526", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "abstract": "We present a practical, differentiated private algorithm to answer a large number of requests for high-dimensional data sets. Like all algorithms for this task, our algorithm necessarily has an exponential worst-case complexity in the dimension of the data. However, our algorithm packs the mathematically hard step into a concisely defined integer program that cannot be solved privately with standard solutions. We demonstrate accuracy and privacy theories for our algorithm and then demonstrate experimentally that our algorithm works well in practice. For example, our algorithm can efficiently and accurately answer millions of requests on the Netflix dataset with over 17,000 attributes; this is an improvement over the state of the art by several orders of magnitude.", "histories": [["v1", "Thu, 6 Feb 2014 23:20:43 GMT  (229kb,D)", "http://arxiv.org/abs/1402.1526v1", null], ["v2", "Thu, 19 Nov 2015 04:36:00 GMT  (338kb,D)", "http://arxiv.org/abs/1402.1526v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.DB cs.LG", "authors": ["marco gaboardi", "emilio jes\u00fas gallego arias", "justin hsu", "aaron roth", "zhiwei steven wu"], "accepted": true, "id": "1402.1526"}, "pdf": {"name": "1402.1526.pdf", "metadata": {"source": "CRF", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "authors": ["Marco Gaboardi", "Emilio Jes\u00fas Gallego Arias", "Justin Hsu", "Aaron Roth", "Zhiwei Steven Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Privacy is becoming a paramount concern for machine learning and data analysis tasks, which often operate on personal data. The Netflix challenge exemplifies the tension between machine learning and data privacy. Netflix released an anonymized dataset of user movie ratings for teams competing to develop an improved recommendation mechanism. The competition was a great success (the winning team improved on the existing recommendation system by more than 10%), but the ad hoc anonymization was not as successful: Narayanan and Shmatikov [27] were later able to re-identify individuals in the dataset. This eventually led to a lawsuit and the cancellation of subsequent competitions.\nDifferentially private query release is an attempt to solve this problem. Differential privacy is a strong formal privacy guarantee (that, among other things, provably prevents re-identification attacks), and the problem of query release is to release accurate answers to a set of statistical queries. As observed early on by Blum et al. [6], performing private query release is sufficient to simulate any learning algorithm in the \u201cstatistical query model\u201d of Kearns [22].\nSince then, the query release problem has been extensively studied in the differential privacy literature. While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19]. A natural approach, employed by many of these algorithms, is to answer queries by generating synthetic data: a fresh, safe version of the dataset approximates the real dataset on every statistical query of interest.\nUnfortunately, even the most efficient approaches have a per-query running time linear in the size of the data universe, which is exponential in the dimension of the data [18]. Moreover, this running time is necessary in the worst case [32], especially if the algorithm produces synthetic data [33].\nar X\niv :1\n40 2.\n15 26\nv1 [\ncs .D\nS] 6\nF eb\n2 01\nThis exponential runtime has hampered practical evaluation of query release algorithms. One notable exception is due to Hardt et al. [19], who perform a thorough experimental evaluation of one such algorithm, which they called MWEM. They find that MWEM has quite good accuracy in practice and scales to higher dimensional data than suggested by a theoretical (worst-case) analysis. Nevertheless, running time remains a problem, and the approach does not seem to scale to high dimensional data (with more than 30 or so attributes for general queries, and more when the queries satisfy special structure1). The critical bottleneck is the size of the state maintained by the algorithm: MWEM, like many query release algorithms, needs to manipulate an object that has size linear in the size of the data universe (i.e., exponential in the dimension). This quickly becomes impractical as the record space grows more complex.\nWe present DualQuery, an alternative algorithm which is dual to MWEM in a sense that we will make precise. Rather than manipulate an object of exponential size, DualQuery needs to solve a concisely represented (but NP-hard) optimization problem. Critically, the optimization step does not require a solution that is either private or exact: it can be solved by existing, off-the-shelf optimization packages quickly in practice. Except for this step, all parts of our algorithm are extremely efficient. As a result, DualQuery requires (worst-case) space and (in practice) time only linear in the number of queries of interest, which is often significantly smaller than the number of possible records. Like existing algorithms for query release, DualQuery has a provable accuracy guarantee and satisfies the strong differential privacy guarantee.\nWe evaluate DualQuery on a variety of datasets by releasing 3-way marginals (also known as conjunctions or contingency tables), demonstrating that it solves the query release problem accurately and efficiently even when the data includes hundreds of thousands of features. We know of no other algorithm to perform accurate, private query release for rich classes of queries on real data with more than even 100 features."}, {"heading": "1.1 Related work", "text": "Differentially private learning has been studied since Blum et al. [6] showed how to convert learning algorithms in the SQ model of Kearns [22] into differentially private learning algorithms with similar accuracy guarantees. Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].\nIn parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17]. These results are extremely strong in an information theoretic sense: they ensure the consistency of the synthetic data with respect to an exponentially large family of statistics. But, all of these algorithms (including the notable multiplicative weights algorithm of Hardt and Rothblum [18], which achieves the theoretically optimal accuracy and runtime) have running time exponential in the dimension of the data. With standard cryptographic assumptions, this is necessary in the worst case for mechanisms that answer many arbitrary statistical queries [32].\nNevertheless, there have been some experimental evaluations of these approaches on real datasets. Most related to our work is the evaluation of the MWEM mechanism by Hardt et al. [19], which is based on the private multiplicative weights mechanism [18]. This algorithm is inefficient (it manipulates a probability distribution over a set exponentially large in the dimension of the data space) but with some heuristic optimizations, Hardt et al. [19] were able to implement the multiplicative weights algorithm on several real datasets with up to 77 attributes (and even more when the queries are restricted to take positive values only\n1 Hardt et al. [19] are able to scale up to 1000 features on synthetic data when the features are partitioned into a number of small buckets, and the queries are chosen to never depend on features in more than one bucket.\non a small number of disjoint groups of features). However, it seems difficult to scale this approach to higher dimensional data.\nAnother family of query release algorithms are based on the Matrix Mechanism [25, 24]. The runtime guarantees of the matrix mechanism are similar to the approaches based on multiplicative weights\u2014the algorithm manipulates a \u201cmatrix\u201d of queries with dimension exponential in the number of features. Yaroslavtsev et al. [34] evaluate an approach based on this family of algorithms on low dimensional datasets, but scaling to high dimensional data also seems challenging.\nOur algorithm is inspired by the view of the synthetic data generation problem as a zero-sum game, first proposed by Hsu et al. [20]. In this interpretation, Hardt et al. [19] solves the game by having a data player use a no-regret learning algorithm, while the query player repeatedly best responds by optimizing over queries. In contrast, our algorithm swaps the roles of the two players: the query player now uses the no-regret learning algorithm, whereas the data player now finds best responses by solving an optimization problem. This is reminiscent of \u201cBoosting for queries,\u201d proposed by Dwork et al. [14]; the main difference is that our optimization problem is over single records rather than sets of records. As a result, our optimization can be handled non-privately."}, {"heading": "2 Differential privacy background", "text": "Differential privacy has become a standard algorithmic notion for protecting the privacy of individual records in a statistical database. It formalizes the requirement that the addition or removal of a data record does not change the probability of any outcome of the mechanism by much.\nWe consider databases which are multisets of elements from an abstract domain X , representing the set of all possible data records. We often consider elements in X as bit-strings of length d. Two databases D,D\u2032 \u2282 X are neighboring if they differ in a single data element: i.e., their symmetric difference \u2016D4D\u2032\u2016 is at most 1.\nDefinition 2.1 (Dwork et al. [15]). A mechanism M : X n \u2192 R satisfies (\u03b5, \u03b4)-differential privacy if for every S \u2286 R and for all neighboring databases D,D\u2032 \u2208 X n, the following holds:\nPr[M(D) \u2208 S] \u2264 e\u03b5 Pr[M(D\u2032) \u2208 S] + \u03b4\nIf \u03b4 = 0 we say M satisfies \u03b5-differential privacy.\nDefinition 2.2. The (global) sensitivity of a query Q is its maximum difference when evaluated on two neighboring databases:\nGSf = max D,D\u2032\u2208Xn:\u2016D4D\u2032\u2016=1\n|f(D)\u2212 f(D\u2032)|.\nIn this paper, we consider the private release of information for the classes of linear queries, which have sensitivity 1/n.\nDefinition 2.3. For any predicate \u03d5 : X \u2192 {0, 1}, the linear query Q\u03d5 : X n \u2192 [0, 1] is defined by\nQ\u03d5(D) =\n\u2211 x\u2208D \u03d5(x)\n|D| .\nThe following composition theorem is a fundamental tool for private algorithm analysis: we can bound the privacy cost of an algorithm as a function of the privacy costs of its subcomponents.\nLemma 2.4 (Dwork et al. [14]). Let M1, . . . ,Mk be such that each Mi is (\u03b5i, 0)-private with \u03b5i \u2264 \u03b5\u2032. Then M(D) = (M1(D), . . . ,Mk(D)) is (\u03b5, 0)-private for \u03b5 = \u2211k i=1 \u03b5i, and (\u03b5, \u03b4)-private for\n\u03b5 = \u221a 2 log(1/\u03b4)k\u03b5\u2032 + k\u03b5\u2032(e\u03b5 \u2032 \u2212 1)\nfor any \u03b4 \u2208 (0, 1)."}, {"heading": "3 The query release game", "text": "The analysis of our algorithm relies on the interpretation of query release as a two player, zero-sum game [20]. In the present section, we review this idea and related tools."}, {"heading": "3.1 Game definition", "text": "Suppose we want to answer all queries inQ. For each query q \u2208 Q, we can form the negated query q, which takes values q(D) = 1\u2212 q(D) for every database D. Equivalently, for a linear query defined by a predicate \u03d5, the negated query is defined by the negation \u00ac\u03d5 of the predicate. For the remainder, we will assume that Q is closed under negation; if not, we may add negated copies of each query to Q.\nLet there be two players, whom we call the data player and query player. The data player has action set equal to the data universe X , while the query player has action set equal to the query class Q. Given a play x \u2208 X and q \u2208 Q, we let the payoff be\nA(x, q) := q(D)\u2212 q(x), (1)\nwhere D is the true database. As a zero sum game, the data player will try to minimize the payoff, while the query player will try to maximize the payoff."}, {"heading": "3.2 Equilibrium of the game", "text": "Let \u2206(X ) and \u2206(Q) be the set of probability distributions over X andQ. We consider how well each player can do if they randomize over their actions, i.e., if they play from a probability distribution over their actions. By von Neumann\u2019s minimax theorem,\nmin u\u2208\u2206(X ) max w\u2208\u2206(Q) A(u,w) = max w\u2208\u2206(Q) min u\u2208\u2206(Q) A(u,w),\nfor any two player zero-sum game, where\nA(u,w) := Ex\u223cu,q\u223cwA(x, q)\nis the expected payoff. The common value is called the value of the game, which we denote by vA. Intuitively, von Neumann\u2019s theorem states that there is no advantage in a player going first: the minimizing player can always force payoff at most vA, while the maximizing player can always force payoff at least vA.\nThis suggests that each player can play an optimal strategy, assuming best play from the opponent\u2014 this is the notion of equilibrium strategies, which we now define. We will soon interpret these strategies as solutions to the query release problem.\nDefinition 3.1. Let \u03b1 > 0. Let A be the payoffs for a two player, zero-sum game with action sets X ,Q. Then, a pair of strategies u\u2217 \u2208 \u2206(X ) and w\u2217 \u2208 \u2206(Q) form an \u03b1-approximate mixed Nash equilibrium if\nA(u\u2217, w) \u2264 vA + \u03b1 and A(u,w\u2217) \u2265 vA \u2212 \u03b1\nfor every strategy u \u2208 \u2206(X ), w \u2208 \u2206(Q).\nIf the true database D is normalized to be a distribution D\u0302 in \u2206(X ), then D\u0302 always has zero payoff:\nA(D\u0302, w) = E x\u223cD\u0302,q\u223cw[q(x)\u2212 q(D)] = 0.\nHence, the value of the game vA is at most 0. Also, for any data strategy u, the payoff of query q is the negated payoff of the negated query q:\nA(u, q) = Ex\u223cu[q(x)\u2212 q(D)] = Ex\u223cu[q(D)\u2212 q(x)],\nwhich is A(u, q). Thus, any query strategy that places equal weight on q and q has expected payoff zero, so vA is at least 0. Hence, vA = 0.\nNow, let (u\u2217, w\u2217) be an \u03b1-approximate equilibrium. Suppose that the data player plays u\u2217, while the query player always plays query q. By the equilibrium guarantee, we then have A(u\u2217, q) \u2264 \u03b1, but the expected payoff on the left is simply q(D)\u2212 q(u\u2217). Likewise, if the query player plays the negated query q, then \u2212q(D) + q(u\u2217) = A(u\u2217, q) \u2264 \u03b1, so q(D)\u2212 q(u\u2217) \u2265 \u2212\u03b1. Hence for every query q \u2208 Q, we know |q(u\u2217)\u2212 q(D)| \u2264 \u03b1. This is precisely what we need for query release: we just need to privately calculate an approximate equilibrium."}, {"heading": "3.3 Solving the game", "text": "To construct the approximate equilibrium, we will use the multiplicative weights update algorithm (MW).2 This algorithm maintains a distribution over actions (initially uniform) over a series of steps. At each step, the MW algorithm receives a (possibly adversarial) loss for each action. Then, MW reweights the distribution to favor actions with less loss. The algorithm is presented in Algorithm 1.\nAlgorithm 1 The Multiplicative Weights Algorithm Let \u03b7 > 0 be given, let A be the action space Initialize A\u03031 uniform distribution on A For t = 1, 2, . . . , T :\nReceive loss vector `t For each a \u2208 A: Update At+1a = e\n\u2212\u03b7`taA\u0303ta for every a \u2208 A Normalize A\u0303t+1 = A\nt+1\u2211 i A t+1 i\nFor our purposes, the most important application of MW is to solving zero-sum games. Freund and Schapire [16] showed that if one player maintains a distribution over actions using MW, while the other player selects a best-response action versus the current MW distribution (i.e., an action that maximizes his expected payoff), the average MW distribution and empirical best-response distributions will converge to an approximate equilibrium rapidly.\n2 The MW algorithm has wide applications; it has been rediscovered in various guises several times. More details can be found in the comprehensive survey by Arora et al. [1].\nTheorem 3.2. Let \u03b1 > 0, and let A(i, j) \u2208 [\u22121, 1]m\u00d7n be the payoff matrix for a zero-sum game. Suppose the first player uses multiplicative weights over their actions to play distributions p1, . . . , pT , while the second player plays (\u03b1/2)-approximate best responses x1, . . . , xT , i.e.,\nA(pt, xt) \u2265 max x A(pt, x)\u2212 \u03b1/2.\nSetting T = 16 log n/\u03b12 and \u03b7 = \u03b1/4 in the MW algorithm, the empirical distributions\n1\nT T\u2211 i=1 pi and 1 T T\u2211 i=1 xi\nform an \u03b1-approximate mixed Nash equilibrium.\nInterpreted this way, the private multiplicative weights algorithm of Hardt and Rothblum [18] (and the MWEM algorithm of Hardt et al. [19]) uses MW for the data player, while the query player plays best responses. For privacy, their algorithm selects the query best-responses privately via the exponential mechanism of McSherry and Talwar [26]. Our algorithm plays the same game, but with the roles reversed."}, {"heading": "4 Dual query release", "text": "At a high level, our algorithm solves the query release game using an approach dual to the MWEM algorithm of Hardt et al. [19]. While MWEM uses a no-regret algorithm to maintain the data player\u2019s distribution, we will instead use a no-regret algorithm for the query player\u2019s distribution. Likewise, instead of finding a maximum payoff query at each round, our algorithm selects a minimum payoff record at each turn. The full algorithm can be found in Algorithm 2.\nOur privacy argument differs slightly from the analysis for MWEM. There, the data distribution is public, and finding a query with high error requires access to the private data. Our algorithm instead maintains a distribution Q over queries which depends directly on the private data, so we cannot use Q directly. Instead, we argue that queries sampled from Q are privacy preserving. Then, we can use a non-private optimization method to find a minimal error record versus queries sampled from Q. We then trade off privacy (which degrades as we take more samples) with accuracy (which improves as we take more samples, since the distribution of sampled queries converges to Q).\nGiven known hardness results for the query release problem [32], our algorithm must have worst-case runtime polynomial in the universe size |X |, so is not theoretically more efficient than prior approaches. In fact, even compared to prior work on query release (e.g., Hardt and Rothblum [18]), our algorithm has a worse accuracy guarantee.\nHowever, our approach has important practical benefits, the most important being that that hard step can be handled with standard, non-private solvers. The rest of our algorithm runs in time linear in the number of queries, independent of the dimension of the data.\nThe iterative structure of our algorithm, combined with our use of constraint solvers, allows for several heuristics improvements. For instance, we may run for fewer iterations than predicted by theory. Or, if the optimization problem turns out to be hard (even in practice), we can stop the solver early at a suboptimal (but often still good) solution. These heuristic tweaks can improve accuracy beyond what is guaranteed by our accuracy theorem, while always maintaining a strong provable privacy guarantee.\nAlgorithm 2 DualQuery Input: Database D \u2208 R|X | (normalized) and linear queries q1, . . . , qk \u2208 {0, 1}|X |. Initialize: Let Q = \u22c3k j=1 qj \u222a qj , Q1 uniform distribution on Q,\nT = 16 log |Q|\n\u03b12 , \u03b7 =\n\u03b1 4 , s =\n48 log (\n2|X |T \u03b2 ) \u03b12 .\nFor t = 1, . . . , T : Sample s queries {qi} from Q according to Qt. Let q := 1s \u2211 i qi.\nFind xt with \u3008q, xt\u3009 \u2265 maxx\u3008q, x\u3009 \u2212 \u03b1/4. Update: For each q \u2208 Q: Qt+1q := exp(\u2212\u03b7\u3008q, xt \u2212D\u3009) \u00b7Qtq.\nNormalize Qt+1. Output synthetic database D\u0302 := \u22c3T t=1 x t."}, {"heading": "4.1 Privacy", "text": "The privacy proofs are largely routine, based on the composition theorems. Rather than fixing \u03b5 and solving for the other parameters, we present the privacy cost \u03b5 as function of parameters T, s, \u03b7. Later, we will tune these parameters for our experimental evaluation. We will use the privacy of the following mechanism (due to McSherry and Talwar [26]) as an ingredient in our privacy proof.\nDefinition 4.1 (McSherry and Talwar [26]). Given some arbitrary output range R, the exponential mechanism with score function S selects and outputs an element r \u2208 R with probability proportional to\nexp\n( \u03b5S(D, r)\n2\u2206\n) ,\nwhere \u2206 is the sensitivity of S, defined as\n\u2206 = max D,D\u2032:|D4D\u2032|=1,r\u2208R\n|S(D, r)\u2212 S(D\u2032, r)|.\nThe exponential mechanism is \u03b5-differentially private.\nWe first prove pure \u03b5-differential privacy.\nTheorem 4.2. DualQuery is \u03b5-differentially private for\n\u03b5 = \u03b7T (T \u2212 1)s\nn .\nProof. We will argue that sampling from Qt is equivalent to running the exponential mechanism with some quality score. At round t, let {xi} for i \u2208 [t \u2212 1] be the best responses for the previous rounds. Let r(q, d) be defined by\nr(q,D) = t\u22121\u2211 i=1 (q(xi)\u2212 q(D)),\nwhere q \u2208 Q is a query and D is the true database. This function is evidently ((t \u2212 1)/n)-sensitive in D: changing D changes each q(D) by at most 1/n. Now, note that sampling from Qt is simply sampling from the exponential mechanism, with quality score r(q,D). Thus, the privacy cost of each sample in round t is \u03b5\u2032t = 2\u03b7(t\u2212 1)/n (Definition 4.1).\nBy the standard composition theorem (Lemma 2.4), the total privacy cost is\n\u03b5 = T\u2211 t=1 s\u03b5\u2032t = 2\u03b7s n \u00b7 T\u2211 t=1 (t\u2212 1) = \u03b7T (T \u2212 1)s n .\nWe next show that DualQuery is (\u03b5, \u03b4)-differentially private, for a much smaller \u03b5.\nTheorem 4.3. Let 0 < \u03b4 < 1. DualQuery is (\u03b5, \u03b4)-differentially private for\n\u03b5 = 2\u03b7(T \u2212 1) n \u00b7 [\u221a 2s(T \u2212 1) log(1/\u03b4) + s(T \u2212 1) ( exp ( 2\u03b7(T \u2212 1) n ) \u2212 1 )] .\nProof. Let \u03b5 be defined by the above equation. By the advanced composition theorem (Lemma 2.4), running a composition of k \u03b5\u2032-private mechanisms is (\u03b5, \u03b4)-private, for\n\u03b5 = \u221a 2k log(1/\u03b4)\u03b5\u2032 + k\u03b5\u2032(exp(\u03b5\u2032)\u2212 1).\nAgain, note that sampling from Qt is simply sampling from the exponential mechanism, with a (T \u2212 1)/nsensitive quality score. Thus, the privacy cost of each sample is \u03b5\u2032 = 2\u03b7(T \u2212 1)/n (Definition 4.1). We plug in k = s(T \u2212 1) samples, as in the first round our samplings are 0-differentially private."}, {"heading": "4.2 Accuracy", "text": "The accuracy proof proceeds in two steps. First, we show that \u201caverage query\u201d formed from the samples is close to the true weighted distribution Qt. We will need a standard Chernoff bound.\nLemma 4.4 (Chernoff bound). Let X1, . . . , XN be IID random variables with mean \u00b5, taking values in [0, 1]. Let X = 1N \u2211 iXi be the empirical mean. Then,\nPr[|X \u2212 \u00b5| > T ] < 2 exp(\u2212NT 2/3)\nfor any T .\nLemma 4.5. Let \u03b2 \u2208 (0, 1), and let p be a distribution over queries. Suppose we draw\ns = 48 log\n( 2|X | \u03b2 ) \u03b12\nsamples {q\u0302i} from p, and let q be the aggregate query\nq = 1\ns s\u2211 i=1 q\u0302i.\nDefine the true weighted answer Q(x) to be\nQ(x) = |Q|\u2211 i=1 piqi(x).\nThen with probability at least 1\u2212 \u03b2, we have |q(x)\u2212Q(x)| < \u03b1/4 for every x \u2208 X .\nProof. For any fixed x, note that q(x) is the average of random variables q\u03021(x), . . . , q\u0302s(x). Also, note that E[q(x)] = Q(x). Thus, by the Chernoff bound (Lemma 4.4) and our choice of s,\nPr[|q(x)\u2212Q(x)| > \u03b1/4] < 2 exp(\u2212s\u03b12/48) = \u03b2/|X |.\nBy a union bound over x \u2208 X , this equation holds for all x \u2208 X with probability at least 1\u2212 \u03b2.\nNext, we show that we compute an approximate equilibrium of the query release game. In particular, the record best responses form a synthetic database that answer all queries in Q accurately. Note that our algorithm doesn\u2019t require an exact best response for the data player; an approximate best response will do.\nTheorem 4.6. With probability at least 1\u2212\u03b2, DualQuery finds a synthetic database that answers all queries in Q within additive error \u03b1.\nProof. As discussed in Section 3, it suffices to show that the distribution of best responses x1, . . . , xT forms is an \u03b1-approximate equilibrium strategy in the query release game. First, we set the number of samples s according to in Lemma 4.5 with failure probability \u03b2/T . By a union bound over T rounds, sampling is successful for every round with probability at least 1\u2212 \u03b2; condition on this event.\nSince we are finding an \u03b1/4 approximate best response to the sampled aggregate query q, which differs from the true distribution by at most \u03b1/4 (by Lemma 4.5), each xi is an \u03b1/4 + \u03b1/4 = \u03b1/2 approximate best response to the true distribution Qt. Since q takes values in [0, 1], the payoffs are all in [\u22121, 1]. Hence, Theorem 3.2 applies; setting T and \u03b7 accordingly gives the result.\nRemark 4.7. The guarantee in Theorem 4.6 may seem a little unusual, since the convention in the literature is to treat \u03b5, \u03b4 as inputs to the algorithm. We can do the same: from Theorem 4.3 and plugging in for T, \u03b7, s, we have\n\u03b5 = 4\u03b7T\n\u221a 2sT log(1/\u03b4)\nn\n= 256 log3/2 |Q| \u221a 6 log(1/\u03b4) log(2|X |T/\u03b2) \u03b13n .\nSolving for \u03b1, we find\n\u03b1 = O\n( log1/2 |Q| log1/6(1/\u03b4) log1/6(2|X |/\u03b3)\nn1/3\u03b51/3\n) ,\nfor \u03b3 < \u03b2/T ."}, {"heading": "5 Case study: 3-way marginals", "text": "In our algorithm, the computationally difficult step is finding the data player\u2019s approximate best response against the query player\u2019s distribution. As mentioned above, the form of this problem depends on the particular query classQ. In this section, we first discuss the optimization problem in general, and then specifically for the well-studied class of marginal queries."}, {"heading": "5.1 The best-response problem", "text": "Recall that the query release game has payoff A(x, q) defined by Equation (1); the data player tries to minimize the payoff, while the query player tries to maximize it. If the query player has distribution Qt over queries, the data player\u2019s best response minimizes the expected loss:\nargmin x\u2208X E q\u2190Qt [q(D)\u2212 q(x)] .\nFor privacy reasons, the data player actually plays against the distribution of samples q\u03021, . . . , q\u0302s. So, the best-response problem is\nargmin x\u2208X\n1\ns s\u2211 i=1 q\u0302i(D)\u2212 q\u0302i(x) = argmax x\u2208X s\u2211 i=1 q\u0302i(x),\nsinceD is fixed and q\u0302i are linear queries. By Theorem 4.6 it even suffices to find an approximate maximizer, in order to guarantee accuracy."}, {"heading": "5.2 3-way marginal queries", "text": "To look at the precise form of the best-response problem, we consider 3-way marginal queries. We think of records as having d binary attributes, so that the data universe |X | is all bitstrings of length d. We write xi for x \u2208 X to mean the ith bit of record x.\nDefinition 5.1. Let X = {0, 1}d. A 3-way marginal query is a linear query specified by 3 integers a 6= b 6= c \u2208 [d], taking values\nqabc(x) = { 1 : xa = xb = xc = 1 0 : otherwise.\nThough everything we do will apply to general k-way marginals, for concreteness we work with 3-way marginals. Recall that the query class Q includes each query and its negation. So, we also have negated conjunctions:\nqabc(x) = { 0 : xa = xb = xc = 1 1 : otherwise.\nGiven sampled conjunctions {u\u0302i} and negated conjunctions {v\u0302i}, the best-response problem is\nargmax x\u2208X \u2211 i u\u0302i(x) + \u2211 j v\u0302j(x).\nIn other words, this is a MAXCSP problem\u2014we can associate a clause to each conjunction:\nqabc \u21d2 (xa \u2227 xb \u2227 xc) and qabc \u21d2 (xa \u2228 xb \u2228 xc),\nand we want to find x \u2208 {0, 1}d satisfying as many clauses as possible.3 Since most solvers do not directly handle MAXCSP problems, we convert this optimization problem into a more standard, integer program form. We introduce a variable xi for each literal xi, a variable ci 3 Note that this is almost a MAX3SAT problem, except there are also \u201cnegative\u201d clauses.\nfor each sampled conjunction u\u0302i, a variable di for each sampled negated conjunction v\u0302i, and we form the following integer program.\nmax \u2211 i ci + \u2211 j dj\nwith \u2200u\u0302i = qabc : xa + xb + xc \u2265 3ci \u2200v\u0302j = qabc : (1\u2212 xa) + (1\u2212 xb) + (1\u2212 xc) \u2265 dj xi, ci, di \u2208 {0, 1}\nNote xi, 1 \u2212 xi corresponds to the literals xi, xi, ci = 1, di = 1 exactly when their respective clauses are satisfied. Thus, the objective is the number of satisfied clauses. The resulting integer program can be solved using many existing solvers; we use CPLEX.\n6 Experimental evaluation\nWe evaluate DualQuery on a large collection of 3-way marginal queries on several real datasets (Figure 1) and high dimensional synthetic data. Adult (census data) and KDD99 (network packet data) are from the UCI repository [2], and have a mixture of discrete (but non-binary) and continuous attributes, which we discretize into binary attributes. We also use the (in)famous Netflix [28] movie ratings dataset, with more than 17,000 binary attributes. More precisely, we can consider each attribute (corresponding to a movie) to be 1 if a user has watched that movie, and 0 otherwise.\nRather than set the parameters as in Algorithm 2, we experiment with a range of parameters. For instance, we frequently run for fewer rounds (lower T ) and take fewer samples (lower s). As such, the accuracy guarantee (Theorem 4.6) need not hold for our parameters. However, we find that our algorithm gives good error, often much better than predicted. In all cases, our parameters satisfy the privacy guarantee Theorem 4.3."}, {"heading": "6.1 Accuracy", "text": "We evaluate the accuracy of the algorithm on 500,000 3-way marginals on Adult, KDD99 and Netflix. We report maximum error in Figure 2, averaged over 5 runs. (Marginal queries have range [0, 1], so error 1 is trivial.) The runs are (\u03b5, 0.001)-differentially private, with \u03b5 ranging from 0.25 to 5.4 For the Adult and KDD99 datasets, we set step size \u03b7 = 2.0, sample size s = 1000 while varying the number of steps T according to the privacy budget \u03b5, using the formula from Theorem 4.3. For the Netflix dataset, we adopt the same heuristic except we set s to be 5000.\nThe accuracy improves noticeably when \u03b5 increases from 0.25 to 1 across 3 datasets, and the improvement diminishes gradually with larger \u03b5. With larger sizes, both KDD99 and Netflix datasets allow DualQuery to run with more steps and get significantly better error."}, {"heading": "6.2 Scaling to More Queries", "text": "Next, we evaluate accuracy and runtime when varying the number of queries. We use a set of 40,000 to 2 million randomly generated marginals Q on the KDD99 dataset and run DualQuery with (1, 0.001)- privacy. For all experiments, we use the same set of parameters: \u03b7 = 1.2, T = 170 and s = 1750. By Theorem 4.3, each run of the experiment satisfies (1, 0.001)-differential privacy. These parameters give stable performance as the query class Q grows. As shown in Figure 3, both average and max error remain mostly stable, demonstrating improved error compared to simpler perturbation approaches. For example,\n4 Since our privacy analysis follows from Lemma 2.4, our algorithm actually satisfies (\u03b5, \u03b4)-privacy for smaller values of \u03b4. For example, our algorithm is also ( \u221a 2\u03b5, \u03b4\u2032)-private for \u03b4\u2032 = 10\u22126. Similarly, we could choose any arbitrarily small value of \u03b4, and Lemma 2.4 would tell us that our algorithm was (\u03b5\u2032, \u03b4)-differentially private for an appropriate value \u03b5\u2032, which depends only sub-logarithmically on 1/\u03b4.\nthe Laplace mechanism\u2019s error growth rate is O( \u221a |Q|) under (\u03b5, \u03b4)-differential privacy. The runtime grows almost linearly in the number of queries, since we maintain a distribution over all the queries."}, {"heading": "6.3 Scaling to Higher Dimensional Data", "text": "Finally, we evaluate accuracy and runtime behavior for data dimension ranging from 50 to 512,000. We evaluate DualQuery under (1, 0.001)-privacy on 100,000 3-way marginals on synthetically genearted datasets. We report runtime, max, and average error over 3 runs in Figure 4; note the logarithmic scale for attributes axis. We do not include query evaluation in our time measurements\u2014this overhead is common to all approaches that answer a set of queries.\nWhen generating the synthetic data, one possibility is to set each attribute to be 0 or 1 uniformly at random. However, this generates very uniform synthetic data: a record satisfies any 3-way marginal with probability 1/8, so most marginals will have value near 1/8. To generate more challenging and realistic data, we pick a separate bias pi \u2208 [0, 1] uniformly at random for each attribute i. For each data point, we then set attribute i to be 1 independently with probability equal to pi. As a result, different 3-way marginals have different answers on our synthetic data.\nFor parameters, we fix step size \u03b7 to be 0.4, and increase the sample size s with the dimension of the data (from 200 to 50,000) at the expense of running fewer steps. For these parameters, our algorithm is (1, 0.001)-differentially private by Theorem 4.3. With this set of parameters, we are able to obtain 8% average error in an average of 10 minutes of runtime, excluding query evaluation."}, {"heading": "6.4 Methodology", "text": "In this section, we discuss our experimental setup in more detail.\nImplementation details. The implementation is written in OCaml, using the CPLEX constraint solver. We ran the experiments on a mid-range desktop machine with a 4-core Intel Xeon processor and 12 Gb of RAM. Heuristically, we set a timeout for each CPLEX call to 20 seconds, accepting the best current solution if we hit the timeout. For the experiments shown, the timeout was rarely reached.\nData discretization. We discretize KDD99 and Adult datasets into binary attributes by mapping each possible value of a discrete attribute to a new binary feature. We bucket continuous attributes, mapping each bucket to a new binary feature. We also ensure that our randomly generated 3-way marginal queries are sensible (i.e., they don\u2019t require an original attribute to take two different values).\nSetting free attributes. Since the collection of sampled queries may not involve all of the attributes, CPLEX often finds solutions that leave some attributes unspecified. We set these free attributes heuristically: for real data, we set the attributes to 0 as these datasets are fairly sparse; 5 for synthetic data, we set attributes to 0 or 1 uniformly at random. 6\n5 The adult and KDD99 datasets are sparse due to the way we discretize the data; for the Netflix dataset, most users have only viewed a tiny fraction of the 17,000 movies.\n6 For a more principled way to set these free attributes, the sparsity of the dataset could be estimated at a small additional cost to privacy.\nParameter tuning. DualQuery has three parameters that can be set in a wide variety of configurations without altering the privacy guarantee (Theorem 4.3): number of iterations (T ), number of samples (s), and learning rate (\u03b7), which controls how aggressively to update the distribution. For a fixed level of \u03b5 and \u03b4, there are many feasible private parameter settings.\nWhile the accuracy guarantee (Theorem 4.6) may not hold for a particular set of parameters, the empirical accuracy is much better than predicted. Performance depends strongly on the choice of parameters: T has an obvious impact, increasing s increases the number of constraints in the integer program for CPLEX. We have investigated a range of parameters; for the experiments we have used some informal heuristics coming from our observations.\nFor sparse datasets like the ones in Figure 1, with larger \u03b7 (in the range of 1.5 to 2.0) and smaller s (in the same order as the number of attributes), we obtain good accuracy when we set the free attributes to be 0. But for denser data like our synthetic data, we get better accuracy when we have smaller \u03b7 (around 0.4) and set the free attributes randomly. As for runtime, we observe that CPLEX could finish running quickly (in seconds) when the sample size is in about the same range as the number of attributes (factor of 2 or 3 different).\nParameter setting should be done under differential privacy for a truly realistic evaluation. Overall, we do not know of a principled approach to handling this issue; the problem of private parameter tuning is an area of active research (see e.g., Chaudhuri and Vinterbo [9])."}, {"heading": "7 Discussion and conclusion", "text": "We have given a new private query release mechanism that can handle datasets with dimensionality multiple orders of magnitude larger than what was previously possible. Indeed, it seems we have not reached the limits of our approach\u2014even on synthetic data with more than 500,000 attributes, DualQuery continues to generate useful answers with about 30 minutes of overhead on top of query evaluation (which by itself is on the scale of hours). We believe that DualQuery makes private analysis of high dimensional data practical for the first time.\nHowever, this remarkable improvement in running time is not free: our theoretical accuracy bounds are worse than those of previous approaches [18, 19]. For low dimensional datasets for which it is possible to maintain a distribution over records, the MWEM algorithm of Hardt et al. [19] likely remains the state of the art. Our work complements MWEM by allowing private data analysis on higher-dimensional data sets."}, {"heading": "A Case study: Parity queries", "text": "In this section, we show how to apply DualQuery to another well-studied class of queries: parities. Each specified by a subset S of features, these queries measure the number of records with an even number of bits on in S compared to the number of records with an odd number of bits on in S.\nDefinition A.1. Let X = {\u22121,+1}d. A k-wise parity query is a linear query specified by a subset of features S \u2286 [d] with |S| = k, taking values\nqS(x) = { +1 : even number of xi = +1 for i \u2208 S \u22121 : otherwise.\nLike before, we can define a negated k-wise parity query:\nqS(x) = { +1 : odd number of xi = +1 for i \u2208 S \u22121 : otherwise.\nBarak et al. [3] observed that answering k-way marginal queries can be reduced to answering k-wise parity queries to the same accuracy, in the following sense.\nTheorem A.2 (Barak et al. [3]). Let qS be a k-way marginal query specified by the set of features S, and let D be a database of records. Then,\nqS(D) = 1\n2k \u2211 T\u2286S pT (D),\nwhere pT is the parity query for features T .\nNote that the coefficients sum to 1: hence, answering parity queries with additive error \u03b1 is enough to answer marginal queries with additive error \u03b1. We now consider how to handle these queries with our algorithm. For the remainder, we specialize to k = 3. Like marginal queries, it suffices to give the bestresponse optimization problem; unlike marginal queries, we need to handle k-wise parities for every k \u2264 3 in order to apply Theorem A.2.\nGiven sampled parity queries {u\u0302i} and negated parity queries {v\u0302i}, the best response problem is to find the record x \u2208 X that takes value 1 on as many of these queries as possible. We can construct an integer program for this task: introduce d variables xi, and two variables cq, dq for each sampled query. The following integer program encodes the best-response problem.\nmax \u2211 i ci\nsuch that \u2200u\u0302i = qS . \u2211 j xSj = 2di + ci \u2212 1\n\u2200v\u0302i = qS . \u2211 j xSj = 2di + ci\nxi, ci, di \u2208 {0, 1}\nConsider the (non-negated) parity queries first. The idea is that each variable ci can be set to 1 exactly when the corresponding parity query takes value 1, i.e., when x has an even number of bits in S set to +1. Since |S| \u2264 3, this even number will either be 0 or 2, hence is equal to 2di for di \u2208 {0, 1}. A similar argument holds for the negated parity queries."}], "references": [{"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["Boaz Barak", "Kamalika Chaudhuri", "Cynthia Dwork", "Satyen Kale", "Frank McSherry", "Kunal Talwar"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS), Beijing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Characterizing the sample complexity of private learners", "author": ["Amos Beimel", "Kobbi Nissim", "Uri Stemmer"], "venue": "In ACM SIGACT Innovations in Theoretical Computer Science (ITCS), Berkeley,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A learning theory approach to noninteractive database privacy", "author": ["A. Blum", "K. Ligett", "A. Roth"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Practical privacy: the sulq framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Sample complexity bounds for differentially private learning", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A stability-based validation procedure for differentially private machine learning", "author": ["Kamalika Chaudhuri", "Staal A. Vinterbo"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand Sarwate", "Kaushik Sinha"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Local privacy and statistical minimax rates", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS), Berkeley, California,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "author": ["C. Dwork", "M. Naor", "O. Reingold", "G.N. Rothblum", "S.P. Vadhan"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Boosting and differential privacy", "author": ["C. Dwork", "G.N. Rothblum", "S. Vadhan"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Game theory, on-line prediction and boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Conference on Computational Learning Theory (CoLT), Desenzano sul Garda, Italy,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Iterative constructions and private data release", "author": ["A. Gupta", "A. Roth", "J. Ullman"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A multiplicative weights mechanism for privacy-preserving data analysis", "author": ["Moritz Hardt", "Guy N. Rothblum"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A simple and practical algorithm for differentially private data release", "author": ["Moritz Hardt", "Katrina Ligett", "Frank McSherry"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Differential privacy for the analyst via private equilibrium computation", "author": ["Justin Hsu", "Aaron Roth", "Jonathan Ullman"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael J. Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Private convex empirical risk minimization and high-dimensional regression", "author": ["Daniel Kifer", "Adam Smith", "Abhradeep Thakurta"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "An adaptive mechanism for accurate query answering under differential privacy", "author": ["Chao Li", "Gerome Miklau"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Optimizing linear counting queries under differential privacy", "author": ["Chao Li", "Michael Hay", "Vibhor Rastogi", "Gerome Miklau", "Andrew McGregor"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "In IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Nearly) optimal algorithms for private online learning in full-information and bandit settings", "author": ["Abhradeep G. Thakurta", "Adam Smith"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Answering n2+o(1) counting queries with differential privacy is hard", "author": ["J. Ullman"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC), Palo Alto, California,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "PCPs and the hardness of generating private synthetic data", "author": ["J. Ullman", "S.P. Vadhan"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}], "referenceMentions": [{"referenceID": 25, "context": "The competition was a great success (the winning team improved on the existing recommendation system by more than 10%), but the ad hoc anonymization was not as successful: Narayanan and Shmatikov [27] were later able to re-identify individuals in the dataset.", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "[6], performing private query release is sufficient to simulate any learning algorithm in the \u201cstatistical query model\u201d of Kearns [22].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[6], performing private query release is sufficient to simulate any learning algorithm in the \u201cstatistical query model\u201d of Kearns [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 11, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 12, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 16, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 15, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 17, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 16, "context": "Unfortunately, even the most efficient approaches have a per-query running time linear in the size of the data universe, which is exponential in the dimension of the data [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 28, "context": "Moreover, this running time is necessary in the worst case [32], especially if the algorithm produces synthetic data [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Moreover, this running time is necessary in the worst case [32], especially if the algorithm produces synthetic data [33].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "[19], who perform a thorough experimental evaluation of one such algorithm, which they called MWEM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] showed how to convert learning algorithms in the SQ model of Kearns [22] into differentially private learning algorithms with similar accuracy guarantees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[6] showed how to convert learning algorithms in the SQ model of Kearns [22] into differentially private learning algorithms with similar accuracy guarantees.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 5, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 2, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 10, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 6, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 8, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 26, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 21, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 9, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 27, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 3, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 11, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 12, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 16, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 15, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 16, "context": "But, all of these algorithms (including the notable multiplicative weights algorithm of Hardt and Rothblum [18], which achieves the theoretically optimal accuracy and runtime) have running time exponential in the dimension of the data.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "With standard cryptographic assumptions, this is necessary in the worst case for mechanisms that answer many arbitrary statistical queries [32].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "[19], which is based on the private multiplicative weights mechanism [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19], which is based on the private multiplicative weights mechanism [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "[19] were able to implement the multiplicative weights algorithm on several real datasets with up to 77 attributes (and even more when the queries are restricted to take positive values only 1 Hardt et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] are able to scale up to 1000 features on synthetic data when the features are partitioned into a number of small buckets, and the queries are chosen to never depend on features in more than one bucket.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Another family of query release algorithms are based on the Matrix Mechanism [25, 24].", "startOffset": 77, "endOffset": 85}, {"referenceID": 22, "context": "Another family of query release algorithms are based on the Matrix Mechanism [25, 24].", "startOffset": 77, "endOffset": 85}, {"referenceID": 18, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] solves the game by having a data player use a no-regret learning algorithm, while the query player repeatedly best responds by optimizing over queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14]; the main difference is that our optimization problem is over single records rather than sets of records.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For any predicate \u03c6 : X \u2192 {0, 1}, the linear query Q\u03c6 : X n \u2192 [0, 1] is defined by", "startOffset": 62, "endOffset": 68}, {"referenceID": 12, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The analysis of our algorithm relies on the interpretation of query release as a two player, zero-sum game [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Freund and Schapire [16] showed that if one player maintains a distribution over actions using MW, while the other player selects a best-response action versus the current MW distribution (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Interpreted this way, the private multiplicative weights algorithm of Hardt and Rothblum [18] (and the MWEM algorithm of Hardt et al.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "[19]) uses MW for the data player, while the query player plays best responses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "For privacy, their algorithm selects the query best-responses privately via the exponential mechanism of McSherry and Talwar [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Given known hardness results for the query release problem [32], our algorithm must have worst-case runtime polynomial in the universe size |X |, so is not theoretically more efficient than prior approaches.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": ", Hardt and Rothblum [18]), our algorithm has a worse accuracy guarantee.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "We will use the privacy of the following mechanism (due to McSherry and Talwar [26]) as an ingredient in our privacy proof.", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "1 (McSherry and Talwar [26]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": ", XN be IID random variables with mean \u03bc, taking values in [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Since q takes values in [0, 1], the payoffs are all in [\u22121, 1].", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "(Marginal queries have range [0, 1], so error 1 is trivial.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "To generate more challenging and realistic data, we pick a separate bias pi \u2208 [0, 1] uniformly at random for each attribute i.", "startOffset": 78, "endOffset": 84}, {"referenceID": 7, "context": ", Chaudhuri and Vinterbo [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "However, this remarkable improvement in running time is not free: our theoretical accuracy bounds are worse than those of previous approaches [18, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 17, "context": "However, this remarkable improvement in running time is not free: our theoretical accuracy bounds are worse than those of previous approaches [18, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 17, "context": "[19] likely remains the state of the art.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.", "creator": "LaTeX with hyperref package"}}}