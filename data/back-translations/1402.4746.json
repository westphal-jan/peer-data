{"id": "1402.4746", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2014", "title": "Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures", "abstract": "Statistical and machine learning algorithms are often applied to high-dimensional data. In many of these applications, data is scarce and often much more expensive than computing time. We offer the first sample-efficient polynomial time estimator for high-dimensional spherical Gaussian mixtures.", "histories": [["v1", "Wed, 19 Feb 2014 17:59:55 GMT  (61kb)", "http://arxiv.org/abs/1402.4746v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT stat.ML", "authors": ["ananda theertha suresh", "alon orlitsky", "jayadev acharya", "ashkan jafarpour"], "accepted": true, "id": "1402.4746"}, "pdf": {"name": "1402.4746.pdf", "metadata": {"source": "CRF", "title": "Near-optimal-sample estimators for spherical Gaussian mixtures", "authors": ["Jayadev Acharya", "Ashkan Jafarpour"], "emails": ["jacharya@ucsd.edu", "ashkan@ucsd.edu", "alon@ucsd.edu", "asuresh@ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n47 46\nv1 [\ncs .L\nG ]\n1 9\nFe b\nFor mixtures of any k d-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses Ok(d log2 d\u01eb4 ) samples and runs in time Ok,\u01eb(d3 log5 d), both significantly lower than previously known. The constant factor Ok is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that \u2126k( d\u01eb2 ) samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions.\nWe also derive a simple estimator for k-component one-dimensional mixtures that uses O(k log k\u01eb \u01eb2 ) samples and runs in time O\u0303 ((k\n\u01eb )3k+1). Our other technical contributions include a faster algorithm for\nchoosing a density estimate from a set of distributions, that minimizes the \u21131 distance to an unknown underlying distribution.\n\u2217jacharya@ucsd.edu \u2020ashkan@ucsd.edu \u2021alon@ucsd.edu \u00a7asuresh@ucsd.edu"}, {"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Background", "text": "Meaningful information often resides in high-dimensional spaces: voice signals are expressed in many frequency bands, credit ratings are influenced by multiple parameters, and document topics are manifested in the prevalence of numerous words. Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].\nTypically, information can be generated by different types of sources: voice is spoken by men or women, credit parameters correspond to wealthy or poor individuals, and documents address topics such as sports or politics. In such cases the overall data follow a mixture distribution [26, 36, 38].\nMixtures of high-dimensional distributions are therefore central to the understanding and processing of many natural phenomena. Methods for recovering the mixture components from the data have consequently been extensively studied by statisticians, engineers, and computer scientists.\nInitially, heuristic methods such as expectation-maximization (EM) were developed [27, 35]. Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23]. Many of these algorithms consider mixtures where the \u21131 distance between the mixture components is 2 \u2212 od(1), namely approaches the maximum of 2 as d increases. They identify the distribution components in time and samples that grow polynomially in the dimension d. Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time. However, their algorithm uses > d100 time and samples.\nA different approach that avoids the large component-distance requirement and the high time and sample complexity, considers a slightly more relaxed notion of approximation, sometimes called PAC learning. PAC learning [24] does not approximate each mixture component, but instead derives a mixture distribution that is close to the original one. Specifically, given a distance bound \u01eb > 0, error probability \u03b4 > 0, and samples from the underlying mixture f , where we use boldface letters for d-dimensional objects, PAC learning seeks a mixture estimate f\u0302 with at most k components such that D(f , f\u0302) \u2264 \u01eb with probability \u2265 1\u2212 \u03b4, where D(\u22c5, \u22c5) is some given distance measure, for example \u21131 distance or KL divergence. This notion of estimation is also known as proper learning in the literature.\nAn important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means. Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42]."}, {"heading": "1.2 Sample complexity", "text": "Reducing the number of samples is of great practical significance. For example, in topic modeling every sample is a whole document, in credit analysis every sample is a person\u2019s credit history, and in genetics, every sample is a human DNA. Hence samples can be very scarce and obtaining them can be very costly. By contrast, current CPUs run at several Giga Hertz, hence samples are typically much more scarce of a resource than time.\nNote that for one-dimensional statistical problems, the need for sample-efficient algorithms has been broadly recognized. The sample complexity of many problems is known quite accurately, often to within a constant factor. For example, for discrete distributions over {1, . . . ,s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.\nLearning one-dimensional m-modal distributions over {1, . . . ,s} requires \u0398(m log(s/m)/\u01eb3) samples [14]. Similarly, one-dimensional mixtures of k structured distributions (log-concave, monotone hazard rate, and unimodal) over {1, . . . ,s} can be learned with O(k/\u01eb4), O(k log(s/\u01eb)/\u01eb4), and O(k log(s)/\u01eb4) samples, respectively, and these bounds are tight up to a factor of \u01eb [31].\nCompared to one dimensional problems, in high dimensions there is a polynomial gap in the sample complexity. For example, for learning spherical Gaussian mixtures, the number of samples required by previous algorithms is O(d12) for k = 2 components, and increased exponentially with k [19]. In this paper we bridge this gap, by constructing near-linear sample complexity estimators."}, {"heading": "1.3 Previous and new results", "text": "Our main contribution is PAC learning d dimensional Gaussian mixtures with near-linear samples. We show few auxiliary results for one-dimensional Gaussians.\n1.3.1 d-dimensional Gaussian mixtures\nSeveral papers considered PAC learning of discrete- and Gaussian-product mixtures. [20] considered mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0. They showed that this class is PAC learnable in O\u0303(d2/\u01eb4) time and samples, where the O\u0303 notation hides logarithmic factors. [18] eliminated the probability constraints and generalized the results from binary to arbitrary discrete alphabets, and from 2 to k mixture components. They showed that mixtures of k discrete products are PAC learnable in O\u0303((d/\u01eb)2k2(k+1)) time, and although they did not explicitly mention sample complexity, their algorithm uses O\u0303((d/\u01eb)4(k+1)) samples. [19] generalized these results to Gaussian products, showing in particular that mixtures of k Gaussians, where the difference between the means normalized by the ratio of standard deviations is bounded by B, are PAC learnable in O\u0303((dB/\u01eb)2k2(k+1)) time, and can be shown to use O\u0303((dB/\u01eb)4(k+1)) samples. These algorithms consider the KL divergence between the distribution and its estimate, but it can be shown that the \u21131 distance would result in similar complexities. It can also be shown that these algorithms or their simple modifications have similar time and sample complexities for spherical Gaussians as well.\nOur main contribution shows that mixtures of spherical-Gaussians are PAC learnable in \u21131 distance with sample complexity that is nearly linear in the dimension. Specifically, Theorem 8 shows that mixtures of k spherical-Gaussian distributions can be learned in\nn = O(dk9 \u01eb4 log2 d \u03b4 ) = Ok,\u01eb(d log2 d)\nsamples and\nO(n2d logn + d2(k7 \u01eb3 log d \u03b4 )k2) = O\u0303k,\u01eb(d3).\ntime. Observe that recent algorithms typically construct the covariance matrix [19,42], hence require \u2265 nd2 time. In that sense, for small values of k, the time complexity we derive is comparable to the best such algorithms can hope for. Observe also that the exponential dependence on k is of the form d2(k7 \u01eb3 log d \u03b4 )k2 , which is significantly lower than the dO(k 3) dependence in previous results.\nBy contrast, Theorem 2 shows that PAC learning k-component spherical Gaussian mixtures require \u2126(dk/\u01eb2) samples for any algorithm, hence our distribution learning algorithms are nearly sample optimal. In addition, their time complexity significantly improves on previously known ones."}, {"heading": "1.3.2 One-dimensional Gaussian mixtures", "text": "Independently and around the same time as this work [15] showed that mixtures of two one-dimensional Gaussians can be learnt with O\u0303(\u01eb\u22122) samples and in time O(\u01eb\u22127.01). We provide a natural estimator for learning mixtures of k one dimensional Gaussians using some basic properties of Gaussian distributions and show that mixture of any k-one dimensional Gaussians can be learnt with O\u0303(k\u01eb\u22122) samples and in time O\u0303 ((k\n\u01eb )3k+1)."}, {"heading": "1.4 The approach and technical contributions", "text": "The popular SCHEFFE estimator takes a collection F of distributions and uses O(log \u2223F\u2223) independent samples from an underlying distribution f to find a distribution in F whose distance from f is at most a constant factor larger than that of the distribution in F that is closet to f [16]. In Lemma 1, we lower the time complexity of the Scheffe algorithm from O(\u2223F\u22232) time to O\u0303(\u2223F\u2223), helping us reduce the time complexity of our algorithms.\nOur goal is therefore to construct a small class of distributions that is \u01eb-close to any possible underlying distribution. For simplicity, consider spherical Gaussians with the same variance and means bounded by B. Take the collection of all distributions derived by quantizing the means of all components in all coordinates to \u01ebm accuracy, and quantizing the weights to \u01ebw accuracy. It can be shown that to get distance \u01eb from the underlying distribution, it suffices to take \u01ebm, \u01ebw \u2264 1/poly\u01eb(dk). There are at most ( B\u01ebm )dk \u22c5 ( 1\u01ebw )k = 2O\u0303\u01eb(dk) possible combinations of the k mean vectors and weights. Hence SCHEFFE implies an exponential-time algorithm with sample complexity O\u0303(dk).\nTo reduce the dependence on d, one can approximate the span of the k mean vectors. This reduces the problem from d to k dimensions, allowing us to consider a distribution collection of size 2O(k\n2), with SCHEFFE sample complexity of just O(k2). [18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors. This approach requires the k columns of the sample correlation matrix to be very close to the actual correlation matrix, and thus requires a lot more samples.\nWe derive a spectral algorithm that uses the top k eigenvectors of the sample covariance matrix to approximate the span of the k mean vectors. Since we use the entire covariance matrix instead of just k columns, a weaker concentration is sufficient and we gain on the sample complexity.\nUsing recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in O\u0303(d) samples. This result allows us to address most \u201creasonable\u201d distributions, but still there are some \u201ccorner cases\u201d that need to be analyzed separately. To address them, we modify some known clustering algorithms such as single-linkage, and spectral projections. While the basic algorithms were known before, our contribution here, which takes a fair bit of effort and space, is to show that judicious modifications of the algorithms and rigorous statistical analysis yield polynomial time algorithms with near optimal sample complexity.\nOur approach applies most directly to mixtures of spherical Gaussians. We provide a simple and practical recursive clustering and spectral algorithm that estimates all such distributions in Ok(d log2 d) samples.\nThe paper is organized as follows. In Section 2, we introduce notations, describe results on the Scheffe estimator, and state a lower bound. In Section 3, we present the algorithm for k-spherical Gaussians. In Section 4 we show a simple learning algorithm for one-dimensional Gaussian mixtures. To preserve readability, most of the technical details and proofs are given in the appendix."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Notation", "text": "For arbitrary product distributions p1, . . . ,pk over a d dimensional space let pj,i be the distribution of pj over coordinate i, and let \u00b5j,i and \u03c3j,i be the mean and variance of pj,i respectively. Let f = (w1, . . . ,wk,p1, . . . ,pk) be the mixture of these distributions with mixing weights w1, . . . ,wk . We denote estimates of a quantity x by x\u0302. It can be empirical mean or a more complex estimate. \u2223\u2223\u22c5\u2223\u2223 denotes the spectral norm of a matrix and\u2223\u2223\u22c5\u2223\u22232 denotes the \u21132 norm of a vector."}, {"heading": "2.2 Selection from a pool of distributions", "text": "Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20]. Our algorithm also obtains a set of distributions containing at least one that is close to the underlying in \u21131 distance. The estimation problem now reduces to the following. Given a class F of distributions and samples from an unknown distribution f , find a distribution in F that is close to f . Let D(f ,F) def= minfi\u2208F D(f , fi).\nThe well-known Scheffe\u2019s method [16] uses O(\u01eb\u22122 log \u2223F\u2223) samples from the underlying distribution f , and in time O(\u01eb\u22122\u2223F\u22232T log \u2223F\u2223) outputs a distribution in F with \u21131 distance of at most 9.1max(D(f ,F), \u01eb) from f , where T is the time required to compute the probability of an x \u2208 X by a distribution in F . A naive application of this algorithm requires time quadratic in the number of distributions in F . We propose a variant of this, that works in near linear time, albeit requiring slightly more samples. More precisely,\nLemma 1 (Appendix B). Let \u01eb > 0. For some constant c, given c \u01eb2 log( \u2223F\u2223 \u03b4 ) independent samples from a distribution f , with probability \u2265 1 \u2212 \u03b4, the output f\u0302 of MODIFIED SCHEFFE D(f\u0302 , f) \u2264 1000max(\u01eb,D(f ,F)). Furthermore, the algorithm runs in time O( \u2223F\u2223T log(\u2223F\u2223/\u03b4)\n\u01eb2 ).\nWe therefore find a small class F with at least one distribution close to the underlying mixture. For our problem of estimating k component mixtures in d-dimensions, T = O(dk) and \u2223F\u2223 = O\u0303k,\u01eb(d2). Note that we have not optimized the constant 1000 in the above lemma."}, {"heading": "2.3 Lower bound", "text": "Using Fano\u2019s inequality, we show an information theoretic lower bound of \u2126(dk/\u01eb2) samples to learn kcomponent d-dimensional mixtures of spherical Gaussians for any algorithm. More precisely,\nTheorem 2 (Appendix C). Any algorithm that learns all k-component d-dimensional spherical Gaussian mixtures up to \u21131 distance \u01eb with probability \u2265 1/2 requires at least \u2126(dk\u01eb2 ) samples."}, {"heading": "3 Mixtures in d dimensions", "text": "3.1 Description of LEARN k-SPHERE\nAlgorithm LEARN K-SPHERE learns mixtures of k spherical Gaussians using near-linear samples. For clarity, we assume that all components have the same variance \u03c32, i.e., pi = N(\u00b5i, \u03c32Id) for 1 \u2264 i \u2264 k. A modification of this algorithm works for components with different variances. The core ideas are same and we include it in the final version of the paper.\nThe easy part of the algorithm is estimating \u03c32. If X(1) and X(2) are two samples from the same component, then X(1)\u2212X(2) is distributed N(0,2\u03c32Id). Hence for large d, \u2223\u2223X(1) \u2212X(2)\u2223\u222322 concentrates around 2d\u03c32. By the pigeon-hole principle, given k+1 samples, two of them are from the same component. Therefore, the minimum pairwise distance between k + 1 samples is close to 2d\u03c32. This constitutes the first step of our algorithm.\nWe now concentrate on estimating the means. As stated in the introduction, given the span of the mean vectors \u00b5i, we can grid the k dimensional span to the required accuracy \u01ebg and use SCHEFFE, to obtain a polynomial time algorithm. One of the natural and well-used methods to estimate the span of mean vectors is using the correlation matrix [42]. Consider the correlation-type matrix,\nS = 1\nn n\u2211 i=1 X(i)X(i)t \u2212 \u03c32Id. In expectation, the fraction of terms from pi is wi. Furthermore for a sample X from a particular component j, E[XXt] = \u03c32Id +\u00b5j\u00b5j t. It follows that\nE[S] = k\u2211 j=1 wj\u00b5j\u00b5j t.\nTherefore, as n\u2192\u221e, the matrix S converges to \u2211kj=1wj\u00b5j\u00b5jt, and its top k eigenvectors span of means. While the above intuition is well understood, the number of samples necessary for convergence is not well studied. Ideally, irrespective of the values of the means, we wish O\u0303(d) samples to be sufficient for the convergence. However this is not true, as we demonstrate by a simple example. Example 3. Consider the special case, d = 1, k = 2, \u03c32 = 1, w1 = w2 = 1/2, and the difference of means\u2223\u00b51 \u2212 \u00b52\u2223 = L for a large L \u226b 1. Given this prior information, one can estimate the the average of the mixture, that yields \u00b51+\u00b52\n2 . Solving equations obtained by \u00b51 + \u00b52 and \u00b51 \u2212 \u00b52 = L, yields \u00b51 and \u00b52. The\nvariance of the mixture is 1 + L2 4 > L2 4 . With additional Chernoff type bounds, one can show that given n samples the error in estimating the average is\n\u2223\u00b51 + \u00b52 \u2212 \u00b5\u03021 \u2212 \u00b5\u03022\u2223 \u2248 \u0398( L\u221a n ) .\nTherefore to estimate the means to a small accuracy we need n \u2265 L2, i.e., more the separation, more samples are necessary.\nA similar phenomenon happens in the convergence of the correlation matrices, where the variances of quantities of interest increases with separation. In other words, for the span to be accurate the number of samples necessary increases with the separation. To overcome this phenomenon, a natural idea is to cluster the Gaussians such that the means of components in the same cluster are close and then apply SCHEFFE on the span within each cluster.\nEven though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here. We use a simple recursive clustering algorithm that takes a cluster C with average \u00b5(C). If there is a component in the cluster such that \u221awi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 is \u2126(log(n/\u03b4)), then the algorithm divides the cluster into two nonempty clusters without any mis-clustering.\nFor technical reasons similar to the above example, we also use a coarse clustering algorithm that ensures that the mean separation is O\u0303(d1/4) within each cluster. The algorithm can be summarized as:\n1. Variance estimation: Use first k+1 samples and estimate the minimum distance among sample-pairs to estimate \u03c32.\n2. Coarse clustering: Using a single-linkage algorithm, group the samples such that within each cluster formed, the mean separation is smaller than O\u0303(d1/4).\n3. Recursive clustering: As long as there is a cluster that has samples from more than one component with means far apart, (described by a condition on the norm of its covariance matrix in the algorithm) estimate its largest eigenvector and project samples of this cluster onto this eigenvector and cluster them. This hierarchical method is continued until there are clusters that contain close-by-components. 4. Search in the span: The resulting clusters contain components that are close-by, i.e., \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u22232 <O(k3/2\u03c3\u03022 log n \u03b4 ). We approximate the span of means by the top k \u2212 1 eigenvectors and the mean\nvector, and perform an exhaustive search using MODIFIED SCHEFFE.\nWe now describe these steps stating the performance of each step.\nAlgorithm LEARN K-SPHERE Input: n samples x(1),x(2), . . . ,x(n) from f and \u01eb.\n1. Sample variance: \u03c3\u03022 =mina\u2260b\u2236a,b\u2208[k+1] \u2223\u2223x(a) \u2212 x(b)\u2223\u222322 /2d. 2. Coarse single-linkage clustering: Start with each sample as a cluster,\n\u2022 While \u2203 two clusters with squared-distance \u2264 2d\u03c3\u03022 + 23\u03c3\u03022\u221ad log n2 \u03b4 , merge them.\n3. Recursive spectral-clustering: While there is a new cluster C with \u2223C \u2223 \u2265 n\u01eb/5k and spectral norm of its sample covariance matrix \u2265 12k2\u03c3\u03022 logn3/\u03b4,\n\u2022 Use n\u01eb/8k2 of the samples to find the largest eigenvector and discard these samples. \u2022 Project the remaining samples on the largest eigenvector.\n\u2022 Perform single-linkage in the projected space (as before) till the distance between clusters > 3\u03c3\u0302 \u221a logn2k/\u03b4 creating new clusters.\n4. Exhaustive search: Let \u01ebg = \u01eb/(16k3/2), L = 200\u221ak4\u01eb\u22121 log n2\u03b4 , and G ={\u2212L, . . . ,\u2212\u01ebg,0, \u01ebg ,2\u01ebg, . . . L}. Let W = {0, \u01eb/(4k),2\u01eb/(4k), . . . 1} and \u03a3 def= {\u03c32 \u2236 \u03c32 = \u03c3\u03022(1 + i/d)\u2200 \u2212 d < i \u2264 d}.\n\u2022 For each cluster C find its top k \u2212 1 eigenvectors u1,u2 . . .uk\u22121 and let Span(C) = {\u00b5\u0302(C) +\u2211k\u22121i=1 gi\u03c3\u0302ui \u2236 g1, g2 . . . gk\u22121 \u2208 G}. \u2022 Let Span = {Span(C) \u2236 \u2223C \u2223 \u2265 n\u01eb/5k}. \u2022 For all w\u2032i \u2208 W , \u03c3\n\u20322 \u2208 \u03a3, \u00b5\u0302i \u2208 Span, add {(w\u20321, . . . ,w\u2032k\u22121,1 \u2212\u2211k\u22121i=1 w\u2032i,N(\u00b5\u03021, \u03c3\u20322), . . . ,N(\u00b5\u0302k, \u03c3\u20322)} in F .\n5. Run MODIFIED SCHEFFE on F and output the resulting distribution."}, {"heading": "3.2 Sketch of correctness", "text": "To simplify the bounds and expressions, we assume that d > 1000 and \u03b4 \u2265 min(2n2e\u2212d/10,1/3). For smaller values of \u03b4, we run the algorithm with error 1/3 and repeat it O(log 1 \u03b4 ) times to choose a set of\ncandidate mixtures F\u03b4. By Chernoff-bound with error \u2264 \u03b4, F\u03b4 contains a mixture \u01eb-close to f . Finally, we run MODIFIED SCHEFFE on F\u03b4 to obtain a mixture that is close to f . By the union bound and Lemma 1, the error is \u2264 2\u03b4.\nVariance estimation: Let \u03c3\u0302 be the variance estimate from step 1. In high dimensions, the difference between two random samples from a Gaussian concentrates. This is made precise in the next lemma which states \u03c3\u0302 is a good estimate of the variance. Then the following is a simple application of Gaussian tail bounds.\nLemma 4 (Appendix D.1). Given n samples from the k-component mixture, with probability 1 \u2212 2\u03b4, \u2223\u03c3\u03022 \u2212 \u03c32\u2223 \u2264 2.5\u03c32 \u221a log(n2/\u03b4)\nd .\nCoarse single-linkage clustering: The second step is a single-linkage routine that clusters mixture components with far means. Single-linkage is a simple clustering scheme that starts out with each data point as a cluster, and at each step merges the two that are closest to form larger clusters. The algorithm stops when the distance between clusters is larger than a pre-specified threshold.\nSuppose the samples are generated by an one-dimensional mixture of k components that are far, then with high probability, when the algorithm generates k clusters and all the samples within a cluster are generated by a single component. More precisely, if \u2200i, j \u2208 [k], \u2223\u00b5i \u2212 \u00b5j \u2223 = \u2126(\u03c3 logn), then all the n samples concentrate around their respective means and the separation between any two samples from different components would be larger than the largest separation between any two samples from the same component. Hence for a suitable value of threshold, single-linkage correctly identifies the clusters. For d-dimensional Gaussian mixtures a similar notion holds true, with minimum separation \u2126(d1/4 log n \u03b4 ). More precisely, Lemma 5 (Appendix D.2). After Step 2 of LEARN K-SPHERE, with probability \u2265 1 \u2212 2\u03b4, all samples from each component will be in the same cluster and the maximum distance between two components within each cluster is \u2264 10k\u03c3(d log n2 \u03b4 )1/4.\nRecursive spectral-clustering: The clusters formed at this step consists of components with mean separation O(d1/4 log n \u03b4 ). We now recursively zoom into the clusters formed and show that it is possible to cluster the components with much smaller mean separation. Note that since the matrix is symmetric, the largest magnitude of the eigenvalue is same as the spectral norm. We first find the largest eigenvector of\nS(C) def= 1\u2223C \u2223( \u2211 x\u2208C (x \u2212 \u00b5\u0302(C))(x \u2212 \u00b5\u0302(C))t) \u2212 \u03c3\u03022Id, which is the sample covariance matrix with its diagonal term reduced by \u03c3\u03022. If there are two components with means far apart, then using single-linkage we divide the cluster into two. The following lemma shows that this step performs accurate clustering of components with means well separated.\nLemma 6 (Appendix D.3). Let n \u2265 c \u22c5 dk4 \u01eb log n 3 \u03b4 . After recursive clustering, with probability \u2265 1 \u2212 4\u03b4. the\nsamples are divided into clusters such that for each component i within any cluster C , \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2264\n25\u03c3 \u221a k3 log n 3\n\u03b4 . Furthermore, all the samples from one component remain in a single cluster.\nExhaustive search and Scheffe: After step 3, all clusters have a small weighted radius \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2264\n25\u03c3 \u221a k3 log n 3 \u03b4 , the the eigenvectors give an accurate estimate of the span of \u00b5i\u2212\u00b5(C) within each cluster. More precisely,\nLemma 7 (Appendix D.4). Let n \u2265 c \u22c5 dk9 \u01eb4 log2 d \u03b4\nfor some constant c. After step 3, with probability \u2265 1 \u2212 7\u03b4 the following holds: if \u2223C \u2223 \u2265 n\u01eb/5k, then the projection of [\u00b5i \u2212\u00b5(C)]/ \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 on the space orthogonal to the span of top k \u2212 1 eigenvectors has magnitude \u2264 \u01eb\u03c3\n8 \u221a 2k \u221a wi\u2223\u2223\u00b5i\u2212\u00b5(C)\u2223\u22232 .\nWe now have accurate estimates of the spans of the clusters and each cluster has components with close means. It is now possible to grid the set of possibilities in each cluster to obtain a set of distributions such that one of them is close to the underlying. There is a trade-off between a dense grid to obtain a good estimation and the computation time required. The final step takes the sparsest grid possible to ensure an error \u2264 \u01eb. This is quantized below.\nTheorem 8 (Appendix D.5). Let n \u2265 c \u22c5 dk9 \u01eb4 log2 d \u03b4 for some constant c. Then Algorithm LEARN K-SPHERE\nwith probability \u2265 1 \u2212 9\u03b4, outputs a distribution f\u0302 such that D(f\u0302 , f) \u2264 1000\u01eb. Furthermore, the algorithm runs in time O(n2d logn + d2(k7\n\u01eb3 log d \u03b4 )k2).\nNote that the run time is calculated based on the efficient implementation of single-linkage [37] and the exponential term is not optimized. We now study mixtures in one-dimension and provide an estimator using MODIFIED SCHEFFE."}, {"heading": "4 Mixtures in one dimension", "text": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41]. We now provide a simple estimator for learning one dimensional mixtures using the MODIFIED SCHEFFE estimator proposed earlier. The d-dimension estimator uses spectral projections to find the span of means, whereas for one dimension case, we use a simple observation on properties of samples from Gaussians for estimation. Formally, given samples from f , a mixture of Gaussian distributions pi def = N(\u00b5i, \u03c32i ) with weights w1,w2, . . . wk, our goal is to find a mixture f\u0302 = (w\u03021, w\u03022, . . . w\u0302k, p\u03021, p\u03022, . . . p\u0302k)\nsuch that D(f, f\u0302) \u2264 \u01eb. Note that we make no assumption on the weights, means or the variances of the components.\nWe provide an algorithm that, using O\u0303(k\u01eb\u22122) samples and in time O\u0303(k\u01eb\u22123k\u22121), outputs an estimate that is at most \u01eb from the underlying in \u21131 distance with probability \u2265 1 \u2212 \u03b4. Our algorithm is an immediate consequence of the following observation for samples from a Gaussian distribution. Lemma 9. Given n independent samples x1, . . . , xn from N(\u00b5,\u03c32), there are two samples xj, xk such that\u2223xj \u2212 \u00b5\u2223 \u2264 \u03c3 7 log 2/\u03b42n and \u2223xj \u2212 xk \u2212 \u03c3\u2223 \u2264 2\u03c3 7 log 2/\u03b42n with probability \u2265 1 \u2212 \u03b4. Proof The density of N(\u00b5,\u03c32) is \u2265 (7\u03c3)\u22121 in the interval [\u00b5\u2212\u221a2\u03c3,\u00b5+\u221a2\u03c3]. Therefore, the probability that a sample occurs in the interval \u00b5\u2212\u01eb\u03c3,\u00b5+\u01eb\u03c3 is \u2265 2\u01eb/7. Hence, the probability that none of the n samples occurs in [\u00b5 \u2212 \u01eb\u03c3,\u00b5 + \u01eb\u03c3] is \u2264 (1 \u2212 2\u01eb/7)n \u2264 e\u22122n\u01eb/7. If \u01eb \u2265 7 log 2/\u03b4\n2n , then the probability that none of the\nsamples occur in the interval is \u2264 \u03b4/2. A similar argument shows that there is a sample within interval,[\u00b5 + \u03c3 \u2212 \u01eb\u03c3,\u00b5 + \u03c3 + \u01eb\u03c3], proving the lemma. The above observation can be translated into selecting a pool of candidate distributions such that one of the distributions is close to the underlying distribution.\nLemma 10. Given n \u2265 120k log 4k \u03b4 \u01eb samples from a mixture f of k Gaussians. Let S = {N(xj , (xj \u2212 xk)2) \u2236\n1 \u2264 j, k \u2264 n} be a set of Gaussians and W = {0, \u01eb 2k , 2\u01eb 2k . . . ,1} be the set of weights. Let F def= {w\u03021, w\u03022, . . . , w\u0302k\u22121,1 \u2212 k\u22121\u2211\ni=1 w\u0302i, p\u03021, p\u03022, . . . p\u0302k \u2236 w\u0302i \u2208W, p\u0302i \u2208 S} be a set of n2k(2k \u01eb )k\u22121 \u2264 n3k\u22121 candidate mixture distributions. There exists a f\u0302 \u2208 F such that D(f, f\u0302) \u2264 \u01eb. Proof. Let f = (w1,w2, . . . wk, p1, p2, . . . pk). For f\u0302 = (w\u03021, w\u03022, . . . , w\u0302k\u22121,1 \u2212 \u2211k\u22121i=1 w\u0302i, p\u03021, p\u03022, . . . p\u0302k), by the triangle inequality,\nD(f, f\u0302) \u2264 k\u22121\u2211 i=1 2\u2223w\u0302i \u2212wi\u2223 + k\u2211 i=1 wiD(pi, p\u0302i). We show that there is a distribution in f\u0302 \u2208 F such that the sum above is bounded by \u01eb. Since we quantize the grids as multiples of \u01eb/2k, we consider distributions in F such that each \u2223w\u0302i \u2212wi\u2223 \u2264 \u01eb/4k, and therefore\u2211i \u2223w\u0302i \u2212wi\u2223 \u2264 \u01eb2 .\nWe now show that for each pi there is a p\u0302i such that wiD(pi, p\u0302i) \u2264 \u01eb2k , thus proving that D(f, f\u0302) \u2264 \u01eb. If wi \u2264 \u01eb 4k\n, then wiD(pi, p\u0302i) \u2264 \u01eb2k . Otherwise, let w\u2032i > \u01eb4k be the fraction of samples from pi. By Lemma 9 and 14, with probability \u2265 1 \u2212 \u03b4/2k,\nD(pi, p\u0302i)2 \u2264 2(\u00b5i \u2212 \u00b5\u2032i)2 \u03c32i + 16(\u03c3i \u2212 \u03c3\u2032i)2 \u03c32i\n\u2264 25 log2 4k \u03b4(nw\u2032i)2 + 800 log2 4k\n\u03b4(nw\u2032i)2 \u2264 825 log2 4k\n\u03b4(nw\u2032i)2 . Therefore,\nwiD(pi, p\u0302i) \u2264 30wi log 4k\u03b4 nw\u2032i .\nSince wi > \u01eb/4k, with probability \u2265 1 \u2212 \u03b4/2k, wi \u2264 2w\u2032i. By the union bound with probability \u2265 1 \u2212 \u03b4/k, wiD(pi, p\u0302i) \u2264 60 log 4k\u03b4n . Hence if n \u2265 120k log 4k\u03b4\u01eb , the above quantity is less than \u01eb/2k. The total error probability is \u2264 \u03b4 by the union bound.\nRunning MODIFIED SCHEFFE algorithm on the above set of candidates F yields a mixture that is close to the underlying one. By Lemma 1 and the above lemma we get\nCorollary 11. Let n \u2265 c \u22c5 k log k\u01eb\u03b4 \u01eb2 for some constant c. There is an algorithm that runs in time\nO\u239b\u239c\u239d \u239b\u239dk log k \u01eb\u03b4 \u01eb \u239e\u23a0 3k\u22121 k2 log k \u01eb\u03b4 \u01eb2 \u239e\u239f\u23a0 , and returns a mixture f\u0302 such that D(f, f\u0302) \u2264 1000\u01eb with error probability \u2264 2\u03b4.\nProof. Use n\u2032 def= 120k log 4k \u03b4\n\u01eb samples to generate a set of at most n\u20323k\u22121 candidate distributions as stated in\nLemma 10. With probability \u2265 1 \u2212 \u03b4, one of the candidate distributions is \u01eb-close to the underlying one. Run MODIFIED SCHEFFE on this set of candidate distributions to obtain a 1000\u01eb-close estimate of f with probability \u2265 1 \u2212 \u03b4 (Lemma 1). The run time is dominated by the run time of MODIFIED SCHEFFE which is O ( \u2223F \u2223T log \u2223F\u2223\u03b4 \u01eb2 ), where \u2223F \u2223 = n\u20323k\u22121 and T = k. The total error probability is \u2264 2\u03b4 by the union bound.\nRemark 12. The above bound matches the independent and contemporary result by [15] for k = 2. While the process of identifying the candidate means is same for both the papers, the process of identifying the variances and proof techniques are different."}, {"heading": "5 Acknowledgements", "text": "We thank Sanjoy Dasgupta, Todd Kemp, and Krishnamurthy Vishwanathan for helpful discussions."}, {"heading": "A Useful tools", "text": ""}, {"heading": "A.1 Bounds on \u21131 distance", "text": "For two d dimensional product distributions p1 and p2, if we bound the \u21131 distance on each coordinate by \u01eb, then by triangle inequality D(p1,p2) \u2264 d\u01eb. However this bound is often weak. One way to obtain a stronger bound is to relate \u21131 distance to Bhattacharyya parameter, which is defined as follows: Bhattacharyya parameter B(p1, p2) between two distributions p1 and p2 is\nB(p1, p2) = \u222b x\u2208X\n\u221a p1(x)p2(x)dx.\nWe use the fact that for two product distributions p1 and p2, B(p1,p2) = \u220fdi=1B(p1,i, p2,i) to obtain stronger bounds on the \u21131 distance. We first bound Bhattacharyya parameter for two one-dimensional Gaussian distributions. Lemma 13. The Bhattacharyya parameter for two one dimensional Gaussian distributions p1 = N(\u00b51, \u03c321) and p2 = N(\u00b52, \u03c322) is\nB(p1, p2) \u2265 1 \u2212 (\u00b51 \u2212 \u00b52)2) 4(\u03c321 + \u03c322) \u2212 (\u03c321 \u2212 \u03c322)2(\u03c321 + \u03c322)2 . Proof. For Gaussian distributions the Bhattacharyya parameter is (see [8]), B(p1, p2) = ye\u2212x, where x = (\u00b51\u2212\u00b52)2) 4(\u03c32\n1 +\u03c32 2 ) and y = \u221a 2\u03c31\u03c32 \u03c32 1 +\u03c32 2 . Observe that\ny =\n\u221a 2\u03c31\u03c32 \u03c321 + \u03c322 = \u00bf\u00c1\u00c1\u00c01 \u2212 (\u03c31 \u2212 \u03c32)2 \u03c321 + \u03c322 \u2265 1 \u2212 (\u03c31 \u2212 \u03c32)2 \u03c321 + \u03c322 \u2265 1 \u2212\n(\u03c321 \u2212 \u03c322)2(\u03c321 + \u03c322)2 . Hence,\nB(p1, p2) = ye\u2212x \u2265 y(1 \u2212 x) \u2265 (1 \u2212 x)(1 \u2212 (\u03c321 \u2212 \u03c322)2(\u03c321 + \u03c322)2) \u2265 1 \u2212 x \u2212 (\u03c321 \u2212 \u03c322)2(\u03c321 + \u03c322)2 .\nSubstituting the value of x results in the lemma.\nThe next lemma follows from the relationship between Bhattacharyya parameter and \u21131 distance (see [34]), and the previous lemma.\nLemma 14. For any two Gaussian product distributions p1 and p2,\nD(p1,p2)2 \u2264 8( d\u2211 i=1 1 \u2212B(p1,i, p2,i)) \u2264 d\u2211 i=1 2 (\u00b51,i \u2212 \u00b52,i)2 \u03c321,i + \u03c322,i + 8 (\u03c321,i \u2212 \u03c322,i)2(\u03c321,i + \u03c322,i)2 ."}, {"heading": "A.2 Concentration inequalities", "text": "We use the following concentration inequalities for Gaussian, Chi-Square, and sum of Bernoulli random variables in the rest of the paper.\nLemma 15. For a Gaussian random variable X with mean \u00b5 and variance \u03c32,\nPr(\u2223X \u2212 \u00b5\u2223 \u2265 t\u03c3) \u2264 e\u2212t2/2. Lemma 16 ( [25]). If Y1, Y2, . . . Yn be n i.i.d.Gaussian variables with mean 0 and variance \u03c32, then\nPr( n\u2211 i=1 Y 2i \u2212 n\u03c32 \u2265 2(\u221ant + t)\u03c32) \u2264 e\u2212t, and Pr( n\u2211 i=1 Y 2i \u2212 n\u03c32 \u2264 \u22122\u221ant\u03c32) \u2264 e\u2212t. Furthermore for a fixed vector a,\nPr(\u2223 n\u2211 i=1 ai(Y 2i \u2212 1)\u2223 \u2264 2(\u2223\u2223a\u2223\u22232\u221at + \u2223\u2223a\u2223\u2223\u221e t)\u03c32) \u2264 2e\u2212t. Lemma 17 (Chernoff bound). If X1,X2 . . .Xn are distributed according to Bernoulli p, then with probability 1 \u2212 \u03b4,\n\u2223\u2211ni=1Xi n\n\u2212 p\u2223 \u2264 \u221a\n2p(1 \u2212 p) n log 2 \u03b4 + 2 3 log 2 \u03b4 n .\nWe now state a non-asymptotic concentration inequality for random matrices that helps us bound errors in spectral algorithms. Lemma 18 ( [43] Remark 5.51). Let y(1),y(2), . . . ,y(n) be generated according to N(0,\u03a3). For every \u01eb \u2208 (0,1) and t \u2265 1, if n \u2265 c\u2032d( t \u01eb )2 for some constant c\u2032, then with probability \u2265 1 \u2212 2e\u2212t2n,\n\u2223\u2223 n\u2211 i=1 1 n y(i)yt(i) \u2212\u03a3\u2223\u2223 \u2264 \u01eb \u2223\u2223\u03a3\u2223\u2223 ."}, {"heading": "A.3 Matrix eigenvalues", "text": "We now state few simple lemmas on the eigenvalues of perturbed matrices.\nLemma 19. Let \u03bbA1 \u2265 \u03bb A \u2265 . . . \u03bbAd \u2265 0 and \u03bb B 1 \u2265 \u03bb B \u2265 . . . \u03bbBd \u2265 0 be the eigenvalues of two symmetric\nmatrices A and B respectively. If \u2223\u2223A \u2212B\u2223\u2223 \u2264 \u01eb, then \u2200 i, \u2223\u03bbAi \u2212 \u03bbBi \u2223 \u2264 \u01eb. Proof. Let u1,u2, . . .ud be a set of eigenvectors of A that corresponds to \u03bbA1 , \u03bb A 2 , . . . \u03bb A d . Similarly let v1,v2, . . .vd be eigenvectors of B Consider the first eigenvalue of B,\n\u03bbB1 = \u2223\u2223B\u2223\u2223 = \u2223\u2223A + (B \u2212A)\u2223\u2223 \u2265 \u2223\u2223A\u2223\u2223 \u2212 \u2223\u2223B \u2212A\u2223\u2223 \u2265 \u03bbA1 \u2212 \u01eb.\nNow consider an i > 1. If \u03bbBi < \u03bb A i \u2212 \u01eb, then by definition of eigenvalues\nmax v\u2236\u2200j\u2264i\u22121,v\u22c5vj=0 \u2223\u2223Bv\u2223\u22232 < \u03bbAi \u2212 \u01eb. Now consider a unit vector \u2211ij=1\u03b1juj in the span of u1, . . .ui, that is orthogonal to v1, . . .vi\u22121. For this vector,\nRRRRRRRRRRR RRRRRRRRRRRB i \u2211 j=1 \u03b1juj RRRRRRRRRRR RRRRRRRRRRR2 \u2265 RRRRRRRRRRR RRRRRRRRRRRA i \u2211 j=1 \u03b1juj RRRRRRRRRRR RRRRRRRRRRR2 \u2212 RRRRRRRRRRR RRRRRRRRRRR(A \u2212B) i \u2211 j=1 \u03b1juj RRRRRRRRRRR RRRRRRRRRRR2 \u2265 \u00bf\u00c1\u00c1\u00c1\u00c0 i\u2211 j=1\n\u03b12j(\u03bbAj )2 \u2212 \u01eb \u2265 \u03bbAi \u2212 \u01eb, a contradiction. Hence, \u2200i \u2264 d, \u03bbBi \u2265 \u03bbAi \u2212 \u01eb. The proof in the other direction is similar and omitted. Lemma 20. Let A = \u2211ki=1 \u03b72i uiuti be a positive semidefinite symmetric matrix for k \u2264 d. Let u1,u2, . . .uk span a k \u2212 1 dimensional space. Let B = A + R, where \u2223\u2223R\u2223\u2223 \u2264 \u01eb. Let v1,v2, . . .vk\u22121 be the top k \u2212 1 eigenvectors of B. Then the projection of ui in space orthogonal to v1,v2, . . .vk\u22121 is \u2264 2 \u221a \u01eb\n\u03b7i .\nProof. Let \u03bbBi be the i th largest eigenvalue of B. Observe that B + \u01ebId is a positive semidefinite matrix as\nfor any vector v, vt(A +R + \u01ebId)v \u2265 0. Furthermore \u2223\u2223A +R + \u01ebId \u2212A\u2223\u2223 \u2264 2\u01eb. Since eigenvalues of B + \u01ebId is \u03bbB + \u01eb, by Lemma 19, for all i \u2264 d, \u2223\u03bbAi \u2212 \u03bbBi \u2212 \u01eb\u2223 \u2264 2\u01eb. Therefore, \u2223\u03bbBi \u2223 for i \u2265 k is \u2264 3\u01eb.\nLet ui = \u2211k\u22121j=1 \u03b1i,jvj + \u221a\n1 \u2212\u2211k\u22121j=1 \u03b12i,ju\u2032, for a vector u\u2032 orthogonal to v1,v2, . . .vk\u22121. We compute u\u2032tAu\u2032 in two ways. Since A = B \u2212R,\n\u2223u\u2032t(B \u2212R)u\u2032\u2223 \u2264 \u2223u\u2032tBu\u2032\u2223 + \u2223u\u2032tRu\u2032\u2223 \u2264 \u2223\u2223Bu\u2032\u2223\u2223 2 + \u2223\u2223R\u2223\u2223 .\nSince u\u2032 is orthogonal to first k eigenvectors, we have \u2223\u2223Bu\u2032\u2223\u22232 \u2264 3\u01eb and hence \u2223u\u2032(B \u2212R)u\u2032\u2223 \u2264 4\u01eb. u\u2032tAu\u2032 \u2265 \u03b72i (1 \u2212 k\u22121\u2211\nj=1 \u03b12i,j). We have shown that the above quantity is \u2264 4\u01eb. Therefore (1 \u2212\u2211k\u22121j=1 \u03b12i,j)1/2 \u2264 2\u221a\u01eb/\u03b7i."}, {"heading": "B Selection from a set of candidate distributions", "text": "Given samples from an unknown distribution f , the objective is to output a distribution from a known collection F of distributions with \u21131 distance close to D(f,F). Scheffe estimate [16] outputs a distribution from F whose \u21131 distance from f is at most 9.1max(D(f,F), \u01eb) The algorithm requires O(\u01eb\u22122 log \u2223F \u2223) samples and the runs in time O(\u2223F \u22232T (n + \u2223X \u2223)), where T is the time to compute the probability fj(x) of x, for any fj \u2208 F . An approach to reduce the time complexity, albeit using exponential pre-processing, was proposed in [28]. We present the modified Scheffe algorithm with near linear time complexity and then prove Lemma 1.\nWe first present the algorithm SCHEFFE* with running time O\u0303(\u2223F \u22232Tn).\nAlgorithm SCHEFFE* Input: a set F of candidate distributions, \u01eb \u2236 upper bound on D(f,F), n independent samples x1, . . . , xn from f .\nFor each pair (p, q) in F do: 1. \u00b5f = 1 n \u2211ni=1 I{p(xi) > q.(xi)}.\n2. Generate independent samples y1, . . . , yn and z1, . . . , zn from p and q respectively. 3. \u00b5p = 1 n \u2211ni=1 I{p(yi) > q(yi)}, \u00b5q = 1n \u2211ni=1 I{p(zi) > q(zi)}.\n4. If \u2223\u00b5p \u2212 \u00b5f \u2223 < \u2223\u00b5q \u2212 \u00b5f \u2223 declare p as winner, else q. Output the distribution with most wins, breaking ties arbitrarily.\nWe make the following modification to the algorithm where we reduce the size of potential distributions by half in every iteration.\nAlgorithm MODIFIED SCHEFFE Input: set F of candidate distributions, \u01eb \u2236 upper bound on minfi\u2208F D(f, fi), n independent samples x1, . . . , xn from f .\n1. Let G = F , C \u2190 \u2205 2. Repeat until \u2223G\u2223 > 1:\n(a) Randomly form \u2223G\u2223/2 pairs of distributions in G and run SCHEFFE* on each pair using the n samples. (b) Replace G with the \u2223G\u2223/2 winners. (c) Randomly select a set A of min{\u2223G\u2223, \u2223F \u22231/3} elements from G. (d) Run SCHEFFE* on each pair in A and add the distributions with most wins to C.\n3. Run SCHEFFE* on C and output the winner\nRemark 21. For the ease of proof, we assume that \u03b4 \u2265 10 log \u2223F \u2223\u2223F \u22231/3 . If \u03b4 < 10 log \u2223F \u2223 \u2223F \u22231/3 , we run the algorithm with\nerror probability 1/3 and repeat it O(log 1 \u03b4 ) times to choose a set of candidate mixtures F\u03b4 . By Chernoffbound with error probability \u2264 \u03b4, F\u03b4 contains a mixture close to f . Finally, we run SCHEFFE* on F\u03b4 to obtain a mixture that is close to f .\nProof sketch of Lemma 1. For any set A and a distribution p, given n independent samples from p the empirical probability \u00b5n(A) has a distribution around p(A) with standard deviation \u223c 1\u221an . Together with an observation in Scheffe estimation in [16] one can show that if the number of samples n = O ( log \u2223F\u2223\u03b4 \u01eb2 ), then SCHEFFE* has a guarantee 10max(\u01eb,D(f,F)) with probability \u2265 1 \u2212 \u03b4.\nSince we run SCHEFFE* at most \u2223F \u2223(2 log \u2223F \u2223+ 1) times, choosing \u03b4 = \u03b4/(4\u2223F \u2223 log \u2223F \u2223+ 2\u2223F \u2223) results in the sample complexity of\nO\u239b\u239d log \u2223F \u22232(4 log \u2223F \u2223+2) \u03b4 \u01eb2 \u239e\u23a0 = O\u239b\u239d log \u2223F \u2223 \u03b4 \u01eb2 \u239e\u23a0 , and the total error probability of \u03b4/2 for all runs of SCHEFFE* during the algorithm. The above value of n dictates our sample complexity. We now consider the following two cases:\n\u2022 If at some stage \u2265 log(2/\u03b4)\u2223F \u22231/3 fraction of elements in A have an \u21131 distance \u2264 10\u01eb from f , then at that stage with probability \u2265 1 \u2212 \u03b4/2 an element with distance \u2264 10\u01eb from f is added to A. Therefore a distribution with distance \u2264 100\u01eb is selected to C.\n\u2022 If at no stage this happens, then consider the element that is closest to f , i.e., at \u21131 distance at most \u01eb.\nWith probability \u2265 (1 \u2212 log(2/\u03b4)\u2223F \u22231/3 )log \u2223F \u2223 it always competes with an element at a distance at least 10\u01eb from f and it wins all these games with probability \u2265 1 \u2212 \u03b4/2.\nTherefore with probability \u2265 1\u2212\u03b4/2 there is an element in C at \u21131 distance at most 100\u01eb. Running SCHEFFE* on this set yields a distribution at a distance \u2264 100 \u22c5 10\u01eb = 1000\u01eb. The error probability is \u2264 \u03b4 by the union bound."}, {"heading": "C Lower bound", "text": "We first show a lower bound for a single Gaussian distribution and generalize it to mixtures."}, {"heading": "C.1 Single Gaussian distribution", "text": "The proof is an application of the following version of Fano\u2019s inequality [9, 45]. It states that we cannot simultaneously estimate all distributions in a class using n samples if they satisfy certain conditions.\nLemma 22. (Fano\u2019s Inequality) Let f1, . . . , fr+1 be a collection of distributions such that for any i \u2260 j, D(fi, fj) \u2265 \u03b1, and KL(fi, fj) \u2264 \u03b2. Let f be an estimate of the underlying distribution using n i.i.d. samples from one of the fi\u2019s. Then,\nsup i E[D(fi, f)] \u2265 \u03b1 2 (1 \u2212 n\u03b2 + log 2 log r ).\nWe consider d\u2212dimensional spherical Gaussians with identity covariance matrix, with means along any coordinate restricted to \u00b1 c\u01eb\u221a\nd . The KL divergence between two spherical Gaussians with identity covariance\nmatrix is the squared distance between their means. Therefore, any two distributions we consider have KL distance at most\n\u03b2 = d\n\u2211 i=1 (2 c\u01eb\u221a d )2 = 4c2\u01eb2,\nWe now consider a subset of these 2d distributions to obtain a lower bound on \u03b1. By the Gilbert-Varshamov bound, there exists a binary code with \u2265 2d/8 codewords of length d and minimum distance d/8. Consider one such code. Now for each codeword, map 1\u2192 c\u01eb\u221a\nd and 0\u2192 \u2212 c\u01eb\u221a d to obtain a distribution in our class. We\nconsider this subset of \u2265 2d/8 distributions as our fi\u2019s.\nConsider any two fi\u2019s. Their means differ in at least d/8 coordinates. We show that the \u21131 distance between them is \u2265 c\u01eb/4. Without loss of generality, let the means differ in the first d/8 coordinates, and furthermore, one of the distributions has means c\u01eb/\u221ad and the other has \u2212c\u01eb/\u221ad in the first d/8 coordinates. The sum of the first d/8 coordinates is N(c\u01eb\u221ad/8, d/8) and N(\u2212c\u01eb\u221ad/8, d/8). The \u21131 distance between these normal random variables is a lower bound on the \u21131 distance of the original random variables. For small values of c\u01eb the distance between the two Gaussians is at least \u2265 c\u01eb/4. This serves as our \u03b1.\nApplying the Fano\u2019s Inequality, the \u21131 error on the worst distribution is at least\nc\u01eb 8 (1 \u2212 n4c2\u01eb2 + log 2 d/8 ), which for c = 16 and n < d\n214\u01eb2 is at least \u01eb. In other words, the smallest n to approximate all spherical\nnormal distributions to \u21131 distance at most \u01eb is > d\n214\u01eb2 ."}, {"heading": "C.2 Mixtures of k Gaussians", "text": "We now provide a lower bound on the sample complexity of learning mixtures of k Gaussians in d dimensions. We extend the construction for learning a single spherical Gaussian to mixtures of k Gaussians and show a lower bound of \u2126(kd/\u01eb2) samples. We will again use Fano\u2019s inequality over a class of 2kd/64 distributions as described next.\nTo prove the lower bound on the sample complexity of learning spherical Gaussians, we designed a class of 2d/8 distributions around the origin. Let P def= {P1, . . . , PT }, where T = 2d/8, be this class. Recall that each Pi is a spherical Gaussian with unit variance. For a distribution P over Rd and \u00b5 \u2208 Rd, let P + \u00b5 be the distribution P shifted by \u00b5.\nWe now choose \u00b51, . . . ,\u00b5k\u2019s extremely well-separated. The class of distributions we consider will be a mixture of k components, where the jth component is a distribution from P shifted by \u00b5j . Since the \u00b5\u2019s will be well separated, we will use the results from last section over each component.\nFor i \u2208 [T ], and j \u2208 [k], Pij def= Pi +\u00b5j . Each (i1, . . . , ik) \u2208 [T ]k corresponds to the mixture 1\nk (Pi11 + Pi22 + . . . + Pikk)\nof k spherical Gaussians. We consider this class of T k = 2kd/8 distributions. By the Gilbert-Varshamov bound, for any T \u2265 2, there is a T -ary codes of length k, with minimum distance \u2265 k/8 and number of codewords \u2265 2k/8. This implies that among the T k = 2dk/8 distributions, there are 2kd/64 distributions such that any two tuples (i1, . . . , ik) and (i\u20321, . . . , i\u2032k) corresponding to different distributions differ in at least k/8 locations.\nIf we choose the \u00b5\u2019s well separated, the components of any mixture distribution have very little overlap. For simplicity, we choose \u00b5j\u2019s satisfying\nmin j1\u2260j2 \u2223\u2223\u00b5j1 \u2212\u00b5j2 \u2223\u22232 \u2265 (2kd\u01eb ) 100 .\nThis implies that for j \u2260 l, \u2223\u2223Pij \u2212 Pi\u2032l\u2223\u22231 < (\u01eb/2dk)10. Therefore, for two different mixture distributions, \u2223\u22231 k (Pi11 +Pi22 + . . . +Pikk) \u2212 1k (Pi\u203211 + Pi\u203222 + . . . + Pi\u2032kk)\u2223\u22231\n(a) \u2265 1\nk \u2211 j\u2208[k],ij ,i\u2032j\u2208[T ] \u2223Pijj \u2212 Pi\u2032jj \u2223 \u2212 k2(\u01eb/2dk)10\n(b) \u2265 1\n8\nc\u01eb 4 \u2212 k2(\u01eb/2dk)10.\nwhere (a) follows form the fact that two mixtures have overlap only in the corresponding components, (b) uses the fact that at least in k/8 components ij \u2260 i\u2032j , and then uses the lower bound from the previous section.\nTherefore, the \u21131 distance between any two of the 2kd/64 distributions is \u2265 c1\u01eb/32 for c1 slightly smaller than c. We take this as \u03b1.\nNow, to upper bound the KL divergence, we simply use the convexity, namely for any distributions P1 . . . Pk and Q1 . . . Qk, let P\u0304 and Q\u0304 be the mean distributions. Then,\nD(P\u0304 \u2223\u2223Q\u0304) \u2264 1 k k \u2211 i=1 D(Pi\u2223\u2223Qi). By the construction and from the previous section, for any j, D(Pijj \u2223\u2223Pi\u2032jj) =D(Pi\u2223\u2223Pi\u2032) \u2264 4c2\u01eb2. Therefore, we can take \u03b2 = 4c2\u01eb2.\nTherefore by the Fano\u2019s inequality, the \u21131 error on the worst distribution is at least\nc1\u01eb 64 (1 \u2212 n4c2\u01eb2 + log 2 dk/64 ), which for c1 = 128, c = 128.1 and n < dk 88\u01eb2 is at least \u01eb."}, {"heading": "D Proofs for k spherical Gaussians", "text": "We first state a simple concentration result that helps us in other proofs.\nLemma 23. Given n samples from a set of Gaussian distributions, with probability \u2265 1 \u2212 2\u03b4, for every pair of samples X \u223c N(\u00b51, \u03c32Id) and Y \u223c N(\u00b52, \u03c32Id),\n\u2223\u2223X \u2212Y\u2223\u222322 \u2264 2d\u03c32 + 4\u03c32 \u221a d log n2 \u03b4 + \u2223\u2223\u00b51 \u2212\u00b52\u2223\u222322 + 4\u03c3 \u2223\u2223\u00b51 \u2212\u00b52\u2223\u22232 \u221a log n2 \u03b4 + 4\u03c32 log n2 \u03b4 . (1)\nand\n\u2223\u2223X \u2212Y\u2223\u222322 \u2265 2d\u03c32 \u2212 4\u03c32 \u221a d log n2 \u03b4 + \u2223\u2223\u00b51 \u2212\u00b52\u2223\u222322 \u2212 4\u03c3 \u2223\u2223\u00b51 \u2212\u00b52\u2223\u22232 \u221a log n2 \u03b4 . (2)\nProof. We prove the lower bound, the proof for the upper bound is similar and omitted. Since X and Y are Gaussians, X \u2212Y is distributed as N(\u00b51 \u2212\u00b52,2\u03c32). Rewriting \u2223\u2223X \u2212Y\u2223\u22232 \u2223\u2223X \u2212Y\u2223\u222322 = \u2223\u2223X \u2212Y \u2212 (\u00b51 \u2212\u00b52)\u2223\u222322 + \u2223\u2223\u00b51 \u2212\u00b52\u2223\u222322 + 2(\u00b51 \u2212\u00b52) \u22c5 (X \u2212Y \u2212 (\u00b51 \u2212\u00b52)). Let Z =X \u2212Y \u2212 (\u00b51 \u2212\u00b52), then Z \u223c N(0,2\u03c32Id). Therefore by Lemma 16, with probability 1 \u2212 \u03b4/n2,\n\u2223\u2223Z\u2223\u222322 \u2265 2d\u03c32 \u2212 4\u03c32 \u221a d log n2\n\u03b4 .\nFurthermore (\u00b51\u2212\u00b52) \u22c5Z is sum of Gaussians and hence a Gaussian distribution. It has mean 0 and variance 2\u03c32 \u2223\u2223\u00b51 \u2212\u00b52\u2223\u222322. Therefore, by Lemma 15 with probability 1 \u2212 \u03b4/n2,\n(\u00b51 \u2212\u00b52) \u22c5Z \u2265 \u22122\u03c3 \u2223\u2223\u00b51 \u2212\u00b52\u2223\u22232 \u221a log n2\n\u03b4 .\nBy the union bound with probability 1 \u2212 2\u03b4/n2, \u2223\u2223X \u2212Y\u2223\u222322 \u2265 2d\u03c32 \u2212 4\u03c32 \u221a d log n2 \u03b4 + \u2223\u2223\u00b51 \u2212\u00b52\u2223\u222322 \u2212 4\u03c3 \u2223\u2223\u00b51 \u2212\u00b52\u2223\u22232 \u221a log n2 \u03b4 . There are (n 2 ) pairs and the lemma follows by the union bound."}, {"heading": "D.1 Proof of Lemma 4", "text": "We show that if Equations (1) and (2) are satisfied, then the lemma holds. The error probability is that of Lemma 23 and is \u2264 2\u03b4. Since the minimum is over k + 1 indices, at least two samples are from the same component. Applying Equations (1) and (2) for these two samples\n2d\u03c3\u03022 \u2264 2d\u03c32 + 4\u03c32 \u221a d log n2\n\u03b4 + 4\u03c32 log n2 \u03b4 .\nSimilarly by Equations (1) and (2) for any two samples X(a),X(b) in [k + 1], \u2223\u2223X(a) \u2212X(b)\u2223\u222322 \u2265 2d\u03c32 \u2212 4\u03c32 \u221a d log n2 \u03b4 + \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u222322 \u2212 4\u03c3 \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u22232 \u221a log n2 \u03b4\n\u2265 2d\u03c32 \u2212 4\u03c32 \u221a d log n2\n\u03b4 \u2212 4\u03c32 log n2 \u03b4 ,\nwhere the last inequality follows from the fact that \u03b12\u22124\u03b1\u03b2 \u2265 \u22124\u03b22. The result follows from the assumption that d > 20 log n2/\u03b4."}, {"heading": "D.2 Proof of Lemma 5", "text": "We show that if Equations (1) and (2) are satisfied, then the lemma holds. The error probability is that of Lemma 23 and is \u2264 2\u03b4. Since Equations (1) and (2) are satisfied, by the proof of Lemma 4, \u2223\u03c3\u03022 \u2212 \u03c32\u2223 \u2264 2.5\u03c32 \u221a log(n2/\u03b4) d . If two samples X(a) and X(b) are from the same component, by Lemma 23,\n\u2223\u2223X(a) \u2212X(b)\u2223\u222322 \u2264 2d\u03c32 + 4\u03c32 \u221a d log n2 \u03b4 + 4\u03c32 log n2 \u03b4 \u2264 2d\u03c32 + 5\u03c32 \u221a d log n2 \u03b4 .\nBy Lemma 4, the above quantity is less than 2d\u03c3\u03022 + 23\u03c3\u03022\u221ad log n2 \u03b4\n. Hence all the samples from the same component are in a single cluster.\nSuppose there are two samples from different components in a cluster, then by Equations (1) and (2),\n2d\u03c3\u03022 + 23\u03c3\u03022 \u221a d log n2\n\u03b4 \u2265 2d\u03c32 \u2212 4\u03c32\n\u221a d log n2\n\u03b4 + \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u222322 \u2212 4\u03c3 \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u22232\n\u221a log n2\n\u03b4 .\nRelating \u03c3\u03022 and \u03c32 using Lemma 4,\n2d\u03c32 + 40\u03c32 \u221a d log n2\n\u03b4 \u2265 2d\u03c32 \u2212 4\u03c32\n\u221a d log n2\n\u03b4 + \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u222322 \u2212 4\u03c3 \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u22232\n\u221a log n2\n\u03b4 .\nHence \u2223\u2223\u00b5i \u2212\u00b5j \u2223\u22232 \u2264 10\u03c3(d log n2\u03b4 )1/4. There are at most k components; therefore, any two components within the same cluster are at a distance \u2264 10k\u03c3(d log n2 \u03b4 )1/4."}, {"heading": "D.3 Proof of Lemma 6", "text": "The proof is involved and we show it in steps. We first show few concentration bounds which we use later to argue that the samples are clusterable when the sample covariance matrix has a large eigenvalue. Let w\u0302i be the fraction of samples from component i. Let \u00b5\u0302i be the empirical average of samples from pi. Let \u00b5\u0302(C) be the empirical average of samples in cluster C . If C is the entire set of samples we use \u00b5\u0302 instead of \u00b5\u0302(C). We first show a concentration inequality that we use in rest of the calculations. Lemma 24. Given n samples from a k-component Gaussian mixture with probability \u2265 1 \u2212 2\u03b4, for every component i\n\u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u222322 \u2264 (d + 3 \u221a d log 2k\n\u03b4 ) \u03c32 nw\u0302i\nand \u2223w\u0302i \u2212wi\u2223 \u2264 \u00bf\u00c1\u00c1\u00c02wi log 2k\u03b4\nn + 2 3\nlog 2k \u03b4\nn . (3)\nProof. Since \u00b5\u0302i \u2212\u00b5i is distributed N(0, \u03c32Id/nw\u0302i), by Lemma 16 with probability \u2265 1 \u2212 \u03b4/k, \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u222322 \u2264 (d + 2 \u221a d log 2k\n\u03b4 + 2 log 2k \u03b4 ) \u03c32 nw\u0302i\n\u2264 (d + 3 \u221a d log 2k\n\u03b4 ) \u03c32 nw\u0302i .\nThe second inequality uses the fact that d \u2265 20 log n2/\u03b4. For bounding the weights, observe that by Lemma 17 with probability \u2265 1 \u2212 \u03b4/k,\n\u2223w\u0302i \u2212wi\u2223 \u2264 \u221a 2wi log 2k/\u03b4 n + 2 3 log 2k/\u03b4 n .\nBy the union bound the error probability is \u2264 2k\u03b4/2k = \u03b4. A simple application of triangle inequality yields the following lemma.\nLemma 25. Given n samples from a k-component Gaussian mixture if Equation (3) holds, then\n\u2223\u2223 k\u2211 i=1\nw\u0302i(\u00b5\u0302i \u2212\u00b5i)(\u00b5\u0302i \u2212\u00b5i)t\u2223\u2223 \u2264 (d + 3 \u221a d log 2k\n\u03b4 )k\u03c32 n .\nLemma 26. Given n samples from a k-component Gaussian mixture, if Equation (3) holds and the maximum distance between two components is \u2264 10k\u03c3(d log n2 \u03b4 )1/4, then \u2223\u2223\u00b5\u0302 \u2212\u00b5)\u2223\u2223 2 \u2264 c\u03c3\n\u221a dk log n 2\n\u03b4 n , for a constant c.\nProof. Observe that\n\u00b5\u0302 \u2212\u00b5 = k\u2211 i=1 w\u0302i\u00b5\u0302i \u2212wi\u00b5i = k \u2211 i=1 w\u0302i(\u00b5\u0302i \u2212\u00b5i) + (w\u0302i \u2212wi)\u00b5i = k\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212\u00b5i) + (w\u0302i \u2212wi)(\u00b5i \u2212\u00b5). (4) Hence by Equation (3) and the fact that the maximum distance between two components is \u2264 10k\u03c3(d log n2 \u03b4 )1/4,\n\u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u2223 2 \u2264 k\n\u2211 i=1 w\u0302i\n\u00bf\u00c1\u00c1\u00c0(d + 3 \u221a d log 2k\n\u03b4 ) \u03c3\u221a nw\u0302i + (\n\u221a 2wi log 2k/\u03b4\nn + 2 3 log 2k/\u03b4 n )10k(d log n2 \u03b4 )1/4\u03c3.\nFor n \u2265 d \u2265max(k4,20 log n2/\u03b4,1000), we get the above term is \u2264 c\u221akd logn2/\u03b4 n \u03c3, for some constant c.\nWe now make a simple observation on covariance matrices.\nLemma 27. Given n samples from a k-component mixture,\n\u2223\u2223 k\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 \u2264 2 \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u22232\n2 + k\u2211\ni=1 2w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u222322 + 2(\u221ak \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u22232 + k\u2211 i=1 \u221a w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u22232)max j \u221a w\u0302j \u2223\u2223\u00b5j \u2212\u00b5\u2223\u22232 .\nProof. Observe that for any two vectors u and v, uut \u2212 vvt = u(ut \u2212 vt) + (u \u2212 v)vt = (u \u2212 v)(u \u2212 v)t + v(u \u2212 v)t + (u \u2212 v)vt. Hence by triangle inequality, \u2223\u2223uut \u2212 vvt\u2223\u2223 \u2264 \u2223\u2223u \u2212 v\u2223\u222322 + 2 \u2223\u2223v\u2223\u22232 \u2223\u2223u \u2212 v\u2223\u22232 . Applying the above observation to u = \u00b5\u0302i \u2212 \u00b5\u0302 and v = \u00b5i \u2212\u00b5, we get\nk\n\u2211 i=1 w\u0302i \u2223\u2223(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 (\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 \u2264 k\n\u2211 i=1 (w\u0302i \u2223\u2223\u00b5\u0302i \u2212 \u00b5\u0302 \u2212\u00b5i \u2212\u00b5\u2223\u222322 + 2\u221aw\u0302i \u2223\u2223\u00b5i \u2212\u00b5\u2223\u22232\u221aw\u0302i \u2223\u2223\u00b5\u0302i \u2212 \u00b5\u0302 \u2212\u00b5i \u2212\u00b5\u2223\u22232) \u2264 k\n\u2211 i=1 (2w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u222322 + 2w\u0302i \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u222322 + 2maxj \u221aw\u0302j \u2223\u2223\u00b5j \u2212\u00b5\u2223\u22232 ( \u221a w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u22232 +\u221aw\u0302i \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u22232))\n\u2264 2 \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u22232 2 + k\u2211\ni=1 2w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u222322 + 2(\u221ak \u2223\u2223\u00b5\u0302 \u2212\u00b5\u2223\u22232 + k\u2211 i=1 \u221a w\u0302i \u2223\u2223\u00b5\u0302i \u2212\u00b5i\u2223\u22232)max j \u221a w\u0302j \u2223\u2223\u00b5j \u2212\u00b5\u2223\u22232 .\nThe lemma follows from triangle inequality.\nThe following lemma immediately follows from Lemmas 26 and 27.\nLemma 28. Given n samples from a k-component Gaussian mixture, if Equation (3) and the maximum distance between two components is \u2264 10k\u03c3(d log n2 \u03b4 )1/4, then\n\u2223\u2223 k\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 \u2264 c\u03c32dk2 log n 2 \u03b4 n + c\u03c3 \u00bf\u00c1\u00c1\u00c0dk2 log n2\u03b4 n max i \u221a w\u0302i \u2223\u2223\u00b5i \u2212\u00b5\u2223\u22232 ,\nfor a constant c. Lemma 29. For a set of samples X(1), . . .X(n) from a k-component mixture, n \u2211 i=1 (X(i) \u2212 \u00b5\u0302)(X(i) \u2212 \u00b5\u0302)t n = k \u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 w\u0302i(\u00b5\u0302i \u2212\u00b5i)(\u00b5\u0302i \u2212\u00b5i)t + \u2211 j\u2223X(j)\u223cpi (X(j) \u2212\u00b5i)(X(j) \u2212\u00b5i)t n .\nwhere w\u0302i and \u00b5\u0302i are the empirical weights and averages of components i and \u00b5\u0302 = 1 n \u2211ni=1Xi.\nProof. The given expression can be rewritten as\n1\nn\nn\n\u2211 i=1 (X(i) \u2212 \u00b5\u0302)(X(i) \u2212 \u00b5\u0302)t = k\u2211 i=1 w\u0302i \u2211 j\u2223X(j)\u223cpi 1 nw\u0302i X(j) \u2212 \u00b5\u0302)(X(j) \u2212 \u00b5\u0302)t.\nFirst observe that for any set of points xi and their average x\u0302 and any value a,\n\u2211 i (xi \u2212 a)2 =\u2211 i (xi \u2212 x\u0302)2 + (x\u0302 \u2212 a)2. Hence for samples from a component i,\n\u2211 j\u2223X(j)\u223cpi\n1 nw\u0302i (X(j) \u2212 \u00b5\u0302)(X(j) \u2212 \u00b5\u0302)t\n= \u2211 j\u2223X(j)\u223cpi\n1 nw\u0302i (\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t + \u2211\nj\u2223X(j)\u223cpi\n1 nw\u0302i (X(j) \u2212 \u00b5\u0302i)(X(j) \u2212 \u00b5\u0302i)t\n= (\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t + \u2211 j\u2223X(j)\u223cpi 1 nw\u0302i (X(j) \u2212 \u00b5\u0302i)(X(j) \u2212 \u00b5\u0302i)t = (\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t + \u2211 j\u2223X(j)\u223cpi 1 nw\u0302i (X(j) \u2212\u00b5i)(X(j) \u2212\u00b5i)t \u2212 (\u00b5\u0302i \u2212\u00b5i)(\u00b5\u0302i \u2212\u00b5i)t.\nSumming over all components results in the lemma.\nWe now bound the error in estimating the eigenvalue of the covariance matrix.\nLemma 30. Given X(1), . . .X(n), n samples from a k-component Gaussian mixture, if Equations (1), (2), and (3) hold, then with probability \u2265 1 \u2212 2\u03b4,\n\u2223\u2223 1 n n \u2211 i=1 (X(i) \u2212 \u00b5\u0302)(X(i) \u2212 \u00b5\u0302)t \u2212 \u03c3\u03022Id \u2212 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 \u2264 c(n) def= c\u03c32 \u00bf\u00c1\u00c1\u00c0d log n2\u03b4 n + c\u03c32 dk2 log n 2 \u03b4 n + c\u03c3 \u00bf\u00c1\u00c1\u00c0dk2 log n2\u03b4 n max i \u221a w\u0302i \u2223\u2223\u00b5i \u2212\u00b5\u2223\u22232 ,\n(5)\nfor a constant c.\nProof. Since Equations (1), (2), and (3) hold, conditions in Lemmas 26 and 28 are satisfied. By Lemma 28,\n\u2223\u2223 k\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 = O \u239b\u239c\u239c\u239d\u03c3 2 dk2 log n 2 \u03b4 n + \u03c3 \u00bf\u00c1\u00c1\u00c0dk2 log n2\u03b4 n max i \u221a w\u0302i \u2223\u2223\u00b5i \u2212\u00b5\u2223\u22232 \u239e\u239f\u239f\u23a0 . Hence it remains to show,\n\u2223\u2223 1 n n \u2211 i=1 (X(i) \u2212 \u00b5\u0302)(X(i) \u2212 \u00b5\u0302)t \u2212 k\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t\u2223\u2223 = O \u239b\u239c\u239c\u239d \u00bf\u00c1\u00c1\u00c0kd log 5k2\u03b4 n \u03c32 \u239e\u239f\u239f\u23a0 . By Lemma 29, the covariance matrix can be rewritten as\nk\n\u2211 i=1 w\u0302i(\u00b5\u0302i \u2212 \u00b5\u0302)(\u00b5\u0302i \u2212 \u00b5\u0302)t \u2212 w\u0302i(\u00b5\u0302i \u2212\u00b5i)(\u00b5\u0302i \u2212\u00b5i)t + k\u2211 i=1 \u2211 j\u2223X(j)\u223cpi 1 n (X(j) \u2212\u00b5i)(X(j) \u2212\u00b5i)t \u2212 \u03c3\u03022Id. (6)\nWe now bound the norms of second and third terms in the above equation. Consider the third term, \u2211ki=1\u2211j\u2223X(j)\u223cpi 1n(X(j) \u2212 \u00b5i)(X(j) \u2212 \u00b5i)t. Conditioned on the fact that X(j) \u223c pi, X(j) \u2212 \u00b5i is distributed N(0, \u03c32Id), therefore by Lemma 18 and Lemma 4 ,with probability \u2265 1 \u2212 2\u03b4,\nRRRRRRRRRRRR RRRRRRRRRRRR k \u2211 i=1 \u2211 j\u2223X(j)\u223cpi 1 n (X(j) \u2212\u00b5i)(X(j) \u2212\u00b5i)t \u2212 \u03c3\u03022Id RRRRRRRRRRRR RRRRRRRRRRRR \u2264 c \u2032 \u00bf\u00c1\u00c1\u00c0d log 2d\u03b4 n \u03c32 + 2.5\u03c32 \u00bf\u00c1\u00c1\u00c0 log n2\u03b4 d .\nThe second term in Equation (6) is bounded by Lemma 25. Hence together with the fact that d \u2265 20 log n2/\u03b4 we get that with probability \u2265 1 \u2212 2\u03b4, the second and third terms are bounded by O (\u03c32\u221adk\nn log n\n2 \u03b4 ) .\nLemma 31. Let u be the largest eigenvector of the sample covariance matrix and n \u2265 c \u22c5 dk2 log n2 \u03b4\n. If maxi \u221a w\u0302i \u2223\u2223\u00b5i \u2212\u00b5\u2223\u22232 = \u03b1\u03c3 and Equation (5) holds, then there exists i such that \u2223u \u22c5 (\u00b5i \u2212 \u00b5)\u2223 \u2265 \u03c3(\u03b1 \u2212 1 \u2212\n1/\u03b1)/\u221ak. Proof. Observe that \u2223\u2223\u2211j wjvjvtj \u2223\u2223 \u2265 \u2223\u2223\u2211j wjvjvtj vi\u2223\u2223vi\u2223\u2223 \u2223\u22232 \u2265 wi \u2223\u2223vi\u2223\u222322. Therefore\n\u2223\u2223 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)t\u2223\u2223 \u2265 RRRRRRRRRRR RRRRRRRRRRR k \u2211 j=1 w\u0302j(\u00b5j \u2212\u00b5)(\u00b5j \u2212\u00b5)t(\u00b5i \u2212\u00b5)/ \u2223\u2223\u00b5i \u2212\u00b5\u2223\u2223RRRRRRRRRRR RRRRRRRRRRR2 \u2265 \u03b1 2\u03c32.\nHence by Lemma 30 and the triangle inequality, the largest eigenvalue of the sample-covariance matrix is \u2265 \u03b12\u03c32\u2212c(n). Similarly by applying Lemma 30 again we get,\u2223\u2223\u2211ki=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)tu\u2223\u22232 \u2265 \u03b12\u03c32\u22122c(n). By triangle inequality and Cauchy-Schwartz inequality,\n\u2223\u2223 k\u2211 i=1 w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)tu\u2223\u2223 2 \u2264 k \u2211 i=1 \u2223\u2223w\u0302i(\u00b5i \u2212\u00b5)(\u00b5i \u2212\u00b5)tu\u2223\u22232 \u2264 k\n\u2211 i=1 w\u0302i \u2223\u2223(\u00b5i \u2212\u00b5)\u2223\u22232max j \u2223(\u00b5j \u2212\u00b5) \u22c5 u\u2223 \u2264\n\u00bf\u00c1\u00c1\u00c0 k\u2211 i=1 w\u0302i \u2223\u2223(\u00b5i \u2212\u00b5)\u2223\u222322max j\n\u2223(\u00b5j \u2212\u00b5) \u22c5 u\u2223 \u2264 \u221a k\u03b1\u03c3max\nj \u2223(\u00b5j \u2212\u00b5) \u22c5 u\u2223.\nHence \u221a k\u03b1\u03c3maxi \u2223(\u00b5i \u2212 \u00b5) \u22c5 u\u2223 \u2265 \u03b12\u03c32 \u2212 2c(n). The lemma follows by substituting the bound on n in\nc(n). We now make a simple observation on Gaussian mixtures.\nFact 32. The samples from a subset of components A of the Gaussian mixture are distributed according to a Gaussian mixture of components A with weights being w\u2032i = wi/(\u2211j\u2208Awj).\nWe now prove Lemma 6.\nProof of Lemma 6. Observe that we run the recursive clustering at most n times. At every step, the underlying distribution within a cluster is a Gaussian mixture. Let Equations (1), (2) hold with probability 1\u2212 2\u03b4. Let Equations (3) (5) all hold with probability \u2265 1 \u2212 \u03b4\u2032, where \u03b4\u2032 = \u03b4/2n at each of n steps. By the union bound the total error is \u2264 2\u03b4 + \u03b4\u2032 \u22c5 2n \u2264 3\u03b4. Since Equations (1), (2) holds, the conditions of Lemmas 4 and 5 hold. Furthermore it can be shown that discarding at most n\u01eb/4k samples at each step does not affect the calculations.\nWe first show that if \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2265 25\u221ak3 log(n3/\u03b4)\u03c3, then the algorithm gets into the loop.\nLet w\u2032i be the weight of the component within the cluster and n \u2032 \u2265 n\u01eb/5k be the number of samples in\nthe cluster. Let \u03b1 = 25 \u221a k3 log(n3/\u03b4). By Fact 32, the components in cluster C have weight w\u2032i \u2265 wi.\nHence \u221a w\u2032i \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2265 \u03b1\u03c3. Since \u221aw\u2032i \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2265 \u03b1\u03c3, and by Lemma 5 \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u2223 \u2264\n10k\u03c3(d log n2/\u03b4)1/4, we have w\u2032i \u2265 \u03b12/(100k2\u221ad log n2/\u03b4). Hence by lemma 24, w\u2032i \u2265 wi/2 and \u221aw\u0302\u2032i \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2265 \u03b1\u03c3/\u221a2. Hence by Lemma 30 and triangle inequality the largest eigenvalue of S(C) is \u2265 \u03b12\u03c32/2 \u2212 c(n\u2032) \u2265 \u03b12\u03c32/4 \u2265 \u03b12\u03c3\u03022/8 \u2265 12\u03c3\u03022k3 logn2/\u03b4\u2032 = 12\u03c3\u03022k3 logn3/\u03b4. Therefore the algorithm gets into the loop.\nIf n\u2032 \u2265 n\u01eb/8k2 \u2265 c\u22c5dk2 log n3 \u03b4 , then by Lemma 31, there exists a component i such that \u2223u\u22c5(\u00b5i\u2212\u00b5(C))\u2223 \u2265 \u03c3(\u03b1/\u221a2 \u2212 1 \u2212\u221a2/\u03b1)/\u221ak, where u is the top eigenvector of the first n\u01eb/4k2 samples.\nObserve that \u2211i\u2208C wiu \u22c5 (\u00b5i \u2212\u00b5(C)) = 0 and maxi \u2223u \u22c5 (\u00b5i \u2212\u00b5(C))\u2223 \u2265 \u03c3(\u03b1/\u221a2 \u2212 1 \u2212\u221a2/\u03b1)/\u221ak. Let \u00b5i be sorted according to their values of u \u22c5 (\u00b5i \u2212\u00b5(C)), then\nmax i \u2223u \u22c5 (\u00b5i \u2212\u00b5i+1)\u2223 \u2265 \u03c3\u03b1/ \u221a 2 \u2212 1 \u2212\u221a2/\u03b1 k3/2 \u2265 12\u03c3 \u221a log n3 \u03b4 \u2265 9\u03c3\u0302 \u221a log n3 \u03b4 ,\nwhere the last inequality follows from Lemma 4 and the fact that d \u2265 20 log n2/\u03b4. For a sample from component pi, similar to the proof of Lemma 5, by Lemma 15, with probability \u2265 1 \u2212 \u03b4/n2k, \u2223\u2223u \u22c5 (X(i) \u2212\u00b5i)\u2223\u2223 \u2264 \u03c3\u221a2 log(n2k/\u03b4)2 \u2264 2\u03c3\u0302\u221alog(n2k/\u03b4), where the second inequality follows from Lemma 4. Since there are two components that are far apart by\n\u2265 9\u03c3\u0302 \u221a log n 2\n\u03b4 \u03c3\u0302 and the maximum distance between a sample and its mean is \u2264 2\u03c3\u0302\n\u221a log(n2k/\u03b4) and the\nalgorithm divides into at-least two non-empty clusters such that no two samples from the same distribution are clustered into two clusters.\nFor the second part observe that by the above concentration on u, no two samples from the same component are clustered differently irrespective of the mean separation. Note that we are using the fact that each sample is clustered at most 2k times to get the bound on the error probability. The total error probability by the union bound is \u2264 4\u03b4."}, {"heading": "D.4 Proof of Lemma 7", "text": "We show that if the conclusions in Lemmas 6 and 24 holds, then the lemma is satisfied. We also assume that the conclusions in Lemma 30 holds for all the clusters with error probability \u03b4\u2032 = \u03b4/k. By the union bound the total error probability is \u2264 7\u03b4.\nBy Lemma 6 all the components within each cluster satisfy \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2264 25\u03c3\u221ak3 log(n3/\u03b4). Let n \u2265 c \u22c5dk9\u01eb\u22124 log2 d/\u03b4. For notational convenience let S(C) = 1\u2223C\u2223 \u2211\u2223C\u2223i=1(X(i)\u2212\u00b5(C))(X(i)\u2212\u00b5(C))t\u2212 \u03c3\u03022Id. Therefore by Lemma 30 for large enough c,\n\u2223\u2223S(C) \u2212 n\u2223C \u2223 \u2211i\u2208C w\u0302i(\u00b5i \u2212\u00b5(C))(\u00b5i \u2212\u00b5(C))t\u2223\u2223 \u2264 \u01eb2\u03c32 1000k2 n\u2223C \u2223 .\nLet v1,v2, . . .vk\u22121 be the top eigenvectors of 1 \u2223C\u2223 \u2211i\u2208C wi(\u00b5i\u2212\u00b5(C))(\u00b5i\u2212\u00b5(C))t. Let \u03b7i =\u221aw\u0302\u2032i \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 =\u221a\nw\u0302i \u221a\nn \u2223C\u2223 \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232. Let \u2206i = \u00b5i\u2212\u00b5(C))\u2223\u2223(\u00b5i\u2212\u00b5(C))\u2223\u22232 . Therefore,\n\u2211 i\u2208C n\u2223C \u2223 \u2211i\u2208C w\u0302i(\u00b5i \u2212\u00b5(C))(\u00b5i \u2212\u00b5(C))t = \u2211i\u2208C \u03b72i\u2206i\u2206ti. Hence by Lemma 20, the projection of \u2206i on the space orthogonal to top k \u2212 1 eigenvectors of S(C) is\n\u2264 \u00bf\u00c1\u00c1\u00c0 \u01eb2\u03c32 1000k2 n\u2223C \u2223 1\u03b7i \u2264 \u01eb\u03c3 16 \u221a w\u0302i \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 k \u2264\n\u01eb\u03c3 8 \u221a 2 \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 k .\nThe last inequality follows from the bound on w\u0302i in Lemma 24."}, {"heading": "D.5 Proof of Theorem 8", "text": "We show that the theorem holds if the conclusions in Lemmas 7 and 26 holds with error probability \u03b4\u2032 = \u03b4/k. Since in the proof of Lemma 7, the probability that Lemma 6 holds is included, Lemma 6 also holds with the same probability. Since there are at most k clusters, by the union bound the total error probability is \u2264 9\u03b4.\nFor every component i, we show that there is a choice of mean vector and weight in the search step such that wiD(pi, p\u0302i) \u2264 \u01eb/2k and \u2223wi \u2212 w\u0302i\u2223 \u2264 \u01eb/4k. That would imply that there is a f\u0302 during the search such that\nD(f , f\u0302) \u2264\u2211 C \u2211 i\u2208C wiD(pi, p\u0302i) + 2 k\u22121\u2211 i=1 \u2223wi \u2212 w\u0302i\u2223 \u2264 \u01eb 2k + \u01eb 2k = \u01eb.\nSince the weights are gridded by \u01eb/4k, there exists a w\u0302i such that \u2223wi \u2212 w\u0302i\u2223 \u2264 \u01eb/4k. We now show that there exists a choice of mean vector such that wiD(pi, p\u0302i) \u2264 \u01eb/2k. Note that if a component has weight \u2264 \u01eb/4k, the above inequality follows immediately. Therefore we only look at those components with wi \u2265 \u01eb/4k, by Lemma 24, for such components w\u0302i \u2265 \u01eb/5k and therefore we only look at clusters such that \u2223C \u2223 \u2265 n\u01eb/5k. By Lemmas 14 and for any i,\nD(pi, p\u0302i)2 \u2264 2 d\u2211 j=1 (\u00b5i,j \u2212 \u00b5\u0302i,j)2 \u03c32 + 8d(\u03c32 \u2212 \u03c3\u03022)2 \u03c34 .\nNote that since we are discarding at most n\u01eb/8k2 random samples at each step. A total number of \u2264 n\u01eb/8k random samples are discarded. It can be shown that this does not affect our calculations and we ignore it in\nthis proof. By Lemma 4, the first estimate of \u03c32 satisfies \u2223\u03c3\u03022\u2212\u03c32\u2223 \u2264 2.5\u03c32\u221alogn2/\u03b4. Hence while searching over values of \u03c3\u03022, there exist one such that \u2223\u03c3\u20322 \u2212 \u03c32\u2223 \u2264 \u01eb\u03c32/\u221a64dk2. Hence,\nD(pi, p\u0302i)2 \u2264 2 \u2223\u2223\u00b5i \u2212 \u00b5\u0302i\u2223\u222322 \u03c32 + \u01eb2 8k2 .\nTherefore if we show that there is a mean vector \u00b5\u0302i during the search such that \u2223\u2223\u00b5i \u2212 \u00b5\u0302i\u2223\u22232 \u2264 \u01eb\u03c3/\u221a16k2w\u0302i, that would prove the Lemma. By triangle inequality, \u2223\u2223\u00b5i \u2212 \u00b5\u0302i\u2223\u22232 \u2264 \u2223\u2223\u00b5(C) \u2212 \u00b5\u0302(C)\u2223\u22232 + \u2223\u2223\u00b5i \u2212\u00b5(C) \u2212 (\u00b5\u0302i \u2212 \u00b5\u0302(C))\u2223\u22232 . By Lemma 26 for large enough n,\n\u2223\u2223\u00b5(C) \u2212 \u00b5\u0302(C)\u2223\u2223 2 \u2264 c\u03c3 \u00bf\u00c1\u00c1\u00c0dk log2 n2/\u03b4\u2223C \u2223 \u2264 \u01eb\u03c38k\u221awi . The second inequality follows from the bound on n and the fact that \u2223C \u2223 \u2265 nw\u0302i. Since wi \u2265 \u01eb/4k, by Lemma 24, w\u0302i \u2265 wi/2, we have \u2223\u2223\u00b5i \u2212 \u00b5\u0302i\u2223\u22232 \u2264 \u2223\u2223\u00b5i \u2212\u00b5(C) \u2212 (\u00b5\u0302i \u2212 \u00b5\u0302(C))\u2223\u22232 + \u01eb\u03c38k\u221awi . Let u1 . . .uk\u22121 are the top eigenvectors the sample covariance matrix of cluster C . We now prove that during the search, there is a vector of the form \u2211k\u22121j=1 gj\u01ebg\u03c3\u0302uj such that \u2223\u2223\u00b5i \u2212\u00b5(C) \u2212\u2211k\u22121j=1 gj\u01ebg\u03c3\u0302uj \u2223\u22232 \u2264 \u01eb\u03c38k\u221awi , during the search, thus proving the lemma. Let \u03b7i = \u221a wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232. By Lemma 7, there are set of coefficients \u03b1i such that \u00b5i \u2212\u00b5(C)\u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 = k\u22121 \u2211 j=1 \u03b1juj + \u221a\n1 \u2212 \u2223\u2223\u03b1\u2223\u22232u\u2032, where u\u2032 is perpendicular to u1 . . .uk\u22121 and \u221a 1 \u2212 \u2223\u2223\u03b1\u2223\u22232 \u2264 \u01eb\u03c3/(8\u221a2\u03b7ik). Hence, we have\n\u00b5i \u2212\u00b5(C) = k\u22121\u2211 j=1 \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u03b1juj + \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232\u221a1 \u2212 \u2223\u2223\u03b1\u2223\u222322u\u2032, Since wi \u2265 \u01eb/4k and by Lemma 6, \u03b7i \u2264 25\u221ak3\u03c3 log(n3/\u03b4), and \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u22232 \u2264 100\u221ak4\u01eb\u22121\u03c3 log(n3/\u03b4). Therefore \u2203gj such that \u2223gj \u03c3\u0302 \u2212\u03b1j \u2223 \u2264 \u01ebg\u03c3\u0302 on each eigenvector. Hence,\nwi \u2223\u2223\u00b5i \u2212\u00b5(C) \u2212 k\u22121\u2211 i=1 gj\u01ebg\u03c3\u0302uj \u2223\u22232 2 \u2264 wik\u01eb 2 g\u03c3\u0302 2 +wi \u2223\u2223\u00b5i \u2212\u00b5(C)\u2223\u222322 (1 \u2212 \u2223\u2223\u03b1\u2223\u22232) \u2264 k\u01eb2g\u03c3\u0302 2 + \u03b72i \u01eb 2\u03c32\n128\u03b72i k 2\n\u2264 \u01eb2\u03c32 128k2 + \u01eb2\u03c32 128k2 \u2264 \u01eb2\u03c32 64k2 .\nThe last inequality follows by Lemma 4 and the fact that \u01ebg \u2264 \u01eb/16k3/2, and hence the theorem. The run time can be easily computed by retracing the steps of the algorithm and using an efficient implementation of single-linkage."}], "references": [{"title": "Optimal probability estimation with applications to prediction and classification", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Strong converse for identification via quantum channels", "author": ["Rudolf Ahlswede", "Andreas Winter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures", "author": ["Joseph Anderson", "Mikhail Belkin", "Navin Goyal", "Luis Rademacher", "James R. Voss"], "venue": "CoRR, abs/1311.2891,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Minimax theory for high-dimensional gaussian mixtures with sparse mean separation", "author": ["Martin Azizyan", "Aarti Singh", "Larry A. Wasserman"], "venue": "CoRR, abs/1306.2035,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani"], "venue": "CoRR, abs/0912.0086,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Image segmentation by clustering", "author": ["G.B. Coleman", "Harry C. Andrews"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1979}, {"title": "Elements of information theory (2", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A two-round variant of EM for gaussian mixtures", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "In Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "Journal on Machine Learning Research (JMLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Learning k-modal distributions via testing", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio"], "venue": "In SODA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Learning poisson binomial distributions", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio"], "venue": "In Proceedings of the 44th Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians", "author": ["Constantinos Daskalakis", "Gautam Kamath"], "venue": "CoRR, abs/1312.1054,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan"], "venue": "In Proceedings of the 2nd Industrial Conference on Data Mining (ICDM),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["Jon Feldman", "Ryan O\u2019Donnell", "Rocco A. Servedio"], "venue": "In Proceedings of the 46th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "PAC learning axis-aligned mixtures of gaussians with no separation assumption", "author": ["Jon Feldman", "Rocco A. Servedio", "Ryan O\u2019Donnell"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Estimating a mixture of two product distributions", "author": ["Yoav Freund", "Yishay Mansour"], "venue": "In Proceedings of the 13th Annual Conference on Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M. Kakade"], "venue": "In Proceedings of the 4th Innovations in Theoretical Computer Science Conference (ITCS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 42nd Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "On the learnability of discrete distributions", "author": ["Michael J. Kearns", "Yishay Mansour", "Dana Ron", "Ronitt Rubinfeld", "Robert E. Schapire", "Linda Sellie"], "venue": "In Proceedings of the 26th Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "Pascal Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Mixture Models: Theory, Geometry and Applications. NSF-CBMS Conference series in Probability and Statistics, Penn", "author": ["Bruce G. Lindsay"], "venue": "State University,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Asymptotic convergence rate of the em algorithm for gaussian mixtures", "author": ["Jinwen Ma", "Lei Xu", "Michael I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "CoRR, abs/1305.3207,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 24th Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "On modeling profiles instead of values", "author": ["Alon Orlitsky", "Narayana P. Santhanam", "Krishnamurthy Viswanathan", "Junan Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Variational minimax estimation of discrete distributions under kl loss", "author": ["Liam Paninski"], "venue": "In Proceedings of the 18th Annual Conference on Neural Information Processing (NIPS),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Mixture densities, maximum likelihood and the em algorithm", "author": ["Richard A. Redner", "Homer F. Walker"], "venue": "SIAM Review,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1984}, {"title": "Robust text-independent speaker identification using gaussian mixture speaker models", "author": ["Douglas A. Reynolds", "Richard C. Rose"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Slink: An optimally efficient algorithm for the single-link cluster method", "author": ["Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1973}, {"title": "Statistical analysis of finite mixture distributions, volume 7", "author": ["D Michael Titterington", "Adrian FM Smith", "Udi E Makov"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1985}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts", "author": ["G. Valiant", "P. Valiant"], "venue": "Proceedings of the 43rd Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Estimating the unseen: A sublinear-sample canonical estimator of distributions", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["Santosh Vempala", "Grant Wang"], "venue": "In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "CoRR, abs/1011.3027,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Feature selection for high-dimensional genomic microarray data", "author": ["Eric P. Xing", "Michael I. Jordan", "Richard M. Karp"], "venue": "In Proceedings of the 18th Annual International Conference on Machine Learning (ICML),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}], "referenceMentions": [{"referenceID": 16, "context": "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 41, "context": "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 25, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 33, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 35, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 26, "context": "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].", "startOffset": 82, "endOffset": 90}, {"referenceID": 32, "context": "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].", "startOffset": 82, "endOffset": 90}, {"referenceID": 4, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 10, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 11, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 20, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 39, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 1, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 3, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 5, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 9, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 21, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 27, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 22, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 218, "endOffset": 222}, {"referenceID": 21, "context": "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.", "startOffset": 10, "endOffset": 18}, {"referenceID": 27, "context": "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "PAC learning [24] does not approximate each mixture component, but instead derives a mixture distribution that is close to the original one.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 10, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 11, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 20, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 39, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 10, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 11, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 39, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 30, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 69, "endOffset": 77}, {"referenceID": 38, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 69, "endOffset": 77}, {"referenceID": 13, "context": ",s} requires \u0398(m log(s/m)/\u01eb3) samples [14].", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": ",s} can be learned with O(k/\u01eb4), O(k log(s/\u01eb)/\u01eb4), and O(k log(s)/\u01eb4) samples, respectively, and these bounds are tight up to a factor of \u01eb [31].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "For example, for learning spherical Gaussian mixtures, the number of samples required by previous algorithms is O(d12) for k = 2 components, and increased exponentially with k [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "[20] considered mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] eliminated the probability constraints and generalized the results from binary to arbitrary discrete alphabets, and from 2 to k mixture components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] generalized these results to Gaussian products, showing in particular that mixtures of k Gaussians, where the difference between the means normalized by the ratio of standard deviations is bounded by B, are PAC learnable in \u00d5((dB/\u01eb)2k(k+1)) time, and can be shown to use \u00d5((dB/\u01eb)4(k+1)) samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require \u2265 nd time.", "startOffset": 73, "endOffset": 80}, {"referenceID": 39, "context": "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require \u2265 nd time.", "startOffset": 73, "endOffset": 80}, {"referenceID": 14, "context": "2 One-dimensional Gaussian mixtures Independently and around the same time as this work [15] showed that mixtures of two one-dimensional Gaussians can be learnt with \u00d5(\u01eb\u22122) samples and in time O(\u01eb\u22127.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "4 The approach and technical contributions The popular SCHEFFE estimator takes a collection F of distributions and uses O(log \u2223F\u2223) independent samples from an underlying distribution f to find a distribution in F whose distance from f is at most a constant factor larger than that of the distribution in F that is closet to f [16].", "startOffset": 326, "endOffset": 330}, {"referenceID": 17, "context": "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 36, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 40, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 13, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 17, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 19, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 15, "context": "The well-known Scheffe\u2019s method [16] uses O(\u01eb\u22122 log \u2223F\u2223) samples from the underlying distribution f , and in time O(\u01eb\u22122\u2223F\u22232T log \u2223F\u2223) outputs a distribution in F with l1 distance of at most 9.", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "One of the natural and well-used methods to estimate the span of mean vectors is using the correlation matrix [42].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.", "startOffset": 58, "endOffset": 64}, {"referenceID": 39, "context": "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.", "startOffset": 58, "endOffset": 64}, {"referenceID": 34, "context": "Note that the run time is calculated based on the efficient implementation of single-linkage [37] and the exponential term is not optimized.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 12, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 13, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 14, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 28, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 29, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 31, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 38, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 14, "context": "The above bound matches the independent and contemporary result by [15] for k = 2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "[1] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Dimitris Achlioptas and Frank McSherry.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Rudolf Ahlswede and Andreas Winter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Martin Azizyan, Aarti Singh, and Larry A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Mikhail Belkin and Kaushik Sinha.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Kamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Thomas M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sanjoy Dasgupta and Leonard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Sanjoy Dasgupta and Leonard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Constantinos Daskalakis and Gautam Kamath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Luc Devroye and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Inderjit S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Jon Feldman, Ryan O\u2019Donnell, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Jon Feldman, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Yoav Freund and Yishay Mansour.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Daniel Hsu and Sham M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ravindran Kannan, Hadi Salmasian, and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Bruce G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Jinwen Ma, Lei Xu, and Michael I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Ankur Moitra and Gregory Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Siu on Chan, Ilias Diakonikolas, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Siu on Chan, Ilias Diakonikolas, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Alon Orlitsky, Narayana P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Liam Paninski.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] Richard A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[36] Douglas A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] Robin Sibson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] D Michael Titterington, Adrian FM Smith, and Udi E Makov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[39] Joel A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41] Gregory Valiant and Paul Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[42] Santosh Vempala and Grant Wang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[43] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[44] Eric P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For Gaussian distributions the Bhattacharyya parameter is (see [8]), B(p1, p2) = ye, where x = (\u03bc1\u2212\u03bc2)) 4(\u03c32 1 +\u03c3 2 ) and y = \u221a 2\u03c31\u03c32 \u03c3 1 +\u03c3 2 .", "startOffset": 63, "endOffset": 66}, {"referenceID": 24, "context": "Lemma 16 ( [25]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 40, "context": "Lemma 18 ( [43] Remark 5.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "Scheffe estimate [16] outputs a distribution from F whose l1 distance from f is at most 9.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Together with an observation in Scheffe estimation in [16] one can show that if the number of samples n = O ( log \u2223F\u2223 \u03b4 \u01eb ), then SCHEFFE* has a guarantee 10max(\u01eb,D(f,F)) with probability \u2265 1 \u2212 \u03b4.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "1 Single Gaussian distribution The proof is an application of the following version of Fano\u2019s inequality [9, 45].", "startOffset": 105, "endOffset": 112}], "year": 2014, "abstractText": "Statistical and machine-learning algorithms are frequently applied to high-dimensional data. In many of these applications data is scarce, and often much more costly than computation time. We provide the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any k d-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses Ok(d log d \u01eb ) samples and runs in time Ok,\u01eb(d3 log d), both significantly lower than previously known. The constant factor Ok is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that \u03a9k( d \u01eb ) samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions. We also derive a simple estimator for k-component one-dimensional mixtures that uses O(k log k\u01eb \u01eb ) samples and runs in time \u00d5 ((k \u01eb )). Our other technical contributions include a faster algorithm for choosing a density estimate from a set of distributions, that minimizes the l1 distance to an unknown underlying distribution. jacharya@ucsd.edu ashkan@ucsd.edu alon@ucsd.edu asuresh@ucsd.edu", "creator": "LaTeX with hyperref package"}}}