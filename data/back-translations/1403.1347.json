{"id": "1403.1347", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction", "abstract": "The prediction of the secondary structure of proteins is a fundamental problem in the prediction of protein structures. Here, we present a new method based on supervised generative stochastic network (GSN) to predict local secondary structures with deep hierarchical representations. GSN is a recently proposed method of deep learning (Bengio & amp; Thibodeau-Laufer, 2013) to globally learn a deep generative model. We introduce the supervised extension of GSN, in which a Markov chain is selected from a conditional distribution and applied to the prediction of protein structures. To scale the model to complete, high-dimensional data, such as protein sequences with hundreds of amino acids, we introduce a revolutionary architecture that enables efficient learning across multiple levels of hierarchical representations. Our architecture focuses exclusively on predicting low-level structured labels that correspond to both low and high-dimensional data as learned by our model of structural minerals.", "histories": [["v1", "Thu, 6 Mar 2014 05:18:26 GMT  (264kb,D)", "http://arxiv.org/abs/1403.1347v1", "Accepted by ICML 2014"]], "COMMENTS": "Accepted by ICML 2014", "reviews": [], "SUBJECTS": "q-bio.QM cs.CE cs.LG", "authors": ["jian zhou", "olga g troyanskaya"], "accepted": true, "id": "1403.1347"}, "pdf": {"name": "1403.1347.pdf", "metadata": {"source": "META", "title": "Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction", "authors": ["Jian Zhou", "Olga G. Troyanskaya"], "emails": ["JZTHREE@PRINCETON.EDU", "OGT@CS.PRINCETON.EDU"], "sections": [{"heading": "1. Introduction", "text": "Understanding complex dependency between protein sequence and structure is one of the greatest challenges in computational biology (Cheng et al., 2008). Although it\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nis widely accepted that the amino-acid sequence contains sufficient information to determine the three dimensional structure of a protein, it is extremely difficult to predict protein structure based on sequence. Understanding protein structure is critical for analyzing protein function and applications like drug design. Protein secondary structure prediction determines structural states of local segments of amino-acid residues, such as \u03b1-helix and \u03b2-strand, and it provides important information for further elucidating the three-dimensional structure of protein. Thus it is also being used as input by many other protein sequence and structure analysis algorithms.\nProtein secondary structure prediction has been extensively studied with machine learning approaches (Singh, 2005).The key challenge in this field is predicting for protein sequences with no close homologs that have known 3D structures. Since the early work by (Qian & Sejnowski, 1988), neural networks have been widely applied and are core components of many most successful approaches (Rost & Sander, 1993; Jones, 1999; Baldi et al., 1999). Most significant improvement to secondary structure prediction has been achieved by leveraging evolutionary information by using sequence profiles from multiple-sequence alignment (Rost & Sander, 1993) or position-specific scoring matrices from PSI-BLAST (Jones, 1999). Other developments include better capturing spatial dependencies using bidirectional recurrent neural networks (BRNN) (Baldi et al., 1999; Pollastri et al., 2002), probabilistic graphical models (Schmidler et al., 2000; Chu et al., 2004; van der Maaten et al., 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al., 2009; Wang et al., 2011). Secondary structures are commonly classified to 8 states (Kabsch & Sander, 1983) or be further combined into 3 states (Singh, 2005). Most secondary structure prediction studies have been focusing on coarse-grained 3-state secondary structure prediction. Achieving fine-grained 8- state secondary secondary prediction, while it reveals more structural details than 3-state predicitons, is a more chalar X iv :1 40 3. 13 47 v1 [\nqbi\no. Q\nM ]\n6 M\nar 2\n01 4\nlenging and less-addressed problem (Pollastri et al., 2002; Wang et al., 2011). Here we address the 8-state classification problem for proteins with no close homologs with known structure.\nIt is widely believed that introducing global information from the whole protein sequence is crucial for further improving prediction performance of local secondary structure labels, since secondary structure formation depends on both local and long-range interactions. For example, a secondary structure state \u03b2-strand is stabilized by hydrogen bonds formed with other \u03b2-strands that can be far apart from each other in the protein sequence. Many contemporary methods such as BRNN can capture some form of spatial dependency, but they are still limited in capturing complex spatial dependency. To our knowledge none of these methods learns and leverages deep hierarchical representation, which has enabled construction of better intelligent systems interpreting various types of other complex structured natural data, like images, speech, and text. Models with deep representation have been exceptionally successful in capturing complex dependency in these data (Bengio et al., 2013a; Salakhutdinov & Hinton, 2009). Learning features automatically can be especially helpful for proteins, as we lack the necessary intuition or knowledge for hand-crafting mid- and high- level features involving multiple amino acids.\nIn this work, we introduce a series of techniques to tackle challenges of protein secondary structure prediction with deep learning, and we demonstrate their superior accuracy on 8-state protein secondary structure prediction over previous methods. Although we focus on the secondary structure prediction application here, our methods are general and can be applied to a broad range of problem within and outside of computational biology. First, we extended the recently developed generative stochastic network (GSN) technique to supervised structured output prediction as proposed in (Bengio et al., 2013a). Supervised GSN learns a Markov chain to sample from the distribution of output conditioned on input data, and like GSN it avoided intractable explicit marginalization over hidden variables in learning deep models with hidden layers. The advantage of the supervised GSN over GSN is analogous to the advantage of conditional random field (CRF) over Markov random field (MRF), or discriminative versus generative classifiers, that is, we focus the resources on dependencies that are important for our desired prediction, or the distribution of labels conditional on data. The supervised GSN can be applied to generate structured output of secondary structure states for all amino-acid residues of each protein.\nAnother important element of our approach that enables learning hierarchical representation of full-size data, like protein sequences with hundreds of amino-acids, is intro-\nducing a convolutional architecture for GSN that allows for learning multiple layers of hierarchical representations. The convolutional architecture also allows efficient communication between high- and low- level features and features at different spatial locations. Convolutional GSN is well suited for making predictions sensitive to low-level or local information, while informed with high-level or global information."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Generative Stochastic Networks", "text": "The generative stochastic network (GSN) is a recently proposed model that utilizes a new unconventional approach to learn a generative model of data distribution without explicitly specifying a probabilistic graphical model, and allows learning deep generative model through global training via back-propagation. Generative stochastic network learns a Markov chain to sample from data distribution P (X). During training, GSN trains a stochastic computational graph to reconstruct the input X , which generalizes the generalized denoising auto-encoder (Bengio et al., 2013b). The difference of GSN is that it allows the computational graph to have a latent state, and noise is injected to not just the input, but also to the intermediate computations.\nCompared to other deep generative models like deep Boltzmann machine, a major advantage of GSN is that it avoided intractable inference and explicit marginalization over latent variables, as it does not explicitly learn parameters of a probabilistic graphical model but rather learns to directly sample from the data distribution. GSN also enjoys feasible global training by back-propagating the reconstruction cost.\nThe following are proven in (Bengio et al., 2013b) and (Bengio et al., 2013a) as theoretical basis for generalized denoising autoencoder and generative stochastic network:\nTheorom 1. If P\u03b8(X|X\u0303) is a consistent estimator of the true conditional distribution P (X|X\u0303) ,and Tn defines an irreducible and ergodic Markov chain, then as n \u2192 \u221e, the stationary distribution of the generated samples \u03c0(X) converges to the data generating distribution P (X).\nIn (Bengio et al., 2013a) this result is extended to introduce a latent state Ht and replace the adding noise and denoising operators with Ht+1 \u223c P\u03b81(H|Ht, Xt) and Xt+1 \u223c P\u03b82(X|Ht+1) respectively.\nTheorem 2. Let training data X \u223c P (X) and independent noise Z \u223c P (Z) and introduce a sequence of latent variables H defined iteratively through a function f with Ht = f\u03b81(Xt\u22121, Zt\u22121, Ht\u22121) for a given sequence of Xt\u2019s. Consider a model P\u03b82(X|f\u03b81(X,Zt\u22121, Ht\u22121))\ntrained (over both \u03b81 and \u03b82) so that for a given \u03b81, it is a consistent estimator of the true P (X|H). Consider the Markov chain defined above and assume that it converges to a stationary distribution \u03c0n over the X and H and with marginal \u03c0n(X), even in the limit as number of training examples n\u2192\u221e. Then \u03c0n(X)\u2192 P (X) as n\u2192\u221e.\nIt is suggested in (Bengio et al., 2013a) that the GSN can be extended to tackle structured output problems. As focusing on conditional distribution while sharing the advantages of GSN seems to provide an attractive new approach of leveraging deep representations for structured output prediction. We explored this idea of supervised generative stochastic network in the following."}, {"heading": "2.2. Supervised Generative Stochastic Network", "text": "We phrase the supervised generative stochastic network as follows. We use X to denote input random variable and Y to denote output random variable. Supervised GSN aims to capture conditional dependency structure of Y given X. Analogous to GSN, it learns a Markov chain that samples from P (Y |X).\nLet C(Y\u0303 |Y ) be a corruption process that transforms Y into a random variable Y\u0303 . Let P\u03b8(Y |Y\u0303 , X) be a denoising autoencoder that computes probability of Y given Y\u0303 and X . Then we have the following.\nCorollary 1. If P\u03b8(Y |Y\u0303 , X) is a consistent estimator of the true conditional distribution P (Y |Y\u0303 , X), and Tn defines an irreducible and ergodic Markov chain, then as n\u2192\u221e, the stationary distribution of the generated samples \u03c0(Y |X) converges to the data generating distribution P (Y |X).\nProof. Let P (X \u2032) = P (Y |X); C(X\u0303 \u2032|X \u2032) = C(Y\u0303 |Y );\nP\u03b8(X \u2032|X\u0303 \u2032) = P(Y |Y\u0303 , X); Then this corollary is equivalent to theorem 1.\nDuring training, we learn a computational graph that estimates P\u03b8(Y |Y\u0303 , X). Therefore we provide both Y\u0303 and X\u0303 as input and the training procedure should minimize reconstruction cost. The reconstruction cost should be selected to be interpretable as (regularized) log likelihood of uncorrupted labels given the reconstructon distribution.\nTo generalize the supervised training to generative stochastic networks, similar to Corollary 1 we consider independent noise Z \u223c P (Z). Corruption process is now Ht = f\u03b81(Yt\u22121, Zt\u22121, Ht\u22121, X), where f\u03b81 is an arbitrary differentiable function. The reconstruction function P\u03b82(Y |f\u03b81(Yt\u22121, Zt\u22121, Ht\u22121, X)) is trained by regularized conditional maximum likelihood with examples of (Y,Z,X) triplets.\nCorollary 2. Assume that the trained model P\u03b82(Y |f\u03b81(Yt\u22121, Zt\u22121, Ht\u22121, X)) is a consistent estimator of the true P (Y |H), and the Markov chain Y \u223c P\u03b82(Y |f\u03b81(Yt\u22121, Zt\u22121, Ht\u22121, X)) converges to a stationary distribution \u03c0. Then we have \u03c0(Y |X) = P (Y |X) following Theorem 2.\nTo make a prediction from a trained supervised GSN model, we can sample from P\u03b81,\u03b82(Y |X) by initializing from arbitrary Y and then running sampling via Markov chain Y \u223c P\u03b82(Y |f\u03b81(Yt\u22121, Zt\u22121, Ht\u22121, X)).\nSupervised GSN allows flexible network architecture and noise forms just as GSN. It enjoys the same advantage of avoiding marginalization over hidden variables in both training and inference. Both Supervised GSN and GSN can benefit from converting a difficult learning task of cap-\nturing P (X) or P (Y |X) into easier tasks of learning reconstruction distributions."}, {"heading": "3. Algorithms", "text": "3.1. Convolutional Generative Stochastic Network\n\u210e1 \u210e2 \ufffd, \ufffd Conv Pool Conv\nFigure 2. Show the spatial connectivity architecture of a convolutional GSN architecture with 2 convolutional GSN layers. Each convolution layer (conv) feature is connected to all feature maps of the lower layer, while pooling layer (pool) features connect only to features in the corresponding feature map. In the computational graph of convolutional GSN (Figure 1), a convolutional GSN layer may contain a single convolutional layer (h1) or both a pooling and a convolutional layer (h2), and pooling layers are considered as intermediate computations for calculating input of convolutional layers.\nThe original DBM-like GSN architecture proposed in (Bengio et al., 2013a) does not leverage the spatial structure of the data, therefore each feature is connected to data for every location, making it hard to scale to real-sized data. Therefore we introduce convolutional GSN (Figure 1, 2) to learn a hierarchical representation with gradually more global representation in higher levels. Given the sequential nature of protein data, we use 1D convolution here, but the architecture can be easily generalized to higherdimensional convolution. This architecture is applicable to both GSN and supervised GSN.\nThe simplest convolutional GSN includes only an input layer and a convolution layer. Assume the input layer consists of Nv \u00d7 Cv visible units, where Nv is the size of input and Cv is number of input channels, and the convolutional layer includes K feature maps, and each map is associated with a sizeNw\u00d7Cv filter, where Nw is the convolutional window size. The convolution layer thus consists of (Nv \u2212Nw + 1)K hidden units each connects with Nw \u00d7Cv visible units. The filter weights are shared across the map. Each map also has a hidden unit bias b and a visible unit bias b\u2032. We use \u2217 to denote 1D convolution, then\nthe upward and downward pass between visible layer and convolutional layer is computed as:\nyk = \u03c3\u0303(x\u0303 \u2217W k + bk, z) x\u2032 = \u03c3( \u2211 k yk \u2217W \u2032k + b\u2032k)\nNoise is injected through an independent random variable z \u223c P (Z) with the noisy activation function \u03c3\u0303. In this work we used the same noisy activation function as used in (Bengio et al., 2013a):\n\u03c3\u0303(x, z) = apostz + tanh(x+ aprez)\nGradient over stochastic neurons is calculated by only considering differentiable part, or \u201cstraight-through estimator\u201d, as in (Bengio & Thibodeau-Laufer, 2013).\nConvolutional layers can be stacked to create a deep representation. To allow learning high-level representations, it is usually desirable to reduce the size of feature maps in upper layers by pooling. Therefore we applied mean-pooling that pools several adjacent features in a feature map and output their average value. An alternative to mean-pooling is max-pooling, which outputs max rather than average value; because the max operation is not reversible, we can do the downward pass as if the upward pass used mean pooling. Pooling also effectively hard-codes translational invariance in higher level features.\nIn the computational graph of convolutional GSN, we use layer-wise sampling similar to the deep Boltzmann machine-like network architecture proposed in (Bengio et al., 2013a). As pooling layer contains redundant information with the lower layer, it is considered together with its upper-level convolutional layer as a convolutional GSN layer. Intuitively in the layer-wise sampling, pooling layers can be considered as only providing intermediate computation for input of convolutional layers, and convolutional layers activations are stored and used for the next sampling step.\nNote that the recurrent structure of its computational graph makes convolutional GSN significantly different from the feed-forward convolutional networks. In convolutional GSN, the representations from different layers and different spatial positions can communicate with each other through bidirectional connections. With a sufficient number of alternating upward and downward passes, information from any position of the input/feature map can be propagated to any other positions through multiple different paths (the number of passes required depends directly on the architecture; if the top layer feature is connect to all of the input positions, then 2Nlayer rounds is enough).\nWhile the depth of representation can be considered as the number of layers in convolutional GSN, the depth of com-\nputational graph grows as the number of \u2018sampling\u2019 iterations, which is also called number of walkbacks (Bengio et al., 2013a), increases, so the computational graph can be arbitrarily deep with replicated parameters.\nWhen using this convolutional GSN architecture for supervised training like protein secondary structure prediction (Figure 1), we will consider two types of input channels, feature channels (X) and label channels (Y). In the computational graph of supervised convolutional GSN (Figure 1), we only corrupt and reconstruct label channels, and feature channels are not corrupted and provided as part of input to compute the activations of the first hidden layer."}, {"heading": "4. Experimental Results", "text": ""}, {"heading": "4.1. Features and dataset", "text": "Our goal is to correctly predict secondary structure labels for each amino-acid of any given protein sequence. We focused on 8-state secondary structure prediction as this is a more challenging problem than 3-state prediction and reveals more structural information. We used multi-task learning to simultaneously predict both secondary structure and amino-acid solvent accessibility, because learning to predict other structural properties with the same network allows sharing feature representations and may thus improve feature learning.\nEvolutionary information as position-specific scoring matrix (PSSM) is considered the most informative feature for predicting secondary structure from previous research (Jones, 1999). To generate PSSM, which are n\u00d7bmatrices, where n is protein length and b is the number of amino-acid types, we ran PSI-BLAST against UniRef90 database with inclusion threshold 0.001 and 3 iterations. We used pfilt program from PSI-PRED package to pre-filter the database for removing low information content and coiled-coil like regions (Jones, 1999). To use PSSM as the input for the supervised GSN model, we transform the PSSM scores to 0-1 range by the sigmoid function. We also provide the original protein sequence for amino-acid residues encoded by n \u00d7 b binary matrices as input features. Two other input features encode start and end positions of the protein sequence. While other features could be considered to further improve performance, we focus here on evaluating the model architecture.\nWe used a large non-homologous sequence and structure dataset with 6128 proteins (after all filtering), and divided it randomly into training (5600), validation (256), and testing (272) sets. This dataset is produced with PISCES Cull PDB server (Wang & Dunbrack, 2003) which is commonly used for evaluating structure prediction algorithms. We retrieved a subset of solved protein structures with better than 2.5A\u030a resolution while sharing less than 30% identity, which is\nthe same set up as used in (Wang et al., 2011). We also removed protein chains with less than 50 or more than 700 amino acids or discontinuous chains.\n8-states secondary structure labels and solvent accessibility score were inferred from the 3D PDB structure by the DSSP program (Kabsch & Sander, 1983). We discretized solvent accessibility scores to absolute solvent accessibility and relative solvent accessibility following (Qi et al., 2012).\nThe resulting training data including both feature and labels has 57 channels (22 for PSSM, 22 for sequence, 2 for terminals, 8 for secondary structure labels, 2 for solvent accessibility labels), and the overall channel size is 700. The 700 amino-acids length cutoff was chosen to provide a good balance between efficiency and coverage as the majority of protein chains are shorter than 700AA. Proteins shorter than 700AA were padded with all-zero features.\nWe evaluated performance of secondary structure prediction on the test dataset containing 272 proteins. We also performed a seperate evaluation on CB513 dataset while training on Cull PDB dataset further filtered to remove sequences with > 25% identity with the CB513 dataset. Secondary structure prediction performance was measured by Q8 accuracy, or the proportion of amino-acids being correctly labeled. We ran 5 parallel Markov chains each generating 5000 consecutive reconstruction distributions from the model and evaluated performance by averaging the last 500 reconstruction distributions from each Markov chain."}, {"heading": "4.2. Training setup", "text": "To inject noise to labels, the input labels were corrupted by randomly setting half of the values to zero. Gaussian noise with standard deviation 2 was added pre- and post- activation on all except the first convolutional GSN layer. Each yt was obtained by sampling from the reconstructed distribution. We consider reconstructed distribution as multinomial distribution for secondary structure and binomial distribution for solvent accessibility. Walkback number is set to 12 to ensure flow of information across the whole network.\nMotivated to better learn the reconstruction distribution when the chain is initilized by an arbitrary label Y \u2032 far away from where the probability density of true P (Y |X) lies, which is important for supervised prediction tasks, during training for half of the training samples we initalized Y by the same arbitrary initiation as used for prediction rather than its true value. We call this trick \u201dkick-start\u201d.\nTanh activation function is used for all cases except the visible layer, which uses sigmoid activation function for reconstruction. All layers use untied weights, so W and W\u2019 do not equal. We regularize the network by constraining the norm of each weight vector.\nParameters for all layers were trained globally by backpropagating the reconstruction error. Mini-batch stochastic gradient descent with learning rate 0.25 and momentum 0.5 was used for training model parameters. All models for comparisons were trained for 300 epochs. The model is implemented based on Theano and Pylearn2 libraries and trained on Tesla K20m GPU."}, {"heading": "4.3. Performance", "text": "We achieved 72.1 \u00b1 0.6% Q8 accuracy on our Cull PDB test set sequences (Table 2). The best single model we tested features a 3-layer convolutional structure. We denote our structure as {80\u00d7 conv5} \u2212 {pool5\u2212 80\u00d7 conv5} \u2212 {pool5\u221280\u00d7conv4}. \u2018conv\u2019 or \u2018pool\u2019 denotes whether the layer type is convolutional or pooling layer, and the number that follows layer type denotes the size of filter or pooling window. Mean pooling is used for all pooling layers in this architecture. We used 80 channels in all convolutional layers.\nFigure 3 shows an example of consecutive samples generated from the trained model conditional on input features from a test set protein. We observe that as the chain runs the samples get gradually closer to the experimentally measured secondary structure, and some incorrect predictions are fixed in later steps of sampling. The later samples also seems to have more spatially consistent organization.\nPrediction performance for individual secondary structure states are shown in (Table 2). We achieved high accuracy\nTable 3. Classification accuracies for individual secondary structure states.\nIndividual secondary structure state prediction sensitivities, precisions, and frequecies calculated across amino acid residues of all proteins in our Cull PDB test set are shown below. State I cannot be evaluated as it is too rare to even appear in the test set.\nSec. Sensitivity Precision Frequency Description\nH 0.935 0.828 0.354 \u03b1-helix E 0.823 0.748 0.218 \u03b2-strand L 0.633 0.541 0.186 loop or irregular T 0.506 0.548 0.111 \u03b2-turn S 0.159 0.423 0.079 bend G 0.133 0.496 0.041 310-helix B 0.001 0.5 0.011 \u03b2-bridge I - - 0 \u03c0-helix\nfor four major states H, E, L and T. Prediction for less frequent states G and S are known to be difficult, because they have a limited number of training standard leading to significantly unbalanced labels. We did not specifically try to address the unbalanced label problem here. More efforts should be made in future to better identify these states. State I is extremely rare, so we did not even encounter one in the test set, thus it potentially limits improvement on its prediction.\nWe also perfomed validation on a public standard benchmark dataset CB513. For this validation we trained a model aftering filtering the Cull PDB dataset to remove sequences with homology with CB513 sequences (i.e. > 25% identity). Our model achieves Q8 accuracy of 0.664, outperforming previous state-of-the-art result by CNF/RaptorSS8 (0.649) (Wang et al., 2011)."}, {"heading": "4.4. Analysis of architecture", "text": "To discover important factors for the success of our architecture, we experimented with alternative model architectures with varying number of layers, and tried to remove \u201dkick-start\u201d training or Gaussian noise injection (Table 2, Figure 4).\nFor model architectures with different number of layers, we removed the top 1 and 2 convolutional GSN layers from the original best 3-layer architecture. The 2-layer model outperforms the 1-layer model, and the 3-layer model has slightly better performance than the 2-layer model.\nNot using \u201dkick-start\u201d during training dramatically decreases the speed of convergence to optimal performance during sampling and the prediction accuracy. This is likely because \u201dkick-start\u201d model learned better reconstruction distribution when chain is initalized from an arbitrary state.\nNoise injected in computation of the upper layers seems to be necessary for optimal performance for deep models. We also note that lower reconstruction error on validation data during training does not always correspond to better prediction performance. For our 3-layer model with no Gaussian noise injected to intermediate computations, although the validation reconstruction error goes lower than for the model with noise, the samples from the model with noise give much better predictions (0.721 vs. 0.687)."}, {"heading": "5. Conclusions", "text": "To apply deep representations to protein secondary structure prediction, we implemented a supervised generative stochastic network (GSN) and introduced a convolutional architecture to allow it to learn hierarchical representation on full-sized data. While we demonstrated its success on secondary structure prediction, such architecture can be potentially applied to other protein structure prediction tasks. Our experiments suggest supervised generative stochastic network to be an effective algorithm for structured prediction, extending the success of generative stochastic network in capturing complex dependency in the data.\nThe combination of convolutional and supervised generative stochastic network we applied is well suited for lowlevel structured prediction that is sensitive to local infor-\nmation, while being informed of high-level and distant features. Thus, such an architecture may also be applicable to a wide range of structured prediction problems outside bioinformatics such as scene parsing and image segmentation.\nFor further development of the protein sequence and structure modeling, one limitation of the current architecture is that the convolutional structure is hard-coded, thus it may not be optimal to capture the spatial organization of protein sequence in some cases, especially for structures formed by long-range interactions. To better model the long-range interactions in a protein, adaptive and dynamic architecture that changes the connectivity adaptively based on input, analogous to unfolding recursive auto-encoder (Socher et al., 2011), may further improve the quality of representation in future."}, {"heading": "Acknowledgments", "text": "We would like to thank Robert E. Schapire for helpful discussions. We also acknowledge the TIGRESS high performance computer center at Princeton University for computational resource support. This work was supported by National Science Foundation (NSF) CAREER award (DBI-0546275), National Institutes of Health (NIH) (R01 GM071966, R01 HG005998 and T32 HG003284) and\nNational Institute of General Medical Sciences (NIGMS) Center of Excellence (P50 GM071508). O.G.T. is a Senior Fellow of the Canadian Institute for Advanced Research."}], "references": [{"title": "Exploiting the past and the future in protein secondary structure", "author": ["Baldi", "Pierre", "Brunak", "Sren", "Frasconi", "Paolo", "Soda", "Giovanni", "Pollastri", "Gianluca"], "venue": "prediction. Bioinformatics,", "citeRegEx": "Baldi et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 1999}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Better mixing via deep representations", "author": ["Bengio", "Yoshua", "Mesnil", "Gregoire", "Dauphin", "Yann", "Rifai", "Salah"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1305.6663,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Machine learning methods for protein structure prediction", "author": ["Cheng", "Jianlin", "Tegge", "Allison N", "Baldi", "Pierre"], "venue": "Biomedical Engineering, IEEE Reviews in,", "citeRegEx": "Cheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2008}, {"title": "Protein secondary structure prediction based on position-specific scoring matrices", "author": ["Jones", "David T"], "venue": "Journal of molecular biology,", "citeRegEx": "Jones and T.,? \\Q1999\\E", "shortCiteRegEx": "Jones and T.", "year": 1999}, {"title": "Dictionary of protein secondary structure: pattern recognition of hydrogenbonded and geometrical features", "author": ["Kabsch", "Wolfgang", "Sander", "Christian"], "venue": "Biopolymers, 22(12):2577\u20132637,", "citeRegEx": "Kabsch et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kabsch et al\\.", "year": 1983}, {"title": "Conditional neural fields", "author": ["Peng", "Jian", "Bo", "Liefeng", "Xu", "Jinbo"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "A unified multitask architecture for predicting local protein properties", "author": ["Qi", "Yanjun", "Oja", "Merja", "Weston", "Jason", "Noble", "William Stafford"], "venue": "PLoS One,", "citeRegEx": "Qi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2012}, {"title": "Predicting the secondary structure of globular proteins using neural network models", "author": ["Qian", "Ning", "Sejnowski", "Terrence J"], "venue": "Journal of molecular biology,", "citeRegEx": "Qian et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Qian et al\\.", "year": 1988}, {"title": "Prediction of protein secondary structure at better than 70accuracy", "author": ["Rost", "Burkhard", "Sander", "Chris"], "venue": "Journal of molecular biology,", "citeRegEx": "Rost et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Rost et al\\.", "year": 1993}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Bayesian segmentation of protein secondary structure", "author": ["Schmidler", "Scott C", "Liu", "Jun S", "Brutlag", "Douglas L"], "venue": "Journal of computational biology,", "citeRegEx": "Schmidler et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Schmidler et al\\.", "year": 2000}, {"title": "Predicting protein secondary and supersecondary structure", "author": ["Singh", "Mona"], "venue": "Handbook of Computational Molecular Biology. Chapman & Hall CRC Computer and Information Science Series,", "citeRegEx": "Singh and Mona.,? \\Q2005\\E", "shortCiteRegEx": "Singh and Mona.", "year": 2005}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennin", "Jeffrey", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Hidden-unit conditional random fields", "author": ["van der Maaten", "Laurens", "Welling", "Max", "Saul", "Lawrence K"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "Maaten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2011}, {"title": "Pisces: a protein sequence culling", "author": ["G. Wang", "R.L. Dunbrack", "Jr."], "venue": "server. Bioinformatics,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Protein 8class secondary structure prediction using conditional neural fields. Proteomics", "author": ["Wang", "Zhiyong", "Zhao", "Feng", "Peng", "Jian", "Xu", "Jinbo"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "9% (Wang et al., 2011) for this challenging secondary structure prediction problem.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "Understanding complex dependency between protein sequence and structure is one of the greatest challenges in computational biology (Cheng et al., 2008).", "startOffset": 131, "endOffset": 151}, {"referenceID": 0, "context": "Since the early work by (Qian & Sejnowski, 1988), neural networks have been widely applied and are core components of many most successful approaches (Rost & Sander, 1993; Jones, 1999; Baldi et al., 1999).", "startOffset": 150, "endOffset": 204}, {"referenceID": 0, "context": "Other developments include better capturing spatial dependencies using bidirectional recurrent neural networks (BRNN) (Baldi et al., 1999; Pollastri et al., 2002), probabilistic graphical models (Schmidler et al.", "startOffset": 118, "endOffset": 162}, {"referenceID": 12, "context": ", 2002), probabilistic graphical models (Schmidler et al., 2000; Chu et al., 2004; van der Maaten et al., 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al.", "startOffset": 40, "endOffset": 111}, {"referenceID": 7, "context": ", 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al., 2009; Wang et al., 2011).", "startOffset": 184, "endOffset": 222}, {"referenceID": 17, "context": ", 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al., 2009; Wang et al., 2011).", "startOffset": 184, "endOffset": 222}, {"referenceID": 17, "context": "lenging and less-addressed problem (Pollastri et al., 2002; Wang et al., 2011).", "startOffset": 35, "endOffset": 78}, {"referenceID": 17, "context": "5\u00c5 resolution while sharing less than 30% identity, which is the same set up as used in (Wang et al., 2011).", "startOffset": 88, "endOffset": 107}, {"referenceID": 8, "context": "We discretized solvent accessibility scores to absolute solvent accessibility and relative solvent accessibility following (Qi et al., 2012).", "startOffset": 123, "endOffset": 140}, {"referenceID": 17, "context": "005 Previous state-of-the-art (Wang et al., 2011) 0.", "startOffset": 30, "endOffset": 49}, {"referenceID": 17, "context": "649) (Wang et al., 2011).", "startOffset": 5, "endOffset": 24}, {"referenceID": 14, "context": "To better model the long-range interactions in a protein, adaptive and dynamic architecture that changes the connectivity adaptively based on input, analogous to unfolding recursive auto-encoder (Socher et al., 2011), may further improve the quality of representation in future.", "startOffset": 195, "endOffset": 216}], "year": 2014, "abstractText": "Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of aminoacids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.", "creator": "LaTeX with hyperref package"}}}