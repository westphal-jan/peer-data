{"id": "1404.3840", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Surpassing Human-Level Face Verification Performance on LFW with GaussianFace", "abstract": "Facial verification remains a challenging problem under very complex conditions with large variations such as pose, lighting, expression and occlusion. This problem is compounded when we unrealistically rely on a single training data source, often insufficient to cover the inherently complex facial variations. This paper proposes a principle-driven multi-task learning approach based on the discriminatory Gaussian process, Latent Variable Model, called GaussianFace, to expand the diversity of training data. Compared to existing methods, our model uses additional data from multiple source ranges to enhance the generalization performance of facial verification in an unknown target range. Importantly, our model can automatically adapt to complex data distributions and can therefore well capture complex facial variations inherent in multiple sources. Extensive experiments show the effectiveness of the proposed model in learning from different data sources and generalize in invisible ranges, reaching an impressive 98.5% of our human algorithm accuracy in particular.", "histories": [["v1", "Tue, 15 Apr 2014 07:51:23 GMT  (3154kb)", "http://arxiv.org/abs/1404.3840v1", null], ["v2", "Mon, 16 Jun 2014 14:37:38 GMT  (630kb,D)", "http://arxiv.org/abs/1404.3840v2", null], ["v3", "Sat, 20 Dec 2014 03:37:36 GMT  (630kb,D)", "http://arxiv.org/abs/1404.3840v3", "Appearing in Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI-15), Oral Presentation"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["chaochao lu", "xiaoou tang"], "accepted": true, "id": "1404.3840"}, "pdf": {"name": "1404.3840.pdf", "metadata": {"source": "CRF", "title": "Surpassing Human-Level Face Verification Performance on LFW with GaussianFace", "authors": ["Chaochao Lu", "Xiaoou Tang"], "emails": ["xtang}@ie.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 4.\n38 40\nv1 [\ncs .C\nV ]\n1 5\nA pr\n2 01\nFace verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark [23]. For the first time, the human-level performance in face verification (97.53%) [28] on LFW is surpassed."}, {"heading": "1. Introduction", "text": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9]. It has many important applications, including surveillance, access control, image retrieval, and automatic log-on for personal computer or mobile devices. However, various visual complications deteriorate the performance of face verification, as shown by numerous studies on real-world face images from the wild [23]. The Labeled Faces in the Wild (LFW) dataset is well known as a challenging benchmark for face ver-\nification. The dataset provides a large set of relatively unconstrained face images with complex variations in pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters. Not surprisingly, LFW has proven difficult for automatic face verification methods [23, 28]. Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.02% [56] to 97.25% [53] since LFW is established in 2007, these studies have not closed the gap to human-level performance [28] in face verification.\nWhy could not we surpass the human-level performance? Two possible reasons are found as follows:\n1) Most existing face verification methods assume that the training data and the test data are drawn from the same feature space and follow the same distribution. When the distribution changes, these methods may suffer a large performance drop [58]. However, many practical scenarios involve cross-domain data drawn from different facial appearance distributions. Learning a model solely on a single source data often leads to overfitting due to dataset bias [55]. Moreover, it is difficult to collect sufficient and necessary training data to rebuild the model in new scenarios, for highly accurate face verification specific to the target domain. In such cases, it becomes critical to exploit more data from multiple source-domains to improve the generalization of face verification methods in the targetdomain.\n2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33]. Although these existing methods have made great progress in face verification, most of them are less flexible when dealing with complex data distributions. For the methods in the first category, for example, low-level features such as SIFT [36], LBP [3], and Gabor [34] are handcrafted. Even for features learned from data [10, 24], the algorithm parameters (such as the depth of random projection tree, or the number of centers in k-means) also need to be specified by users. Similarly,\nfor the methods in the second category, the architectures of deep networks in [62, 50, 63, 51] (for example, the number of layers, the number of nodes in each layer, etc.), and the parameters of the models in [31, 5, 28, 47] (for example, the number of Gaussians, the number of classifiers, etc.) must also be determined in advance. Since most existing methods require some assumptions to be made about the structures of the data, they cannot work well when the assumptions are not valid. Moreover, due to the existence of the assumptions, it is hard to capture the intrinsic structures of data using these methods.\nTo this end, we propose the Multi-Task Learning approach based on Discriminative Gaussian Process Latent Variable Model (DGPLVM) [57], named GaussianFace, for face verification. Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM. Here, we investigate the asymmetric multi-task learning because we only focus on the performance improvement of the target task. From the perspective of information theory, this constraint aims to maximize the mutual information between the distributions of target-domain data and multiple source-domains data. Moreover, the GaussianFace model is a reformulation based on the Gaussian Processes (GPs) [42], which is a non-parametric Bayesian kernel method. Therefore, our model also can adapt its complexity flexibly to the complex data distributions in the real-world, without any heuristics or manual tuning of parameters.\nReformulating GPs for large-scale multi-task learning is non-trivial. To simplify calculations, we introduce a more efficient equivalent form of Kernel Fisher Discriminant Analysis (KFDA) to DGPLVM. Despite that the GaussianFace model can be optimized effectively using the Scaled Conjugate Gradient (SCG) technique, the inference is slow for large-scale data. We make use of GP approximations [42] and anchor graphs [35] to speed up the process of inference and prediction, so as to scale our model to largescale data. Our model can be applied to face verification in two different ways: as a binary classifier and as a feature extractor. In the former mode, given a pair of face images, we can directly compute the posterior likelihood for each class to make a prediction. In the latter mode, our model can automatically extract high-dimensional features for each pair of face images, and then feed them to a classifier to make the final decision.\nThe main contributions of this paper are as follows:\n\u2022 We propose a novel GaussianFace model for face verification by virtue of the multi-task learning constraint to DGPLVM. Our model can adapt to complex distributions, avoid over-fitting, exploit discriminative information, and take advantage of multiple source-\ndomains data.\n\u2022 We introduce a computationally more efficient equivalent form of KFDA to DGPLVM. This equivalent form reformulates KFDA to the kernel version consistent with the covariance function in GPs, which greatly simplifies calculations.\n\u2022 We introduce approximation in GPs and anchor graphs to speed up the process of inference and prediction.\n\u2022 We achieve superior performance on the challenging LFW benchmark [23], with an accuracy rate of 98.52%, beyond human-level performance reported in [28]."}, {"heading": "2. Related Work", "text": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8]. These studies have shown that computer-based algorithms were more accurate than humans in well-controlled environments (e.g., frontal view, natural expression, and controlled illumination), whilst still comparable to humans in the poor condition (e.g., frontal view, natural expression, and uncontrolled illumination). However, the above conclusion is only verified on face datasets with controlled variations, where only one factor changes at a time [40, 38]. To date, there has been virtually no work showing that computerbased algorithms could surpass human performance on unconstrained face datasets, such as LFW, which exhibits natural (multifactor) variations in pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters.\nThere has been much work dealing with multifactor variations in face verification. For example, Simonyan et al. applied the Fisher vector to face verification and achieved a good performance [47]. However, the Fisher vector is derived from the Gaussian mixture model (GMM), where the number of Gaussians need to be specified by users, which means it cannot cover complex data automatically. Li et al. proposed a non-parametric subspace analysis [33, 32], but it is only a linear transformation and cannot cover the complex distributions. Besides, there also exist some approaches for utilizing plentiful sourcedomain data. Based on the Joint Bayesian algorithm [13], Cao et al. proposed a transfer learning approach [9] by merging source-domain data with limited target-domain data. Since this transfer learning approach is based on the joint Bayesian model of original visual features, it is not suitable for handling the complex nonlinear data and the data with complex manifold structures. Moreover, the transfer learning approach in [9] only considered two different domains, restricting its wider applications in largescale data from multiple domains. More recently, Zhu\net al. [63] learned the transformation from face images under various poses and lighting conditions to a canonical view with a deep convolutional network. Sun et al. [51] learned face representation with a deep model through face identification, which is a challenging multi-class prediction task. Taigman et al. [52] first utilized explicit 3D face modeling to apply a piecewise affine transformation, and then derived a face representation from a nine-layer deep neural network. Although these methods have achieved high performances on LFW, many parameters of them must be determined in advance so that they are less flexible when dealing with complex data distributions.\nThe core of our algorithm is GPs. To the best of our knowledge, GPs methods and Multi-task learning with related GPs methods (MTGP) have not been applied for face verification. Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26]. However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks. Leen et al. proposed a MTGP model in the asymmetric setting [30] to focus on improving performance on the target task, and Kim et al. developed a GP model for clustering [26], but their methods do not take the discriminative information of the covariance function into special account like DGPLVM. Although the discriminative information is considered in [57], it does not apply multi-task learning to improve its performance. Salakhutdinov et al. used a deep belief net to learn a good covariance kernel for GPs [45]. The limitation of such deep methods is that it is hard to determine which architecture for this network is optimal. Also, multi-task learning constraint was not considered in [45]."}, {"heading": "3. Preliminary", "text": "In this section, we briefly review Gaussian Processes (GPs) for classification and clustering [26], and Gaussian Process Latent Variable Model (GPLVM) [29]. We use GPs method mainly due to the following three notable advantages. Firstly, as mentioned previously, it is a nonparametric method, which means it adapts its complexity flexibly to the complex data distributions in the real-world, without any heuristics or manual tuning of parameters. Secondly, GPs method can be computed effectively because of its closed-form marginal probability computation. Furthermore, its hyper-parameters can be learned from data automatically without using model selection methods such as cross validation, thereby avoiding the high computational cost. Thirdly, the inference of GPs is based on Bayesian rules, resulting in robustness to overfitting. We recommend Rasmussen and Williams\u2019s excellent monograph for further\nreading [42]."}, {"heading": "3.1. Gaussian Processes for Binary Classification", "text": "Formally, for two-class classification, suppose that we have a training set D of N observations,D = {(xi, yi)}Ni=1, where the i-th input point xi \u2208 RD and its corresponding output yi is binary, with y = 1i for one class and yi = \u22121 for the other. Let X be the N \u00d7 D matrix, where the row vectors represent all n input points, and y be the column vector of all n outputs. We define a latent variable fi for each input point xi, and let f = [f1, . . . , fN ]\u22a4. A sigmoid function \u03c0(\u00b7) is imposed to squash the output of the latent function into [0, 1], \u03c0(fi) = p(yi = 1|fi). Assuming the data set is i.i.d, then the joint likelihood factorizes to\np(y|f) = N \u220f\ni=1\np(yi|fi) = N \u220f\ni=1\n\u03c0(yifi). (1)\nMoreover, the posterior distribution over latent functions is\np(f |X,y, \u03b8) = p(y|f)p(f |X) p(X,y|\u03b8) . (2)\nSince neither p(f |X,y, \u03b8) nor p(y|f) can be computed analytically, the Laplace method is utilized to approximate the posterior\np(f |X,y, \u03b8) = N (f\u0302 , (K\u22121 +W)\u22121), (3)\nwhere f\u0302 = argmaxf p(f |X,y, \u03b8) and W = \u2212\u25bd\u25bd log p(f |X,y, \u03b8)|\nf=f\u0302 . Then, we can obtain\nlog p(y|X, \u03b8) = \u22121 2 f\u0302\u22a4K\u22121f\u0302 + log p(y|f\u0302 )\u2212 1 2 log |B|.\n(4)\nwhere |B| = |K| \u00b7 |K\u22121 +W| = |In +W 1 2KW 1 2 |. The optimal value of \u03b8 can be acquired by using the gradient method to maximize Equation (4). Given any unseen test point x\u2217, the probability of its latent function f\u2217 is\nf\u2217|X,y, x\u2217 \u223c N (K\u2217K\u22121f\u0302 ,K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 ), (5)\nwhere K\u0303 = K + W\u22121. Finally, we squash f\u2217 to find the probability of class membership as follows\n\u03c0\u0304(f\u2217) =\n\u222b\n\u03c0(f\u2217)p(f\u2217|X,y, x\u2217)df\u2217. (6)"}, {"heading": "3.2. Gaussian Processes for Clustering", "text": "The principle of GP clustering is based on the key observation that the variances of predictive values are smaller in dense areas and larger in sparse areas. The variances can be employed as a good estimate of the support of a probability density function, where each separate support domain can be considered as a cluster. This observation can\nbe explained from the variance function of any predictive data point x\u2217\n\u03c32(x\u2217) = K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 . (7)\nIf x\u2217 is in a sparse region, then K\u2217K\u0303\u22121K\u22a4\u2217 becomes small, which leads to large variance \u03c32(x\u2217), and vice versa. Another good property of Equation (7) is that it does not depend on the labels, which means it can be applied to the unlabeled data.\nTo perform clustering, the following dynamic system associated with Equation (7) can be written as\nF (x) = \u2212\u25bd\u03c32(x). (8) The theorem in [26] guarantees that almost all the trajectories approach one of the stable equilibrium points detected from Equation (8). After each data point finds its corresponding stable equilibrium point, we can employ a complete graph [4, 26] to assign cluster labels to data points with the stable equilibrium points. Obviously, the variance function in Equation (7) completely determines the performance of clustering."}, {"heading": "3.3. Gaussian Process Latent Variable Model", "text": "Let Z = [z1, . . . , zN ]\u22a4 denote the matrix whose rows represent corresponding positions of X in latent space, where zi \u2208 Rd (d \u226a D). The Gaussian Process Latent Variable Model (GPLVM) can be interpreted as a Gaussian process mapping from a low dimensional latent space to a high dimensional data set, where the locale of the points in latent space is determined by maximizing the Gaussian process likelihood with respect to Z. Given a covariance function for the Gaussian process, denoted by k(\u00b7, \u00b7), the likelihood of the data given the latent positions is as follows,\np(X|Z, \u03b8) = 1\u221a (2\u03c0)ND|K|D exp ( \u2212 1 2 tr(K\u22121XX\u22a4) ) ,\n(9)\nwhere Ki,j = k(zi, zj). Therefore, the posterior can be written as\np(Z, \u03b8|X) = 1Za p(X|Z, \u03b8)p(Z)p(\u03b8), (10)\nwhere Za is a normalization constant, the uninformative priors over \u03b8, and the simple spherical Gaussian priors over Z are introduced [57]. To obtain the optimal \u03b8 and Z, we need to optimize the above likelihood (10) with respect to \u03b8 and Z, respectively."}, {"heading": "4. GaussianFace", "text": "In order to automatically learn discriminative features or covariance function, and to take advantage of sourcedomain data to improve the performance in face verification, we develop a principled GaussianFace model by\nincluding the multi-task learning constraint into Discriminative Gaussian Process Latent Variable Model (DGPLVM) [57]."}, {"heading": "4.1. DGPLVM Reformulation", "text": "The DGPLVM is an extension of GPLVM, where the discriminative prior is placed over the latent positions, rather than a simple spherical Gaussian prior. The DGPLVM uses the discriminative prior to encourage latent positions of the same class to be close and those of different classes to be far. Since face verification is a binary classification problem and the GPs mainly depend on the kernel function, it is natural to use Kernel Fisher Discriminant Analysis (KFDA) [27] to model class structures in kernel spaces. For simplicity of inference in the followings, we introduce another equivalent formulation of KFDA to replace the one in [57].\nKFDA is a kernelized version of linear discriminant analysis method. It finds the direction defined by a kernel in a feature space, onto which the projections of positive and negative classes are well separated by maximizing the ratio of the between-class variance to the within-class variance. Formally, let {z1, . . . , zN+} denote the positive class and {zN++1, . . . , zN} the negative class, where the numbers of positive and negative classes are N+ and N\u2212 = N \u2212 N+, respectively. Let K be the kernel matrix. Therefore, in the feature space, the two sets {\u03c6K(z1), . . . , \u03c6K(zN+)} and {\u03c6K(zN++1), . . . , \u03c6K(zN )} represent the positive class and the negative class, respectively. The optimization criterion of KFDA is to maximize the ratio of the betweenclass variance to the within-class variance\nJ(\u03c9,K) = (w\u22a4(\u00b5+ K \u2212 \u00b5\u2212 K ))2\nw\u22a4(\u03a3+ K +\u03a3\u2212 K + \u03bbIN )w\n, (11)\nwhere \u03bb is a positive regularization parameter, \u00b5+ K\n= 1\nN+\n\u2211N+ i=1 \u03c6K(zi), \u00b5 \u2212 K = 1\nN\u2212 \u2211N i=N++1 \u03c6K(zi), \u03a3+K = 1\nN+\n\u2211N+ i=1(\u03c6K(zi) \u2212 \u00b5+K)(\u03c6K(zi) \u2212 \u00b5+K)\u22a4, and \u03a3\u2212K =\n1 N\u2212\n\u2211N i=N++1 (\u03c6K(zi)\u2212 \u00b5\u2212K)(\u03c6K(zi)\u2212 \u00b5\u2212K)\u22a4.\nIn this paper, however, we focus on the covariance function rather than the latent positions. To simplify calculations, we represent Equation (11) with the kernel function, and let the kernel function have the same form as the covariance function. Therefore, it is natural to introduce a more efficient equivalent form of KFDA with certain assumptions as Kim et al. points out [27], i.e., maximizing Equation (11) is equivalent to maximizing the following equation\nJ\u2217 = 1\n\u03bb\n( a\u22a4Ka\u2212 a\u22a4KA(\u03bbIn +AKA)\u22121AKa ) , (12)\nwhere\na =[ 1\nn+ 1\u22a4N+ ,\u2212\n1\nN\u2212 1\u22a4N\u2212 ]\nA =diag ( 1 \u221a\nN+\n( IN+ \u2212 1\nN+ 1N+1\n\u22a4 N+\n)\n,\n1 \u221a\nN\u2212\n( IN\u2212 \u2212 1\nN\u2212 1N\u22121\n\u22a4 N\u2212\n)\n)\n.\nHere, IN denotes the N\u00d7N identity matrix and 1N denotes the length-N vector of all ones in RN .\nTherefore, the discriminative prior over the latent positions in DGPLVM can be written as\np(Z) = 1\nZb exp\n( \u2212 1 \u03c32 J\u2217 ) , (13)\nwhere Zb is a normalization constant, and \u03c32 represents a global scaling of the prior.\nThe covariance matrix obtained by DGPLVM is discriminative and more flexible than the one used in conventional GPs for classification (GPC), since they are learned based on a discriminative criterion, and more degrees of freedom are estimated than conventional kernel hyper-parameters."}, {"heading": "4.2. Multi-task Learning Constraint", "text": "From an asymmetric multi-task learning perspective, the tasks should be allowed to share common hyper-parameters of the covariance function. Moreover, from an information theory perspective, the information cost between target task and multiple source tasks should be minimized. A natural way to quantify the information cost is to use the mutual entropy, because it is the measure of the mutual dependence of two distributions. For multi-task learning, we extend the mutual entropy to multiple distributions as follows\nM = H(pt)\u2212 1\nS\nS \u2211\ni=1\nH(pt|pi), (14)\nwhereH(\u00b7) is the marginal entropy,H(\u00b7|\u00b7) is the conditional entropy,S is the number of source tasks, {pi}Si=1, and pt are the probability distributions of source tasks and target task, respectively."}, {"heading": "4.3. GaussianFace Model", "text": "In this section, we describe our GaussianFace model in detail. Suppose we have S source-domain datasets {X1, . . . ,XS} and a target-domain data XT . For each source-domain data or target-domain data Xi, according to Equation (9), we write its marginal likelihood\np(Xi|Zi, \u03b8) = 1 \u221a\n(2\u03c0)ND|K|D exp\n( \u2212 1 2 tr(K\u22121XiX\u22a4i ) ) .\n(15)\nwhere Zi represents the domain-relevant latent space. For each source-domain data and target-domain data, their covariance functions K have the same form because they share the same hyper-parameters \u03b8. In this paper, we use a widely used kernel\nKi,j = k\u03b8(xi,xj) =\u03b80 exp ( \u2212 1 2\nd \u2211\nm=1\n\u03b8m(x m i \u2212 xmj )2\n)\n+ \u03b8d+1 + \u03b4xi,xj\n\u03b8d+2 , (16)\nwhere \u03b8 = {\u03b8i}d+2i=0 and d is the dimension of the data point. Then, from Equations (10), learning the DGPLVM is equivalent to optimizing\np(Zi, \u03b8|Xi) = 1\nZa p(Xi|Zi, \u03b8)p(Zi)p(\u03b8), (17)\nwhere p(Xi|Zi, \u03b8) and p(Zi) are respectively represented in (15) and (13). According to the multi-task learning constraint in Equation (14), we can attain\nM =H(p(ZT , \u03b8|XT ))\n\u2212 1 S\nS \u2211\ni=1\nH(p(ZT , \u03b8|XT )|p(Zi, \u03b8|Xi)). (18)\nFrom Equations (15), (17), and (18), we know that learning the GaussianFace model amounts to minimizing the following marginal likelihood\nLModel = \u2212 log p(ZT , \u03b8|XT )\u2212 \u03b2M, (19)\nwhere the parameter \u03b2 balances the relative importance between the target-domain data and the multi-task learning constraint."}, {"heading": "4.4. Optimization", "text": "For the model optimization, we first expand Equation (19) to obtain the following equation (ignoring the constant items)\nLModel =\u2212 logPT + \u03b2PT logPT\n+ \u03b2\nS\nS \u2211\ni=1\n( PT,i logPT \u2212 PT,i logPT,i ) , (20)\nwhere Pi = p(Zi, \u03b8|Xi) and Pi,j means that its corresponding covariance function is computed on both Xi and Xj . We can now optimize Equation (20) with respect to the hyper-parameters\u03b8 and the latent positions Zi by the Scaled Conjugate Gradient (SCG) technique. Since we focus on the covariance matrix in this paper, here we only present\nthe derivations of hyper-parameters. It is easy to get\n\u2202LModel \u2202\u03b8j = ( \u03b2(logPT + 1) + \u03b2 SPT S \u2211\ni=1\nPT,i \u2212 1\nPT\n)\u2202PT\n\u2202\u03b8j\n+ \u03b2\nS\nS \u2211\ni=1\n(logPT \u2212 logPT,i \u2212 1) \u2202PT,i\n\u2202\u03b8j .\nThe above equation depends on the form \u2202Pi \u2202\u03b8j as follows (ignoring the constant items)\n\u2202Pi \u2202\u03b8j =Pi \u2202 logPi \u2202\u03b8j\n\u2248Pi (\u2202 log p(Xi|Zi, \u03b8)\n\u2202\u03b8j + \u2202 log p(Zi) \u2202\u03b8j + \u2202 log p(\u03b8) \u2202\u03b8j ) .\nThe above three terms can be easily obtained (ignoring the constant items) by\n\u2202 log p(Xi|Zi, \u03b8) \u2202\u03b8j \u2248D 2 tr ( K\u22121 \u2202K \u2202\u03b8j )\n\u2212 1 2 K\u2212\u22a4XiX \u22a4 i K \u2212\u22a4 \u2202K \u2202\u03b8j ,\n\u2202 log p(Zi) \u2202\u03b8j \u2248\u2212 1 \u03c32 \u2202J\u2217i \u2202\u03b8j\n=\u2212 1 \u03bb\u03c32 ( a\u22a4 \u2202K \u2202\u03b8j a\u2212 a\u22a4 \u2202K \u2202\u03b8j A\u0303a\n+ a\u22a4KA\u0303 \u2202K \u2202\u03b8j A\u0303Ka\u2212 a\u22a4KA\u0303\u2202K \u2202\u03b8j a ) ,\n\u2202 log p(\u03b8)\n\u2202\u03b8j =\n1 \u03b8j ,\nwhere A\u0303 = A(\u03bbIn + AKA)\u22121A. Thus, the desired derivatives have been obtained."}, {"heading": "4.5. Speedup", "text": "In the GaussianFace model, we need to invert the large matrix when doing inference and prediction. For large problems, both storing the matrix and solving the associated linear systems are computationally prohibitive. In this paper, we use the anchor graphs method [35] to speed up this process. To put it simply, we first select q (q \u226a n) anchors to cover a cloud of n data points, and form an n \u00d7 q matrix Q, where Qi,j = k\u03b8(xi,xj). xi and xj are from n training data points and q anchors, respectively. Then the original kernel matrix K can be approximated as K = QQ\u22a4. Using the Woodbury identity [21], computing the n\u00d7 n matrix QQ\u22a4 can be transformed into computing the q \u00d7 q matrix Q\u22a4Q, which is more efficient.\nSpeedup on Inference When optimizing Equation (19), we need to invert the matrix (\u03bbIn + AKA). During\ninference, we take q k-means clustering centers as anchors to form Q. Substituting K = QQ\u22a4 into (\u03bbIn + AKA), and then using the Woodbury identity, we get\n(\u03bbIn +AKA) \u22121 = (\u03bbIn +AQQ \u22a4A)\u22121\n= \u03bb\u22121In \u2212 \u03bb\u22121AQ(\u03bbIq +Q\u22a4AAQ)\u22121Q\u22a4A. Speedup on Prediction When we compute the predictive variance \u03c3(x\u2217), we need to invert the matrix (K + W\u22121). At this time, we can use the method in Section 3.2 to calculate the accurate clustering centers that can be regarded as the anchors. Using the Woodbury identity again, we obtain\n(K+W\u22121)\u22121 = W \u2212WQ(Iq +Q\u22a4WQ)\u22121Q\u22a4W, where (Iq +Q\u22a4WQ) is only a q\u00d7 q matrix, and its inverse matrix can be computed more efficiently."}, {"heading": "5. GaussianFace Model for Face Verification", "text": "In this section, we describe two applications of the GaussianFace model to face verification: as a binary classifier and as a feature extractor.\nEach face image is first normalized to 150 \u00d7 120 size by an affine transformation based on five landmarks (two eyes, nose, and two mouth corners). The image is then divided into overlapped patches of 25 \u00d7 25 pixels with a stride of 2 pixels. Each patch within the image is mapped to a vector by a certain descriptor, and the vector is regarded as the feature of the patch, denoted by {xAp }Pp=1 where P is the number of patches within the face image A. In this paper, the multi-scale LBP feature of each patch is extracted [14]. The difference is that the multi-scale LBP descriptors are extracted at the center of each patch instead of accurate landmarks."}, {"heading": "5.1. GaussianFace Model as a Binary Classifier", "text": "For classification, our model can be regarded as an approach to learn a covariance function for GPC, as shown in Figure 1 (a). Here, for a pair of face images A and B from the same (or different) person, let the similarity vector xi = [s1, . . . , sp, . . . , sP ]\n\u22a4 be the input data point of the GaussianFace model, where sp is the similarity of xAp and xBp , and its corresponding output is yi = 1 (or \u22121). With the learned hyper-parameters of covariance function from the training data, given any un-seen pair of face images, we first compute its similarity vector x\u2217 using the above method, then predict whether the pair is from the same person through Equation (6). In this paper, we prescribe the sigmoid function \u03c0(\u00b7) to be the cumulative Gaussian distribution \u03a6(\u00b7), which can be solved analytically as \u03c0\u0304\u2217 = \u03a6 (\nf\u0304\u2217(x\u2217)\u221a 1+\u03c32(x\u2217)\n)\n, where \u03c32(x\u2217) = K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 and f\u0304\u2217(x\u2217) = K\u2217K\n\u22121f\u0302 from Equation (5) [42]. We call the method GaussianFace-BC."}, {"heading": "5.2. GaussianFace Model as a Feature Extractor", "text": "As a feature extractor, our model can be regarded as an approach to automatically extract facial features, shown in Figure 1 (b). Here, for a pair of face images A and B from the same (or different) person, we regard the joint feature vector xi = [(xAi ) \u22a4, (xBi ) \u22a4]\u22a4 as the input data point of the GaussianFace model, and its corresponding output is yi = 1 (or \u22121). To enhance the robustness of our approach, the flipped form of xi is also included; for example, xi = [(x B i ) \u22a4, (xAi ) \u22a4]\u22a4. After the hyper-parameters of covariance function are learnt from the training data, we can use the method in Section 3.2 to group the input data points into different clusters automatically. Suppose that we finally obtain C clusters. The centers of these clusters are denoted by {ci}Ci=1, the variances of these clusters by {\u03a32i }Ci=1, and their weights by {wi}Ci=1 where wi is the ratio of the number of data points from the i-th cluster to the number of all data points. Then we refer to each ci as the input of Equation (5), and we can obtain its corresponding probability pi and variance \u03c32i . In fact, {ci}Ci=1 can be regarded as a codebook generated by our model.\nFor any un-seen pair of face images, we also first compute its joint feature vectorx\u2217 for each pair of patches. Then we compute its first-order and second-order statistics to the centers. The statistics and variance of x\u2217 are represented as its high-dimensional facial features, denoted by x\u0302\u2217 = [\u220611,\u2206 2 1,\u2206 3 1,\u2206 4 1, . . . ,\u2206 1 C ,\u2206 2 C ,\u2206 3 C ,\u2206 4 C ] \u22a4, where \u22061i =\nwi\n(\nx\u2217\u2212ci\n\u03a3i\n) , \u22062i = wi ( x\u2217\u2212ci\n\u03a3i\n)2\n, \u22063i = pi, and \u2206 4 i =\n\u03c32i . We then concatenate all of the new high-dimensional features from each pair of patches to form the final new high-dimensional feature for the pair of face images. The new high-dimensional facial features not only describe how the distribution of features of an un-seen face image differs from the distribution fitted to the features of all training images, but also encode the predictive information including the probabilities of label and uncertainty. We call this approach GaussianFace-FE."}, {"heading": "6. Experimental Settings", "text": "In this section, we conduct experiments on face verification. We start by introducing the source-domain datasets and the target-domain dataset in all of our experiments (see Figure 2 for examples). The source-domain datasets include four different types of datasets as follows: Multi-PIE [19]. This dataset contains face images from 337 subjects under 15 view points and 19 illumination conditions in four recording sessions. These images are collected under controlled conditions. MORPH [43]. The MORPH database contains 55,000 images of more than 13,000 people within the age ranges of 16 to 77. There are an average of 4 images per individual.\nWeb Images2. This dataset contains around 40,000 facial images from 3261 subjects; that is, approximately 10 images for each person. The images were collected from the Web with significant variations in pose, expression, and illumination conditions. Life Photos2. This dataset contains approximately 5000 images of 400 subjects collected online. Each subject has roughly 10 images.\nIf not otherwise specified, the target-domain dataset is the benchmark of face verification as follows: LFW [23]. This dataset contains 13,233 uncontrolled face images of 5749 public figures with variety of pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters. All of these images are collected from the Web.\nWe use the LFW dataset as the target-domain dataset because it is well known as a challenging benchmark. Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16]. Besides, this dataset provides a large set of relatively unconstrained face images with complex variations as described above, and has proven difficult for automatic face verification methods [23, 28]. In all the experiments conducted on LFW, we strictly follow the standard unrestricted protocol of LFW [23]. More precisely, during the training procedure, the four source-domain datasets are: Web Images, MultiPIE, MORPH, and Life Photos, the target-domain dataset is\n2These two datasets are collected by our own from the Web. It is guaranteed that these two datasets are mutually exclusive with the LFW dataset.\nthe training set in View 1 of LFW, and the validation set is the test set in View 1 of LFW. At the test time, we follow the standard 10-fold cross-validation protocol to test our model in View 2 of LFW.\nFor each one of the four source-domain datasets, we randomly sample 20,000 pairs of matched images and 20,000 pairs of mismatched images. The training partition and the testing partition in all of our experiments are mutually exclusive. In other words, there is no identity overlap among the two partitions.\nFor the experiments below, \u201cThe Number of SD\u201d means \u201cthe Number of Source-Domain datasets that are fed into the GaussianFace model for training\u201d. By parity of reasoning, if \u201cThe Number of SD\u201d is i, that means the first i source-domain datasets are used for model training. Therefore, if \u201cThe Number of SD\u201d is 0, models are trained with the training data from target-domain data only.\nImplementation details. Our model involves four important parameters: \u03bb in (12), \u03c3 in (13), \u03b2 in (19), and the number of anchors q in Speedup on Inference 3. Following the same setting in [27], the regularization parameter \u03bb in (12) is fixed to 10\u22128. \u03c3 reflects the tradeoff between our method\u2019s ability to discriminate (small \u03c3) and its ability to generalize (large \u03c3), and \u03b2 balances the relative importance between the target-domain data and the multi-task learning constraint. Therefore, the validation set (the test set in View 1 of LFW) is used for selecting \u03c3 and \u03b2. Each time we use different number of source-domain datasets for training, the corresponding optimal \u03c3 and \u03b2 should be selected on the validation set.\nSince we collected a large number of image pairs for training (20,000 matched pairs and 20,000 mismatched pairs from each source-domain dataset), and our model is based on the kernel method, thus an important consideration is how to efficiently approximate the kernel matrix using a low-rank method in the limited space and time. We adopt the anchor graphs method (see Section 4.5) for kernel approximation. In our experiments, we take two steps to determine the number of anchor points. In the first step, the optimal \u03c3 and \u03b2 are selected on the validation set in each experiment. In the second step, we fix \u03c3 and \u03b2, and then tune the number of anchor points. We vary the number of anchor points to train our model on the training set, and test it on the validation set. We report the average accuracy for our model over 10 trials. After we consider the trade-off between memory and running time in practice, the number of anchor points with the best average accuracy is determined in each experiments.\n3The other parameters, such as the hyper-parameters in the kernel function and the number of anchors in Speedup on Prediction, can be automatically learned from the data."}, {"heading": "7. Experimental Results", "text": "In this section, we conduct five experiments to demonstrate the validity of the GaussianFace model."}, {"heading": "7.1. Comparisons with Other MTGP/GP Methods", "text": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [42], MTGP prediction [6], GPLVM [29], and DGPLVM [57]. For fair comparisons, all these models are trained on multiple source-domain datasets using the same two methods as our GaussianFace model described in Section 5. After the hyper-parameters of covariance function are learnt for each model, we can regard each model as a binary classifier and a feature extractor like ours, respectively. Figure 3 shows that our model significantly outperforms the other four GPs models, and the superiority of our model becomes more obvious as the number of source-domain datasets increases."}, {"heading": "7.2. Comparisons with Other Binary Classifiers", "text": "Since our model can be regarded as a binary classifier, we have also compared our method with other classical binary classifiers. For this paper, we chose three popular representatives: SVM [12], logistic regression (LR) [17], and Adaboost [18]. Table 1 demonstrates that the performance of our method GaussianFace-BC is much better than those of the other classifiers. Furthermore, these experimental results demonstrates the effectiveness of the multi-task learning constraint. For example, our GaussianFace-BC has about 7.5% improvement when all four source-domain datasets are used for training, while the best one of the other three binary classifiers has only around 4% improvement."}, {"heading": "7.3. Comparisons with Other Feature Extractors", "text": "Our model can also be regarded as a feature extractor, which is implemented by clustering to generate a codebook. Therefore, we evaluate our method by comparing it with three popular clustering methods: K-means [24], Random Projection (RP) tree [15], and Gaussian Mixture Model (GMM) [47]. Since our method can determine the\nnumber of clusters automatically, for fair comparison, all the other methods generate the same number of clusters as ours. As shown in Table 2, our method GaussianFace-FE significantly outperforms all of the compared approaches, which verifies the effectiveness of our method as a feature extractor. The results have also proved that the multi-task\nlearning constraint is effective. Each time one different type of source-domain dataset is added for training, the performance can be improved significantly. Our GaussianFace-FE model achieves over 8% improvement when the number of SD varies from 0 to 4, which is much higher than the \u223c3% improvement of the other methods."}, {"heading": "7.4. Comparison with the state-of-art Methods", "text": "Motivated by the appealing performance of both GaussianFace-BC and GaussianFace-FE, we further combine them for face verification. Specifically, after facial features are extracted using GaussianFace-FE, GaussianFaceBC 4 is used to make the final decision. Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16]. The best published result on the LFW benchmark is 96.33% 5, which is achieved by [9]. Our GaussianFace model can improve the accuracy to 98.52%, which for the first time beats the human-level performance (97.53%, cropped) [28]. Figure 5 presents some example pairs that were always incorrectly classified by our model. Obviously, even for humans, it is also difficult to verify some of them. Here, we emphasize that the centers of patches, instead of the accurate and dense facial landmarks like [9], are utilized to extract multi-scale features in our method. This makes our method simpler and easier to use."}, {"heading": "7.5. Further Validations: Shuffling the SourceTarget", "text": "To further prove the validity of our model, we also consider to treat Multi-PIE and MORPH respectively as the target-domain dataset and the others as the sourcedomain datasets. The target-domain dataset is split into two mutually exclusive parts: one consisting of 20,000 matched pairs and 20,000 mismatched pairs is used for training, the\n4Here, the GaussianFace BC is trained with the extracted highdimensional features using GaussianFace-FE.\n5In fact, [51] and [53] have achieved higher accuracies 97.15% and 97.25%, respectively. We do not report their performances in Figure 4, since they have not reported their ROC curves on the LFW website so that we cannot obtain the results to draw their ROC curves.\nother is used for test. In the test set, similar to the protocol of LFW, we select 10 mutually exclusive subsets, where each subset consists of 300 matched pairs and 300 mismatched pairs. The experimental results are presented in Figure 6. Each time one dataset is added to the training set, the performance can be improved, even though the types of data are very different in the training set."}, {"heading": "8. General Discussion", "text": "There is an implicit belief among many psychologists and computer scientists that human face verification abilities are currently beyond existing computer-based face verification algorithms [39]. This belief, however, is supported more by anecdotal impression than by scientific evidence. By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8]. It has been shown that the best current face verification algorithms perform better than humans in the good and moderate conditions. So, it is really not that difficult to beat human performance in some specific scenarios.\nAs pointed out by [38, 48], humans and computer-based algorithms have different strategies in face verification. Indeed, by contrast to performance with unfamiliar faces, human face verification abilities for familiar faces are relatively robust to changes in viewing parameters such as illumination and pose. For example, Bruce [7] found human recognition memory for unfamiliar faces dropped substantially when there were changes in viewing parameters. Besides, humans can take advantages of non-face configurable information from the combination of the face and body (e.g., neck, shoulders). It has also been examined in [28], where the human performance drops from 99.20% (tested using the original LFW images) to 97.53% (tested using the cropped LFW images). Hence, the experiments comparing human and computer performance may not show human face verification skill at their best, because humans were asked to match the cropped faces of people previously\nunfamiliar to them. To the contrary, those experiments can fully show the performance of computer-based face verification algorithms. First, the algorithms can exploit information from enough training images with variations in all viewing parameters to improve face verification performance, which is similar to information humans acquire in developing face verification skills and in becoming familiar with individuals. Second, the algorithms might exploit useful, but subtle, image-based detailed information that give them a slight, but consistent, advantage over humans.\nTherefore, surpassing the human-level performance may only be symbolically significant. In reality, a lot of challenges still lay ahead. To compete successfully with humans, more factors such as the robustness to familiar faces and the usage of non-face information, need to be considered in developing future face verification algorithms."}, {"heading": "9. Conclusion and Future Work", "text": "This paper presents a principled Multi-Task Learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, for face verification by including a computationally more efficient equivalent form of KFDA and the multi-task learning constraint to the DGPLVM model. We use Gaussian Processes approximation and anchor graphs to speed up the inference and prediction of our model. Based on the GaussianFace model, we propose two different approaches for face verification. Extensive experiments on challenging datasets validate the efficacy of our model. The GaussianFace model finally surpassed human-level face verification accuracy, thanks to exploiting additional data from multiple source-domains to improve the generalization performance of face verification in the target-domain and adapting automatically to complex face variations.\nAlthough several techniques such as the Laplace approx-\nimation and anchor graph are introduced to speed up the process of inference and prediction in our GaussianFace model, it still takes a long time to train our model for the high performance. In addition, large memory is also necessary. Therefore, for specific application, one needs to balance the three dimensions: memory, running time, and performance. Generally speaking, higher performance requires more memory and more running time. In the future, the issue of running time can be further addressed by the distributed parallel algorithm or the GPU implementation of large matrix inversion. To address the issue of memory, some online algorithms for training need to be developed. Another more intuitive method is to seek a more efficient sparse representation for the large covariance matrix."}, {"heading": "Acknowledgements", "text": "We would like to thank Deli Zhao and Chen Change Loy for their insightful discussions. This work is partially supported by \u201dCUHK Computer Vision Cooperation\u201d grant from Huawei, and by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project No.CUHK 416510 and 416312) and Guangdong Innovative Research Team Program (No.201001D0104648280)."}, {"heading": "1. Introduction", "text": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [?, ?, ?, ?, ?, ?, ?, ?]. It has many important applications, including surveillance, access control, image retrieval, and automatic log-on for personal computer or mobile devices. However, various visual complications deteriorate the performance of face verification, as shown by numerous studies on real-world face images from the wild [?]. The Labeled Faces in the Wild (LFW) dataset is well known as a challenging benchmark for face ver-\nification. The dataset provides a large set of relatively unconstrained face images with complex variations in pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters. Not surprisingly, LFW has proven difficult for automatic face verification methods [?, ?]. Although there has been significant work [?, ?, ?, ?, ?, ?, ?, ?, ?, ?] on LFW and the accuracy rate has been improved from 60.02% [?] to 97.25% [?] since LFW is established in 2007, these studies have not closed the gap to human-level performance [?] in face verification.\nWhy could not we surpass the human-level performance? Two possible reasons are found as follows:\n1) Most existing face verification methods assume that the training data and the test data are drawn from the same feature space and follow the same distribution. When the distribution changes, these methods may suffer a large performance drop [?]. However, many practical scenarios involve cross-domain data drawn from different facial appearance distributions. Learning a model solely on a single source data often leads to overfitting due to dataset bias [?]. Moreover, it is difficult to collect sufficient and necessary training data to rebuild the model in new scenarios, for highly accurate face verification specific to the target domain. In such cases, it becomes critical to exploit more data from multiple source-domains to improve the generalization of face verification methods in the targetdomain.\n2) Modern face verification methods are mainly divided into two categories: extracting low-level features [?, ?, ?, ?, ?], and building classification models [?, ?, ?, ?, ?, ?, ?, ?, ?, ?]. Although these existing methods have made great progress in face verification, most of them are less flexible when dealing with complex data distributions. For the methods in the first category, for example, low-level features such as SIFT [?], LBP [?], and Gabor [?] are handcrafted. Even for features learned from data [?, ?], the algorithm parameters (such as the depth of random projection tree, or the number of centers in k-means) also need to be specified by users. Similarly, for the methods in the second category, the architectures of deep networks in\n[?, ?, ?, ?] (for example, the number of layers, the number of nodes in each layer, etc.), and the parameters of the models in [?, ?, ?, ?] (for example, the number of Gaussians, the number of classifiers, etc.) must also be determined in advance. Since most existing methods require some assumptions to be made about the structures of the data, they cannot work well when the assumptions are not valid. Moreover, due to the existence of the assumptions, it is hard to capture the intrinsic structures of data using these methods.\nTo this end, we propose the Multi-Task Learning approach based on Discriminative Gaussian Process Latent Variable Model (DGPLVM) [?], named GaussianFace, for face verification. Unlike most existing studies [?, ?, ?, ?, ?] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM. Here, we investigate the asymmetric multi-task learning because we only focus on the performance improvement of the target task. From the perspective of information theory, this constraint aims to maximize the mutual information between the distributions of target-domain data and multiple source-domains data. Moreover, the GaussianFace model is a reformulation based on the Gaussian Processes (GPs) [?], which is a non-parametric Bayesian kernel method. Therefore, our model also can adapt its complexity flexibly to the complex data distributions in the real-world, without any heuristics or manual tuning of parameters.\nReformulating GPs for large-scale multi-task learning is non-trivial. To simplify calculations, we introduce a more efficient equivalent form of Kernel Fisher Discriminant Analysis (KFDA) to DGPLVM. Despite that the GaussianFace model can be optimized effectively using the Scaled Conjugate Gradient (SCG) technique, the inference is slow for large-scale data. We make use of GP approximations [?] and anchor graphs [?] to speed up the process of inference and prediction, so as to scale our model to large-scale data. Our model can be applied to face verification in two different ways: as a binary classifier and as a feature extractor. In the former mode, given a pair of face images, we can directly compute the posterior likelihood for each class to make a prediction. In the latter mode, our model can automatically extract high-dimensional features for each pair of face images, and then feed them to a classifier to make the final decision.\nThe main contributions of this paper are as follows:\n\u2022 We propose a novel GaussianFace model for face verification by virtue of the multi-task learning constraint to DGPLVM. Our model can adapt to complex distributions, avoid over-fitting, exploit discriminative information, and take advantage of multiple sourcedomains data.\n\u2022 We introduce a computationally more efficient equivalent form of KFDA to DGPLVM. This equivalent form reformulates KFDA to the kernel version consistent with the covariance function in GPs, which greatly simplifies calculations.\n\u2022 We introduce approximation in GPs and anchor graphs to speed up the process of inference and prediction.\n\u2022 We achieve superior performance on the challenging LFW benchmark [?], with an accuracy rate of 98.52%, beyond human-level performance reported in [?]."}, {"heading": "2. Related Work", "text": "Human and computer performance on face recognition has been compared extensively [?, ?, ?, ?, ?, ?]. These studies have shown that computer-based algorithms were more accurate than humans in well-controlled environments (e.g., frontal view, natural expression, and controlled illumination), whilst still comparable to humans in the poor condition (e.g., frontal view, natural expression, and uncontrolled illumination). However, the above conclusion is only verified on face datasets with controlled variations, where only one factor changes at a time [?, ?]. To date, there has been virtually no work showing that computerbased algorithms could surpass human performance on unconstrained face datasets, such as LFW, which exhibits natural (multifactor) variations in pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters.\nThere has been much work dealing with multifactor variations in face verification. For example, Simonyan et al. applied the Fisher vector to face verification and achieved a good performance [?]. However, the Fisher vector is derived from the Gaussian mixture model (GMM), where the number of Gaussians need to be specified by users, which means it cannot cover complex data automatically. Li et al. proposed a non-parametric subspace analysis [?, ?], but it is only a linear transformation and cannot cover the complex distributions. Besides, there also exist some approaches for utilizing plentiful sourcedomain data. Based on the Joint Bayesian algorithm [?], Cao et al. proposed a transfer learning approach [?] by merging source-domain data with limited target-domain data. Since this transfer learning approach is based on the joint Bayesian model of original visual features, it is not suitable for handling the complex nonlinear data and the data with complex manifold structures. Moreover, the transfer learning approach in [?] only considered two different domains, restricting its wider applications in largescale data from multiple domains. More recently, Zhu et al. [?] learned the transformation from face images under various poses and lighting conditions to a canonical view with a deep convolutional network. Sun et al. [?]\nlearned face representation with a deep model through face identification, which is a challenging multi-class prediction task. Taigman et al. [?] first utilized explicit 3D face modeling to apply a piecewise affine transformation, and then derived a face representation from a nine-layer deep neural network. Although these methods have achieved high performances on LFW, many parameters of them must be determined in advance so that they are less flexible when dealing with complex data distributions.\nThe core of our algorithm is GPs. To the best of our knowledge, GPs methods and Multi-task learning with related GPs methods (MTGP) have not been applied for face verification. Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [?, ?, ?, ?, ?, ?, ?, ?, ?]. However, most of them [?, ?, ?, ?, ?, ?, ?] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks. Leen et al. proposed a MTGP model in the asymmetric setting [?] to focus on improving performance on the target task, and Kim et al. developed a GP model for clustering [?], but their methods do not take the discriminative information of the covariance function into special account like DGPLVM. Although the discriminative information is considered in [?], it does not apply multitask learning to improve its performance. Salakhutdinov et al. used a deep belief net to learn a good covariance kernel for GPs [?]. The limitation of such deep methods is that it is hard to determine which architecture for this network is optimal. Also, multi-task learning constraint was not considered in [?]."}, {"heading": "3. Preliminary", "text": "In this section, we briefly review Gaussian Processes (GPs) for classification and clustering [?], and Gaussian Process Latent Variable Model (GPLVM) [?]. We use GPs method mainly due to the following three notable advantages. Firstly, as mentioned previously, it is a nonparametric method, which means it adapts its complexity flexibly to the complex data distributions in the real-world, without any heuristics or manual tuning of parameters. Secondly, GPs method can be computed effectively because of its closed-form marginal probability computation. Furthermore, its hyper-parameters can be learned from data automatically without using model selection methods such as cross validation, thereby avoiding the high computational cost. Thirdly, the inference of GPs is based on Bayesian rules, resulting in robustness to overfitting. We recommend Rasmussen and Williams\u2019s excellent monograph for further reading [?]."}, {"heading": "3.1. Gaussian Processes for Binary Classification", "text": "Formally, for two-class classification, suppose that we have a training set D of N observations,D = {(xi, yi)}Ni=1, where the i-th input point xi \u2208 RD and its corresponding output yi is binary, with y = 1i for one class and yi = \u22121 for the other. Let X be the N \u00d7 D matrix, where the row vectors represent all n input points, and y be the column vector of all n outputs. We define a latent variable fi for each input point xi, and let f = [f1, . . . , fN ]\u22a4. A sigmoid function \u03c0(\u00b7) is imposed to squash the output of the latent function into [0, 1], \u03c0(fi) = p(yi = 1|fi). Assuming the data set is i.i.d, then the joint likelihood factorizes to\np(y|f) = N \u220f\ni=1\np(yi|fi) = N \u220f\ni=1\n\u03c0(yifi). (1)\nMoreover, the posterior distribution over latent functions is\np(f |X,y, \u03b8) = p(y|f)p(f |X) p(X,y|\u03b8) . (2)\nSince neither p(f |X,y, \u03b8) nor p(y|f) can be computed analytically, the Laplace method is utilized to approximate the posterior\np(f |X,y, \u03b8) = N (f\u0302 , (K\u22121 +W)\u22121), (3)\nwhere f\u0302 = argmaxf p(f |X,y, \u03b8) and W = \u2212\u25bd\u25bd log p(f |X,y, \u03b8)|\nf=f\u0302 . Then, we can obtain\nlog p(y|X, \u03b8) = \u22121 2 f\u0302\u22a4K\u22121f\u0302 + log p(y|f\u0302 )\u2212 1 2 log |B|.\n(4)\nwhere |B| = |K| \u00b7 |K\u22121 +W| = |In +W 12KW 12 |. The optimal value of \u03b8 can be acquired by using the gradient method to maximize Equation (4). Given any unseen test point x\u2217, the probability of its latent function f\u2217 is\nf\u2217|X,y, x\u2217 \u223c N (K\u2217K\u22121f\u0302 ,K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 ), (5)\nwhere K\u0303 = K + W\u22121. Finally, we squash f\u2217 to find the probability of class membership as follows\n\u03c0\u0304(f\u2217) =\n\u222b\n\u03c0(f\u2217)p(f\u2217|X,y, x\u2217)df\u2217. (6)"}, {"heading": "3.2. Gaussian Processes for Clustering", "text": "The principle of GP clustering is based on the key observation that the variances of predictive values are smaller in dense areas and larger in sparse areas. The variances can be employed as a good estimate of the support of a probability density function, where each separate support domain can be considered as a cluster. This observation can be explained from the variance function of any predictive data point x\u2217\n\u03c32(x\u2217) = K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 . (7)\nIf x\u2217 is in a sparse region, then K\u2217K\u0303\u22121K\u22a4\u2217 becomes small, which leads to large variance \u03c32(x\u2217), and vice versa. Another good property of Equation (7) is that it does not depend on the labels, which means it can be applied to the unlabeled data.\nTo perform clustering, the following dynamic system associated with Equation (7) can be written as\nF (x) = \u2212\u25bd\u03c32(x). (8)\nThe theorem in [?] guarantees that almost all the trajectories approach one of the stable equilibrium points detected from Equation (8). After each data point finds its corresponding stable equilibrium point, we can employ a complete graph [?, ?] to assign cluster labels to data points with the stable equilibrium points. Obviously, the variance function in Equation (7) completely determines the performance of clustering."}, {"heading": "3.3. Gaussian Process Latent Variable Model", "text": "Let Z = [z1, . . . , zN ]\u22a4 denote the matrix whose rows represent corresponding positions of X in latent space, where zi \u2208 Rd (d \u226a D). The Gaussian Process Latent Variable Model (GPLVM) can be interpreted as a Gaussian process mapping from a low dimensional latent space to a high dimensional data set, where the locale of the points in latent space is determined by maximizing the Gaussian process likelihood with respect to Z. Given a covariance function for the Gaussian process, denoted by k(\u00b7, \u00b7), the likelihood of the data given the latent positions is as follows,\np(X|Z, \u03b8) = 1\u221a (2\u03c0)ND|K|D exp ( \u2212 1 2 tr(K\u22121XX\u22a4) ) ,\n(9)\nwhere Ki,j = k(zi, zj). Therefore, the posterior can be written as\np(Z, \u03b8|X) = 1Za p(X|Z, \u03b8)p(Z)p(\u03b8), (10)\nwhere Za is a normalization constant, the uninformative priors over \u03b8, and the simple spherical Gaussian priors over Z are introduced [?]. To obtain the optimal \u03b8 and Z, we need to optimize the above likelihood (10) with respect to \u03b8 and Z, respectively."}, {"heading": "4. GaussianFace", "text": "In order to automatically learn discriminative features or covariance function, and to take advantage of sourcedomain data to improve the performance in face verification, we develop a principled GaussianFace model by including the multi-task learning constraint into Discriminative Gaussian Process Latent Variable Model (DGPLVM) [?]."}, {"heading": "4.1. DGPLVM Reformulation", "text": "The DGPLVM is an extension of GPLVM, where the discriminative prior is placed over the latent positions, rather than a simple spherical Gaussian prior. The DGPLVM uses the discriminative prior to encourage latent positions of the same class to be close and those of different classes to be far. Since face verification is a binary classification problem and the GPs mainly depend on the kernel function, it is natural to use Kernel Fisher Discriminant Analysis (KFDA) [?] to model class structures in kernel spaces. For simplicity of inference in the followings, we introduce another equivalent formulation of KFDA to replace the one in [?].\nKFDA is a kernelized version of linear discriminant analysis method. It finds the direction defined by a kernel in a feature space, onto which the projections of positive and negative classes are well separated by maximizing the ratio of the between-class variance to the within-class variance. Formally, let {z1, . . . , zN+} denote the positive class and {zN++1, . . . , zN} the negative class, where the numbers of positive and negative classes are N+ and N\u2212 = N \u2212 N+, respectively. Let K be the kernel matrix. Therefore, in the feature space, the two sets {\u03c6K(z1), . . . , \u03c6K(zN+)} and {\u03c6K(zN++1), . . . , \u03c6K(zN )} represent the positive class and the negative class, respectively. The optimization criterion of KFDA is to maximize the ratio of the betweenclass variance to the within-class variance\nJ(\u03c9,K) = (w\u22a4(\u00b5+ K \u2212 \u00b5\u2212 K ))2\nw\u22a4(\u03a3+ K +\u03a3\u2212 K + \u03bbIN )w\n, (11)\nwhere \u03bb is a positive regularization parameter, \u00b5+ K\n= 1\nN+\n\u2211N+ i=1 \u03c6K(zi), \u00b5 \u2212 K = 1\nN\u2212\n\u2211N\ni=N++1 \u03c6K(zi), \u03a3 + K\n= 1\nN+\n\u2211N+ i=1(\u03c6K(zi) \u2212 \u00b5+K)(\u03c6K(zi) \u2212 \u00b5+K)\u22a4, and \u03a3\u2212K =\n1 N\u2212\n\u2211N i=N++1 (\u03c6K(zi)\u2212 \u00b5\u2212K)(\u03c6K(zi)\u2212 \u00b5\u2212K)\u22a4.\nIn this paper, however, we focus on the covariance function rather than the latent positions. To simplify calculations, we represent Equation (11) with the kernel function, and let the kernel function have the same form as the covariance function. Therefore, it is natural to introduce a more efficient equivalent form of KFDA with certain assumptions as Kim et al. points out [?], i.e., maximizing Equation (11) is equivalent to maximizing the following equation\nJ\u2217 = 1\n\u03bb\n( a\u22a4Ka\u2212 a\u22a4KA(\u03bbIn +AKA)\u22121AKa ) , (12)\nwhere\na =[ 1\nn+ 1\u22a4N+ ,\u2212\n1\nN\u2212 1\u22a4N\u2212 ]\nA =diag ( 1 \u221a\nN+\n( IN+ \u2212 1\nN+ 1N+1\n\u22a4 N+\n)\n,\n1 \u221a\nN\u2212\n( IN\u2212 \u2212 1\nN\u2212 1N\u22121\n\u22a4 N\u2212\n)\n)\n.\nHere, IN denotes the N\u00d7N identity matrix and 1N denotes the length-N vector of all ones in RN .\nTherefore, the discriminative prior over the latent positions in DGPLVM can be written as\np(Z) = 1\nZb exp\n( \u2212 1 \u03c32 J\u2217 ) , (13)\nwhere Zb is a normalization constant, and \u03c32 represents a global scaling of the prior.\nThe covariance matrix obtained by DGPLVM is discriminative and more flexible than the one used in conventional GPs for classification (GPC), since they are learned based on a discriminative criterion, and more degrees of freedom are estimated than conventional kernel hyper-parameters."}, {"heading": "4.2. Multi-task Learning Constraint", "text": "From an asymmetric multi-task learning perspective, the tasks should be allowed to share common hyper-parameters of the covariance function. Moreover, from an information theory perspective, the information cost between target task and multiple source tasks should be minimized. A natural way to quantify the information cost is to use the mutual entropy, because it is the measure of the mutual dependence of two distributions. For multi-task learning, we extend the mutual entropy to multiple distributions as follows\nM = H(pt)\u2212 1\nS\nS \u2211\ni=1\nH(pt|pi), (14)\nwhereH(\u00b7) is the marginal entropy,H(\u00b7|\u00b7) is the conditional entropy,S is the number of source tasks, {pi}Si=1, and pt are the probability distributions of source tasks and target task, respectively."}, {"heading": "4.3. GaussianFace Model", "text": "In this section, we describe our GaussianFace model in detail. Suppose we have S source-domain datasets {X1, . . . ,XS} and a target-domain data XT . For each source-domain data or target-domain data Xi, according to Equation (9), we write its marginal likelihood\np(Xi|Zi, \u03b8) = 1 \u221a\n(2\u03c0)ND|K|D exp\n( \u2212 1 2 tr(K\u22121XiX\u22a4i ) ) .\n(15)\nwhere Zi represents the domain-relevant latent space. For each source-domain data and target-domain data, their covariance functions K have the same form because they share the same hyper-parameters \u03b8. In this paper, we use a widely used kernel\nKi,j = k\u03b8(xi,xj) =\u03b80 exp ( \u2212 1 2\nd \u2211\nm=1\n\u03b8m(x m i \u2212 xmj )2\n)\n+ \u03b8d+1 + \u03b4xi,xj\n\u03b8d+2 , (16)\nwhere \u03b8 = {\u03b8i}d+2i=0 and d is the dimension of the data point. Then, from Equations (10), learning the DGPLVM is equivalent to optimizing\np(Zi, \u03b8|Xi) = 1\nZa p(Xi|Zi, \u03b8)p(Zi)p(\u03b8), (17)\nwhere p(Xi|Zi, \u03b8) and p(Zi) are respectively represented in (15) and (13). According to the multi-task learning constraint in Equation (14), we can attain\nM =H(p(ZT , \u03b8|XT ))\n\u2212 1 S\nS \u2211\ni=1\nH(p(ZT , \u03b8|XT )|p(Zi, \u03b8|Xi)). (18)\nFrom Equations (15), (17), and (18), we know that learning the GaussianFace model amounts to minimizing the following marginal likelihood\nLModel = \u2212 log p(ZT , \u03b8|XT )\u2212 \u03b2M, (19)\nwhere the parameter \u03b2 balances the relative importance between the target-domain data and the multi-task learning constraint."}, {"heading": "4.4. Optimization", "text": "For the model optimization, we first expand Equation (19) to obtain the following equation (ignoring the constant items)\nLModel =\u2212 logPT + \u03b2PT logPT\n+ \u03b2\nS\nS \u2211\ni=1\n( PT,i logPT \u2212 PT,i logPT,i ) , (20)\nwhere Pi = p(Zi, \u03b8|Xi) and Pi,j means that its corresponding covariance function is computed on both Xi and Xj . We can now optimize Equation (20) with respect to the hyper-parameters\u03b8 and the latent positions Zi by the Scaled Conjugate Gradient (SCG) technique. Since we focus on the covariance matrix in this paper, here we only present\nthe derivations of hyper-parameters. It is easy to get\n\u2202LModel \u2202\u03b8j = ( \u03b2(logPT + 1) + \u03b2 SPT S \u2211\ni=1\nPT,i \u2212 1\nPT\n)\u2202PT\n\u2202\u03b8j\n+ \u03b2\nS\nS \u2211\ni=1\n(logPT \u2212 logPT,i \u2212 1) \u2202PT,i\n\u2202\u03b8j .\nThe above equation depends on the form \u2202Pi \u2202\u03b8j as follows (ignoring the constant items)\n\u2202Pi \u2202\u03b8j =Pi \u2202 logPi \u2202\u03b8j\n\u2248Pi (\u2202 log p(Xi|Zi, \u03b8)\n\u2202\u03b8j + \u2202 log p(Zi) \u2202\u03b8j + \u2202 log p(\u03b8) \u2202\u03b8j ) .\nThe above three terms can be easily obtained (ignoring the constant items) by\n\u2202 log p(Xi|Zi, \u03b8) \u2202\u03b8j \u2248D 2 tr ( K\u22121 \u2202K \u2202\u03b8j )\n\u2212 1 2 K\u2212\u22a4XiX \u22a4 i K \u2212\u22a4 \u2202K \u2202\u03b8j ,\n\u2202 log p(Zi) \u2202\u03b8j \u2248\u2212 1 \u03c32 \u2202J\u2217i \u2202\u03b8j\n=\u2212 1 \u03bb\u03c32 ( a\u22a4 \u2202K \u2202\u03b8j a\u2212 a\u22a4 \u2202K \u2202\u03b8j A\u0303a\n+ a\u22a4KA\u0303 \u2202K \u2202\u03b8j A\u0303Ka\u2212 a\u22a4KA\u0303\u2202K \u2202\u03b8j a ) ,\n\u2202 log p(\u03b8)\n\u2202\u03b8j =\n1 \u03b8j ,\nwhere A\u0303 = A(\u03bbIn + AKA)\u22121A. Thus, the desired derivatives have been obtained."}, {"heading": "4.5. Speedup", "text": "In the GaussianFace model, we need to invert the large matrix when doing inference and prediction. For large problems, both storing the matrix and solving the associated linear systems are computationally prohibitive. In this paper, we use the anchor graphs method [?] to speed up this process. To put it simply, we first select q (q \u226a n) anchors to cover a cloud of n data points, and form an n \u00d7 q matrix Q, where Qi,j = k\u03b8(xi,xj). xi and xj are from n training data points and q anchors, respectively. Then the original kernel matrix K can be approximated as K = QQ\u22a4. Using the Woodbury identity [?], computing the n\u00d7 n matrix QQ\u22a4 can be transformed into computing the q \u00d7 q matrix Q\u22a4Q, which is more efficient.\nSpeedup on Inference When optimizing Equation (19), we need to invert the matrix (\u03bbIn + AKA). During\ninference, we take q k-means clustering centers as anchors to form Q. Substituting K = QQ\u22a4 into (\u03bbIn + AKA), and then using the Woodbury identity, we get\n(\u03bbIn +AKA) \u22121 = (\u03bbIn +AQQ \u22a4A)\u22121\n= \u03bb\u22121In \u2212 \u03bb\u22121AQ(\u03bbIq +Q\u22a4AAQ)\u22121Q\u22a4A. Speedup on Prediction When we compute the predictive variance \u03c3(x\u2217), we need to invert the matrix (K + W\u22121). At this time, we can use the method in Section 3.2 to calculate the accurate clustering centers that can be regarded as the anchors. Using the Woodbury identity again, we obtain\n(K+W\u22121)\u22121 = W \u2212WQ(Iq +Q\u22a4WQ)\u22121Q\u22a4W, where (Iq +Q\u22a4WQ) is only a q\u00d7 q matrix, and its inverse matrix can be computed more efficiently."}, {"heading": "5. GaussianFace Model for Face Verification", "text": "In this section, we describe two applications of the GaussianFace model to face verification: as a binary classifier and as a feature extractor.\nEach face image is first normalized to 150 \u00d7 120 size by an affine transformation based on five landmarks (two eyes, nose, and two mouth corners). The image is then divided into overlapped patches of 25 \u00d7 25 pixels with a stride of 2 pixels. Each patch within the image is mapped to a vector by a certain descriptor, and the vector is regarded as the feature of the patch, denoted by {xAp }Pp=1 where P is the number of patches within the face image A. In this paper, the multi-scale LBP feature of each patch is extracted [?]. The difference is that the multi-scale LBP descriptors are extracted at the center of each patch instead of accurate landmarks."}, {"heading": "5.1. GaussianFace Model as a Binary Classifier", "text": "For classification, our model can be regarded as an approach to learn a covariance function for GPC, as shown in Figure 1 (a). Here, for a pair of face images A and B from the same (or different) person, let the similarity vector xi = [s1, . . . , sp, . . . , sP ]\n\u22a4 be the input data point of the GaussianFace model, where sp is the similarity of xAp and xBp , and its corresponding output is yi = 1 (or \u22121). With the learned hyper-parameters of covariance function from the training data, given any un-seen pair of face images, we first compute its similarity vector x\u2217 using the above method, then predict whether the pair is from the same person through Equation (6). In this paper, we prescribe the sigmoid function \u03c0(\u00b7) to be the cumulative Gaussian distribution \u03a6(\u00b7), which can be solved analytically as \u03c0\u0304\u2217 = \u03a6 (\nf\u0304\u2217(x\u2217)\u221a 1+\u03c32(x\u2217)\n)\n, where \u03c32(x\u2217) = K\u2217\u2217 \u2212K\u2217K\u0303\u22121K\u22a4\u2217 and f\u0304\u2217(x\u2217) = K\u2217K\n\u22121f\u0302 from Equation (5) [?]. We call the method GaussianFace-BC."}, {"heading": "5.2. GaussianFace Model as a Feature Extractor", "text": "As a feature extractor, our model can be regarded as an approach to automatically extract facial features, shown in Figure 1 (b). Here, for a pair of face images A and B from the same (or different) person, we regard the joint feature vector xi = [(xAi ) \u22a4, (xBi ) \u22a4]\u22a4 as the input data point of the GaussianFace model, and its corresponding output is yi = 1 (or \u22121). To enhance the robustness of our approach, the flipped form of xi is also included; for example, xi = [(x B i ) \u22a4, (xAi ) \u22a4]\u22a4. After the hyper-parameters of covariance function are learnt from the training data, we can use the method in Section 3.2 to group the input data points into different clusters automatically. Suppose that we finally obtain C clusters. The centers of these clusters are denoted by {ci}Ci=1, the variances of these clusters by {\u03a32i }Ci=1, and their weights by {wi}Ci=1 where wi is the ratio of the number of data points from the i-th cluster to the number of all data points. Then we refer to each ci as the input of Equation (5), and we can obtain its corresponding probability pi and variance \u03c32i . In fact, {ci}Ci=1 can be regarded as a codebook generated by our model.\nFor any un-seen pair of face images, we also first compute its joint feature vectorx\u2217 for each pair of patches. Then we compute its first-order and second-order statistics to the centers. The statistics and variance of x\u2217 are represented as its high-dimensional facial features, denoted by x\u0302\u2217 = [\u220611,\u2206 2 1,\u2206 3 1,\u2206 4 1, . . . ,\u2206 1 C ,\u2206 2 C ,\u2206 3 C ,\u2206 4 C ] \u22a4, where \u22061i =\nwi\n(\nx\u2217\u2212ci\n\u03a3i\n) , \u22062i = wi ( x\u2217\u2212ci\n\u03a3i\n)2\n, \u22063i = pi, and \u2206 4 i =\n\u03c32i . We then concatenate all of the new high-dimensional features from each pair of patches to form the final new high-dimensional feature for the pair of face images. The new high-dimensional facial features not only describe how the distribution of features of an un-seen face image differs from the distribution fitted to the features of all training images, but also encode the predictive information including the probabilities of label and uncertainty. We call this approach GaussianFace-FE."}, {"heading": "6. Experimental Settings", "text": "In this section, we conduct experiments on face verification. We start by introducing the source-domain datasets and the target-domain dataset in all of our experiments (see Figure 2 for examples). The source-domain datasets include four different types of datasets as follows: Multi-PIE [?]. This dataset contains face images from 337 subjects under 15 view points and 19 illumination conditions in four recording sessions. These images are collected under controlled conditions. MORPH [?]. The MORPH database contains 55,000 images of more than 13,000 people within the age ranges of 16 to 77. There are an average of 4 images per individual.\nWeb Images2. This dataset contains around 40,000 facial images from 3261 subjects; that is, approximately 10 images for each person. The images were collected from the Web with significant variations in pose, expression, and illumination conditions. Life Photos2. This dataset contains approximately 5000 images of 400 subjects collected online. Each subject has roughly 10 images.\nIf not otherwise specified, the target-domain dataset is the benchmark of face verification as follows: LFW [?]. This dataset contains 13,233 uncontrolled face images of 5749 public figures with variety of pose, lighting, expression, race, ethnicity, age, gender, clothing, hairstyles, and other parameters. All of these images are collected from the Web.\nWe use the LFW dataset as the target-domain dataset because it is well known as a challenging benchmark. Using it also allows us to compare directly with other existing face verification methods [?, ?, ?, ?, ?, ?, ?, ?, ?]. Besides, this dataset provides a large set of relatively unconstrained face images with complex variations as described above, and has proven difficult for automatic face verification methods [?, ?]. In all the experiments conducted on LFW, we strictly follow the standard unrestricted protocol of LFW [?]. More precisely, during the training procedure, the four sourcedomain datasets are: Web Images, Multi-PIE, MORPH, and Life Photos, the target-domain dataset is the training set\n2These two datasets are collected by our own from the Web. It is guaranteed that these two datasets are mutually exclusive with the LFW dataset.\nin View 1 of LFW, and the validation set is the test set in View 1 of LFW. At the test time, we follow the standard 10-fold cross-validation protocol to test our model in View 2 of LFW.\nFor each one of the four source-domain datasets, we randomly sample 20,000 pairs of matched images and 20,000 pairs of mismatched images. The training partition and the testing partition in all of our experiments are mutually exclusive. In other words, there is no identity overlap among the two partitions.\nFor the experiments below, \u201cThe Number of SD\u201d means \u201cthe Number of Source-Domain datasets that are fed into the GaussianFace model for training\u201d. By parity of reasoning, if \u201cThe Number of SD\u201d is i, that means the first i source-domain datasets are used for model training. Therefore, if \u201cThe Number of SD\u201d is 0, models are trained with the training data from target-domain data only.\nImplementation details. Our model involves four important parameters: \u03bb in (12), \u03c3 in (13), \u03b2 in (19), and the number of anchors q in Speedup on Inference 3. Following the same setting in [?], the regularization parameter \u03bb in (12) is fixed to 10\u22128. \u03c3 reflects the tradeoff between our method\u2019s ability to discriminate (small \u03c3) and its ability to generalize (large \u03c3), and \u03b2 balances the relative importance between the target-domain data and the multi-task learning constraint. Therefore, the validation set (the test set in View 1 of LFW) is used for selecting \u03c3 and \u03b2. Each time we use different number of source-domain datasets for training, the corresponding optimal \u03c3 and \u03b2 should be selected on the validation set.\nSince we collected a large number of image pairs for training (20,000 matched pairs and 20,000 mismatched pairs from each source-domain dataset), and our model is based on the kernel method, thus an important consideration is how to efficiently approximate the kernel matrix using a low-rank method in the limited space and time. We adopt the anchor graphs method (see Section 4.5) for kernel approximation. In our experiments, we take two steps to determine the number of anchor points. In the first step, the optimal \u03c3 and \u03b2 are selected on the validation set in each experiment. In the second step, we fix \u03c3 and \u03b2, and then tune the number of anchor points. We vary the number of anchor points to train our model on the training set, and test it on the validation set. We report the average accuracy for our model over 10 trials. After we consider the trade-off between memory and running time in practice, the number of anchor points with the best average accuracy is determined in each experiments.\n3The other parameters, such as the hyper-parameters in the kernel function and the number of anchors in Speedup on Prediction, can be automatically learned from the data."}, {"heading": "7. Experimental Results", "text": "In this section, we conduct five experiments to demonstrate the validity of the GaussianFace model."}, {"heading": "7.1. Comparisons with Other MTGP/GP Methods", "text": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [?], MTGP prediction [?], GPLVM [?], and DGPLVM [?]. For fair comparisons, all these models are trained on multiple source-domain datasets using the same two methods as our GaussianFace model described in Section 5. After the hyper-parameters of covariance function are learnt for each model, we can regard each model as a binary classifier and a feature extractor like ours, respectively. Figure 3 shows that our model significantly outperforms the other four GPs models, and the superiority of our model becomes more obvious as the number of source-domain datasets increases."}, {"heading": "7.2. Comparisons with Other Binary Classifiers", "text": "Since our model can be regarded as a binary classifier, we have also compared our method with other classical binary classifiers. For this paper, we chose three popular representatives: SVM [?], logistic regression (LR) [?], and Adaboost [?]. Table 1 demonstrates that the performance of our method GaussianFace-BC is much better than those of the other classifiers. Furthermore, these experimental results demonstrates the effectiveness of the multi-task learning constraint. For example, our GaussianFace-BC has about 7.5% improvement when all four source-domain datasets are used for training, while the best one of the other three binary classifiers has only around 4% improvement."}, {"heading": "7.3. Comparisons with Other Feature Extractors", "text": "Our model can also be regarded as a feature extractor, which is implemented by clustering to generate a codebook. Therefore, we evaluate our method by comparing it with three popular clustering methods: K-means [?], Random Projection (RP) tree [?], and Gaussian Mixture\nModel (GMM) [?]. Since our method can determine the number of clusters automatically, for fair comparison, all the other methods generate the same number of clusters as ours. As shown in Table 2, our method GaussianFace-FE significantly outperforms all of the compared approaches, which verifies the effectiveness of our method as a feature extractor. The results have also proved that the multi-task learning constraint is effective. Each time one different type of source-domain dataset is added for training, the performance can be improved significantly. Our GaussianFace-FE model achieves over 8% improvement when the number of SD varies from 0 to 4, which is much higher than the \u223c3% improvement of the other methods."}, {"heading": "7.4. Comparison with the state-of-art Methods", "text": "Motivated by the appealing performance of both GaussianFace-BC and GaussianFace-FE, we further combine them for face verification. Specifically, after facial features are extracted using GaussianFace-FE, GaussianFaceBC 4 is used to make the final decision. Figure 4 shows the results of this combination compared with state-of-the-art methods [?, ?, ?, ?, ?, ?, ?, ?, ?]. The best published result on the LFW benchmark is 96.33% 5, which is achieved by [?]. Our GaussianFace model can improve the accuracy to 98.52%, which for the first time beats the human-level performance (97.53%, cropped) [?]. Figure 5 presents some example pairs that were always incorrectly classified by our model. Obviously, even for humans, it is also difficult to verify some of them. Here, we emphasize that the centers of patches, instead of the accurate and dense facial landmarks like [?], are utilized to extract multi-scale features in our method. This makes our method simpler and easier to use."}, {"heading": "7.5. Further Validations: Shuffling the SourceTarget", "text": "To further prove the validity of our model, we also consider to treat Multi-PIE and MORPH respectively as the target-domain dataset and the others as the sourcedomain datasets. The target-domain dataset is split into two mutually exclusive parts: one consisting of 20,000 matched pairs and 20,000 mismatched pairs is used for training, the other is used for test. In the test set, similar to the protocol of\n4Here, the GaussianFace BC is trained with the extracted highdimensional features using GaussianFace-FE.\n5In fact, [?] and [?] have achieved higher accuracies 97.15% and 97.25%, respectively. We do not report their performances in Figure 4, since they have not reported their ROC curves on the LFW website so that we cannot obtain the results to draw their ROC curves.\nLFW, we select 10 mutually exclusive subsets, where each subset consists of 300 matched pairs and 300 mismatched pairs. The experimental results are presented in Figure 6. Each time one dataset is added to the training set, the performance can be improved, even though the types of data are very different in the training set."}, {"heading": "8. General Discussion", "text": "There is an implicit belief among many psychologists and computer scientists that human face verification abilities are currently beyond existing computer-based face verification algorithms [?]. This belief, however, is supported more by anecdotal impression than by scientific evidence. By contrast, there have already been a number of papers comparing human and computer-based face verification performance [?, ?, ?, ?, ?, ?]. It has been shown that the best current face verification algorithms perform better than humans in the good and moderate conditions. So, it is really not that difficult to beat human performance in some specific scenarios.\nAs pointed out by [?, ?], humans and computer-based algorithms have different strategies in face verification. Indeed, by contrast to performance with unfamiliar faces, human face verification abilities for familiar faces are relatively robust to changes in viewing parameters such as illumination and pose. For example, Bruce [?] found human recognition memory for unfamiliar faces dropped substantially when there were changes in viewing parameters. Besides, humans can take advantages of non-face configurable information from the combination of the face and body (e.g., neck, shoulders). It has also been examined in [?], where the human performance drops from 99.20% (tested using the original LFW images) to 97.53% (tested using the cropped LFW images). Hence, the experiments comparing human and computer performance may not show human face verification skill at their best, because humans were asked to match the cropped faces of people previously unfamiliar to them. To the contrary, those experiments\ncan fully show the performance of computer-based face verification algorithms. First, the algorithms can exploit information from enough training images with variations in all viewing parameters to improve face verification performance, which is similar to information humans acquire in developing face verification skills and in becoming familiar with individuals. Second, the algorithms might exploit useful, but subtle, image-based detailed information that give them a slight, but consistent, advantage over humans.\nTherefore, surpassing the human-level performance may only be symbolically significant. In reality, a lot of challenges still lay ahead. To compete successfully with humans, more factors such as the robustness to familiar faces and the usage of non-face information, need to be considered in developing future face verification algorithms."}, {"heading": "9. Conclusion and Future Work", "text": "This paper presents a principled Multi-Task Learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, for face verification by including a computationally more efficient equivalent form of KFDA and the multi-task learning constraint to the DGPLVM model. We use Gaussian Processes approximation and anchor graphs to speed up the inference and prediction of our model. Based on the GaussianFace model, we propose two different approaches for face verification. Extensive experiments on challenging datasets validate the efficacy of our model. The GaussianFace model finally surpassed human-level face verification accuracy, thanks to exploiting additional data from multiple source-domains to improve the generalization performance of face verification in the target-domain and adapting automatically to complex face variations.\nAlthough several techniques such as the Laplace approximation and anchor graph are introduced to speed up the\nprocess of inference and prediction in our GaussianFace model, it still takes a long time to train our model for the high performance. In addition, large memory is also necessary. Therefore, for specific application, one needs to balance the three dimensions: memory, running time, and performance. Generally speaking, higher performance requires more memory and more running time. In the future, the issue of running time can be further addressed by the distributed parallel algorithm or the GPU implementation of large matrix inversion. To address the issue of memory, some online algorithms for training need to be developed. Another more intuitive method is to seek a more efficient sparse representation for the large covariance matrix."}, {"heading": "Acknowledgements", "text": "We would like to thank Deli Zhao and Chen Change Loy for their insightful discussions. This work is partially supported by \u201dCUHK Computer Vision Cooperation\u201d grant from Huawei, and by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project No.CUHK 416510 and 416312) and Guangdong Innovative Research Team Program (No.201001D0104648280)."}], "references": [{"title": "Comparing human and automatic face recognition performance", "author": ["A. Adler", "M.E. Schuckers"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 37(5):1248\u20131255,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Face description with local binary patterns: Application to face recognition", "author": ["T. Ahonen", "A. Hadid", "M. Pietikainen"], "venue": "TPAMI, 28(12):2037\u20132041,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector clustering", "author": ["A. Ben-Hur", "D. Horn", "H.T. Siegelmann", "V. Vapnik"], "venue": "JMLR, 2,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Tom-vs-pete classifiers and identity-preserving alignment for face verification", "author": ["T. Berg", "P.N. Belhumeur"], "venue": "BMVC, volume 1, page 5,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-task gaussian process prediction", "author": ["E. Bonilla", "K.M. Chai", "C. Williams"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Changing faces: Visual and non-visual coding processes in face recognition", "author": ["V. Bruce"], "venue": "British Journal of Psychology, 73(1):105\u2013116,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1982}, {"title": "Comparisons between human and computer recognition of faces", "author": ["V. Bruce", "P.J. Hancock", "A.M. Burton"], "venue": "Automatic Face and Gesture Recognition, pages 408\u2013413,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "A practical transfer learning algorithm for face verification", "author": ["X. Cao", "D. Wipf", "F. Wen", "G. Duan"], "venue": "ICCV.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition with learning-based descriptor", "author": ["Z. Cao", "Q. Yin", "X. Tang", "J. Sun"], "venue": "CVPR, pages 2707\u2013 2714,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task learning with gaussian processes", "author": ["K.M. Chai"], "venue": "The University of Edinburgh,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM TIST, 2(3):27,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian face revisited: A joint formulation", "author": ["D. Chen", "X. Cao", "L. Wang", "F. Wen", "J. Sun"], "venue": "ECCV, pages 566\u2013579.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification", "author": ["D. Chen", "X. Cao", "F. Wen", "J. Sun"], "venue": "CVPR.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Random projection trees for vector quantization", "author": ["S. Dasgupta", "Y. Freund"], "venue": "IEEE Transactions on Information Theory, 55(7):3229\u20133242,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep face representation", "author": ["H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin", "C. Doudou"], "venue": "arXiv preprint arXiv:1403.2802,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "JMLR, 9:1871\u20131874,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R. Schapire", "N. Abe"], "venue": "Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Multi-pie", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": "Image and Vision Computing, 28(5):807\u2013813,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Aurora face recognition technical report: Evaluation of algorithm aurora-c-2014-1 on labeled faces in the wild", "author": ["T. Heseltine", "P. Szeptycki", "J. Gomes", "M. Ruiz", "P. Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Accuracy and Stability of Numberical Algorithms", "author": ["N.J. Higham"], "venue": "Number 48. Siam,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning hierarchical representations for face verification with convolutional deep belief networks", "author": ["G. Huang", "H. Lee", "E. Learned-Miller"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned- Miller"], "venue": "Technical Report 07-49, University of Massachusetts, Amherst,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Face recognition using local quantized patterns", "author": ["S.U. Hussain", "T. Napol\u00e9on", "F. Jurie"], "venue": "In BMVC,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Appearance-based gender classification with gaussian processes", "author": ["H.-C. Kim", "D. Kim", "Z. Ghahramani", "S.Y. Bang"], "venue": "Pattern Recognition Letters, 27(6):618\u2013 626,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering based on gaussian processes", "author": ["H.-C. Kim", "J. Lee"], "venue": "Neural computation, 19(11),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal kernel selection in kernel fisher discriminant analysis", "author": ["S.-J. Kim", "A. Magnani", "S. Boyd"], "venue": "ICML, pages 465\u2013472,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "ICCV, pages 365\u2013372,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "NIPS, volume 2, page 5,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Focused multitask learning using gaussian processes", "author": ["G. Leen", "J. Peltonen", "S. Kaski"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 310\u2013325.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic elastic matching for pose variant face verification", "author": ["H. Li", "G. Hua", "Z. Lin", "J. Brandt", "J. Yang"], "venue": "CVPR.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonparametric discriminant analysis for face recognition", "author": ["Z. Li", "D. Lin", "X. Tang"], "venue": "TPAMI, 31(4):755\u2013 761,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonparametric subspace analysis for face recognition", "author": ["Z. Li", "W. Liu", "D. Lin", "X. Tang"], "venue": "CVPR, volume 2, pages 961\u2013966,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition", "author": ["C. Liu", "H. Wechsler"], "venue": "TIP,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S.-F. Chang"], "venue": "ICML, pages 679\u2013686,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 60(2):91\u2013110,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian face recognition", "author": ["B. Moghaddam", "T. Jebara", "A. Pentland"], "venue": "Pattern Recognition, 33(11):1771\u2013 1782,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Comparing face recognition algorithms to humans on challenging tasks", "author": ["A.J. O\u2019Toole", "X. An", "J. Dunlop", "V. Natu", "P.J. Phillips"], "venue": "ACM Transactions on Applied Perception,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Predicting human performance for face recognition", "author": ["A.J. OToole", "F. Jiang", "D. Roark", "H. Abdi"], "venue": "Face Processing: Advanced Methods and Models. Elsevier, Amsterdam,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Face recognition algorithms surpass humans matching faces over changes", "author": ["A.J. O\u2019Toole", "P.J. Phillips", "F. Jiang", "J. Ayyad", "N. P\u00e9nard", "H. Abdi"], "venue": "in illumination. TPAMI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Comparison of human and computer performance across face recognition experiments", "author": ["P.J. Phillips", "A.J. O\u2019Toole"], "venue": "Image and Vision Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Morph: A longitudinal image database of normal adult age-progression", "author": ["K. Ricanek", "T. Tesafaye"], "venue": "Automatic Face and Gesture Recognition, pages 341\u2013 345,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2006}, {"title": "Coupled gaussian process regression for pose-invariant facial expression recognition", "author": ["O. Rudovic", "I. Patras", "M. Pantic"], "venue": "ECCV.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Using deep belief nets to learn covariance kernels for gaussian processes", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Face verification using the lark representation", "author": ["H.J. Seo", "P. Milanfar"], "venue": "TIFS, 6(4):1275\u20131286,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Fisher vector faces in the wild", "author": ["K. Simonyan", "O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "IJCV, 60(2):91\u2013 110,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Face recognition by humans: 20 results all computer vision researchers should know about", "author": ["P. Sinha", "B. Balas", "Y. Ostrovsky", "R. Russell"], "venue": "Department of Brain and Cognitive Sciences, MIT, Cambridge, MA,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Bayesian multitask classification with gaussian process priors", "author": ["G. Skolidis", "G. Sanguinetti"], "venue": "IEEE Transactions on Neural Networks, 22(12),", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid deep learning for face verification", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "ICCV.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "CVPR,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple oneshots for utilizing class label information", "author": ["Y. Taigman", "L. Wolf", "T. Hassner"], "venue": "BMVC, pages 1\u201312,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep- Face: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "CVPR,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Face sketch recognition", "author": ["X. Tang", "X. Wang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 14(1):50\u201357,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "CVPR, pages 1521\u20131528,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Face recognition using eigenfaces", "author": ["M.A. Turk", "A.P. Pentland"], "venue": "CVPR, pages 586\u2013591,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1991}, {"title": "Discriminative gaussian process latent variable model for classification", "author": ["R. Urtasun", "T. Darrell"], "venue": "ICML, pages 927\u2013934,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "Implicit elastic matching with random projections for pose-variant face recognition", "author": ["J. Wright", "G. Hua"], "venue": "CVPR, pages 1502\u20131509,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "An associate-predict model for face recognition", "author": ["Q. Yin", "X. Tang", "J. Sun"], "venue": "CVPR, pages 497\u2013504,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "ICML, pages 1012\u20131019,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-task warped gaussian process for personalized age estimation", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "CVPR, pages 2622\u20132629,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning identity preserving face space", "author": ["Z. Zhu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Recover canonical-view faces in the wild with deep neural networks", "author": ["Z. Zhu", "P. Luo", "X. Wang", "X. Tang"], "venue": "arXiv:1404.3543,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "53%) [28] on LFW is surpassed.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 20, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 44, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 3, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 45, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 29, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 12, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 7, "context": "Face verification, which is the task of determining whether a pair of face images are from the same person, has been an active research topic in computer vision for decades [28, 22, 46, 5, 47, 31, 14, 9].", "startOffset": 173, "endOffset": 203}, {"referenceID": 21, "context": "However, various visual complications deteriorate the performance of face verification, as shown by numerous studies on real-world face images from the wild [23].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "Not surprisingly, LFW has proven difficult for automatic face verification methods [23, 28].", "startOffset": 83, "endOffset": 91}, {"referenceID": 26, "context": "Not surprisingly, LFW has proven difficult for automatic face verification methods [23, 28].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 7, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 3, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 12, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 45, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 11, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 57, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 48, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 49, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 51, "context": "Although there has been significant work [22, 9, 5, 14, 47, 13, 59, 50, 51, 53] on LFW and the accuracy rate has been improved from 60.", "startOffset": 41, "endOffset": 79}, {"referenceID": 54, "context": "02% [56] to 97.", "startOffset": 4, "endOffset": 8}, {"referenceID": 51, "context": "25% [53] since LFW is established in 2007, these studies have not closed the gap to human-level performance [28] in face verification.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "25% [53] since LFW is established in 2007, these studies have not closed the gap to human-level performance [28] in face verification.", "startOffset": 108, "endOffset": 112}, {"referenceID": 56, "context": "When the distribution changes, these methods may suffer a large performance drop [58].", "startOffset": 81, "endOffset": 85}, {"referenceID": 53, "context": "Learning a model solely on a single source data often leads to overfitting due to dataset bias [55].", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 106, "endOffset": 125}, {"referenceID": 1, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 106, "endOffset": 125}, {"referenceID": 32, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 106, "endOffset": 125}, {"referenceID": 8, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 106, "endOffset": 125}, {"referenceID": 22, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 106, "endOffset": 125}, {"referenceID": 60, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 48, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 11, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 35, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 29, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 54, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 3, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 26, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 45, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 31, "context": "2) Modern face verification methods are mainly divided into two categories: extracting low-level features [36, 3, 34, 10, 24], and building classification models [62, 50, 13, 37, 31, 56, 5, 28, 47, 33].", "startOffset": 162, "endOffset": 201}, {"referenceID": 34, "context": "For the methods in the first category, for example, low-level features such as SIFT [36], LBP [3], and Gabor [34] are handcrafted.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "For the methods in the first category, for example, low-level features such as SIFT [36], LBP [3], and Gabor [34] are handcrafted.", "startOffset": 94, "endOffset": 97}, {"referenceID": 32, "context": "For the methods in the first category, for example, low-level features such as SIFT [36], LBP [3], and Gabor [34] are handcrafted.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "Even for features learned from data [10, 24], the algorithm parameters (such as the depth of random projection tree, or the number of centers in k-means) also need to be specified by users.", "startOffset": 36, "endOffset": 44}, {"referenceID": 22, "context": "Even for features learned from data [10, 24], the algorithm parameters (such as the depth of random projection tree, or the number of centers in k-means) also need to be specified by users.", "startOffset": 36, "endOffset": 44}, {"referenceID": 60, "context": "for the methods in the second category, the architectures of deep networks in [62, 50, 63, 51] (for example, the number of layers, the number of nodes in each layer, etc.", "startOffset": 78, "endOffset": 94}, {"referenceID": 48, "context": "for the methods in the second category, the architectures of deep networks in [62, 50, 63, 51] (for example, the number of layers, the number of nodes in each layer, etc.", "startOffset": 78, "endOffset": 94}, {"referenceID": 61, "context": "for the methods in the second category, the architectures of deep networks in [62, 50, 63, 51] (for example, the number of layers, the number of nodes in each layer, etc.", "startOffset": 78, "endOffset": 94}, {"referenceID": 49, "context": "for the methods in the second category, the architectures of deep networks in [62, 50, 63, 51] (for example, the number of layers, the number of nodes in each layer, etc.", "startOffset": 78, "endOffset": 94}, {"referenceID": 29, "context": "), and the parameters of the models in [31, 5, 28, 47] (for example, the number of Gaussians, the number of classifiers, etc.", "startOffset": 39, "endOffset": 54}, {"referenceID": 3, "context": "), and the parameters of the models in [31, 5, 28, 47] (for example, the number of Gaussians, the number of classifiers, etc.", "startOffset": 39, "endOffset": 54}, {"referenceID": 26, "context": "), and the parameters of the models in [31, 5, 28, 47] (for example, the number of Gaussians, the number of classifiers, etc.", "startOffset": 39, "endOffset": 54}, {"referenceID": 45, "context": "), and the parameters of the models in [31, 5, 28, 47] (for example, the number of Gaussians, the number of classifiers, etc.", "startOffset": 39, "endOffset": 54}, {"referenceID": 55, "context": "To this end, we propose the Multi-Task Learning approach based on Discriminative Gaussian Process Latent Variable Model (DGPLVM) [57], named GaussianFace, for face verification.", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM.", "startOffset": 29, "endOffset": 48}, {"referenceID": 3, "context": "Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": "Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM.", "startOffset": 29, "endOffset": 48}, {"referenceID": 45, "context": "Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM.", "startOffset": 29, "endOffset": 48}, {"referenceID": 11, "context": "Unlike most existing studies [22, 5, 14, 47, 13] that rely on a single training data source, in order to take advantage of more data from multiple source-domains to improve the performance in the target-domain, we introduce the multi-task learning constraint to DGPLVM.", "startOffset": 29, "endOffset": 48}, {"referenceID": 40, "context": "Moreover, the GaussianFace model is a reformulation based on the Gaussian Processes (GPs) [42], which is a non-parametric Bayesian kernel method.", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "We make use of GP approximations [42] and anchor graphs [35] to speed up the process of inference and prediction, so as to scale our model to largescale data.", "startOffset": 33, "endOffset": 37}, {"referenceID": 33, "context": "We make use of GP approximations [42] and anchor graphs [35] to speed up the process of inference and prediction, so as to scale our model to largescale data.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "\u2022 We achieve superior performance on the challenging LFW benchmark [23], with an accuracy rate of 98.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "52%, beyond human-level performance reported in [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 36, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 0, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 52, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 39, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 6, "context": "Human and computer performance on face recognition has been compared extensively [40, 38, 2, 54, 41, 8].", "startOffset": 81, "endOffset": 103}, {"referenceID": 38, "context": "However, the above conclusion is only verified on face datasets with controlled variations, where only one factor changes at a time [40, 38].", "startOffset": 132, "endOffset": 140}, {"referenceID": 36, "context": "However, the above conclusion is only verified on face datasets with controlled variations, where only one factor changes at a time [40, 38].", "startOffset": 132, "endOffset": 140}, {"referenceID": 45, "context": "applied the Fisher vector to face verification and achieved a good performance [47].", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "proposed a non-parametric subspace analysis [33, 32], but it is only a linear transformation and cannot cover the complex distributions.", "startOffset": 44, "endOffset": 52}, {"referenceID": 30, "context": "proposed a non-parametric subspace analysis [33, 32], but it is only a linear transformation and cannot cover the complex distributions.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "Based on the Joint Bayesian algorithm [13], Cao et al.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "proposed a transfer learning approach [9] by merging source-domain data with limited target-domain data.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Moreover, the transfer learning approach in [9] only considered two different domains, restricting its wider applications in largescale data from multiple domains.", "startOffset": 44, "endOffset": 47}, {"referenceID": 61, "context": "[63] learned the transformation from face images under various poses and lighting conditions to a canonical view with a deep convolutional network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] learned face representation with a deep model through face identification, which is a challenging multi-class prediction task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] first utilized explicit 3D face modeling to apply a piecewise affine transformation, and then derived a face representation from a nine-layer deep neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 58, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 9, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 23, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 28, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 42, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 47, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 59, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 24, "context": "Actually, MTGP/GPs have been extensively studied in machine learning and computer vision in recent years [6, 60, 11, 25, 30, 44, 49, 61, 26].", "startOffset": 105, "endOffset": 140}, {"referenceID": 58, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 9, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 4, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 42, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 23, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 47, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 59, "context": "However, most of them [60, 11, 6, 44, 25, 49, 61] have only considered the symmetric multi-task learning, which means that all tasks have been assumed to be of equal importance, whereas our purpose is to enhance performance on a target task given all other source tasks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 28, "context": "proposed a MTGP model in the asymmetric setting [30] to focus on improving performance on the target task, and Kim et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "developed a GP model for clustering [26], but their methods do not take the discriminative information of the covariance function into special account like DGPLVM.", "startOffset": 36, "endOffset": 40}, {"referenceID": 55, "context": "Although the discriminative information is considered in [57], it does not apply multi-task learning to improve its performance.", "startOffset": 57, "endOffset": 61}, {"referenceID": 43, "context": "used a deep belief net to learn a good covariance kernel for GPs [45].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "Also, multi-task learning constraint was not considered in [45].", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "In this section, we briefly review Gaussian Processes (GPs) for classification and clustering [26], and Gaussian Process Latent Variable Model (GPLVM) [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "In this section, we briefly review Gaussian Processes (GPs) for classification and clustering [26], and Gaussian Process Latent Variable Model (GPLVM) [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "We recommend Rasmussen and Williams\u2019s excellent monograph for further reading [42].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The theorem in [26] guarantees that almost all the trajectories approach one of the stable equilibrium points detected from Equation (8).", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "After each data point finds its corresponding stable equilibrium point, we can employ a complete graph [4, 26] to assign cluster labels to data points with the stable equilibrium points.", "startOffset": 103, "endOffset": 110}, {"referenceID": 24, "context": "After each data point finds its corresponding stable equilibrium point, we can employ a complete graph [4, 26] to assign cluster labels to data points with the stable equilibrium points.", "startOffset": 103, "endOffset": 110}, {"referenceID": 55, "context": "where Za is a normalization constant, the uninformative priors over \u03b8, and the simple spherical Gaussian priors over Z are introduced [57].", "startOffset": 134, "endOffset": 138}, {"referenceID": 55, "context": "In order to automatically learn discriminative features or covariance function, and to take advantage of sourcedomain data to improve the performance in face verification, we develop a principled GaussianFace model by including the multi-task learning constraint into Discriminative Gaussian Process Latent Variable Model (DGPLVM) [57].", "startOffset": 331, "endOffset": 335}, {"referenceID": 25, "context": "Since face verification is a binary classification problem and the GPs mainly depend on the kernel function, it is natural to use Kernel Fisher Discriminant Analysis (KFDA) [27] to model class structures in kernel spaces.", "startOffset": 173, "endOffset": 177}, {"referenceID": 55, "context": "For simplicity of inference in the followings, we introduce another equivalent formulation of KFDA to replace the one in [57].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "points out [27], i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "In this paper, we use the anchor graphs method [35] to speed up this process.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Using the Woodbury identity [21], computing the n\u00d7 n matrix QQ can be transformed into computing the q \u00d7 q matrix QQ, which is more efficient.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "In this paper, the multi-scale LBP feature of each patch is extracted [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": ", where \u03c3(x\u2217) = K\u2217\u2217 \u2212K\u2217K\u0303K\u2217 and f\u0304\u2217(x\u2217) = K\u2217K f\u0302 from Equation (5) [42].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "The source-domain datasets include four different types of datasets as follows: Multi-PIE [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 41, "context": "MORPH [43].", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "If not otherwise specified, the target-domain dataset is the benchmark of face verification as follows: LFW [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 3, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 12, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 45, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 11, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 57, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 18, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 14, "context": "Using it also allows us to compare directly with other existing face verification methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 90, "endOffset": 123}, {"referenceID": 21, "context": "Besides, this dataset provides a large set of relatively unconstrained face images with complex variations as described above, and has proven difficult for automatic face verification methods [23, 28].", "startOffset": 192, "endOffset": 200}, {"referenceID": 26, "context": "Besides, this dataset provides a large set of relatively unconstrained face images with complex variations as described above, and has proven difficult for automatic face verification methods [23, 28].", "startOffset": 192, "endOffset": 200}, {"referenceID": 21, "context": "In all the experiments conducted on LFW, we strictly follow the standard unrestricted protocol of LFW [23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 25, "context": "Following the same setting in [27], the regularization parameter \u03bb in (12) is fixed to 10.", "startOffset": 30, "endOffset": 34}, {"referenceID": 40, "context": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [42], MTGP prediction [6], GPLVM [29], and DGPLVM [57].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [42], MTGP prediction [6], GPLVM [29], and DGPLVM [57].", "startOffset": 123, "endOffset": 126}, {"referenceID": 27, "context": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [42], MTGP prediction [6], GPLVM [29], and DGPLVM [57].", "startOffset": 134, "endOffset": 138}, {"referenceID": 55, "context": "Since our model is based on GPs, it is natural to compare our model with four popular GP models: GPC [42], MTGP prediction [6], GPLVM [29], and DGPLVM [57].", "startOffset": 151, "endOffset": 155}, {"referenceID": 10, "context": "For this paper, we chose three popular representatives: SVM [12], logistic regression (LR) [17], and Adaboost [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "For this paper, we chose three popular representatives: SVM [12], logistic regression (LR) [17], and Adaboost [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "For this paper, we chose three popular representatives: SVM [12], logistic regression (LR) [17], and Adaboost [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "Therefore, we evaluate our method by comparing it with three popular clustering methods: K-means [24], Random Projection (RP) tree [15], and Gaussian Mixture Model (GMM) [47].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Therefore, we evaluate our method by comparing it with three popular clustering methods: K-means [24], Random Projection (RP) tree [15], and Gaussian Mixture Model (GMM) [47].", "startOffset": 131, "endOffset": 135}, {"referenceID": 45, "context": "Therefore, we evaluate our method by comparing it with three popular clustering methods: K-means [24], Random Projection (RP) tree [15], and Gaussian Mixture Model (GMM) [47].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "SVM [12] 83.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "31 LR [17] 81.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "75 Adaboost [18] 82.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "K-means [24] 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "68 RP Tree [15] 85.", "startOffset": 11, "endOffset": 15}, {"referenceID": 45, "context": "34 GMM [47] 86.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 3, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 12, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 45, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 11, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 57, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 18, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 14, "context": "Figure 4 shows the results of this combination compared with state-of-theart methods [9, 5, 14, 47, 13, 59, 1, 20, 16].", "startOffset": 85, "endOffset": 118}, {"referenceID": 7, "context": "33% 5, which is achieved by [9].", "startOffset": 28, "endOffset": 31}, {"referenceID": 26, "context": "53%, cropped) [28].", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "Here, we emphasize that the centers of patches, instead of the accurate and dense facial landmarks like [9], are utilized to extract multi-scale features in our method.", "startOffset": 104, "endOffset": 107}, {"referenceID": 49, "context": "5In fact, [51] and [53] have achieved higher accuracies 97.", "startOffset": 10, "endOffset": 14}, {"referenceID": 51, "context": "5In fact, [51] and [53] have achieved higher accuracies 97.", "startOffset": 19, "endOffset": 23}, {"referenceID": 37, "context": "There is an implicit belief among many psychologists and computer scientists that human face verification abilities are currently beyond existing computer-based face verification algorithms [39].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 52, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 38, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 39, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 36, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 6, "context": "By contrast, there have already been a number of papers comparing human and computer-based face verification performance [2, 54, 40, 41, 38, 8].", "startOffset": 121, "endOffset": 143}, {"referenceID": 36, "context": "As pointed out by [38, 48], humans and computer-based algorithms have different strategies in face verification.", "startOffset": 18, "endOffset": 26}, {"referenceID": 46, "context": "As pointed out by [38, 48], humans and computer-based algorithms have different strategies in face verification.", "startOffset": 18, "endOffset": 26}, {"referenceID": 5, "context": "For example, Bruce [7] found human recognition memory for unfamiliar faces dropped substantially when there were changes in viewing parameters.", "startOffset": 19, "endOffset": 22}, {"referenceID": 26, "context": "It has also been examined in [28], where the human performance drops from 99.", "startOffset": 29, "endOffset": 33}], "year": 2014, "abstractText": "Face verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark [23]. For the first time, the human-level performance in face verification (97.53%) [28] on LFW is surpassed.", "creator": "LaTeX with hyperref package"}}}