{"id": "1405.2434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2014", "title": "Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation", "abstract": "Minimum error rate training (MERT) is a widely used training method for statistical machine translation. A general problem with this approach is that the search space can be easily adapted to a local optimum and the weight set acquired does not correspond to the real distribution of feature functions. In this paper, the selection of the coordinate system (RSS) is introduced into the search algorithm for MERT. Unlike previous approaches, where each dimension corresponds only to an independent feature function, we create multiple coordinate systems by shifting one of the dimensions in a new direction. The basic idea is quite simple, but crucial that the training method of MERT should be based on a coordinate system formed by search guidelines, but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be achieved without any knowledge of foreign languages.", "histories": [["v1", "Sat, 10 May 2014 13:55:03 GMT  (498kb)", "http://arxiv.org/abs/1405.2434v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chen lijiang"], "accepted": false, "id": "1405.2434"}, "pdf": {"name": "1405.2434.pdf", "metadata": {"source": "CRF", "title": "Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation", "authors": ["Lijiang Chen"], "emails": ["ljchen97@126.com"], "sections": [{"heading": null, "text": "Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge."}, {"heading": "1 Introduction", "text": "Statistical machine translation technologies convert a natural language into another natural language automatically by using large-scale corpus-based statistical models. From noise channel model (Brown, 1990), maximum entropy model (Och, 2002) to minimum error rate training model (Och, 2003), statistical machine translation\ndecision-making methods for obtaining translations, increasing the system performance gradually. Meanwhile, the machine translation units start to transit from words (Brown, 1990) to phrases (Koehn, 2003). For example, being the most widely used machine translation system, Moses contains two main technologies which are minimum error rate training model and statistical phrase-based translation (Koehn, 2007).\nMinimum error rate training is a method proposed by Och (2003) to obtain all weights of feature functions according to the translation error rate. Unlike log-linear model based on maximum entropy, the MERT model does not use language model, distortion model and sentence length et al. directly to evaluate translations, but evaluates translations by a standard evaluation method (such as BLEU). Experiments show that when using the same evaluation method to carry out training and evaluating, the evaluation results can get better scores (Och, 2003). Although this method is not a real optimum algorithm, it can adapt translations to different evaluation methods. So it is more beneficial than the traditional method to obtain higher scores.\nAssuming the translation e of the source\nlanguage sentence f has a reference\nsentence r . The function ),( erE indicate\nthe number of errors received comparing r with e . Multiple errors of sentences can be cumulative:\n \n S\ns\nss\nss erEerE\n1\n11 ),(),(\n(1)\nThe goal of minimum error rate training is to optimize the parameters, so as to obtain the smallest number of errors on the\ndevelopment set Sf1 with given reference\ntranslations Se1\u02c6 and a set of N-best\ncandidate translations },...,{ ,1, Ksss eeC \nfor each sentence sf .\n        \nS\ns\nM\nss I ferE I i 1 11 ));(\u02c6,(minarg \u02c6 \n (2)\nwith\n        \nM\nm\nsmm Ce\nM\ns fehfe d 1 1 )|(maxarg);(\u02c6 \n(3)\nA standard algorithm for optimizing the parameters of eq. 3 is Koehn\u2019s Coordinate Descent (KCD). When training parameters, greedy algorithm is used to adjust the parameters of a particular dimension by turn, while other dimensions of the parameters are constant.\nAlthough KCD can achieve a global optimum on one dimension each time, it can not guarantee that the entire feature set converge to a global optimum in all dimensions (Moore, 2008). In this paper, we introduce coordinate system selection (RSS)\ninto KCD, so that we can change search directions in order to get better weight set to reflect the real distribution of feature functions. For closed test results and open test results 1 are often consistent, closed test results are used to choose better coordinate systems (RS) for the open test.\nThe goal of this paper is to investigate the role of coordinate system selection to optimize the parameters for MERT. Section 2 introduces Koehn\u2019s Coordinate Descent for minimum error training. Section 3 elaborates RSS-MERT including the theoretical basis of RSS and search progress. Section 4 gives the specific experimental results of three kinds of RSS. Section 5 presents the related research in recent years about improving MERT. Section 6 concludes the experiment, and gives future research directions of RSS."}, {"heading": "2 Koehn\u2019s Coordinate Descent (KCD)", "text": "KCD is a variant of coordinate descent algorithm that, at each iteration, moves along the coordinate which allows for the most progress in the objective (Cer, 2008). Liu (2008) gives the progress steps of KCD (See Algorithm 1)."}, {"heading": "3 Coordinate systems Selection", "text": "Model"}, {"heading": "3.1 KCD Independence Assumption", "text": "In KCD algorithm the search space is formed by a number of dimensions that correspond to all the feature functions. It tries to find a better scoring point in the parameter space by optimizing one parameter along one direction while keeping other parameters fixed (och, 2003).\n1 We use the tuning set to do the closed test and the test set to do the open test.\n         \nS\ns\nK\nk\nks\nM\nskss efeerE I i 1 1 ,1, )),;(\u02c6(),(minarg  \nAlgotithm 1 Koehn\u2019s Coordinate Descent method to find the minimum error rate\n1 iter=1;lasterror=0;newerror=1;epsilon=0.001\n2 While (iter<Maxiter and abs (lasterror-newerror) > epsilon)\n3 for d=1 to M // for each dimension\n4 for s=1 to S // for each sentence in training corpus\n5 Compute critical value and delta error\n6 endfor\n7 merge all critical value and sort them\n8 compute error within each pair of non-identical boundaries 9 select the weight as the midpoint of the interval corresponding to the lowest error 10 end for\n11 iter++\n12 lasterror=newerror\n13 newerror=lowest error\n14 end while\nGiven that all dimensions form a coordinate system, each dimension of the coordinate system corresponds to one and only one feature function. As each dimension of the coordinate system in the search process is independent, one to one correspondence coordinate system assumes that every two feature functions are independent, namely makes the assumption of independence.\n\u2026\u2026\nHowever, not all features functions are\nindependent. Some feature functions such as language model and lexical length model, positive phrase translation model and positive lexical weight model are all related in machine translation systems."}, {"heading": "3.2 Coordinate systems", "text": "In the KCD approach, feature\nfunctions are directly used by the iterative search process. When searching for a better scoring point, all the search directions do not interfere with each other. However, most of the feature functions in machine translation system are interrelated. In fact, there is a default insufficient assumption that each search direction corresponds to one feature function.\nAs alternative to KCD, in RSS-MERT,\none direction corresponding to one feature function is only a basic form of the coordinate system. Coordinate system (RS) is consisted of a number of search directions which can be moved from one feature function near to another. This improved method helps to break the original assumption of feature function independence, and enhance the links between context-sensitive feature functions in the search process to get better weight set that is closer to the real distribution of feature functions.\nDimension A )|( ef\nDimension B\nDimension C\nDimension D\n)(eP\n)|( efLex\n)(elength\nFigure 1 compares the difference between KCD and RSS-KCD.\nKCD\n\u2026\u2026 RSS-KCD\nFigure 1 RSS-KCD adds a coordinate system to KCD"}, {"heading": "3.3 RSS-MERT Progress", "text": "RSS-MERT algorithm is generated by\nadding selected coordinate systems to KCD. The association between two functions will change according to some objective factors (such as training corpus). Therefore, in RSS-MERT process, we need the results of closed test to adjust the coordinate system. The premise that we select coordinate\nsystem according to the closed test results is the consistency between the closed and open test results. We change coordinate system by moving search directions of three context-sensitive feature functions. The experiments show that it is feasible to select coordinate systems by the closed test results. Algorithm 2 presents RSS-MERT progress in details.\nFeature Function Set\nRS1\nSearch Direction\nSelection\nMERT\nRS2 RS3\nAlgorithm 2 Adding Coordinate systems to Koehn\u2019s Coordinate Descent Method\n1 Select two associated feature functions A and B; 2 for  \u2190 -1 to 1 step 0.1 (initial value, final value and step size can be adjusted according to the actual\nrequire)\n3 Initialize the coordinate system: let each dimension corresponds to one feature function\n4 rotate dimension A to dimension B by arctg to form a new coordinate system\n5 iter=1;lasterror=0;newerror=1;epsilon=0.001\n6 While (iter<Maxiter and abs(lasterror-newerror)>epsilon)\n7 for d=1 to M // for each dimension\n8 for s=1 to S // for each sentence in training corpus\n9 Compute critical value and delta error\n10 endfor\n11 merge all critical value and sort them\n12 compute error within each pair of non-identical boundaries 13 select the weight as the midpoint of the interval corresponding to the lowest error 14 end for\n15 iter++\n16 lasterror=newerror\n17 newerror=lowest error\n18 end while\n19 do closed test and open test for the current model\n20 end for 21 Select the best "}, {"heading": "3.4 Establishing Coordinate system", "text": "Coordinate system is established based\non search directions. Each search direction can be moved from one feature function to another. In order to add function associating factors, we introduce a parameter  based\non the original function )|( sfeh ,\ncombining with another feature function\n)|( sfeh to form a dynamic search\ndirection )|( sfeD :\n(4)\nDynamic search direction is\ndetermined by three parts: the main feature\nfunction )|( sfeh , the subsidiary feature\nfunction )|( sfeh , and the parameter .\n)|( sfeh and )|( sfeh are two\ninterrelated feature functions, and the relationships between them are determined by the parameter .\nIn the original system, each search\ndirection is in accord with one of the feature function. By introducing the parameters , the search direction can be moved to another feature, so as to create a new coordinate system (see Figure 3).\nThe establishment of dynamic search\ndirections makes use of a parameter to consider the intrinsic link between feature functions. By changing the parameter, the machine translation system can construct\n)|()|()|( sss fehfehfeD  \nseveral coordinate systems and use the error rate to adjust the feature weight proportion on different RS. The local optimum point will change while search direction is moved, thus we have the chance to obtain a weight set of translation models with higher evaluation results."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Baseline Translation System", "text": "We use a free and complete machine translation decoding tool Moses 2 as the baseline system. Moses is a phrase-based statistical machine translation system developed by Philipp Koehn et al. The whole system is written in C++ language. From training to decoding, the code is completely open source, and can be run on both Linux and Windows platforms. Moses is no longer confined to the phrase-based translation table, but is a translation model that can integrate part of speech, word type, word stemming and other features. Moses uses a hybrid network\n2 http://www.statmt.org/moses/\ndecoding technology that allows a variety of possible input forms, such as the output of named entity or speech recognition. In addition, when using Moses we must also install the language model tool SRILM 3 and word alignment tool GIZA++ 4 ."}, {"heading": "4.2 Corpus Preparation", "text": "Our data come from two kinds of parallel corpus. One is the corpus of aligned sentences in Chinese and English collected by Computing Technology Institute of the Chinese Academy of Science. It including 11 fields such as arts, economy, legal, environment, life, politics, science etc, which are all sentence-level aligned. Another is 863 Chinese-English sentence aligned corpus."}, {"heading": "4.3 Training and Development Data", "text": "Chinese and English corpus used in the translation model has 86,933 sentences. Additionally, there are 187,014 translation dictionary entries. The English corpus used by language model is consisted by 251909 sentences. The Chinese-English test corpus is provided by the Fourth National Machine Translation Conference (CWMT2008). It has a total of 1006 Chinese sentences. Each sentence corresponds to 4 English translations. For parameter adjusting, we use Chinese-English translation corpus offered by NIST2008. It has a total of 691 Chinese sentences. Each Chinese sentence corresponds to 4 English translations.\n3 ftp://ftp.speech.sri.com/pub/people/stolcke/srilm/srilm -1.5.7.tar.gz 4 http://code.google.com/p/giza-pp/downloads/list"}, {"heading": "4.4 Results", "text": "Figures 4 and 5 present the closed and\nopen test results respectively when the search direction is moved from language model to distortion model and n-gram is 6. The results show that when the parameter is greater than 0, the open test and closed test results are significantly better than when the parameter is less than 0. Closed test and open test are approximately consistenct: closed test results reach a highest point when the parameter is 0.3, while the open test reach a highest point when the parameter is between 0.1 and 0.3. At the time the parameters is 0.3, the BLEU score of the closed test is 15.80, and of the open test is 18.14, significantly higher than the baseline of 15.68 and 17.86.\nTable 1 and table 2 compare the best\nresults of three different kinds of search direction movements with the baseline system when n-gram is 5 and 6 respectively. Three movements including (1) move from language model direction to distortion model direction (2) move from language model direction to length model direction (3) move from distortion model direction to length model direction. As the closed test and open test evaluation results have a moderate consistency, at the best result point in the closed test, open test result also improves significantly than the baseline system. Experimental results show that RSS-MERT method can effectively overcome the limitation that the iterative method always converges to a local optimum, and make the open test and closed test results improved significantly.\nbaseline lm to wp dm to wp lm to dm\n -0.7 +0.7 +0.1 open test 17.75 17.99 17.92 17.98 close test 15.52 15.69 15.61 15.73\nbaseline system when n-gram is 6\nIn addition, the two tables show that the experiment results of 6-gram is significantly better than that of 5-gram, which is consistent with the baseline system. When moving from language model direction to distortion model direction, the system achieved the best results: the BLEU score of closed test is 15.80 and of open test is 18.14.\nWe also try to move two search directions: one is from language model to distortion model and the other is from distortion model to length model. The parameters are 0.3 and 0.1 respectively. The results are showed below.\nIt can be seen from figure 6 that while transforming the search direction of the coordinate system, there is also a corresponding movement of the weights of two related features in the coordinate system. Therefore, changing the coordinate system is actually equivalent to moving the weight values in the coordinate system. As a whole, in RSS-MERT, by changing the search direction of the coordinate system and making the minimum error rate point moved, we can avoid the training progress converge to a local optimum."}, {"heading": "5 Related Research", "text": "Minimum error training (MERT) is a\nsubject that still needs to be explored further. Cer (2008) proposes regularization and random search to ease the problem of local optimum. They select random search directions based on Powell method (Press, 2007) to overcome the limitation that the assumption it uses to build up the diagonal search directions do not hold in the present context . In addition, by checking and combining the adjacent peak in a fixed window of search directions, they solve the problem that one optimal value is surrounded by several bad objective function values in minimum error rate training. Their results show that these two improved methods can significantly improve the system performance compared to traditional Powell algorithm and coordinate descent method. Machery (2008) replaces word n-best list by lattices to improve the estimates of the expected translation score. In the algorithm, they efficiently construct and represent the exact error surface of all the translations which are encoded in a phrase lattice. They use this novel method to train the feature function weights for a phrase-based statistical machine translation system. Experiments show that they obtain significant improved BLEU score over N-best MERT.\nThe optimal solution point in MERT is often related to the initial weights of feature functions. Some current studies focus on the choice of initial weights. In order to avoid getting into a bad local optimum, Koehn et al. (2007) start from different initial weights, and use multiple optimizing search methods on each set of extended hypotheses in the translation. Moore et al.(2008) use two methods to adjust the initial weight for MERT: (1) randomly select initial weight according to uniform distribution; (2) use random walk method that select the initial weight according to the local optimum\nwhich the previous iteration converges to. Their experiments also make the BLEU evaluation results of translations improved significantly."}, {"heading": "6 Conclusion", "text": "Statistical machine translation systems\noften contain a number of feature functions.\nSo the problem of correlated feature\nfunctions is critical that any statistical\nmachine translation system must face.\nHowever, in Koehn coordinate descent\nmethod, each search direction corresponds\nto one feature function, ignoring the\ncorrelation between the feature functions. In\nthis paper we change the search directions\nto form several coordinate systems for KCD\nso that we can get better weight points\nwhich are closer to the real distribution of\nfeature functions.\nThe weights of minimum error rate\ntraining tend to converge to a local optimum. Experiments show that the changing trends of feature function weights are the same as the moving trends of search directions. By making use of the correlation between the functions and selecting coordinate systems, we can obtain multiple weight sets that satisfy minimum error rate. For different weight sets, the closed test and open test results of BLEU evaluation are often identical. Therefore, we do not need to change the initial weights of training, instead we only need to select the best model for the closed test, so as to improve the open test results. This method overcomes the limitation of the local optimum in MERT and can get better results on the overall situation.\nWithout increasing the system\u2019s execute time and storage space, this method is simple and convenient to be\nwidely used. Moreover, the SMT performance has a chance to be significantly improved by finding better coordinate systems. In the future, we will extend RSS-MERT methods to other models, such as phrase translation model and lexical weight model. In addition, we will also test on a larger corpus, hoping to expand such a simple way to the practical aspects."}, {"heading": "Acknowledgments", "text": "This work was supported by 2010 Jiangsu Graduate Research Innovation Plan CX10B_063R and 2010 Outstanding Ph.D. Dissertation Breeding Program in Nanjing Normal University 2010bs0006.\nReference\nDaniel Cer, Daniel Jurafsky and Christopher D. Manning. 2008. Regularization and Search for Minimum Error Rate Training.In: Preceedings of the Third Workshop on Statistical Machine Translation, pages:26-34.\nFranz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. 40th Annual Meeting of the Association for Computational Linguistics, pages 295-302, Philadelphia, Penn, July.\nFranz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. 41th Annual Meeting of the Association for Computational Linguistics, pages 160-167. Sapporo, Japan, July.\nHideharu Nakajima, PHirofumi Yamamoto and PTaro Watanabe. 2002. Language model adaptation with additional text generated by machine translation. In COLING'02: The 19th\nInt. Conf. on Computational linguistics, pages\n716\u2013722, Taipei, Taiwan, August.\nNicola Bertoldi, Richard Zens, Marcello Federico and Wade Shen, Efficient Speech Translation through Confusion Network Decoding. 2008. IEEE Transactions on Audio, Speech, and Language Processing, 16,(9): 1696-1705.\nPeter F. Brown, Jennifer C. Lai and Robert L. Mercer. 1991. Aligning Sentence in Parallel Corpora. In Proc. 29th Annual Meeting of the\nAssociation for Computational Linguistics pages 169-176, Berkeley, California, June.\nPeter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical Approach to Machine Translation. Computational Linguistics, 16(2):79-85.\nPhilipp Koehn and Hieu Hoang. Factored Translation Models. 2007. Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), page: 868-876, Prague, Czech Republic, June.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. Moses: Open Source Toolkit for Statistical Machine Translation. 2007. In Proc. 45th Annual Meeting\nof the Association for Computational Linguistics, Demo and Poster Sessions, page 177-180, Prague, Czech Republic, June.\nRichard Zens and Hermann Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Proc. 41th Annual Meeting of the Association for Computational Linguistics, pages 144-151. Sapporo, Japan, July.\nRichard Zens and Hermann Ney. Efficient\nPhrase-table Representation for Machine Translation with Applications to Online MT and Speech Translation. 2007. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 492-499, Rochester, NY, April.\nRobert C. Moore and Chris Quirk. 2008. Random Restarts in Minimum Error Rate Training for Statistical Machine Translation. In\nCOLING\u2019 08: The 19th Int. Conf. on Computational linguistics, pages 585\u2013592, Manchester, UK, August.\nShankar Kumar and William Byrne. 2005. Local Phrase Reordering Models for Statistical Machine Translation. In Proceedings of HLT-EMNLP\u201905, pages 161-168, Vancouver, Canada, October.\nSpence Green, Michel Galley, Christopher D. Manning. 2010. Improved Models of Distortion\nCost for Statistical Machine Translation. In Proceedings of HLT-NAACL'2010. pages 867-875, Los Angeles, California, June.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In Proc. 45th Annual Meeting of the Association for Computational Linguistics, pages 858-867, Prague, Czech Republic, June..\nWilliam H. Press\uff0cSaul A. Teukolsky\uff0cWilliam T. Vetterling \uff0c Brian P. Flannery. 2007. Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press.\nWolfgang Macherey,Franz Josef Och,Ignacio Thayer,Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural\nLanguage Processing, pages 725-734, Honolulu,\nHawaii, October.\nYaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proc. 44th Annual Meeting of the Association for Computational Linguistics, pages 529\u2013536, Sydney, Australia, June.\nYeyi Wang and Alex Waibel. 1997. Decoding Algorithm in Statistical Machine Translation. In Proc. 35th Annual Meeting of the Association for Computational Linguistics, pages: 366-372,\nMadrid, Spain, July."}], "references": [{"title": "Regularization and Search for Minimum Error Rate Training.In", "author": ["Daniel Cer", "Daniel Jurafsky", "Christopher D. Manning"], "venue": "Preceedings of the Third Workshop on Statistical Machine Translation,", "citeRegEx": "Cer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cer et al\\.", "year": 2008}, {"title": "Minimum Error Rate", "author": ["Franz Josef Och"], "venue": null, "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "IEEE Transactions on Audio, Speech, and Language Processing, 16,(9): 1696-1705", "author": ["Decoding."], "venue": "Peter F. Brown, Jennifer C. Lai and Robert L. Mercer. 1991. Aligning Sentence in Parallel", "citeRegEx": "Decoding.,? 2008", "shortCiteRegEx": "Decoding.", "year": 2008}, {"title": "A statistical Approach to Machine Translation", "author": ["Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin."], "venue": "Computational Linguistics, 16(2):79-85. Philipp Koehn and Hieu Hoang. Factored", "citeRegEx": "Jelinek et al\\.,? 1990", "shortCiteRegEx": "Jelinek et al\\.", "year": 1990}, {"title": "Conference on Empirical Methods in Natural Language Processing (EMNLP), page: 868-876, Prague, Czech Republic, June", "author": ["Translation Models."], "venue": "Philipp Koehn, Hieu Hoang, Alexandra Birch,", "citeRegEx": "Models.,? 2007", "shortCiteRegEx": "Models.", "year": 2007}, {"title": "In Proc", "author": ["Translation."], "venue": "45th Annual Meeting of the Association for Computational Linguistics, Demo and Poster Sessions, page 177-180, Prague, Czech Republic, June. Richard Zens and Hermann Ney. 2003. A", "citeRegEx": "Translation.,? 2007", "shortCiteRegEx": "Translation.", "year": 2007}, {"title": "Efficient Phrase-table Representation for Machine Translation with Applications to Online MT and Speech Translation", "author": ["Richard Zens", "Hermann Ney"], "venue": "In Proceedings of the Human Language Technology Conference", "citeRegEx": "Zens and Ney.,? \\Q2007\\E", "shortCiteRegEx": "Zens and Ney.", "year": 2007}, {"title": "Training for Statistical Machine Translation. In COLING", "author": ["UK Manchester", "August. Shankar Kumar", "William Byrne"], "venue": "The 19th Int. Conf. on Computational linguistics,", "citeRegEx": "Manchester et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Manchester et al\\.", "year": 2005}, {"title": "Improved Models of Distortion Cost for Statistical Machine Translation", "author": ["Manning."], "venue": "Proceedings of HLT-NAACL'2010. pages 867-875, Los Angeles, California, June. Thorsten Brants, Ashok C. Popat, Peng Xu,", "citeRegEx": "Manning.,? 2010", "shortCiteRegEx": "Manning.", "year": 2010}, {"title": "Large Language Models in Machine Translation", "author": ["Franz J. Och", "Jeffrey Dean."], "venue": "Proc. 45th Annual Meeting of the Association for Computational Linguistics, pages 858-867, Prague, Czech Republic, June..", "citeRegEx": "Och and Dean.,? 2007", "shortCiteRegEx": "Och and Dean.", "year": 2007}, {"title": "Numerical Recipes: The Art of Scientific Computing (3rd ed.)", "author": ["William H. Press", "Saul A. Teukolsky", "William T. Vetterling", "Brian P. Flannery"], "venue": null, "citeRegEx": "Press et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Press et al\\.", "year": 2007}, {"title": "Lattice-based Minimum Error Rate Training for Statistical Machine Translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural", "citeRegEx": "Macherey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Decoding Algorithm in Statistical Machine Translation", "author": ["Sydney", "Australia", "June. Yeyi Wang", "Alex Waibel"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Sydney et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sydney et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "From noise channel model (Brown, 1990), maximum entropy model (Och, 2002) to minimum error rate training model (Och, 2003), statistical machine translation systems continuously revise decision-making methods for obtaining translations, increasing the system performance gradually.", "startOffset": 111, "endOffset": 122}, {"referenceID": 1, "context": "Experiments show that when using the same evaluation method to carry out training and evaluating, the evaluation results can get better scores (Och, 2003).", "startOffset": 143, "endOffset": 154}, {"referenceID": 1, "context": "From noise channel model (Brown, 1990), maximum entropy model (Och, 2002) to minimum error rate training model (Och, 2003), statistical machine translation systems continuously revise decision-making methods for obtaining translations, increasing the system performance gradually. Meanwhile, the machine translation units start to transit from words (Brown, 1990) to phrases (Koehn, 2003). For example, being the most widely used machine translation system, Moses contains two main technologies which are minimum error rate training model and statistical phrase-based translation (Koehn, 2007). Minimum error rate training is a method proposed by Och (2003) to obtain all weights of feature functions according to the translation error rate.", "startOffset": 63, "endOffset": 658}, {"referenceID": 5, "context": "Machery (2008) replaces word n-best list by lattices to improve the estimates of the expected translation score. In the algorithm, they efficiently construct and represent the exact error surface of all the translations which are encoded in a phrase lattice. They use this novel method to train the feature function weights for a phrase-based statistical machine translation system. Experiments show that they obtain significant improved BLEU score over N-best MERT. The optimal solution point in MERT is often related to the initial weights of feature functions. Some current studies focus on the choice of initial weights. In order to avoid getting into a bad local optimum, Koehn et al. (2007) start from different initial weights, and use multiple optimizing search methods on each set of extended hypotheses in the translation.", "startOffset": 94, "endOffset": 697}, {"referenceID": 5, "context": "Machery (2008) replaces word n-best list by lattices to improve the estimates of the expected translation score. In the algorithm, they efficiently construct and represent the exact error surface of all the translations which are encoded in a phrase lattice. They use this novel method to train the feature function weights for a phrase-based statistical machine translation system. Experiments show that they obtain significant improved BLEU score over N-best MERT. The optimal solution point in MERT is often related to the initial weights of feature functions. Some current studies focus on the choice of initial weights. In order to avoid getting into a bad local optimum, Koehn et al. (2007) start from different initial weights, and use multiple optimizing search methods on each set of extended hypotheses in the translation. Moore et al.(2008) use two methods to adjust the initial weight for MERT: (1) randomly select initial weight according to uniform distribution; (2) use random walk method that select the initial weight according to the local optimum which the previous iteration converges to.", "startOffset": 94, "endOffset": 852}], "year": 2014, "abstractText": "Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge.", "creator": "Microsoft\u00ae Office Word 2007"}}}