{"id": "1405.3487", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "COCOpf: An Algorithm Portfolio Framework", "abstract": "Algorithm portfolios are a strategy to compose multiple heuristic algorithms, each of which is suited to a different class of problems, within a single generic approach that selects the most appropriate algorithm for each input. Recently, this approach has gained popularity mainly for solving combinatorial problems, but optimization applications are still emerging. The COCO platform of the BBOB workshop series is the current standard method for measuring the performance of continuous black box optimization algorithms.", "histories": [["v1", "Wed, 14 May 2014 13:26:57 GMT  (1127kb)", "http://arxiv.org/abs/1405.3487v1", "POSTER2014. arXiv admin note: text overlap witharXiv:1206.5780by other authors without attribution"]], "COMMENTS": "POSTER2014. arXiv admin note: text overlap witharXiv:1206.5780by other authors without attribution", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["petr baudi\\v{s}"], "accepted": false, "id": "1405.3487"}, "pdf": {"name": "1405.3487.pdf", "metadata": {"source": "CRF", "title": "COCOpf: An Algorithm Portfolio Framework", "authors": ["Petr Baudi\u0161"], "emails": ["pasky@ucw.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n34 87\nv1 [\ncs .A\nI] 1\n4 M\nay 2\n01 4\nAs an extension to the COCO platform, we present the Python-based COCOpf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies. In our framework, we focus on black-box algorithm portfolio and online adaptive selection. As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies. We confirm that even a naive selection strategy can provide improved performance across problem classes.\nKeywords Algorithm portfolio, continuous black-box optimization, hyperheuristics, software engineering.\n1. Introduction The problem of Algorithm Selection is not new [16], but has only recently gained traction in particular in the field of combinatoric problem solvers. [9] There are multiple ways to approach the issue, in our work we adopt the abstraction of algorithm portfolios. [3] The basic idea is that we have a set of heuristic algorithms at our disposal (each suitable for a different class of problem instances), and for each problem instance on input, we apply a selection strategy to pick an algorithm from this portfolio and apply it on the problem. We can perform the selection once or along a fixed schedule (offline selection) based on features of the problem instance, or in multiple rounds first exploring the suitability of portfolio members, then allocating time to them based on their previous performance. In successive rounds,\nalgorithms can be either resumed from their previous state or restarted; we take the approach of resuming them, reserving the restart schedule to be an internal matter of each algorithm.1\nApplying algorithm portfolios on continuous black-box optimization is still a fresh area of research.2 The main results so far lie either in population methods, combining a variety of genetic algorithms together, e.g. the AMALGAMSO algorithm [17], or in offline methods based chiefly on exploratory landscape analysis [11].\nIn our work, we aim to allow the more traditional optimization methods to enter the mix as we regard the algorithms as black-box, i.e. we completely avoid modifying their inner working and we simply call them to retrieve a single result. This allows state-of-art methods to be combined in a single portfolio easily and offers easy implementation of whatever selection strategy desired as the existing algorithm modules can be simply reused without modification. Furthermore, we focus on online adaptive selection that selects algorithms based on their performance so far and does not require possibly expensive feature extraction and training.\nThe currently accepted de-facto standard for benchmarking optimization methods is the COmparing Continuous Optimisers COCO platform [6] [5] that was originally developed for the BBOB workshop series. It provides the infrastructure, glue code for both running experiments and preparing high quality figures, a set of common reference results and the code for a set of benchmark functions. Therefore, we chose to add algorithm portfolio support to this platform. The glue code is available in multiple languages, we extended the Python implementation. We are in part motivated by the convenient availability of optimization algorithms distributed along the popular SciPy library [8].\nWe have two options when testing algorithm selection methods in practice: either using only previously published data on preformance of individual algorithms, or re-running optimization algorithms from scratch under the guide of a particular selection method. The former approach is certainly an easier option while benchmarking, nevertheless we\n1That is, sometimes resumption will simply continue the algorithm to a next iteration, on reaching a local optimum the algorithm may attempt to escape it and failing that, a complete restart may occur.\n2Especially if we do not consider applying related methods to individual operator selection within genetic algorithms used for optimization.\nopted for the latter model so that we can also readily apply all code on practical function optimization aside of mere benchmarking. Apart of that, the available performance data does not include information about algorithm iterations, listingonly individual function evaluations. That makes it unclear when is a good time to switch. Lastly, the re-running model allows migration of intermediate solutions between algorithms as a simple extension.\nIn Section 2, we introduce the COCOpf software, explaining its architecture, general API and a few interesting implementation points. In Section 3, we establish a reference algorithm portfolio, benchmarking its individual members and two algorithm selection methods implemented in the framework. We present the benchmark results in Section 4 and conclude with a results summary and outline of a few directions of future work in Section 5.\n2. COCOpf Framework Our framework has been implemented as a Python module that can be imported by user applications. The framework provides the means to run the experiment on a given set of benchmark scenarios, maintains a population of algorithm instances and provides a complex wrapper to individual algorithms that uses an existing per-iteration callback mechanism to suspend and resume execution of the algorithm after a single iteration. Algorithm wrappers for the SciPy optimization methods and the reference implementation of CMA are available out-of-the box as well as two example algorithm selection strategies that are benchmarked below. The framework is publicly available as free software under the permissible MIT licence at https://github.com/pasky/cocopf.\nIn design of such a framework, a choice presents itself \u2014 to either build a generic main program which allows an experiment to choose a certain configuration and plug in custom callbacks, or to prepare a set of \u201dlego bricks\u201d that replace common parts of the main program but letting each experiment provide a separate main program. We opted for the latter; it reduces encapsulation and requires slightly more repetitive code, but it allows a much greater degree of flexibility encouraging diverse experiments.\n2.1. Experiment Wrapper\nThe Experiment class wraps up the common code that sets up the COCO benchmark, initializing the logs, sets of dimensions and function instances etc. It also provides an iterator that in turn returns individual function instances to be minimized, and a pretty-printing progress report function. After an experiment is finished, the standard BBOB postprocessing tools may be used to examine and plot the performance data. An example of usage is shown in Fig. 1.\n2.2. Minimization Wrappers\nAll algorithms are executed via the MinimizeMethod class. Its primary purpose is to produce a callable Python object from a user-provided string name of algorithm. By default, it calls the scipy.optimize.minimize API with unchanged method name and wraps it in the scipy.optimize.basinhopping API. However, it is expected the user will create a subclass adding custom optimizers; a demo subclass adding the CMA optimization is included.3\nWe were presented with a software engineering problem of how to suspend, resume and cancel the algorithms between iterations without requiring any modification to the algorithm code, while all we have is a callback executed after each iteration. Our answer is the MinimizeStepping class which creates a separate thread for each algorithm instance and uses the standard Python Queue mechanism to pause execution of each thread within the callback code based on instructions from the main thread. At any time, only a single thread is running. To cancel execution (when time is up or a solution is found) we pass a special message through the queue that raises an exception within the callback, eventually cancelling the whole algorithm and thread orderly. An \u201cmlog\u201d facility is provided to log function evaluation status at the end of each algorithm iteration.\n2.3. Portfolio Population\nThe Population class maintains a population of algorithms drawn from the algorithm portfolio; each population member corresponds to a tuple of method, solution coordinates, current value and number of iterations invested. Usually, the size of population is static, but dynamically adding new members is also supported.\nIn a typical case, the population size is the same as the portfolio size, meaning that only a single solution is tracked for each algorithm. However, in principle this may not be the case. In fact, we can also make a population from solutions all using a single algorithm \u2014 this special case corresponds\n3The custom optimizers may also legitimately want to use the Basin Hopping algorithm (see Sec. 3.1), but SciPy allows using custom minimization routines with the basinhopping API only starting with version 0.14.\nto the problem of finding an efficient restart or resume schedule for a particular algorithm.\n3. Benchmarking Simple Portfolios To showcase COCOpf, we composed a reference portfolio from readily available open source Python optimizers and implemented two selection strategies.\n3.1. Reference Algorithm Portfolio\nAs a testbed algorithm portfolio, we have chosen the six stock minimizers provided by SciPy v0.13 [8] that are available for direct use:4\n\u2022 Nelder-Mead uses the Simplex algorithm. [12] [19]\n\u2022 Powell implements a tweaked version of the Powell\u2019s conjugate direction method. [14]\n\u2022 CG is a nonlinear conjugate gradient method using the Fletcher-Reeves method. [13, p. 120\u2013122]\n\u2022 BFGS uses the quasi-Newton method of Broyden, Fletcher, Goldfarb and Shanno [13, p.136] with numerically approximated derivations.\n\u2022 L-BFGS-B is a limited-memory variant of BFGS with box constraints. [1] [20]\n\u2022 SLSQP implements Sequential Least SQuares Programming [10] with inequalities as the box constraints.\nAll of these SciPy minimizers perform local minimization; to achieve global optimization, we wrap them in the \u201cBasin Hopping\u201d restart strategy [18] (also provided by SciPy), which is conceptually similar to Simulated Annealing with a fixed temperature. The strategy performs the hopping step 100 times (the default) before restarting from scratch. On restart, the starting position is determined uniformly randomly in the [\u22125,5]k space.\nTo improve the portfolio performance on more difficult functions, we also included the popular CMA algorithm [4] (in the \u201cproduction\u201c version of its official reference Python implementation), i.e. a genetic algorithm that follows the Covariance Matrix Adaptation evolution strategy.\nIn accordance with our black-box approach, we kept all the algorithms at their default settings and did not tune any of their parameters. Where applicable, we set the box constraints to [\u22126,6]k bounds.5\n4Other minimizers are also available in SciPy, but they are either unsuitable for black-box minimization or do not provide a callback mechanism to allow pausing execution between individual iterations.\n5The optimum is guaranteed to be in [\u22125,5]k , but some minimizers may behave erraticaly when the optimum is near to or at the bounds.\nEach algorithm provides a callback method that is invoked after a single iteration of the algorithm; we suspend the algorithm after each callback invocation.\nThe reference portfolio is not perfectly balanced and certainly would benefit from algorithms with more diverse performance. Our motivation for going with these is that the implementations are free software, trivial to obtain and therefore representing a very low barrier of entry. Also, a SciPy-based portfolio gives valuable feedback to SciPy users regarding the best algorithm to use on a certain function, and provides a gateway to introduce an algorithm portfolio selection strategy to SciPy itself in the future.\n3.2. Algorithm Selection Strategies\nWhile the core of our followup work involves researching various algorithm selection strategies, in the context of presentation of the COCOpf framework, we decided to demonstrate just the performance of two simple simplest methods whose source code is also a part of the framework:\n\u2022 The UNIF strategy performs a number of rounds where in every round, a uniformly randomly selected algorithm is run for a single iteration.\n\u2022 The EG50 strategy performs a number of rounds where in every round, an epsilon-greedy policy with \u03b5 = 0.5 selects the algorithm to run for one iteration. That is, the algorithm with the currently best solution is run with p = 0.5 and a randomly chosen algorithm is run otherwise.\n4. Results Results from experiments according to [5] on the benchmark functions given in [2, 7] are presented in Figures 2, 3 and 4.6 The expected running time (ERT) used in the figures depends on a given target function value, ft = fopt+\u2206f , and is computed over all relevant trials as the number of function evaluations executed during each trial while the best function value did not reach ft, summed over all trials and divided by the number of trials that actually reached ft [5, 15].\nIf we review the results in context of choosing a best (black-box function) SciPy optimizer, we can see that there is no single right choice of a SciPy optimizer that works on all function instances and computational budgets. We can observe that Powell generally works best for separable functions while BFGS and SLSQP are good choices for many functions, especially in high dimensions. For non-separable functions, CMA sooner or later overtakes the SciPy optimizers but often with a slow start (which is relevant in case a\n6The full results dataset is available at http://pasky.or.cz/sci/cocopf-scipy.\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n1 Sphere\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n2 Ellipsoid separable\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n3 Rastrigin separable\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n4 Skew Rastrigin-Bueche separ\n2 3 5 10 20 40 0\n1\n2\n3\n4\nftarget=1e-08\n5 Linear slope\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n6 Attractive sector\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n7 Step-ellipsoid\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n8 Rosenbrock original\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n9 Rosenbrock rotated\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n10 Ellipsoid\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n11 Discus\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n12 Bent cigar\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n13 Sharp ridge\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n14 Sum of different powers\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n15 Rastrigin\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n16 Weierstrass\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n17 Schaffer F7, condition 10\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n18 Schaffer F7, condition 1000\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n19 Griewank-Rosenbrock F8F2\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\nftarget=1e-08\n20 Schwefel x*sin(x)\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n21 Gallagher 101 peaks\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n22 Gallagher 21 peaks\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n23 Katsuuras\n2 3 5 10 20 40 0\n1\n2\n3\n4\n5\n6\n7\nftarget=1e-08\n24 Lunacek bi-Rastrigin\nFig.2. Expected running time divided by dimension versus dimension. Expected running time (ERT in number of f -evaluations) divided by dimension for target function value 10\u22128 as log10 values versus dimension. Light symbols give the maximum number of function evaluations from the longest trial divided by dimension. Horizontal lines give linear scaling, slanted dotted lines give quadratic scaling. Black stars indicate statistically better result compared to all other algorithms with p < 0.01 and Bonferroni correction number of dimensions (six). Legend: \u25cb:Nelder-Mead, \u25bd:Powell, \u22c6:CG, \u25fb:BFGS, \u25b3:L-BFGS-B, \u2662:SLSQP, 9:CMA, D:UNIF, 7:EG50\nseparable fcts moderate fcts\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nCG\nBFGS\nCMA\nNelder-Mead\nL-BFGS-B\nSLSQP\nUNIF\nPowell\nEG50\nbest 2009f1-5,5-D best 2009\nEG50\nPowell\nUNIF\nSLSQP\nL-BFGSB\nNelder\nCMA\nBFGS\nCG 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nL-BFGS-B\nCG\nSLSQP\nBFGS\nPowell\nNelder-Mead\nUNIF\nEG50\nCMA\nbest 2009f6-9,5-D best 2009\nC A\nEG50\nUNIF\nNelder\nPowell\nBFGS\nSLSQP\nCG\nL-BFGSB\nill-conditioned fcts multi-modal fcts\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nPowell\nCG\nL-BFGS-B\nSLSQP\nBFGS\nNelder-Mead\nUNIF\nEG50\nCMA\nbest 2009f10-14,5-D best 2009\nC A\nEG50\nUNIF\nNelder\nBFGS\nSLSQP\nL-BFGSB\nCG\nPowell 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8 1.0 Pr op or tio n of fu nc tio n+ ta rg et p ai rs\nCG\nNelder-Mead\nBFGS\nPowell\nSLSQP\nL-BFGS-B\nUNIF\nCMA\nEG50\nbest 2009f15-19,5-D best 2009\nEG50\nC A\nUNIF\nL-BFGSB\nSLSQP\nPowell\nBFGS\nNelder\nCG\nweakly structured multi-modal fcts all functions\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nBFGS\nCG\nPowell\nUNIF\nNelder-Mead\nCMA\nL-BFGS-B\nSLSQP\nEG50\nbest 2009f20-24,5-D best 2009\nEG50\nSLSQP\nL-BFGSB\nCMA\nNelder\nUNIF\nPowell\nCG\nBFGS 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nCG\nBFGS\nPowell\nNelder-Mead\nSLSQP\nL-BFGS-B\nCMA\nUNIF\nEG50\nbest 2009f1-24,5-D best 2009\nEG50\nUNIF\nCMA\nL-BFGSB\nSLSQP\nNelder\nPowell\nBFGS\nCG\nFig.3. Bootstrapped empirical cumulative distribution of the number of objective function evaluations divided by dimension (FEvals/D) for 50 targets in 10[\u22128..2] for all functions and subgroups in 5-D. The \u201cbest 2009\u201d line corresponds to the best ERT observed during BBOB 2009 for each single target.\nseparable fcts moderate fcts\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nNelder-Mead\nL-BFGS-B\nCG\nSLSQP\nBFGS\nCMA\nEG50\nUNIF\nPowell\nbest 2009f1-5,20-D best 2009\nPowell\nUNIF\nEG50\nCMA\nBFGS\nSLSQP\nCG\nL-BFGSB\nNelder 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nNelder-Mead\nPowell\nSLSQP\nL-BFGS-B\nBFGS\nCG\nEG50\nUNIF\nCMA\nbest 2009f6-9,20-D best 2009\nC A\nUNIF\nEG50\nCG\nBFGS\nL-BFGSB\nSLSQP\nPowell\nNelder\nill-conditioned fcts multi-modal fcts\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nNelder-Mead\nPowell\nCG\nBFGS\nL-BFGS-B\nSLSQP\nUNIF\nEG50\nCMA\nbest 2009f10-14,20-D best 2009\nC A\nEG50\nUNIF\nSLSQP\nL-BFGSB\nBFGS\nCG\nPowell\nNelder 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nCG\nNelder-Mead\nPowell\nL-BFGS-B\nBFGS\nSLSQP\nUNIF\nEG50\nCMA\nbest 2009f15-19,20-D best 2009\nC A\nEG50\nUNIF\nSLSQP\nBFGS\nL-BFGSB\nPowell\nNelder\nCG\nweakly structured multi-modal fcts all functions\n0 1 2 3 4 5 6 7 8 log10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nNelder-Mead\nPowell\nEG50\nSLSQP\nCG\nBFGS\nL-BFGS-B\nUNIF\nCMA\nbest 2009f20-24,20-D best 2009\nC A\nUNIF\nL-BFGSB\nBFGS\nCG\nSLSQP\nEG50\nPowell\nNelder 0 1 2 3 4 5 6 7 8\nlog10 of (# f-evals / dimension)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPr op or tio n of fu nc tio n+ ta rg et p ai rs\nNelder-Mead\nPowell\nCG\nL-BFGS-B\nBFGS\nSLSQP\nUNIF\nEG50\nCMA\nbest 2009f1-24,20-D best 2009\nC A\nEG50\nUNIF\nSLSQP\nBFGS\nL-BFGSB\nCG\nPowell\nNelder\nFig.4. Bootstrapped empirical cumulative distribution of the number of objective function evaluations divided by dimension (FEvals/D) for 50 targets in 10[\u22128..2] for all functions and subgroups in 20-D. The \u201cbest 2009\u201d line corresponds to the best ERT observed during BBOB 2009 for each single target.\nfunction evaluation is very expensive) and is not competitive with the SciPy optimizers on separable functions.\n5. Discussion and Conclusion We described an easy-to-use framework that allows users to plug in and experiment with custom algorithm selection strategies and then directly move to use these strategies on practical problems instead of benchmarks. The framework currently focuses on online black-box algorithm portfolios, being careful not to require any modification to stock and third-party optimization routines besides a simple callback mechanism that is commonly provided. The framework could be easily modified to deal with off-line scenarios too.\nOur insights may provide guidance for users of the SciPy optimizers. Regarding algorithm portfolios, the results show that even a very naive algorithm selection strategy can be beneficial; while it cannot beat the best algorithm CMA in high-dimensional cases on average over all functions, it provides a more consistent performance than CMA, translating to best-of-all performance in low dimensions.\n5.1. Future Work\nThe currently chosen reference portfolio is somewhat ad hoc, chosen in part to contribute value to the SciPy project, but not optimal for further general research into algorithm portfolios. In the long run, we hope to transition to a better balanced portfolio.\nA common scheme of online algorithm selection schemes is that a credit is assigned to an algorithm in each round based on its performance (e.g. relative improvement, absolute value) and accrued over the rounds (e.g. averaging it or using an adaptation rule). Credit-based selection is so common that it should be directly supported by the framework; diverse pluggable strategies would then enable exploration of various interesting strategy combinations. A workin-progress preview implementation is already available.\nActually running the individual algorithms is appropriate for optimizing non-benchmark functions and useful in the future when solutions may migrate within the portfolio population. However, a useful feature would be to allow testing selection strategies based on just the saved \u201cmlog\u201c data (see Sec. 2.2) from individual algorithm runs.\nAcknowledgements Research described in the paper was supervised by Dr. Petr Pos\u030c\u0131\u0301k and supported by the CTU grant SGS14/194/ OHK3/3T/13 \u201cAutomatic adaptation of search algorithms\u201d.\nReferences [1] BYRD, R. H., LU, P., NOCEDAL, J., AND ZHU, C. A limited mem-\nory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing 16, 5 (1995), 1190\u20131208. [2] FINCK, S., HANSEN, N., ROS, R., AND AUGER, A. Realparameter black-box optimization benchmarking 2009: Presentation of the noiseless functions. Tech. Rep. 2009/20, Research Center PPE, 2009. Updated February 2010. [3] GOMES, C. P., AND SELMAN, B. Algorithm portfolios. Artificial Intelligence 126, 1 (2001), 43\u201362. [4] HANSEN, N. The CMA evolution strategy: a comparing review. In Towards a new evolutionary computation. Advances on estimation of distribution algorithms, J. Lozano, P. Larranaga, I. Inza, and E. Bengoetxea, Eds. Springer, 2006, pp. 75\u2013102. [5] HANSEN, N., AUGER, A., FINCK, S., AND ROS, R. Real-parameter black-box optimization benchmarking 2012: Experimental setup. Tech. rep., INRIA, 2012. [6] HANSEN, N., ET AL. Comparing continuous optimisers: Coco. http://coco.gforge.inria.fr/. [7] HANSEN, N., FINCK, S., ROS, R., AND AUGER, A. Real-parameter black-box optimization benchmarking 2009: Noiseless functions definitions. Tech. Rep. RR-6829, INRIA, 2009. Updated February 2010. [8] JONES, E., OLIPHANT, T., PETERSON, P., ET AL. SciPy: Open source scientific tools for Python, 2001\u2013. [9] KOTTHOFF, L. Algorithm selection for combinatorial search problems: A survey. AI Magazine (2014). [10] KRAFT, D. A software package for sequential quadratic programming. DFVLR Obersfaffeuhofen, Germany, 1988. [11] MUN\u0303OZ, M., KIRLEY, M., AND HALGAMUGE, S. The algorithm selection problem on the continuous optimization domain. In Computational Intelligence in Intelligent Data Analysis, C. Moewes and A. Nu\u0308rnberger, Eds., vol. 445 of Studies in Computational Intelligence. Springer Berlin Heidelberg, 2013, pp. 75\u201389. [12] NELDER, J. A., AND MEAD, R. A simplex method for function minimization. The Computer Journal 7, 4 (1965), 308\u2013313. [13] NOCEDAL, J., AND WRIGHT, S. Numerical optimization. SpringerVerlag, 2006. [14] POWELL, M. J. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The computer journal 7, 2 (1964), 155\u2013162. [15] PRICE, K. Differential evolution vs. the functions of the second ICEO. In Proceedings of the IEEE International Congress on Evolutionary Computation (1997), pp. 153\u2013157. [16] RICE, J. R. The algorithm selection problem. Advances in Computers 15 (1976), 65\u2013118. [17] VRUGT, J. A., ROBINSON, B. A., AND HYMAN, J. M. Self-adaptive multimethod search for global optimization in real-parameter spaces. IEEE Trans. on Evolutionary Computation 13, 2 (2009), 243\u2013259. [18] WALES, D. J., AND DOYE, J. P. K. Global optimization by basinhopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A 101, 28 (1997), 5111\u20135116. [19] WRIGHT, M. H. Direct search methods: Once scorned, now respectable. Pitman Research Notes in Math. Series (1996), 191\u2013208. [20] ZHU, C., BYRD, R. H., LU, P., AND NOCEDAL, J. Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS) 23, 4 (1997), 550\u2013560."}, {"heading": "About Author. . .", "text": "Petr Baudis\u030c has received his Bachelors degree and Masters degree in Theoretical Computer Science at the Charles University in Prague. His Master thesis \u201cMCTS with Information Sharing\u201d presented a state-of-art Computer Go program.\nCurrently, he is a PhD student at the Czech Technical University with principal research interest in Algorithm Portfolios and applications of Monte Carlo Tree Search."}], "references": [{"title": "A limited memory algorithm for bound constrained optimization", "author": ["BYRD R. H", "LU P", "NOCEDAL J", "ZHU"], "venue": "SIAM Journal on Scientific Computing 16,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Realparameter black-box optimization benchmarking 2009: Presentation of the noiseless functions", "author": ["S. FINCK", "N. HANSEN", "R. ROS", "A. AUGER"], "venue": "Research Center PPE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The CMA evolution strategy: a comparing review", "author": ["N. HANSEN"], "venue": "Advances on estimation of distribution algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "AND ROS, R. Real-parameter black-box optimization benchmarking 2012", "author": ["N. HANSEN", "A. AUGER", "S. FINCK"], "venue": "Experimental setup. Tech. rep., INRIA,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Real-parameter black-box optimization benchmarking 2009: Noiseless functions definitions", "author": ["N. HANSEN", "S. FINCK", "R. ROS", "A. AUGER"], "venue": "Tech. Rep. RR-6829,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "SciPy: Open source scientific tools for Python, 2001", "author": ["E. JONES", "T. OLIPHANT", "P PETERSON"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Algorithm selection for combinatorial search problems: A survey", "author": ["L. KOTTHOFF"], "venue": "AI Magazine", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A software package for sequential quadratic programming", "author": ["D. KRAFT"], "venue": "DFVLR Obersfaffeuhofen,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "The algorithm selection problem on the continuous optimization domain", "author": ["M. MU\u00d1OZ", "M. KIRLEY", "S. HALGAMUGE"], "venue": "In Computational Intelligence in Intelligent Data Analysis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "A simplex method for function minimization", "author": ["J.A. NELDER", "R. MEAD"], "venue": "The Computer Journal 7,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1965}, {"title": "An efficient method for finding the minimum of a function of several variables without calculating derivatives", "author": ["M.J. POWELL"], "venue": "The computer journal 7,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1964}, {"title": "Differential evolution vs. the functions of the second ICEO", "author": ["K. PRICE"], "venue": "In Proceedings of the IEEE International Congress on Evolutionary Computation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "The algorithm selection problem", "author": ["J.R. RICE"], "venue": "Advances in Computers", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1976}, {"title": "Self-adaptive multimethod search for global optimization in real-parameter spaces", "author": ["J.A. VRUGT", "B.A. ROBINSON", "J.M. HYMAN"], "venue": "IEEE Trans. on Evolutionary Computation 13,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Global optimization by basinhopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms", "author": ["D.J. WALES", "J.P.K. DOYE"], "venue": "The Journal of Physical Chemistry A 101,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Direct search methods: Once scorned, now respectable", "author": ["M.H. WRIGHT"], "venue": "Pitman Research Notes in Math. Series", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. ZHU", "R.H. BYRD", "P. LU", "J. NOCEDAL"], "venue": "ACM Transactions on Mathematical Software (TOMS) 23,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}], "referenceMentions": [{"referenceID": 3, "context": "The COCO platform [6] [5] of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies.", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "Introduction The problem of Algorithm Selection is not new [16], but has only recently gained traction in particular in the field of combinatoric problem solvers.", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "[9] There are multiple ways to approach the issue, in our work we adopt the abstraction of algorithm portfolios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "the AMALGAMSO algorithm [17], or in offline methods based chiefly on exploratory landscape analysis [11].", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "the AMALGAMSO algorithm [17], or in offline methods based chiefly on exploratory landscape analysis [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The currently accepted de-facto standard for benchmarking optimization methods is the COmparing Continuous Optimisers COCO platform [6] [5] that was originally developed for the BBOB workshop series.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "We are in part motivated by the convenient availability of optimization algorithms distributed along the popular SciPy library [8].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "13 [8] that are available for direct use:4", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "[12] [19] \u2022 Powell implements a tweaked version of the Powell\u2019s conjugate direction method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[12] [19] \u2022 Powell implements a tweaked version of the Powell\u2019s conjugate direction method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "[14] \u2022 CG is a nonlinear conjugate gradient method using the Fletcher-Reeves method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] [20] \u2022 SLSQP implements Sequential Least SQuares Programming [10] with inequalities as the box constraints.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[1] [20] \u2022 SLSQP implements Sequential Least SQuares Programming [10] with inequalities as the box constraints.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "[1] [20] \u2022 SLSQP implements Sequential Least SQuares Programming [10] with inequalities as the box constraints.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "All of these SciPy minimizers perform local minimization; to achieve global optimization, we wrap them in the \u201cBasin Hopping\u201d restart strategy [18] (also provided by SciPy), which is conceptually similar to Simulated Annealing with a fixed temperature.", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "To improve the portfolio performance on more difficult functions, we also included the popular CMA algorithm [4] (in the \u201cproduction\u201c version of its official reference Python implementation), i.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "Results Results from experiments according to [5] on the benchmark functions given in [2, 7] are presented in Figures 2, 3 and 4.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Results Results from experiments according to [5] on the benchmark functions given in [2, 7] are presented in Figures 2, 3 and 4.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "Results Results from experiments according to [5] on the benchmark functions given in [2, 7] are presented in Figures 2, 3 and 4.", "startOffset": 86, "endOffset": 92}, {"referenceID": 3, "context": "6 The expected running time (ERT) used in the figures depends on a given target function value, ft = fopt+\u2206f , and is computed over all relevant trials as the number of function evaluations executed during each trial while the best function value did not reach ft, summed over all trials and divided by the number of trials that actually reached ft [5, 15].", "startOffset": 349, "endOffset": 356}, {"referenceID": 11, "context": "6 The expected running time (ERT) used in the figures depends on a given target function value, ft = fopt+\u2206f , and is computed over all relevant trials as the number of function evaluations executed during each trial while the best function value did not reach ft, summed over all trials and divided by the number of trials that actually reached ft [5, 15].", "startOffset": 349, "endOffset": 356}], "year": 2014, "abstractText": "Algorithm portfolios represent a strategy of composing multiple heuristic algorithms, each suited to a different class of problems, within a single general solver that will choose the best suited algorithm for each input. This approach recently gained popularity especially for solving combinatoric problems, but optimization applications are still emerging. The COCO platform [6] [5] of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms. As an extension to the COCO platform, we present the Python-based COCOpf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies. In our framework, we focus on black-box algorithm portfolio and online adaptive selection. As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies. We confirm that even a naive selection strategy can provide improved performance across problem classes.", "creator": "LaTeX with hyperref package"}}}