{"id": "1405.4589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm", "abstract": "A large amount of experimental data shows that the SVM (Support Vector Machine) algorithm has obvious advantages in the areas of text classification, handwriting recognition, image classification, bioinformatics and some other areas. To some extent, the optimization of the SVM depends on its core function and the slack variable, the determinants of which are its parameters $\\ delta $and c in the classification function. To optimize the SVM algorithm, the optimization of the two parameters plays a major role. Ant Colony Optimization (ACO) is an optimization algorithm that simulates ants to find the optimal path.In the available literature, we mix the ACO algorithm and the parallel algorithm to find good parameters.", "histories": [["v1", "Mon, 19 May 2014 03:50:21 GMT  (137kb)", "https://arxiv.org/abs/1405.4589v1", "3 pages, 2 figures, 2 tables"], ["v2", "Tue, 20 May 2014 11:53:39 GMT  (142kb)", "http://arxiv.org/abs/1405.4589v2", "3 pages, 2 figures, 2 tables"]], "COMMENTS": "3 pages, 2 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["chao zhang", "hong-cen mei", "hao yang"], "accepted": false, "id": "1405.4589"}, "pdf": {"name": "1405.4589.pdf", "metadata": {"source": "CRF", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm", "authors": ["Chao Zhang", "Hong-Cen Mei", "Hao Yang"], "emails": ["zhch040200@gmail.com", "hongcenmei@yeah.net", "chongqingyanghao@yeah.net"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n45 89\nv2 [\ncs .N\nE ]\n2 0\nM ay\n2 01\nI. SUPPORT VECTOR CLASSIFICATION AND PARAMETERS\nSVM is based on the principle of structural risk minimization,using limited training samples to obtain the higher generalization ability of decision function. Suppose a sample set (xi, yi) , where i = 1, 2...N means the number of training samples, x\u2208R means the sample characteristics, y \u2208 {+1,\u22121} means the sample classification. SVM Classification function:\ny = \u03c9x+ b (\u03c9 means weight vector,b means setover)\nFunctional margin :\n\u03b31 = y(x+ b) = yf(x)\nfunctional margin is the minimum margin from hyperplane (\u03c9, b) to T (xi, yi)\nGeometrical margin:\n\u03b32 = y(\u03c9x+ b)\n\u2016 \u03c9 \u2016 =\n|f(x)|\n\u2016 \u03c9 \u2016\nWhen classifying a data point, the larger the margin is, the more credible the classification is. So to improve the credibility is to maximize the margin. The \u03c9 and b can be proportional scaled through the functional margin, thus the value of y = \u03c9x + b can be any large. Such that it is not appropriate to maximize a value. But in the geometrical margin, when scaling the \u03c9 and b, the value of \u03b32 can\u2019t be change. So it is appropriate to maximize a value. Slack variable: \u03b5i > 0 Using to allow the data to deviate from the hyperplane\nto a certain extent. Radial Basis Function kernel:\nk\u3008x1, x2\u3009 = e \u2212\n\u2016x1\u2212x2\u2016 2\n2\u03c32\nMaximum margin classifier:\nMAX\u03b32 s.t\u03b31i \u2265 \u03b31 \u2212 \u03b5i, i = 1, 2, 3...n\nLet: \u03b31 = 1,so\nMAX 1\n\u2016 \u03c9 \u2016 + C\nn\u2211\ni=1\n\u03b5is.t\u03b31i \u2265 \u03b31 \u2212 \u03b5i, i = 1, 2, 3...n\nThe constraints is associated with objective function through the Lagrange function:\nL(\u03c9, b, \u03b1) = 1\n2 \u2016 \u03c9 \u2016\n2 \u2212\nn\u2211\ni=1\n\u03b1i(yi((\u03c9x+ b)\u2212 1) + C\nn\u2211\ni=1\n\u03b5i\nLet: \u0398(\u03c9) = MAXL(\u03c9, b, \u03b1)\nWhen all the constraints is contented, the \u0398(\u03c9) = 1 2 \u2016 \u03c9 \u20162 is the value that we first want to minimized.So objective function:\nMIN\u0398(\u03c9) = MINMAXL(\u03c9, b, \u03b1)\nDual function:\nMIN\u0398(\u03c9) = MAXMINL(\u03c9, b, \u03b1)\nTo solve the problem, requiring:\n\u2202L \u2202\u03c9 = 0 \u21d2 \u03c9 =\nn\u2211\ni=1\n\u03b1ixiyi\n\u2202L \u2202b = 0 \u21d2\nn\u2211\ni=1\n\u03b1iyi = 0\n\u2202L \u2202\u03b5i = 0 means C \u2212 \u03b1i \u2212 \u03b3i = 0, i = 1, 2...n\nMAXMINL(\u03c9, b, \u03b1) = 1\n2 \u2016 \u03c9 \u20162 \u2212\nn\u2211\ni=1\n\u03b1i(yi((\u03c9x+ b)\u2212 1)\n= MAX\nn\u2211\ni=1\n\u03b1i \u2212 1\n2\nn\u2211\ni,j=1\n\u03b1i\u03b1jyiyjk\u3008x1, x\u3009\nThe minimized duel problem:\nMAX\nn\u2211\ni=1\n\u03b1i \u2212 1\n2\nn\u2211\ni,j=1\n\u03b1i\u03b1jyiyjk\u3008x1, x\u3009\ns.t0 \u2264 \u03b1i \u2264 C, i = 1, 2, 3...n;\nn\u2211\ni=1\n\u03b1iyi = 0\nThe final classification function:\ny = \u03c9x+ b\nn\u2211\ni=1\n\u03b1iyik\u3008x1, x\u3009+ b\nII. MODIFIED ANT COLONY OPTIMIZATION ALGORITHM\nDifferent from traditional problem of TCP, in this algorithm,the coordinate is used to represented the node. In a twodimensional rectangular coordinate system, the significance of X is defined as the significant digit of parameters C and \u03b4, Y is varied from 0 to 10.[1] The significant digit of C are assumed to be the five, and the highest level of C is hundred\u2019s place. Similarly, assume that the significant digit of \u03b4 are also assumed to be the five, and the highest level of \u03b4 is The Unit. To realize the ant colony optimization, we follow the steps below:\n1) Suppose there are m ants. Each ant k(k = 1\u223cm)has a one-dimensional array Path(k) which has n elements(n is the total significant digit of C and \u03b4), its used to store the vertical coordinates y of each point which the ant k visited. 2) Set the loop time N=0 to Nmax. Initialize the each points pheromone concentration \u03c4(x, y) = \u03c40(x = 0 \u223c n, y = 0 \u223c 9).Set x = 1. 3) Set y = 1 to n.Calculate the deflection probability Pk of each ant move to the node vertical line Lx.Then select the next point via roulette wheel and store the points vertical coordinates y to Pathk\nPk(x, y) = \u03c4\u03b1(x, y)\u03b7\u03b2(x, y)\u22119 j=0 \u03c4 \u03b1(x, y)\u03b7\u03b2(x, y)\n\u03c4(x, y) = \u03c1\u03c4(x, y) + \u2206\u03c4(x, y)\n\u2206\u03c4(x, y) =\nm\u2211\nk=1\n\u2206\u03c4k(x, y)\n\u2206\u03c4(x, y) =\n{ Q\n1\u2212Acck ,if ant k visited the point(x,y) 0 ,others\nThe Acck mean the ant k s accuracy of cross-validation\nPk means the deflection probability point(x\u2212 1, y) to (x, y)\nQ is a constant W:the weight coefficient;\nSet:the minimum acceptable accuracy; 4) Let :x = x + 1,if x \u2264 n,turn to step 3;else turn to step 5. 5) Record this motion path Pathk calculate the mapping\ndata (c, \u03b4).\n6) Make the training samples evenly divided into k mutually exclusive subsets of S1, S2, ..., Sk; 7) Calculate the cross-validation accuracy: a) Initialize i=1; b) Make the Si subset reserved for test sets, and\nset the rest as the training set, training SVM; c) Calculate the ith subsets Sample Classifi-\ncation Accuracy Acci,set i = i + 1,When i < k + 1, repeat the step b);\nd) Calculate the mean of k Sample Classification Accuracy A\u0302cc :\nAcc = Right\nError +Right\nA\u0302cc = \u2211k i=1 Acci k\nRight: Correctly classified (+1) number of samples Error:Misclassification (-1) number of samples Q\u0302 :the mean of Sample Classification Accuracy\n8) Update the pheromone concentration \u03c4(x, y) at each points by A\u0302cc . Clear Path(ki); 9) Reset N = N + 1.When N \u2264 Nmax , and the entire colony has not converged to follow the same path, then turn to step 3; N \u2264 Nmax, and the entire colony substantially converged to follow the same path, then the algorithm ends. Take the last update of the path and its mapping data to (c, \u03b4),that is the SVM parameters C, \u03b4 final optimization results.\nIn order to validate the algorithm,we do simulation experiment on matlab R2010b and PC with windows7 64-bit operating system, 4 G memory, core i5 processor. We divide the wine SVM data set into a sample set and a test set. Then we will have 90 training datas and 88 test datas. We set ACO parameter m=30, N=500, \u03c1=0.7, Q=100, \u03b1=1, \u03b2=1. In order to calculate the classification accuracy, we introduced the LIBSVM.[3].LIBSVM has gained wide popularity in machine learning and many other areas.[2]\n0 2 4 6 8 10 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nc,sigma\ny\n(a) The best accuracy\u2019s path\n0 2 4 6 8 10 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) All accuracy\u2019s paths\nThe best accuracy is 95.4545% and the convergence accuracy is 86.3636% (76/88). The parameter c: 18.605, parameter \u03c3: 0.6643\nIII. PARALLEL OPTIMIZE THE PARAMETERS\nThe Open Computing Language, is an open specification for heterogeneous computing released by the Khronos Group2 in 2008. It resembles the NVIDIA CUDA3 platform, but can\nbe considered as a superset of the latter, they basically differ in the following points[4]:\n\u2022 OpenCL is an open specification that is managed by a set of distinct representatives from industry, software development, academia and so forth.\n\u2022 OpenCL is meant to be implemented by any compute device vendor, whether they produce CPUs, GPUs, hybrid processors, or other accelerators such as digital signal processors (DSP) andfield-programmable gate arrays (FPGA).\n\u2022 OpenCL is portable across architectures, meaning that a parallel code written in OpenCL is guaranteed to correctly run on every other supporte device.\nIn this ant colony optimization algorithm, we have m ants and we loop it N max times. If we give m a large number, such as ten thousand or one hundred thousand, our update of node pheromone concentration will be more accurate and reliable, but we will spend more time. As we all know, every cycle, each ant\u2019s access to nodes are unrelated with others,so we can parallel the ant access process. In this article, we use openCL to realize the parallel of ant[4].\nOpenCL kernel for the ant-based solution construction globalsize = numberants; visited[globalesize \u00d7 numbernodes] = {\u22121};\nfor ( i=0 to n-1) do { sumprob = 0; for (j=0 to 9) do { selectionprob[globalid \u00d7 numbernodes + j] = choiceinfo[i\u00d7 n+ j];\nSumprob = sumprob+ selectionprob[globalid \u00d7 numbernodes + j]; } j=0; p = selectionprob[globalid \u00d7 numbernodes + j]; while p < random(0, sumprob) do {\nj=j+1; p = p+ selectionprob[globalid \u00d7 numbernodes + j];\n} visited[globalid \u00d7 numbernodes + i] = j;\n}\nTraining samples are divided into subset number subset on average and each subset has sample number samples. One subset(Si) see as test set and the other subsets (S1\u223ci, Si\u223ck) see as a training set(each subset has c samples), then according to the current parameters (c, \u03b4)training the SVM, calculating error of K-flod cross validation.\nOpenCL kernel for sample classification accuracy groupsize = subset number; localsize = sample number;\nfor (i=0 to groupsize \u2212 1) { Right = Error = Q = 0; for(j = 0 to localsize \u2212 1) { f(x) = sign( \u2211n i=1 \u03b1iyik\u3008groupid, localid\u3009+b);\nif f(x) = 1 Right++; esle Error ++;\n} Q = Q + Right Right+Error\n; } Q = Q/groupsize;\nIV. CONCLUSION\nThrough the ant colony optimization algorithm, we can find a satisfactory parameter of SVM, and the convergence accuracy can be guaranteed more than 85%. There are also many other ways to optimize parameters, such as Genetic algorithm (GA)[5], dynamic encoding algorithm [6] for handwritten digit recognition, Particle swarm optimization(PSO)[7]. We also can parallel Ant Colony Optimization,article [8] introduced a new way which parallel Ant Colony Optimization on Graphics Processing Units.Article [9] improving ant colony optimization algorithm for data clustering.\nACKNOWLEDGMENT\nREFERENCES\n[1] P. F. LIU Chun-bo, WANG Xian-fang, \u201cParamters selection and stimulation of support vector machines based on ant colony optimization algorithm,\u201d J.Cent,South Univ, 2008.\n[2] C. chung Chang and C.-J. Lin, \u201cLibsvm : a library for support vector machines,\u201d Linux Journal, 2001.\n[3] C.-C. Chang and C.-J. Lin, \u201cLIBSVM: A library for support vector machines,\u201d vol. 2, pp. 1\u201327, 2011.\n[4] E. by Helio J.C. Barbosa, Ant Colony Optimization - Techniques and Applications. InTech, Chapters published February 20, 2013 under CC BY 3.0 license, ISBN 978-953-51-1001-9,203 pages.\n[5] J. L.-c. ZHENG Chun-hong, \u201cAutomatic parameters selection for SVM based on GA[C],\u201d NJ:IEEE Press,2004:1869-1872.\n[6] Y. Park, S.-W. Kim, and H.-S. Ahn, \u201cSupport vector machine parameter tuning using dynamic encoding algorithm for handwritten digit recognition,\u201d 2005.\n[7] X. Li, S. dong Yang, and J. xun Qi, \u201cA new support vector machine optimized by improved particle swarm optimization and its application,\u201d Journal of Central South University of Technology, vol. 13, pp. 568\u2013572, 2006.\n[8] A. Delevacq, P. Delisle, and M. Gravel, \u201cParallel Ant Colony Optimization on Graphics Processing Units,\u201d Journal of Parallel and Distributed Computing, vol. 73, 2013.\n[9] R. Tiwari, M. Husain, S. Gupta, and A. Srivastava, \u201cImproving ant colony optimization algorithm for data clustering,\u201d pp. 529\u2013534, 2010."}], "references": [{"title": "Paramters selection and stimulation of support vector machines based on ant colony optimization algorithm", "author": ["P.F. LIU Chun-bo", "WANG Xian-fang"], "venue": "J.Cent,South Univ, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Libsvm : a library for support vector machines", "author": ["C. chung Chang", "C.-J. Lin"], "venue": "Linux Journal, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Ant Colony Optimization - Techniques and Applications", "author": ["E. by Helio J.C. Barbosa"], "venue": "InTech, Chapters published February", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Automatic parameters selection for SVM based on GA[C", "author": ["J.L.-c. ZHENG Chun-hong"], "venue": "NJ:IEEE Press,2004:1869-1872.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machine parameter tuning using dynamic encoding algorithm for handwritten digit recognition", "author": ["Y. Park", "S.-W. Kim", "H.-S. Ahn"], "venue": "2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A new support vector machine optimized by improved particle swarm optimization and its application", "author": ["X. Li", "S. dong Yang", "J. xun Qi"], "venue": "Journal of Central South University of Technology, vol. 13, pp. 568\u2013572, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel Ant Colony Optimization on Graphics Processing Units", "author": ["A. Delevacq", "P. Delisle", "M. Gravel"], "venue": "Journal of Parallel and Distributed Computing, vol. 73, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving ant colony optimization algorithm for data clustering", "author": ["R. Tiwari", "M. Husain", "S. Gupta", "A. Srivastava"], "venue": "pp. 529\u2013534, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1] The significant digit of C are assumed to be the five, and the highest level of C is hundred\u2019s place.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "be considered as a superset of the latter, they basically differ in the following points[4]:", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In this article, we use openCL to realize the parallel of ant[4].", "startOffset": 61, "endOffset": 64}], "year": 2014, "abstractText": "A large number of experimental data shows that Support Vector Machine (SVM) algorithm has obvious a large advantages in text classification, handwriting recognition, image classification, bioinformatics, and some other fields. To some degree, the optimization of SVM depends on its kernel function and Slack variable, the determinant of which is its parameters \u03b4 and c in the classification function. That is to say, to optimize the SVM algorithm, the optimization of the two parameters play a huge role. Ant Colony Optimization (ACO) is optimization algorithm which simulate ants to find the optimal path. In the available literature, we mix the ACO algorithm and Parallel algorithm together to find a well parameters. Keyword: SVM, Parameters, ACO, OpenCL, Parallel I. SUPPORT VECTOR CLASSIFICATION AND PARAMETERS SVM is based on the principle of structural risk minimization,using limited training samples to obtain the higher generalization ability of decision function. Suppose a sample set (xi, yi) , where i = 1, 2...N means the number of training samples, x\u2208R means the sample characteristics, y \u2208 {+1,\u22121} means the sample classification. SVM Classification function: y = \u03c9x+ b (\u03c9 means weight vector,b means setover)", "creator": "LaTeX with hyperref package"}}}