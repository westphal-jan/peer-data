{"id": "1406.1837", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "A Credit Assignment Compiler for Joint Prediction", "abstract": "We improve \"learning to search\" in structured predictions in two ways: First, we show that the search space can be defined by any imperative program, reducing by orders of magnitude the number of lines of code needed to develop new structured prediction tasks; second, we make structured predictions faster through various algorithmic improvements.", "histories": [["v1", "Sat, 7 Jun 2014 00:24:42 GMT  (100kb,D)", "https://arxiv.org/abs/1406.1837v1", null], ["v2", "Fri, 13 Jun 2014 13:16:36 GMT  (101kb,D)", "http://arxiv.org/abs/1406.1837v2", null], ["v3", "Mon, 1 Jun 2015 23:44:33 GMT  (841kb,D)", "http://arxiv.org/abs/1406.1837v3", null], ["v4", "Fri, 5 Jun 2015 16:26:05 GMT  (845kb,D)", "http://arxiv.org/abs/1406.1837v4", null], ["v5", "Wed, 1 Jun 2016 05:35:31 GMT  (622kb,D)", "http://arxiv.org/abs/1406.1837v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kai-wei chang", "he he", "st\u00e9phane ross", "hal daum\u00e9 iii", "john langford"], "accepted": true, "id": "1406.1837"}, "pdf": {"name": "1406.1837.pdf", "metadata": {"source": "CRF", "title": "A Credit Assignment Compiler for Joint Prediction", "authors": ["Kai-Wei Chang"], "emails": ["kw@kwchang.net", "hhe@cs.umd.edu", "me@hal3.name", "jcl@microsoft.com", "stephaneross@google.com"], "sections": [{"heading": "1 Introduction", "text": "Many applications require a predictor to make coherent decisions. As an example, consider recognizing a handwritten word where each character might be recognized in turn to understand the word. Here, it is commonly observed that exposing information from related predictions (i.e. adjacent letters) aids individual predictions. Furthermore, optimizing a joint loss function can improve the gracefulness of error recovery. Despite these advantages, it is empirically common to build independent predictors, in settings where joint prediction naturally applies, because they are simpler to implement and faster to run. Can we make joint prediction algorithms as easy and fast to program and compute while maintaining their theoretical benefits?\nMethods making a sequence of sub-decisions have been proposed for handling complex joint predictions in a variety of applications, including sequence tagging [30], dependency parsing (known as transition-based method) [35], machine translation [18], and co-reference resolution [44]. Recently, general search-based joint prediction approaches (e.g., [10, 12, 14, 22, 41]) have been investigated. The key issue of these search-based approaches is credit assignment: When something goes wrong do you blame the first, second, or third prediction? Existing methods often take two strategies:\n\u2022 The system ignores the possibility that a previous prediction may have been wrong, different costs have different errors, or the difference between train-time and test-time prediction.\n\u2022 The system may use handcrafted credit assignment heuristics to cope with errors that the underlying algorithm makes and the long-term outcomes of decisions.\nBoth approaches may lead to statistical inconsistency: when features are not rich enough for perfect prediction, the machine learning may converge sub-optimally.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n40 6.\n18 37\nv5 [\ncs .L\nG ]\n1 J\nun 2\nAlgorithm 1 MYRUN(X) % for sequence tagging, X: input sequence, Y: output 1: Y \u2190 [] 2: for t = 1 to LEN(X) do 3: ref \u2190 X[t].true_label 4: Y[t]\u2190 PREDICT(x=examples[t], y=ref , tag=t, condition=[1:t-1]) 5: LOSS(number of Y[t] 6= X[t].true_label) 6: return Y\nIn contrast, learning to search approaches [5, 11, 40] automatically handle the credit assignment problem by decomposing the production of the joint output in terms of an explicit search space (states, actions, etc.); and learning a control policy that takes actions in this search space. These have formal correctness guarantees which differ qualitatively from graphical models such as the Conditional Random Fields [28] and structured SVMs [46, 47]. Despite the good properties, none of these methods have been widely adopted because the specification of a search space as a finite state machine is awkward and naive implementations do not fully demonstrate the ability of these methods.\nIn this paper, we cast learning to search into a credit assignment compiler with a new programming abstraction for representing a search space. Together with several algorithmic improvements, this radically reduces both the complexity of programming and the running time. The programming interface has the following advantages:\n\u2022 The same decoding function (see Alg. 1 and Sec. 2 for example) is used for training and prediction so a developer need only code desired test time behavior and gets training \u201cfor free. \u201d This simple implementation prevents common train/test asynchrony bugs. \u2022 The compiler automatically ensures the model learns to avoid compounding errors and makes a sequence of coherent decisions. \u2022 The library functions are in a reduction stack so as base classifiers and learning to search approaches improve, so does joint prediction performance.\nWithout extra implementation cost1, implementations enabled by the credit assignment compiler achieve outstanding empirical performance both in accuracy and in speed. This provides strong simple baselines for future research and demonstrates the compiler approach to solving complex prediction problems may be of broad interest.2"}, {"heading": "2 Programmable Learning to Search", "text": "We first describe the proposed programmable joint prediction paradigm. Algorithm 1 shows sample code for a part of speech tagger (or generic sequence labeler) under Hamming loss. The algorithm takes as input a sequence of examples (e.g., words), and predicts the meaning of each element in turn. The ith prediction depends on previous predictions.3 It uses two underlying library functions, PREDICT(...) and LOSS(...). The function PREDICT(...) returns individual predictions based on x while LOSS(...) allows the declaration of an arbitrary loss for the point set of predictions. The LOSS(...) function and the reference y inputted to PREDICT(...) are only used in the training phase and it has no effect in the test phase. Surprisingly, this single library interface is sufficient for both testing and training, when augmented to include label \u201cadvice\u201d from a training set as a reference decision (by the parameter y). This means that a developer only has to specify the desired test time behavior and gets training with minor additional decoration. The underlying system works as a credit assignment compiler to translate the user-specified decoding function and labeled data into updates of the learning model.\nHow can you learn a good PREDICT function given just an imperative program like Algorithm 1? In the following, we show that it is essential to run the MYRUN(...) function (e.g., Algorithm 1) many times, \u201ctrying out\u201d different versions of PREDICT(...) to learn one that yields low LOSS(...). We begin with formal definitions of joint prediction and a search space.\n1In fact, with library supports, developing a new task often requires only a few lines of code. 2Experiments and implementations will be released. 3In this example, we use the library\u2019s support for generating implicit features based on previous predictions.\nJoint Prediction. Joint prediction aims to induce a function f such that for anyX \u2208 X (the input space), f produces an output f(X) = Y \u2208 Y(X) in a (possibly input-dependent) space Y(X). The output Y often can be decomposed into smaller pieces (e.g., y1, y2, . . .), which are tied together by features, by a loss function and/or by statistical dependence. There is a task-specific loss function ` : Y \u00d7 Y \u2192 R\u22650, where `(Y \u2217, Y\u0302 ) tells us how bad it is to predict Y\u0302 when the true is Y \u2217.\nSearch Space. In our framework, the joint variable Y\u0302 is produced incrementally by traversing a search space, which is defined by states s \u2208 S and a mapping A : S \u2192 2S defining the set of valid next states.4 One of the states is a unique start state S while some of the others are end states e \u2208 E. Each end state corresponds to some output variable Ye. The goal of learning is finding a function f : Xs \u2192 S that uses the features of an input state (xs) to choose the next state so as to minimize the loss `(Y \u2217, Ye) on a holdout test set.5 Follow reinforcement learning terminology, we call the function a policy and call the learned function f a learned policy \u03c0f .\nTurning Search Space into an Imperative Program Surprisingly, search space can be represented by a class of imperative program, called Terminal Discrete Oracle Loss Reporting (TDOLR) programs. The formal definition of TDOLR is listed in Figure 2. Without loss of generality, we assume the number of choices is fixed in a search space, and the following theorem holds:\nTheorem 1. For every TDOLR program, there exist an equivalent search space and for every search space there exists an equivalent TDOLR program.\nProof. A search space is defined by (A,E, S, l). We show there is a TDOLR program which can simulate the search space in algorithm 2. This algorithm does a straightforward execution of the search space, followed by reporting of the loss on termination. This completes the second claim.\nFor the first claim, we need to define, (A,E, S, l) given a TDOLR program such that the search space can simulate the TDOLR program. At any point in the execution of TDOLR, we define an equivalent state s = (O(X1), ..., O(Xn)) where n is the number of calls to the oracle. We define a as the sequence of zero length, and we define E as the set of states after which TDOLR terminates.\n4Comprehensive strategies for defining search space have been discussed [14]. The theoretical properties do not depend on which search space definition is used.\n5Note that we useX and Y to represent joint input and output and use x and y to represent input and output to function f and PREDICT.\nAlgorithm 3 LEARN(X, F) 1: T, ex, cache\u2190 0, [], [] 2: define PREDICT(x, y) := { T++ ; ex[T-1]\u2190 x; cache[T-1]\u2190 F(x, y, rollin) ; return cache[T-1] } 3: define LOSS(l) := no-op 4: MYRUN(X) 5: for t0 = 1 to T do 6: losses, t\u2190 \u30080, 0, . . . , 0\u3009, 0 7: for a0 = 1 to A(ex[t0]) do\n8: Define PREDICT(x, y) := { t++ ; return  cache[t-1] if t < t0a0 if t = t0F(x,y,rollout) if t > t0 } 9: Define LOSS(val) := { losses[a0] += val }\n10: MYRUN(X) 11: Online update with cost-sensitive example (ex[t0], losses)\nFor each s \u2208 E we define l(s) as the loss reported on termination. This search space manifestly outputs the same loss as the TDOLR program.\nThe practical implication of this theorem is that instead of specifying search spaces, we can specify a TDOLR program (such as Algorithm 1), radically reducing the programming complexity of joint prediction."}, {"heading": "3 Credit Assignment Compiler for Training Joint Predictor", "text": "Now, we show how credit assignment compiler turn a TDOLR program and training data into model updates. In the training phase, the supervised signals are used in two places: 1) to define the loss function, and 2) to construct a reference policy \u03c0\u2217. The reference policy returns at any prediction point a \u201csuggestion\u201d as to a good next state.6 The general strategy is, for some number of epochs, and for each example (X,Y ) in the training data, to do the following:\n1. Execute MYRUN(...) on X with a rollin policy to obtain a trajectory of actions ~a and loss `0 2. Many times:\n(a) For some (or for all) time step t \u2264 |~a| (b) For some (or for all) alternative action a\u2032t 6= at (at is the action taken by ~a in time step t) (c) Execute MYRUN(...) on X , with PREDICT returning a1:t\u22121 initially, then a\u2032t, then acting\naccording to a rollout policy to obtain a new loss `t,a\u2032t (d) Compare the overall losses `t,at and `t,a\u2032t to construct a classification/regression example\nthat demonstrates how much better or worse a\u2032t is than at in this context. 3. Update the learned policy\nThe rollin and rollout policies can be the reference \u03c0\u2217, the current classifier \u03c0f or a mixture between them. By varying them and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN [11], DAGGER [41], AGGREVATE [40], and LOLS [5].7\nThe full learning algorithm (for a single joint input X) is depicted in Algorithm 3.8 In lines 1\u20134, a rollin pass of MYRUN is executed. MYRUN can generally be any TDOLR program as discussed (e.g., Alg. 1). In this pass, predictions are made according to the current policy, F, flagged as rollin (this is to enable support of arbitrary rollin and rollout policies). Furthermore, the examples (feature vectors) encountered during prediction are stored in ex, indexed by their position in the sequence (T), and the rollin predictions are cached in the variable cache (see Sec. 4).\n6Some papers assume the reference policy is optimal. An optimal policy always chooses the best next state assuming it gets to make all future decisions as well.\n7E.g., rollin in LOLS is \u03c0f and rollout is a stochastic interpolation of \u03c0f and oracle \u03c0\u2217 constructed by y. 8This algorithm is awkward because standard computational systems have a single stack. We have elected to give MYRUN control of the stack to ease the implementation of joint prediction tasks. Consequently, the learning algorithm does not have access to the machine stack and must be implemented as a state machine.\nThe algorithm then initiates one-step deviations from this rollin trajectory. For every time step, (line 5), we generate a single cost-sensitive classification example; its features are ex[t0], and there are M(ex[t0]) possible labels (=actions). For each action (line 7), we compute the cost of that action by executing MYRUN again (line 10) with a \u201ctweaked\u201d PREDICT which returns the cached predictions at steps before t0, returns the perturbed action a0 at t0, and at future timesteps calls F for rollouts. The LOSS function accumulates the loss for the query action. Finally, a cost-sensitive classification example is generated (line 11) and fed into an online learning algorithm."}, {"heading": "4 Optimizing the Credit Assignment Compiler", "text": "We present two algorithmic improvements which make training orders of magnitude faster.\nOptimization 1: Memoization The primary computational cost of Alg. 3 is making predictions: namely, calling the underlying classifier in Step 10. In order to avoid redundant predictions, we cache previous predictions. The challenge is understanding how to know when two predictions are going to be identical, faster than actually computing the prediction. To accomplish this, the user may decorate calls to the PREDICT function with tags. For a graphical models, a tag is effectively the \u201cname\u201d of a particular variable in the graphical model. For a sequence labeling problem, the tag for a given position might just be its index. When calling PREDICT, the user specifies both the tag of the current prediction, and the tag of all previous predictions on which the current prediction depends. The user is guaranteeing that if the predictions for all the tags in the dependent variables are the same, then the prediction for the current example are the same.\nUnder this assumption, we store a cache that maps triples of \u3008tag, condition tags, condition predictions\u3009 to \u3008current prediction\u3009. The added overhead of maintaining this data structure is tiny in comparison to making repeated predictions on the same features. In line 11 the learned policy changes making correctness subtle. For data mixing algorithms (like DAgger), this potentially changes Fi implying the memoized predictions may no longer be up-to-date. Thus this optimization is okay if the policy does not change much. We evaluate this empirically in Section 5.3.\nOptimization 2: Forced Path Collapse The second optimization we can use is a heuristic that only makes rollout predictions for a constant number of steps (e.g., 2 or 4). The intuition is that optimizing against a truly long term reward may be impossible if features are not available at the current time t0 which enable the underlying learner to distinguish between the outcome of decisions far in the future. The optimization stops rollouts after some fixed number of rollout steps.\nThis intuitive reasoning is correct, except for accumulating LOSS(...). If LOSS(...) is only declared at the end of MYRUN, then we must execute T\u2212t0 time steps making (possibly memoized) predictions. However, for many problems, it is possible to declare loss early as with Hamming loss (= number of incorrect predictions). There is no need to wait until the end of the sequence to declare a persequence loss: one can declare it after every prediction, and have the total loss accumulate (hence the \u201c+=\u201d on line 9). We generalize this notion slightly to that of a history-independent loss: Definition 1 (History-independent loss). A loss function is history-independent at state s0 if, for any final state e reachable from s0, and for any sequence s0s1s2 . . . si = e: it holds that LOSS(e) = A(s0) +B(s1s2 . . . si), where B does not depend on any state before s1.\nFor example, Hamming loss is history-independent: A(s0) corresponds to loss through s0 and B(s1 . . . si) is the loss after s0.9 When the loss function being optimized is history-independent, we allow LOSS(...) to be declared early for this optimization. In addition, for tasks like transition-base dependency parsing, although LOSS(...) is not decomposable over actions, expected cost per action can be directly computed based on gold labels [19] so the array losses can be directly specified.\nSpeed Up We analyize the time complexity for the sequence tagging task. Suppose that the cost of calling the policy is d and each state has k actions.10 Without any speed enhancements, each exe-\n9Any loss function that decomposes over the structure, as required by structured SVMs, is guaranteed to also be history-independent; the reverse is not true. Furthermore, when structured SVMs are run with a nondecomposable loss function, their runtime becomes exponential in t. When our approach is used with a loss function that\u2019s not history-independent, our runtime increases by a factor of t.\n10Because the policy is a multiclass classifier, d might hide a factor of k or log k.\ncution of MYRUN takesO(T ) time, and we execute it Tk+1 times, yielding an overall complexity of O(kT 2d) per joint example. For comparison, structured SVMs or CRFs with first order Markov dependencies run in O(k2T ) time. When both memoization and forced path collapse are in effect, the complexity of training drops to O(Tkd), similar to independent prediction. In particular, if the ith prediction only depends on the i\u22121th prediction, then at most Tk unique predictions are made.11"}, {"heading": "5 System Performance", "text": "We present two sets of experiments. In the first set, we compare the credit assignment compiler with existing libraries on two sequence tagging problems: Part of Speech tagging (POS) on the Wall Street Journal portion of the Penn Treebank; and sequence chunking problem: named entity recognition (NER) based on standard Begin-In-Out encoding on the CoNLL 2003 dataset. In the second set of experiments, we demonstrate a simple dependency parser built by our approach achieves strong results when comparing with system with similar complexity. The parser is evaluated on the standard WSJ (English, Stanford-style labels), CTB (Chinese) datasets and the CoNLL-X datasets for 10 other languages.12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42]. Details of dataset statistics, experimental settings, additional results on other applications, and pseudocode are in the appendix."}, {"heading": "5.1 Sequence Tagging Tasks", "text": "We compare our system with other freely available systems/algorithms, including CRF++ [27], CRF SGD [4], Structured Perceptron [9], Structured SVM [23], Structured SVM (DEMI-DCD) [6], and an unstructured baseline predicting each label independently, using one-against-all classification [3]13.\nFor each system, we consider two situations, either the default hyperparameters or the tuned hyperparameters that achieved the best performance on holdout data. We report both conditions to give a sense of how sensitive each approach is to the setting of hyperparameters (the amount of hyperparameter tuning directly affects effective training time). We use the built-in feature template of CRF++ to generate features and use them for other systems. The templates included neighboring words and, in the case of NER, neighboring POS tags. The CRF++ templates generate 630k unique features for the training data. However, because L2S is also able to generate features from its own templates, we also provide results for L2S (ft) in which it uses its own feature template generation.\nTraining time. In Figure 3, we show trade-offs between training time (x-axis, log scaled) and prediction accuracy (y-axis) for the aforementioned six systems. For POS tagging, the independent classifier is the fastest (trains in less than one minute) but its performance peaks at 95% accuracy. Three other approaches are in roughly the same time/accuracy trade-off: L2s, L2S (ft) and Structured\n11We use tied randomness [34] to ensure that for any time step, the same policy is called. 12PTB and CTB are prepared by following [8], and CoNLL-X is from the CoNLL shared task 06. 13 Structured Perceptron and Structured SVM (DEMI-DCD) are implemented in Illioins-SL[7]. DEMI-\nDCD is a multi-core dual approach, while Structured SVM uses cutting-planes.\nPerceptron. CRF SGD takes about twice as long. DEMI-DCD (taking a half hour) and CRF++ (taking over five hours) are not competitive. Structured SVM runs out of memory before achieving competitive performance (likely due to too many constraints). For NER the story is a bit different. The independent classifiers are not competitive. Here, the two variants of L2S totally dominate. In this case, Structured Perceptron is no longer competitive and is essentially dominated by CRF SGD. The only system coming close to L2S\u2019s performance is DEMI-DCD, although it\u2019s performance flattens out after a few minutes.14 The trends in the runs with default hyperparameters show similar behavior to those with tuned, though some of the competing approaches suffer significantly in prediction performance. Structured Perceptron has no hyperparameters.\nTest Time. In addition to training time, one might care about test time behavior. On NER, prediction times where 5.3k tokens/second (DEMI-DCD and Structured Perceptron, 20k (CRF SGD and Structured SVM), 100k (CRF++), 220k (L2S (ft)), and 285k (L2S). Although CRF SGD and Structured Perceptron fared well in terms of training time, their test-time behavior is suboptimal. When the number of labels increases from 9 (NER) to 45 (POS) the relative advantage of L2S increases further. The speed of L2S is about halved while for others, it is cut down by as much as a factor of 8 due to the O(k) vs O(k2) dependence on the label set size."}, {"heading": "5.2 Dependency Parsing", "text": "To demonstrate how the credit segment compiler handles predictions with complex dependencies, we implement an arc-eager transition-based dependency parser [35]. At each state, it takes one of the four actions {Shift, Reduce, Left,Right} based on a simple neural network with one hidden layer of size 5 and generates a dependency parse to a sentence in the end. The rollin policy is the current (learned) policy. The probability of executing the reference policy (dynamic oracle) [19] for rollout decreases over each round. We compare our model with two recent greedy transitionbased parsers implemented by the original authors, the dynamic oracle parser (DYNA) [19] and the Stanford neural network parser (SNN) [8]. We also present the best results in CoNLL-X and the best published results for CTB and PTB. The performances are evaluated by unlabeled attachment scores (UAS). Punctuation is excluded.\nTable 1 shows the results. Our implementation with only \u02dc300 lines of C++ code is competitive with DYNA and SNN, which are specifically designed for parsing. Remarkably, our system achieves strong performance on CoNLL-X without tuning any hyper-parameters, even beating heavily tuned systems participating in the challenge on one dataset. The best system to date on PTB [2] uses a global normalization, more complex neural network layers and k-best POS tags. Similarly, the best system for CTB [16] uses stack LSTM architectures tailored for dependency parsing.\n14We also tried giving CRF SGD the features computed by L2S (ft) on both POS and NER. On POS, its accuracy improved to 96.5 with essentially the same speed. On NER it\u2019s performance decreased.\n16(\u2217) SNN makes assumptions about the structure of languages and hence obtains substantially worse performance on languages with multi-root trees. (+) Languages contains more than 1% non-projective arcs, where a transition-based parser (e.g. L2S) likely underperforms graph-based parser (Best) due to the model assumptions. (#) Numbers reported in the published papers [8, 16, 2]."}, {"heading": "5.3 Empirical evaluation of optimizations", "text": "In Section 3, we discussed two approaches for computational improvements. Memoization avoids re-predicting on the same input multiple times while path collapse stops rollouts at a particular point in time. The effect of the different optimizations depends greatly on the underlying learning algorithm. For example, DAgger does not do rollouts at all, so no efficiency is gained by either optimization.17 The affected algorithms are LOLS (with mixed rollouts) and Searn.\nFigure 4 shows the effect of these optimizations on the best NER and POS systems we trained without using external resources. In the left table, we can see that memoization alone reduces overall training runtime by about 25% on NER and about 70% on POS, essentially because the overhead for the classifier on POS tagging is so much higher (45 labels versus 9). When rollouts are terminated early, the speed increases are much more modest, essentially because memoization is already accounting for much of these gains. In all cases, the final performance of the predictors is within statistical significance of each other (p-value of 0.95, paired sign test), except for Collapse@2+Memoization on NER, where the performance decrease is only insignificant at the 0.90 level. The right figure demonstrates that when \u03b1 increases, more prediction is required during the training time, and the speedup increases from a factor of 1 (no change) to a factor of as much as 9. However, as the history length increases, the speedup is more modest due to low cache hits."}, {"heading": "6 Related Work", "text": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49]. Some fit this framework.\nProbabilistic programming [21] has been an active area of research. These approaches have a different goal: Providing a flexible framework for specifying graphical models and performing inference in those models. The credit assignment compiler instead allows a developer to learn to make coherent decisions for joint prediction (\u201clearning to search\u201d). We also differ by not designing a new programming language. Instead, we have a two-function library which makes adoption and integration into existing code bases much easier.\nThe closest work to ours is Factorie [31]. Factorie is essentially an embedded language for writing factor graphs compiled into Scala to run efficiently.18 Factorie acts more like a library than a language although it\u2019s abstraction is still factor graph distributions. Similarly, Infer.NET [33], Markov Logic Networks (MNLs) [39], and Probabilistic Soft Logic (PSL) [25] concisely construct and use probabilistic graphical models. BLOG [32] falls in the same category, though with a very different focus. Similarly, Dyna [17] is a related declarative language for specifying probabilistic dynamic programs, and Saul [26] is a declarative language embedded in Scala that deals with joint prediction via integer linear programming. All of these examples have picked particular aspects of the probabilistic modeling framework to focus on. Beyond these examples, there are several approaches that essentially \u201creinvent\u201d an existing programming language to support probabilistic reasoning at\n17Training speed is only degraded by about 0.5% with optimizations on, demonstrating negligible overhead. 18Factorie-based implementations of simple tasks are still less efficient than systems like CRF SGD.\nthe first order level. IBAL [36] derives from O\u2019Caml; Church [20] derives from LISP. IBAL uses a (highly optimized) form of variable elimination for inference that takes strong advantage of the structure of the program; Church uses MCMC techniques, coupled with a different type of structural reasoning to improve efficiency."}, {"heading": "A Example TDOLR programs", "text": "In this section, a few other TDOLR programs which illustrate the ease and flexibility of programming.\nAlgorithm 4 is for a sequential detection task where the goal is to detect whether or not a sequence contains some rare element. This illustrates outputs of lengths other than the number of examples, explicit loss functions.\nIn Algorithm 5, we show an implementation of a shift-reduce dependency parser for natural language. We discuss each subcomponent below. The detailed introduction to dependency parsing is provided in the next section.\n\u2022 GETVALIDACTION returns valid actions that can be taken by the current configuration.\n\u2022 GETFEAT extracts features based on the current configuration. A detailed list of our features is in the supplementary material.\n\u2022 GETGOLDACTION implements the dynamic oracle described in [19]. The dynamic oracle returns the optimal action in any state that leads to a reachable end state with the minimal loss.\n\u2022 PREDICT is a library call implemented in the L2S system. Given training samples, L2S can learn the policy automatically. In the test phase, it returns a predicted action leading to an end state with small structured loss.\n\u2022 TRANS implements the hybrid-arc transition system described above.\n\u2022 LOSS returns the number of words whose parents are wrong. It has no effect in the test phase.\nAlgorithm 6 ENTITY_RELATION_RUN(sent as X) 1: output\u2190 INITIALIZE_STRUCTURE() 2: K\u2190 NUMBER_OF_ENTITIES(sent) 3: for n = 1 to K do 4: ref \u2190 sent.entity_type[n] 5: output.entity_type[n]\u2190 PREDICT(x=sent.entity[i], y=ref , tag=n 6: LOSS(output.entity_type[n] 6= sent.entity_type[n].true_label) 7: for n = 1 to K-1 do 8: for m = n+1 to K) do 9: ref \u2190 sent.relation_type[n,m].true_label\n10: valid_relations\u2190 FIND_VALID_RELATIONS(output.entity_type[n], output.entity_type[m]) 11: output.relation_type \u2190 PREDICT(x=sent.relation[n,m], y=ref , tag=K*(n+1)+m, valid_labels=valid_relations, condition=[n,m]) 12: LOSS(output.relation_type[n,m] 6= sent.entity_type[n,m].true_label ) 13: return output\nWe show that this parser achieves strong results across ten languages from the CoNLL-X challenge and performs well on two standard evaluation data sets, and requires only about 300 lines of readable C++ code.\nFinally, Algorithm 6 provides an implementation for jointly assigning types to name entities in a sentence and recognizing relations between them [43]. Besides features used for predicting entity and relation types. We also consider constraints that ensure the entity-type assignments and relationtype assignments are compatible with each other. For example, the first argument of the work_for relation need to be tagged as person, and the second argument has to be an organization."}, {"heading": "B Dependency Parsing", "text": "In the following, we provide a brief overview of transition-based dependency parsing. A transitionbased dependency parser takes a sequence of actions and parses a sentence from left to right by maintaining a stack S, a buffer B, and a set of dependency arcs A. The stack maintains partial parses, the buffer stores the words to be parsed, and A keeps the arcs that have been generated so far. The configuration of the parser at each stage can be defined by a triple (S,B,A). For the ease of notation, we use wp to represent the leftmost word in the buffer and use s1 and s2 to denote the top and the second top words in the stack. A dependency arc (wh, wm) is a directed edge that indicates word wh is the parent of word wm. When the parser terminates, the arcs in A form a projective dependency tree. We assume that each word only has one parent in the derived dependency parse tree, and use A[wm] to denote the parent of word wm. For labeled dependency parsing, we further assign a tag to each arc representing the dependency type between the head and the modifier. For\nAlgorithm 7 TRANS(S, B, A, action) 1: Let wp be the leftmost element in B 2: if action = SHIFT then 3: S.push(wp) 4: remove wp from B 5: else if action= REDUCE-LEFT then 6: top\u2190 S.pop() 7: A\u2190 A\u222a (wp,top) 8: else if action = REDUCE-RIGHT then 9: top\u2190 S.pop()\n10: A\u2190 A\u222a (S.top(), top) 11: return S,B,A\nsimplicity, we assume an unlabeled parser in the following description. The extension from an unlabeled parser to a labeled parser is straightforward, and is discussed at the end of this section.\nIn the following, we describe an arc-hybrid transition system due to its simplicity. The arc-eager system used in the experiments share the same spirit. In the initial configuration, the buffer B contains all the words in the sentence, a dummy root node is pushed in the stack S, and the set of arcs A is empty. The root node cannot be popped out at anytime during parsing. The system then takes a sequence of actions until the buffer is empty and the stack contains only the root node (i.e., |B| = 0 and S = {Root}). When the process terminates, a parse tree is derived. At each state, the system can take one of the following actions:\n1. SHIFT: push wp to S and move p to the next word. (Valid when |B| > 0). 2. REDUCE-LEFT: add an arc (wp, s1) to A and pop s1. (Valid when |B| > 0 and |S| > 1). 3. REDUCE-RIGHT: add an arc (s2, s1) to A and pop s1. (Valid when |S| > 1).\nAlgorithm 7 shows the execution of these actions during parsing, and Figure 5 demonstrates an example of transition-based dependency parsing."}, {"heading": "C Additional Experiment Results", "text": ""}, {"heading": "C.1 Sequential Tagging", "text": "In Figure 6, we enlarge the figures in 3 and provide the results of NER with default parameters."}, {"heading": "C.2 Dependency Parsing", "text": "Table 2 show the complete experiment results for dependency parsing. The system is evaluated on both unlabeled attachment score (UAS) and labeled attachment score. Again, conducting fair comparisons across different systems is difficult because different systems use different sets of features and different assumptions about the structure of languages. Table 3 summarizes the differences."}, {"heading": "D Experiment details", "text": ""}, {"heading": "D.1 Datasets and Tasks", "text": "We conduct experiments based on two variants of the sequence labeling problem (Algorithm 1) The first is a pure sequence labeling problem: Part of Speech tagging based on data from the Wall Street Journal portion of the Penn Treebank. The second is a sequence chunking problem: named entity recognition using data from the CoNLL 2003 dataset. See Figure 7 for example inputs and outputs for these tasks.\n20(\u2217) SNN makes assumptions about the structure of languages and hence obtains substantially worse performance on languages with multi-root trees. (+) Languages contains more than 1% non-projective arcs, where a transition-based parser (e.g. L2S) likely underperforms graph-based parser (Best) due to the model assumptions. (#) Numbers reported in the published papers [8, 16, 2].\nPart of speech tagging for English is based on the Penn Treebank tagset that includes 45 discrete labels. The accuracy reported represents number of tokens tagged correctly. This is a pure sequence labeling task. Named entity recognition for English is based on the CoNLL 2003 dataset that includes four entity types: Person, Organization, Location and Miscellaneous. We use the standard evaluation metric to report performance as macro-averaged F-measure. In order to cast this chunking task as a sequence labeling task, we use the standard Begin-In-Out (BIO) encoding, though some results suggest other encodings may be preferable [37] (we tried BILOU and our accuracies decreased). The example sentence from Figure 7 in this encoding is:\nLOC\ufe37 \ufe38\ufe38 \ufe37 Germany \u2019s rep to the ORG\ufe37 \ufe38\ufe38 \ufe37 European Union \u2019s committee PER\ufe37 \ufe38\ufe38 \ufe37 Werner Zwingmann said . . .\nB-LOC O O O O B-ORG I-ORG O O B-PER I-PER O\nDependency parser is test on the English Penn Treebank (PTB) and the CoNLL-X datasets for 9 other languages, including Arabic, Bulgarian, Chinese, Danish, Dutch, Japanese, Portuguese, Slovene and Swedish. For PTB, we convert the constituency trees to dependencies by the Stanford parser 3.3.0. We follow the standard split: sections 2 to 21 for training, section 22 for development and section 23 for testing. The POS tags in the evaluation data is assigned by the Stanford POS tagger, which has an accuracy of 97.2% on the PTB test set. For CoNLL-X, we use the given train/test splits and reserve the last 10% of training data for development if needed. The gold POS tags given in the CoNLL-X datasets are used. The CTB is prepared following the instructions in [8]."}, {"heading": "D.2 Methodology", "text": "Comparing different systems is challenging because one wishes to hold constant as many variables as possible. In particular, we want to control for both features and hyperparameters. In general, if a methodological decision cannot be made \u201cfairly,\u201d we made it in favor of competing approaches.\nTo control for features, for the two sequential tagging tasks (POS and NER), we use the built-in feature template approach of CRF++ (duplicated in CRF SGD) to generate features. The other ap-\nproaches (Structured SVM, VW Search and VW Classification) all use the features generated (offline) by CRF++. For each task, we tested six feature templates and picked the one with best development performance using CRF++. The templates included neighboring words and, in the case of NER, neighboring POS tags. However, because VW Search is also able to generate features from its own templates, we also provide results for VW Search (own fts) in which it uses its own, internal, feature template generation, which were tuned to maximize it\u2019s holdout performance on the most time-consuming run (4 passes) and include neighboring words (and POS tags, for NER) and word prefixes/suffixes.21 In all cases we use first order Markov dependencies, which lessens the speed advantage of search based structured prediction.\nTo control for hyperparameters, we first separated each system\u2019s hyperparameters into two sets: (1) those that affect termination condition and (2) those that otherwise affect model performance. When available, we tune hyperparameters for (a) learning rate and (b) regularization strength22. Additionally, we vary the termination conditions to sweep across different amounts of time spent training. For each termination condition, we can compute results using either the default hyperparameters or the tuned hyperparameters that achieved best performance on holdout data. We report both conditions to give a sense of how sensitive each approach is to the setting of hyperparameters (the amount of hyperparameter tuning directly affects effective training time).\nOne final confounding issue is that of parallelization. Of the baseline approaches, only CRF++ supports parallelization via multiple threads at training time. In our reported results, CRF++\u2019s time is the total CPU time (i.e., effectively using only one thread). Experimentally, we found that wall clock time could be decreased by a factor of 1.8 by using 2 threads, a factor of 3 using 4 threads, and a (plateaued) factor of 4 using 8 threads. This should be kept in mind when interpreting results. DEMI-DCD (for structured SVMs) also must use multiple threads. To be as fair as possible, we used 2 threads. Likewise, it can be sped up more using more threads [6]. VW (Search and Classification) can also easily be parallelized using AllReduce [1]. We do not conduct experiments with this option here because none of our training times warranted parallelization (a few minutes to train, max).\nFor dependency parsing, we fixed the hyper-parameters when test on CoNLL-X. For CTB and PTB, we tune the size of beam in beam search and the history length of predictions. For PTB, we further use dictionary features from Brown cluster."}, {"heading": "D.3 Hardware Used", "text": "All timing results were obtained on the same machine with the following configuration. Nothing else was run on this machine concurrently:\n% 2 * Intel(R) Core(TM)2 Duo CPU E8500 @ 3.16GHz 6144 KB cache 8 GB RAM, 4 GB Swap Red Hat Enterprise Linux Workstation release 6.5 (Santiago) Linux 2.6.32-431.17.1.el6.x86_64 #1 SMP\nfrom Fri Apr 11 17:27:00 EDT 2014 x86_64 x86_64 x86_64 GNU/Linux"}, {"heading": "D.4 Software Used", "text": "The precise software versions used for comparison are:\nCRF++ The popular CRF++ toolkit [27] for conditional random fields [28]. CRF SGD A stochastic gradient descent conditional random field package [4]. Structured Perceptron An implementation of the structured perceptron [9] due to [6].\n\u2022 ] The cutting-plane implementation [23] of the structured SVMs [47] for \u201cHMM\u201d problems.\nStructured SVM (DEMI-DCD) A multicore algorithm for optimizing structured SVMs called DEcoupled Model-update and Inference with Dual Coordinate Descent. Our approach is implemented in the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] that reduces to regression trained with an online rule incorporating AdaGrad [15], per-feature normalized updates [42], and importance invariant updates [24].\n21The exact templates used are provided in the supplementary materials. 22Precise details of hyperparameters tuned and their ranges is in the supplementary materials.\nVW Classification An unstructured baseline that predicts each label independently, using oneagainst-all multiclass classification [3].\n\u2022 latest Vowpal Wabbit version (May 2016) commit 2dfb1225c8b89c14552932161b95237fc90b636c\n\u2022 CRF++ version 0.58\n\u2022 crfsgd version 2.0\n\u2022 svm_hmm_learn version 3.10, 14.08.08 includes SVM-struct V3.10 for learning complex outputs, 14.08.08 includes SVM-light V6.20 quadratic optimizer, 14.08.08\n\u2022 Illinois-SL version 0.2.2"}, {"heading": "D.5 Hyperparameters Tuned", "text": "The hyperparameters tuned and the values we considered for each system are:\nCRF++\n% termination parameters: number of passes (--max_iter) { 2, 4, 8, 16, 32, 64, 128 } termination criteria (--eta) 0.000000000001 (to prevent termination)\ntuned hyperparameters (default is *): learning algorithm (--algorithm) { CRF*, MIRA } cost parameter (--cost) { 0.0625, 0.125, 0.25, 0.5, 1*, 2, 4, 8, 16 }\nCRF SGD\n% termination parameters: number of passes (-r) { 1, 2, 4, 8, 16, 32, 64, 128 }\ntuned hyperparameters (default is *): regularization parameter (-c) { 0.0625, 0.125, 0.25, 0.5, 1*, 2, 4, 8, 16 } learning rate (-s) { auto*, 0.1, 0.2, 0.5, 1, 2, 5 }\nStructured SVM\n% termination parameters: epsilon tolerance (-e) { 4, 2, 1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001 }\ntuned hyperparameters (default is *): regularization parameter (-c) { 0.0625, 0.125, 0.25, 0.5, 1*, 2, 4, 8, 16 }\nStructured Perceptron\n% termination parameters: number of passes (MAX_NUM_ITER) { 1, 2, 4, 8, 16, 32, 64, 128 }\ntuned hyperparameters (default is *): NONE\nStructured SVM (DEMI-DCD)\n% termination parameters: number of passes (MAX_NUM_ITER) { 1, 2, 4, 8, 16, 32, 64, 128 }\ntuned hyperparameters (default is *): regularization (C_FOR_STRUCTURE) { 0.01, 0.05, 0.1*, 0.5, 1.0 }\nL2S\ntermination parameters: number of passes (--passes) { 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 4 } (note: a number of passes < 1 means that we perform one full pass, but _subsample_ the training positions for each sequence at the given rate)\ntuned hyperparameters (default is *): base classifier { csoaa*} interpolation rate 10^{-10, -9, -8, -7, -6 }\nVW Classifier\ntermination parameters: number of passes (--passes) { 1, 2, 4 }\ntuned hyperparameters (default is *): learning rate (-l) { 0.25, 0.5*, 1.0 }"}, {"heading": "E Templates Used", "text": "For part of speech tagging (CRF++):\nU00:%x[-2,0] U01:%x[-1,0] U02:%x[0,0] U03:%x[1,0] U04:%x[2,0]\nFor named entity recognition (CRF++):\nU00:%x[-2,0] U01:%x[-1,0] U02:%x[0,0] U03:%x[1,0] U04:%x[2,0]\nU10:%x[-2,1] U11:%x[-1,1] U12:%x[0,1] U13:%x[1,1] U14:%x[2,1]\nU15:%x[-2,1]/%x[-1,1] U16:%x[-1,1]/%x[0,1] U17:%x[0,1]/%x[1,1] U18:%x[1,1]/%x[2,1]\n(where words are in position 0 and POS is in 1)\nAdditional features for L2S (ft) on POS Tagging:\n-- the left and the right tokens of each word -- the first and the last 2 characters for each token\nFor L2S (ft) on NER:\n-- the left and the right two tokens of each word -- the POS tags of the left and the right tokens for each word -- the last charaster for each token"}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u00edk", "John Langford"], "venue": "arXiv preprint arXiv:1110.4198,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Error limiting reductions between classification tasks", "author": ["Alina Beygelzimer", "Varsha Dani", "Tom Hayes", "John Langford", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Learning to search better than your teacher", "author": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Multi-core structural SVM training", "author": ["Kai-Wei Chang", "Vivek Srikumar", "Dan Roth"], "venue": "In ECML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Illinoissl: A JAVA library for structured prediction", "author": ["Kai-Wei Chang", "Shyam Upadhyay", "Ming-Wei Chang", "Vivek Srikumar", "Dan Roth"], "venue": "Arxiv,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning"], "venue": "In EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu"], "venue": "Machine Learning Journal,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Output space search for structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "HC-Search: A learning framework for search-based structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Compiling comp ling: Practical weighted dynamic programming and the dyna language", "author": ["Jason Eisner", "Eric Goldlust", "Noah A. Smith"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Fast decoding and optimal decoding for machine translation", "author": ["Ulrich Germann", "Mike Jahr", "Kevin Knight", "Daniel Marcu", "Kenji Yamada"], "venue": "Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre"], "venue": "Transactions of the ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Church: a language for generative models", "author": ["Noah Goodman", "Vikash Mansinghka", "Daniel Roy", "Keith Bonawitz", "Josh Tenenbaum"], "venue": "In UAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Probabilistic programming", "author": ["Andrew D. Gordon", "Thomas A. Henzinger", "Aditya V. Nori", "Sriram K. Rajamani"], "venue": "In International Conference on Software Engineering (ICSE, FOSE track),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In NAACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Cutting-plane training of structural SVMs", "author": ["Thorsten Joachims", "Thomas Finley", "Chun-Nam Yu"], "venue": "Machine Learning Journal,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Online importance weight aware updates", "author": ["Nikos Karampatziakis", "John Langford"], "venue": "In UAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "A short introduction to probabilistic soft logic", "author": ["Angelika Kimmig", "Stephen Bach", "Matthias Broecheler", "Bert Huang", "Lise Getoor"], "venue": "In NIPS Workshop on Probabilistic Programming,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Saul: Towards declarative learning based programming", "author": ["Parisa Kordjamshidi", "Dan Roth", "Hao Wu"], "venue": "In IJCAI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "http://hunch.net/ ~vw", "author": ["John Langford", "Alex Strehl", "Lihong Li"], "venue": "Vowpal wabbit,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Maximum entropy Markov models for information extraction and segmentation", "author": ["Andrew McCallum", "Dayne Freitag", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "FACTORIE: probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "BLOG: probabilistic models with unknown objects", "author": ["Brian Milch", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L Ong", "Andrey Kolobov"], "venue": "Statistical relational learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["Andrew Ng", "Michael Jordan"], "venue": "In UAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In IWPT, pages 149\u2013160,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Ibal: A probabilistic rational programming language", "author": ["Avi Pfeffer"], "venue": "In IJCAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth"], "venue": "In CoNLL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Boosting structured prediction for imitation learning", "author": ["Nathan Ratliff", "David Bradley", "J. Andrew Bagnell", "Joel Chestnutt"], "venue": "In NIPS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["St\u00e9phane Ross", "J. Andrew Bagnell"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Stephane Ross", "Geoff J. Gordon", "J. Andrew Bagnell"], "venue": "In AI-Stats,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Normalized online learning", "author": ["St\u00e9phane Ross", "Paul Mineiro", "John Langford"], "venue": "In UAI,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Global inference for entity and relation identification via a linear programming formulation. In Introduction to Statistical Relational Learning", "author": ["Dan Roth", "Scott Wen-Tau Yih"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["Wee Meng Soon", "Hwee Tou Ng", "Daniel Chung Yong Lim"], "venue": "Computational Linguistics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Umar Syed", "Robert E. Schapire"], "venue": "In NIPS,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Max-margin Markov networks", "author": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller"], "venue": "In NIPS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2003}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasmine Altun"], "venue": "In ICML,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}, {"title": "On learning linear ranking functions for beam search", "author": ["Yuehua Xu", "Alan Fern"], "venue": "In ICML,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}], "referenceMentions": [{"referenceID": 27, "context": "Can we make joint prediction algorithms as easy and fast to program and compute while maintaining their theoretical benefits? Methods making a sequence of sub-decisions have been proposed for handling complex joint predictions in a variety of applications, including sequence tagging [30], dependency parsing (known as transition-based method) [35], machine translation [18], and co-reference resolution [44].", "startOffset": 284, "endOffset": 288}, {"referenceID": 31, "context": "Can we make joint prediction algorithms as easy and fast to program and compute while maintaining their theoretical benefits? Methods making a sequence of sub-decisions have been proposed for handling complex joint predictions in a variety of applications, including sequence tagging [30], dependency parsing (known as transition-based method) [35], machine translation [18], and co-reference resolution [44].", "startOffset": 344, "endOffset": 348}, {"referenceID": 16, "context": "Can we make joint prediction algorithms as easy and fast to program and compute while maintaining their theoretical benefits? Methods making a sequence of sub-decisions have been proposed for handling complex joint predictions in a variety of applications, including sequence tagging [30], dependency parsing (known as transition-based method) [35], machine translation [18], and co-reference resolution [44].", "startOffset": 370, "endOffset": 374}, {"referenceID": 39, "context": "Can we make joint prediction algorithms as easy and fast to program and compute while maintaining their theoretical benefits? Methods making a sequence of sub-decisions have been proposed for handling complex joint predictions in a variety of applications, including sequence tagging [30], dependency parsing (known as transition-based method) [35], machine translation [18], and co-reference resolution [44].", "startOffset": 404, "endOffset": 408}, {"referenceID": 8, "context": ", [10, 12, 14, 22, 41]) have been investigated.", "startOffset": 2, "endOffset": 22}, {"referenceID": 10, "context": ", [10, 12, 14, 22, 41]) have been investigated.", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": ", [10, 12, 14, 22, 41]) have been investigated.", "startOffset": 2, "endOffset": 22}, {"referenceID": 20, "context": ", [10, 12, 14, 22, 41]) have been investigated.", "startOffset": 2, "endOffset": 22}, {"referenceID": 36, "context": ", [10, 12, 14, 22, 41]) have been investigated.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "In contrast, learning to search approaches [5, 11, 40] automatically handle the credit assignment problem by decomposing the production of the joint output in terms of an explicit search space (states, actions, etc.", "startOffset": 43, "endOffset": 54}, {"referenceID": 9, "context": "In contrast, learning to search approaches [5, 11, 40] automatically handle the credit assignment problem by decomposing the production of the joint output in terms of an explicit search space (states, actions, etc.", "startOffset": 43, "endOffset": 54}, {"referenceID": 35, "context": "In contrast, learning to search approaches [5, 11, 40] automatically handle the credit assignment problem by decomposing the production of the joint output in terms of an explicit search space (states, actions, etc.", "startOffset": 43, "endOffset": 54}, {"referenceID": 25, "context": "These have formal correctness guarantees which differ qualitatively from graphical models such as the Conditional Random Fields [28] and structured SVMs [46, 47].", "startOffset": 128, "endOffset": 132}, {"referenceID": 41, "context": "These have formal correctness guarantees which differ qualitatively from graphical models such as the Conditional Random Fields [28] and structured SVMs [46, 47].", "startOffset": 153, "endOffset": 161}, {"referenceID": 42, "context": "These have formal correctness guarantees which differ qualitatively from graphical models such as the Conditional Random Fields [28] and structured SVMs [46, 47].", "startOffset": 153, "endOffset": 161}, {"referenceID": 12, "context": "Comprehensive strategies for defining search space have been discussed [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "By varying them and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN [11], DAGGER [41], AGGREVATE [40], and LOLS [5].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "By varying them and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN [11], DAGGER [41], AGGREVATE [40], and LOLS [5].", "startOffset": 156, "endOffset": 160}, {"referenceID": 35, "context": "By varying them and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN [11], DAGGER [41], AGGREVATE [40], and LOLS [5].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "By varying them and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN [11], DAGGER [41], AGGREVATE [40], and LOLS [5].", "startOffset": 187, "endOffset": 190}, {"referenceID": 17, "context": ") is not decomposable over actions, expected cost per action can be directly computed based on gold labels [19] so the array losses can be directly specified.", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42].", "startOffset": 106, "endOffset": 109}, {"referenceID": 13, "context": "12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42].", "startOffset": 138, "endOffset": 150}, {"referenceID": 22, "context": "12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42].", "startOffset": 138, "endOffset": 150}, {"referenceID": 37, "context": "12 Our approach is implemented using the Vowpal Wabbit [29] toolkit on top of a cost-sensitive classifier [3] trained with online updates [15, 24, 42].", "startOffset": 138, "endOffset": 150}, {"referenceID": 7, "context": "We compare our system with other freely available systems/algorithms, including CRF++ [27], CRF SGD [4], Structured Perceptron [9], Structured SVM [23], Structured SVM (DEMI-DCD) [6], and an unstructured baseline predicting each label independently, using one-against-all classification [3]13.", "startOffset": 127, "endOffset": 130}, {"referenceID": 21, "context": "We compare our system with other freely available systems/algorithms, including CRF++ [27], CRF SGD [4], Structured Perceptron [9], Structured SVM [23], Structured SVM (DEMI-DCD) [6], and an unstructured baseline predicting each label independently, using one-against-all classification [3]13.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "We compare our system with other freely available systems/algorithms, including CRF++ [27], CRF SGD [4], Structured Perceptron [9], Structured SVM [23], Structured SVM (DEMI-DCD) [6], and an unstructured baseline predicting each label independently, using one-against-all classification [3]13.", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "We compare our system with other freely available systems/algorithms, including CRF++ [27], CRF SGD [4], Structured Perceptron [9], Structured SVM [23], Structured SVM (DEMI-DCD) [6], and an unstructured baseline predicting each label independently, using one-against-all classification [3]13.", "startOffset": 287, "endOffset": 290}, {"referenceID": 30, "context": "Three other approaches are in roughly the same time/accuracy trade-off: L2s, L2S (ft) and Structured We use tied randomness [34] to ensure that for any time step, the same policy is called.", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "PTB and CTB are prepared by following [8], and CoNLL-X is from the CoNLL shared task 06.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "13 Structured Perceptron and Structured SVM (DEMI-DCD) are implemented in Illioins-SL[7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 31, "context": "To demonstrate how the credit segment compiler handles predictions with complex dependencies, we implement an arc-eager transition-based dependency parser [35].", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "The probability of executing the reference policy (dynamic oracle) [19] for rollout decreases over each round.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "We compare our model with two recent greedy transitionbased parsers implemented by the original authors, the dynamic oracle parser (DYNA) [19] and the Stanford neural network parser (SNN) [8].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "We compare our model with two recent greedy transitionbased parsers implemented by the original authors, the dynamic oracle parser (DYNA) [19] and the Stanford neural network parser (SNN) [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 1, "context": "The best system to date on PTB [2] uses a global normalization, more complex neural network layers and k-best POS tags.", "startOffset": 31, "endOffset": 34}, {"referenceID": 14, "context": "Similarly, the best system for CTB [16] uses stack LSTM architectures tailored for dependency parsing.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "() Numbers reported in the published papers [8, 16, 2].", "startOffset": 44, "endOffset": 54}, {"referenceID": 14, "context": "() Numbers reported in the published papers [8, 16, 2].", "startOffset": 44, "endOffset": 54}, {"referenceID": 1, "context": "() Numbers reported in the published papers [8, 16, 2].", "startOffset": 44, "endOffset": 54}, {"referenceID": 8, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 113, "endOffset": 121}, {"referenceID": 20, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 133, "endOffset": 141}, {"referenceID": 12, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 133, "endOffset": 141}, {"referenceID": 10, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 154, "endOffset": 174}, {"referenceID": 34, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 154, "endOffset": 174}, {"referenceID": 40, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 154, "endOffset": 174}, {"referenceID": 43, "context": "Several algorithms are similar to learning to search approaches, including the incremental structured perceptron [10, 22], HC-Search [13, 14], and others [12, 38, 45, 48, 49].", "startOffset": 154, "endOffset": 174}, {"referenceID": 19, "context": "Probabilistic programming [21] has been an active area of research.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "The closest work to ours is Factorie [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "NET [33], Markov Logic Networks (MNLs) [39], and Probabilistic Soft Logic (PSL) [25] concisely construct and use probabilistic graphical models.", "startOffset": 80, "endOffset": 84}, {"referenceID": 29, "context": "BLOG [32] falls in the same category, though with a very different focus.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Similarly, Dyna [17] is a related declarative language for specifying probabilistic dynamic programs, and Saul [26] is a declarative language embedded in Scala that deals with joint prediction via integer linear programming.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Similarly, Dyna [17] is a related declarative language for specifying probabilistic dynamic programs, and Saul [26] is a declarative language embedded in Scala that deals with joint prediction via integer linear programming.", "startOffset": 111, "endOffset": 115}], "year": 2016, "abstractText": "Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.", "creator": "LaTeX with hyperref package"}}}