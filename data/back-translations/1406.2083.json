{"id": "1406.2083", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "On the Decreasing Power of Kernel and Distance Based Nonparametric Hypothesis Tests in High Dimensions", "abstract": "This paper deals with two related methods for two random sampling and independence tests that have emerged in the last decade: the maximum mean discrepancy (MMD) for the first problem and the distance correlation (dCor) for the second. Both methods have been proposed for high-dimensional problems and sometimes claim not to be affected by the increasing dimensionality of the samples. We will show theoretically and practically that the performance of both methods (for different reasons) actually decreases polynomially with dimension. We will also analyze median heuristics, a method for selecting tuning parameters of translation invariant nuclei. We show that different bandwidth decisions could cause the MMD to decay polynomially or even exponentially in dimension.", "histories": [["v1", "Mon, 9 Jun 2014 05:59:21 GMT  (96kb,D)", "http://arxiv.org/abs/1406.2083v1", "15 pages, 7 figures"], ["v2", "Mon, 24 Nov 2014 00:23:35 GMT  (97kb,D)", "http://arxiv.org/abs/1406.2083v2", "19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference on Artificial Intelligence (with author order reversed from ArXiv)"]], "COMMENTS": "15 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "authors": ["aaditya ramdas", "sashank jakkam reddi", "barnab\u00e1s p\u00f3czos", "aarti singh", "larry a wasserman"], "accepted": true, "id": "1406.2083"}, "pdf": {"name": "1406.2083.pdf", "metadata": {"source": "CRF", "title": "Kernel MMD, the Median Heuristic and Distance Correlation in High Dimensions", "authors": ["Sashank J. Reddi", "Aaditya Ramdas", "Barnab\u00e1s P\u00f3czos", "Larry Wasserman"], "emails": ["sjakkamr@cs.cmu.edu", "aramdas@cs.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu", "larry@stat.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Nonparametric two-sample testing and independence testing are two related problems of paramount importance in statistics. In the former, we have two sets of samples and we would like to determine if these were drawn from the same or different distributions. In the latter, we have one set of samples from a multivariate distribution, and we would like to determine if the joint is the product of marginals or not. The two problems are related because an algorithm for testing the former can be used to test the latter.\nKernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]). The corresponding test uses empirical distributions for plug-in estimators (described later) and is consistent (for fixed dimension, power tends to one as number of samples becomes infinite) against any single fixed alternative.\nDistance correlation is a quantity introduced in Sze\u0301kely et al. [2007] to tackle the second problem using distances between pairs of points. The population quantity is a weighted norm of difference between characteristic functions of the joint and product-of-marginal distributions, which is zero if and only if the random variables are independent. Empirically, one can calculate the matrix dot-product between the two pairwise centered distance matrices (one for each random variable) giving a consistent test against any dependent alternative.\nWe will explore the behavior of these related methods when the number of dimensions could be as large as, or larger than the number of samples. We will challenge existing folklore that the \u201cperformance\u201d of both tests is unaffected by the underlying\n\u2217Both student authors had equal contribution.\nar X\niv :1\n40 6.\n20 83\nv1 [\nst at\ndimensionality by explaining the source of both misconceptions. We demonstrate theoretically and experimentally that the power of both these methods actually goes down as d increases relative to n. We will also see explicit examples where the median heuristic for bandwidth selection leads to good power and when it is suboptimal."}, {"heading": "2 Summary of Contributions", "text": "Kernel Maximum Mean Discrepancy (MMD) Gretton et al. [2012a] showed that the estimated MMD converges to the true MMD at rate O(n\u22121/2) independently of dimension d. This gives the impression that the two sample test works well for large d. The result is correct but possibly misleading. We will see that the true value of the population MMD can be polynomially or even exponentially small in d (we were notified that a special case of Corollary 1 was earlier independently noted by Balakrishnan [2013], but do not know other examples). Also, while it is known that MMD2 is smaller than the KL-divergence, for the first time we give several examples where it can be polynomially or exponentially smaller in d than KL. This does indicate (not imply) that the test might have low power, and we indeed experimentally demonstrate that the power against fair alternatives (discussed later) degrades polynomially in d.\nMedian Heuristic A crucial issue when using the Laplace kernel (exp(\u2212\u2016x\u2212x\u2032\u20161/\u03b3)) or the Gaussian kernel (exp(\u2212\u2016x\u2212 x\u2032\u20162/2\u03b32) for MMD is the choice of the associated bandwidth \u03b3. One of the most common heuristic choices for \u03b3 in the literature, is to choose it as the median distance between all pairs of points. This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic). However, in all simulations, power goes to zero as d\u2192\u221e for all settings of the bandwidth.\nDistance Correlation (dCor) Sze\u0301kely and Rizzo [2013] studied distance correlation in high dimensions where they considered the following example. (X,Y ) are drawn from a standard normal, and even though they are independent, as dimension is increased (keeping number of samples fixed), the sample test statistic approaches one even though the true dCor is zero. So even though the test statistic is consistent with n increasing and d fixed, in high dimensions its value approaches 1. They thus motivate an unbiased dCor statistic (\u201cudCor\u201d), and show that for the above example, it is well behaved (centered at zero) as d increases. However, this only tells half the story - several other facts also matter for the complete picture. Specifically, the quantity that matters is the power, and the behavior of null and alternate distributions of biased and unbiased test statistics determine their power. We will empirically show that there is no difference in the polynomial decay of power of dCor and udCor against fair alternatives. We also experimentally demonstrate the different reasons that they have low power. To the best of our knowledge, there has been no prior attempt to study the power of distance correlation in the literature, in either low or high dimensions.\nDue to limited space, we only provide a brief introduction to MMD and dCor, and we refer the reader to the aforementioned papers for detailed treatment. We will first get into the details of our results about MMD and the median heuristic (Sec. 3.5), returning to dCor in Sec. 4."}, {"heading": "3 The Power of MMD in High Dimensions", "text": "Let P be a class of continuous distributions on topological space X . Our goal is to test"}, {"heading": "H0 : p = q against H1 : p 6= q", "text": "where p, q \u2208 P We construct a test for the hypothesis from samples (x1, . . . , xn) and (y1, . . . , ym) from distributions p and q, respectively. To do so, one defines a divergence measure \u03c1(p, q) such that: (a) \u03c1(p, q) \u2265 0 for all p, q \u2208 F and (b) \u03c1(p, q) = 0 if and only if p = q. We are interested in the high-dimensional regime i.e X \u2286 Rd for large d, possibly larger than n. Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn\u2019t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard\n[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse.\nLet us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD\u2019s power decays with d against fair alternatives.\nLet F be a class of functions f : X \u2192 R. The MMD is defined as:\nMMD(F , p, q) := sup f\u2208F Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)].\nWe restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := \u2016\u00b5p \u2212 \u00b5q\u2016H where \u00b5p = Ex\u223cp[k(x, .)] for any distribution p \u2208 P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD2 from Gretton et al. [2012a]:\nMMD2b(p, q) = 1\nn2 n\u2211 i=1 n\u2211 j=1 k(xi, xj) + 1 m2 m\u2211 i=1 m\u2211 j=1 k(yi, yj)\u2212 2 n\u2211 i=1 m\u2211 j=1 k(xi, yj).\nA similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result.\nTheorem 1. Gretton et al. [2012a] Suppose 0 \u2264 k(x, x) \u2264 K, then with probability at least 1\u2212 \u03b4, we have\n|MMD2b(p, q)\u2212MMD 2(p, q)| \u2264 2\n(( K\nn\n)1/2 + ( K\nm\n)1/2)( 1 + log ( 2\n\u03b4\n)) .\nThe unbiased estimator has a very similar convergence rate. Since this rate is independent of dimension, it is sometimes claimed not to suffer from any curse of dimensionality. We will show that this claim is misleading, i.e. hypothesis testing using such quantities can still suffer from the curse."}, {"heading": "3.1 The Difficulty of Analytically Characterizing Power", "text": "The power of a test depends on the distribution of the statistic under H0 and H1. If the distributions are nearly Gaussian, the mean statistic and its standard deviation (s.d.) under both H0 (\u00b50, \u03c30) and H1 (\u00b51, \u03c31) play a role in determining power (the probability mass of the alternate distribution beyond a predetermined \u03b1 in the right tail of the null distribution). Characterizing the asymptotic (as d is fixed and n goes to infinity) behavior of the test statistic under the null and alternative is usually hard. For example, the above MMD2b estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMD2l is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities \u00b50, \u03c30, \u00b51, \u03c31 actually vary with d and n. Further, in the highdimensional setting, classical large sample theory does not apply as d can be comparable to or larger than n, and calculations assuming the \u201casymptotically\u201d normal distribution can be misleading.\nSpecifically, consider Q := \u221a n(MMD2l\u2212MMD\n2) \u03c31\nN(0, 1) d\n=: Z as shown in Gretton et al. [2012a]. Thus, an asymptotic level \u03b1 test rejects when MMD2l > \u03c30z\u03b1/ \u221a n. Under the alternate, the power is\nP (\nMMD2l > \u03c30z\u03b1\u221a n\n) = P ( Q >\n\u03c30z\u03b1 \u03c31 \u2212 \u221a nMMD2 \u03c31\n) \u2248 P ( Z >\n\u03c30z\u03b1 \u03c31 \u2212 \u03bdn\n) ,\nwhere \u03bdn,d = \u221a nMMD2/\u03c31 is the non-centrality parameter. The power will tend to one and the test will be consistent only if \u03bdn,d \u2192 \u221e. Hence, one might be tempted to use \u03bdn,d to measure the effectiveness of the test, and indeed choosing the kernel (or bandwidth) to maximize \u03bdn,d was studied by Gretton et al. [2012b]. However, in the high dimensional setting the normal approximation used in the last step can be extremely poor, as we have experimentally verified. Development of high dimensional theory, like Barry-Esseen bounds to explicitly characterize the closeness of Q and Z, is needed.\nHence on the issue of power, we will only demonstrate carefully designed experiments showing that MMD does suffer from the curse of dimensionality against reasonable alternatives. We will give two examples where explicit calculations for the population value of MMD2 are possible (not necessarily implying anything about power) and demonstrate that MMD2 can be much smaller than the KL-divergence. These examples will also yield insights into the crucial bandwidth choice.\n3.2 Relating MMD2, TV, KL To simplify our analysis, let us restrict ourselves to translation invariant kernels i.e. for all \u03b4, we have k(x + \u03b4, x\u2032 + \u03b4) = k(x, x\u2032). For these kernels, it is relatively easy to characterize MMD2.\nLemma 1. For translation invariant kernels, there exists a pdf s such that\nMMD2(p, q) = \u222b s(w)|\u03a6p(w)\u2212 \u03a6q(w)|2dw,\nwhere \u03a6p,\u03a6q denote the characteristic functions of p, q respectively.\nThe above lemma can be proved using Bochner\u2019s theorem (Appendix). Note that |\u03a6p(w)\u2212 \u03a6q(w)| = \u2223\u2223\u2223\u2223\u222b x exp(iw>x)(p(x)\u2212 q(x))dx \u2223\u2223\u2223\u2223 \u2264 \u222b x |p(x)\u2212 q(x)|dx = TV(p, q).\nFrom the fact that |\u03a6p(w)\u2212 \u03a6q(w)| \u2264 TV(p, q) and Pinsker\u2019s inequality, we can conclude\nLemma 2. For translation invariant kernels, MMD2(p, q) \u2264 TV2(p, q) \u2264 2KL(p, q).\nA more general version of the above lemma for all kernels (with a different constant than 1) is presented in Sriperumbudur et al. [2012] (Proposition 5.1). The aforementioned result gives an intuitive justification that, in general, MMD2 is smaller than the other well known non-parametric divergence measures, whose estimators suffer from the curse of dimensionality. We will see that MMD2 can be polynomially, and sometimes exponentially smaller than KL, and while that does not immediately imply lower power, it is an important determining factor. The proofs of the following examples are in the Appendix."}, {"heading": "3.3 Example: Gaussian Kernel for Different Mean, Same Covariance Normal Distributions", "text": "Theorem 2. Let \u00b51, \u00b52 \u2208 Rd. Suppose p : N (\u00b51,\u03a3) and q : N (\u00b52,\u03a3). Then MMD2 between p and q using a Gaussian kernel with bandwidth \u03b3 is,\nMMD2(p, q) = 2\n( \u03b32\n2\n)d/2 1\u2212 exp(\u2212(\u00b51 \u2212 \u00b52)>(\u03a3 + \u03b32I/2)\u22121(\u00b51 \u2212 \u00b52)/4)\n|\u03a3 + \u03b32I/2|1/2 .\nSuppose \u03a3 = \u03c32I . Using Taylor\u2019s theorem for 1\u2212 e\u2212x \u2248 x and ignoring\u2212x 2\n2 and other smaller remainder terms for clarity, Then the above expression simplifies to\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n\u03b32(1 + 2\u03c32/\u03b32)d/2+1 .\nKeep in mind that the KL-divergence in the case of \u03a3 = \u03c32I is given by\nKL(p, q) = 1\n2 (\u00b51 \u2212 \u00b52)T\u03a3\u22121(\u00b51 \u2212 \u00b52) =\n\u2016\u00b51 \u2212 \u00b52\u20162\n2\u03c32 ."}, {"heading": "3.4 Example: Gaussian Kernel for Same Mean, Different Covariance Normal Distribution", "text": "The next example is for the Gaussian kernel with product Gaussian distributions having the same mean and different variances. Example 3 in Sec. 4.2 of Sriperumbudur et al. [2012] has related calculations, with a different aim that we discuss in Section 3.7.\nTheorem 3. Suppose p : \u2297d\u22121i=1N (0, \u03c32)\u2297N (0, \u03c42) and q : \u2297di=1N (0, \u03c32). Then MMD 2 between q and p using a Gaussian kernel with bandwidth \u03b3 is\nMMD2(p, q) \u2248 (\u03c4 2 \u2212 \u03c32)2\n\u03b34(1 + 4\u03c32/\u03b32)d/2\u22121/2 .\nBy Taylor\u2019s theorem for log x, the KL divergence in this case is approximately given by\nKL(p, q) = 1\n2 (tr(\u03a3\u221211 \u03a30)\u2212 d\u2212 log(det \u03a30/ det \u03a31))\n= 1 2 (\u03c42/\u03c32 \u2212 1\u2212 log(\u03c42/\u03c32)) \u2248 (\u03c4\n2 \u2212 \u03c32)2\n4\u03c34 ."}, {"heading": "3.5 Bandwidth Choice and the Median Heuristic", "text": "We investigate how bandwidth choice affects the population MMD2 for the example in Theorem 2 (corollaries for Theorem 3 are similar). In what follows, scaling bandwidth choices by a constant does not change the qualitative behavior, so we leave out constants for simplicity. For clarity in the following corollaries, we also ignore the Taylor residuals, and use (1+1/d)d \u2248 e for large d.\nUnderestimated bandwidth\nCorollary 1. Suppose \u03a3 = \u03c32I . If we choose \u03b3 = \u03c3d1/2\u2212 for 0 < \u2264 1/2, then\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n\u03c32(d1\u22122 + 2) exp(d2 /2) .\nHence, the population MMD2 goes to zero exponentially fast in d (verified by experiments that follow). The special case of a constant bandwidth with = 1/2 has already been noted by Balakrishnan [2013].\nThe median heuristic. We approximate the choice of the median heuristic by \u03b32 = E\u2016xi\u2212xj\u20162. Note that when \u03a3 = \u03c32I , we have E\u2016xi \u2212 xj\u20162 \u2248 2\u03c32d + \u2016\u00b51 \u2212 \u00b52\u20162 (the first term dominates, see Sec.3.7 for explanation). Also, the experimental median (Sec.3.7) is exactly of this order. Corollary 2. Suppose \u03a3 = \u03c32I . If we choose \u03b3 = \u03c3 \u221a d, then\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n\u03c32(d+ 2)e .\nNote the population MMD2 goes to zero polynomially as 1/d. This is the largest MMD value one can hope for, but it is still smaller than the KL divergence by a factor of 1/d.\nOverestimating the bandwidth\nCorollary 3. Suppose \u03a3 = \u03c32I . If \u03b3 = \u03c3d1/2+ for > 0, then\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n\u03c32(d1+2 + 2) exp(1/2d2 ) .\nHence, the population MMD2 goes to zero polynomially as 1/d1+2 , since exp(1/2d2 ) \u2248 1 for large d. So one pays very little for overestimating the bandwidth, compared to underestimating it."}, {"heading": "3.6 Example : Laplace Kernel for Different Mean, Same Variance Laplace Distributions", "text": "Theorem 4 (MMD2 Approximation). Let \u00b51, \u00b52 \u2208 Rd. Suppose p : \u2297iLaplace(\u00b51,i, \u03c3) and q : \u2297iLaplace(\u00b52,i, \u03c3). Using a Laplace kernel with bandwidth \u03b3, we have\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n2\u03c3\u03b3 (1 + \u03c3/\u03b3) d .\nNote that by applying Taylor\u2019s theorem for e\u2212x \u2248 1\u2212 x+ x2/2, we have\nKL(p, q) = e\u2212 \u2016\u00b51\u2212\u00b52\u2016 \u03c3 \u2212 1 + \u2016\u00b51\u2212\u00b52\u2016\u03c3 \u2248 \u2016\u00b51 \u2212 \u00b52\u20162\n2\u03c32 .\nOnce again, E\u2016xi \u2212 xj\u20162 \u2248 2\u03c32d and indeed experimentally the median heuristic chooses \u03b3 \u2248 \u03c3 \u221a d. This time, the median heuristic is suboptimal and MMD2 drops exponentially in d. A larger bandwidth of \u03b3 = \u03c3d is optimal, making the denominator \u2248 \u03c32de. An overestimated bandwidth again leads to only a slow polynomial drop in MMD. In summary: Corollary 4 (Underestimated bandwidth, median heuristic). If we choose \u03b3 = \u03c3d1\u2212 for 0 < < 1,\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n2\u03c32d1\u2212 exp(d ) .\nCorollary 5 (Correct or Overestimated bandwidth). If we choose \u03b3 = \u03c3d1+ , for \u2265 0\nMMD2(p, q) \u2248 \u2016\u00b51 \u2212 \u00b52\u2016 2\n2\u03c32d1+ exp(1/d ) .\n3.7 MMD2 Power for the mean-separated Gaussian and Laplace examples The null hypothesis is either chosen as p = q = N (0, \u03c32I) or p = q = \u2297di=1Laplace(0, \u03c3). For the alternative, p is the same and we choose q = N(\u00b5, \u03c32I) or q = \u2297di=1Laplace(\u00b5i, \u03c3). The choice of \u00b5 is subtle - any effect on power should not arise from the unfair choice of alternative. We choose to keep \u2016\u00b5\u20162/\u03c32 constant, for example by setting \u00b5 = (1, 0, ..., ) for all d. This can be justified by :\n\u2022 The KL divergence between the two distributions equals (or scales like) \u2016\u00b5\u20162/2\u03c32 in both cases, and hence by keeping the KL constant with d, we are not making it information theoretically harder or easier to distinguish the hypotheses as d grows.\n\u2022 This quantity represents the Mahalanobis distance \u00b5T\u03a3\u22121\u00b5 which is considered as signal-to-noise-ratio, and stays constant with d.\nFig. 1 does confirm that power drops with dimension in both settings. The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD2, where the authors choose to let the mean separation be (1, 1, 1, ..., 1) which makes the problem easier with dimension. Also, they use it to argue that the mean squared error (as summarised by Thm 1) with increasing n is indeed independent of d. Another relevant comparison is with Fig. 5A in Gretton et al. [2012a] where they show an extremely slow decrease in power with dimension for the same example of mean-shifted Gaussians with Gaussian kernel that we consider. Since the details are not in the paper, it was verified by personal communication that the bandwidth was chosen to maximize MMD, but that the means we chosen such that \u2016\u00b51\u2212\u00b52\u2016 equaled d.\nWe also ran a simulation to verify that our derived expressions and approximations for MMD2 are accurate. Fig 3 in the Appendix shows the results."}, {"heading": "4 The Power of Distance Correlation (dCor) in high dimensions", "text": "Now we discuss nonparametric independence testing. Given n samples (xi, yi) \u2208 Rdx+dy (dx 6= dy is allowed) drawn from a joint distribution PXY with marginals PX , PY , we would like to test\nH0 : PXY = PXPY against H1 : PXY 6= PXPY .\nThe authors of Sze\u0301kely et al. [2007] introduce a test statistic called (squared) distance covariance which is defined as\ndCov2n(X,Y ) = 1\nn2 tr(A\u0303B\u0303) =\n1\nn2 n\u2211 i,j=1 A\u0303ijB\u0303ij . (1)\nHere, A\u0303 = HAH, B\u0303 = HBH where H = I \u2212 11T /n is a centering matrix, and A,B are distance matrices for X,Y respectively, i.e. Aij = \u2016xi \u2212 xj\u2016, Bij = \u2016yi \u2212 yj\u2016. One can use other negative definite metrics instead of Euclidean norms to generalize the definition to metric spaces Lyons [2013]. The above expression is different from the presentation in the original papers (but mathematically equivalent). They then define (squared) distance correlation as the normalized version of dCov2:\ndCor2n(X,Y ) = dCov2n(X,Y )\u221a\ndCov2n(X,X)dCov 2 n(Y, Y )\n.\ndCor2n is always between [0, 1], and unlike correlation, the population dCor 2 = 0 iff X,Y are independent, and Sze\u0301kely et al. [2007] proves it is consistent against any fixed alternatives (with finite second moments). There is an interesting connection with MMD2 which justifies its appearance in this paper. The MMD2 between \u00b5PXY and \u00b5PX\u00d7PY is called HSIC (see Gretton et al. [2005]), which has the sample expression:\nHSICn = 1\nn2 tr(K\u0303L\u0303) =\n1\nn2 n\u2211 i,j=1 K\u0303ijL\u0303ij , (2)\nwhere K\u0303 = HKH, L\u0303 = HLH , H is defined as before and K,L are kernel matrices i.e. Kij = k(xi, xj), Lij = l(yi, yj). The striking similarity between Eqs.(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov2. Hence, dCor2 and MMD are very strongly related quantities, for very related problems.\nIn Sze\u0301kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor2 and udCor2 as test statistics both suffer in high d, but for a slightly different reason than for MMD2.\nWhen (X,Y ) are drawn from a standard Gaussian, the authors of Sze\u0301kely and Rizzo [2013] show that the biased dCorn \u2192 1, if n is kept fixed and dx, dy are increased. Then, they show that their unbiased udCorn hovers around 0 in the same situation (even when dx, dy n), and conclude that it performs well in high-dimensions. The bias is indeed zero, but we argue that the variance of udCorn remains the same order as dCorn. Sze\u0301kely and Rizzo [2013] show that when the null is true, udCorn is well behaved. However, the right followup question to ask is - when the alternate hypothesis is true, how does the statistic behave in high dimensions? Below we demonstrate that in this case it fails to detect such dependence in high dimensions, i.e. its power goes to zero."}, {"heading": "4.1 Experimental Verification", "text": "Here we carefully design a simple simulation experiment to demonstrate this decrease in power with dimension, with some subtleties in choice of the alternative hypothesis that are quite crucial. For the null hypothesis of independent variables, we let (x, y) be sampled from a d-dimensional standard normal, like in Sze\u0301kely and Rizzo [2013]. For the alternative hypothesis, we need to make a choice about how to change the covariance matrix, such that as d increases, the problem neither gets easier nor harder. We choose to make a constant number of off-diagonal elements non-zero, i.e. we don\u2019t change the number or value of non-zero off-diagonal elements as d increases. The marginals are still standard Gaussians; the non-zero elements are only in the cross-diagonal blocks indicating dependence between X ,Y .\nOne can argue that a constant number of non-zeros (not growing with d) is the fairest choice, which does not increase/decrease (with d) the amount of information provided to the statistician:\n1. All the information to decide between null and alternate is captured in the covariance matrix. From classical information theory, the Gaussian entropy log det \u03a3 is the amount of information encoded in \u03a3, which (with our choice) remains constant as d increases.\n2. Another information theoretic quantity of relevance is the mutual information (MI) between X,Y . Since the MI between Gaussians is given by log det \u03a3det \u03a3X det \u03a3Y , one can easily check that the mutual information between X,Y stays constant as dimension increases.\n3. All one is trying to do is differentiate \u03a3 from I , so \u2016\u03a3 \u2212 I\u20162F is also a measure of difficulty of the problem. Even if the statistician detects dependence caused by a single off-diagonal element, he will reject the null. With our choice, \u2016\u03a3\u2212 I\u20162F does stay constant with d.\nThis is similar in spirit to the MMD2 case, where we justified our alternative by verifying that the signal to noise ratio and the KL-divergence weren\u2019t changing with d. Figure 2 provides a detailed analysis of the power of dCor, udCor. P0, P1 represent the distributions of the corresponding test statistic (possibly not normally distributed) under H0 and H1 and \u00b50, \u03c30, \u00b51, \u03c31 are their mean and standard deviation. The explanations in the figure subtext bring out the complicated scenario of \u00b50, \u00b51, \u03c30, \u03c31 all changing with d - i.e. P0, P1 change with d."}, {"heading": "5 Conclusion", "text": "In summary, we believe that we have made a strong case for the first time that the power of the closely related kernel and distance based tests both suffer from the curse of dimensionality against fair alternatives. In the process, we also undertook a detailed study of bandwidth choices and explicitly demonstrated cases when and why the median heuristic works and fails, and made a case for overestimating the bandwidth. The reasons for the observed power decay can be complicated, and a better theory is necessary to understand the null and alternate distributions in high dimensions."}, {"heading": "A Proof of Lemma 1", "text": "Proof. From definition of MMD2, we have\nMMD2(p, q) = \u222b x,x\u2032 k(x, x\u2032)p(x)p(x\u2032)dxdx\u2032 + \u222b x,x\u2032 k(x, x\u2032)q(x)q(x\u2032)dxdx\u2032 \u2212 2 \u222b x,x\u2032 k(x, x\u2032)p(x)q(x\u2032)dxdx\u2032.\nFrom Bochner\u2019s theorem (see [Rudin, 1962]) for translation invariant kernels, we know k(x, x\u2032) = \u222b w s(w)eiw >xe\u2212iw >x\u2032dw where s is the fourier transform of the kernel. Substituting the above equality in the definition of MMD2, we have the required result."}, {"heading": "B Proof of Theorem 2", "text": "Proof. Since Gaussian kernel is a translation invariant kernel, we can use Lemma 1 to derive the MMD2 in this case. It is well-known that the Fourier transform s(w) of Gaussian kernel is Gaussian distribution. Substituting the characteristic function of normal distribution in Lemma 1, we have\nMMD2(p, q) = \u222b w ( \u03b32/2\u03c0 )d/2 exp ( \u2212\u03b32\u2016w\u20162/2 ) \u2223\u2223exp(i\u00b5>1 w \u2212 w>\u03a3w/2)\u2212 exp(i\u00b5>1 w \u2212 w>\u03a3w/2)\u2223\u22232 dw = ( \u03b32/2\u03c0\n)d/2 \u222b w exp ( \u2212w>\u03a3w ) exp ( \u2212\u03b32\u2016w\u20162/2 ) \u2223\u2223exp(i\u00b5>1 w)\u2212 exp(i\u00b5>2 w)\u2223\u22232 dw = ( \u03b32/2\u03c0\n)d/2 \u222b w exp ( \u2212w>(\u03a3 + \u03b32I/2)w ) ( 2\u2212 exp ( \u2212i(\u00b51 \u2212 \u00b52)>w ) \u2212 exp ( \u2212i(\u00b52 \u2212 \u00b51)>w )) dw\n= 2 ( \u03b32/2\u03c0 )d/2 \u222b w exp ( \u2212w>(\u03a3 + \u03b32I/2)w ) ( 1\u2212 exp ( \u2212i(\u00b51 \u2212 \u00b52)>w )) dw (3)\nThe third step follows from definition of complex conjugate. In what follows, we do the following change of variable u = (\u03a3 + \u03b32I/2)1/2w. Consider the following term:\u222b\nw\nexp ( \u2212w>(\u03a3 + \u03b32I/2)w ) exp ( \u2212i(\u00b51 \u2212 \u00b52)>w ) dw\n= \u222b u exp\u2212 ( u>u+ i(\u00b51 \u2212 \u00b52)>(\u03a3 + \u03b32I/2)\u22121/2u ) |\u03a3 + \u03b32I/2|\u22121/2du = |\u03a3 + \u03b32I/2|\u22121/2 exp(\u2212(\u00b51 \u2212 \u00b52)>(\u03a3 + \u03b32I/2)\u22121(\u00b51 \u2212 \u00b52)/4)\u00d7\u222b u exp\u2212 ( \u2016u\u2212 i(\u03a3 + \u03b32I/2)\u22121/2(\u00b51 \u2212 \u00b52)/2\u20162 ) du = \u03c0d/2|\u03a3 + \u03b32I/2|\u22121/2 exp(\u2212(\u00b51 \u2212 \u00b52)>(\u03a3 + \u03b32I/2)\u22121(\u00b51 \u2212 \u00b52)/4)\nThe second step follows from well-known theory of change of variables (see Theorem 263D of Fremlin [2000]). By substituting the above equality in Equation 3, we get the required result."}, {"heading": "C Proof of Proposition 1", "text": "Proposition 1. Suppose \u03bb 6= \u03c3, then we have,\n\u221e\u222b \u2212\u221e exp ( \u2212|x\u2212 \u03bb| \u03b3 ) exp ( \u2212|x| \u03c3 ) dx =\ne\u2212|\u03bb|/\u03c3\n1/\u03b3 + 1/\u03c3 +\ne\u2212|\u03bb|/\u03b3\n1/\u03c3 \u2212 1/\u03b3 \u2212 e\n\u2212|\u03bb|/\u03c3\n1/\u03c3 \u2212 1/\u03b3 +\ne\u2212|\u03bb|/\u03b3\n1/\u03b3 + 1/\u03c3\nand when \u03bb = \u03c3, we have,\n\u221e\u222b \u2212\u221e exp ( \u2212|x\u2212 \u03bb| \u03c3 ) exp ( \u2212|x| \u03c3 ) dx =\ne\u2212|\u03bb|/\u03c3\n1/\u03b3 + 1/\u03c3 + |\u03bb|e\u2212|\u03bb|/\u03c3 + e\n\u2212|\u03bb|/\u03b3\n1/\u03b3 + 1/\u03c3\nProof. We show this when \u03bb \u2264 0 as an example proof:\n\u221e\u222b \u2212\u221e exp ( \u2212|x\u2212 \u03bb| \u03b3 ) exp ( \u2212|x| \u03c3 ) dx = \u03bb\u222b \u2212\u221e exp ( x\u2212 \u03bb \u03b3 ) exp (x \u03c3 ) dx+ 0\u222b \u03bb exp ( \u03bb\u2212 x \u03b3 ) exp (x \u03c3 ) dx\n+ \u221e\u222b 0 exp ( \u03bb\u2212 x \u03b3 ) exp ( \u2212x \u03c3 ) dx\n= e\u2212\u03bb/\u03b3e\u03bb/\u03c3+\u03bb/\u03b3 1/\u03b3 + 1/\u03c3 + e\u2212\u03bb/\u03b3(1\u2212 e\u2212\u03bb/\u03b3+\u03bb/\u03c3) 1/\u03c3 \u2212 1/\u03b3 +\ne\u03bb/\u03b3\n1/\u03b3 + 1/\u03c3\nAlso, when \u03b3 = \u03c3, we obtain the same expression for the first and last terms. However, the middle term has the following constant integrand, thereby, leading to the required expression.\n0\u222b \u03bb exp ( \u03bb\u2212 x \u03b3 ) exp (x \u03c3 ) dx = |\u03bb|e\u2212|\u03bb|/\u03c3."}, {"heading": "D Proof of Proposition 2", "text": "Proposition 2. Let \u03c8 = \u03c3/\u03b3. Then we have,\n\u221e\u222b \u2212\u221e \u221e\u222b \u2212\u221e exp ( \u2212|x\u2212 x \u2032| \u03b3 ) 1 4\u03c32 exp ( \u2212|x\u2212 \u00b5| \u03c3 ) exp ( \u2212|x \u2032| \u03c3 ) dxdx\u2032\n= \u22121 2 e\u2212|\u00b5|/\u03c3\n( \u03c8 + |\u00b5|/\u03b3\n1\u2212 \u03c82\n) +\n1\n1\u2212 \u03c82\n( \u2212\u03c8e \u2212|\u00b5|/\u03c3\n1\u2212 \u03c82 + e\u2212|\u00b5|/\u03b3 1\u2212 \u03c82 ) = \u2212 \u00b5 2\n4\u03c3\u03b3(1 + \u03c8)2 +\n2 + \u03c8\n2(1 + \u03c8)2 +O\n( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2 ) Proof. We first integrate with respect to x\u2032 using the Proposition 1 to get\n1\n4\u03c32 \u221e\u222b \u2212\u221e ( e\u2212|x|/\u03c3 1/\u03b3 + 1/\u03c3 + e\u2212|x|/\u03b3 1/\u03c3 \u2212 1/\u03b3 \u2212 e \u2212|x|/\u03c3 1/\u03c3 \u2212 1/\u03b3 + e\u2212|x|/\u03b3 1/\u03b3 + 1/\u03c3 ) exp ( \u2212|x\u2212 \u00b5| \u03c3 ) dx\nWe then integrate these terms once again using both parts of Proposition 1 to get the first equality. We simplify the second\nequation in the following manner:\n\u22121 2 e\u2212|\u00b5|/\u03c3\n( \u03c8 + |\u00b5|/\u03b3\n1\u2212 \u03c82\n) +\n1\n1\u2212 \u03c82\n( \u2212\u03c8e \u2212|\u00b5|/\u03c3\n1\u2212 \u03c82 + e\u2212|\u00b5|/\u03b3 1\u2212 \u03c82 ) = \u22121\n2\n( 1\u2212 |\u00b5|\n\u03c3 + |\u00b5|2 2\u03c32\n)( \u03c8 + |\u00b5|/\u03b3\n1\u2212 \u03c82\n) +\n1\n1\u2212 \u03c82\n( \u2212 (\u03c3/\u03b3 \u2212 |\u00b5|/\u03b3 + \u00b5 2/2\u03c3\u03b3)\n1\u2212 \u03c82 +\n1\u2212 |\u00b5|/\u03b3 + \u00b52/2\u03b32\n1\u2212 \u03c82 ) +O ( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2 ) = \u2212 1\n2(1\u2212 \u03c82)\n( \u03c8 \u2212 \u00b5 2\n2\u03c3\u03b3 + |\u00b5|3 2\u03c32\u03b3\n) +\n1\n(1\u2212 \u03c82)2\n( 1\u2212 \u03c8 \u2212 \u00b5 2\n2\u03c3\u03b3 +\n\u00b52\n2\u03b32 ) +O ( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2 ) = \u2212 1\n2(1\u2212 \u03c82)\n( \u03c8 \u2212 \u00b5 2\n2\u03c3\u03b3\n) +\n(1\u2212 \u00b52/2\u03c3\u03b3)(1\u2212 \u03c8) (1\u2212 \u03c82)2 +O\n( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2 ) = 1\n1\u2212 \u03c82\n( \u2212\u03c8\n2 +\n1\n2\n\u00b52\n2\u03c3\u03b3\n) +\n1\n1\u2212 \u03c82\n( 1\n1 + \u03c8 \u2212 \u00b5\n2\n(1 + \u03c8)2\u03c3\u03b3\n) +O ( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2 ) = \u2212 \u00b5 2\n4\u03c3\u03b3(1 + \u03c8)2 +\n2 + \u03c8\n2(1 + \u03c8)2 +O\n( |\u00b5|3\n\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5|3\n\u03b33(1\u2212 \u03c82)2\n)"}, {"heading": "E Proof of Theorem 4", "text": "Proof. Recall that we use Laplace kernel, i.e., k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20161/\u03b3). By using the definition of MMD2, we have\nMMD2 = \u222b x,x\u2032 (p(x)p(x\u2032) + q(x)q(x\u2032)\u2212 2p(x)q(x\u2032))k(x, x\u2032)dxdx\u2032. (4)\nConsider the term \u222b x,x\u2032\np(x)q(x\u2032)k(x, x\u2032)dxdx\u2032. The other terms can be calculated in a similar manner. Let \u03c8 = \u03c3/\u03b3 and \u03b2 = (1 + \u03c8/2)/(1 + \u03c8)2. We have,\u222b\nx,x\u2032 p(x)q(x\u2032)k(x, x\u2032)dxdx\u2032 = d\u220f i=1 \u222b xi,x\u2032i exp ( \u2212|x\u2212 x \u2032| \u03b3 ) 1 4\u03c32 exp ( \u2212|x\u2212 \u00b5| \u03c3 ) exp ( \u2212|x \u2032| \u03c3 ) dxidx)i \u2032\n= d\u220f i=1 \u03b2 ( 1\u2212 \u00b5 2 i 4\u03b2\u03c3\u03b3(1 + \u03c8)2 +O ( |\u00b5i|3 \u03b2\u03c32\u03b3(1\u2212 \u03c82)2 ) \u2212O ( |\u00b5i|3 \u03b2\u03b33(1\u2212 \u03c82)2 )) = \u03b2d ( 1\u2212 \u2016\u00b5\u2016 2\n4\u03b2\u03c3\u03b3(1 + \u03c8) +O\n( |\u00b5i|3\n\u03b2\u03c32\u03b3(1\u2212 \u03c82)2\n) \u2212O ( |\u00b5i|3\n\u03b2\u03b33(1\u2212 \u03c82)2 )) The first step follows from the fact that both Laplace kernel and Laplace distribution decompose over the coordinates. The second step follows from Proposition 2. Substituting the above expression in Equation 4, we get,\nMMD2 = \u03b2d\u22121\u2016\u00b5\u20162\n2\u03c3\u03b3(1 + \u03c8) \u2212O\n( \u03b2d\u22121\u2016\u00b5\u201633\n\u03c32\u03b3(1\u2212 \u03c82)2\n) +O ( \u03b2d\u22121\u2016\u00b5\u201633 \u03b33(1\u2212 \u03c82)2 ) .\nF Verifying MMD approximations"}, {"heading": "G Biased MMD for Gaussian Distribution", "text": "H Verifying Power Plots decay polynomially\nI Standard deviation of udCor, dCor"}, {"heading": "J MMD between Gaussians with same mean, different variances", "text": "Suppose P = \u2297di=1N(0, \u03c32) \u2297 N(0, a2) and Q = \u2297di=1N(0, \u03c32) \u2297 N(0, b2). If a, b are of the same order as \u03c3 then the median heuristic will still pick \u03b3 \u2248 \u03c3 \u221a d for bandwidth \u03b3 of the Gaussian kernel. First we note that for distributions with the same mean, by Taylor\u2019s theorem,\nKL(P,Q) = 1\n2 (tr(\u03a3\u221211 \u03a30 \u2212 d\u2212 log(det \u03a30)/det \u03a31)) =\n1 2 (a2/b2 \u2212 1\u2212 log(a2/b2))\n\u2248 (a 2/b2 \u2212 1)2\n4\nThe MMD2 can be derived (approximated using (1 + x)n \u2248 1 + nx for small x) as\n1\n(1 + 4\u03c32/\u03b32)d/2\u22121/2\n( 1\u221a\n1 + 4a2/\u03b32 + 1\u221a 1 + 4b2/\u03b32 \u2212 2\u221a 1 + 2(a2 + b2)/\u03b32\n)\n\u2248 1 (1 + 4\u03c32/\u03b32)d/2\u22121/2\n( 1\n1 + 2a2/\u03b32 +\n1 1 + 2b2/\u03b32 \u2212 2 1 + (a2 + b2)/\u03b32\n)\n\u2248 1 (1 + 4\u03c32/\u03b32)d/2\u22121/2\n( 1\u221a\n1 + 2a2/\u03b32 \u2212 1\u221a 1 + 2b2/\u03b32 )2 \u2248 1\n(1 + 4\u03c32/\u03b32)d/2\u22121/2 ( (1\u2212 a2/\u03b32)\u2212 (1\u2212 b2/\u03b32) )2 = b4/\u03b34\n(1 + 4\u03c32/\u03b32)d/2\u22121/2 (a2/b2 \u2212 1)2\nIf \u03b3 is chosen by the median heuristic (optimal in this case), we see that this is smaller than KL by \u03c34d2e/b4. If it is chosen as constant, it can be exponentially smaller than KL."}], "references": [{"title": "Finding and Leveraging Structure in Learning Problems", "author": ["S. Balakrishnan"], "venue": "PhD thesis,", "citeRegEx": "Balakrishnan.,? \\Q2013\\E", "shortCiteRegEx": "Balakrishnan.", "year": 2013}, {"title": "Estimating integrated squared density derivatives: sharp best order of convergence estimates", "author": ["P. Bickel", "Y. Ritov"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Bickel and Ritov.,? \\Q1988\\E", "shortCiteRegEx": "Bickel and Ritov.", "year": 1988}, {"title": "Estimation of integral functionals of a density", "author": ["L. Birge", "P. Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "Birge and Massart.,? \\Q1995\\E", "shortCiteRegEx": "Birge and Massart.", "year": 1995}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of Algorithmic Learning Theory,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schoelkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Estimation of R\u00e9nyi information divergence via pruned minimal spanning trees", "author": ["A. Hero", "O. Michel"], "venue": "In Higher-Order Statistics,", "citeRegEx": "Hero and Michel.,? \\Q1999\\E", "shortCiteRegEx": "Hero and Michel.", "year": 1999}, {"title": "Estimating nonquadratic functionals of a density using haar wavelets", "author": ["G. Kerkyacharian", "D. Picard"], "venue": "The Annals of Statistics,", "citeRegEx": "Kerkyacharian and Picard.,? \\Q1996\\E", "shortCiteRegEx": "Kerkyacharian and Picard.", "year": 1996}, {"title": "Efficient estimation of integral functionals of a density", "author": ["B. Laurent"], "venue": "The Annals of Statistics,", "citeRegEx": "Laurent.,? \\Q1996\\E", "shortCiteRegEx": "Laurent.", "year": 1996}, {"title": "Distance covariance in metric spaces", "author": ["R. Lyons"], "venue": "Annals of Probability,", "citeRegEx": "Lyons.,? \\Q2013\\E", "shortCiteRegEx": "Lyons.", "year": 2013}, {"title": "Fourier analysis on groups", "author": ["W. Rudin"], "venue": null, "citeRegEx": "Rudin.,? \\Q1962\\E", "shortCiteRegEx": "Rudin.", "year": 1962}, {"title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "The Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "On the empirical estimation of integral probability metrics", "author": ["B. Sriperumbudur", "K. Fukumizu", "A. Gretton", "B. Schoelkopf", "G. Lanckriet"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2012}, {"title": "The distance correlation t-test of independence in high dimension", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo"], "venue": "J. Multivariate Analysis,", "citeRegEx": "Sz\u00e9kely and Rizzo.,? \\Q2013\\E", "shortCiteRegEx": "Sz\u00e9kely and Rizzo.", "year": 2013}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo", "Bakirov N.K"], "venue": "The Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs).", "startOffset": 60, "endOffset": 83}, {"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]).", "startOffset": 60, "endOffset": 483}, {"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]). The corresponding test uses empirical distributions for plug-in estimators (described later) and is consistent (for fixed dimension, power tends to one as number of samples becomes infinite) against any single fixed alternative. Distance correlation is a quantity introduced in Sz\u00e9kely et al. [2007] to tackle the second problem using distances between pairs of points.", "startOffset": 60, "endOffset": 785}, {"referenceID": 2, "context": "2 Summary of Contributions Kernel Maximum Mean Discrepancy (MMD) Gretton et al. [2012a] showed that the estimated MMD converges to the true MMD at rate O(n\u22121/2) independently of dimension d.", "startOffset": 65, "endOffset": 88}, {"referenceID": 0, "context": "We will see that the true value of the population MMD can be polynomially or even exponentially small in d (we were notified that a special case of Corollary 1 was earlier independently noted by Balakrishnan [2013], but do not know other examples).", "startOffset": 195, "endOffset": 215}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially.", "startOffset": 36, "endOffset": 59}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic).", "startOffset": 36, "endOffset": 965}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic). However, in all simulations, power goes to zero as d\u2192\u221e for all settings of the bandwidth. Distance Correlation (dCor) Sz\u00e9kely and Rizzo [2013] studied distance correlation in high dimensions where they considered the following example.", "startOffset": 36, "endOffset": 1200}, {"referenceID": 2, "context": "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn\u2019t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard", "startOffset": 301, "endOffset": 326}, {"referenceID": 2, "context": "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn\u2019t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard", "startOffset": 301, "endOffset": 342}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).", "startOffset": 8, "endOffset": 32}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).", "startOffset": 8, "endOffset": 56}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD\u2019s power decays with d against fair alternatives. Let F be a class of functions f : X \u2192 R. The MMD is defined as: MMD(F , p, q) := sup f\u2208F Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := \u2016\u03bcp \u2212 \u03bcq\u2016H where \u03bcp = Ex\u223cp[k(x, .)] for any distribution p \u2208 P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al.", "startOffset": 8, "endOffset": 851}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD\u2019s power decays with d against fair alternatives. Let F be a class of functions f : X \u2192 R. The MMD is defined as: MMD(F , p, q) := sup f\u2208F Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := \u2016\u03bcp \u2212 \u03bcq\u2016H where \u03bcp = Ex\u223cp[k(x, .)] for any distribution p \u2208 P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al. [2012a]:", "startOffset": 8, "endOffset": 924}, {"referenceID": 3, "context": "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result.", "startOffset": 76, "endOffset": 99}, {"referenceID": 3, "context": "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result. Theorem 1. Gretton et al. [2012a] Suppose 0 \u2264 k(x, x) \u2264 K, then with probability at least 1\u2212 \u03b4, we have", "startOffset": 76, "endOffset": 236}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al.", "startOffset": 121, "endOffset": 144}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities \u03bc0, \u03c30, \u03bc1, \u03c31 actually vary with d and n.", "startOffset": 121, "endOffset": 268}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities \u03bc0, \u03c30, \u03bc1, \u03c31 actually vary with d and n. Further, in the highdimensional setting, classical large sample theory does not apply as d can be comparable to or larger than n, and calculations assuming the \u201casymptotically\u201d normal distribution can be misleading. Specifically, consider Q := \u221a n(MMD2l\u2212MMD ) \u03c31 N(0, 1) d =: Z as shown in Gretton et al. [2012a]. Thus, an asymptotic level \u03b1 test rejects when MMDl > \u03c30z\u03b1/ \u221a n.", "startOffset": 121, "endOffset": 660}, {"referenceID": 3, "context": "Hence, one might be tempted to use \u03bdn,d to measure the effectiveness of the test, and indeed choosing the kernel (or bandwidth) to maximize \u03bdn,d was studied by Gretton et al. [2012b]. However, in the high dimensional setting the normal approximation used in the last step can be extremely poor, as we have experimentally verified.", "startOffset": 160, "endOffset": 183}, {"referenceID": 12, "context": "A more general version of the above lemma for all kernels (with a different constant than 1) is presented in Sriperumbudur et al. [2012] (Proposition 5.", "startOffset": 109, "endOffset": 137}, {"referenceID": 12, "context": "2 of Sriperumbudur et al. [2012] has related calculations, with a different aim that we discuss in Section 3.", "startOffset": 5, "endOffset": 33}, {"referenceID": 0, "context": "The special case of a constant bandwidth with = 1/2 has already been noted by Balakrishnan [2013].", "startOffset": 78, "endOffset": 98}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error).", "startOffset": 35, "endOffset": 58}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, .", "startOffset": 35, "endOffset": 286}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, ..., 1) which makes the problem easier with dimension. Also, they use it to argue that the mean squared error (as summarised by Thm 1) with increasing n is indeed independent of d. Another relevant comparison is with Fig. 5A in Gretton et al. [2012a] where they show an extremely slow decrease in power with dimension for the same example of mean-shifted Gaussians with Gaussian kernel that we consider.", "startOffset": 35, "endOffset": 698}, {"referenceID": 14, "context": "The authors of Sz\u00e9kely et al. [2007] introduce a test statistic called (squared) distance covariance which is defined as", "startOffset": 15, "endOffset": 37}, {"referenceID": 9, "context": "One can use other negative definite metrics instead of Euclidean norms to generalize the definition to metric spaces Lyons [2013]. The above expression is different from the presentation in the original papers (but mathematically equivalent).", "startOffset": 117, "endOffset": 130}, {"referenceID": 11, "context": "dCor n is always between [0, 1], and unlike correlation, the population dCor 2 = 0 iff X,Y are independent, and Sz\u00e9kely et al. [2007] proves it is consistent against any fixed alternatives (with finite second moments).", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": "The MMD between \u03bcPXY and \u03bcPX\u00d7PY is called HSIC (see Gretton et al. [2005]), which has the sample expression: HSICn = 1 n2 tr(K\u0303L\u0303) = 1 n2 n \u2211", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov.", "startOffset": 34, "endOffset": 59}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d.", "startOffset": 34, "endOffset": 374}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Sz\u00e9kely and Rizzo [2013] show that the biased dCorn \u2192 1, if n is kept fixed and dx, dy are increased.", "startOffset": 34, "endOffset": 735}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Sz\u00e9kely and Rizzo [2013] show that the biased dCorn \u2192 1, if n is kept fixed and dx, dy are increased. Then, they show that their unbiased udCorn hovers around 0 in the same situation (even when dx, dy n), and conclude that it performs well in high-dimensions. The bias is indeed zero, but we argue that the variance of udCorn remains the same order as dCorn. Sz\u00e9kely and Rizzo [2013] show that when the null is true, udCorn is well behaved.", "startOffset": 34, "endOffset": 1094}, {"referenceID": 13, "context": "For the null hypothesis of independent variables, we let (x, y) be sampled from a d-dimensional standard normal, like in Sz\u00e9kely and Rizzo [2013]. For the alternative hypothesis, we need to make a choice about how to change the covariance matrix, such that as d increases, the problem neither gets easier nor harder.", "startOffset": 121, "endOffset": 146}, {"referenceID": 13, "context": "The left panel shows that dCorn \u2192 1, udCorn \u2248 0, as predicted by Sz\u00e9kely and Rizzo [2013]. The middle panel shows that dCorn \u2192 1, udCorn \u2192 0, similar to the null.", "startOffset": 65, "endOffset": 90}], "year": 2017, "abstractText": "This paper is about two related methods for two sample testing and independence testing which have emerged over the last decade: Maximum Mean Discrepancy (MMD) for the former problem and Distance Correlation (dCor) for the latter. Both these methods have been suggested for high-dimensional problems, and sometimes claimed to be unaffected by increasing dimensionality of the samples. We will show theoretically and practically that the power of both methods (for different reasons) does actually decrease polynomially with dimension. We also analyze the median heuristic, which is a method for choosing tuning parameters of translation invariant kernels. We show that different bandwidth choices could result in the MMD decaying polynomially or even exponentially in dimension.", "creator": "LaTeX with hyperref package"}}}