{"id": "1406.3124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2014", "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and Reasoning", "abstract": "We present an initial theoretical analysis of the power of polynomial time preprocessing for important combinatorial problems from different areas of AI. We consider problems from the areas of constraint satisfaction, global constraints, satisfiability, nonmonotonic and Bayesian reasoning under structural constraints. All these problems involve two tasks: (i) identifying the structure in the input according to the requirements of the constraint, and (ii) using the identified structure to efficiently solve the thought task. We show that task (i) for most of the problems considered allows polynomial time preprocessing to a problem nucleus whose size is polynomial in a structural problem parameter of input, as opposed to task (ii) which does not allow such a reduction to a problem nucleus of polynomial size, subject to a complexity theory assumption. As a notable exception, we show that the consistency problem for the AtMost-NValue constraint is a polynomial problem that allows for a number of polynomial constraints, the number of polynomial problems for a consistent problem.", "histories": [["v1", "Thu, 12 Jun 2014 05:44:06 GMT  (41kb)", "http://arxiv.org/abs/1406.3124v1", "arXiv admin note: substantial text overlap witharXiv:1104.2541,arXiv:1104.5566"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1104.2541,arXiv:1104.5566", "reviews": [], "SUBJECTS": "cs.AI cs.CC cs.DS", "authors": ["serge gaspers", "stefan szeider"], "accepted": false, "id": "1406.3124"}, "pdf": {"name": "1406.3124.pdf", "metadata": {"source": "CRF", "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and Reasoning", "authors": ["Serge Gaspers", "Stefan Szeider"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n31 24\nv1 [\ncs .A\nI] 1\n2 Ju\nn 20\n14\nKeywords: Fixed-Parameter Tractability; Kernelization; Constraint Satisfaction; Reasoning; Computational Complexity"}, {"heading": "1 Introduction", "text": "Many important computational problems that arise in various areas of AI are intractable. Nevertheless, AI research has been very successful in developing and implementing heuristic solvers that work well on real-world instances. An important component of virtually every solver is a powerful polynomial-time preprocessing procedure that reduces the problem input. For instance, preprocessing techniques for the propositional satisfiability problem are based on Boolean Constraint Propagation (see, e.g., [27]), CSP solvers make use of various local consistency algorithms that filter the domains of variables (see, e.g., [4]); similar preprocessing methods are used by solvers for Nonmonotonic and Bayesian reasoning problems (see, e.g., [38, 13], respectively). The history of preprocessing, like applying reduction rules to simplify truth functions, can be traced back to the 1950\u2019s [55]. A natural question in this regard is how to measure the quality of preprocessing rules proposed for a specific problem.\nUntil recently, no provable performance guarantees for polynomial-time preprocessing methods have been obtained, and so preprocessing was only subject of empirical studies. A possible reason for the lack of theoretical results is a certain inadequacy of the P vs NP framework for such an analysis: if we could reduce in polynomial time an instance of an NP-hard problem just by one bit, then we can solve the entire problem in polynomial time by repeating the reduction step a polynomial number of times, and P = NP follows.\nWith the advent of parameterized complexity [25], a new theoretical framework became available that provides suitable tools to analyze the power of preprocessing. Parameterized complexity considers a problem in a two-dimensional setting, where in addition to the input size n, a problem parameter k is taken into consideration. This parameter can encode a structural aspect of the problem instance. A problem is called fixed-parameter tractable (FPT) if it can be solved in time f(k)p(n) where f is a function of the parameter k and p is a polynomial of the input size n. Thus, for FPT problems, the combinatorial explosion can be\n\u2217Preliminary and shorter versions of this paper appeared in the proceedings of IJCAI 2011 [36] and AAAI 2011 [62]. \u2020UNSW Australia and NICTA, Sydney, Australia \u2021Institute of Information Systems, Vienna University of Technology, Austria\nconfined to the parameter and is independent of the input size. It is known that a problem is fixed-parameter tractable if and only if every problem input can be reduced by polynomial-time preprocessing to an equivalent input whose size is bounded by a function of the parameter [24]. The reduced instance is called the problem kernel, the preprocessing is called kernelization. The power of polynomial-time preprocessing can now be benchmarked in terms of the size of the kernel. Once a small kernel is obtained, we can apply any method of choice to solve the kernel: brute-force search, heuristics, approximation, etc. [42]. Because of this flexibility a small kernel is generally preferable to a less flexible branching-based fixed-parameter algorithm. Thus, small kernels provide an additional value that goes beyond bare fixed-parameter tractability.\nKernelization is an important algorithmic technique that has become the subject of a very active field in state-of-the-art combinatorial optimization (see, e.g., the references in [28, 42, 45, 57]). Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].\nOnce a kernel is obtained, the time required to solve the instance is a function of the parameter only and therefore independent of the input size. While, in general, the time needed to solve an instance does not necessarily depend on the size of the instance alone, the kernelization view is that it preprocesses the easy parts of an instance, leaving a core instance encoding the hard parts of the problem instance. Naturally one aims at kernels that are as small as possible, in order to guarantee good worst-case running times as a function of the parameter, and the kernel size provides a performance guarantee for the preprocessing. Some NP-hard combinatorial problems such as k-Vertex Cover admit polynomially sized kernels, for others such as k-Path an exponential kernel is the best one can hope for [11].\nAs an example of a polynomial kernel, consider the k-Vertex Cover problem, which, for a graph G = (V,E) and an integer parameter k, is to decide whether there is a set S of at most k vertices such that each edge from E has at least one endpoint in S. Buss\u2019 kernelization algorithm for k-Vertex Cover (see [14]) computes the set U of vertices with degree at least k+1 in G. If |U | > k, then reject the instance, i.e., output a trivial No-instance (e.g., the graph K2 consisting of one edge and the parameter 0), since every vertex cover of size at most k contains each vertex from U . Otherwise, if G \\ U has more than k(k \u2212 |U |) edges, then reject the instance, since each vertex from G \\ U covers at most k edges. Otherwise, output the instance (G \\ (U \u222a L), k \u2212 |U |), where L is the set of degree-0 vertices in G \\ U . This instance has O(k2) vertices and edges. Thus, Buss\u2019 kernelization algorithm gives a quadratic kernel for k-Vertex Cover.\nIn previous research several NP-hard AI problems have been shown to be fixed-parameter tractable. We list some important examples from various areas:\n1. Constraint satisfaction problems (CSP) over a fixed universe of values, parameterized by the induced width [41].\n2. Consistency and generalized arc consistency for intractable global constraints, parameterized by the cardinalities of certain sets of values [5].\n3. Propositional satisfiability (SAT), parameterized by the size of backdoors [50].\n4. Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets [52, 9].\n5. Nonmonotonic reasoning with normal logic programs, parameterized by feedback width [41].\nAll these problems involve the following two tasks.\n(i) Structure Recognition Task : identify the structure in the input as required by the considered parameter.\n(ii) Reasoning Task : use the identified structure to solve a reasoning task efficiently.\nFor most of the considered problems we observe the following pattern: the Structure Recognition Task admits a polynomial kernel, in contrast to the Reasoning Task, which does not admit a polynomial kernel, unless the Polynomial Hierarchy collapses to its third level.\nA negative exception to this pattern is the recognition problem for CSPs of small induced width, which most likely does not admit a polynomial kernel.\nA positive exception to this pattern is the AtMost-NValue global constraint, for which we obtain a polynomial kernel. As in [5], the parameter is the number of holes in the domains of the variables, measuring how close the domains are to being intervals. More specifically, we present a linear time preprocessing algorithm that reduces an AtMost-NValue constraint C with k holes to a consistency-equivalentAtMostNValue constraint C\u2032 of size polynomial in k. In fact, C\u2032 has at most O(k2) variables and O(k2) domain values. We also give an improved branching algorithm checking the consistency of C\u2032 in time O(1.6181k + n). The combination of kernelization and branching yields efficient algorithms for the consistency and propagation of (AtMost-)NValue constraints.\nOutline\nThis article is organized as follows. Parameterized complexity and kernelization are formally introduced in Section 2. Section 3 describes the tools we use to show that certain parameterized problems do not have polynomial kernels. Sections 4\u20138 prove kernel lower bounds for parameterized problems in the areas of constraint networks, satisfiability, global constraints, Bayesian reasoning, and nonmonotonic reasoning. Each of these sections also gives all necessary definitions, relevant background, and related work for the considered problems. In addition, Section 6 describes a polynomial kernel for the consistency problem for the AtMost-NValue constraint parameterized by the number of holes in the variable domains, and an FPT algorithm that uses this kernel as a first step. The correctness and performance guarantees of the kernelization algorithm are only outlined in Section 6 and proved in detail in A. The conclusion, Section 9, broadly recapitulates the results and suggests the study of Turing kernels to overcome the shortcomings of (standard) kernels for many fundamental AI and Resoning problems."}, {"heading": "2 Formal Background", "text": "A parameterized problem P is a subset of \u03a3\u2217 \u00d7 N for some finite alphabet \u03a3. For a problem instance (x, k) \u2208 \u03a3\u2217 \u00d7 N we call x the main part and k the parameter. We assume the parameter is represented in unary. For the parameterized problems considered in this paper, the parameter is a function of the main part, i.e., k = \u03c0(x) for a function \u03c0. We then denote the problem as P (\u03c0), e.g., U-CSP(width) denotes the problem U-CSP parameterized by the width of the given tree decomposition.\nA parameterized problem P is fixed-parameter tractable if there exists an algorithm that solves any input (x, k) \u2208 \u03a3\u2217\u00d7N in time O(f(k)\u00b7p(|x|)) where f is an arbitrary computable function of k and p is a polynomial in |x|.\nA kernelization for a parameterized problem P \u2286 \u03a3\u2217 \u00d7 N is an algorithm that, given (x, k) \u2208 \u03a3\u2217 \u00d7 N, outputs in time polynomial in |x|+ k a pair (x\u2032, k\u2032) \u2208 \u03a3\u2217 \u00d7 N such that\n1. (x, k) \u2208 P if and only if (x\u2032, k\u2032) \u2208 P , and\n2. |x\u2032|+ k\u2032 \u2264 g(k), where g is an arbitrary computable function, called the size of the kernel.\nIn particular, for constant k the kernel has constant size g(k). If g is a polynomial then we say that P admits a polynomial kernel.\nEvery fixed-parameter tractable problem admits a kernel. This can be seen by the following argument due to Downey et al. [24]. Assume we can decide instances (x, k) of problem P in time f(k)|x|O(1). We kernelize an instance (x, k) as follows. If |x| \u2264 f(k) then we already have a kernel of size f(k). Otherwise, if |x| > f(k), then f(k)|x|O(1) = |x|O(1) is a polynomial; hence we can decide the instance in polynomial time and replace it with a small decision-equivalent instance (x\u2032, k\u2032). Thus we always have a kernel of size at most f(k). However, f(k) is super-polynomial for NP-hard problems (unless P = NP), hence this generic construction does not provide polynomial kernels.\nWe understand preprocessing for an NP-hard problem as a polynomial-time procedure that transforms an instance of the problem to a (possible smaller) solution-equivalent instance of the same problem. Kernelization is such a preprocessing with a performance guarantee, i.e., we are guaranteed that the preprocessing yields a kernel whose size is bounded in terms of the parameter of the given problem instance. In the literature also different forms of preprocessing have been considered. An important one is knowledge compilation, a two-phases approach to reasoning problems where in a first phase a given knowledge base is (possibly in\nexponential time) preprocessed (\u201ccompiled\u201d), such that in a second phase various queries can be answered in polynomial time [15]."}, {"heading": "3 Tools for Kernel Lower Bounds", "text": "In the sequel we will use recently developed tools to obtain kernel lower bounds. Our kernel lower bounds are subject to the widely believed complexity theoretic assumption NP 6\u2286 coNP/poly. In other words, the tools allow us to show that a parameterized problem does not admit a polynomial kernel unless NP \u2286 coNP/poly. In particular, NP \u2286 coNP/poly would imply the collapse of the Polynomial Hierarchy to the third level: PH = \u03a33p [51].\nA composition algorithm for a parameterized problem P \u2286 \u03a3\u2217\u00d7N is an algorithm that receives as input a sequence (x1, k), . . . , (xt, k) \u2208 \u03a3\u2217\u00d7N, uses time polynomial in \u2211t i=1 |xi|+k, and outputs (y, k\n\u2032) \u2208 \u03a3\u2217\u00d7N with (i) (y, k\u2032) \u2208 P if and only if (xi, k) \u2208 P for some 1 \u2264 i \u2264 t, and (ii) k\u2032 is polynomial in k. A parameterized problem is compositional if it has a composition algorithm. With each parameterized problem P \u2286 \u03a3\u2217 \u00d7 N we associate a classical problem\nUP[P ] = { x#1k : (x, k) \u2208 P }\nwhere 1 denotes an arbitrary symbol from \u03a3 and # is a new symbol not in \u03a3. We call UP[P ] the unparameterized version of P .\nThe following result is the basis for our kernel lower bounds.\nTheorem 1 ([11, 34]). Let P be a parameterized problem whose unparameterized version is NP-complete. If P is compositional, then it does not admit a polynomial kernel unless NP \u2286 coNP/poly.\nLet P,Q \u2286 \u03a3\u2217 \u00d7 N be parameterized problems. We say that P is polynomial parameter reducible to Q if there exists a polynomial time computable function K : \u03a3\u2217 \u00d7 N \u2192 \u03a3\u2217 \u00d7 N and a polynomial p, such that for all (x, k) \u2208 \u03a3\u2217 \u00d7 N we have (i) (x, k) \u2208 P if and only if K(x, k) = (x\u2032, k\u2032) \u2208 Q, and (ii) k\u2032 \u2264 p(k). The function K is called a polynomial parameter transformation.\nThe following theorem allows us to transform kernel lower bounds from one problem to another.\nTheorem 2 ([12]). Let P and Q be parameterized problems such that UP[P ] is NP-complete, UP[Q] is in NP, and there is a polynomial parameter transformation from P to Q. If Q has a polynomial kernel, then P has a polynomial kernel."}, {"heading": "4 Constraint Networks", "text": "Constraint networks have proven successful in modeling everyday cognitive tasks such as vision, language comprehension, default reasoning, and abduction, as well as in applications such as scheduling, design, diagnosis, and temporal and spatial reasoning [21]. A constraint network is a triple I = (V, U,C) where V is a finite set of variables, U is a finite universe of values, and C = {C1, . . . , Cm} is set of constraints. Each constraint Ci is a pair (Si, Ri) where Si is a list of variables of length ri called the constraint scope, and Ri is an ri-ary relation over U , called the constraint relation. The tuples of Ri indicate the allowed combinations of simultaneous values for the variables Si. A solution is a mapping \u03c4 : V \u2192 U such that for each 1 \u2264 i \u2264 m and Si = (x1, . . . , xri), we have (\u03c4(x1), . . . , \u03c4(xri)) \u2208 Ri. A constraint network is satisfiable if it has a solution.\nWith a constraint network I = (V, U,C) we associate its constraint graph G = (V,E) where E contains an edge between two variables if and only if they occur together in the scope of a constraint. A width w tree decomposition of a graph G is a pair (T, \u03bb) where T is a tree and \u03bb is a labeling of the nodes of T with sets of vertices of G such that the following properties are satisfied: (i) every vertex of G belongs to \u03bb(p) for some node p of T ; (ii) every edge of G is is contained in \u03bb(p) for some node p of T ; (iii) For each vertex v of G the set of all tree nodes p with v \u2208 \u03bb(p) induces a connected subtree of T ; (iv) |\u03bb(p)| \u2212 1 \u2264 w holds for all tree nodes p. The treewidth of G is the smallest w such that G has a width w tree decomposition. The induced width of a constraint network is the treewidth of its constraint graph [22].\nKernelization fits perfectly into the context of Constraint Processing where preprocessing and data reduction (e.g., in terms of local consistency algorithms, propagation, and domain filtering) are key methods [4, 63].\nLet U be a fixed universe containing at least two elements. We consider the following parameterized version of the constraint satisfaction problem (CSP).\nU-CSP(width)\nInstance: A constraint network I = (V, U,C) and a width w tree decomposition of the constraint graph of I.\nParameter: The integer w.\nQuestion: Is I satisfiable?\nAssociated with this problem is also the task of recognizing instances of small treewidth. We state this problem in form of the following decision problem.\nRec-U-CSP(width)\nInstance: A constraint network I = (V, U,C) and an integer w \u2265 0.\nParameter: The integer w.\nQuestion: Does I admit a tree decomposition of width \u2264 w?\nIt is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]). We contrast this classical result and show that it is unlikely that U-CSP(width) admits a polynomial kernel, even in the simplest case where U = {0, 1}.\nTheorem 3. {0, 1}-CSP(width) does not admit a polynomial kernel unless NP \u2286 coNP/poly.\nProof. We show that {0, 1}-CSP(width) is compositional. Let (Ii, Ti), 1 \u2264 i \u2264 t, be a given sequence of instances of {0, 1}-CSP(width) where Ii = (Vi, {0, 1},Ci) is a constraint network and Ti is a width w tree decomposition of the constraint graph of Ii. We may assume, w.l.o.g., that Vi \u2229 Vj = \u2205 for 1 \u2264 i < j \u2264 t (otherwise we can simply change the names of variables). We form a new constraint network I = (V, {0, 1},C) as follows. We put V = \u22c3t i=1 Vi \u222a {a1, . . . , at, b0, . . . , bt} where ai, bi are new variables. We define the set C of constraints in three groups.\n1. For each 1 \u2264 i \u2264 t and each constraint C = ((x1, . . . , xr), R) \u2208 Ci we add to C a new constraint C\u2032 = ((x1, . . . , xr, ai), R\u2032)) where R\u2032 = { (u1, . . . , ur, 0) : (u1, . . . , ur) \u2208 R } \u222a {(1, . . . , 1)}.\n2. We add t ternary constraints C\u22171 , . . . , C \u2217 t where C \u2217 i = ((bi\u22121, bi, ai), R \u2217) and R\u2217 = {(0, 0, 1), (0, 1, 0), (1, 1, 1)}.\n3. Finally, we add two unary constraints C0 = ((b0), (0)) and C 1 = ((bt), (1)) which force the values of b0\nand bt to 0 and 1, respectively.\nLet G,Gi be the constraint graphs of I and Ii, respectively. Fig. 1 shows an illustration of G for t = 4. We observe that a1, . . . , at are cut vertices of G. Removing these vertices separates G into independent parts P,G\u20321, . . . , G \u2032 t where P is the path b0, b1, . . . , bt, and G \u2032 i is isomorphic to Gi. By standard techniques (see,\ne.g., [43]), we can put the given width w tree decompositions T1, . . . , Tt of G \u2032 1, . . . , G \u2032 t and the trivial width 1 tree decomposition of P together to a width w+1 tree decomposition T of G. Clearly (I, T ) can be obtained from (Ii, Ti), 1 \u2264 i \u2264 t, in polynomial time.\nWe claim that I is satisfiable if and only if at least one of the Ii is satisfiable. This claim can be verified by means of the following observations: The constraints in groups (2) and (3) provide that for any satisfying assignment there will be some 0 \u2264 i \u2264 t \u2212 1 such that b0, . . . , bi are all set to 0 and bi+1, . . . , bt are all set to 1; consequently ai is set to 0 and all aj for j 6= i are set to 1. The constraints in group (1) provide that if we set ai to 0, then we obtain from C\n\u2032 the original constraint C; if we set ai to 1 then we obtain a constraint that can be satisfied by setting all remaining variables to 1. We conclude that {0, 1}-CSP(width) is compositional.\nIn order to apply Theorem 1, we need to establish that the unparameterized version of {0, 1}-CSP(width) is NP-complete. Deciding whether a constraint network I over the universe {0, 1} is satisfiable is well-known to be NP-complete (say by reducing 3-SAT). To a constraint network I on n variables we can always add a trivial width w = n \u2212 1 tree decomposition of its constraint graph (taking a single tree node t where \u03bb(t) contains all variables of I). Hence UP[{0, 1}-CSP(width)] is NP-complete.\nLet us turn now to the recognition problem Rec-U-CSP(width). By Bodlaender\u2019s Theorem [10], the problem is fixed-parameter tractable. However, the problem is unlikely to admit a polynomial kernel. In fact, Bodlaender et al. [11] showed that the related problem of testing whether a graph has treewidth at most w does not have a polynomial kernel (taking w as the parameter), unless a certain \u201cAND-conjecture\u201d fails. In turn, Drucker [26] showed that a failure of the AND-conjecture implies NP \u2286 coNP/poly. The combination of these two results relates directly to Rec-U-CSP(width).\nProposition 1. Rec-{0, 1}-CSP(width) does not admit a polynomial kernel unless NP \u2286 coNP/poly."}, {"heading": "5 Satisfiability", "text": "The propositional satisfiability problem (SAT) was the first problem shown to be NP-hard [18]. Despite its hardness, SAT solvers are increasingly leaving their mark as a general-purpose tool in areas as diverse as software and hardware verification, automatic test pattern generation, planning, scheduling, and even challenging problems from algebra [40]. SAT solvers are capable of exploiting the hidden structure present in real-world problem instances. The concept of backdoors, introduced by Williams et al. [65], provides a means for making the vague notion of a hidden structure explicit. Backdoors are defined with respect to a \u201csub-solver\u201d which is a polynomial-time algorithm that correctly decides the satisfiability for a class C of CNF formulas. More specifically, Gomes et al. [40] define a sub-solver to be an algorithm A that takes as input a CNF formula F and has the following properties:\n1. Trichotomy: A either rejects the input F , or determines F correctly as unsatisfiable or satisfiable;\n2. Efficiency: A runs in polynomial time;\n3. Trivial Solvability: A can determine if F is trivially satisfiable (has no clauses) or trivially unsatisfiable (contains only the empty clause);\n4. Self-Reducibility: if A determines F , then for any variable x and value \u03b5 \u2208 {0, 1}, A determines F [x = \u03b5]. F [\u03c4 ] denotes the formula obtained from F by applying the partial assignment \u03c4 , i.e., satisfied clauses are removed and false literals are removed from the remaining clauses.\nWe identify a sub-solver A with the class CA of CNF formulas whose satisfiability can be determined by A. A strong A-backdoor set (or A-backdoor, for short) of a CNF formula F is a set B of variables such that for each possible truth assignment \u03c4 to the variables in B, the satisfiability of F [\u03c4 ] can be determined by sub-solver A in time O(nc). The smaller the backdoor B, the more useful it is for satisfiability solving, which makes the size of the backdoor a natural parameter to consider (see [37] for a survey on the parameterized complexity of backdoor problems). If we know an A-backdoor of size k, we can decide the satisfiability of F by running A on 2k instances F [\u03c4 ], yielding a time bound of O(2knc). Hence SAT decision is fixed-parameter tractable in the backdoor size k for any sub-solver A. Hence the following problem is clearly fixed-parameter tractable for any sub-solver A.\nSAT(A-backdoor)\nInstance: A CNF formula F , and an A-backdoor B of F of size k.\nParameter: The integer k.\nQuestion: Is F satisfiable?\nWe also consider for every subsolver A the associated recognition problem.\nRec-SAT(A-backdoor)\nInstance: A CNF formula F , and an integer k \u2265 0.\nParameter: The integer k.\nQuestion: Does F have an A-backdoor of size at most k?\nWith the problem SAT(A-backdoor) we are concerned with the question of whether instead of trying all 2k possible partial assignments we can reduce the instance to a polynomial kernel. We will establish a very general result that applies to all possible sub-solvers.\nTheorem 4. SAT(A-backdoor) does not admit a polynomial kernel for any sub-solver A unless NP \u2286 coNP/poly.\nProof. We will devise polynomial parameter transformations from the following parameterized problem which is known to be compositional [34] and therefore unlikely to admit a polynomial kernel.\nSAT(vars)\nInstance: A propositional formula F in CNF on n variables.\nParameter: The number n of variables.\nQuestion: Is F satisfiable?\nLet F be a CNF formula and V the set of all variables of F . Due to trivial solvability (Property 3) of a sub-solver, V is an A-backdoor set for any A. Hence, by mapping (F, n) (as an instance of SAT(vars)) to (F, V ) (as an instance of SAT(A-backdoor)) provides a (trivial) polynomial parameter transformation from SAT(vars) to SAT(A-backdoor). Since the unparameterized versions of both problems are clearly NP-complete, the result follows by Theorem 2.\nLet us denote by rCNF the class of CNF formulas where each clause has at most r literals, and by Horn the class of CNF formulas where each clause has at most one positive literal. Sub-solvers for Horn and 2CNF follow from [23] and [44], respectively.\nLet 3SAT(\u03c0) (where \u03c0 is an arbitrary parameterization) denote the problem SAT(\u03c0) restricted to 3CNF formulas. In contrast to SAT(vars), the parameterized problem 3SAT(vars) has a trivial polynomial kernel: if we remove duplicate clauses, then any 3CNF formula on n variables contains at most O(n3) clauses, and so is a polynomial kernel. Hence the easy proof of Theorem 4 does not carry over to 3SAT(A-backdoor). We therefore consider the cases 3SAT(Horn-backdoor) and 3SAT(2CNF-backdoor) separately, these cases are important since the detection of Horn and 2CNF-backdoors is fixed-parameter tractable [50].\nTheorem 5. Neither 3SAT(Horn-backdoor) nor 3SAT(2CNF-backdoor) admit a polynomial kernel unless NP \u2286 coNP/poly.\nProof. Let C \u2208 {Horn, 2CNF}. We show that 3SAT(C-backdoor) is compositional. Let (Fi, Bi), 1 \u2264 i \u2264 t, be a given sequence of instances of 3SAT(C-backdoor) where Fi is a 3CNF formula and Bi is a C-backdoor set of Fi of size k. We distinguish two cases.\nCase 1: t > 2k. Let \u2016Fi\u2016 := \u2211\nC\u2208Fi |C| and n := max t i=1 \u2016Fi\u2016. Whether Fi is satisfiable or not can be\ndecided in time O(2kn) since the satisfiability of a Horn or 2CNF formula can be decided in linear time. We can check whether at least one of the formulas F1, . . . , Ft is satisfiable in time O(t2\nkn) = O(t2n) which is polynomial in t + n. If some Fi is satisfiable, we output (Fi, Bi); otherwise we output (F1, B1) (F1 is unsatisfiable). Hence we have a composition algorithm.\nCase 2: t \u2264 2k. This case is more involved. We construct a new instance (F,B) of 3SAT(C-backdoor) as follows.\nLet s = \u2308log2 t\u2309. Since t \u2264 2 k, s \u2264 k follows.\nLet Vi denote the set of variables of Fi. We may assume, w.l.o.g., that B1 = \u00b7 \u00b7 \u00b7 = Bt and that Vi\u2229Vj = B1 for all 1 \u2264 i < j \u2264 t since otherwise we can change names of variable accordingly. In a first step we obtain from every Fi a CNF formula F \u2032 i as follows. For each variable x \u2208 Vi \\ B1 we take s + 1 new variables x0, . . . , xs. We replace each positive occurrence of a variable x \u2208 Vi \\ B1 in Fi with the literal x0 and each negative occurrence of x with the literal \u00acxs.\nWe add all clauses of the form (\u00acxj\u22121 \u2228xj) for 1 \u2264 j \u2264 s; we call these clauses \u201cconnection clauses.\u201d Let F \u2032i be the formula obtained from Fi in this way. We observe that F \u2032 i and Fi are SAT-equivalent, since the connection clauses form an implication chain. Since the connection clauses are both Horn and 2CNF, B1 is also a C-backdoor of F \u2032i .\nFor an illustration of this construction see Example 1 below. We take a set Y = {y1, . . . , ys} of new variables. Let C1, . . . , C2s be the sequence of all 2s possible clauses (modulo permutation of literals within a clause) containing each variable from Y either positively or negatively. Consequently we can write Ci as (\u2113 i 1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u2113 i s) where \u2113 i j \u2208 {yj,\u00acyj}.\nFor 1 \u2264 i \u2264 t we add to each connection clause (\u00acxj\u22121 \u2228 xj) of F \u2032i the literal \u2113 i j \u2208 Ci. Let F \u2032\u2032 i denote the\n3CNF formula obtained from F \u2032i this way. For t < i \u2264 2s we define 3CNF formulas F \u2032\u2032i as follows. If s \u2264 3 then F \u2032\u2032 i consists just of the clause Ci. If s > 3 then we take new variables zi2, . . . , z i s\u22122 and let F \u2032\u2032 i consist of the clauses (\u2113 i 1 \u2228 \u2113 i 2 \u2228 \u00acz i 2), (\u2113 i 3 \u2228 z i 2 \u2228 \u00aczi3), . . . , (\u2113 i s\u22122 \u2228 z i s\u22123 \u2228\u00acz i s\u22122), (\u2113 i s\u22121 \u2228 \u2113 i s \u2228 z i s\u22122). Finally, we let F be the 3CNF formula containing all the clauses from F \u2032\u20321 , . . . , F \u2032\u2032 2s . Any assignment \u03c4 to Y \u222a B1 that satisfies Ci can be extended to an assignment that satisfies F \u2032\u2032i since such assignment satisfies at least one connection clause (xj\u22121 \u2228 xj \u2228 \u2113 i j) and so the chain of implications from from xo to xs is broken. It is not difficult to verify the following two claims. (i) F is satisfiable if and only if at least one of the formulas Fi is satisfiable. (ii) B = Y \u222aB1 is a C-backdoor of F . Hence we have also a composition algorithm in Case 2, and thus 3SAT(C-backdoor) is compositional. Clearly UP[3SAT(C-backdoor)] is NP-complete, hence the result follows from Theorem 1.\nExample 1. We illustrate the constructions of this proof with a running example, where we let s = 2, t = 4, i = 2, and B1 = {b}. Assume that we have\nFi = (x \u2228 \u00acu \u2228 v) \u2227 (\u00acx \u2228 u \u2228 v) \u2227 (\u00acx \u2228 \u00acu).\nFrom this we obtain the following formula, containing four connection clauses\nF \u2032i = (x0 \u2228 \u00acu2 \u2228 v) \u2227 (\u00acx2 \u2228 u0 \u2228 v) \u2227 (\u00acx2 \u2228 \u00acu2)\u2227\n(\u00acx0 \u2228 x1) \u2227 (\u00acx1 \u2228 x2) \u2227 (\u00acu0 \u2228 u1) \u2227 (\u00acu1 \u2228 u2).\nNow assume Ci = (y1 \u2228 \u00acy2). We add to the connection clauses literals from Ci and we obtain\nF \u2032\u2032i = (x0 \u2228 \u00acu2 \u2228 v) \u2227 (\u00acx2 \u2228 u0 \u2228 v) \u2227 (\u00acx2 \u2228 \u00acu2)\u2227\n(\u00acx0 \u2228 x1 \u2228 y1) \u2227 (\u00acx1 \u2228 x2 \u2228 \u00acy2) \u2227 (\u00acu0 \u2228 u1 \u2228 y1) \u2227 (\u00acu1 \u2228 u2 \u2228 \u00acy2).\nAssigning y1 to false and y2 to true reduces F \u2032\u2032 i to F \u2032 i . The other three possibilities of assigning truth values to y1, y2 break the connection clauses and make the formula trivially satisfiable. \u22a3\nWe now turn to the recognition problem Rec-SAT(A-backdoor), in particular for A \u2208 {Horn, 2CNF} for which, as mentioned above, the problem is known to be fixed-parameter tractable [50]. Here we are able to obtain positive results.\nProposition 2. Both Rec-SAT(Horn-backdoor) and Rec-SAT(2CNF-backdoor) admit polynomial kernels, with a linear and quadratic number of variables, respectively.\nProof. Let (F, k) be the instance of Rec-SAT(Horn-backdoor). We construct a graph G(F ) whose vertices are the variables of F and which contains an edge between two variables u, v if and only if both variables appear as positive literals together in a clause. It is well-known and easy to see that the vertex covers of G(F ) are exactly the Horn-backdoor sets of F [60]. Recall that a vertex cover of a graph is a set of vertices that contains at least one end of each edge of the graph. Now, we apply the known kernelization algorithm for vertex covers [17] to (G(F ), k) and obtain in polynomial time an equivalent instance (G\u2032, k\u2032) where G\u2032 has at most 2k vertices. Now it only remains to consider G\u2032 as a CNF formula F \u2032 where each edge gives rise to a binary clause on two positive literals. Since evidently G(F \u2032) = G\u2032, we conclude that (F \u2032, k\u2032) constitutes a polynomial kernel for Rec-SAT(Horn-backdoor).\nFor Rec-SAT(2CNF-backdoor) we proceed similarly. Let (F, k) be an instance of this problem. We construct a 3-uniform hypergraphH(F ) whose vertices are the variables of F and which contains a hyperedge on any three variables that appear (positively or negatively) together in a clause of F . Again, it is well-known and easy to see that the hitting sets of H(F ) are exactly the 2CNF-backdoor sets of F [60]. Recall that a hitting set of a hypergraph is a set of vertices that contains at least one vertex from each hyperedge. Now we apply a known kernelization algorithm for the hitting set problem on 3-uniform hypergraphs (3HS) [1] to (H(F ), k) and obtain in polynomial time an equivalent instance (H \u2032, k\u2032) where H \u2032 has at most O(k2) vertices. It remains to consider H \u2032 as a CNF formula F \u2032 where each hyperedge gives rise to a ternary clause on three positive literals. Since evidently H(F \u2032) = H \u2032, we conclude that (F \u2032, k\u2032) constitutes a polynomial kernel for Rec-SAT(2CNF-backdoor)."}, {"heading": "6 Global Constraints", "text": "Constraint programming (CP) offers a powerful framework for efficient modeling and solving of a wide range of hard problems [58]. At the heart of efficient CP solvers are so-called global constraints that specify patterns that frequently occur in real-world problems. Efficient propagation algorithms for global constraints help speed up the solver significantly [63]. For instance, a frequently occurring pattern is that we require that certain variables must all take different values (e.g., activities requiring the same resource must all be assigned different times). Therefore most constraint solvers provide a globalAllDifferent constraint and algorithms for its propagation. Unfortunately, for several important global constraints a complete propagation is NPhard, and one switches therefore to incomplete propagation such as bound consistency [8].\nIn their AAAI\u201908 paper, Bessi\u00e8re et al. [5] showed that a complete propagation of several intractable constraints can efficiently be done as long as certain natural problem parameters are small, i.e., the propagation is fixed-parameter tractable [25]. Among others, they showed fixed-parameter tractability of the AtLeast-NValue and Extended Global Cardinality (EGC) constraints parameterized by the number of \u201choles\u201d in the domains of the variables. If there are no holes, then all domains are intervals and complete propagation is polynomial by classical results; thus the number of holes provides a way of scaling up the nice properties of constraints with interval domains.\nIn the sequel we bring this approach a significant step forward, picking up a long-term research objective suggested by Bessi\u00e8re et al. [5] in their concluding remarks: whether intractable global constraints admit a reduction to a problem kernel or kernelization.\nMore formally, a global constraint is defined for a set S of variables, each variable x \u2208 S ranges over a finite domain dom(x) of values. For a set X of variables we write dom(X) = \u22c3 x\u2208X dom(x). An instantiation is an assignment \u03b1 : S \u2192 dom(S) such that \u03b1(x) \u2208 dom(x) for each x \u2208 S. A global constraint defines which instantiations are legal and which are not. This definition is usually implicit, as opposed to classical constraints, which list all legal tuples. Examples of global constraints include:\n1. The global constraint NValue is defined over a set X of variables and a variable N and requires from a legal instantiation \u03b1 that |{\u03b1(x) : x \u2208 X }| = \u03b1(N);\n2. The global constraint AtMost-NValue is defined for fixed values of N over a set X of variables and requires from a legal instantiation \u03b1 that |{\u03b1(x) : x \u2208 X }| \u2264 N ;\n3. The global constraint Disjoint is specified by two sets of variables X,Y and requires that \u03b1(x) 6= \u03b1(y) for each pair x \u2208 X and y \u2208 Y ;\n4. The global constraint Uses is also specified by two sets of variables X,Y and requires that for each x \u2208 X there is some y \u2208 Y such that \u03b1(x) = \u03b1(y).\n5. The global constraint EGC is specified by a set of variables X , a set of values D = dom(X), and a finite domain dom(v) \u2286 N for each value v \u2208 D, and it requires that for each v \u2208 D we have |{\u03b1(x) = v : x \u2208 X }| \u2208 dom(v).\nA global constraint C is consistent if there is a legal instantiation of its variables. The constraint C is hyper arc consistent (HAC ) if for each variable x \u2208 scope(C) and each value v \u2208 dom(x), there is a legal instantiation \u03b1 such that \u03b1(x) = v (in that case we say that C supports v for x). In the literature, HAC is also called domain consistent or generalized arc consistent. The constraint C is bound consistent if when a variable x \u2208 scope(C) is assigned the minimum or maximum value of its domain, there are compatible values between the minimum and maximum domain value for all other variables in scope(C). The main algorithmic problems for a global constraint C are the following: Consistency, to decide whether C is consistent, and Enforcing HAC, to remove from all domains the values that are not supported by the respective variable.\nIt is clear that if HAC can be enforced in polynomial time for a constraint C, then the consistency of C can also be decided in polynomial time (we just need to see if any domain became empty). The reverse is true if for each x \u2208 scope(C) and v \u2208 dom(x), the consistency of C \u2227 (x \u2190 v), requiring x to be assigned the value v, can be decided in polynomial time (see [63, Theorem 17]). This is the case for most constraints of practical use, and in particular for all constraints considered below. The same correspondence holds with respect to fixed-parameter tractability. Hence, we will focus mainly on Consistency.\nFor several important types T of global constraints, the problem of deciding whether a constraint of type T is consistent is NP-hard. This includes the 5 global constraints NValue, AtMost-NValue, Disjoint, Uses, and EGC defined above (see [8]).\nEach global constraint of type T and parameter par gives rise to a parameterized problem:\nT -Cons(par)\nInstance: A global constraint C of type T .\nParameter: The integer par.\nQuestion: Is C consistent?\nBessi\u00e8re et al. [5] considered dx = |dom(X)| as parameter for NValue, dxy = |dom(X) \u2229 dom(Y )| as parameter for Disjoint, and dy = |dom(Y )| as parameter for Uses. They showed that consistency checking is fixed-parameter tractable for the constraints under the respective parameterizations, i.e., the problems NValue-Cons(dx), Disjoint-Cons(dxy), and Uses-Cons(dy) are fixed-parameter tractable.\nBessi\u00e8re et al. [5] also showed that polynomial time algorithms for enforcing bounds consistency imply that the corresponding consistency problem is fixed-parameter tractable parameterized by the number of holes. This is the case for the global constraints NValue, AtMost-NValue, and EGC.\nDefinition 1. When D is totally ordered, a hole in a subset D\u2032 \u2286 D is a couple (u,w) \u2208 D\u2032 \u00d7D\u2032, such that there is a v \u2208 D \\D\u2032 with u < v < w and there is no v\u2032 \u2208 D\u2032 with u < v\u2032 < w.\nWe denote the number of holes in the domain of a variable x \u2208 X by #holes(x). The parameter of the consistency problem for AtMost-NValue constraints is holes = \u2211 x\u2208X #holes(x)."}, {"heading": "6.1 Kernel Lower Bounds", "text": "We show that it is unlikely that most of the FPT results of Bessi\u00e8re et al. [5] can be improved to polynomial kernels.\nTheorem 6. The problems NValue-Cons(dx), Disjoint-Cons(dxy), Uses-Cons(dy) do not admit polynomial kernels unless NP \u2286 coNP/poly.\nProof. We devise a polynomial parameter transformation from SAT(vars). We use a construction of Bessi\u00e8re et al. [8]. Let F = {C1, . . . , Cm} be a CNF formula over variables x1, . . . , xn. We consider the clauses and variables of F as the variables of a global constraint with domains dom(xi) = {\u2212i, i}, and dom(Cj) =\n{ i : xi \u2208 Cj } \u222a {\u2212i : \u00acxi \u2208 Cj }. Now F can be encoded as an NValue constraint with X = {x1, . . . , xn, C1, . . . , Cm} and dom(N) = {n}. By the pigeonhole principle, a legal instantiation \u03b1 for this constraint has |{\u03b1(xi) : 1 \u2264 i \u2264 n}| = N . Setting \u03b1(xi) = i corresponds to setting the variable xi of F to 1 and setting \u03b1(xi) = \u2212i corresponds to setting the variable xi of F to 0. Now, for each Cj \u2208 F , \u03b1(Cj) \u2208 {\u03b1(xi) : 1 \u2264 i \u2264 n} since only n values are available for \u03b1, and the literal corresponding to \u03b1(Cj) satisfies the clause Cj . Since dx = 2n we have a polynomial parameter reduction from SAT(vars) to NValue-Cons(dx). Similarly, as observed by Bessi\u00e8re et al. [7], F can be encoded as a Disjoint constraint with X = {x1, . . . , xn} and Y = {C1, . . . , Cm} (dxy \u2264 2n), or as a Uses constraint with X = {C1, . . . , Cm} and Y = {x1, . . . , xn} (dy = 2n). Since the unparameterized problems are clearly NP-complete, and SAT(vars) is known to have no polynomial kernel unless NP \u2286 coNP/poly (as remarked in the proof of Theorem 4), the result follows by Theorem 2.\nThe Consistency problem for EGC constraints is NP-hard [54]. However, if all sets dom(\u00b7) are intervals, then consistency can be checked in polynomial time using network flows [56]. By the result of Bessi\u00e8re et al. [5], the Consistency problem for EGC constraints is fixed-parameter tractable, parameterized by the number of holes in the sets dom(\u00b7). Thus R\u00e9gin\u2019s result generalizes to instances that are close to the interval case.\nHowever, it is unlikely that EGC constraints admit a polynomial kernel.\nTheorem 7. EGC-Cons(holes) does not admit a polynomial kernel unless NP \u2286 coNP/poly.\nProof. We use the following result of Quimper et al. [54]: Given a CNF formula F on k variables, one can construct in polynomial time an EGC constraint CF such that\n1. for each value v of CF , dom(v) = {0, iv} for an integer iv > 0,\n2. iv > 1 for at most 2k values v, and\n3. F is satisfiable if and only if CF is consistent.\nThus, the number of holes in CF is at most twice the number of variables of F . We observe that this result provides a polynomial parameter reduction from SAT(vars) to EGCCons(holes). As remarked in the proof of Theorem 4, SAT(vars) is known to have no polynomial kernel unless NP \u2286 coNP/poly. Hence the theorem follows."}, {"heading": "6.2 A Polynomial Kernel for NValue Constraints", "text": "Beldiceanu [3] and Bessi\u00e8re et al. [6] decompose NValue constraints into two other global constraints: AtMost-NValue and AtLeast-NValue, which require that at most N or at least N values are used for the variables in X , respectively. The Consistency problem is NP-complete for NValue and AtMostNValue constraints, and polynomial time solvable for AtLeast-NValue constraints.\nIn this subsection, we will present a polynomial kernel for AtMost-NValue-Cons(holes).\nAtMost-NValue-Cons(holes)\nInstance: An instance I = (X,D, dom, N), where X = {x1, . . . , xn} is a set of variables, D is a totally ordered set of values, dom : X \u2192 2D is a map assigning a non-empty domain dom(v) \u2286 D to each variable x \u2208 X , and an integer N .\nParameter: The integer k = #holes(X).\nQuestion: Is there a set S \u2286 D, |S| \u2264 N , such that for every variable x \u2208 X , dom(x) \u2229 S 6= \u2205?\nTheorem 8. The problem AtMost-NValue-Cons(holes) has a polynomial kernel. In particular, an AtMost-NValue constraint with k holes can be reduced in linear time to a consistency-equivalent AtMostNValue constraint with O(k2) variables and O(k2) domain values.\nThe proof of the theorem is based on a kernelization algorithm that we will describe in the remaining part of this section.\nWe say that a subset of D is an interval if it has no hole. An interval I = [v1, v2] of a variable x is an inclusion-wise maximal hole-free subset of its domain. Its left endpoint l(I) and right endpoint r(I) are the values v1 and v2, respectively. Fig. 2 gives an example of an instance and its interval representation. We assume that instances are given by a succinct description, in which the domain of a variable is given by the left and right endpoint of each of its intervals. As the number of intervals of the instance I = (X,D, dom , N) is n+ k, its size is |I| = O(n+ |D|+ k). In case dom is given by an extensive list of the values in the domain of each variable, a succinct representation can be computed in linear time.\nAlso, in a variant of AtMost-NValue-Cons(holes) where D is not part of the input, we may construct D by sorting the set of all endpoints of intervals in time O((n + k) log(n + k)). Since, w.l.o.g., a solution contains only endpoints of intervals, this step does not compromise the correctness.\nA greedy algorithm by Beldiceanu [3] checks the consistency of an AtMost-NValue constraint in linear time when all domains are intervals (i.e., k = 0). Further, Bessi\u00e8re et al. [5] have shown that Consistency (and Enforcing HAC) is fixed-parameter tractable, parameterized by the number of holes, for all constraints for which bound consistency can be enforced in polynomial time. A simple algorithm for checking the consistency of AtMost-NValue goes over all instances obtained from restricting the domain of each variable to one of its intervals, and executes the algorithm of [3] for each of these 2k instances. The running time of this algorithm is clearly bounded by O(2k \u00b7 |I|).\nLet I = (X,D, dom , N) be an instance for the consistency problem for AtMost-NValue constraints. The algorithm is more intuitively described using the interval representation of the instance.The friends of an interval I are the other intervals of I\u2019s variable. An interval is optional if it has at least one friend, and required otherwise. For a value v \u2208 D, let ivl(v) denote the set of intervals containing v.\nA solution for I is a subset S \u2286 D of at most N values such that there exists an instantiation assigning the values in S to the variables in X . The algorithm may detect for some value v \u2208 D, that, if the problem has a solution, then it has a solution containing v. In this case, the algorithm selects v, i.e., it removes all variables whose domain contains v, it removes v from D, and it decrements N by one. The algorithm may detect for some value v \u2208 D, that, if the problem has a solution, then it has a solution not containing v. In this case, the algorithm discards v, i.e., it removes v from every domain and from D. (Note that no new holes are created since D is replaced by D \\ {v}.) The algorithm may detect for some variable x, that every solution for (X \\ {x}, D, dom|X\\{x}, N) contains a value from dom(x). In that case, it removes x.\nThe algorithm sorts the intervals by increasing right endpoint (ties are broken arbitrarily). Then, it exhaustively applies the following three reduction rules.\nRed-\u2286: If there are two intervals I, I \u2032 such that I \u2032 \u2286 I and I \u2032 is required, then remove the variable of I (and its intervals).\nRed-Dom: If there are two values v, v\u2032 \u2208 D such that ivl(v\u2032) \u2286 ivl(v), then discard v\u2032.\nRed-Unit: If |dom(x)| = 1 for some variable x, then select the value in dom(x).\nIn the example from Fig. 2, Red-\u2286 removes the variables x5 and x8 because x10 \u2286 x\u20325 and x7 \u2286 x8, RedDom removes the values 1 and 5, Red-Unit selects 2, which deletes variables x1 and x2, and Red-Dom removes 3 from D. The resulting instance is depicted in Fig. 3.\nAfter none of the previous rules apply, the algorithm scans the remaining intervals from left to right (i.e., by increasing right endpoint). An interval that has already been scanned is either a leader or a follower of a subset of leaders. Informally, for a leader L, if a solution contains r(L), then there is a solution containing r(L) and the right endpoint of each of its followers.\nThe algorithm scans the first intervals up to, and including, the first required interval. All these intervals become leaders.\nThe algorithm then continues scanning intervals one by one. Let I be the interval that is currently scanned and Ip be the last interval that was scanned. The active intervals are those that have already been scanned and intersect Ip. A popular leader is a leader that is either active or has at least one active follower.\n\u2022 If I is optional, then I becomes a leader, the algorithm continues scanning intervals until scanning a required interval; all these intervals become leaders.\n\u2022 If I is required, then it becomes a follower of all popular leaders that do not intersect I and that have no follower intersecting I. If all popular leaders have at least two followers, then set N := N \u2212 1 and merge the second-last follower of each popular leader with the last follower of the corresponding leader; i.e., for every popular leader, the right endpoint of its second-last follower is set to the right endpoint of its last follower, and then the last follower of every popular leader is removed.\nAfter having scanned all the intervals, the algorithm exhaustively applies the reduction rules Red-\u2286, RedDom, and Red-Unit again.\nIn the example from Fig. 3, the interval of variable x6 is merged with x9\u2019s interval, and the interval of x7 with the interval of x10. Red-Dom then removes the values 7 and 8, resulting in the instance depicted in Fig. 4.\nThe correctness and performance guarantee of this kernelization algorithm are proved in A. In particular, for the correctness, we prove that a solution S for an instance I can be obtained from a solution S\u2032 for an instance I \u2032 that is obtained from I by one merge-operation by adding to S\u2032 one value that is common to all second-last followers of the popular leaders that were merged. We can easily bound the number of leaders by 4k and we prove that each leader has at most 4k followers. Since each interval is a leader or a follower of at least one leader, this bounds the total number of intervals by O(k2). Using the succinct description of the domains, the size of the kernel is O(k2). We also give some details for a linear-time implementation of the algorithm.\nRemark: Denoting var(v) = {x \u2208 X : v \u2208 dom(x)}, Rule Red-Dom can be generalized to discard any v\u2032 \u2208 D for which there exists a v \u2208 D such that var(v\u2032) \u2286 var(v) at the expense of a higher running time.\nThe kernel for AtMost-NValue-Cons(holes) can now be used to derive a kernel for NValue-Cons(holes).\nCorollary 1. The problem NValue-Cons(holes) has a polynomial kernel. In particular, an NValue constraint with k holes can be reduced in O((|X |+ |D|)\u03c9/2) time to a consistency-equivalent NValue constraint with O(k2) variables and O(k2) domain values, where \u03c9 < 2.3729 is the exponent of matrix multiplication.\nProof. As in [6], we determine the largest possible value for N if its domain were the set of all integers. This can be done in O((|X |+ |D|)\u03c9/2) time [48, 64] by computing a maximum matching in the graph whose vertices are X \u222a D with an edge between x \u2208 X and v \u2208 D iff v \u2208 dom(x). Suppose this largest possible value is N+. Now, set dom(N) := {v \u2208 dom(N) : v \u2264 N+}, giving a consistency-equivalent NValue constraint. Note that if this constraint has a legal instantiation \u03b1 with \u03b1(N) \u2264 max(dom(N)), then it has a legal instantiation \u03b1\u2032 with \u03b1\u2032(N) = max(dom(N)). Therefore, it suffices to compute a kernel for AtMost-NValue-Cons(holes) with the same variables and domains and value N = max(dom(N)), and return it."}, {"heading": "6.3 Improved FPT Algorithm using the Kernel", "text": "Using the kernel from Theorem 8 and the simple algorithm described in the beginning of this section, one arrives at a O(2kk2 + |I|) time algorithm for checking the consistency of an AtMost-NValue constraint. Borrowing ideas from the kernelization algorithm, we now reduce the exponential dependency on k in the running time. The speed-ups due to this branching algorithm and the kernelization algorithm lead to a speed-up for enforcing HAC for AtMost-NValue constraints (by Corollary 2) and for enforcing HAC for NValue constraints (by the decomposition of [6]).\nTheorem 9. The Consistency problem for AtMost-NValue constraints admits a O(\u03d5kk2 + |I|) time algorithm, where k is the number of holes in the domains of the input instance I, and \u03d5 = 1+ \u221a 5\n2 < 1.6181.\nProof. The first step of the algorithm invokes the kernelization algorithm and obtains an equivalent instance I \u2032 with O(k2) intervals in time O(|I|).\nNow, we describe a branching algorithm checking the consistency of I \u2032. Let I1 denote the first interval of I \u2032 (in the ordering by increasing right endpoint). I1 is optional. Let I1 denote the instance obtained from I \u2032 by selecting r(I1) and exhaustively applying Reduction Rules Red-Dom and Red-Unit. Let I2 denote the instance obtained from I \u2032 by removing I1 (if I1 had exactly one friend, this friend becomes required) and exhaustively applying Reduction Rules Red-Dom and Red-Unit. Clearly, I \u2032 is consistent if and only if I1 or I2 is consistent.\nNote that both I1 and I2 have at most k \u2212 1 holes. If either I1 or I2 has at most k \u2212 2 holes, the algorithm recursively checks whether at least one of I1 and I2 is consistent. If both I1 and I2 have exactly k \u2212 1 holes, we note that in I \u2032,\n1. I1 has one friend,\n2. no other optional interval intersects I1, and\n3. the first interval of both I1 and I2 is If , which is the third optional interval in I \u2032 if the second optional interval is the friend of I1, and the second optional interval otherwise.\nThus, the instance obtained from I1 by removing I1\u2019s friend and applying Red-Dom and Red-Unit may differ from I2 only in N . Let s1 and s2 denote the number of values smaller than r(If ) that have been selected to obtain I1 and I2 from I\n\u2032, respectively. If s1 \u2264 s2, then the non-consistency of I1 implies the non-consistency of I2. Thus, the algorithm need only recursively check whether I1 is consistent. On the other hand, if s1 > s2, then the non-consistency of I2 implies the non-consistency of I1. Thus, the algorithm need only recursively check whether I2 is consistent.\nThe recursive calls of the algorithm may be represented by a search tree labeled with the number of holes of the instance. As the algorithm either branches into only one subproblem with at most k\u2212 1 holes, or two subproblems with at most k \u2212 1 and at most k \u2212 2 holes, respectively, the number of leaves of this search tree is T (k) \u2264 T (k \u2212 1) + T (k \u2212 2), with T (0) = T (1) = 1. Using standard techniques in the analysis of exponential time algorithms (see, e.g., [32, Chapter 2] and [35, Lemma 2.3]), it suffices to find a value c > 1 for the base of the exponential function bounding the running time, that we will minimize, such that\nck\u22121 + ck\u22122 \u2264 ck for all k \u2265 0,\nor, equivalently, such that\nc+ 1 \u2264 c2\nIt now suffices to find the unique positive real root of x2 \u2212 x\u2212 1, which is \u03d5 = 1+ \u221a 5\n2 < 1.6181, to determine the optimal value of c for this analysis.\nSince the size of the search tree is O(\u03d5k) and the number of operations executed at each node of the search tree is O(k2), the running time of the branching algorithm can be upper bounded by O(\u03d5kk2).\nFor the example of Fig. 4, the instances I1 and I2 are computed by selecting the value 4, and removing the interval x3, respectively. The reduction rules select the value 9 for I1 and the values 6 and 10 for I2. Both instances start with the interval x11, and the algorithm recursively solves I1 only, where the values 12 and 13 are selected, leading to the solution {4, 9, 12, 13} for the kernelized instance, which corresponds to the solution {2, 4, 7, 9, 12, 13} for the instance of Fig. 2.\nCorollary 2. HAC for an AtMost-NValue constraint can be enforced in time O(\u03d5k \u00b7k2\u00b7|D|+|I|\u00b7|D|), where k is the number of holes in the domains of the input instance I = (X,D, dom , N), and \u03d5 = 1+ \u221a 5\n2 < 1.6181.\nProof. We first remark that if a value v can be filtered from the domain of a variable x (i.e., v has no support for x), then v can be filtered from the domain of all variables, as for any legal instantiation \u03b1 with \u03b1(x\u2032) = v, x\u2032 \u2208 X \\ {x}, the assignment obtained from \u03b1 by setting \u03b1(x) := v is a legal instantiation as well. Also, filtering the value v creates no new holes as the set of values can be set to D \\ {v}.\nNow we enforce HAC by applying O(|D|) times the algorithm from Theorem 9. Assume the instance I = (X,D, dom , N) is consistent. If (X,D, dom , N\u22121) is consistent, then no value can be filtered. Otherwise, check, for each v \u2208 D, whether the instance obtained from selecting v is consistent and filter v if this is not the case.\nUsing the same reasoning as in Corollary 1, we now obtain the following corollary for NValue.\nCorollary 3. HAC for an NValue constraint can be enforced in time O((\u03d5k \u00b7 k2 + (|X |+ |D|)\u03c9/2) \u00b7 |D|), where k is the number of holes in the domains of the input instance I = (X,D, dom , N), \u03d5 = 1+ \u221a 5\n2 < 1.6181, and \u03c9 < 2.3729 is the exponent of matrix multiplication."}, {"heading": "7 Bayesian Reasoning", "text": "Bayesian networks (BNs) have emerged as a general representation scheme for uncertain knowledge [53]. A BN models a set of stochastic variables, the independencies among these variables, and a joint probability distribution over these variables. For simplicity we consider the important special case where the stochastic variables are Boolean. The variables and independencies are modeled in the BN by a directed acyclic graph G = (V,A), the joint probability distribution is given by a table Tv for each node v \u2208 V which defines a probability Tv|U for each possible instantiation U = (d1, . . . , ds) \u2208 {true, false}\ns of the parents v1, . . . , vs of v in G. The probability Pr(U) of a complete instantiation U of the variables of G is given by the product of Tv|U over all variables v. We consider the problem Positive-BN-Inference which takes as input a Boolean BN (G, T ) and a variable v, and asks whether Pr(v = true) > 0. The problem is NP-complete [19] and moves from NP to #P if we ask to compute Pr(v = true) [59]. The problem can be solved in polynomial time if the BN is singly connected, i.e, if there is at most one undirected path between any two variables [52]. It is natural to parametrize the problem by the number of variables one must delete in order to make the BN singly connected (the deleted variables form a loop cutset). This yields the following parameterized problem.\nPositive-BN-Inference(loop cutset size)\nInstance: A Boolean BN (G, T ), a variable v, and a loop cutset S of size k.\nParameter: The integer k.\nQuestion: Is Pr(v = true) > 0?\nAgain we also state a related recognition problem.\nRec-Positive-BN-Inference(loop cutset size)\nInstance: A Boolean BN (G, T ) and an integer k \u2265 0.\nParameter: The integer k.\nQuestion: Does (G, T ) has a loop cutset of size \u2264 k?.\nNow, Positive-BN-Inference(loop cutset size) is easily seen to be fixed-parameter tractable as we can determine whether Pr(v = true) > 0 by taking the maximum of Pr(v = true | U) over all 2k possible instantiations of the k cutset variables, each of which requires processing of a singly connected network. However, although fixed-parameter tractable, it is unlikely that the problem admits a polynomial kernel.\nTheorem 10. Positive-BN-Inference(loop cutset size) does not admit a polynomial kernel unless NP \u2286 coNP/poly.\nProof. We give a polynomial parameter transformation from SAT(vars) and apply Theorem 2. The reduction is based on the reduction from 3SAT given by Cooper [19]. Let F be a CNF formula on n variables. We construct a BN (G, T ) such that for a variable v we have Pr(v = true) > 0 if and only if F is satisfiable. Cooper uses input nodes ui for representing variables of F , clause nodes ci for representing the clauses of F , and conjunction nodes di for representing the conjunction of the clauses. For instance, if F has three clauses and four variables, then Cooper\u2019s reduction produces a BN (G, T ) where G has the following shape:\nu1 u2 u3 u4\nc1 c2 c3\nd1 d2 d3\nClearly, the input nodes form a loop cutset of G. However, in order to get a polynomial parameter transformation from SAT(vars) we must allow in F that clauses contain an arbitrary number of literals, not just three. If we apply Cooper\u2019s reduction directly, then a single clause node ci with many parents requires a table Tci of exponential size. To overcome this difficulty we simply split clause nodes ci containing more than 3 literals into several clause nodes, as indicated below, where the last one feeds into a conjunction node di.\nu1 u2 u3 u4\nc1\nc \u2032 1\nc \u2032\u2032 1\nIt remains to observe that the set of input nodes E = {u1, . . . , un} still form a loop cutset of the constructed BN, hence we have indeed a polynomial parameter transformation from SAT(vars) to Positive-BN-Inference(loop cutset size). The result follows by Theorem 2.\nLet us now turn to the recognition problem Rec-Positive-BN-Inference(loop cutset size).\nProposition 3. Rec-Positive-BN-Inference(loop cutset size) admits a polynomial kernel with O(k2) nodes.\nProof. Let ((G, T ), k) be an instance of Rec-Positive-BN-Inference(loop cutset size). We note that loop cutsets of (G, T ) are just the so-called feedback vertex sets of G. Hence we can apply a known kernelization algorithm for feedback vertex sets [16] to G and obtain a kernel (G\u2032, k) with at most O(k2) many vertices. We translate this into an instance (G\u2032, T \u2032, k\u2032) of Rec-Positive-BN-Inference(loop cutset size) by taking an arbitrary table T \u2032."}, {"heading": "8 Nonmonotonic Reasoning", "text": "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning [39, 46]. A (normal) logic program P is a finite set of rules r of the form\nh \u2190\u2212 a1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 am \u2227 \u00acb1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u00acbn\nwhere h, ai, bi are atoms, where h forms the head and the ai, bi from the body of r. We write H(r) = h, B+(r) = {a1, . . . , am}, and B\u2212(r) = {b1, . . . , bn}. Let I be a finite set of atoms. The GF reduct P I of a logic program P under I is the program obtained from P by removing all rules r with B\u2212(r) \u2229 I 6= \u2205, and removing from the body of each remaining rule r\u2032 all literals \u00acb with b \u2208 I. I is a stable model of P if I is a minimal model of P I , i.e., if (i) for each rule r \u2208 P I with B+(r) \u2286 I we have H(r) \u2208 I, and (ii) there is no proper subset of I with this property. The undirected dependency graph U(P ) of P is formed as follows. We take the atoms of P as vertices and add an edge x \u2212 y between two atoms x, y if there is a rule r \u2208 P with H(r) = x and y \u2208 B+(r), and we add a path x\u2212 u\u2212 y if H(r) = x and y \u2208 B\u2212(r) (u is a new vertex of degree 2). The feedback width of P is the size of a smallest set V of atoms such that every cycle of U(P ) runs through an atom in V (such a set V is called a feedback vertex set).\nA fundamental computational problems is Stable Model Existence (SME), which asks whether a given normal logic program has a stable model. The problem is well-known to be NP-complete [47]. Gottlob et al. [41] considered the following parameterization of the problem and showed fixed-parameter tractability (see [31] for generalizations).\nSME(feedback width)\nInstance: A logic program P and feedback vertex set V of size k.\nParameter: The integer k.\nQuestion: Does P have a stable model?\nAgain we also state a related recognition problem.\nRec-SME(feedback width)\nInstance: A logic program P and an integer k \u2265 0.\nParameter: The integer k.\nQuestion: Does P have a a feedback vertex set of size at most k?\nWe show that the result of Gottlob et al. [41] cannot be strengthened towards a polynomial kernel.\nTheorem 11. SME(feedback width) does not admit a polynomial kernel unless NP \u2286 coNP/poly.\nProof. We give a polynomial parameter transformation from SAT(vars) to SME(feedback width) using a construction of Niemel\u00e4 [49]. Given a CNF formula F on n variables, we construct a logic program P as follows. For each variable x of F we take two atoms x and x\u0302 and include the rules (x\u0302 \u2190 \u00acx) and (x \u2190 \u00acx\u0302); for each clause C of F we take an atom c and include for each positive literal a of C the rule (c \u2190 a), and for each negative literal \u00aca of C the rule (c \u2190 a\u0302); finally, we take two atoms s and f and include the rule (f \u2190 \u00acf \u2227 \u00acs) and for each clause C of F the rule (s \u2190 \u00acc). The formula F is satisfiable if and only if P has a stable model [49]. It remains to observe that each cycle of U(P ) runs through a vertex in V = { x, x\u0302 : x \u2208 vars(F ) }, hence the feedback width of P is at most 2n. Hence we have a polynomial parameter transformation from SAT(vars) to SME(feedback width). The result follows by Theorem 2.\nUsing a similar approach as for Proposition 3 we can establish the following result.\nProposition 4. Rec-SME(feedback width) admits a polynomial kernel with O(k2) atoms."}, {"heading": "9 Conclusion", "text": "We have provided the first theoretical evaluation of the guarantees and limits of polynomial-time preprocessing for hard AI problems. In particular we have established super-polynomial kernel lower bounds for many problems, providing firm limitations for the power of polynomial-time preprocessing for these problems. On the positive side, we have developed an efficient linear-time kernelization algorithm for the consistency problem for AtMost-NValue constraints, and have shown how it can be used to speed up the complete propagation of NValue and related constraints.\nSubsequent to our work, Fellows et al. [29] investigated the parameterized complexity and kernelization for various parameterizations of Abductive Reasoning. Their kernelization results were mostly negative, showing that many parameterizations for the Abduction problem have no polynomial kernels unless NP \u2286 coNP/poly. Similarly negative are the kernelization results of B\u00e4ckstr\u00f6m et al. [2] for planning problems, parameterized by the length of the plan.\nWe conclude from these results that in contrast to many optimization problems (see Section 1), typical AI problems do not admit polynomial kernels. Our results suggest the consideration of alternative approaches. For example, it might still be possible that some of the considered problems admit polynomially sized Turing kernels, i.e., a polynomial-time preprocessing to a Boolean combination of a polynomial number of polynomial kernels. In the area of optimization, parameterized problems are known that do not admit polynomial kernels but admit polynomial Turing kernels [30]. This suggests a theoretical and empirical study of Turing kernels for the AI problems considered."}, {"heading": "A Appendix: Proof of Theorem 8", "text": "In this appendix, we prove Theorem 8 by proving the correctness of the algorithm, upper bounding the size of the kernel, and analyzing its running time.\nLet I \u2032 = (X \u2032, D\u2032, dom \u2032, N \u2032) be the instance resulting from applying one operation of the kernelization algorithm to an instance I = (X,D, dom , N). An operation is an instruction which modifies the instance: Red-\u2286, Red-Dom, Red-Unit, and merge. We show that there exists a solution S for I if and only if there exists a solution S\u2032 for I \u2032. A solution is nice if each of its elements is the right endpoint of some interval. Clearly, for every solution, a nice solution of the same size can be obtained by shifting each value to the next right endpoint of an interval. Thus, when we construct S\u2032 from S (or vice-versa), we may assume that S is nice.\nReduction Rule Red-\u2286 is sound because a solution for I is a solution for I \u2032 and vice-versa, because any solution I \u2032 contains a value v of I \u2286 I \u2032, as I is required. Reduction Rule Red-Dom is correct because if v\u2032 \u2208 S, then S\u2032 := (S \\ {v\u2032}) \u222a {v} is a solution for I \u2032 and for I. Reduction Rule Red-Unit is obviously correct (S = S\u2032 \u222a dom(x)).\nAfter having applied these 3 reduction rules, observe that the first interval is optional and contains only one value. Suppose the algorithm has started scanning intervals. By construction, the following properties apply to I \u2032.\nProperty 1. A follower does not intersect any of its leaders. Property 2. If I, I \u2032 are two distinct followers of the same leader, then I and I \u2032 do not intersect.\nBefore proving the correctness of the merge operation, let us first show that the subset of leaders of a follower is not empty.\nClaim 1. Every interval that has been scanned is either a leader or a follower of at least one leader.\nProof. First, note that Red-Dom ensures that each domain value in D is the left endpoint of some interval and the right endpoint of some interval. We show that when an interval I is scanned it either becomes a leader or a follower of at least one leader. By induction, assume this is the case for all previously scanned intervals. Denote by Ip the interval that was scanned prior to I. If Ip or I is optional, then I becomes a leader. Suppose I and Ip are required. We have that l(I) > l(Ip), otherwise I would have been removed by Red-\u2286. By Rule Red-Dom, there is some interval I\u2113 with r(I\u2113) = l(Ip). If I\u2113 is a leader, I becomes a follower of I\u2113; otherwise I becomes a follower of I\u2113\u2019s leader.\nWe will now prove the correctness of the merge operation. Recall that I \u2032 is an instance obtained from I by one application of the merge operation. Let I denote the interval that is scanned when the merge operation is applied. At this computation step, each popular leader has at least two followers and the algorithm merges the last two followers of each popular leader and decrements N by one. Let F2 denote the set of all intervals that are the second-last follower of a popular leader, and F1 the set of all intervals that are the last follower of a popular leader before merging. Let M denote the set of merged intervals. Clearly, every interval of F1 \u222a F2 \u222aM is required as all followers are required.\nLemma 1. Every interval in F1 intersects l(I).\nProof. Let I1 \u2208 F1. By construction, r(I1) \u2208 I, as I becomes a follower of every popular leader that has no follower intersecting I, and no follower has a right endpoint larger than r(I). Moreover, l(I1) \u2264 l(I) as no follower is a strict subset of I by Red-\u2286 and the fact that all followers are required.\nThe correctness of the merge operation will follow from the next two lemmas.\nLemma 2. If S is a nice solution for I, then there exists a solution S\u2032 for I \u2032 with S\u2032 \u2286 S.\nProof. Let I\u2212 be the interval of F2 with the largest right endpoint. Let L be a leader of I\u2212. By construction and Red-\u2286, L is a leader of I as well and is therefore popular. Let t1 \u2208 S \u2229 I be the smallest value of S that intersects I and let t2 \u2208 S \u2229 I\u2212 be the largest value of S that intersects I\u2212. By Property 2, we have that t2 < t1.\nClaim 2. The set S contains no value t0 such that t2 < t0 < t1.\nProof. For the sake of contradiction, suppose S contains a value t0 such that t2 < t0 < t1. Since S is nice, t0 is the right endpoint of some interval I0. Since t2 is the rightmost value intersecting S and any interval in F2, the interval I0 is not in F2. Since I0 has already been scanned, and was scanned after every interval in F2, the interval I0 is in F1. However, by Lemma 1, I0 intersects l(I). Since no scanned interval has a larger right endpoint than I, we have that t0 \u2208 S \u2229 I, which contradicts the fact that t1 is the smallest value in S \u2229 I and that t0 < t1.\nClaim 3. Suppose I1 \u2208 F1 and I2 \u2208 F2 are the last and second-last follower of a popular leader L\u2032, respectively. Let M12 \u2208 M denote the interval obtained from merging I2 with I1. If t2 \u2208 I2, then t1 \u2208 M12.\nProof. For the sake of contradiction, assume t2 \u2208 I2, but t1 /\u2208 M12. As t2 < t1, we have that t1 > r(M12) = r(I1). But then S is not a solution as S \u2229 I1 = \u2205 by Claim 2 and the fact that t2 < l(I1).\nClaim 4. If I \u2032 is an interval with t2 \u2208 I \u2032, then I \u2032 \u2208 F2 \u222a F1.\nProof. First, suppose I \u2032 is a leader. As every leader has at least two followers when I is scanned, I \u2032 has two followers whose left endpoint is larger than r(I \u2032) \u2265 t2 (by Property 1) and smaller than l(I) \u2264 t1 (by Red-\u2286). Thus, at least one of them is included in the interval (t2, t1) by Property 2, which contradicts S being a solution by Claim 2.\nSimilarly, if I \u2032 is a follower of a popular leader, but not among the last two followers of any popular leader, Claim 2 leads to a contradiction as well.\nFinally, if I \u2032 is a follower, but has no popular leader, then it is to the left of some popular leader, and thus to the left of t2.\nConsider the set T2 of intervals that intersect t2. By Claim 4, we have that T2 \u2286 F2 \u222a F1. For every interval I \u2032 \u2208 T2 \u2229 F2, the corresponding merged interval of I \u2032 intersects t1 by Claim 3. For every interval I \u2032 \u2208 T2 \u2229F1, and every interval I \u2032\u2032 \u2208 F2 with which I \u2032 is merged, S contains some value x \u2208 I \u2032\u2032 with x < t2. Thus, S\u2032 := S \\ {t2} is a solution for I \u2032.\nLemma 3. If S\u2032 is a nice solution for I \u2032, then there exists a solution S for I with S\u2032 \u2286 S.\nProof. As in the previous proof, consider the step where the kernelization algorithm applies the merge operation. Recall that the currently scanned interval is I. Let F2 and F1 denote the set of all intervals that are the second-last and last follower of a popular leader before merging, respectively. Let M denote the set of merged intervals.\nBy Lemma 1, every interval of M intersects l(I). On the other hand, every interval of I \u2032 whose right endpoint intersects I is in M , by construction. Thus, S\u2032 contains the right endpoint of some interval of M . Let t1 denote the smallest such value, and let I1 denote the interval of I with r(I1) = t1 (due to Red-\u2286, there is a unique such interval). Let I2 denote the interval of I with the smallest right endpoint such that there is a leader L whose second-last follower is I2 and whose last follower is I1, and let t2 := r(I2). Claim 5. Let I \u20321 \u2208 F1 and I \u2032 2 \u2208 F2 be two intervals from I that are merged into one interval M \u2032 12 of I\n\u2032. If t1 \u2208 M \u203212, then t2 \u2208 I \u2032 2.\nProof. For the sake of contradiction, suppose t1 \u2208 M \u203212 but t2 /\u2208 I \u2032 2. We consider two cases. In the first case, I \u20322 \u2286 (t2, l(I \u2032 1)). But then, I \u2032 2 would have become a follower of L, which contradicts that I1 is the last follower of L. In the second case, r(I \u20322) < t2. But then, I1 is a follower of the same leader as I \u2032 1, as l(I1) \u2264 l(I \u2032 1), and thus I1 = I \u2032 1. By the definition of I2, however, t2 = r(I2) \u2264 r(I \u2032 2), a contradiction.\nBy the previous claim, a solution S for I is obtained from a solution S\u2032 for I \u2032 by setting S := S\u2032 \u222a {t2}.\nAfter having scanned all the intervals, Reduction Rules Red-\u2286, Red-Dom, and Red-Unit are applied again, and we have already proved their correctness.\nThus, the kernelization algorithm returns an equivalent instance. To bound the kernel size by a polynomial in k, let I\u2217 = (V \u2217, D\u2217, dom\u2217, N\u2217) be the instance resulting from applying the kernelization algorithm to an instance I = (V,D, dom , N). Property 3. The instances I and I\u2217 have at most 2k optional intervals.\nProperty 3 holds for I as every optional interval of a variable x is adjacent to at least one hole and each hole is adjacent to two optional intervals of x. It holds for I\u2217 as the kernelization algorithm introduces no holes.\nLemma 4. The instance I\u2217 has at most 4k leaders.\nProof. Consider the unique step of the algorithm that creates leaders. An optional interval is scanned, the algorithm continues scanning intervals until scanning a required interval, and all these scanned intervals become leaders. As every interval is scanned only once, we have that for every optional interval there are at most 2 leaders. By Property 3, the number of leaders is thus at most 4k.\nLemma 5. Every leader has at most 4k followers.\nProof. Consider all steps where a newly scanned interval becomes a follower, but is not merged with another interval. In each of these steps, the popular leader Lr with the rightmost right endpoint either\n(a) has no follower and intersects I, or\n(b) has no follower and does not intersect I, or\n(c) has one follower and intersects I.\nNow, let L be some leader and let us consider a period where no optional interval is scanned. Let us bound the number of intervals that become followers of L during this period without being merged with another interval. If the number of followers of L increases in Situation (a), it does not increase in Situation (a) again during this period, as no other follower of L may intersect I. After Situation (b) occurs, Situation (b) does not occur again during this period, as I becomes a follower of Lr. Moreover, the number of followers of L\ndoes not increase during this period in Situation (c) after Situation (b) has occurred, as no other follower of L may intersect I. After Situation (c) occurs, the number of followers of L does not increase in Situation (c) again during this period, as no other follower of L may intersect I. Thus, at most 2 followers are added to L in each period. As the first scanned interval is optional, Property 3 bounds the number of periods by 2k. Thus, L has at most 4k followers.\nAs, by Claim 1, every interval of I\u2217 is either a leader or a follower of at least one leader, Lemmas 4 and 5 imply that I\u2217 has O(k2) intervals, and thus |X\u2217| = O(k2). Because of Reduction Rule Red-Dom, every value in D\u2217 is the right endpoint and the left endpoint of some interval, and thus, |D\u2217| = O(k2).\nThis bounds the kernel size, and we will now show that the algorithm can be implemented to run in linear time. First, using a counting sort algorithm with satellite data (see, e.g., [20]), the initial sorting of the n + k intervals can be done in time O(n + |D| + k). To facilitate the application of Red-\u2286, counting sort is used a second time to also sort by increasing left endpoint the sets of intervals with coinciding right endpoint. An optimized implementation applies Red-\u2286, Red-Dom and Red-Unit simultaneously in one pass through the intervals, as one rule might trigger another. To guarantee a linear running time for the scan-and-merge phase of the algorithm, only the first follower of a leader stores a pointer to the leader; all other followers store a pointer to the previous follower. This proves Theorem 8."}, {"heading": "Acknowledgments", "text": "Both authors acknowledge support by the European Research Council, grant reference 239962 (COMPLEX REASON). Serge Gaspers is the recipient of an Australian Research Council Discovery Early Career Researcher Award (project number DE120101761). NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program."}], "references": [{"title": "A kernelization algorithm for d-hitting", "author": ["Faisal N. Abu-Khzam"], "venue": "set. J. of Computer and System Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Parameterized complexity and kernel bounds for hard planning problems", "author": ["Christer B\u00e4ckstr\u00f6m", "Peter Jonsson", "Sebastian Ordyniak", "Stefan Szeider"], "venue": "Algorithms and Complexity, 8th International Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Pruning for the minimum constraint family and for the number of distinct values constraint family", "author": ["Nicolas Beldiceanu"], "venue": "Principles and Practice of Constraint Programming - CP 2001, 7th International Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "The parameterized complexity of global constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Claude-Guy Quimper", "Toby Walsh"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Filtering algorithms for the NValue", "author": ["Christian Bessiere", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Toby Walsh"], "venue": "constraint. Constraints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Range and roots: Two common patterns for specifying and propagating counting and occurrence constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Toby Walsh"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "The complexity of global constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Toby Walsh"], "venue": "Proceedings of the Nineteenth National Conference on Artificial Intelligence, July 25-29,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Cutset sampling for Bayesian networks", "author": ["Bozhena Bidyuk", "Rina Dechter"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A linear-time algorithm for finding tree-decompositions of small treewidth", "author": ["Hans L. Bodlaender"], "venue": "SIAM J. Comput.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "On problems without polynomial kernels", "author": ["Hans L. Bodlaender", "Rodney G. Downey", "Michael R. Fellows", "Danny Hermelin"], "venue": "J. of Computer and System Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Kernel bounds for disjoint cycles and disjoint paths", "author": ["Hans L. Bodlaender", "St\u00e9phan Thomass\u00e9", "Anders Yeo"], "venue": "ESA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Preprocessing the MAP problem", "author": ["J.H. Bolt", "L.C van der Gaag"], "venue": "Proceedings of the Third European Workshop on Probabilistic Graphical Models,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Nondeterminism within P", "author": ["Jonathan F. Buss", "Judy Goldsmith"], "venue": "SIAM J. Comput.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Preprocessing of intractable problems", "author": ["Marco Cadoli", "Francesco M. Donini", "Paolo Liberatore", "Marco Schaerf"], "venue": "Information and Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "On feedback vertex set new measure and new structures", "author": ["Yixin Cao", "Jianer Chen", "Yang Liu"], "venue": "Algorithm Theory - SWAT 2010, 12th Scandinavian Symposium and Workshops on Algorithm Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Improved upper bounds for vertex cover", "author": ["Jianer Chen", "Iyad A. Kanj", "Ge Xia"], "venue": "Theoretical Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "The complexity of theorem-proving procedures", "author": ["Stephen A. Cook"], "venue": "In Proc. 3rd Annual Symp. on Theory of Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["Gregory F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1990}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Constraint satisfaction. In The CogNet Library: References Collection", "author": ["Rina Dechter"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Tree clustering for constraint networks", "author": ["Rina Dechter", "Judea Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "Linear-time algorithms for testing the satisfiability of propositional Horn formulae", "author": ["William F. Dowling", "Jean H. Gallier"], "venue": "J. Logic Programming,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1984}, {"title": "Parameterized complexity: A framework for systematically confronting computational intractability", "author": ["R. Downey", "M.R. Fellows", "U. Stege"], "venue": "Contemporary Trends in Discrete Mathematics: From DI- MACS and DIMATIA to the Future, volume 49 of AMS-DIMACS, pages 49\u201399. American Mathematical Society", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Parameterized Complexity", "author": ["R.G. Downey", "M.R. Fellows"], "venue": "Monographs in Computer Science. Springer Verlag, New York", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "New limits to classical and quantum instance compression", "author": ["Andrew Drucker"], "venue": "In 53rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Effective preprocessing in SAT through variable and clause elimination", "author": ["Niklas E\u00e9n", "Armin Biere"], "venue": "In Fahiem Bacchus and Toby Walsh, editors, Theory and Applications of Satisfiability Testing, 8th International Conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "The lost continent of polynomial time: Preprocessing and kernelization", "author": ["Michael R. Fellows"], "venue": "IWPEC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "The parameterized complexity of abduction", "author": ["Michael R. Fellows", "Andreas Pfandler", "Frances A. Rosamond", "Stefan R\u00fcmmele"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Kernel(s) for problems with no kernel: On out-trees with many leaves", "author": ["Henning Fernau", "Fedor V. Fomin", "Daniel Lokshtanov", "Daniel Raible", "Saket Saurabh", "Yngve Villanger"], "venue": "26th International Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Backdoors to tractable answer-set programming", "author": ["Johannes Klaus Fichte", "Stefan Szeider"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Exact Exponential Algorithms", "author": ["F.V. Fomin", "D. Kratsch"], "venue": "Springer Verlag", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Infeasibility of instance compression and succinct PCPs for NP", "author": ["Lance Fortnow", "Rahul Santhanam"], "venue": "Proceedings of the 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Exponential Time Algorithms - Structures, Measures, and Bounds", "author": ["Serge Gaspers"], "venue": "VDM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Kernels for global constraints", "author": ["Serge Gaspers", "Stefan Szeider"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Backdoors to satisfaction", "author": ["Serge Gaspers", "Stefan Szeider"], "venue": "Essays Dedicated to Michael R. Fellows on the Occasion of His 60th Birthday,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Advanced preprocessing for answer set solving", "author": ["Martin Gebser", "Benjamin Kaufmann", "Andr\u00e9 Neumann", "Torsten Schaub"], "venue": "ECAI 2008 - 18th European Conference on Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "The stable model semantics for logic programming", "author": ["Michael Gelfond", "Vladimir Lifschitz"], "venue": "Logic Programming, Proceedings of the Fifth International Conference and Symposium,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1988}, {"title": "Satisfiability solvers. In Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence, pages 89\u2013134", "author": ["Carla P. Gomes", "Henry Kautz", "Ashish Sabharwal", "Bart Selman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Fixed-parameter complexity in AI and nonmonotonic reasoning", "author": ["Georg Gottlob", "Francesco Scarcello", "Martha Sideri"], "venue": "Artificial Intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Invitation to data reduction and problem kernelization", "author": ["Jiong Guo", "Rolf Niedermeier"], "venue": "ACM SIGACT News,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Treewidth: Computations and Approximations", "author": ["T. Kloks"], "venue": "Springer Verlag, Berlin", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "The decision problem for a class of first-order formulas in which all disjunctions are binary", "author": ["Melven R. Krom"], "venue": "Zeitschrift fu\u0308r Mathematische Logik und Grundlagen der Mathematik,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1967}, {"title": "Kernelization - preprocessing with a guarantee", "author": ["Daniel Lokshtanov", "Neeldhara Misra", "Saket Saurabh"], "venue": "The Multivariate Algorithmic Revolution and Beyond - Essays Dedicated to Michael R. Fellows on the Occasion of His 60th Birthday,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Stable models and an alternative logic programming paradigm. In The Logic Programming Paradigm: a 25-Year Perspective, pages 169\u2013181", "author": ["V. Wiktor Marek", "Miroslaw Truszczynski"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1999}, {"title": "Maximum matchings via Gaussian elimination", "author": ["Marcin Mucha", "Piotr Sankowski"], "venue": "In Proceedings of the 45th Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["Ilkka Niemel\u00e4"], "venue": "Ann. Math. Artif. Intell.,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1999}, {"title": "Detecting backdoor sets with respect to Horn and binary clauses", "author": ["Naomi Nishimura", "Prabhakar Ragde", "Stefan Szeider"], "venue": "In Proceedings of SAT 2004 (Seventh International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference. The Morgan Kaufmann Series in Representation and Reasoning", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1988}, {"title": "Bayesian networks. In The CogNet Library: References Collection", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "P", "author": ["C.-G. Quimper", "A. L\u00f3pez-Ortiz"], "venue": "van Beek, and A. Golynski. Improved algorithms for the global cardinality constraint. In M. Wallace, editor, Principles and Practice of Constraint Programming ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "The problem of simplifying truth functions", "author": ["W.V. Quine"], "venue": "Amer. Math. Monthly, 59:521\u2013531", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1952}, {"title": "Generalized arc consistency for global cardinality constraint", "author": ["J.-C. R\u00e9gin"], "venue": "14th National Conference on Artificial Intelligence ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1996}, {"title": "Table of races", "author": ["Frances Rosamond"], "venue": "http://fpt.wikidot.com/", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "P", "author": ["F. Rossi"], "venue": "van Beek, and T. Walsh, editors. Handbook of Constraint Programming. Elsevier", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "On the hardness of approximate reasoning", "author": ["Dan Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1996}, {"title": "Fixed-parameter tractability", "author": ["Marko Samer", "Stefan Szeider"], "venue": "Handbook of Satisfiability,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "Constraint satisfaction with bounded treewidth revisited", "author": ["Marko Samer", "Stefan Szeider"], "venue": "J. of Computer and System Sciences,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Limits of preprocessing", "author": ["Stefan Szeider"], "venue": "Proceedings of the Twenty-Fifth Conference on Artificial Intelligence,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2011}, {"title": "Multiplying matrices faster than Coppersmith-Winograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference (STOC", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2012}, {"title": "On the connections between backdoors, restarts, and heavy-tailedness in combinatorial search", "author": ["Ryan Williams", "Carla Gomes", "Bart Selman"], "venue": "In Informal Proc. of the Sixth International Conference on Theory and Applications of Satisfiability Testing, S. Margherita Ligure - Portofino,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": ", [27]), CSP solvers make use of various local consistency algorithms that filter the domains of variables (see, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 35, "context": ", [38, 13], respectively).", "startOffset": 2, "endOffset": 10}, {"referenceID": 11, "context": ", [38, 13], respectively).", "startOffset": 2, "endOffset": 10}, {"referenceID": 50, "context": "The history of preprocessing, like applying reduction rules to simplify truth functions, can be traced back to the 1950\u2019s [55].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "With the advent of parameterized complexity [25], a new theoretical framework became available that provides suitable tools to analyze the power of preprocessing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 33, "context": "Thus, for FPT problems, the combinatorial explosion can be Preliminary and shorter versions of this paper appeared in the proceedings of IJCAI 2011 [36] and AAAI 2011 [62].", "startOffset": 148, "endOffset": 152}, {"referenceID": 57, "context": "Thus, for FPT problems, the combinatorial explosion can be Preliminary and shorter versions of this paper appeared in the proceedings of IJCAI 2011 [36] and AAAI 2011 [62].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "It is known that a problem is fixed-parameter tractable if and only if every problem input can be reduced by polynomial-time preprocessing to an equivalent input whose size is bounded by a function of the parameter [24].", "startOffset": 215, "endOffset": 219}, {"referenceID": 39, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 39, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 42, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 52, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 26, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 39, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 42, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 9, "context": "Some NP-hard combinatorial problems such as k-Vertex Cover admit polynomially sized kernels, for others such as k-Path an exponential kernel is the best one can hope for [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Buss\u2019 kernelization algorithm for k-Vertex Cover (see [14]) computes the set U of vertices with degree at least k+1 in G.", "startOffset": 54, "endOffset": 58}, {"referenceID": 38, "context": "Constraint satisfaction problems (CSP) over a fixed universe of values, parameterized by the induced width [41].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "Consistency and generalized arc consistency for intractable global constraints, parameterized by the cardinalities of certain sets of values [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 46, "context": "Propositional satisfiability (SAT), parameterized by the size of backdoors [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets [52, 9].", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets [52, 9].", "startOffset": 117, "endOffset": 124}, {"referenceID": 38, "context": "Nonmonotonic reasoning with normal logic programs, parameterized by feedback width [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "As in [5], the parameter is the number of holes in the domains of the variables, measuring how close the domains are to being intervals.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "exponential time) preprocessed (\u201ccompiled\u201d), such that in a second phase various queries can be answered in polynomial time [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "Theorem 1 ([11, 34]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 31, "context": "Theorem 1 ([11, 34]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "Theorem 2 ([12]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "Constraint networks have proven successful in modeling everyday cognitive tasks such as vision, language comprehension, default reasoning, and abduction, as well as in applications such as scheduling, design, diagnosis, and temporal and spatial reasoning [21].", "startOffset": 255, "endOffset": 259}, {"referenceID": 20, "context": "The induced width of a constraint network is the treewidth of its constraint graph [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 90, "endOffset": 98}, {"referenceID": 56, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 40, "context": ", [43]), we can put the given width w tree decompositions T1, .", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "By Bodlaender\u2019s Theorem [10], the problem is fixed-parameter tractable.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "[11] showed that the related problem of testing whether a graph has treewidth at most w does not have a polynomial kernel (taking w as the parameter), unless a certain \u201cAND-conjecture\u201d fails.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In turn, Drucker [26] showed that a failure of the AND-conjecture implies NP \u2286 coNP/poly.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "The propositional satisfiability problem (SAT) was the first problem shown to be NP-hard [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 37, "context": "Despite its hardness, SAT solvers are increasingly leaving their mark as a general-purpose tool in areas as diverse as software and hardware verification, automatic test pattern generation, planning, scheduling, and even challenging problems from algebra [40].", "startOffset": 255, "endOffset": 259}, {"referenceID": 59, "context": "[65], provides a means for making the vague notion of a hidden structure explicit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] define a sub-solver to be an algorithm A that takes as input a CNF formula F and has the following properties:", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The smaller the backdoor B, the more useful it is for satisfiability solving, which makes the size of the backdoor a natural parameter to consider (see [37] for a survey on the parameterized complexity of backdoor problems).", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "We will devise polynomial parameter transformations from the following parameterized problem which is known to be compositional [34] and therefore unlikely to admit a polynomial kernel.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Sub-solvers for Horn and 2CNF follow from [23] and [44], respectively.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "Sub-solvers for Horn and 2CNF follow from [23] and [44], respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 46, "context": "We therefore consider the cases 3SAT(Horn-backdoor) and 3SAT(2CNF-backdoor) separately, these cases are important since the detection of Horn and 2CNF-backdoors is fixed-parameter tractable [50].", "startOffset": 190, "endOffset": 194}, {"referenceID": 46, "context": "\u22a3 We now turn to the recognition problem Rec-SAT(A-backdoor), in particular for A \u2208 {Horn, 2CNF} for which, as mentioned above, the problem is known to be fixed-parameter tractable [50].", "startOffset": 181, "endOffset": 185}, {"referenceID": 55, "context": "It is well-known and easy to see that the vertex covers of G(F ) are exactly the Horn-backdoor sets of F [60].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "Now, we apply the known kernelization algorithm for vertex covers [17] to (G(F ), k) and obtain in polynomial time an equivalent instance (G\u2032, k\u2032) where G\u2032 has at most 2k vertices.", "startOffset": 66, "endOffset": 70}, {"referenceID": 55, "context": "Again, it is well-known and easy to see that the hitting sets of H(F ) are exactly the 2CNF-backdoor sets of F [60].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Now we apply a known kernelization algorithm for the hitting set problem on 3-uniform hypergraphs (3HS) [1] to (H(F ), k) and obtain in polynomial time an equivalent instance (H \u2032, k\u2032) where H \u2032 has at most O(k) vertices.", "startOffset": 104, "endOffset": 107}, {"referenceID": 53, "context": "Constraint programming (CP) offers a powerful framework for efficient modeling and solving of a wide range of hard problems [58].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "Unfortunately, for several important global constraints a complete propagation is NPhard, and one switches therefore to incomplete propagation such as bound consistency [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "[5] showed that a complete propagation of several intractable constraints can efficiently be done as long as certain natural problem parameters are small, i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": ", the propagation is fixed-parameter tractable [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "[5] in their concluding remarks: whether intractable global constraints admit a reduction to a problem kernel or kernelization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This includes the 5 global constraints NValue, AtMost-NValue, Disjoint, Uses, and EGC defined above (see [8]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "[5] considered dx = |dom(X)| as parameter for NValue, dxy = |dom(X) \u2229 dom(Y )| as parameter for Disjoint, and dy = |dom(Y )| as parameter for Uses.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] also showed that polynomial time algorithms for enforcing bounds consistency imply that the corresponding consistency problem is fixed-parameter tractable parameterized by the number of holes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] can be improved to polynomial kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7], F can be encoded as a Disjoint constraint with X = {x1, .", "startOffset": 0, "endOffset": 3}, {"referenceID": 49, "context": "The Consistency problem for EGC constraints is NP-hard [54].", "startOffset": 55, "endOffset": 59}, {"referenceID": 51, "context": "However, if all sets dom(\u00b7) are intervals, then consistency can be checked in polynomial time using network flows [56].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "[5], the Consistency problem for EGC constraints is fixed-parameter tractable, parameterized by the number of holes in the sets dom(\u00b7).", "startOffset": 0, "endOffset": 3}, {"referenceID": 49, "context": "[54]: Given a CNF formula F on k variables, one can construct in polynomial time an EGC constraint CF such that 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Beldiceanu [3] and Bessi\u00e8re et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "[6] decompose NValue constraints into two other global constraints: AtMost-NValue and AtLeast-NValue, which require that at most N or at least N values are used for the variables in X , respectively.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A greedy algorithm by Beldiceanu [3] checks the consistency of an AtMost-NValue constraint in linear time when all domains are intervals (i.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "[5] have shown that Consistency (and Enforcing HAC) is fixed-parameter tractable, parameterized by the number of holes, for all constraints for which bound consistency can be enforced in polynomial time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A simple algorithm for checking the consistency of AtMost-NValue goes over all instances obtained from restricting the domain of each variable to one of its intervals, and executes the algorithm of [3] for each of these 2 instances.", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "As in [6], we determine the largest possible value for N if its domain were the set of all integers.", "startOffset": 6, "endOffset": 9}, {"referenceID": 44, "context": "This can be done in O((|X |+ |D|)) time [48, 64] by computing a maximum matching in the graph whose vertices are X \u222a D with an edge between x \u2208 X and v \u2208 D iff v \u2208 dom(x).", "startOffset": 40, "endOffset": 48}, {"referenceID": 58, "context": "This can be done in O((|X |+ |D|)) time [48, 64] by computing a maximum matching in the graph whose vertices are X \u222a D with an edge between x \u2208 X and v \u2208 D iff v \u2208 dom(x).", "startOffset": 40, "endOffset": 48}, {"referenceID": 4, "context": "The speed-ups due to this branching algorithm and the kernelization algorithm lead to a speed-up for enforcing HAC for AtMost-NValue constraints (by Corollary 2) and for enforcing HAC for NValue constraints (by the decomposition of [6]).", "startOffset": 232, "endOffset": 235}, {"referenceID": 48, "context": "Bayesian networks (BNs) have emerged as a general representation scheme for uncertain knowledge [53].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "The problem is NP-complete [19] and moves from NP to #P if we ask to compute Pr(v = true) [59].", "startOffset": 27, "endOffset": 31}, {"referenceID": 54, "context": "The problem is NP-complete [19] and moves from NP to #P if we ask to compute Pr(v = true) [59].", "startOffset": 90, "endOffset": 94}, {"referenceID": 47, "context": "e, if there is at most one undirected path between any two variables [52].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "The reduction is based on the reduction from 3SAT given by Cooper [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Hence we can apply a known kernelization algorithm for feedback vertex sets [16] to G and obtain a kernel (G\u2032, k) with at most O(k) many vertices.", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning [39, 46].", "startOffset": 114, "endOffset": 122}, {"referenceID": 43, "context": "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning [39, 46].", "startOffset": 114, "endOffset": 122}, {"referenceID": 38, "context": "[41] considered the following parameterization of the problem and showed fixed-parameter tractability (see [31] for generalizations).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[41] considered the following parameterization of the problem and showed fixed-parameter tractability (see [31] for generalizations).", "startOffset": 107, "endOffset": 111}, {"referenceID": 38, "context": "[41] cannot be strengthened towards a polynomial kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "We give a polynomial parameter transformation from SAT(vars) to SME(feedback width) using a construction of Niemel\u00e4 [49].", "startOffset": 116, "endOffset": 120}, {"referenceID": 45, "context": "The formula F is satisfiable if and only if P has a stable model [49].", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "[29] investigated the parameterized complexity and kernelization for various parameterizations of Abductive Reasoning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] for planning problems, parameterized by the length of the plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "In the area of optimization, parameterized problems are known that do not admit polynomial kernels but admit polynomial Turing kernels [30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": ", [20]), the initial sorting of the n + k intervals can be done in time O(n + |D| + k).", "startOffset": 2, "endOffset": 6}], "year": 2014, "abstractText": "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning under structural restrictions. All these problems involve two tasks: (i) identifying the structure in the input as required by the restriction, and (ii) using the identified structure to solve the reasoning task efficiently. We show that for most of the considered problems, task (i) admits a polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, in contrast to task (ii) which does not admit such a reduction to a problem kernel of polynomial size, subject to a complexity theoretic assumption. As a notable exception we show that the consistency problem for the AtMost-NValue constraint admits a polynomial kernel consisting of a quadratic number of variables and domain values. Our results provide a firm worst-case guarantees and theoretical boundaries for the performance of polynomial-time preprocessing algorithms for the considered problems.", "creator": "LaTeX with hyperref package"}}}