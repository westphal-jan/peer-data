{"id": "1406.3497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation Supplementary Material", "abstract": "This paper is about learning a continuous approach to the Pareto boundary in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy approach that uses gradient information to generate solutions close to the Pareto boundary. Unlike previous strategies with multi-objective gradient algorithms that used n optimization routines to have n solutions, our approach performs a single gradient ascent path that generates an improved continuous approach to the Pareto boundary at each step. The idea is to use a gradient-based approach to optimize the parameters of a function that defines diversity in the parameter space so that the corresponding image in the target space of the Pareto boundary comes as close as possible.", "histories": [["v1", "Fri, 13 Jun 2014 10:49:38 GMT  (398kb,D)", "http://arxiv.org/abs/1406.3497v1", null], ["v2", "Tue, 18 Nov 2014 21:31:32 GMT  (1262kb,D)", "http://arxiv.org/abs/1406.3497v2", "AAAI-15 Supplement. Updated upon acceptance at the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["matteo pirotta", "simone parisi", "marcello restelli"], "accepted": false, "id": "1406.3497"}, "pdf": {"name": "1406.3497.pdf", "metadata": {"source": "CRF", "title": "Multi\u2013objective Reinforcement Learning with Continuous Pareto Frontier Approximation", "authors": ["Matteo Pirotta", "Simone Parisi", "Marcello Restelli", "Leonardo Da Vinci"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many real-world control problems (e.g., economic systems, water resource problems, robotic systems, just to mention a few) are characterized by the presence of multiple, conflicting objectives. Such problems are often modeled as Multi-Objective Markov Decision Processes (MOMDPs), where the concept of optimality typical of MDPs is replaced by the one of Pareto optimality, i.e., a set of policies providing a compromise among the different objectives. In the last decades, Reinforcement Learning (RL) [1] has established as an effective and theoretically-grounded framework that allows to solve single-objective MDPs whenever either no (or little) prior knowledge is available about system dynamics, or the dimensionality of the system to be controlled is too high for classical optimal control methods. Despite the successful developments in RL theory and a high demand for multi-objective control applications, Multi-Objective Reinforcement Learning (MORL) [2, 3] is still a relatively young and unexplored research topic.\nMORL approaches can be divided into two main categories, based on the number of policies they learn [4]: single\u2013policy and multiple policy. Although, the majority of MORL approaches belong to former category, in this paper, we focus on latter approaches. Multiple-policy approaches aim at learning a set of policies in order to approximate the Pareto frontier. When the number d of decision variables (i.e., policy parameters) is greater than or equal to the number q of objectives, the local Pareto\u2013optimal solutions form a (q \u2212 1)\u2013dimensional manifold [5]. The superiority of multiple\u2013policy methods resides in the ability to represent the Pareto\u2013optimal manifold, allowing a posteriori selection of the solution, a graphical representation of the frontier that can give better insight into the relationships among the objectives, and encapsulate all the trade-offs among the multiple objectives. Building the exact frontier is generally impractical in real-world problems, the goal is thus to compute an approximation of the Pareto frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the one of the actual front [6]. Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10]. While value\u2013based approaches suffer from the course\n\u2217matteo.pirotta@polimi.it \u2020simone.parisi@mail.polimi.it \u2021marcello.restelli@polimi.it\nar X\niv :1\n40 6.\n34 97\nv1 [\ncs .A\nI] 1\n3 Ju\nn 20\n14\nof dimensionality problem and, in general, are not able to identify concave frontiers, gradient\u2013based techniques lack of guarantees of uniform covering of the Pareto frontier.\nIn this paper, we propose a novel gradient\u2013based MORL approach that generates a continuous approximation of the local Pareto\u2013optimal solution manifold in the policy space. Exploiting this approximation it is possible to generate an arbitrarily dense representation of the Pareto frontier. The main contributions of this paper are: the derivation of the gradient approach in the general case \u2013i.e., independent from the metric used to measure the quality of the current solution\u2013 (Section 3), how to estimate such gradient from sample trajectories (Section 4), a discussion of frontier quality measures that can be effectively integrated in the proposed gradient approach (Section 5), and an empirical evaluation of its performance in a multi\u2013objective extension of the discrete-time Linear-Quadratic Gaussian regulator and in a water reservoir management domain."}, {"heading": "2 Preliminaries", "text": "Multi-objective Markov Decision Processes (MOMDPs) are an extension of the MDP model, where several pairs of reward functions and discount factors are defined, one for each objective. Formally, a MOMDP is described by a tuple \u3008S,A,P,R,\u03b3, D\u3009, where S \u2286 Rn is the continuous state space, A \u2286 Rm is the continuous action space, P is a Markovian transition model where P(s\u2032|s, a) defines the transition density between state s and s\u2032 under action a, R = [R1, . . . ,Rq]T and \u03b3 = [\u03b31, . . . , \u03b3q]T are q-dimensional column vectors of reward functions Ri : S \u00d7 A\u00d7 S \u2192 R and discount factors \u03b3i \u2208 [0, 1), respectively, and D is a distribution from which the initial state is drawn. In MOMDPs, any policy \u03c0 is associated to q expected returns J\u03c0 = [ J\u03c01 , . . . , J \u03c0 q ] , where\nJ\u03c0i = E { T\u2211 t=0 \u03b3tiri(t+ 1)|x0 \u223c D,\u03c0 } ,\nbeing ri(t+ 1) = Ri(st, at, st+1) the i-th immediate reward obtained when state st+1 is reached from state st and action at, and T the finite or infinite horizon.\nIn policy\u2013gradient approaches, a parameterized space of policies \u03a0\u03b8 = { \u03c0\u03b8 : \u03b8 \u2208 \u0398 \u2286 Rd } (where \u03c0\u03b8 is a compact notation for \u03c0(a|s,\u03b8)) is considered. Given a policy parametrization \u03b8, we assume the policy performance J : \u0398\u2192 F to be at least C2. J is defined as the expected reward over the space of all possible trajectories T: J (\u03b8) = \u222b T p (\u03c4 |\u03b8) r(\u03c4)d\u03c4, where \u03c4 \u2208 T is a trajectory drawn from density distribution p(\u03c4 |\u03b8) with reward vector r(\u03c4) that represents the accumulated expected discounted reward over trajectory \u03c4 : ri(\u03c4) = \u2211T t=0 \u03b3 t iri(t+ 1).\nIn MOMDPs for each policy parameter \u03b8, q gradient directions are defined [11]\n\u2207\u03b8Ji(\u03b8) = \u222b T \u2207\u03b8p (\u03c4 |\u03b8) ri(\u03c4)d\u03c4 = E\u03c4\u2208T [ \u2207\u03b8 log p (\u03c4 |\u03b8) ri(\u03c4) ] = E\u03c4\u2208T [ ri(\u03c4) T\u2211 t=1 \u2207\u03b8 log \u03c0 (a\u03c4t |s\u03c4t ,\u03b8) ] ,\nwhere each direction \u2207\u03b8Ji is associated to a particular discount factor\u2013reward function pair < \u03b3i,Ri >. As shown in previous equation, the differentiability of the performance measure is connected to the differentiability of the policy class by: \u2207\u03b8 log p (\u03c4 |\u03b8) = \u2211T k=1\u2207\u03b8 log \u03c0(ak|sk,\u03b8).\nDespite what happens in MDPs, in MOMDPs a single policy which dominates all the others usually does not exist; in fact, when conflicting objectives are considered, no policy can simultaneously maximize all the objectives. For these reasons, in Multi-Objective Optimization (MOO) a different dominance concept is used. Policy \u03c0 dominates policy \u03c0\u2032, which is denoted by \u03c0 \u03c0\u2032, if:\n\u2200i \u2208 {1, . . . , q} , J\u03c0i \u2265 J\u03c0 \u2032 i \u2227 \u2203i \u2208 {1, . . . , q} , J\u03c0i > J\u03c0 \u2032 i .\nIf there is no policy \u03c0\u2032 such that \u03c0\u2032 \u03c0, the policy \u03c0 is Pareto\u2013optimal. In general, there are multiple Pareto-optimal policies. Solving a MOMDP is equivalent to determine the set of Pareto-optimal policies\n\u03a0\u2217 = { \u03c0 | @\u03c0\u2032, \u03c0\u2032 \u03c0 } , which maps to the so-called Pareto frontier J \u2217 = { J\u03c0 \u2217 |\u03c0\u2217 \u2208 \u03a0\u2217 } .1\nA remark on notation. In the following we will use the symbol DXF to denote the derivative 2 of a generic function F : Rm\u00d7n \u2192 Rp\u00d7q w.r.t. matrix X. Notice that the following relationship holds for scalar functions of vector variable: \u2207xf = (Dxf)T. Finally, the symbol Ix will be used to denote an x\u00d7 x identity matrix.\n1As done in [12], we suppose that local Pareto-optimal solutions that are not Pareto-optimal do not exist. 2The derivative operator is well defined for matrices, vectors and scalar functions. Refer to [13] for details."}, {"heading": "3 Gradient on Policy Manifold for Continuous Pareto Front", "text": "Approximation\nIn this section we first provide a general definition of the optimization problem that we want to solve and then we explain how we can solve it in the MOMDP case using a gradient\u2013based approach."}, {"heading": "3.1 Parametric Pareto Front in MOO", "text": "It has been shown [5] that local Pareto\u2013optimal solutions locally form a (q \u2212 1)\u2013dimensional manifold, assuming d > q. It follows that in two-objective problems, the Pareto\u2013optimal solutions can be described by curves both in decision and objective spaces. The idea behind this work is to parametrize the local Pareto\u2013optimal solution curve in the objective space, in order to produce a continuous representation of the Pareto frontier.\nLet T be open in Rb with b \u2264 q. The high\u2013dimensional analogous of a parameterized curve is a smooth map \u03c8\u03c1 : T \u2192 Rq of class Cl(l \u2265 1), where t \u2208 T and \u03c1 \u2208 P \u2286 Rk are the free variable and the parameters, respectively. The set F = \u03c8\u03c1(T ), together with the map \u03c8\u03c1 constitute a parametrized manifold of dimension b, denoted by F\u03c1(T ) [14]. This manifold represents our approximation of the true Pareto frontier. The goal is to find the best approximation, i.e., to the parameters \u03c1 that minimize the distance from the real frontier \u03c1\u2217 = arg min\u03c1\u2208P I\u2217 (F\u03c1 (T )) , where I\u2217 : Rq \u2192 R is some loss function that measures the discrepancy between the Pareto\u2013optimal frontier and F\u03c1 (T ). However, since the Pareto frontier is not known, a different indicator function is needed. The definition of such metric is an open problem in literature. Recently [4], several metrics have been defined, but every candidate presents some intrinsic limits that prevent the definition of a unique superior metric. Furthermore, as we will see in the rest of the paper, the proposed approach needs a metric differentiable w.r.t. policy parameters. We will come back to this topic in Section 5.\nIn general, MOO algorithms compute the value of the frontier as sum of the value of the points composing the discrete approximation. In our scenario, where a continuous frontier approximation is available, it maps to an integration on the Pareto manifold\nJ (\u03c1) = \u222b F(T ) IV dV,\nwhere dV is a symbol used to denote the integral w.r.t. the volume of the manifold [14] and I : F (T )\u2192 R is a continuous indicator function that for each point of F (T ) measures its Pareto\u2013optimality. A standard way to maximize previous equation is to perform gradient ascent, updating the parameters according to the gradient direction: \u03c1t+1 = \u03c1t + \u03b1t \u2207\u03c1J (\u03c1) ."}, {"heading": "3.2 Parametric Pareto Front in MOMDP", "text": "Given a continuous indicator function I, the integral of I over F (T ), with respect to volume, is defined by the equation [14] \u222b\nF(T ) IV dV = \u222b T (I \u25e6 \u03c8\u03c1)V ol (Dt\u03c8\u03c1(t)) dt,\nprovided this integral exists and V ol (X) = [ det ( XT \u00b7X )] 1 2 .\nNow, the key point is the definition of the map \u03c8\u03c1. Notice that the direct map between the parameter space T and the objective space is unknown, but can be easily defined through a reparametrization that involves the policy space \u0398, as shown in Figure 1. In previous section we have mention that there is a tight relationship between the (local) manifold in the objective space and the (local) manifold in the variable space. This mapping is well known and it is defined by the performance function J(\u03b8), that defines the goodness a policy \u03c0\u03b8 w.r.t. the objectives. This means that, given a set of policy parametrizations, we can define the associated points in the objective space. As a consequence, the optimization problem can be reformulated as the search for the best approximation of the Pareto manifold in the policy paramater space, i.e., to the search of the manifold in the policy parameter space that describe the optimal Pareto frontier.\nFormally, let \u03c6\u03c1 : T \u2192 \u0398 be a smooth map of class Cl(l \u2265 1) defined on the same domain of \u03c8\u03c1. We think of the map \u03c6\u03c1 as a parameterization of the subset \u03c6\u03c1(T ) of \u0398: each choice of a point t \u2208 T gives rise to a point \u03c6\u03c1(t) in \u03c6\u03c1(T ). This means that only a subset \u0398\u03c1(T ) of the space \u0398 can be spanned by map \u03c6\u03c1, i.e., \u0398\u03c1(T ) is a b\u2013dimensional parametrized manifold in the policy space\n\u0398\u03c1(T ) = {\u03b8 : \u03b8 = \u03c6\u03c1(t), \u2200t \u2208 T } ,\nand, as a consequence, the associated parametrized Pareto frontier is the open set defined as\nF (\u0398\u03c1(T )) = {J (\u03b8) : \u03b8 \u2208 \u0398\u03c1(T )} .\nLemma 3.1. Let T be an open set in Rb, let F\u03c1 (T ) be a manifold parametrized by a smooth map \u03c8\u03c1 : T \u2192 Rq. Provided that the map \u03c8\u03c1 can be expressed as composition of maps J and \u03c6\u03c1, i.e., \u03c8\u03c1 = J\u25e6\u03c6\u03c1, and that I is a continuous function defined at each point of F\u03c1(T ), the integral w.r.t. the volume is given by\nJ(\u03c1) = \u222b F(T ) IV dV = \u222b T (I \u25e6 (J \u25e6 \u03c6\u03c1))V ol (D\u03b8J(\u03b8)Dt\u03c6\u03c1(t)) dt.\nThe associated gradient w.r.t. the map parameters \u03c1 is given component\u2013wise by\n\u2202J (\u03c1)\n\u2202\u03c1i = \u222b T \u2202 \u2202\u03c1i (I \u25e6 (J \u25e6 \u03c6\u03c1))V ol (T) dt\n+ \u222b T (I \u25e6 (J \u25e6 \u03c6\u03c1))V ol (T) ( vec ( TTT )\u2212T)T Nb ( Ib \u2297TT ) D\u03c1iTdt\nwhere T = D\u03b8J(\u03b8)Dt\u03c6\u03c1(t), \u2297 is the Kronecker product, Nb = 12 (Ib2 +Kbb) is a symmetric (b 2 \u00d7 b2) idempotent matrix with rank 1 2 b(b+ 1) and Kbb is a permutation matrix [13].\nAs the reader may have noticed, we have left the term D\u03c1iT unexpanded. This term represents the rate of expansion/compression of an infinitesimal volume block of the manifold under reparametrization. The derivation of this quantity is not trivial and requires a special focus. Exploiting algebraic tools, we can write\nD\u03c1iT = ( Dt\u03c6\u03c1(t) T \u2297 Iq ) D\u03b8 (D J(\u03b8))D\u03c1i \u03c6\u03c1(t) + (Ib \u2297D\u03b8J(\u03b8))D\u03c1i (Dt\u03c6\u03c1(t))\nwhere D\u03b8 (D J(\u03b8)) is a transformation of the Hessian matrix of the performance w.r.t. policy parameters, that is, it contains the same elements but in different order. In fact, the Hessian matrix is defined as the derivative of the transpose Jacobian, that is, H J(\u03b8) = D\u03b8(D J(\u03b8))\nT. However, the two matrices contain the same elements, but in different order. The following equation relates the Hessian matrix to D\u03b8 (D J(\u03b8)):\nHm,n\u03b8 Ji = D 2 n,mJi(\u03b8) =\n\u2202\n\u2202\u03b8n ( \u2202Ji \u2202\u03b8m ) = Dp,n\u03b8 (D J(\u03b8))\nwhere p = i+ q(m\u2212 1), where q is the number of rows of the Jacobian matrix. Up to now, little research has been done on second order methods and in particular on Hessian formulation. A first analysis was performed in [15] where the authors provided a formulation based on the policy gradient theorem [16]. However, we provide a different derivation of the Hessian coming from the trajectory\u2013based definition of the expected discounted reward for episodic MDPs.\nLemma 3.2. For any MOMDP, the Hessian H J(\u03b8) of the expected discounted reward J w.r.t. the policy parameters \u03b8 is a qd\u00d7 d matrix obtained by stacking the Hessian of each component\nH J(\u03b8) = \u2202\n\u2202\u03b8T vec\n( \u2202Ji(\u03b8)\n\u2202\u03b8T\n)T =  H J1(\u03b8)\n... H Jq(\u03b8)\n ,\nwhere\nH Ji(\u03b8) = \u222b T p (\u03c4 |\u03b8) ri(\u03c4) [ \u2207\u03b8 log p (\u03c4 |\u03b8)\u2207\u03b8 log p (\u03c4 |\u03b8)T +D\u03b8 (\u2207\u03b8 log p (\u03c4 |\u03b8)) ] d\u03c4. (1)"}, {"heading": "4 Gradient Estimation from Sample Trajectories", "text": "In the RL setting, having no prior knowledge about the reward function and the state transition model, we need to estimate the gradient \u2207\u03c1J(\u03c1) from trajectory samples. In this section we present standard results related to the estimation approaches used in RL literature and we provide a theoretical analysis of the Hessian estimate.\nThe formulation of the gradient \u2207\u03c1J(\u03c1) provided in Lemma 3.1 is composed by terms related to the parameterization of the manifold in the policy space and terms related to the MDP. Since the map \u03c6\u03c1 is free to be designed, the associated terms (e.g., Dt\u03c6\u03c1(t)) can be computed exactly. On the other hand, the term related to the MDP (J\u03b8, D\u03b8J(\u03b8) and H J(\u03b8)) need to be estimated. While the estimate of the expected discounted reward and the associated gradient is an old topic in RL literature and several results have been proposed [15, 17], the estimate of the Hessian is not addressed in literature. Recently, the simultaneous perturbation stochastic approximation technique was exploited to estimate the Hessian [18]. Here we provide a Hessian estimate from trajectory samples obtained through the current policy, removing the necessity of generating policy perturbations.\nSuppose to have access to a set of N trajectory, since p (\u03c4 |\u03b8) is unknown, the expectation is approximated by the empirical average:\nH\u0302 Ji(\u03b8) = 1\nN N\u2211 n=1 ( T\u2211 l=1 \u03b3l\u22121i r n l,i )[ T\u2211 k=1 \u2207\u03b8 log \u03c0\u03b8an k ,sn k ( T\u2211 k=1 \u2207\u03b8 log \u03c0\u03b8an k ,sn k )T + T\u2211 k=1 H log \u03c0\u03b8an k ,sn k ] (2)\nwhere { snk , a n k , r n \u00b7,k }T k=1\ndenotes the n-th trajectory. This formulation resemble the definition of REINFORCE estimate given in [19] for the gradient \u2207\u03b8J(\u03b8). Such types of estimate, known as likelihood ratio methods, overcome the problem of finite\u2013difference methods, that is, the problem of control the perturbation of the parameters.\nIn order to simplify the theoretical analysis we make the following assumptions.\nAssumption 4.1 (Uniform boundedness). The reward function, the log\u2013Jacobian and the log\u2013Hessian of the policy are uniformly bounded: \u2200i = 1, . . . , q, \u2200m = 1, . . . , d, \u2200n = 1, . . . , d, (s, a, s\u2032) \u2208 S \u00d7A\u00d7S \u03b8 \u2208 \u0398\u2223\u2223\u2223Ri(s, a, s\u2032)\u2223\u2223\u2223 \u2264 Ri, \u2223\u2223\u2223D(m)\u03b8 log \u03c0(a|s,\u03b8)\u2223\u2223\u2223 \u2264 D, \u2223\u2223\u2223H(mn)\u03b8 log \u03c0(a|s,\u03b8)\u2223\u2223\u2223 \u2264 G. Lemma 4.2. Given a parametrized policy \u03c0(a|s,\u03b8), under the assumption Assumption 4.1, the i\u2013th component of the log\u2013Hessian of the expected return can be bounded by\n\u2016HJi(\u03b8)\u2016max \u2264 RiT\u03b3\nT\n1\u2212 \u03b3\n( TD 2 +G ) .\nNote that the max norm of a matrix is defined as \u2016A\u2016max = maxi,j {aij}. Previous result can be used to derive a bound on the sample complexity of the Hessian estimate.\nTheorem 4.3. Given a parametrized policy \u03c0(a|s,\u03b8), under Assumption 4.1, using the following number of H\u2013step trajectories\nN = 1\n2 2i\n( RiT\u03b3 T\n(1\u2212 \u03b3)\n( TD 2 +G ))2 log 2\n\u03b4\nthe gradient estimate H\u0302Ji(\u03b8) generated by Equation (2) is such that with probability 1\u2212 \u03b4:\u2225\u2225\u2225H\u0302Ji(\u03b8)\u2212HJi(\u03b8)\u2225\u2225\u2225 max \u2264 i.\nFinally, the estimate of the integral can be computed using standard Monte\u2013Carlo techniques. Several statistical bounds have been proposed in literature, we refer to [20] for a survey on Monte\u2013Carlo methods."}, {"heading": "5 Metrics for Multi\u2013objective Optimization", "text": "In this section we review some indicator functions proposed in literature underlying advantages and drawbacks and we propose some alternatives.\nRecently, MOO has focused on the use of performance indicators to turn a multi\u2013objective optimization problem into a single-objective one by optimizing the indicator itself. The indicator function is used to assign to every point a single\u2013objective measure, or, in other words, to give an approximate measure of the discrepancy between the candidate frontier and the Pareto one. Since, instead of optimizing the objective functions directly, indicator\u2013based algorithms aim at finding a solution set that maximizes the indicator metric, a natural question arises about the correctness of this change in the optimization procedure and on the properties the indicator functions enjoy.\nFor instance, hypervolume indicator and its weighted version are among the most widespread metrics in literature. These metrics have gained popularity because they are refinements of the Pareto dominance relation [21]. Recently, several works have been proposed in order to theoretically investigate the properties of hypervolume indicator [22]. Nevertheless, it has been argued that the hypervolume indicator may introduce a bias in the search. From our perspective, the main issue of this metric is the high computational complexity3 and, above all, the non differentiability. Several other metrics have been defined in the field of MOO, we refer to [23] for an extensive survey. However, MOO literature has not been able to provide a superior metric and among the candidates no one is suited for this scenario. Again the main problems are the non differentiability and the capability of evaluate only discrete representations of the Pareto frontier.\nIn order to overcome these issues we have try to mix different indicator concepts in order to obtain a metric with the desired properties. The insights that have guided or metric definition are related to the MOO desiderata. Recall that the goal of MOO is to compute an approximation of the frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the actual one [6]. Note that the uniformity of the frontier is intrinsically guaranteed by the continuity of the approximation we have introduced. Having in mind these concepts we need to impose accuracy and extension of the frontier through the indicator function. Given a reference point p, a simple indicator can be obtained by computing the distance between every point of the frontier F and the reference point\nI1(J,p) = \u2016J\u2212 p\u201622 .\nAs shown in the hypervolume indicator, the choice of the reference point may be critical. However, a natural choice is the utopia (ideal) point (pu), i.e., the point that optimizes all the objective functions. In this case the goal is the minimization of such indicator function. Since any dominated policy is farther from the utopia than at least one Pareto optimal solution, the accuracy can be easily guaranteed. On the other hand, it is also easy to show that this measure forces the solution to collapse into a single point. If the extension of the frontier is the primary concern, maximizing the distance from the antiutopia (pau) results in a metric that grows with the frontier dimension. However, since we are trying to maximize a possibly unbounded function that is not related to the Pareto optimality, this measure does not provide any guarantees about accuracy.\nConcerning the accuracy of the frontier, from a theoretical perspective, it is possible to define a metric based on the Pareto optimality. A point \u03b8 is Pareto optimal when\nl(\u03b8,\u03b1) = q\u2211 i=1 \u03b1i\u2207\u03b8Ji(\u03b8) = 0, q\u2211 i=1 \u03b1i = 1, \u03b1 \u2208 Rq+,\nthis means that it is not possible to identify an ascent direction that simultaneously improves all the objectives. As a consequence, any point on the frontier minimizes the norm of direction l. Formally, a metric that respect the Pareto\u2013optimality can be defined as follows\nI2(J) = min \u03b1 \u2016l(\u03b8,\u03b1)\u201622 ,\n\u2211 i \u03b1i = 1,\u03b1 \u2208 Rq+.\nAs for the utopia\u2013based metric, the extent of the frontier is not taken into account. To summarize, all the mentioned indicators provide only one of the desiderata, but we deserve more since achieving only one property may result in a frontier arbitrary far from the actual one. In order to consider both the desiderata we have decided to mix previous concepts into a single indicator\nI3(J) = I1(J,pau) \u00b7 w(J) 3The computation of the hypervolume indicator is a #P\u2013hard problem. [22]\nwhere w(J) is a penalization term, i.e., it is a monotonic function that decreases as I2(J) increases, e.g., w(J) = 1\u2212\u03bbL2(J). Metric I3 takes advantage of the expansive behavior of the antiutopia\u2013based indicator and the accuracy of the optimality\u2013based indicator I2. In this way all the desiderata can be met by a single scalar measure, that is also Cl (l \u2265 1) differentiable."}, {"heading": "6 Experiments", "text": "In this section, results related to the numerical simulations of the proposed algorithms, in continuous and discrete domains, are presented. In particular, the performance are compared against some existing algorithms [8, 10]. In all the experiments the learning rate \u03b1 was set by hand\u2013tuning.\nWe start considering a multi\u2013objective version of the standard discrete-time Linear-Quadratic Gaussian regulator (LQG) with multidimensional and continuous state and action spaces [11]. For a complete description of the LQG problem and for the settings, we refer to [10]. This scenario is particular instructive since all the terms can be computed exactly, we focus on showing the properties of the parametrization and we demand the analysis of the estimate phase to the water reservoir domain. Initially we present the results for a 2\u2013dimensional LQG problem. The LQG is a problematic domain since it is defined only for control actions in the range [\u22121, 0], controls outside this range leads to divergence of the system. Our primary concern was related to the boundness of the control actions, leading to the following parametrization of the manifold in the policy space: \u03c61\u03c1(t) = \u2212 1/1 + exp(\u03c11 + \u03c12t) and \u03c6 2 \u03c1(t) = \u2212 1/1 + exp(\u03c13 + \u03c14t) with t \u2208 [0, 1]. While metrics I1 and I2 suffer from the problems described in Section 5 that prevent the algorithm to obtain a correct approximation of the Pareto frontier, mixed metric (I3) is able to achieve both accuracy and covering. An example of the learning process obtained setting \u03bb to 2.5 and starting from \u03c1(0) = [1, 2, 0, 3]T is shown in Figure 2(a). Notice that first accuracy is obtained by pushing the parametrization onto the Pareto frontier, then the frontier is expanded toward the extrema in order to attain covering.\nAn alternative approach consists in the computation of the optimal parametrizations of the single objectives, for instance through policy gradient techniques, exploiting such information for constraining the policy manifold to pass through these points. Recall that, in general, this information is required to compute the utopia and antiutopia points. Following such approach, two improvements can be easily obtained. First, the number of free parameters decreases and, as a consequence, the learning process simplifies. Second, the approximate frontier is forced to have a sufficiently large area to cover all the extrema. In this way, the problem of covering shown by indicators I1 and I2 can be alleviate or, in some cases, completely eliminated. For instance, forcing the parametrization to cover the extrema, has permitted to achieve both accuracy and covering using metric I1 (utopia) and I2 in the 2\u2013dimensional LQG problem. Figure 2(b) shows the learning process obtained through metric I2 under these settings. Clearly, no advantages have been found using antiutopia\u2013based metric. Although, this approach is effective for almost all 2\u2013objective problems, it does not generalize to higher dimensions as shown in the supplementary material for a 3\u2013dimensional LQG.\nConsider the 3\u2013dimensional LQG domain described in the supplementary material. Despite the parametrization was forced through the single objective optimums, the solution obtained with the utopia\u2013 based metric tends to concentrate on the centre of the frontier, i.e., toward the points that minimize the distance from the utopia. It is important to underline that all the obtained solutions belong to the Pareto frontier, i.e., no dominant solutions are found. The same happens with indicator function I2. Mixing the antiutopia with the Pareto optimality, i.e., using indicator function I3, provides a way to obtain both accuracy and covering. Figure 2(c) compares the true Pareto frontier with the parametric approximation obtained using I3 with \u03bb = 135.\nConcerning the approximate framework, we consider the water reservoir problem, a continuous MOMDP that, differently from the LQG, do not admit a closed\u2013form solution. In order to compare our frontier with the one obtained by other algorithms, we consider the domain, settings and policy parametrization as described in [10]. A simple second\u2013order polynomial in t \u2208 [0, 1] with 5 parameters has been used to parametrize the policy manifold. The parameters drop to 5 since we have constrained the policy manifold to pass through the optimal points. The reader may refer to the supplementary material for details. In order to show the capability of the approximate algorithm we have decided to test the simplest metric, that is, the utopia\u2013based indicator. The integral estimate was performed using a Monte\u2013Carlo algorithm fed with only 100 random points. For each instance of variable t, 100 trajectory by 100 steps were used to estimate the gradient and Hessian of the policy performance. We start the learning from an arbitrary parametrization with all the parameters \u03c1i set to \u221220. Figure 2(d) reports the final frontier obtained with different algorithms. The approximation obtained by our algorithm is\ncomparable to the other results, however, our approach is able to produce a continuous frontier approximation. As shown in Figure 2(e), despite the low number of exploited samples, the algorithm presents a almost monotonic trend during the learning process."}, {"heading": "7 Conclusions", "text": "In this paper we have proposed a novel gradient\u2013based approach to learn a continuous approximation of the Pareto frontier in MOMDPs. The idea is to define a parametric function \u03c6\u03c1 that describes a manifold in the policy\u2013parameter space, that maps to a manifold in the objective space. Given a metric that measures the quality of the manifold in the objective space (i.e., the candidate frontier), we have shown how to compute (and estimate from trajectory samples) its gradient w.r.t. the parameters of \u03c6\u03c1. Updating the parameters along the gradient direction generates a new policy manifold associated to an improved (w.r.t. the chosen metric), continuous frontier in the objective space. Although we have provided a derivation that is independent from the specific metric used to measure the quality of the candidate solutions, the choice of such metric strongly influences the final result. We have presented different alternatives, discussed pros and cons of each one, and shown their properties through an empirical analysis.\nFuture research will further address the study of metrics that can produce good results in general settings. Another interesting research direction consists in using importance sampling techniques for reducing the sample complexity in the gradient estimate. Since the frontier is composed of a continuum of policies, it is likely that a trajectory generated by a specific policy can be partially used also for the estimation of quantities related to similar policies, thus decreasing the number of samples need for the Monte Carlo estimate of the integral."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Multiobjective reinforcement learning: A comprehensive overview", "author": ["C. Liu", "X. Xu", "D. Hu"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A survey of multiobjective sequential decision-making", "author": ["Diederik M. Roijers", "Peter Vamplew", "Shimon Whiteson", "Richard Dazeley"], "venue": "JAIR, 48:67\u2013113,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["Peter Vamplew", "Richard Dazeley", "Adam Berry", "Rustam Issabekov", "Evan Dekker"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Uniform sampling of local paretooptimal solution curves by pareto path following and its applications in multi-objective ga", "author": ["Ken Harada", "Jun Sakuma", "Shigenobu Kobayashi", "Isao Ono"], "venue": "In Proceedings of GECCO", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Performance assessment of multiobjective optimizers: an analysis and review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V.G. da Fonseca"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Linear fitted-q iteration with multiple reward functions", "author": ["Daniel J. Lizotte", "Michael Bowling", "Susan A. Murphy"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A multiobjective reinforcement learning approach to water resources systems operation: Pareto frontier approximation in a single run", "author": ["A Castelletti", "F Pianosi", "M Restelli"], "venue": "Water Resources Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Importance Sampling for Reinforcement Learning with Multiple Objectives", "author": ["Christian Robert Shelton"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Policy gradient approaches for multi-objective sequential decision making", "author": ["Simone Parisi", "Matteo Pirotta", "Nicola Smacchia", "Luca Bascetta", "Marcello Restelli"], "venue": "IJCNN", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Local search for multiobjective function optimization: pareto descent method", "author": ["Ken Harada", "Jun Sakuma", "Shigenobu Kobayashi"], "venue": "In GECCO,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics", "author": ["J.R. Magnus", "H. Neudecker"], "venue": "Wiley Ser. Probab. Statist.: Texts and References Section", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Analysis On Manifolds", "author": ["J.R. Munkres"], "venue": "Adv. Books Classics Series. Westview Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Optimizing average reward using discounted rewards", "author": ["Sham Kakade"], "venue": "In COLT/EuroCOLT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Adaptive step-size for policy gradient methods", "author": ["Matteo Pirotta", "Marcello Restelli", "Luca Bascetta"], "venue": "In NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Simultaneous perturbation algorithms for batch off-policy", "author": ["Raphael Fonteneau", "Prashanth L. A"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "Monte Carlo statistical methods, volume 319", "author": ["Christian P Robert", "George Casella"], "venue": "New York: Springer,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "On set-based multiobjective optimization", "author": ["Eckart Zitzler", "Lothar Thiele", "Johannes Bader"], "venue": "Trans. Evol. Comp,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Multiplicative approximations and the hypervolume indicator", "author": ["Tobias Friedrich", "Christian Horoba", "Frank Neumann"], "venue": "In GECCO", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A critical survey of performance indices for multi-objective optimisation", "author": ["T. Okabe", "Y. Jin", "B. Sendhoff"], "venue": "In CEC \u201903.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "In the last decades, Reinforcement Learning (RL) [1] has established as an effective and theoretically-grounded framework that allows to solve single-objective MDPs whenever either no (or little) prior knowledge is available about system dynamics, or the dimensionality of the system to be controlled is too high for classical optimal control methods.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Despite the successful developments in RL theory and a high demand for multi-objective control applications, Multi-Objective Reinforcement Learning (MORL) [2, 3] is still a relatively young and unexplored research topic.", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "Despite the successful developments in RL theory and a high demand for multi-objective control applications, Multi-Objective Reinforcement Learning (MORL) [2, 3] is still a relatively young and unexplored research topic.", "startOffset": 155, "endOffset": 161}, {"referenceID": 3, "context": "MORL approaches can be divided into two main categories, based on the number of policies they learn [4]: single\u2013policy and multiple policy.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": ", policy parameters) is greater than or equal to the number q of objectives, the local Pareto\u2013optimal solutions form a (q \u2212 1)\u2013dimensional manifold [5].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Building the exact frontier is generally impractical in real-world problems, the goal is thus to compute an approximation of the Pareto frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the one of the actual front [6].", "startOffset": 267, "endOffset": 270}, {"referenceID": 6, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 85, "endOffset": 91}, {"referenceID": 7, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 85, "endOffset": 91}, {"referenceID": 8, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 116, "endOffset": 123}, {"referenceID": 10, "context": "In MOMDPs for each policy parameter \u03b8, q gradient directions are defined [11]", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "1As done in [12], we suppose that local Pareto-optimal solutions that are not Pareto-optimal do not exist.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Refer to [13] for details.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "1 Parametric Pareto Front in MOO It has been shown [5] that local Pareto\u2013optimal solutions locally form a (q \u2212 1)\u2013dimensional manifold, assuming d > q.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "The set F = \u03c8\u03c1(T ), together with the map \u03c8\u03c1 constitute a parametrized manifold of dimension b, denoted by F\u03c1(T ) [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "Recently [4], several metrics have been defined, but every candidate presents some intrinsic limits that prevent the definition of a unique superior metric.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "the volume of the manifold [14] and I : F (T )\u2192 R is a continuous indicator function that for each point of F (T ) measures its Pareto\u2013optimality.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "2 Parametric Pareto Front in MOMDP Given a continuous indicator function I, the integral of I over F (T ), with respect to volume, is defined by the equation [14] \u222b", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "where T = D\u03b8J(\u03b8)Dt\u03c6\u03c1(t), \u2297 is the Kronecker product, Nb = 12 (Ib2 +Kbb) is a symmetric (b 2 \u00d7 b) idempotent matrix with rank 1 2 b(b+ 1) and Kbb is a permutation matrix [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "A first analysis was performed in [15] where the authors provided a formulation based on the policy gradient theorem [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "A first analysis was performed in [15] where the authors provided a formulation based on the policy gradient theorem [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "While the estimate of the expected discounted reward and the associated gradient is an old topic in RL literature and several results have been proposed [15, 17], the estimate of the Hessian is not addressed in literature.", "startOffset": 153, "endOffset": 161}, {"referenceID": 16, "context": "While the estimate of the expected discounted reward and the associated gradient is an old topic in RL literature and several results have been proposed [15, 17], the estimate of the Hessian is not addressed in literature.", "startOffset": 153, "endOffset": 161}, {"referenceID": 17, "context": "Recently, the simultaneous perturbation stochastic approximation technique was exploited to estimate the Hessian [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "This formulation resemble the definition of REINFORCE estimate given in [19] for the gradient \u2207\u03b8J(\u03b8).", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Several statistical bounds have been proposed in literature, we refer to [20] for a survey on Monte\u2013Carlo methods.", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "These metrics have gained popularity because they are refinements of the Pareto dominance relation [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Recently, several works have been proposed in order to theoretically investigate the properties of hypervolume indicator [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "Several other metrics have been defined in the field of MOO, we refer to [23] for an extensive survey.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "Recall that the goal of MOO is to compute an approximation of the frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the actual one [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 21, "context": "[22]", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In particular, the performance are compared against some existing algorithms [8, 10].", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "In particular, the performance are compared against some existing algorithms [8, 10].", "startOffset": 77, "endOffset": 84}, {"referenceID": 10, "context": "We start considering a multi\u2013objective version of the standard discrete-time Linear-Quadratic Gaussian regulator (LQG) with multidimensional and continuous state and action spaces [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 9, "context": "For a complete description of the LQG problem and for the settings, we refer to [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "Our primary concern was related to the boundness of the control actions, leading to the following parametrization of the manifold in the policy space: \u03c6\u03c1(t) = \u2212 1/1 + exp(\u03c11 + \u03c12t) and \u03c6 2 \u03c1(t) = \u2212 1/1 + exp(\u03c13 + \u03c14t) with t \u2208 [0, 1].", "startOffset": 227, "endOffset": 233}, {"referenceID": 0, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 1, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 2, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 9, "context": "In order to compare our frontier with the one obtained by other algorithms, we consider the domain, settings and policy parametrization as described in [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "A simple second\u2013order polynomial in t \u2208 [0, 1] with 5 parameters has been used to parametrize the policy manifold.", "startOffset": 40, "endOffset": 46}], "year": 2017, "abstractText": "This paper is about learning a continuous approximation of the Pareto frontier in Multi\u2013Objective Markov Decision Problems (MOMDPs). We propose a policy\u2013based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy\u2013gradient multi\u2013objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient\u2013ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient\u2013based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non\u2013trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs.", "creator": "LaTeX with hyperref package"}}}