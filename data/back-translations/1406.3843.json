{"id": "1406.3843", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2014", "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models", "abstract": "Due to the strong correlations between the model parameters and the hyperparameters, sampling from hierarchical Bayesian models is often difficult for MCMC methods. Recent methods of the varied Hamiltonian Monte Carlo (RMHMC) have significant potential advantages in this environment, but are mathematically expensive. We are introducing a new RMHMC method, which we call the semi-separable Hamiltonian Monte Carlo, which uses a specially developed mass matrix that allows the common Hamiltonian supermodel to divide parameters and hyperparameters into two simpler Hamiltonian ones. This structure is exploited by a new integrator called the alternating block-by-jump algorithm, which can mix faster than simpler Gibbs samples and is simpler and more efficient than previous cases of RMHMC.", "histories": [["v1", "Sun, 15 Jun 2014 19:03:46 GMT  (756kb,D)", "http://arxiv.org/abs/1406.3843v1", null]], "reviews": [], "SUBJECTS": "stat.CO cs.AI cs.LG", "authors": ["yichuan zhang", "charles a sutton"], "accepted": true, "id": "1406.3843"}, "pdf": {"name": "1406.3843.pdf", "metadata": {"source": "CRF", "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models", "authors": ["Yichuan Zhang", "Charles Sutton"], "emails": ["Y.Zhang-60@sms.ed.ac.uk", "c.sutton@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Bayesian statistics provides a natural way to manage model complexity and control overfitting, with modern problems involving complicated models with a large number of parameters. One of the most powerful advantages of the Bayesian approach is hierarchical modeling, which allows partial pooling across a group of datasets, allowing groups with little data to borrow information from similar groups with larger amounts of data. However, such models pose problems for Markov chain Monte Carlo (MCMC) methods, because the joint posterior distribution is often pathological due to strong correlations between the model parameters and the hyperparameters [3]. For example, one of the most powerful MCMC methods is Hamiltonian Monte Carlo (HMC). However, for hierarchical models even the mixing speed of HMC can be unsatisfactory in practice, as has been noted several times in the literature [3, 4, 11]. Riemannian manifold Hamiltonian Monte Carlo (RMHMC) [7] is a recent extension of HMC that aims to efficiently sample from challenging posterior distributions by exploiting local geometric properties of the distribution of interest. However, it is computationally too expensive to be applicable to large scale problems. In this work, we propose a simplified RMHMC method, called Semi-Separable Hamiltonian Monte Carlo (SSHMC), in which the joint Hamiltonian over parameters and hyperparameters has special structure, which we call semi-separability, that allows it to be decomposed into two simpler, separable Hamiltonians. This condition allows for a new efficient algorithm which we call the alternating blockwise leapfrog algorithm. Compared to Gibbs sampling, SSHMC can make significantly larger moves in hyperparameter space due to shared terms between the two simple Hamiltonians. Compared to previous RMHMC methods, SSHMC yields simpler and more computationally efficient samplers for many practical Bayesian models."}, {"heading": "2 Hierarchical Bayesian Models", "text": "Let D = {Di}Ni=1 be a collection of data groups where ith data group is a collection of iid observations yj = {yji}Nii=1 and their inputs xj = {xji}Nii=1. We assume the data follows a parametric\nar X\niv :1\n40 6.\n38 43\nv1 [\nst at\n.C O\n] 1\n5 Ju\nn 20\n14\ndistribution p(yi|xi,\u03b8i), where \u03b8i is the model parameter for group i. The parameters are assumed to be drawn from a prior p(\u03b8i|\u03c6), where \u03c6 is the hyperparameter with prior distribution p(\u03c6). The joint posterior over model parameters \u03b8 = (\u03b81, . . . ,\u03b8N ) and hyperparameters \u03c6 is then\np(\u03b8,\u03c6|D) \u221d N\u220f i=1 p(yi|xi,\u03b8i)p(\u03b8i|\u03c6)p(\u03c6). (1)\nThis hierarchical Bayesian model is popular because the parameters \u03b8i for each group are coupled, allowing the groups to share statistical strength. However, this property causes difficulties when approximating the posterior distribution. In the posterior, the model parameters and hyperparameters are strongly correlated. In particular, \u03c6 usually controls the variance of p(\u03b8|\u03c6) to promote partial pooling, so the variance of \u03b8|\u03c6,D depends strongly on \u03c6. This causes difficulties for many MCMC methods, such as the Gibbs sampler and HMC. An illustrative example of pathological structure in hierarchical models is the Gaussian funnel distribution [11]. Its density function is defined as p(x, v) = \u220fn i=1N (xi|0, e\u2212v)N (v|0, 32), where x is the vector of low-level parameters and v is the variance hyperparameters. The pathological correlation between x and v is illustrated by Figure 1."}, {"heading": "3 Hamiltonian Monte Carlo on Posterior Manifold", "text": "Hamiltonian Monte Carlo (HMC) is a gradient-based MCMC method with auxiliary variables. To generate samples from a target density \u03c0(z), HMC constructs an ergodic Markov chain with the invariant distribution \u03c0(z, r) = \u03c0(z)\u03c0(r), where r is an auxiliary variable. The most common choice of \u03c0(r) is a Gaussian distribution N (0, G\u22121) with precision matrix G. Given the current sample z, the transition kernel of the HMC chain includes three steps: first sample r \u223c \u03c0(r), second propose a new sample (z\u2032, r\u2032) by simulating the Hamiltonian dynamics and finally accept the proposed sample with probability \u03b1 = min {1, \u03c0(z\u2032, r\u2032)/\u03c0(z, r)}, otherwise leave z unchanged. The last step is a Metropolis-Hastings (MH) correction. Define H(z, r) := \u2212 log \u03c0(z, r). The Hamiltonian dynamics is defined by the differential equations z\u0307 = \u2202rH r\u0307 = \u2212\u2202zH , where z is called the position and r is called the momentum.\nIt is easy to see that H\u0307(z, r) = \u2202zH z\u0307 +\u2202rH r\u0307 = 0, which is called the energy preservation property [10, 11]. In physics, H(z, r) is known as the Hamiltonian energy, and is decomposed into the sum of the potential energy U(z) := \u2212 log \u03c0(z) and the kinetic energy K(r) := \u2212 log \u03c0(r). The most used discretized simulation in HMC is the leapfrog algorithm, which is given by the recursion\nr(\u03c4 + /2) = r(\u03c4)\u2212 2 \u2207zU(\u03c4) (2a)\nz(\u03c4 + ) = z(\u03c4) + \u2207rK(\u03c4 + /2) (2b) r(\u03c4 + ) = r(\u03c4 + /2)\u2212\n2 \u2207\u03b8U(\u03c4 + ), (2c)\nwhere is the step size of discretized simulation time. After L steps from the current sample (z(0), r(0)) = (z, r), the new sample is proposed as the last point (z\u2032, r\u2032) = (z(L ), r(L )). In Hamiltonian dynamics, the matrix G is called the mass matrix. If G is constant w.r.t. z, then z and r are independent in \u03c0(z, r). In this case we say that H(z, r) is a separable Hamiltonian. In particular, we use the term standard HMC to refer to HMC using the identity matrix as G. Although HMC methods often outperform other popular MCMC methods, they may mix slowly if there are strong correlations between variables in the target distribution. Neal [11] showed that HMC can mix faster if G is not the identity matrix. Intuitively, such a G acts like a preconditioner. However, if the curvature of \u03c0(z) varies greatly, a global preconditioner can be inadequate. For this reason, recent work, notably that on Riemannian manifold HMC (RMHMC) [7], has considered non-separable Hamiltonian methods, in which G(z) varies with position z, so that z and r are no longer independent in \u03c0(z, r). The resulting Hamiltonian H(z, r) = \u2212 log \u03c0(z, r) is called a non-separable Hamiltonian. For example, for Bayesian inference problems, Girolami et al. [7] proposed using the Fisher Information Matrix (FIM) of \u03c0(\u03b8), which is the metric tensor of posterior manifold. However, for a non-separable Hamiltonian, the simple leapfrog dynamics (2a)-(2c) do not yield a valid MCMC method, as they are no longer reversible. Simulation of general non-separable systems requires the generalized leapfrog integrator (GLI) [7], which requires computing higher order derivatives to solve a system of non-linear differential equations. The computational cost of GLI in general is O(d3) where d is the number of parameters, which is prohibitive for large d.\nIn hierarchical models, there are two ways to sample the posterior using HMC. One way is to sample the joint posterior \u03c0(\u03b8,\u03c6) directly. The other way is to sample the conditional \u03c0(\u03b8|\u03c6) and \u03c0(\u03c6|\u03b8), simulating from each conditional distribution using HMC. This strategy is called HMC within Gibbs [11]. In either case, HMC chains tend to mix slowly in hyperparameter space, because the huge variation of potential energy across different hyperparameter values can easily overwhelm the kinetic energy in separable HMC [11]. Hierarchical models also pose a challenge to RMHMC, if we want to sample the model parameters and hyperparameters jointly. In particular, the closed-form FIM of the joint posterior \u03c0(\u03b8,\u03c6) is usually unavailable. Due to this problem, even sampling some toy models like the Gaussian funnel using RMHMC becomes challenging. Betancourt [2] proposed a new metric that uses a transformed Hessian matrix of \u03c0(\u03b8), and Betancourt and Girolami [3] demonstrate the power of this method for efficiently sampling hyperparameters of hierarchical models on some simple benchmarks like Gaussian funnel. However, the transformation requires computing eigendecomposition of the Hessian matrix, which is infeasible in high dimensions. Because of these technical difficulties, RMHMC for hierarchical models is usually used within a block Gibbs sampling scheme, alternating between \u03b8 and \u03c6. This RMHMC within Gibbs strategy is useful because the simulation of the non-separable dynamics for the conditional distributions may have much lower computational cost than that for the joint one. However, as we have discussed, in hierarchical models these variables tend be very strongly correlated, and it is well-known that Gibbs samplers mix slowly in such cases [13]. So, the Gibbs scheme limits the true power of RMHMC."}, {"heading": "4 Semi-Separable Hamiltonian Monte Carlo", "text": "In this section we propose a non-separable HMC method that does not have the limitations of Gibbs sampling and that scales to relatively high dimensions, based on a novel property that we will call semi-separability. We introduce new HMC methods that rely on semi-separable Hamiltonians, which we call semi-separable Hamiltonian Monte Carlo (SSHMC)."}, {"heading": "4.1 Semi-Separable Hamiltonian", "text": "In this section, we define the semi-separable Hamiltonian system. Our target distribution will be the posterior \u03c0(\u03b8,\u03c6) = log p(\u03b8,\u03c6|D) of a hierarchical model (1), where \u03b8 \u2208 Rn and \u03c6 \u2208 Rm. Let (r\u03b8, r\u03c6) \u2208 Rm+n be the vector of momentum variables corresponding to \u03b8 and \u03c6 respectively. The non-separable Hamiltonian is defined as\nH(\u03b8,\u03c6, r\u03b8, r\u03c6) = U(\u03b8,\u03c6) +K(r\u03b8, r\u03c6|\u03b8,\u03c6), (3) where the potential energy is U(\u03b8,\u03c6) = \u2212 log \u03c0(\u03b8,\u03c6) and the kinetic energy is K(r\u03b8, r\u03c6|\u03b8,\u03c6) = \u2212 logN (r\u03b8, r\u03c6; 0, G(\u03b8,\u03c6)\u22121), which includes the normalization term log |G(\u03b8,\u03c6)|. The mass matrix G(\u03b8,\u03c6) can be an arbitrary p.d. matrix. For example, previous work on RMHMC [7] has chosen G(\u03b8,\u03c6) to be FIM of the joint posterior \u03c0(\u03b8,\u03c6), resulting in an HMC method that requires O ( (m+ n) 3 ) time. This limits applications of RMHMC to large scale problems.\nTo attack these computational challenges, we introduce restrictions on the mass matrix G(\u03b8,\u03c6) to enable efficient simulation. In particular, we restrict G(\u03b8,\u03c6) to have the form\nG(\u03b8,\u03c6) =\n( G\u03b8(\u03c6,x) 0\n0 G\u03c6(\u03b8)\n) ,\nwhere G\u03b8 and G\u03c6 are the precision matrices of r\u03b8 and r\u03c6, respectively. Importantly, we restrict G\u03b8(\u03c6,x) to be independent of \u03b8 and G\u03c6(\u03b8) to be independent of \u03c6. If G has these properties, we call the resulting Hamiltonian a semi-separable Hamiltonian. A semi-separable Hamiltonian is still in general non-separable, as the two random vectors (\u03b8,\u03c6) and (r\u03b8, r\u03c6) are not independent. The semi-separability property has important computational advantages. First, because G is block diagonal, the cost of matrix operations reduces from O((n + m)k) to O(nk). Second, and more important, substituting the restricted mass matrix into (3) results in the potential and kinetic energy:\nU(\u03b8,\u03c6) = \u2212 \u2211 i [log p(yi|\u03b8i,xi) + log p(\u03b8i|\u03c6)]\u2212 log p(\u03c6), (4)\nK(r\u03b8, r\u03c6|\u03c6,\u03b8) = 1\n2 [rT\u03b8G\u03b8(x,\u03c6)r\u03b8 + r T \u03c6G\u03c6(\u03b8)r\u03c6 + log |G\u03b8(x,\u03c6)|+ log |G\u03c6(\u03b8)|]. (5)\nIf we fix (\u03b8, r\u03b8) or (\u03c6, r\u03c6), the non-separable Hamiltonian (3) can be seen as a separable Hamiltonian plus some constant terms. In particular, define the notation\nA(r\u03b8|\u03c6) = 1\n2 rT\u03b8G\u03b8(x,\u03c6)r\u03b8, A(r\u03c6|\u03b8) =\n1 2 rT\u03c6G\u03c6(\u03b8)r\u03c6.\nThen, considering (\u03c6, r\u03c6) as fixed, the non-separable Hamiltonian H in (3) is different from the following separable Hamiltonian\nH1(\u03b8, r\u03b8) = U1(\u03b8|\u03c6, r\u03c6) +K1(r\u03b8|\u03c6), (6) U1(\u03b8|\u03c6, r\u03c6) = \u2212 \u2211 i [log p(yi|\u03b8i,xi) + log p(\u03b8i|\u03c6)] +A(r\u03c6|\u03b8) + 1 2 log |G\u03c6(\u03b8)| , (7)\nK1(r\u03b8|\u03c6) = A(r\u03b8|\u03c6) (8) only by some constant terms that do not depend on (\u03b8, r\u03b8). What this means is that any update to (\u03b8, r\u03b8) that leaves H1 invariant leaves the joint Hamiltonian H invariant as well. An example is the leapfrog dynamics on H1, where U1 is considered the potential energy, and K1 the kinetic energy. Similarly, if (\u03b8, r\u03b8) are fixed, then H differs from the following separable Hamiltonian\nH2(\u03c6, r\u03c6) = U2(\u03c6|\u03b8, r\u03b8) +K2(r\u03c6|\u03b8), (9) U2(\u03c6|\u03b8, r\u03b8) = \u2212 \u2211 i log p(\u03b8i|\u03c6)\u2212 log p(\u03c6) +A(r\u03b8|\u03c6) + 1 2 log |G\u03b8(x,\u03c6)| , (10)\nK2(r\u03c6|\u03b8) = A(r\u03c6|\u03b8) (11) only by terms that are constant with respect to (\u03c6, r\u03c6). Notice thatH1 andH2 are coupled by the terms A(r\u03b8|\u03c6) and A(r\u03c6|\u03b8). Each of these terms appears in the kinetic energy of one of the separable Hamiltonians, but in the potential energy of the other one. We call these terms auxiliary potentials because they are potential energy terms introduced by the auxiliary variables. These auxiliary potentials are key to our method (see Section 4.3).\n4.2 Alternating block-wise leapfrog algorithm432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15]. Algorithm 1 SSHMC by ABLA Require: (\u2713, ) Sample r \u21e0 N (0, G\u2713( ,x)) and r \u21e0 N (0, G (\u2713)) for l in 1, 2, . . . , L do (\u2713(l+\u270f/2), r(l+\u270f/2)) leapfrog(\u2713(l), r(l)\u2713 , H1, \u270f/2) ( (l+\u270f), r (l+\u270f) ) leapfrog( (l), r (l) , H2, \u270f) (\u2713(l+\u270f), r(l+\u270f)) leapfrog(\u2713(l), r(l), H1, \u270f/2) end for Draw u \u21e0 U(0, 1) if u < min(1, eH(\u2713, ,r\u2713,r ) H(\u2713 (L\u270f), (L\u270f),r(L\u270f),r (L\u270f) )) then (\u27130, 0, r0\u2713, r 0 ) (\u2713(L\u270f), (L\u270f), r (L\u270f) \u2713 , r (L\u270f) ) else (\u27130, 0, r0\u2713, r 0 ) (\u2713, , r\u2713, r ) end if return (\u27130, 0)\nNow we introduce an efficient SSHMC method that exploits the semi-separability property. As described in the previous section, any update to (\u03b8, r\u03b8) that leaves H1 invariant also leaves the joint Hamiltonian H invariant, as does any update to (\u03c6, r\u03c6) that leaves H2 invariant. So a natural idea is simply to alternate between simulating the Hamiltonian dynamics for H1 and that for H2. Crucially, even though the total HamiltonianH is not separable in general, both H1 and H2 are separable. Therefore when simulating H1 and H2, the simple leapfrog method can be used, and the more complex GLI method is not required. We call this method the alternating block-wise leapfrog algorithm (ABLA), shown in Algorithm 1. In this figure the function \u201cleapfrog\u201d returns the result of the leapfrog dynamics (2a)-(2c) for the given starting point, Hamiltonian, and step size. We call each iteration of the loop from 1 . . . L an ALBA step. For simplicity, we have shown one leapfrog step for H1 and H2 for each ALBA step, but in practice it is useful to use multiple leapfrog steps per ALBA step. ABLA has discretization error due to the leapfrog discretization, so the MH correction is required. If it is possible to simulateH1 andH2 exactly, thenH is preserved exactly and there is no need for MH correction. To show that the SSHMC method by ALBA preserves the distribution \u03c0(\u03b8,\u03c6), we also need to show the ABLA is a time-reversible and volume-preserving transformation in the joint space of (\u03b8, r\u03b8,\u03c6, r\u03c6). LetX = X\u03b8,r\u03b8\u00d7X\u03c6,r\u03c6 where (\u03b8, r\u03b8) \u2208 X\u03b8,r\u03b8 and (\u03c6, r\u03c6) \u2208 X\u03c6,r\u03c6 . Obviously, any reversible and volume-preserving transformation in a subspace of X is also reversible and volumepreserving in X . It is easy to see that each leapfrog step in the ABLA algorithm is reversible and volume-preserving in either X\u03b8,r\u03b8 or X\u03c6,r\u03c6 . One more property of integrator of interest is\n4\nsymplecticity. Because each leapfrog integrator is symplectic in the subspace of X [10], they are also symplectic in X . Then because ABLA is a composition of symplectic leapfrog integrators, and the composition of symplectic transformations is symplectic, we know ABLA is symplectic. We emphasize that ABLA is actually not a discretized simulation of the semi-separable Hamiltonian systemH , that is, if starting at a point (\u03b8, r\u03b8,\u03c6, r\u03c6) in the joint space, we run the exact Hamiltonian dynamics for H for a length of time L, the resulting point will not be the same as that returned by ALBA at time L even if the discretized time step is infinitely small. For example, ALBA simulates H1 with step size 1 and H2 with step size 2 where 1 = 2 2, when 2 \u2192 0 that preserves H ."}, {"heading": "4.3 Connection to other methods", "text": "Although the SSHMC method may seem similar to RMHMC within Gibbs (RMHMCWG), SSHMC is actually very different. The difference is in the last two terms of (7) and (10); if these are omitted from SSHMC and the Hamiltonians for \u03c0(\u03b8|\u03c6), then we obtain HMC within Gibbs. Particularly important among these two terms is the auxiliary potential, because it allows each of the separable Hamiltonian systems to borrow energy from the other one. For example, if the previous leapfrog step increases the kinetic energy K1(r\u03b8|\u03c6) in H1(\u03b8, r\u03b8), then, in the next leapfrog step for H2(\u03c6, r\u03c6), we see that \u03c6 will have greater potential energy U2(\u03c6|\u03b8, r\u03b8), because the auxiliary potential A(r\u03b8|\u03c6) is shared. That allows the leapfrog step to accommodate a larger change of log p(\u03c6|\u03b8) using A(r\u03b8|\u03c6). So, the chain will mix faster in X\u03c6. By the symmetry of \u03b8 and \u03c6, the auxiliary potential will also accelerate the mixing in X\u03b8. Another way to see this is that the dynamics in RMHMCWG for (r\u03c6,\u03c6) preserves the distribution \u03c0(\u03b8, r\u03c6,\u03c6) = \u03c0(\u03b8,\u03c6)N (r\u03c6; 0, G\u03c6(\u03c6)\u22121) but not the joint \u03c0(\u03b8,\u03c6, r\u03b8, r\u03c6). That is because the Gibbs sampler does not take into account the effect of \u03c6 on r\u03b8. In other words, the Gibbs step has the stationary distribution \u03c0(\u03c6, r\u03c6|\u03b8) rather than \u03c0(\u03c6, r\u03c6|\u03b8, r\u03b8). The difference between the two is the auxiliary potential. In contrast, the SSHMC methods preserve the Hamiltonian of \u03c0(\u03b8,\u03c6, r\u03b8, r\u03c6)."}, {"heading": "4.4 Choice of mass matrix", "text": "The choice of G\u03b8 and G\u03c6 in SSHMC is usually similar to RMHMCWG. If the Hessian matrix of \u2212 log p(\u03b8|y,x,\u03c6) is independent of \u03b8 and always p.d., it is natural to defineG\u03b8 as the inverse of the Hessian matrix. However, for some popular models, e.g., logistic regression, the Hessian matrix of the likelihood function depends on the parameters \u03b8. In this case, one can use any approximate Hessian B, like the Hessian at the mode, and define G\u03b8 := (B +B(\u03c6))\u22121, where B(\u03c6) is the Hessian of the prior distribution. Such a rough approximation is usually good enough to improve the mixing speed, because the main difficulty is the correlation between model parameters and hyperparameters. In general, because the computational bottleneck in HMC and SSHMC is computing the gradient of the target distribution, both methods have the same computational complexity O(lg), where g is the cost of computing the gradient and l is the total number of leapfrog steps per iteration. However, in practice we find it very beneficial to use multiple steps in each blockwise leapfrog update in ALBA; this can cause SSHMC to require more time than HMC. Also, depending on the mass matrixG\u03b8, the cost of leapfrog a step in ABLA may be different from those in standard HMC. For some choices of G\u03b8, the leapfrog step in ABLA can be even faster than one leapfrog step of HMC. For example, in many models the computational bottleneck is the gradient \u2207\u03c6 logZ(\u03c6), Z(\u03c6) is the normalization in prior. Recall that G\u03b8 is a function of \u03c6. If |G\u03b8| = Z(\u03c6)\u22121, Z(\u03c6) will be canceled out, avoiding computation of \u2207\u03c6 logZ(\u03c6). One example is using Gx = evI in Gaussian funnel distribution aforementioned in Section 2. A potential problem of such G\u03b8 is that the curvature of the likelihood function p(D|\u03b8) is ignored. But when the data in each group is sparse and the parameters \u03b8 are strongly correlated, this G\u03b8 can give nearly optimal mixing speed and make SSHMC much faster. In general, any choice ofG\u03b8 andG\u03c6 that would be valid for separable HMC with Gibbs is also valid for SSHMC."}, {"heading": "5 Experimental Results", "text": "In this section, we compare the performance of SSHMC with the standard HMC and RMHMC within Gibbs [7] on four benchmark models.1 The step size of all methods are manually tuned so\n1Our use of a Gibbs scheme for RMHMC follows standard practice [7].\nthat the acceptance rate is around 70-85%. The number of leapfrog steps are tuned for each method using preliminary runs. The implementation of RMHMC we used is from [7]. The running time is wall-clock time measured after burn-in. The performance is evaluated by the minimum Effective Sample Size (ESS) over all dimensions (see [6]). When considering the different computational complexity of methods, our main efficiency metric is time normalized ESS."}, {"heading": "5.1 Demonstration on Gaussian Funnel", "text": "We demonstrate SSHMC by sampling the Gaussian Funnel (GF) defined in Section 2. We consider n = 100 dimensional low-level parameters x and 1 hyperparameter v. RMHMC within Gibbs on GF has block diagonal mass matrix defined as Gx = \u2212\u22022v log p(x, v)\u22121 = evI and Gv = \u2212Ex[\u22022v log p(x, v)]\u22121 = (n + 19 )\u22121. We use the same mass matrix in SSHMC, because it is semi-separable. We use 2 leapfrog steps for low-level parameters and 1 leapfrog step for the hyperparameter in ABLA and the same leapfrog step size for the two separable Hamiltonians. We generate 5000 samples from each method after 1000 burn-in iterations. The ESS per second (ESS/s) and mean squared error (MSE) of the sample estimated mean and variance of the hyperparameter are given in Table 1. Notice that RMHMC is much more efficient for the low-level variable because of the adaptive mass matrix with hyperparameter. Figure 1 illustrates a dramatic difference between HMC and SSHMC. It is clear that HMC suffers from oscillation of the hyperparameter in a narrow region. That is because the kinetic energy limits the change of hyperparameters [3, 11]. In contrast, SSHMC has much wider energy variation and the trajectory spans a larger range of hy-\nperparameter v. The energy variation of SSHMC is similar to the RMHMC with Soft-Abs metric (RMHMC-Soft-Abs) reported in [2], an instance of general RMHMC without Gibbs. But compared with [2], each ABLA step is about 100 times faster than each generalized leapfrog step and SSHMC can generate around 2.5 times more effective samples per second than RMHMC-Soft-Abs. Although RHMC within Gibbs has better ESS/s on the low level variables, its estimation of the mean and variance is biased, indicating that the chain has not yet mixed. More important, Table 1 shows that the samples generated by SSHMC give nearly unbiased estimates of the mean and variance of the hyperparameter, which neither of the other methods are able to do."}, {"heading": "5.2 Hierarchical Bayesian Logistic Regression", "text": "In this experiment, we consider hierarchical Bayesian logistic regression with exponential prior for the variance hyperparameter v, that is\np(w, \u03c6|D) \u221d \u220f i \u220f j \u03c3(yijw T i xij)N (wi|0, vI)Exp(v|\u03bb),\nwhere \u03c3 is the logistic function \u03c3(z) = 1/(1 + exp(\u2212z)) and (yij ,xij) is the jth data points the ith group. We use the Statlog (German credit) dataset from [1]. This dataset includes 1000 data points and each data has 16 categorical features and 4 numeric features. Bayesian logistic regression on this dataset has been considered as a benchmark for HMC [7, 8], but the previous work uses only one group in their experiments. To make the problem more interesting, we partition the dataset into 10 groups according to the feature Purpose. The size of group varies from 9 to 285. There are 200 model parameters (20 parameters for each group) and 1 hyperparameter. We consider the reparameterization of the hyperparameter \u03b3 = log v. For RMHMC within Gibbs, the mass matrix for group i is Gi := I(x,\u03b8)\u22121, where I(x,\u03b8) is the Fisher Information matrix for model parameter wi and constant mass Gv . In each iteration of the Gibbs sampler, each wi is sampled from by RMHMC using 6 generalized leapfrog steps and v is sampled using 6 leapfrog steps. For SSHMC, Gi := Cov(x) + exp(\u03b3)I and the same constant mass Gv . The results are shown in Table 2. SSHMC again has much higher ESS/s than the other methods."}, {"heading": "5.3 Stochastic Volatility", "text": "A stochastic volatility model we consider is studied in [9], in which the latent volatilities are modeled by an auto-regressive AR(1) process such that the observations are yt = t\u03b2 exp(xt/2) with latent variable xt+1 = \u03c6xt + \u03b7t+1. We consider the distributions x1 \u223c N (0, \u03c32/(1\u2212 \u03c62)), t \u223c N (0, 1) and \u03b7t \u223c (0, \u03c32). The joint probability is defined as\np(y,x, \u03b2, \u03c6, \u03c3) = T\u220f t=1 p(yt|xt, \u03b2)p(x1) T\u220f t=2 p(xt|xt\u22121, \u03c6, \u03c3)\u03c0(\u03b2)\u03c0(\u03c6)\u03c6(\u03c3),\nwhere the prior \u03c0(\u03b2) \u221d 1/\u03b2, \u03c32 \u223c Inv-\u03c72(10, 0.05) and (\u03c6 + 1)/2 \u223c beta(20, 1.5). The FIM of p(x|\u03b1, \u03b2, \u03c6,y) depends on the hyperparameters but not x, but the FIM of p(\u03b1, \u03b2, \u03c6|x,y) depends on (\u03b1, \u03b2, \u03c6). For RMHMC within Gibbs we consider FIM as the metric tensor following [7]. For SSHMC, we define G\u03b8 as inverse Hessian of log p(x|\u03b1, \u03b2, \u03c6,y), but G\u03c6 as an identity matrix. In each ABLA step, we use 5 leapfrog steps for updates of x and 2 leapfrog steps for updates of the hyperparameters, so that the running time of SSHMC is about 7 times that of standard HMC.\nWe generate 20000 samples using each method after 10000 burn-in samples. The histogram of hyperparameters is shown in Figure 2. It is clear that all methods approximately converge to the same distribution. But from Table 3, we see that SSHMC generates almost two times as many ESS/s as RMHMC within Gibbs."}, {"heading": "5.4 Log-Gaussian Cox Point Process", "text": "The log-Gaussian Cox Point Process (LGCPP) is another popular testing benchmark [5, 7, 14]. We follow the experimental setting of Girolami and Calderhead [7]. The observations Y = {yij} are counts at the location (i, j), i, j = 1, . . . , d on a regular spatial grid, which are conditionally independent given a latent intensity process \u039b = {\u03bb(i, j)} with mean m\u03bb(i, j) = m exp(xi,j), where m = 1/d2, X = {xi,j}, x = Vec(X) and y = Vec(Y). X is assigned by a Gaussian process prior, with mean function m(xi,j) = \u00b51 and covariance function \u03a3(xi,j , xi\u2032,j\u2032) = \u03c32 exp(\u2212\u03b4(i, i\u2032, j, j\u2032)/\u03b2d) where \u03b4(\u00b7) is the Euclidean distance between (i, j) and (i\u2032, j\u2032). The log joint probability is given by log p(y,x|\u00b5, \u03c3, \u03b2) = \u2211i,j yi,jxi,j\u2212m exp(xi,j)\u2212 12 (x\u2212\u00b51)T\u03a3\u22121(x\u2212 \u00b51). We consider a 32\u00d732 grid that has 1024 latent variables. Each latent variable xi,j corresponds to a single observation yi,j . We consider RMHMC within Gibbs with FIM of the conditional posteriors. See [7] for the definition of FIM. The generalized leapfrog steps are required for updating (\u03c3, \u03b2), but only the leapfrog steps are required for updating x. Each Gibbs iteration takes 20 leapfrog steps for x and 1 general leapfrog step for (\u03c3, \u03b2). In SSHMC, we use Gx = \u03a3\u22121 and G(\u03c3,\u03b2) = I. In each ABLA step, the update of x takes 2 leapfrog steps and the update of (\u03b1, \u03b2) takes 1 leapfrog step. Each SSHMC transition takes 10 ALBA steps. We do not consider HMC on LGCPP, because it mixes extremely slowly for hyperparameters. The results of ESS are given in Table 4. The mean of sampled latent variable and the histogram of sampled hyperparameters are given in Figure 3. It is clear that the samples of RMHMC and SSHMC are consistent, so both methods are mixing well. However, SSHMC generates about six times as many effective samples per hour as RMHMC within Gibbs."}, {"heading": "6 Conclusion", "text": "We have presented Semi-Separable Hamiltonian Monte Carlo (SSHMC), a new version of Riemannian manifold Hamiltonian Monte Carlo (RMHMC) that aims to retain the flexibility of RMHMC for difficult Bayesian sampling problems, while achieving greater simplicity and lower computational complexity. We tested SSHMC on several different hierarchical models, and on all the models we considered, SSHMC outperforms both HMC and RMHMC within Gibbs in terms of number of effective samples produced in a fixed amount of computation time. Future work could consider other choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15]."}], "references": [{"title": "A General Metric for Riemannian Manifold Hamiltonian Monte Carlo", "author": ["M.J. Betancourt"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Hamiltonian Monte Carlo for Hierarchical Models", "author": ["M.J. Betancourt", "M. Girolami"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning hyperparameters for neural network models using Hamiltonian dynamics", "author": ["K. Choo"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Scaling limits for the transient phase of local Metropolis\u2013Hastings algorithms", "author": ["O.F. Christensen", "G.O. Roberts", "J.S. Rosenthal"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Practical Markov Chain Monte Carlo", "author": ["C.J. Geyer"], "venue": "Statistical Science, pages 473\u2013483,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Stochastic volatility: likelihood inference and comparison with ARCH models", "author": ["S. Kim", "N. Shephard", "S. Chib"], "venue": "The Review of Economic Studies,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Simulating Hamiltonian dynamics, volume 14", "author": ["B. Leimkuhler", "S. Reich"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Auxiliary-variable exact Hamiltonian Monte Carlo samplers for binary distributions", "author": ["A. Pakman", "L. Paninski"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Monte Carlo statistical methods, volume 319", "author": ["C.P. Robert", "G. Casella"], "venue": "Citeseer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Adaptive Hamiltonian and Riemann manifold Monte Carlo samplers", "author": ["Z. Wang", "S. Mohamed", "N. de Freitas"], "venue": "In International Conference on Machine Learning (ICML), pages 1462\u20131470,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Continuous relaxations for discrete Hamiltonian Monte Carlo", "author": ["Y. Zhang", "C. Sutton", "A. Storkey", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "However, such models pose problems for Markov chain Monte Carlo (MCMC) methods, because the joint posterior distribution is often pathological due to strong correlations between the model parameters and the hyperparameters [3].", "startOffset": 223, "endOffset": 226}, {"referenceID": 1, "context": "However, for hierarchical models even the mixing speed of HMC can be unsatisfactory in practice, as has been noted several times in the literature [3, 4, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 2, "context": "However, for hierarchical models even the mixing speed of HMC can be unsatisfactory in practice, as has been noted several times in the literature [3, 4, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 8, "context": "However, for hierarchical models even the mixing speed of HMC can be unsatisfactory in practice, as has been noted several times in the literature [3, 4, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 5, "context": "Riemannian manifold Hamiltonian Monte Carlo (RMHMC) [7] is a recent extension of HMC that aims to efficiently sample from challenging posterior distributions by exploiting local geometric properties of the distribution of interest.", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "An illustrative example of pathological structure in hierarchical models is the Gaussian funnel distribution [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "It is easy to see that \u1e22(z, r) = \u2202zH \u017c +\u2202rH \u1e59 = 0, which is called the energy preservation property [10, 11].", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "It is easy to see that \u1e22(z, r) = \u2202zH \u017c +\u2202rH \u1e59 = 0, which is called the energy preservation property [10, 11].", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "Neal [11] showed that HMC can mix faster if G is not the identity matrix.", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "For this reason, recent work, notably that on Riemannian manifold HMC (RMHMC) [7], has considered non-separable Hamiltonian methods, in which G(z) varies with position z, so that z and r are no longer independent in \u03c0(z, r).", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "[7] proposed using the Fisher Information Matrix (FIM) of \u03c0(\u03b8), which is the metric tensor of posterior manifold.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Simulation of general non-separable systems requires the generalized leapfrog integrator (GLI) [7], which requires computing higher order derivatives to solve a system of non-linear differential equations.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "This strategy is called HMC within Gibbs [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "In either case, HMC chains tend to mix slowly in hyperparameter space, because the huge variation of potential energy across different hyperparameter values can easily overwhelm the kinetic energy in separable HMC [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 0, "context": "Betancourt [2] proposed a new metric that uses a transformed Hessian matrix of \u03c0(\u03b8), and Betancourt and Girolami [3] demonstrate the power of this method for efficiently sampling hyperparameters of hierarchical models on some simple benchmarks like Gaussian funnel.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "Betancourt [2] proposed a new metric that uses a transformed Hessian matrix of \u03c0(\u03b8), and Betancourt and Girolami [3] demonstrate the power of this method for efficiently sampling hyperparameters of hierarchical models on some simple benchmarks like Gaussian funnel.", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "However, as we have discussed, in hierarchical models these variables tend be very strongly correlated, and it is well-known that Gibbs samplers mix slowly in such cases [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "For example, previous work on RMHMC [7] has chosen G(\u03b8,\u03c6) to be FIM of the joint posterior \u03c0(\u03b8,\u03c6), resulting in an HMC method that requires O ( (m+ n) 3 ) time.", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "2 Alternating block-wise leapfrog algorithm 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15].", "startOffset": 404, "endOffset": 412}, {"referenceID": 12, "context": "2 Alternating block-wise leapfrog algorithm 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15].", "startOffset": 404, "endOffset": 412}, {"referenceID": 7, "context": "Because each leapfrog integrator is symplectic in the subspace of X [10], they are also symplectic in X .", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "In this section, we compare the performance of SSHMC with the standard HMC and RMHMC within Gibbs [7] on four benchmark models.", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Our use of a Gibbs scheme for RMHMC follows standard practice [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The implementation of RMHMC we used is from [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "The performance is evaluated by the minimum Effective Sample Size (ESS) over all dimensions (see [6]).", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "That is because the kinetic energy limits the change of hyperparameters [3, 11].", "startOffset": 72, "endOffset": 79}, {"referenceID": 8, "context": "That is because the kinetic energy limits the change of hyperparameters [3, 11].", "startOffset": 72, "endOffset": 79}, {"referenceID": 0, "context": "The energy variation of SSHMC is similar to the RMHMC with Soft-Abs metric (RMHMC-Soft-Abs) reported in [2], an instance of general RMHMC without Gibbs.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "But compared with [2], each ABLA step is about 100 times faster than each generalized leapfrog step and SSHMC can generate around 2.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Bayesian logistic regression on this dataset has been considered as a benchmark for HMC [7, 8], but the previous work uses only one group in their experiments.", "startOffset": 88, "endOffset": 94}, {"referenceID": 6, "context": "A stochastic volatility model we consider is studied in [9], in which the latent volatilities are modeled by an auto-regressive AR(1) process such that the observations are yt = t\u03b2 exp(xt/2) with latent variable xt+1 = \u03c6xt + \u03b7t+1.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "For RMHMC within Gibbs we consider FIM as the metric tensor following [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "The log-Gaussian Cox Point Process (LGCPP) is another popular testing benchmark [5, 7, 14].", "startOffset": 80, "endOffset": 90}, {"referenceID": 5, "context": "The log-Gaussian Cox Point Process (LGCPP) is another popular testing benchmark [5, 7, 14].", "startOffset": 80, "endOffset": 90}, {"referenceID": 11, "context": "The log-Gaussian Cox Point Process (LGCPP) is another popular testing benchmark [5, 7, 14].", "startOffset": 80, "endOffset": 90}, {"referenceID": 5, "context": "We follow the experimental setting of Girolami and Calderhead [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "See [7] for the definition of FIM.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "Future work could consider other choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15].", "startOffset": 177, "endOffset": 185}, {"referenceID": 12, "context": "Future work could consider other choices of mass matrix within the semi-separable framework, or the use of SSHMC within discrete models, following previous work in discrete HMC [12, 15].", "startOffset": 177, "endOffset": 185}], "year": 2014, "abstractText": "Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.", "creator": "LaTeX with hyperref package"}}}