{"id": "1406.5298", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Semi-supervised Learning with Deep Generative Models", "abstract": "The ever-growing size of modern data sets, combined with the difficulty of obtaining information about labels, has made semi-supervised learning one of the most practical problems in modern data analysis. We are rethinking the approach of semi-supervised learning with generative models and are developing new models that allow effective generalization from small labeled data sets to large unlabeled ones. Generative approaches have previously been either inflexible, inefficient, or unscalable. We show that deep generative models and approximate Bayesian conclusions that take advantage of recent advances in variation methods can lead to significant improvements, making generative approaches to semi-supervised learning highly competitive.", "histories": [["v1", "Fri, 20 Jun 2014 07:52:18 GMT  (5762kb,D)", "http://arxiv.org/abs/1406.5298v1", null], ["v2", "Fri, 31 Oct 2014 22:43:31 GMT  (4748kb,D)", "http://arxiv.org/abs/1406.5298v2", "To appear in the proceedings of Neural Information Processing Systems (NIPS) 2014"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["diederik p kingma", "shakir mohamed", "danilo jimenez rezende", "max welling"], "accepted": true, "id": "1406.5298"}, "pdf": {"name": "1406.5298.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Deep Generative Models", "authors": ["Diederik P. Kingma", "Danilo J. Rezende", "Shakir Mohamed"], "emails": ["M.Welling}@uva.nl", "shakir}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Semi-supervised learning considers the problem of classification when only a small subset of the observations have corresponding class labels. Such problems are of immense practical interest in a wide range of applications, including image search (Fergus et al., 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set. The question that is then asked is: how can properties of the data be used to improve decision boundaries and to allow for classification that is more accurate than that based on classifiers constructed using the labelled data alone. In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference, namely auto-encoding variational Bayes and stochastic backpropagation (Kingma and Welling, 2014; Rezende et al., 2014).\nAmongst existing approaches, the simplest algorithm for semi-supervised learning is based on a self-training scheme (Rosenberg et al., 2005) where the the model is bootstrapped with additional labelled data obtained from its own highly confident predictions; this process being repeated until some termination condition is reached. These methods are heuristic and prone to error since they can reinforce poor predictions. Transductive SVMs (TSVM) (Joachims, 1999) extend SVMs with the aim of max-margin classification while ensuring that there are as few unlabelled observations near the margin as possible. These approaches have difficulty extending to a large amounts of unlabelled data and efficient optimisation in this setting is still an open problem. Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations with label information propagating through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration (Blum et al., 2004; Zhu et al., 2003). Graph-based approaches are sensitive to the graph structure and require eigen-analysis of the graph Laplacian, which limits the scale to which these methods can be applied (though efficient spectral methods are now available (Fergus et al., 2009)). Neural network-based approaches combine unsupervised and\nar X\niv :1\n40 6.\n52 98\nv1 [\ncs .L\nG ]\n2 0\nJu n\nsupervised learning by training feed-forward classifiers with an additional penalty from an autoencoder or other unsupervised embedding of the data (Ranzato and Szummer, 2008; Weston et al., 2012). Currently, state-of-the-art results on a semi-supervised version of the MNIST benchmark data set have been achieved by the Manifold Tangent Classifier (MTC) (Rifai et al., 2011), which trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp (Simard et al., 1991) to train a classifier that is approximately invariant to local perturbations along the manifold.\nIn this paper, we instead, choose to exploit the power of generative models, which recognise the semi-supervised learning problem as a specialised missing data imputation task for the classification problem. Existing generative approaches based on models such as Gaussian mixture or hidden Markov models (Zhu, 2006), have not been very successful due to the limited capacity and the need for many states to perform well. More recent solutions have used non-parametric density models, either based on trees (Kemp et al., 2003) or Gaussian processes (Adams and Ghahramani, 2009), but accurate inference and the scalability of these approaches is still lacking. Variational approximations for semi-supervised clustering have also been explored previously (Li et al., 2009; Wang et al., 2009).\nThus, while a small set of generative approaches have been previously explored, a generalised and scalable probabilistic approach for semi-supervised learning is still lacking. It is this gap that we address through the following contributions:\n\u2022 We describe a new framework for semi-supervised learning with generative models, employing rich parametric density estimators formed by the fusion of probabilistic modelling and deep neural networks. \u2022 We show for the first time how variational inference can be brought to bear on the problem of semi-supervised classification. In particular, we develop a variational inference algorithm with stochastic backpropagation that allows for joint optimisation of all model and variational parameters and that is scalable to large datasets. \u2022 We demonstrate the performance of our approach on a number of data sets providing stateof-the-art results on benchmark problems. \u2022 Finally, we show qualitatively that the class-conditional models learn to separate the image classes from the intra-class variabilities (styles), allowing in a very straightforward fashion to simulate analogies of images on a variety of datasets."}, {"heading": "2 Deep Generative Models for Semi-supervised Learning", "text": "We are faced with data that appears as pairs (X,Y) = {(x(1), y(1)), . . . , (x(N), y(N))}, with the nth observation x(n) \u2208 RD and the corresponding class label y(n) \u2208 {1, . . . , L}. In semi-supervised classification, only a subset of the observations have corresponding class labels and we denote the set of observed data as O. We now develop two models that exploit generative descriptions of the data to improve upon the classification performance that would be obtained using the unlabelled data alone.\nLatent-feature discriminative model (M1): A commonly used approach is to construct a model of the data x(n) that provides an embedding or feature representation of the data, and using these features, a separate classifier is thereafter trained. The embedding allows for a clustering of related observations in a latent feature space that allows for accurate classification, even with a limited number of labels. Instead of a linear embedding, or features obtained from a regular auto-encoder, we construct a deep generative model of the data that is able to provide a more robust set of latent features. The generative model we use is:\np(z(n)) = N (0, I); p\u03b8(x(n)|z(n)) = f(x(n); z(n), \u03b8) (1)\nwhere f(x(n); z(n), \u03b8) is a suitable likelihood function (e.g. a Gaussian or Bernoulli PDF), conditioned on latent variables z(n) and model parameters \u03b8, that is formed by a non-linear transformation of a set of latent variables z(n). This non-linear transformation is essential to allow for higher moments of the data to be captured by the density model, and we choose these non-linear functions to be deep neural networks. Approximate samples from the posterior distribution over the latent variables p(z(n)|x(n)) are used as features to train a classifier that predicts class labels y(n), such as a (transductive) SVM or multinomial regression. This approach has the advantage that it allows for\nclassification in a lower dimensional, more easily-separable space and allows for easy optimisation. We demonstrate the efficacy of features obtained in this way and explore the effectiveness of this approach in section 4.\nConditional generative model (M2): We propose a probabilistic model that describes the data as being generated by a latent class variable y(n), in addition to a continuous latent variable z(n). The data is explained by the generative process:\np(y(n)) = Cat(\u03c0); p(z(n)) = N (0, I); p\u03b8(x(n)|z(n)) = f(x(n); y, z(n), \u03b8) (2)\nwhere z(n) are latent variables, the class labels y(n) are treated as latent variables if no class label is available and Cat(y) is the categorical distribution. These latent variables are marginally independent and allow us, e.g. in case of digit generation, to separate the class specification from the writing style of the digit. As before, f(x(n), y, z(n), \u03b8) is a suitable likelihood function, e.g. a Bernoulli or Gaussian, whose distribution parameters are the output of a differentiable non-linear function of z(n) and \u03b8. In our experiments, we choose deep neural networks as this non-linear function. Since most labels y(n) are unobserved, this model allows us to integrate over the class of any unlabelled data during the inference process and highlights a classification-as-inference approach, since predictions for missing labels are obtained from the posterior distribution p\u03b8(y(n)|x(n)). This model can also be seen as a hybrid continuous-discrete mixture model where the different mixture components share parameters.\nIn both cases, exact inference will be intractable, but we exploit recent advances in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) to efficiently obtain accurate posterior distributions for latent variables as well as to perform efficient parameter learning."}, {"heading": "3 Scalable Semi-supervised Variational Inference", "text": ""}, {"heading": "3.1 Variational Free Energy Objective", "text": "Given the specification of the two models in the previous section, we can perform inference and parameter learning using the marginal likelihood as the objective function, which involves an integration over the latent variables. To allow for tractability, we follow the variational principle of introducing an approximate posterior distribution q\u03c6(z\n(n)|x(n)) with variational parameters \u03c6 and obtain a lower bound on the marginal likelihood, using this objective for optimisation instead. By conditioning the posterior approximation on the datapoint, we avoid variational parameters per datapoint, instead only requiring to fit global variational parameters \u03c6.\nFor the latent feature discriminative model (M1), we only require an approximate posterior q\u03c6(z|x). The variational bound obtained is:\nM1 : log p\u03b8(X) \u2265 \u2211 n Eq\u03c6(z(n)|x(n)) [ log p\u03b8(x (n)|z(n)) ] \u2212 KL [ q\u03c6(z (n)|x(n))\u2016p(z(n)) ]\n= \u2212 \u2211 n F(x(n)) (3)\nwhere F(x(n)) is the variational free energy associated with data point x(n), q\u03c6(z|x) is any distribution with the same support as the generative distribution p\u03b8(x, z), and KL [ q\u03c6(z (n)|x(n))\u2016p(z(n)) ] is the KL-divergence between the variational distribution q\u03c6(z(n)|x(n)) (also referred to as inference network) and the prior p(zn). We use this objective for optimisation and use draws from q\u03c6(z\n(n)|x(n)) as features for the observed data, which we then use to train a classifier. For the conditional generative model (M2), we have an extra set of label variables y which can be both observed or latent (when there is no label available). Computing the loss for the case where the labels are observed is an extension of equation 3 where the distributions are conditioned on the labels. When the labels are unobserved they must be inferred. This requires the introduction of a complementary variational distribution q\u03c6(y, z|x):\nq\u03c6(y, z|x) = q\u03c6(y|x)q\u03c6(z|y,x), (4)\nwhere q\u03c6(y|x) and q\u03c6(z|y,x) are parametrised conditional distributions. Here, the variational distribution q\u03c6(y|x) acts as a classifier, and it is this distribution that we use to make label predictions\nat test time. The resulting lower bound neatly separates into terms handling the observed and unobserved data:\n\u2212 log p\u03b8(X,Y) \u2264 \u2211 n\u2208O F(x(n), y(n)) + \u2211 n/\u2208O Eq\u03c6(y|x(n)) [ F(x(n), y) + log q\u03c6(y|x(n)) ] = F (5)\nwhere F(x(n), y(n)) = Eq\u03c6(z(n)|y(n),x(n)) [ \u2212 log p\u03b8(x(n), y(n), z(n)) + log q\u03c6(z(n)|y(n),x(n)) ] . (6)\nIn the objective function (5), the label predictive distribution q\u03c6(y|x) contributes only to the second term relating to the unlabelled data, which is an undesirable property if we wish to use this distribution as a classifier. Ideally, parameter learning should take place for both observed and unobserved data. To remedy this, we add a classification loss to (5) such that the distribution q\u03c6(y|x) learns for all data. The modified objective function is:\nM2: F\u03b1 = (1\u2212 \u03b1)F \u2212 \u03b1 \u2211 n\u2208O log q\u03c6(y (n)|x(n)) (7)\nwhere \u03b1 \u2208 [0, 1) controls the relative weight between generative and purely discriminative learning. The bound in equation (3) and (7) provides an unified objective function for optimisation of both the parameters of the model as well as the inference network. This optimisation can be done jointly, without resort to the variational EM algorithm, using the stochastic backpropagation technique introduced by Kingma and Welling (2014) and Rezende et al. (2014), which we discuss when developing optimisation algorithms for these loss functions in section 3.3."}, {"heading": "3.2 Efficiency with an Inference Network", "text": "The inference network q\u03c6(z|x) is a variational distribution that is conditioned on the observed data and allows for efficient inference by sharing statistical strength across the posterior estimates for all latent variables. The implication of using a parametric variational distribution is: faster convergence during training and faster inference at test time since we only require a single pass through the inference network (unlike with VEM, in which we repeat the generalized E-step optimisation for every test data point).\nFor model (M1) we choose the inference network to be\nq\u03c6(z|x) = N (z|\u00b5(x),R(x)R(x)>) (8) where R(x) is either a diagonal matrix, or a structured covariance composed of a rank-one matrix with a diagonal correction term since this allows one principal direction of correlation to be accounted for while maintaining linear cost. Both \u00b5(x) and R(x) are represented by MLPs.\nFor the conditional generative model (M2), we define an inference network q\u03c6(z|y,x) for the Gaussian latent variables z that is also conditioned on the label y and a second inference network q\u03c6(y|x) for the missing labels\nq\u03c6(z|y,x) = N (z|\u00b5(y,x),R(y,x)R(y,x)>); q\u03c6(y|x)) = Cat(y|\u03c0(x)) (9) We parameterise the inference networks using neural networks of the form:\nf(z) = g(W2(g(W1z + b1)) + b2) (10)\nwhich, in this case, is a two layer neural network with rectified or tanh non-linearities g(\u00b7). We denote the set of parameters of these variational distributions by \u03c6. Hence, the inference network for q\u03c6(y|x) consists of a neural network that takes the observation x as the input and produces the parameters \u03c0(x) of a discrete distribution over the L classes. Similarly, the inference network for the latent variables q\u03c6(z|y,x) is a neural network that takes y and x as input and provides as output the canonical parameters \u00b5(x),R(x) of a Gaussian distribution."}, {"heading": "3.3 Free Energy Optimisation", "text": "We use the objective functions (3) and (7) to optimise the parameters of the generative model \u03b8 and the parameters of the variational distributions \u03c6 using stochastic gradient descent. We exploit\nthe stochastic backpropagation technique described by Kingma and Welling (2014) and Rezende et al. (2014) to allow for efficient optimisation. We describe the core strategy for the latent-feature discriminative model (M1), since the same computations are used for the conditional generative model (M2).\nWhen the prior p(z) is a spherical Gaussian distribution p(z) = N (z|0, I) and the variational distribution q\u03c6(z|x) is also a Gaussian distribution as in (9), the KL term in equation (3) can be computed analytically and the log-likelihood term can be rewritten, using the location-scale transformation for the Gaussian distribution, as:\nEq\u03c6(z|x) [log p\u03b8(x|z)] = EN ( |0,I) [log p\u03b8(x|\u00b5(x) + R(x) )] . (11)\nWhile the expectation (11) still cannot be solved analytically, its gradients with respect to the generative parameters \u03b8 and variational parameters \u03c6 can be efficiently estimated by a simple estimator:\n\u2202\n\u2202\u03b8 Eq\u03c6(z|x) [log p\u03b8(x|z)] = EN ( |0,I)\n[ \u2202\n\u2202\u03b8 log p\u03b8(x|z)\n] (12)\n\u2202\n\u2202\u03c6 Eq\u03c6(z|x) [log p\u03b8(x|z)] = EN ( |0,I)\n[ \u2202\n\u2202z log p\u03b8(x|z)\n( \u2202\u00b5(x)\n\u2202\u03c6 + \u2202R(x) \u2202\u03c6\n)] , (13)\nwhere z is evaluated at the point z = \u00b5(x) + R(x) . Combining equations (13) , (12) and (3), we can compute an unbiased estimate of the gradients of the free energy w.r.t. a single datapoint x as\n\u2202\n\u2202\u03b8 F(x) = \u2212EN ( |0,I)\n[ \u2202\n\u2202\u03b8 log p\u03b8(x|z)\n] ' \u2212 1\nS S\u2211 s=1 \u2202 \u2202\u03b8 log p\u03b8(x|z(s)) (14)\n\u2202\n\u2202\u03c6 F(x) = \u2212EN ( |0,I)\n[ \u2202\n\u2202z log p\u03b8(x|z)\n( \u2202\u00b5(x)\n\u2202\u03c6 + \u2202R(x) \u2202\u03c6\n)] + \u2202\n\u2202\u03c6 KL [q\u03c6(z|x)\u2016p(z)]\n' \u2212 1 S S\u2211 s=1 [ \u2202 \u2202z(s) log p\u03b8(x|z(s)) ( \u2202\u00b5(x) \u2202\u03c6 + \u2202R(x) \u2202\u03c6 (s) )] + \u2202 \u2202\u03c6 KL [q\u03c6(z|x)\u2016p(z)]\n(15)\nwhere (s) \u223c N (0, I) and z(s) = \u00b5(x) + R(x) (s). In practice, it suffices to use a small S (e.g. S = 1) and then estimate the gradient using minibatches of data points. We also we the same random numbers (s) for both estimator to have lower variance.\nThe gradients of the loss (7) for model (M2) can be computed by a direct application of the chain rule and by noting that the conditional free energy Fy(xn) contains the same type of terms as the loss (7). The gradients of the latter can then be efficiently estimated using (14) and (15).\nDuring optimization we use the estimated gradients in conjunction with standard stochastic gradientbased optimization methods such as SGD, RMSprop or AdaGrad (Duchi et al., 2010). This results in parameter updates of the form: (\u03b8t+1, \u03c6t+1) \u2190 (\u03b8t, \u03c6t) + \u0393t(gt\u03b8,gt\u03c6), where \u0393 is a diagonal preconditioning matrix that adaptively scales the gradients for faster minimization. The training procedure for the latent-feature discriminative model (M1) is summarised in algorithm 1, and that for the conditional generative model (M2) is summarised in algorithm 2. Our experimental results were obtained using AdaGrad."}, {"heading": "3.4 Computational Complexity", "text": "The overall algorithmic complexity of a single joint update of the parameters (\u03b8, \u03c6) for the model (M1) using the estimators (14) and (15) is CM1 = MSCMLP where M is the minibatch size used for the gradient estimate, S is the number of samples of per data point and CMLP is the cost of evaluating the MLPs in the conditional distributions p\u03b8(x|z) and q\u03c6(z|x) once per data point and z sample. The cost CMLP is of the form O(KD2) where K is the total number of layers and D is the average dimension of the layers of the MLPs in the model. Training the model (M1) also requires training a supervised classifier on top, which will have its own algorithmic complexity. For instance, if it is a neural net, it will have a complexity of the form CMLP .\nFor the model (M2) the algorithmic complexity is of the form CM2 = LCM1, where L is the number of labels and CM1 is the cost of evaluating the gradients of each conditional free energy Fy(x),\nAlgorithm 1 Learning in model M1 while generativeTraining() do D \u2190 getRandomMiniBatch() z(n) \u223c q\u03c6(z(n)|x(n)) \u2200x(n) \u2208 D F \u2190 \u2211 n F(x(n))\n(g\u03b8, g\u03c6)\u2190 ( \u2202F \u2202\u03b8 , \u2202F \u2202\u03c6 )\n(\u03b8, \u03c6)\u2190 (\u03b8, \u03c6) + \u0393(g\u03b8, g\u03c6) end while while discriminativeTraining() do D \u2190 getLabeledRandomMiniBatch() z(n) \u223c q\u03c6(z(n)|x(n)) \u2200{x(n), y(n)} \u2208 D trainClassifier({z(n), y(n)} ) end while\nAlgorithm 2 Learning in model M2 while training() do D \u2190 getRandomMiniBatch() y(n) \u223c q\u03c6(y(n)|x(n)) \u2200{x(n), y(n)} /\u2208 O z(n)q\u03c6(z\n(n)|yn,x(n)) F\u03b1 \u2190 eq. (7) (g\u03b8, g\u03c6)\u2190 ( \u2202F\u03b1 \u2202\u03b8 , \u2202F \u03b1 \u2202\u03c6 )\n(\u03b8, \u03c6)\u2190 (\u03b8, \u03c6) + \u0393(g\u03b8, g\u03c6) end while\nwhich is the same as for the model (M1). These complexities make this approach extremely appealing, since they are no more expensive than alternative approaches based on auto-encoder or neural models, which have the lowest computational complexity amongst existing approaches. In addition, our models are fully probabilistic models, allowing for a wide range of inferential queries, which is not possible with competing approaches."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Benchmark Classification", "text": "We test performance on the standard MNIST digit classification benchmark. In this benchmark, each model is trained on a partially labelled training set with varying amounts of labelled instances chosen randomly from the complete training set of 50,000 data points.\nFor all models we used a 50-dimensional latent variable z. The MLPs that form part of the likelihood function, as well as the MLPs for the inference network, were constructed with one or two hidden layers, each with 500 or 1000 hidden units, using a rectified-linear or softplus (logistic log-partition) activation functions. All parameters were initialized by sampling randomly fromN (0, 0.012I), and we employed a small weight decay, equivalent to a Gaussian prior of N (0, I). The objectives were optimized using minibatch-based gradient estimates together with AdaGrad (Duchi et al., 2010), with a learning rate selected from the set {0.1, 0.01, 0.02, 0.005}, based on convergence on the first few iterations. For MNIST, minibatches for training were generated by treating normalised pixel intensities of the image as Bernoulli probabilities and sampling binary images from this distribution.\nTable 1 shows classification results for the two models we described. We compare to a broad range of existing solutions in semi-supervised learning, in particular to classification using nearest neighbours (NN), support vector machines on the labelled set (SVM), the transductive SVM (TSVM), the Embedded neural networks Weston et al. (2012), and contractive auto-encoders (CAE). Some of the best results currently are obtained by the manifold tangent classifier (MTC) (Rifai et al., 2011). Results in the table for these methods are reproduced from Rifai et al. (2011). The latent-feature discriminative model (M1) performs better than other models based on simple embeddings of the data, demonstrating the effectiveness of the latent space in providing robust features that allow for easier classification. By combining these features with a classification mechanism directly in the same model, as in the conditional generative model (M2), we are able to then provide the best result over all testing conditions. While results in earlier work was reported based on a single generated data set (source: private communication), we provide confidence bounds to the mean performance under repeated draws of data sets. Unlike the other models in this comparison, our models are fully probabilistic but have a cost in the same order as these alternatives."}, {"heading": "4.2 Conditional Generation", "text": "The conditional generative model can be used to explore the underlying structure of the data, which we demonstrate through two forms of analogical reasoning. Firstly, we demonstrate style and content separation by fixing the class label y, and then varying the latent variables z over a range of\nFigure 1: Handwriting styles for MNIST obtained by fixing the class label and varying the latent variable over a 2D grid.\nFigure 2: Analogical reasoning for MNIST.\nvalues. Figure 1 shows four MNIST classes in which, using a trained model with two latent variables, and the 2D latent variable varied over a range from -5 to 5. In all cases, we see that the same parts of z-space correspond to similar writing styles, independent of the class; the left region represents upright writing styles, while the right-side represents slanted styles. We also show a similar visualisation for the street view house numbers (SVHN) data set (Netzer et al., 2011), which consists of more than 70,000 images of house numbers, in figure 3 (top). Here we use a 300-dimensional latent variable; each column represents a class label, and we show a number of independent samples generated from the model in each row. We show similar results for the NORB image recognition data set in figure 4(top), where again each column represents one of the five main classes. These results show that our model has learned to capture the global luminance and rotation of the images in a purely unsupervised way. This opens up interesting possibilities for pose estimation using generative models.\nAs a second approach, we use a test image and pass it through the inference network to obtain a sample from the latent variables corresponding to that image. We then fix the latent variables z to this sample and vary the class label to again demonstrate the disentanglement of style from class. We show this on MNIST (figure 2), SVHN (figure 3, bottom) and NORB (figure 4, bottom). The SVHN data set is a far more complex data set than MNIST, but the model is able to fix the style of house number and vary the digit that appears in that style well. These generations on SVHN and NORB leave room for improvement in terms of visual appearance, but represent the best current performance in simulation from generative models on these data sets.\nThe model used in this way also provides an alternative model to the stochastic feed-forward networks (SFNN) described by Tang and Salakhutdinov (2013). The performance of our model significantly improves on SFNN, since instead of an inefficient Monte Carlo EM algorithm relying on importance sampling, we are able to perform efficient joint inference that is easy to scale."}, {"heading": "4.3 Image Classification", "text": "We demonstrate the performance of image classification on the SVHN, and NORB image data sets. Since no comparative results in the semi-supervised setting exist, we perform nearest-neighbour and TSVM classification with RBF kernels and compare performance on features generated by our latent-feature discriminative model to the original features. The results are presented in tables 2 and 3, and we again demonstrate the effectiveness of our approach for semi-supervised classification.\nTable 3: Semi-supervised classification on the NORB dataset with 1000 labels.\nKNN TSVM M1(KNN) M1(TSVM) 78.71 26.00 65.39 18.79 (\u00b1 0.02) (\u00b1 0.06) (\u00b1 0.09) (\u00b1 0.05)"}, {"heading": "5 Discussion and Conclusion", "text": "We have developed new models for semi-supervised learning that allow us to improve the quality of prediction by exploiting information in the data density using generative models. We have developed an efficient variational optimisation algorithm for approximate Bayesian inference in these models and demonstrated that they are amongst the most competitive models currently available for semisupervised learning. We hope that these results stimulate the development of even more powerful semi-supervised classification methods based on generative models, of which there remains much scope.\nThe approximate inference methods introduced here can be easily extended to the model\u2019s parameters, harnessing the full power of variational learning. Such an extension also provides a principled ground for performing model selection. Efficient model selection is particularly important when the amount of available data is not large, such as in semi-supervised learning.\nFor image classification tasks, one area of interest is to combine such methods with convolutional neural networks that form the gold-standard for current supervised classification methods. Since all the components of our model are parametrized by neural networks we can readily exploit convolutional or more general locally connected architectures. This seems to be a very promising avenue for future exploration.\nA limitation of the models we have presented is that they scale linearly in the number of classes in the data sets. Having to re-evaluate the generative likelihood for each class during training is an expensive operation. Potential reduction of the number of evaluations could be achieved by using a truncation of the posterior mass. For instance we could combine our method with the truncation algorithm suggested by Pal et al. (2005). The extension of our model to multi-label classification problems that is essential for image-tagging is also possible, but requires similar approximations to reduce the number of likelihood-evaluations per class.\nIn order to achieve the property that the classification component learns at all times, we have introduced the augmented loss (7). It would be desirable to have a single principled loss function similar to (Blum et al., 2004) or (Zhu et al., 2003)."}], "references": [{"title": "Archipelago: nonparametric Bayesian semi-supervised learning", "author": ["R.P. Adams", "Z. Ghahramani"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Adams and Ghahramani,? \\Q2009\\E", "shortCiteRegEx": "Adams and Ghahramani", "year": 2009}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. Lafferty", "M.R. Rwebangira", "R. Reddy"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Blum et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Semi-supervised learning in gigantic image collections", "author": ["R. Fergus", "Y. Weiss", "A. Torralba"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceeding of the International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Semi-supervised learning with trees", "author": ["C. Kemp", "T.L. Griffiths", "S. Stromsten", "J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kemp et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2003}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "A variational approach to semi-supervised clustering", "author": ["P. Li", "Y. Ying", "C. Campbell"], "venue": "In Proceedings of the European Symposium on Artificial Neural Networks (ESANN),", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "PhD thesis, Massachusetts Institute of Technology", "citeRegEx": "Liang,? \\Q2005\\E", "shortCiteRegEx": "Liang", "year": 2005}, {"title": "Graph-based semi-supervised learning for phone and segment classification", "author": ["Y. Liu", "K. Kirchhoff"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Liu and Kirchhoff,? \\Q2013\\E", "shortCiteRegEx": "Liu and Kirchhoff", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast inference and learning with sparse belief propagation", "author": ["C. Pal", "C. Sutton", "A. McCallum"], "venue": "In Advances in Neural Information Processing Systems (NIPS). Citeseer", "citeRegEx": "Pal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2005}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["M. Ranzato", "M. Szummer"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "Ranzato and Szummer,? \\Q2008\\E", "shortCiteRegEx": "Ranzato and Szummer", "year": 2008}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Semi-supervised self-training of object detection models", "author": ["C. Rosenberg", "M. Hebert", "H. Schneiderman"], "venue": "In Proceedings of the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION\u201905)", "citeRegEx": "Rosenberg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2005}, {"title": "Semi-supervised learning improves gene expression-based prediction of cancer recurrence", "author": ["M. Shi", "B. Zhang"], "venue": null, "citeRegEx": "Shi and Zhang,? \\Q2011\\E", "shortCiteRegEx": "Shi and Zhang", "year": 2011}, {"title": "Tangent prop\u2013a formalism for specifying selected invariances in an adaptive network", "author": ["P. Simard", "B. Victorri", "Y. LeCun", "J.S. Denker"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Simard et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1991}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Tang and Salakhutdinov,? \\Q2013\\E", "shortCiteRegEx": "Tang and Salakhutdinov", "year": 2013}, {"title": "A rate distortion approach for semi-supervised conditional random fields", "author": ["Y. Wang", "G. Haffari", "S. Wang", "G. Mori"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical report,", "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J Lafferty"], "venue": "In Proceddings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Such problems are of immense practical interest in a wide range of applications, including image search (Fergus et al., 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 104, "endOffset": 125}, {"referenceID": 16, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 18, "endOffset": 39}, {"referenceID": 8, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 66, "endOffset": 79}, {"referenceID": 9, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 101, "endOffset": 126}, {"referenceID": 6, "context": "In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference, namely auto-encoding variational Bayes and stochastic backpropagation (Kingma and Welling, 2014; Rezende et al., 2014).", "startOffset": 343, "endOffset": 391}, {"referenceID": 13, "context": "In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference, namely auto-encoding variational Bayes and stochastic backpropagation (Kingma and Welling, 2014; Rezende et al., 2014).", "startOffset": 343, "endOffset": 391}, {"referenceID": 15, "context": "Amongst existing approaches, the simplest algorithm for semi-supervised learning is based on a self-training scheme (Rosenberg et al., 2005) where the the model is bootstrapped with additional labelled data obtained from its own highly confident predictions; this process being repeated until some termination condition is reached.", "startOffset": 116, "endOffset": 140}, {"referenceID": 4, "context": "Transductive SVMs (TSVM) (Joachims, 1999) extend SVMs with the aim of max-margin classification while ensuring that there are as few unlabelled observations near the margin as possible.", "startOffset": 25, "endOffset": 41}, {"referenceID": 1, "context": "Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations with label information propagating through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration (Blum et al., 2004; Zhu et al., 2003).", "startOffset": 247, "endOffset": 284}, {"referenceID": 22, "context": "Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations with label information propagating through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration (Blum et al., 2004; Zhu et al., 2003).", "startOffset": 247, "endOffset": 284}, {"referenceID": 3, "context": "Graph-based approaches are sensitive to the graph structure and require eigen-analysis of the graph Laplacian, which limits the scale to which these methods can be applied (though efficient spectral methods are now available (Fergus et al., 2009)).", "startOffset": 225, "endOffset": 246}, {"referenceID": 12, "context": "supervised learning by training feed-forward classifiers with an additional penalty from an autoencoder or other unsupervised embedding of the data (Ranzato and Szummer, 2008; Weston et al., 2012).", "startOffset": 148, "endOffset": 196}, {"referenceID": 20, "context": "supervised learning by training feed-forward classifiers with an additional penalty from an autoencoder or other unsupervised embedding of the data (Ranzato and Szummer, 2008; Weston et al., 2012).", "startOffset": 148, "endOffset": 196}, {"referenceID": 14, "context": "Currently, state-of-the-art results on a semi-supervised version of the MNIST benchmark data set have been achieved by the Manifold Tangent Classifier (MTC) (Rifai et al., 2011), which trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp (Simard et al.", "startOffset": 157, "endOffset": 177}, {"referenceID": 17, "context": ", 2011), which trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp (Simard et al., 1991) to train a classifier that is approximately invariant to local perturbations along the manifold.", "startOffset": 140, "endOffset": 161}, {"referenceID": 21, "context": "Existing generative approaches based on models such as Gaussian mixture or hidden Markov models (Zhu, 2006), have not been very successful due to the limited capacity and the need for many states to perform well.", "startOffset": 96, "endOffset": 107}, {"referenceID": 5, "context": "More recent solutions have used non-parametric density models, either based on trees (Kemp et al., 2003) or Gaussian processes (Adams and Ghahramani, 2009), but accurate inference and the scalability of these approaches is still lacking.", "startOffset": 85, "endOffset": 104}, {"referenceID": 0, "context": ", 2003) or Gaussian processes (Adams and Ghahramani, 2009), but accurate inference and the scalability of these approaches is still lacking.", "startOffset": 30, "endOffset": 58}, {"referenceID": 7, "context": "Variational approximations for semi-supervised clustering have also been explored previously (Li et al., 2009; Wang et al., 2009).", "startOffset": 93, "endOffset": 129}, {"referenceID": 19, "context": "Variational approximations for semi-supervised clustering have also been explored previously (Li et al., 2009; Wang et al., 2009).", "startOffset": 93, "endOffset": 129}, {"referenceID": 6, "context": "In both cases, exact inference will be intractable, but we exploit recent advances in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) to efficiently obtain accurate posterior distributions for latent variables as well as to perform efficient parameter learning.", "startOffset": 108, "endOffset": 156}, {"referenceID": 13, "context": "In both cases, exact inference will be intractable, but we exploit recent advances in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) to efficiently obtain accurate posterior distributions for latent variables as well as to perform efficient parameter learning.", "startOffset": 108, "endOffset": 156}, {"referenceID": 6, "context": "This optimisation can be done jointly, without resort to the variational EM algorithm, using the stochastic backpropagation technique introduced by Kingma and Welling (2014) and Rezende et al.", "startOffset": 148, "endOffset": 174}, {"referenceID": 6, "context": "This optimisation can be done jointly, without resort to the variational EM algorithm, using the stochastic backpropagation technique introduced by Kingma and Welling (2014) and Rezende et al. (2014), which we discuss when developing optimisation algorithms for these loss functions in section 3.", "startOffset": 148, "endOffset": 200}, {"referenceID": 6, "context": "the stochastic backpropagation technique described by Kingma and Welling (2014) and Rezende et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 6, "context": "the stochastic backpropagation technique described by Kingma and Welling (2014) and Rezende et al. (2014) to allow for efficient optimisation.", "startOffset": 54, "endOffset": 106}, {"referenceID": 2, "context": "During optimization we use the estimated gradients in conjunction with standard stochastic gradientbased optimization methods such as SGD, RMSprop or AdaGrad (Duchi et al., 2010).", "startOffset": 158, "endOffset": 178}, {"referenceID": 2, "context": "The objectives were optimized using minibatch-based gradient estimates together with AdaGrad (Duchi et al., 2010), with a learning rate selected from the set {0.", "startOffset": 93, "endOffset": 113}, {"referenceID": 14, "context": "Some of the best results currently are obtained by the manifold tangent classifier (MTC) (Rifai et al., 2011).", "startOffset": 89, "endOffset": 109}, {"referenceID": 19, "context": "We compare to a broad range of existing solutions in semi-supervised learning, in particular to classification using nearest neighbours (NN), support vector machines on the labelled set (SVM), the transductive SVM (TSVM), the Embedded neural networks Weston et al. (2012), and contractive auto-encoders (CAE).", "startOffset": 251, "endOffset": 272}, {"referenceID": 14, "context": "Some of the best results currently are obtained by the manifold tangent classifier (MTC) (Rifai et al., 2011). Results in the table for these methods are reproduced from Rifai et al. (2011). The latent-feature discriminative model (M1) performs better than other models based on simple embeddings of the data, demonstrating the effectiveness of the latent space in providing robust features that allow for easier classification.", "startOffset": 90, "endOffset": 190}, {"referenceID": 10, "context": "We also show a similar visualisation for the street view house numbers (SVHN) data set (Netzer et al., 2011), which consists of more than 70,000 images of house numbers, in figure 3 (top).", "startOffset": 87, "endOffset": 108}, {"referenceID": 18, "context": "The model used in this way also provides an alternative model to the stochastic feed-forward networks (SFNN) described by Tang and Salakhutdinov (2013). The performance of our model significantly improves on SFNN, since instead of an inefficient Monte Carlo EM algorithm relying on importance sampling, we are able to perform efficient joint inference that is easy to scale.", "startOffset": 122, "endOffset": 152}, {"referenceID": 11, "context": "For instance we could combine our method with the truncation algorithm suggested by Pal et al. (2005). The extension of our model to multi-label classification problems that is essential for image-tagging is also possible, but requires similar approximations to reduce the number of likelihood-evaluations per class.", "startOffset": 84, "endOffset": 102}, {"referenceID": 1, "context": "It would be desirable to have a single principled loss function similar to (Blum et al., 2004) or (Zhu et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 22, "context": ", 2004) or (Zhu et al., 2003).", "startOffset": 11, "endOffset": 29}], "year": 2014, "abstractText": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}