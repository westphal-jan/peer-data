{"id": "1406.5667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "Correlation Clustering with Noisy Partial Information", "abstract": "In this thesis we propose a semi-random model for the correlation clustering problem and examine it. We give an approximation algorithm that, taking into account an instance from this model, will most likely find a solution to the value (1 +\\ delta) opt-cost + O _ {\\ delta} (n log ^ 3 n), where opt-cost is the value of the optimal solution (for each\\ delta & gt; 0).", "histories": [["v1", "Sun, 22 Jun 2014 03:07:55 GMT  (17kb)", "https://arxiv.org/abs/1406.5667v1", "13 pages"], ["v2", "Tue, 12 May 2015 19:33:12 GMT  (28kb)", "http://arxiv.org/abs/1406.5667v2", "To appear at Conference on Learning Theory (COLT) 2015. Substantial changes from previous version, including a new section on recovery of the ground truth clustering. 20 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["konstantin makarychev", "yury makarychev", "aravindan vijayaraghavan"], "accepted": false, "id": "1406.5667"}, "pdf": {"name": "1406.5667.pdf", "metadata": {"source": "CRF", "title": "Correlation Clustering with Noisy Partial Information", "authors": ["Konstantin Makarychev", "Aravindan Vijayaraghavan"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n56 67\nv2 [\ncs .D\nS] 1\n2 M\nay 2\n01 5\n3 n) with high probability, where opt-cost is the value of the optimal solution (for every \u03b4 > 0). The second algorithm finds the ground truth clustering with an arbitrarily small classification error \u03b7 (under some additional assumptions on the instance)."}, {"heading": "1 Introduction", "text": "One of the most commonly used algorithmic tools in data analysis and machine learning is clustering \u2013 partitioning a corpus of data into groups based on similarity. The data observed in several application domains \u2013 e.g., protein-protein interaction data, links between web pages, and social ties on social networks \u2013 carry relational information between pairs of nodes, which can be represented using a graph. Clustering based on relational information can reveal important structural information such as functional groups of proteins [Bader and Hogue, 2003, Girvan and Newman, 2002], communities on web and social networks [Fortunato, 2010, Karrer and Newman, 2011], and can be used for predictive tasks such as link prediction [Taskar et al., 2004].\nCorrelation clustering tackles this problem of clustering objects when we are given qualitative information about the similarity or dissimilarity between some pairs of these objects. This qualitative information is represented in the form of a graph G(V,E, c) in which edges E are labeled with signs {+,\u2212}; we denote the set of \u2018+\u2019 edges by E+ and the set of \u2018\u2212\u2019 edges by E\u2212. Each edge (u, v) in E+ indicates that u and v are similar, and each edge (u, v) \u2208 E\u2212 indicates that u and v are dissimilar; the cost c(u, v) of the edge shows the amount of similarity or dissimilarity between u and v.1 In the ideal case, this qualitative information is consistent with the intended (\u201cground truth\u201d) clustering. However, the qualitative information may be noisy due to errors in the observations. Hence, the goal is to find a partition P of G that minimizes the cost of inconsistent edges:\nmin P\n\u2211\n(u,v)\u2208E+:P(u)6=P(v)\nc(u, v) + \u2211\n(u,v)\u2208E\u2212:P(u)=P(v)\nc(u, v),\n\u2217Supported by NSF CAREER award CCF-1150062 and NSF award IIS-1302662. \u2020Supported by the Simons Collaboration on Algorithms and Geometry. 1One can also think of the instance as a graph G(V,E, c) with the edge costs c : E \u2192 [\u22121, 1]. If c(u, v) > 0 then (u, v) \u2208 E+,\nand If c(u, v) < 0 then (u, v) \u2208 E\u2212.\nwhere P(u) denotes the cluster that contains the vertex u. The objective captures the cost of inconsistent edges \u2013 cut edges in E+ and uncut edges in E\u2212. (For a partition P, we say that an edge (u, v) \u2208 E is consistent with P if either (u, v) \u2208 E+ and P(u) = P(v) or (u, v) \u2208 E\u2212 and P(u) 6= P(v).)\nNote that the underlying graph G(V,E) can be reasonably sparse; this is desirable since collecting pairwise information can be expensive. One important feature of correlation clustering is that it, unlike most other clustering problems, allows us not to specify the number of clusters. Hence, it is particularly useful when we have no prior knowledge of the number of clusters that the data divides into.\nCorrelation clustering also comes up naturally in MAP inference in graphical models and structured prediction tasks for such tasks as image segmentation, parts-of-speech tagging and dependency parsing in natural language processing [Nowozin and Lampert, 2010, Smith, 2011]. In structured prediction, we are given some observations as input (e.g., image data, sentences), and the goal is to predict a labeling x \u2208 X that encodes the high-level information that we would like to infer. For instance, in image segmentation, the variables x \u2208 {0, 1}n indicate whether each pixel is in the foreground or background. This is naturally modeled as a Correlation Clustering instance on the set of pixels (with 2 clusters), where edges connect adjacent pixels, and the costs (with signs) are set based on the similarity or dissimilarity of the corresponding pixels in the given image. The clusters in these inference problems then consist of the sets of variables that receive the same assignment in the MAP solution. Correlation clustering is also used in the context of consensus clustering and agnostic learning.\nCorrelation clustering was introduced in [Bansal et al., 2004], and implicitly in [Ben-Dor et al., 1999] as \u2018Cluster Editing\u2019. The problem is APX-hard even on complete graphs2 (when we are given the similarity information for every pair of objects) [Charikar et al., 2005]. The state-of-the-art approximation algorithm [Charikar et al., 2005, Demaine et al., 2006] achieves an O(log n) approximation for minimizing disagreements in the worst-case. Furthermore, there is a gap-preserving reduction from the classic Minimum Multicut problem [Charikar et al., 2005, Demaine et al., 2006], for which the current state-ofthe-art algorithm gives a \u0398(log n) factor approximation [Garg et al., 1993]. The complementary objective of maximizing agreements is easier from the approximability standpoint, and a 0.766 factor approximation is known [Charikar et al., 2005, Swamy, 2004]. For the special case of complete graphs (with unit costs on edges), small constant factor approximations have been obtained in a series of works [Bansal et al., 2004, Ailon et al., 2008, Chawla et al., 2014]. Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009]. To summarize, despite our best efforts, we only know logarithmic factor approximation algorithms for Correlation Clustering; moreover, we cannot get a constant factor approximation for worst-case instances if the Unique Games Conjecture is true.\nHowever, our primary interest in solving Correlation Clustering comes from its numerous applications, and the instances that we encounter in these applications are not worst-case instances. This motivates the study of the average-case complexity of the problem and raises the following question:\nCan we design algorithms with better provable guarantees for realistic average-case models of Correlation Clustering?\nSeveral natural average-case models of Correlation Clustering have been studied previously. Ben-Dor et al. [1999] consider a model in which we start with a ground-truth clustering \u2013 an arbitrary partitioning of the vertices \u2013 of a complete graph. Initially, edges inside clusters of the ground truth solution are labeled \u2018+\u2019 and edges between clusters are labeled \u2018-\u2019. We flip the label of each edge (change \u2018+\u2019 to \u2018\u2212\u2019 and \u2018\u2212\u2019 to \u2018+\u2019) with probability \u03b5 independently at random and obtain a Correlation Clustering instance (the flipped edges\n2This rules out (1 + \u01eb) factor approximations for some small constant \u01eb > 0.\nmodel the noisy observations) . In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering. Mathieu and Schudy consider a generalization of this model where there is an adversary: for each edge, we keep the initial label with probability (1 \u2212 \u03b5), and we let the adversary decide whether to flip the edge label or not with probability \u03b5. The major drawback of these models is that they only consider the case of complete graphs, i.e. they require that the Correlation Clustering instance contains similarity information for every pair of nodes. Chen et al. extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos\u2013Renyi random graphs. In their model, the underlying unlabeled graph G(V,E) comes from an Erdo\u0308s\u2013Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1\u2212 \u03b5 and inconsistent with probability \u03b5.\nWhile these average-case models are natural, they are unrealistic in practice since most real-world graphs are neither dense nor captured by Erdo\u0308s\u2013Renyi distributions. For instance, real-world graphs in community detection have many structural properties (presence of large cliques, large clustering coefficients, heavy-tailed degree distribution) that are not exhibited by graphs that are generated by Erdo\u0308s\u2013Renyi models [Newman et al., 2006, Kumar et al., 1999]. Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012]. Further, these models assume that every pair of vertices have the same amount of similarity or dissimilarity (all costs are unit). Our semi-random model tries to address these issues by assuming very little about the observations \u2013 the underlying unlabeled graph G(V,E) \u2013 and allowing non-uniform costs."}, {"heading": "1.1 Our Semi-random Model", "text": "In this paper, we propose and study a new semi-random model for generating general instances of Correlation Clustering, which we believe captures many properties of real world instances. It generalizes the model of Mathieu and Schudy [2010] to arbitrary graphs G(V,E, c) with costs. A semi-random instance {G(V,E, c), (E+ , E\u2212)} is generated as follows:\n1. The adversary chooses an undirected graph G(V,E, c) and a partition P\u2217 of the vertex set V (referred to as the planted clustering or ground truth clustering).\n2. Every edge is E is included in set ER independently with probability \u03b5.\n3. Every edge (u, v) \u2208 E \\ER with u and v in the same cluster of P\u2217 is included in E+, and every edge (u, v) \u2208 E \\ ER, with u and v in different clusters of P\u2217 is included in E\u2212.\n4. The adversary adds every edge from ER either to E+ or to E\u2212 (but not to both sets).\nThis model can be further generalized to an adaptive semi-random model as described in Section 3.1."}, {"heading": "1.2 Our Results", "text": "We develop two algorithms for semi-random instances of Correlation Clustering. The first algorithm gives a polynomial-time approximation scheme (PTAS) for instances from our semi-random model. The second algorithm recovers the planted partition with a small classification error \u03b7.\nTheorem 1.1. For every \u03b4 > 0, there is a polynomial-time algorithm that given a semi-random instance {G(V,E, c), (E+ , E\u2212)} of Correlation Clustering (with noise probability \u03b5 < 1/4), finds a clustering that has disagreement cost (1 + \u03b4) opt-cost +O((1 \u2212 2\u03b5)\u22124\u03b4\u22123n log3 n) w.h.p. over the randomness in the instance, where opt-cost is the cost of disagreements of the optimal solution for the instance.\nThe approximation additive term is much smaller than the cost of the planted solution if the average degree \u2206 \u226b \u03b5\u22121polylog n. Note that we compare the performance of our algorithm with the cost of the optimal solution. Further, these guarantees hold even in a more general adaptive semi-random model that is described in Section 3.1.\nThe above result gives a good approximation guarantee with respect to the objective. But what about recovering the ground truth clustering? Our semi-random model is too general to allow recovery. For instance, there could be large disconnected pieces inside some clusters of G, or there could be no edges between some clusters \u2014 in both cases, recovery is statistically impossible. Hence, we need some additional conditions for approximate recovery in our model, that guarantee at the very least that the ground truth clustering is uniquely optimal (in a robust sense).\nOur first assumption is that there is mild expansion inside clusters \u2014 this connectivity assumption prevents large pieces inside clusters that are almost disconnected, which might get separated in an almost optimal clustering. The second and third assumptions are that there are enough edges from vertices in one cluster to other clusters, to prevent these clusters (or parts of them) from coalescing in near-optimal clusterings. Finally, we assume (approximate) regularity in degrees inside clusters, since it is hard to correctly classify vertices with very few edges incident on them. These assumptions are described formally in Assumptions 5.1. We now informally describe the algorithmic guarantees for approximate recovery:\nTheorem 1.2. There exists a polynomial-time algorithm that given a semi-random instance I = {G = (V,E, c), (E+, E\u2212)} satisfying mild expansion inside clusters, regularity and inter-cluster density conditions (see Assumptions 5.1 for details) finds a partition P with classification error at most 4\u03b7 w.h.p. over the randomness in the instance, where\n\u03b7 = C2\n1\u2212 2\u03b5\n(\nn log n\ncost(E)\n)1/12 \u00b7 ( 1\n\u03b2\u03bbgap\n)1/2\n. (1)\nOur algorithm outputs a clustering such that only O(\u03b7n) vertices are misclassified (up to a renaming of the clusters). We note that the expansion and regularity assumptions are satisfied by Erdo\u0308s\u2013Renyi graphs: for instance, such random graphs have strong expansion both inside and between clusters (\u03bbgap = 1\u2212 o(1)) and have strong concentration of degrees. Our assumptions for recovery are soft: if there is bad expansion inside clusters (\u03bbgap is small), or if there are not sufficient edges between vertices in different clusters, we just need more observations (edges) to approximately recover the clusters. We note that the regularity conditions in Assumptions 5.1 are more for convenience and may be significantly relaxed. In particular, the same algorithm and analysis works even when the degrees are approximately regular (up to poly-logarithmic factors, for example) \u2014 this irregularity just appears in equation (1) as an extra multiplicative factor. We defer these details to the journal version of our paper."}, {"heading": "1.3 Related Work on Semi-random Models", "text": "Over the last two decades, there has been extensive research on average-case complexity of many important combinatorial optimization problems. Semi-random instances typically allow much more structure then completely random instances. Research on semi-random models was initiated by [Blum and Spencer, 1995], who introduced and investigated semi-random models for k-coloring. Semi-random models have also been studied for graph partitioning problems [Feige and Kilian, 1998, Chen et al., 2012, Makarychev et al., 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al., 2011], and other problems. Most related to our work, both in the nature of the model and in the techniques used, is a recent result of [Makarychev et al., 2013] on semi-random instances of Minimum\nFeedback Arc Set. While the techniques used in both papers are conceptually similar, the semidefinite (SDP) relaxation for Correlation Clustering that we use in this paper is very different from the SDP relaxation for Minimum Feedback Arc Set used in [Makarychev et al., 2013]. Further, we get a true 1 + \u03b4 approximation scheme (with an extra additive approximation term). This is in contrast to previous semi-random model results [Makarychev et al., 2012, 2013], which compare the cost of the solution that the algorithm finds to the cost of the planted solution. Moreover, this work gives not only a PTAS for the problem, but also a simple algorithm for recovery the ground truth solution.\nMathieu and Schudy recently considered a semi-random model for Correlation Clustering on complete graphs with unit edge costs. Later, Elsner and Schudy conducted an empirical evaluation of algorithms for the complete graph setting. Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erdo\u0308s\u2013Renyi graphs. Very recently, Globerson et al. [2014] considered a semi-random model for Correlation Clustering for recovery in grid graphs and planar graphs, and gave conditions for approximate recovery in terms of an expansion-related condition.\nComparison of Results. The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al., 2014]. Mathieu and Schudy designed an algorithm based on semidefinite programming (SDP relaxations with \u211322-triangle inequality constraints) for their semi-random model on complete graphs. It finds a clustering of cost at most 1 + O(n\u22121/6) times the cost of the optimal clustering (as long as \u03b5 \u2264 1/2 \u2212 O(n\u22121/3)) and manages to approximately recover the ground truth solution (when the clusters have size at least \u221a n). However, this algorithm only works on complete graphs and assumes unit edge costs. Chen et al. studied the problem on sparser graphs from the Erdo\u0308s\u2013Renyi distribution, and using weaker convex relaxations gave an algorithm that recovers the ground-truth when p \u2265 k2 logO(1) n/n. In the case of Erdo\u0308s\u2013Renyi graphs, our algorithms obtain similar guarantees for smaller values of k (the implicit dependence on k is a worse polynomial than in [Chen et al., 2014], however). The main advantage of our algorithms is that they work for more general graphs G: the first algorithm requires only that the average degree of G is some poly-log of n, while the second algorithm requires additionally that the graph has a mild expansion and regularity; its performance depends softly on the expansion and regularity parameters of the graph."}, {"heading": "1.4 Empirical Results", "text": "This paper focuses on designing an algorithm with provable theoretical guarantees for correlation clustering in a natural semi-random model. We have tested our algorithm to confirm that it is easily implementable and scalable. We used the SDPNAL MATLAB library to solve the semidefinite programming (SDP) relaxation for the problem [Zhao et al., 2010]. We implemented the recovery algorithm from Section 2 in C++, and also used a simple cleanup step that merges small clusters with the larger clusters based on their average inner products (this extra step can only improve our theoretical guarantees). We note that we could solve the SDP relaxation for instances with thousands of vertices since we used a very basic SDP relaxation without \u211322-triangle inequality constraints.\nWe tested the algorithm on random G(n, p) graphs with 4 planted clusters of size n/4 each, with the error rate (the probability of flipping the label) \u03b5 = 0.2. We used the same values of n as were used in [Chen et al., 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al., 2014] works (Chen et al. do not report the exact values of probabilities p; we took approximate values from Figure 2 in their paper). We summarize our results in Table 1."}, {"heading": "2 Overview of the Algorithms and Structural Insights", "text": "SDP relaxation. We use a simple SDP relaxation for the problem [Swamy, 2004]. For every vertex u, we have a unit vector u\u0304. For two vertices u and v, we interpret the inner product \u3008u\u0304, v\u0304\u3009 \u2208 [0, 1] as the indicator of the event: u and v lie in the same partition. The SDP is given below:\nmin P\n\u2211\n(u,v)\u2208E+\nc(u, v)(1 \u2212 \u3008u\u0304, v\u0304\u3009) + \u2211\n(u,v)\u2208E\u2212\nc(u, v)\u3008u\u0304, v\u0304\u3009.\nsubject to: for all u, v \u2208 V , \u3008u\u0304, v\u0304\u3009 \u2208 [0, 1]; \u2016u\u0304\u20162 = 1.\nThe intended vector (SDP) solution has one co-ordinate for every cluster of the clustering P: the vector u\u0304 for vertex u has 1 in the co-ordinate corresponding to P(u) and 0 otherwise. Hence this SDP is a valid relaxation. We note that this relaxation is weaker than the SDP used in [Mathieu and Schudy, 2010] because it does not have \u211322-triangle inequalities constraints. Hence, this semidefinite program is more scalable, and it is efficiently solvable for instances with a few thousand nodes.\nApproximation Algorithm (PTAS). We now describe the algorithm that gives a PTAS. Fix a parameter \u03b4 = o(1) \u2208 (0, 1/2). To simplify the notation, denote by f(u, v) (for (u, v) \u2208 E) the SDP value of the edge (without cost):\nf(u, v) = 1\u2212 \u3008u\u0304, v\u0304\u3009 if (u, v) \u2208 E+, and f(u, v) = \u3008u\u0304, v\u0304\u3009, otherwise. (2)\nOur PTAS is based on a surprising structural result about near-integrality of the SDP relaxation on the edges of the graph (see Theorem 3.1 for a formal statement).\nInformal Structural Theorem. In any feasible SDP solution of cost at most OPT , the SDP value of edge f(u, v) \u2265 1\u2212 \u03b4 for a 1\u2212 o\u03b4(1/ log n) fraction of the inconsistent edges (u, v) \u2208 E(G).\nHence, the structural result suggests that by removing all edges that contribute at least (1 \u2212 \u03b4) to the objective, the remaining instance has a solution of very small cost. We then run the O(log n) worst-case approximation algorithm of [Charikar et al., 2005] or [Demaine et al., 2006] on the remaining graph to obtain a PTAS overall.\nRecovery. The algorithm outlined above finds a solution of near optimal cost. Under additional assumptions, we show that we can in fact design a very simple greedy rounding scheme that can also efficiently recover the ground truth clustering approximately.\nThe structural theorem above shows that the SDP vectors are highly correlated for pairs of adjacent vertices. Under the additional conditions, we show that the vectors are in fact globally clustered according to the ground truth clustering:\nInformal Structural Theorem. When the semi-random instance {G = (V,E, c), E+, E\u2212} satisfies Assumption 5.1, we have w.h.p. that: for a (1 \u2212 O(\u03b7)) fraction of the clusters P \u2217i we can choose centers ui \u2208 P \u2217i and define cores core(P \u2217i ) = {v \u2208 P \u2217i : \u2016v\u0304 \u2212 u\u0304i\u2016 \u2264 1/10} \u2286 P \u2217i (balls of radius 1/10 around centers u\u0304i) such that core(P \u2217i ) \u2265 (1 \u2212 \u03b7)|P \u2217i | (the core of P \u2217i contains all but an \u03b7 fraction of vertices of P \u2217i ) and centers ui are mutually separated by a distance of at least 4/5.\nThe recovery algorithm is a greedy algorithm that finds heavy regions \u2013 sets of vectors that are clumped together \u2013 and puts them into clusters.\nInput: an optimal SDP solution {u\u0304}u\u2208V . Output: partition P1, . . . , Pt of V (for some t).\ni = 1, \u03c1core = 0.1 Define an auxiliary graph Gaux = (V,Eaux) with Eaux = {(u, v) : \u2016u\u0304\u2212 v\u0304\u2016 \u2264 \u03c1core} while V \\ (P1 \u222a . . . Pi\u22121) 6= \u2205\nLet u be the vertex of maximum degree in Gaux[V \\ (P1 \u222a . . . Pi\u22121)]. Let Pi = {v /\u2208 P1 \u222a \u00b7 \u00b7 \u00b7 \u222a Pi\u22121 : (u, v) \u2208 Eaux} / note that Pi contains u i = i+ 1\nreturn clusters P1, . . . , Pi\u22121.\nThis structural result about the global clustering and near integrality of the SDP vectors is consistent with empirical evidence. While our algorithm succeeds when the SDP is tight (as in [Chen et al., 2014]), the analysis of our algorithm also shows how to deal with nearly integral solutions, in which most inner products \u3008u\u0304, v\u0304\u3009 are only close to 0 or 1 (but may not be tight). We believe that many instances arising in practice have SDP solutions that are nearly integral, but not integral. Hence, we believe that in practice, our algorithm will work better than previously known algorithms."}, {"heading": "3 Polynomial-time Approximation Scheme", "text": "In this section, we present the analysis of our polynomial-time approximation scheme for correlation clustering, which we presented in Section 2. The PTAS works in a very general Adaptive Model, which we describe first."}, {"heading": "3.1 Adaptive Model", "text": "We study a more general \u201cadaptive\u201d semi-random model. A semi-random instance is generated as follows. We start with a graph G0(V,\u2205) on n vertices with no edges and a partition P\u2217 of V into disjoint sets, which we call the planted partition. The adversary adds edges one by one. We denote the edge chosen at step t by et and its cost c(et) \u2208 [0, 1]. After the adversary adds an edge et to the set of edges, the nature flips a coin and with probability \u03b5 adds e to the set of random edges ER. The next edge et+1 chosen by the adversary may depend on whether et belongs to ER or not. The adversary stops the semi-random process at a stopping time T . Thus, we obtain a graph G\u2217(V, {e1, . . . , eT }, c) and a set of random edges ER. We denote the set of all edges by E\u2217 = {e1, . . . , eT }. The adversary may remove some edges belonging to ER from the set E\u2217. Denote the set of the remaining edges by E. Note that E\u2217 \\ ER \u2282 E \u2282 E\u2217.\nOnce the graph G(V,E) and the set ER are generated, we perform steps 3 and 4 from the basic semirandom model for the graph G(V,E) and random set of edges ER \u2229 E (as described in Section 1.1). We obtain a semi-random instance. This is the instance the algorithm gets. Of course, the algorithm does not\nget the set of random edges ER. Note that the cost of the planted solution P\u2217 is at most the cost of the edges ER \u2229 E i.e. c ( ER \u2229 E )\n, since all edges in E \\ ER are consistent with P\u2217. This Adaptive Model is more general than the Basic Semi-random model we introduced earlier. The basic semi-random model corresponds to the case when the whole set of edges E\u2217 is fixed in advance independent of the random choices made in ER, and E = E\u2217. However, in the adaptive model the edge et can be chosen based on which of the edges e1, . . . et\u22121 belong to ER. For instance, the adversary can choose edge et from the portion of the graph where many of the previously chosen edges belong to ER."}, {"heading": "3.2 Analysis of the Algorithm", "text": "Now we analyze the algorithm presented in Section 2. We need to bound the number of edges removed at the first step (that is, edges (u, v) with f(u, v) > 1 \u2212 \u03b4) and the number of edges cut by the O(log n) approximation algorithm at the second step. The SDP contribution of every edge (u, v) removed at the first step is at least c(u, v)(1\u2212\u03b4). Thus the cost of edges removed at the first step is bounded by SDP/(1\u2212\u03b4) \u2264 (1 + 2\u03b4)OPT . To bound the cost of the solution produced by the approximation algorithm at the second step, we need to bound the cost of the optimal solution for the remaining instance i.e., the instance with the set of edges {(u, v) \u2208 E : f(u, v) \u2264 1\u2212 \u03b4}.\nFor any subset of edges F \u2282 E, let c(F ) represent the cost of the edges in F i.e. c(F ) = \u2211e\u2208F c(e). Denote E\u2217+ and E \u2217 \u2212: E \u2217 + = {(u, v) : P\u2217(u) = P\u2217(v)} and E\u2217\u2212 = {(u, v) : P\u2217(u) 6= P\u2217(v)}. Now define a function f\u2217(u, v), which slightly differs from f(u, v). For all (u, v) \u2208 E,\nf\u2217(u, v) =\n{\n1\u2212 \u3008u\u0304, v\u0304\u3009, if P\u2217(u) = P\u2217(v); \u3008u\u0304, v\u0304\u3009, if P\u2217(u) 6= P\u2217(v).\n(3)\nHere, P\u2217 is the planted partition. Note that P\u2217 and f\u2217(u, v) are not known to the algorithm. Observe that f(u, v) = f\u2217(u, v) if the edge (u, v) is consistent with the planted partition P\u2217, and f(u, v) = 1\u2212 f\u2217(u, v) otherwise. Our goal is to show that the algorithm removes all but very few edges inconsistent with P\u2217, i.e., edges (u, v) with f(u, v) = 1 \u2212 f\u2217(u, v). We prove the following theorem in Section 3.3. The proof relies on Theorem 4.3 presented in Section 4.\nTheorem 3.1. Let {G = (V,E, c), (E+, E\u2212)} be a semi-random instance of the correlation clustering problem. Let ER be the set of random edges, and P\u2217 be the planted partition. Denote by Q \u2282 ER the set of random edges not consistent with P\u2217. Then, for some universal constant C and every \u03b4, \u03b3 > 0, and for \u039b = C(1\u2212 2\u03b5)\u22122\u03b3\u22122\u03b4\u22123n log n,\nPr\n\n\n\u2211\n(u,v)\u2208Q:f(u,v)\u22641\u2212\u03b4\nc(u, v) \u2265 \u039b+ 6\u03b3 1\u2212 2\u03b5c(Q)\n\n = o(1).\nwhere f corresponds to any feasible SDP solution of cost at most OPT .\nRemark 3.1. In the statement of Theorem 3.1, c(Q) is the value of the solution given by the planted solution P\u2217. If OPT = c(Q), then the planted solution P\u2217 is indeed an optimal clustering. The function f(u, v) in the theorem that corresponds to the SDP contribution of edge (u, v) could come from any (not necessarily optimal) SDP solution of cost at most OPT . This will be useful in Lemma 3.2.\nLet D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al. [2006]. We apply Theorem 3.1 with \u03b3 = \u03b4(1\u22122\u03b5)6D . The cost of edges in {(u, v) \u2208 Q : f(u, v) \u2264 1 \u2212 \u03b4} is bounded\nby\n\u039b+ 6\u03b3\n1\u2212 2\u03b5c(Q) \u2264 \u039b+D \u22121\u03b4 c(Q), (4)\nw.h.p., where \u039b = O((1\u2212 2\u03b5)\u22124\u03b4\u22123n log3 n). Thus, after removing edges with f(u, v) \u2265 (1\u2212 \u03b4), the cost of the optimal solution is at most (4) w.h.p. The approximation algorithm finds a solution of cost at most D times (4). Thus, the total cost of the solution returned by the algorithm is at most\n(1 + 2\u03b4)OPT +D \u00d7 (\u039b +D\u22121\u03b4 \u00b7 c(Q)) = (1 + 3\u03b4)c(Q) +D\u039b = (1 + 3\u03b4)c(Q) +O((1\u2212 2\u03b5)\u22124\u03b4\u22123n log3 n).\nThe above argument shows that the solution has small cost compared to the cost of the planted solution P\u2217. We can in fact use Theorem 3.1 to give a true approximation i.e., compared to the cost of the optimal solution OPT . This follows from the following lower bound on OPT in terms of c(Q) for semi-random instances.\nLemma 3.2. In the notation of Theorem 3.1, with probability 1\u2212 o(1),\nc(Q) \u2264 (1 + 2\u03b4)OPT +O ( (1\u2212 2\u03b5)\u22124\u03b4\u22123n log3 n ) .\nProof. Let fOPT correspond to the \u201cintegral\u201d SDP solution corresponding to the optimal solution OPT . In this solution, fOPT (u, v) = 1 for positive edges (u, v) which are across different clusters and negative edges (u, v) which are in the same cluster. This SDP solution has cost OPT and satisfies the conditions of Theorem 3.1. Hence, w.h.p., c (Q \\ (Q \u2229OPT )) \u2264 \u03b4D \u00b7 c(Q) + \u039b. Hence,\nc(Q)\u2212OPT \u2264 \u03b4 D c(Q) + \u039b and OPT \u2265 (1\u2212 \u03b4 D ) \u00b7 c(Q)\u2212 \u039b.\nWe now conclude the analysis of the algorithm.\nProof of Theorem 1.1. From Theorem 3.1, we get the total cost of the solution is bounded by\n(1 + 2\u03b4)OPT +D \u00d7 (\u039b +D\u22121\u03b4 \u00b7 c(Q)) = (1 + 2\u03b4)OPT +D \u00d7 \u039b+ \u03b4 1\u2212 \u03b4/D \u00b7 (OPT + \u039b)\n\u2264 (1 + 4\u03b4)OPT + 2D\u039b = (1 + 4\u03b4)OPT +O((1\u2212 2\u03b5)\u22124\u03b4\u22123n log3 n).\nThis finishes the analysis of the algorithm."}, {"heading": "3.3 Structural Theorem \u2013 Proof of Theorem 3.1", "text": "We now prove the Structural Theorem (Theorem 3.1) assuming Theorem 4.3. In order to use Theorem 4.3, we need to prove that the set of all SDP solutions to our problem has a small epsilon net. We use the following lemma from Makarychev et al. [2013].\nLemma 3.3 (ITCS, Lemma 2.7). For every graph G = (V,E) on n vertices (V = {1, . . . , n}) with the average degree \u2206 = 2|E|/|V |, real M \u2265 1, and \u03b3 \u2208 (0, 1), there exists a set of matrices W of size at most |W| \u2264 exp(O(nM4 log\u2206\n2\u03b32 + n log n)) such that: for every collection of vectors L(1), . . . , L(n),\nR(1), . . . R(n) with \u2016L(u)\u2016 = M , \u2016R(v)\u2016 = M and \u3008L(u), R(v)\u3009 \u2208 [0, 1], there exists W \u2208 W satisfying for every (u, v) \u2208 E:\nwuv \u2264 \u3008L(u), R(v)\u3009 \u2264 wuv + \u03b3; wuv \u2208 [0, 1].\nBy letting G be the complete graph, M = 1, L(u) = R(u) = f(u), we get the following corollary.\nCorollary 3.4. For every \u03b3 \u2208 (0, 1), there exists a set of matrices W of size at most |W| \u2264 exp ( O(n\u03b3\u22122 log n) ) such that: For every collection of vectors {f(u)}, there exists W \u2208 W satisfying for every (u, v):\n|wuv \u2212 \u3008f(u), f(v)\u3009| \u2264 \u03b3.\nDefine f and f\u2217 as in (2) and (3). Recall, that the algorithm removes all edges (u, v) \u2208 E with f(u, v) \u2265 (1\u2212 \u03b3). We show that the number of edges inconsistent with the planted partition P\u2217 that are remain in the graph after the fist step of the algorithm is small with high probability.\nProof of Theorem 3.1. For (u, v) \u2208 E, let\nX(u,v) =\n{\n1, if (u, v) \u2208 ER; \u22121, otherwise.\nLet Q+ = ER and Q\u2212 = E\u2217 \\ ER. Then, Q \u2282 Q+. Observe, that f(u, v) = f\u2217(u, v) if (u, v) \u2208 E \\Q = Q\u2212 and f(u, v) = 1\u2212 f\u2217(u, v) if (u, v) \u2208 Q \u2282 Q+. The SDP value is upper bounded by the optimal value OPT , which in turn is at most c(Q). Write,\nSDP = \u2211\n(u,v)\u2208E\nc(u, v)f(u, v) = \u2211\n(u,v)\u2208E\\Q\nc(u, v)f\u2217(u, v) + \u2211\n(u,v)\u2208Q\nc(u, v)(1 \u2212 f\u2217(u, v)) \u2264 c(Q).\nTherefore, \u2211\n(u,v)\u2208E\\Q\nc(u, v)f\u2217(u, v) \u2264 c(Q)\u2212 \u2211\n(u,v)\u2208Q\nc(u, v)(1 \u2212 f\u2217(u, v)) = \u2211\n(u,v)\u2208Q\nc(u, v)f\u2217(u, v).\nWe rewrite this expression as follows, \u2211\n(u,v)\u2208Q\u222aQ\u2212\nX(u,v)c(u, v)f \u2217(u, v) \u2265 0. (5)\nSuppose that \u2211\n(u,v)\u2208Q:f(u,v)\u22641\u2212\u03b4\nc(u, v) \u2265 \u039b + 6\u03b3 1\u2212 2\u03b5c(Q).\nFor (u, v) \u2208 Q, f(u, v) = 1\u2212 f\u2217(u, v). Thus, {(u, v) \u2208 Q : f(u, v) \u2264 1\u2212 \u03b4} = {(u, v) \u2208 Q : f\u2217(u, v) \u2265 \u03b4}, and\n\u2211\n(u,v)\u2208Q\nc(u, v)f\u2217(u, v) \u2265 \u03b4\u039b + 6\u03b4\u03b3 1\u2212 2\u03b5c(Q). (6)\nBy Theorem 4.3 and Corollary 3.4, the probability that inequalities (5) and (6) hold is at most\n2 exp ( O(n\u03b3\u22122\u03b4\u22122 log n) ) exp ( \u2212 1/5(1\u2212 2\u03b5)2\u03b4\u039b ) = o(1),\nfor an appropriate choice of the constant C in the bound on \u039b."}, {"heading": "4 Betting with Stakes Depending on the Outcome", "text": "We first informally describe the theorem we prove in this section. Consider the following game. Assume that we are given a set of vectors W \u2282 [0, 1]m. At every step t, the player (adversary) picks an arbitrary not yet chosen coordinate et \u2208 {1, . . . ,m}, and the casino (nature) flips a coin such that with probability \u03b5 < 1/2, the player wins, and with probability (1 \u2212 \u03b5) > 1/2, the player looses. In the former case, we set Xt = 1; and in the latter case we set Xt = \u22121. At some point T \u2264 m the player stops the game. At that point, he picks a vector w \u2208 W and declares that at time t his stake was w(et) dollars. We stress that the vector w may depend on the outcomes Xt. Then, the player\u2019s payoff equals\nT \u2211\nt=1\nXtw(et).\nIf the player could pick an arbitrary w after the outcomes Xt are revealed, then clearly he could get a significant payoff by letting w(et) = 1, for Xt = 1, and w(et) = 0, otherwise. However, we assume that the set W of possible bets is relatively small. Then, we show that with high probability the payoff is negative unless the total amount of bets \u2211\nt w(et) is very small. The precise statement of the theorem (see below) is slightly more technical.\nThe main idea of the proof is that for any w \u2208 W fixed in advance, the player is expected to loose with high probability, since the coin is not fair (\u03b5 < 1/2), and thus the casino has an advantage. In fact, the probability that the player wins is exponentially small if the coordinates of w are sufficiently large. Now we union bound over all w\u2019s in W and conclude that with high probability for every w \u2208 W , the player\u2019s payoff is negative.\nWhen we apply this theorem to a semi-random instance of Correlation Clustering (with unit costs i.e. c(et) = 1), the stakes are defined by the solution of the SDP: for an edge et = (u, v), w(et) = f\u2217(u, v). Loosely speaking, we show that since the SDP value is at most OPT , the game is profitable for the adversary. This implies that most stakes f\u2217(u, v) are close to 0. Now, if an edge (u, v) is consistent with the planted partition P\u2217, then f(u, v) = f\u2217(u, v) \u2248 0, and hence we do not remove this edge. On the other hand, if the edge is not consistent with the planted partition, then f(u, v) = 1\u2212 f\u2217(u, v) \u2248 1, hence we remove the edge.\nLemma 4.1. LetW \u2282 [0, 1]m be a set of vectors. Consider a stochastic process (e1,X1, c1), . . . , (eT ,XT , cT ). Each et \u2208 {1, . . . ,m} \\ {e1, . . . , et\u22121}, Xt \u2208 {\u00b11}, ct \u2208 [0, 1]. Let Ft be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct), and F \u2032t be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct) and (et+1, ct+1). The random variable T \u2208 {1, . . . ,m} is a stopping time w.r.t. Ft. Each Xt is a Bernoulli random variable independent of F \u2032t\u22121.\nXt =\n{\n1, with probability \u03b5;\n\u22121, with probability 1\u2212 \u03b5;\nwhere \u03b5 < 1/2. Then, for all \u039b > 3(1\u2212 2\u03b5)\u22122,\nPr ( \u2203w \u2208 W s.t. T \u2211\nt=1\nXtw(et)ct + 1\u2212 2\u03b5\n2\nT \u2211\nt=1\nw(et)ct \u2265 0 and T \u2211\nt=1\nw(et)ct \u2265 \u039b ) \u2264\n\u2264 2|W|e\u22121/5(1\u22122\u03b5)2\u039b. (7)\nProof. To prove the desired upper bound (7), we estimate the probability that \u2211T t=1 Xtw(et)ct+ 1\u22122\u03b5 2 \u2211T t=1 w(et)ct \u2265 0 and \u2211T\nt=1 w(et)ct \u2208 [\u039b\u2032, 2\u039b\u2032] for a fixed w \u2208 W and \u039b\u2032 \u2265 \u039b. Then we apply the union bound for all w \u2208 W , and \u039b\u2032 of the form 2i\u039b.\nFix a w \u2208 W and \u039b\u2032 = 2i. Each Xt+1 is independent of F \u2032t , hence E[Xt+1w(et+1)ct+1 | F \u2032t ] = E[Xt+1]w(et+1)ct+1 = (2\u03b5\u2212 1)w(et+1)ct+1. Thus,\nS\u03c4 \u2261 \u03c4 \u2211\nt=1\n(Xt + 1\u2212 2\u03b5)w(et)ct\nis a martingale. Note that |St+1 \u2212 St| \u2264 w(et+1)ct+1 \u2264 ct+1 and\nVar[Xt+1w(et+1ct+1) | F \u2032t] = 4\u03b5(1 \u2212 \u03b5)w(et+1)2c2t+1 \u2264 4\u03b5(1 \u2212 \u03b5)w(et+1)ct+1.\nIf \u2211T t=1 Xtw(et)ct + 1\u22122\u03b5 2 \u2211T t=1 w(et)ct \u2265 0 and \u2211T t=1 w(et)ct \u2208 [\u039b\u2032, 2\u039b\u2032], then\nST =\n[\nT \u2211\nt=1\nXtw(et)ct + (1\u2212 2\u03b5)\n2\nT \u2211\nt=1\nw(et)ct\n]\n+ (1\u2212 2\u03b5)\n2\nT \u2211\nt=1\nw(et)ct \u2265 (1\u2212 2\u03b5)\n2 \u039b\u2032,\nand T \u2211\nt=1\nVar[Xtw(et)ct | F \u2032t\u22121] = 4(\u03b5\u2212 \u03b52) T \u2211\nt=1\nw(et)ct \u2264 8\u03b5(1\u2212 \u03b5)\u039b\u2032.\nNow, by Freedman\u2019s inequality (see Freedman [1975]),\nPr ( ST \u2265 (1\u2212 2\u03b5)\u039b\u2032 and T \u2211\nt=1\nVar[Xtw(et)ct | Ft\u22121] \u2264 8\u03b5(1 \u2212 \u03b5)\u039b\u2032 ) \u2264 e\u2212 (1\u22122\u03b5)2\u039b\u20322 2((1\u22122\u03b5)\u039b\u2032+8\u03b5(1\u2212\u03b5)\u039b\u2032)\n= e\u2212 (1\u22122\u03b5)2\u039b\u2032 5 ,\nand\nPr (\nT \u2211\nt=1\nXtw(et)ct \u2265 0 and T \u2211\nt=1\nw(et)ct \u2208 [\u039b\u2032, 2\u039b\u2032] ) \u2264 Pr ( ST \u2265 (1\u2212 2\u03b5)\u039b\u2032 and T \u2211\nt=1\nw(et) 2c2t \u2264 2\u039b\u2032\n)\n\u2264 e\u22121/5 (1\u22122\u03b5)2\u039b\u2032 = ( e\u2212 1/5 (1\u22122\u03b5)2\u039b)2 i .\nSumming up this upper bound over all w \u2208 W and \u039b\u2032 = 2i\u039b, we get (7).\nWe now slightly generalize this theorem. In our application, the set of all possible stakes can be infinite, however, we know that there is a relatively small epsilon net for it.\nDefinition 4.2. We say that a set W \u2282 Rm is a \u03b3\u2013net for a set Z \u2282 Rm in the \u2113\u221e norm, if for every z \u2208 Z , there exists w \u2208 W such that \u2016z \u2212 w\u2016\u221e \u2261 maxi{|z(i) \u2212 w(i)|} \u2264 \u03b3.\nRemark 4.1. If W is a \u03b3\u2013net for Z \u2282 [0, 1]m, then there exists W \u2032 \u2282 [0, 1]m of the same size as W (|W \u2032| = |W|), such that for every z \u2208 Z , there exists w\u2032 \u2208 W \u2032 satisfying w\u2032(i) \u2264 z(i) \u2264 w\u2032(i) + 2\u03b3 for all i. To obtain W \u2032 we simply subtract min(\u03b3,w(i)) from each coordinate of w and then truncate each w\u2032(i) at the threshold of 1.\nTheorem 4.3. Consider a stochastic process (e1,X1, c1), . . . , (eT ,XT , cT ) such that each et \u2208 {1, . . . ,m}\\ {e1, . . . , et\u22121}, Xt \u2208 {\u00b11} and ct \u2208 [0, 1]. Let Ft be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct), and F \u2032t be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct) and (et+1, ct+1). The random variable T \u2208 {1, . . . ,m} is a stopping time w.r.t. Ft. Each Xt is a Bernoulli random variable independent of F \u2032t\u22121.\nXt =\n{\n1, with probability \u03b5;\n\u22121, with probability 1\u2212 \u03b5;\nwhere \u03b5 < 1/2. Let Z \u2282 [0, 1]m be a set of vectors having a \u03b3\u2013net in the L\u221e norm of size N . Define two random sets depending on {Xt}:"}, {"heading": "Q+ = {t : Xt = 1} and Q\u2212 = {t : Xt = \u22121}.", "text": "Then, for all \u039b > 3(1 \u2212 2\u03b5)2, we have\nPr ( \u2203z \u2208 Z, Q\u2295 \u2282 Q+ s.t. \u2211\nt\u2208Q\u2295\u222aQ\u2212\nXtz(et)ct \u2265 0\nand \u2211\nt\u2208Q\u2295\nz(et)ct \u2265 \u039b + 6\u03b3 1\u2212 2\u03b5 \u2211\nt\u2208Q\u2295\nct\n)\n\u2264 2Ne\u22121/5(1\u22122\u03b5)2\u039b. (8)\nProof. Let W be a \u03b3\u2013net for Z . For simplicity of exposition we subtract min(\u03b3,w(i)) from all coordinates of vectors w \u2208 W . Thus, we assume that for all z \u2208 Z , there exists w \u2208 W such that w(i) \u2264 z(i) \u2264 w(i) + 2\u03b3 and w(i) \u2265 0 for all i (see Remark 4.1).\nSuppose that for some z \u2208 Z and Q\u2295 \u2282 Q+, the inequalities \u2211\nt\u2208Q\u2295\u222aQ\u2212\nXtz(et)ct \u2265 0 (9)\nand \u2211\nt\u2208Q\u2295\nz(et)ct \u2265 \u039b+ 6\u03b3 1\u2212 2\u03b5 \u2211\nt\u2208Q\u2295\nct (10)\nhold. Pick a w \u2208 W , such that w(i) \u2264 z(i) \u2264 w(i) + 2\u03b3 for all i. We replace z(et) with w(et) in (10): \u2211\nt\u2208Q\u2295\nw(et)ct \u2265 \u2211\nt\u2208Q\u2295\n(z(et)\u2212 2\u03b3)ct \u2265 \u039b+ 4\u03b3 (1\u2212 2\u03b5) \u00b7 \u2211\nt\u2208Q\u2295\nct. (11)\nThen,\nT \u2211\nt=1\nXtw(et)ct + 1\u2212 2\u03b5\n2\nT \u2211\nt=1\nw(et)ct \u2265 \u2211\nt\u2208Q\u2295\u222aQ\u2212\nXtw(et)ct + 1\u2212 2\u03b5\n2\n\u2211\nt\u2208Q\u2295\nw(et)ct (12)\n\u2265\n\n\n\u2211\nt\u2208Q\u2295\n(z(et)\u2212 2\u03b3)ct \u2212 \u2211\nQ\u2212\nz(et)ct\n\n+ 2\u03b3 \u2211\nt\u2208Q\u2295\nct\n= \u2211\nt\u2208Q\u2295\u222aQ\u2212\nXtz(et)ct \u2265 0.\nBy Lemma 4.1, there exists a w \u2208 W satisfying (11) and (12) with probability at most 2Ne\u22121/5(1\u22122\u03b5)2\u039b. This concludes the proof."}, {"heading": "5 Recovery Algorithm", "text": "In this section, we prove Theorem 1.2 that shows that under some additional assumptions on the graph G and partition P\u2217, we can recover the planted partition P\u2217 with an arbitrarily small classification error \u03b7. The recovery algorithm is a very fast and very simple greedy algorithm (presented in Section 2).\nAssumptions 5.1. Consider a semi-random instance I = {G = (V,E, c), (E+, E\u2212)}. Let P\u2217 be the planted partition. Denote the clusters of G w.r.t clustering P\u2217 by P \u22171 , . . . , P \u2217k . Let \u03b2 = c(E\u2217+)/c(E) (note that E\u2217+ is the set of edges that lie within clusters) and \u03b2ij = c({(u, v) : u \u2208 P \u2217i , v \u2208 P \u2217j })/c(E) (here, {(u, v) : u \u2208 P \u2217i , v \u2208 P \u2217j } is the set of edges between clusters P \u2217i and P \u2217j ). Assume that the instance I satisfies the following conditions:\n\u2022 Cluster Expansion. All induced graphs G[P \u2217i ] are spectral expanders with spectral expansion at least \u03bbgap; that is, the second smallest eigenvalue of the normalized Laplacian of G[P \u2217i ] is at least \u03bbgap. \u2022 Intercluster Density. For some sufficiently large constant C1, and every two clusters P \u2217i and P \u2217j ,\n\u03b2ij > C1\n(1\u22122\u03b5)2\n(\nn logn c(E)\n)1/6 .\n\u2022 Intercluster Regularity. The set of edges between every two clusters P \u2217i and P \u2217j forms a regular graph with respect to the cost function c: for every u\u2032, u\u2032\u2032 \u2208 P \u2217i we have c({(u\u2032, v) : v \u2208 P \u2217j }) = c({(u\u2032\u2032, v) : v \u2208 P \u2217j }). \u2022 Cluster Regularity. All induced graphs G[P \u2217i ] are regular graphs with the same degree w.r.t to the cost function c. That is, for some number c0, every cluster P \u2217i , and every vertex u \u2208 P \u2217i , c0 = c({(u, v) \u2208 E : v \u2208 P \u2217i }).\nRemark The Intercluster and Cluster Regularity assumptions can be significantly relaxed; in fact, we only need that degrees are equal up to some multiplicative factor (say, a poly-log factor). We include the regularity assumptions to simplify the exposition.\nDefinition 5.2. Let I = {G = (V,E, c), (E+, E\u2212)} be a semi-random instance of correlation clustering, P\u2217 be the planted partition, and P \u22171 , . . . , P \u2217k be the planted clusters. We say that a partition P of V into clusters P1, . . . , Pt has an \u03b7 classification error if there is a partial matching between clusters P \u22171 , . . . , P \u2217 k and clusters P1, . . . , Pt such that\n\u2211\nP \u2217i is matched with Pj\n|P \u2217i \u2229 Pj | \u2265 (1\u2212 \u03b7)|V |.\nTheorem 1.2 relies on the following theorem that describes the structure of optimal SDP solutions to semi-random instances of correlation clustering that satisfy conditions in Assumption 5.1.\nTheorem 5.3. Assume that a semi-random instance I = {G = (V,E, c), (E+, E\u2212)} satisfies Assumptions 5.1. Let {u\u0304} be the optimal SDP solution to I . With probability 1 \u2212 o(1), there exist a subset of clusters C \u2282 {P \u22171 , . . . , P \u2217k } and a vertex ui in each cluster P \u2217i satisfying the following properties. Let \u03c1core = 1/10 and \u03c1inter = 4/5. Let core(P \u2217i ) = {v \u2208 P \u2217i : \u2016v\u0304 \u2212 u\u0304i\u2016 \u2264 \u03c1core} for P \u2217i \u2208 C, then\n1. | \u222aP \u2217i \u2208C P \u2217 i | \u2265 (1\u2212 \u03b7)|V |.\n2. | core(P \u2217i )| \u2265 (1\u2212 \u03b7)|P \u2217i |.\n3. In particular, \u2211 P \u2217i \u2208C |Pi| \u2265 (1\u2212 \u03b7)2|V |.\n4. \u2016u\u0304i \u2212 u\u0304j\u2016 \u2265 \u03c1inter for every two distinct clusters P \u2217i , P \u2217j \u2208 C.\nWe now use Theorem 5.3 to prove the recovery guarantees of our algorithm.\nProof of Theorem 1.2. Consider a cluster Pi. Let u be the vertex we choose at iteration i of the while\u2013loop. If Pi intersects a core core(P \u2217j ) of a cluster P \u2217 j then \u2016u\u2212 uj\u2016 \u2264 2\u03c1core. Note that Pi cannot intersect cores core(P \u2217j\u2032) and core(P \u2217 j\u2032\u2032) of two distinct clusters P \u2217 j\u2032 and P \u2217 j\u2032\u2032 , since \u2016u\u2212 uj\u2032\u2016+ \u2016u\u2212uj\u2032\u2032\u2016 \u2265 \u2016uj\u2032 \u2212 uj\u2032\u2016 \u2265 \u03c1inter > 4\u03c1core. Thus each cluster Pi intersects at most the core of one cluster P \u2217j . We match every cluster P \u2217j \u2208 C to the first cluster Pi that intersects core(P \u2217j ). Consider a cluster P \u2217j \u2208 C and the matching cluster Pi. Since core(P \u2217j ) \u2229 (P1 \u222a \u00b7 \u00b7 \u00b7 \u222a Pi\u22121) = \u2205, we have, in particular, that uj /\u2208 P1 \u222a \u00b7 \u00b7 \u00b7 \u222a Pi\u22121 and uj has degree at least | core(P \u2217j )| in Gaux[V \\ (P1 \u222a . . . Pi\u22121)]. Thus the vertex u that we choose at iteration i has degree at least | core(P \u2217j )| and |Pi| \u2265 | core(P \u2217j )|; in particular, |Pi \\ core(P \u2217j )| \u2265 | core(P \u2217j ) \\ Pi|. We have,\n|P \u2217j \u2229 Pi| \u2265 | core(P \u2217j ) \u2229 Pi| = | core(P \u2217j )| \u2212 | core(P \u2217j ) \\ Pi| \u2265 | core(P \u2217j )| \u2212 |Pi \\ core(P \u2217j )|. Note that by Theorem 5.3 (item 3)\n\u2211\nPi is matched with P \u2217j\n|Pi \\ core(P \u2217j )| \u2264 |V | \u2212 | \u22c3\nP \u2217j \u2208C\ncore(P \u2217j )| \u2264 n\u2212 (1\u2212 \u03b7)2n \u2264 2\u03b7n.\nTherefore, \u2211\nPi is matched with P \u2217j\n|Pi \u2229 P \u2217j | \u2265 ( \u2211\nP \u2217j \u2208C\n| core(P \u2217j )| ) \u2212 2\u03b7n \u2265 (1\u2212 \u03b7)2n\u2212 2\u03b7 \u2265 (1\u2212 4\u03b7)n.\nWe proved that the algorithm finds a clustering with classification error at most 4\u03b7."}, {"heading": "5.1 Structure of the optimal SDP solution: Proof of Theorem 5.3", "text": "We now prove Theorem 5.3 which gives the structure of optimal SDP solutions to semi-random instances of correlation clustering that satisfy conditions in Assumption 5.1. As seen earlier, completing this proof concludes the proof of Theorem 1.2.\nLet \u03b4 = \u03b3 = (n log n/c(E))1/6. Let \u039b and Q be as in Theorem 3.1. Let \u03c3 = 6\u03b4/(1 \u2212 2\u03b5). Note that \u039b = O(\u03b4/(1 \u2212 2\u03b5)2).\nDefine f as in (2):\nf(u, v) =\n{\n1\u2212 \u3008u\u0304, v\u0304\u3009, if (u, v) \u2208 E+; \u3008u\u0304, v\u0304\u3009, if (u, v) \u2208 E\u2212.\n(13)\nConsider the set of edges Eflip = {(u, v) \u2208 E : f(u, v) > 1\u2212 \u03b4}. Change the sign of each edge in Eflip and obtain a new partitioning of E into positive and negative edges, E\u0302+ and E\u0302\u2212:\nE\u0302+ = E+\u25b3Eflip = {(u, v) \u2208 E+ : f(u, v) \u2264 1\u2212 \u03b4} \u222a {(u, v) \u2208 E\u2212 : f(u, v) > 1\u2212 \u03b4} , E\u0302\u2212 = E\u2212\u25b3Eflip = {(u, v) \u2208 E\u2212 : f(u, v) \u2264 1\u2212 \u03b4} \u222a {(u, v) \u2208 E+ : f(u, v) > 1\u2212 \u03b4} .\nLet us now consider the corresponding instance I\u0302 = { G = (V,E, c), (E\u0302+, E\u0302\u2212) } . Let f\u0302 be the analog of\nfunction f for I\u0302:\nf\u0302(u, v) =\n{\n1\u2212 \u3008u\u0304, v\u0304\u3009, if (u, v) \u2208 E\u0302+ \u3008u\u0304, v\u0304\u3009, if (u, v) \u2208 E\u0302\u2212\n=\n{\nf(u, v), if (u, v) /\u2208 Eflip; 1\u2212 f(u, v), if (u, v) \u2208 Eflip.\n(14)\nSimilarly, let S\u0302DP = \u2211 (u,v)\u2208E c(u, v)f\u0302 (u, v) be the cost of the SDP solution {u\u0304} for I\u0302.\nLemma 5.4. With probability 1\u2212 o(1), the following properties hold.\n1. c(Q \\ Eflip) \u2264 \u03c3c(Q) + \u039b.\n2. c(Eflip \\Q) \u2264 (2\u03b4 + \u03c3)c(Q) + \u039b.\n3. Then S\u0302DP \u2264 (2\u03b4 + \u03c3)c(Q) + \u039b.\nProof. 1. From Theorem 3.1, we get that c(Q \\ Eflip) \u2264 \u03c3c(Q) + \u039b with probability 1\u2212 o(1). 2. Write c(Eflip \\Q) = c(Eflip)\u2212 c(Q \u2229Eflip). Now we bound c(Eflip) and c(Q \u2229Eflip). Note that\nSDP = \u2211\n(u,v)\u2208E\nc(u, v)f(u, v) \u2265 \u2211\n(u,v)\u2208Eflip\nc(u, v)(1 \u2212 \u03b4) = (1\u2212 \u03b4)c(Eflip).\nHence, c(Eflip) \u2264 SDP/(1\u2212 \u03b4) \u2264 c(Q)/(1 \u2212 \u03b4) \u2264 (1 + 2\u03b4)c(Q),\nhere, we used that {u\u0304} is an optimal SDP solution and therefore SDP \u2264 c(Q). By item 1, c(Q \u2229 Eflip) = c(Q)\u2212 c(Q \\ Eflip) \u2265 (1\u2212 \u03c3)c(Q) \u2212 \u039b. We get that\nc(Eflip \\Q) \u2264 (1 + 2\u03b4)c(Q) \u2212 (1\u2212 \u03c3)c(Q) \u2212 \u039b = (2\u03b4 + \u03c3)c(Q) + \u039b.\n3. From the second formula for f\u0302(u, v) in (14), we get that f(u, v)\u2212 f\u0302(u, v) = 2f(u, v) \u2212 1 \u2265 1\u2212 2\u03b4 for (u, v) \u2208 Eflip, and f(u, v)\u2212 f\u0302(u, v) = 0 for (u, v) /\u2208 Eflip. Therefore,\nc(Q)\u2212 S\u0302DP \u2265 SDP \u2212 S\u0302DP = \u2211\n(u,v)\u2208E\nc(u, v)(f(u, v) \u2212 f\u0302(u, v))\n= \u2211\n(u,v)\u2208Eflip\nc(u, v)(f(u, v) \u2212 f\u0302(u, v)) \u2265 (1\u2212 2\u03b4)c(Eflip) \u2265 (1\u2212 2\u03b4)c(Q \u2229 Eflip)\n\u2265 (1\u2212 2\u03b4)((1 \u2212 \u03c3)c(Q)\u2212 \u039b) \u2265 (1\u2212 2\u03b4 \u2212 \u03c3)c(Q) \u2212 \u039b.\nTherefore, S\u0302DP \u2264 (2\u03b4 + \u03c3)c(Q) + \u039b.\nWe now bound the total squared Euclidean length of all edges in E\u2217+.\nLemma 5.5. With probability 1\u2212 o(1), we have\n1\n2\n\u2211\n(u,v)\u2208E\u2217+\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 (4\u03b4 + 3\u03c3)c(Q) + 3\u039b\nProof. Note that for (u, v) \u2208 E\u0302+, 12\u2016u\u0304\u2212 v\u0304\u20162 = f\u0302(u, v) and thus\n1\n2\n\u2211\n(u,v)\u2208E\u0302+\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 S\u0302DP .\nAlso, E\u2217+\u2229E\u0302\u2212 \u2282 (Q\\Eflip)\u222a(Eflip \\Q). Thus, by Lemma 5.4, c(E\u2217+\u2229E\u0302\u2212) \u2264 c(Q\\Eflip)+c(Eflip \\Q) \u2264 2(\u03b4 + \u03c3)c(Q) + 2\u039b. We have,\n1\n2\n\u2211\n(u,v)\u2208E\u2217+\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 1 2\n\u2211\n(u,v)\u2208E\u2217+\u2229E\u0302+\nc(u, v)\u2016u \u2212 v\u20162 + 1 2\n\u2211\n(u,v)\u2208E\u2217+\u2229E\u0302\u2212\nc(u, v)\u2016u \u2212 v\u20162\n\u2264 S\u0302DP + c(E\u2217+ \u2229 E\u0302\u2212) = (4\u03b4 + 3\u03c3)c(Q) + 3\u039b.\nWe are ready to prove Theorem 5.3.\nProof of Theorem 5.3. We assume that \u03b7 < 1/4 as otherwise the statement of theorem is trivial. Let\n\u03c12avg = 1\nc(E\u2217+)\n\u2211\n(u,v)\u2208E\u2217+\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 O(\u03c3c(Q) + \u039b)/c(E\u2217+) \u2264 O(\u03c3 + \u039b/c(E))/\u03b2.\nLet E\u2217+(i) = { (u, v) \u2208 E\u2217+ : u, v \u2208 P \u2217i } be the set of edges within cluster P \u2217i . Write\nk \u2211\ni=1\n\u2211\n(u,v)\u2208E\u2217+(i)\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 = c(E\u2217+)\u03c12avg.\nLet C be the set of clusters P \u2217i such that \u2211 (u,v)\u2208E\u2217+(i) c(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 c(E\u2217+(i))\u03c12avg/\u03b7. By Markov\u2019s inequality, \u2211\nP \u2217i \u2208C c(E\u2217+(i)) \u2265 (1 \u2212 \u03b7)c(E\u2217+). By the Cluster Regularity condition in Assumptions 5.1,\nc(E\u2217+(i)) = (|P \u2217i |/n)c(E\u2217+). We get that \u2211 P \u2217i \u2208C |Pi| \u2265 (1\u2212\u03b7)n and item 1 in the statement of the theorem holds. By the Poincare\u0301 inequality3 we have for each cluster P \u2217i \u2208 C,\n1\n|P \u2217i |2 \u2211\nu,v\u2208P \u2217i\n\u2016u\u0304\u2212 v\u0304\u20162 \u2264 1 \u03bbgap\n1\nc(E\u2217+(i))\n\u2211\n(u,v)\u2208E\u2217+(i)\nc(u, v)\u2016u\u0304 \u2212 v\u0304\u20162 \u2264 \u03c12avg \u03bbgap\u03b7 .\nTherefore,\nmin u\u2208P \u2217i\n\n\n1\n|P \u2217i | \u2211\nv\u2208P \u2217i\n\u2016u\u0304\u2212 v\u0304\u20162 \n \u2264 1|P \u2217i | \u2211\nu\u2208P \u2217i\n\n\n1\n|P \u2217i | \u2211\nv\u2208P \u2217i\n\u2016u\u0304\u2212 v\u0304\u20162 \n \u2264 \u03c12avg \u03bbgap\u03b7 .\nThus we can choose ui in each P \u2217i \u2208 C such that 1|P \u2217i | \u2211 v\u2208P \u2217 i \u2016u\u0304i \u2212 v\u0304\u20162 \u2264 \u03c12avg \u03bbgap\u03b7 . This choice of vertices ui defines sets core(P \u2217i ), as in the statement of the theorem. Using again Markov\u2019s inequality, we get that for at least a 1 \u2212 \u03b7 fraction of vertices v in P \u2217i , \u2016u\u0304i \u2212 v\u0304\u20162 \u2264 \u03c12avg/(\u03bbgap\u03b72). From the bound \u03c12avg = O(\u03c3 + \u039b/c(E))/\u03b2 and formula 1, we get \u03c12core \u2265 \u03c12avg/(\u03bbgap\u03b72) and\n| core(P \u2217i )| \u2265 | { v \u2208 P \u2217i : \u2016u\u0304i \u2212 v\u0304\u20162 \u2264 \u03c12avg/(\u03bbgap\u03b72) } | \u2265 (1\u2212 \u03b7)|P \u2217i |. 3Recall that the Poincare\u0301 inequality states that for every every expander graph H = (VH , EH , cH) with spectral expansion \u03bb\nand every set of vectors {u\u0304}u\u2208VH , we have 1\n|VH | 2\n\u2211 u,v\u2208V \u2016u\u0304\u2212 v\u0304\u2016 2 \u2264 1 \u03bb\u00b7cH(EH ) \u2211 (u,v)\u2208EH cH(u, v)\u2016u\u0304\u2212 v\u0304\u2016 2. Here, we apply\nthe Poincare\u0301 inequality to the induced graph G[P \u2217i ]\nWe showed that item 2 in the statement of the theorem holds. We get item 3 from items 1 and 2. Finally, we show that \u2016u\u0304i \u2212 u\u0304j\u2016 \u2265 \u03c1inter for every two distinct clusters P \u2217i , P \u2217j \u2208 C. To this end, we show that there are vertices v\u2032 \u2208 core(P \u2217i ) and v\u2032\u2032 \u2208 core(P \u2217j ) such that \u2016v\u0304\u2032 \u2212 v\u0304\u2032\u2032\u2016 \u2265 \u03c1inter + 2\u03c1core, and thus \u2016u\u0304i \u2212 u\u0304j\u2016 \u2265 (\u03c1inter + 2\u03c1core) \u2212 \u2016ui \u2212 v\u2032\u2016 \u2212 \u2016uj \u2212 v\u2032\u2032\u2016 \u2265 \u03c1inter. Assume to the contrary that \u2016v\u0304\u2032 \u2212 v\u0304\u2032\u2032\u2016 < \u03c1inter + 2\u03c1core for every v\u2032 \u2208 core(P \u2217i ) and v\u2032\u2032 \u2208 core(P \u2217j ). Let Eij = {\n(v\u2032, v\u2032\u2032) \u2208 E : v\u2032 \u2208 core(P \u2217i ), v\u2032\u2032 \u2208 core(P \u2217j ) } .\nSince Eij \u2282 E\u2217\u2212, we have for every (v\u2032, v\u2032\u2032) \u2208 Eij \\ (Q\u25b3Eflip),\nf\u0302(v\u2032, v\u2032\u2032) = \u3008v\u0304\u2032, v\u0304\u2032\u2032\u3009 = 1\u2212 \u2016v\u0304\u2032 \u2212 v\u0304\u2032\u2032\u20162/2 \u2265 1\u2212 (\u03c1inter + 2\u03c1core)2/2 = 1/2.\nTherefore, S\u0302DP \u2265\n\u2211\n(v\u2032,v\u2032\u2032)\u2208Eij\\(Q\u25b3Eflip)\nc(v\u2032, v\u2032\u2032)f(v\u2032, v\u2032\u2032) \u2265 c(Eij \\ (Q\u25b3Eflip))/2.\nFrom the Intercluster Regularity condition and bounds | core(P \u2217i )| \u2265 (1 \u2212 \u03b7)|P \u2217i | and | core(P \u2217j )| \u2265 (1 \u2212 \u03b7)|P \u2217j |, we get c(Eij) \u2265 (1\u2212 2\u03b7)\u03b2ijc(E) . By Lemma 5.4,\nc(Q\u25b3Eflip) \u2264 2(\u03b4 + \u03c3)c(Q) + 2\u039b \u2264 2(\u03b4 + \u03c3)c(E) + 2\u039b.\nBy the Intercluster Density condition in Assumptions 5.1 and our choice of \u03b4, we have\nc(Eij \\ (Q\u25b3Eflip)) \u2265 ((1\u2212 2\u03b7)\u03b2ij \u2212 2\u03b4 \u2212 2\u03c3)c(E) \u2212 2\u039b \u2265 \u03b2ijc(E)/3.\nWe get that (2\u03b4 + \u03c3)c(Q) + \u039b \u2265 S\u0302DP \u2265 \u03b2ijc(E)/6,\nwhich contradicts to the Intercluster Density condition and our choice of \u03b4."}], "references": [{"title": "Aggregating inconsistent information: Ranking and clustering", "author": ["Nir Ailon", "Moses Charikar", "Alantha Newman"], "venue": "J. ACM,", "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "An automated method for finding molecular complexes in large protein interaction networks", "author": ["G D Bader", "C W Hogue"], "venue": "BMC Bioinformatics,", "citeRegEx": "Bader and Hogue.,? \\Q2003\\E", "shortCiteRegEx": "Bader and Hogue.", "year": 2003}, {"title": "Finding low error clusterings", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Balcan and Braverman.,? \\Q2009\\E", "shortCiteRegEx": "Balcan and Braverman.", "year": 2009}, {"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla"], "venue": "Machine Learning,", "citeRegEx": "Bansal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2004}, {"title": "Clustering gene expression patterns", "author": ["Amir Ben-Dor", "Ron Shamir", "Zohar Yakhini"], "venue": "Journal of Computational Biology,", "citeRegEx": "Ben.Dor et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ben.Dor et al\\.", "year": 1999}, {"title": "Coloring random and semi-random k-colorable graphs", "author": ["Avrim Blum", "Joel Spencer"], "venue": "J. Algorithms,", "citeRegEx": "Blum and Spencer.,? \\Q1995\\E", "shortCiteRegEx": "Blum and Spencer.", "year": 1995}, {"title": "Clustering with qualitative information", "author": ["Moses Charikar", "Venkatesan Guruswami", "Anthony Wirth"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Charikar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2005}, {"title": "Near optimal LP rounding algorithm for correlation clustering on complete and complete k-partite graphs", "author": ["Shuchi Chawla", "Konstantin Makarychev", "Tselil Schramm", "Grigory Yaroslavtsev"], "venue": "CoRR, abs/1412.0681,", "citeRegEx": "Chawla et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2014}, {"title": "Clustering sparse graphs", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Huan Xu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Correlation clustering in general weighted graphs", "author": ["Erik D. Demaine", "Dotan Emanuel", "Amos Fiat", "Nicole Immorlica"], "venue": "Theoretical Computer Science,", "citeRegEx": "Demaine et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demaine et al\\.", "year": 2006}, {"title": "Bounding and comparing methods for correlation clustering beyond ilp", "author": ["Micha Elsner", "Warren Schudy"], "venue": "In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,", "citeRegEx": "Elsner and Schudy.,? \\Q2009\\E", "shortCiteRegEx": "Elsner and Schudy.", "year": 2009}, {"title": "Heuristics for finding large independent sets, with applications to coloring semirandom graphs", "author": ["Uriel Feige", "Joe Kilian"], "venue": "In Proceedings of Symposium on Foundations of Computer Science,", "citeRegEx": "Feige and Kilian.,? \\Q1998\\E", "shortCiteRegEx": "Feige and Kilian.", "year": 1998}, {"title": "Finding and certifying a large hidden clique in a semirandom graph", "author": ["Uriel Feige", "Robert Krauthgamer"], "venue": "Random Struct. Algorithms,", "citeRegEx": "Feige and Krauthgamer.,? \\Q2000\\E", "shortCiteRegEx": "Feige and Krauthgamer.", "year": 2000}, {"title": "Community detection in graphs", "author": ["Santo Fortunato"], "venue": "Physics Reports,", "citeRegEx": "Fortunato.,? \\Q2010\\E", "shortCiteRegEx": "Fortunato.", "year": 2010}, {"title": "On tail probabilities for martingales", "author": ["David A Freedman"], "venue": "The Annals of Probability,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Approximate max-flow min-(multi)cut theorems and their applications", "author": ["Naveen Garg", "Vijay V. Vazirani", "Mihalis Yannakakis"], "venue": "In Proceedings of Symposium on Theory of Computing,", "citeRegEx": "Garg et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garg et al\\.", "year": 1993}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Girvan and Newman.,? \\Q2002\\E", "shortCiteRegEx": "Girvan and Newman.", "year": 2002}, {"title": "Tight error bounds for structured prediction", "author": ["Amir Globerson", "Tim Roughgarden", "David Sontag", "Cafer Yildirim"], "venue": "CoRR, abs/1409.5834,", "citeRegEx": "Globerson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2014}, {"title": "Stochastic blockmodels and community structure in networks", "author": ["Brian Karrer", "M.E.J. Newman"], "venue": "Phys. Rev. E,", "citeRegEx": "Karrer and Newman.,? \\Q2011\\E", "shortCiteRegEx": "Karrer and Newman.", "year": 2011}, {"title": "How to play unique games against a semi-random adversary: Study of semi-random models of unique games", "author": ["Alexandra Kolla", "Konstantin Makarychev", "Yury Makarychev"], "venue": "In Proceeding of Symposium on Foundations of Computer Science,", "citeRegEx": "Kolla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kolla et al\\.", "year": 2011}, {"title": "Trawling the web for emerging cyber-communities", "author": ["Ravi Kumar", "Prabhakar Raghavan", "Sridhar Rajagopalan", "Andrew Tomkins"], "venue": "In Computer Networks,", "citeRegEx": "Kumar et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 1999}, {"title": "Approximation algorithms for semi-random partitioning problems", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of Symposium on Theory of Computing,", "citeRegEx": "Makarychev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2012}, {"title": "Sorting noisy data with partial information", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Makarychev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2013}, {"title": "Constant factor approximation for balanced cut in the random PIE model", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of Symposium on Theory of Computing,", "citeRegEx": "Makarychev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2014}, {"title": "Correlation clustering with noisy input", "author": ["Claire Mathieu", "Warren Schudy"], "venue": "In Proceedings of Symposium on Discrete Algorithms,", "citeRegEx": "Mathieu and Schudy.,? \\Q2010\\E", "shortCiteRegEx": "Mathieu and Schudy.", "year": 2010}, {"title": "The Structure and Dynamics of Networks: (Princeton Studies in Complexity)", "author": ["Mark Newman", "Albert-Laszlo Barabasi", "Duncan J. Watts"], "venue": null, "citeRegEx": "Newman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2006}, {"title": "Structured learning and prediction in computer vision", "author": ["Sebastian Nowozin", "Christoph H. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Nowozin and Lampert.,? \\Q2010\\E", "shortCiteRegEx": "Nowozin and Lampert.", "year": 2010}, {"title": "Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies", "author": ["Noah A. Smith"], "venue": null, "citeRegEx": "Smith.,? \\Q2011\\E", "shortCiteRegEx": "Smith.", "year": 2011}, {"title": "Correlation clustering: Maximizing agreements via semidefinite programming", "author": ["Chaitanya Swamy"], "venue": "In Proceedings of Symposium on Discrete Algorithms,", "citeRegEx": "Swamy.,? \\Q2004\\E", "shortCiteRegEx": "Swamy.", "year": 2004}, {"title": "Fast planar correlation clustering for image segmentation", "author": ["Julian Yarkony", "Alexander T. Ihler", "Charless C. Fowlkes"], "venue": "In 12th European Conference on Computer Vision (ECCV),", "citeRegEx": "Yarkony et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yarkony et al\\.", "year": 2012}, {"title": "A newton-cg augmented lagrangian method for semidefinite programming", "author": ["Xinyuan Zhao", "Defeng Sun", "Kim-Chuan Toh"], "venue": "SIAM J. Optimization,", "citeRegEx": "Zhao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Correlation clustering was introduced in [Bansal et al., 2004], and implicitly in [Ben-Dor et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": ", 2004], and implicitly in [Ben-Dor et al., 1999] as \u2018Cluster Editing\u2019.", "startOffset": 27, "endOffset": 49}, {"referenceID": 6, "context": "The problem is APX-hard even on complete graphs2 (when we are given the similarity information for every pair of objects) [Charikar et al., 2005].", "startOffset": 122, "endOffset": 145}, {"referenceID": 16, "context": ", 2006], for which the current state-ofthe-art algorithm gives a \u0398(log n) factor approximation [Garg et al., 1993].", "startOffset": 95, "endOffset": 114}, {"referenceID": 2, "context": "Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009].", "startOffset": 125, "endOffset": 153}, {"referenceID": 0, "context": ", 2004, Ailon et al., 2008, Chawla et al., 2014]. Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009]. To summarize, despite our best efforts, we only know logarithmic factor approximation algorithms for Correlation Clustering; moreover, we cannot get a constant factor approximation for worst-case instances if the Unique Games Conjecture is true. However, our primary interest in solving Correlation Clustering comes from its numerous applications, and the instances that we encounter in these applications are not worst-case instances. This motivates the study of the average-case complexity of the problem and raises the following question: Can we design algorithms with better provable guarantees for realistic average-case models of Correlation Clustering? Several natural average-case models of Correlation Clustering have been studied previously. Ben-Dor et al. [1999] consider a model in which we start with a ground-truth clustering \u2013 an arbitrary partitioning of the vertices \u2013 of a complete graph.", "startOffset": 8, "endOffset": 979}, {"referenceID": 3, "context": "In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering.", "startOffset": 62, "endOffset": 83}, {"referenceID": 4, "context": "extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos\u2013Renyi random graphs.", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "In their model, the underlying unlabeled graph G(V,E) comes from an Erd\u00f6s\u2013Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1\u2212 \u03b5 and inconsistent with probability \u03b5.", "startOffset": 128, "endOffset": 150}, {"referenceID": 30, "context": "Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012].", "startOffset": 88, "endOffset": 110}, {"referenceID": 3, "context": "In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering. Mathieu and Schudy consider a generalization of this model where there is an adversary: for each edge, we keep the initial label with probability (1 \u2212 \u03b5), and we let the adversary decide whether to flip the edge label or not with probability \u03b5. The major drawback of these models is that they only consider the case of complete graphs, i.e. they require that the Correlation Clustering instance contains similarity information for every pair of nodes. Chen et al. extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos\u2013Renyi random graphs. In their model, the underlying unlabeled graph G(V,E) comes from an Erd\u00f6s\u2013Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1\u2212 \u03b5 and inconsistent with probability \u03b5. While these average-case models are natural, they are unrealistic in practice since most real-world graphs are neither dense nor captured by Erd\u00f6s\u2013Renyi distributions. For instance, real-world graphs in community detection have many structural properties (presence of large cliques, large clustering coefficients, heavy-tailed degree distribution) that are not exhibited by graphs that are generated by Erd\u00f6s\u2013Renyi models [Newman et al., 2006, Kumar et al., 1999]. Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012]. Further, these models assume that every pair of vertices have the same amount of similarity or dissimilarity (all costs are unit). Our semi-random model tries to address these issues by assuming very little about the observations \u2013 the underlying unlabeled graph G(V,E) \u2013 and allowing non-uniform costs. 1.1 Our Semi-random Model In this paper, we propose and study a new semi-random model for generating general instances of Correlation Clustering, which we believe captures many properties of real world instances. It generalizes the model of Mathieu and Schudy [2010] to arbitrary graphs G(V,E, c) with costs.", "startOffset": 63, "endOffset": 2162}, {"referenceID": 5, "context": "Research on semi-random models was initiated by [Blum and Spencer, 1995], who introduced and investigated semi-random models for k-coloring.", "startOffset": 48, "endOffset": 72}, {"referenceID": 12, "context": ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 13, "context": ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al.", "startOffset": 72, "endOffset": 101}, {"referenceID": 20, "context": ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al., 2011], and other problems.", "startOffset": 116, "endOffset": 136}, {"referenceID": 23, "context": "Most related to our work, both in the nature of the model and in the techniques used, is a recent result of [Makarychev et al., 2013] on semi-random instances of Minimum", "startOffset": 108, "endOffset": 133}, {"referenceID": 23, "context": "While the techniques used in both papers are conceptually similar, the semidefinite (SDP) relaxation for Correlation Clustering that we use in this paper is very different from the SDP relaxation for Minimum Feedback Arc Set used in [Makarychev et al., 2013].", "startOffset": 233, "endOffset": 258}, {"referenceID": 25, "context": "The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al.", "startOffset": 68, "endOffset": 94}, {"referenceID": 9, "context": "The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al., 2014].", "startOffset": 99, "endOffset": 118}, {"referenceID": 9, "context": "In the case of Erd\u00f6s\u2013Renyi graphs, our algorithms obtain similar guarantees for smaller values of k (the implicit dependence on k is a worse polynomial than in [Chen et al., 2014], however).", "startOffset": 160, "endOffset": 179}, {"referenceID": 31, "context": "We used the SDPNAL MATLAB library to solve the semidefinite programming (SDP) relaxation for the problem [Zhao et al., 2010].", "startOffset": 105, "endOffset": 124}, {"referenceID": 9, "context": "We used the same values of n as were used in [Chen et al., 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": ", 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al., 2014] works (Chen et al.", "startOffset": 101, "endOffset": 120}, {"referenceID": 8, "context": "Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erd\u00f6s\u2013Renyi graphs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erd\u00f6s\u2013Renyi graphs. Very recently, Globerson et al. [2014] considered a semi-random model for Correlation Clustering for recovery in grid graphs and planar graphs, and gave conditions for approximate recovery in terms of an expansion-related condition.", "startOffset": 0, "endOffset": 147}, {"referenceID": 29, "context": "We use a simple SDP relaxation for the problem [Swamy, 2004].", "startOffset": 47, "endOffset": 60}, {"referenceID": 25, "context": "We note that this relaxation is weaker than the SDP used in [Mathieu and Schudy, 2010] because it does not have l2-triangle inequalities constraints.", "startOffset": 60, "endOffset": 86}, {"referenceID": 6, "context": "We then run the O(log n) worst-case approximation algorithm of [Charikar et al., 2005] or [Demaine et al.", "startOffset": 63, "endOffset": 86}, {"referenceID": 10, "context": ", 2005] or [Demaine et al., 2006] on the remaining graph to obtain a PTAS overall.", "startOffset": 11, "endOffset": 33}, {"referenceID": 9, "context": "While our algorithm succeeds when the SDP is tight (as in [Chen et al., 2014]), the analysis of our algorithm also shows how to deal with nearly integral solutions, in which most inner products \u3008\u016b, v\u0304\u3009 are only close to 0 or 1 (but may not be tight).", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": "Let D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 6, "context": "Let D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al. [2006]. We apply Theorem 3.", "startOffset": 52, "endOffset": 100}, {"referenceID": 22, "context": "We use the following lemma from Makarychev et al. [2013]. Lemma 3.", "startOffset": 32, "endOffset": 57}, {"referenceID": 15, "context": "Now, by Freedman\u2019s inequality (see Freedman [1975]),", "startOffset": 8, "endOffset": 51}], "year": 2015, "abstractText": "In this paper, we propose and study a semi-random model for the Correlation Clustering problem on arbitrary graphsG. We give two approximation algorithms for Correlation Clustering instances from this model. The first algorithm finds a solution of value (1+\u03b4) opt-cost+O\u03b4(n log 3 n) with high probability, where opt-cost is the value of the optimal solution (for every \u03b4 > 0). The second algorithm finds the ground truth clustering with an arbitrarily small classification error \u03b7 (under some additional assumptions on the instance).", "creator": "LaTeX with hyperref package"}}}