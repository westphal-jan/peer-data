{"id": "1407.0202", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2014", "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "abstract": "In this paper, we present a new optimization method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves the theory behind SAG and SVRG, with better theoretical convergence rates, and supports compound targets using a proximal operator on the regulator. Unlike SDCA, SAGA does not directly support strongly convex problems and is adaptable to any inherent strong convexity of the problem. We give experimental results that demonstrate the effectiveness of our method.", "histories": [["v1", "Tue, 1 Jul 2014 11:47:56 GMT  (715kb,D)", "https://arxiv.org/abs/1407.0202v1", null], ["v2", "Tue, 22 Jul 2014 06:57:50 GMT  (716kb,D)", "http://arxiv.org/abs/1407.0202v2", null], ["v3", "Tue, 16 Dec 2014 08:44:27 GMT  (725kb,D)", "http://arxiv.org/abs/1407.0202v3", "Advances In Neural Information Processing Systems, Nov 2014, Montreal, Canada"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["aaron defazio", "francis r bach", "simon lacoste-julien"], "accepted": true, "id": "1407.0202"}, "pdf": {"name": "1407.0202.pdf", "metadata": {"source": "CRF", "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "authors": ["Aaron Defazio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Remarkably, recent advances [1, 2] have shown that it is possible to minimise strongly convex finite sums provably faster in expectation than is possible without the finite sum structure. This is significant for machine learning problems as a finite sum structure is common in the empirical risk minimisation setting. The requirement of strong convexity is likewise satisfied in machine learning problems in the typical case where a quadratic regulariser is used.\nIn particular, we are interested in minimising functions of the form\nf(x) = 1\nn n\u2211 i=1 fi(x),\nwhere x \u2208 Rd, each fi is convex and has Lipschitz continuous derivatives with constant L. We will also consider the case where each fi is strongly convex with constant \u00b5, and the \u201ccomposite\u201d (or proximal) case where an additional regularisation function is added:\nF (x) = f(x) + h(x),\nwhere h : Rd \u2192 Rd is convex but potentially non-differentiable, and where the proximal operation of h is easy to compute \u2014 few incremental gradient methods are applicable in this setting [3][4].\nOur contributions are as follows. In Section 2 we describe the SAGA algorithm, a novel incremental gradient method. In Section 5 we prove theoretical convergence rates for SAGA in the strongly convex case better than those for SAG [1] and SVRG [5], and a factor of 2 from the SDCA [2] convergence rates. These rates also hold in the composite setting. Additionally, we show that \u2217The first author completed this work while under funding from NICTA. This work was partially supported by the MSR-Inria Joint Centre and a grant by the European Research Council (SIERRA project 239993).\nar X\niv :1\n40 7.\n02 02\nv3 [\ncs .L\nG ]\nlike SAG but unlike SDCA, our method is applicable to non-strongly convex problems without modification. We establish theoretical convergence rates for this case also. In Section 3 we discuss the relation between each of the fast incremental gradient methods, showing that each stems from a very small modification of another."}, {"heading": "2 SAGA Algorithm", "text": "We start with some known initial vector x0 \u2208 Rd and known derivatives f \u2032i(\u03c60i ) \u2208 Rd with \u03c60i = x0 for each i. These derivatives are stored in a table data-structure of length n, or alternatively a n\u00d7 d matrix. For many problems of interest, such as binary classification and least-squares, only a single floating point value instead of a full gradient vector needs to be stored (see Section 4). SAGA is inspired both from SAG [1] and SVRG [5] (as we will discuss in Section 3). SAGA uses a step size of \u03b3 and makes the following updates, starting with k = 0:\nSAGA Algorithm: Given the value of xk and of each f \u2032i(\u03c6ki ) at the end of iteration k, the updates for iteration k + 1 is as follows:\n1. Pick a j uniformly at random.\n2. Take \u03c6k+1j = x k, and store f \u2032j(\u03c6 k+1 j ) in the table. All other entries in the table remain\nunchanged. The quantity \u03c6k+1j is not explicitly stored.\n3. Update x using f \u2032j(\u03c6 k+1 j ), f \u2032 j(\u03c6 k j ) and the table average: wk+1 = xk \u2212 \u03b3 [ f \u2032j(\u03c6 k+1 j )\u2212 f \u2032j(\u03c6kj ) + 1\nn n\u2211 i=1 f \u2032i(\u03c6 k i )\n] , (1)\nxk+1 = proxh\u03b3 ( wk+1 ) . (2)\nThe proximal operator we use above is defined as\nproxh\u03b3 (y) := argmin x\u2208Rd\n{ h(x) + 1\n2\u03b3 \u2016x\u2212 y\u20162\n} . (3)\nIn the strongly convex case, when a step size of \u03b3 = 1/(2(\u00b5n+L)) is chosen, we have the following convergence rate in the composite and hence also the non-composite case:\nE \u2225\u2225\u2225xk \u2212 x\u2217\u2225\u2225\u22252 \u2264 (1\u2212 \u00b5\n2(\u00b5n+ L) )k [\u2225\u2225x0 \u2212 x\u2217\u2225\u22252 + n \u00b5n+ L [ f(x0)\u2212 \u2329 f \u2032(x\u2217), x0 \u2212 x\u2217 \u232a \u2212 f(x\u2217) ]] .\nWe prove this result in Section 5. The requirement of strong convexity can be relaxed from needing to hold for each fi to just holding on average, but at the expense of a worse geometric rate (1 \u2212\n\u00b5 6(\u00b5n+L) ), requiring a step size of \u03b3 = 1/(3(\u00b5n+ L)).\nIn the non-strongly convex case, we have established the convergence rate in terms of the average iterate, excluding step 0: x\u0304k = 1k \u2211k t=1 x t. Using a step size of \u03b3 = 1/(3L) we have\nE [ F (x\u0304k) ] \u2212 F (x\u2217) \u2264 4n\nk\n[ 2L\nn \u2225\u2225x0 \u2212 x\u2217\u2225\u22252 + f(x0)\u2212 \u2329f \u2032(x\u2217), x0 \u2212 x\u2217\u232a\u2212 f(x\u2217)] . This result is proved in the supplementary material. Importantly, when this step size \u03b3 = 1/(3L) is used, our algorithm automatically adapts to the level of strong convexity \u00b5 > 0 naturally present, giving a convergence rate of (see the comment at the end of the proof of Theorem 1):\nE \u2225\u2225\u2225xk \u2212 x\u2217\u2225\u2225\u22252 \u2264 (1\u2212min{ 1\n4n , \u00b5 3L })k [\u2225\u2225x0 \u2212 x\u2217\u2225\u22252 + 2n 3L [ f(x0)\u2212 \u2329 f \u2032(x\u2217), x0 \u2212 x\u2217 \u232a \u2212 f(x\u2217) ]] .\nAlthough any incremental gradient method can be applied to non-strongly convex problems via the addition of a small quadratic regularisation, the amount of regularisation is an additional tunable parameter which our method avoids."}, {"heading": "3 Related Work", "text": "We explore the relationship between SAGA and the other fast incremental gradient methods in this section. By using SAGA as a midpoint, we are able to provide a more unified view than is available in the existing literature. A brief summary of the properties of each method considered in this section is given in Figure 1. The method from [3], which handles the non-composite setting, is not listed as its rate is of the slow type and can be up to n times smaller than the one for SAGA or SVRG [5].\nSAGA: midpoint between SAG and SVRG/S2GD\nIn [5], the authors make the observation that the variance of the standard stochastic gradient (SGD) update direction can only go to zero if decreasing step sizes are used, thus preventing a linear convergence rate unlike for batch gradient descent. They thus propose to use a variance reduction approach (see [7] and references therein for example) on the SGD update in order to be able to use constant step sizes and get a linear convergence rate. We present the updates of their method called SVRG (Stochastic Variance Reduced Gradient) in (6) below, comparing it with the non-composite form of SAGA rewritten in (5). They also mention that SAG (Stochastic Average Gradient) [1] can be interpreted as reducing the variance, though they do not provide the specifics. Here, we make this connection clearer and relate it to SAGA.\nWe first review a slightly more generalized version of the variance reduction approach (we allow the updates to be biased). Suppose that we want to use Monte Carlo samples to estimate EX and that we can compute efficiently EY for another random variable Y that is highly correlated with X . One variance reduction approach is to use the following estimator \u03b8\u03b1 as an approximation to EX: \u03b8\u03b1 := \u03b1(X\u2212Y )+EY , for a step size \u03b1 \u2208 [0, 1]. We have that E\u03b8\u03b1 is a convex combination of EX and EY : E\u03b8\u03b1 = \u03b1EX + (1\u2212 \u03b1)EY . The standard variance reduction approach uses \u03b1 = 1 and the estimate is unbiased E\u03b81 = EX . The variance of \u03b8\u03b1 is: Var(\u03b8\u03b1) = \u03b12[Var(X) + Var(Y )\u2212 2 Cov(X,Y )], and so if Cov(X,Y ) is big enough, the variance of \u03b8\u03b1 is reduced compared to X , giving the method its name. By varying \u03b1 from 0 to 1, we increase the variance of \u03b8\u03b1 towards its maximum value (which usually is still smaller than the one for X) while decreasing its bias towards zero.\nBoth SAGA and SAG can be derived from such a variance reduction viewpoint: here X is the SGD direction sample f \u2032j(x k), whereas Y is a past stored gradient f \u2032j(\u03c6 k j ). SAG is obtained by using \u03b1 = 1/n (update rewritten in our notation in (4)), whereas SAGA is the unbiased version with \u03b1 = 1 (see (5) below). For the same \u03c6\u2019s, the variance of the SAG update is 1/n2 times the one of SAGA, but at the expense of having a non-zero bias. This non-zero bias might explain the complexity of the convergence proof of SAG and why the theory has not yet been extended to proximal operators. By using an unbiased update in SAGA, we are able to obtain a simple and tight theory, with better constants than SAG, as well as theoretical rates for the use of proximal operators.\n(SAG) xk+1 = xk \u2212 \u03b3 [ f \u2032j(x\nk)\u2212 f \u2032j(\u03c6kj ) n + 1 n n\u2211 i=1 f \u2032i(\u03c6 k i )\n] , (4)\n(SAGA) xk+1 = xk \u2212 \u03b3 [ f \u2032j(x k)\u2212 f \u2032j(\u03c6kj ) + 1\nn n\u2211 i=1 f \u2032i(\u03c6 k i )\n] , (5)\n(SVRG) xk+1 = xk \u2212 \u03b3 [ f \u2032j(x k)\u2212 f \u2032j(x\u0303) + 1\nn n\u2211 i=1 f \u2032i(x\u0303)\n] . (6)\nThe SVRG update (6) is obtained by using Y = f \u2032j(x\u0303) with \u03b1 = 1 (and is thus unbiased \u2013 we note that SAG is the only method that we present in the related work that has a biased update direction). The vector x\u0303 is not updated every step, but rather the loop over k appears inside an outer loop, where x\u0303 is updated at the start of each outer iteration. Essentially SAGA is at the midpoint between SVRG and SAG; it updates the \u03c6j value each time index j is picked, whereas SVRG updates all of \u03c6\u2019s as a batch. The S2GD method [8] has the same update as SVRG, just differing in how the number of inner loop iterations is chosen. We use SVRG henceforth to refer to both methods.\nSVRG makes a trade-off between time and space. For the equivalent practical convergence rate it makes 2x-3x more gradient evaluations, but in doing so it does not need to store a table of gradients, but a single average gradient. The usage of SAG vs. SVRG is problem dependent. For example for linear predictors where gradients can be stored as a reduced vector of dimension p\u2212 1 for p classes, SAGA is preferred over SVRG both theoretically and in practice. For neural networks, where no theory is available for either method, the storage of gradients is generally more expensive than the additional backpropagations, but this is computer architecture dependent.\nSVRG also has an additional parameter besides step size that needs to be set, namely the number of iterations per inner loop (m). This parameter can be set via the theory, or conservatively as m = n, however doing so does not give anywhere near the best practical performance. Having to tune one parameter instead of two is a practical advantage for SAGA.\nFinito/MISO\u00b5\nTo make the relationship with other prior methods more apparent, we can rewrite the SAGA algorithm (in the non-composite case) in term of an additional intermediate quantity uk, with u0 := x0 + \u03b3 \u2211n i=1 f \u2032 i(x 0), in addition to the usual xk iterate as described previously:\nSAGA: Equivalent reformulation for non-composite case: Given the value of uk and of each f \u2032i(\u03c6 k i ) at the end of iteration k, the updates for iteration k + 1, is as follows:\n1. Calculate xk: xk = uk \u2212 \u03b3 n\u2211 i=1 f \u2032i(\u03c6 k i ). (7)\n2. Update u with uk+1 = uk + 1n (x k \u2212 uk).\n3. Pick a j uniformly at random.\n4. Take \u03c6k+1j = x k, and store f \u2032j(\u03c6 k+1 j ) in the table replacing f \u2032 j(\u03c6 k j ). All other entries in\nthe table remain unchanged. The quantity \u03c6k+1j is not explicitly stored.\nEliminating uk recovers the update (5) for xk. We now describe how the Finito [9] and MISO\u00b5 [10] methods are closely related to SAGA. Both Finito and MISO\u00b5 use updates of the following form, for a step length \u03b3:\nxk+1 = 1\nn \u2211 i \u03c6ki \u2212 \u03b3 n\u2211 i=1 f \u2032i(\u03c6 k i ). (8)\nThe step size used is of the order of 1/\u00b5n. To simplify the discussion of this algorithm we will introduce the notation \u03c6\u0304 = 1n \u2211 i \u03c6 k i .\nSAGA can be interpreted as Finito, but with the quantity \u03c6\u0304 replaced with u, which is updated in the same way as \u03c6\u0304, but in expectation. To see this, consider how \u03c6\u0304 changes in expectation:\nE [ \u03c6\u0304k+1 ] = E [ \u03c6\u0304k + 1\nn\n( xk \u2212 \u03c6kj )] = \u03c6\u0304k + 1\nn\n( xk \u2212 \u03c6\u0304k ) .\nThe update is identical in expectation to the update for u, uk+1 = uk + 1n (x k \u2212 uk). There are three advantages of SAGA over Finito/MISO\u00b5. SAGA does not require strong convexity to work, it has support for proximal operators, and it does not require storing the \u03c6i values. MISO has proven support for proximal operators only in the case where impractically small step sizes are used [10]. The big advantage of Finito/MISO\u00b5 is that when using a per-pass re-permuted access ordering, empirical speed-ups of up-to a factor of 2x has been observed. This access order can also be used with the other methods discussed, but with smaller empirical speed-ups. Finito/MISO\u00b5 is particularly useful when fi is computationally expensive to compute compared to the extra storage costs required over the other methods.\nSDCA\nThe Stochastic Dual Coordinate Descent (SDCA) [2] method on the surface appears quite different from the other methods considered. It works with the convex conjugates of the fi functions. However, in this section we show a novel transformation of SDCA into an equivalent method that only works with primal quantities, and is closely related to the MISO\u00b5 method.\nConsider the following algorithm:\nSDCA algorithm in the primal Step k + 1:\n1. Pick an index j uniformly at random.\n2. Compute \u03c6k+1j = prox fj \u03b3 (z), where \u03b3 = 1\u00b5n and z = \u2212\u03b3 \u2211n i6=j f \u2032 i(\u03c6 k i ).\n3. Store the gradient f \u2032j(\u03c6 k+1 j ) = 1 \u03b3 ( z \u2212 \u03c6k+1j ) in the table at location j. For i 6= j, the\ntable entries are unchanged (f \u2032i(\u03c6 k+1 i ) = f \u2032 i(\u03c6 k i )).\nAt completion, return xk = \u2212\u03b3\u2211ni f \u2032i(\u03c6ki ) . We claim that this algorithm is equivalent to the version of SDCA where exact block-coordinate maximisation is used on the dual.1 Firstly, note that while SDCA was originally described for onedimensional outputs (binary classification or regression), it has been expanded to cover the multiclass predictor case [11] (called Prox-SDCA there). In this case, the primal objective has a separate strongly convex regulariser, and the functions fi are restricted to the form fi(x) := \u03c8i(XTi x), where Xi is a d\u00d7p feature matrix, and \u03c8i is the loss function that takes a p dimensional input, for p classes. To stay in the same general setting as the other incremental gradient methods, we work directly with the fi(x) functions rather than the more structured \u03c8i(XTi x). The dual objective to maximise then becomes\nD(\u03b1) = \u2212\u00b5 2 \u2225\u2225\u2225\u2225\u2225 1\u00b5n n\u2211 i=1 \u03b1i \u2225\u2225\u2225\u2225\u2225 2 \u2212 1 n n\u2211 i=1 f\u2217i (\u2212\u03b1i)  , where \u03b1i\u2019s are d-dimensional dual variables. Generalising the exact block-coordinate maximisation update that SDCA performs to this form, we get the dual update for block j (with xk the current primal iterate):\n\u03b1k+1j = \u03b1 k j + argmax\n\u2206aj\u2208Rd\n{ \u2212f\u2217j ( \u2212\u03b1kj \u2212\u2206\u03b1j ) \u2212 \u00b5n\n2 \u2225\u2225\u2225\u2225xk + 1\u00b5n\u2206\u03b1j \u2225\u2225\u2225\u22252 } . (9)\nIn the special case where fi(x) = \u03c8i(XTi x), we can see that (9) gives exactly the same update as Option I of Prox-SDCA in [11, Figure 1], which operates instead on the equivalent p-dimensional dual variables \u03b1\u0303i with the relationship that \u03b1i = Xi\u03b1\u0303i.2 As noted by Shalev-Shwartz & Zhang [11], the update (9) is actually an instance of the proximal operator of the convex conjugate of fj . Our primal formulation exploits this fact by using a relation between the proximal operator of a function and its convex conjugate known as the Moreau decomposition:\nproxf \u2217 (v) = v \u2212 proxf (v).\nThis decomposition allows us to compute the proximal operator of the conjugate via the primal proximal operator. As this is the only use in the basic SDCA method of the conjugate function, applying this decomposition allows us to completely eliminate the \u201cdual\u201d aspect of the algorithm, yielding the above primal form of SDCA. The dual variables are related to the primal representatives \u03c6i\u2019s through \u03b1i = \u2212f \u2032i(\u03c6i). The KKT conditions ensure that if the \u03b1i values are dual optimal then xk = \u03b3 \u2211 i \u03b1i as defined above is primal optimal. The same trick is commonly used to interpret Dijkstra\u2019s set intersection as a primal algorithm instead of a dual block coordinate descent algorithm [12].\nThe primal form of SDCA differs from the other incremental gradient methods described in this section in that it assumes strong convexity is induced by a separate strongly convex regulariser, rather than each fi being strongly convex. In fact, SDCA can be modified to work without a separate regulariser, giving a method that is at the midpoint between Finito and SDCA. We detail such a method in the supplementary material.\n1More precisely, to Option I of Prox-SDCA as described in [11, Figure 1]. We will simply refer to this method as \u201cSDCA\u201d in this paper for brevity.\n2This is because f\u2217i (\u03b1i) = inf \u03b1\u0303i s.t. \u03b1i=Xi\u03b1\u0303i \u03c8\u2217i (\u03b1\u0303i).\nSDCA variants\nThe SDCA theory has been expanded to cover a number of other methods of performing the coordinate step [11]. These variants replace the proximal operation in our primal interpretation in the previous section with an update where \u03c6k+1j is chosen so that: f \u2032 j(\u03c6 k+1 j ) = (1\u2212\u03b2)f \u2032j(\u03c6kj )+\u03b2f \u2032j(xk),\nwhere xk = \u2212 1\u00b5n \u2211 i f \u2032 i(\u03c6 k i ). The variants differ in how \u03b2 \u2208 [0, 1] is chosen. Note that \u03c6k+1j does not actually have to be explicitly known, just the gradient f \u2032j(\u03c6 k+1 j ), which is the result of the above interpolation. Variant 5 by Shalev-Shwartz & Zhang [11] does not require operations on the conjugate function, it simply uses \u03b2 = \u00b5nL+\u00b5n . The most practical variant performs a line search involving the convex conjugate to determine \u03b2. As far as we are aware, there is no simple primal equivalent of this line search. So in cases where we can not compute the proximal operator from the standard SDCA variant, we can either introduce a tuneable parameter into the algorithm (\u03b2), or use a dual line search, which requires an efficient way to evaluate the convex conjugates of each fi."}, {"heading": "4 Implementation", "text": "We briefly discuss some implementation concerns:\n\u2022 For many problems each derivative f \u2032i is just a simple weighting of the ith data vector. Logistic regression and least squares have this property. In such cases, instead of storing the full derivative f \u2032i for each i, we need only to store the weighting constants. This reduces the storage requirements to be the same as the SDCA method in practice. A similar trick can be applied to multi-class classifiers with p classes by storing p\u2212 1 values for each i.\n\u2022 Our algorithm assumes that initial gradients are known for each fi at the starting point x0. Instead, a heuristic may be used where during the first pass, data-points are introduced oneby-one, in a non-randomized order, with averages computed in terms of those data-points processed so far. This procedure has been successfully used with SAG [1].\n\u2022 The SAGA update as stated is slower than necessary when derivatives are sparse. A just-intime updating of u or x may be performed just as is suggested for SAG [1], which ensures that only sparse updates are done at each iteration.\n\u2022 We give the form of SAGA for the case where each fi is strongly convex. However in practice we usually have only convex fi, with strong convexity in f induced by the addition of a quadratic regulariser. This quadratic regulariser may be split amongst the fi functions evenly, to satisfy our assumptions. It is perhaps easier to use a variant of SAGA where the regulariser \u00b52 ||x||2 is explicit, such as the following modification of Equation (5):\nxk+1 = (1\u2212 \u03b3\u00b5)xk \u2212 \u03b3 [ f \u2032j(x k)\u2212 f \u2032j(\u03c6kj ) + 1\nn \u2211 i f \u2032i(\u03c6 k i )\n] .\nFor sparse implementations instead of scaling xk at each step, a separate scaling constant \u03b2k may be scaled instead, with \u03b2kxk being used in place of xk. This is a standard trick used with stochastic gradient methods.\nFor sparse problems with a quadratic regulariser the just-in-time updating can be a little intricate. In the supplementary material we provide example python code showing a correct implementation that uses each of the above tricks."}, {"heading": "5 Theory", "text": "In this section, all expectations are taken with respect to the choice of j at iteration k + 1 and conditioned on xk and each f \u2032i(\u03c6 k i ) unless stated otherwise.\nWe start with two basic lemmas that just state properties of convex functions, followed by Lemma 1, which is specific to our algorithm. The proofs of each of these lemmas is in the supplementary material. Lemma 1. Let f(x) = 1n \u2211n i=1 fi(x). Suppose each fi is \u00b5-strongly convex and has Lipschitz continuous gradients with constant L. Then for all x and x\u2217:\n\u3008f \u2032(x), x\u2217 \u2212 x\u3009 \u2264 L\u2212 \u00b5 L [f(x\u2217)\u2212 f(x)]\u2212 \u00b5 2 \u2016x\u2217 \u2212 x\u20162\n\u2212 1 2Ln \u2211 i \u2016f \u2032i(x\u2217)\u2212 f \u2032i(x)\u2016 2 \u2212 \u00b5 L \u3008f \u2032(x\u2217), x\u2212 x\u2217\u3009 .\nLemma 2. We have that for all \u03c6i and x\u2217:\n1\nn \u2211 i \u2016f \u2032i(\u03c6i)\u2212 f \u2032i(x\u2217)\u2016 2 \u2264 2L\n[ 1\nn \u2211 i fi(\u03c6i)\u2212 f(x\u2217)\u2212 1 n \u2211 i \u3008f \u2032i(x\u2217), \u03c6i \u2212 x\u2217\u3009 ] .\nLemma 3. It holds that for any \u03c6ki , x\u2217, xk and \u03b2 > 0, with wk+1 as defined in Equation 1: E \u2225\u2225\u2225wk+1 \u2212 xk \u2212 \u03b3f \u2032(x\u2217)\u2225\u2225\u22252 \u2264 \u03b32(1 + \u03b2\u22121)E\u2225\u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u2225\u22252 + \u03b32(1 + \u03b2)E \u2225\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u2225\u22252\n\u2212 \u03b32\u03b2 \u2225\u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u2225\u22252 .\nTheorem 1. With x\u2217 the optimal solution, define the Lyapunov function T as:\nT k := T (xk, {\u03c6ki }ni=1) := 1\nn \u2211 i fi(\u03c6 k i )\u2212 f(x\u2217)\u2212 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6ki \u2212 x\u2217 \u232a + c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 .\nThen with \u03b3 = 12(\u00b5n+L) , c = 1 2\u03b3(1\u2212\u03b3\u00b5)n , and \u03ba = 1 \u03b3\u00b5 , we have the following expected change in the Lyapunov function between steps of the SAGA algorithm (conditional on T k):\nE[T k+1] \u2264 (1\u2212 1 \u03ba )T k.\nProof. The first three terms in T k+1 are straight-forward to simplify:\nE\n[ 1\nn \u2211 i fi(\u03c6 k+1 i )\n] = 1\nn f(xk) +\n( 1\u2212 1\nn\n) 1\nn \u2211 i fi(\u03c6 k i ).\nE [ \u2212 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6k+1i \u2212 x\u2217 \u232a] = \u2212 1 n \u2329 f \u2032(x\u2217), xk \u2212 x\u2217 \u232a \u2212 ( 1\u2212 1 n ) 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6ki \u2212 x\u2217 \u232a .\nFor the change in the last term of T k+1, we apply the non-expansiveness of the proximal operator3: c \u2225\u2225xk+1 \u2212 x\u2217\u2225\u22252 = c\u2225\u2225prox\u03b3(wk+1)\u2212 prox\u03b3(x\u2217 \u2212 \u03b3f \u2032(x\u2217))\u2225\u22252\n\u2264 c \u2225\u2225wk+1 \u2212 x\u2217 + \u03b3f \u2032(x\u2217)\u2225\u22252 .\nWe expand the quadratic and apply E[wk+1] = xk \u2212 \u03b3f \u2032(xk) to simplify the inner product term: cE \u2225\u2225wk+1 \u2212 x\u2217 + \u03b3f \u2032(x\u2217)\u2225\u22252 = cE\u2225\u2225xk \u2212 x\u2217 + wk+1 \u2212 xk + \u03b3f \u2032(x\u2217)\u2225\u22252\n= c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 + 2cE [\u2329wk+1 \u2212 xk + \u03b3f \u2032(x\u2217), xk \u2212 x\u2217\u232a]+ cE\u2225\u2225wk+1 \u2212 xk + \u03b3f \u2032(x\u2217)\u2225\u22252\n= c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 \u2212 2c\u03b3 \u2329f \u2032(xk)\u2212 f \u2032(x\u2217), xk \u2212 x\u2217\u232a+ cE\u2225\u2225wk+1 \u2212 xk + \u03b3f \u2032(x\u2217)\u2225\u22252\n\u2264 c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 \u2212 2c\u03b3 \u2329f \u2032(xk), xk \u2212 x\u2217\u232a+ 2c\u03b3 \u2329f \u2032(x\u2217), xk \u2212 x\u2217\u232a\u2212 c\u03b32\u03b2 \u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u22252\n+ ( 1 + \u03b2\u22121 ) c\u03b32E \u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252 + (1 + \u03b2) c\u03b32E\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252 . (Lemma 7) The value of \u03b2 shall be fixed later. Now we apply Lemma 1 to bound \u22122c\u03b3 \u2329 f \u2032(xk), xk \u2212 x\u2217 \u232a and\nLemma 6 to bound E \u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252:\ncE \u2225\u2225xk+1 \u2212 x\u2217\u2225\u22252 \u2264 (c\u2212 c\u03b3\u00b5)\u2225\u2225xk \u2212 x\u2217\u2225\u22252 + ((1 + \u03b2)c\u03b32 \u2212 c\u03b3\nL\n) E \u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252\n\u2212 2c\u03b3(L\u2212 \u00b5) L\n[ f(xk)\u2212 f(x\u2217)\u2212 \u2329 f \u2032(x\u2217), xk \u2212 x\u2217 \u232a] \u2212 c\u03b32\u03b2 \u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u22252 + 2 ( 1 + \u03b2\u22121 ) c\u03b32L [ 1\nn \u2211 i fi(\u03c6 k i )\u2212 f(x\u2217)\u2212 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6ki \u2212 x\u2217 \u232a] .\n3Note that the first equality below is the only place in the proof where we use the fact that x\u2217 is an optimality point.\nWe can now combine the bounds that we have derived for each term in T , and pull out a fraction 1\u03ba of T k (for any \u03ba at this point). Together with the inequality \u2212 \u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u22252 \u2264\n\u22122\u00b5 [ f(xk)\u2212 f(x\u2217)\u2212 \u2329 f \u2032(x\u2217), xk \u2212 x\u2217 \u232a] [13, Thm. 2.1.10], that yields:\nE[T k+1]\u2212 T k \u2264 \u2212 1 \u03ba T k +\n( 1\nn \u2212 2c\u03b3(L\u2212 \u00b5) L \u2212 2c\u03b32\u00b5\u03b2\n)[ f(xk)\u2212 f(x\u2217)\u2212 \u2329 f \u2032(x\u2217), xk \u2212 x\u2217 \u232a] + ( 1\n\u03ba + 2(1 + \u03b2\u22121)c\u03b32L\u2212 1 n\n)[ 1\nn \u2211 i fi(\u03c6 k i )\u2212 f(x\u2217)\u2212 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6ki \u2212 x\u2217 \u232a]\n+\n( 1\n\u03ba \u2212 \u03b3\u00b5\n) c \u2225\u2225\u2225xk \u2212 x\u2217\u2225\u2225\u22252 + ((1 + \u03b2)\u03b3 \u2212 1\nL\n) c\u03b3E \u2225\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u2225\u22252 . (10) Note that each of the terms in square brackets are positive, and it can be readily verified that our assumed values for the constants (\u03b3 = 12(\u00b5n+L) , c = 1 2\u03b3(1\u2212\u03b3\u00b5)n , and \u03ba = 1 \u03b3\u00b5 ), together with \u03b2 = 2\u00b5n+LL ensure that each of the quantities in round brackets are non-positive (the constants were determined by setting all the round brackets to zero except the second one \u2014 see [14] for the details). Adaptivity to strong convexity result: Note that when using the \u03b3 = 13L step size, the same c as above can be used with \u03b2 = 2 and 1\u03ba = min { 1 4n , \u00b5 3L } to ensure non-positive terms.\nCorollary 1. Note that c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 \u2264 T k, and therefore by chaining the expectations, plugging in the constants explicitly and using \u00b5(n\u2212 0.5) \u2264 \u00b5n to simplify the expression, we get:\nE [\u2225\u2225\u2225xk \u2212 x\u2217\u2225\u2225\u22252] \u2264 (1\u2212 \u00b5\n2(\u00b5n+ L) )k [\u2225\u2225x0 \u2212 x\u2217\u2225\u22252 + n \u00b5n+ L [ f(x0)\u2212 \u2329 f \u2032(x\u2217), x0 \u2212 x\u2217 \u232a \u2212 f(x\u2217) ]] .\nHere the expectation is over all choices of index jk up to step k."}, {"heading": "6 Experiments", "text": "We performed a series of experiments to validate the effectiveness of SAGA. We tested a binary classifier on MNIST, COVTYPE, IJCNN1 and a least squares predictor on MILLIONSONG. Details of these datasets can be found in [9]. We used the same code base for each method, just changing the main update rule. SVRG was tested with the recalibration pass used every n iterations, as suggested in [8]. Each method had its step size parameter chosen so as to give the fastest convergence.\nWe tested with a L2 regulariser, which all methods support, and with a L1 regulariser on a subset of the methods. The results are shown in Figure 2. We can see that Finito (perm) performs the best on a per epoch equivalent basis, but it can be the most expensive method per step. SVRG is similarly fast on a per epoch basis, but when considering the number of gradient evaluations per epoch is double that of the other methods for this problem, it is middle of the pack. SAGA can be seen to perform similar to the non-permuted Finito case, and to SDCA. Note that SAG is slower than the other methods at the beginning. To get the optimal results for SAG, an adaptive step size rule needs to be used rather than the constant step size we used. In general, these tests confirm that the choice of methods should be done based on their properties as discussed in Section 3, rather than their convergence rate."}, {"heading": "A The SDCA/Finito Midpoint Algorithm", "text": "Using Lagrangian duality theory, SDCA can be shown at step k as minimising the following lower bound:\nAk(x) = 1\nn fj(x) +\n1\nn n\u2211 i 6=j [ fi(\u03c6 k i ) + \u2329 f \u2032i(\u03c6 k i ), x\u2212 \u03c6ki \u232a] + \u00b5 2 \u2016x\u20162 .\nInstead of directly including the regulariser in this bound, we can use the standard strong convexity lower bound for each fi, by removing \u00b52 \u2016x\u2016\n2 and changing the expression in the summation to fi(\u03c6 k i )+ \u2329 f \u2032i(\u03c6 k i ), x\u2212 \u03c6ki \u232a + \u00b52 \u2016x\u2212 \u03c6i\u2016\n2. The transformation to having strong convexity within the fi functions yields the following simple modification to the algorithm: \u03c6k+1j = prox fj (\u00b5(n\u22121))\u22121(z), where:\nz = 1 n\u2212 1 \u2211 i 6=j \u03c6ki \u2212 1 \u00b5(n\u2212 1) \u2211 i 6=j f \u2032i(\u03c6 k i ).\nIt can be shown that after this update:\nxk+1 = \u03c6k+1j = 1\nn \u2211 i \u03c6k+1i \u2212 1 \u00b5n \u2211 i f \u2032i(\u03c6 k+1 i ).\nNow the similarity to Finito is apparent if this equation is compared Equation 8: xk+1 = 1n \u2211 i \u03c6 k i \u2212\n\u03b3 \u2211n i=1 f \u2032 i(\u03c6 k i ). The only difference is that the vectors on the right hand side of the equation are at their values at step k+1 instead of k. Note that there is a circular dependency here, as \u03c6k+1j := x k+1 but \u03c6k+1j appears in the definition of x k+1. Solving the proximal operator is the resolution of the circular dependency. This mid-point between Finito and SDCA is interesting in it\u2019s own right, as it appears experimentally to have similar robustness to permuted orderings as Finito, but it has no tunable parameters like SDCA.\nWhen the proximal operator above is fast to compute, say on the same order as just evaluating fj , then SDCA can be the best method among those discussed. It is a little slower than the other methods discussed here, but it has no tunable parameters at all. It is also the only choice when each fi is not differentiable. The major disadvantage of SDCA is that it can not handle non-strongly convex problems directly. Although like most methods, adding a small amount of quadratic regularisation can be used to recover a convergence rate. It is also not adapted to use proximal operators for the regulariser in the composite objective case. The requirement of computing the proximal operator of each loss fi initially appears to be a big disadvantage, however there are variants of SDCA that remove this requirement, but they introduce additional downsides."}, {"heading": "B Lemmas", "text": "Lemma 4. Let f be \u00b5-strongly convex and have Lipschitz continuous gradients with constant L. Then we have for all x and y:\nf(x) \u2265 f(y) + \u3008f \u2032(y), x\u2212 y\u3009+ 1 2 (L\u2212 \u00b5) \u2016f \u2032(x)\u2212 f \u2032(y)\u20162\n+ \u00b5L\n2 (L\u2212 \u00b5) \u2016y \u2212 x\u2016 2\n+ \u00b5\n(L\u2212 \u00b5) \u3008f \u2032(x)\u2212 f \u2032(y), y \u2212 x\u3009 .\nProof. Define the function g as g(x) = f(x) \u2212 \u00b52 \u2016x\u2016 2. Then the gradient is g\u2032(x) = f \u2032(x) \u2212 \u00b5x. g has a Lipschitz gradient with constant L\u2212 \u00b5. By convexity, we have [13, Thm. 2.1.5]: g(x) \u2265 g(y) + \u3008g\u2032(y), x\u2212 y\u3009+ 1\n2(L\u2212 \u00b5) \u2016g \u2032(x)\u2212 g\u2032(y)\u20162 .\nSubstituting in the definition of g and g\u2032, and simplifying the terms gives the result. Lemma 5. Let f(x) = 1n \u2211n i=1 fi(x). Suppose each fi is \u00b5-strongly convex and has Lipschitz continuous gradients with constant L. Then for all x and x\u2217:\u2329 f \u2032(x), x\u2217 \u2212 x \u232a \u2264 L\u2212 \u00b5\nL [f(x\u2217)\u2212 f(x)]\u2212\u00b5 2 \u2016x\u2217 \u2212 x\u20162\u2212 1 2Ln \u2211 i \u2225\u2225f \u2032i(x\u2217)\u2212 f \u2032i(x)\u2225\u22252\u2212\u00b5 L \u2329 f \u2032(x\u2217), x\u2212 x\u2217 \u232a .\nProof. This is a straight-forward corollary of Lemma 4, using y = x\u2217, and averaging over the fi functions.\nLemma 6. We have that for all \u03c6i and x\u2217:\n1\nn \u2211 i \u2016f \u2032i(\u03c6i)\u2212 f \u2032i(x\u2217)\u2016 2 \u2264 2L\n[ 1\nn \u2211 i fi(\u03c6i)\u2212 f(x\u2217)\u2212 1 n \u2211 i \u3008f \u2032i(x\u2217), \u03c6i \u2212 x\u2217\u3009 ] .\nProof. Apply the standard inequality f(y) \u2265 f(x) + \u3008f \u2032(x), y \u2212 x\u3009 + 12L \u2016f \u2032(x)\u2212 f \u2032(y)\u2016 2, with y = \u03c6i and x = x\u2217, for each fi, and sum.\nLemma 7. It holds that for any \u03c6ki , x\u2217, xk and \u03b2 > 0, with wk+1 as defined in Equation 1: E \u2225\u2225wk+1 \u2212 xk \u2212 \u03b3f \u2032(x\u2217)\u2225\u22252 \u2264 \u03b32(1 + \u03b2\u22121)E\u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252 + \u03b32(1 + \u03b2)E\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252\n\u2212 \u03b32\u03b2 \u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u22252 .\nProof. We follow a similar argument as occurs in the SVRG proof [5] for this term, but with a tighter argument. The tightening comes from using \u2016x+ y\u20162 \u2264 (1 + \u03b2\u22121) \u2016x\u20162 + (1 + \u03b2) \u2016y\u20162 instead of the simpler \u03b2 = 1 case they use. The other key trick is the use of the standard variance decomposition E[\u2016X \u2212 E[X]\u20162] = E[\u2016X\u20162]\u2212 \u2016E[X]\u20162 three times.\nE \u2225\u2225\u2225wk+1 \u2212 xk + \u03b3f \u2032(x\u2217)\u2225\u2225\u22252\n= E \u2225\u2225\u2225\u2225\u2212\u03b3n\u2211\ni\nf \u2032i(\u03c6 k i ) + \u03b3f \u2032(x\u2217) + \u03b3 [ f \u2032j(\u03c6 k j )\u2212 f \u2032j(xk) ] \ufe38 \ufe37\ufe37 \ufe38\n:= \u03b3X\n\u2225\u2225\u2225\u22252\n= \u03b32E \u2225\u2225\u2225\u2225\u2225 X\ufe37 \ufe38\ufe38 \ufe37[ f \u2032j(\u03c6 k j )\u2212 f \u2032j(x\u2217)\u2212 1 n \u2211 i f \u2032i(\u03c6 k i ) + f \u2032(x\u2217) ] \u2212 [ f \u2032j(x k)\u2212 f \u2032j(x\u2217)\u2212 E[X]\ufe37 \ufe38\ufe38 \ufe37 f \u2032(xk) + f \u2032(x\u2217) ]\u2225\u2225\u2225\u2225\u2225 2 + \u03b32 \u2225\u2225\u2225\u2225 E[X]\ufe37 \ufe38\ufe38 \ufe37 f \u2032(xk)\u2212 f \u2032(x\u2217) \u2225\u2225\u2225\u22252\n\u2264 \u03b32(1 + \u03b2\u22121)E \u2225\u2225\u2225\u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2212 1n\u2211 i f \u2032i(\u03c6 k i ) + f \u2032(x\u2217) \u2225\u2225\u2225\u2225\u2225 2\n+ \u03b32(1 + \u03b2)E \u2225\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2212 f \u2032(xk) + f \u2032(x\u2217)\u2225\u2225\u22252 + \u03b32 \u2225\u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u2225\u22252\n(use variance decomposition twice more): \u2264 \u03b32(1 + \u03b2\u22121)E \u2225\u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u2225\u22252 + \u03b32(1 + \u03b2)E \u2225\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u2225\u22252 \u2212 \u03b32\u03b2 \u2225\u2225\u2225f \u2032(xk)\u2212 f \u2032(x\u2217)\u2225\u2225\u22252 ."}, {"heading": "C Non-strongly-convex Problems", "text": "Theorem 2. When each fi is convex, using \u03b3 = 13L , we have for x\u0304 k = 1k \u2211k t=1 x t that:\nE [ F (x\u0304k) ] \u2212 F (x\u2217) \u2264 4n\nk\n[ 2L\nn \u2225\u2225x0 \u2212 x\u2217\u2225\u22252 + f(x0)\u2212 \u2329f \u2032(x\u2217), x0 \u2212 x\u2217\u232a\u2212 f(x\u2217)] . Here the expectation is over all choices of index jk up to step k.\nProof. A more detailed version of this proof is available in [14]. We proceed by using a similar argument as in Theorem 1, but we add an additional \u03b1 \u2225\u2225xk \u2212 x\u2217\u2225\u22252 together with the existing c \u2225\u2225xk \u2212 x\u2217\u2225\u22252 term in the Lyapunov function.\nWe will bound \u03b1 \u2225\u2225xk \u2212 x\u2217\u2225\u22252 in a different manner to c\u2225\u2225xk \u2212 x\u2217\u2225\u22252. Define \u2206 =\n\u2212 1\u03b3 ( wk+1 \u2212 xk ) \u2212 f \u2032(xk), the difference between our approximation to the gradient at xk and\ntrue gradient. Then instead of using the non-expansiveness property at the beginning, we use a result proved for prox-SVRG [4, 2nd eq. on p.12]:\n\u03b1E \u2225\u2225xk+1 \u2212 x\u2217\u2225\u22252 \u2264 \u03b1 \u2225\u2225xk \u2212 x\u2217\u2225\u22252 \u2212 2\u03b1\u03b3E [F (xk+1)\u2212 F (x\u2217)]+ 2\u03b1\u03b32E \u2016\u2206\u20162 .\nAlthough their quantity \u2206 is different, they only use the property that E[\u2206] = 0 to prove the above equation. A full proof of this property for the SAGA algorithm that follows their argument appears in [14].\nTo bound the \u2206 term, a small modification of the argument in Lemma 7 can be used, giving: E \u2016\u2206\u20162 \u2264 ( 1 + \u03b2\u22121 ) E \u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252 + (1 + \u03b2)E\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252 .\nApplying this gives: \u03b1E \u2225\u2225xk+1 \u2212 x\u2217\u2225\u22252 \u2264 \u03b1 \u2225\u2225xk \u2212 x\u2217\u2225\u22252 \u2212 2\u03b1\u03b3E [F (xk+1)\u2212 F (x\u2217)]\n+ 2(1 + \u03b2\u22121)\u03b1\u03b32E \u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252 + 2 (1 + \u03b2)\u03b1\u03b32E\u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252 .\nAs in Theorem 1, we then apply Lemma 6 to bound E \u2225\u2225f \u2032j(\u03c6kj )\u2212 f \u2032j(x\u2217)\u2225\u22252. Combining with the rest of the Lyapunov function as was derived in Theorem 1 gives (we basically add the \u03b1 terms to inequality (10) with \u00b5 = 0):\nE[T k+1]\u2212 T k \u2264 ( 1\nn \u2212 2c\u03b3\n)[ f(xk)\u2212 f(x\u2217)\u2212 \u2329 f \u2032(x\u2217), xk \u2212 x\u2217 \u232a] \u2212 2\u03b1\u03b3E [ F (xk+1)\u2212 F (x\u2217) ] + ( 4(1 + \u03b2\u22121)\u03b1L\u03b32 + 2(1 + \u03b2\u22121)cL\u03b32 \u2212 1\nn\n)[ 1\nn \u2211 i fi(\u03c6 k i )\u2212 f(x\u2217)\u2212 1 n \u2211 i \u2329 f \u2032i(x \u2217), \u03c6ki \u2212 x\u2217 \u232a]\n+ (\n(1 + \u03b2)c\u03b3 + 2(1 + \u03b2)\u03b1\u03b3 \u2212 c L\n) \u03b3E \u2225\u2225f \u2032j(xk)\u2212 f \u2032j(x\u2217)\u2225\u22252 .\nAs before, the terms in square brackets are positive by convexity. Given that our choice of step size is \u03b3 = 13L (to match the adaptive to strong convexity step size), we can set the three round brackets to zero by using \u03b2 = 1, c = 3L2n and \u03b1 = 3L 8n . We thus obtain:\nE[T k+1]\u2212 T k \u2264 \u2212 1 4n\nE [ F (xk+1)\u2212 F (x\u2217) ] .\nThese expectations are conditional on information from step k. We now take the expectation with respect to all previous steps, yielding E[T k+1] \u2212 E[T k] \u2264 \u2212 14nE [ F (xk+1)\u2212 F (x\u2217) ] , where all expectations are unconditional. Further negating and summing for k from 0 to k \u2212 1 results in telescoping of the T terms, giving:\n1\n4n E [ k\u2211 t=1 [ F (xt)\u2212 F (x\u2217) ]] \u2264 T 0 \u2212 E[T k].\nWe can drop the \u2212E [ T k ]\nterm since T k is always positive. Then we apply convexity to pull the summation inside of F , and multiply through by 4n/k, giving:\nE [ F ( 1\nk k\u2211 t=1 xt)\u2212 F (x\u2217) ] \u2264 1 k E [ k\u2211 t=1 [ F (xt)\u2212 F (x\u2217) ]] \u2264 4n k T 0.\nWe get a (c+ \u03b1) = 15L8n \u2264 2Ln term that we use in T 0 for simplicity."}, {"heading": "D Example Code for Sparse Least Squares & Ridge Regression", "text": "The SAGA method is quite easy to implement for dense gradients, however the implementation for sparse gradient problems can be tricky. The main complication is the need for just-in-time updating of the elements of the iterate vector. This is needed to avoid having to do any full dense vector operations at each iteration. We provide below a simple implementation for the case of least-squares problems that illustrates how to correctly do this. The code is in the compiled Python (Cython) language.\nimport random\nimport numpy as np\ncimport numpy as np\ncimport cython\nfrom cython.view cimport array as cvarray\n# Performs the lagged update of x by g.\ncdef inline lagged update(long k, double[:] x, double[:] g, unsigned long[:] lag,\nlong[:] yindices, int ylen, double[:] lag scaling , double a):\ncdef unsigned int i\ncdef long ind\ncdef unsigned long lagged amount = 0\nfor i in range(ylen):\nind = yindices[i] lagged amount = k\u2212lag[ind] lag[ind] = k x[ind] += lag scaling[lagged amount]\u2217(a\u2217g[ind])\n# Performs x += a\u2217y, where x is dense and y is sparse. cdef inline add weighted(double[:] x, double[:] ydata , long[:] yindices, int ylen, double a):\ncdef unsigned int i\nfor i in range(ylen):\nx[yindices[i]] += a\u2217ydata[i]\n# Dot product of a dense vector with a sparse vector\ncdef inline spdot(double[:] x, double[:] ydata , long[:] yindices, int ylen):\ncdef unsigned int i\ncdef double v = 0.0\nfor i in range(ylen):\nv += ydata[i]\u2217x[yindices[i]]\nreturn v\ndef saga lstsq(A, double[:] b, unsigned int maxiter, props):\n# temporaries\ncdef double[:] ydata\ncdef long[:] yindices\ncdef unsigned int i, j, epoch, lagged amount\ncdef long indstart , indend, ylen, ind\ncdef double cnew, Aix, cchange, gscaling\n# Data points are stored in columns in CSC format.\ncdef double[:] data = A.data\ncdef long[:] indices = A.indices\ncdef long[:] indptr = A.indptr\ncdef unsigned int m = A.shape[0] # dimensions\ncdef unsigned int n = A.shape[1] # datapoints\ncdef double[:] xk = np.zeros(m)\ncdef double[:] gk = np.zeros(m)\ncdef double eta = props[\u2019eta\u2019] # Inverse step size = 1/gamma\ncdef double reg = props.get(\u2019reg\u2019, 0.0) # Default 0\ncdef double betak = 1.0 # Scaling factor for xk.\n# Tracks for each entry of x, what iteration it was last updated at.\ncdef unsigned long[:] lag = np.zeros(m, dtype=\u2019I\u2019)\n# Initialize gradients cdef double gd = \u22121.0/n for i in range(n):\nindstart = indptr[i]\nindend = indptr[i+1]\nydata = data[indstart:indend]\nyindices = indices[indstart:indend] ylen = indend\u2212indstart add weighted(gk, ydata, yindices, ylen, gd\u2217b[i])\n# This is just a table of the sum the geometric series (1\u2212reg/eta) # It is used to correctly do the just\u2212in\u2212time updating when # L2 regularisation is used. cdef double[:] lag scaling = np.zeros(n\u2217maxiter+1) lag scaling[0] = 0.0\nlag scaling[1] = 1.0\ncdef double geosum = 1.0 cdef double mult = 1.0 \u2212 reg/eta for i in range(2,n\u2217maxiter+1):\ngeosum \u2217= mult lag scaling[i] = lag scaling[i\u22121] + geosum\n# For least\u2212squares, we only need to store a single # double for each data point, rather than a full gradient vector. # The value stored is the A i \u2217 betak \u2217 x product cdef double[:] c = np.zeros(n)\ncdef unsigned long k = 0 # Current iteration number\nfor epoch in range(maxiter):\nfor j in range(n):\nif epoch == 0:\ni = j\nelse:\ni = np.random.randint(0, n)\n# Selects the (sparse) column of the data matrix containing datapoint i.\nindstart = indptr[i]\nindend = indptr[i+1]\nydata = data[indstart:indend]\nyindices = indices[indstart:indend] ylen = indend\u2212indstart\n# Apply the missed updates to xk just\u2212in\u2212time lagged update(k, xk, gk, lag, yindices, ylen, lag scaling , \u22121.0/(eta\u2217betak))\nAix = betak \u2217 spdot(xk, ydata, yindices, ylen)\ncnew = Aix cchange = cnew\u2212c[i]\nc[i] = cnew betak \u2217= 1.0 \u2212 reg/eta\n# Update xk with sparse step bit (with betak scaling) add weighted(xk, ydata, yindices, ylen, \u2212cchange/(eta\u2217betak))\nk += 1\n# Perform the gradient\u2212average part of the step lagged update(k, xk, gk, lag, yindices, ylen, lag scaling , \u22121.0/(eta\u2217betak))\n# update the gradient average\nadd weighted(gk, ydata, yindices, ylen, cchange/n)\n# Perform the just in time updates for the whole xk vector, so that all entries are up\u2212to\u2212date. gscaling = \u22121.0/(eta\u2217betak) for ind in range(m):\nlagged amount = k\u2212lag[ind] lag[ind] = k\nxk[ind] += lag scaling[lagged amount]\u2217gscaling\u2217gk[ind] return betak \u2217 np.asarray(xk)"}], "references": [{"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "Technical report, INRIA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "JMLR, 14:567\u2013599,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Incrementally updated gradient methods for constrained and regularized optimization", "author": ["Paul Tseng", "Sangwoon Yun"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Stochastic dual coordinate ascent with alternating direction method of multipliers", "author": ["Taiji Suzuki"], "venue": "Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L. Bartlett", "Jonathan Baxter"], "venue": "JMLR, 5:1471\u20131530,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semi-stochastic gradient descent methods", "author": ["Jakub Kone\u010dn\u00fd", "Peter Richt\u00e1rik"], "venue": "ArXiv e-prints,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Finito: A faster, permutable incremental gradient method for big data problems", "author": ["Aaron Defazio", "Tiberio Caetano", "Justin Domke"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Incremental majorization-minimization optimization with application to largescale machine learning", "author": ["Julien Mairal"], "venue": "Technical report, INRIA Grenoble Rho\u0302ne-Alpes / LJK Laboratoire Jean Kuntzmann,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Technical report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Proximal Splitting Methods in Signal Processing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering", "author": ["Patrick Combettes", "Jean-Christophe Pesquet"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Remarkably, recent advances [1, 2] have shown that it is possible to minimise strongly convex finite sums provably faster in expectation than is possible without the finite sum structure.", "startOffset": 43, "endOffset": 49}, {"referenceID": 1, "context": "1 Introduction Remarkably, recent advances [1, 2] have shown that it is possible to minimise strongly convex finite sums provably faster in expectation than is possible without the finite sum structure.", "startOffset": 43, "endOffset": 49}, {"referenceID": 2, "context": "We will also consider the case where each fi is strongly convex with constant \u03bc, and the \u201ccomposite\u201d (or proximal) case where an additional regularisation function is added: F (x) = f(x) + h(x), where h : R \u2192 R is convex but potentially non-differentiable, and where the proximal operation of h is easy to compute \u2014 few incremental gradient methods are applicable in this setting [3][4].", "startOffset": 380, "endOffset": 383}, {"referenceID": 3, "context": "We will also consider the case where each fi is strongly convex with constant \u03bc, and the \u201ccomposite\u201d (or proximal) case where an additional regularisation function is added: F (x) = f(x) + h(x), where h : R \u2192 R is convex but potentially non-differentiable, and where the proximal operation of h is easy to compute \u2014 few incremental gradient methods are applicable in this setting [3][4].", "startOffset": 383, "endOffset": 386}, {"referenceID": 0, "context": "In Section 5 we prove theoretical convergence rates for SAGA in the strongly convex case better than those for SAG [1] and SVRG [5], and a factor of 2 from the SDCA [2] convergence rates.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "In Section 5 we prove theoretical convergence rates for SAGA in the strongly convex case better than those for SAG [1] and SVRG [5], and a factor of 2 from the SDCA [2] convergence rates.", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "In Section 5 we prove theoretical convergence rates for SAGA in the strongly convex case better than those for SAG [1] and SVRG [5], and a factor of 2 from the SDCA [2] convergence rates.", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "SAGA is inspired both from SAG [1] and SVRG [5] (as we will discuss in Section 3).", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "SAGA is inspired both from SAG [1] and SVRG [5] (as we will discuss in Section 3).", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "The method from [3], which handles the non-composite setting, is not listed as its rate is of the slow type and can be up to n times smaller than the one for SAGA or SVRG [5].", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "The method from [3], which handles the non-composite setting, is not listed as its rate is of the slow type and can be up to n times smaller than the one for SAGA or SVRG [5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "3 ? 3[6] 3 7 Non-smooth 7 7 3 7 7 Low Storage Cost 7 7 7 3 7 Simple(-ish) Proof 3 7 3 3 3 Adaptive to SC 3 3 7 ? ? Figure 1: Basic summary of method properties.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "In [5], the authors make the observation that the variance of the standard stochastic gradient (SGD) update direction can only go to zero if decreasing step sizes are used, thus preventing a linear convergence rate unlike for batch gradient descent.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "They thus propose to use a variance reduction approach (see [7] and references therein for example) on the SGD update in order to be able to use constant step sizes and get a linear convergence rate.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "They also mention that SAG (Stochastic Average Gradient) [1] can be interpreted as reducing the variance, though they do not provide the specifics.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "One variance reduction approach is to use the following estimator \u03b8\u03b1 as an approximation to EX: \u03b8\u03b1 := \u03b1(X\u2212Y )+EY , for a step size \u03b1 \u2208 [0, 1].", "startOffset": 135, "endOffset": 141}, {"referenceID": 7, "context": "The S2GD method [8] has the same update as SVRG, just differing in how the number of inner loop iterations is chosen.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "We now describe how the Finito [9] and MISO\u03bc [10] methods are closely related to SAGA.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "We now describe how the Finito [9] and MISO\u03bc [10] methods are closely related to SAGA.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "MISO has proven support for proximal operators only in the case where impractically small step sizes are used [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "SDCA The Stochastic Dual Coordinate Descent (SDCA) [2] method on the surface appears quite different from the other methods considered.", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "1 Firstly, note that while SDCA was originally described for onedimensional outputs (binary classification or regression), it has been expanded to cover the multiclass predictor case [11] (called Prox-SDCA there).", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "As noted by Shalev-Shwartz & Zhang [11], the update (9) is actually an instance of the proximal operator of the convex conjugate of fj .", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The same trick is commonly used to interpret Dijkstra\u2019s set intersection as a primal algorithm instead of a dual block coordinate descent algorithm [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "SDCA variants The SDCA theory has been expanded to cover a number of other methods of performing the coordinate step [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "The variants differ in how \u03b2 \u2208 [0, 1] is chosen.", "startOffset": 31, "endOffset": 37}, {"referenceID": 10, "context": "Variant 5 by Shalev-Shwartz & Zhang [11] does not require operations on the conjugate function, it simply uses \u03b2 = \u03bcn L+\u03bcn .", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "This procedure has been successfully used with SAG [1].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "A just-intime updating of u or x may be performed just as is suggested for SAG [1], which ensures that only sparse updates are done at each iteration.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Details of these datasets can be found in [9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "SVRG was tested with the recalibration pass used every n iterations, as suggested in [8].", "startOffset": 85, "endOffset": 88}], "year": 2014, "abstractText": "In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.", "creator": "LaTeX with hyperref package"}}}