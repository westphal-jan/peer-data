{"id": "1407.5158", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2014", "title": "Tight convex relaxations for sparse matrix factorization", "abstract": "Based on a new atomic standard, we propose a new convex formulation for sparse matrix factorization problems, where the number of unequal elements of the factors is fixed and known, taking into account sparse PCAs with multiple factors, subspace clusters, and low bilinear regression as potential applications. We calculate slow rates and an upper limit for the statistical dimension of the proposed standard for rank-1 matrices, and show that their statistical dimension is an order of magnitude smaller than the usual $\\ ell _ 1 $standard, tracking norm, and their combinations. Although our convex formulation is theoretically difficult and does not lead to demonstrable polynomial time schemes, we propose an active fixed algorithm that uses the structure of the convex problem to solve it and show promising numerical results.", "histories": [["v1", "Sat, 19 Jul 2014 07:04:08 GMT  (204kb)", "http://arxiv.org/abs/1407.5158v1", null], ["v2", "Thu, 4 Dec 2014 11:19:07 GMT  (576kb)", "http://arxiv.org/abs/1407.5158v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["emile richard", "guillaume obozinski", "jean-philippe vert"], "accepted": true, "id": "1407.5158"}, "pdf": {"name": "1407.5158.pdf", "metadata": {"source": "CRF", "title": "Tight convex relaxations for sparse matrix factorization", "authors": ["Emile Richard", "Guillaume Obozinski", "Jean-Philippe Vert"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n51 58\nv1 [\nst at\n.M L\n] 1"}, {"heading": "1 Introduction", "text": "A range of machine learning problems such as link prediction in graphs containing community structure (Richard et al., 2014), phase retrieval (Cand\u00e8s et al., 2013), subspace clustering (Wang et al., 2013) or dictionary learning for sparse coding (Mairal et al., 2010) amount to solve sparse matrix factorization problems, i.e., to infer a low-rank matrix that can be factorized as the product of two sparse matrices with few columns (left factor) and few rows (right factor). Such a factorization allows for more efficient storage, faster computation, more interpretable solutions, and, last but not least, it leads to more accurate estimates in many situations. In the case of interaction networks for example, the assumption that the network is organized as a collection of highly connected communities which can overlap implies that the adjacency matrix admits such a factorization. More generally, considering sparse low-rank matrices combines two natural forms of sparsity, in the spectrum and in the support, which can be motivated by the need to explain systems behaviors by a superposition of latent processes which only involve a few parameters. Landmark applications of sparse matrix factorization are sparse principal components analysis (SPCA, d\u2019Aspremont et al., 2007; Zou et al., 2006) or sparse canonical correlation analysis (SCCA, Witten et al., 2009), which are widely used to analyze high-dimensional data such as genomic data. From a computational point of view, however, sparse matrix factorization is challenging since it typically leads to non-convex, NP-hard problems (Moghaddam et al., 2006). For instance, Berthet and Rigollet (2013) noted that solving sparse PCA with a single component is equivalent to the planted clique\nproblem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component. These algorithms perform well empirically and have been proved to be efficient theoretically under mild conditions by Yuan and Zhang (2013). Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the \u21131 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise \u21131 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone. Krauthgamer et al. (2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works. Moreover, these existing convex formulations either aim at finding only a rank one matrix, or a low rank matrix whose factors themselves are not necessarily guaranteed to be sparse. In this work, we propose two new matrix norms which, when used as regularizer for various optimization problems, do yield estimates for low-rank matrices with multiple sparse factors that are provably more efficient statistically than the \u21131 and trace norms. The price to pay for this statistical efficiency is that, although convex, the resulting optimization problems are NP-hard, and we must resort to heuristic procedures to solve them. Our numerical experiments however confirm that we obtain the desired theoretical gain to estimate low-rank sparse matrices."}, {"heading": "1.1 Contributions and organization of the paper", "text": "More precisely, our contributions are:\n\u2022 Two new matrix norms (Section 2). In order to properly define matrix factorization, given sparsity levels of the factors denoted by integers k and q, we first introduce in Section 2.1 the (k, q)-rank of a matrix as the minimum number of left and right factors, having respectively k and q nonzeros, required to reconstruct a matrix. This index is a more involved complexity measure for matrices than the rank in that it conditions on the number of nonzero elements of the left and right factors of a matrix. Using this index, we propose in Section 2.2 two new atomic norms for matrices (Chandrasekaran et al., 2012). (i) Considering the convex hull unit\noperator norm matrices with (k, q)-rank = 1, we build a convex surrogate to low (k, q)-rank matrix estimation problem. (ii) We introduce a polyhedral norm built upon (k, q)-rank = 1 matrices with all non-zero entries of absolute value equal to 1. We provide in Section 2.3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al. (2012).\n\u2022 Using these norms to estimate sparse low-rank matrices (Section 3). We show how several problems such as bilinear regression or sparse PCA can be formulated as convex optimization problems with our new norms, and clarify that the resulting problems can however be NP-hard.\n\u2022 Statistical Analysis (Section 4). We study the statistical performance of the new norms and compare them with existing penalties. Our analysis goes first in Section 4.1 using slow rate type of upper bounds on the denoising error, which despite sub-optimality gives a first insight on the gap between the statistical performance of our (k, q)-trace norm and that of the \u21131 and trace norms. Next we show in Section 4.2, using cone inclusions and estimates of statistical dimension, that our norms are superior to any convex combination of the trace norm and the \u21131 norm in a number of different tasks. However, our analysis also shows that the factors gained over the rivals to estimate sparse low-rank matrices vanishes when we use our norm to estimate sparse vectors.\n\u2022 A working set algorithm (Section 5). While in the vector case the computation remains feasible in polynomial time, the norms we introduce for matrices can not be evaluated in polynomial time. We propose algorithmic schemes to approximately learn with the new norms. The same norms and meta-algorithms can be used as a regularizer in supervised problems such as bilinear and quadratic regression. Our algorithmic contribution does not consist in providing more efficient solutions to the rank-1 SPCA problem, but to combine atoms found by the rank-1 solvers in a principled way.\n\u2022 Numerical experiments (Section 6). We numerically evaluate the performance of our new norms on simulated data, and confirm the theoretical results. While our theoretical analysis only focuses on the estimation of sparse matrices with (k, q)-rank one, our simulations allow us to conjecture that the statistical dimension scales linearly with the (k, q)-rank and decays with the overlap between blocks. We also show that our model is competitive with the stateof-the-art on the problem of sparse PCA.\nDue to their length and technicality, all proofs are postponed to the appendices."}, {"heading": "1.2 Notations", "text": "For any integers 1 \u2264 k \u2264 p, [1, p] = {1, . . . , p} is the set of integers from 1 to p and Gpk denotes the set of subsets of k indices in [1, p]. For a vector w \u2208 Rp, \u2016w\u20160 is the number of non-zero coefficients in w, \u2016w\u20161 = \u2211p i=1 |wi| is its \u21131 norm, \u2016w\u20162 = (\u2211p i=1 w 2 i ) 1 2 is its Euclidean norm, \u2016w\u2016\u221e = maxi |wi| is its \u2113\u221e norm and supp(w) \u2208 Gp\u2016w\u20160 is its support, i.e., the set of indices of the nonzero entries of w. For any I \u2282 [1, p], wI \u2208 Rp is the vector that is equal to w on I, and has 0 entries elsewhere. Given matrices A and B of the same size, \u3008A,B\u3009 = tr(A\u22a4B) is the standard inner product of matrices. For any matrix Z \u2208 Rm1\u00d7m2 the notations \u2016Z\u20160, \u2016Z\u20161, \u2016Z\u2016\u221e, \u2016Z\u2016Fro, \u2016Z\u2016\u2217, \u2016Z\u2016op and rank(Z) stand respectively for the number of nonzeros, entry-wise \u21131 and \u2113\u221e norms, the standard \u21132 (or Frobenius) norm, the trace-norm (or nuclear norm, the sum of the singular values), the operator\nnorm (the largest singular value) and the rank of Z, while supp(Z) \u2282 [1,m1]\u00d7 [1,m2] is the support of Z, i.e., the set of indices of nonzero elements of Z. When dealing with a matrix Z whose nonzero elements form a block of size k \u00d7 q, supp(Z) takes the form I \u00d7 J where (I, J) \u2208 Gm1k \u00d7 Gm2q . For a matrix Z and two subsets of indices I \u2282 [1,m1] and J \u2282 [1,m2], ZI,J is the matrix having the same entries as Z inside the index subset I \u00d7 J , and 0 entries outside. This notation should not be confused with the notation Z(IJ) which we will sometimes use to denote a general matrix with support contained in I \u00d7 J ."}, {"heading": "2 Tight convex relaxations of sparse factorization constraints", "text": "In this section we propose two new matrix norms allowing to formulate various sparse matrix factorization problems as convex optimization problems. We start by defining the (k, q)-rank of a matrix in Section 2.1, a useful generalization of the rank which also quantifies the sparseness of a matrix factorization. We then introduce two atomic norms defined as tight convex relaxations of the (k, q)-rank in Section 2.2: the (k, q)-trace norm, obtained by relaxing the (k, q)-rank over the operator norm ball, and the (k, q)-CUT norm, obtained by a similar construction with extra-constraints on the element-wise \u2113\u221e of factors. In Section 2.3 we relate these matrix norms to vector norms using the concept of nuclear norms, establishing in particular a connection of the (k, q)-trace norm for matrices with the k-support norm of Argyriou et al. (2012), and the (k, q)-CUT norm to the vector k-norm, defined as the sum of the k largest components in absolute value of a vector (Bhatia, 1997, Exercise II.1.15).\n2.1 The (k, q)-rank of a matrix\nThe rank of a matrix Z \u2208 Rm1\u00d7m2 is the minimum number of rank-1 matrices (i.e., outer products of vectors of the form ab\u22a4 for a \u2208 Rm1 and b \u2208 Rm2) needed to express Z as a linear combination of the form Z = \u2211r i=1 aib \u22a4 i . It is a versatile concept in linear algebra, central in particular to solve matrix factorization problems and low-rank approximations. The following definition generalizes this notion to incorporate constraints on the sparseness of the rank-1 elements:\nDefinition 1 ((k, q)-SVD and (k, q)-rank) For a matrix Z \u2208 Rm1\u00d7m2 , we call (k, q)-sparse singular value decomposition of Z (or (k, q)-SVD) any decomposition of the form Z = \u2211r i=1 ciaib \u22a4 i where c1 \u2265 c2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 cr > 0, ai (resp. bi) are unit vectors with at most k (resp. q) nonzero elements, and with minimal r, which we call the (k, q)-rank of Z. In such a decomposition, we refer to vectors ai and bi as left and right (k, q)-sparse singular vectors of Z, and to ci as its (k, q)-sparse singular values.\nThe (k, q)-rank and (k, q)-SVD of Z can equivalently be defined as the optimal value and one of the solutions of the optimization problem:\nmin \u2016c\u20160 s.t. Z = \u221e\u2211\ni=1\nciaib \u22a4 i , (ai, bi, ci) \u2208 Am1k \u00d7Am2q \u00d7 R+ , (1)\nwhere for any 1 \u2264 j \u2264 n, Anj = {a \u2208 Rn : \u2016a\u20160 \u2264 j, \u2016a\u20162 = 1} .\nrefers to the set of n-dimensional unit vectors with at most j non-zero components. When k = m1 and q = m2, we recover the usual notions of rank and SVD of a matrix. In general, however, the (k, q)-rank and (k, q)-SVD do not share several important properties of the usual rank and SVD, as the following proposition shows:\nProposition 2 (Properties of the (k, q)-SVD)\n1. The (k, q)-rank of a matrix Z \u2208 Rm1\u00d7m2 can be strictly larger than m1 and m2.\n2. The (k, q)-SVD is not necessarily unique.\n3. The (k, q)-sparse singular vectors are not necessarily orthogonal to each other.\nFor k = q = 1, the (1, 1)-SVD decomposes Z as a sum of matrices with only one non-zero element, showing that (1, 1)-rank(Z) = \u2016Z\u20160. Since Ani \u2282 Anj when i \u2264 j, we deduce from the expression of the (k, q)-rank as the optimal value of (1) that the following tight inequalities hold:\n\u2200(k, q) \u2208 [1,m1]\u00d7 [1,m2] , rank(Z) \u2264 (k, q)-rank(Z) \u2264 \u2016Z\u20160 .\nThe (k, q)-rank is useful to formulate problems in which a matrix should be modeled as or approximated by a matrix with sparse low rank factors, with the assumption that the sparsity level of the factors is fixed and known. For example, the standard rank-1 SPCA problem consists in finding the symmetric matrix with (k, k)-rank equal to 1 and providing the best approximation of the sample covariance matrix (Zou et al., 2006).\n2.2 Two convex relaxations for the (k, q)-rank\nThe (k, q)-rank is obviously a discrete, nonconvex index, like the rank or the cardinality, leading to computational difficulties when one wants to estimate matrices with small (k, q)-rank. In this section, we propose two convex relaxations of the (k, q)-rank aimed at mitigating these difficulties. They are both instances of the atomic norms introduced by Chandrasekaran et al. (2012), which we first review.\nDefinition 3 (Atomic norm) Given a centrally symmetric compact subset A \u2282 Rp of elements called atoms, the atomic norm induced by A on Rp is the gauge function1 of A, defined by\n\u2016x\u2016A = inf {t > 0 : x \u2208 t conv (A)} , (2)\nwhere conv(A) denotes the convex hull of A.\nChandrasekaran et al. (2012) show that the atomic norm induced by A is indeed a norm, which can be rewritten as\n\u2016x\u2016A = inf { \u2211\na\u2208A ca : x =\n\u2211 a\u2208A caa, ca \u2265 0, \u2200a \u2208 A\n} , (3)\nand whose dual norm satisfies\n\u2016x\u2016\u2217A := sup {\u3008x, z\u3009 : \u2016z\u2016A \u2264 1} = sup {\u3008x, a\u3009 : a \u2208 A} . (4)\nWe can now define our first convex relaxation of the (k, q)-rank:\nDefinition 4 ((k, q)-trace norm) For a matrix Z \u2208 Rm1\u00d7m2 , the (k, q)-trace norm \u2126k,q(Z) is the atomic norm induced by the set of atoms:\nAk,q = { ab\u22a4 : a \u2208 Am1k , b \u2208 Am2q } . (5)\n1see Rockafellar (1997), p. 28, for a precise definition of gauge functions.\nIn words, Ak,q is the set of matrices Z \u2208 Rm1\u00d7m2 such that (k, q)-rank(Z) = 1 and \u2016Z\u2016op = 1. Plugging (5) into (3), we obtain an equivalent definition of the (k, q)-trace norm as the optimal value of the following optimization problem:\n\u2126k,q(Z) = min { \u2016c\u20161 : Z = \u221e\u2211\ni=1\nciaib \u22a4 i , (ai, bi, ci) \u2208 Am1k \u00d7Am2q \u00d7 R+\n} . (6)\nComparing (6) to (1) shows that the (k, q)-trace norm is derived from the (k, q)-rank by replacing the non-convex \u21130 pseudo-norm of c by its convex \u21131 norm in the optimization problem. In particular, in the case k = m1 and q = m2, the (k, q)-trace norm is the usual trace norm (equal to the \u21131-norm of singular values), i.e. the usual relaxation of the rank (which is the \u21130-norm of the singular values). Similarly, when k = q = 1, the (k, q)-trace norm is simply the \u21131 norm. Just like the (k, q)-rank interpolates between the \u21130 pseudo-norm and the rank, the (k, q)-trace norm interpolates between the \u21131 norm and the trace norm. Indeed, since Ani \u2282 Anj when i \u2264 j, we deduce from the expression of \u2126k,q as the optimal value of (6) that the following tight inequalities hold for any 1 \u2264 k \u2264 m1 and 1 \u2264 q \u2264 m2: \u2126m1,m2(Z) = \u2016Z\u2016\u2217 \u2264 \u2126k,q(Z) \u2264 \u2016Z\u20161 = \u21261,1(Z) . (7) While the SVD decomposition of a matrix used to define its trace norm is the same as the one used to define its rank, this may not be the case anymore for the (k, q)-trace norm. Indeed, in the general case, the (k, q)-trace norm may not be simply the sum of (k, q)-sparse singular values associated to a (k, q)-SVD according to Definition 1, because the vectors c that solve (6) and (1) can be different. This justifies the following definition:\nDefinition 5 (Soft-(k, q)-SVD and soft-(k, q)-rank) For a matrix Z \u2208 Rm1\u00d7m2 , we call soft(k, q)-sparse singular value decomposition (or soft-(k, q)-SVD) any decomposition Z = \u2211r i=1 ciaib \u22a4 i that solves (6) with c1 \u2265 c2 \u2265 . . . \u2265 cr > 0. The soft-(k, q)-rank of Z is the minimum number of terms in a soft-(k, q)-SVD of Z.\nSimilar to the (k, q)-SVD, the soft-(k, q)-SVD lacks many important properties of the trace norm when k < m1 and q < m2:\nProposition 6 1. The soft-(k, q)-rank of a matrix can be strictly larger than its (k, q)-rank.\n2. The soft-(k, q)-SVD is not necessarily unique.\n3. The soft-(k, q)-sparse singular vectors are not necessarily orthogonal to each other.\nIn addition to (6), the next lemma provides another explicit formulation for the (k, q)-trace norm, its dual and its sub differential:\nLemma 7 For any Z \u2208 Rm1\u00d7m2 we have\n\u2126k,q(Z) = inf   \n\u2211\n(I,J)\u2208Gm1 k \u00d7Gm2q\n\u2225\u2225\u2225Z(I,J) \u2225\u2225\u2225 \u2217 : Z = \u2211\n(I,J)\nZ(I,J) , supp(Z(I,J)) \u2282 I \u00d7 J    , (8)\nand \u2126\u2217k,q(Z) = max { \u2016ZI,J\u2016op : I \u2208 G m1 k , J \u2208 Gm2q } . (9)\nThe subdifferential of \u2126k,q at an atom A = ab \u22a4 \u2208 Ak,q with I0 = supp(a) and J0 = supp(b) is\n\u2202\u2126k,q(A) = { A+ Z : AZ\u22a4I0,J0 = 0, A \u22a4ZI0,J0 = 0, \u2200(I, J) \u2208 Gm1k \u00d7 Gm2q \u2016AI,J + ZI,J\u2016op \u2264 1 } .\n(10)\nOur second norm is again an atomic norm, but is obtained by focusing on a more restricted set of atoms. It is motivated by applications where we want to estimate matrices which, in addition to being sparse and low-rank, are constant over blocks, such as adjacency matrices of graphs with non-overlapping communities. For that purpose, consider first the subset of Amk made of vectors whose nonzero entries are all equal in absolute value:\nA\u0303mk = { a \u2208 Rm, \u2016a\u20160 = k , \u2200i \u2208 supp(a), |ai| = 1\u221ak } .\nWe can then define our second convex relaxation of the (k, q)-rank:\nDefinition 8 ((k, q)-CUT norm) We define the (k, q)-CUT norm \u2126\u0303k,q(Z) as the atomic norm induced by the set of atoms\nA\u0303k,q = { ab\u22a4 : a \u2208 A\u0303m1k , b \u2208 A\u0303m2q } . (11)\nIn other words, the atoms in A\u0303k,q are the atoms of Ak,q whose nonzero elements all have the same amplitude. Our choice of terminology is motivated by the following relation of our norm to the CUT-polytope: in the case k = m1 and q = m2, the unit ball of \u2126\u0303k,q coincides (up to a scaling factor of \u221a m1m2) with the polytope known as the CUT polytope of the complete graph on n vertices (Deza and Laurent, 1997), defined by CUT = conv {ab\u22a4 , a \u2208 {\u00b11}m1 , b \u2208 {\u00b11}m2} . The norm obtained as the gauge of the CUT polytope is therefore to the trace norm as \u2126\u0303k,q is to \u2126k,q."}, {"heading": "2.3 Equivalent nuclear norms built upon vector norms", "text": "In this section we show that the (k, q)-trace norm (Definition 4) and the (k, q)-CUT norm (Definition 8), which we defined as atomic norms induced by specific atom sets, can alternatively be seen as instances of nuclear norms considered by Jameson (1987). For that purpose it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1.9 and 1.11):\nProposition 9 (nuclear norm) Let \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2 denote any vector norms on Rm1 and Rm2, respectively, then\n\u03bd(Z) := inf\n{ \u2211\ni\n\u2016ai\u2016\u03b1 \u2016bi\u2016\u03b2 : Z = \u2211\ni\naib \u22a4 i\n} ,\nwhere the infimum is taken over all summations of finite length, is a norm over Rm1\u00d7m2 called the nuclear norm induced by \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2. Its dual is given by\n\u03bd\u2217(Z) = sup {a\u22a4Zb : \u2016a\u2016\u03b1 \u2264 1 , \u2016b\u2016\u03b2 \u2264 1} . (12)\nThe following lemma shows that the nuclear norm induced by two atomic norms is itself an atomic norm.\nLemma 10 If \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2 are two atomic norms on Rm1 and Rm2 induced respectively by two atom sets A1 and A2, then the nuclear norm on Rm1\u00d7m2 induced by \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2 is an atomic norm induced by the atom set:\nA = {ab\u22a4 : a \u2208 A1 , b \u2208 A2} .\nWe can deduce from it that the (k, q)-trace norm and (k, q)-CUT are nuclear norms, associated to particular vector norms:\nTheorem 11 1. The (k, q)-trace norm is the nuclear norm induced by \u03b8k on R m1 and \u03b8q on\nRm2 , where for any j \u2265 1, \u03b8j is the j-support norm introduced by Argyriou et al. (2012).\n2. The (k, q)-CUT norm is the nuclear norm induced by \u03bak on R m1 and \u03baq on R m2, where for any j \u2265 1:\n\u03baj(w) = 1\u221a j max\n( \u2016w\u2016\u221e, 1\nj \u2016w\u20161\n) . (13)\nFor the sake of completeness, let us recall the closed-form expression of the k-support norm \u03b8k shown by Argyriou et al. (2012). For any vector w \u2208 Rp, let w\u0304 \u2208 Rp be the vector obtained by sorting the entries of w by decreasing order of absolute values. Then it holds that\n\u03b8k(w) =    k\u2212r\u22121\u2211\ni=1\n|w\u0304i|2 + 1\nr + 1\n( p\u2211\ni=k\u2212r |w\u0304i|\n)2  1 2 , (14)\nwhere r \u2208 {0, \u00b7 \u00b7 \u00b7 , k \u2212 1} is the unique integer such that |w\u0304k\u2212r\u22121| > 1r+1 \u2211p\ni=k\u2212r |w\u0304i| \u2265 |w\u0304k\u2212r|, and where by convention |w\u03040| = \u221e. Of course, Theorem 11 implies that in the vector case (m2 = 1), the (k, q)-trace norm is simply equal to \u03b8k and the (k, q)-CUT norm is equal to \u03bak. A representation of the \u201csharp edges\" of unit balls of \u03b8k, \u03bak and a appropriately scaled \u21131 norm can be found in Figure 1 for the case m1 = 3 and k = 2. In addition, the following results shows that the dual norms of \u03b8k and \u03bak have simple explicit forms:\nProposition 12 The dual norms of \u03b8k and \u03bak satisfy respectively:\n\u03b8\u2217k(s) = max I:|I|=k \u2016sI\u20162 and \u03ba\u2217k(s) = 1\u221a k max I:|I|=k \u2016sI\u20161 .\nTo conclude this section, let us observe that nuclear norms provide a natural framework to construct matrix norms from vector norms, and that other choices beyond \u03b8k and \u03bak may lead to interesting norms for sparse matrix factorization. It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector \u21131-norm is simply the \u21131 of the matrix which fails to induce low rank (except in the very sparse case). However Bach et al. (2012) proposed nuclear norms associated with vectors norms that are similar to the elastic net penalty."}, {"heading": "3 Learning matrices with sparse factors", "text": "In this section, we briefly discuss how the (k, q)-trace norm and (k, q)-CUT norm can be used to attack various problems involving estimation of sparse low-rank matrices."}, {"heading": "3.1 Denoising", "text": "Suppose X \u2208 Rm1\u00d7m2 is a noisy observation of a low-rank matrix with sparse factors, assumed to have low (k, q)-rank. A natural convex formulation to recover the noiseless matrix is to solve:\nmin Z\n1 2 \u2016Z \u2212X\u20162Fro + \u03bb\u2126k,q(Z) , (15)\nwhere \u03bb is a parameter to be tuned. Note that in the limit when \u03bb \u2192 0, one simply obtains a soft-(k, q)-SVD of X."}, {"heading": "3.2 Bilinear regression", "text": "More generally, given some empirical risk L(Z), it is natural to consider formulations of the form min Z L(Z) + \u03bb\u2126k,q(Z)\nto learn matrices that are a priori assumed to have a low (k, q)-rank. A particular example is bilinear regression, where, given two inputs x \u2208 Rm1 and x\u2032 \u2208 Rm2 , one observes as output a noisy version of y = x\u22a4Zx\u2032. Assuming that Z has low (k, q)-rank means that the noiseless response is a sum of a small number of terms, each involving only a small number of features from either of the input vectors. To estimate such a model from observations (xi, x \u2032 i, yi)i=1,...,n, one can consider the following convex formulation:\nmin Z\nn\u2211\ni=1\n\u2113 ( x\u22a4i Zx \u2032 i, yi ) + \u03bb\u2126k,q(Z) , (16)\nwhere \u2113 is a loss function. A particular instance of (16) of interest is the quadratic regression problem, where m1 = m2 and xi = x \u2032 i for i = 1, . . . , n. Quadratic regression combined with additional constraints on Z is closely related to phase retrieval (Cand\u00e8s et al., 2013). It should be noted that if \u2113 is the least-square loss, (16) can be rewritten in the form\nmin Z\n1 2 \u2016X (Z)\u2212 y\u201622 + \u03bb\u2126k,q(Z) ,\nwhere X (Z) is a linear transformation of Z, so that the problem is from the point of view of the parameter Z a linear regression with a well chosen feature map."}, {"heading": "3.3 Subspace clustering.", "text": "In subspace clustering, one assumes that the data can be clustered in such a way that the points in each cluster belong to a low dimensional space. If we have a design matrix X \u2208 Rn\u00d7p with each row corresponding to an observation, then the previous assumption means that if X(j) \u2208 Rnj\u00d7p is a matrix formed by the rows of cluster j, there exist a low rank matrix Z(j) \u2208 Rnj\u00d7nj such that Z(j)X(j) = X(j). This means that there exists a block-diagonal matrix Z such that ZX = X with low-rank diagonal blocks. This idea, exploited recently by Wang et al. (2013) implies that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown. We therefore suggest that if all subspaces are of dimension k, Z may be estimated via\nmin Z \u2126k,k(Z) s.t. ZX = X ."}, {"heading": "3.4 Sparse PCA", "text": "In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors. Although this is similar to the denoising problem discussed in Section 3.1, one may wish in addition that the estimated sparse low-rank matrix be symmetric and positive semi-definite (PSD), in order to represent a plausible covariance matrix. This suggests to formulate sparse PCA as follows:\nmin Z {\u2225\u2225\u2225\u03a3\u0302n \u2212 Z \u2225\u2225\u2225 Fro : (k, k)-rank(Z) \u2264 r and Z 0 } , (17)\nwhere k is the maximum number of non-zero coefficient allowed in each principal direction. In contrast to sequential approaches that estimate the principal components one by one (Mackey, 2009), this formulation requires to find simultaneously a set of factors which are complementary to one another in order to explain as much variance as possible. A natural convex relaxation of (17) is\nmin Z\n{ 1\n2\n\u2225\u2225\u2225\u03a3\u0302n \u2212 Z \u2225\u2225\u2225 2\nFro + \u03bb\u2126k,k(Z) : Z 0\n} , (18)\nwhere \u03bb is a parameter that controls in particular the rank of the approximation. However, although the solution of (18) is always PSD, its soft-(k, k)-SVD Z = \u2211r i=1 ciaib \u22a4 i may not be composed of symmetric matrices (if ai 6= bi), and even if ai = bj the corresponding ci may be negative, as the following proposition shows:\nProposition 13 1. The (k, k)-SVD of a PSD matrix is not necessarily a sum of symmetric terms.\n2. Some PSD matrices cannot be written as a positive combination of rank one (k, k)-sparse matrices, even for k > 1.\nThis may be unappealing, as one would like to interpret the successive rank-1 matrices as covariance matrices over a subspace that explain some of the total variance. One may therefore prefer a decomposition with less sparse or more factors, potentially capturing less variance. One solution is to replace \u2126k,k in (18) by another penalty which directly imposes symmetric factors with non-negative weights. This is easily obtained by replacing the set of atoms Ak,k in Definition 4 by Ak, = {aa\u22a4, a \u2208 Ak}, and considering the corresponding atomic norm which we denote by \u2126k, . To be precise, \u2126k, is not a norm but only a gauge because the set Ak, is not centrally symmetric. Instead of (18), it possible to use the following convex formulation of sparse PCA:\nmin Z\n1\n2\n\u2225\u2225\u2225\u03a3\u0302n \u2212 Z \u2225\u2225\u2225 2\nFro + \u03bb\u2126k, (Z) . (19)\nBy construction, the solution of (19) is not only PSD, but can be expanded as a sum of matrices Z = \u2211r i=1 ciaia \u22a4 i , where for all i = 1, . . . , r, the factor ai is k-sparse and the coefficient ci is positive. This formulation is therefore particularly relevant if \u03a3\u0302n is believed to be a noisy matrix of this form. It should be noted however that, by Proposition 13, \u2126k, is infinite for some PSD matrices2, which implies that some PSD matrices cannot be approximated well with this formulation."}, {"heading": "3.5 NP-hard convex problems", "text": "Although the (k, q)-trace norm and related norms allow us to formulate several problems of sparse low-rank matrix estimation as convex optimization problems, it should be pointed out that this does not guarantee the existence of efficient computational procedures to solve them. Here we illustrate this with the special case of the best (k, q)-sparse and rank 1 approximation to a matrix, which turns out to be a NP-hard problem. Indeed, let us consider the three following optimization problems, which are equivalent since they return the same rank one subspace spanned by ab\u22a4:\nmin (a,b,c)\u2208Ak\u00d7Aq\u00d7R+ \u2016X \u2212 cab\u22a4\u20162Fro ; max (a,b)\u2208Ak\u00d7Aq a\u22a4Xb ; max Z: \u2126k,q(Z)\u22641 tr(XZ\u22a4) . (20)\nIn particular, if k = q and X = \u03a3\u0302n is an empirical covariance matrix, then the symmetric solutions of the problem considered are the solution to the following rank 1 SPCA problem\nmax z\n{ z\u22a4\u03a3\u0302nz : \u2016z\u20162 = 1 , \u2016z\u20160 \u2264 k } , (21)\nwhich it is known to be NP-hard (Moghaddam et al., 2008). This shows that, in spite of being a convex formulation involving the (k, q)-trace norm, the third formulation in (20) is actually NPhard. In practice, we will propose heuristics in Section 6 to approximate the solution of convex optimization problems involving the (k, q)-trace norm.\n4 Statistical properties of the (k, q)-trace norm and the (k, q)-CUT norm\nIn this section we study theoretically the benefits of using the new penalties \u2126k,q and \u2126\u0303k,q to infer low-rank matrices with sparse factors, as suggested in Section 3, postponing the discussion of how to do it in practice to Section 5. Building upon techniques proposed recently to analyze the statistical properties of sparsity-inducing penalties, such as the \u21131 penalty or more general atomic norms, we investigate two approaches to derive statistical guarantees. In Section 4.1 we study the expected dual norm of some noise process, from which we can deduce upper bounds on the learning rate for least squares regression and a simple denoising task. In Section 4.2 we estimate the statistical dimension of objects of interest both in the matrix and vector cases and compare the asymptotic rates, which shed light on the power of the norms we study when used as convex penalties. The results in Section 4.1 are technically easier to derive and contain bounds for a matrix of arbitrary (k, q)-rank. The results provided in Section 4.2 rely on a more involved set of tools, they provide more powerful bounds but we do not derive results for matrices of arbitrary (k, q)-rank.\n4.1 Performance of the (k, q)-trace norm in denoising\nIn this Section we consider the simple denoising setting (Section 3.1) where we wish to recover a low-rank matrix with sparse factors Z\u22c6 \u2208 Rm1\u00d7m2 from a noisy observation Y \u2208 Rm1\u00d7m2 corrupted\n2This is possible because \u2126k, is only a gauge and not a norm.\nby additive Gaussian noise: Y = Z\u22c6 + \u03c3G , where \u03c3 > 0 and G is a random matrix with entries i.i.d. from N (0, 1). Given a convex penalty \u2126 : Rm1\u00d7m2 \u2192 R, we consider, for any \u03bb > 0, the estimator\nZ\u0302\u03bb\u2126 \u2208 argmin Z\n1 2 \u2016Z \u2212 Y \u20162Fro + \u03bb\u2126(Z) .\nThe following result, valid for any norm \u2126, provides a general control of the estimation error in this setting, involving the dual norm of the noise: Lemma 14 If \u03bb \u2265 \u03c3\u2126\u2217(G) then \u2225\u2225\u2225Z\u0302\u03bb\u2126 \u2212 Z\u22c6 \u2225\u2225\u2225 2\nFro \u2264 4\u03bb\u2126(Z\u22c6) .\nThis suggests to study the dual norm of a random noise matrix \u2126\u2217(G) in order to derive a upper bound on the estimation error. The following result provides such upper bounds, in expectation, for the (k, q)-trace norm as well as the standard \u21131 and trace norms: Proposition 15 Let G \u2208 Rm1\u00d7m2 be a random matrix with entries i.i.d. from N (0, 1). The expected dual norm of G for the (k, q)-trace norm, the \u21131 norm and the trace norm is respectively bounded by:\nE\u2126\u2217k,q(G) \u2264 4 (\u221a\nk log m1 k + 2k +\n\u221a q log\nm2 q + 2q\n) ,\nE \u2016G\u2016\u22171 \u2264 \u221a 2 log(m1m2) , E \u2016G\u2016\u2217\u2217 \u2264 \u221a m1 + \u221a m2 .\n(22)\nTo derive an upper bound in estimation errors from these inequalities, we consider for simplicity3 the oracle estimate Z\u0302Oracle\u2126 equal to Z\u0302 \u03bb \u2126 where \u03bb = \u03c3\u2126\n\u2217(G). From Lemma 14 we immediately get the following control of the mean estimation error of the oracle estimator, for any penalty \u2126:\nE \u2225\u2225\u2225Z\u0302Oracle\u2126 \u2212 Z\u22c6 \u2225\u2225\u2225 2\nFro \u2264 4\u03c3\u2126(Z\u22c6)E \u2126\u2217(G) . (23)\nWe can now derive upper bounds in estimation errors for the different penalties in the so-called single spike model, where the signal Z\u22c6 consists of an atom ab\u22a4 \u2208 Ak,q, and we observed a noisy matrix Y = ab\u22a4+\u03c3G. Since for an atom ab\u22a4 \u2208 Ak,q while \u2016ab\u22a4\u20161 \u2264 kq/ \u221a (kq) = \u221a kq, \u2126k,q(ab\n\u22a4) = \u2016ab\u22a4\u2016\u2217 = 1, we immediately get the following by plugging the upper bounds of Proposition 15 into (23):\nCorollary 16 When Z\u22c6 \u2208 Ak,q is an atom, the expected errors of the oracle estimators using respectively the (k, q)-trace norm, the \u21131 norm and the trace norm are respectively upper bounded by:\nE \u2225\u2225\u2225Z\u0302Oracle\u2126k,q \u2212 Z \u22c6 \u2225\u2225\u2225 2\nFro \u2264 8 \u03c3\n(\u221a k log\nm1 k + 2k +\n\u221a q log\nm2 q + 2q\n) ,\nE \u2225\u2225\u2225Z\u0302Oracle1 \u2212 Z\u22c6 \u2225\u2225\u2225 2\nFro \u2264 2\u03c3\u2016Z\u22c6\u20161\n\u221a 2 log(m1m2) \u2264 2\u03c3 \u221a 2kq log(m1m2) ,\nE \u2225\u2225\u2225Z\u0302Oracle\u2217 \u2212 Z\u22c6 \u2225\u2225\u2225 2\nFro \u2264 2\u03c3(\u221am1 + \u221a m2) .\n(24)\nTo make the comparison easy, orders of magnitudes of these upper bounds are gathered in Table 1 for the case where Z\u22c6 \u2208 A\u0303k,q, and for the case where m1 = m2 = m and k = q = \u221a m. In the later\n3Similar bounds could be derived with large probability for the non-oracle estimator by controlling the deviations of \u2126\u2217(G) from its expectation.\ncase, we see in particular that the (k, q)-trace norm has a better rate than the \u21131 and trace norms, in m 1 4 instead of m 1 2 (up to logarithmic terms). Note that the largest value of \u2016Z\u22c6\u20161 is reached\nwhen Z\u22c6 \u2208 A\u0303k,q and equals \u221a kq. By contrast, when Z\u22c6 \u2208 Ak,q gets far from A\u0303k,q elements then the expected error norm diminishes for the \u21131-penalized denoiser Z\u0302 Oracle 1 reaching \u03c3 \u221a 2 log(m1m2) on e1e \u22a4 1 while not changing for the two other norms.\nObviously the comparison of upper bounds is not enough to conclude to the superiority of (k, q)-trace norm and, admittedly, the problem of denoising considered here is a special instance of linear regression in which the design matrix is the identity, and, since this is a case in which the design is trivially incoherent, it is possible to obtain fast rates for decomposable norms such as the \u21131 or trace norm (Negahban et al., 2012); however, slow rates are still valid in the presence of an incoherent design, or when the signal to recover is only weakly sparse, which is not the case for the fast rates. Moreover, the result proved here is valid for matrices of rank greater than 1. We present in the next section more involved results, based on lower and upper bounds on the so-called statistical dimension of the different norms (Amelunxen et al., 2013), a measure which is closely related to Gaussian widths."}, {"heading": "4.2 Performance through the statistical dimension", "text": "Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty. These results rely essentially on the fact that if the tangent cone4 of the regularizer at a point of interest Z is thiner, then the regularizer is more efficient at solving problems of denoising, demixing and compressed sensing of Z. The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al. (2013). In this section, we study the statistical dimensions induced by different matrix norms in order to compare their theoretical properties for exact or approximate recovery of sparse low-rank matrices. In particular, we will consider the norms \u2126k,q, \u2126\u0303k,q and linear combinations of the \u21131 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012). For convenience we therefore introduce the notation \u0393\u00b5 for the norm that linearly interpolates between the trace norm and the (scaled) \u21131 norm:\n\u2200\u00b5 \u2208 [0, 1], \u2200Z \u2208 Rm1\u00d7m2 , \u0393\u00b5(Z) := \u00b5\u221a kq \u2016Z\u20161 + (1\u2212 \u00b5) \u2016Z\u2016\u2217 , (25)\nso that \u03930 is the trace norm and \u03931 is the \u21131 norm up to a constant 5.\n4As detailed later, the tangent cone is the closure of the cone of descent directions. 5Note that the scaling ensures that \u0393\u00b5(A) = 1 for \u00b5 \u2208 [0, 1] and A \u2208 A\u0303k,q ."}, {"heading": "4.2.1 The statistical dimension and its properties", "text": "Let us first briefly recall what the statistical dimension of a convex regularizer \u2126 : Rm1\u00d7m2 \u2192 R refers to, and how it is related to efficiency of the regularizer to recover a matrix Z \u2208 Rm1\u00d7m2 . For that purpose, we first define the tangent cone T\u2126(Z) of \u2126 at Z as the closure of the cone of descent directions, i.e.,\nT\u2126(Z) := \u22c3\n\u03c4>0\n{H \u2208 Rm1\u00d7m2 : \u2126(Z + \u03c4H) \u2264 \u2126(Z)} . (26)\nThe statistical dimension S(Z,\u2126) of \u2126 at Z can then be formally defined as\nS(Z,\u2126) :=E [\u2225\u2225\u03a0T\u2126(Z)(G) \u2225\u22252 Fro ] , (27)\nwhere G is a random matrix with i.i.d. standard normal entries and \u03a0T\u2126(Z)(G) is the orthogonal projection of G onto the cone T\u2126(Z). The statistical dimension is a powerful tool to quantify the statistical performance of a regularizer in various contexts, as the following non-exhaustive list of results shows.\n\u2022 Exact recovery with random measurements. Suppose we observe y = X (Z\u22c6) where X : Rm1\u00d7m2 \u2192 Rn is a random linear map represented by random design matrices Xi i = 1, . . . , n having iid entries drawn from N (0, 1/n). Then Chandrasekaran et al. (2012, Corollary 3.3) shows that\nZ\u0302 = argmin Z\n\u2126(Z) s.th. X (Z) = y (28)\nis equal to Z\u22c6 with overwhelming probability as soon as n \u2265 S(Z\u22c6,\u2126). In addition Amelunxen et al. (2013, Theorem II) show that a phase transition occurs at n = S(Z\u22c6,\u2126) between a situation where recovery fails with large probability (for n \u2264 S(Z\u22c6,\u2126)\u2212 \u03b3\u221am1m2, for some \u03b3 > 0) to a situation where recovery works with large probability (for n \u2265 S(Z\u22c6,\u2126) + \u03b3\u221am1m2).\n\u2022 Robust recovery with random measurements. Suppose we observe y = X (Z\u22c6) + \u01eb where X is again a random linear map, and in addition the observation is corrupted by a random noise \u01eb \u2208 Rn. If the noise is bounded as \u2016\u01eb\u20162 \u2264 \u03b4, then Chandrasekaran et al. (2012, Corollary 3.3) show that\nZ\u0302 = argmin Z\n\u2126(Z) s.th. \u2016X (Z)\u2212 y\u20162 \u2264 \u03b4 (29)\nsatisfies \u2225\u2225\u2225Z\u0302 \u2212 Z\u22c6 \u2225\u2225\u2225 Fro\n\u2264 2\u03b4/\u03b7 with overwhelming probability as soon as n \u2265 (S(Z\u22c6,\u2126) + 3 2)/(1 \u2212 \u03b7)2.\n\u2022 Denoising. Assume a collection of noisy observations Xi = Z\u22c6 + \u03c3\u01ebi for i = 1, \u00b7 \u00b7 \u00b7 , n is available where \u01ebi \u2208 Rm1\u00d7m2 has i.i.d. N (0, 1) entries, and let Y = 1n \u2211n i=1 Xi denote their\naverage. Chandrasekaran and Jordan (2013, Proposition 4) prove that\nZ\u0302 = argmin Z\n\u2016Z \u2212 Y \u2016Fro s.th. \u2126(Z) \u2264 \u2126(Z\u22c6) (30)\nsatisfies E \u2225\u2225\u2225Z\u0302 \u2212 Z\u22c6 \u2225\u2225\u2225 2\nFro \u2264 \u03c32n S(Z\u22c6,\u2126).\n\u2022 Demixing. Given two matrices Z\u22c6, V \u22c6 \u2208 Rm1\u00d7m2 , suppose we observe y = U(Z\u22c6) + V \u22c6 where U : Rm1\u00d7m2 7\u2192 Rm1\u00d7m2 is a random orthogonal operator. Given two convex functions \u0393,\u2126 : Rm1\u00d7m2 \u2192 R, Amelunxen et al. (2013, Theorem III) show that\n(Z\u0302, V\u0302 ) = argmin (Z,V )\n\u2126(Z) s.th. \u0393(V ) \u2264 \u0393(V \u22c6) and y = U(Z) + V\nis equal to (Z\u22c6, V \u22c6) with probability at least 1\u2212 \u03b7 provided that\nS(Z\u22c6,\u2126) +S(V \u22c6,\u0393) \u2264 m1m2 \u2212 4 \u221a m1m2 log 4\n\u03b7 .\nConversely if S(Z\u22c6,\u2126) +S(V \u22c6,\u0393) \u2265 m1m2 + 4 \u221a m1m2 log 4 \u03b7 , the demixing fails with probability at least 1\u2212 \u03b7."}, {"heading": "4.2.2 Some cone inclusions and their consequences", "text": "In this and subsequent sections, we wish to compare the behavior of \u2126k,q and \u2126\u0303k,q and \u0393\u00b5, as defined in (25). Before estimating and comparing the statistical dimensions of these norms, which requires rather technical proofs, let us first show through simple geometric arguments that for a number of matrices, the tangent cones of the different norms are actually nested. This will allow us to derive deterministic improvement in performance when a norm is used as regularizer instead of another, which should be contrasted with the kind of guarantees that will be derived from bounds on the statistical dimension and which are typically statements holding with very high probability. The results in this section are proved in Appendix C.\nProposition 17 The norms considered satisfy the following equalities and inequalities:\n\u2200\u00b5 \u2208 [0, 1], \u2200Z \u2208 Rm1\u00d7m2 , \u0393\u00b5(Z) \u2264 \u2126k,q(Z) \u2264 \u2126\u0303k,q(Z), \u2200\u00b5 \u2208 [0, 1], \u2200A \u2208 A\u0303k,q, \u0393\u00b5(A) = \u2126k,q(A) = \u2126\u0303k,q(A) = 1.\nPut informally, the unit balls of \u2126\u0303k,q, \u2126k,q and of all convex combinations of the trace norm and the scaled \u21131-norm are nested and meet for matrices in A\u0303k,q. This property is illustrated in the vector case (for \u00b5 = 1) on Figure 1. In fact A\u0303k,q is a subset of the extreme points of the unit norms of all those norms except for the scaled \u21131-norm (corresponding to the case \u00b5 = 1). Given that the unit balls meet on A\u0303k,q and are nested, their tangent cones on A\u0303k,q must also be nested: Corollary 18 The following nested inclusions of tangent cones hold:\n\u2200\u00b5 \u2208 [0, 1], \u2200A \u2208 A\u0303k,q, T\u0393\u00b5(A) \u2283 T\u2126k,q (A) \u2283 T\u2126\u0303k,q(A) . (31)"}, {"heading": "As a consequence, for any A \u2208 A\u0303k,q, the statistical dimensions of the different norms satisfy:", "text": "S(A, \u2126\u0303k,q) \u2264 S(A,\u2126k,q) \u2264 S(A,\u0393\u00b5) . (32) As reviewed in Section 4.2.1, statistical dimensions provide estimates for the performance of the different norms in different contexts. Plugging (32) in these results shows that to estimate an atom in A\u0303k,q, using \u2126\u0303k,q is at least as good as using \u2126k,q which itself is at least as good as using any convex combination of the \u21131 and trace norms. Note that the various statements in Section 4.2.1 provide upper bounds on the performance of the different norms, with are guarantees that are either probabilistic or hold in expectation. In fact, the inclusion of the tangent cones (31) and a fortiori the tangential inclusion of the unit balls imply much stronger results since it can also lead some deterministic statements, such as the following:\nCorollary 19 (Improvement in exact recovery) Consider the problem of exact recovery of a matrix Z\u2217 \u2208 A\u0303k,q from random measurements y = X (Z\u2217) by solving (28) with the different norms. For any realization of the random measurements, exact recovery with \u0393\u00b5 for any 0 \u2264 \u00b5 \u2264 1 implies exact recovery with \u2126k,q which itself implies exact recovery with \u2126\u0303k,q.\nNote that in the vector case (m2 = 1), where the (k, q)-trace norm \u2126k,1 boils down to the k-support norm \u03b8k, the tangent cone inclusion (31) is not always strict:\nProposition 20 For any a \u2208 A\u0303mk , T\u03931(a) = T\u03b8k(a). In words, the tangent cone of the \u21131 norm and of the the k-support norm are equal on k-sparse vectors with constant non-zero entries, which can be observed in Figure 1. This suggests that, in the vector case, the k-support norm is not better than the \u21131 norm to recover such constant sparse k-vectors."}, {"heading": "4.2.3 Bounds on the statistical dimensions", "text": "The results presented in Section 4.2.2 apply only to a very specific set of matrices (A\u0303k,q), and do not characterize quantitatively the relative performance of the different norms. In this Section, we turn to more explicit estimations of the statistical dimension of the different norms at atoms in A\u0303k,q and Ak,q. We consider first the statistical dimension of the (k, q)-CUT norm \u2126\u0303k,q on its atoms A\u0303k,q. The unit ball of \u2126\u0303k,q is a vertex-transitive polytope with 2 k+q (m1\nk )(m2 q ) vertices. As a consequence, it\nfollows immediately from Corollary 3.14 in Chandrasekaran et al. (2012) and from the upper bound log (m k ) \u2264 k(1 + log(m/k)), that6\nProposition 21 For any A \u2208 A\u0303k,q, we have\nS(A, \u2126\u0303k,q) \u2264 16(k + q) + 9 ( k log\nm1 k + q log m2 q\n) .\nUpper bounding the statistical dimension of the (k, q)-trace norm on its atoms Ak,q requires more work. First, atoms with very small coefficients are likely to be more difficult to estimate than atoms with large coefficients only. In the vector case, for example, it is known that the recovery of a sparse vector \u03b2 with support I0 depends on its smallest coefficient \u03b2min = mini\u2208I0 \u03b2 2 i (Wainwright, 2009). The ratio between \u03b2min and the noise level can be thought of as the worst signal-to-noise ratio for the signal \u03b2. We generalize this idea to atoms in Ak,q as follows. Definition 22 (Atom strength) Let A = ab\u22a4 \u2208 Ak,q with I0 = supp(a) and J0 = supp(b). Denote a2min = mini\u2208I0 a 2 i and b 2 min = minj\u2208J0 b 2 j . The atom strength \u03b3(a, b) \u2208 (0, 1] is\n\u03b3(a, b) := (k a2min) \u2227 (q b2min).\nNote that the atoms with maximal strength value 1 are the elements of A\u0303k,q. With this notion in hand we can now formulate an upper bound on the statistical dimension of \u2126k,q:\nProposition 23 For A = ab\u22a4 \u2208 Ak,q with strength \u03b3 = \u03b3(a, b), we have\nS(A,\u2126k,q) \u2264 322\n\u03b32 (k + q + 1) +\n160\n\u03b3 (k \u2228 q) log (m1 \u2228m2) . (33)\n6This result is actually stated informally for the special case of k = q = \u221a m = with m = m1 = m2 in the context\nof a discussion of the planted clique problem in Chandrasekaran and Jordan (2013).\nNote that the upper bounds obtained on atoms of A\u0303k,q for \u2126\u0303k,q (Proposition 21) and \u2126k,q (Proposition 23, with \u03b3 = 1) have the same rate up to k log k + q log q which is negligible compared to k logm1 + q logm2 when k \u226a m1 and q \u226a m2. Note that once the support is specified, the number of degrees of freedom for elements of A\u0303k,q is k + q \u2212 1, which is matched up to logarithmic terms. It is interesting to compare these estimates to the statistical dimension of the \u21131 norm, the trace norm, and their combinations \u0393\u00b5. Table 2 summarizes the main results. The statistical dimension the \u21131 norm on atoms in A\u0303k,q is of order kq log(m1m2/(kq)), which is worse than the statistical dimensions of \u2126k,q and \u2126\u0303k,q by a factor k \u2227 q. On Ak,q, though, the statistical dimension of \u2126k,q increases when the atom strength decreases, while the statistical dimension of the \u21131 norm is independent of it and even decreases when the size of the support decreases. As for the trace norm alone, its statistical dimension is at least of order m1 + m2, which is unsurprisingly much worse that the statistical dimensions of \u2126k,q and \u2126\u0303k,q since it does not exploit the sparsity of the atoms. Finally, regarding the combination \u0393\u00b5 of the \u21131 norm and of the trace norm, Oymak et al. (2012) has shown that it does not improve rates up to constants over the best of the two norms. More precisely, we can derive from Oymak et al. (2012, Theorem 3.2) the following result\nProposition 24 There exists M > 0 and C > 0 such that for any m1,m2, k, q \u2265 M with m1/k \u2265 M and m2/q \u2265 M , for any A \u2208 Ak,q and for any \u00b5 \u2208 [0, 1], the following holds:\nS (A,\u0393\u00b5) \u2265 C \u03b6(a, b) ( (kq) \u2227 (m1 +m2 \u2212 1) ) \u2212 2 ,\nwith\n\u03b6(a, b) = 1\u2212 ( 1\u2212 \u2016a\u2016 2 1\nk\n)( 1\u2212 \u2016b\u2016 2 1\nq\n) .\nNote that \u03b6(a, b) \u2264 1 with equality if either a \u2208 A\u0303m1k or b \u2208 A\u0303m2q , so in particular \u03b6(a, b) = 1 for ab\u22a4 \u2208 A\u0303k,q. In that case, we see that, as stated by Oymak et al. (2012), \u0393\u00b5 does not bring any improvement over the \u21131 and trace norms taken imdividually, and in particular has a worse statistical dimension than \u2126k,q and \u2126\u0303k,q."}, {"heading": "4.2.4 The vector case", "text": "We have seen in Section 4.2.3 that the statistical dimension of the (k, q)-trace norm and of the (k, q)-CUT norm were smaller than that of the \u21131 and the trace norms, and of their combinations,\nmeaning that theoretically they are more efficient regularizers to recover rank-one sparse matrices. In this section, we look more precisely at these properties in the vector case (m2 = q = 1), and show that, surprisingly, the benefits are lost in this case. Remember that, in the vector case, \u2126k,q boils down to the k-support norm \u03b8k (14), while \u2126\u0303k,q boils down to the norm \u03bak (13). For the later, we can upper bound the statistical dimension at a k-sparse vector by specializing Proposition 21 to the vector case, and also derive a specific lower bound as follows:\nProposition 25 For any k-sparse vector a \u2208 A\u0303pk,\nk\n2\u03c0 log ( p\u2212 k k + 1 ) \u2264 S(a, \u03bak) \u2264 9k log p k + 16(k + 1) .\nFrom the explicit formulation of \u03b8k (14) we can derive an upper bound of the statistical dimension of \u03b8k on any sparse vector with at least k non-zero coefficients:\nProposition 26 For any s \u2265 k, the statistical dimension of the k-support norm \u03b8k at an s-sparse vector w \u2208 Rp is bounded by\nS(w, \u03b8k) \u2264 5\n4 s+ 2\n{ (r + 1)2 \u2016w\u0303I2\u201622\n\u2016w\u0303I1\u201621 + |I1|\n} log p\ns , (34)\nwhere w\u0303 \u2208 Rp denotes the vector with the same entries as w sorted by decreasing absolute values, r is as defined in equation (14), I2 = [1, k \u2212 r \u2212 1] and I1 = [k \u2212 r, s]. In particular, when s = k, the following holds for any atom a \u2208 Apk with strength \u03b3 = ka2min:\nS(a, \u03b8k) \u2264 5\n4 k +\n2k\n\u03b3 log\np k . (35)\nWe note that (35) has the same rate but tighter constants than the general upper bound (33) specialized to the vector case. In particular, this suggests that the \u03b3\u22122 term in (35) may not be required. In the lasso case (k = 1), we recover the standard bound (Chandrasekaran et al., 2012):\nS(w, \u03b8k) \u2264 5\n4 s+ 2s log\np s , (36)\nwhich is also reached by \u03b8k on an atom a \u2208 A\u0303pk because in that case \u03b3 = 1 in (35). On the other hand, for general atoms in Apk the upper bound (35) is always worse than the upper bound for the standard Lasso (36), and more generally the upper bound for general sparse vectors (34) is also never better than the one for the Lasso. Although these are only upper bounds, this raises questions on the utility of the k-support norm compared to the lasso to recover sparse vectors. The statistical complexities of the different regularizers in the vector case are summarized in Table 2. We note that, contrary to the low-rank sparse matrix case, the \u21131-norm, the k-support norm, and the norm \u03bak all have the same statistical dimension up to constants. Note that the tangent cone of the elastic net equals the tangent cone of the \u21131-norm in any point (because the tangent cone of the \u21132 norm is a half space that always contains the tangent cone of the \u21131-norm) so that the elastic net has always the exact same statistical dimension as the \u21131-norm."}, {"heading": "5 Algorithms", "text": "As seen in Section 3, many problems involving sparse low-rank matrix estimation can be formulated as optimization problems of the form:\nmin Z\u2208Rm1\u00d7m2\nL(Z) + \u03bb\u2126k,q(Z). (37)\nUnfortunately, although convex, this problem may be computationally challenging (Section 3.5). In this section, we present a working set algorithm to approximately solve such problems in practice when L is differentiable."}, {"heading": "5.1 A working set algorithm", "text": "Given a set S \u2282 Gm1k \u00d7 Gm2q of pairs of row and column subsets, let us consider the optimization problem:\nmin (Z(IJ))\n(I,J)\u2208S\n  L ( \u2211\n(I,J)\u2208S Z(IJ)\n) + \u03bb \u2211\n(I,J)\u2208S\n\u2225\u2225Z(IJ) \u2225\u2225 \u2217 : \u2200(I, J) \u2208 S, supp(Z (IJ)) \u2282 I \u00d7 J    . (PS)\nLet (Z\u0302(IJ))(I,J)\u2208S be a solution of this optimization problem. Then, by the characterization of \u2126k,q(Z) in (8), Z = \u2211 (I,J)\u2208S Z\u0302 (IJ) is the solution of (37) when S = Gm1k \u00d7 Gm2q . Clearly, it is still the solution of (37) if S is reduced to the set of non-zero matrices Z\u0302(IJ) at optimality often called active components. We propose to solve problem (37) using a so-called working set algorithm which solves a sequence of problems of the form (PS) for a growing sequence of working sets S, so as to keep a small number of non-zero matrices Z(IJ) throughout. Working set algorithms (Bach et al., 2011, Chap. 6) are typically useful to speed up algorithm for sparsity inducing regularizer; they have been used notably in the case of the overlapping group Lasso of Jacob et al. (2009) which is also naturally formulated via latent components. To derive the algorithm we write the optimality condition for (PS):\n\u2200(I, J) \u2208 S , \u2207L(Z)IJ \u2208 \u2212\u03bb\u2202 \u2225\u2225\u2225Z(IJ) \u2225\u2225\u2225 \u2217 .\nFrom the characterization of the subdifferential of the trace norm (Watson, 1992), writing Z(IJ) = U (IJ)\u03a3(IJ)V (IJ) the SVD of Z(IJ), this is equivalent to, for all (I, J) in S,\neither Z(IJ) 6=0 and \u2207L(Z)IJ = \u2212\u03bb ( U (IJ)V (IJ) \u22a4 +A )\nwith \u2016A\u2016op \u2264 1 and AU (IJ) = A\u22a4V (IJ) = 0 , (38) or Z(IJ)=0 and \u2016\u2207L(Z)]IJ\u2016op \u2264 \u03bb . (39)\nThe principle of the working set algorithm is to solve problem (PS) for the current set S so that (38) and (39) are (approximately) satisfied for (I, J) in S, and to check subsequently if there are any components not in S which violate (39). If not, this guarantees that we have found a solution to problem (37), otherwise the new pair (I, J) corresponding to the most violated constraint is added to S and problem (PS) is initialized with the previous solution and solved again. The resulting algorithm is Algorithm 1 (where the routine SSVDTPI is described in the next section). Problem (PS) is solved easily using the approximate block coordinate descent of Tseng and Yun (2009) (see\nalso Bach et al., 2011, Chap. 4), which consists in iterating proximal operators. The modifications to the algorithm to solve problems regularized by the norm \u2126k, are relatively minor (they amount to replace the trace norms by penalization of the trace of the matrices Z(IJ) and by positive definite cone constraints) and we therefore do not describe them here. Determining efficiently which pair (I, J) possibly violates condition (39) is in contrast a more difficult problem that we discuss next.\nAlgorithm 1 Active set algorithm Require: L, tolerance \u01eb > 0, parameters \u03bb, k, q Set S = \u2205, Z = 0 while c = true do\nRecompute optimal values of Z, (Z(IJ))(I,J)\u2208S for (PS) using warm start (I, J) \u2190 SSVDTPI(\u2207L(Z), k, q, \u01eb) if \u2016[\u2207L(Z)]I,J\u2016op > \u03bb then S \u2190 S \u222a {(I, J)} else c \u2190 false\nend if end while return Z, S, (Z(IJ))(I,J)\u2208S"}, {"heading": "5.2 Finding new active components", "text": "Once (PS) is solved for a given set S, (38) and (39) are satisfied for all (I, J) \u2208 S. Note that (38) implies in particular that \u2016\u2207L(Z)]IJ\u2016op = \u03bb when Z(IJ) 6= 0 at optimality. Therefore, (39) is also satisfied for all (I, J) /\u2208 S if and only if\nmax (I,J)\u2208Gm1\nk \u00d7Gm2q\n\u2016[\u2207L(Z)]IJ\u2016op \u2264 \u03bb , (40)\nand if this is not the case then any (I, J) that violates this condition is a candidate to be included in S. This corresponds to solving the following sparse singular value problem\nmax a,b\na\u22a4\u2207L(Z)b s.t. ab\u22a4 \u2208 Ak,q . (k, q)-linRank-1\nThis problem is unfortunately NP-hard since rank 1 sparse PCA problem is a particular instance of it (when \u2207L(Z) is replaced by a covariance matrix), and we therefore cannot hope to solve it exactly with efficient algorithms. Still, sparse PCA has been the object of a significant amount of research, and several relaxations and other heuristics have been proposed to solve it approximately. In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions. Algorithm 2 provides a natural generalization of this algorithm to the non-PSD case. The algorithm follows the steps of a power method, the standard method for computing leading singular vectors of a matrix, with the difference that at each iteration a truncation step is use. We denote the truncation operator by Tk. It consists of keeping the k largest components (in absolute value) and setting the others to 0. Note that Algorithm 2 may fail to find a new active component for Algorithm 1 if it\nAlgorithm 2 SSVDTPI: Bi-truncated power iteration for (k, q)-linRank-1 Require: A \u2208 Rm1\u00d7m2 , k, q and tolerance \u01eb > 0 Pick a random initial point b(0) \u223c N (0, Im2) and let while |a(t)\u22a4Ab(t) \u2212 a(t\u22121)\u22a4Ab(t\u22121)|/|a(t\u22121) \u22a4Ab(t\u22121)| > \u01eb do\na \u2190 Ab(t) \\\\ Power a \u2190 Tk(a) \\\\ Truncate b \u2190 A\u22a4a \\\\ Power b \u2190 Tq(b) \\\\ Truncate a(t+1) \u2190 a/\u2016a\u20162 and b(t+1) \u2190 b/\u2016b\u20162 \\\\ Normalize t \u2190 t+ 1\nend while I \u2190 Supp(a(t)) and J \u2190 Supp(b(t)) return (I, J)\nfinds a local maximum of ((k, q)-linRank-1) smaller than \u03bb, and therefore result in the termination of Algorithm 1 on a suboptimal solution. On the positive side, note that Algorithm 1 is robust to some errors of Algorithm 2. For instance, if an incorrect component is added to S at some iteration, but the correct components are identified later, Algorithm 1 will eventually shrink the incorrect components to 0. One of the causes of failure of TPI type of methods is the presence of a large local maximum in the sparse PCA problem corresponding to a suboptimal component; incorporating this component in S will reduce the size of that local maximum, thereby increasing the chance of selecting a correct component the next time around."}, {"heading": "5.3 Computational cost", "text": "Note that when m1,m2 are large, solving PS involves the minimizations of trace norms of matrices of size k \u00d7 q which, when k and q are small compared to m1 and m2 have low computational cost. The bottleneck for providing a computational complexity of the algorithm is the (k, q)-linRank-1 step. It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time. If the conditions hold at every step of gradient, the overall cost of an iteration can be cast into the cost of evaluating the gradient and the evaluation of thin SVDs: O(k2q). Evaluating the gradient has a cost dependent on the risk function L. This cost for usual applications is O(m1m2). So assuming the RIP conditions required by Yuan and Zhang (2013) hold, the cost of Algorithm 2 is dominated by matrix-vector multiplications so of the order O(m1m2). The total cost of the algorithm for reaching a \u03b4-accurate solution is therefore O((m1m2 + k\n2q)/\u03b4). However the worst case complexity of the algorithm is non-polynomial as (k, q)-linRank-1 is nonpolynomial in general. We would like to point out that in our numerical experiments a warm start with singular vectors and multiple runs of the algorithm (k, q)-linRank-1 keeping track of the highest found variance has provided us a very fast and reliable solver. Further discussion on this step go beyond the scope of this work."}, {"heading": "6 Numerical experiments", "text": "In this section we report experimental results to assess the performance of sparse low-rank matrix estimation using different techniques. We start in Section 6.1 with simulations aiming at validating the theoretical results on statistical dimension of \u2126k,q and assessing how they generalize to matrices\nwith (k, q)-rank larger than 1. In Section 6.2 we compare several techniques for sparse PCA on simulated data."}, {"heading": "6.1 Empirical estimates of the statistical dimension.", "text": "In order to numerically estimate the statistical dimension S(Z,\u2126) of a regularizer \u2126 at a matrix Z, we add to Z a random Gaussian noise matrix and observe Y = Z + \u03c3G where G has normal i.i.d. entries following N (0, 1). We then denoise Y using (30) to form an estimate Z\u0302 of Z. For small \u03c3, the normalized mean-squared error (NMSE) defined as\nNMSE(\u03c3) := E\n\u2225\u2225\u2225Z\u0302 \u2212 Z \u2225\u2225\u2225 2\nFro\n\u03c32\nis a good estimate of the statistical dimension, since Oymak and Hassibi (2013) show that\nS(Z,\u2126) = lim \u03c3\u21920 NMSE(\u03c3) .\nNumerically, we therefore estimate S(Z,\u2126) by taking \u03c3 = 10\u22124 and measuring the empirical NMSE averaged over 20 repeats. We consider square matrices with m1 = m2 = 1000, and estimate the statistical dimension of \u2126k,q, the \u21131 and the trace norms at different matrices Z. The constrained denoiser (30) has a simple close-form for the \u21131 and the trace norm. For \u2126k,q, it can be obtained by a series of proximal projections (15) with different parameters \u03bb until \u2126k,q(Z\u0302) has the correct value \u2126k,q(Z). Since the noise is small, we found that it was sufficient and faster to perform a soft-(k, q)-SVD of Y by solving (15) with a small \u03bb, and then apply the \u21131 constrained denoiser to the set of soft-(k, q)-sparse singular values. We first estimate the statistical dimensions of the three norms at an atom Z \u2208 A\u0303k,q, for different values of k = q. Figure 2 (top left) shows the results, which confirm the theoretical bounds summarized in Table 2. The statistical dimension of the trace norm does not depend on k, while that of the \u21131 norm increases almost quadratically with k and that of \u2126k,q increases linearly with k. As expected, \u2126k,q interpolates between the \u21131 norm (for k = 1) and the trace norm (for k = m1), and outperforms both norms for intermediary values of k. This experiments therefore confirms that our upper bound (33) on S(Z,\u2126k,q) captures the correct order in k, although the constants can certainly be much improved, and that Algorithm 1 manages, in this simple setting, to correctly approximate the solution of the convex minimization problem. Second, we estimate the statistical dimension of \u2126k,q on matrices with (k, q)-rank larger than 1, a setting for which we proved no theoretical result. Figure 2 (top left) shows the numerical estimate of S(Z,\u2126k,q) for matrices Z which are sums of r atoms in A\u0303k,k with non-overlapping support, for k = 10 and varying r. We observe that the increase in statistical dimension is roughly linear in the (k, q)-rank. For a fixed (k, q)-rank of 3, the bottom plots of Figure 2 compare the estimated statistical dimensions of the three regularizers on matrices Z which are sums of 3 atoms in A\u0303k,k with non-overlapping (bottom left) or overlapping (bottom right) supports. The shapes of the different curves are overall similar to the rank 1 case, although the performance of \u2126k,q degrades as the supports of atoms overlap. In both cases, \u2126k,q consistently outperforms the two other norms. Overall these experiments suggest that the statistical dimension of \u2126k,q at a linear combination of r atoms increases as Cr (k logm1 + q logm2) where the coefficient C increases with the overlap among the supports of the atoms."}, {"heading": "6.2 Comparison of algorithms for sparse PCA", "text": "In this section we compare the performance of different algorithms in estimating a sparsely factored covariance matrix that we denote \u03a3\u22c6. The observed sample consists of n random vector vectors generated i.i.d. according to N (0,\u03a3\u22c6+\u03c32Idp), where (k, k)-rank(\u03a3\u22c6) = 3. The matrix \u03a3\u22c6 is formed by adding 3 blocks of rank 1, \u03a3\u22c6 = a1a \u22a4 1 +a2a \u22a4 2 +a3a \u22a4 3 , having all the same sparsity \u2016ai\u20160 = k = 10,\n3\u00d73 overlaps and nonzero entries equal to 1/ \u221a k. See Figure 3, bottom right plot for a representation of the ground truth \u03a3\u22c6. The noise level \u03c3 = 0.8 is set in order to make the signal to noise ratio below the level \u03c3 = 1 where a spectral gap appears and makes the spectral baseline (penalizing the trace of the PSD matrix) work. In our experiments the number of variables is p = 200 and n = 80 points are observed. To estimate the true covariance matrix from the noisy observation, first the sample covariance matrix is formed as\n\u03a3\u0302n = 1\nn\nn\u2211\ni=1\nxix \u22a4 i ,\nand given as input to various algorithms which provide a new estimate \u03a3\u0302. The methods we compared are the following:\n\u2022 Raw sample covariance. The most basic is to output \u03a3\u0302n as the estimate of the covariance, which is not accurate due to presence of noise and underdeterminedness n < p.\n\u2022 Trace penalty on the PSD cone. This spectral algorithm solves the following optimization problem in the cone of PSD matrices:\nmin Z 0\n1\n2\n\u2225\u2225\u2225Z \u2212 \u03a3\u0302n \u2225\u2225\u2225 2\nFro + \u03bbTrZ .\n\u2022 \u21131 penalty. In order to approximate the sample covariance \u03a3\u0302n by a sparse matrix a basic idea is to soft-threshold it element-by-element. This is equivalent to solving the following convex optimization problem:\nmin Z\n1\n2\n\u2225\u2225\u2225Z \u2212 \u03a3\u0302n \u2225\u2225\u2225 2\nFro + \u03bb\u2016Z\u20161 .\n\u2022 Trace + \u21131 penalty. The restriction of \u0393\u00b5 to the PSD cone, which is equivalent to solving the following SDP\nmin Z 0\n1\n2\n\u2225\u2225\u2225Z \u2212 \u03a3\u0302n \u2225\u2225\u2225 2\nFro + \u03bb\u0393\u00b5(Z) .\nThis approach needs to tune two parameters \u03bb > 0, \u00b5 \u2208 [0, 1].\n\u2022 Sequential sparse PCA. This is the standard way of estimating multiple sparse principal components which consists of solving the problem for a single component at each step t = 1 \u00b7 \u00b7 \u00b7 r, and deflate to switch to the next (t+ 1)st component. The deflation step used in this algorithm is the orthogonal projection\nZt+1 = (Idp \u2212 utu\u22a4t )Zt (Idp \u2212 utu\u22a4t ) .\nThe tuning parameters for this approach are the sparsity level k and the number of principal components r.\nSample covariance Trace \u21131 Trace + \u21131 Sequential \u2126k, 4.20 \u00b1 0.02 0.98 \u00b1 0.01 2.07 \u00b1 0.01 0.96 \u00b1 0.01 0.93 \u00b1 0.08 0.59 \u00b1 0.03\nWe report the relative errors \u2225\u2225\u2225\u03a3\u0302\u2212 \u03a3\u22c6 \u2225\u2225\u2225 Fro\n/ \u2016\u03a3\u22c6\u2016Fro over 10 runs of our experiments in Table 3, and a representation of the estimated matrices can be found in Figure 3. We observe that sparse PCA methods using \u2126k, and also the sequential method using deflation steps outperform spectral and \u21131 baselines. In addition, penalizing \u2126k, is superior to the sequential approach. This was expected since our algorithm minimizes a loss function that is close to the test errors reported, whereas the sequential scheme does not optimize a well-defined objective."}, {"heading": "7 Conclusion", "text": "In this work, we proposed two new convex penalties, the (k, q)-trace norm and the (k, q)-CUT norm, specifically tailored to the estimation of low-rank matrices with sparse factors. Our motivation for proposing such convex formulations for sparse low-rank matrix inference was twofold. First, it allowed us to consider algorithmic schemes that are better understood when a problem is formulated as a convex optimization problem, even though the complexity of solving the problem exactly remains super-polynomial. Second, using convex geometry allowed us to provide sample complexity and statistical guarantees, and notably to show that the proposed estimators have much better statistical dimension than more standard convex combinations of the \u21131 and trace norms. We observed that the improvement exists only for matrices: for sparse vectors, using our penalty (which boils down to the k-support norm in this case) does not improve over the standard \u21131 norm, in terms of statistical dimension increase rate. One limitation of this work is that we assume that the sparsity of the factors is known and fixed. Lifting this constraint and investigating procedures that can adapt to the size of the blocks (like the \u21131 norm adapts to the size of the support) is an interesting direction for future research. Another interesting direction is to use the nuclear norm formulation of the (k, q)-trace norm as in Lemma 10 to optimize the regularized problem."}, {"heading": "Acknowledgments", "text": "We would like to thank Francis Bach for interesting discussions related to this work. This work was supported by the European Research Council (SMAC-ERC-280032)."}, {"heading": "A Proofs of results in Sections 2 and 3.", "text": "Proof [Proposition 2] To prove the first claim, note that a matrix of the form ab\u22a4 for a \u2208 Am1k and b \u2208 Am2q has at most kq non-zero terms. Therefore, the decomposition of a matrix with no null entries as a linear combination of such sparse matrices must count at least m1m2kq terms, which is larger than m1 \u2228m2 when kq \u2264 m1 \u2227m2. To prove second claim, consider the case of the (2, 2)-SVD for the matrix Z = 11\u22a4 \u2208 R3. It is impossible to write Z as the sum of two (2, 2)-sparse matrices, because it would then have at most 8 non-zero coefficients. But we have the decomposition.\n( 1 1 1 1 1 1 1 1 1 ) = ( 2 1 0 1 1 2 0 0 0 0 ) + ( 0 0 1 0 1 2 1 0 1 2 ) \u2212 ( 1 0 \u22121 0 0 0 \u22121 0 1 ) ,\nwhich shows that the (2, 2)-rank of Z is 3. Now, given that Z is invariant by any of the 6 permutations of the rows and any of the 6 permutations of the columns, Z admits at least 36 different (2, 2)-SVDs. To prove the third claim, observe that the decomposition proposed above for Z = 11\u22a4 \u2208 R3 yields 9 left- and right-(2, 2)-sparse singular vectors that are obviously not orthogonal. It can actually be\nshown by systematic enumeration of all possible cases that it is impossible to find any (2, 2)-sparseSVD of Z whose left or right singular vectors are orthogonal.\nProof [Proposition 6] To prove the first claim, let us consider the matrix Z = 11\u22a4 \u2208 R3. We showed in the proof of Proposition 2 above that its (2, 2)-rank is equal to 3. We now show that its soft-(2, 2)-rank is equal to 9. For that purpose, we express any soft-(2, 2)-SVD of Z as a minimizer of (8), and write the corresponding Lagrangian:\nL((Z(IJ))I,J ,K) = \u2211\nI,J\u2208G2\n\u2225\u2225Z(IJ) \u2225\u2225 \u2217 + tr ( K\u22a4 ( Z \u2212 \u2211\nI,J\u2208G2 Z(IJ)\n)) ,\nwhere (Z(IJ))I,J and K are the primal and dual variables. It is easy to check that the dual solution is the unique subgradient of \u21262,2 at Z which is equal to K\n\u2217 = 12Z. But any primal solution must satisfy tr(K\u2217\u22a4Z(IJ)) = \u2016Z(IJ)\u2016\u2217. This implies that any primal solution (Z(IJ))I,J satisfies Z(IJ) \u221d 1I1\u22a4J . Then, one can check that ((12)1I1 \u22a4 J )I,J\u2208G2 forms a basis of R\n3\u00d73 so that any matrix Z admits a unique set of decomposition coefficients on that basis. This proves that the unique solution of (8) is the one such that Z(IJ) = 141I1 \u22a4 J for all pairs (I, J) \u2208 G2 \u00d7 G2. This unique soft-(k, q)-SVD is composed of 9 terms, meaning that the soft-(k, q)-rank of Z is 9 while its (k, q)-rank is 3. To prove the second claim, let us consider the soft-(2, 2)-SVDs of Z = 1211\n\u22a4 \u2208 R4. By proposition 17, 1 2\u2016Z\u20161 \u2264 \u21262,2(Z), but 12\u2016Z\u20161 = 4 and 2Z = (1{1,2} + 1{3,4})(1{1,2} + 1{3,4})\u22a4 which shows that \u21262,2(Z) \u2264 4. So \u21262,2(Z) = 4. Considering that there are 3 ways to partition {1, 2, 3, 4} into sets of cardinality 2, Z admits at least 9 different optimal decompositions in the sense of the (2, 2)-softSVD since Z can be written in 9 different ways as the sum of four matrices of A\u03032,2 with disjoint supports. Each of these decompositions attains the (2, 2)-rank which is equal to 4. Note also that by convexity any convex combination of these decompositions is also an optimal decomposition in the sense of the soft-(2, 2)-SVD, but can contain up to 36 terms! To prove the third claim, let us consider\nZ1 = (\n1 1 0 1 1 0 0 0 0\n) , Z2 = ( 0 0 0 0 1 1 0 1 1 ) , Z = Z1 + Z2 = ( 1 1 0 1 2 1 0 1 1 ) .\nAs Z1, Z2, Z are all positive semidefinite we have \u2016Z1\u2016\u2217 = 2, \u2016Z2\u2016\u2217 = 2, and \u2016Z\u2016\u2217 = 4. By inequality (7), \u21262,2(Z) \u2265 \u2016Z\u2016\u2217 = 4 which proves that the decomposition Z = Z1 + Z2 is optimal: \u21262,2(Z) = 4. But \u3008Z1, Z2\u3009 = 1. So this decomposition is a decomposition of Z onto linear combination of atoms 12Z1, 1 2Z2 \u2208 A2,2 which are not orthogonal.\nProof [Lemma 7] We first show (9) from the definition of the dual norm \u2126\u2217k,q:\n\u2126\u2217k,q(Z) = max K {\u3008K,Z\u3009 : \u2126k,q(K) \u2264 1}\n= max a,b\n{\u3008Z, ab\u22a4\u3009 : ab\u22a4 \u2208 Ak,q}\n= max a,b\n{a\u22a4Zb : \u2016a\u20160 \u2264 k , \u2016b\u20160 \u2264 q , \u2016a\u20162 = \u2016b\u20162 = 1}\n= max I,J\n{ \u2016ZI,J\u2016op : I \u2208 G m1 k , J \u2208 Gm2q } ,\nwhere the second equality follows from the fact that the maximization of a linear form over a bounded convex set is attained at one of the extreme points of the set. Given this closed-form expression of the dual norm, we prove the variational formulation (8) for the primal norm \u2126k,q. Consider the function \u02c7\u2126k,q defined by\n\u02c7\u2126k,q(Z) = inf   \n\u2211\n(I,J)\u2208Gm1 k \u00d7Gm2q\n\u2225\u2225\u2225Z(I,J) \u2225\u2225\u2225 \u2217 : Z = \u2211\n(I,J)\nZ(I,J) , supp(Z(I,J)) \u2282 I \u00d7 J    .\nSince \u02c7\u2126k,q(Z) is defined as the infimum of a jointly convex function of Z and (Z (I,J))I\u2208Gm1\nk , J\u2208Gm2q\nobtained by minimizing w.r.t. to the latter variables, it is a an elementary fact from convex analysis that \u02c7\u2126k,q is a convex function of Z. It is also symmetric and positively homogeneous, which together with convexity prove that \u02c7\u2126k,q defines a norm. We can compute its dual norm as\n\u02c7\u2126k,q \u2217 (K) = max\nZ\n{ \u3008K,Z\u3009 : \u02c7\u2126k,q(Z) \u2264 1 }\n= max (Z(IJ))(I,J)\n  \u3008K, \u2211\n(I,J)\nZ(IJ)\u3009 : \u2211\n(I,J)\n\u2225\u2225Z(IJ) \u2225\u2225 \u2217 \u2264 1 , supp(Z (IJ)) \u2282 I \u00d7 J   \n= max (Z(IJ))(I,J),(\u03b7 (IJ))(I,J)\n   \u2211\n(I,J)\n\u03b7(I,J)\u3008KI,J , Z(IJ)\u3009 : \u2225\u2225Z(IJ) \u2225\u2225 \u2217 \u2264 \u03b7 (IJ), \u2211\n(I,J)\n\u03b7(IJ) \u2264 1   \n= max (\u03b7(IJ))(I,J)\n   \u2211\n(I,J)\n\u03b7(IJ) \u2016KI,J\u2016op : \u2211\n(I,J)\n\u03b7(IJ) \u2264 1   \n= max (I,J) \u2016KI,J\u2016op = \u2126\u2217k,q(K) .\nThis proves that \u2126k,q(K) = \u02c7\u2126k,q(K) since a norm is uniquely characterized by its dual norm. Finally, to show (10) we use the general characterization of the subdifferential of a norm (e.g., Watson, 1992):\nG \u2208 \u2202\u2126k,q(A) \u21d4 { \u2126k,q(A) = \u3008G,A\u3009 , \u2126\u2217k,q(G) \u2264 1 .\nLet us denote a subgradient by G = A + Z. Since A = ab\u22a4 is an atom, we have \u2126k,q(A) = 1. In addition, \u2016A\u20162Fro = Tr(ba\u22a4ab\u22a4) = 1, therefore the condition \u2126k,q(A) = \u3008G,A\u3009 boils down to \u3008Z,A\u3009 = 0. Given the characterization of the dual norm (9), we therefore get:\n\u2202\u2126k,q(A) = { A+ Z : \u3008A,Z\u3009 = 0, \u2200(I, J) \u2208 Gm1k \u00d7 Gm2q \u2016AI,J + ZI,J\u2016op \u2264 1 } .\nLet now\nD(A) = { A+ Z : AZ\u22a4I0,J0 = 0, A \u22a4ZI0,J0 = 0, \u2200(I, J) \u2208 Gm1k \u00d7 Gm2q \u2016AI,J + ZI,J\u2016op \u2264 1 } .\nSince \u3008A,Z\u3009 = \u3008A,ZI0,J0\u3009 = Tr (A\u22a4ZI0,J0), it is clear that D(A) \u2282 \u2202\u2126k,q(A). Conversely, let G = A+Z \u2208 \u2202\u2126k,q(A). Then \u3008A,Z\u3009 = \u3008ab\u22a4, ZI0,J0\u3009 = a\u22a4ZI0,J0b = 0, and therefore, by Pythagorean equality applied to the orthogonal vectors a and ZI0,J0b:\n\u2016(AI0,J0 + ZI0,J0) b\u201622 = \u2016ab\u22a4b+ ZI0,J0b\u2016 2 2 = \u2016a+ ZI0,J0b\u2016 2 2 = 1 + \u2016ZI0,J0b\u2016 2 2 ,\nbut since \u2016AI0,J0 + ZI0,J0\u2016op \u2264 1 and \u2016b\u20162 = 1 we must have \u2016ZI0,J0b\u20162 = 0. This shows that AZ\u22a4I0,J0 = ab \u22a4Z\u22a4I0,J0 = 0. The same reasoning starting with the orthogonal vectors b and Z \u22a4 I0,J0\na shows that we also have A\u22a4ZI0,J0 = 0, implying that \u2202\u2126k,q(A) \u2282 D(A). This concludes the proof that \u2202\u2126k,q(A) = D(A), as claimed in (10).\nProof [Lemma 10] Let \u03bd be the nuclear norm induced by two atomic norms \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2 , induced themselves respectively by the two atom sets A1 and A2. Let A = { ab\u22a4 : a \u2208 A1 , b \u2208 A2 } and B = Conv ( A ) , then the key argument is to note that we have\n{ ab\u22a4 : \u2016a\u2016\u03b1 \u2264 1, \u2016b\u2016\u03b2 \u2264 1 } \u2282 B .\nIndeed, if a = \u2211 i \u03bbiai and b = \u2211 j \u03bb \u2032 jbj with ai \u2208 A1, bj \u2208 A2 and \u2211 i \u03bbi = \u2211 j \u03bb \u2032 j = 1, then with \u00b5ij := \u03bbi\u03bb \u2032 j, we have ab \u22a4 = \u2211 i,j \u00b5ijaib \u22a4 j and \u2211 i,j \u00b5ij = 1. The inclusion is then proved by density. By (12) the dual norm of \u03bd satisfies\n\u03bd\u2217(Z) = sup { a\u22a4Zb : \u2016a\u2016\u03b1 \u2264 1 , \u2016b\u2016\u03b2 \u2264 1 } ,\nso that \u03bd\u2217(Z) \u2264 sup {\u3008Z, ab\u22a4\u3009 : ab\u22a4 \u2208 B} = sup {\u3008Z, ab\u22a4\u3009 : ab\u22a4 \u2208 A} \u2264 \u03bd\u2217(Z) ,\nwhere the middle equality is due to the fact that the maximum of a linear function on a convex set is attained at a vertex. We therefore have \u03bd\u2217(Z) = sup {\u3008Z,A\u3009 : A \u2208 A}. Given (4), this shows that \u03bd is the atomic norm induced by A.\nProof [Theorem 11] Since the (k, q)-trace norm is the atomic norm induced by the atom set (5), Lemma 10 tells us that it is also the nuclear norm induced by the two atomic norms with atom sets Am1k and Am2q , which correspond exactly to the so-called k- and q-support norms of Argyriou et al. (2012). To prove the second statement, we proceed similarly to get that the (k, q)-CUT norm is the nuclear norm induced by the two atomic norms with atom sets A\u0303m1k and A\u0303m2k . Calling \u03bak and \u03baq these norms, we obtain an explicit formulation as follows:\n\u03bak(w) = max s\n{\u3008s,w\u3009 : \u03ba\u2217k(s) \u2264 1}\n= max { \u3008s,w\u3009 : 1\u221a\nk\nk\u2211\ni=1\n|s(i)| \u2264 1 }\n=    1 k \u221a k \u2016w\u20161 if \u2016w\u20161 \u2265 k \u2016w\u2016\u221e\n1\u221a k \u2016w\u2016\u221e if \u2016w\u20161 \u2264 k \u2016w\u2016\u221e\n= 1\u221a k max\n( \u2016w\u2016\u221e , 1\nk \u2016w\u20161\n) .\nProof [Lemma 12]\nThe form of \u03b8\u2217k follows immediately from the fact that \u03b8 \u2217 k(w) = max{a\u22a4w : a \u2208 Ak}. Similarly for \u03ba\u2217k, we have\n\u03ba\u2217k(s) = max { \u3008a, s\u3009 : a \u2208 A\u0303k } = max\nI:|I|=k \u2016sI\u20161 = 1\u221a k\nk\u2211\ni=1\n|s(i)|,\nwhere s(i) denotes the the ith largest element of s in absolute value. This norm is proportional to a norm known as the vector k-norm or 1-k symmetric norm gauge.\nProof [Proposition 13] To prove the first claim, we show a counterexample for the (2, 2)-SVD in R4\u00d74. Let I = {1, 2} and J = {3, 4}. The matrix Z = 11\u22a4 \u2208 R4 can be written as Z = ZI,I + ZI,J + ZJ,I + ZJ,J , so its (2, 2)-rank is less than 4. However, it is not possible to write it as a sum of less than 6 symmetric (2, 2)-sparse matrices, because each of these matrices can only make one coefficient above the nondiagonal non-zero. Therefore, its (2, 2)-SVD must contain non-symmetric terms. To prove the second claim, note first that the case k = 1 is peculiar and not representative of the general case because the span of the PSD matrices of sparsity 1 are only the diagonal matrices, while the span of rank one PSD matrices of sparsity k \u00d7 k for k > 1 is all the symmetric matrices. Now, we claim that it is not possible to write Z = 11\u22a4 \u2208 R3 as a sum of PSD matrices that are (2, 2)-sparse and PSD. Indeed, if this was the case, this would imply the existence of a non zero vector v with a support of size at most 2 such that Z \u2212 vv\u22a4 \u227b 0. Since the only eigenvector of Z associated with a non-zero eigenvalue is the constant vector this is impossible."}, {"heading": "B Proofs of results in Section 4.1", "text": "Proof [Lemma 14] We prove a more general result than Lemma 14. Let \u2126 : Rm1\u00d7m2 \u2192 R be any matrix norm, and X : Rm1\u00d7m2 \u2192 Rn be a linear map. We denote by Xi (i = 1, . . . , n) the i-th design matrix defined by X (Z)i = \u3008Z,Xi\u3009. For a given matrix Z\u22c6 \u2208 Rm1\u00d7m2 , assume we observe:\nY = X (Z\u22c6) + \u01eb , (41)\nwhere \u01eb \u2208 Rn is a centered random noise vector. We consider the following estimator of Z\u22c6:\nZ\u0302\u2126 \u2208 argmin Z\n1\n2n \u2016Y \u2212 X (Z)\u201622 + \u03bb\u2126(Z) , (42)\nfor some value of the parameter \u03bb > 0. The following result generalizes standard results known for the \u21131 and trace norms (e.g., Koltchinskii et al., 2011, Theorem 1) to any norm \u2126. Theorem 27 If \u03bb \u2265 1n\u2126\u2217( \u2211n i=1 \u01ebiXi) then\n1\n2n\n\u2225\u2225\u2225X (Z\u0302\u2126 \u2212 Z\u22c6) \u2225\u2225\u2225 2\n2 \u2264 inf Z\n{ 1\n2n \u2016X (Z \u2212 Z\u22c6)\u201622 + 2\u03bb\u2126(Z)\n} . (43)\nLemma 14 is then a simple consequence of Theorem 27 by taking for X the identity map, upper bounding the right-hand side of (43) by the value 2\u03bb\u2126(Z\u22c6) it takes for Z = Z\u22c6, and replacing \u03bb by \u03bb/n.\nProof [Theorem 27] By definition of Z\u0302\u2126 (42), we have for all Z:\n1\n2n\n\u2225\u2225\u2225Y \u2212 X (Z\u0302\u2126) \u2225\u2225\u2225 2\n2 \u2264 1 2n \u2016Y \u2212 X (Z)\u201622 + \u03bb\n( \u2126(Z)\u2212 \u2126(Z\u0302\u2126) ) ,\nwhich after developing the squared norm and replacing Y by (41) gives\n1\n2n\n\u2225\u2225\u2225X (Z\u0302\u2126) \u2225\u2225\u2225 2\n2 \u2212 1 n \u3008X (Z\u22c6)+ \u01eb,X (Z\u0302\u2126)\u3009 \u2264 1 2n \u2016X (Z)\u201622 \u2212 1 n \u3008X (Z\u22c6)+ \u01eb,X (Z)\u3009+\u03bb\n( \u2126(Z)\u2212 \u2126(Z\u0302\u2126) ) ,\nand therefore\n1\n2n\n\u2225\u2225\u2225X (Z\u0302\u2126 \u2212 Z\u22c6) \u2225\u2225\u2225 2\n2 \u2264 1 2n \u2016X (Z \u2212 Z\u22c6)\u201622 + 1 n \u3008\u01eb,X (Z\u0302\u2126 \u2212 Z)\u3009+ \u03bb\n( \u2126(Z)\u2212 \u2126(Z\u0302\u2126) ) . (44)\nNow, using the fact (true for any norm) that \u2126(A)\u2126\u22c6(B) \u2265 \u3008A,B\u3009 for any vectors A,B \u2208 Rn, and taking \u03bb \u2265 1n\u2126\u2217( \u2211n i=1 \u01ebiXi), we can upper bound the second term of the right-hand side of (44) by:\n1 n \u3008\u01eb,X (Z\u0302\u2126 \u2212 Z)\u3009 = 1 n\nn\u2211\ni=1\n\u01ebiX (Z\u0302\u2126 \u2212 Z)i\n= 1\nn\nn\u2211\ni=1\n\u01ebi\u3008Xi, Z\u0302\u2126 \u2212 Z\u3009\n= 1\nn \u3008\nn\u2211\ni=1\n\u01ebiXi, Z\u0302\u2126 \u2212 Z\u3009\n\u2264 1 n \u2126\u22c6\n( n\u2211\ni=1\n\u01ebiXi ) \u2126 ( Z\u0302\u2126 \u2212 Z )\n\u2264 \u03bb\u2126 ( Z\u0302\u2126 \u2212 Z )\nPlugging this bound back in (44) finally gives\n1\n2n\n\u2225\u2225\u2225X (Z\u0302\u2126 \u2212 Z\u22c6) \u2225\u2225\u2225 2\n2 \u2264 1 2n \u2016X (Z \u2212 Z\u22c6)\u201622 + \u03bb\u2126(Z\u0302\u2126 \u2212 Z) + \u03bb\n( \u2126(Z)\u2212 \u2126(Z\u0302\u2126) )\n\u2264 1 2n \u2016X (Z \u2212 Z\u22c6)\u201622 + 2\u03bb\u2126(Z) ,\nthe last inequality being due to the triangle inequality.\nBefore proving Propositon 15, let us first derive an intermediary results useful to obtain an upper bound on the dual (k, q)-trace norm of a random matrix with i.i.d. normal entries.\nLemma 28 Let G be a m1 \u00d7m2 random matrix with i.i.d. normally distributed entries. Then\nE max I\u2208Gk,J\u2208Gq\n\u2016GI,J\u20162op \u2264 16 [( k log m1 k + q log m2 q ) + 2(k + q) ] .\nProof [Lemma 28] For a random matrix H \u2208 Rk\u00d7q with i.i.d. standard normal entries, we have the following concentration inequality (e.g., Davidson and Szarek, 2001): for s \u2265 0,\nP[\u2016H\u2016op > \u221a k + \u221a q + s] \u2264 exp(\u2212s2/2) . (45)\nDenoting R = 2 (\u221a k + \u221a q ) , and f(x) = etx 2 , we have the sequence of inequalities\nE exp(t \u2016H\u20162op) = Ef(\u2016H\u2016op)\n=\n\u222b \u221e\n1 P[f(\u2016H\u2016op) > h] dh\n\u2264 \u222b 1+f(R)\n1 1 dh+\n\u222b \u221e\n1+f(R) P[f(\u2016H\u2016op) > h]dh\n= f(R) +\n\u222b \u221e\n0 P[\u2016H\u2016op > f\u22121(f(R) + 1 + \u03b6)]d\u03b6\n\u2264 f(R) + \u222b \u221e\n0 P[\u2016H\u2016op >\n1 2 R+ 1 2 f\u22121(1 + \u03b6)]d\u03b6 (46)\n\u2264 f(R) + \u222b \u221e\n0 8ts exp\n( \u2212s2/2 + 4ts2 ) ds (47)\n\u2264 f(R) + 4 t1 2 \u2212 4t\n(48)\n\u2264 exp(8t(k + q)) + 8t 1\u2212 8t ,\nwhere the change of variable used in (47) is 1 + \u03b6 = f(2s) = e4ts 2 , (48) is true for any t < 18 , and\n(46) follows from the property of the inverse f\u22121(z) = \u221a\nlog(z) t that it is strictly increasing on [1;\u221e)\nand sandwiched via\n1\n2\n{ f\u22121(z) + f\u22121(z\u2032) } \u2264 f\u22121(z + z\u2032) \u2264 f\u22121(z) + f\u22121(z\u2032) . (49)\nTake now t = 18 \u2212 18(k+q) . Since k + q \u2265 2, we have 1/16 \u2264 t < 1/8. Therefore,\nEmax I,J\n\u2016GI,J\u20162op = 1\nt log\n{ exp tEmax\nI,J \u2016GI,J\u20162op\n}\n\u2264 1 t log\n{ E exp(tmax\nI,J \u2016GI,J\u20162op)\n}\n\u2264 1 t log\n{\u2211\nI,J\nE exp(t \u2016GI,J\u20162op) }\n\u2264 1 t log {( m1 k )( m2 q ) E exp(t \u2016H\u20162op) }\n\u2264 1 t log {(e m1 k )k (e m2 q )q ( e8t(k+q) + 8t 1\u2212 8t )}\n= 1\nt\n[( k log\nm1 k + q log m2 q\n) + k + q + 8t(k + q) + log ( 1 + 8t\n1\u2212 8te \u22128t(k+q)\n)]\n\u2264 16 [(\nk log m1 k + q log m2 q\n) + k + q ] + 8(k + q) +\n8\n1\u2212 8te \u22128t(k+q)\n\u2264 16 [(\nk log m1 k + q log m2 q\n) + 2(k + q) ] ,\nwhere in the last inequality we simply used 8/(1 \u2212 8t) = 8(k + q) and exp(\u22128t(k + q)) \u2264 1.\nProof [Propositon 15] From Lemma 28 we have:\nE\u2126\u2217k,q(G) = E max I\u2208Gk,J\u2208Gq \u2016GI,J\u2016op\n\u2264 ( E max\nI\u2208Gk,J\u2208Gq \u2016GI,J\u20162op\n) 1 2\n\u2264 4 [(\nk log m1 k + q log m2 q\n) + 2(k + q) ] 1 2\n\u2264 4 (\u221a\nk log m1 k + 2k +\n\u221a q log\nm2 q + 2q\n)\nThe upper bounds for the \u21131 and trace norms are standard. See Vershynin (2012, Theorem. 5.32) for the tight upper bound on the operator norm E \u2016G\u2016op \u2264 \u221a m1 + \u221a m2, and for the upper bound on the element-wise \u2113\u221e norm of G, use Jensen inequality followed by upper bounding the maximum of nonnegative scalars by their sum:\nexp (t E \u2016G\u2016\u221e) \u2264 E exp (t \u2016G\u2016\u221e) \u2264 m1m2 exp(t2/2) .\nTaking t = \u221a 2 log(m1m2) in the logarithms of the last inequality gives E \u2016G\u2016\u221e \u2264 \u221a 2m1m2."}, {"heading": "C Some cone inclusions (Proofs of results in Section 4.2.2)", "text": "Let us start with a simple result useful to prove inclusions of tangent cones.\nLemma 29 Let f and g two convex functions from Rd such that f \u2264 g and let x\u2217 such that f(x\u2217) = g(x\u2217). Then Tg(x\u2217) \u2282 Tf (x\u2217).\nProof [Lemma 29] Let h \u2208 Rd and \u03c4 > 0 such that g(x\u2217 + \u03c4h) \u2264 g(x\u2217). Then we also have\nf(x\u2217 + \u03c4h) \u2264 g(x\u2217 + \u03c4h) \u2264 g(x\u2217) = f(x\u2217) ,\nand therefore, for any \u03c4 > 0,\n{ h \u2208 Rd : g(x\u2217 + \u03c4h) \u2264 g(x\u2217) } \u2282 { h \u2208 Rd : f(x\u2217 + \u03c4h) \u2264 f(x\u2217) } .\nFrom the definition (26) of the tangent cone we deduce, by taking the union over \u03c4 > 0 and the closure of this inclusion, that Tg(x\n\u2217) \u2282 Tf (x\u2217). We can now prove the results in Section 4.2.2 Proof [Proposition 17] Consider a matrix A = ab\u22a4 \u2208 A\u0303k,q. We have \u2016A\u2016\u2217 = \u2016a\u20162\u2016b\u20162 = 1, and \u2016A\u20161 = \u2016a\u20161 \u2016b\u20161 = \u221a kq. Since A is an atom of both the norm \u2126k,q and the norm \u2126\u0303k,q we have \u2126k,q(A) = \u2126\u0303k,q(A) = 1 so that, for any \u00b5 \u2208 [0, 1],\n\u0393\u00b5(A) = \u2016A\u2016\u2217 = 1\u221a kq \u2016A\u20161 = \u2126k,q(A) = \u2126\u0303k,q(A) = 1 .\nBesides, for any matrix K \u2208 Rm1\u00d7m2 , for all (I, J) \u2208 Gm1k \u00d7 Gm2q , we have \u2016KI,J\u2016op \u2264 \u2016K\u2016op and \u2016KI,J\u2016op \u2264 \u2016KI,J\u2016Fro \u2264 \u221a kq \u2016KI,J\u2016\u221e so that \u2126\u2217k,q(K) \u2264 \u2016K\u2016op and \u2126\u2217k,q(K) \u2264 \u221a kqmaxI,J \u2016KI,J\u2016\u221e =\u221a\nkq \u2016K\u2016\u221e. Given that A\u0303k,q \u2282 Ak,q, we also have that\n\u2126\u0303\u2217k,q(K) = max A\u2208A\u0303k,q \u3008A,K\u3009 \u2264 max A\u2208Ak,q \u3008A,K\u3009 = \u2126\u2217k,q(K) .\nBy Fenchel duality, we therefore have for any Z \u2208 Rm1\u00d7m2 and \u00b5 \u2208 [0, 1]: \u00b5\u221a kq \u2016Z\u20161 + (1\u2212 \u00b5) \u2016Z\u2016\u2217 \u2264 \u2126k,q(Z) \u2264 \u2126\u0303k,q(Z) .\nProof [Corollary 18] Combining Proposition 17 with Lemma 29 directly gives (31). (32) is then a direct consequence of the definition of the statistical dimension (27).\nProof [Corollary 19] A necessary and sufficient condition for exact recovery is the so called null space property which is the event that T\u2126(Z\n\u2217) \u2229Ker(X ) = {0}, where Ker(X ) is the kernel of the linear transformation X (Chandrasekaran et al., 2012, Proposition 2.1). The result therefore follows from the inclusion of the cones stated in Corollary 18.\nProof [Proposition 20]\nLet a \u2208 A\u0303mk with supp(a) = I0, meaning that |ai| = 1/ \u221a k for i \u2208 I0 and ai = 0 for i \u2208 I\u22010 . The sub differential of the scaled \u21131 norm \u03931 at a is\n\u2202\u03931(a) = { s \u2208 Rm : si = sign(ai) for i \u2208 I0 , |si| \u2264 1 for i \u2208 I\u22010 } .\nFrom (10), we get that the subdifferential of \u03b8k at a is\n\u2202\u03b8k(a) = {a+ z : \u2200i , aizi = 0 and \u2200I \u2208 Gmk , \u2016aI + zI\u2016 \u2264 1} .\nThe first condition is equivalent to zi = 0 for i \u2208 I0, which implies that the second is equivalent to |zi| \u2264 1/ \u221a k for i \u2208 I\u22010 . We deduce that s = a + z \u2208 \u2202\u03b8k(a) if and only if si = ai for i \u2208 I0 and\n|si| \u2264 1/ \u221a k for i \u2208 I\u22010 , i.e.,\n\u2202\u03b8k(a) = 1\u221a k \u2202\u03931(a) .\nThis shows that the subdifferentials of \u03931 and \u03b8k have the same conic hull, and Proposition 20 follows by noting that the tangent cone is the polar cone of the conic hull of the subdifferential (Rockafellar, 1997, Theorem 23.7)."}, {"heading": "D Upper bound on the statistical dimension of \u2126k,q (proof of Proposition 23)", "text": "The aim of this appendix is to prove the upper bound on the statistical dimension \u2126k,q given in Proposition 23. Given its level of technicality, we split the proof in several parts. We start with preliminaries and notations in Section D.1, before proving Proposition 23 in Section D.2. The proofs of several technical results needed in Section D.2 are postponed to Section D.3, D.4 and D.5.\nD.1 Preliminaries and notations\nLet us start with some notations used throughout Appendix D. A = ab\u22a4 \u2208 Ak,q is an atom of \u2126k,q, with I0 = supp(a) and J0 = supp(b). \u03b3 = \u03b3(a, b) refers to the atom strength of A (Definition 22). For any I \u2208 Gm1k and J \u2208 Gm2q , let uI = aI/ \u2016aI\u20162 and vJ = bJ/ \u2016bJ\u20162. Note that while aI is a subvector of a, the notation uI does not refer to a subvector of some vector u and that therefore [uI ]I0 6= [uI0 ]I = aI since \u2016aI0\u2016 = \u2016a\u2016 = 1. To analyze the statistical dimension (27) of \u2126k,q at A, it is useful to express it as follows (Chandrasekaran et al., 2012, Proposition 3.6):\nS(A,\u2126k,q) := E [ dist ( G,N\u2126k,q (A) )2] , (50)\nwhere N\u2126k,q (A) is the normal cone of \u2126k,q at A (i.e., the conic hull of the subdifferential of \u2126k,q at A) and dist ( G,N\u2126k,q (A) ) denotes the Frobenius distance of the Gaussian matrix G with i.i.d. standard normal entries to N\u2126k,q (A). In order to upper bound this quantity, it is therefore important to characterize precisely the normal cone N\u2126k,q(A) . For that purpose, let us introduce further notations. We consider the following subspace of Rm1\u00d7m2\nspan(A) = { LA+AR : L \u2208 Rm1\u00d7m1 , R \u2208 Rm2\u00d7m2 } ,\nand denote by PA and P\u22a5A the orthogonal projectors onto span(A) and span\u22a5(A) respectively. Since A = ab\u22a4 with \u2016a\u20162 = \u2016b\u20162 = 1, we have the closed-form expressions P\u22a5A (Z) = (Idm1\u2212aa\u22a4)Z(Idm2\u2212 bb\u22a4).\nFor any (I, J) \u2208 Gm1k \u00d7 Gm2q , consider now the subspace\nspanI,J(A) = { LI,IAI,J +AI,JRJ,J : L \u2208 Rm1\u00d7m1 , R \u2208 Rm2\u00d7m2 } ,\nand its orthogonal\nspan\u22a5I,J(A) = { Z \u2208 Rm1\u00d7m2 : AI,JZ\u22a4I,J = A\u22a4I,JZI,J = 0 } .\nNote that span\u22a5I0,J0(A) is related to the subdifferential of \u2126k,q at A, since according to (10) we can write it as\n\u2202\u2126k,q(A) = { A+ Z : Z \u2208 span\u22a5I0,J0(A) , \u2200(I, J) \u2208 G m1 k \u00d7 Gm2q \u2016AI,J + ZI,J\u2016op \u2264 1 } . (51)\nIt is possible to estimate the dimension of span\u22a5I0,J0(A) as follows:\nLemma 30 The dimension of spanI0,J0(A) is k + q \u2212 1.\nProof [Lemma 30] For A = ab\u22a4, the range of L 7\u2192 LI0,I0AI0,J0 equals the range of \u03b1I0 7\u2192 \u03b1I0b\u22a4 which has dimension |I0| = k. By the same token, the range of R 7\u2192 AI0,J0RJ0,J0 has dimension q. By definition of spanI0,J0(A) we therefore have\nspanI0,J0(A) = { \u03b1I0b \u22a4 + a\u03b2\u22a4J0 : \u03b1 \u2208 Rm1 , \u03b2 \u2208 Rm2 }\nand therefore by the inclusion-exclusion principle dim ( spanI0,J0(A) ) = k + q \u2212 1.\nFinally we denote by \u03a0A,I,J the projector onto spanI,J(A), and by \u03a0 \u22a5 A,I,J the projector onto span\u22a5I,J(A). They satisfy respectively\n\u03a0A,I,J(Z) = PAI,J (ZI,J) and \u03a0\u22a5A,I,J(Z) = Z \u2212\u03a0A,I,J(Z) = Z \u2212 PAI,J (ZI,J) .\nD.2 Proof of Proposition 23\nProof [Proposition 23] In order to upper bound the statistical dimension of \u2126k,q at A, we associate to any matrix G a matrix \u039e(G) belonging to the normal cone N\u2126k,q(A), where \u039e : R\nm1\u00d7m2 \u2192 Rm1\u00d7m2 is measurable. From the characterization of the statistical dimension (50), since dist ( G,N\u2126k,q (A) ) \u2264 \u2016G\u2212 \u039e(G)\u2016Fro, we will then get the upper bound:\nS(A,\u2126k,q) = E [ dist ( G,N\u2126k,q (A) )2] \u2264 E \u2016G\u2212 \u039e(G)\u20162Fro . (52)\nThe main steps in the proof are then (i) to define the mapping \u039e, (ii) to show that \u039e(G) \u2208 N\u2126k,q(A) for all G, and (iiii) to upper bound E \u2016G\u2212 \u039e(G)\u20162Fro in order to derive an upper bound on S(A,\u2126k,q) by (52). Given a measurable function \u01eb : Rm1\u00d7m2 \u2192 R, let us therefore consider the mapping \u039e:\n\u2200G \u2208 Rm1\u00d7m2 , \u039e(G) := \u01eb(G)A+\u03a0\u22a5A,I0,J0(G) . (53)\nThe following lemma provides a mapping \u01eb to ensure that \u039e(G) \u2208 N\u2126k,q(A).\nLemma 31 Let \u01eb(G)2 be equal to\n16 \u03b32 \u2016GI0,J0\u20162op \u2228 max\nI\u2208G m1 k J\u2208G m2 q\n\u2016GIJ\u20162op \u2228 max0\u2264i<k 0\u2264j<q\n(i,j) 6=(0,0)\n8\n\u03b3 (\ni k + j q ) max |I\\I0|=i |J\\J0|=j\n[\u2225\u2225\u2225G\u22a4I\u2229I0,J\\J0uI \u2225\u2225\u2225 2\n2 +\n\u2225\u2225GI\\I0,J\u2229J0vJ \u2225\u22252 2\n] .\n(54) Then, for every G \u2208 Rm1\u00d7m2 , the matrix \u039e(G) defined in (53) belongs to the normal cone of \u2126k,q at A.\nBy choosing \u01eb(G) as in Lemma 31, the upper bound (52) because \u039e(G) \u2208 N\u2126k,q(A). Using the decomposition G = \u03a0A,I0,J0(G) + \u03a0 \u22a5 A,I0,J0 (G) we deduce\nS(A,\u2126k,q) \u2264 E \u2016G\u2212 \u039e(G)\u20162Fro = E \u2016\u01eb(G)A \u2212\u03a0A,I0,J0(G)\u20162Fro \u2264 2E \u2016\u01eb(G)A\u20162Fro + 2E \u2016\u03a0A,I0,J0(G)\u20162Fro = 2E \u01eb(G)2 + 2(k + q \u2212 1), (55)\nwhere (55) is due to \u2016A\u2016Fro = 1 and the fact that \u2016\u03a0A,I0,J0(G)\u20162Fro follows a chi-square distribution with k+ q\u22121 degrees of freedom, since by Lemma 30 this is the dimension of spanI0,J0(A). In order to upper bound E \u01eb(G)2 we need the following two lemmata in addition to Lemma 28.\nLemma 32 E \u2016GI0,J0\u20162op \u2264 4(k + q) + 4 . (56)\nLemma 33\nEmax i,j\n8\n\u03b3 (\ni k + j q ) max |J\\J0|=j |I\\I0|=i\n[ \u2016G\u22a4I\u2229I0,J\\J0uI\u2016 2 2 + \u2016GI\\I0,J\u2229J0vJ\u201622 ]\n\u2264 48 \u03b3 (k \u2228 q) log ((m1 \u2212 k) \u2228 (m2 \u2212 q)) + 64 \u03b3 (k \u2228 q) .\nCombining Lemmata 28, 56 and 33 with the definition of \u01eb(G) in (54) we deduce\nE \u01eb(G)2 \u2264 16 \u03b32 [4(k + q) + 4] + 16\n[( k log\nm1 k + q log m2 q\n) + 2(k + q) ]\n+ 48\n\u03b3 (k \u2228 q) log ((m1 \u2212 k) \u2228 (m2 \u2212 q)) +\n64\n\u03b3 (k \u2228 q)\n\u2264 ( 64\n\u03b32 +\n64\n\u03b3 + 32\n) (k + q + 1) + 16 ( k log\nm1 k + q log m2 q\n)\n+ 48\n\u03b3 (k \u2228 q) log (m1 \u2228m2)\n\u2264 160 \u03b32 (k + q + 1) + 80 \u03b3 (k \u2228 q) log (m1 \u2228m2) .\nPlugging this upper bound into (55) finally proves Proposition 23.\nD.3 The scaling factor \u01eb(G) ensures that \u039e(G) \u2208 N\u2126k,q(A) (proof of Lemma 31) Proof [Lemma 31] To simplify notations let us denote\nG\u0303 :=\u03a0\u22a5A,I0,J0(G) ,\nso that (53) becomes \u039e(G) = \u01eb(G)A + G\u0303. To prove that \u039e(G) belongs to the normal cone of \u2126k,q at A, it is sufficient to prove that \u01eb(G)\u22121\u039e(G) = A + \u01eb(G)\u22121G\u0303 is a subgradient of \u2126k,q at A. By the characterization of the subgradient in (51), and since G\u0303 \u2208 span\u22a5I0,J0(A), this is equivalent to\u2225\u2225\u2225AIJ + \u01eb(G)\u22121 G\u0303IJ\n\u2225\u2225\u2225 op\n\u2264 1 for any (I, J) \u2208 Gm1k \u00d7 Gm2q , which itself is equivalent to \u2225\u2225\u2225AIJ + \u01eb(G)\u22121 \u03a0A,I,J(G\u0303)\n\u2225\u2225\u2225 op \u2264 1 and \u01eb(G)\u22121 \u2225\u2225\u2225P\u22a5A (G\u0303I,J) \u2225\u2225\u2225 op \u2264 1 . (57)\nFirst, the second inequality of (57) is satisfied since\n\u2225\u2225\u2225P\u22a5A (G\u0303I,J) \u2225\u2225\u2225 op \u2264 \u2225\u2225\u2225G\u0303I,J \u2225\u2225\u2225 op = \u2225\u2225\u2225 [ \u03a0\u22a5A,I0,J0(G) ] IJ \u2225\u2225\u2225 op \u2264 \u2016[G]IJ\u2016op \u2264 \u01eb(G) .\nThere thus remains to prove the first inequality of (57). Note that the matrix AIJ+\u01eb(G) \u22121 \u03a0A,I,J(G\u0303)\nhas rank 2, so its Frobenius norm is larger than its operator norm by at most a factor of \u221a 2. Working with the Frobenius norm is more convenient, so knowing that\n\u2225\u2225\u2225AIJ + \u01eb(G)\u22121 \u03a0A,I,J(G\u0303) \u2225\u2225\u2225 2\nop \u2264\n\u2225\u2225\u2225AIJ + \u01eb(G)\u22121 \u03a0A,I,J(G\u0303) \u2225\u2225\u2225 2\nFro ,\nwe will establish an upper bound on the latter quantity which we denote by \u03bdI,J(G). Noting that AIJ = \u2016aI\u20162 \u2016bJ\u20162 uIv\u22a4J and that\n\u03a0A,I,J(G\u0303) = uIu \u22a4 I G\u0303IJ + G\u0303IJvJv \u22a4 J \u2212 uIu\u22a4I G\u0303IJvJ v\u22a4J ,\nwe get\n\u03bdI,J(G) = \u2225\u2225\u2225\u2016aI\u20162 \u2016bJ\u20162 uIv\u22a4J + \u01eb(G)\u22121 ( uIu \u22a4 I G\u0303IJ + G\u0303IJvJv \u22a4 J \u2212 uIu\u22a4I G\u0303IJvJv\u22a4J )\u2225\u2225\u2225 2\nFro\n= \u2016aI\u201622 \u2016bJ\u201622 + 2\n\u01eb(G) \u2016aI\u20162 \u2016bJ\u20162 u\u22a4I G\u0303IJvJ\n+ 1\n\u01eb(G)2\n( u\u22a4I G\u0303IJG\u0303 \u22a4 IJuI + v \u22a4 J G\u0303 \u22a4 IJ G\u0303IJvJ \u2212 2(u\u22a4I G\u0303IJvJ)2 )\n\u2264 \u2016aI\u201622 \u2016bJ\u201622 + 2\n\u01eb(G) \u2016aI\u20162 \u2016bJ\u20162 u\u22a4I G\u0303IJvJ +\n1\n\u01eb(G)2\n( u\u22a4I G\u0303IJ G\u0303 \u22a4 IJuI + v \u22a4 J G\u0303 \u22a4 IJ G\u0303IJvJ ) .\nThe following Lemma provides upper bounds on the different terms.\nLemma 34 We have\nu\u22a4I G\u0303IJvJ \u2264 \u2225\u2225aI0\\I \u2225\u2225 2 \u2225\u2225bJ0\\J \u2225\u2225 2 \u2016GI0J0\u2016op ,\nu\u22a4I G\u0303IJG\u0303 \u22a4 IJuI \u2264 \u2225\u2225\u2225G\u22a4I\u2229I0,J\\J0uI \u2225\u2225\u2225 2\n2 + 2\n\u2225\u2225aI0\\I \u2225\u22252 2 \u2016GI0,J0\u20162op ,\nv\u22a4J G\u0303 \u22a4 IJG\u0303IJvJ \u2264 \u2225\u2225GI\\I0,J\u2229J0vJ \u2225\u22252 2 + 2 \u2225\u2225bJ0\\J \u2225\u22252 2 \u2016GI0,J0\u20162op .\nThis yields\n\u03bdI,J(G) \u2264 \u2016aI\u201622 \u2016bJ\u201622 + 2\n\u01eb(G) \u2016aI\u20162 \u2016bJ\u20162 \u2225\u2225aI0\\I \u2225\u2225 2 \u2225\u2225bJ0\\J \u2225\u2225 2 \u2016GI0J0\u2016op\n+ 1\n\u01eb(G)2\n(\u2225\u2225\u2225G\u22a4I\u2229I0,J\\J0uI \u2225\u2225\u2225 2\n2 + 2\n\u2225\u2225aI0\\I \u2225\u22252 2 \u2016GI0,J0\u20162op\n)\n+ 1\n\u01eb(G)2\n(\u2225\u2225GI\\I0,J\u2229J0vJ \u2225\u22252 2 + 2 \u2225\u2225bJ0\\J \u2225\u22252 2 \u2016GI0,J0\u20162op )\n\u2264 \u2016aI\u201622 \u2016bJ\u201622 + \u03b3\n2 \u2016aI\u20162 \u2016bJ\u20162 \u2225\u2225aI0\\I \u2225\u2225 2 \u2225\u2225bJ0\\J \u2225\u2225 2\n+ \u03b3\n8\n( i\nk +\nj\nq\n) + \u03b32\n8\n(\u2225\u2225aI0\\I \u2225\u22252 2 + \u2225\u2225bJ0\\J \u2225\u22252 2 ) ,\nwhere we used the definition of \u01eb(G) (54) to derive the last inequality. Define \u03b1 := \u2016aI0\\I\u20162 = 1 \u2212 \u2016aI\u20162 and \u03b2 := \u2016bJ0\\J\u20162 = 1 \u2212 \u2016bJ\u20162. With these notations and rearranging the terms, we can rewrite the above inequality as\n\u03bdI,J(G) \u2264 (1\u2212 \u03b1)(1 \u2212 \u03b2) + \u03b3\n2\n\u221a \u03b1\u03b2(1 \u2212 \u03b1)(1 \u2212 \u03b2) + \u03b3 2\n8 (\u03b1+ \u03b2) +\n\u03b3\n8\n( i\nk +\nj\nq\n) .\nSince 0 \u2264 \u03b1, \u03b2 \u2264 1 and using \u221a\u03b1\u03b2 \u2264 12(\u03b1+ \u03b2), we have\n\u03b1\u03b2 \u2264 1 2 (\u03b1+ \u03b2) and\n\u221a \u03b1\u03b2(1 \u2212 \u03b1)(1 \u2212 \u03b2) \u2264 1\n2 (\u03b1+ \u03b2) .\nThese inequalities yield\n\u03bdI,J(G) \u2264 1 + (\u03b1+ \u03b2) ( \u2212 1 + 1\n2 +\n\u03b3 4 +\n\u03b32\n8\n) + \u03b3\n8\n( i\nk +\nj\nq\n) .\nBy definition of \u03b3 = min \u03b9\u2208I0 \u03b9\u2032\u2208J0\n( k a2\u03b9 , q b 2 \u03b9\u2032 ) , we have ik \u2264 \u03b1\u03b3 and j q \u2264 \u03b2 \u03b3 . Moreover, given that\n0 \u2264 \u03b3 \u2264 1, we have 4\u03b3 \u22122\u2212\u03b3 = 1\u03b3 (4\u22122\u03b3\u2212\u03b32) \u2265 1\u03b3 , so that factorizing \u03b3 8 in the previous expression, we obtain\n\u03bdI,J(G) \u2264 1 + \u03b3\n8\n[( \u2212 4\n\u03b3 + 2 + \u03b3\n) (\u03b1+ \u03b2) +\n( i\nk +\nj\nq\n)]\n\u2264 1 + \u03b3 8 [ \u22121 \u03b3 (\u03b1+ \u03b2) + ( i k + j q )] \u2264 1 ,\nwhich concludes the proof.\nD.4 Proof of Lemma 34\nLet us first start with a few useful technical lemmas.\nLemma 35 The matrix G\u0303IJ = [\u03a0 \u22a5 A,I0,J0 (G)]IJ is of the form G\u0303IJ = G\u03031 + G\u03032 with\nG\u03031 = GIJ \u2212GI\u2229I0,J\u2229J0 and G\u03032 = (IdI \u2212 aIa\u22a4)GI0J0 (IdJ \u2212 bb\u22a4J ).\nProof [Lemma 35]\n\u03a0\u22a5A,I0,J0(G) = G\u2212\u03a0A,I0,J0(G) = G\u2212 aI0a\u22a4I0GI0J0 \u2212GI0J0bJ0b\u22a4J0 + aI0a\u22a4I0GI0J0bJ0b\u22a4J0 = G\u2212GI0J0 + (IdI0 \u2212 aI0a\u22a4I0)GI0J0 (IdJ0 \u2212 bJ0b\u22a4J0),\nso that [\u03a0\u22a5A,I0,J0(G)]IJ = GIJ \u2212GI\u2229I0,J\u2229J0 + (IdI \u2212 aIa\u22a4)GI0J0 (IdJ \u2212 bb\u22a4J ).\nLemma 36 We have u\u22a4I G\u03031 = u \u22a4 I GI\u2229I0,J\\J0 and G\u03031vJ = GI\\I0,J\u2229J0vJ .\nProof [Lemma 36] Given that supp(uI) \u2282 I0, we have\nu\u22a4I G\u03031 = u \u22a4 I (GIJ \u2212GI\u2229I0,J\u2229J0) = u\u22a4I (GI\u2229I0,J \u2212GI\u2229I0,J\u2229J0) = u\u22a4I GI\u2229I0,J\\J0 ,\nwhich proves the first equality. The second one is proved similarly.\nLemma 37 \u2016Id\u2212 bJb\u22a4\u20162op \u2264 4\n3\nProof [Lemma 37] The largest singular value is attained on the span of bJ and bJc both on the left and on the right. Given that \u2016b\u2016 = 1, it is therefore also the largest eigenvalue of the matrix of the linear operator restricted to this span which is equal to\n[ (1\u2212 x) \u2212 \u221a (1\u2212 x)x\n0 1\n] ,\nfor x = \u2016bJ\u20162. Tedious but simple calculations show that the squared operator norm of this matrix is equal to 1\u2212 x/2 + 1/2 \u221a x(4\u2212 3x), which takes its maximum value 4/3 for x = 1/3.\nProof [Lemma 34] Given that G\u0303IJ = G\u03031 + G\u03032 and u \u22a4 I G\u03031 = uIG\u0303I\u2229I0,J\\J0 , we have u \u22a4 I G\u03031vJ = u \u22a4 I G\u03031vJ\u2229J0 = 0, so that\nu\u22a4I G\u0303IJvJ = u \u22a4 I G\u03032vJ\n= u\u22a4I (IdI \u2212 aIa\u22a4)GI0J0 (IdJ \u2212 bb\u22a4J )vJ \u2264 \u2225\u2225uI \u2212 \u2016aI\u2016 a \u2225\u2225\u2225\u2225GI0J0 \u2225\u2225 op \u2225\u2225vJ \u2212 \u2016bJ\u2016 b \u2225\u2225 \u2264 \u2016aI0\\I\u2016 \u2016bJ0\\J\u2016 \u2016GI0J0\u2016op,\nbecause \u2016u\u22a4I (IdI \u2212 aIa\u22a4)\u20162 = \u2225\u2225uI \u2212 \u2016aI\u2016 a \u2225\u22252 = 1 \u2212 2\u2016aI\u20162 + \u2016aI\u20162 = \u2016aI0\\I\u20162, and symmetrically\u2225\u2225vJ \u2212 \u2016bJ\u2016 b \u2225\u2225 = \u2016bJ0\\J\u2016. This shows the first inequality. For the two next inequalities, note that\nu\u22a4I G\u0303IJG\u0303 \u22a4 IJuI = \u2016G\u0303\u22a4IJuI\u20162 = \u2016G\u0303\u22a41 uI\u20162 + \u2016G\u0303\u22a42 uI\u20162\nbecause \u3008G\u0303\u22a41 uI , G\u0303\u22a42 uI\u3009 = 0 as a result of the fact that by lemma 36, G\u0303\u22a41 uI and G\u0303\u22a42 uI have disjoint supports. Now \u2016G\u0303\u22a41 uI\u20162 = \u2016G\u22a4I\u2229I0,J\\J0uI\u2016 2 2 and \u2016G\u0303\u22a42 uI\u2016 \u2264 2 \u2016aI0\\I\u20162 \u2016GI0,J0\u20162op, because \u2016Id \u2212 bJb\u22a4\u20162op \u2264 2 (see Lemma 37 for a proof). This shows the second inequality and the third follows by symmetry.\nD.5 Upper bounds for \u01eb(G)2 (Proofs of Lemmata 32 and 33)\nProof [Lemma 32]\nUsing (45) and the fact that (\u221a k + \u221a q + s )2 \u2264 2 ( ( \u221a k + \u221a q)2 + s2 ) gives\nP [ \u2016GI0,J0\u20162op > 2 ( ( \u221a k + \u221a q)2 + s2 )] \u2264 exp(\u2212s2/2) .\nSetting t = 2s2 yields\nP [ \u2016GI0,J0\u20162op > 4(k + q) + t ] \u2264 exp(\u2212t/4) .\nIt follows that\nE \u2016GI0,J0\u20162op = \u222b \u221e\n0 P(\u2016GI0,J0\u20162op \u2265 t\u2032)dt\u2032\n=\n\u222b 4(k+q)\n0 dt\u2032 +\n\u222b \u221e\n4(k+q) P(\u2016GI0,J0\u20162op \u2265 t\u2032)dt\u2032\n\u2264 4(k + q) + \u222b \u221e\n0 exp(\u2212t/4)dt\n= 4(k + q) + 4 .\nProof [Lemma 33] As the sets I \u2229 I0 \u00d7 J\\J0 and I\\I0 \u00d7 J \u2229 J0 are disjoint, and uI , vJ of unit length, the random variable\nMI,J = \u2225\u2225\u2225G\u22a4I\u2229I0,J\\J0uI \u2225\u2225\u2225 2\n2 +\n\u2225\u2225GI\\I0,J\u2229J0vJ \u2225\u22252 2\nfollows a chi-square distribution with i + j degrees of freedom, where i = |I\\I0| and j = |J\\J0|. Using Chernoff\u2019s inequality and the form of the chi-square moment generating function, we have that for any fixed real number \u03b1 and fixed index sets I and J , for all t \u2208 (0, 1/2),\nP [ MI,J > \u03b1 ] = P [ etMI,J > et\u03b1 ] \u2264 e\u2212t\u03b1 E etMI,J = e\u2212t\u03b1(1\u2212 2t)\u2212 i+j2 .\nTaking the maximum over index sets I and J with the same intersection sizes with I0 and J0 respectively, and using a union bound on the independent choices of I and J , yields\nP   max\n|I\\I0|=i |J\\J0|=j\nMI,J > \u03b1\n  \u2264 ( m1 \u2212 k\ni\n)( m2 \u2212 q\nj\n) exp { \u2212t\u03b1\u2212 i+ j\n2 log(1\u2212 2t)\n}\n\u2264 exp { \u2212t\u03b1\u2212 i+ j\n2 log(1\u2212 2t) + i log(m1 \u2212 k) + j log(m2 \u2212 q)\n} .\nTaking \u03b1 = \u03bb(i+ j), we have for any t < 1/2 (assuming w.l.o.g. m1 \u2212 k \u2265 m2 \u2212 q)\nP   max\n|I\\I0|=i |J\\J0|=j\nMI,J > \u03bb(i+ j)\n  \u2264 exp { \u2212t\u03bb(i+ j)\u2212 i+ j\n2 log(1\u2212 2t) + i log(m1 \u2212 k) + j log(m2 \u2212 q)\n}\n\u2264 exp { (i+ j) ( \u2212t\u03bb\u2212 1\n2 log(1\u2212 2t) + log(m1 \u2212 k)\n)} .\nLet us introduce Mi,j = 1i+j max |I\\I0|=i |J\\J0|=j MI,J , and take t = 1 2\n( 1\u2212 1m1\u2212k ) < 12 . Then\nP   max\n0\u2264i<k 0\u2264j<q\n(i,j) 6=(0,0)\nMi,j > \u03bb   \u2264 \u2211\n0\u2264i<k 0\u2264j<q\n(i,j) 6=(0,0)\nexp { (i+ j) ( \u22121 2 ( 1\u2212 1 m1 \u2212 k ) \u03bb+ 3 2 log(m1 \u2212 k) )}\n=\nk\u22121\u2211\ni=0\n\u03b2i q\u22121\u2211\nj=0\n\u03b2j \u2212 1 = 1\u2212 \u03b2 k 1\u2212 \u03b2 1\u2212 \u03b2q 1\u2212 \u03b2 \u2212 1 \u2264 2\u03b2 ,\nwhere\n\u03b2 = exp { \u22121 2 ( 1\u2212 1 m1 \u2212 k ) \u03bb+ 3 2 log(m1 \u2212 k) } .\nAs a consequence, we have\nE[max i,j\nMi,j ] = \u222b \u221e\n0 P[max i,j Mi,j > \u03bb]d\u03bb\n\u2264 \u222b 3(m1\u2212k) m1\u2212k\u22121 log k\n0 d\u03bb+ 2 \u222b \u221e 3(m1\u2212k) m1\u2212k\u22121 log(m1\u2212k) exp { 3 2 log(m1 \u2212 k)\u2212 1 2 ( 1\u2212 1 m1 \u2212 k ) \u03bb } d\u03bb\n\u2264 3(m1 \u2212 k) m1 \u2212 k \u2212 1 log k + 4 m1 \u2212 k\nm1 \u2212 k \u2212 1 \u2264 6 log(m1 \u2212 k) + 8 .\nIt follows that\nE max 0\u2264i<k 0\u2264j<q\n(i,j) 6=(0,0)\n8\n\u03b3 (\ni k + j q ) max |J\\J0|=j |I\\I0|=i \u2016G\u22a4I\u2229I0,J\\J0uI\u2016 2 2 + \u2016GI\\I0,J\u2229J0vJ\u201622\n\u2264 48 \u03b3 (k \u2228 q) log ((m1 \u2212 k) \u2228 (m2 \u2212 q)) + 64 \u03b3 (k \u2228 q) . (58)"}, {"heading": "E Lower bound on the statistical dimension of \u0393\u00b5 (Proof of Proposition 24)", "text": "Let us start with a technical lemma:\nLemma 38 Let ab\u22a4 \u2208 Ak,q, X : Rm1\u00d7m2 \u2192 Rn a linear map from the standard Gaussian ensemble and y = X (ab\u22a4). If n \u2264 19m1m2 and further\nn \u2264 n0 := \u03b6(a, b) 1 64 ( (kq) \u2227 (m1 +m2 \u2212 1) ) \u2212 2, with \u03b6(a, b) = 1\u2212 ( 1\u2212 \u2016a\u2016 2 1 k )( 1\u2212 \u2016b\u2016 2 1 q ) ,\nthen, with probability 1\u2212 c1 exp(\u2212c2n0), solving formulation (28) with the norm \u0393\u00b5 fails to recover ab\u22a4 simultaneously for any values of \u00b5 \u2208 [0, 1], where c1 and c2 are universal constants.\nProof [Lemma 38] The proof consists in applying theorem 3.2 in Oymak et al. (2012) for the combination of the \u21131norm with the trace norm. We adapt slightly the notations of that paper to reflect the fact that we are working with matrices. Since we consider conic combinations of the \u21131 and trace norms, the number of norms is therefore \u03c4 = 2. To apply the theorem we need to specify \u03ba, \u03b8, dmin, \u03b3 and C\u25e6 in the notations of that paper. For each decomposable norm \u03bdj for j \u2208 {1, 2}, with \u03bd1 the \u21131-norm and and \u03bd2 the trace norm, given a point ab\u22a4 (which corresponds to the point x0 in Oymak et al., 2012), the authors define\n\u2022 Tj the supporting subspaces and Ej (ej in the paper), the orthogonal projection of any subgradient of the norm in ab\u22a4 (Definition 2.1),\n\u2022 Lj the Lipschitz constant of \u03bdj with respect to the Euclidean norm (Definition 2.2),\n\u2022 \u03baj = \u2016Ej\u20162Fro\nL2j m1m2 dim(Tj) (Definition 2.2).\nLet ab\u22a4 \u2208 Ak,q with support I0\u00d7J0 and sa = sign(a), sb = sign(b). Denoting eij the element of the canonical basis of Rm1\u00d7m2 , we have\n\u2022 T1 = span({eij}(i,j)\u2208I0\u00d7J0) so that dim(T1) = kq,\n\u2022 T2 = {av\u22a4 + ub\u22a4 | u \u2208 Rm1 , v \u2208 Rm2} so that dim(T2) = m1 +m2 \u2212 1.\nBy definition dmin = dim(T1) \u2227 dim(T2). We have\nE1 = sas \u22a4 b , \u2016E1\u20162Fro = kq, E2 = ab\u22a4, \u2016E2\u20162Fro = 1, L1 = \u221a kq, L2 = \u221a m1 \u2227m2 ,\nand thus \u03ba1 = m1m2 kq , \u03ba2 = m1m2 (m1 \u2227m2)(m1 +m2 \u2212 1) , so that \u03ba = \u03ba1 \u2227 \u03ba2 \u2265 1 2 .\nWe then have \u03b8 defined as \u03b8 = \u03b81 \u2227 \u03b82 with \u03b8j = \u2016E\u2229,j\u20162/\u2016Ej\u20162 where E\u2229,j is the projection of Ej on T1 \u2229 T2. But E2 \u2208 T1 so that \u03b82 = 1. The situation is less simple for E1. Indeed, E\u2229,1 = \u2016a\u20161as\u22a4b + \u2016b\u20161sab\u22a4 \u2212 ab\u22a4\u2016a\u20161\u2016b\u20161. Some calculations lead to\n\u03b821 = \u2016a\u201621 k + \u2016b\u201621 q \u2212 \u2016a\u2016 2 1 k \u2016b\u201621 q ,\nhence the definition of \u03b6(a, b) = \u03b82 = \u03b821 \u2227 \u03b822. Theorem 3.2 in Oymak et al. (2012) offers the possibility of constraining the estimator to lie in a cone C. In our case, C = Rm1\u00d7m2 , given the definition of \u03b3 we therefore have \u03b3 \u2264 2. The result follows from applying the theorem with \u03b82 = \u03b6(a, b) and using \u03ba\n81\u03b32\u03c4 \u2265 1/2 34.22.2 = 1 64 .\nProof [Proposition 24] Take M such that when m1,m2, k, q,m1/k,m2/q \u2265 M then n0 is large enough to ensure 1 \u2212 c1 exp(\u2212c2n0) > 4 exp (\u221232/17). Then, according to Lemma 38, solving (28) with the norm \u0393\u00b5 fails to recover A = ab\u22a4 with probability at least 4 exp (\u221232/17). On the other hand, Amelunxen et al. (2013, Theorem 7.1) shows that, when n \u2265 S (A,\u0393\u00b5)+ \u03bb, for any \u03bb \u2265 0, then solving (28) with the norm \u0393\u00b5 correctly recovers A with probability at least\n4 exp ( \u2212\u03bb2/8 \u03c92(A,\u0393\u00b5) + \u03bb ) , (59)\nwhere \u03c92(A,\u0393\u00b5) = S (A,\u0393\u00b5)\u2227 (m1m2 \u2212S (A,\u0393\u00b5)). Take \u03bb = 16\u03c9(A,\u0393\u00b5), then using the fact that \u03c9(A,\u0393\u00b5) \u2265 1 we get that the probability (59) is smaller than 4 exp (\u221232/17). This implies that\nn0 \u2264 S (A,\u0393\u00b5) + \u03bb \u2264 S (A,\u0393\u00b5) + 16 \u221a S (A,\u0393\u00b5) \u2264 17S (A,\u0393\u00b5) ."}, {"heading": "F Bounds on the statistical dimension in the vector case (proofs of", "text": "results of Section 4.2.4)\nF.1 Lower bound on the statistical dimension of \u03bak (Proof of Proposition 25)\nLet us start with two technical lemmata.\nLemma 39 Let X(k) denote the kth order statistics of an i.i.d. sample X1, . . . ,Xn whose common distribution has a cdf F . Assume that F\u22121 is a convex function7 from [0, 1] to R. Then\nE[X(k)] \u2265 F\u22121 ( k n+ 1 ) .\nProof [Lemma 39] Let f denote the pdf of X. We have\nE[X(k)] = n!\n(k \u2212 1)!(n \u2212 k)!\n\u222b \u221e\n\u2212\u221e uF (u)k\u22121\n( 1\u2212 F (u) )n\u2212r f(u) du\n= \u0393(n+ 1)\n\u0393(k) \u0393(n \u2212 k + 1)\n\u222b 1\n0 F\u22121(v) vk\u22121(1\u2212 v)n\u2212rdv = E[F\u22121(V )] ,\nwith V \u223c Beta(k, n\u2212k+1). Assuming that F\u22121 is a convex function, we have by Jensen\u2019s inequality\nE[X(k)] = E[F \u22121(V )] \u2265 F\u22121(E[V ]) = F\u22121 ( k n+ 1 ) .\n7Note that this implies that the essential support of the random variable is bounded below.\nLemma 40 Let G \u2208 Rn be an standard normal vector, then we have\nE[\u03ba\u2217k(G)] \u2265 \u221a 2\n\u03c0\n\u221a k log (n+ 1 k + 1 ) .\nProof [Lemma 40] Denote by F the cdf of the absolute value of a standard normal variable. Then,\nF (x) = \u03a6(x)\u2212 \u03a6(\u2212x) = erf ( x\u221a\n2\n) ,\nwhere \u03a6 is the cdf of a standard Gaussian and erf denotes the error function. We use the following inequality due to Chu (1954):\n\u221a 1\u2212 e\u2212x2 \u2264 erf(x) \u2264 \u221a 1\u2212 e\u2212\u03c04 x2 ,\nto deduce that\nF\u22121(y) \u2265 \u221a \u2212 2 \u03c0 log(1\u2212 y2) .\nBy definition, we have E[\u03ba\u2217k(G)] = 1\u221a k E[X(n) + . . . +X(n\u2212k+1)] where Xi = |Gi| and G is a vector of independent standard normal variables. It can easily be checked that F\u22121 is a convex function. This implies, using Lemma 39, that\nE[\u03ba\u2217k(G)] \u2265 1\u221a k\nk\u2211\nj=1\nF\u22121 ( 1\u2212 j\nn+ 1\n)\n\u2265 \u221a k F\u22121 (1 k k\u2211\nj=1\n( 1\u2212 j\nn+ 1\n)) (again by Jensen\u2019s inequality)\n= \u221a k F\u22121 ( 1\u2212 k + 1\n2(n+ 1)\n)\n\u2265 \u221a k \u221a \u2212 2 \u03c0 log ( k + 1 (n+ 1) \u2212 ( k + 1 2(n + 1) )2) \u2265 \u221a 2\n\u03c0\n\u221a k log (n+ 1 k + 1 ) .\nProof [Proposition 25] We will denote the squared Gaussian width of the tangent cone intersected with a Euclidean unit ball by\nw(T\u03bak(a) \u2229 Sm\u22121) = E [\nmax t\u2208T\u03bak (a)\u2229Sm\u22121\n\u3008t,G\u3009 ] ,\nwhereG \u2208 Rm denotes a standard Gaussian vector. We have w(T\u03bak(a)\u2229Sm\u22121)2 \u2264 S(a, \u03bak)(Chandrasekaran et al., 2012, Proposition 3.6). We thus seek a lower bound of w(T\u03bak(a)\u2229 Sm\u22121). Since the tangent cone is polar to the normal cone, we have that\nT\u03bak(a) = {t \u2208 Rm | \u3008s, t\u3009 \u2264 0, \u2200s \u2208 \u2202\u03bak(a)} .\nGiven a random Gaussian vector G, denote I0 the support of a and IG the indices of the k largest coefficients of G in absolute value outside of I0. Denote by s\u0303G = sign(GIG), i.e., the vector whose entries are zero outside of IG and equal to the sign of the corresponding coefficient of G otherwise. Define tG =\n1\u221a 2k (s\u0303G \u2212 a). By construction tG \u2208 Sm\u22121. Let now consider s \u2208 \u2202\u03bak(a), we have\n\u221a 2k \u3008s, tG\u3009 = \u2212\u3008s, a\u3009+ \u3008s, s\u0303G\u3009 \u2264 \u22121 + \u03bak(s\u0303G)\u03ba\u2217k(s) \u2264 \u22121 + 1 = 0,\nso that tG \u2208 T\u03bak(a). Therefore w(T\u03bak(a) \u2229 Sm\u22121) \u2265 E[\u3008tG, G\u3009] = 12\u221akE[\u3008s\u0303G, G\u3009] = 1 2E[\u03ba \u2217 k(G)], whence the result using Lemma 40 and w(T\u03bak(a) \u2229 Sm\u22121)2 \u2264 S(a, \u03bak).\nF.2 Upper bound on the statistical dimension of \u03b8k (Proof of Proposition 26)\nProof [Proposition 26] Without loss of generality, let us assume that w \u2208 Rp is a fixed vector having nonincreasing \u2013 in absolute value \u2013 coordinates, the first s of which are assumed to be nonzero. We compute the subdifferential of \u03b8k(w) directly by using (14). Remember that one characterization of the subdifferential is \u2202\u03b8k(w) = {\u03b1 \u2208 Rp : \u03b8\u2217k(\u03b1) \u2264 1 , \u03b1\u22a4w = \u03b8k(w)} . Letting r \u2208 {0, \u00b7 \u00b7 \u00b7 , k \u2212 1} being the unique integer such that |wk\u2212r\u22121| > 1r+1 \u2211p i=k\u2212r |wi| \u2265 |wk\u2212r|, let us partition the set of entries {1, \u00b7 \u00b7 \u00b7 , p} into I2 = {1, \u00b7 \u00b7 \u00b7 , k \u2212 r \u2212 1}, I1 = {k \u2212 r, \u00b7 \u00b7 \u00b7 , s} and I0 = {s + 1, \u00b7 \u00b7 \u00b7 , p} (where each set may be empty). Then we can rewrite the expression of the k-support norm (14) as\n\u03b8k(w) 2 = \u2016wI2\u201622 +\n1\nr + 1 \u2016wI1\u201621 .\nThen necessarily each element \u03b1 \u2208 \u2202\u03b8k(w) must satisfy    \u03b1i = wi \u03b8k(w) for i \u2208 I2 , \u03b1i = \u2016wI1\u20161 sign(wi) (r+1)\u03b8k(w) for i \u2208 I1 .\nAs for i \u2208 I0, the coefficients \u03b1i do not impact \u03b1\u22a4w so they should also not impact \u03b8\u2217k(\u03b1). If s < k this implies \u03b1i = 0, and if s \u2265 k this means |\u03b1i| \u2264 |\u03b1k|, and in that case k \u2208 I1. With the convention \u2016wI1\u20161 = 0 when I1 = \u2205, we finally get the following expression for the subdifferential:\n\u2202\u03b8k(w) = 1\n\u03b8k(w)\n{ wI2 + 1\nr + 1 \u2016wI1\u20161 (sgn(wI1) + hI0) : \u2016h\u2016\u221e \u2264 1\n} . (60)\nIn the case s < k, we have s = k \u2212 r + 1, I2 = [1, s], I1 = \u2205 and I0 = [s+ 1, p]. In that case \u03b8k(w) = \u2016w\u20162 and \u2202\u03b8k(w) = w/ \u2016w\u20162, showing that \u03b8k is differentiable at w, meaning \u03b8k is useless to recover w. Let us therefore only consider the case s \u2265 k, in which case I1 6= \u2205 and \u2016wI1\u20161 > 0. In order to compute the statistical dimension of \u03b8k at w, we use the characterization (50)\nS(w, \u03b8k) = E [ dist (g,N\u03b8k(A)) 2 ] ,\nwhere g is a p-dimensional random vector with i.i.d. normal entries and N\u03b8k(A) is the conic hull of \u2202\u03b8k(w). We then get:\nS(w, \u03b8k) = E\n[ inf\nt>0 & u\u2208t\u2202\u03b8k(w) \u2016u\u2212 g\u201622\n]\n\u2264 inf t>0 E\n[ inf\nu\u2208t\u2202\u03b8k(w) \u2016u\u2212 g\u201622\n]\n\u2264 inf t>0 E inf h\u2208Rp,\u2016h\u2016\u221e\u22641\n{\u2225\u2225\u2225\u2225gI2 \u2212 t (r + 1)\n\u2016wI1\u20161 wI2\n\u2225\u2225\u2225\u2225 2\n2\n+ \u2016gI1 \u2212 t sgn(wI1)\u201622\n+ \u2016gI0 \u2212 thI0\u201622 }\n\u2264 inf t>0\n{ |I2|+\n(r + 1)2 \u2016wI2\u201622 \u2016wI1\u201621 t2 + |I1|(1 + t2) + |I0| 2\u221a 2\u03c0 1 t exp\n( \u2212 t 2\n2\n)} (61)\n= inf t>0\n{ s+ t2 { (r + 1)2 \u2016wI2\u201622\n\u2016wI1\u201621 + |I1|\n} + (p\u2212 s) 2\u221a\n2\u03c0\n1 t exp\n( \u2212 t 2\n2\n)}\n\u2264 5 4 s+ 2\n{ (r + 1)2 \u2016wI2\u201622\n\u2016wI1\u201621 + |I1|\n} log p\ns , (62)\nwhere following Chandrasekaran et al. (2012, Annex C), for (61) we used the fact that for a standard normal random variable G \u223c N (0, 1)\nEG inf |\u03b7|\u22641 (G\u2212 t\u03b7)2 \u2264 2\u221a 2\u03c0 1 t e\u2212 t2 2 ,\nwhile (62) is obtained by taking b = \u221a\n2 log(p/s) and using s(1\u2212s/p)\u221a \u03c0 log(p/s) \u2264 14 . For the lasso case (k = 1), we have r = 0, I2 = \u2205 and I1 = [1, s]. Plugging this into (62) we recover the standard bound (Chandrasekaran et al., 2012):\nS(w, \u03b8k) \u2264 5\n4 s+ 2s log\np s . (63)\nIn the general case 1 \u2264 k \u2264 s remember that, by definition of r,\n|wk\u2212r\u22121| > \u2016wI1\u20161 r + 1 \u2265 |wk\u2212r| ,\nand therefore\n|I2| \u2264 \u2016wI2\u201622 |wk\u2212r\u22121|2 \u2264 (r + 1) 2 \u2016wI2\u201622 \u2016wI1\u201621 \u2264 \u2016wI2\u2016 2 2 |wk\u2212r|2 . (64)\nPlugging the left-hand inequality of (64) into (62) and remembering that |I2|+ |I1| = s shows that the bound (62) obtained for \u03b8k, for any 1 \u2264 k \u2264 s, is never better than the bound (63) obtained for the lasso case k = 1. In the case s = k, the right-hand inequality of (64) applied to an atom w \u2208 Akp with atom strength \u03b3 = k|wk|2 and unit \u21132 norm leads to\n(r + 1)2 \u2016wI2\u201622 \u2016wI1\u201621 + |I1| \u2264 \u2016wI2\u201622 |wk\u2212r|2 + |I1| \u2264 \u2016wI2\u201622 |wk|2 + \u2016wI1\u201622 |wk|2 = 1 |wk|2 = k \u03b3 ,\nfrom which we deduce by (62) the upper bound on the statistical dimension\n\u2200w \u2208 Akp , S(w, \u03b8k) \u2264 5\n4 k +\n2k\n\u03b3 log\np k ."}], "references": [{"title": "Living on the edge: Phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Technical Report 1303.6672,", "citeRegEx": "Amelunxen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amelunxen et al\\.", "year": 2013}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M.J. Wainwright"], "venue": "Ann. Stat.,", "citeRegEx": "Amini and Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2009}, {"title": "Sparse prediction with the k-support norm", "author": ["A. Argyriou", "R. Foygel", "N. Srebro"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Argyriou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2012}, {"title": "Convex relaxations of structured matrix factorizations", "author": ["F. Bach"], "venue": "Technical Report 1309.3117,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "Convex sparse matrix factorizations", "author": ["F. Bach", "J. Mairal", "J. Ponce"], "venue": "Technical Report 0812.1869,", "citeRegEx": "Bach et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2008}, {"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Stat. Sci.,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E.J. Cand\u00e8s", "T. Strohmer", "V. Voroninski"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2013}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["V. Chandrasekaran", "M.I. Jordan"], "venue": "Proc. Natl. Acad. Sci. USA,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "The convex geometry of linear inverse problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Found. Comput. Math.,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2012}, {"title": "On bounds for the normal integral", "author": ["J.T. Chu"], "venue": "Biometrika, 42(1/2):263\u2013265,", "citeRegEx": "Chu.,? \\Q1954\\E", "shortCiteRegEx": "Chu.", "year": 1954}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. El Ghaoui"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook of the Geometry of Banach Spaces,", "citeRegEx": "Davidson and Szarek.,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek.", "year": 2001}, {"title": "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics", "author": ["M.M. Deza", "M. Laurent"], "venue": null, "citeRegEx": "Deza and Laurent.,? \\Q1997\\E", "shortCiteRegEx": "Deza and Laurent.", "year": 1997}, {"title": "Finding approximately rank-one submatrices with the nuclear norm and l1 norms", "author": ["X.V. Doan", "S.A. Vavasis"], "venue": "SIAM J. Optimiz.,", "citeRegEx": "Doan and Vavasis.,? \\Q2013\\E", "shortCiteRegEx": "Doan and Vavasis.", "year": 2013}, {"title": "Corrupted sensing: Novel guarantees for separating structured signals", "author": ["R. Foygel", "L. Mackey"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Foygel and Mackey.,? \\Q2014\\E", "shortCiteRegEx": "Foygel and Mackey.", "year": 2014}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Summing and Nuclear Norms in Banach Space Theory. Number 8 in London Mathematical Society Student Texts", "author": ["G.J.O. Jameson"], "venue": null, "citeRegEx": "Jameson.,? \\Q1987\\E", "shortCiteRegEx": "Jameson.", "year": 1987}, {"title": "Large cliques elude the Metropolis process", "author": ["M. Jerrum"], "venue": "Random Struct. Alg.,", "citeRegEx": "Jerrum.,? \\Q1992\\E", "shortCiteRegEx": "Jerrum.", "year": 1992}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Nuclear norm penalization and optimal rates for noisy matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A.B. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2011}, {"title": "Do semidefinite relaxations really solve sparse PCA", "author": ["R. Krauthgamer", "B. Nadler", "D. Vilenchik"], "venue": "Technical Report 1306:3690,", "citeRegEx": "Krauthgamer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krauthgamer et al\\.", "year": 2013}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Conditional gradient algorithms for rank-one matrix approximations with a sparsity constraint", "author": ["R. Luss", "M. Teboulle"], "venue": "SIAM Rev.,", "citeRegEx": "Luss and Teboulle.,? \\Q2013\\E", "shortCiteRegEx": "Luss and Teboulle.", "year": 2013}, {"title": "Deflation methods for sparse PCA", "author": ["L.W. Mackey"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Mackey.,? \\Q2009\\E", "shortCiteRegEx": "Mackey.", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Spectral bounds for sparse PCA: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "Sh. Avidan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Sparse regression as a sparse eigenvalue problem", "author": ["B. Moghaddam", "A. Gruber", "Y. Weiss", "S. Avidan"], "venue": "In Information Theory and Applications Workshop,", "citeRegEx": "Moghaddam et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2008}, {"title": "A unified framework for highdimensional analysis of M-estimators", "author": ["S. N Negahban", "P. Ravikumar", "M. J Wainwright", "B. Yu"], "venue": "Statistical Science,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "Sharp mse bounds for proximal denoising", "author": ["S. Oymak", "B. Hassibi"], "venue": "Technical Report 1305.2714,", "citeRegEx": "Oymak and Hassibi.,? \\Q2013\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2013}, {"title": "Simultaneously structured models with application to sparse and low-rank matrices", "author": ["S. Oymak", "A. Jalali", "M. Fazel", "Y.C. Eldar", "B. Hassibi"], "venue": "Technical Report 1212.3753,", "citeRegEx": "Oymak et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oymak et al\\.", "year": 2012}, {"title": "The squared-error of generalized LASSO: A precise analysis", "author": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"], "venue": "In 51st Annual Allerton Conference on Communication,", "citeRegEx": "Oymak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oymak et al\\.", "year": 2013}, {"title": "Estimation of simultaneously sparse and low-rank matrices", "author": ["E. Richard", "P.-A. Savalle", "N. Vayatis"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Richard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2012}, {"title": "Intersecting singularities for multi-structured estimation", "author": ["E. Richard", "F. Bach", "J.-P. Vert"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Richard et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2013}, {"title": "Link prediction in graphs with autoregressive features", "author": ["E. Richard", "S. Ga\u00efffas", "N. Vayatis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Richard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2014}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Math. Program.,", "citeRegEx": "Tseng and Yun.,? \\Q2009\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Wainwright.", "year": 2009}, {"title": "Provable subspace clustering: When LRR meets SSC", "author": ["Y.-X. Wang", "H. Xu", "C. Leng"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Lin. Alg. Appl.,", "citeRegEx": "Watson.,? \\Q1992\\E", "shortCiteRegEx": "Watson.", "year": 1992}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "URL http://dx.doi.org/10.1093/biostatistics/kxp008", "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X.-T. Yuan", "T. Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Yuan and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2013}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}, {"title": "offers the possibility of constraining the estimator to lie in a cone C. In our case, C = Rm1\u00d7m2 , given the definition of \u03b3 we therefore have \u03b3 \u2264 2. The result follows from applying the theorem with \u03b82 = \u03b6(a", "author": ["Oymak"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "q,m1/k,m2/q \u2265 M then n0 is large enough to ensure 1 \u2212 c1 exp(\u2212c2n0) > 4 exp (\u221232/17). Then, according to Lemma 38, solving (28) with the norm \u0393\u03bc fails to recover A = ab with probability at least 4 exp (\u221232/17)", "author": ["M Take"], "venue": "On the other hand, Amelunxen et al", "citeRegEx": "Take,? \\Q2013\\E", "shortCiteRegEx": "Take", "year": 2013}, {"title": "Annex C), for (61) we used the fact that for a standard normal random variable G", "author": ["Chandrasekaran"], "venue": null, "citeRegEx": "Chandrasekaran,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations.", "startOffset": 70, "endOffset": 94}, {"referenceID": 35, "context": "A range of machine learning problems such as link prediction in graphs containing community structure (Richard et al., 2014), phase retrieval (Cand\u00e8s et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 7, "context": ", 2014), phase retrieval (Cand\u00e8s et al., 2013), subspace clustering (Wang et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 40, "context": ", 2013), subspace clustering (Wang et al., 2013) or dictionary learning for sparse coding (Mairal et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 26, "context": ", 2013) or dictionary learning for sparse coding (Mairal et al., 2010) amount to solve sparse matrix factorization problems, i.", "startOffset": 49, "endOffset": 70}, {"referenceID": 44, "context": "Landmark applications of sparse matrix factorization are sparse principal components analysis (SPCA, d\u2019Aspremont et al., 2007; Zou et al., 2006) or sparse canonical correlation analysis (SCCA, Witten et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 27, "context": "From a computational point of view, however, sparse matrix factorization is challenging since it typically leads to non-convex, NP-hard problems (Moghaddam et al., 2006).", "startOffset": 145, "endOffset": 169}, {"referenceID": 6, "context": "For instance, Berthet and Rigollet (2013) noted that solving sparse PCA with a single component is equivalent to the planted clique", "startOffset": 14, "endOffset": 42}, {"referenceID": 19, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix.", "startOffset": 8, "endOffset": 22}, {"referenceID": 23, "context": "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).", "startOffset": 158, "endOffset": 197}, {"referenceID": 26, "context": "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).", "startOffset": 158, "endOffset": 197}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008).", "startOffset": 102, "endOffset": 162}, {"referenceID": 25, "context": "Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix.", "startOffset": 143, "endOffset": 157}, {"referenceID": 18, "context": "(2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 196, "endOffset": 211}, {"referenceID": 15, "context": "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).", "startOffset": 204, "endOffset": 282}, {"referenceID": 31, "context": "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).", "startOffset": 204, "endOffset": 282}, {"referenceID": 1, "context": "(2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.", "startOffset": 161, "endOffset": 189}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 648}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 674}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 697}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component. These algorithms perform well empirically and have been proved to be efficient theoretically under mild conditions by Yuan and Zhang (2013). Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al.", "startOffset": 9, "endOffset": 889}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 103, "endOffset": 497}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 103, "endOffset": 621}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al.", "startOffset": 103, "endOffset": 1387}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al.", "startOffset": 103, "endOffset": 1408}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone.", "startOffset": 103, "endOffset": 1429}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone. Krauthgamer et al. (2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.", "startOffset": 103, "endOffset": 1580}, {"referenceID": 9, "context": "2 two new atomic norms for matrices (Chandrasekaran et al., 2012).", "startOffset": 36, "endOffset": 65}, {"referenceID": 17, "context": "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 2, "context": "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al. (2012). \u2022 Using these norms to estimate sparse low-rank matrices (Section 3).", "startOffset": 155, "endOffset": 178}, {"referenceID": 2, "context": "3 we relate these matrix norms to vector norms using the concept of nuclear norms, establishing in particular a connection of the (k, q)-trace norm for matrices with the k-support norm of Argyriou et al. (2012), and the (k, q)-CUT norm to the vector k-norm, defined as the sum of the k largest components in absolute value of a vector (Bhatia, 1997, Exercise II.", "startOffset": 188, "endOffset": 211}, {"referenceID": 44, "context": "For example, the standard rank-1 SPCA problem consists in finding the symmetric matrix with (k, k)-rank equal to 1 and providing the best approximation of the sample covariance matrix (Zou et al., 2006).", "startOffset": 184, "endOffset": 202}, {"referenceID": 9, "context": "They are both instances of the atomic norms introduced by Chandrasekaran et al. (2012), which we first review.", "startOffset": 58, "endOffset": 87}, {"referenceID": 9, "context": "Chandrasekaran et al. (2012) show that the atomic norm induced by A is indeed a norm, which can be rewritten as \u2016x\u2016A = inf { \u2211", "startOffset": 0, "endOffset": 29}, {"referenceID": 36, "context": "see Rockafellar (1997), p.", "startOffset": 4, "endOffset": 23}, {"referenceID": 14, "context": "Our choice of terminology is motivated by the following relation of our norm to the CUT-polytope: in the case k = m1 and q = m2, the unit ball of \u03a9\u0303k,q coincides (up to a scaling factor of \u221a m1m2) with the polytope known as the CUT polytope of the complete graph on n vertices (Deza and Laurent, 1997), defined by CUT = conv {ab , a \u2208 {\u00b11}1 , b \u2208 {\u00b11}2} .", "startOffset": 277, "endOffset": 301}, {"referenceID": 18, "context": "3 Equivalent nuclear norms built upon vector norms In this section we show that the (k, q)-trace norm (Definition 4) and the (k, q)-CUT norm (Definition 8), which we defined as atomic norms induced by specific atom sets, can alternatively be seen as instances of nuclear norms considered by Jameson (1987). For that purpose it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1.", "startOffset": 291, "endOffset": 306}, {"referenceID": 2, "context": "The (k, q)-trace norm is the nuclear norm induced by \u03b8k on R m1 and \u03b8q on Rm2 , where for any j \u2265 1, \u03b8j is the j-support norm introduced by Argyriou et al. (2012). 2.", "startOffset": 140, "endOffset": 163}, {"referenceID": 2, "context": "For the sake of completeness, let us recall the closed-form expression of the k-support norm \u03b8k shown by Argyriou et al. (2012). For any vector w \u2208 Rp, let w\u0304 \u2208 Rp be the vector obtained by sorting the entries of w by decreasing order of absolute values.", "startOffset": 105, "endOffset": 128}, {"referenceID": 5, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case).", "startOffset": 41, "endOffset": 81}, {"referenceID": 15, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al.", "startOffset": 26, "endOffset": 41}, {"referenceID": 3, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case). However Bach et al. (2012) proposed nuclear norms associated with vectors norms that are similar to the elastic net penalty.", "startOffset": 51, "endOffset": 253}, {"referenceID": 7, "context": "Quadratic regression combined with additional constraints on Z is closely related to phase retrieval (Cand\u00e8s et al., 2013).", "startOffset": 101, "endOffset": 122}, {"referenceID": 40, "context": "This idea, exploited recently by Wang et al. (2013) implies that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown.", "startOffset": 33, "endOffset": 52}, {"referenceID": 11, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 42, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 44, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 25, "context": "In contrast to sequential approaches that estimate the principal components one by one (Mackey, 2009), this formulation requires to find simultaneously a set of factors which are complementary to one another in order to explain as much variance as possible.", "startOffset": 87, "endOffset": 101}, {"referenceID": 28, "context": "which it is known to be NP-hard (Moghaddam et al., 2008).", "startOffset": 32, "endOffset": 56}, {"referenceID": 29, "context": "Obviously the comparison of upper bounds is not enough to conclude to the superiority of (k, q)-trace norm and, admittedly, the problem of denoising considered here is a special instance of linear regression in which the design matrix is the identity, and, since this is a case in which the design is trivially incoherent, it is possible to obtain fast rates for decomposable norms such as the l1 or trace norm (Negahban et al., 2012); however, slow rates are still valid in the presence of an incoherent design, or when the signal to recover is only weakly sparse, which is not the case for the fast rates.", "startOffset": 411, "endOffset": 434}, {"referenceID": 0, "context": "We present in the next section more involved results, based on lower and upper bounds on the so-called statistical dimension of the different norms (Amelunxen et al., 2013), a measure which is closely related to Gaussian widths.", "startOffset": 148, "endOffset": 172}, {"referenceID": 9, "context": "The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al.", "startOffset": 168, "endOffset": 197}, {"referenceID": 31, "context": "In particular, we will consider the norms \u03a9k,q, \u03a9\u0303k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).", "startOffset": 177, "endOffset": 219}, {"referenceID": 33, "context": "In particular, we will consider the norms \u03a9k,q, \u03a9\u0303k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).", "startOffset": 177, "endOffset": 219}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al.", "startOffset": 117, "endOffset": 141}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.", "startOffset": 117, "endOffset": 171}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.", "startOffset": 117, "endOffset": 197}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty.", "startOffset": 117, "endOffset": 218}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty. These results rely essentially on the fact that if the tangent cone of the regularizer at a point of interest Z is thiner, then the regularizer is more efficient at solving problems of denoising, demixing and compressed sensing of Z. The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al. (2013). In this section, we study the statistical dimensions induced by different matrix norms in order to compare their theoretical properties for exact or approximate recovery of sparse low-rank matrices.", "startOffset": 117, "endOffset": 857}, {"referenceID": 9, "context": "14 in Chandrasekaran et al. (2012) and from the upper bound log (m k ) \u2264 k(1 + log(m/k)), that Proposition 21 For any A \u2208 \u00c3k,q, we have S(A, \u03a9\u0303k,q) \u2264 16(k + q) + 9 ( k log m1 k + q log m2 q ) .", "startOffset": 6, "endOffset": 35}, {"referenceID": 39, "context": "In the vector case, for example, it is known that the recovery of a sparse vector \u03b2 with support I0 depends on its smallest coefficient \u03b2min = mini\u2208I0 \u03b2 2 i (Wainwright, 2009).", "startOffset": 157, "endOffset": 175}, {"referenceID": 8, "context": "This result is actually stated informally for the special case of k = q = \u221a m = with m = m1 = m2 in the context of a discussion of the planted clique problem in Chandrasekaran and Jordan (2013).", "startOffset": 161, "endOffset": 194}, {"referenceID": 31, "context": "Finally, regarding the combination \u0393\u03bc of the l1 norm and of the trace norm, Oymak et al. (2012) has shown that it does not improve rates up to constants over the best of the two norms.", "startOffset": 76, "endOffset": 96}, {"referenceID": 31, "context": "In that case, we see that, as stated by Oymak et al. (2012), \u0393\u03bc does not bring any improvement over the l1 and trace norms taken imdividually, and in particular has a worse statistical dimension than \u03a9k,q and \u03a9\u0303k,q.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "In the lasso case (k = 1), we recover the standard bound (Chandrasekaran et al., 2012):", "startOffset": 57, "endOffset": 86}, {"referenceID": 3, "context": "Working set algorithms (Bach et al., 2011, Chap. 6) are typically useful to speed up algorithm for sparsity inducing regularizer; they have been used notably in the case of the overlapping group Lasso of Jacob et al. (2009) which is also naturally formulated via latent components.", "startOffset": 24, "endOffset": 224}, {"referenceID": 41, "context": "From the characterization of the subdifferential of the trace norm (Watson, 1992), writing Z = U \u03a3V (IJ) the SVD of Z, this is equivalent to, for all (I, J) in S, either Z 6=0 and \u2207L(Z)IJ = \u2212\u03bb ( U V (IJ) \u22a4 +A )", "startOffset": 67, "endOffset": 81}, {"referenceID": 37, "context": "Problem (PS) is solved easily using the approximate block coordinate descent of Tseng and Yun (2009) (see", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 24, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 43, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 20, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 134, "endOffset": 260}, {"referenceID": 43, "context": "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time.", "startOffset": 22, "endOffset": 44}, {"referenceID": 43, "context": "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time. If the conditions hold at every step of gradient, the overall cost of an iteration can be cast into the cost of evaluating the gradient and the evaluation of thin SVDs: O(k2q). Evaluating the gradient has a cost dependent on the risk function L. This cost for usual applications is O(m1m2). So assuming the RIP conditions required by Yuan and Zhang (2013) hold, the cost of Algorithm 2 is dominated by matrix-vector multiplications so of the order O(m1m2).", "startOffset": 22, "endOffset": 469}, {"referenceID": 30, "context": "Fro \u03c32 is a good estimate of the statistical dimension, since Oymak and Hassibi (2013) show that", "startOffset": 62, "endOffset": 87}], "year": 2017, "abstractText": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.", "creator": "LaTeX with hyperref package"}}}