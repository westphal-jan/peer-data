{"id": "1409.7495", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2014", "title": "Unsupervised Domain Adaptation by Backpropagation", "abstract": "In the absence of tagged data for a particular task, domain adaptation often offers an attractive option, since tagged data of a similar nature is available from a different domain (e.g. synthetic images). At this point, we propose a new approach to domain adaptation in deep architectures that can be trained on large amounts of tagged data from the source domain and large amounts of untagged data from the target domain (no tagged target domain data is required).", "histories": [["v1", "Fri, 26 Sep 2014 08:22:21 GMT  (5856kb,D)", "http://arxiv.org/abs/1409.7495v1", null], ["v2", "Fri, 27 Feb 2015 14:54:37 GMT  (6158kb,D)", "http://arxiv.org/abs/1409.7495v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["yaroslav ganin", "victor s lempitsky"], "accepted": true, "id": "1409.7495"}, "pdf": {"name": "1409.7495.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Domain Adaptation by Backpropagation", "authors": ["Yaroslav Ganin", "Victor Lempitsky"], "emails": ["ganin@skoltech.ru", "lempitsky@skoltech.ru"], "sections": [{"heading": null, "text": "As the training progresses, the approach promotes the emergence of \u201cdeep\u201d features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall the whole approach can be implemented with little effort using any of the deep-learning packages."}, {"heading": "1. Introduction", "text": "Deep feed-forward architectures have brought impressive advances to the state-of-the-art across a wide variety tasks within computer vision and beyond. At the moment, however, these leaps in performance emerge only when a large amount of labeled training data is available. At the same time, for problems lacking labeled data, it may be still possible to obtain training sets that are big enough for training large-scale deep models, but that suffer from the shift in data distribution from the actual data encountered at \u201ctest time\u201d. One particularly important example is synthetic or semi-synthetic imagery, which may come in abundance and fully labeled, but which inevitably look different from real data [13, 20, 23, 21].\nLearning a discriminative classifier or other predictor in\nthe presence of a shift between training and testing distributions is known as domain adaptation (DA). A number of approaches to domain adaptation has been suggested in the context of shallow learning, e.g. in the situation when data representation/features are given and fixed. The proposed approaches then build the mappings between the source (training-time) and the target (test-time) domains, so that the classifier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains. The appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully unlabeled (unsupervised domain annotation) or have few labeled samples (semi-supervised domain adaptation). Below, we focus on the harder unsupervised case, although the proposed approach can be generalized to the semi-supervised case rather straightforwardly.\nUnlike most papers previous papers on domain adaptation that worked with fixed feature representations, we focus on combining domain adaptation and deep feature learning within one training process (deep domain adaptation). Our goal is to embed domain adaptation into the process of learning representation, so that the final classification decisions are made based on features that are both discriminative and invariant to the change of domains, i.e. have the same or very similar distributions in the source and the target domains. In this way, the obtained feed-forward network can be applicable to the target domain without being hindered by the shift between the two domains.\nWe thus focus on learning features that combine (i) discriminativeness and (ii) domain-invariance. This is achieved by jointly optimizing the underlying features as well as two discriminative classifiers operating on these features: (i) the label predictor that predicts class labels and is used at test time and (ii) the domain classifier that discriminates between the source and the target domains during training. While the parameters of the classifiers are optimized in order to minimize their error on the training set, the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classifier and to maximize the loss of the domain classifier. The\n1\nar X\niv :1\n40 9.\n74 95\nv1 [\nst at\n.M L\n] 2\n6 Se\np 20\n14\nlatter encourages domain-invariant features to emerge in the course of the optimization.\nCrucially, we show that all three training processes can be embedded into an appropriately composed deep feedforward network (Figure 1) that uses standard layers and loss functions, and can be trained using standard backpropagation algorithms based on stochastic gradient descent or its modifications (e.g. momentum). Our approach is generic as it can be used to add domain adaptation to any existing feed-forward architecture that is trainable by backpropagation. In practice, the only non-standard component of the proposed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation.\nBelow, we detail the proposed approach to domain adaptation in deep architectures, and present prelimenary results on traditional deep learning datasets (such as MNIST [12] and SVHN [14]) that clearly demonstrate the unsupervised domain adaptation ability of the proposed method."}, {"heading": "2. Related work", "text": "A large number of domain adaptation methods have been proposed over the recent years, and here we focus on the most related ones. Multiple methods perform unsupervised domain adaptation by matching the feature distributions in the source and the target domains. Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2]. An important aspect of the distribution matching approach is the way the (dis)similarity between distributions is measured. Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space [3, 11], whereas [8, 5] map the principal axes associated with each of the distributions. Our approach also attempts to match feature space distributions, however this is accomplished by modifying the feature representation itself rather than by reweighing or geometric transformation. Also, our method uses (implicitly) a rather different way to measure the disparity between distributions based on their separability by a deep discriminatively-trained classifier.\nSeveral approaches perform gradual transition from the source to the target domain [10, 8] by a gradual change of the training distribution. Among these methods, [17] does this in a \u201cdeep\u201d way by the layerwise training of a sequence of deep autoencoders, while gradually replacing source-domain samples with target-domain samples. This improves over a similar approach of [6] that simply trains a single deep autoencoder for both domains. In both approaches, the actual classifier/predictor is learned in a separate step using the feature representation learned by au-\ntoencoder(s). In contrast to [6, 17], our approach performs feature learning, domain adaptation and classifier learning jointly, in a unified architecture, and using a single learning algorithm (backpropagation). We therefore argue that our approach is much simpler (both conceptually and in terms of its implementation).\nWhile the above approaches perform unsupervised domain adaptation, there are approaches that perform supervised domain adaptation by exploiting labeled data from the target domain. In the context of deep feed-forward architectures, such data can be used to \u201cfine-tune\u201d the network trained on the source domain [24, 15, 1]. Our approach does not require labeled target-domain data. At the same time, it can easily incorporate such data when they are available.\nThe work that is most related to ours is the recent (and concurrent) technical report [9] on adversarial networks. While their goal is quite different (building generative deep networks that can synthesize samples), the way they measure and minimize the discrepancy between the distribution of the training data and the distribution of the synthesized data is very similar to the way our architecture measures and minimizes the discrepancy between feature distributions for the two domains."}, {"heading": "3. Deep Domain Adaptation", "text": ""}, {"heading": "3.1. The model", "text": "We now detail the proposed model for the domain adaptation. We assume that the model works with input samples x \u2208 X , where X is some input space (e.g. images of a certain size) and certain labels (output) y from the label space Y . Below, we assume classification problems where Y is a finite set (Y = {1, 2, . . . L}), however our approach is generic and can handle any output label space that other deep feed-forward models can handle. We further assume that there exist two distributions S(x, y) and T (x, y) on X \u2297 Y , which will be referred to as the source distribution and the target distribution (or the source domain and the target domain). Both distributions are assumed complex and unknown, and furthermore similar but different (in other words, S is \u201cshifted\u201d from T by some domain shift).\nOur ultimate goal is to be able to predict labels y given the input x for the target distribution. At training time, we have an access to a large set of training samples {x1,x2, . . . ,xN} from both the source and the target domains distributed according to the marginal distributions S(x) and T (x). We denote with di the binary variable (domain label) for the ith example, which indicates whether xi come from the source distribution (xi\u223cS(x) if di=0) or from the target distribution (xi\u223cT (x) if di=1). For the examples from the source distribution (di=0) the corresponding labels yi \u2208 Y are known at training time. For the examples from the target domains, we do not know the labels\nat training time, and we want to predict such labels at test time.\nWe now define a deep feed-forward architecture that for each input x predicts its label y \u2208 Y and its domain label d \u2208 {0, 1}. We decompose such mapping into three parts. We assume that the input x is first mapped by a mapping Gf (a feature extractor) to a D-dimensional feature vector f \u2208 RD. The feature mapping may also include several feed-forward layers and we denote the vector of parameters of all layers in this mapping as \u03b8f , i.e. f = Gf (x; \u03b8f ). Then, the feature vector f is mapped by a mapping Gy (label predictor) to the label y, and we denote the parameters of this mapping with \u03b8y . Finally, the same feature vector f is mapped to the domain label d by a mapping Gd (domain classifier) with the parameters \u03b8d (Figure 1).\nDuring the learning stage, we aim to minimize the label prediction loss on the annotated part (i.e. the source part) of the training set, and the parameters of both the feature extractor and the label predictor are thus optimized in order to minimize the empirical loss for the source domain samples. This ensures the discriminativeness of the features f and the overall good prediction performance of the combination of the feature extractor and the label predictor on the source domain.\nAt the same time, we want to make the features f domain-invariant. That is, we want to make the distributions S(f) = {Gf (x; \u03b8f ) |x\u223cS(x)} and T (f) = {Gf (x; \u03b8f ) |x\u223cT (x)} to be similar. Under the covariate shift assumption, this would make the label prediction ac-\ncuracy on the target domain to be the same as on the source domain [18]. Measuring the dissimilarity of the distributions S(f) and T (f) is however non-trivial, given that f is high-dimensional, and that the distributions themselves are constantly changing as learning progresses. One way to estimate the dissimilarity is to look at the loss of the domain classifierGd, provided that the parameters \u03b8d of the domain classifier have been trained to discriminate between the two feature distributions in an optimal way.\nThis leads us to our idea. At training time, in order to obtain domain-invariant features, we seek the parameters \u03b8f of the feature mapping that maximize the loss of the domain classifier (by making the two feature distributions as similar as possible), while simultaneously seeking the parameters \u03b8d of the domain classifier that minimize the loss of the domain classifier. In addition, we seek to minimize the loss of the label predictor.\nMore formally, we consider the functional:\nE(\u03b8f , \u03b8y, \u03b8d) = \u2211 i=1..N di=0 Ly ( Gy(Gf (xi; \u03b8f ); \u03b8y), yi ) \u2212\n\u03bb \u2211 i=1..N Ld ( Gd(Gf (xi; \u03b8f ); \u03b8d), yi ) =\n= \u2211 i=1..N di=0 Liy(\u03b8f , \u03b8y)\u2212 \u03bb \u2211 i=1..N Lid(\u03b8f , \u03b8d) (1)\nHere, Ly(\u00b7, \u00b7) is the loss for label prediction (e.g. multinomial), Ld(\u00b7, \u00b7) is the loss for the domain classification (e.g.\nlogistic), while Liy and L i d denote the corresponding loss functions evaluated at the ith training example. Based on our idea, we are seeking the parameters \u03b8\u0302f , \u03b8\u0302y, \u03b8\u0302d that deliver a saddle point of the functional (1):\n(\u03b8\u0302f , \u03b8\u0302y) = arg min \u03b8f ,\u03b8y E(\u03b8f , \u03b8y, \u03b8\u0302d) (2)\n\u03b8\u0302d = arg max \u03b8d E(\u03b8\u0302f , \u03b8\u0302y, \u03b8d) . (3)\nAt the saddle point, the parameters \u03b8d of the domain classifier \u03b8d minimize the domain classification loss (since it enters into (1) with the minus sign) while the parameters \u03b8y of the label predictor minimize the label prediction loss. The feature mapping parameters \u03b8f minimize the label prediction loss (i.e. the features are discriminative), while maximizing the domain classification loss (i.e. the features are domain-invariant). The parameter \u03bb controls the trade-off between the two objectives that shape the features during learning.\nBelow, we demonstrate that standard stochastic gradient solvers (SGD) can be adapted for the search of the saddle point (2)-(3)."}, {"heading": "3.2. Optimization with backpropagation", "text": "A saddle point (2)-(3) can be found as a stationary point of the following stochastic updates:\n\u03b8f \u2190\u2212 \u03b8f \u2212 \u00b5 ( \u2202Liy \u2202\u03b8f \u2212 \u03bb\u2202L i d \u2202\u03b8f ) (4)\n\u03b8y \u2190\u2212 \u03b8y \u2212 \u00b5 \u2202Liy \u2202\u03b8y\n(5)\n\u03b8d \u2190\u2212 \u03b8d \u2212 \u00b5 \u2202Lid \u2202\u03b8d\n(6)\nwhere \u00b5 is the learning rate (which can vary over time). The updates (4)-(6) are very similar to stochastic gradient descent (SGD) updates for a feed-forward deep model that comprises feature extractor fed into the label predictor and into the domain classifier. The difference is the\u2212\u03bb factor in (4) (the difference is important, as without such factor, stochastic gradient descent would try to make features dissimilar across domains in order to minimize the domain classification loss). Although direct implementation of (4)- (6) as SGD is not possible, it is highly desirable to reduce the updates (4)-(6) to some form of SGD, since SGD (and its variants) is the main learning algorithm implemented in most packages for deep learning.\nFortunately, such reduction can be accomplished by introducing a special gradient reversal layer (GRL) defined as follows. The gradient reversal layer has no parameters associated with it (apart from the meta-parameter \u03bb, which\nis not updated by backpropagation). During the forward propagation, GRL acts as an identity transform. During the backpropagation though, GRL takes the gradient from the subsequent level, multiplies it by \u2212\u03bb and passes it to the preceding layer. Implementing such layer using existing object-oriented packages for deep learning is thus as simple as defining a new layer can be, as defining procedures for forwardprop (identity transform), backprop (multiplying by a constant), and parameter update (nothing) is trivial.\nThe GRL as defined above is inserted between the feature extractor and the domain classifier, resulting in the architecture depicted in Figure 1. As the backpropagation process passes through the GRL, the partial derivatives of the loss that is downstream the GRL (i.e. Ld) w.r.t. the layer parameters that are upstream the GRL (i.e. \u03b8f ) get multiplied by \u2212\u03bb, i.e. \u2202Ld\u2202\u03b8f is effectively replaced with \u2212\u03bb \u2202Ld \u2202\u03b8f\n. Therefore, running SGD in the resulting model implements the updates (4)-(6) and converges to a saddle point of (1).\nMathematically, we can formally treat the gradient reversal layer as a \u201cpseudo-function\u201d R\u03bb(x) defined by two (incompatible) equations describing its forward- and backpropagation behaviour:\nR\u03bb(x) = x (7) dR\u03bb dx = \u2212\u03bbI (8)\nwhere I is an identity matrix. We can then define the objective \u201cpseudo-function\u201d of (\u03b8f , \u03b8y, \u03b8d) that is being optimized by the stochastic gradient descent within our method:\nE\u0303(\u03b8f , \u03b8y, \u03b8d) = \u2211 i=1..N di=0 Ly ( Gy(Gf (xi; \u03b8f ); \u03b8y), yi ) +\n\u2211 i=1..N Ld ( Gd(R\u03bb(Gf (xi; \u03b8f )); \u03b8d), yi ) (9)\nRunning SGD for (9) thus leads to the emergence of features that are domain-invariant and discriminative at the same time. After the learning, the label predictor y(x) = Gy(Gf (x; \u03b8f ); \u03b8y) can be used to predict labels for samples from the target domain (as well as from the source domain).\nThe simple learning procedure outlined above can be rederived/generalized along the lines suggested in [9] (see Appendix)."}, {"heading": "4. Experiments", "text": ""}, {"heading": "Datasets", "text": "In this report, we validate our approach within a set experiments on digit image classification. In each case, we train on the source dataset and test on a different target domain dataset, with considerable shifts between domains (see Figure 2). Overall, we have six datasets involved into the experiments that are described below.\nThe first four datasets are the well-known MNIST dataset [12] and its modifications. The modifications include:\n\u2022 MNIST (D): Binary dilation with a 3\u00d73 all-ones structuring element. This operation makes strokes thicker and may fill small holes thereby introducing additional challenge to the classification task.\n\u2022 MNIST (max, BG): Blending white digits over the patches randomly extracted from photos (BSDS500). While leaving meaningful pixels as they are, this modification adds strong background clutter.\n\u2022 MNIST (|\u2206|, BG): Difference-blending digits over the patches randomly extracted from photos (BSDS500). This operation is formally defined for two images I1, I2 as Ioutijk = |I1ijk \u2212 I2ijk|, where i, j are the coordinates of a pixel and k is a channel index. In other words, an output sample is produced by taking a patch\nfrom a photo and inverting its pixels at positions corresponding to the pixels of a digit. For a human the classification task becomes only slightly harder compared to the original dataset (the digits are still clearly distinguishable) whereas for a CNN trained on MNIST this domain is quite distinct: the background and the strokes are no longer constant.\nThe last two datasets is the well-known Street View House Number (SVHN) dataset [14] and a new synthetic dataset Syn. Numbers of 500,000 images generated by ourselves from Windows fonts by varying the text (that includes different one-, two-, and three-digit numbers), positioning, orientation, background and stroke colors, and the amount of blur. The degrees of variation were chosen manually to simulate SVHN, however the two datasets are still rather distinct, the biggest difference being the structured clutter in the background of SVHN images."}, {"heading": "Baselines", "text": "In each experiment, we compare the results of our approach with the two natural baselines, i.e. training on the source domain without adaptation (a lower bound on any reasonable DA method) and training on the target domain while exploiting target domain labels (an upper bound on DA methods, assuming that target data are abundant and the shift between the domains is considerable).\nIn addition, we compare our approach against the recently proposed unsupervised DA method based on subspace alignment (SA) [5]. We detail the protocol for the use of SA further below.\nThe SA algorithm has one important free parameter, namely the number of principal components. In each of the experiments, we give this baseline an advantage by picking this value from the range {2, . . . , 60}, so that the performance on the target domain is maximized."}, {"heading": "CNN architectures.", "text": "Two different architectures were used in our experiments (Figure 3). We employ the smaller one if the source domain is MNIST and the bigger one otherwise. The chosen architectures are fairly standard for these datasets in terms of feature extractor and label predictor parts: the \u201cMNIST\u201d design is inspired by the classical LeNet-5 [12], while the second CNN is adopted from [19]. The domain classifier branch in both cases is somewhat arbitrary \u2013 the effect of changing its design is yet to be analyzed.\nAs for the loss functions, we set Ly and Ld to be logistic regression cost and binomial cross-entropy respectively."}, {"heading": "Training procedure.", "text": "The model is trained on 128-element batches of 32 \u00d7 32 color patches (we replicate channels for the original MNIST). No preprocessing is done except for the overall mean subtraction. A half of each batch is populated by the samples from the source domain (with known labels), the rest is comprised of the target domain (with unknown labels).\nWe use stochastic gradient descent with 0.9 momentum and the learning rate annealing described by the following formula:\n\u00b5p = \u00b50\n(1 + \u03b1 \u00b7 p)\u03b2 , (10)\nwhere p is the training progress linearly changing from 0 to 1, \u00b50 = 0.01, \u03b1 = 10 and \u03b2 = 0.75 (the schedule was optimized to promote convergence and low error on the source domain).\nIn order to suppress noisy signal from the domain classifier at the early stages of the training procedure instead of\nfixing the adaptation factor \u03bb, we gradually change it from 0 to 1 using the following schedule:\n\u03bbp = 2\n1 + exp(\u2212\u03b3 \u00b7 p) \u2212 1, (11)\nwhere \u03b3 was set to 10 in all experiments (the schedule was not optimized/tweaked).\nFollowing [19] we also use dropout and `2-norm restriction when we train the SVHN architecture.\nFor the SA baseline, we consider the activations of the last hidden layer in the label predictor (before the final linear classifier) as descriptors/features, and learn the mapping between the source and the target domains [5].\nSince the SA baseline requires to train a new classifier after adapting the features, and in order to put all the compared methods on an equal footing, we retrain the last layer of the label predictor using a standard linear SVM [4] for all four compared methods (including ours; the performance on the target domain remains approximately the same after the retraining)."}, {"heading": "Visualizations.", "text": "We use t-SNE [22] projection to visualize feature distributions at different points of the network, while color-coding the domains (Figure 4). Overall, we observe quite strong correlation between the success of the adaptation in terms of the classification accuracy for the target domain, and the amount of discrepancy between the domain distributions in our visualizations."}, {"heading": "4.1. Results.", "text": "We test our approach as well as the baselines for six different domain pairs. The results obtained by the composition of the feature extractor and the label predictor for the three baseline methods and our approach are summarized in Table 1 and are discussed below."}, {"heading": "MNIST\u2192 its variations", "text": "In the first three experiments, we deal with the MNIST dataset: a classifier is trained on the original dataset while being adapted to perform well on a particular modification of the source domain. The three target domains can be ordered in terms of the similarity to the source domain (which can be judged based on the performance of the classifier trained on the source domain and applied to the target domain). As expected, domain adaptation is easiest for the target domain that is closest to the source (MNIST (D)), and our method is able to cover three quarters of the performance gap between the source-trained and target-trained classifiers (i.e. lower and upper bounds).\nThe adaptation task is harder in the case of digits blended over the color background. Although samples from these domains have significant background clutter, our approach succeeded at intermixing the features (Figure 4), which led to very successful adaptation results (considering that the adaptation is unsupervised). The performance of the unsupervised DA method [5] for all three datasets is much more modest, thus highlighting the difficulty of the adaptation task."}, {"heading": "Synthetic numbers\u2192 SVHN", "text": "To address a common scenario of training on synthetic images and testing on challenging real images, we use SVHN as a target domain and synthetic digits as a source. The proposed backpropagation-based technique works well covering two thirds of the gap between training with source data only and training on target domain data with known target labels. In contrast, [5] does not result in any significant improvement in the classification accuracy, thus highlighting that the adaptation task is even more challenging than in the case of MNIST experiments."}, {"heading": "MNIST\u2194 SVHN", "text": "Finally, we test our approach on the two most distinct domains, namely, MNIST and SVHN. Training on SVHN even without adaptation is challenging \u2014 classification error stays high during the first 150 epochs. In order to avoid ending up in a poor local minimum we, therefore, do not use learning rate annealing here. Obviously, the two directions (MNIST-to-SVHN and SVHN-to-MNIST) are not equally difficult. As SVHN is more diverse, a model trained on\nSVHN is expected to be more generic and to perform reasonably on the MNIST dataset. This, indeed, turns out to be the case and is supported by the appearance of the feature distributions. We observe a quite strong separation between the domains when we feed them into the CNN trained solely on MNIST, whereas for the SVHN-trained network the features are much more intermixed. This difference probably explains why our method succeeded in improving the performance by adaptation in the SVHN \u2192 MNIST scenario (see Table 1) but not in the opposite direction (SA is not able to perform adaptation in this case either). Unsupervised adaptation from MNIST to SVHN thus remains a challenge to be addressed in the future work."}, {"heading": "5. Discussion", "text": "We have proposed a new approach to unsupervised domain adaptation of deep feed-forward architectures, which allows large-scale training based on large amount of annotated data in the source domain and large amount of unannotated data in the target domain. Similarly to many previous shallow and deep DA techniques, the adaptation is achieved through aligning the distributions of features across the two domains. However, unlike previous approaches, the alignment is accomplished through standard backpropagation training. The approach is therefore rather scalable, and can be implemented using any deep learning package.\nIn the experiments with digit image classification, the approach demonstrated its efficiency, significantly outperforming a state-of-the-art unsupervised DA method. Further evaluation on larger-scale tasks constitutes the immediate future work. It is also interesting whether the approach can benefit from a good initialization of the feature extrac-\nMNIST\u2192 MNIST (max, BG): top feature extractor layer\ntor. For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in a similar vein to [6, 17], effectively using [6, 17] as an initialization to our method."}, {"heading": "Appendix: An alternative optimization approach", "text": "There exists an alternative construction (inspired by [9]) that leads to the same updates (4)-(6). Rather than using the gradient reversal layer, the construction introduces two different loss functions for the domain classifier. Minimization of the first domain loss (Ld+) should lead to a better domain discrimination, while the second domain loss (Ld\u2212) is minimized when the domains are distinct. Stochastic updates\nfor \u03b8f and \u03b8d are then defined as:\n\u03b8f \u2190\u2212 \u03b8f \u2212 \u00b5 ( \u2202Liy \u2202\u03b8f + \u2202Lid\u2212 \u2202\u03b8f ) (12)\n\u03b8d \u2190\u2212 \u03b8d \u2212 \u00b5 \u2202Lid+ \u2202\u03b8d , (13)\nThus, different parameters participate in the optimization of different losses\nIn this framework, the gradient reversal layer constitutes a special case, corresponding to the pair of domain losses (Ld,\u2212\u03bbLd). However, other pairs of loss functions can be used. One example would be the binomial cross-entropy [9]:\nLd+(q, d) = \u2211 i=1..N di log(qi)+(1\u2212di) log(1\u2212qi) , (14)\nwhere d indicates domain indices and q is an output of the predictor. In that case \u201cadversarial\u201d loss is easily obtained by swapping domain labels, i.e. Ld\u2212(q, d) = Ld+(q, 1\u2212d). This particular pair has a potential advantage of producing stronger gradients at early learning stages if the domains are quite dissimilar. In our experiments, however, we did not observe any significant improvement resulting from this choice of losses."}], "references": [{"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V.S. Lempitsky"], "venue": "ECCV, pp. 584\u2013599", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann"], "venue": "ICCV, pp. 769\u2013776", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["K.M. Borgwardt", "A. Gretton", "M.J. Rasch", "H. Kriegel", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "ISMB, pp. 49\u201357", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u20131874", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "ICCV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML, pp. 513\u2013520", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Connecting the dots with landmarks: Discriminatively learning domaininvariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "ICML, pp. 222\u2013230", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "CVPR, pp. 2066\u20132073", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adversarial networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A.C. Courville", "Y. Bengio"], "venue": "CoRR, abs/1406.2661", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "ICCV, pp. 999\u20131006", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "NIPS, pp. 601\u2013608", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Multi-view object class detection with a 3d geometric model", "author": ["J. Liebelt", "C. Schmid"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks, 22(2):199\u2013 210", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Dlid: Deep learning for domain adaptation by interpolating between domains", "author": ["S.B.S. Chopra", "R. Gopalan"], "venue": "ICML Workshop on Challenges in Representation Learning", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference, 90(2):227\u2013244", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Back to the future: Learning shape models from 3d CAD data", "author": ["M. Stark", "M. Goesele", "B. Schiele"], "venue": "BMVC, pp. 1\u201311", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains", "author": ["B. Sun", "K. Saenko"], "venue": "BMVC", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["D. V\u00e1zquez", "A.M. L\u00f3pez"], "venue": "Mar\u0131\u0301n, D. Ponsa, and D. G. Gomez. Virtual and real world adaptationfor pedestrian detection. IEEE Trans. Pattern Anal. Mach. Intell., 36(4):797\u2013809", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "CoRR, abs/1311.2901", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "One particularly important example is synthetic or semi-synthetic imagery, which may come in abundance and fully labeled, but which inevitably look different from real data [13, 20, 23, 21].", "startOffset": 173, "endOffset": 189}, {"referenceID": 19, "context": "One particularly important example is synthetic or semi-synthetic imagery, which may come in abundance and fully labeled, but which inevitably look different from real data [13, 20, 23, 21].", "startOffset": 173, "endOffset": 189}, {"referenceID": 21, "context": "One particularly important example is synthetic or semi-synthetic imagery, which may come in abundance and fully labeled, but which inevitably look different from real data [13, 20, 23, 21].", "startOffset": 173, "endOffset": 189}, {"referenceID": 20, "context": "One particularly important example is synthetic or semi-synthetic imagery, which may come in abundance and fully labeled, but which inevitably look different from real data [13, 20, 23, 21].", "startOffset": 173, "endOffset": 189}, {"referenceID": 11, "context": "Below, we detail the proposed approach to domain adaptation in deep architectures, and present prelimenary results on traditional deep learning datasets (such as MNIST [12] and SVHN [14]) that clearly demonstrate the unsupervised domain adaptation ability of the proposed method.", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "Below, we detail the proposed approach to domain adaptation in deep architectures, and present prelimenary results on traditional deep learning datasets (such as MNIST [12] and SVHN [14]) that clearly demonstrate the unsupervised domain adaptation ability of the proposed method.", "startOffset": 182, "endOffset": 186}, {"referenceID": 2, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 87, "endOffset": 97}, {"referenceID": 10, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 87, "endOffset": 97}, {"referenceID": 6, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 87, "endOffset": 97}, {"referenceID": 15, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 214, "endOffset": 225}, {"referenceID": 9, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 214, "endOffset": 225}, {"referenceID": 1, "context": "Some approaches perform this by reweighing or selecting samples from the source domain [3, 11, 7], while others seek an explicit feature space transformation that would map source distribution into the target ones [16, 10, 2].", "startOffset": 214, "endOffset": 225}, {"referenceID": 2, "context": "Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space [3, 11], whereas [8, 5] map the principal axes associated with each of the distributions.", "startOffset": 100, "endOffset": 107}, {"referenceID": 10, "context": "Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space [3, 11], whereas [8, 5] map the principal axes associated with each of the distributions.", "startOffset": 100, "endOffset": 107}, {"referenceID": 7, "context": "Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space [3, 11], whereas [8, 5] map the principal axes associated with each of the distributions.", "startOffset": 117, "endOffset": 123}, {"referenceID": 4, "context": "Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space [3, 11], whereas [8, 5] map the principal axes associated with each of the distributions.", "startOffset": 117, "endOffset": 123}, {"referenceID": 9, "context": "Several approaches perform gradual transition from the source to the target domain [10, 8] by a gradual change of the training distribution.", "startOffset": 83, "endOffset": 90}, {"referenceID": 7, "context": "Several approaches perform gradual transition from the source to the target domain [10, 8] by a gradual change of the training distribution.", "startOffset": 83, "endOffset": 90}, {"referenceID": 16, "context": "Among these methods, [17] does this in a \u201cdeep\u201d way by the layerwise training of a sequence of deep autoencoders, while gradually replacing source-domain samples with target-domain samples.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "This improves over a similar approach of [6] that simply trains a single deep autoencoder for both domains.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "In contrast to [6, 17], our approach performs feature learning, domain adaptation and classifier learning jointly, in a unified architecture, and using a single learning algorithm (backpropagation).", "startOffset": 15, "endOffset": 22}, {"referenceID": 16, "context": "In contrast to [6, 17], our approach performs feature learning, domain adaptation and classifier learning jointly, in a unified architecture, and using a single learning algorithm (backpropagation).", "startOffset": 15, "endOffset": 22}, {"referenceID": 22, "context": "In the context of deep feed-forward architectures, such data can be used to \u201cfine-tune\u201d the network trained on the source domain [24, 15, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 14, "context": "In the context of deep feed-forward architectures, such data can be used to \u201cfine-tune\u201d the network trained on the source domain [24, 15, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 0, "context": "In the context of deep feed-forward architectures, such data can be used to \u201cfine-tune\u201d the network trained on the source domain [24, 15, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 8, "context": "The work that is most related to ours is the recent (and concurrent) technical report [9] on adversarial networks.", "startOffset": 86, "endOffset": 89}, {"referenceID": 17, "context": "Under the covariate shift assumption, this would make the label prediction accuracy on the target domain to be the same as on the source domain [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "The simple learning procedure outlined above can be rederived/generalized along the lines suggested in [9] (see Appendix).", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "SA [5] .", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "For each of the two DA methods (ours and [5]) we show how much of the gap between the lower and the upper bounds was covered (in brackets).", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "For all five cases, our approach outperforms [5] considerably, and covers a big portion of the gap.", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "The first four datasets are the well-known MNIST dataset [12] and its modifications.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "The last two datasets is the well-known Street View House Number (SVHN) dataset [14] and a new synthetic dataset Syn.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "In addition, we compare our approach against the recently proposed unsupervised DA method based on subspace alignment (SA) [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 11, "context": "The chosen architectures are fairly standard for these datasets in terms of feature extractor and label predictor parts: the \u201cMNIST\u201d design is inspired by the classical LeNet-5 [12], while the second CNN is adopted from [19].", "startOffset": 177, "endOffset": 181}, {"referenceID": 18, "context": "The chosen architectures are fairly standard for these datasets in terms of feature extractor and label predictor parts: the \u201cMNIST\u201d design is inspired by the classical LeNet-5 [12], while the second CNN is adopted from [19].", "startOffset": 220, "endOffset": 224}, {"referenceID": 18, "context": "Following [19] we also use dropout and `2-norm restriction when we train the SVHN architecture.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "For the SA baseline, we consider the activations of the last hidden layer in the label predictor (before the final linear classifier) as descriptors/features, and learn the mapping between the source and the target domains [5].", "startOffset": 223, "endOffset": 226}, {"referenceID": 3, "context": "Since the SA baseline requires to train a new classifier after adapting the features, and in order to put all the compared methods on an equal footing, we retrain the last layer of the label predictor using a standard linear SVM [4] for all four compared methods (including ours; the performance on the target domain remains approximately the same after the retraining).", "startOffset": 229, "endOffset": 232}, {"referenceID": 4, "context": "The performance of the unsupervised DA method [5] for all three datasets is much more modest, thus highlighting the difficulty of the adaptation task.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "In contrast, [5] does not result in any significant improvement in the classification accuracy, thus highlighting that the adaptation task is even more challenging than in the case of MNIST experiments.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in a similar vein to [6, 17], effectively using [6, 17] as an initialization to our method.", "startOffset": 153, "endOffset": 160}, {"referenceID": 16, "context": "For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in a similar vein to [6, 17], effectively using [6, 17] as an initialization to our method.", "startOffset": 153, "endOffset": 160}, {"referenceID": 5, "context": "For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in a similar vein to [6, 17], effectively using [6, 17] as an initialization to our method.", "startOffset": 180, "endOffset": 187}, {"referenceID": 16, "context": "For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in a similar vein to [6, 17], effectively using [6, 17] as an initialization to our method.", "startOffset": 180, "endOffset": 187}], "year": 2014, "abstractText": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of \u201cdeep\u201d features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall the whole approach can be implemented with little effort using any of the deep-learning packages.", "creator": "LaTeX with hyperref package"}}}