{"id": "1409.8498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2014", "title": "Non-Myopic Learning in Repeated Stochastic Games", "abstract": "This paper is about learning in repetitive stochastic games (RSGs) played against unknown partners. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) balances, making attempts to solve them by means of balance analysis and rationality assumptions completely inadequate. Therefore, the existing learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or behavior of the partners. In this paper, we suggest and evaluate the notion of expert abstraction of the game (Gabe) for general two-player RSGs. Gabe reduces an RSG to a multi-arm bandit problem that can then be solved with the help of an expert algorithm. Gabe maintains many aspects of the original game, including safety and pareto of optimal Nash balances. We show that Gabe clearly outperforms existing algorithms in many scenarios.", "histories": [["v1", "Tue, 30 Sep 2014 11:46:29 GMT  (91kb,D)", "http://arxiv.org/abs/1409.8498v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["jacob w crandall"], "accepted": false, "id": "1409.8498"}, "pdf": {"name": "1409.8498.pdf", "metadata": {"source": "CRF", "title": "Non-Myopic Learning in Repeated Stochastic Games", "authors": ["Jacob W. Crandall"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many scenarios require computers, robots, and other intelligent devices to repeatedly interact with people and other machines. In these interactions, associates\u2019 behaviors are often not known a priori. For example, in the context of Ad Hoc team coordination (Stone et al. 2010), a machine must be able to effectively interact with arbitrary teammates without prior coordination. These teammates are created by other stakeholders, and may not share all the same objectives or run the same algorithms. Such interactions occur in new-age power systems, electronic commerce, and other domains.\nMany repeated interactions can be formulated as repeated stochastic games (RSGs). Because such interactions are finite, a successful player must learn effective strategies within short time scales. Unfortunately, this is extremely challenging. Existing learning algorithms for this domain either require thousands of interactions to learn (Claus and Boutilier 1998; Crandall and Goodrich 2011) or make narrow assumptions about associates\u2019 behaviors (and, thus, learn ineffective strategies when these assumptions fail).\nThese deficiencies are due to a number of characteristics of the domain. RSGs are punctuated by large strategy spaces, even for games with relatively few states. Furthermore, the strategies of associates are unknown a priori, and can change over time (when they also learn). Since\nthese games typically have multiple (often infinite) equilibria, associates\u2019 future behaviors can be very difficult to infer or learn \u2013 reliance on so-called rationality and equilibrium computation is insufficient.\nGame abstraction has emerged in recent years as an effective methodology for dealing with games having large state and action spaces (Gilpin and Sandholm 2006; Schnizlein, Bowling, and Szafron 2009; Ganzfried, Sandholm, and Waugh 2012; Sandholm and Singh 2012). In most approaches, the game is abstracted to a smaller game. An equilibrium strategy is then computed for this smaller game, but is then executed in the original game. While this abstraction methodology is effective in large zero-sum, extensive-form games, it does little to help an algorithm deal with multiple equilibria and adaptive associates.\nWe introduce a new game-abstraction method called game abstraction by experts (Gabe). Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm such as Exp3 (Auer et al. 1995), UCB (Auer, Cesa-Bianchi, and Fischer 2002), or EEE (de Farias and Megiddo 2004). It does this by computing a finite set of expert strategies or learning rules, which are followed based on the selections made by the expert algorithm. The effectiveness of the resulting algorithm depends on both the set of expert strategies and the ability of the expert algorithm to select the best experts for the given scenario.\nIn this paper, we describe a general set of experts for twoplayer general-sum RSGs. While this set of experts effectively reduces the set of strategies that a player can play, it maintains important characteristics of the original game, such as its security level and various Pareto optimal Nash equilibria (NEs). We demonstrate that, when combined with an effective expert algorithm, Gabe outperforms many other algorithms in three two-player RSGs played against a variety of both non-learning and learning associates. In short, Gabe provides a mechanism for agents to quickly learn profitable non-myopic strategies that are otherwise difficult to learn."}, {"heading": "2 Notation", "text": "We consider two-player RSGs played by players i and \u2212i. An RSG consists of a set of stage games (or states) S. In each state s \u2208 S, both players choose an action from a finite set. Let A(s) = Ai(s)\u00d7A\u2212i(s) be the set of joint actions available in s, where Ai(s) and A\u2212i(s) are the action sets of players i\nar X\niv :1\n40 9.\n84 98\nv1 [\ncs .G\nT ]\n3 0\nSe p\n20 14\nand \u2212i, respectively. Each episode (or round) of an RSG begins in the start state s\u0302\u2208 S and terminates when some goal state sg \u2208G\u2286 S is reached. Once a goal state is encountered, a new episode begins in state s\u0302.\nWhen joint action a = (ai,a\u2212i) is played in state s, each players receive the finite rewards ri(s,a) and r\u2212i(s,a), respectively. The world also transitions to some new state s\u2032 with probability defined by PM(s,a,s\u2032). We assume that the transition model PM and the reward functions ri(s,a) and r\u2212i(s,a) are known to both players before the game begins, and that the players can observe each other\u2019s actions."}, {"heading": "3 Example RSG", "text": "For illustrative purposes, we consider a Microgrid Scenario (Figure 1) involving two players. These players interact for an unspecified number of days, each seeking to maximize their own utility over this time. To gain utility, a player executes its tasks within the specified time windows (Figure 1ab). These tasks require the specified electricity loads. A task can be executed no more than once a day, and must be executed within a single time period.\nThe players share a limited electricity supply, which has the per-hour generation characteristics shown in Figure 1(c). We assume that generation occurs at the beginning of each hour. Additionally, the system automatically stores up to five units of unused electricity for later use. For simplicity, we assume that all storage is lost at the end of each day.\nIf the players attempt to consume more electricity in an hour than is available, a blackout occurs. In a blackout, electricity storage empties, and the tasks that the players attempted to execute are left unfinished (as if they had not been initiated). A cost of two utility units is incurred to bring the microgrid back online, which is evenly distributed among the players that attempted to execute tasks in that hour.\nIn this RSG, the state of the world is defined by (1) the current time (hour), (2) the amount of stored electricity, and (3) the set of currently active tasks (i.e., unexecuted tasks whose time windows correspond to the current hour). In total, this game has 2,033 unique states. The beginning state is the state of the system in hour t = 0. The goal states consist of all states with time t = 24. Each player\u2019s action set for each state is the set of all subsets of its active tasks.\nThere is not sufficient electricity generated in a day for all tasks to be executed. Furthermore, some high-value tasks can only be executed when the other player complies. For example, there is only sufficient electricity for Player 2 to execute Task 12 if Player 1 refrains from executing Task 1 prior to time t = 6. Similarly, Player 1 can only successfully execute Task 10 if Player 2 refrains from Task 20. The players must coordinate and compromise to be successful.\nOur observations are that existing learning algorithms are unable to learn effective strategies in this RSG, especially when they are paired with each other. For example, model-free reinforcement learning (RL; e.g., Q-learning (Watkins and Dayan 1992)) is unable to learn effective strategies within a reasonable number of days due to the game\u2019s large state-joint action space. Existing algorithms that learn at faster time scales still fail to learn collaborative\nTask Time Load UtilityID window (units) 1 [0,8) 2.0 7.0 2 [5,8) 2.0 1.5 3 [8,12) 3.6 0.8 4 [10,11) 2.4 1.6 5 [11,13) 3.9 2.7 6 [14,17) 3.8 1.4 7 [17,18) 3.6 2.9 8 [18,21) 1.2 1.5 9 [18,23) 1.5 2.4 10 [23,24) 5.0 20.2\n(a) Player 1\u2019s tasks\nTask Time Load UtilityID window (units) 11 [0,3) 1.5 2.0 12 [4,6) 5.0 22.2 13 [7,8) 1.5 0.9 14 [9,13) 1.3 1.4 15 [11,15) 0.7 2.4 16 [13,17) 4.5 2.6 17 [15,18) 2.7 1.7 18 [17,18) 5.0 1.6 19 [18,22) 2.8 1.5 20 [22,23) 4.0 5.7\n(b) Player 2\u2019s tasks\nsolutions. For example, when both players utilize modelbased RL (MBRL) or counterfactual regret (CFR, an effective algorithm for the poker domain (Zinkevich et al. 2007; Johanson et al. 2012)), they fail to learn a collaborative solution in self play. As a result, as shown in Figure 1(d), their average daily utilities are far below those obtained by FolkEgal (de Cote and Littman 2008). On the other hand, FolkEgal does not perform effectively when associating with algorithms that do not meet its assumptions (see Table 1(a)).\nBecause RSGs are punctuated with large state-action spaces, multiple (often infinite) equilibria, and associates with unknown (potentially changing) strategies, it is infeasible to learn non-myopic solutions using algorithms that use locally optimal computations and assumptions. While such methods might be sufficient if all consequences of the repeated interactions could be fully represented, the stateaction spaces of such representations are prohibitively large. Novel game-abstraction methods are needed. In this paper, we study game abstraction by experts (Gabe)."}, {"heading": "4 Game Abstraction By Experts (Gabe)", "text": "Gabe (summarized in Algorithm 1) reduces the strategy space of an RSG to a finite set \u03a6 of strategies or algorithms. Each \u03c6 \u2208\u03a6 defines a policy for all states s \u2208 S. In this way, Gabe converts an RSG into a multi-armed bandit problem, where each arm is an expert \u03c6 \u2208\u03a6. Thus, rather than learning a separate policy for each state s\u2208 S, the agent must learn which high-level strategies or algorithms \u03c6 \u2208\u03a6 are the most profitable, and then follow these strategies.\nGabe has similarities to the work of Elidrisi et al. (2014), in which the RSG is reduced to a normal-form game by identifying common sequences of actions (or paths) for each player. These paths are then used as the actions of a normalform game. Identifying these actions relies on effective exploration strategies and clustering and thresholding algorithms. Gabe instead defines \u03a6 to reduce the RSG to a multiarmed bandit problem. In so doing, it maintains important\nAlgorithm 1 Game abstraction by experts (Gabe). Input: An expert algorithm A Initialize: Compute a set \u03a6 of experts Run: A on the set \u03a6\nIn each round t - Select select an expert \u03c6t \u2208\u03a6 using A - Follow the strategy prescribed by \u03c6t throughout round t - Update each \u03c6 \u2208\u03a6 and A as specified\nattributes of the original RSG. Algorithm 1 requires that we solve two technical problems. First, we must identify an expert algorithm A that learns effectively in repeated interactions with unknown associates. A common goal for such algorithms is to learn to play nearly as well as the best expert would have performed had it always been followed. This problem has been well-studied in the literature (Auer et al. 1995; Bowling 2004; Arora, Dekel, and Tewari 2012; Crandall 2014; Cesa-Bianchi, Dekel, and Shamir 2013).\nSecond, we must define the set of experts \u03a6. Ideally, in any scenario (i.e., RSG and associate) that a player is likely to encounter, at least one \u03c6 \u2208\u03a6 should play near-optimally. However, no single expert need be effective in all situations.\nGiven that expert algorithms have been well-studied, we focus in the next section on identifying and computing a general set of experts \u03a6 for two-player general-sum RSGs."}, {"heading": "5 Experts", "text": "Our set \u03a6 for two-player RSGs consists of three genres of algorithms. The first two genres, leader and follower strategies, were defined by Littman and Stone (2001). We introduce the third genre, call preventative strategies, in this paper. These three genres define appropriate responses to many algorithms that associates are likely to use.\nLeader Strategies A leader strategy tries to encourage its associate to follow a target solution (Littman and Stone 2005). The leader plays its own portion of the target solution as long as its associate plays its part. However, when the associate deviates from this solution, the leader retaliates in subsequent moves to ensure that the associate does not profit from the deviation.\nTo derive an effective set of leader strategies, we must do three things. First, we must efficiently compute desirable target solutions. Second, we must select a set of target solutions, one solution corresponding to each leader strategy we place in \u03a6. Third, we must determine how to punish deviations from the target solution.\nComputing Target Solutions Possible target solutions of an RSG can be computed by solving a Markov decision process (MDP). This MDP is defined by PM , the RSG\u2019s jointaction space, and a payoff function that is a convex combination of the players\u2019 rewards (de Cote and Littman 2008). That is, for \u03c9 \u2208 [0,1], the payoff function is given by\n\u03c3\u03c9(s,a) = \u03c9ri(s,a)+(1\u2212\u03c9)r\u2212i(s,a). (1)\nThen, the value for taking joint-action a in state s is\nQ\u03c9(s,a) = \u03c3\u03c9(s,a)+ \u2211 s\u2032\u2208S PM(s,a,s\u2032)V \u03c9(s\u2032), (2)\nwhere V \u03c9(s\u2032) = maxa\u2208A(s) Q\u03c9(s\u2032,a), which can be solved in polynomial time using linear programming (Papadimitriou and Tsitsiklis 1987; Littman, Dean, and Kaelbling 1995).\nLet MDP(\u03c9) denote the joint strategy produced by solving the MDP for a particular \u03c9 . Also, let V \u03c9i (s) be player i\u2019s expected future payoff from state s when MDP(\u03c9) is followed. Then, the ordered pair ( V \u03c9i (s\u0302),V \u03c9 \u2212i(s\u0302) ) is the joint payoff vector for the target solution defined by MDP(\u03c9). This payoff vector is Pareto optimal.\nBy varying \u03c9 , we can compute a variety of possible target solutions. For example, Table 1 gives the solutions computed for the Microgrid Scenario using this method. We call these solutions pure solutions. Additional possible target solutions can be obtained by alternating between solutions produced using different \u03c9 . We call these solutions alternating solutions. For example, the players could play MDP(0.1) in odd-numbered rounds, and MDP(0.3) in even-numbered rounds. This would produce the average joint payoff vector of (24.05,36.35) in the Microgrid Scenario. In this paper, we only include alternating cycles of length 2, since longer cycles are likely to be difficult to reinforce using a leader strategy, and hence will likely be difficult to agree upon.\nWhich Target Solutions? Figure 2 shows the joint payoff vectors of possible pure and alternating target solutions in the Microgrid Scenario. As with many other RSGs, the oneshot NEs (one shown) are Pareto dominated by several possible target solutions. Furthermore, the possible target solutions in which each player\u2019s payoff exceeds its maximin strategy can be sustained as NEs of the repeated game (Gintis 2000). As such, these possible target solutions offer a variety of potentially desirable equilibrium solutions, each differing in the value provided to each player.\nIn practice, more experts makes the task of finding the best expert more difficult for the expert algorithm. Thus, we form leader experts for only up to five of the possible target solutions we have identified. First, we select the solution that maximizes the minimum payoff between the players (i.e., the egalitarian solution). Next, we select the two points that give each player the highest payoff while providing the other player with at least its security level. The last two points are chosen to maximize the Euclidean distance from the other selected payoff vectors. For the Microgrid Scenario, our algorithm selects the target solutions shown in red in Figure 2.\nAdding punishment When the associate deviates from the target solution, the leader plays an attack strategy (usually its minimax strategy (Littman and Stone 2005)) in subsequent moves of the game. In practice, we deviate slightly from this protocol: player i only punishes deviations by player\u2212i that substantially lower i\u2019s payoffs. Formally, let s\u03c4 denote the state of the \u03c4th move of the round. Then, player i begins punishing \u2212i when its action deviates from the target solution and the following two conditions hold:\n(1) V \u03c9i (s\u03c4)+ r t i(\u03c4\u22121)<V \u03c9i (s\u03c4\u22121), (3) (2) \u03c4\n\u2211 j=1 rti( j)+V \u03c9 i (s\u03c4)< \u03b1 t i , (4)\nwhere rti( j) is i\u2019s payoff after the jth move of round t. Also, \u03b1 ti = \u03bb\u03b1 t\u22121 i +(1\u2212\u03bb )Rti , \u03bb \u2208 (0,1), and Rti is i\u2019s total payoff in round t. \u03b10i is set to i\u2019s payoff in the egalitarian solution. When player\u2212i deviates and conditions (1-2) hold, i punishes \u2212i for the remainder of round t, and continues to do so in subsequent rounds until \u2212i\u2019s payoffs are at least \u03b4 less than they would have been had \u2212i not deviated. We chose this punishment mechanism because RSGs are inherently noisy in practice. The associate\u2019s payoffs are often uncertain, and the reward and transition functions of RSGs can be non-deterministic. In such situations, being more lenient helps avoid cycles of unnecessary punishment.\nSince the attack strategy can also be computed in polynomial time, a leader strategy can be built in polynomial time.\nFollower Strategies We also include follower strategies in \u03a6. Rather than attempt to drive play toward a particular solution, followers seek to maximize their payoffs against the strategy they attribute to their associate. We use followers that estimate their associate\u2019s strategy in three different ways. The first set of followers assume their associate is playing a leader strategy. Against such associates, a player maximizes its payoffs by\nplaying its part of the corresponding target solution. Thus, we form a separate follower strategy for each of the selected target solutions mentioned earlier, each of which plays its part of its target solution unconditionally.\nThe set \u03a6 also includes two other follower strategies: its maximin strategy, which is a best response to an associate seeking to minimize its payoffs, and MBRL, which plays a best response to stationary associates it can model. MBRL estimates its associate\u2019s strategy using the Fictitious-play assessment (Fudenberg and Levine 1998). That is, in state s, i assumes that \u2212i plays action a\u2212i proportionally to the frequency it has played it in past visits to s. We denote this probability as \u03b3\u2212i(s,a\u2212i).\nPreventative Strategies Due to their representations of the world, some learning associates have difficulty perceiving the punishment signals communicated by a leader strategy. As such, they do not learn to play the target solution intended by the leader. In such cases, preventative strategies can sometimes be more effective. Rather than punishing past deviations, preventative strategies seek to make deviations unprofitable in first place. They do this by anticipating profitable deviations the associate might make, and then acting in advance so that such deviations are unprofitable to the associate.\nWe include one preventative algorithm in \u03a6, which we call Bouncer. Bouncer seeks to minimize the difference between the players\u2019 payoffs, without further regard for its own payoffs. Formally, Bouncer computes both Qi(s,a) and Q\u2212i(s,a) using SARSA (Rummery and Niranjan 1994), where the estimated strategies for the players are given by the Fictitious-play assessments \u03b3i(s,ai) and \u03b3\u2212i(s,a\u2212i), respectively. It then selects actions as follows:\na\u2217i (s) = min ai\u2208Ai(s) \u2211 a\u2212i\u2208A\u2212i(s) \u03b3\u2212i(s,a\u2212i)U(s,(ai,a\u2212i)), (5)\nwhere U(s,a) = |Qi(s,a)\u2212Q\u2212i(s,a)|."}, {"heading": "6 Properties of the Abstraction", "text": "The set \u03a6 as defined in Section 5 is an abstraction of the strategy space of the original RSG. This abstraction limits the set of strategies available to a player. However, it preserves the follow three attributes of the original RSG. Property 1 (\u03b5-Pareto optimal NEs) \u03a6 contains leader strategies that seek to play \u03b5-Pareto optimal NEs of the repeated game. These same NEs exist in the original RSG. Property 2 (Security) Because \u03a6 contains the maximin strategy, it maintains the original RSG\u2019s security level. Property 3 (Best response) \u03a6 contains MBRL, which learns a best response (in the original game) to the play of stationary associates having the same state representation.\nThus, \u03a6 preserves important attribute of the original RSG, attributes that correspond to previously stated goals for learning in games (Powers and Shoham 2005). In the next section, we show that this simplified strategy space allows an expert algorithm to learn non-myopic strategies in many scenarios, which helps it to outperform existing algorithms."}, {"heading": "7 Empirical Performance", "text": "We combined the expert algorithms Exp3 (Auer et al. 1995) and S++ (Crandall 2014) with the set \u03a6 to form two new algorithms: Gabe-Exp3 and Gabe-S++. Exp3 is a commonly used expert algorithm, while S++ is a recently developed algorithm that has demonstrated high empirical performance in repeated normal-form games. We evaluated these algorithms against ten associates in three separate games.\nAssociates We evaluated Gabe-Exp3 and Gabe-S++ when paired with both non-learning and learning algorithms. The non-learning algorithms were Coop, Bully, FolkEgal (de Cote and Littman 2008), Maximin, Bouncer, and CFRNE. Coop plays the strategy that maximizes its associate\u2019s payoffs. Bully is identical to FolkEgal except that it seeks to enforce the target solution that maximizes its own payoffs subject to the other player receiving at least its security level. CFR-NE computes an \u03b5-NE prior to the game using counterfactual regret in simulated self play (Zinkevich et al. 2007; Johanson et al. 2012). The learning algorithms were MBRL, CFR, Gabe-Exp3, and Gabe-S++. CFR is identical to CFRNE except that it also updates its strategy after each round based on its associate\u2019s observed actions.\nGames We evaluated the algorithms in three RSGs: the Microgrid Scenario, the SGPD (Figure 3(a)), and a block game. These games require the players to make and accept different kinds of compromises to be successful. The SGPD (fully explained by Goodrich, Crandall, and Stimpson, 2003) is a prisoners\u2019 dilemma in maze form with 26,896 states. A player receives 30 points when it moves to the other player\u2019s start position, and loses one point every time it attempts a move. A successful player in this RSG must effectively balance cooperation and defection.\nThe Block Game is a turn-taking game in which the two players share a set of blocks (Figure 3(b)). In each round, the players take turns selecting blocks (the oldest player always goes first). A round ends once both players have three blocks. If a player\u2019s blocks form a valid set (i.e., all blocks of the same color, all blocks of the same shape, or none of the blocks have the same color or shape), her payoff is the sum of the numbers on her blocks. Otherwise, her payoff is the sum of the numbers divided by 4. The sub-game perfect one-shot NEs of this game give each player 18 points. However, these solutions are Pareto dominated by the solution in which the players alternate between taking all of the squares and all of the triangles (which gives each player an average\npayoff of 25). Even better, a player could potentially bully the other by always insisting on taking all of the squares.\nResults Figure 4 provides a summary of the average empirical performance of the four learning algorithms in each game over the first 500 rounds. Table 2 shows the results of each pairing in each game in 365-round games. Averaged over all pairings, Gabe-S++ was the highest performer in each game. The order of the other players varied from game to game. The consistently high performance of Gabe-S++ can be traced to the two components of Gabe: the strategy abstraction \u03a6 and the quality of the expert algorithm A .\nFirst, the strategy abstraction \u03a6 allows Gabe-S++ to (1) make profitable compromises with associates that are apt to compromise, (2) exploit naive associates, and (3) avoid being exploited by antagonistic associates. For example, FolkEgal, Gabe-S++, and Bouncer (in the SGPD only) are apt to play the egalitarian solution in each game. Gabe-S++ is able to learn to play these solutions against these associates (and, hence, performs well) because at least one of the leader and follower strategies in \u03a6 play these compromises. \u03a6 also contains Bouncer, which allows it to learn mutual cooperation against MBRL in the SGPD in some trials. Meanwhile, the follower strategies in \u03a6 permit Gabe-S++ to exploit Coop and to avoid being perpetually exploited by Bully, CFR, and CFR-NE in each game. Thus, \u03a6 contains a set of strategies that allow it to perform effectively against each of these ten associates in each game.\nMBRL and CFR, on the other hand, cannot effectively represent the egalitarian solution, or other effective nonmyopic compromises. Thus, while they can exploit naive algorithms (such as Coop) and also avoid being exploited, they do not typically perform well against associates that are apt to cooperate in these games. This is because they cannot represent non-myopic strategies in general-sum RSGs.\nSecond, Gabe-S++ performs well because of its expert algorithm. While Gabe-Exp3 used the same set of expert strategies \u03a6 as Gabe-S++, it did not perform nearly as well in any of these three games. Despite sometimes sharing theoretical properties (such as no-regret), both the short- and long-term empirical performance of expert algorithms can vary substantially in repeated interactions (Crandall 2014). Thus, both the selection of the expert algorithm and the strategy set \u03a6 are important components of Gabe."}, {"heading": "8 Conclusion and Discussion", "text": "In this paper, we have proposed game abstractions by experts (Gabe) for two player, repeated stochastic games (RSGs). Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many important characteristics of the original game, including security, best response, and Pareto optimality. Furthermore, we demonstrated empirically that, given an effective expert algorithm, Gabe outperforms existing learning algorithms in a number of general-sum RSGs.\nGabe differs from previous game-abstraction methods (Gilpin and Sandholm 2006; Schnizlein, Bowling, and Szafron 2009; Ganzfried, Sandholm, and Waugh 2012; Sandholm and Singh 2012). Whereas previous methods seek\nto reduce the number of states and actions in the game (to make equilibrium computation feasible), Gabe abstracts the high-level strategies of the game. This allows it to represent non-myopic strategies. Since these abstraction methods have different purposes, Gabe can be used simultaneously with previous game-abstraction methods where necessary."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "This paper addresses learning in repeated stochastic games (RSGs) played against unknown associates. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) equilibria, making attempts to solve them via equilibrium analysis and rationality assumptions wholly insufficient. As such, previous learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or associates\u2019 behaviors. In this paper, we propose and evaluate the notion of game abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an RSG to a multiarmed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many aspects of the original game, including security and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially outperforms existing algorithms in many scenarios.", "creator": "LaTeX with hyperref package"}}}