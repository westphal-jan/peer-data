{"id": "1410.0083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2014", "title": "Integrating active sensing into reactive synthesis with temporal logic constraints under partial observations", "abstract": "In the case of incomplete information about the dynamic environment, reactive controller synthesis amounts to solving a two-player game of partial observations that is practically mathematically complex. In order to reduce the high computing load, online rescheduling through sensing actions avoids solving the strategy in the reactive system under partial observations. Instead, we solve only for a strategy that ensures that a predetermined temporal logic can be fulfilled if the system has complete observations of its environment. Such a strategy is then transformed into one in which control decisions are made based on the observed sequence of states (of the interacting system and its environment). If the system encounters a belief - a set of all possible hypotheses that the system has for the current state - for which the observation-based strategy is undefined, a sequence of sensory actions is triggered, selected by an active sensitization strategy - a set of all possible hypotheses that the system has for the present observation.", "histories": [["v1", "Wed, 1 Oct 2014 01:05:01 GMT  (189kb,D)", "http://arxiv.org/abs/1410.0083v1", "7 pages, 2 figures, submitted to American Control Conference 2015"]], "COMMENTS": "7 pages, 2 figures, submitted to American Control Conference 2015", "reviews": [], "SUBJECTS": "cs.SY cs.AI cs.RO", "authors": ["jie fu", "ufuk topcu"], "accepted": false, "id": "1410.0083"}, "pdf": {"name": "1410.0083.pdf", "metadata": {"source": "CRF", "title": "Integrating active sensing into reactive synthesis with temporal logic constraints under partial observations", "authors": ["Jie Fu", "Ufuk Topcu"], "emails": ["utopcu@seas.upenn.edu."], "sections": [{"heading": null, "text": "Keywords: Reactive synthesis; Active sensing; Partial observation; Temporal logic.\nI. INTRODUCTION Control synthesis under partial observations has been an important topic since complete and precise information (about the system and environment states) during the execution of a controller is often not available in practice. However, synthesis methods for systems under partial observations are of high complexity and have limitations in their applications. With incomplete information, the problem of synthesizing a controller in a partially observable Markov decision process (POMDP) has been shown to be PSPACEcomplete, even for finite planning horizons [9]. When the control specification is given in temporal logic and the environment is dynamic and possibly adversarial, the interaction between a system and its environment can be captured in a two-player partially observable game with infinite stages, for which the qualititive-analysis problem under finite-memory strategies is EXPTIME-complete [3].\nFor temporal logic constraints, synthesis algorithms for stochastic systems modeled as POMDPs have been studied\nThis work is supported by AFOSR grant number FA9550-12-1-0302, ONR grant number N000141310778 and NSF CNS award number 1446479.\n1Jie Fu and Ufuk Topcu are with the Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, 19104, USA jief, utopcu@seas.upenn.edu.\nin [11], [12]. To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers. A sure-winning controller ensures the satisfaction of a specification whereas an almost-sure winning controller is a randomized strategy and ensures satisfaction with probability 1. These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].\nAn interesting question that has not been investigated much is the following: Since the high computational complexity is caused by incomplete information, is it possible to reduce the computational effort and still ensure correctness of the control design by acquiring new information at run time? In this paper, we give a method that provides a partial, affirmative answer to this question. Particularly, we study a system with actions to obtain information, referred to as sensing actions, and show how to utilize these actions in a way that a given linear temporal logic (LTL) specification is satisfied almost surely with reduced computational effort.\nThe new approach in this paper is inspired by [10], where the authors propose a method of online planning with partial observations and sensing actions as a way to overcome such complexity since the system only needs to compute a strategy for a finite number of steps, and replans with new information obtained through sensing actions. For temporal logic specifications, online planning method in [10] has no correctness guarantee. We propose a similar framework of active sensing and reactive synthesis under temporal logic constraints. The basic approach is the following: During control execution, the system maintains a belief, which is a set of states it thinks the current state must be in based on its partial observation for the game history. The belief is updated under two cases: In one of these cases, the system or the environment makes a move, the belief is updated to the set of states possibly arrived at as a result of move. Alternatively, the system can activate a sensor, detecting the value of some propositional formula and revises its belief according to the additional information obtained through sensing. In the second case, the system applies an active sensing strategy. A sequence of sensor queries are made to obtain the most useful information for reducing the system\u2019s uncertainty in the current state. The benefit of performing the combined active sensing and reactive planning is that we can indeed avoid solving a two-player zero-sum game with partial observations. Rather, we transform the sure-winning ar X iv :1 41 0.\n00 83\nv1 [\ncs .S\nY ]\n1 O\nct 2\n01 4\nstrategy for the system in the same game with perfect observations, into a randomized, belief-based strategy. By construction, the randomized strategy may not be defined for every belief the system can encounter at run time. During control execution, the system alternates between the randomized strategy and the active sensing strategy. We prove that if the set of available sensors meets a sufficient condition, the temporal logic specification can be satisfied with probability 1, i.e., almost surely.\nThe rest of the paper is organized as follows. We begin with some preliminaries and the formulation of the problem in section II. Section III presents the main results on synthesizing provably correct, online reactive controllers with sensing actions for temporal logic constraints. In Section IV we illustrate the method using a robot motion planning example in a partially observed environment."}, {"heading": "II. PROBLEM FORMULATION AND PRELIMINARIES", "text": "A probability distribution on a finite set S is a function D : S \u2192 [0, 1] such that \u2211 s\u2208S D(s) = 1. The set of probability distributions on a finite set S is denoted D(S). The support of D is the set Supp(D) = {s \u2208 S | D(s) > 0}. Let \u03a3 be a finite alphabet. \u03a3\u2217, \u03a3\u03c9 , and \u03a3+ are sets of strings over \u03a3 with finite length, infinite length, and length greater than or equal 1, respectively. Given u and v in \u03a3\u2217, uv is the concatenation of u with v. A string u \u2208 \u03a3\u2217 is a prefix of w \u2208 \u03a3\u2217 (or w \u2208 \u03a3\u03c9) if there exists v \u2208 \u03a3\u2217 (or v \u2208 \u03a3\u03c9) such that w = uv. For a string w, the set of symbols occurring infinitely often in w is denoted Inf(w). The last symbol in a finite string w is denoted Last(w)."}, {"heading": "A. Game, specification and strategies", "text": "Through abstraction for systems with continuous and discrete dynamics, the interaction of a system and its dynamic environment can be captured by a labeled finite-state transition system [7], [8]:\nM = \u3008S,\u03a3, \u03b4, s0,AP, L\u3009\nwhere 1) S = S1 \u222a S2 is the set of states. At each state in S1, the system takes an action. At each state in S2, the environment takes an action. 2) \u03a3 = \u03a31 \u222a \u03a32 is the set of actions. \u03a31 is the set of actions for the system, and \u03a32 is the set of actions for the environment. 3) s0 is the initial state. 4) \u03b4 : S\u00d7\u03a3\u2192 S is the transition function. 5) L : S \u2192 2AP is the labeling function that maps a state s \u2208 S to a set of atomic propositions L(s) \u2286 AP that evaluate true at s.\nWe use a fragment of LTL [1] to specify the desired system properties such as safety, reachability, liveness and stability. Given a temporal logic formula \u03d5 in this class, one can always represent it by a deterministic Bu\u0308chi automaton (DBA) A\u03d5 = \u3008H, 2AP , \u03b4\u03d5, h0, F\u03d5\u3009 where H is the set of states, 2AP is the set of alphabet, \u03b4\u03d5 : H \u00d7 2AP \u2192 H is the transition function. h0 is the initial state and F\u03d5 is the set of final states. A word w = a0a1 . . . \u2208 (2AP)\u03c9 induces a state sequence h0h1 . . . \u2208 H\u03c9 where hi+1 = \u03b4\u03d5(hi, ai), for all i \u2265 0. A word w is accepted in A\u03d5 if and only if the\nstate sequence \u03c1 \u2208 H\u03c9 induced from w visits some states in F\u03d5 infinitely often.\nA product operation is applied to incorporate the temporal logic specification into the labeled transition system, giving rise to a two-player turn-based Bu\u0308chi game between the system (player 1) and its environment (player 2):\nG = \u3008Q,\u03a3, T, q0, F \u3009 = M nA\u03d5\nwhere the components are defined as follows. \u2022 Q = Q1 \u222aQ2 is the set of states, where Q1 = S1\u00d7H\nand Q2 = S2 \u00d7H . \u2022 T : Q \u00d7 \u03a3 \u2192 Q is the transition function. Given\n(s, h) \u2208 Q, \u03c3 \u2208 \u03a3, if \u03b4(s, \u03c3) = s\u2032, then T (q, \u03c3) = q\u2032 where q\u2032 = (s\u2032, \u03b4\u03d5(h, L(s\u2032))). \u2022 q0 = (s0, \u03b4\u03d5(h0, L(s0))) is the initial state. \u2022 F \u2286 Q \u00d7 F\u03d5 is a subset of states that determines a\nBu\u0308chi winning condition. A play in G is either a finite sequence of interleaving states and actions \u03c1 = q0a0q1a1 . . . qn \u2208 (Q \u222a \u03a3)\u2217Q or an infinite sequence \u03c1 = q0a0q1a1 . . . \u2208 (Q\u222a\u03a3)\u03c9 such that q0 is the initial state and T (qi, ai) = qi+1 for all i \u2265 0. If \u03c1 is finite, the last element of \u03c1 is a state, denoted Last(\u03c1). An infinite play \u03c1 is winning for player 1 in G if and only if Inf(\u03c1) \u2229 F 6= \u2205.\nIn game G, each state in Q is associated with a truth assignment to a set P of predicates. Note that P may not equal AP . This association is captured by the interpretation function \u03c0 such that for any q \u2208 Q, for any predicate p \u2208 P , \u03c0(q)(p) \u2208 {true, false}. We write \u03c0(q) = \u2227p\u2208P`p where `p = p if \u03c0(q)(p) = true and `p = \u00acp if \u03c0(q)(p) = false, \u2227, \u00ac are the logical connectives for conjunction and negation, respectively. In the set P , there is a predicate t indicating whose turn it is to play: If t = 1, then the system takes an action, otherwise the environment makes a move. It is assumed that the value of t is globally observable, which means, the system always knows whose turn it is to play.\nWe consider the case when the system has partial observation of values for the set P of predicates. Following [4], this partial observation can be defined by an equivalence relation over the set of states, denoted R \u2286 Q\u00d7Q. Two states q and q\u2032 are observation-equivalent, that is, (q, q\u2032) \u2208 R, if both q and q\u2032 provide the same state information observable by the system, i.e., the value of p \u2208 P is observable at q if and only if it is observable at q\u2032, and \u03c0(q)(p) = \u03c0(q\u2032)(p). We denote the observations of states for the system by O \u2286 2Q, which is defined by the observation-equivalence classes. Clearly, O is a partition of the state space. We define an observation function Obs : Q\u222a\u03a3\u2192 O\u222a\u03a31\u222a{\u2212} such that 1) q \u2208 Obs(q); 2) for every q1, q2 \u2208 Obs(q), (q1, q2) \u2208 R, 3) if \u03c3 \u2208 \u03a31 Obs(\u03c3) = \u03c3; and 4) if \u03c3 \u2208 \u03a32, Obs(\u03c3) = \u2212 . The last two properties express that the system observes (knows) which action it performed but does not directly observe the action of the environment. The information received by the system on the environment\u2019s action is from the effect of that action, reflected in the observed arrived state.\nThe observation sequence of a play \u03c1 = q0a0q1 . . . is a sequence Obs(\u03c1) = Obs(q0)Obs(a0)Obs(q1) . . .. It is worth mentioning that two states q = (s, h) and q = (s\u2032, h\u2032) can be observation-equivalent even if h 6= h\u2032. Therefore, two observation-equivalent \u03c1 and \u03c1\u2032 can differ in their state projections onto the set Q of states in the specification automaton A\u03d5.\nLet Pref(G) denote the set of finite prefixes of all plays in G, each of which ends with a state in Q. For both players 1 and 2, a deterministic strategy for player i is a function fi : Pref(G)\u2192 \u03a3i and a randomized strategy is a function fi : Pref(G)\u2192 D(\u03a3i). We say that player i follows strategy fi if for any finite prefix \u03c1 \u2208 Pref(G) at which fi is defined, player i takes the action fi(\u03c1) if fi is deterministic, or an action \u03c3 \u2208 Supp(fi(\u03c1)) with probability fi(\u03c1)(\u03c3) if fi is randomized. Since the system has partial information of the states, it can only execute an observation-based strategy f1, in the sense that if for any two prefixes \u03c1 and \u03c1\u2032 \u2208 Pref(G), if Obs(\u03c1) = Obs(\u03c1\u2032), then f1(\u03c1) = f1(\u03c1\u2032). A strategy is memoryless if and only if fi(\u03c1) = fi(Last(\u03c1)). For Bu\u0308chi game G with complete information, there exists a deterministic, memoryless winning strategy for one of the players."}, {"heading": "B. Partial observation, belief and sensing actions", "text": "With partial observations, the system keeps track of the play in the game by maintaining and updating a set B \u2286 Q of states, referred to as the belief, which is the set of states the system thinks the game can be in, given the observation history. In which follows, we show how the belief is obtained and updated. The set of beliefs in the game is denoted B \u2286 2Q. We define a function \u03b1 : Pref(G)\u2192 B that maps a prefix of g into a belief as follows: given a prefix \u03c1 = q0a0 . . . qn, the belief of the system is \u03b1(\u03c1) = {Last(\u03c1\u2032) \u2208 Q | \u03c1\u2032 \u2208 Pref(G) and Obs(\u03c1\u2032) = Obs(\u03c1)}.\nDuring the interaction with the environment, the system\u2019s belief is updated in two ways: (i) The system applies a control action, obtains a new observation of the arrived state, and updates its belief to one in which the current state could be. (ii) The environment takes some action. The system obtains an observation o \u2208 O of the arrived state, and subsequently updates its belief that includes its hypothesis for the current state. Formally, this process is called belief update, which can be captured by the function\nUpdate : B \u00d7 (\u03a31 \u222a {\u2212})\u00d7O \u2192 B, (1)\nIt is reminded that the symbol \u201c\u2212\u201d is the observation for an action of the environment. Given a belief B, the system takes an action a \u2208 \u03a31 and gets an observation o \u2208 O. Then it updates its belief to B\u2032 = o \u2229Update(B, a, o) = {q\u2032 | \u2203q \u2208 B such that T (q, a) = q\u2032}. If it is the environment\u2019s turn, after the environment takes some action, the system gets an observation o \u2208 O and then updates its current belief B to B\u2032 = Update(B,\u2212, o) = o \u2229 {q\u2032 | \u2203q \u2208 B, \u2203\u03c3 \u2208 \u03a32 such that T (q, \u03c3) = q\u2032}.\nWe distinguish a set \u0393 of sensing actions for the system and explain how the sensing actions affects the system\u2019s belief as follows.\nDefinition 1: Consider the set P of atomic propositions and the set \u0393 of sensing actions. For each sensing action a \u2208 \u0393, there exists at least one propositional formula \u03c6 over P such that after applying the sensing action a, the truth value of \u03c6 is known. Depending on the value of \u03c6, the system can partition a belief B into two subsets, expressed by\nKnows(\u03c6, a,B) := (B\u2032, B \\B\u2032),\nwhere B\u2032 is the set of states in which \u03c6 evaluates true and B \\B\u2032 is the set of states in which \u03c6 evaluates false. Hence, if \u03c6 is true, the belief is revised to be B\u2032, otherwise to be B \\B\u2032. To capture both global and local sensing capabilities, for a given state q, we denote \u0393q \u2286 \u0393 to be a set of sensing actions enabled at q. The set of sensing actions enabled at a belief B \u2286 Q is \u22c2 q\u2208B \u0393q .\nThe following assumption is made for sensing actions. Assumption 1: A sensing action will not change the value of variables and/or predicates in P . The assumption is not restrictive because if an action introduces both physical and epistemic changes, we simply consider it as an ordinary control action and include it into \u03a31. We call an action in \u0393 sensing to emphasize that it provides information of the current state, and an action in \u03a3 physical to emphasize it changes the state of the game. We assume that at each turn of the system, it can either choose a physical action, or several sensing actions followed by a physical action.\nWe solve the following problem in this paper. Problem 1: Given a two-player turn-based Bu\u0308chi game G = \u3008Q,\u03a3, T, q0, F \u3009, and a set \u0393 of sensing actions, design an observation-based strategy f : Q\u2217 \u2192 D(\u03a31) \u222a \u0393\u2217 with which the specification is satisfied with probability 1, i.e., almost surely, whenever such a strategy exists."}, {"heading": "III. MAIN RESULTS", "text": "For games with partial information, algorithms in [5] can be used to synthesize observation-based controllers which ensure given temporal logic specifications are satisfied surely, or almost surely, i.e., with probability 1, whenever such controllers exist. In this paper, we only consider the cases in which observation-based controllers do not exist and thus require additional information at run time for satisfying given temporal logic specifications. We distinguish two phases in the online planning: Progress phase and sensing phase. As the names suggest, during the progress phase, the system takes physical actions in order to satisfy the temporal logic constraints, and during the sensing phase, the system takes sensing actions to reduce the uncertainty in its belief for the current game state. The transition from one phase to another will be explained after we introduce the methods for synthesizing strategies used in both phases."}, {"heading": "A. A belief-based strategy for making progress", "text": "For a game with partial observation, we aim to synthesize a belief-based, memoryless and randomized strategy fP : B \u2192 D(\u03a31) that can be applied for making progress towards satisfying the given LTL fragment formula \u03d5.\nIn the two-player Bu\u0308chi game G, the deterministic surewinning strategy WS : Q \u2192 \u03a31 can be computed (with methods in [6]) but requires complete information to execute at run time. The belief-based strategy fP is constructed from the sure-winning strategy WS in the following way: Let Win1 \u2286 Q be the set of states at which WS are defined. Given B \u2208 B, let\nProgress(B) = \u22c3 q\u2208B WS(q), and\nallow(B) = \u22c2 q\u2208B allow(q),\nwhere allow(q) = {\u03c3 \u2208 \u03a31 | T (q, \u03c3) \u2208Win1}.\nFor each state q \u2208 B, the sure-winning strategy will suggest action WS(q) to be taken by the system, which is then included into a set Progress(B). The set allow(B) is a set of actions with the following property: No matter in which state of B the game is, by taking an action in allow(B), the next state will still be one for which the sure-winning strategy is defined. Then, if Progress(B) \u2286 allow(B), we let fP (B)(\u03c3) = 1|Progress(B)| for each \u03c3 \u2208 Progress(B). Otherwise, fP is undefined for B. Note that since the computation fP can be essentially reduced to computing the interaction of two sets, there is no need to compute fP for all possible subset of Q. Rather, we can efficiently compute fP for each belief B encountered at run time.\nWe have transformed the sure-winning strategy with complete information in the Bu\u0308chi game into a randomized, belief-based strategy. During control execution, the system maintains its current belief. At each turn of the system, after applying an action \u03c3 \u2208 \u03a31 at the state B, the system receives an observation o \u2208 O, updates its belief to B\u2032 = Update(B, \u03c3, o). When it is a move made by the environment, the system obtains another observation o\u2032 \u2208 O, updates its belief to B\u2032\u2032 = Update(B\u2032,\u2212, o\u2032). The system applies fP (B\u2032\u2032) as long as fP is defined for B\u2032\u2032. When fP is undefined for the current belief B, then we switch to the sensing phase for actively acquiring more information to reduce the uncertainty in its current belief."}, {"heading": "B. An active sensing strategy for reducing uncertainty", "text": "During the progress phase with the randomized, beliefbased strategy fP , if the system runs into a belief at which fP is undefined, it needs to update its belief through sensing until either it finds itself in a state for which fP is defined, or it cannot further refine its belief: A belief B cannot be refined if for any sensing action a enabled at B and for any formula \u03c6 such that (B1, B2) = Knows(\u03c6, a,B), it holds that for either i = 1 or i = 2, Bi = B. We represent the process of belief revision with sensing actions as a tree structure, referred to as a belief revision tree, and then\npropose a synthesis method for an active sensing strategy using the belief revision tree.\nGiven a belief Bo \u2208 B, the belief revision tree with the root Bo is a tuple BRTree(Bo) = \u3008N , E\u3009, where N is the set of nodes in the tree, consisting a subset of beliefs, and E \u2286 N \u00d7 \u0393 \u00d7 N is the set of edges. It is constructed as follows.\n1) The root of the tree is Bo. 2) At each node B \u2208 N , for each enabled sensing\naction a \u2208 \u0393B , if there exists a formula \u03c6 such that (B1, B2) = Knows(\u03c6, a,B) and both B1, B2 are not empty, then we add two children B1, B2 of B, and include edges (B, a,B1), (B, a,B2) into the edges E . 3) A node B is a leaf of the tree if and only if either 1) B cannot be further revised by any sensing action, or 2) fP is defined for B.\nThe active sensing strategy fS : B \u2192 \u0393 is computed as follows. First, in the tree BRTree(Bo), we compute a set of target nodes Reach \u2282 N such that a node B\u2032 is included in Reach if and only if fP (B\u2032) is defined. The objective is to apply the least number of sensing actions in order to reach a belief in Reach for which fP is defined. For this purpose, we have the following recursion:\n1) X0 = Reach, i = 0. 2) Xi+1 = Xi \u222a {B \u2208 B | \u2203a \u2208 \u0393, such that \u2200B\u2032 \u2208 B, (B, a,B\u2032) \u2208 E , B\u2032 \u2208 Xi} and let fS(B) = a. In other words, a belief B is included into Xi+1 if there exists a sensing action a such that when a is applied at B, no matter which belief the system might reach, it must be in Xi. 3) Until i is increased to some number m \u2208 N such that Xm+1 = Xm, we output the sensing strategy fS obtained so far.\nWe denote Xm = attr(Reach), following the notion of an attractor of the set Reach. For any state in attr(Reach), there exists a sensing strategy fS such that for whatever outcome resulted by applying sensing actions, the system can arrive at some belief in Reach in finitely many steps by following fS . Furthermore, it can be proven that fS minimizes the number of sensing actions required for the sensing phase under the constraint that the system will not run into a dead end, which is a belief that cannot be further refined yet is undefined by fP . The number of sensing actions during the sensing phase is upper bounded by the index i for which Bo \u2208 Xi and Bo /\u2208 Xi\u22121. The proof follows from the property of attractor [6] and is omitted here.\nRemark: It is worth mentioning that for a given belief B, the active sensing strategy is unique. Thus, we can store and continuously update a set of active sensing strategies synthesized at run time: When the system encounters a belief B for which fP is undefined but it has seen before, it can use the stored active sensing strategy for B without recomputing a new one. For a large-scale system with a large number of sensing actions, one can also pre-compute a library of active sensing strategies and then augment the library with\nnew active sensing strategies computed at run time."}, {"heading": "C. A composite, almost-sure winning strategy", "text": "At run time, the system alternates between strategy fP for making progress and strategy fS for refining its belief. We name the system\u2019s strategy at run time a composite strategy, denoted f : B \u2192 D(\u03a31) \u222a \u0393, defined by,\nf(B) = { fP (B) if fP (B) is defined. fS(B) if fS(B) is defined.\n(2)\nNote that by construction, the domains of fP and fS is always disjoint.\nThe following assumption provides a sufficient condition for avoiding dead-ends at run time.\nAssumption 2: For each state B encountered during the progress phase, if fP (B) is undefined, then fS(B) is defined.\nSince we cannot predict which beliefs the system might have during control execution with online planning, in the extreme case, for each predicate p \u2208 P , we need to have a sensing action or a combination of sensing actions to detect its truth value. However, this condition is not necessary and may include some sensing actions that will never be used at run time. As the system does not need to know the exact state by extensive sensing, it is at the system\u2019s disposal whether to apply a sensing action and what shall be applied.\nNext we prove the correctness of the composite strategy. To this end, we recall some property in the solution for Bu\u0308chi games with complete information from [6]: The winning region of the Bu\u0308chi game G can be partitioned as Win1 = \u22c3m i=0Wi for some m \u2208 N, m \u2265 0. For any state q \u2208Win1, there exists a unique ordinal i such that q \u2208Wi. If q \u2208 Q1\u2229Wi for some 0 < i \u2264 m, then the winning strategy on q outputs \u03c3 \u2208 \u03a31, with which the system reaches a state q\u2032 \u2208 Wi\u22121 \u2229Q2. If i = 0, then with the action WS(q), we arrive at a state q\u2032 \u2208 Win1. If q \u2208 Q2, then for any action \u03c3 \u2208 \u03a32 enabled at q, T (q, \u03c3) \u2208Wi\u22121 if i 6= 0, or q\u2032 \u2208Win1 otherwise.\nLemma 1: Given a game G = \u3008Q,\u03a3, T, q0, F \u3009. Let B0 = Obs(q0) be the initial belief. If Assumption 2 is satisfied and q0 \u2208Win1, the composite strategy f defined by (2) ensures that some states in F of G is infinitely often visited with probability 1.\nProof: Consider an arbitrary belief B \u2208 B for which fP is defined. By definition of fP , for each \u03c3 \u2208 Progress(B), the probability of choosing action \u03c3 is 1u , where u = |Progress(B)|. If the actual state is q and q \u2208 Wi, for some i 6= 0, then with probability 1u , the system will reach a state in Wi\u22121. Thus, the probability of the next state being in Wi\u22121 is 1u \u2265 1 |Q| > 0. For other \u03c3\n\u2032 \u2208 fP (s), \u03c3\u2032 6= WS(q), the next state after taking \u03c3\u2032 is in Wj for some 0 \u2264 j \u2264 m. Let Pr(q,\u2666iW0) denote the probability of reaching W0 from state q in i turns. When system applies the strategy f , it is Pr(q,\u2666iW0) \u2265 ( 1|Q| )\ni > 0 and the probability of not reaching W0 in i turns is less than or equal to 1 \u2212 ( 1|Q| ) i \u2264 1 \u2212 ( 1|Q| ) m+1 = r < 1 where m + 1 is the total number of partitions in Win1. If after\ni steps the state is not in W0, it must be in Wj for some 0 < j \u2264 m, and again the probability of not reaching W0 in m steps is less than or equal to r. Therefore, under the policy f , the probability eventually reaching W0 from any state q \u2208 Win1 is Pr(v,\u2666W0) = limk\u2192\u221e Pr(v,\u2666kW0) = limk\u2192\u221e(1 \u2212 Pr(v,\u00ac\u2666kW0)) = limk\u2192\u221e(1 \u2212 rk/m) = 1\u2212 limk\u2192\u221e rk/m = 1.\nOnce entering W0, the system will take an action to remain in Win1, and the above reasoning applies again. In this way, in the absence of dead ends (Assumption 2), the system can revisit the set W0 of states with probability 1 by following the composite strategy f . Since W0 \u2286 F , the probability of system always eventually visiting some states in F is 1.\nTo conclude this section, Algorithm 1 describes the procedure of online planning with sensing actions."}, {"heading": "IV. EXAMPLES", "text": "We apply the algorithm to a robotic motion planning example, which is a variant of the so-called \u201cWumpus game\u201d in a 7\u00d7 7 gridworld. Figure 2 consists of one mobile robot, one monster called \u201cWumpus\u201d. The robot is capable of moving in eight compass directions with actions \u2018N\u2019, \u2018S\u2019 , \u2018E\u2019, \u2018W\u2019, \u2018NE\u2019, \u2018NW\u2019, \u2018SE\u2019, \u2018SW\u2019 (horizontally, vertically and diagonally), one step at a time. The robot and the Wumpus does not move concurrently. The Wumpus can move in four compass directions with actions \u2018N\u2019, \u2018S\u2019, \u2018E\u2019 and \u2018W\u2019 within a restricted area Region and emits stench to its surrounding cells. The objective of the robot is to infinitely revisit region R1, R2, and R3 in this order, while avoiding running into\nthe Wumpus. Formally, the temporal logic formula is \u03d5 = \u2666(xr, yr) = R1 \u2227 \u2666 ((xr, yr) = R2 \u2227 \u2666(xr, yr) = R3) \u2227 \u00ac(xr = xw \u2227 yr = yw) where (xr, yr), (xw, yw) are the positions of the robot and the Wumpus, respectively. Yet, the robot only knows his own position. For this case of partial observation, without the inclusion of sensing actions, it can be shown that with the algorithms in [5], observation-based, sure-winning strategies and almost-sure winning strategies do not exist.\nHere, we introduce a set of sensing actions to the game. For the robot to know the position of the moving obstacles, it needs to apply a sensing action \u2014 smell(x, y) to detect if there exists stench at cell (x, y). Thus, when the robot applies smell(x, y), if the result is True, then the Wumpus must be some cells in the set S = {(x\u2032, y\u2032) | x\u2032 \u2264 x+1, y\u2032 \u2264 y+1, x\u2032, y\u2032 \u2208 N}\u2229Region. Otherwise, it is not possible that the Wumpus is in any cell in S.\nWe illustrate how the robot updates his belief using sensing action smell(x, y) where (x, y) is a cell in the gridworld. Suppose that the robot does not know where the Wumpus is and hypothesizes it can be in any cell in the Region. Once it applies the sensing action (2, 2), since the cell has stench and the sensor returns True. Then, immediately the robot will know the Wumpus is in one of the cells in the set S = {(1, 1), (2, 1), (3, 1), (1, 2), (2, 2), (3, 2), (3, 1), (3, 2), (3, 3)}, because only if the Wumpus is in a cell of S, there can be stench in cell (2, 2).\nFrom the numerical experimental result, after 1000 steps (a step includes either a robot\u2019s (sensing or physical) action or a movement of the Wumpus), the robot visited the set F in the formulated two-player game G 14 times and can continue to visit F infinite often. In Figure 3 we show the belief updates by applying alternatively the exploitation strategy and active sensing strategy for the initial 100 steps. It is observed that the maximum cardinality of the belief set is 43 over the control execution, which means that the robot thinks the Wumpus can be in any cell in its restricted region. However, if there is no danger of running into the Wumpus in a few next steps, there is no need to exercising any sensing action. The implementations are in Python on a desktop with Intel(R) Core(TM) i5 processor and 16 GB of\nmemory. The average time for the robot making a decision is 8.55 \u00d7 10\u22124 seconds. The computation of the product game took 40.14 seconds and the winning strategy under complete information is computed within 14 seconds."}, {"heading": "V. CONCLUSIONS", "text": "Our work shows that when additional information can be obtained through sensing actions, one can transform a sure-winning strategy with complete information to a beliefbased, randomized strategy, which is then combined, at run time, with an active sensing strategy to ensure a given temporal logic specification is satisfied with probability 1. The synthesis method avoids a subset construction for solving games with partial information. Meanwhile, the active sensing strategy leads to a cost-efficient way of sensor design: Although we require a sufficient set of sensing actions to avoid dead-ends at run time, the system minimizes the usage of sensing actions by asking the most revealing queries, depending on what specification is to be satisfied, and how much uncertainty the system has about the game state at run time. In future work, we will consider more examples for practical robotic motion planning under partial observations. It is also important to consider the uncertainty in the sensors. For example, a sensor query might return a probabilistic distribution over a set of states, rather than a\nbinary answer to proposition logical formulae considered herein. For this extension, we are currently investigating modifications that need to be made to account for delays, uncertainty in the information provided by the sensors."}], "references": [{"title": "Deterministic generators and games for LTL fragments", "author": ["Rajeev Alur", "Salvatore La Torre"], "venue": "ACM Transactions on Computational Logic,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Games for synthesis of controllers with partial observation", "author": ["A Arnold", "A Vincent", "I Walukiewicz"], "venue": "Theoretical Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "The complexity of partialobservation parity games. In Logic for Programming, Artificial Intelligence, and Reasoning, pages", "author": ["Krishnendu Chatterjee", "Laurent Doyen"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Partial-Observation Stochastic Games: How to Win When Belief Fails", "author": ["Krishnendu Chatterjee", "Laurent Doyen"], "venue": "Annual IEEE Symposium on Logic in Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Algorithms for omega-regular games with imperfect information", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A Henzinger", "Jean-Fran\u00e7ois Raskin"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Automata Logics, and Infinite Games: A Guide to Current Research", "author": ["Erich Gr\u00e4del", "Wolfgang Thomas", "Thomas Wilke", "editors"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A fully automated framework for control of linear systems from temporal logic specifications", "author": ["M. Kloetzer", "C. Belta"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Correct, reactive robot control from abstraction and temporal logic specifications", "author": ["Hadas Kress-Gazit", "Tichakorn Wongpiromsarn", "Ufuk Topcu"], "venue": "IEEE Robotics and Automation Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Algorithms for sequential decision making", "author": ["Michael Lederman Littman"], "venue": "PhD thesis, Brown University,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Replanning in domains with partial information and sensing actions", "author": ["Guy Shani", "Ronen I Brafman"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2026}, {"title": "Formal methods for control synthesis in partially observed environments : application to autonomous robotic manipulation", "author": ["Rangoli Sharan"], "venue": "Dissertation (Ph.D.), California Institute of Technology. PhD thesis, California Institute of Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Control of probabilistic systems under dynamic, partially known environments with temporal logic specifications", "author": ["Tichakorn Wongpiromsarn", "Emilio Frazzoli"], "venue": "In Proceedings of the 51th IEEE Conference on Decision and Control,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "complete, even for finite planning horizons [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "which the qualititive-analysis problem under finite-memory strategies is EXPTIME-complete [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "in [11], [12].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "in [11], [12].", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers.", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers.", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "The new approach in this paper is inspired by [10], where the authors propose a method of online planning with partial", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "For temporal logic specifications, online planning method in [10] has no correctness guarantee.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "A probability distribution on a finite set S is a function D : S \u2192 [0, 1] such that \u2211 s\u2208S D(s) = 1.", "startOffset": 67, "endOffset": 73}, {"referenceID": 6, "context": "namic environment can be captured by a labeled finite-state transition system [7], [8]:", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "namic environment can be captured by a labeled finite-state transition system [7], [8]:", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "We use a fragment of LTL [1] to specify the desired system properties such as safety, reachability, liveness and stability.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "Following [4], this", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "For games with partial information, algorithms in [5] can be used to synthesize observation-based controllers which ensure given temporal logic specifications are satisfied surely, or almost surely, i.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "In the two-player B\u00fcchi game G, the deterministic surewinning strategy WS : Q \u2192 \u03a31 can be computed (with methods in [6]) but requires complete information to execute at run time.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "The proof follows from the property of attractor [6] and is omitted here.", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "To this end, we recall some property in the solution for B\u00fcchi games with complete information from [6]: The winning region of the B\u00fcchi game G can be partitioned as Win1 = \u22c3m i=0Wi for some m \u2208 N, m \u2265 0.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "For this case of partial observation, without the inclusion of sensing actions, it can be shown that with the algorithms in [5], observation-based, sure-winning strategies and almost-sure winning strategies do not exist.", "startOffset": 124, "endOffset": 127}], "year": 2014, "abstractText": "We introduce the notion of online reactive planning with sensing actions for systems with temporal logic constraints in partially observable and dynamic environments. With incomplete information on the dynamic environment, reactive controller synthesis amounts to solving a two-player game with partial observations, which has impractically computational complexity. To alleviate the high computational burden, online replanning via sensing actions avoids solving the strategy in the reactive system under partial observations. Instead, we only solve for a strategy that ensures a given temporal logic specification can be satisfied had the system have complete observations of its environment. Such a strategy is then transformed into one which makes control decisions based on the observed sequence of states (of the interacting system and its environment). When the system encounters a belief\u2014a set including all possible hypotheses the system has for the current state\u2014for which the observation-based strategy is undefined, a sequence of sensing actions are triggered, chosen by an active sensing strategy, to reduce the uncertainty in the system\u2019s belief. We show that by alternating between the observation-based strategy and the active sensing strategy, under a mild technical assumption of the set of sensors in the system, the given temporal logic specification can be satisfied with probability 1.", "creator": "LaTeX with hyperref package"}}}