{"id": "1410.3935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling", "abstract": "Conditional Random Fields (CRFs) are usually specified by graphical models, but in this paper we propose to use probabilistic logic programs and to specify generative. Our intention is firstly to offer a unified approach for CRFs to complex modeling through the use of a Turing overall language, and secondly to offer a convenient method for realizing generative-discriminatory pairs in machine learning in order to compare generative and discriminatory models and choose the best model. We implemented our approach as a D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while using its dynamic programming mechanism for efficient probability compression. We tested D-PRISM with logistic regression, a linear chain CRF and a CRF-CFG, and empirically confirmed their excellent discriminatory performance in comparison to their CFG counterparts, NCF-CRF-CRF-CRF-CRF-CRF-CRF and NCF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRFs are also a new, NCG and NCG-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRFs.", "histories": [["v1", "Wed, 15 Oct 2014 06:01:03 GMT  (148kb)", "http://arxiv.org/abs/1410.3935v1", "20 pages, 2 figures"]], "COMMENTS": "20 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["taisuke sato", "keiichi kubota", "yoshitaka kameya"], "accepted": false, "id": "1410.3935"}, "pdf": {"name": "1410.3935.pdf", "metadata": {"source": "CRF", "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling", "authors": ["Taisuke Sato"], "emails": ["sato@mi.cs.titech.ac.jp", "kubota@mi.cs.titech.ac.jp", "ykameya@meijo-u.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n39 35\nv1 [\ncs .L\nG ]\n1 5\nO ct\nKeywords CRFs \u00b7 D-PRISM \u00b7 logic-based\nTaisuke Sato Tokyo Institute of Technology, Japan Tel.: +81-3-5743-2186 E-mail: sato@mi.cs.titech.ac.jp\nKeiichi Kubota Tokyo Institute of Technology, Japan Tel.: +81-3-5743-2186 E-mail: kubota@mi.cs.titech.ac.jp\nYoshitaka Kameya Meijo University, Japan Tel. : +81-52-838-2567 E-mail: ykameya@meijo-u.ac.jp"}, {"heading": "1 Introduction", "text": "Conditional random fields (CRFs) [11] are probabilistic models for discriminative modeling defining a conditional distribution p(y | x) over output y given input x. They are quite popular for labeling sequence data such as text data and biological sequences [26]. Although they are usually specified by graphical models, we here propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs [19] in machine learning to compare generative and discriminative models and choose the best model.\nThe use of logical expressions to specify CRFs is not new but they have been used solely as feature functions [8,21]. For example in Markov logic networks (MLNs)[21], weighted clauses are used as feature functions to define (conditional) Markov random fields and probabilities are obtained by Gibbs sampling. In contrast, our approach is implemented by a generative modeling language PRISM [22,23] where clauses have no weights; they simply constitute a logic program DB computing possible output y from input x by proving a top-goal Gx,y that relates x to y. In addition probabilities are exactly computed by dynamic programming. DB however contains special atoms of the form msw(i, v) having weights exp(\u03bbi,v) where i and v are arbitrary terms. They are called msw atoms here as in PRISM. We define the weight q(x, y) of a top-goal Gx,y as a sum-product of such weights associated with msw atoms appearing in a proof of Gx,y and consider q(x, y) as an unnormalized distribution. By modifying the dynamic programming mechanism of PRISM slightly, we can efficiently compute, when possible and feasible, the unnormalized marginal distribution q(x) = \u2211\ny q(x, y) and obtain a\nCRF p(y | x) = q(x, y)/q(x). We implemented our idea by modifying PRISM and termed the resulting language D-PRISM (discriminative PRISM). D-PRISM is a general programming language that generatively defines CRFs and provides built-in predicates for parameter learning and Viterbi inference of CRFs.\nOur approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.e. we can write arbitrary CRFs as long as they are described by D-PRISM. We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26]. Furthermore by dynamic programming, probabilistic inference can be efficiently carried out with the same time complexity as their generative counterparts as exemplified by linear-chain CRFs and hidden Markov models (HMMs).\nIn machine learning it is well-known that naive Bayes and logistic regression form a generative-discriminative pair [19]. That is, any conditional distribution p(y | x) computed from a joint distribution p(x, y) = p(x | y)p(y) defined generatively by naive Bayes, where y is a class and x is a feature vector, can also be defined directly by logistic regression and vice versa. As is empirically demonstrated in [19], classification accuracy by discriminative models such as logistic regression is generally better than their corresponding generative models such as naive Bayes when there is enough data but generative models reach their best performance more quickly than discriminative ones w.r.t. the amount of available\ndata. Also the theoretical analysis in [12] suggests that when a model is wrong in generative modeling, the deterioration of prediction accuracy is more severe than in discriminative modeling. It seems therefore reasonable to say \u201c...For any particular data set, it is impossible to predict in advance whether a generative or a discriminative model will perform better\u201d [26]. Hence what is desirable is to provide a modeling environment in which the user can test both types of modeling smoothly without pain and D-PRISM provides such an environment that makes it easy to test and compare discriminative modeling and generative modeling for the same class or related family of probabilistic models.\nIn what follows, we review CRFs in Section 2 and also review three basic models, i.e. logistic regression, linear-chain CRFs and CRF-CFGs in Section 3. We then introduce D-PRISM in Section 4. We empirically verify the effectiveness of our approach in Section 5 using the three models. Section 6 introduces new CRF models, CRF-BNCs and CRF-LCGs, both easily implementable in D-PRISM. In Section 7, we discuss program transformation which derives a program for incomplete data from one for complete data. Section 8 contains related work and discussion and Section 9 is the conclusion."}, {"heading": "2 Conditional random fields", "text": "Conditional random fields (CRFs) [11] are popular probabilistic models defining a conditional distribution p(y | x) over the output sequence y given an input sequence x which takes the following form1:\np(y | x) \u2261 1\nZ(x) exp\n{ K \u2211\nk=1\n\u03bbkfk(x,y)\n}\n.\nHere fk(x,y) and \u03bbk (1 \u2264 k \u2264 K) are respectively a real valued function (feature function) and the associated weight (parameter) and Z(x) a normalizing constant. As Z(x) is the sum of exponentially many terms, the exact computation is generally intractable and takes O(M |y|) time where M is the maximum number of possible values for each component of y and hence approximation methods have been developed [26]. However when p(y | x) has recursive structure of specific type as a graphical model like linear-chain CRFs, Z(x) is efficiently computable by dynamic programming.\nNow let D = {(x(1),y(1)), . . . , (x(T ),y(T ))} be a training set. The regularised conditional log-likelihood l(\u03bb | D) of D is given by\nl(\u03bb | D) \u2261\nT \u2211\nt=1\nlog p(y(t) | x(t))\u2212 \u00b5\n2\nK \u2211\nk=1\n\u03bb2k\n=\nT \u2211\nt=1\n{ K \u2211\nk=1\n\u03bbkfk(x (t),y(t))\u2212 logZ(x(t))\n}\n\u2212 \u00b5\n2\nK \u2211\nk=1\n\u03bb2k\n1 Bold italic letters are (values of) random vectors in this paper.\nwhere \u03bb = \u03bb1, . . . , \u03bbK are parameters and \u00b5\n2\nK \u2211\nk=1\n\u03bb2k is a penalty term. Parameters\nare then estimated as the ones that maximize l(\u03bb | D) by Newton\u2019s method or quasi-Newton methods. The gradient required for parameter learning is computed as\n\u2202l(\u03bb | D)\n\u2202\u03bbk =\nT \u2211\nt=1\n{\nfk(x (t),y(t))\u2212 E(fk | x (t))\n}\n\u2212 \u00b5\u03bbk.\nThe problem here is that the expectation E(fk | x (t)) is difficult to compute and hence a variety of approximation methods such as stochastic gradient descent (SDG) [26] have been proposed. However in this paper we focus on cases where exact computation by dynamic programming is possible and use an algorithm that generalizes inside probability computation in probabilistic context free grammars (PCFGs) [15].\nAfter parameter learning, we apply our model to prediction tasks and infer the most-likely output y\u0302 for an input sequence x using\ny\u0302 \u2261 argmaxyp(y | x)\n= argmaxy 1\nZ(x) exp\n{ K \u2211\nk=1\n\u03bbkfk(x,y)\n}\n= argmaxy\nK \u2211\nk=1\n\u03bbkfk(x,y).\nAs naively computing y\u0302 is straightforward but too costly, we again consider only cases where dynamic programming is feasible and apply a variant of the Viterbi algorithm for HMMs.\n3 Basic models\n3.1 Logistic regression\nLogistic regression specifies a conditional distribution p(y | x) over a class variable y given the input x = x1, . . . , xK , a vector of attributes. It assumes log p(y | x) is a linear function of x and given by\np(y | x) \u2261 1\nZ(x) exp\n{\n\u03bby +\nK \u2211\nj=1\n\u03bby,jxj\n}\n.\nWe here confirm that logistic regression is a CRF. Rewrite \u03bby = \u2211\ny\u2032 \u03bby\u20321{y\u2032=y}\nand \u03bby,jxj = \u2211\ny\u2032 \u03bby\u2032,j1{y\u2032=y}xj and substitute them for \u03bby and \u03bby,jxj in the\nabove formula2. We obtain\np(y | x) = 1\nZ(x) exp\n{\n\u2211\ny\u2032\n\u03bby\u20321{y\u2032=y} + \u2211\ny\u2032\nK \u2211\nj=1\n\u03bby\u2032,j1{y\u2032=y}xj\n}\n.\n2 1{y\u2032=y} is a binary function of y taking 1 if y = y \u2032, otherwise 0.\nBy considering 1{y\u2032=y} and 1{y\u2032=y}xj as feature functions (of y and x), we can see logistic regression is a CRF.\n3.2 Linear-chain CRFs\nCRFs [11] are generally intractable and a variety of approximation methods such as sampling and loopy BP have been developed. There is however a tractable subclass called linear-chain CRF s. They are of the following the form:\np(y | x) \u2261 1\nZ(x) exp\n{ K \u2211\nk=1\n\u03bbk\nN \u2211\ni=2\nfk(x, yi, yi\u22121)\n}\nwhere Z(x) is a normalizing constant. They define, as CRFs, a conditional distribution p(y | x) over output sequences y given an input sequence x such that |x| = |y| = N (|x| denotes the length of vector x) but feature functions are restricted to the form f(x, yi, yi\u22121) (y = y1, . . . , yN , 2 \u2264 i \u2264 N) which only refers to two consecutive components in y. Thanks to this local reference restriction exact probability computation is possible for linear-chain CRFs in time linear in the input length |x| by a variant of the forward-backward algorithm for HMMs. Linearchain CRFs are considered as a generalized and undirected version of HMMs which enable us to use far richer feature functions other than transition probabilities and emission probabilities used in HMMs.\n3.3 CRF-CFGs\nPCFGs [15] are a basic class of probabilistic grammars extending CFGs by assigning selection probabilities \u03b8 to production rules. In PCFGs, the probability of a sentence is the sum of probabilities of parse trees and the probability of a parse tree is the product of probabilities associated with production rules used in the tree. PCFGs are generative models and parameters are usually learned by maximum likelihood estimation (MLE). So given parse trees \u03c41, . . . , \u03c4T and the corresponding sentences s1, . . . , sT , parameters are estimated as \u03b8 \u2217 = argmax\u03b8 \u220fT\nt=1 p(\u03c4t, st | \u03b8). Seeking better parsing accuracy, Johnson attempted parameter learning by\nmaximizing conditional likelihood: \u03b8\u2020 = argmax\u03b8 \u220fT\nt=1 p(\u03c4t | st, \u03b8) but found the improvement is not statistically significant [9]. Later Finkel et al. generalized PCFGs to conditional random field context free grammars (CRF-CFGs) [5] where the conditional probability p(\u03c4 | s) of a parse tree \u03c4 given a sentence s is defined by\np(\u03c4 | s) \u2261 1\nZ(s) exp\n{ K \u2211\nk=1\n\u03bbk \u2211\nr\u2208\u03c4\nfk(r, s)\n}\n.\nHere Z(s) is a normalizing constant. \u03bb1, . . . , \u03bbK are parameters and r \u2208 \u03c4 is a CFG rule (possibly enriched with other information) appearing in the parse tree \u03c4 of s and fk(r, s) is a feature function. Finkel et al. conducted learning experiments with a CRF-CFG using the Penn Treebank [16]. They learned parameters from\nparse trees \u03c41, . . . , \u03c4T and the corresponding sentences s1, . . . , sT in the corpus by maximizing conditional likelihood just like [9] but this time they obtained a significant gain in parsing accuracy [5]. Their experiments clearly demonstrate the advantage of extensive use of features and discriminative parameter learning."}, {"heading": "4 D-PRISM", "text": "Having seen basic models of CRFs, we next show how they are uniformly subsumed by a logic-based modeling language PRISM [22,23] with a simple modification of its probability computation. The modified language is termed D-PRISM (discriminative PRISM).\n4.1 PRISM at a glance\nBefore proceeding we quickly review PRISM3. PRISM is a high-level generative modeling language based on Prolog, extended by a rich array of probabilistic builtin predicates for various types of probabilistic inference and parameter learning. Specifically it offers, in addition to MLE by the EM algorithm, Viterbi training (VT), variational Bayes (VB), variational VT (VB-VT) and MCMC for Bayesian inference. PRISM has been applied to music and bioinformatics [25,1,18].\nSyntactically a PRISM program DB is a Prolog program and runs like Prolog. Fig. 1 is an example of PRISM program for naive Bayes. DB is basically a set of definite clauses. The difference from usual Prolog programs is that the clause body may contain special atoms, msw atoms4, of the form msw(i,v) representing a probabilistic choice made by (analogically speaking) rolling a die i and choosing the outcome v. Here i is a ground term naming the msw atom and v belongs to a set Vi of possible outcomes declared by values/2 declaration. The probability of msw(i,v) (v \u2208 Vi) being true is denoted by \u03b8i,v and called a parameter for msw(i,v). Naturally \u2211\nv\u2208Vi \u03b8i,v = 1 holds. Executing msw(i, X) with a variable X\nreturns a value v \u2208 Vi in X with probability \u03b8i,v. So msw(season,S) in Fig. 1 probabilistically returns one of {spring, summer, fall, winter} in S.\n3 http://sato-www.cs.titech.ac.jp/prism/ 4 msw stands for \u201cmulti-valued switch.\u201d\nDB defines a probability measure pDB(\u00b7) over Herbrand interpretations (possible worlds)[23]. The probability pDB(G) of a top-goal G then is computed as a sum-product of parameters in two steps. First G is reduced using DB by SLD search to a disjunction E1\u2228 \u00b7 \u00b7 \u00b7\u2228EM such that each Ej (1 \u2264 j \u2264 M) is a conjunction of msw atoms representing a sequence of probabilistic choices. Ej is called an explanation for G because it explains why G is true or how G is proved as a result of probabilistic choices encoded by Ej . Let \u03c6(G) \u2261 {E1, . . . , EM} be the set of all explanations for G. pDB(G) is computed as pDB(G) = \u2211\nE\u2208\u03c6(G) pDB(E) and\npDB(E) = \u220fN\nk=1 \u03b8k for E = msw1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 mswN , where \u03b8k is a parameter for mswk (1 \u2264 k \u2264 N).\nLet p(x, y) be a joint distribution over input x (or observation) and output y (or hidden state) which we wish to compute by a PRISM program. We write a program DB that probabilistically proves Gx,y, a top-goal that relates x to y, using msw atoms, in such a way that p(x, y) = pDB(Gx,y) holds. Since (x, y) forms complete data, Gx,y has only one explanation Ex,y for Gx,y 5, so we have p(x, y) = pDB(Gx,y) = pDB(Ex,y) = \u220f i,v \u03b8 \u03c3i,v(Ex,y) i,v where \u03c3i,v(Ex,y) is the count of msw(i,v) in Ex,y. Introduce Gx = \u2203y Gx,y. Then the marginal probability p(x) is obtained as pDB(Gx) because p(x) = \u2211\ny p(x, y) =\n\u2211\ny pDB(Gx,y) = pDB(Gx)\nholds. Hence the conditional distribution p(y | x) is computed as\np(y | x) = pDB(Gx,y)\npDB(Gx) =\npDB(Ex,y)\npDB(Gx) =\n\u220f i,v \u03b8 \u03c3i,v(Ex,y) i,v\n\u2211\nEx,y\u2208\u03c6(Gx)\n\u220f i,v \u03b8 \u03c3i,v(Ex,y) i,v\n. (1)\nWe next apply (1) to the naive Bayes programDB0 in Fig. 1.DB0 is intended to infer a season S from temperature T and humidity H and generatively defines a joint distribution p([T,H], S) = pDB0(nb([T, H], S)). To draw a sample from p([T,H], S) or equivalently from pDB0(nb([T,H],S)), it first samples a season S by executing msw(season,S), then similarly samples a value T of temperature and a value H of humidity, each conditioned on S, by executing msw(attr(temp,S),T) and msw(attr(humidity,S),H) in turn6. Note that the program also includes a clause nb([T,H]):-nb([T,H], ) to compute a marginal distribution pDB0(nb([T,H])) (= p([T,H])). The correspondence to (1) is that x = [T,H], y = S, Gx,y = nb([T,H],S) and Gx = nb([T,H]). The conditional distribution p(S | [T,H]) is computed as pDB0(nb([T,H],S))/pDB0(nb([T,H])).\n4.2 From probability to weight\nThe basic idea of our approach to discriminative modeling is to generalize (1) by replacing probability \u03b8i,v for msw(i,v) with arbitrary weight \u03b7i,v = exp(\u03bbi,v). In D-PRISM we further perform normalization to obtain a CRF. More precisely, we first introduce an unnormalized distribution q(x, y) = qDB(Gx,y) defined by:\nqDB(Gx,y) \u2261 exp\n(\n\u2211\ni,v\n\u03bbi,v\u03c3i,v(Ex,y)\n)\nwhere Ex,y is a unique explanation for Gx,y\n5 This is an assumption but generally true with programs for complete data. 6 For example msw(attr(temp,S),T) samples T from the conditional distribution p(T | S).\nassuming that for any complete data (x, y) and the corresponding top-goal Gx,y, our program, DB, always has only one explanation Ex,y. By setting \u03bbi,v = ln \u03b8i,v, qDB(Gx,y) is reduced to pDB(Gx,y) again.\nNext we rewrite (1) as follows by putting p(Ex,y | Gx) \u2261 qDB(Gx,y) \u2211\ny qDB(Gx,y)\nand\nusing \u03b7i,v = exp(\u03bbi,v).\np(Ex,y | Gx) = 1\nZ(Gx) exp\n(\n\u2211\ni,v\n\u03bbi,v\u03c3i,v(Ex,y)\n)\n= 1\nZ(Gx)\n\u220f\ni,v\n\u03b7 \u03c3i,v(Ex,y) i,v (2)\nZ(Gx) = \u2211\nEx,y\u2208\u03c6(Gx)\nexp\n(\n\u2211\ni,v\n\u03bbi,v\u03c3i,v(Ex,y)\n)\n= \u2211\nEx,y\u2208\u03c6(Gx)\n\u220f\ni,v\n\u03b7 \u03c3i,v(Ex,y) i,v (3)\n(2) and (3) are fundamental equations for D-PRISM describing how a CRF p(y | x) = p(Ex,y | Gx) is defined and computed. By comparing (1) to (2) and (3), we notice that the most computationally demanding task in D-PRISM, computing Z(Gx) in (3), can be carried out efficiently by dynamic programming just by replacing probability \u03b8i,v in PRISM with weight \u03b7i,v, resulting in the same time complexity for probability computation as in PRISM.\nIt is also seen from (2) and (3) that in our formulation of CRFs by D-PRISM, \u03c3i,v(Ex,y), the count of msw(i,v) in Ex,y, works as a (default) feature function 7 over the input x and output y. \u03c3i,v(Ex,y) becomes binary when msw(i,v) occurs at most once in Ex,y. For a binary feature function f(x, y) in general, let msw(f(x,y),1) be a dummy msw atom which is unique to f(x, y) and always true. We assume that corresponding to f(x, y), there is a goal f(x,y) provable in PRISM if and only if f(x, y) = 1. Then it is easy to see that a PRISM goal (f(x,y) -> msw(f(x,y),1) ; true) realizes f(x, y).\nFrom the viewpoint of modeling, we emphasize that for the user, D-PRISM programs are just PRISM programs that proves two top-goals, Gx,y for complete data (x, y) and Gx for incomplete data x. For example, the PRISM program in Fig. 1 for naive Bayes is also a D-PRISM program defining logistic regression.\nIn D-PRISM, parameters are learned discriminatively from complete data. Consider the regularised (log) conditional likelihood l(\u03bb | D) of a set of observed data D = {d1, d2, . . . , dT } where dt = (Gx(t) , Ex(t),y(t)) = (Gt, Et) (1 \u2264 t \u2264 T ). l(\u03bb | D) is given by\nl(\u03bb | D) \u2261\nT \u2211\nt=1\nlog p(Et | Gt)\u2212 \u00b5\n2\n\u2211\ni,v\n\u03bb2i,v\n=\nT \u2211\nt=1\n{\n\u2211\ni,v\n\u03bbi,v\u03c3i,v(Et)\u2212 logZ(Gt)\n}\n\u2212 \u00b5\n2\n\u2211\ni,v\n\u03bb2i,v\n7 Since we assume that the top-goal Gx,y has only one explanation Ex,y for a complete data (x, y), (x, y) uniquely determines \u03c3i,v(Ex,y).\nand parameters \u03bb = {\u03bbi,v} are determined as the ones that maximize l(\u03bb | D). Currently we use L-BFGS [13] to maximize l(\u03bb | D). The gradient used in the maximization is computed as\n\u2202l(\u03bb|D)\n\u2202\u03bbi,v =\nT \u2211\nt=1\n{\n\u03c3i,v(Et)\u2212 \u2202\n\u2202\u03bbi,v logZ(Gt)\n}\n\u2212 \u00b5\u03bbi,v\n=\nT \u2211\nt=1\n{\n\u03c3i,v(Et)\u2212 \u2211\nE\u2032\u2208\u03c6(Gt)\n\u03c3i,v(E \u2032)p(E\u2032 | Gt)\n}\n\u2212 \u00b5\u03bbi,v.\nFinally, Viterbi inference, computing the most likely output y for the input x, or the most likely explanation E\u2217x,y for the top-goal Gx, is formulated as (4) in D-PRISM and computed by dynamic programming just like PRISM.\nE\u2217x,y = argmaxEx,y\u2208\u03c6(Gx)p(Ex,y | Gx)\n= argmaxEx,y\u2208\u03c6(Gx) 1\nZ(Gx) exp\n(\n\u2211\ni,v\n\u03bbi,v\u03c3i,v(Ex,y)\n)\n= argmaxEx,y\u2208\u03c6(Gx) \u2211\ni,v\n\u03bbi,v\u03c3i,v(Ex,y) (4)"}, {"heading": "5 Experiments with three basic models", "text": "In this section, we conduct learning experiments with CRFs8. CRFs are encoded by D-PRISM programs while their generative counterparts are encoded by PRISM programs. We compare their accuracy in discriminative tasks. We consider three basic models, logistic regression, a linear-chain CRF and a CRF-CFG, and learn their parameters by L-BFGS.\n5.1 Logistic regression with UCI datasets\nWe select four datasets with no missing data from the UCI Machine Learning Repository [6] and compare prediction accuracy, one by logistic regression written in D-PRISM and the other by a naive Bayes model (NB) written in PRISM.\nWe use the program in Fig. 1 with an appropriate modification of values/2 declarations. The result by ten-fold cross-validation is shown in Table 1 with standard deviation in parentheses. Table 2 contains learning time for each dataset. We can see, except for the zoo dataset, logistic regression by D-PRISM outperforms naive Bayes by PRISM at the cost of considerably increased learning time for larger datasets9.\n8 Experiments in this paper are done on a single machine with Core i7 Quad 2.67GHz\u00d72 CPU and 72GB RAM running OpenSUSE 11.2.\n9 In this paper, accuracies in bold letters indicate that they are the best performance and the difference is statistically significant by t-test at 0.05 significance level. Learning time is an average over five runs.\n5.2 Linear-chain CRF with the Penn Treebank\nWe here compare a linear-chain CRF encoded by a D-PRISM program and an HMM encoded by a PRISM program using sequence data extracted from the Penn Treebank [16]. What we actually do is to write an HMM program in PRISM for complete data and another program for incomplete data and consider their union as a D-PRISM program defining a linear-chain CRF, similarly to the case of naive Bayes and logistic regression. For simplicity we employ default features, i.e. the count of various msw atoms in an explanation.\nFig. 2 is a sample D-PRISM program for a CRF with two states {s0, s1} and two emission symbols {a, b}. hmm0/2 describes complete data and corresponds to\nGx,y in (1) whereas hmm0/1 is for incomplete data and corresponds to Gx in (1) 10. As a CRF program, ground msw atoms such as msw(init,s0), msw(tr(s0),s1) and msw(out(s0,a)) represent binary feature functions over sequences of state transitions and emitted symbols. For example msw(tr(s0),s1) returns 1 (true) if the state transition sequence contains a transition from s0 to s1 else 0 (false).\nWe conduct a comparison of prediction accuracy by a linear-chain CRF and an HMM using the D-PRISM program in Fig. 2. The task is to predict the POS (Part Of Speech) tag sequence (hidden state sequence) for a given sentence (emitted symbol sequence). As learning data, we use two sets of pairs of sentence and POS tag sequence extracted from the Penn Treebank [16]: section-02 in the WSJ (Wall Street Journal articles) corpus referred to here as WSJ02-ALL and its subset referred to as WSJ02-15, consisting of data of length less-than or equal to 15. Their statistics are shown in Table 3.\nTable 4 contains prediction accuracy (%) by eight-fold cross-validation and learning time taken for WSJ02-ALL. Parameters are learned by L-BFGS for DPRISM and by counting for PRISM. The table clearly demonstrates again that we can achieve better prediction performance at the cost of increased learning time; D-PRISM gains 5.94% increase in prediction accuracy for the WSJ02-15 dataset but learning time by L-BFGS in D-PRISM is about 60 times longer than that by counting in PRISM.\n5.3 CRF-CFG with the ATR tree corpus\nWe here deal with probabilistic grammars which graphical models are unable even to represent. We compare the parsing accuracy of a CRF-CFG described by a D-PRISM program and that of a PCFG described by a PRISM program. We do not use features other than the count of a rule in the parsing tree. To save space, we omit programs though they are (almost) identical.\n10 Using \u201chmm0([X0|Xs]):- hmm0([X0|Xs], )\u201d to define hmm0/1 is possible and theoretically correct but would kill the effect of tabling. This problem is discussed in Section 7.\nAs a dataset, we use the ATR tree corpus and its associated CFG [28]. Their statistics are shown in Table 5. After parameter learning by regularised conditional likelihood for the CRF-CFG and by the usual likelihood for the PCFG, we compare their parsing accuracy by ten-fold cross-validation. The task is to predict a parse tree given a sentence and the predicted tree is considered correct when it exactly coincides with the one for the sentence in the ATR tree corpus (exact match). As a reference, we also measure parsing accuracy by PCFG whose parameters are learned from incomplete data, i.e. sentences by the EM algorithm in PRISM.\nTable 6 tells us that when a tree corpus is available, as reported in [5], shifting from PCFG (PRISM) to CRF-CFG (D-PRISM) yields much better prediction performance (and shifting cost is almost zero if we use D-PRISM) but at the same time this shifting incurs almost two orders of magnitude longer learning time."}, {"heading": "6 Exploring new models", "text": "In this section, we demonstrate how the power of D-PRISM is exploited to explore new probabilistic models. We propose two new models. One is CRF-BNCs which are a CRF version of Bayesian networks classifiers. The other is CRF-LCGs which are a CRF version of probabilistic left-corner grammars that generatively formalize probabilistic left-corner parsing. We first introduce CRF-BNCs.\n6.1 CRF-BNCs\nBayesian network classifiers (BNCs) [7,2] are a generalization of naive Bayes classifiers. They use general Bayesian networks (BNs) as a classifier and allow dependencies among attributes unlike naive Bayes classifiers. Although BNCs outperform NBs classifiers in accuracy, they are still generative. We here introduce a CRF version of BNCs, conditional random field BNC s (CRF-BNCs), and empirically show that CRF-BNCs can outperform BNCs. Due to space limitations, we explain CRF-BNCs by an example.\nCRF-BNCs are obtained, roughly speaking, by generalizing conditional probability tables in Bayesian networks to potential functions followed by normalization\nw.r.t. the class variable11. Fig. 3 is an example of Bayesian network for the car dataset in the UCI Machine Learning Repository [6]. It has a class variable C and six attribute variables, B, M, D, P, L and S. We assume they have dependencies designated in Fig. 3.\nImplementing a CRF-BNC for Fig. 3 is easy in D-PRISM. We have only to write a usual generative Bayesian network programDB1 in PRISM shown in Fig. 4 and run it as a D-PRISM program. In Fig. 4, the first clause about bn predicate defines an unnormalized probability qDB1(bn(Attrs)) and the second one defines qDB1(bn(Attrs,C)). So the conditional distribution p(C | Attrs) is computed as qDB1(bn(Attrs,C))/qDB1(bn(Attrs))\nWe conduct a learning experiment similarly to Section 5 to compare the CRFBNC in Fig. 4 and its original BNC and obtain Table 7 for accuracy by ten-fold cross-validation and Table 8 for learning time of each dataset12. Our experiment, though small, strongly suggests that when datasets are large enough, CRF-BNCs can outperform BNCs by a considerable margin at the cost of long learning time.\n11 Since CRF-BNCs preserve the graph structure of Bayesian networks, probabilistic inference by belief propagation can be efficiently carried for both of them with the same time complexity. 12 Due to space limitations Bayesian networks for zoo, kr-vs-kp and nursery are omitted.\n6.2 CRF-LCGs\nA second new model class is CRF-left-corner grammars (CRF-LCGs). CRF-LCGs are a CRF version of probabilistic left-corner grammars (PLCGs) and considered dual to CRF-CFGs [5] in the sense that the former is based on bottom-up parsing, i.e. left-corner parsing [14,29] whereas the latter is based on top-down parsing. Although left-corner parsing is more context-dependent than top-down parsing and accordingly CRF-LCGs are expected to perform better than CRF-CFGs in parsing, no proposal of CRF-LCGs has been made yet to our knowledge.\nRecall that left-corner parsing is procedurally defined through three parsing operations, i.e. shift, attach and project. However it can be defined logically by a pure logic program that describes various relationships among partial parse trees spanning substrings of the input sentence. Let N be a nonterminal and call a partial parse tree with root N N-tree. Fig. 5 is a snapshot of left-corner parsing when the\nB-tree is projected by a CFG rule A -> B C to complete a G-tree where G and A are in the left-corner relation [14,15].\nBy translating the relationships that hold among various partial parse trees in Fig. 5 into a logic program, we obtain a bottom-up parser for probabilistic left-corner grammars as illustrated on the left in Fig. 6. There, for example, lc call(G,B,L,L2) holds true for the parsing configuration described in Fig. 5 (details omitted)[14]. Similarly we write a parsing program in PRISM for complete data (sentence L0 and its tree T) placed on the right in Fig. 6, which is almost isomorphic to the left program. The left and right PRISM programs combined together constitute a D-PRISM program for CRF-LCGs (values declarations that specify CFG rules are not shown).\nFollowing the case of CRF-CFG and PCFG in Section 5, we measure the parsing accuracy by ten-fold cross-validation of CRF-LCG and PLCG for the ATR corpus and the associated CFG using the D-PRISM program in Fig. 6. The result is shown in Table 9. CRF-LCG achieves the highest parsing accuracy compared to PLCG, PCFG and CRF-CFG but again at the cost of long learning time."}, {"heading": "7 Program transformation for incomplete data", "text": "As explained in Section 4, in D-PRISM the user needs to define two top-goals,Gx,y for complete data (x, y) and Gx for incomplete data x, each defining unnormalized distributions qDB(Gx,y) and qDB(Gx) respectively. Then p(y | x) is computed as qDB(Gx,y)\nqDB(Gx) . However sinceGx,y and Gx are logically connected as Gx \u21d4 \u2203yGx,y, it is theoretically enough and correct to add a clause \u201cg(X):-g(X,Y)\u201d to the program for Gx,y to obtain a program for qDB(Gx)\n13. This is what we did for the naive Bayes program in Fig. 1.\nUnfortunately this simple approach does not work in general. The reason is that the search for all explanations for g(X) causes an exhaustive search for proofs of g(X, Y) for all possible values of Y. Consequently when a subgoal occurring in the search process that carries Y is proved with some value Y = a and tabled, i.e. stored in the memory for reuse, it has little chance of being reused later because Y in the subgoal mostly takes different values from a. As a result the effect of tabling is almost nullified, causing an exponential search time for all explanations for g(X).\nTo avoid this negative effect of the redundant argument, Y, we often have to write a specialized program for g(X), independently of a program for g(X, Y), that does not refer to Y; in the case of linear-chain CRF program in Fig. 2, we wrote two programs, one for hmm0(X, Y) (complete data) and the other for hmm0(X) (incomplete data). The latter is a usual HMM program and efficient tabling is possible that guarantees linear time search for all explanations. However, writing a specialized program for g(X) invites another problem of program correctness. When we independently write two programs, DB1 for g(X, Y) and DB2 for g(X), they do not necessarily satisfy qDB1(\u2203Xg(X,Y)) = qDB2(g(X)) which is required for sound computation of the conditional distribution of p(y | x). It is therefore hoped to find a way of obtaining DB2 satisfying this property.\nOne way to achieve this is to use meaning preserving unfold/fold transformation for logic programs [27,20]. It is a system of program transformation containing rules for unfolding and folding operations. Unfolding replaces a goal with the matched body of a clause whose head unifies with the goal and folding is a reverse operation. There are conditions on transformation that must be met to ensure that the transformation is meaning preserving, i.e. the least model of programs is preserved through transformation (see [27,20] for details). Note that meaning preserving unfold/fold program transformation also preserves the set of all explanations for a goal. So if DB2 is obtained from DB1 \u222a {g(X):-g(X,Y)} by such transformation, both programs have the same set of all explanations for g(X), and hence the desired property qDB1(\u2203Xg(X,Y)) = qDB2(g(X)) holds. In addition, usually, DB2 does not refer to the cumbersome Y.\nFig. 7 illustrates a process of such program transformation. It derives an HMM program for hmm0(X) computing incomplete data from a program for hmm0(X,Y) computing complete data using a transformation system described in [27]. It starts with the initial program defining hmm0(X,Y) consisting of {(1), (2), (3)} together with two defining clauses for new predicates, i.e. (4) for hmm0(X) and (5) for hmm1(Y0,Xs). The transformation process begins by unfolding the body goal hmm0(X,Y) in (4) by (1) and folding hmm1(Y0,Xs,Ys) by (5) follows, resulting in (7). The defining clause (5) for hmm1(Y0,Xs) is processed similarly. The final program obtained is {(7), (8), (10)} which coincides with the HMM program for hmm0(X) in Fig. 2.\nThis example exemplifies that unfold/fold transformation has the power of eliminating the redundant argument Y in g(X):- g(X,Y) and deriving a specialized program for g(X) that does not refer to Y and hence is suitable for tabling. However how far this transformation is generally applicable and how far it can be automated is future work."}, {"heading": "8 Discussion and future work", "text": "There are already discriminative modeling languages for CRFs such as Alchemy [10] based on MLNs and Factorie [17] based on factor graphs. To define potential functions and hence models, the former uses weighted clauses whereas the latter uses imperatively defined factor graphs. Both use Markov chain Monte-Carlo (MCMC) for probabilistic inference. D-PRISM differs from them in that although programs are used to define CRFs like MLNs and Factorie, they are purely generative, computing output from input, and probabilities are computed exactly by dynamic programming. TildeCRF [8] learns CRFs over sequences of ground atoms. Potential functions are computed by weighted sums of relational regression trees applied to an input sequence with a fixed size window. TildeCRF is purely discriminative and unlike D-PRISM uses a fixed type of potential function. Also it is designed for linear-chain CRFs and more complex CRFs such as CRF-CFGs are not intended or implemented.\nD-PRISM is interesting from the viewpoint of statistical machine learning in that it builds discriminative models from generative models and offers a general approach to implementing generative-discriminative pairs. This unique feature also makes it relatively easy and smooth to develop new discriminative models from generative models as we demonstrated in Section 6. In addition, as is shown by\nevery experiment in this paper, there is a clear trade-off between accuracy (by discriminative models) and learning time (by generative models), and hence we have to choose which type of model to use, depending on our purpose. D-PRISM assists our choice by providing a unified environment to test both types.\nCompared to PRISM, D-PRISM has no restriction on programs such as the uniqueness condition, exclusiveness condition and independence condition [23]. Consequently non-exclusive or is permitted in a program. Also probability computation is allowed to fail by constraints. For example it is straightforward to add linguistic constraints such as subject-verb agreement to a PCFG by adding an extra argument carrying such agreement information to the program. Although loss of probability mass occurs due to disagreement in the generating process, normalization recovers a distribution and we obtain a constraint CRF-CFG as a result. Of course this freedom is realized at the cost of normalization which may be prohibitive even when dynamic programming is possible. This would happen when adding too many constraints, e.g., agreement in number, gender, tense and so on to a PCFG. Thanks to the removal of restrictive conditions however, D-PRISM is now more amenable to structure learning in ILP than PRISM, which is expected to open up a new line of research of learning CRFs in ILP.\nIn this paper we concentrated on learning from complete data in CRFs and missing value is not considered. When there are missing values, for example when some labels on a sequence in a linear-chain CRF are missing, the data is incomplete and parameter learning becomes much harder, if not impossible. There is a method of parameter learning from incomplete data for conditional distributions using EM. It is developed for PRISM programs with failure [24] and learns parameters from a conditional distribution of the form pDB(Gx | success) where success = \u2203xGx and Gx is a goal for incomplete data x that may fail. The point in [24] is to automatically synthesize failure predicate such that pDB(success) = 1 \u2212 pDB(failure) and rewrite the conditional distribution as an infinite series pDB(Gx | success) = pDB(Gx)(1+pDB(failure)+pDB(failure)\n2+ \u00b7 \u00b7 \u00b7) to which EM is applicable (the FAM algorithm [4]). Although whether the adaptation of this technique to EM learning of CRFs with incomplete data is possible or not is unknown, it seems worth pursuing considering the simplicity of EM compared to complicated gradient-based parameter learning algorithms for incomplete data.\nIn Section 7, the unfold/fold program transformation is used to remove the redundant argument Y from hmm0(X, Y). Y is a non-discriminating argument in the sense of [3]. Christiansen and Gallagher gave a deterministic algorithm to eliminate such non-discriminating arguments without affecting the program\u2019s runtime behavior [3]. Actually deleting non-discriminating arguments from clauses for hmm0(X, Y) in Fig. 2 results in the same HMM program obtained by program transformation. Compared to their approach however, our approach is based on non-deterministic unfold/fold program transformation and allows for an introduction of new predicates. Clarifying the relationship between these two approaches is future work.\nCurrently only binary features or their counts are allowed in D-PRISM. Introducing real-valued features is also a future work and so is a mechanism of parameter\ntying. Finally, D-PRISM is experimentally implemented at the moment and we hope it is part of the PRISM package in the future."}, {"heading": "9 Conclusion", "text": "We have introduced D-PRISM, a logic-based generative language for discriminative modeling. As examples show, D-PRISM programs are just PRISM programs with probabilities replaced by weights. It is the first modeling language to our knowledge that generatively defines CRFs and their extensions to probabilistic grammars. We can freely build logistic regression, linear-chain CRFs, CRF-CFGs or new models generatively with almost the same modeling cost as PRISM while achieving better performance in discriminative tasks."}], "references": [{"title": "Stochastic simulation and modelling of metabolic networks in a machine learning framework", "author": ["M. Biba", "F. Xhafa", "F. Esposito", "S. Ferilli"], "venue": "Simulation Modelling Practice and Theory 19(9), 1957\u20131966", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing bayesian network classifiers", "author": ["J. Cheng", "R. Greiner"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (bUAI\u201999), pp. 101\u2013108", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Non-discriminating arguments and their uses", "author": ["H. Christiansen", "J. Gallagher"], "venue": "Proceedings of the 25th International Conference on Logic Programming (ICLP\u201909), LNCS 5649, pp. 55\u201369", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Parameter estimation in stochastic logic programs", "author": ["J. Cussens"], "venue": "Machine Learning 44(3), 245\u2013271", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["J. Finkel", "A. Kleeman", "C. Manning"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL\u201908), pp. 959\u2013967", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "UCI machine learning repository", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine learning 29(2), 131\u2013163", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "TildeCRF: Conditional random fields for logical sequences", "author": ["B. Gutmann", "K. Kersting"], "venue": "In Proceedings of the 15th European Conference on Machine Learning (ECML-06), pp. 174\u2013185", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint and conditional estimation of tagging and parsing models", "author": ["M. Johnson"], "venue": "Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-01), pp. 322\u2013329", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "The Alchemy system for statistical relational AI", "author": ["S. Kok", "P. Singla", "M. Richardson", "P. Domingos"], "venue": "Technical report, Department of Computer Science and Engineering, University of Washington", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML\u201901), pp. 282\u2013289", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M. Jordan"], "venue": "Proceedings of the 25th international conference on Machine learning (ICML\u201908), pp. 584\u2013591", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming 45, 503\u2013528", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Probabilistic parsing using left corner language models", "author": ["C. Manning"], "venue": "Proceedings of the 5th International Conference on Parsing Technologies (IWPT-97), pp. 147\u2013158. MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["M. Marcus", "B. Santorini", "M. Marcinkiewicz"], "venue": "Computational Linguistics 19, 313\u2013330", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "Advances in Neural Information Processing Systems 22, pp. 1249\u20131257", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Evaluating bacterial gene-finding HMM structures as probabilistic logic programs", "author": ["S. M\u00f8rk", "I. Holmes"], "venue": "Bioinformatics 28(5), 636\u2013642", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "On Discriminative vs", "author": ["A. Ng", "M. Jordan"], "venue": "Generative Classifiers: A comparison of logistic regression and naive Bayes. In: NIPS, pp. 841\u2013848", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Transformation of logic programs: Foundations and techniques", "author": ["A. Pettorossi", "M. Proietti"], "venue": "Journal of Logic Programming 19,20, 261\u2013320", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62, 107\u2013136", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "PRISM: a language for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI\u201997), pp. 1330\u20131335", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Parameter learning of logic programs for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Journal of Artificial Intelligence Research 15, 391\u2013454", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Generative modeling with failure in PRISM", "author": ["T. Sato", "Y. Kameya", "N.F. Zhou"], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI\u201905), pp. 847\u2013852", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic-logical modeling of music", "author": ["J. Sneyers", "J. Vennekens", "D. De Schreye"], "venue": "Proceedings of the 8th International Symposium on Practical Aspects of Declarative Languages (PADL\u201906), vol.3819, LNCS, pp. 60\u201372", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning 4(4), 267\u2013373", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Unfold/fold transformation of logic programs", "author": ["H. Tamaki", "T. Sato"], "venue": "Proceedings of the 2nd International Conference on Logic Programming (ICLP\u201984), Lecture Notes in Computer Science, pp. 127\u2013138. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1984}, {"title": "ATR integrated speech and language database", "author": ["N. Uratani", "T. Takezawa", "H. Matsuo", "C. Morita"], "venue": "Technical Report TR-IT-0056, ATR Interpreting Telecommunications Research Laboratories", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum-likelihood training of the PLCG-based language model", "author": ["D. Van Uytsel", "D. Van Compernolle", "P. Wambacq"], "venue": "Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop 2001 (ASRU\u201901), pp. 210\u2013213", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 10, "context": "Conditional random fields (CRFs) [11] are probabilistic models for discriminative modeling defining a conditional distribution p(y | x) over output y given input x.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "They are quite popular for labeling sequence data such as text data and biological sequences [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs [19] in machine learning to compare generative and discriminative models and choose the best model.", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "The use of logical expressions to specify CRFs is not new but they have been used solely as feature functions [8,21].", "startOffset": 110, "endOffset": 116}, {"referenceID": 20, "context": "The use of logical expressions to specify CRFs is not new but they have been used solely as feature functions [8,21].", "startOffset": 110, "endOffset": 116}, {"referenceID": 20, "context": "For example in Markov logic networks (MLNs)[21], weighted clauses are used as feature functions to define (conditional) Markov random fields and probabilities are obtained by Gibbs sampling.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "In contrast, our approach is implemented by a generative modeling language PRISM [22,23] where clauses have no weights; they simply constitute a logic program DB computing possible output y from input x by proving a top-goal Gx,y that relates x to y.", "startOffset": 81, "endOffset": 88}, {"referenceID": 22, "context": "In contrast, our approach is implemented by a generative modeling language PRISM [22,23] where clauses have no weights; they simply constitute a logic program DB computing possible output y from input x by proving a top-goal Gx,y that relates x to y.", "startOffset": 81, "endOffset": 88}, {"referenceID": 20, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 119, "endOffset": 126}, {"referenceID": 16, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 119, "endOffset": 126}, {"referenceID": 22, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 213, "endOffset": 217}, {"referenceID": 8, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 4, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 25, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 18, "context": "In machine learning it is well-known that naive Bayes and logistic regression form a generative-discriminative pair [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "As is empirically demonstrated in [19], classification accuracy by discriminative models such as logistic regression is generally better than their corresponding generative models such as naive Bayes when there is enough data but generative models reach their best performance more quickly than discriminative ones w.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Also the theoretical analysis in [12] suggests that when a model is wrong in generative modeling, the deterioration of prediction accuracy is more severe than in discriminative modeling.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "For any particular data set, it is impossible to predict in advance whether a generative or a discriminative model will perform better\u201d [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "Conditional random fields (CRFs) [11] are popular probabilistic models defining a conditional distribution p(y | x) over the output sequence y given an input sequence x which takes the following form:", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "As Z(x) is the sum of exponentially many terms, the exact computation is generally intractable and takes O(M ) time where M is the maximum number of possible values for each component of y and hence approximation methods have been developed [26].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "The problem here is that the expectation E(fk | x ) is difficult to compute and hence a variety of approximation methods such as stochastic gradient descent (SDG) [26] have been proposed.", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "However in this paper we focus on cases where exact computation by dynamic programming is possible and use an algorithm that generalizes inside probability computation in probabilistic context free grammars (PCFGs) [15].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "2 Linear-chain CRFs CRFs [11] are generally intractable and a variety of approximation methods such as sampling and loopy BP have been developed.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "3 CRF-CFGs PCFGs [15] are a basic class of probabilistic grammars extending CFGs by assigning selection probabilities \u03b8 to production rules.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "Seeking better parsing accuracy, Johnson attempted parameter learning by maximizing conditional likelihood: \u03b8 = argmax\u03b8 \u220fT t=1 p(\u03c4t | st, \u03b8) but found the improvement is not statistically significant [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "generalized PCFGs to conditional random field context free grammars (CRF-CFGs) [5] where the conditional probability p(\u03c4 | s) of a parse tree \u03c4 given a sentence s is defined by", "startOffset": 79, "endOffset": 82}, {"referenceID": 15, "context": "conducted learning experiments with a CRF-CFG using the Penn Treebank [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": ", sT in the corpus by maximizing conditional likelihood just like [9] but this time they obtained a significant gain in parsing accuracy [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": ", sT in the corpus by maximizing conditional likelihood just like [9] but this time they obtained a significant gain in parsing accuracy [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 21, "context": "Having seen basic models of CRFs, we next show how they are uniformly subsumed by a logic-based modeling language PRISM [22,23] with a simple modification of its probability computation.", "startOffset": 120, "endOffset": 127}, {"referenceID": 22, "context": "Having seen basic models of CRFs, we next show how they are uniformly subsumed by a logic-based modeling language PRISM [22,23] with a simple modification of its probability computation.", "startOffset": 120, "endOffset": 127}, {"referenceID": 24, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 0, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 17, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 22, "context": "DB defines a probability measure pDB(\u00b7) over Herbrand interpretations (possible worlds)[23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Currently we use L-BFGS [13] to maximize l(\u03bb | D).", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "1 Logistic regression with UCI datasets We select four datasets with no missing data from the UCI Machine Learning Repository [6] and compare prediction accuracy, one by logistic regression written in D-PRISM and the other by a naive Bayes model (NB) written in PRISM.", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "We here compare a linear-chain CRF encoded by a D-PRISM program and an HMM encoded by a PRISM program using sequence data extracted from the Penn Treebank [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "As learning data, we use two sets of pairs of sentence and POS tag sequence extracted from the Penn Treebank [16]: section-02 in the WSJ (Wall Street Journal articles) corpus referred to here as WSJ02-ALL and its subset referred to as WSJ02-15, consisting of data of length less-than or equal to 15.", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "As a dataset, we use the ATR tree corpus and its associated CFG [28].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Table 6 tells us that when a tree corpus is available, as reported in [5], shifting from PCFG (PRISM) to CRF-CFG (D-PRISM) yields much better prediction performance (and shifting cost is almost zero if we use D-PRISM) but at the same time this shifting incurs almost two orders of magnitude longer learning time.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "1 CRF-BNCs Bayesian network classifiers (BNCs) [7,2] are a generalization of naive Bayes classifiers.", "startOffset": 47, "endOffset": 52}, {"referenceID": 1, "context": "1 CRF-BNCs Bayesian network classifiers (BNCs) [7,2] are a generalization of naive Bayes classifiers.", "startOffset": 47, "endOffset": 52}, {"referenceID": 5, "context": "3 is an example of Bayesian network for the car dataset in the UCI Machine Learning Repository [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "CRF-LCGs are a CRF version of probabilistic left-corner grammars (PLCGs) and considered dual to CRF-CFGs [5] in the sense that the former is based on bottom-up parsing, i.", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "left-corner parsing [14,29] whereas the latter is based on top-down parsing.", "startOffset": 20, "endOffset": 27}, {"referenceID": 28, "context": "left-corner parsing [14,29] whereas the latter is based on top-down parsing.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "B-tree is projected by a CFG rule A -> B C to complete a G-tree where G and A are in the left-corner relation [14,15].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "B-tree is projected by a CFG rule A -> B C to complete a G-tree where G and A are in the left-corner relation [14,15].", "startOffset": 110, "endOffset": 117}, {"referenceID": 13, "context": "5 (details omitted)[14].", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "One way to achieve this is to use meaning preserving unfold/fold transformation for logic programs [27,20].", "startOffset": 99, "endOffset": 106}, {"referenceID": 19, "context": "One way to achieve this is to use meaning preserving unfold/fold transformation for logic programs [27,20].", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "the least model of programs is preserved through transformation (see [27,20] for details).", "startOffset": 69, "endOffset": 76}, {"referenceID": 19, "context": "the least model of programs is preserved through transformation (see [27,20] for details).", "startOffset": 69, "endOffset": 76}, {"referenceID": 26, "context": "It derives an HMM program for hmm0(X) computing incomplete data from a program for hmm0(X,Y) computing complete data using a transformation system described in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "There are already discriminative modeling languages for CRFs such as Alchemy [10] based on MLNs and Factorie [17] based on factor graphs.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "There are already discriminative modeling languages for CRFs such as Alchemy [10] based on MLNs and Factorie [17] based on factor graphs.", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "TildeCRF [8] learns CRFs over sequences of ground atoms.", "startOffset": 9, "endOffset": 12}, {"referenceID": 22, "context": "Compared to PRISM, D-PRISM has no restriction on programs such as the uniqueness condition, exclusiveness condition and independence condition [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "It is developed for PRISM programs with failure [24] and learns parameters from a conditional distribution of the form pDB(Gx | success) where success = \u2203xGx and Gx is a goal for incomplete data x that may fail.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "The point in [24] is to automatically synthesize failure predicate such that pDB(success) = 1 \u2212 pDB(failure) and rewrite the conditional distribution as an infinite series pDB(Gx | success) = pDB(Gx)(1+pDB(failure)+pDB(failure) + \u00b7 \u00b7 \u00b7) to which EM is applicable (the FAM algorithm [4]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "The point in [24] is to automatically synthesize failure predicate such that pDB(success) = 1 \u2212 pDB(failure) and rewrite the conditional distribution as an infinite series pDB(Gx | success) = pDB(Gx)(1+pDB(failure)+pDB(failure) + \u00b7 \u00b7 \u00b7) to which EM is applicable (the FAM algorithm [4]).", "startOffset": 282, "endOffset": 285}, {"referenceID": 2, "context": "Y is a non-discriminating argument in the sense of [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Christiansen and Gallagher gave a deterministic algorithm to eliminate such non-discriminating arguments without affecting the program\u2019s runtime behavior [3].", "startOffset": 154, "endOffset": 157}], "year": 2014, "abstractText": "Conditional random fields (CRFs) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model. We implemented our approach as the D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while exploiting its dynamic programming mechanism for efficient probability computation. We tested D-PRISM with logistic regression, a linear-chain CRF and a CRF-CFG and empirically confirmed their excellent discriminative performance compared to their generative counterparts, i.e. naive Bayes, an HMM and a PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF versions of Bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in D-PRISM. We empirically showed that they outperform their generative counterparts as expected.", "creator": "LaTeX with hyperref package"}}}