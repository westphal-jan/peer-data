{"id": "1411.1147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "abstract": "We present a framework for unsupervised learning of structured predictors with overlapping global characteristics; the latent representation of each input is predicted based on observable data using a feature-rich conditional random field; then a reconstruction of the input is (re) generated depending on the latent structure, using models for which the maximum probability estimate is self-contained; our autoencoder formulation allows efficient learning without making unrealistic assumptions of independence or limiting the type of applicable characteristics; we illustrate instructive links to traditional autoencoders, posterior regulation, and multivisionary learning; we show competitive results with instantiations of the model for two canonical NLP tasks: speech induction and bittext alignment; and show that the formation of our model can be much more efficient than comparable, feature-rich baselines.", "histories": [["v1", "Wed, 5 Nov 2014 04:49:38 GMT  (557kb,D)", "http://arxiv.org/abs/1411.1147v1", null], ["v2", "Mon, 10 Nov 2014 05:58:04 GMT  (556kb,D)", "http://arxiv.org/abs/1411.1147v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["waleed ammar", "chris dyer", "noah a smith"], "accepted": true, "id": "1411.1147"}, "pdf": {"name": "1411.1147.pdf", "metadata": {"source": "CRF", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "authors": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith"], "emails": ["wammar@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Conditional random fields [24] are used to model structure in numerous problem domains, including natural language processing (NLP), computational biology, and computer vision. They enable efficient inference while incorporating rich features that capture useful domain-specific insights. Despite their ubiquity in supervised settings, CRFs\u2014and, crucially, the insights about effective feature sets obtained by developing them\u2014play less of a role in unsupervised structure learning, a problem which traditionally requires jointly modeling observations and the latent structures of interest. For unsupervised structured prediction problems, less powerful models with more independence assumptions are standard.1 This state of affairs is suboptimal in at least three ways: (i) adhering to inconvenient independence assumptions when designing features is limiting\u2014we contend that effective feature engineering is a crucial mechanism for incorporating inductive bias in unsupervised learning problems; (ii) features and their weights have different semantics in joint and conditional models (see \u00a73.1); and (iii) modeling the generation of high-dimensional observable data with feature-rich models is computationally challenging, requiring expensive marginal inference in the inner loop of iterative parameter estimation algorithms (see \u00a73.1). Our approach leverages the power and flexibility of CRFs in unsupervised learning without sacrificing their attractive computational properties or changing the semantics of well-understood feature sets. Our approach replaces the standard joint model of observed data and latent structure with a twolayer conditional random field autoencoder that first generates latent structure with a CRF (conditional on the observed data) and then (re)generates the observations conditional on just the predicted structure. For the reconstruction model, we use distributions which offer closed-form maximum\n1For example, a first-order hidden Markov model requires that yi \u22a5 xi+1 | yi+1 for a latent sequence y = \u3008y1, y2, . . .\u3009 generating x = \u3008x1, x2, . . .\u3009, while a first-order CRF allows yi to directly depend on x.\nar X\niv :1\n41 1.\n11 47\nv1 [\ncs .L\nG ]\n5 N\nov 2\nlikelihood estimates (\u00a72). The proposed architecture provides several mechanisms for encouraging the learner to use its latent variables to find intended (rather than common but irrelevant) correlations in the data. First, hand-crafted feature representations\u2014engineered using knowledge about the problem\u2014provide a key mechanism for incorporating inductive bias. Second, by reconstructing a transformation of the structured input. Third, it is easy to simultaneously learn from labeled and unlabeled examples in this architecture, as we did in [26]. In addition to the modeling flexibility, our approach is computationally efficient in a way achieved by no other unsupervised, feature-based model to date: under a set of mild independence assumptions regarding the reconstruction model, inference required for learning is no more expensive than when training a supervised CRF with the same independence assumptions.\nIn the next section we describe the modeling framework, then review related work and show that although our model admits more powerful features, the inference required for learning is simpler (\u00a73). We conclude with experiments showing that our proposed model achieves state-of-the-art performance on two unsupervised tasks from NLP, POS induction and word alignment, and find that it is substantially more efficient than an existing approach using the same features (\u00a74)."}, {"heading": "2 Conditional Random Field Autoencoder", "text": "Given a training set T of observations (e.g., sentences or pairs of sentences that are translationally equivalent), consider the problem of inducing hidden structure. Examples of hidden structures include shallow syntactic properties (part-of-speech or POS tags), correspondences between words in translation (word alignment), syntactic parses and morphological analyses.\nNotation. Let each observation be denoted x = \u3008x1, . . . , x|x|\u3009 \u2208 X |x|, a variable-length tuple of discrete variables, x \u2208 X . The hidden variables y = \u3008y1, . . . , y|y|\u3009 \u2208 Y |y| form a tuple whose length is determined by x, also taking discrete values.2 We assume that |Y||y| |X ||x|, which is typical in unsupervised structured prediction problems. Fig. 1 (right) instantiates x, X , y, and Y for two NLP tasks.\nOur model introduces a new observed variable, x\u0302 = \u3008x\u03021, . . . , x\u0302|x\u0302|\u3009. In the basic model in Fig. 2 (left), x\u0302 is a copy of x. The intuition behind this model is that a good hidden structure should be a likely encoding of the data permitting reconstruction of the data with high probability.\nSequential latent structure. In this paper, we focus on sequential latent structures with firstorder Markov properties, i.e., yi \u22a5 yj | {yi\u22121, yi+1}, as illustrated in Fig. 2 (right). This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others. Importantly, we make no assumptions about conditional independence between any yi and x.\n2In the interest of notational simplicity, we conflate random variables with their values.\nEq. 1 gives the parameteric form of our model for POS induction. \u03bb and \u03b8 are the parameters of the encoding and reconstruction models, respectively. g is a vector of clique-local feature functions.3\np\u03bb,\u03b8(x\u0302 | x) = \u2211 y p\u03bb(y | x)p\u03b8(x\u0302 | y) = \u2211\ny\u2208Y|x|\ne \u2211|x|\ni=1 \u03bb >g(x,yi,yi\u22121,i)\u2211\ny\u2032\u2208Y|x| e \u2211|x| i=1 \u03bb >g(x,y\u2032i,y \u2032 i\u22121,i) |x|\u220f i=1 p\u03b8(x\u0302i | yi)\n=\n\u2211 y\u2208Y|x| exp (\u2211|x| i=1 log \u03b8x\u0302i|yi + \u03bb >g(x, yi, yi\u22121, i) )\n\u2211 y\u2032\u2208Y|x| exp \u2211|x| i=1 \u03bb >g(x, y\u2032i, y \u2032 i\u22121, i)\n(1)\nEncoding and reconstruction. We model the encoding part with a CRF, which allows us to exploit features with global scope in the structured observation x, while keeping exact inference tractable (since the model does not generate x, only conditions on it). The reconstruction part, on the other hand, grounds the model by generating a copy of the structured observations. We use a simple categorical (i.e., multinomial) distribution to independently generate x\u0302i given yi.4 Fig. 2 (right) is an instance of the model for POS induction with a sequential latent structure; each x\u0302i is generated from p\u03b8(x\u0302i | yi). We emphasize the importance of allowing modelers to define intuitive feature templates in a flexible manner. The need to efficiently add inductive bias via feature engineering has been the primary drive for developing CRF autoencoders. For example, morphology, word spelling information, and other linguistic knowledge encoded as features were shown to improve POS induction [38], word alignment [14], and other unsupervised learning problems. The proposed model enables the modeler to define such features at a lower computational cost, and enables more expressive features with global scope in the structured input. For example, we found that using predictions of other models as features is an effective method for model combination in unsupervised word alignment tasks, and found that conjoining sub-word-level features of consecutive words help disambiguate their POS labels (see Appendix A).\nExtension: side information. Our model can be easily extended to condition on more context in the encoding part, the reconstruction part, or in both parts. Let \u03c6 represent side information: additional context which we condition on in both the encoding and reconstruction models. In our running example, side information includes a POS tag dictionary (i.e., list of possible tags for each word), a common form of \u201cweak supervision\u201d shown to help unsupervised POS learners. In word alignment, where yi = j indicates that xi translates to the jth source token, we treat the source sentence as side information, making its word forms available to features.\n3We define y0 to be a fixed \u201cstart\u201d tag. Note that the cliques here are inside y; they are not visible in the high-level view of Fig. 2 (left), but are visible in Fig. 2 (right).\n4Other possible parameterizations of the reconstruction model we would like to experiment with include a multivariate Gaussian for generating word embeddings and a Na\u0131\u0308ve Bayes model for generating individual features of a word independently conditioned on the corresponding label.\nExtension: partial reconstruction. In our running POS example, the reconstruction model p\u03b8(x\u0302i | yi) defines a distribution over words given tags. Because word distributions are heavytailed, estimating such a distribution reliably is quite challenging. Our solution is to define a function \u03c0 : X \u2192 X\u0302 such that |X\u0302 | |X |, and let x\u0302i = \u03c0(xi). For POS tagging, we use Brown clusters [5]; other alternatives might introduce other helpful forms of bias (e.g., morphological or spelling features).\nMore general graphs. We presented the CRF autoencoder in terms of sequential Markovian assumptions for ease of exposition; however, this framework can be used to model arbitrary hidden structures. For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision. The requirements for applying the CRF autoencoder model are:\n\u2022 An encoding graphical model defining p\u03bb(y | x). The encoder may be any model family where supervised learning from \u3008x,y\u3009 pairs is efficient.\n\u2022 A reconstruction model that defines p\u03b8(x\u0302 | y,\u03c6) such that inference over y given \u3008x, x\u0302\u3009 is efficient.\n\u2022 The independencies among y | x, x\u0302 are not strictly weaker than those among y | x."}, {"heading": "2.1 Learning & Inference", "text": "Model parameters are selected to maximize the regularized conditional log likelihood of reconstructed observations x\u0302 given the structured observation x:\n``(\u03bb,\u03b8) = R1(\u03bb) +R2(\u03b8) + \u2211 (x,x\u0302)\u2208T log \u2211 y p\u03bb(y | x)\u00d7 p\u03b8(x\u0302 | y) (2)\nWe apply block coordinate descent, alternating between maximizing with respect to the CRF parameters (\u03bb-step) and the reconstruction parameters (\u03b8-step). Each \u03bb-step applies one or two iterations of a gradient-based convex optimizer.5 The \u03b8-step applies one or two iterations of EM [10], with a closed-form solution in the M-step in each EM iteration. The independence assumptions among y make the marginal inference required in both steps straightforward; we omit details for space.\nIn the experiments below, for the CRF parameters \u03bb, we applied a squared L2 regularizer. For all categorical parameters \u03b8 we applied a symmetric Dirichlet prior. Regularization strengths were tuned on datasets for a language or language-pair not included in the tests (English for POS, EnglishFrench for word alignment) and fixed, separately for each method.\nThe asymptotic runtime complexity of each block coordinate descent iteration, assuming the firstorder Markov dependencies in Fig. 2 (right), is:\nO ( |\u03b8|+ |\u03bb|+ |T | \u00d7 |x|max \u00d7 |Y|max \u00d7 (|Y|max \u00d7 |Fyi\u22121,yi |+ |Fx,yi |) ) (3)\nwhere Fyi\u22121,yi are the active \u201clabel bigram\u201d features used in \u3008yi\u22121, yi\u3009 factors, Fx,yi are the active emission-like features used in \u3008x, yi\u3009 factors. |x|max is the maximum length of an observation sequence. |Y|max is the maximum cardinality6 of the set of possible assignments of yi. After learning the \u03bb and \u03b8 parameters of the CRF autoencoder, test-time predictions are made using MAP, conditioning on both observations and reconstructions: y\u0302MAP = argmaxy p\u03bb,\u03b8(y | x, x\u0302)."}, {"heading": "3 Connections To Previous Work", "text": "This work relates to several strands of work in unsupervised learning. Unsupervised learning with flexible feature representations has long been studied, and there are broadly two types of models that\n5We experimented with AdaGrad [12] and L-BFGS. When using AdaGrad, we accummulate the gradient vectors across block coordinate ascent iterations.\n6In POS induction, |Y| is a constant, the number of syntactic classes which we configure to 12 in our experiments. In word alignment, |Y| is the size of the source sentence plus one, therefore |Y|max is the maximum length of a source sentence in the bitext corpus.\nsupport this. Both are fully generative models that define joint distributions over x and y. We will refer to these as the \u201cundirected\u201d and \u201cdirected\u201d alternatives. We discuss these next and then turn to less closely related methods."}, {"heading": "3.1 Existing Alternatives to Learning with Features", "text": "Undirected models. The undirected alternative uses an undirected model to encode the distribution through local potential functions parameterized using features. Such models \u201cnormalize globally,\u201d requiring during training the calculation of a partition function summing over all values of both (in our notation):\nZ(\u03b8) = \u2211 x\u2208X\u2217 \u2211 y\u2208Y|x| exp\u03bb>g\u0304(x,y) (4)\nwhere g\u0304 collects all the local factorization by cliques of the graph, for clarity. The key difficulty is in the summation over all possible observations. Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al. [14]) and noise contrastive estimation [30].\nDirected models. The directed alternative avoids the global partition function by factorizing the joint distribution in terms of locally normalized conditional probabilities, which are parameterized in terms of features. For unsupervised sequence labeling, the model was called a \u201cfeature HMM\u201d by Berg-Kirkpatrick et al. [3]. The local emission probabilities p(xi | yi) in a first-order HMM for POS tagging are reparameterized as follows (again, using notation close to ours):\np\u03bb(xi | yi) = exp\u03bb>g(xi, yi)\u2211 x\u2208X exp\u03bb >g(x, yi) (5)\nThe features relating hidden to observed variables must be local within the factors implied by the directed graph. We show below that this locality restriction excludes features that are useful (\u00a7A.1). Put in these terms, our proposed autoencoding model is a hybrid directed-undirected model.\nAsymptotic Runtime Complexity of Inference. The models just described cannot condition on arbitrary amounts of x without increasing inference costs. Despite the strong independence assumptions of those models, the computational complexity of inference required for learning with CRF autoencoders is better (\u00a72.1). Consider learning the parameters of an undirected model by maximizing likelihood of the observed data. Computing the gradient for a training instance x requires time\nO ( |\u03bb|+ |T | \u00d7 |x| \u00d7 |Y| \u00d7 (|Y| \u00d7 |Fyi\u22121,yi |+|X | \u00d7 |Fxi,yi |) ) ,\nwhere Fxi\u2212yi are the emission-like features used in an arbitrary assignment of xi and yi. When the multiplicative factor |X | is large, inference is slow compared to CRF autoencoders. Inference in directed models is faster than in undirected models, but still slower than CRF autoencoder models. In directed models [3], each iteration requires time\nO ( |\u03bb|+ |T | \u00d7 |x| \u00d7 |Y| \u00d7 (|Y| \u00d7 |Fyi\u22121,yi |+ |Fxi,yi |)+|\u03b8 \u2032| \u00d7max(|Fyi\u22121,yi |, |FX ,yi |) ) ,\nwhere Fxi,yi are the active emission features used in an arbitrary assignment of xi and yi, FX ,\u2020\u3009 is the union of all emission features used with an arbitrary assignment of yi, and \u03b8\u2032 are the local emission and transition probabilities. When |X | is large, the last term |\u03b8\u2032| \u00d7max(|Fyi\u22121,yi |, |FX ,yi | dominates runtime."}, {"heading": "3.2 Other Related Work", "text": "The proposed CRF autoencoder is more distantly related to several important ideas in less-thansupervised learning.\nArabic Basque Danish Greek Hungarian Italian Turkish Average\nV \u2212M\nea su\nre\n0. 0\n0. 1\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6 Standard HMM\nFeaturized HMM CRF autoencoder Figure 3: V-Measure [37] of induced\nparts of speech in seven languages. The CRF autoencoder model with features spanning multiple words and with Brown cluster reconstructions achieves the best results in all languages but Italian, closely followed by the featurerich HMM model of Berg-Kirkpatrick et al. [3]. The standard multinomial HMM model consistently ranks last.\nAutoencoders and other \u201cpredict self\u201d methods. Our framework borrows its general structure, Fig. 2 (left), as well as its name, from neural network autoencoders. The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39]. In contrast, the goal of CRF autoencoders is to learn specific interpretable regularities of interest.7 We emphasize that it is not clear how neural autoencoders could be used to learn the latent structures CRF autoencoders learn, without providing supervised training examples. Stoyanov et al. [40] presented a related approach for discriminative graphical model learning, including features and latent variables, based on backpropagation, which could be used to instantiate the CRF autoencoder.\nDaume\u0301 III [9] introduced a reduction of an unsupervised problem instance to a series of singlevariable supervised classification; the first series of these construct a latent structure y given the entire x, then the second series reconstruct the input again using only y. The approach can make use of any supervised learner; if feature-based probabilistic models were used, a |X | summation (akin to Eq. 5) would be required. On unsupervised POS induction, this approach performed on par with the undirected model of [38].\nMinka [29] proposed cascading a generative model and a discriminative model, where class labels (to be predicted at test time) are marginalized out in the generative part first, and then (re)generated in the discriminative part. In CRF autoencoders, observations (available at test time) are conditioned on in the discriminative part first, and then (re)generated in the generative part.\nPosterior regularization. Introduced by Ganchev et al. [16], posterior regularization imposes constraints on the learned model\u2019s posterior; a similar idea was proposed independently [2]. For example, in POS induction, every sentence might be expected to contain at least one verb. This is imposed as a soft constraint, i.e., a feature whose expected value under the model\u2019s posterior is fixed to a predefined value. Such expectation constraints are specified directly by the domain-aware model designer.8 The approach was applied to unsupervised POS induction, word alignment, and parsing. Though they applied posterior regularization to directed generative models that were not featurized, the idea is orthogonal to the model family and could be applied as well with a CRF autoencoder."}, {"heading": "4 Evaluation", "text": "We evaluate the effectiveness of CRF autoencoders for learning from unlabeled examples in POS induction and word alignment. We defer the detailed experimental setup to Appendix A to meet the page limit.\n7This is possible in CRF autoencoders due to the interdependencies among variables in the hidden structure and the manually specified feature templates which capture the relationship between observations and their hidden structures.\n8In a semi-supervised setting, when some labeled examples of the hidden structure are available, Druck and McCallum [11] used labeled examples to estimate desirable expected values. We leave semi-supervised applications of CRF autoencoders to future work; see also Suzuki and Isozaki [41].\nPart-of-Speech Induction Results. Fig. 3 compares predictions of the CRF autoencoder model in seven languages to those of a featurized first-order HMM model Berg-Kirkpatrick et al. [3] and a standard (feature-less) first-order HMM, using the V-Measure evaluation metric [37] (higher is better). First, note the large gap between both feature-rich models on the one hand, and the featureless HMM model on the other hand. Second, note that CRF autoencoders outperform featurized HMMs in all languages, except Italian, with an average relative improvement of 12%.\nThese results serve as an empirical evidence that feature engineering is an important source of inductive bias for unsupervised structured prediction problems. In particular, we found that using Brown cluster reconstructions and specifying features which span multiple words significantly improve the performance. Similar conclusions can be drawn from Fig. 4 (in Appendix A.1) which uses the manyto-one evaluation metric [20], albeit the difference in performance between CRF autoencoders and featurized HMMs, on average, is much smaller.\nBitext Word Alignment Results. First, we consider an intrinsic evaluation on a Czech-English dataset of manual alignments, measuring the alignment error rate (AER; [32]). We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a machine translation system (cdec [13]) built using the word alignment predictions of each model.\nAER for variants of each model (forward, reverse, and symmetrized) are shown in Table 1 (left). Our model signifcantly outperforms both baselines. Bleu scores on the three language pairs are shown in Table 1; alignments obtained with our CRF autoencoder model improve translation quality of the Czech-English and Urdu-English translation systems, but not of Chinese-English. This is unsurprising, given that Chinese orthography does not use letters, so that source-language spelling and morphology features our model incorporates introduce only noise here. Better feature engineering, or more data, is called for.\nWe have argued that the feature-rich CRF autoencoder will scale better than its feature-rich alternatives. Fig. ?? (right) shows the average per-sentence inference runtime for the CRF autoencoder compared to exact inference in the undirected joint model of Dyer et al. [14] with a similar feature set, as a function of the number of sentences in the corpus. For CRF autoencoders, the average inference runtime grows slightly due to the increased number of parameters, while for Dyer et al. [14] it grows substantially with the vocabulary size.9,10"}, {"heading": "5 Conclusion", "text": "We have presented a general and scalable framework for unsupervised learning of latent structures. The technique allows features with global scope in observation variables with favorable asymptotic inference runtime. We achieve this by embedding a CRF as the encoding model in the input layer of an autoencoder, and using categorical distributions to reconstruct the observations in the output layer. A key advantage of the proposed model is scalability, since inference is no more expensive\n9We only compare runtime, instead of alignment quality, because retraining the MRF model with exact inference was too expensive.\n10We did not observe a similar pattern when comparing the runtimes of the CRF autoencoder and the feature HMM of Berg-Kirkpatrick et al. [3] who informed us in personal communication of a computational trick to avoid expensive log operations in the forward-backward algorithm which sped up their training by an order of magnitude. We plan to implement this method and update the arXiv version of this paper with the outcome. Nevertheless, the asymptotic runtime analysis of inference in the CRF autoencoder model is more favorable, as shown in \u00a72.\nthan a supervised CRF model. We applied the model to POS induction and bitext word alignment, obtaining results on both tasks that are competitive with the state of the art."}, {"heading": "Acknowledgments", "text": "We thank Brendan O\u2019Connor, Jeffrey Flanigan, Dani Yogatama, Nathan Schneider, Manaal Faruqui and the anonymous reviewers for helpful suggestions. We also thank Taylor Berg-Kirkpatrick for providing his implementation of the POS induction baseline, and Phil Blunsom for sharing POS induction evaluation scripts. This work was sponsored by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. The statements made herein are solely the responsibility of the authors."}, {"heading": "A Experimental Details", "text": "In this section, we describe our experimental setup for part of speech induction and bitext word alignment in some detail.\nA.1 Part-of-Speech Induction Experimental Setup\nThe first task, part-of-speech (POS) induction, is a classic NLP problem which aims at discovering syntactic classes of tokens in a monolingual corpus, with a predefined number of classes. An example of a POS-tagged English sentence is in Fig. 1.\nData. We use the plain text from CoNLL-X [7] and CoNLL 2007 [31] training data in seven languages to train the models: Arabic, Basque, Danish, Greek, Hungarian, Italian and Turkish.11 For evaluation, we obtain gold-standard POS tags by deterministically mapping the language-specific POS tags from the shared task training data to the corresponding universal POS tag set12 [34].\nSetup. We configure our model (as well as baseline models) to induce |Y| = 12 classes. We use zero initialization of the CRF parameters, and initialize the reconstruction model parameters with a basic first-order HMM model. In each block-coordinate ascent iteration, we run one LBFGS iteration (including a line search) to optimize \u03bb, followed by one EM iteration to optimize \u03b8. Training converges after 70 block-coordinate ascent iterations.\nEvaluation. Since we do not use a tagging dictionary, the word classes induced by our model are unidentifiable. We use two cluster evaluation metrics commonly used for POS induction: a) manyto-one [20] infers a mapping across the syntactic clusters in the gold vs. predicted labels (higher is better), b) V-Measure [37] is an entropy-based metric which explicitly measures the homogeneity and completeness of predicted clusters (again, higher is better).\nCRF Autoencoder Model Instantiation. Table 1 (Right) describes the symbols and variables we use in context of the POS induction problem. We use a first-order linear CRF for the encoding part with the following feature templates:\n\u2022 \u3008yi, yi\u22121\u3009,\u2200i \u2022 \u3008yi, subj(xi)\u3009,\u2200i, j \u2022 \u3008yi, subj(xi), subk(xi\u22121)\u3009,\u2200i, j, k \u2022 \u3008yi, subj(xi), subk(xi+1)\u3009,\u2200i, j, k\nWhere subj(xi) is one of the following sub-word-level feature percepts:\n\u2022 Prefixes and suffixes of lengths two and three, iff the affix appears in more than 0.02% of all word types,\n\u2022 Whether the word contains a digit, \u2022 Whether the word contains a hyphen, \u2022 Whether the word starts with a capital letter, \u2022 Word shape features which map sequences of the same character classes into a single character\n(e.g., \u2018McDonalds\u2019\u2192 \u2018AaAa\u2019, \u2018-0.5\u2019\u2192 \u2018#0#0\u2019), \u2022 The lowercased word, iff it appears more than 100 times in the corpus.\nIn the reconstruction model, we generate the Brown cluster13 [5] of a word, conditioned on the POS tag using a categorical (i.e., multinomial) distribution. No side information is used in this model instantiation. Predictions are the best value of the latent structure according to the posterior p(y | x, x\u0302,\u03c6).\n11We plan to add results for the remaining data sets in CoNLL-X and CoNLL 2007 and update the paper on arXiv.org\n12http://code.google.com/p/universal-pos-tags/ 13We use brown-cluster -c 100 v1.3, available at https://github.com/percyliang/brown-cluster\n[25] with data from http://corpora.informatik.uni-leipzig.de/\nBaselines. We use two baselines:\n\u2022 hmm: a standard first-order hidden Markov model learned with EM;14\n\u2022 fhmm: the directed alternative discussed in the main paper, as implemented by Berg-Kirkpatrick et al. [3], with the feature set from Smith and Eisner [38].\nTuning Hyperparameters. In our models, we use a squared L2 regularizer for CRF parameters \u03bb, and a symmetric Dirichlet prior for categorical parameters \u03b8 with the same regularization strength for all languages. The fhmm baseline also uses a squared L2 regularizer for the log-linear parameters. The hyperparameters of our model, as well as baseline models, were tuned to maximize many-to-one accuracy for The English PennTreebank. The fhmm model uses L2 strength = 0.3. The auto model uses L2 strength = 2.5, \u03b1 = 0.1.\nA.2 Bitext Word Alignment Experimental Setup\nWord alignment, is an essential step in the training pipeline of most statistical machine translation systems [22]. Given a sentence in the source language and its translation in the target language, the task is to find which source token, if any, corresponds to each token in the target translation. We make the popular assumption that each token in the target sentence corresponds to zero or one token in the source sentence. Fig. 1 shows a Spanish sentence and its English translation with word alignments. As shown in Table 1 (Right), an observation x consists of tokens in the target sentence, while side information \u03c6 are tokens in the source sentence. Conditioned on a source word, we use a categorical (i.e., multinomial) distribution to generate the corresponding target word according to the inferred alignments.\nData. We consider three language-pairs: Czech-English, Urdu-English, and Chinese-English. For Czech-English, we use 4.3M bitext tokens for training from the NewsCommentary corpus, WMT10 data set for development, and WMT11 for testing. For Urdu-English, we use the train (2.4M bitext tokens), development, and test sets provided for NIST open MT evaluations 2009. For ChineseEnglish, we use the BTEC train (0.7M bitext tokens), development, and test sets (travel domain).\n14Among 32 Gaussian initializations of model parameters, we use the HMM model which gives the highest likelihood after 30 EM iterations.\nCRF Autoencoder Model Instantiation. For word alignment, we define the reconstruction model as follows: p\u03b8(x\u0302 | y,\u03c6) = \u220f|x| i=1 \u03b8x\u0302i|\u03c6yi , where x\u0302i is the Brown cluster\n15 of the word at position i in the target sentence. We use a squared L2 regularizer for the log-linear parameters \u03bb and a symmetric Dirichlet prior for the categorical parameters \u03b8 with the same regularization strength for all language pairs (L2 strength = 0.01, Dirichlet \u03b1 = 1.5). The hyperparameters were optimized to minimize Alignment Error Rate (AER) on a development dataset of French-English bitext. The reconstruction model parameters \u03b8 are initialized with the parameters taken from IBM Model 1 after five EM iterations [6]. In each block-coordinate ascent iteration, we use L-BFGS to optimize \u03bb, followed by two EM iterations to optimize \u03b8. Training converges when the relative improvement in objective value falls below 0.03 in one block-coordinate ascent iteration, typically in less than 10 iterations of block-coordinate ascent.\nWe follow the common practice of training two word alignment models for each dataset, one with English as the target language (forward) and another with English as the source language (reverse). We then use the grow-diag-final-and heuristic [23] to symmetrize alignments before extracting translation rules.\nFeatures. We use the following features: deviation from diagonal word alignment | yi|\u03c6| \u2212 i |x| |; log alignment jump log |yi \u2212 yi\u22121|; agreement with forward, reverse and symmetrized baseline alignments of mgiza++ and fast align; Dice measure of the word pair xi and \u03c6yi ; difference in character length between xi and \u03c6yi ; orthograhpic similarity between xi and \u03c6yi , punctuation token aligned to a non-punctuation token; punctuation token aligned to an identical token; 4-bit prefix of the Brown cluster of xi conjoined with 4-bit prefix of the Brown cluster of \u03c6yi ; forward and reverse probability of the word pair xi, \u03c6yi with fast align, as well as their product. We note here that the outputs of other unsupervised aligners are standard (and important!) features in supervised CRF aligners [4]; however, they are nonsensical in a joint model over alignments and sentence pairs.\nBaselines. Due to the cost of estimating feature-rich generative models for unsupervised word alignment on the data sizes we are using (e.g., fhmm and dyer-11), we only report the per-sentence computational cost of inference on these baselines. For alignment quality baselines, we report on results from two state-of-the-art baselines that use multinomial parameterizations which support Mstep analytic solutions, rather than feature-rich parameterizations: fast align [15]16 and model 4 [6]. fast align is a recently proposed reparameterization of IBM Model 2 [6]. model 4, as implemented in\n15We again use [25] with 80 word classes. 16https://github.com/clab/fastalign\nmgiza++ [17] is the most commonly used word alignment tool in state-of-the-art machine translation systems.\nEvaluation. When gold standard word alignments are available (i.e., for Czech-English), we use AER [32] to evaluate the alignment predictions of each model. We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a hierarchical MT system built using the word alignment predictions of each model."}], "references": [{"title": "Unsupervised learning of multiple motifs in biopolymers using expectation maximization", "author": ["T.L. Bailey", "C. Elkan"], "venue": "Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Alternating projections for learning with expectation constraints", "author": ["K. Bellare", "G. Druck", "A. McCallum"], "venue": "In Proc. of UAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-C\u00f4t\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proc. of NAACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Discriminative word alignment with conditional random fields", "author": ["P. Blunsom", "T. Cohn"], "venue": "In Proc. of Proceedings of ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "In Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["S. Buchholz", "E. Marsi"], "venue": "In CoNLL-X,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proc. of ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Unsupervised search-based structured prediction", "author": ["H. Daum\u00e9 III"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal statistical Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "High-performance semi-supervised learning using discriminatively constrained generative models", "author": ["G. Druck", "A. McCallum"], "venue": "In Proc. of ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. of ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unsupervised word alignment with arbitrary features", "author": ["C. Dyer", "J. Clark", "A. Lavie", "N.A. Smith"], "venue": "In Proc. of ACL-HLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Parallel implementations of word alignment tool", "author": ["Q. Gao", "S. Vogel"], "venue": "In In Proc. of the ACL workshop,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proc. of NAACL- HLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical Methods for Speech Recognition", "author": ["F. Jelinek"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In Proc. of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proc. of NAACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. of ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "In Thesis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The cmu submission for the shared task on language identification in code-switched data", "author": ["C.-C. Lin", "W. Ammar", "C. Dyer", "L. Levin"], "venue": "In First Workshop on Computational Approaches to Code Switching at EMNLP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Genemark. hmm: new solutions for gene finding", "author": ["A.V. Lukashin", "M. Borodovsky"], "venue": "Nucleic acids research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "In Comp. Ling.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Discriminative models, not discriminative training", "author": ["T. Minka"], "venue": "Technical report, Technical Report MSR-TR-2005-144, Microsoft Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "In Proc. of ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["J. Nivre", "J. Hall", "S. Kubler", "R. McDonald", "J. Nilsson", "S. Riedel", "D. Yuret"], "venue": "In Proc. of CoNLL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "In Proc. of LREC,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Joint unsupervised coreference resolution with Markov logic", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. of EMNLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Substring-based transliteration with conditional random fields", "author": ["S. Reddy", "S. Waxmonsky"], "venue": "In Proc. of the Named Entities Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N.A. Smith", "J. Eisner"], "venue": "In Proc. of ACL,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS workshop,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["V. Stoyanov", "A. Ropson", "J. Eisner"], "venue": "In Proc. of AISTATS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data", "author": ["J. Suzuki", "H. Isozaki"], "venue": "In Proc. of ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Unsupervised semantic role labelling", "author": ["R. Swier", "S. Stevenson"], "venue": "In Proc. of EMNLP,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "Non-local contrastive objectives", "author": ["D. Vickrey", "C.C. Lin", "D. Koller"], "venue": "In Proc. of ICML,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proc. of ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Hmm-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "In Proc. of COLING,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Unsupervised learning of models for recognition", "author": ["M. Weber", "M. Welling", "P. Perona"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}], "referenceMentions": [{"referenceID": 23, "context": "Conditional random fields [24] are used to model structure in numerous problem domains, including natural language processing (NLP), computational biology, and computer vision.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "Third, it is easy to simultaneously learn from labeled and unlabeled examples in this architecture, as we did in [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 44, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 3, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 27, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 172, "endOffset": 180}, {"referenceID": 19, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 172, "endOffset": 180}, {"referenceID": 18, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 200, "endOffset": 204}, {"referenceID": 26, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 219, "endOffset": 223}, {"referenceID": 35, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 245, "endOffset": 249}, {"referenceID": 37, "context": "For example, morphology, word spelling information, and other linguistic knowledge encoded as features were shown to improve POS induction [38], word alignment [14], and other unsupervised learning problems.", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "For example, morphology, word spelling information, and other linguistic knowledge encoded as features were shown to improve POS induction [38], word alignment [14], and other unsupervised learning problems.", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "For POS tagging, we use Brown clusters [5]; other alternatives might introduce other helpful forms of bias (e.", "startOffset": 39, "endOffset": 42}, {"referenceID": 20, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 95, "endOffset": 99}, {"referenceID": 41, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 187, "endOffset": 190}, {"referenceID": 45, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 229, "endOffset": 233}, {"referenceID": 9, "context": "5 The \u03b8-step applies one or two iterations of EM [10], with a closed-form solution in the M-step in each EM iteration.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "We experimented with AdaGrad [12] and L-BFGS.", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 100, "endOffset": 108}, {"referenceID": 42, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 100, "endOffset": 108}, {"referenceID": 17, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "[14]) and noise contrastive estimation [30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[14]) and noise contrastive estimation [30].", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In directed models [3], each iteration requires time", "startOffset": 19, "endOffset": 22}, {"referenceID": 36, "context": "6 Standard HMM Featurized HMM CRF autoencoder Figure 3: V-Measure [37] of induced parts of speech in seven languages.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 7, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 38, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 39, "context": "[40] presented a related approach for discriminative graphical model learning, including features and latent variables, based on backpropagation, which could be used to instantiate the CRF autoencoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Daum\u00e9 III [9] introduced a reduction of an unsupervised problem instance to a series of singlevariable supervised classification; the first series of these construct a latent structure y given the entire x, then the second series reconstruct the input again using only y.", "startOffset": 10, "endOffset": 13}, {"referenceID": 37, "context": "On unsupervised POS induction, this approach performed on par with the undirected model of [38].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Minka [29] proposed cascading a generative model and a discriminative model, where class labels (to be predicted at test time) are marginalized out in the generative part first, and then (re)generated in the discriminative part.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "[16], posterior regularization imposes constraints on the learned model\u2019s posterior; a similar idea was proposed independently [2].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[16], posterior regularization imposes constraints on the learned model\u2019s posterior; a similar idea was proposed independently [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "In a semi-supervised setting, when some labeled examples of the hidden structure are available, Druck and McCallum [11] used labeled examples to estimate desirable expected values.", "startOffset": 115, "endOffset": 119}, {"referenceID": 40, "context": "We leave semi-supervised applications of CRF autoencoders to future work; see also Suzuki and Isozaki [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "[3] and a standard (feature-less) first-order HMM, using the V-Measure evaluation metric [37] (higher is better).", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[3] and a standard (feature-less) first-order HMM, using the V-Measure evaluation metric [37] (higher is better).", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "1) which uses the manyto-one evaluation metric [20], albeit the difference in performance between CRF autoencoders and featurized HMMs, on average, is much smaller.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "First, we consider an intrinsic evaluation on a Czech-English dataset of manual alignments, measuring the alignment error rate (AER; [32]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 32, "context": "We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a machine translation system (cdec [13]) built using the word alignment predictions of each model.", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a machine translation system (cdec [13]) built using the word alignment predictions of each model.", "startOffset": 153, "endOffset": 157}, {"referenceID": 13, "context": "[14] with a similar feature set, as a function of the number of sentences in the corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] it grows substantially with the vocabulary size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] who informed us in personal communication of a computational trick to avoid expensive log operations in the forward-backward algorithm which sped up their training by an order of magnitude.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input\u2019s latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. Finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.", "creator": "LaTeX with hyperref package"}}}