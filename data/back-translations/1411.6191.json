{"id": "1411.6191", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Kickback Cuts Backprop's Red-Tape: Biologically Plausible Credit Assignment in Neural Networks", "abstract": "Error backpropagation is an extremely effective algorithm for lending in artificial neural networks. However, weight updates under backprop depend on lengthy recursive calculations and require separate output and error messages -- features that are not shared by biological neurons and may be unnecessary. In this paper, we revisit backprop and the problem of lending. First, we split backprop into a collection of interacting learning algorithms, express regret for the performance of these subalgorithms, and factorize the error signals of backprop. From these results, we derive a new credit algorithm for non-parametric regression, kickback, which is much easier than backprop. Finally, we provide a sufficient condition for kickback to follow error histories, and show that kickback matches the performance of backprop on real regression benchmarks.", "histories": [["v1", "Sun, 23 Nov 2014 04:58:22 GMT  (317kb,D)", "http://arxiv.org/abs/1411.6191v1", "7 pages. To appear, AAAI-15"]], "COMMENTS": "7 pages. To appear, AAAI-15", "reviews": [], "SUBJECTS": "cs.LG cs.NE q-bio.NC", "authors": ["david balduzzi", "hastagiri vanchinathan", "joachim m buhmann"], "accepted": true, "id": "1411.6191"}, "pdf": {"name": "1411.6191.pdf", "metadata": {"source": "CRF", "title": "Kickback cuts Backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "authors": ["David Balduzzi", "Hastagiri Vanchinathan", "Joachim Buhmann"], "emails": ["david.balduzzi@vuw.ac.nz", "hastagiri@inf.ethz.ch", "jbuhmann@inf.ethz.ch"], "sections": [{"heading": "Introduction", "text": "The discovery of error backpropagation was hailed as a breakthrough because it solved the main problem of distributed learning \u2013 the spatial credit assignment problem (Werbos 1974; Rumelhart, Hinton, and Williams 1986). Decades later, Backprop is the workhorse underlying most deep learning algorithms, and a major component of the state-of-the-art in supervised learning.\nSince Backprop\u2019s introduction, there has been tremendous progress improving the performance of neural networks. An enormous amount of effort has been expended exploring the effects of: the activation functions of nodes; network architectures (e.g. number of layers and number of nodes); regularizers such as dropout (Srivastava et al. 2014); modifications to accelerate gradient descent; and unsupervised methods for pre-training to find better local optima.\nHowever, it was known from the start that Backprop is not biologically plausible (Crick 1989). Implementing Backprop requires that neurons produce two distinct signals \u2013 outputs and errors \u2013 whereas only one has been observed in cortex (Lamme and Roelfsema 2000; Roelfsema and van Ooyen 2005).\nIt is therefore remarkable that almost no attempts have been made to rethink the core algorithm \u2013 backpropagation\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\u2013 and the problem that it solves \u2013 credit assignment. This paper revisits the credit assignment problem and takes a fresh look at the signaling architecture that underlies Backprop.\nOutline. Our starting point is to decompose Backprop into local learning algorithms, Theorem 1. Nodes under Backprop are modeled as agents that minimize their losses. Backprop ensures that nodes cooperate, towards the shared goal of minimizing the output layer\u2019s error, by gluing together their loss functions using recursively computed error signals.\nReformulating Backprop as local learners immediately suggests modifying the signaling architecture (the glue) whilst keeping the learners. In this paper, we aim to simplify Backprop\u2019s error signals.\nTheorem 2 lays the groundwork, by providing a regret bound for local learners that holds for any scalar feedback \u2013 and not just the error signals used by Backprop.\nThe next step is to show that, when a neural network has 1-dimensional outputs (e.g. nonparametric regression), Backprop\u2019s error signals factorize into two components, Theorem 3. The first component is a scalar error computed at the output layer that is analogous to a neuromodulatory signal; the second is a complicated sum over paths to the output layer that has no biological analog.\nOur proposed algorithm, Kickback, modifies Backprop by truncating the second component. Kickback is not gradient descent on the output error. Nevertheless, Theorem 4 provides a simple sufficient condition, coherence, for Kickback to follow the error gradient.\nIt turns out that many of the components of Kickback have close neurophysiological analogs. We discuss Kickback\u2019s biological significance by relating it to a recently developed, discrete-time model neuron (Balduzzi and Besserve 2012).\nFinally, we present experiments demonstrating that Kickback matches Backprop\u2019s performance on standard benchmark datasets.\nSynopsis. Our contribution is twofold. Firstly, we provide a series of simple, fundamental theorems on Backprop, one of the most heavily used learning algorithms. In particular, Theorem 1 suggests that ideas from multi-agent learning and mechanism design have a role to play in deep learning.\nar X\niv :1\n41 1.\n61 91\nv1 [\ncs .L\nG ]\n2 3\nN ov\n2 01\n4\nSecondly, we propose Kickback, a stripped-down variant of Backprop that simultaneously performs well and ties in nicely with the signaling architecture of cortical neurons.\nRelated work. The idea of building learning algorithms out of individual learning agents dates back to at least (Selfridge 1958). More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al. 2013). None of these approaches have led to algorithms that are competitive on benchmarks.\nThe algorithm closest to Kickback is attention-gated reinforcement learning (AGREL), which also eliminates the error signals from Backprop (Roelfsema and van Ooyen 2005). AGREL and Kickback are analogous at a high level, however the details differ markedly. In terms of results, the main differences are as follows. Firstly, we implement Kickback for networks with 2 and 3 hidden layers; whereas AGREL was only implemented for 1 hidden layer. Indeed, as discussed in (Roelfsema and van Ooyen 2005), extending AGREL to multiple hidden layers is problematic. Secondly, AGREL achieved comparable performance to Backprop on toy datasets: XOR, counting, and a mine detection dataset containing \u00b1200 inputs; whereas Kickback matches Backprop on much larger, real-world nonparametric regression problems. Finally, AGREL converges 1.5 to 10 times slower than Backprop, whereas Kickback\u2019s convergence is essentially identical to Backprop."}, {"heading": "Error Backpropagation", "text": "Recent work has shown that using rectilinear functions instead of sigmoids can significantly improve the performance of neural networks. We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al. 1993).\nDenote the positive and negative rectifiers by P (a) := max(0, a) and N(a) := \u2212max(0, a) respectively. Rectifiers are continuous everywhere and differentiable everywhere except at 0. The subgradients are:\n\u2207P (a) := { 1 a > 0\n0 else \u2207N(a) := { \u22121 a > 0 0 else.\nLet S(a) denote either P (a) or N(a); the notation is useful when discussing positive and negative rectifiers simultaneously. Similarly, let 1 denote either subgradient. The subgradient 1 acts as a signed indicator function.\nThe output of node j is Swj (x) := S(\u3008wj ,x\u3009). We say that node j fires if \u3008wj ,x\u3009 > 0; the firing rate is |Swj (x)|.\nFrom global to local learning. Under Backprop the entire neural network optimizes a single objective function using gradient descent on the network\u2019s error. The partial derivatives with respect to weights are computed via the chain rule.\nIn more detail, suppose a neural network has error function E(x, y) that depends on the output layer xo and labels y. Backprop recursively updates weight vectors using the chain rule. For nodes in the output layer, \u03b4o := \u2202E\u2202xo . For hidden node j, the error signal is derived via\n\u03b4j := \u2211\n{k|j\u2192k}\nwjk1k\u03b4k. (1)\nOur first result is that, when the hidden nodes are rectilinear, Backprop decomposes into many interacting learning algorithms that maximize local objective functions.\nConsider the following setup.\nDefinition 1 (rectilinear loss). A node with a rectilinear activation function Sw(\u2022) receives input x and incurs rectilinear loss\n`RL(w,x, \u03d5) := \u03d5\u00b7Sw(x) = { \u00b1\u03d5 \u00b7 \u3008w,x\u3009 if \u3008w,x\u3009 > 0 0 else\nthat depends on an externally provided scalar \u03d5.\nIf the node fires then the rectilinear loss is the linear loss `L(w, \u03d5 \u00b7x) := \u3008w, \u03d5 \u00b7x\u3009, which has been extensively analyzed in online learning (Cesa-Bianchi and Lugosi 2006). If the node does not fire then the rectilinear loss is zero.\nTheorem 1 (Backprop decomposes into local learners). The weight updates induced by Backprop on rectilinear hidden node j are the same as gradient descent on the rectilinear loss:\n\u2207wj`RL(wj ,x, \u03b4j) = \u2207wjE(xo, y).\nThe rectilinear loss resembles the hinge loss. However, it is not convex since, even if the node has a positive rectifier, \u03d5 is not necessarily positive.\nProof Sketch. Let aj = \u3008wj ,x\u3009 and xj = S(aj). Weight updates under Backprop are\n\u2206wij \u221d \u2212 \u2202E \u2202wij = \u2212 \u2202E \u2202aj \u2202aj \u2202wij = \u2212\u03b4j \u00b7 xi \u00b7 1j .\nWeight updates for gradient descent on the rectilinear loss are\n\u2206wij \u221d \u2212 \u2202`RL \u2202wij = \u2212\u03d5 \u00b7 xi \u00b7 1j . (2)\nSubstituting \u03d5\u2190 \u03b4j yields the theorem.\nBackprop is thus a collection of local optimizations glued together by the recursively computed error signals.\nA regret bound. Since the rectilinear loss has not been previously studied, our second result is a guarantee on the predictive performance of the local learners.\nTheorem 2 (regret bound for local learners). Suppose that weights are projected into a compact convex set K at each time step. Let F := {t |Swt(xt) > 0} denote the timepoints when the node fired.\nThe following guarantee holds for any sequence of inputs and scalar feedback when |F | \u2265 1:\n1\n|F | [\u2211 t\u2208F `RL(w t,xt, \u03d5t)\u2212 inf w\u2208K \u2211 t\u2208F `RL(w,x t, \u03d5t) ]\n\u2264\n\u221a 8DE\n|F | where D = maxt\u2208F { \u2016\u03d5t \u00b7 xt\u201622 } and E = maxw\u2208K \u2016w\u201622 \u2212 \u2016w1\u201622. Theorem 2 shows that the loss incurred by rectifiers on the inputs that cause them to fire converges towards the loss of the best weight-vector in hindsight. The theorem is shown for hard constraints (i.e. projecting into K); similar results hold for convex regularizers.\nThe result holds for arbitrary sequences of inputs and feedbacks, including adversarial. It is therefore more realistic than the standard i.i.d. assumption. Indeed, even if a network\u2019s inputs are i.i.d., the inputs to nodes in deeper layers are not \u2013 due to weight-updates within the network.\nProof Sketch. Standard results on online learning do not directly apply, since the rectilinear loss is not convex. To adapt these results, observe that, by (2), nodes only learn from the inputs that cause them to fire.\nClearly, Swt(xt) = \u3008wt,xt\u3009 for all t \u2208 F . That is, a node\u2019s output is linear on the inputs for which it fires. Further, the rectilinear loss is linear on F . The theorem follows from a well-known result on gradient descent for the linear loss, see (Hazan 2012).\nTheorem 2 is not restricted to Backprop\u2019s error signals; it holds for any sequence of scalars {\u03d5t}. This suggests exploring alternate ways of gluing together local learners.\nKickback: truncated error backpropagation Backprop has two unfortunate properties. Firstly, the error signals \u03b4j are computationally expensive: they depend on the activity and weights of all downstream nodes in the network. Secondly, nodes produce two distinct signals: outputs that are fed forward and errors that are fed back. In contrast, cortical neurons communicate with only one signal type, spikes, which are sent in all directions. This suggests that it may be possible to make do with less.\nViewed from a distance, Backprop is a single distributed optimization, performing gradient descent on the network\u2019s error. Zooming in, via Theorem 1, reveals that Backprop is a collection of local learners glued together by recursively computed error signals. We thus have a framework for experimenting with alternate feedback signals (Balduzzi 2014).\nKickback takes the same local learners as Backprop but weakens the glue that binds them, thereby reducing communication complexity and increasing biological plausibility.\nFactorizing Backprop\u2019s error signals. It is necessary to distinguish between global and local error signals. Local errors signals are the recursively computed signals \u03b4j . The\nglobal error is the derivative of the network\u2019s error function with respect to the activity of the output layer. Definition 2 (influence). The influence of node j on node k is \u03c4jk := wjk1k. The influence of node j on the next layer is \u03c4j := \u2211 {k|j\u2192k} \u03c4jk. The total influence of node j on downstream nodes is\n\u03c0j :=  \u2211 {k|j\u2192k} \u03c4jk  \u2211 {l|k\u2192l} \u03c4kl  \u2211 {m|l\u2192m} \u00b7 \u00b7 \u00b7  , (3) the sum over all paths from j to the output layer. Our third result is that Backprop\u2019s error signals factorize whenever a neural network has 1-dimensional outputs. Theorem 3 (error signal factorization). Suppose neural network N has scalar output and let \u03b2 = \u2202E\u2202xo be the global error. Then, the error signal of a hidden node j factorizes as\n\u03b4j = \u03b2 \u00b7 \u03c0j = ( global error ) \u00b7 ( total influencej ) . (4)\nThe theorem holds in the setting of nonparametric regression. Multi-label classification is excluded.\nProof Sketch. Backprop recursively updates weight vectors using the chain rule, recall (1). When the output is onedimensional, xo contributes \u03b2 to the recursive computation of \u03c0j over hidden nodes.\nKickback. We are now ready to introduce Kickback. Algorithm 1 (Kickback). The truncated feedback j at node j is\nj := \u03b2 \u00b7 \u03c4j = ( global error ) \u00b7 ( influencej ) . (5)\nUnder Kickback, hidden nodes perform gradient descent on the rectilinear loss with truncated feedback:\n\u2206wij \u221d \u2212\u2207wij`RL(wj ,x, j) = \u2212\u03b2 \u00b7 \u03c4j \u00b7 xi \u00b7 1j . (6) Kickback and Backprop are contrasted in Figure 1 and in equations (4) versus (5). Importantly, Kickback eliminates the need for nodes to communicate error signals \u2013 as distinct from their outputs.\nKickback as time-averaged Backprop. Truncating the feedback signal, from (4) to (5), preserves more information than appears at first glance. The truncated signal received by node j explicitly depends on j\u2019s influence on the next layer. However, Kickback implicitly incorporates information about the influence of multiple layers.\nFor simplicity, suppose there is no regularizer and that the learning rate \u03b7 is constant. Then, summing over the updates in (2), a weight at time T is wTij = \u03b7 \u2211 t\u2208Fj \u03d5 t jx t i. In the specific case of Kickback, the weight is\nwTij = \u03b7 \u2211 t\u2208Fj ( \u03b2txti\u03c4 t j ) = \u03b7 \u2211 t\u2208Fj ( \u03b2txti \u2211 {k|j\u2192k} wtjk1k ) .\nThe weight wTij thus implicitly incorporates the effect of interactions \u03c4 tjk = w t jk1 t k in the next layer down, and so on recursively.\nCoherence. With a small enough learning rate, gradient descent will tend towards a local minimum. Kickback does not perform gradient descent on the error function since it uses modified feedback signals. Thus, without further assumptions, it is not guaranteed to improve performance. Our fourth result is to provide a sufficient condition. Definition 3 (coherence). Node j is coherent when \u03c4j > 0. A network is coherent when all its nodes are coherent. Example 1 (signed coherence). An easy way to guarantee coherence for every node is to impose the purely local condition that all connections targeting positive nodes have positive weights, and similarly that all connections targeting negative nodes have negative weights.\nIf a network is coherent, then increasing a positive node\u2019s firing rate increases the average (signed) activity in the next layer and all downstream layers. Increasing the activity of negative nodes has the opposite effect.\nOn the other hand, if a network is not coherent, then nothing can be said in general about how the activity of nodes in one layer affects other layers.\nCoherence thus enforces interpretability: it ensures that a node\u2019s influence on the next layer is indicative of its total influence on all downstream layers. Theorem 4 (coherence =\u21d2 Kickback reduces error). If a network is coherent then weight updates under Kickback, with a sufficiently small learning rate, improve performance.\nProof Sketch. It suffices to show that the feedback has the same sign under Backprop, \u03b4j = \u03b2 \u00b7 \u03c0j , and Kickback, j = \u03b2 \u00b7 \u03c4j for an arbitrary hidden node j.\nIf j is coherent then \u03c4j > 0. If, furthermore, all downstream nodes are coherent, then unraveling (3) obtains that \u03c0j > 0. The result follows.\nUnder Backprop, each node\u2019s total influence is computed explicitly. Kickback makes do with less information: a node \u201cknows\u201d its influence on the next layer, but does not \u201cknow\u201d its total influence."}, {"heading": "Biological relevance", "text": "There is a direct link from Kickback to neurobiology provided by the selectron: a simplified model neuron (Balduzzi\nand Besserve 2012). The selectron is derived from standard models of neural dynamics and learning \u2013 the Spike Response Model (SRM) and Spike-Timing Dependent Plasticity (STDP) \u2013 by taking the so-called \u201cfast-time constant limit\u201d to go from continuous to discrete time. Theorem 5 (selectron). The fast time-constant limit of the SRM (Gerstner and Kistler 2002) is a node that outputs 1 if \u3008w,x\u3009 > 0 and 0 otherwise.\nWeight updates in the fast time-constant limit of neuromodulated STDP (Song, Miller, and Abbott 2000) are\n\u2206wij \u221d \u03bd \u00b7 xi \u00b7 1j = { \u03bd \u00b7 xi if \u3008wj ,x\u3009 > 0 0 else,\n(7)\nwhere \u03bd is a global, scalar-valued neuromodulatory signal. The weight updates in (7) are gradient ascent on\nReward(w,x, \u03bd) := \u03bd\u00b7Pw(x) = { \u03bd\u3008w,x\u3009 if \u3008wj ,x\u3009 > 0 0 else.\nSetting \u03d5 := \u2212\u03bd in Reward(w,x, \u03bd) recovers the rectilinear loss in Definition 1. The selectron thus maximizes a rectilinear reward via the same weight updates used to minimize the rectilinear loss. The difference between the two models is that the selectron has 0/1-valued outputs (spikes), whereas nodes have real-valued outputs (firing rates).\nProof. (Balduzzi and Besserve 2012).\nKickback\u2019s weight updates are \u2206wij \u221d \u2212\u03b2 \u00b7 \u03c4j \u00b7 xi \u00b7 1j . Each factor has a biological analog. The global error, \u03b2, corresponds to neuromodulators, such as dopamine, that have been experimentally observed to signal prediction errors for future rewards (Schultz, Dayan, and Montague 1997). The kickback term, \u03c4j , corresponds to NMDA backconnections that have a multiplicative effect on synaptic updates, proportional to the weighted sum of downstream activity (Vargas-Caballero and Robinson 2003; Roelfsema and van Ooyen 2005). The feedforward term, xi, corresponds to presynaptic spiking activity (Song, Miller, and Abbott 2000). Finally, the signed indicator function 1j , ensures that only active nodes update their weights \u2013 thereby playing the role of post-synaptic activity in STDP.\nThe regret bound in Theorem 2 is also biologically significant. Synapses incur a significant metabolic cost (Tononi\nand Cirelli 2014). Regularizing synaptic weights provides a way to quantify metabolic costs. Indeed, limits on the physical size and metabolic budget of synapses suggest that synaptic weights may be constrained to an `1-ball (Balduzzi and Besserve 2012).\nTo the best of our knowledge, Theorem 2 is the first adversarial generalization bound for a biologically derived model. The generalization bound for the selectron in (Balduzzi and Besserve 2012) assumes that inputs are i.i.d. Moving beyond the i.i.d. assumption is important because biological organisms face adversarial environments.\nThe final ingredient is coherence. Investigating biologically plausible mechanisms that ensure coherence (or some other sufficient condition) is deferred to future work."}, {"heading": "Experiments", "text": "Goals. Our primary aim is to compare Kickback\u2019s performance to Backprop. We present results on two robotics datasets, SARCOS1 and Barrett WAM2. Kickback\u2019s performance across multiple hidden layers is of particular interest, since it truncates errors. Results for 3 hidden layers are reported; results for 1 and 2 hidden layers were similar.3 A secondary aim is to investigate the effect of coherence.\nCompeting on the datasets tackled by deep learning algorithms is not yet feasible. Further work is required to adapt Kickback to multiclass learning.\nArchitecture. Experiments were performed on a 5-layer network with 2 output nodes, 10, 100 and 200 nodes in three hidden layers, and with the input layer directly drawn from the data. Experiments were implemented in Theano (Bergstra et al. 2010). All nodes are rectifiers. We set half of nodes as positive and half as negative. Output nodes perform rectilinear regression, see below, whereas hidden nodes minimize the rectilinear loss on feedback implementing either Kickback or Backprop.\nTraining was performed in batch sizes of 20. Lower batchsizes yield better performance at the cost of longer training times. We chose 20 as a reasonable compromise.\nRectilinear regression. Recently, (Glorot, Bordes, and Bengio 2011) introduced an `1 penalty on firing rates, which encourages sparsity and can improve performance. Here, we consider an `2-penalty: `RL(w,x, \u03d5) \u2212 12Sw(x)\n2. Weight updates under gradient descent are\n\u2206w \u221d {\n(\u03d5\u2212 \u3008w,x\u3009)x if \u3008w,x\u3009 > 0 0 else.\n(8)\nNotice that the penalty \u3008w,x\u3009 in (8) is the firing rate. Comparing with the gradient (\u03d5\u2212\u3008w,x\u3009)x of the mean-squared error 12 (\u03d5 \u2212 \u3008w,x\u3009)\n2 shows that the `2-activation penalty leads nodes to perform linear regression on the inputs that\n1Taken from www.gaussianprocess.org/gpml/data/. 2Taken from http://www.ias.tu-darmstadt.de/\nMiscellaneous/Miscellaneous. 3In short: the performance of both Kickback and Backprop is worse, but still comparable, with fewer layers.\ncause them to fire (Balduzzi 2013). A regret bound analogous to Theorem 2 holds for rectilinear regression, with a faster convergence rate of O( log |F ||F | ).\nTraining error is the MSE of the output node with the correct sign4; test error is the sum of the output nodes\u2019 MSEs.\nInitialization and coherence. No pretraining was used. We consider two network initializations. The first is uniform: draw weights uniformly at random from an interval symmetric about 0, without regard to coherence. The second initialization, signed is taken from Example 1: draw weights uniformly, then change their signs so that connections targeting positive nodes have positive weights and conversely for negative nodes. Signed guarantees coherence at initialization. Although it is possible to impose coherence during training, we found that doing so was unnecessary in practice.\nResults are plotted under both initializations for Kickback \u2013 excepting Panel (e), where uniform failed to converge. For Backprop, the initialization that yielded the better performance is reported.\nResults. We report normalized mean-squared errors. To directly compare the behavior of the algorithms, we report individual runs. Performance was robust to significant changes in tuning parameters: e.g. changing parameters by 2\u00d7 increased the MSE on SARCOS 3 from .6% to .8%.\nEach SARCOS dataset consists of 44,484 training and 4,449 test points; Barrett split as 12,000 and 3,000. Parameters were tuned via grid-search with 5-fold cross-validation. Backprop\u2019s only parameter is the learning rate. Kickback was implemented with a learning rate tuned for Backprop. Kickback has two additional parameters that rescale the feedback to hidden layers 1 & 2. We observed that tuning via cross-validation typically set the rescaling factors such that the truncated errors are rescaled to about same magnitude, on average, as Backprop\u2019s feedback.\nKickback and Backprop are competitive with nonparametric methods such as kernel regression, e.g. (Kpotufe and Boularias 2013). Kickback performs best with signed initialization, as expected from Theorem 4. With signed initialization, Kickback almost exactly matches Backprop in all 6 datasets. Importantly, Kickback continues to reduce the MSE after 100s of epochs; following the correct gradient even when the error is small.\nThe comparison between Backprop and Kickback is not completely fair: Kickback\u2019s additional parameters cause it to outperform Backprop in panel (b). We have endeavored to keep the comparison as level as possible.\nThe effect of coherence. Kickback\u2019s performance was better than expected: coherence was not imposed after initialization under signed; and no guarantees are applicable to uniform. A possible explanation is that Kickback preserves or increases coherence.\n4Recall there is one positive and one negative output rectifier.\nTo test this hypothesis, we quantified the coherence of layer \u03b1 as coh(L\u03b1) = \u2211 j\u2208L\u03b1 \u03c4j\u2211 j\u2208L\u03b1 |\u03c4j |\n, which lies in [\u22121, 1]. With signed initialization, coherence consistently remained above 0.9 under Kickback; but exhibited considerable variability under Backprop. With uniform initialization, Kickback increased the coherence of hidden layers 2 & 3, from 0 to > 0.5, with the exception of panel (c). Backprop did not alter coherence in any consistent way.\nBarrett 4 is the only dataset where nodes become incoherent (coh < 0) on average. The oscillations in Panel (c) for uniform arise because Kickback is not guaranteed to follow the training error gradient in the absence of coherence. It is surprising the network learns at all. Note that oscillations do not occur when networks are given a signed initialization."}, {"heading": "Conclusion", "text": "A necessary step towards understanding how the brain assigns credit is to develop a minimal working model that fits basic constraints.\nBackprop solves the credit assignment problem. It is one of the simplest and most effective methods for learning representations. In combination with various tricks and optimizations, it continues to yield state-of-the-art performance. However, it flouts a basic constraint imposed by neurobiology: it requires that nodes produce error signals that are distinct from their outputs.\nKickback is a stripped-down version of Backprop motivated by theoretical (Theorems 1\u20134) and biological (Fig. 1 and Theorem 5) considerations. Under Kickback, nodes perform gradient descent, or ascent, on the representation \u2013 that is, the kicked back activity \u2013 produced by the next layer. The\nsign of the global error determines whether nodes follow the gradient downwards, or upwards.\nKickback is the first competitive algorithm with biologically plausible credit-assignment. Earlier proposals were not competitive and only worked for one hidden-layer (Kickback works well for \u2264 3 hidden-layers; we have not tested \u2265 4). Kickback\u2019s simplified signaling is suited to hardware implementations (Indiveri et al. 2011; Nere et al. 2012).\nAn important outcome of the paper is a new formulation of Backprop in terms of interacting local learners, that may connect deep learning to recent developments in multi-agent systems (Seuken and Zilberstein 2008; Sutton et al. 2011) and mechanism design (Balduzzi 2014).\nKickback\u2019s rescaling factors (1 per hidden layer) are a loose-end that require addressing in future work.\nPerhaps the most important direction is to extend Kickback to multiclass learning. For this, it is necessary to consider multidimensional outputs, in which case the derivative of the energy function with respect to the output layer is not a scalar. A natural approach to tackle this setting is to use more sophisticated global error signals. Indeed, modeling the neuromodulatory system as producing scalar outputs is a vast oversimplification (Dayan 2012).\nFinally, reinforcement learning is a better model of how an agent adapts to its environment than supervised learning (Veness et al. 2010). A natural avenue to explore is how Kickback, suitably modified, performs in this setting.\nAcknowledgements. We thank Jacob Abernethy and Satinder Singh for useful conversations. This research was supported in part by SNSF grant 200021 137971."}], "references": [{"title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity", "author": ["D. Balduzzi", "M. Besserve"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Balduzzi and Besserve,? 2012", "shortCiteRegEx": "Balduzzi and Besserve", "year": 2012}, {"title": "Randomized co-training: from cortical neurons to machine learning and back again", "author": ["D. Balduzzi"], "venue": "Randomized Methods for Machine Learning Workshop, Neural Inf Proc Systems (NIPS).", "citeRegEx": "Balduzzi,? 2013", "shortCiteRegEx": "Balduzzi", "year": 2013}, {"title": "Cortical prediction markets", "author": ["D. Balduzzi"], "venue": "Proc. 13th Int Conf on Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "Balduzzi,? 2014", "shortCiteRegEx": "Balduzzi", "year": 2014}, {"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. Python for Scientific Comp. Conf. (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Prediction, Learning and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press.", "citeRegEx": "Cesa.Bianchi and Lugosi,? 2006", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "The recent excitement about neural networks", "author": ["F. Crick"], "venue": "Nature 337(12):129\u2013132.", "citeRegEx": "Crick,? 1989", "shortCiteRegEx": "Crick", "year": 1989}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).", "citeRegEx": "Dahl et al\\.,? 2013", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Twenty-Five Lessons from Computational Neuromodulation", "author": ["P. Dayan"], "venue": "Neuron 76:240\u2013256.", "citeRegEx": "Dayan,? 2012", "shortCiteRegEx": "Dayan", "year": 2012}, {"title": "Spiking Neuron Models", "author": ["W. Gerstner", "W. Kistler"], "venue": "Cambridge University Press.", "citeRegEx": "Gerstner and Kistler,? 2002", "shortCiteRegEx": "Gerstner and Kistler", "year": 2002}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. 14th International Conference on Artificial Intelligence and Statistics (AISTATS).", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The convex optimization approach to regret minimization", "author": ["E. Hazan"], "venue": "Sra, S.; Nowozin, S.; and Wright, S. J., eds., Optimization for machine learning. MIT Press.", "citeRegEx": "Hazan,? 2012", "shortCiteRegEx": "Hazan", "year": 2012}, {"title": "A Neuron as a Signal Processing Device", "author": ["T. Hu", "Z.J. Towfic", "C. Pehlevan", "A. Genkin", "D.B. Chklovskii"], "venue": "Asilomar Conference on Signals, Systems and Computers.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Neuromorphic silicon neuron circuits", "author": ["G. Indiveri", "B. Linares-Barranco", "T.J. Hamilton", "A. van Schaik", "R. Etienne-Cummings", "T. Delbruck", "S.-C. Liu", "P. Dudek", "P. H\u00e4fliger"], "venue": null, "citeRegEx": "Indiveri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Indiveri et al\\.", "year": 2011}, {"title": "What is the Best Multi-Stage Architecture for Object Recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Gradient Weights help Nonparametric Regressors", "author": ["S. Kpotufe", "A. Boularias"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Kpotufe and Boularias,? 2013", "shortCiteRegEx": "Kpotufe and Boularias", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The distinct modes of vision offered by feedforward and recurrent processing", "author": ["V. Lamme", "P. Roelfsema"], "venue": "Trends in Neurosci. 23(11):571\u2013579.", "citeRegEx": "Lamme and Roelfsema,? 2000", "shortCiteRegEx": "Lamme and Roelfsema", "year": 2000}, {"title": "Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks 6:861\u2013867.", "citeRegEx": "Leshno et al\\.,? 1993", "shortCiteRegEx": "Leshno et al\\.", "year": 1993}, {"title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models", "author": ["A.L. Maas", "A.Y. Hannun", "A. Ng"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Maas et al\\.,? 2013", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML).", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "A neuromorphic architecture for object recognition and motion anticipation using burst-STDP", "author": ["A. Nere", "U. Olcese", "D. Balduzzi", "G. Tononi"], "venue": "PLoS One 7(5):e36958.", "citeRegEx": "Nere et al\\.,? 2012", "shortCiteRegEx": "Nere et al\\.", "year": 2012}, {"title": "Attention-gated reinforcement learning of internal representations for classification", "author": ["P.R. Roelfsema", "A. van Ooyen"], "venue": "Neural Comput 17(10):2176\u20132214", "citeRegEx": "Roelfsema and Ooyen,? \\Q2005\\E", "shortCiteRegEx": "Roelfsema and Ooyen", "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature 323:533\u2013 536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P. Montague"], "venue": "Science 275(1593-1599).", "citeRegEx": "Schultz et al\\.,? 1997", "shortCiteRegEx": "Schultz et al\\.", "year": 1997}, {"title": "Pandemonium: a paradigm for learning", "author": ["O.G. Selfridge"], "venue": "Mechanisation of Thought Processes: Proceedings of a Symposium Held at the National Physics Laboratory.", "citeRegEx": "Selfridge,? 1958", "shortCiteRegEx": "Selfridge", "year": 1958}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Auton Agent Multi-Agent Syst 17(2):190\u2013250.", "citeRegEx": "Seuken and Zilberstein,? 2008", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission", "author": ["H.S. Seung"], "venue": "Neuron 40(10631073).", "citeRegEx": "Seung,? 2003", "shortCiteRegEx": "Seung", "year": 2003}, {"title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "author": ["S. Song", "K.D. Miller", "L.F. Abbott"], "venue": "Nat Neurosci 3(9).", "citeRegEx": "Song et al\\.,? 2000", "shortCiteRegEx": "Song et al\\.", "year": 2000}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Motor Interaction", "author": ["R. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "Proc. 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Sleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration", "author": ["G. Tononi", "C. Cirelli"], "venue": "Neuron 81(1):12\u201334.", "citeRegEx": "Tononi and Cirelli,? 2014", "shortCiteRegEx": "Tononi and Cirelli", "year": 2014}, {"title": "A slow fraction of Mg2+ unblock of NMDA receptors limits their contribution to spike generation in cortical pyramidal neurons", "author": ["M. Vargas-Caballero", "H.P. Robinson"], "venue": "J Neurophysiol 89(5):2778\u201383.", "citeRegEx": "Vargas.Caballero and Robinson,? 2003", "shortCiteRegEx": "Vargas.Caballero and Robinson", "year": 2003}, {"title": "Reinforcement Learning via AIXI Approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "D. Silver"], "venue": "Proc. 24th AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Veness et al\\.,? 2010", "shortCiteRegEx": "Veness et al\\.", "year": 2010}, {"title": "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences", "author": ["P.J. Werbos"], "venue": "Ph.D. Dissertation, Harvard.", "citeRegEx": "Werbos,? 1974", "shortCiteRegEx": "Werbos", "year": 1974}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "author": ["R.J. Williams"], "venue": "Machine Learning 8:229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "On Rectified Linear Units for Speech Processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G. Hinton"], "venue": "IEEE Int Conf on Acoustics, Speech and Signal Proc (ICASSP).", "citeRegEx": "Zeiler et al\\.,? 2013", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "The discovery of error backpropagation was hailed as a breakthrough because it solved the main problem of distributed learning \u2013 the spatial credit assignment problem (Werbos 1974; Rumelhart, Hinton, and Williams 1986).", "startOffset": 167, "endOffset": 218}, {"referenceID": 28, "context": "number of layers and number of nodes); regularizers such as dropout (Srivastava et al. 2014); modifications to accelerate gradient descent; and unsupervised methods for pre-training to find better local optima.", "startOffset": 68, "endOffset": 92}, {"referenceID": 5, "context": "However, it was known from the start that Backprop is not biologically plausible (Crick 1989).", "startOffset": 81, "endOffset": 93}, {"referenceID": 16, "context": "Implementing Backprop requires that neurons produce two distinct signals \u2013 outputs and errors \u2013 whereas only one has been observed in cortex (Lamme and Roelfsema 2000; Roelfsema and van Ooyen 2005).", "startOffset": 141, "endOffset": 197}, {"referenceID": 0, "context": "We discuss Kickback\u2019s biological significance by relating it to a recently developed, discrete-time model neuron (Balduzzi and Besserve 2012).", "startOffset": 113, "endOffset": 141}, {"referenceID": 24, "context": "The idea of building learning algorithms out of individual learning agents dates back to at least (Selfridge 1958).", "startOffset": 98, "endOffset": 114}, {"referenceID": 34, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al.", "startOffset": 41, "endOffset": 56}, {"referenceID": 26, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al.", "startOffset": 84, "endOffset": 96}, {"referenceID": 11, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al. 2013).", "startOffset": 147, "endOffset": 163}, {"referenceID": 13, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 19, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 35, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 17, "context": "2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al. 1993).", "startOffset": 207, "endOffset": 227}, {"referenceID": 4, "context": "If the node fires then the rectilinear loss is the linear loss `L(w, \u03c6 \u00b7x) := \u3008w, \u03c6 \u00b7x\u3009, which has been extensively analyzed in online learning (Cesa-Bianchi and Lugosi 2006).", "startOffset": 144, "endOffset": 174}, {"referenceID": 10, "context": "The theorem follows from a well-known result on gradient descent for the linear loss, see (Hazan 2012).", "startOffset": 90, "endOffset": 102}, {"referenceID": 2, "context": "We thus have a framework for experimenting with alternate feedback signals (Balduzzi 2014).", "startOffset": 75, "endOffset": 90}, {"referenceID": 0, "context": "Biological relevance There is a direct link from Kickback to neurobiology provided by the selectron: a simplified model neuron (Balduzzi and Besserve 2012).", "startOffset": 127, "endOffset": 155}, {"referenceID": 8, "context": "The fast time-constant limit of the SRM (Gerstner and Kistler 2002) is a node that outputs 1 if \u3008w,x\u3009 > 0 and 0 otherwise.", "startOffset": 40, "endOffset": 67}, {"referenceID": 0, "context": "(Balduzzi and Besserve 2012).", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "The kickback term, \u03c4j , corresponds to NMDA backconnections that have a multiplicative effect on synaptic updates, proportional to the weighted sum of downstream activity (Vargas-Caballero and Robinson 2003; Roelfsema and van Ooyen 2005).", "startOffset": 171, "endOffset": 237}, {"referenceID": 0, "context": "Indeed, limits on the physical size and metabolic budget of synapses suggest that synaptic weights may be constrained to an `1-ball (Balduzzi and Besserve 2012).", "startOffset": 132, "endOffset": 160}, {"referenceID": 0, "context": "The generalization bound for the selectron in (Balduzzi and Besserve 2012) assumes that inputs are i.", "startOffset": 46, "endOffset": 74}, {"referenceID": 3, "context": "Experiments were implemented in Theano (Bergstra et al. 2010).", "startOffset": 39, "endOffset": 61}, {"referenceID": 1, "context": "cause them to fire (Balduzzi 2013).", "startOffset": 19, "endOffset": 34}, {"referenceID": 14, "context": "(Kpotufe and Boularias 2013).", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Kickback\u2019s simplified signaling is suited to hardware implementations (Indiveri et al. 2011; Nere et al. 2012).", "startOffset": 70, "endOffset": 110}, {"referenceID": 20, "context": "Kickback\u2019s simplified signaling is suited to hardware implementations (Indiveri et al. 2011; Nere et al. 2012).", "startOffset": 70, "endOffset": 110}, {"referenceID": 25, "context": "An important outcome of the paper is a new formulation of Backprop in terms of interacting local learners, that may connect deep learning to recent developments in multi-agent systems (Seuken and Zilberstein 2008; Sutton et al. 2011) and mechanism design (Balduzzi 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 29, "context": "An important outcome of the paper is a new formulation of Backprop in terms of interacting local learners, that may connect deep learning to recent developments in multi-agent systems (Seuken and Zilberstein 2008; Sutton et al. 2011) and mechanism design (Balduzzi 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 2, "context": "2011) and mechanism design (Balduzzi 2014).", "startOffset": 27, "endOffset": 42}, {"referenceID": 7, "context": "Indeed, modeling the neuromodulatory system as producing scalar outputs is a vast oversimplification (Dayan 2012).", "startOffset": 101, "endOffset": 113}, {"referenceID": 32, "context": "Finally, reinforcement learning is a better model of how an agent adapts to its environment than supervised learning (Veness et al. 2010).", "startOffset": 117, "endOffset": 137}], "year": 2014, "abstractText": "Error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks. However, weight updates under Backprop depend on lengthy recursive computations and require separate output and error messages \u2013 features not shared by biological neurons, that are perhaps unnecessary. In this paper, we revisit Backprop and the credit assignment problem. We first decompose Backprop into a collection of interacting learning algorithms; provide regret bounds on the performance of these sub-algorithms; and factorize Backprop\u2019s error signals. Using these results, we derive a new credit assignment algorithm for nonparametric regression, Kickback, that is significantly simpler than Backprop. Finally, we provide a sufficient condition for Kickback to follow error gradients, and show that Kickback matches Backprop\u2019s performance on real-world regression benchmarks.", "creator": "TeX"}}}