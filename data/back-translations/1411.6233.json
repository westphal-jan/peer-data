{"id": "1411.6233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "A Convex Sparse PCA for Feature Analysis", "abstract": "The Principal Component Analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for various applications in engineering, biology and social sciences. Classical PCA and its variants strive for linear projections of the original variables in order to obtain a low dimensional representation of the characteristics with maximum variance. One limitation is that it is very difficult to interpret the results of PCA. Furthermore, classical PCA can be formulated as a low-ranking regression optimization problem. In this paper, we propose a convex, sparse Principal Component Analysis (CSPCA) algorithm and apply it to the characteristic analysis. First, we show that PCA can be formulated as a low-level regression optimization problem. Based on the discussion, the l 2, 1 standard minimization is integrated into the objective function to make the regression coefficients sparent and thus robustly affect the outliers of the SPA, which, in turn, will have an optimal effect on the CA based on the original SPA.", "histories": [["v1", "Sun, 23 Nov 2014 13:06:43 GMT  (1210kb)", "http://arxiv.org/abs/1411.6233v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "yi yang", "heng huang"], "accepted": false, "id": "1411.6233"}, "pdf": {"name": "1411.6233.pdf", "metadata": {"source": "CRF", "title": "A Convex Sparse PCA for Feature Analysis", "authors": ["Xiaojun Chang", "Feiping Nie", "Yi Yang", "Heng Huang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n62 33\nv1 [\ncs .L\nG ]\n2 3\nN ov\n2 01\n4 JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2007 1\nIndex Terms\u2014Principal Component Analysis, Convex PCA, Sparse PCA, Feature Analysis\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high. It is computationally expensive to analyze the highdimensional data directly. Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7]. To improve the efficiency and accuracy, researchers have demonstrated that dimensionality reduction is one of the most effective approaches for data analysis, and plays a significant role in data mining. Because of its simplicity and effectiveness, Principal Component Analysis (PCA) has been widely applied to various applications. The goal of PCA is to find a projection matrix that maximizes the variance of the samples after the projection, while preserving the structure of the original dataset as much as possible. PCA seeks a linear projection for the original highdimensional feature vectors so as to obtain a low dimensional representation of data, which captures as much information as possible. One may obtain principal components (PCs) by performing singular value decomposition (SVD) of the original data matrix and choose the first k PCs to represent the data, which is a more compact feature representation. There are two main reasons why PCA usually obtains good\n\u2022 Xiaojun Chang and Yi Yang are with School of Information Technology and Electrical Engineering, The University of Queensland, Australia. (E-mail: x.chang@uq.edu.au; yi.yang@uq.edu.au).\n\u2022 Feiping Nie and Heng Huang are with Department of Computer Science and Engineering, University of Texas at Arlington. (E-mail: feipingnie@gmail.com, heng@uta.edu).\nperformance in the real world applications: (1) all the PCs are uncorrelated; (2) minimal information loss is guaranteed by the fact that PCs sequentially capture maximum variability among columns of data matrix. Nevertheless, PCA still has some inherent drawbacks, which this paper will address. One problem of the classical PCA is that each PC is obtained by a linear combination of original variables and loadings are normally non-zero, which makes it often difficult to interpret the results. To address this problem, Hui Zou etal. integrate the lasso penalty [8], which is a variable selection technique, into the regression criterion in [9]. In their paper, they propose a new approach for estimating PCs with sparse loadings, sparse principal component analysis (SPCA). Lasso penalty is implemented via elastic net, which is a generalization of lasso proposed in [10]. However, their algorithm is non-convex and it is difficult to obtain the global optima. Thus the performance may vary dramatically with different local optima. Another drawback of the classical PCA methods is that they are least square estimation approaches, which are commonly known not to be robust in the sense that outlying measurements can arbitrarily skew the solution from the desired solution [11]. To make PCA robust to outliers, Xu etal. [12] propose to recover a low-rank matrix from highly corrupted measurements. It has been experimentally demonstrated in [11] that robust PCA gains promising performance on noisy data analysis. However, despite of its robustness to the outliers, the algorithm proposed in [11] is transductive, and is not able to deal with the out-of-sample data which are unseen during the training phrase. It is very restrictive to have all the data beforehand. Therefore, the robust PCA algorithm proposed in [12] is less practical for many real world\napplication. In this paper, we propose a novel convex sparse PCA for feature analysis. It has been demonstrated in [9] that the sparse model is a good measure for feature analysis, especially for feature weighting. We therefore impose the l2,1-norm on the regression coefficient so as to make our algorithm able to evaluate the importance of each feature. Besides, we adopt the l2,1norm based loss function, which is robust to the outliers, to achieve robust performance. Different from [13], our algorithm is inductive and can be directly used to map the unseen data which are outside the training set. We name the proposed algorithm Convex Sparse PCA (CSPCA). The main contributions of this paper can be summarized as follows:\n1) We have theoretically proved the equivalence of the classical PCA and low rank regression. 2) The proposed algorithm combines the recent advances of sparsity and robust PCA into a joint framework to leverage the mutual benefit. To the best of our knowledge, this is the first convex sparse and robust PCA algorithm, which ensures our algorithm always achieves the global optima. 3) Different from the existing robust PCA algorithms [13] [14], which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase. 4) We propose an effective iterative algorithm to optimize the objective function, which simultaneously optimizes the l2,1-norm minimization and the trace norm minimization.\nThe rest of this paper is organized as follows. We briefly review related work on PCA, sparse PCA and robust PCA in Section 2. Then we elaborate the formulation of our method in Section 3, followed by the proposed solution in Section 4. Extensive experiments are conducted in Section 5 to evaluate performance of the proposed algorithm. Section 6 concludes this paper."}, {"heading": "2 RELATED WORK", "text": "In this section, we briefly review three related topics of our work, including the classical PCA, sparse PCA and robust PCA. To begin with, we first define the terms and notations which will be frequently used in this paper. (1) data matrix denoted by X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] where xi \u2208 R\nd(1 \u2264 i \u2264 n) is the i-th datum and n is the total number of the samples; (2) projection matrix denoted by W ; (3) the Frobenius norm denoted by \u2016X\u2016F ; (4) the trace norm denoted by \u2016W\u2016\u2217."}, {"heading": "2.1 The Classical PCA", "text": "The classical PCA is a statistical technique for dimensionality reduction. Classical PCA techniques, also\nknown as Karhunen-Loeve methods, look for a dimensionality reducing linear projection that maximizes the total scatter of all projected data points. To be more specific, PCA computes the PCs by performing eigen-value decomposition of covariance of the convariance matrix of all training data. In general, the entries of corresponding PCs are dense and non-zero. The objective function of classical PCA is\nmax WTW=I\nTr(W tXXW ),\nwhere Tr(\u00b7) denotes trace operator."}, {"heading": "2.2 Sparse PCA", "text": "A common limitation of the classical PCA is the lack of interpretability. All principal components are a linear combination of variables and most of the factor coefficients are non-zero. To get more interpretable results, sparse PCA is proposed, which leads to reduced computation time and improved generalization. There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19]. The objectives of all the methods aim to reduce the dimensionality reduction and the number of explicitly used variables. A straightforward way is to manually set factor coefficients with values below a threshold to zero. This simple and naive thresholding method is often adopted in various applications. Nevertheless, it could be potentially misleading in different aspects. Jolliffe etal. propose SCoTLASS to obtain modified principal components with possible zero factor coefficients [18]. Lasso [8] has shown to be a effective variable selection method, which has been shown effective in a variety of applications. To further improve lasso, Zou etal. propose the elastic net in [10] for sparsity based mining. Based on the fact that PCA can be reformulated as regression-type optimization problem, Zou etal. [9] propose sparse PCA (SPCA) for estimating PCs with sparse factor coefficients, which can be formulated as follows:\nmin A,B\nn\u2211\ni=1\n\u2016xi \u2212AB Txi\u2016 2 + \u03bb\nk\u2211\nj=1\n\u2016\u03b2j\u2016 2 +\nk\u2211\nj=1\n\u03bb1,j\u2016\u03b2j\u20161,\ns.t. ATA = I\nwhere \u03b2 is lasso estimates. All k components share the same \u03bb and different \u03bb1,j \u2019s are allowed for penalizing the loadings of different principal components. Although the algorithm has good performance and attracted more and more attention, it is non-convex and difficult to find the global optima."}, {"heading": "2.3 Robust PCA", "text": "The goal of robust PCA is to recover a low-rank matrix D from highly corrupted measurements X = D + E. The errors E are supposed to be sparsely supported.\nMotivated by recent research on the robust solution of over-determined linear systems of equations in the presence of arbitrary but sparse errors and computing low-rank matrix solutions to underdetermined linear equations, John etal. [13] propose exact recovery of corrupted low-rank matrices by convex optimization. A straightforward solution to robust PCA is to seek the matrix with the lowest rank that could have generated the data under the constraint of sparse errors. The objective function of robust PCA is formulated as follows:\nmin D \u2016X \u2212D\u20160 + \u03b3rank(D) (1)\nHowever, since Eq. (1) involves l0-norm, the objective function is highly non-convex and it is difficult to find an efficient solution. To obtain a tractable optimization problem, it is nature to replace the l0norm with l1-norm and the rank with the trace norm. The objective function can be rewritten as:\nmin D \u2016X \u2212D\u20161 + \u03b3\u2016D\u2016\u2217 (2)\nTo make the objective function robust to outliers, we further replace l1-norm with l2,1-norm as l2,1-norm is indicated to make the objective function robust to outliers in [14]. The objective function arrives at:\nmin D \u2016X \u2212D\u20162,1 + \u03b3\u2016D\u2016\u2217 (3)\nAlthough the robust PCA has attracted much research attention in recent years, it still has a major limitation. As the robust PCA is transductive, despite of its good performance, it cannot be applied to outof-sample problems. In other words, it cannot map the data, which are outside the training set, into the low dimensional subspace."}, {"heading": "3 THE PROPOSED METHOD", "text": "In this section, we first demonstrate the equivalence of PCA and regression, followed by illustrating the formulation of the convex sparse PCA method. Then we describe a detailed approach to solve the objective function."}, {"heading": "3.1 The Equivalence of Classical PCA and Regression", "text": "The proposed CSPCA is designed upon our recent finding that the classical PCA can be reformulated as a regression problem. This conclusion provides us with new insights of PCA in a different perspective, and enables us to design the new convex sparse PCA algorithm. We begin the following theorem.\nTheorem 1. The classical PCA can be reformulated as a low-rank regression optimization problem as follows:\nmin rank(W )=k\n\u2016WTX \u2212X\u20162F (4)\nProof: As we have the constraint rank(W ) = k, we can easily write W = BAT , where A \u2208 Rd\u00d7k is an orthogonal matrix, B \u2208 Rd\u00d7k and the rank of both A and B are k. The above objective function can be rewritten as follows:\nmin A,B\u2208Rd\u00d7k,ATA=I\n\u2016ABTX \u2212X\u20162F\n= min A,b\u2208Rd\u00d7k,ATA=I\nTr(BTXXTB)\u2212 2Tr(BTXXTA).\n(5) By setting the derivatives of (5) w.r.t B to zero, we\nhave:\nXXTB = XXTA (6)\nBy denoting X = U\u03a3V T , U\u22a5 as orthogonal complement standard basis vectors of U and B = U\u03b1+U\u22a5\u03b2 (\u03b2 is an arbitrary vector), we have the following mathematical deduction:\nXXTB = XXTA\n\u21d2U\u03a32UT (U\u03b1+ U\u22a5\u03b2) = U\u03a32UTA\n\u21d2U\u03a32\u03b1 = U\u03a32UTA\n\u21d2\u03b1 = UTA.\n(7)\nHence, we have B = UUTA+U\u22a5\u03b2. By incorporating B into (4), we obtain:\nmin \u03b2,ATA=I\n\u2016AATUUTX +A\u03b2T (U\u22a5)TX \u2212X\u20162F\n\u21d2 min \u03b2,ATA=I\n\u2016AATUUTU\u03a3V T +A\u03b2T (U\u22a5)TU\u03a3V T \u2212X\u20162F\n\u21d2 min ATA=I\n\u2016AATX \u2212X\u20162F .\n(8) Hence, we have A = U1Q, where Q is an arbitrary orthogonal matrix. And we can get\nB = UUTA+ U\u22a5\u03b2 = UUTU1Q+ U \u22a5\u03b2 = U1Q+ U \u22a5\u03b2 (9) With the obtained A and B, we can get:\nW = ABT = U1Q(U1Q+ U \u22a5\u03b2)T\n= U1U T 1 + U1Q\u03b2\nTU\u22a5 T (10)\nThe projected samples can be obtained as follows:\nWTX = ABTX = U1U T 1 U\u03a3V T + U1Q\u03b2 TU\u22a5\nT U\u03a3V T\n= U1\u03a31V1, (11)\nwhich is equivalent to projected samples obtained by classical PCA.\nThe connection between the stated Theorem 1 and Theorem 2 in [9]: Zou etal. claim that when \u03bb > 0, PCA problem can be transformed into a regressiontype problem by the following Theorem:\nTheorem 2. For any \u03bb > 0, let\n(\u03b1\u0302, \u03b2\u0302) = min \u03b1,\u03b2\nn\u2211\ni=1\n\u2016xi \u2212 \u03b1\u03b2 T xi\u2016 2 + \u03bb\u2016\u03b2\u20162\ns.t. \u2016\u03b1\u20162 = 1.\n(12)\nIn the above theorem, \u03b2 is the lasso estimates and \u03b2\u0302 \u221d the space of PCA. Compared with Theorem 2 proposed in [9], our contribution is that we prove that when \u03bb = 0, PCA problem is completely equivalent to a regression-type problem."}, {"heading": "3.2 The Proposed Objective Function", "text": "In this section, we detail the proposed objective function of SCPCA. Motivated by previous work [20], which demonstrate that l2,1-norm of W is capable of making W sparse, we propose our sparse PCA algorithm as follows:\nmin rank(W )=k\n\u2016(WTX \u2212X)T \u201622 + \u03b1\u2016W\u20162,1, (13)\nwhere l2,1-norm of W is defined as\n\u2016W\u20162,1 =\nd\u2211\ni=1\n\u221a\u221a\u221a\u221a d\u2211\nj=1\nW 2ij .\nIn the above function, \u2016WTX\u2212X\u201622 is the most commonly used least square loss function and is mathematically tractable and easily implemented. However, there are still some existing issues which need to take into further consideration. For example, it is well known that the least square loss function is very sensitive to outliers [20]. To address this issue, it is important for us to adopt a more robust loss function in the objective. In [20], Nie etal. demonstrate that l2,1norm is more capable of dealing with the noisy data. Therefore, our proposed algorithm is rewritten as follows:\nmin rank(W )=k\n\u2016(WTX \u2212X)T \u20162,1 + \u03b1\u2016W\u20162,1 (14)\nIn the above formulation, the loss function \u2016WTX\u2212 X\u20162,1 is robust to outliers, as proven in [20]. Meanwhile, \u2016W\u20162,1 in the regularization term is guaranteed to make W sparse in rows. Next, we first give the definition of trace norm. The trace norm of W is defined as\n\u2016W\u2016\u2217 = Tr(WW T )\n1 2 . (15)\nFollowing the work in [13] [21], we restrict W to be a low rank matrix. To have the problem tackable, we propose to minimize the trace norm ofW , which is the convex hull of the rank of W . The objective function of the proposed algorithm is then given by:\nmin W\n\u2016(WTX \u2212X)T \u20162,1 + \u03b1\u2016W\u20162,1 + \u03b2\u2016W\u2016\u2217 (16)\nCompared with directly minimizing the rank of W , our proposed objective function as shown in (16) is convex. We therefore name the proposed algorithm\nconvex sparse PCA (CSPCA). Different from the previous robust PCA algorithms [13] [14], the proposed algorithm is inductive, and able to deal with the out-of-sample data which are unseen in the training phase. Given a new testing data point xt, we can get its low dimensional representation by WTxt directly."}, {"heading": "3.3 Optimization", "text": "As can be seen from Eq. (16), the proposed algorithm involves the l2,1-norm, which is non-smooth and cannot be solved in a closed form. Hence, we proposed to solve this problem as follows. For an arbitrary matrix A, we denote A = [A1, \u00b7 \u00b7 \u00b7 , Ad], where d is the number of features. By setting the derivatives w.r.t W to zero, we have\nXD1X TW + \u03b1D2W + \u03b2D3W = XD1X.\nThen we have\nW = (XD1X T + \u03b1D2 + \u03b2D3) \u22121(XD1X T ), (17)\nwhere D1, D2 and D3 are diagonal matrices defined as follows.\nD1 =   1 2\u2016e1\u20162\n. . . 1\n2\u2016ed\u20162\n ,\nwhere E = (WTX \u2212X)T .\nD2 =   1 2\u2016w1\u20162\n. . . 1\n2\u2016wd\u20162\n \nD3 = 1 2 (WW T )\u2212 1 2\nBased on the above mathematical deduction, we propose an iterative algorithm to optimize the objective function Eq. (16), which is summarized in Algorithm 1. In each iteration, E, D1, D2 and D3 are updated by the current W , and then W is updated based on the current calculated E, D1, D2 and D3. Once W is obtained and a new data point xi, we get the projected representation by computing WTxi. As the project matrix W is sparse, it actually assigned a weight to each feature dimension and thus can be used for feature analysis. The importance score of each feature can be computed by \u2016wi\u20162(1 \u2264 i \u2264 d). Then we can rank each feature according to this score. In this sense, W can be readily used for feature selection and we only select the top k features based on the score \u2016wi\u20162(1 \u2264 i \u2264 d)."}, {"heading": "3.4 Convergence Analysis", "text": "In this section, we validate Algorithm 1 shown above. Specially, we prove that the objective function value converges to the optimal W by the following theorem.\nTheorem 3. The objective function value shown in Eq. (16) monotonically decreases in each iteration until convergence using the iterative approach in Algorithm 1.\nAlgorithm 1: Algorithm to solve the problem in (16)\nData: Data matrix X Parameters \u03b1, \u03b2\nResult: W 1 Set t = 0 ; 2 Initialize W0 \u2208 R\nd\u00d7c randomly ; 3 repeat 4 Compute Et according to Et = (W T t X \u2212X)\nT ; 5 Compute the diagonal matrix D1t as follows:\nD1t =   1 2\u2016e1t\u20162\n. . . 1\n2\u2016edt \u20162\n  ;\n6 Compute the diagonal matrix D2t as follows:\nD2t =   1 2\u2016w1t \u20162\n. . . 1\n2\u2016wdt \u20162\n  ;\n7 Compute D3t according to\nD3t = 1\n2 (WtW\nT t ) \u2212 1 2 ;\n8 Update Wt+1 according to\nWt+1 = (XD1tX T +\u03b1D2t+\u03b2D3t) \u22121(XD1tX T );\n9 t = t+ 1 ; 10 until Converence; 11 Return W = Wt.\nProof: According to the 8th step of Algorithm 1, it can be safely inferred that:\nWt+1 = argmin Tr((W TX \u2212X)D1(W TX \u2212X))\n+ \u03b1Tr(WTD2W ) + \u03b2Tr(W TD3W )\nTherefore, we have:\nTr((WTt+1X \u2212X)D1(W T t+1X \u2212X))\n+ \u03b1Tr(WTt+1D2Wt+1) + \u03b2Tr(Wt+1D3Wt+1)\n\u2264Tr((WTt X \u2212X)D1(W T t X \u2212X))\n+ \u03b1Tr(WTt D2Wt) + \u03b2Tr(WtD3Wt)\n\u21d2\nn\u2211\ni=1\n\u2016WTt+1xi \u2212 xi\u2016 2 2\n2\u2016WTt+1xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit+1\u2016 2 2\n2\u2016wit\u20162\n+ \u03b2\n2 Tr(WTt+1(WtW T t ) \u2212 1 2Wt+1)\n\u2264 n\u2211\ni=1\n\u2016WTt xi \u2212 xi\u2016 2 2\n2\u2016WTt xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit\u2016 2 2\n2\u2016wit\u20162\n+ \u03b2\n2 Tr(WTt (WtW T t ) \u2212 1 2Wt)\n\u21d2\nn\u2211\ni=1\n\u2016WTt+1xi \u2212 xi\u20162 \u2212\nn\u2211\ni=1\n\u2016WTt+1xi \u2212 xi\u20162\n+ \u2016WTt+1xi \u2212 xi\u2016 2 2\n2\u2016WTt+1xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit+1\u20162 \u2212 \u03b1 d\u2211\ni=1\n\u2016wit+1\u20162\n+ \u03b1\nd\u2211\ni=1\n\u2016wit+1\u2016 2 2\n2\u2016wit\u20162 +\n\u03b2 2 Tr((Wt+1W T t+1) 1 2 )\n\u2212 \u03b2\n2 Tr((Wt+1W\nT t+1)\n1 2 ) + \u03b2\n2 Tr(WTt+1(WtW T t ) \u2212 1 2Wt+1)\n\u2264\nn\u2211\ni=1\n\u2016WTt xi \u2212 xi\u20162 \u2212\nn\u2211\ni=1\n\u2016WTt xi \u2212 xi\u20162 + \u2016WTt xi \u2212 xi\u2016 2 2\n2\u2016WTt xi \u2212 xi\u20162\n+ \u03b1\nd\u2211\ni=1\n\u2016wit+1\u20162 \u2212 \u03b1\nd\u2211\ni=1\n\u2016wit\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit\u2016 2 2\n2\u2016wit\u20162\n+ \u03b2\n2 Tr((WtW\nT t )\n1 2 )\u2212 \u03b2\n2 Tr((WtW\nT t )\n1 2 )\n+ \u03b2\n2 Tr(WTt (WtW T t ) \u2212 1 2Wt)\n\u21d2\nn\u2211\ni=1\n\u2016WTt+1xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit+1\u20162 + \u03b2\n2 Tr((WtW\nT t )\n1 2 )\n\u2212 \u03b1( d\u2211\ni=1\n\u2016wit+1\u20162 \u2212 d\u2211\ni=1\n\u2016wit+1\u2016 2 2\n2\u2016wit\u20162 )\n\u2212 \u03b2\n2 (Tr((Wt+1W\nT t+1)\n1 2 )\u2212 Tr(WTt+1(WtW T t ) \u2212 1 2 )Wt+1))\n\u2264\nn\u2211\ni=1\n\u2016WTt xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit\u20162 + \u03b2\n2 Tr((WtW\nT t )\n1 2 )\n\u2212 \u03b1( d\u2211\ni=1\n\u2016wit\u20162 \u2212 d\u2211\ni=1\n\u2016wit\u2016 2 2\n2\u2016wit\u20162 )\n\u2212 \u03b2\n2 (Tr((WtW\nT t )\n1 2 )\u2212 Tr(WTt (WtW T t ) \u2212 1 2 )Wt))\nIt has been proven in [20] that for arbitrary non-zero vectors vit| r i=1 we have:\n\u2211\ni\n\u2016vit+1\u20162 \u2212 \u2211\ni\n\u2016vit+1\u2016 2 2\n2\u2016vit\u20162 \u2264\n\u2211\ni\n\u2016vit\u20162 \u2212 \u2211\ni\n\u2016vit\u2016 2 2\n2\u2016vit\u20162 ,\nwhere r is any non-zero number. Thus, we can obtain the following inequality:\nn\u2211\ni=1\n\u2016WTt+1xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit+1\u20162 + \u03b2\n2 Tr((Wt+1Wt+1)\n1 2 )\n\u2264\nn\u2211\ni=1\n\u2016WTt xi \u2212 xi\u20162 + \u03b1\nd\u2211\ni=1\n\u2016wit\u20162 + \u03b2\n2 Tr((WtWt)\n1 2 )\n\u21d2\u2016WTt+1X \u2212X\u20162,1 + \u03b1\u2016Wt+1\u20162,1 + \u03b2\u2016Wt+1\u2016\u2217\n\u2264\u2016WTt X \u2212X\u20162,1 + \u03b1\u2016Wt\u20162,1 + \u03b2\u2016Wt\u2016\u2217\nwhich indicates that the objective function value of Eq. (16) monotonically decreases until converging to\nthe optimal W via the proposed approach in Algorithm 1. To step further, we prove that the proposed algorithm converges to the global optima by Theorem 4.\nTheorem 4. The objective function value shown in Eq. (16) converges to the global optima using Algorithm 1.\nProof:Once the objective function converges using algorithm 1 and returns W \u2217. According to Eq. (17), we can get the following equation:\nXD1X TW \u2217 + \u03b1D2W \u2217 + \u03b2D3W \u2217 \u2212XD1X = 0\nWe can see that the derivatives w.r.t W equals to zero, and we get the local solution to the objective function. Note that the proposed method is a convex problem. Hence, according to the Karush-Kuhn-Tucker (KKT) conditions, we conclude that the objective function converges to the global optima using Algorithm 4."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we evaluate performance of the proposed algorithm, which can be applied to many applications, such as dimension reduction and unsupervised feature selection. Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection."}, {"heading": "4.1 Experimental Settings", "text": "To demonstrate the effectiveness of the proposed algorithm for feature selection, we compare it with one baseline and several unsupervised feature selection methods. The compared algorithms are described as follows.\n1) Using all features (All-Fea): We directly adopt the original features without performing feature selection. This approach is used as a baseline. 2) Max Variance: This is a feature selection method using the classical PCA criteria. Features with maximum variances are chosen for subsequent tasks. 3) Laplacian Score: To best preserve the local manifold structure, feature consistent with Gaussian Laplacian matrix are selected [22]. The importance of each feature is determined by its power. 4) SPEC: This is a spectral regression based stateof-the-art feature selection algorithm. Features are selected one by one by leveraging the work of spectral graph theory [23]. 5) MCFS: Features are selected based on spectral analysis and sparse regression problem [25]. Specifically, features are selected such that the multi-cluster structure of the data can be best preserved. 6) UDFS: Features are selected by a joint framework of discriminative analysis and l2,1-norm\nminimization [24]. UDFS selects the most discriminative feature subset from the whole feature set in batch mode.\nFor each algorithm, all the parameters (if any) are tuned in the range of {10\u22126, 10\u22124, 10\u22122, 100, 102, 104, 106} and the best results are reported. There are some parameters need to be set in advance. For LS, MCFS and UDFS, we empirically set k = 5 for all the datasets to specify the size of neighborhoods. The number of selected features are set as described in Table 1 for all the datasets. For all the compared algorithms, we report the best clustering result with optimal parameters. In the experiments, we utilize K-means algorithm to cluster samples based on the selected features. Note that performance of K-means varies with different initializations. We randomly repeat the clustering 30 times for each setup and report average results with standard deviation."}, {"heading": "4.2 Datasets", "text": "The datasets used in our experiments are described as follows.\n1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28]. The YaleB dataset contains 2414 near frontal images from 38 persons under different illuminations. We resize each image to 32\u00d732. The ORL dataset consists of 40 different subjects with 10 images each. We also resize each image to 32\u00d732. The Japanese Female Facial Expression (JAFFE) dataset consists of 213 images of different facial expressions from 10 Japanese female models. The images are resized to 26\u00d7 26. 2) 3DMotion Data: The HumanEVA dataset is used to evaluate the performance of our algorithm in terms of 3D motion annotation 1. This dataset contains five types of motions. Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [29] to represent 3D motion data. 3) Object Image Data: We use the Coil20 dataset [30] for object recognition. This dataset includes 1440 grey scale images with 20 different objects. In our experiment, we resize each image to 32\u00d7 32. 4) Handwritten Digit Data: We use the USPS dataset to validate the performance on handwritten digit recognition. The dataset consists of 9298 gray-scale handwritten digit images. We resize the images to 16\u00d7 16."}, {"heading": "4.3 Evaluation Metrics", "text": "Following related unsupervised feature selection work [22] , we adopt clustering accuracy (ACC) and\n1. http://vision.cs.brown.edu/humaneva/\nnormalized mutual information (NMI) as our evaluation metrics in our experiments.\nLet qi represent the clustering label result from a clustering algorithm and pi represent the corresponding ground truth label of arbitrary data point xi. Then ACC is defined as follows:\nACC =\n\u2211n i=1 \u03b4(pi,map(qi))\nn , (18)\nwhere \u03b4(x, y) = 1 if x = y and \u03b4(x, y) = 0 otherwise. map(qi) is the best mapping function that permutes clustering labels to match the ground truth labels using the Kuhn-Munkres algorithm. A larger ACC indicates a better clustering performance.\nFor any two arbitrary variable P and Q, NMI is defined as follows [31]:\nNMI = I(P,Q)\u221a H(P )H(Q) , (19)\nwhere I(P,Q) computes the mutual information between P and Q, and H(P ) and H(Q) are the entropies of P and Q. Let tl represent the number of data in the cluster Cl(1 \u2264 l \u2264 c) generated by a clustering algorithm and t\u0303h represent the number of data points from the h-th ground truth class. NMI metric is then computed as follows [31]:\nNMI =\n\u2211c l=1 \u2211c h=1 tl,hlog( n\u00d7tl,h tl t\u0303h\n) \u221a ( \u2211c l=1 tl log tl n )( \u2211c h=1 t\u0303h log t\u0303h n ) , (20)\nwhere tl,h is the number of data samples that lies in the intersection between Cl and hth ground truth class.\nSimilarly, a larger NMI indicates a better clustering performance."}, {"heading": "4.4 Experimental Results", "text": "Empirical studies are conducted on six real-world data sets to validate the performance of the proposed algorithm and compare to state-of-the-art algorithms. Table 2 and Table 3 summarise ACC and NMI comparison results of all the compared algorithms over the used datasets. From the experimental results, we have the following observations.\n1) The feature selection algorithms generally have better performance than the baseline All-Fea, which demonstrates that feature selection is necessary and effective. It can significantly reduce feature number as well as improve the performance. 2) Both SPEC and MCFS utilize a two-step approach (spectral regression) for feature selection. The difference between them is MCFS select features in a batch mode but SPEC conduct this task separately. We can see MCFS gets better results than SPEC because it is a better way to analyze features jointly for feature selection. 3) We can see from the result tables that UDFS gains the second best result, which indicates that it is beneficial to analyze features jointly and simultaneously adopt discriminative information and local structure of data distribution. 4) From the experimental results, we can observe that the proposed CSPCA consistently outperform the other compared algorithms. This phenomenon demonstrate that the proposed algorithm is able to select the most informative features."}, {"heading": "4.5 Influence of Selected Features", "text": "As the goal of feature selection is to boost accuracy and computation efficiency, experiments are conducted to learn how the number of selected features can affect the clustering performance. From these experiments we can see the general trade-off between performance and computational efficiency over all the used dataset. Fig. 1 shows the performance variance with right to the number of selected features in terms of clustering ACC. From the results, we have the following observations:\n1) When the number of selected features is too small, the clustering ACC is not competitive with using all features without feature selection, which is mainly caused by too much information loss. For example, when only 500 features are selected on YaleB, the clustering ACC is relatively low, at only 0.164. 2) As the number of selected features increases, the clustering ACC rises before its peak in general\non all the used datasets. How many features are selected to get the peak level is different on different datasets. 3) The trend of clustering ACC are varying when different datasets are used. For example, the clustering ACC keeps stable from using 800 features to using 1000 features for YaleB while drops for the other used datasets. The different variance shown on the six datasets are supposed to be related to the properties of the datasets. 4) After all the features are used (without feature selection), the clustering ACC are generally lower than the peak level on all the datasets. We can safely conclude that as the clustering ACC increases, the proposed algorithm is capable of reducing noise and selecting the most discriminating features."}, {"heading": "4.6 Parameter Sensitivity", "text": "Our proposed algorithm involves two regularization parameters, which are denoted as \u03b1 and \u03b2 in Eq. (16). It is beneficial to learn how they influence the feature selection and consequently the performance on clustering. In this section, we conduct several experiments on the parameter sensitivity. We use the clustering ACC to reflect the performance variation.\nFig. 2 demonstrates the clustering ACC variation w.r.t \u03b1 and \u03b2 on the six datasets. From this figure, we learn that the clustering performance changes corresponding to different combinations of \u03b1 and \u03b2. The impact of different combinations of regularization parameters are supposed to be related to the individual properties of the datasets. On the used datasets, we can observe that better experimental results are obtained when the two regularization parameters \u03b1 and \u03b2 are comparable."}, {"heading": "4.7 Performance Variance w.r.t Different Initializations", "text": "In this section, experiments are conducted to evaluate how performance varies when performance variance w.r.t different initializations. Clustering ACC is also used to reflect the performance variation. The Kmeans algorithm has adopted the same initialization. We conduct different initializations, including setting all the diagonal elements of W to 0.5 (1st initialization), 1 (2nd initialization), 2 (3rd initialization), setting all the elements of W to 0.5 (4th initialization), 1 (5th initialization), 2 (6th initialization) and random values (7th initialization). The experimental results are shown in Table 4.\nFrom the experimental results, we can observe that the proposed algorithm always obtains global optima w.r.t different initializations."}, {"heading": "4.8 Convergence Study", "text": "In the previous section, we have proven that the objective function in Eq. (16) monotonically decreases by using the proposed algorithm. It is interesting to learn how fast our algorithm converges. In this section, we conduct several experiments on validate the convergence of the proposed algorithm. We fix the\ntwo regularization parameters \u03b1 and \u03b2 at 1, which is the median value of the range from which the regularization parameters are tuned. Fig. 3 shows the convergence curves of the proposed algorithm according to the objective function value in Eq. (16). From these figures, we can observe that the objective function value converges quickly. To be more specific, the proposed algorithm can converge within 10 iterations on all the used datasets, which is very efficient."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have proposed a novel convex sparse PCA and applied it to feature analysis. We first prove that PCA can be formulated as a low-rank regression\noptimization problem. We further incorporate the l2,1norm minimization into the proposed algorithm to make the regression coefficients sparse and make the model robust to the outliers. Different from state-ofthe-art robust PCA, the proposed algorithm is capable of solving out-of-sample problems. Additionally, we propose an efficient algorithm to optimize the objective function.\nTo validate the performances of our algorithm for feature analysis, we conduct experiments on six realworld datasets on clustering. It can be seen from the experimental reuslts that the proposed algorithm outperforms the other state-of-the-art unsupervised feature selection as well as the baseline using all features. Therefore, we conclude that the proposed algorithm is a robust sparse feature analysis method,\nand its benefits make it especially suitable for feature selection."}], "references": [{"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Trans. PAMI, vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A novel incremental principal component analysis and its application for face recognition", "author": ["P.C.Y. Haitao Zhao", "J. Kwok"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Part B (Cybernetics), 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "On effective conceptual indexing and similarity search in text data", "author": ["C.C. Aggarwal", "P.S. Yu"], "venue": "Proc. ICDM, 2001, pp. 3\u201310.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-task learning for bayesian matrix factorization", "author": ["C. Yuan"], "venue": "Proc. ICDM, 2011, pp. 924\u2013931.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning linear discriminant projections for dimensionality reduction of image descriptors", "author": ["H. Cai", "K. Mikolajczyk", "J. Matas"], "venue": "Trans. PAMI, pp. 338\u2013352, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for feature selection in clustering", "author": ["D.M. Witten", "R. Tibshirani"], "venue": "Journal of the American Statistical Association, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualization of learning in multilayer perceptron networks using principal component analysis", "author": ["M. Gallagher", "T. Downs"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Part B (Cybernetics), 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265\u2013286, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301\u2013320, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust principal component analysis by self-organizing rules based on statistical physics approach", "author": ["L. Xu", "A.L. Yuille"], "venue": "IEEE Trans. Neural Networks, vol. 6, no. 1, pp. 131\u2013143, 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization", "author": ["J. Wright", "Y. Peng", "Y. Ma", "A. Ganesh", "S. Rao"], "venue": "Proc. NIPS, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "Proc. NIPS, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L.E. Ghaoui", "M.I. Jordan", "G.R. Lanckriet"], "venue": "SIAM review, vol. 49, no. 3, pp. 434\u2013448, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Full regularization path for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F.R. Bach", "L.E. Ghaoui"], "venue": "Proc. ICML, 2007, pp. 177\u2013184.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Proc. NIPS, 2005, pp. 915\u2013922.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics, vol. 12, no. 3, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["H. Shen", "J.Z. Huang"], "venue": "Journal of multivariate analysis, vol. 99, no. 6, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Proc. NIPS, 2010, pp. 1813\u20131821.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Proc. NIPS, 2005.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "Proc. ICML, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "l2,1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "Proc. AAAI, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "Proc. ACM KDD, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. PAMI, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Proc. Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "Automatic classification of single facial images", "author": ["M.J. Lyons", "J. Budynek", "S. Akamatsu"], "venue": "IEEE Trans. PAMI, vol. 21, no. 12, pp. 1357\u20131362, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning a 3d human pose distance metric from geometric pose descriptor", "author": ["C. Chen", "Y. Zhuang", "F. Nie", "Y. Yang", "F. Wu", "J. Xiao"], "venue": "IEEE Trans. Visualization and Computer Graphics, vol. 17, no. 11, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "CUCS-005-96, Columbia University, Tech. Rep., 1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "integrate the lasso penalty [8], which is a variable selection technique, into the regression criterion in [9].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "integrate the lasso penalty [8], which is a variable selection technique, into the regression criterion in [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "Lasso penalty is implemented via elastic net, which is a generalization of lasso proposed in [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "[12] propose to recover a low-rank matrix from highly corrupted measurements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Therefore, the robust PCA algorithm proposed in [12] is less practical for many real world", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "It has been demonstrated in [9] that the sparse model is a good measure for feature analysis, especially for feature weighting.", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "Different from [13], our algorithm is inductive and can be directly used to map the unseen data which are outside the training set.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "3) Different from the existing robust PCA algorithms [13] [14], which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "3) Different from the existing robust PCA algorithms [13] [14], which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "propose SCoTLASS to obtain modified principal components with possible zero factor coefficients [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "Lasso [8] has shown to be a effective variable selection method, which has been shown effective in a variety of applications.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "propose the elastic net in [10] for sparsity based mining.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "[9] propose sparse PCA (SPCA) for estimating PCs with sparse factor coefficients, which can be formulated as follows:", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[13] propose exact recovery of corrupted low-rank matrices by convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "To make the objective function robust to outliers, we further replace l1-norm with l2,1-norm as l2,1-norm is indicated to make the objective function robust to outliers in [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "The connection between the stated Theorem 1 and Theorem 2 in [9]: Zou etal.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Compared with Theorem 2 proposed in [9], our contribution is that we prove that when \u03bb = 0, PCA problem is completely equivalent to a regression-type problem.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Motivated by previous work [20], which demonstrate that l2,1-norm of W is capable of making W sparse, we propose our sparse PCA algorithm as follows:", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "For example, it is well known that the least square loss function is very sensitive to outliers [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "In [20], Nie etal.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In the above formulation, the loss function \u2016WX\u2212 X\u20162,1 is robust to outliers, as proven in [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Following the work in [13] [21], we restrict W to be a low rank matrix.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Different from the previous robust PCA algorithms [13] [14], the proposed algorithm is inductive, and able to deal with the out-of-sample data which are unseen in the training phase.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "Different from the previous robust PCA algorithms [13] [14], the proposed algorithm is inductive, and able to deal with the out-of-sample data which are unseen in the training phase.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "It has been proven in [20] that for arbitrary non-zero vectors v t| r i=1 we have: \u2211", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "3) Laplacian Score: To best preserve the local manifold structure, feature consistent with Gaussian Laplacian matrix are selected [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Features are selected one by one by leveraging the work of spectral graph theory [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "5) MCFS: Features are selected based on spectral analysis and sparse regression problem [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "6) UDFS: Features are selected by a joint framework of discriminative analysis and l2,1-norm minimization [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [29] to represent 3D motion data.", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "3) Object Image Data: We use the Coil20 dataset [30] for object recognition.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "3 Evaluation Metrics Following related unsupervised feature selection work [22] , we adopt clustering accuracy (ACC) and", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "For any two arbitrary variable P and Q, NMI is defined as follows [31]: NMI = I(P,Q) \u221a H(P )H(Q) , (19)", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "NMI metric is then computed as follows [31]:", "startOffset": 39, "endOffset": 43}], "year": 2014, "abstractText": "Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l2,1-norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}