{"id": "1412.0100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2014", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images", "abstract": "State-of-the-art visual recognition systems are increasingly relying on large amounts of training data and complex classifiers, making it increasingly expensive to both comment on data sets manually and maintain runtimes at levels acceptable for practical applications. In this article, we propose two solutions to address these problems: First, we introduce a weakly monitored, segmentation-based approach to learn accurate detectors and image classifiers from weak surveillance signals that offer only approximate limitations on target localization. We illustrate our system to the problem of action detection in static images (Pascal VOC Actions 2012) by using human visual search patterns as a training signal. Second, inspired by the sacred and fixed functional principle of the human visual system, we use enhanced learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finding optimal search strategies for a familiar part, and achieving a search function.", "histories": [["v1", "Sat, 29 Nov 2014 12:18:14 GMT  (3284kb,D)", "http://arxiv.org/abs/1412.0100v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["stefan mathe", "cristian sminchisescu"], "accepted": false, "id": "1412.0100"}, "pdf": {"name": "1412.0100.pdf", "metadata": {"source": "CRF", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images", "authors": ["Stefan Mathe", "Cristian Sminchisescu"], "emails": ["stefan.mathe@imar.ro,", "cristian.sminchisescu@math.lth.se"], "sections": [{"heading": "1. Introduction", "text": "The central problem of constructing an object detector can be decomposed into learning a confidence detection response function and estimating a search strategy. Most frequently, confidence functions are learned in a fully supervised setting, and the search is performed exhaustively. This approach is however not well suited to contemporary stateof-the-art systems, which are trained using large amounts of image or video data, and require complex multi-layer models[14]. First, manually annotating the large amounts of data needed by supervised algorithms is increasingly expensive. This underlines the need to exploit alternative sources\nof information to support system accuracy and efficiency. Second, the target search complexity is sometimes high in many practical object detection applications.\nHuman eye movements can provide a rich source of supervision which, due to recent developments in eyetracking hardware, is increasingly less invasive and expensive. Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20]. Such data has been exploited at training time to support visual classification performance in video[17], but its usefulness as a training signal for detection, in the absence of any additional annotations, remains unexplored. Nor has detection performance benefited from insights derived from the \u2018saccade-and-fixate\u2019 operating principles of the human visual system.\nIn this paper, we propose general methods to learn detector confidence functions and search models from weak supervisory signals, circumventing the need for manual image annotations. Our main contributions are:\n\u2022 We demonstrate a novel segmentation-based constrained multiple instance model to learn effective action detectors in static images. Our method is trained using image labels and associated eye movement data and requires no ground truth annotations of the spatial extent of the targets, e.g. bounding boxes or image segments (see \u00a75 and tables 2,3,4).\n\u2022 We show, for the first time, that human eye movement information can be integrated with image labels to train more accurate image classification pipelines. The performance of our system approaches that of pipelines trained under the stronger supervision of target bounding box information (see \u00a75 and table 4).\n\u2022 We develop a weakly supervised reinforcement learning methodology to obtain optimal sequential models that operate in a fixate-and-saccade regime. Our method achieves significant improvements in search\n1\nar X\niv :1\n41 2.\n01 00\nv1 [\ncs .C\nV ]\n2 9\nN ov\nefficiency at virtually the same detection and classification accuracy as exhaustively evaluated sliding window methods (see \u00a76 and tables 2,3,4,5)."}, {"heading": "2. Related Work", "text": "Many methods have been proposed to accelerate detectors. Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28]. In turn, different features have been used in the design of the detector response functions. Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24]. Multiple instance learning[8] formulations can be seen as a generalization of supervised learning, in which class labels are assigned to sets of training examples. Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).\nDatasets of human eye movement have been collected for both images and video. Search targets include pedestrians[9], faces [7] and actions[17, 18, 20]. Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21]. Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22]. Human eye movements have also been recently used to learn object detectors by Papadopoulos et al.[20]. Their method employs supervised learning techniques to predict a bounding box given the human\u2019s scanpath, then reduces to a classical supervised detection learning problem, in which the predicted bounding boxes play the role of ground truth annotations. However, ground truth bounding box information must still be available, at least for a subset of the training images. This can be a limiting factor in many practical applications. In this work, we take a markedly different approach. Instead of splitting the problem into two learning steps, we treat it as a joint optimization problem, recovering both the ground truth regions and the confidence function in a single stage. Unlike [20], our method does not require the availability of ground truth bounding boxes at training time.\nVisual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18]. Here, we derive novel, fully trainable models for two challenging problems, namely action recognition and detection in cluttered natural scenes, and in a reinforcement optimal learning setup[23]."}, {"heading": "3. Action Detection in Static Images", "text": "Given an image showing one or more persons, we want to determine both the actions present (from a predefined list) and their corresponding spatial support. The spatial support is more difficult to objectively define for an action, than it is for an object. Here, we consider the spatial support of an action to be a tight bounding box that encloses both the human actor and the additional objects the action requires. For example, the spatial support for an instance of the action playing guitar is a bounding box that tightly encloses both the artist and the instrument (see fig. 1).\nWe are not aware of any publicly available dataset suited for this problem. While the Pascal VOC Action Classification Challenge[10] contains a large set of images with human actions, the provided annotations are not useful for action detection for several reasons. First, not all action instances are annotated. Second, many images contain crowds of people performing actions (e.g. images from running competitions), with many overlapping targets. These images are incompletely annotated and make the evaluation of an action detector subjective. Third, not all actions are suitable for detection given the image statistics: the extent of many actions is often the entire image, making spatial localization uninteresting (e.g. playing piano or using desktop computer).\nWe address this problem by providing our own complete annotations for a subset of the PASCAL VOC Actions 2012 database1. Our dataset contains 4412 images and 10 action classes: jumping, phoning, playing guitar, reading, riding bike, riding horse, running, walking and using laptop. Note that our action classes are slightly different than in the PASCAL VOC Actions dataset. We have specialized the classes playing instrument and using computer, to keep the extent of the action within bounds that make detection non-trivial. We have also removed all images containing crowds that perform the same action, but are only sparsely annotated.\n1Our annotations will be made publicly available upon publication."}, {"heading": "4. Problem Formulation", "text": "Given an input image, we formulate action detection as the problem of maximizing a confidence function fc : R\u2192 R over the set of image regions R:\nr\u2217 = argmax r\u2208R fc(r) (1)\nThe set of image regions R can be defined either at the coarse level of bounding boxes or at the finer level of image segments. In the present work, the region space R consists of all segments extracted using the CPMC algorithm[6]. There are two reasons that motivate our choice. First, the space of image segments is several orders of magnitude smaller than the one of bounding boxes. Second, figureground image segments produced by CPMC can be mapped with reasonable accuracy to objects or groups of related objects that represent human actions (see table 1, column 3). Furthermore, as shown in \u00a75.1, humans often fixate segments that well match the target during visual search. Hence, for action detection, image segments provide a good tradeoff between computational complexity and spatial descriptive power.\nTypically, the confidence function fc is learnt from a training set of images annotated with ground truth regions containing the target instances (usually bounding boxes). This type of annotation is however expensive to obtain in practice. An alternative, cheaper \u2018annotation\u2019 consists of sequences of eye movements made by human subjects searching for the target, together with a label indicating the presence or absence of the target class in images. In \u00a75, we present a method to learn the confidence function fc from this information only, without the need for region-level annotations.\nWhen looking at an image, the human visual system exhibits efficient search strategies that only explore a few locations before deciding on the presence of a target. When asked to locate an action, humans take on average only 3.6 fixations (stdev=1.8) in the PASCAL VOC Actions 2012 dataset. In \u00a76 we present a model to learn efficient search strategies in an weakly supervised setting, optimally formulated in a reinforcement learning setup. Our model operates in a fixate-and-saccade regime, inspired from the workings of the human visual system."}, {"heading": "5. Learning Confidence Functions using Eye", "text": "Movement Annotations\nIn the classical (fully supervised) approach, the confidence function fc is the response of a classifier, trained to separate ground truth image regions from a set of regions having little or no overlap with the positive region set. In this section we consider the more challenging setup in which no ground truth image regions are available at\ntraining time. Instead, our training signal consists of image labels \u2013 indicating the presence or absence of an action in the image \u2013 and eye movements recorded from human subjects asked to locate instances of the target action (see fig. 1). Such data has been recently acquired and made publicly available[18].\nIn \u00a75.1 we begin with a brief analysis of the localization power of human eye movements. In \u00a75.2, we describe our method in detail. We evaluate the model and discuss experimental results in \u00a77.1."}, {"heading": "5.1. Human Fixations and Action Localization", "text": "The sequence of fixations and saccades made by a human subject is precisely registered with the image but contains weaker information on the location and extent of the search target compared to a bounding box or a segment. Consider for example the case of a human presented with an image of a concert scene where one of the artists is playing a guitar. Typically the last few fixations fall onto relevant parts of the target, i.e. the person and the musical instrument. However, the remaining part of the scanpath is most likely focused on the background and other objects or humans.\nHow weak is the localization signal provided by human fixations? Table 1 lists the percentage of fixated image segments in the Pascal VOC Actions 2012 dataset. It also shows the maximum overlap between a fixated image segment and the ground truth bounding box. Both figures are averaged over all images belonging to an action class. The analysis suggests that, while fixations do not give precise information on the location of the search target, they can serve as a strong filtering signal over the entire segment pool."}, {"heading": "5.2. Method description", "text": "Based on the analysis in \u00a75.1, we develop a multiple instance learning formulation, in which training instances\nare the image regions ri obtained by a segmentation algorithm on the training set, referenced in a global index set I . In a one-vs-all formulation, training image with index j \u2208 J is represented by (1) a bag Bj \u2282 I of region indexes belonging to image j and (2) an associated image label lj \u2208 {\u22121, 1}.\nA positive bag Bj corresponds to an image where the target action is present and contains all regions fixated by humans viewing that image. Some of these regions represent the detection target, and some do not. Negative bags Bj correspond to images where the action is absent, and contain all regions in that image (fixated or not). We know that none of these regions represents the detection target.\nWe wish to simultaneously find the optimal assignments yi of labels to instances and the optimal separating hyperplane for the positive and negative instances under this assignment, subject to several constraints: Positivity constraints: There should be at least one positive instance inside each positive bag, and no positive instances inside negative bags. Inclusion constraints: Positivity constrains implicitly encourage good separation of positive bags from negative bags, under their maximum classifier response. This does not imply, however, good localization performance.\nConsider, for example, a classifier that responds to the presence of some pattern strongly associated to the target, anywhere inside the input region. This classifier cannot distinguish among image regions containing the target. It will separate positive and negative bags well, but will have poor localization capabilities. We deal with this issue by imposing that, for any region that is positively labeled, all its subregions S(r) be labeled negative. We define the set S(r) using a soft threshold criterion:\nS(r) = {r\u2032 s.t. |r \u2229 r\u2032| / |r\u2032| \u2264 TS} (2) Unconstrained fringe: Previous experience with training sliding window detectors[19] has shown that excluding bounding boxes overlapping the ground truth from the set of negative examples can greatly improve the localization power. We define the fringe F(r) of region r as the set of regions that overlap r, down to a threshold: F (r) = {r\u2032 s.t. (r\u2032 6= r) \u2227 (|r \u2229 r\u2032| / |r \u222a r\u2032| \u2265 TF )} (3)\nIn our formulation, regions in the fringe of instances selected as positive do not affect the separating hyperplane and their corresponding labels are constrained to be 0.\nThis leads to the following formulation:\nmin yi min w,b,\u03be\n1 2 \u2016w\u20162 + C \u2211 i \u03bei (4)\ns.t. \u2200i \u2208 I : yi \u2208 {\u22121, 0, 1} (4.1) \u2200i \u2208 I : yi = 1 =\u21d2 (\u2200k) \u2208 F (ri), yk = 0 (4.2) \u2200i \u2208 I : yi = 1 =\u21d2 (\u2200k) \u2208 S(ri) \\ F (ri), yk = \u22121 (4.3) \u2200i \u2208 I : yi 6= 0 =\u21d2 yi ( wT g (ri) + b ) \u2265 1\u2212 \u03bei, \u03bei > 0 (4.4)\n\u2200j \u2208 J : lj = 1 =\u21d2 (\u2203) i \u2208 Bi s.t. yi = 1 (4.5)\n\u2200j \u2208 J : lj = \u22121 =\u21d2 (\u2200)i \u2208 Bj , yi = \u22121 (4.6)\nwhere g : R \u2192 Rn is a feature extractor that takes as input an image region.\nNote that if we remove constraints (4.2), (4.3), and the value 0 for the range of possible image labels from constraint (4.1), we end up with the miSVM formulation[3], which treats all training instances as independent from one another.\nAlgorithm 1 Constained Multiple Instance SVM learning 1: procedure CMI-SVM({ri}i\u2208I , {Bj , lj}j\u2208J ) 2: yi \u2190 rand({\u22121, 1}) 3: repeat 4: (w, b)\u2190 SVM({ri, yi}yi 6=0) 5: for j \u2208 J do 6: if lj = 1 then 7: V \u2190 Bj ; npos \u2190 0 8: repeat 9: r \u2190 maxi\u2208V wT g (ri) 10: k \u2190 argmaxi\u2208V wT g (ri) 11: if r > 0 then 12: yk \u2190 1; npos \u2190 npos + 1 13: yi \u2190 \u22121, for each i \u2208 S(rk) 14: yi \u2190 0, for each i \u2208 F (rk) 15: V \u2190 V \\ (F (rk) \u222a S(rk)) 16: else 17: yk \u2190 0 18: V \u2190 V \\ {k} 19: end if 20: until V = \u2205 21: if npos = 0 then 22: k \u2190 argmaxi\u2208Bj wT g (ri) 23: yk \u2190 1 24: end if 25: else 26: yi \u2190 \u22121, for each i \u2208 Bj 27: end if 28: end for 29: until y has not changed during the iteration 30: return w, b,y 31: end procedure\nWe solve the optimization problem (4) using an iterative procedure, presented in Algorithm 1. We alternate between finding the SVM parameters w and b for the current assignment estimate y (line 4), and determining y given parameters, subject to imposed constraints (lines 5-28).\nFor each bag j with positive label, we iteratively look for a yet unlabeled region with maximal positive SVM response (lines 9-10). If found, we label it 1, its fringe 0 and all its subregions \u22121 (lines 11-15). Otherwise, all remaining regions are labeled as negative (lines 16-19). If no posi-\ntive SVM responses are generated inside a positive bag, we enforce consistency by labeling the region with maximal response as 1 (lines 21-24). Instances in negative bags are always labeled negative (lines 25-27). We detect convergence when bag labels no longer change between iterations."}, {"heading": "6. Weakly Supervised Reinforcement Learning", "text": "of Search Strategies for Detection\nEquation (1) involves an exhaustive search over the entire set R image regions in the test image. In this section, we propose a weakly supervised model that aims to minimize the computational load needed to locate the target, by restricting the search to a subset of R. We present our model in \u00a76.1. The methodology and experimental results are discussed in \u00a77.2.\nAlgorithm 2 Policy sampling algorithm. 1: procedure SAMPLE (st = (Ht, St)) 2: ct \u2190 maxi\u2208Ht fc(ri) 3: dt \u223c p(dt|st) using (6) 4: if dt = 1 then 5: k \u2190 argmaxi\u2208Ht fc(ri) 6: Done. Predict region rk with confidence ct. 7: else 8: et \u223c p(et|st) using (7) 9: zt \u223c p(zt|st, et) using (8) 10: return at = (et, zt) 11: end if 12: end procedure\nAlgorithm 3 State transition algorithm. 1: procedure OBSERVE (st = (Ht, St) , at = (et, zt)) 2: Ot \u2190 {i \u2208 B | zt \u2208 ri} 3: return st+1 = (Ht \u222aOt, St \u222a {et}) 4: end procedure"}, {"heading": "6.1. Saccade-and-Fixate Model for Detection", "text": "When faced with a recognition task, the human visual system has evolved eye movements to sequentially sample promising image regions through an alternation of saccades and fixations. Inspired by the exceptional, yet unmatched, performance of this biological system, we develop a principled optimal sequential model for image exploration.\nAt each time step t, based on the information gathered so far, our model proposes an image location zt that should be sampled. Then, the model updates its internal state st based on information contained in the image regions in the neighbourhood of the sampled location. At each time step, the model may terminate the search. In this case, it returns its confidence for the target\u2019s presence in the image, together with a region hypothesis for spatial support.\nWe now describe in detail the state and action spaces of our model (fig. 2). Model description: At each time step t, the model keeps track of a set Ht \u2286 R of image regions observed so far, deciding on the appropriate action to take in three steps: Termination decision: The model may decide to terminate search (dt = 1). This decision is based on a feature vector of four elements, which consists of the maximum confidence over the set of regions visited so far, the number of saccades already generated, the number of image regions observed so far and a bias:\nv (st) = [ max r\u2208Ht fc(r) t \u2016Ht\u2016 \u2016R\u2016 1 ]> (5)\nThe search termination probability is given by a soft threshold applied to a linear combination of these features, with learned weights \u03b8d:\np\u03b8(dt = 1|st) = sigm [ \u03b8>d v (st) ] (6)\nwhere sigm(x) = (1 + e\u2212x)\u22121 is the sigmoid function. Evidence selection: If the search is not terminated (dt = 0), the model selects an observed image region et \u2208 (Ht \\ St) that provides clues for the location of the target. For example, an image region consisting of the upright torso of a person found in the upper half of the image may offer cues for the presence of a horse in the bottom part.\nTo capture such effects, we define an evidence function fe : B \u2192 R, fe (i) = exp ( \u03b8>e g(ri) ) that evaluates the informativeness of image region i with respect to the target location. We select the region et from a multinomial distribution defined by the evidence function over the set Ht \\St of image regions not selected during previous steps:\np\u03b8(et|st) = fe(et)\u2211\ni\u2208Ht\\St fe(i) (7)\nLocation selection: Once selected, the evidence region et is used to define a 2d Gaussian probability distribution for the next fixation location zt \u2208 R2:\np\u03b8(zt|st, et) = N (\u00b7, fp(et),\u03b8\u03a3) (8)\nwhere the center fp(et) is based on a linear combination of the evidence region features g(et):\nfp(i) = [ \u03b8>p g(ri) ] \u00b7 x2(ri)\u2212 x1(ri)\n2 +\nx1(ri) + x2(ri)\n2 (9)\nWe make the position function invariant to the scale of the image region ri by normalizing with respect to its bounding box, defined by the top-left and bottom right corners x2(ri) and x1(ri).\nThe policy of our agent can be expressed as:\n\u03c0\u03b8 (at|st) = p\u03b8 (dt = 0|st) p\u03b8 (et|st) p\u03b8 (zt|st, et) (10)\nThe model is executed by repeated sampling of the policy \u03c0\u03b8 (at|st), until termination (dt = 1) (Algorithm 2). At each step the state st is updated according to the action at (Algorithm 3). When the search is finished, the observed region rk with maximum confidence ct is returned as detector output. Training algorithm: In our weakly supervised setting, we are given a set of images, represented as bags of regions Bj with associated global image labels lj , together with confidence function fc trained to be maximal at target locations, either with strong or weak supervision, c.f . \u00a75.2. We wish to\nfind the model parameters \u03b8 maximizing image-level classification accuracy based on the confidence ct at the last step (when dt = 1), while at the same time minimize the number of segment evaluations. Note that because the confidence response ct of the agent is the maximal confidence function response, we also encourage good detection performance.\nWe formulate our training objective in a reinforcement learning framework. Our reward function is sensitive to the confidence at the final state and incurs a penalty for each saccade the agent makes:\nrt =  \u2212\u03b1 if dt = 0 min(ct, 1) if dt = 1 \u2227 l = 1 max(ct,\u22121) if dt = 1 \u2227 l = \u22121\n(11)\nwhere l is the label associated with the training image over which the model was executed and \u03b1 is a penalty paid by the model for each saccade.\nDuring training, we maximize the expected reward function on the training set, defined as:\nF (\u03b8) = Ep\u03b8(s)  |s|\u2211 t=1 rt + \u03bb 2 \u03b8>\u03b8 (12)\nwhere s represents a variable length sequence of states, sampled by running the model (Algorithm 2 and 3), starting from an initial state s0 = (H0, S0) and \u03bb is an L2 regularizer. We set H0 to the set of segments observed by fixating the image center and S0 to \u2205.\nThe gradient of the expected reward can be approximated as[29, 23]:\n\u2207\u03b8F (\u03b8) = 1\nM M\u2211 i=1 |si|\u2211 t=1 \u2207\u03b8log\u03c0\u03b8(ait|sit)  |s|\u2211 t=1 rt + \u03bb\u03b8 (13)\nwhere si, ait and r i t, i = 1 . . .M , represent sequences of states, actions and corresponding rewards, sampled by model simulation."}, {"heading": "7. Experimental Results and Discussion", "text": "In this section, we first discuss our experimental methodology and results for learning confidence functions (\u00a75) and optimal search strategies for these functions (\u00a76)."}, {"heading": "7.1. Learning Confidence Functions", "text": "Our confidence function (CMI-EYE-DET) is trained on the set of image regions fixated by human subjects, as described in \u00a75.1. For positive images, where bags consist of fixated segments, we create additional negative bags consisting of all segments not overlapping any fixated segment. We use threholds TS = TF = 0.2 to define the fringe and subregions c.f . (2) and (3). For initialization, we generate 30 random label assignments y, with the ratio of positive/negative labels uniformly sampled from the range [0.1, 1.0]. Our model generally converges in less than 20 iterations. We evaluate the SVM-C parameter on the validation set, and re-train on trainval. We then evaluate our confidence function on all segments extracted from each test image. Detector responses for each image are generated by non-max suppression at overlap threshold 0.2. Features: We use the deep convolutional neural network of Krizhevskyet al.[14] as feature extractor g. Given an image segment, we first compute its bounding box and enlarge it by 10%. We then apply the neural network twice to produce our feature descriptor. First, we apply it on the contents of the bounding box (after proper scaling). Second, to capture context, we consider the entire image, mask the bounding box by filling it with its mean color, rescaled and passed\nthrough the network. For each call we record the output of the last fully connected layer, and end up with two vectors of 4096 real values. Additionally, we build a representation of the bounding box that consists of its normalized image coordinates, size and aspect ratio. Our final feature vector consists of the two concatenated neural response vectors and the bounding box descriptor, and has 8204 entries. Baselines: We compare our method to three baselines: Supervised learning (BB-DET): We train a confidence function where positive examples are the image segments with the highest target overlap (see table 1, column 3), the fringe of these segments is removed from the training set, and the rest of the segments are negative examples. Multiple instance learning (MI-EYE-DET): To capture the contribution of our topological constraints to model performance, we train our confidence function on the set of fixated segments in a similar fashion to CMI-EYE-DET, without constraints (4.2), (4.3), restricting the range of possible image labels in constraint (4.1) to {\u22121, 1}. Image labels alone (CMI-IL-DET): To estimate the power of eye movements as a supervisory signal, we also investigate the setup in which only image labels are available at training time. To this end, we train our model on bags composed of the entire segment pool for the corresponding image, as returned by CPMC. Evaluation: We first evaluate our models under the VOC detection average precision metric, in which a segment is considered a hit if its bounding box overlaps the ground truth bounding box with at least 0.5 overlap.\nNote however that our ground truth bounding boxes contain both the person and the manipulated object. The distinctive pattern that defines an action is often a subregion within this bounding box, e.g. the person\u2019s hands grasping the guitar consistent with playing or the person\u2019s head and the mobile phone close to her ear (see fig. 3). While precisely defining annotations for these patterns is not trivial, we also evaluate our methods on a metric that counts detector responses falling within the predicted bounding box as correct, regardless of the overlap. Under this metric, a segment is a hit if its bounding box falls within the subregion\nset (2) of the ground truth bounding box, with TS = 0.5. We also evaluate image level classification, by computing the maximum confidence of the trained detector for each image, and reporting classification average precision over the images in the test set. Results: Our method (CMI-EYE-DET) outperforms both baselines under all metrics (tables 2, 3 and 4). There is a larger performance gap between supervised and weakly supervised approaches when measured under the average precision metric, than under a less restrictive inclusion metric. This suggests, that weakly supervised approaches recover invariant patterns within the bounding box, although, at least in the absence of prior knowledge, cannot easily estimate the full extent of the actor and person. Our visualizations of the results (fig. 3) also support this intuition.\nThe contribution of topological constraints to the multiple instance learning quality is substantial under all metrics. Additionally, leveraging human eye movement annotations produces image classifiers nearly as good as those learned under full supervision. The standard MI-EYE-DET pipeline fails to exploit the human eye movement training signal for image classification, showing that our proposed topological constraints are crucial for this task."}, {"heading": "7.2. Learning Optimal Search Strategies", "text": "We train sequential detection search models for the confidence function learned using eye movements (CMI-EYESEQ) and bounding boxes (BB-SEQ). Note that for both\ncases, our search model is learned using image labels alone. We optimize our objective function (12) using a BFGS optimizer and we set the \u03bb regularizer to maximize the expected reward on the validation set. We use 8 random initializations of the model parameters \u03b8 and run the model until convergence, which generally takes less than 30 iterations. Evaluation: Our detector is run on the test set using the metrics in \u00a75. We also measure the number of evaluated segments and the total computational time, using a C++/Matlab implementation on an Intel Xeon E5 2660 2.20GHz CPU. Results: We find that our learned optimal search model only requires feature extraction for approximately half of the segments (table 5), while leaving image level classification performance (table 4) at nearly the same levels as in the corresponding pipelines based on exhaustive evaluation (CMI-EYE-DET and BB-DET). At the same time, detection performance is only slightly affected compared to exhaustive search. The total computational time is also improved. Note that our numbers account for both CPMC segment extraction and the running time of the search model itself, and are overall significantly improved. In all experiments we use an optimized version of CPMC, which operates on a reduced search space formed by superpixels, offering significant speedups with respect to standard CPMC."}, {"heading": "8. Conclusions", "text": "We have presented novel, general, weakly supervised segmentation-based methods to learn accurate and efficient\ndetection models in static images. In contrast to methods trained using ground truth bounding box or segment annotations, our detection response model leverages novel multiple instance learning techniques with topological constraints in order to learn accurate confidence functions and image classifiers using eye movement data. Additionally, we develop novel sequential models, in order to achieve optimal, efficient search strategies for detection based on reinforcement learning. In extensive experiments, we show that our proposed methodology achieves significant progress in terms of accuracy and speed, under weak supervision."}], "references": [{"title": "Confidence-rated multiple instance boosting for object detection", "author": ["K. Ali", "K. Saenko"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["J. Amores"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Support vector machines for multiple instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Infomax control of eye movements", "author": ["N.J. Butko", "J.R. Movellan"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts", "author": ["J. Carreira", "C. Sminchisescu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Predicting human gaze using low-level saliency combined with face detection", "author": ["M. Cerf", "J. Harel", "W. Einhuser", "C. Koch"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T.L. Perez"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Modelling search for people in 900 scenes: A combined source model of eye guidance", "author": ["K.A. Ehinger", "B. Hidalgo-Sotelo", "A. Torralba", "A. Oliva"], "venue": "Visual Cognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "The PASCAL Visual Object Classes Challenge 2012", "author": ["M. Everingham", "L.V. Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning to recognize daily actions using gaze", "author": ["A. Fathi", "Y. Li", "J.M. Rehg"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girschick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Shufflets: Shared mid-level parts for fast object detection", "author": ["I. Kokkinos"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Imagenet classi cation with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Beyond sliding windows: Object localization by efficient subwindow search", "author": ["C.H. Lampert", "M.B. Blaschko", "T. Hofmann"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["H. Larochelle", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Dynamic eye movement datasets and learned saliency models for visual action recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Action from still image dataset and inverse optimal control to learn task specific visual scanpaths", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Training object class detectors from eye tracking data", "author": ["D. Padadopoulos", "A. Clarke", "F. Keller", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "An eye fixation database for saliency detection in images", "author": ["S. Ramanathan", "H. Katti", "N. Sebe", "M. Kankanhalli"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Action is in the eye of the beholder: Eye-gaze driven model for spatiotemporal action localization", "author": ["N. Shapovalova", "M. Raptis", "L. Sigal", "G. Mori"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Multiple kernels for object detection", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Weakly supervised structured output learning for semantic segmentation", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Space-variant descriptor sampling for action recognition based on saliency and eye movements", "author": ["E. Vig", "M. Dorr", "D. Cox"], "venue": "In ECCV,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Efficient histogram-based sliding window", "author": ["Y. Wei", "L. Tao"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R. Williams"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": "This approach is however not well suited to contemporary stateof-the-art systems, which are trained using large amounts of image or video data, and require complex multi-layer models[14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 17, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 19, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 16, "context": "Such data has been exploited at training time to support visual classification performance in video[17], but its usefulness as a training signal for detection, in the absence of any additional annotations, remains unexplored.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 61, "endOffset": 69}, {"referenceID": 14, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 61, "endOffset": 69}, {"referenceID": 24, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 18, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 206, "endOffset": 210}, {"referenceID": 18, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 233, "endOffset": 237}, {"referenceID": 23, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 257, "endOffset": 261}, {"referenceID": 7, "context": "Multiple instance learning[8] formulations can be seen as a generalization of supervised learning, in which class labels are assigned to sets of training examples.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 60, "endOffset": 66}, {"referenceID": 25, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 45, "endOffset": 48}, {"referenceID": 16, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 17, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 19, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 16, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 26, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 10, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 21, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 202, "endOffset": 206}, {"referenceID": 26, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 79, "endOffset": 87}, {"referenceID": 16, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 10, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 21, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Unlike [20], our method does not require the availability of ground truth bounding boxes at training time.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 17, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 15, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 149, "endOffset": 152}, {"referenceID": 17, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "Here, we derive novel, fully trainable models for two challenging problems, namely action recognition and detection in cluttered natural scenes, and in a reinforcement optimal learning setup[23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "While the Pascal VOC Action Classification Challenge[10] contains a large set of images with human actions, the provided annotations are not useful for action detection for several reasons.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "In the present work, the region space R consists of all segments extracted using the CPMC algorithm[6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "Such data has been recently acquired and made publicly available[18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "|r \u2229 r\u2032| / |r\u2032| \u2264 TS} (2) Unconstrained fringe: Previous experience with training sliding window detectors[19] has shown that excluding bounding boxes overlapping the ground truth from the set of negative examples can greatly improve the localization power.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "1), we end up with the miSVM formulation[3], which treats all training instances as independent from one another.", "startOffset": 40, "endOffset": 43}, {"referenceID": 28, "context": "The gradient of the expected reward can be approximated as[29, 23]:", "startOffset": 58, "endOffset": 66}, {"referenceID": 22, "context": "The gradient of the expected reward can be approximated as[29, 23]:", "startOffset": 58, "endOffset": 66}, {"referenceID": 13, "context": "[14] as feature extractor g.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.", "creator": "LaTeX with hyperref package"}}}