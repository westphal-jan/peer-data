{"id": "1412.1574", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2014", "title": "Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking", "abstract": "As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated within a spatio-temporal statistical learning framework. However, most existing keypoint trackers are unable to model and balance the following three aspects simultaneously: temporal model coherence between frames, spatial model consistency within frames, and discriminatory feature construction. To solve this problem, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminatory metric learning. Consequently, time model coherence is characterized by structured keypoint model learning across multiple adjacent frames, while spatial model consistency is modeled by solving a geometric verification-based structured learning problem. Discriminating feature construction is characterized by metric learning, allowing for simultaneous separation within the above-mentioned classes, and finally, a common module consistency.", "histories": [["v1", "Thu, 4 Dec 2014 07:42:21 GMT  (1079kb,D)", "http://arxiv.org/abs/1412.1574v1", "Accepted by AAAI-15"]], "COMMENTS": "Accepted by AAAI-15", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["liming zhao", "xi li", "jun xiao", "fei wu", "yueting zhuang"], "accepted": true, "id": "1412.1574"}, "pdf": {"name": "1412.1574.pdf", "metadata": {"source": "META", "title": "Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking", "authors": ["Liming Zhao", "Xi Li", "Jun Xiao", "Fei Wu", "Yueting Zhuang"], "emails": ["yzhuang}@zju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Due to the effectiveness and efficiency in object motion analysis, keypoint-based object tracking (Lucas and Kanade 1981; Santner et al. 2010; Maresca and Petrosino 2013; Nebehay and Pflugfelder 2014) is a popular and powerful tool of video processing, and thus has a wide range of applications such as augmented reality (AR), object retrieval, and video compression. By encoding the local structural information on object appearance (Li et al. 2013), it is generally robust to various appearance changes caused by several complicated factors such as shape deformation, illumination variation, and partial occlusion (Mikolajczyk and Schmid 2005; Bouachir and Bilodeau 2014). Motivated by this observation, we focus on constructing effective and robust keypoint models to well model the intrinsic spatio-temporal structural properties of object appearance in this paper.\nTypically, keypoint model construction consists of keypoint representation and statistical modeling. For keypoint\n\u2217Corresponding Author Copyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nrepresentation, a variety of keypoint descriptors are proposed to encode the local invariance information on object appearance, for example, SIFT (Lowe 2004) and SURF (Bay et al. 2008). To further speed up the feature extraction process, a number of binary local descriptors emerge, including BRIEF (Calonder et al. 2010), ORB (Rublee et al. 2011), BRISK (Leutenegger, Chli, and Siegwart 2011), FREAK (Alahi, Ortiz, and Vandergheynst 2012), etc. Since the way of feature extraction is handcrafted and fixed all the time, these keypoint descriptors are usually incapable of effectively and flexibly adapting to complex time-varying appearance variations as tracking proceeds.\nIn general, statistical modeling is cast as a trackingby-detection problem, which seeks to build an object locator based on discriminative learning such as randomized decision trees (Lepetit and Fua 2006; O\u0308zuysal et al. 2010) and boosting (Grabner, Grabner, and Bischof 2007; Guo and Liu 2013). However, these approaches usually generate the binary classification output for object tracking, and thus ignore the intrinsic structural or geometrical information (e.g., geometric transform across frames) on object localization and matching during model learning. To address this issue, Hare et al. (Hare, Saffari, and Torr 2012) propose a structured SVM-based keypoint tracking approach that incorporates the RANSAC-based geometric matching information into the optimization process of learning keypointspecific SVM models. As a result, the proposed tracking approach is able to simultaneously find correct keypoint correspondences and estimate underlying object geometric transforms across frames. In addition, the model learning proar X\niv :1\n41 2.\n15 74\nv1 [\ncs .C\nV ]\n4 D\nec 2\n01 4\ncess is independently carried out frame by frame, and hence ignores the intrinsic cross-frame interaction information on temporal model coherence, leading to instable tracking results in complicated scenarios.\nIn this work, we propose a joint learning approach that is capable of well balancing the following three important parts: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. As illustrated in Figure 1, the joint learning approach ensures the temporal model coherence by building a multi-task structured model learning scheme, which encodes the cross-frame interaction information by simultaneously optimizing a set of mutually correlated learning subtasks (i.e., a common model plus different biases) over several successive frames. As a result, the interaction information induced by multi-task learning can guide the tracker to produce stable tracking results. Moreover, the proposed approach explores the keypoint-specific structural information on spatial model consistency by performing geometric verification based structured output learning, which aims to estimate a geometric transformation while associating crossframe keypoints. In order to make the keypoint descriptors well adapt to time-varying tracking situations, the proposed approach naturally embeds metric learning to the structured SVM learning process, which enhances the discriminative power of inter-class separability.\nIn summary, we propose a keypoint tracking approach that learns an effective and robust keypoint model through metric learning-driven multi-task structured output optimization. The main contributions of this work are as follows:\n1. We propose a multi-task joint learning scheme to learn structured keypoint models by simultaneously considering spatial model consistency, temporal model coherence, and discriminative feature learning. An online optimization algorithm is further presented to efficiently and effectively solve the proposed scheme. To our knowledge, it is the first time that such a joint learning scheme is proposed for learning-based keypoint tracking.\n2. We create and release a new benchmark video dataset containing four challenging video sequences (covering several complicated scenarios) for experimental evaluations. In these video sequences, the keypoint tracking results are manually annotated as ground truth. Besides, the quantitative results on them are also provided in the experimental section."}, {"heading": "2 Approach", "text": "Our tracking approach is mainly composed of two parts: learning part and prediction part. Namely, an object model is first learned by a multi-task structured learning scheme in a discriminative feature space (induced by metric learning). Based on the learned object model, our approach subsequently produces the tracking results through structured prediction. Using the tracking results, a set of training samples are further collected for structured learning. The above process is repeated as tracking proceeds."}, {"heading": "2.1 Preliminary", "text": "Let the template image O be represented as a set of keypointsO = {(ui,qi)}N O\ni=1, where each keypoint is defined by a location ui and associated descriptor qi. Similarly, let I = {(vj ,dj)}N I\nj=1 denote the input frame with keypoints. Typically, the traditional approaches construct the correspondences between the template keypoints and the input frame keypoints. The correspondences are scored by calculating the distances between {qi}N O i=1 and {dj}N I\nj=1. Following the model learning approaches (Hare, Saffari, and Torr 2012; Stu\u0308ckler and Behnke 2012), we learn a model parameterized by a weight vector wi for the template keypoint ui to score each correspondence. The set of the hypothetical correspondences is defined as C = {(ui, vj , sij)|(ui,qi) \u2208 O, (vj ,dj) \u2208 I, sij = \u3008wi,dj\u3009}, where sij is a correspondence score and \u3008\u00b7, \u00b7\u3009 is the inner product.\nSimilar to (Hare, Saffari, and Torr 2012; Pernici and Del Bimbo 2013), we estimate the homography transformation for planar object tracking as the tracking result based on the hypothetical correspondences."}, {"heading": "2.2 Multi-task Structured Learning", "text": "During the tracking process, the keypoints in the successive frames {I1, I2, . . . } corresponding to the i-th keypoint ui in the template image form a tracklet {v1, v2, . . . }. Based on the observation that the adjacent keypoints in a tracklet are similar to each other, the models learned for the frames {w1i ,w2i , . . . } should be mutually correlated. So we construct K learning tasks over several adjacent frames. For example, task k learns a model wk over the training samples collected from the frames I1 to It+k, where wk = [wk1 , . . . ,w k NO ]\nT is the column concatenation of the model parameter vectors. We model each wk as a linear combination of a common model w0 and an unique part vk (Zheng and Ni 2013):\nwk = w0 + vk, k = 1, . . . ,K (1) where all the vectors {vk}Kk=1 are \u201csmall\u201d when the tasks are similar to each other.\nTo consider the spatial model consistency in the model learning process, the transformation which maps the template to the location of the input frame is regarded as a structure, which can be learned in a geometric verification based structured learning framework. In our approach, the expected transformation y\u0302 is expressed as y\u0302 = arg maxy\u2208Y F (C,y), where F is a compatibility function, scoring all possible transformations generated by using the RANSAC (Fischler and Bolles 1981) method. Before introducing the compatibility function, we give the definition of the inlier set with a specific transformation y: H(C,y) = {(ui, vj)|(ui, vj) \u2208 C, \u2016y(ui)\u2212 vj\u2016 < \u03c4} (2)\nwhere y(ui) is the transformed location in the input frame of the template keypoint location ui, \u03c4 \u2208 R is a spatial distance threshold, and \u2016\u00b7\u2016 denotes the Euclidean norm.\nThe compatibility function with respect to task k is then defined as the total score of the inliers: F k(C,y) = \u2211\n(ui,vj)\u2208H(C,y)\n\u2329 wki ,dj \u232a = \u2329 wk,\u03a6(C,y) \u232a (3)\nwhere \u03a6(C,y) is a joint feature mapping vector concatenated by \u03c6i(C,y) which is defined as:\n\u03c6i(C,y) = { dj \u2203(ui, vj) \u2208 C : \u2016y(ui)\u2212 vj\u2016 < \u03c4 0 otherwise (4)\nGiven training samples {(Ct,yt)}Tt=1 (each Ct is the hypothetical correspondences of the frame It, and yt is the predicted transformation), a structured output maximum margin framework (Taskar, Guestrin, and Koller 2003; Tsochantaridis et al. 2005) is used to learn all the multi-task models, which can be expressed by the following optimization problem:\nmin w0,vk,\u03be\n1 2 \u2016w0\u20162 + \u03bb1 2K K\u2211 k=1 \u2016vk\u20162 + \u03bd1 K\u2211 k=1 T\u2211 t=k \u03bekt\ns.t.\u2200k, t, \u03bekt \u2265 0 \u2200k, t, \u2200y 6= yt : \u03b4F kt (y) \u2265 \u2206(yt,y)\u2212 \u03bekt\n(5)\nwhere \u03b4F kt (y) = \u2329 wk,\u03a6(Ct,yt) \u232a \u2212 \u2329 wk,\u03a6(Ct,y) \u232a and \u2206(yt,y) is a loss function which measures the difference of two transformations (in our case, the loss function \u2206(yt,y) = |#H(C,yt) \u2212 #H(C,y)| is the difference in number of two inlier sets). The nonnegative \u03bb1 is the weight parameter for multiple tasks, and the weighting parameter \u03bd1 determines the trade-off between accuracy and regularization.\nTo better describe the contribution of the multi-task learning, example tracking results of the trackers with and without multi-task learning are shown in Figure 2. From Figure 2(b), we observe that the independent model fails to match the keypoints in the case of drastic rotations, while the multi-task model enables the temporal model coherence to capture the information of rotational changes, thus produces a stable tracking result."}, {"heading": "2.3 Discriminative Feature Space", "text": "In order to make the keypoint descriptors well adapt to timevarying tracking situations, we wish to learn a mapping\nfunction f(d) that maps the original feature space to another discriminative feature space, in which the semantically similar keypoints are close to each other while the dissimilar keypoints are far away from each other, that can be formulated as a metric learning process (Weinberger and Saul 2009; Park et al. 2011). We then use the mapped feature f(d) to replace the original feature d in the structured learning process, to enhance its discriminative power of inter-class separability.\nFigure 3 shows an example of such feature space transformation. Before the mapping procedure, the object keypoints and the background keypoints can not be discriminated in the original feature space. After the transformation, the keypoints in different frames corresponding to the same keypoint in the template, which are semantically similar, get close to each other in the mapped feature space, while the features of the other keypoints have a distribution in another side with a large margin.\nThe following describes how to learn the mapping function. For a particular task k, given the learned model wki , the distance between a doublet (dj ,dk) is defined as follows:\nDki (dj ,dj\u2032) = \u2329 wki , f(dj)\u2212 f(dj\u2032) \u232a (6)\nWe assume that the binary matrix pjj\u2032 \u2208 {0, 1} indicates whether or not the features dj and dj\u2032 are semantically similar (if they are similar, pjj\u2032 = 1). Therefore, the hinge loss function on a doublet is defined as:\n`ki (dj ,dj\u2032) = [(\u22121)pjj\u2032 (1\u2212Dki (dj ,dj\u2032))]+ (7)\nwhere [z]+ = max(z, 0). To learn the effective feature consistently in our mapping process, we wish to find the group-sparsity of the features. So we utilize `2,1-norm (Cai et al. 2011; Li et al. 2012) to learn the discriminative information and feature correlation consistently. Since we use a linear transformation f(d) = MTd as our mapping function, the `2,1-norm for the map-\nping matrix M is defined as: \u2016M\u20162,1 = \u2211\ni \u221a\u2211 j M 2 ij .\nGiven all the keypoint features from the video frames {It}Tt=1, we collect all possible combinations of the features\nas the training set, which is denoted asA = {(dj ,dj\u2032)|dj \u2208 {It}Tt=1, j\u2032 6= j,dj\u2032 \u2208 {It}Tt=1}. We obtain the binary matrix pjj\u2032 by using the tracking results (if dj and dj\u2032 from different frames correspond to the same keypoint in the template, pjj\u2032 is set to 1; otherwise, pjj\u2032 is set to 0). We wish to minimize the following cost function consisting of the empirical loss term and the `2,1-norm regularization term:\u2211\ni,k,(dj ,dj\u2032 )\u2208A\n`ki (dj ,dj\u2032) + \u03bb\u2016M\u20162,1 (8)\nThe cost function is incorporated into our multi-task structured learning framework, and then a unified joint learning scheme for object tracking is obtained. The final optimization problem of our approach is expressed in the following form:\nmin w0,vk,M,\u03be,\u03b3\n1 2 \u2016w0\u20162 + \u03bb1 2K K\u2211 k=1 \u2016vk\u20162 + \u03bb2\u2016M\u20162,1\n+ K\u2211 k=1 ( \u03bd1 T\u2211 t=k \u03bekt + \u03bd2 T\u2211 t=k \u2211 (ui,vj)\u2208H(Ct,yt) \u03b3kti ) s.t.\u2200k, t,\u03bekt \u2265 0 \u2200k, t,\u2200y 6= yt : \u03b4F kt (y) \u2265 \u2206(yt,y)\u2212 \u03bekt \u2200k, t,i : \u03b3kti \u2265 0 \u2200k, t,(ui, vj),\u2200j\u2032 6= j : Dki (dj ,dj\u2032) \u2265 1\u2212 \u03b3kti\n(9) After all the models w1,w2, . . . ,wK are learned, we use the last model w = wK to predict the result of new frame It. We use the RANSAC method to generate hypothetical transformations. Based on the model w, we predict the expected transformation yt from all hypothetical transformations by maximizing Eq. (3). The hypothetical correspondence setCt of the frame It and the predicted transformation yt are then added to our training set. We use all the training samples collected from the results of previous K frames (It\u2212K+1 to It) to update our model. Then the above process is repeated as tracking proceeds."}, {"heading": "2.4 Online Optimization", "text": "The optimization problem presented in Eq. (9) can be solved online effectively. We adopt an alternating optimization algorithm to solve the optimization problem.\nUnconstrained form Let \u03b1kt = [maxy 6=yt{\u2206(yt,y) \u2212 \u03b4F kt (y)}]+ and \u03b2kti = [maxj\u2032 6=j{1 \u2212 Dki (dj ,dj\u2032)}]+. Therefore, Eq. (9) can be rewritten to an unconstrained form:\nmin w0,vk,M\n1 2 \u2016w0\u20162 + \u03bb1 2K K\u2211 k=1 \u2016vk\u20162 + \u03bb2\u2016M\u20162,1\n+ K\u2211 k=1 ( \u03bd1 T\u2211 t=k \u03b1kt + \u03bd2 T\u2211 t=k \u2211 (ui,vj)\u2208H(Ct,yt) \u03b2kti ) (10) For descriptive convenience, let J denote the term of \u03bd1 \u2211T t=k \u03b1kt + \u03bd2 \u2211T t=k \u2211 (ui,vj)\u2208H(Ct,yt) \u03b2kti.\nFix {vk}Kk=1 and w0, solve M Firstly, we fix all {vk}Kk=1 and w0, and learn the transformation matrix M by solving the following problem:\nmin M \u2016M\u20162,1 +\n1\n\u03bb2 K\u2211 k=1 J (11)\nLet Mi denote the i-th row of M, and Tr(\u00b7) denote the trace operator. In mathematics, the Eq. (11) can be converted to the following form:\nmin M\nTr(MTDM) + 1\n\u03bb2 K\u2211 k=1 J (12)\nwhere D is the diagonal matrix of M, and each diagonal element is Dii = 12\u2016Mi\u20162 . We use an alternating algorithm to calculate D and M respectively. We calculate M with the current D by using gradient descent method, and then update D according to the current M. The details of solving Eq. (12) are shown in the supplementary file.\nFix M and {vk}Kk=1, solve w0 Secondly, after M is learned, let {vk}Kk=1 have been the optimal solution of Eq. (10). Then w0 can be obtained by the combination of vk according to (Evgeniou and Pontil 2004):\nw0 = \u03bb1 K K\u2211 k=1 vk (13)\nThe proof can be found in our supplementary material.\nFix M and w0, solve {vk}Kk=1 Finally, {vk}Kk=1 can be learned one by one using gradient descent method. In fact, we learn wk = w0 + vk instead of vk for convenience. Let w\u0304 = 1K \u2211K k=1 w\nk be the average vector of all wk. Then the optimization problem for each wk can be rewritten as:\nmin wk\n\u03c11\u2016wk\u20162 + \u03c12\u2016wk \u2212 w\u0304\u20162 + J (14)\nwhere \u03c11 = \u03bb1/(\u03bb1 + 1) and \u03c12 = \u03bb21/(\u03bb1 + 1) (the derivation proof is given in the supplementary material).\nGiven training samples {(Ct\u2212k,yt\u2212k)}K\u22121k=0 at time t, the subgradient of Eq. (14) with respect to wk is calculated, and we perform a gradient descent step according to:\nwk \u2190 (1\u2212 1 t )wk + \u03b7\u03c12w\u0304 \u2212 \u03b7 \u2202J \u2202wk (15)\nwhere \u03b7 = 1/(\u03c11t+ \u03c12t) is the step size (the details of the term J is described in the supplementary material). We repeat the procedure to obtain an optimal solution until the algorithm converges (on average converges after 5 iterations).\nAll the above is summarized in Algorithm 1, and the details are described in the supplementary material.\n3 Experiments and Results"}, {"heading": "3.1 Experimental Settings", "text": "Dataset The video dataset used in our experiments consists of nine video sequences. Specifically, the first five sequences are from (Hare, Saffari, and Torr 2012), and the last\nAlgorithm 1: Online Optimization for Tracking Input: Input frame It and previous models {w1, . . . ,wK} Output: The predicted transformation yt, updated models\nand mapping matrix for metric learning /* The structured prediction part */\n1 Calculate the correspondences Ct based on the model wK ; 2 Estimate hypothetical transformations y using RANSAC;\nCalculate the inlier set of each y using Eq. (2); 3 Predict the expected yt by maximizing Eq. (3); /* The structured learning part */ 4 Collect the training samples {(Ct\u2212k,yt\u2212k)}K\u22121k=0 ; 5 repeat 6 Calculate J according to Section 2.4; 7 for k = 1, . . . ,K do 8 Update each model wk using Eq. (15); 9 end\n10 Update the mapping matrix M by solving Eq. (12); 11 until Alternating optimization convergence; 12 return yt, {w1, . . . ,wK} and M;\nfour sequences (i.e., \u201cchart\u201d, \u201ckeyboard\u201d, \u201cfood\u201d, \u201cbook\u201d) are recorded by ourselves. All these sequences cover several complicated scenarios such as background clutter, object zooming, object rotation, illumination variation, motion blurring and partial occlusion (example frames can be found in the supplementary material).\nImplementation Details For keypoint feature extraction, we use FAST keypoint detector (Rosten and Drummond 2006) with 256-bit BRIEF descriptor (Calonder et al. 2010). For metric learning, the linear transformation matrix M is initialized to be an identity matrix. For multi-task learning, the number of tasks K is chosen as 5 and we update all the multi-task models frame by frame. All weighting parameters \u03bb1, \u03bb2, \u03bd1, \u03bd2 are set to 1, and remain fixed throughout all the experiments. Similar to (Hare, Saffari, and Torr 2012), we consider the tracking process of estimating homography transformation on the planar object as a trackingby-detection task.\nEvaluation Criteria We use the same criteria as (Hare, Saffari, and Torr 2012) with a scoring function between the predicted homography y and the ground-truth homography y\u2217:\nS(y,y\u2217) = 1\n4 4\u2211 i=1 \u2016y(ci)\u2212 y\u2217(ci)\u20162 (16)\nwhere {ci}4i=1 = {(\u22121,\u22121)T, (1,\u22121)T, (\u22121, 1)T, (1, 1)T} is a normalized square. For each frame, it is regarded as a successfully detected frame if S(y,y\u2217) < 10, and a falsely detected frame otherwise. The average success rate is defined as the number of successfully detected frames divided by the length of the sequence, which is used to evaluate the performance of the tracker. To provide the tracking result frame by frame, we present a criterion of the accumulated false detection number, which is defined as the accumulated number of falsely detected frames as tracking proceeds."}, {"heading": "3.2 Experimental Results", "text": "Comparison with State-of-the-art Methods We compare our approach with some state-of-the-art approaches, including boosting based approach (Grabner, Grabner, and Bischof 2007), structured SVM (SSVM) approach (Hare, Saffari, and Torr 2012) and a baseline static tracking approach (without model updating). All these approaches are implemented by making use of their publicly available code. We also implement our approach in C++ and OPENCV. On average, our algorithm takes 0.0746 second to process one frame with a quad-core 2.4GHz Intel Xeon E5-2609 CPU and 16GB memory. Table 1 shows the experimental results of all four approaches in the average success rate. As shown in this table, our approach performs best on all sequences.\nTo provide an intuitive illustration, we report the detection result on each frame in Figure 4. We observe that both the \u201cBoosting\u201d and \u201cSSVM\u201d approaches obtain a number of incorrect detection results on some frames of the test sequences, while our approach achieves stable tracking results in most situations (the curve corresponding to our approach grows slowly and is almost horizontal).\nFigure 5 shows the tracking results on some sample frames (more experimental results can be found in our supplementary materials). These sequences containing background clutter are challenging for keypoint based tracking. In terms of metric learning and multi-task learning, our approach still performs well in some complicated scenarios with drastic object appearance changes.\nEvaluation of Our Individual Components To explore the contribution of each component in our approach, we compare the performances of the approaches with individual parts, including SSVM(structured SVM), SML(SSVM + metric learning), SMT(SSVM + multi-task learning), and SMM (SSVM + ML + MT, which is exactly our approach). The experimental results of all these approaches in the average success rate are reported in Table 2.\nFrom Table 2, we find that the geometric verification based structured learning approach achieves good tracking results in most situations. Furthermore, we observe from Figure 6 that multi-task structured learning guides the tracker to produce a stable tracking result in the complicated scenarios, and metric learning enhances the capability of the tracker to separate keypoints from background clutter. Our approach consisting of all these components then generates a robust tracker.\n4 Conclusion In this paper, we have presented a novel and robust keypoint tracker by solving a multi-task structured output optimization problem driven by metric learning. Our joint learning approach have simultaneously considered spatial model consistency, temporal model coherence, and discriminative feature construction during the tracking process.\nWe have shown in extensive experiments that geometric verification based structured learning has modeled the spatial model consistency to generate a robust tracker in most scenarios, multi-task structured learning has characterized the temporal model coherence to produce stable tracking results even in complicated scenarios with drastic changes, and metric learning has enabled the discriminative feature construction to enhance the discriminative power of the tracker. We have created a new benchmark video dataset consisting of challenging video sequences, and experimental results performed on the dataset have shown that our tracker outperforms the other state-of-the-art trackers."}, {"heading": "5 Acknowledgments", "text": "All correspondence should be addressed to Prof. Xi Li. This work is in part supported by the National Natural Science Foundation of China (Grant No. 61472353), National Basic Research Program of China (2012CB316400), NSFC (61472353), 863 program (2012AA012505), China Knowledge Centre for Engineering Sciences and Technology and the Fundamental Research Funds for the Central Universities."}], "references": [{"title": "Freak: Fast retina keypoint", "author": ["A. Alahi", "R. Ortiz", "P. Vandergheynst"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Alahi et al\\.,? 2012", "shortCiteRegEx": "Alahi et al\\.", "year": 2012}, {"title": "Speededup robust features (surf)", "author": ["H. Bay", "A. Ess", "T. Tuytelaars", "L.V. Gool"], "venue": "Computer Vision and Image Understanding (CVIU) 110(3):346\u2013359.", "citeRegEx": "Bay et al\\.,? 2008", "shortCiteRegEx": "Bay et al\\.", "year": 2008}, {"title": "Structure-aware keypoint tracking for partial occlusion handling", "author": ["W. Bouachir", "Bilodeau", "G.-A."], "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV).", "citeRegEx": "Bouachir et al\\.,? 2014", "shortCiteRegEx": "Bouachir et al\\.", "year": 2014}, {"title": "Multi-class l2,1-norm support vector machine", "author": ["X. Cai", "F. Nie", "H. Huang", "C. Ding"], "venue": "Proceedings of the IEEE Conference on Data Mining (ICDM).", "citeRegEx": "Cai et al\\.,? 2011", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Brief: Binary robust independent elementary features", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "Proceedings of the European Conference on Computer Vision (ECCV).", "citeRegEx": "Calonder et al\\.,? 2010", "shortCiteRegEx": "Calonder et al\\.", "year": 2010}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Evgeniou and Pontil,? 2004", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2004}, {"title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Communications of the ACM (CACM) 24(6):381\u2013395.", "citeRegEx": "Fischler and Bolles,? 1981", "shortCiteRegEx": "Fischler and Bolles", "year": 1981}, {"title": "Learning features for tracking", "author": ["M. Grabner", "H. Grabner", "H. Bischof"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Grabner et al\\.,? 2007", "shortCiteRegEx": "Grabner et al\\.", "year": 2007}, {"title": "Real-time keypoint-based object tracking via online learning", "author": ["B. Guo", "J. Liu"], "venue": "Proceedings of the International Conference on Information Science and Technology (ICIST).", "citeRegEx": "Guo and Liu,? 2013", "shortCiteRegEx": "Guo and Liu", "year": 2013}, {"title": "Efficient online structured output learning for keypoint-based object tracking", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Hare et al\\.,? 2012", "shortCiteRegEx": "Hare et al\\.", "year": 2012}, {"title": "Keypoint recognition using randomized trees", "author": ["V. Lepetit", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 28(9):1465\u20131479.", "citeRegEx": "Lepetit and Fua,? 2006", "shortCiteRegEx": "Lepetit and Fua", "year": 2006}, {"title": "Brisk: Binary robust invariant scalable keypoints", "author": ["S. Leutenegger", "M. Chli", "R. Siegwart"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Leutenegger et al\\.,? 2011", "shortCiteRegEx": "Leutenegger et al\\.", "year": 2011}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "A survey of appearance models in visual object tracking", "author": ["X. Li", "W. Hu", "C. Shen", "Z. Zhang", "A. Dick", "A.V.D. Hengel"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 4(4):58:1\u201358:48.", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision (IJCV) 60(2):91\u2013110.", "citeRegEx": "Lowe,? 2004", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "An iterative image registration technique with an application to stereo vision", "author": ["B.D. Lucas", "T. Kanade"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Lucas and Kanade,? 1981", "shortCiteRegEx": "Lucas and Kanade", "year": 1981}, {"title": "Matrioska: A multilevel approach to fast tracking by learning", "author": ["M. Maresca", "A. Petrosino"], "venue": "Proceedings of the International Conference on Image Analysis and Processing (ICIAP).", "citeRegEx": "Maresca and Petrosino,? 2013", "shortCiteRegEx": "Maresca and Petrosino", "year": 2013}, {"title": "A performance evaluation of local descriptors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 27(10):1615\u20131630.", "citeRegEx": "Mikolajczyk and Schmid,? 2005", "shortCiteRegEx": "Mikolajczyk and Schmid", "year": 2005}, {"title": "Consensus-based matching and tracking of keypoints for object tracking", "author": ["G. Nebehay", "R. Pflugfelder"], "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV).", "citeRegEx": "Nebehay and Pflugfelder,? 2014", "shortCiteRegEx": "Nebehay and Pflugfelder", "year": 2014}, {"title": "Fast keypoint recognition using random ferns", "author": ["M. \u00d6zuysal", "M. Calonder", "V. Lepetit", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 32(3):448\u2013 461.", "citeRegEx": "\u00d6zuysal et al\\.,? 2010", "shortCiteRegEx": "\u00d6zuysal et al\\.", "year": 2010}, {"title": "Efficiently learning a distance metric for large margin nearest neighbor classification", "author": ["K. Park", "C. Shen", "Z. Hao", "J. Kim"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Park et al\\.,? 2011", "shortCiteRegEx": "Park et al\\.", "year": 2011}, {"title": "Object tracking by oversampling local features", "author": ["F. Pernici", "A. Del Bimbo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) PP(99):1\u20131.", "citeRegEx": "Pernici and Bimbo,? 2013", "shortCiteRegEx": "Pernici and Bimbo", "year": 2013}, {"title": "Machine learning for high-speed corner detection", "author": ["E. Rosten", "T. Drummond"], "venue": "Proceedings of the European Conference on Computer Vision (ECCV).", "citeRegEx": "Rosten and Drummond,? 2006", "shortCiteRegEx": "Rosten and Drummond", "year": 2006}, {"title": "Orb: An efficient alternative to sift or surf", "author": ["E. Rublee", "V. Rabaud", "K. Konolige", "G. Bradski"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Rublee et al\\.,? 2011", "shortCiteRegEx": "Rublee et al\\.", "year": 2011}, {"title": "Prost: Parallel robust online simple tracking", "author": ["J. Santner", "C. Leistner", "A. Saffari", "T. Pock", "H. Bischof"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Santner et al\\.,? 2010", "shortCiteRegEx": "Santner et al\\.", "year": 2010}, {"title": "Model learning and real-time tracking using multi-resolution surfel maps", "author": ["J. St\u00fcckler", "S. Behnke"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "St\u00fcckler and Behnke,? 2012", "shortCiteRegEx": "St\u00fcckler and Behnke", "year": 2012}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Taskar et al\\.,? 2003", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research (JMLR) 6:1453\u20131484.", "citeRegEx": "Tsochantaridis et al\\.,? 2005", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research (JMLR) 10:207\u2013244.", "citeRegEx": "Weinberger and Saul,? 2009", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Time-dependent trajectory regression on road networks via multi-task learning", "author": ["J. Zheng", "L.M. Ni"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Zheng and Ni,? 2013", "shortCiteRegEx": "Zheng and Ni", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction Due to the effectiveness and efficiency in object motion analysis, keypoint-based object tracking (Lucas and Kanade 1981; Santner et al. 2010; Maresca and Petrosino 2013; Nebehay and Pflugfelder 2014) is a popular and powerful tool of video processing, and thus has a wide range of applications such as augmented reality (AR), object retrieval, and video compression.", "startOffset": 113, "endOffset": 215}, {"referenceID": 24, "context": "1 Introduction Due to the effectiveness and efficiency in object motion analysis, keypoint-based object tracking (Lucas and Kanade 1981; Santner et al. 2010; Maresca and Petrosino 2013; Nebehay and Pflugfelder 2014) is a popular and powerful tool of video processing, and thus has a wide range of applications such as augmented reality (AR), object retrieval, and video compression.", "startOffset": 113, "endOffset": 215}, {"referenceID": 16, "context": "1 Introduction Due to the effectiveness and efficiency in object motion analysis, keypoint-based object tracking (Lucas and Kanade 1981; Santner et al. 2010; Maresca and Petrosino 2013; Nebehay and Pflugfelder 2014) is a popular and powerful tool of video processing, and thus has a wide range of applications such as augmented reality (AR), object retrieval, and video compression.", "startOffset": 113, "endOffset": 215}, {"referenceID": 18, "context": "1 Introduction Due to the effectiveness and efficiency in object motion analysis, keypoint-based object tracking (Lucas and Kanade 1981; Santner et al. 2010; Maresca and Petrosino 2013; Nebehay and Pflugfelder 2014) is a popular and powerful tool of video processing, and thus has a wide range of applications such as augmented reality (AR), object retrieval, and video compression.", "startOffset": 113, "endOffset": 215}, {"referenceID": 13, "context": "By encoding the local structural information on object appearance (Li et al. 2013), it is generally robust to various appearance changes caused by several complicated factors such as shape deformation, illumination variation, and partial occlusion (Mikolajczyk and Schmid 2005; Bouachir and Bilodeau 2014).", "startOffset": 66, "endOffset": 82}, {"referenceID": 17, "context": "2013), it is generally robust to various appearance changes caused by several complicated factors such as shape deformation, illumination variation, and partial occlusion (Mikolajczyk and Schmid 2005; Bouachir and Bilodeau 2014).", "startOffset": 171, "endOffset": 228}, {"referenceID": 14, "context": "representation, a variety of keypoint descriptors are proposed to encode the local invariance information on object appearance, for example, SIFT (Lowe 2004) and SURF (Bay et al.", "startOffset": 146, "endOffset": 157}, {"referenceID": 1, "context": "representation, a variety of keypoint descriptors are proposed to encode the local invariance information on object appearance, for example, SIFT (Lowe 2004) and SURF (Bay et al. 2008).", "startOffset": 167, "endOffset": 184}, {"referenceID": 4, "context": "To further speed up the feature extraction process, a number of binary local descriptors emerge, including BRIEF (Calonder et al. 2010), ORB (Rublee et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 23, "context": "2010), ORB (Rublee et al. 2011), BRISK (Leutenegger, Chli, and Siegwart 2011), FREAK (Alahi, Ortiz, and Vandergheynst 2012), etc.", "startOffset": 11, "endOffset": 31}, {"referenceID": 10, "context": "In general, statistical modeling is cast as a trackingby-detection problem, which seeks to build an object locator based on discriminative learning such as randomized decision trees (Lepetit and Fua 2006; \u00d6zuysal et al. 2010) and boosting (Grabner, Grabner, and Bischof 2007; Guo and Liu 2013).", "startOffset": 182, "endOffset": 225}, {"referenceID": 19, "context": "In general, statistical modeling is cast as a trackingby-detection problem, which seeks to build an object locator based on discriminative learning such as randomized decision trees (Lepetit and Fua 2006; \u00d6zuysal et al. 2010) and boosting (Grabner, Grabner, and Bischof 2007; Guo and Liu 2013).", "startOffset": 182, "endOffset": 225}, {"referenceID": 8, "context": "2010) and boosting (Grabner, Grabner, and Bischof 2007; Guo and Liu 2013).", "startOffset": 19, "endOffset": 73}, {"referenceID": 25, "context": "Following the model learning approaches (Hare, Saffari, and Torr 2012; St\u00fcckler and Behnke 2012), we learn a model parameterized by a weight vector wi for the template keypoint ui to score each correspondence.", "startOffset": 40, "endOffset": 96}, {"referenceID": 29, "context": "We model each w as a linear combination of a common model w and an unique part v (Zheng and Ni 2013): w = w + v, k = 1, .", "startOffset": 81, "endOffset": 100}, {"referenceID": 6, "context": "In our approach, the expected transformation \u0177 is expressed as \u0177 = arg maxy\u2208Y F (C,y), where F is a compatibility function, scoring all possible transformations generated by using the RANSAC (Fischler and Bolles 1981) method.", "startOffset": 191, "endOffset": 217}, {"referenceID": 27, "context": "Given training samples {(Ct,yt)}t=1 (each Ct is the hypothetical correspondences of the frame It, and yt is the predicted transformation), a structured output maximum margin framework (Taskar, Guestrin, and Koller 2003; Tsochantaridis et al. 2005) is used to learn all the multi-task models, which can be expressed by the following optimization problem:", "startOffset": 184, "endOffset": 247}, {"referenceID": 28, "context": "function f(d) that maps the original feature space to another discriminative feature space, in which the semantically similar keypoints are close to each other while the dissimilar keypoints are far away from each other, that can be formulated as a metric learning process (Weinberger and Saul 2009; Park et al. 2011).", "startOffset": 273, "endOffset": 317}, {"referenceID": 20, "context": "function f(d) that maps the original feature space to another discriminative feature space, in which the semantically similar keypoints are close to each other while the dissimilar keypoints are far away from each other, that can be formulated as a metric learning process (Weinberger and Saul 2009; Park et al. 2011).", "startOffset": 273, "endOffset": 317}, {"referenceID": 3, "context": "So we utilize `2,1-norm (Cai et al. 2011; Li et al. 2012) to learn the discriminative information and feature correlation consistently.", "startOffset": 24, "endOffset": 57}, {"referenceID": 12, "context": "So we utilize `2,1-norm (Cai et al. 2011; Li et al. 2012) to learn the discriminative information and feature correlation consistently.", "startOffset": 24, "endOffset": 57}, {"referenceID": 5, "context": "Then w can be obtained by the combination of v according to (Evgeniou and Pontil 2004):", "startOffset": 60, "endOffset": 86}, {"referenceID": 22, "context": "Implementation Details For keypoint feature extraction, we use FAST keypoint detector (Rosten and Drummond 2006) with 256-bit BRIEF descriptor (Calonder et al.", "startOffset": 86, "endOffset": 112}, {"referenceID": 4, "context": "Implementation Details For keypoint feature extraction, we use FAST keypoint detector (Rosten and Drummond 2006) with 256-bit BRIEF descriptor (Calonder et al. 2010).", "startOffset": 143, "endOffset": 165}], "year": 2014, "abstractText": "As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.", "creator": "TeX"}}}