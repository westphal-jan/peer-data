{"id": "1412.2007", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "abstract": "Neural machine translation, a recently proposed approach to machine translation based solely on neural networks, has shown promising results compared to existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limits when dealing with a larger vocabulary, as both training complexity and decoding complexity increase proportionally to the number of target words. In this essay, we propose a method that allows us to use a very large target vocabulary without increasing training complexity based on sampling. We show that decoding can be done efficiently even in a model with a very large target vocabulary by selecting only a small subset of the entire target vocabulary. The models trained by the proposed approach are empirically found to meet the basic models with a small vocabulary as well as the LSTM-based neural vocabulary.", "histories": [["v1", "Fri, 5 Dec 2014 14:26:27 GMT  (122kb,D)", "http://arxiv.org/abs/1412.2007v1", null], ["v2", "Wed, 18 Mar 2015 19:41:42 GMT  (124kb,D)", "http://arxiv.org/abs/1412.2007v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["s\u00e9bastien jean", "kyunghyun cho", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1412.2007"}, "pdf": {"name": "1412.2007.pdf", "metadata": {"source": "CRF", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional\ntranslation systems (Sutskever et al., 2014; Bahdanau et al., 2014).\nNeural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003). First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al., 2014) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. Second, the whole system is jointly tuned to maximize the translation performance, unlike the existing phrase-based system which consists of many feature functions that are tuned separately. Lastly, the memory footprint of the NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs.\nDespite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach. That is, the number of target words must be limited. This is mainly because the complexity of training and using an NMT model increases as the number of target words increases. Also, the parametric nature of the neural machine translation makes it difficult for the model to estimate the conditional probabilities of rare words well.\nA usual practice is to construct a target vocabulary of the k most frequent words (a so-called shortlist), where k is often in the range of 30, 000 (Bahdanau et al., 2014) to 80, 000 (Sutskever et al., 2014). Any word not included in this vocabulary is mapped to a special token representing an unknown word [UNK]. This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2014).\nar X\niv :1\n41 2.\n20 07\nv1 [\ncs .C\nL ]\n5 D\nec 2\n01 4\nIn this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an NMT model with a much larger target vocabulary. The proposed algorithm effectively keeps the computational complexity during training at the level of using only a small subset of the full vocabulary. Once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them.\nWe compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English\u2192French and English\u2192German translation using the NMT model introduced in (Bahdanau et al., 2014). The empirical results clearly demonstrate that we can achieve better translation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding. Furthermore, we show that the model trained with this algorithm gets the best translation performance yet achieved by single NMT models on both English\u2192French and English\u2192German translation tasks."}, {"heading": "2 Neural Machine Translation and", "text": "Limited Vocabulary Problem\nIn this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2014). Based on this description we explain the issue of limited vocabularies in neural machine translation."}, {"heading": "2.1 Neural Machine Translation", "text": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and N\u0303eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).\nNeural machine translation is often implemented as the encoder\u2013decoder network. The encoder reads the source sentence x = (x1, . . . , xT ) and encodes it into a sequence of hidden states h = (h1, \u00b7 \u00b7 \u00b7 , hT ):\nht = f (xt, ht\u22121) . (1)\nThen, the decoder, another recurrent neural network, generates a corresponding translation y =\n(y1, \u00b7 \u00b7 \u00b7 , yT \u2032) based on the encoded sequence of hidden states h:\np(yt | y<t, x) \u221d exp {q (yt\u22121, zt, ct)} , (2)\nwhere\nzt = g (yt\u22121, zt\u22121, ct) , (3)\nct = r (zt\u22121, h1, . . . , hT ) , (4)\nand y<t = (y1, . . . , yt\u22121). The whole model is jointly trained to maximize the conditional log-probability of the correct translation given a source sentence with respect to the parameters \u03b8 of the model:\n\u03b8\u2217 = argmax \u03b8 N\u2211 n=1 Tn\u2211 t=1 log p(ynt | yn<t, xn),\nwhere (xn, yn) is the n-th training pair of sentences, and Tn is the length of the n-th target sentence (yn)."}, {"heading": "2.1.1 Detailed Description", "text": "In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (Bahdanau et al., 2014).\nIn (Bahdanau et al., 2014), the encoder in Eq. (1) is implemented by a bi-directional recurrent neural network such that\nht = [\u2190\u2212 h t; \u2212\u2192 h t ] ,\nwhere \u2190\u2212 h t = f ( xt, \u2190\u2212 h t+1 ) , \u2212\u2192 h t = f ( xt, \u2212\u2192 h t\u22121 ) .\nThey used a gated recurrent unit for f (see, e.g., (Cho et al., 2014b)).\nThe decoder, at each time, computes the context vector ct as a convex sum of the hidden states (h1, . . . , hT ) with the coefficients \u03b11, . . . , \u03b1T computed by\n\u03b1t = exp {a (ht, zt\u22121)}\u2211 k exp {a (hk, zt\u22121)} , (5)\nwhere a is a feedforward neural network with a single hidden layer.\nA new hidden state zt of the decoder in Eq. (3) is computed based on the previous hidden state zt\u22121,\nprevious generated symbol yt\u22121 and the computed context vector ct. The decoder also uses the gated recurrent unit, as the encoder does.\nThe probability of the next target word in Eq. (2) is then computed by\np(yt | y<t, x) = 1\nZ exp\n{ w>t \u03c6 (yt\u22121, zt, ct) + bt } ,\n(6)\nwhere \u03c6 is an affine transformation followed by a nonlinear activation, and wt and bt are respectively the target word vector and the target word bias. Z is the normalization constant computed by\nZ = \u2211\nk:yk\u2208V exp\n{ w>k \u03c6 (yt\u22121, zt, ct) + bk } , (7)\nwhere V is the set of all the target words. For the detailed description of the implementation, we refer the reader to the appendix of (Bahdanau et al., 2014)."}, {"heading": "2.2 Limited Vocabulary Issue and Conventional Solutions", "text": "One of the main difficulties in training this neural machine translation model is the computational complexity involved in computing the target word probability (Eq. (6)). More specifically, we need to compute the dot product between the feature \u03c6 (yt\u22121, zt, ct) and the word vectorwt as many times as there are words in a target vocabulary in order to compute the normalization constant (the denominator in Eq. (6)). This has to be done for, on average, 20\u201330 words per sentence, which easily becomes prohibitively expensive even with a moderate number of possible target words. Furthermore, the memory requirement grows linearly with respect to the number of target words. This has been a major hurdle for neural machine translation, compared to the existing non-parametric approaches such as phrase-based translation systems.\nRecently proposed neural machine translation models, hence, use a shortlist of 30,000 to 80,000 most frequent words (Bahdanau et al., 2014; Sutskever et al., 2014). This makes training more feasible, but comes with a number of problems. First of all, the performance of the model degrades heavily if the translation of a source sentence requires\nmany words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages.\nThere are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been used proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010). In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(yt|y<t, x) is factorized as a product of the class probability p(ct|y<t, x) and the intraclass word probability p(yt|ct, y<t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation during test time1.\nOther than these model-specific approaches, there exist translation-specific approaches. A translationspecific approach exploits the properties of the rare target words. For instance, Luong et al. of (Luong et al., 2014) proposed such an approach for neural machine translation. They replace rare words (the words that are not included in the shortlist) in both source and target sentences into corresponding \u3008OOVn\u3009 tokens using the word alignment model. Once a source sentence is translated, each \u3008OOVn\u3009 in the translation will be replaced based on the source word marked by the corresponding \u3008OOVn\u3009.\nIt is important to note that the model-specific approaches and the translation-specific approaches are often complementary and can be used together to further improve the translation performance and reduce the computational complexity.\n1This is due to the fact that the beam search requires the conditional probability of every target word at each time step regardless of the parametrization of the output probability."}, {"heading": "3 Approximate Learning Approach to Very Large Target Vocabulary", "text": ""}, {"heading": "3.1 Description", "text": "In this paper, we propose a model-specific approach that allows us to train a neural machine translation model with a very large target vocabulary. With the proposed approach, the computational complexity of training becomes constant with respect to the size of the target vocabulary. Furthermore, the proposed approach allows us to efficiently use a fast computing device with limited memory, such as a GPU, to train a neural machine translation model with a much larger target vocabulary.\nAs mentioned earlier, the computational inefficiency of training a neural machine translation model arises from the normalization constant in Eq. (6). In order to avoid the growing complexity of computing the normalization constant, we propose here to use only a small subset V \u2032 of the target vocabulary at each update. The proposed approach is based on the earlier work of (Bengio and Se\u0301ne\u0301cal, 2008).\nLet us consider the gradient of the log-probability of the output in Eq. (6). The gradient is composed of a positive and negative part:\n\u2207 log p(yt | y<t, x) (8) =\u2207E(yt)\u2212 \u2211\nk:yk\u2208V p(yk | y<t, x)\u2207E(yk),\nwhere we define the energy E as\nE(yj) = w>j \u03c6 (yj\u22121, zj , cj) + bj .\nThe second, or negative, term of the gradient is in essence the expected gradient of the energy:\nEP [\u2207E(y)] , (9)\nwhere P denotes p(y | y<t, x). The main idea of the proposed approach is to approximate this expectation, or the negative term of the gradient, by importance sampling with a small number of samples. Given a predefined proposal distribution Q and a set V \u2032 of samples from Q, we approximate the expectation in Eq. (9) with\nEP [\u2207E(y)] \u2248 \u2211\nk:yk\u2208V \u2032\n\u03c9k\u2211 k\u2032:yk\u2032\u2208V \u2032 \u03c9k\u2032 \u2207E(yk),\n(10)\nwhere\n\u03c9k = exp {E(yk)\u2212 logQ(yk)} . (11)\nThis approach allows us to compute the normalization constant during training using only a small subset of the target vocabulary, resulting in much lower computational complexity for each parameter update. Intuitively, at each parameter update, we update only the vectors associated with the correct word wt and with the sampled words in V \u2032. Once training is over, we can use the full target vocabulary to compute the output probability of each target word.\nAlthough the proposed approach naturally addresses the computational complexity, using this approach naively does not guarantee that the number of parameters being updated for each sentence pair, which includes multiple target words, is bounded nor can be controlled. This becomes problematic when training is done, for instance, on a GPU with limited memory.\nIn practice, hence, we partition the training corpus and define a subset V \u2032 of the target vocabulary for each partition prior to training. Before training begins, we sequentially examine each target sentence in the training corpus and accumulate unique target words until the number of unique target words reaches the predefined threshold \u03c4 . The accumulated vocabulary will be used for this partition of the corpus during training. We repeat this until the end of the training set is reached. Let us refer to the subset of target words used for the i-th partition by V \u2032i .\nThis may be understood as having a separate proposal distribution Qi for each partition of the training corpus. The distribution Qi assigns equal probability mass to all the target words included in the subset V \u2032i , and zero probability mass to all the other words, i.e.,\nQi(yk) =  1 |V \u2032i | if yt \u2208 V \u2032i\n0 otherwise.\nThis choice of proposal distribution cancels out the correction term \u2212 logQ(yk) from the importance weight in Eqs. (10)\u2013(11), which makes the proposed approach equivalent to approximating the exact out-\nput probability in Eq. (6) with\np(yt | y<t, x)\n= exp\n{ w>t \u03c6 (yt\u22121, zt, ct) + bt }\u2211 k:yk\u2208V \u2032 exp { w>k \u03c6 (yt\u22121, zt, ct) + bk\n} . It should be noted that this choice of Q makes the estimator biased."}, {"heading": "3.1.1 Informal Discussion on Consequence", "text": "The parametrization of the output probability in Eq. (6) can be understood as arranging the vectors associated with the target words such that the dot product between the most likely, or correct, target word\u2019s vector and the current hidden state is maximized. The exponentiation followed by normalization is simply a process in which the dot products are converted into proper probabilities.\nAs learning continues, therefore, the vectors of all the likely target words tend to align with each other but not with the others. This is achieved exactly by moving the vector of the correct word in the direction of \u03c6 (yt\u22121, zt, ct), while pushing all the other vectors away, which happens when the gradient of the logarithm of the exact output probability in Eq. (6) is maximized. Our approximate approach, instead, moves the word vectors of the correct words and of only a subset of sampled target words (those included in V \u2032)."}, {"heading": "3.2 Decoding", "text": "Once the model is trained using the proposed approximation, we can use the full target vocabulary when decoding a translation given a new source sentence. Although this is advantageous as it allows the trained model to utilize the whole vocabulary when generating a translation, doing so may be too computationally expensive, e.g., for real-time applications.\nSince training puts the target word vectors in the space so that they align well with the hidden state of the decoder only when they are likely to be a correct word, we can use only a subset of candidate target words during decoding. This is similar to what we do during training, except that at test time, we do not have access to a set of correct target words.\nThe most na\u0131\u0308ve way to select a subset of candidate target words is to take only the top-K most frequent target words, whereK can be adjusted to meet\nthe computational requirement. This, however, effectively cancels out the whole purpose of training a model with a very large target vocabulary. Instead, we can use an existing word alignment model to align the source and target words in the training corpus and build a dictionary. With the dictionary, for each source sentence, we construct a target word set consisting of the K-most frequent words (according to the estimated unigram probability) and, using the dictionary, at most K \u2032 likely target words for each source word. K and K \u2032 may be chosen either to meet the computational requirement or to maximize the translation performance on the development set. We call a subset constructed in either of these ways a candidate list."}, {"heading": "3.3 Source Words for Unknown Words", "text": "In the experiments, we evaluate the proposed approach with the neural machine translation model called RNNsearch (Bahdanau et al., 2014) (see Sec. 2.1.1). In this model, as a part of decoding process, we obtain the alignments between the target words and source locations via the alignment model in Eq. (5).\nWe can use this feature to infer the source word to which each target word was most aligned (indicated by the largest \u03b1t in Eq. (5)). This is especially useful when the model generated an [UNK] token. Once a translation is generated given a source sentence, each [UNK] may be replaced using a translationspecific technique based on the aligned source word. For instance, in the experiment, we try replacing each [UNK] token with the aligned source word or its most likely translation determined by another word alignment model. Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010)."}, {"heading": "4 Experiments", "text": "We evaluate the proposed approach in English\u2192French and English\u2192German translation tasks. We trained the neural machine translation models using only the bilingual, parallel corpora made available as a part of WMT \u201914. For each pair, the datasets we used are:\n\u2022 English\u2192French2: 2The preprocessed data can be found and downloaded from\nEuroparl v7, Common Crawl, UN, News Commentary, Gigaword\n\u2022 English\u2192German: Europarl v7, Common Crawl, News Commentary\nTo ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014). As for English\u2192German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences.\nWe evaluate the models on the WMT\u201914 test set (news-test 2014)3, while the concatenation of newstest-2012 and news-test-2013 is used for model selection (development set). Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl 4 script on the cased tokenized translations."}, {"heading": "4.1 Settings", "text": "As a baseline for English\u2192French translation, we use the RNNsearch model trained by (Bahdanau et al., 2014) with 30,000 source and target words. Another RNNsearch model is trained for English\u2192German translation with 50,000 source and target words.\nFor each language pair, we train another set of RNNsearch models with much larger vocabularies of 500,000 source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the shortlist used during training (\u03c4 in Sec. 3.1). We tried 15,000 and 30,000 for English\u2192French, and 15,000 and 50,000 for English\u2192German. We report the result later with the best performance on the development set. To stabilize parameters other than the word embeddings, at the end of the training stage, we freeze the word embeddings and tune only the other parameters for approximately two more days after the peak\nhttp://www-lium.univ-lemans.fr/\u02dcschwenk/ nnmt-shared-task/README.\n3To compare with previous submissions, we use the filtered test sets.\n4https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl\nperformance on the development set is observed.5\nWe use beam search to generate a translation given a source. During beam search, we keep a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in (Cho et al., 2014a) 6. For English\u2192French, we use a candidate list of the K = 30, 000-most frequent words and K \u2032 = 10 words per source word from the dictionary, and for English\u2192German, K = 50, 000 and K \u2032 = 20. As explained in Sec. 3.2, we test using a bilingual dictionary to accelerate decoding and to replace unknown words in translations. The bilingual dictionary is built using fast align (Dyer et al., 2013). The decision whether to use the bilingual dictionary is made based on the performance on the development set."}, {"heading": "4.2 Translation Performance", "text": "In Table. 1, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2014), (Buck et al., 2014) and (Durrani et al., 2014). We can clearly see that the RNNsearch-LV, which uses a much larger vocabulary, outperforms the baseline RNNsearch.\nIn the case of the English\u2192French task, RNNsearch-LV reached the performance level of the previous best single neural machine translation (NMT) system, even without any translationspecific techniques (Sec. 3.2\u20133.3). With these, however, the RNNsearch-LV outperformed it. The performance of the RNNsearch-LV is also better than that of a standard phrase-based translation system (Cho et al., 2014b). Furthermore, by combining 7 models, we were able to achieve a translation performance comparable to the state of the art, measured in BLEU.\nA similar trend was also observed in the case of the English\u2192German translation task. The RNNsearch-LV trained with the proposed approach outperformed the RNNsearch, although the difference was smaller. In this case, we were able to surpass the previously reported best translation result\n5This step, which was done only for the RNNsearch-LV model, did help improve BLEU scores.\n6This is why the baseline score for English\u2192French differs from the one reported in (Bahdanau et al., 2014).\nby building an ensemble of 7 models. With \u03c4 = 15, 000, the RNNsearch-LV performance worsened a little, with best BLEU scores of 33.64 and 18.41 respectively for English\u2192French and English\u2192German."}, {"heading": "4.3 Note on Ensembles", "text": "For each language pair, the seven models in the ensemble were collected at different points in time from only two training runs. Some of these models were tuned separately during the last stage of training when the word embeddings were frozen. This likely makes the composition of our ensembles suboptimal. We indirectly confirm this sub-optimality via the high cross-model BLEU scores (Freitag et al., 2014). For the English\u2192French development set, given the translations from the best single model\nas a reference, we get, before UNK replacement, 65.69, 67.06 and 67.39 BLEU scores with the models collected from the training run of the reference model. Meanwhile, with the models collected from the other training run, we get much lower scores of 57.90, 57.99 and 58.11. A better translation performance may be reached with more diverse models in an ensemble."}, {"heading": "4.4 Analysis", "text": ""}, {"heading": "4.4.1 Decoding Speed", "text": "In Table 2, we present the timing information of decoding for different models. Clearly, decoding from RNNsearch-LV with the full target vocabulary is slowest. If we use a candidate list for decoding each translation, the speed of decoding substantially improves and becomes close to the baseline RNNsearch. We find this small slowdown in decoding to be less of an issue, considering the substantial translation performance improvement achieved by the RNNsearch-LV with candidate list over the baseline model.\nA potential issue with using a candidate list is that for each source sentence, we must re-build a target vocabulary and subsequently replace a part of the parameters, which may easily become timeconsuming. We can address this issue, for instance, by building a common candidate list for multiple\nsource sentences. This may, however, let the ordering of test sentences influence the generated translations."}, {"heading": "4.4.2 Decoding Target Vocabulary", "text": "For English\u2192French, we evaluate the influence of the target vocabulary when translating the test sentences by using the union of a fixed set of 30, 000 common words and (at most) K \u2032 likely candidates for each source word according to the dictionary. Results are presented in Figure 1. With K \u2032 = 0 (not shown), the performance of the system is comparable to the baseline when not replacing the unknown words (30.12), but there is not as much improvement when doing so (31.07). As the large vocabulary model does not predict [UNK] as much during training, it is less likely to generate it when decoding, limiting the effectiveness of the post-processing step in this case. With K \u2032 = 1, which limits the diversity of allowed uncommon words, BLEU is not as good as with moderately larger K \u2032, which indicates that our models can, to some degree, correctly choose between rare alternatives. With very large K \u2032, performance degrades slowly as the NMT system may be confused by too many alternatives."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a way to extend the size of the target vocabulary for neural machine translation. The proposed approach allows us to train a model with much larger target vocabulary without any substantial increase in computational complex-\nity. It is based on the earlier work in (Bengio and Se\u0301ne\u0301cal, 2008) which used importance sampling to reduce the complexity of computing the normalization constant of the output word probability in neural language models.\nOn English\u2192French and English\u2192German translation tasks, we observed that the neural machine translation models trained using the proposed method performed better than those using only limited sets of target words. The models trained with the very large target vocabulary were found to perform as well, or sometimes better, when only a selected subset of the target vocabulary was used during decoding. This makes the proposed learning algorithm more practical.\nWhen measured by BLEU, our models showed translation performance comparable to the stateof-the-art translation systems on English\u2192French task and better than the state-of-the-art translation systems on English\u2192German task. On the English\u2192French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from (Luong et al., 2014) by more than 1 BLEU point. The performance of the ensemble of multiple models, despite its relatively less diverse composition, is approximately 0.3 BLEU points away from the best system (Durrani et al., 2014). On the English\u2192German task, the best performance of 20.98 BLEU by our model is higher than that of the previous state of the art (20.67) reported in (Buck et al., 2014)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Technical report, arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["eron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "eron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "eron et al\\.", "year": 2012}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien S\u00e9n\u00e9cal."], "venue": "IEEE Trans. Neural Networks, 19(4):713\u2013722.", "citeRegEx": "Bengio and S\u00e9n\u00e9cal.,? 2008", "shortCiteRegEx": "Bengio and S\u00e9n\u00e9cal.", "year": 2008}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings of the Python for Scientific", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference, Reykjav\u0131\u0301k, Iceland, May.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, October.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97\u2013104. Association for Computa-", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Recursive hetero-associative memories for translation", "author": ["Mikel L. Forcada", "Ram\u00f3n P. \u00d1eco."], "venue": "Jos\u00e9 Mira, Roberto Moreno-D\u0131\u0301az, and Joan Cabestany, editors, Biological and Artificial Computation: From Neuroscience to Technology, volume 1240 of Lecture", "citeRegEx": "Forcada and \u00d1eco.,? 1997", "shortCiteRegEx": "Forcada and \u00d1eco.", "year": 1997}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen."], "venue": "Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910).", "citeRegEx": "Gutmann and Hyvarinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700\u20131709. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1,", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The DCUICTCAS MT system at WMT 2014 on GermanEnglish translation task", "author": ["Liangyou Li", "Xiaofeng Wu", "Santiago Cortes Vaillo", "Jun Xie", "Andy Way", "Qun Liu."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "International Conference on Learning Representations: Workshops Track.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26,", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The RWTH Aachen GermanEnglish machine translation system for WMT 2014", "author": ["Stephan Peitz", "Joern Wuebker", "Markus Freitag", "Hermann Ney."], "venue": "In", "citeRegEx": "Peitz et al\\.,? 2014", "shortCiteRegEx": "Peitz et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS\u20192014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 0, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 20, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 20, "context": "The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 0, "context": "The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 12, "context": "Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003).", "startOffset": 150, "endOffset": 170}, {"referenceID": 20, "context": "For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2014) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words.", "startOffset": 9, "endOffset": 32}, {"referenceID": 11, "context": ", 2014) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words.", "startOffset": 11, "endOffset": 43}, {"referenceID": 0, "context": "A usual practice is to construct a target vocabulary of the k most frequent words (a so-called shortlist), where k is often in the range of 30, 000 (Bahdanau et al., 2014) to 80, 000 (Sutskever et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 20, "context": ", 2014) to 80, 000 (Sutskever et al., 2014).", "startOffset": 19, "endOffset": 43}, {"referenceID": 5, "context": "This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2014).", "startOffset": 205, "endOffset": 247}, {"referenceID": 0, "context": "This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2014).", "startOffset": 205, "endOffset": 247}, {"referenceID": 0, "context": "We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English\u2192French and English\u2192German translation using the NMT model introduced in (Bahdanau et al., 2014).", "startOffset": 176, "endOffset": 199}, {"referenceID": 0, "context": "In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2014).", "startOffset": 100, "endOffset": 123}, {"referenceID": 9, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 11, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 6, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 20, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 0, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 0, "context": "In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (Bahdanau et al., 2014).", "startOffset": 136, "endOffset": 159}, {"referenceID": 0, "context": "In (Bahdanau et al., 2014), the encoder in Eq.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": ", (Cho et al., 2014b)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "For the detailed description of the implementation, we refer the reader to the appendix of (Bahdanau et al., 2014).", "startOffset": 91, "endOffset": 114}, {"referenceID": 0, "context": "Recently proposed neural machine translation models, hence, use a shortlist of 30,000 to 80,000 most frequent words (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 20, "context": "Recently proposed neural machine translation models, hence, use a shortlist of 30,000 to 80,000 most frequent words (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 5, "context": "First of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a).", "startOffset": 159, "endOffset": 178}, {"referenceID": 17, "context": "This has been used proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 40, "endOffset": 90}, {"referenceID": 16, "context": "This has been used proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 40, "endOffset": 90}, {"referenceID": 10, "context": ", 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 46, "endOffset": 75}, {"referenceID": 15, "context": "of (Luong et al., 2014) proposed such an approach for neural machine translation.", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "The proposed approach is based on the earlier work of (Bengio and S\u00e9n\u00e9cal, 2008).", "startOffset": 54, "endOffset": 80}, {"referenceID": 0, "context": "In the experiments, we evaluate the proposed approach with the neural machine translation model called RNNsearch (Bahdanau et al., 2014) (see Sec.", "startOffset": 113, "endOffset": 136}, {"referenceID": 13, "context": "Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010).", "startOffset": 93, "endOffset": 106}, {"referenceID": 11, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 0, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 20, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 19, "context": "As for English\u2192German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences.", "startOffset": 75, "endOffset": 112}, {"referenceID": 14, "context": "As for English\u2192German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences.", "startOffset": 75, "endOffset": 112}, {"referenceID": 18, "context": "Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "As a baseline for English\u2192French translation, we use the RNNsearch model trained by (Bahdanau et al., 2014) with 30,000 source and target words.", "startOffset": 84, "endOffset": 107}, {"referenceID": 5, "context": "During beam search, we keep a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in (Cho et al., 2014a) 6.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "The bilingual dictionary is built using fast align (Dyer et al., 2013).", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": "1, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 15, "context": ", 2014), (Luong et al., 2014), (Buck et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": ", 2014), (Buck et al., 2014) and (Durrani et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": ", 2014) and (Durrani et al., 2014).", "startOffset": 12, "endOffset": 34}, {"referenceID": 6, "context": "The performance of the RNNsearch-LV is also better than that of a standard phrase-based translation system (Cho et al., 2014b).", "startOffset": 107, "endOffset": 126}, {"referenceID": 0, "context": "This is why the baseline score for English\u2192French differs from the one reported in (Bahdanau et al., 2014).", "startOffset": 83, "endOffset": 106}, {"referenceID": 0, "context": "RNNsearch is the model proposed in (Bahdanau et al., 2014), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 20, "context": ", 2014), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014).", "startOffset": 136, "endOffset": 160}, {"referenceID": 20, "context": "(?) (Sutskever et al., 2014), (\u25e6) (Luong et al.", "startOffset": 4, "endOffset": 28}, {"referenceID": 15, "context": ", 2014), (\u25e6) (Luong et al., 2014), (\u2022) (Durrani et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 7, "context": ", 2014), (\u2022) (Durrani et al., 2014), (\u2217) Standard Moses Setting (Cho et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 6, "context": ", 2014), (\u2217) Standard Moses Setting (Cho et al., 2014b), ( ) (Buck et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": ", 2014b), ( ) (Buck et al., 2014).", "startOffset": 14, "endOffset": 33}, {"referenceID": 2, "context": "It is based on the earlier work in (Bengio and S\u00e9n\u00e9cal, 2008) which used importance sampling to reduce the complexity of computing the normalization constant of the output word probability in neural language models.", "startOffset": 35, "endOffset": 61}, {"referenceID": 15, "context": "On the English\u2192French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from (Luong et al., 2014) by more than 1 BLEU point.", "startOffset": 144, "endOffset": 164}, {"referenceID": 7, "context": "3 BLEU points away from the best system (Durrani et al., 2014).", "startOffset": 40, "endOffset": 62}, {"referenceID": 4, "context": "67) reported in (Buck et al., 2014).", "startOffset": 16, "endOffset": 35}, {"referenceID": 3, "context": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 57, "endOffset": 102}], "year": 2014, "abstractText": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English\u2192German translation and almost as high performance as state-of-the-art English\u2192French translation system.", "creator": "TeX"}}}