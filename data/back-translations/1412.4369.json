{"id": "1412.4369", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Incorporating Both Distributional and Relational Semantics in Word Representations", "abstract": "We are investigating the hypothesis that word representations should include both distributional and relational semantics. To this end, we are using the Alternating Direction Multiplier Method (ADMM), which flexibly optimizes a raw-text distribution target and a relationship target on WordNet. Preliminary results to complete the knowledge base, analogy tests and analysis show that word representations trained on both objectives can in some cases bring improvements.", "histories": [["v1", "Sun, 14 Dec 2014 15:18:18 GMT  (132kb,D)", "https://arxiv.org/abs/1412.4369v1", "Under review as a workshop contribution at ICLR2015 (long version of the paper)"], ["v2", "Thu, 18 Dec 2014 12:44:01 GMT  (132kb,D)", "http://arxiv.org/abs/1412.4369v2", "Under review as a workshop contribution at ICLR2015 (long version of the paper)"], ["v3", "Sat, 21 Mar 2015 13:21:20 GMT  (132kb,D)", "http://arxiv.org/abs/1412.4369v3", "This is the long version of a short paper accepted as a workshop contribution at ICLR2015"]], "COMMENTS": "Under review as a workshop contribution at ICLR2015 (long version of the paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fried", "kevin duh"], "accepted": true, "id": "1412.4369"}, "pdf": {"name": "1412.4369.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Fried"], "emails": ["dfried@email.arizona.edu", "kevinduh@is.naist.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "We are interested in algorithms for learning vector representations of words. Recent work has shown that such representations, also known as word embeddings, can successfully capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al., 2013a), and semantic role labeling (Collobert et al., 2011).\nAlthough many kinds of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distributional semantics (Harris, 1954), embodied by J. R. Firth\u2019s dictum: \u201cYou shall know a word by the company it keeps.\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word. Intuitively, these algorithms learn to map words with similar context to nearby points in vector space.\nHowever, distributional semantics is by no means the only theory of word meaning. Relational semantics, exemplified by WordNet (Miller, 1995), defines a word by its relation with other words. Relations such as synonymy, hypernymy, and meronymy (Cruse, 1986) create a graph that links words in terms of our world knowledge and psychological predispositions. For example, stating a relation like \u201cdog is-a mammal\u201d gives a precise hierarchy between the two words, in a way that is very different from the distributional similarities observable from corpora. Arguably, the vector representation of \u201cdog\u201d ought be close to that of \u201cmammal\u201d, regardless of their distributional contexts.\nWe believe both distributional and relational semantics are valuable for word representations. Our goal is to explore how to combine these complementary approaches into a unified learning algorithm. We thus employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) for jointly optimizing both distributional and relational objectives. Its advantages include (a) flexibility in incorporating arbitrary objectives, and (b) relative ease of implementation. \u2217Currently at the University of Cambridge.\nar X\niv :1\n41 2.\n43 69\nv3 [\ncs .C\nL ]\n2 1\nIn the following, we first discuss objectives for independently learning distributional semantics (\u00a72.1) or relational semantics (\u00a72.2). The ADMM framework that optimizes both objectives is described in \u00a73 and analyzed in \u00a74. To test whether our embeddings are widely applicable, we evaluate three specific ADMM instantiations (each using different ways of incorporating relational semantics) on a wide range of tasks (\u00a75)."}, {"heading": "2 OBJECTIVES FOR REPRESENTATION LEARNING", "text": ""}, {"heading": "2.1 DISTRIBUTIONAL SEMANTICS OBJECTIVE", "text": "A standard way to implement distributional semantics in representation learning is the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 Rd, the word\u2019s embedding. An n-length sequence of words (i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1 ;wi2 . . . ;win ]. This vector x is then scored by feeding it through a two-layer neural network with h hidden nodes:\nSNLM (x) = u >(f(Ax+ b)) (1)\nwhere A \u2208 Rh\u00d7(nd) is the weight matrix and b \u2208 Rh is the bias vector for the hidden layer, u \u2208 Rh is the weight vector for the output layer, and f is the sigmoid f(t) = 1/(1 + e\u2212t).\nThe layer parameters and word embeddings of this model are trained using noise contrastive estimation (Smith & Eisner, 2005; Gutmann & Hyva\u0308rinen, 2010; Mnih & Kavukcuoglu, 2013). A sequence of text from the training corpus is corrupted by replacing a word in the sequence with a random word sampled from the vocabulary, providing an implicit negative training example xc. To train the network so that correct sequences receive a higher score than corrupted sequences, the hinge loss function is optimized:\nLNLM (x,xc) = max(0, 1\u2212 SNLM (x) + SNLM (xc)) (2)\nThe word embeddings w and network layer parameters A,u,b are trained with backpropagation, using stochastic gradient descent (SGD) over n-grams in the training corpus. We are concerned with the learned embeddings and disregard the other network parameters after training."}, {"heading": "2.2 RELATIONAL SEMANTICS OBJECTIVE", "text": "Methods for learning word representations based on relational semantics have only recently been explored. We first present a simple new objective based on WordNet graph distance (\u00a72.2.1), then discuss two recent proposals that directly model relation types (\u00a72.2.2). While our objectives focus on relational semantics in WordNet, they are extensible to other kinds of relational data, including knowledge bases like Freebase."}, {"heading": "2.2.1 GRAPH DISTANCE", "text": "In this approach, we aim to train word embeddings such that the distance between word embeddings in the vector space is a function of the distance between corresponding entities in WordNet. The primary entities in WordNet are synonym sets, or synsets. Each synset is a group of words representing one lexical concept. WordNet contains a set of relationships between these synsets, forming a directed graph where vertices are synsets and relationships are edges. The primary relationship is formed by the HYPERNYM (Is-A) relationship.\nBy treating these HYPERNYM relationships as undirected edges between synsets, we approximate semantic relatedness between synsets as the length of the shortest path between two synsets in the graph. We add a common root node at the base of all hypernym trees so that the synset graph is connected, and adopt the similarity function of Leacock & Chodorow (1998):\nSynSim(si, sj) = \u2212 log len(si, sj)\n2\u00d7 max s\u2208WordNet\ndepth(s) (3)\nwhere len(si, sj) is the length of the shortest undirected hypernym path between synsets si and sj in the graph, and depth returns the distance from the root of the hypernym hierarchy to a given synset.\nSince there is a many-to-many relationship between words and WordNet synsets, and embeddings for words, not synsets, are desired, we define the similarity between two words to be the maximum similarity between their corresponding synsets, if both words have associated synsets, and undefined otherwise:\nWordSim(i, j) = max si\u2208syn(i),sj\u2208syn(j) SynSim(si, sj) (4)\nwhere syn(i) is the set of synsets corresponding to word i.\nTo integrate WordNet similarity with word embeddings, we define the following Graph Distance loss, LGD. For a word pair (i, j), we encourage the cosine similarity between their embeddings vi and vj to match that of a scaled version of WordSim(i, j):\nLGD(i, j) =\n( vi \u00b7 vj\n||vi||2||vj ||2 \u2212 [a\u00d7WordSim(i, j) + b]\n)2 (5)\nwhere a and b are parameters that scale WordSim(i, j) to be of the same range as the cosine similarity between embeddings.\nSGD is used to train the word embeddings as well as the scalar parameters a and b. Pairs of words with defined WordSim (i.e. if both words have synsets) are sampled from the vocabulary and used as a single training instance. Details of the sampling process are presented in \u00a74."}, {"heading": "2.2.2 EXISTING RELATIONAL OBJECTIVES", "text": "We are aware of two recent approaches from the Knowledge Base literature which, in addition to representing words (entities) with vector embeddings, directly represent a knowledge base\u2019s relations as operations in the vector embedding space. These models both take as input a tuple (vl, R, vr) representing a possible relationship of type R between words vl and vr, and assign a score to the relationship.\nThe TransE model of Bordes et al. (2013) represents relationships as translations in the vector embedding space. For two words vl and vr, if the relationship R holds, i.e. (vl, R, vr) is true, then the corresponding embeddings vl,vr \u2208 Rd should be close after translation by the relation vector R \u2208 Rd. The score of a relationship tuple is the similarity between vl +R and vr, measured by the negative of the residual:\nSTransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2 (6)\nSocher et al. (2013b) introduce a Neural Tensor Network (NTN) model that allows modeling of the interaction between embeddings using tensors. The NTN model is a two-layer neural network with h hidden units and a bilinear tensor layer directly relating embeddings. This provides a more expressive model than TransE, but also requires training a larger number of parameters for each relation. The scoring function for a relation R is\nSNTN (vl, R, vr) = U >f ( v>l WRvr +VR [ vl vr ] + bR ) (7)\nwhere f is the sigmoid non-linearity applied elementwise, U \u2208 Rh is the weight vector of the output layer, and WR \u2208 Rd\u00d7d\u00d7h, VR \u2208 Rh\u00d72d and bR \u2208 Rk are a tensor, matrix, and bias vector respectively for relationship R.\nAs in the Neural Language Model, embeddings and parameters for these relational models are trained using contrastive estimation and SGD, using the hinge loss as defined in (2), where SNLM is replaced by either the STransE or SNTN scoring function on tuples.1"}, {"heading": "3 JOINT OBJECTIVE OPTIMIZATION BY ADMM", "text": "We aim to train a set of word embeddings that, along with the corresponding model parameters, satisfy both the distributional modeling objective (Sec. 2.1) and one of the relational modeling\n1As in the graph distance objective, we must map from synsets to words. In each SGD iteration, a relationship tuple (sl, R, sr) is sampled from WordNet such that synsets sl and sr contain words in the vocabulary. One word is sampled for each synset from the set of words in the vocabulary contained in the synset, producing a tuple (wl, R, wr). This is the correct tuple to be used in training, treating words as entities for the relational model. To produce the corrupted tuple, one of wl, R, or wr is randomly replaced.\nobjectives (Sec. 2.2). We adopt the Alternating Direction Method of Multipliers (ADMM) approach (Boyd et al., 2011). Rather than use the same set of embeddings to evaluate the loss functions for both the distributional and relational objectives, the embeddings are split into two sets, one for each objective, and allowed to vary independently. An augmented Lagrangian penalty term is added to constrain the corresponding embeddings for each word to have minimal difference. The advantage of this approach is that existing methods for optimizing each objective independently can be re-used, leading to a flexible and easy-to-implement algorithm.\nWe describe the ADMM formulation using graph distance as the WordNet modeling objective, but a similar formulation holds when using other relational objectives. Let w be the set of word embeddings {w1,w2, . . .wN \u2032} for the distributional modeling objective, and v be the set of word embeddings {v1,v2, . . .vN \u2032\u2032} for the relational modeling objective, where N \u2032 is the number of words in the model vocabulary of the corpus, and N \u2032\u2032 is the number of words in the model vocabularies of WordNet. Let I be the set of N words that occur in both the corpus and WordNet, i.e. the intersection. Then we define a set of vectors y = {y1,y2, . . .yN}, which correspond to Lagrange multipliers, to penalize the difference (wi \u2212 vi) between sets of embeddings for each word i in the joint vocabulary I:\nLP (w,v) = \u2211 i\u2208I ( y>i (wi \u2212 vi) ) + \u03c1 2 (\u2211 i\u2208I ||wi \u2212 vi||22 ) (8)\nIn the first term, y has same dimensionality as w and v, so a scalar penalty is maintained for each entry in every embedding vector. The second residual penalty term with hyperparameter \u03c1 is added to avoid saddle points. Later we shall see that \u03c1 can be viewed as a step-size during the update of y.\nFinally, this augmented Lagrangian term (Eq. 8) is added to the sum of the loss terms for each objective (Eq. 2 and Eq. 5). Let \u03b8 = (u,A,b) be the parameters of the language modeling objective, and \u03c6 = (a, b) be the parameters of the WordNet graph distance objective. The final loss function we optimize becomes:\nL = LNLM (w, \u03b8) + LGD(v, \u03c6) + LP (w,v) (9)\nThe ADMM algorithm proceeds by repeating the following three steps until convergence:\n1. Perform stochastic gradient descent on w and \u03b8 to minimize LNLM + LP , with all other parameters fixed.\n2. Perform stochastic gradient descent on v and \u03c6 to minimize LGD + LP , with all other parameters fixed.\n3. For all embeddings i corresponding to words in both the n-gram and relational training sets, update the constraint vector yi:\nyi := yi + \u03c1(wi \u2212 vi) (10)\nSince LNLM and LGD share no parameters, the gradient descent step for w in Step 1 does not depend on \u03c6 (the parameters of the relational objective). The derivative of LNLM (w, \u03b8)+LP (w,v) with respect to wi is simply the derivative of LNLM (w, \u03b8) plus yi + \u03c1 (wi \u2212 vi); the second term acts like a bias term to make wi closer to vi. Similarly, the gradient descent step for v does not depend on \u03b8, so Step 2 can be optimized easily. In Step 3, a large difference (wi \u2212 vi) causes yi to become large, and therefore increases the constraint for the two sets of embeddings to be similar in both Steps 1 and 2.\nWe note that it is possible to introduce a weight parameter \u03b1 \u2208 [0, 1] into the joint loss function (Eq. 9) to prioritize either LNLM or LGD:\nL = \u03b1LNLM (w, \u03b8) + (1\u2212 \u03b1)LGD(v, \u03c6) + LP (w,v)\nEmpirically we found that while the difference between using joint objective compared to single objective is large, the exact value of \u03b1 does not significantly change the results; all experiments here use equal weighting."}, {"heading": "4 DATA SETUP AND ANALYSIS", "text": "The distributional objective LNLM is trained using 5-grams from the Google Books English corpus, distributed by UC Berkeley in the Web 1T format2. This corpus contains over 180 million 5-gram types and 31 billion 5-gram tokens. In our experiments, 5-grams are preprocessed by lowercasing all words. The top 50,000 unigrams by frequency are used as the vocabulary. All less-frequently occurring words are replaced with a token, RARE, which has its own vector space embedding. In each ADMM iteration, a block of 100,000 n-grams is sampled from the corpus. Each n-gram in the block (and a corrupted, noise-contrastive version of the n-gram, see \u00a72.1) is used to perform gradient descent on the distributional loss function LNLM or its ADMM equivalent.\n2http://tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/\nTraining data for the relational objective varies depending on whether the graph distance objective (GD) or one of the two relational objectives (NTN or TransE) is used. When the graph distance objective is used, word pairs with defined graph distance (e.g. words that are contained in WordNet synsets) are randomly sampled and used as training instances. In each ADMM iteration, 100,000 words are sampled with replacement from the vocabulary. For each sampled word w, up to 5 other words v with defined graph distance to word w are sampled from the vocabulary. Each pair w, v is then used as a training instance for gradient descent on the LGD loss function (\u00a72.2.1) or its ADMM equivalent.\nWhen either NTN or TransE is used as the relational objective, WordNet relationship tuples (s1, R, s2) where synsets s1 and s2 contain words in the vocabulary, are used as the training instances for stochastic gradient descent using noise contrastive estimation (\u00a72.2.2). For comparison with the existing work of Socher et al. (2013b), we use their dataset, which contains training, development, and testing splits for 11 WordNet relationships (Table 1). The entire training set is presented to the network in randomized order, one instance at a time, during each iteration of training.\nWe next provide an analysis of the behavior of the ADMM joint model (LNLM + LGD) on the training set in Figure 1. Figure 1(a) plots the learning curve by training iteration using varying values of the \u03c1 hyperparameter. Although establishing convergence guarantees for non-convex loss functions for ADMM is theoretically still an open question (e.g. LNLM is a non-convex multi-layer neural net), we empirically observe convergence on our dataset for various values of \u03c1. Further, in accordance with previous works (Boyd et al., 2011), ADMM attains a reasonable objective value relatively quickly in a few iterations; our loss converges around 100 iterations.3 Figure 1(b) shows the change in the mean norms of the Lagrange multipliers ||yi||2. The magnitude of these norms indicates the degree to which the wi and vi vectors are being constrained in the current ADMM iteration. The norm gradually increases, indicating the tightening of constraints in each iteration. As expected, larger values of \u03c1 lead to faster increases of ||yi||2. Finally, Figure 1(c) shows the normalized difference between the resulting sets of embeddings w and v, which decreases as desired.4\nAs we perform SGD on ever-more data, the norms of w and v generally increase. A conventional solution is to add the L2 norms of w and v as additional regularizers in the objective function. We found that, on the knowledge base completion task (\u00a75), L2 regularization decreased the performance of all ADMM models, but slightly increased the performance of the NTN and TransE single objective models; on the analogy test tasks, it decreased the performance of the GD, NLM, and all ADMM models, but increased the performance of NTN and TransE. Since regularization hurt performance for the majority of models, and the evaluation results converge regardless, we use unregularized models in all experiments reported here."}, {"heading": "5 TASK-SPECIFIC EVALUATION", "text": "We now compare the embeddings learned with different objectives on three different tasks. For all experiments, we use 50-dimensional embeddings taken from iteration 1000 of training, with \u03c1 = 0.05 for ADMM."}, {"heading": "5.1 KNOWLEDGE BASE COMPLETION", "text": "Models trained using either the NTN or TransE relational modeling objective (\u00a72.2.2) learn a vectorspace representation of relationships in WordNet. We use the methodology and datasets of Socher et al. (2013b) to evaluate the models\u2019 ability to classify relationship triplets as correct or incorrect. This relation classification is useful for \u201ccompleting\u201d or \u201cextending\u201d a knowledge base with new facts. The testing set consists of correct relationship tuples that are present in WordNet, and incorrect tuples created by randomly switching entities from correct tuples. A development set is used to determine threshold scores TR for each relation that maximize classification accuracy when tuples\n3On a 3.3Hz Xeon CPU, this took about 9 hours for ADMM, not more than the 7 hours for independent LNLM and 3 hours for independent LGD objectives combined.\n4The reason for the peak around iteration 50 in Figure 1(c) is that the embeddings begin with similar random initializations, so initially differences are small; as ADMM starts to see more data, w and v diverge, but converge eventually as y become large.\n(vl, R, vr) having S(vl, R, vr) \u2265 TR are classified as correct, and tuples having a score lower than TR are classified as incorrect.\nModels trained using a joint objective (NLM with either TransE or NTN)5 are evaluated by taking the v set of vectors (those learned for the relational objective) and passing these through the relational objective function to score a given tuple. The test accuracies are shown in Table 1. Overall, the NTN baseline achieves 80.95% accuracy6, while the joint objective NTN+NLM improves it to 81.27%. Similarly, TransE+NLM (83.10%) outperforms the TransE baseline (82.87%). We conclude that ADMM can give small albeit noticeable improvement to TransE and NTN. Table 1 also shows the accuracies by relation type. We note that TransE+NLM performs at least as well as the TransE single objective across all categories except TYPEOF and MEMBERHOLONYM.\nWe also experimented with using the average of w and v vectors (rather than v itself after ADMM) for relation classification. This produced lower overall accuracies for the joint model (75.38% for NTN+NLM and 71.73% for TransE+NLM), implying that differences between w and v may still be important in actual tasks."}, {"heading": "5.2 ANALOGY TESTS FOR RELATIONAL SIMILARITY", "text": "SemEval-2012 Task 2 (Jurgens et al., 2012) is a relational similarity task similar to SAT-style analogy questions. The task is to score word pairs by the degree to which they belong in a relation class defined by a set of example word pairs. For example, the relation class REVERSE contains the example pairs (attack,defend) and (buy,sell). There are 69 testing relation categories, each with three or four example word pairs. In the evaluation, the model is shown a number of testing relation pairs in each category and scores each testing pair according to its similarity to the example relation pairs. These similarity scores are then compared to human similarity judgements. This is an useful task to test whether the positioning of learned embeddings in vector space leads to some meaningful semantics.\nFollowing Zhila et al. (2013), we evaluate the embeddings in this task on their ability to represent relations as translations in the vector space. A given relation pair (word1, word2) is represented as the vector difference between the two words, w2 \u2212 w1, where w1 and w2 are the embeddings of words word1 and word2. Similarities between the example relations and the relations to be scored are computed using cosine distance of these resulting embedding representations. One evaluation metric is the Spearman\u2019s correlation coefficient between the similarity scores output by the model and the scores assigned to pairs by human judges. The second metric is the MaxDiff accuracy, which involves choosing both the most similar and least similar example pairs to a given target pair from a set of four or five pre-defined choices.\n5GD is not evaluated since it does not model relation types. 6Our NTN results differ from those reported in (Socher et al., 2013b), likely due to differences in the\noptimizer (SGD vs L-BFGS), embedding size, and the use or lack of regularizer.\nA summary of the results is shown in Table 2. We observe:\n1. NLM achieves scores competitive with the recurrent neural language models used in (Zhila et al., 2013), which had a maximum accuracy of 0.41 and correlation of 0.23.\n2. GD by itself also achieves comparable scores, with 0.42 accuracy and 0.28 correlation. It is interesting to note that independent distributional and relational objectives achieve similar results for this task.\n3. The joint objective does not appear to help in this task, however. E.g., GD+NLM does not outperform GD; while NTN+NLM outperforms NTN, it does not outperform NLM.\nTo analyze this result further, we show the scores by relation category in Figure 2. Despite NLM, GD, GD+NLM, and NTN+NLM achieving similar top scores overall, we observe that the scores by category are considerably varied. We conclude our current objectives, either distribution or relational, are too coarse-grained to reliably address the analogy test. In particular, the relation categories for this task do not correspond to the relation types on WordNet. Since it is infeasible to expect a large WordNet-like resource that is annotated in the particular categories for this task, an objective that includes some form of unsupervised relation clustering may be necessary."}, {"heading": "5.3 DEPENDENCY PARSING", "text": "Dependency parsing experiments are performed on the SANCL2012 \u201cParsing the Web\u201d data (Petrov & McDonald, 2012). The setup is to train a parser on news domain (Wall Street Journal) and evaluate on out-of-domain web text. Our goal is to see whether the performance of a standard parser can be improved by simply using embeddings as additional features. This can be seen as a kind of semisupervised feature learning (Koo et al., 2008).\nFollowing (Wu et al., 2013), we first cluster the embeddings using k-means, then incorporate the cluster ids as features. The reasoning is that discrete cluster ids are easier to incorporate into existing parsers as conjunctions of features. We use the standard first-order MST parser7. For simplicity, we only attempt to cluster the embeddings into k = 64 clusters and report results on the development set; a more extensive experiment involving multiple k and model selection is left as future work.\nTable 3 shows the labeled attachment scores (LAS), i.e. the accuracy of predicting both correct syntactic attachment and relation label for each word. We observe:\n1. Incorporating embeddings from joint objective training always helps; all of these embeddings improve upon the case of no embeddings (None) in all five domains. This is a nice result considering that our embeddings are trained on Google Books, not SANCL, and have a 9-13% token out-of-vocabulary rate on the data.\n2. In contrast, improvements from embeddings trained from a single relational objective are mixed, and are in general poorer than those trained from a single distributional objective (NLM). This suggests distributional information may be more effective for this task.\n3. The best results are achieved by the joint models NLM+GD (average LAS of 76.18) and NLM+NTN (76.14). The improvement over NLM (76.03) is not large, but we believe this is\n7sourceforge.net/projects/mstparser/\na promising result nevertheless; it implies that our joint objective does indeed complement the strong results of distributional semantics."}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "We advocate for a word representation learning algorithm that jointly implements both distributional and relational semantics. Our first contribution is an investigation of the ADMM algorithm, which flexibly combines multiple objectives. We show that ADMM converges quickly and is an effective method for combining multiple sources of information and linguistic intuitions into word representations. Note that other approaches for combining objectives besides ADMM are possible, including direct gradient descent on the joint objective, or concatenation of word representations individually optimized on independent objectives. A comparison of various approaches for multi-objective optimization in learning word representations, where the optimization space is riddled with local optima, is worthwhile as future work.\nThe second contribution is a preliminary evaluation of three specific instantiations of ADMM, combining the NLM distributional objective with Graph Distance, TransE, or NTN relational objectives. In both the knowledge base completion and dependency parsing tasks, we demonstrate that the combined objective provides promising minor improvements compared to the single objective case. In the analogy task, we show that the combined objective is comparable to the single objective, and learns a very different kind of word representation.\nTo the best of our knowledge, some recent work (Xu et al., 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours. The main differences are in their optimization methods (i.e. gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al. (2014) introduces a post-processing graph-based method) as well as alternate relational objectives, e.g. Yu & Dredze (2014) formulate a skip-gram objective where word embeddings are trained to predict other words they share relations with. A detailed comparison on the same datasets would be beneficial to understand the impact of these design choices.\nCompared to the large body of existing work in word representations (e.g. LSA, (Deerwester et al., 1990), ESA (Gabrilovich & Markovitch, 2007), SDS (Mitchell & Lapata, 2008)), the promise of recent learning-based approaches is that they enable a flexible definition of optimization objectives. As future work, we hope to further explore objectives beyond distributional and relational semantics and understand what objectives work best for each target task or application."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported by the Flinn Scholarship during the course of this work. We thank Haixun Wang, Jun\u2019ichi Tsujii, Tim Baldwin, Yuji Matsumoto, and several anonymous reviewers for helpful discussions at various stages of the project."}], "references": [{"title": "A neural probabilistic language models", "author": ["REFERENCES Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Lexical Semantics", "author": ["Cruse", "Alan D"], "venue": null, "citeRegEx": "Cruse and D.,? \\Q1986\\E", "shortCiteRegEx": "Cruse and D.", "year": 1986}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Dumais", "Susan T", "Furnas", "George W", "Landauer", "Thomas K", "Harshman", "Richard"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Faruqui", "Manaal", "Dodge", "Jesse", "Jauhar", "Sujay Kumar", "Dyer", "Chris", "Hovy", "Eduard H", "Smith", "Noah A"], "venue": "CoRR, abs/1411.4166,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In IJCAI,", "citeRegEx": "Gabrilovich and Markovitch,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["Jurgens", "David A", "Turney", "Peter D", "Mohammad", "Saif M", "Holyoak", "Keith J"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Jurgens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo", "Terry", "Carreras", "Xavier", "Collins", "Michael"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Claudia", "Chodorow", "Martin"], "venue": "WordNet: An Electronic Lexical Database,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tom\u00e1s", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In ACL, pp", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Continuous space language models", "author": ["Schwenk", "Holger"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk and Holger.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2007}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Smith", "Noah A", "Eisner", "Jason"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervise learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Wang", "Mengqiu", "Manning", "Christopher D"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Generalization of words for chinese dependency parsing", "author": ["Wu", "Xianchao", "Zhou", "Jie", "Sun", "Yu", "Liu", "Zhanyi", "Dianhai", "Hua", "Wang", "Haifeng"], "venue": "In Proceedings of the 13th International Conference on Parsing Technologies (IWPT),", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Xu", "Chang", "Bai", "Yanlong", "Bian", "Jiang", "Gao", "Bin", "Wang", "Gang", "Liu", "Xiaoguang", "Tie-Yan"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Mo", "Dredze", "Mark"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["Zhila", "Alisa", "Yih", "Wen-tau", "Meek", "Christopher", "Zweig", "Geoffrey", "Mikolov", "Tomas"], "venue": "In NAACL-HLT,", "citeRegEx": "Zhila et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhila et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 22, "context": ", 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a), and semantic role labeling (Collobert et al., 2011).", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 2, "context": "We thus employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) for jointly optimizing both distributional and relational objectives.", "startOffset": 123, "endOffset": 142}, {"referenceID": 3, "context": "A standard way to implement distributional semantics in representation learning is the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 R, the word\u2019s embedding.", "startOffset": 118, "endOffset": 142}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relationships as translations in the vector embedding space.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "We adopt the Alternating Direction Method of Multipliers (ADMM) approach (Boyd et al., 2011).", "startOffset": 73, "endOffset": 92}, {"referenceID": 2, "context": "Further, in accordance with previous works (Boyd et al., 2011), ADMM attains a reasonable objective value relatively quickly in a few iterations; our loss converges around 100 iterations.", "startOffset": 43, "endOffset": 62}, {"referenceID": 19, "context": "For comparison with the existing work of Socher et al. (2013b), we use their dataset, which contains training, development, and testing splits for 11 WordNet relationships (Table 1).", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "We use the methodology and datasets of Socher et al. (2013b) to evaluate the models\u2019 ability to classify relationship triplets as correct or incorrect.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": "SemEval-2012 Task 2 (Jurgens et al., 2012) is a relational similarity task similar to SAT-style analogy questions.", "startOffset": 20, "endOffset": 42}, {"referenceID": 9, "context": "SemEval-2012 Task 2 (Jurgens et al., 2012) is a relational similarity task similar to SAT-style analogy questions. The task is to score word pairs by the degree to which they belong in a relation class defined by a set of example word pairs. For example, the relation class REVERSE contains the example pairs (attack,defend) and (buy,sell). There are 69 testing relation categories, each with three or four example word pairs. In the evaluation, the model is shown a number of testing relation pairs in each category and scores each testing pair according to its similarity to the example relation pairs. These similarity scores are then compared to human similarity judgements. This is an useful task to test whether the positioning of learned embeddings in vector space leads to some meaningful semantics. Following Zhila et al. (2013), we evaluate the embeddings in this task on their ability to represent relations as translations in the vector space.", "startOffset": 21, "endOffset": 838}, {"referenceID": 9, "context": "(Jurgens et al., 2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "NLM achieves scores competitive with the recurrent neural language models used in (Zhila et al., 2013), which had a maximum accuracy of 0.", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": "This can be seen as a kind of semisupervised feature learning (Koo et al., 2008).", "startOffset": 62, "endOffset": 80}, {"referenceID": 24, "context": "Following (Wu et al., 2013), we first cluster the embeddings using k-means, then incorporate the cluster ids as features.", "startOffset": 10, "endOffset": 27}, {"referenceID": 25, "context": "To the best of our knowledge, some recent work (Xu et al., 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours.", "startOffset": 47, "endOffset": 105}, {"referenceID": 6, "context": "To the best of our knowledge, some recent work (Xu et al., 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours.", "startOffset": 47, "endOffset": 105}, {"referenceID": 25, "context": "gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al.", "startOffset": 60, "endOffset": 96}, {"referenceID": 5, "context": "LSA, (Deerwester et al., 1990), ESA (Gabrilovich & Markovitch, 2007), SDS (Mitchell & Lapata, 2008)), the promise of recent learning-based approaches is that they enable a flexible definition of optimization objectives.", "startOffset": 5, "endOffset": 30}, {"referenceID": 5, "context": ", 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours. The main differences are in their optimization methods (i.e. gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al. (2014) introduces a post-processing graph-based method) as well as alternate relational objectives, e.", "startOffset": 27, "endOffset": 274}, {"referenceID": 5, "context": ", 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours. The main differences are in their optimization methods (i.e. gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al. (2014) introduces a post-processing graph-based method) as well as alternate relational objectives, e.g. Yu & Dredze (2014) formulate a skip-gram objective where word embeddings are trained to predict other words they share relations with.", "startOffset": 27, "endOffset": 391}], "year": 2015, "abstractText": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "creator": "LaTeX with hyperref package"}}}