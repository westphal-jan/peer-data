{"id": "1412.5617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Learning from Data with Heterogeneous Noise using SGD", "abstract": "We consider learning data of different quality, which can be obtained from different heterogeneous sources, to be a challenging problem. In this paper, we will instead use a model in which data is observed by heterogeneous noise, with the sound level reflecting the quality of the data source. We will study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem occurs naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning data with different quality labels.", "histories": [["v1", "Wed, 17 Dec 2014 21:15:06 GMT  (42kb,D)", "http://arxiv.org/abs/1412.5617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuang song", "kamalika chaudhuri", "anand d sarwate"], "accepted": false, "id": "1412.5617"}, "pdf": {"name": "1412.5617.pdf", "metadata": {"source": "CRF", "title": "Learning from Data with Heterogeneous Noise using SGD", "authors": ["Shuang Song", "Kamalika Chaudhuri", "Anand D. Sarwate"], "emails": ["shs037@eng.ucsd.edu", "kamalika@cs.ucsd.edu", "asarwate@ece.rutgers.edu"], "sections": [{"heading": null, "text": "The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate."}, {"heading": "1 Introduction", "text": "Modern large-scale machine learning systems often integrate data from several different sources. In many cases, these sources provide data of a similar type (i.e. with the same features) but collected under different circumstances. For example, patient records from different studies of a particular drug may be combined to perform a more comprehensive analysis, or a collection of images with annotations from experts as well as non-experts may be combined to learn a predictor. In particular, data from different sources may be of varying quality. In this paper we adopt a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn from data of heterogeneous quality.\nIn full generality, learning from heterogeneous data is essentially the problem of domain adaptation \u2013 a challenge for which good and complete solutions are difficult to obtain. Instead, we focus on the special case of heterogeneous noise and show how to use information about the data quality to improve the performance of learning algorithms which ignore this information.\nTwo concrete instances of this problem motivate our study: locally differentially private learning from multiple sites, and classification with random label noise. Differential privacy (Dwork et al.,\n\u2217Computer Science and Engineering Dept., University of California, San Diego, shs037@eng.ucsd.edu \u2020Computer Science and Engineering Dept., University of California, San Diego, kamalika@cs.ucsd.edu \u2021Electrical and Computer Engineering Dept., Rutgers University, asarwate@ece.rutgers.edu\nar X\niv :1\n41 2.\n56 17\nv1 [\ncs .L\nG ]\n1 7\nD ec\n2 01\n2006b,a) is a privacy model that has received significant attention in machine-learning and datamining applications. A variant of differential privacy is local privacy \u2013 the learner can only access the data via noisy estimates, where the noise guarantees privacy (Duchi et al., 2012, 2013). In many applications, we are required to learn from sensitive data collected from individuals with heterogeneous privacy preferences, or from multiple sites with different privacy requirements; this results in the heterogeneity of noise added to ensure privacy. Under random classification noise (RCN) (Kearns, 1998), labels are randomly flipped before being presented to the algorithm. The heterogeneity in the noise addition comes from combining labels of variable quality \u2013 such as labels assigned by domain experts with those assigned by a crowd.\nTo our knowledge, Crammer et al. (2006) were the first to provide a theoretical study of how to learn classifiers from data of variable quality. In their formulation, like ours, data is observed through heterogeneous noise. Given data with known noise levels, their study focuses on finding an optimal ordering of the data and a stopping rule without any constraint on the computational complexity. We instead shift our attention to studying computationally efficient strategies for learning classifiers from data of variable quality.\nWe propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011). We assume that the training data are accessed through an oracle which provides an unbiased but noisy estimate of the gradient of the objective. The noise comes from two sources: the random sampling of a data point, and additional noise due to the data quality. Our two motivating applications \u2013 learning with local differential privacy and learning from data of variable quality \u2013 can both be modeled as solving a regularized convex optimization problem using SGD. Learning from data with heterogeneous noise in this framework thus reduces to running SGD with noisy gradient estimates, where the magnitude of the added noise varies across iterations.\nMain results. In this paper we study noisy stochastic gradient methods when learning from multiple data sets with different noise levels. For simplicity we consider the case where there are two data sets, which we call Clean and Noisy. We process these data sets sequentially using SGD with learning rate O(1/t). In a future full version of this work we also analyze averaged gradient descent (AGD) with learning rate O(1/ \u221a t). We address some basic questions in this setup:\nIn what order should we process the data? Suppose we use standard SGD on the union of Clean and Noisy. We show theoretically and empirically that the order in which we should process the datasets to get good performance depends on the learning rate of the algorithm: in some cases we should use the order (Clean,Noisy) and in others (Noisy,Clean).\nCan we use knowledge of the noise rates? We show that using separate learning rates that depend on the noise levels for the clean and noisy datasets improves the performance of SGD. We provide a heuristic for choosing these rates by optimizing an upper bound on the error for SGD that depends on the ratio of the noise levels. We analytically quantify the performance of our algorithm in two regimes of interest. For moderate noise levels, we demonstrate empirically that our algorithm outperforms using a single learning rate and using clean data only.\nDoes using noisy data always help? The work of Crammer et al. (2006) suggests that if the noise level of noisy data is above some threshold, then noisy data will not help. Moreover, when the noise levels are very high, our heuristic does not always empirically outperform simply using the clean data. On the other hand, our theoretical results suggest that changing the learning rate can make noisy data useful. How do we resolve this apparent contradiction?\nWe perform an empirical study to address this question. Our experiments demonstrate that very often, there exists a learning rate at which noisy data helps; however, because the actual noise level may be far from the upper bound used in our algorithm, our optimization may not choose the best learning rate for every data set. We demonstrate that by adjusting the learning rate we can\nstill take advantage of noisy data. For simplicity we, like previous work Crammer et al. (2006), assume that the algorithms know the noise levels exactly. However, our algorithms can still be applied in the presence of approximate knowledge of the noise levels, and our result on the optimal data order only needs to know which dataset has more noise.\nRelated Work. There has been significant work on the convergence of SGD assuming analytic properties of the objective function, such as strong convexity and smoothness. When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009).\nThere is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy. Our work is an extension of these papers to heterogeneous privacy requirements.\nCrammer et al. (2006) study classification when the labels in each data set are corrupted by RCN of different rates. Assuming the classifier minimizing the empirical 0/1 classification error can always be found, they propose a general theoretical procedure that processes the datasets in increasing order of noise, and determines when to stop using more data. In contrast, our noise model is more general and we provide a polynomial time algorithm for learning. Our results imply that in some cases the algorithm should process the noisy data first, and finally, our algorithm uses all the data."}, {"heading": "2 The Model", "text": "We consider linear classification in the presence of noise. We are given T labelled examples (x1, y1), . . . , (xT , yT ), where xi \u2208 Rd, and yi \u2208 {\u22121, 1} and our goal is to find a hyperplane w that largely separates the examples labeled 1 from those labeled \u22121. A standard solution is via the following regularized convex optimization problem:\nw\u2217 = argmin w\u2208W\nf(w) := \u03bb 2 \u2016w\u20162 + 1 T T\u2211 i=1 `(w, xi, yi). (1)\nHere ` is a convex loss function, and \u03bb2\u2016w\u2016 2 is a regularization term. Popular choices for ` include the logistic loss `(w, x, y) = log(1 + e\u2212yw >x) and the hinge loss `(w, x, y) = max(0, 1\u2212 yw>x).\nStochastic Gradient Descent (SGD) is a popular approach to solving (1): starting with an initial w1, at step t, SGD updates wt+1 using the point (xt, yt) as follows:\nwt+1 = \u03a0W (wt \u2212 \u03b7t(\u03bbwt +\u2207`(wt, xt, yt))) . (2)\nHere \u03a0 is a projection operator onto the convex feasible set W, typically set to {w : \u2016w\u20162 \u2264 1/\u03bb} and \u03b7t is a learning rate (or step size) which specifies how fast wt changes. A common choice for the learning rate for the case when \u03bb > 0 is c/t, where c = \u0398(1/\u03bb)."}, {"heading": "2.1 The Heterogeneous Noise Model", "text": "We propose an abstract model for heterogeneous noise that can be specialized to two important scenarios: differentially private learning, and random classification noise. By heterogeneous noise we mean that the distribution of the noise can depend on the data points themselves. More formally, we assume that the learning algorithm may only access the labeled data through an oracle G which, given a w \u2208 Rd, draws a fresh independent sample (x, y) from the underlying data distribution, and returns an unbiased noisy gradient of the objective function \u2207f(w), based on the example (x, y):\nE [G(w)] = \u03bbw +\u2207`(w, x, y), E [ \u2016G(w)\u20162 ] \u2264 \u03932. (3)\nThe precise manner in which G(w) is generated depends on the application. Define the noise level for the oracle G as the constant \u0393 in (3); larger \u0393 means more noisy data. Finally, to model finite training datasets, we assume that an oracle G may be called only a limited number of times.\nObserve that in this noise model, we can easily use the noisy gradient returned by G to perform SGD. The update rule becomes:\nwt+1 = \u03a0W (wt \u2212 \u03b7tG(wt)) . (4)\nThe SGD estimate is wt+1. In practice, we can implement an oracle such as G based on a finite labelled training set D as follows. We apply a random permutation on the samples in D, and at each invocation, compute a noisy gradient based on the next sample in the permutation. The number of calls to the oracle is limited to |D|. If the samples in D are drawn iid from the underlying data distribution, and if any extraneous noise added to the gradient at each iteration is unbiased and drawn independently, then this process will implement the oracle correctly.\nTo model heterogeneous noise, we assume that we have access to two oracles G1 and G2 implemented based on datasets D1 and D2, which can be called at most |D1| and |D2| times respectively. For j = 1, 2, the noise level of oracle Gj is \u0393j , and the values of \u03931 and \u03932 are known to the algorithm. In some practical situations, \u03931 and \u03932 will not be known exactly; however, our algorithm in Section 4 also applies when approximate noise levels are known, and our algorithm in Section 3 applies even when only the relative noise levels are known."}, {"heading": "2.1.1 Local Differential Privacy", "text": "Local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012; Kasiviswanathan et al., 2008) is a strong notion of privacy motivated by differential privacy (Dwork et al., 2006b). An untrusted algorithm is allowed to access a perturbed version of a sensitive dataset through a sanitization interface, and must use this perturbed data to perform some estimation. The amount of perturbation is controlled by a parameter , which measures the privacy risk.\nDefinition 1 (Local Differential Privacy). Let D = (X1, . . . , Xn) be a sensitive dataset where each Xi \u2208 D corresponds to data about individual i. A randomized sanitization mechanism M which outputs a disguised version (U1, . . . , Un) of D is said to provide -local differential privacy to individual i, if for all x, x\u2032 \u2208 D and for all S \u2286 S,\nPr(Ui \u2208 S|Xi = x) \u2264 e Pr(Ui \u2208 S|Xi = x\u2032). (5)\nHere the probability is taken over the randomization in the sanitization mechanism, and is a parameter that measures privacy risk where smaller means less privacy risk.\nConsider learning a linear classifier from a sensitive labelled dataset while ensuring local privacy of the participants. This problem can be expressed in our noise model by setting the sanitization mechanism as the oracle. Given a privacy risk , for w \u2208 Rd, the oracle GDP draws a random labelled sample (x, y) from the underlying data distribution, and returns the noisy gradient of the objective function at w computed based on (x, y) as\nGDP(w) = \u03bbw +\u2207`(w, x, y) + Z, (6)\nwhere Z is independent random noise drawn from the density: \u03c1(z) \u221d e\u2212( /2)\u2016z\u2016. Duchi et al. (2012) showed that this mechanism provides -local privacy assuming analytic conditions on the loss function, bounded data, and that the oracle generates a fresh random sample at each invocation. The following result shows how to set the parameters to fit in our heterogeneous noise model. The full proof is provided in Appendix A.2.\nTheorem 1. If \u2016\u2207`(w, x, y)\u2016 \u2264 1 for all w and (x, y), then GDP(w) is -local differentially private. Moreover, for any w such that \u2016w\u2016 \u2264 1\u03bb , E[G DP(w)] = \u03bbw +\u2207E(x,y)[`(w, x, y)], and\nE[\u2016GDP(w)\u20162] \u2264 4 + 4(d 2 + d)\n2 .\nProof. (Sketch) The term 4 comes from upper bounding E[\u2016\u03bbw+\u2207`(w, x, y)\u20162] by maxw,x,y \u2016\u03bbw+ \u2207`(w, x, y)\u20162 using \u2016w\u2016 \u2264 1/\u03bb and \u2016\u2207`(w, x, y)\u2016 \u2264 1. The term 4(d2 + d)/ 2 comes from properties of the noise distribution.\nIn practice, we may wish to learn classifiers from multiple sensitive datasets with different privacy parameters. For example, suppose we wish to learn a classifier from sensitive patient records in two different hospitals holding data sets D1 and D2, respectively. The hospitals have different privacy policies, and thus different privacy parameters 1 and 2. This corresponds to a heterogeneous noise model in which we have two sanitizing oracles \u2013 GDP1 and GDP2 . For j = 1, 2, GDPj implements a differentially private oracle with privacy parameter j based on dataset Dj and may be called at most |Dj | times."}, {"heading": "2.1.2 Random Classification Noise", "text": "In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, y\u03031), . . . , (xT , y\u0303T ), where each y\u0303i \u2208 {\u22121, 1} has been obtained by independently flipping the true label yi with some probability \u03c3. Natarajan et al. (2013) showed that solving\nargmin w\n\u03bb 2 \u2016w\u20162 + 1 T T\u2211 i=1 \u02dc\u0300(w, xi, y\u0303i, \u03c3) (7)\nyields a linear classifier from data with random classification noise, where \u02dc\u0300 is a surrogate loss function corresponding to a convex loss `:\n\u02dc\u0300(w, x, y, \u03c3) = (1\u2212 \u03c3)`(w, x, y)\u2212 \u03c3`(w, x,\u2212y)\n1\u2212 2\u03c3 ,\nand \u03c3 is the probability that each label is flipped. This problem can be expressed in our noise model using an oracle GRCN which on input w draws a fresh labelled example (x, y\u0303) and returns\nGRCN(w) = \u03bbw +\u2207\u02dc\u0300(w, x, y\u0303, \u03c3).\nThe SGD updates in (4) with respect to GRCN minimize (7). If \u2016x\u2016 \u2264 1 and \u2016\u2207`(w, x, y)\u2016 \u2264 1, we have E [GRCN(w)] = \u03bbw + \u2207`(w, x, y) and E [ \u2016GRCN(w)\u201622 ] \u2264 3 + 1/(1\u2212 2\u03c3)2, under the random classification noise assumption, so the oracle GRCN satisfies the conditions in (3) with \u03932 = 3 + 1/(1\u2212 2\u03c3)2.\nIn practice, we may wish to learn classifiers from multiple datasets with different amounts of classification noise (Crammer et al., 2006); for example, we may have a small dataset D1 labeled by domain experts, and a larger noisier dataset D2, labeled via crowdsourcing, with flip probabilities \u03c31 and \u03c32. We model this scenario using two oracles \u2013 GRCN1 and GRCN2 . For j = 1, 2, oracle GRCNj is implemented based on Dj and flip probability \u03c3j , and may be called at most |Dj | times."}, {"heading": "3 Data order depends on learning rate", "text": "Suppose we have two oracles GC (for \u201cclean\u201d) and GN (for \u201cnoisy\u201d) implemented based on datasets DC, DN with noise levels \u0393C,\u0393N (where \u0393C < \u0393N) respectively. In which order should we query the oracle when using SGD? Perhaps surprisingly, it turns out that the answer depends on the learning rate. Below, we show a specific example of a convex optimization problem such that with \u03b7t = c/t, the optimal ordering is to use GC first when c \u2208 (0, 1/\u03bb), and the optimal ordering is to use GN first when c > 1/\u03bb.\nLet |DC|+ |DN| = T and consider the convex optimization problem:\nmin w\u2208W\n\u03bb 2 \u2016w\u20162 \u2212 1 T T\u2211 i=1 yiw >xi, (8)\nwhere the points {(xi, yi)} are drawn from the underlying distribution by GC or GN. Suppose G(w) = \u03bbw \u2212 yx+ Z where Z is an independent noise vector such that E[Z] = 0, E[\u2016Z\u20162] = V 2C if G is GC, and E[\u2016Z\u20162] = V 2N if G is GN with V 2N \u2265 V 2C .\nFor our example, we consider the following three variants of SGD: CF and NF for \u201cclean first\u201d and \u201cnoisy first\u201d and AO for an \u201carbitrary ordering\u201d:\n1. CF: For t \u2264 |DC|, query GC in the SGD update (4). For t > |DC|, query GN.\n2. NF: For t \u2264 |DN|, query GN in the SGD update (4). For t > |DN|, query GC.\n3. AO: Let S be an arbitrary sequence of length T consisting of |DC| C\u2019s and |DN| N\u2019s. In the SGD update (4) in round t, if the t-th element St of S is C, then query GC; else, query GN.\nIn order to isolate the effect of the noise, we consider two additional oracles G\u2032C and G\u2032N; the oracle G\u2032C (resp. G\u2032N) is implemented based on the dataset DC (resp. DN), and iterates over DC (resp. DN) in exactly the same order as GC (resp. GN); the only difference is that for G\u2032C (resp. G\u2032N), no extra noise is added to the gradient (that is, Z = 0). The main result of this section is stated in Theorem 2.\nTheorem 2. Let {wCFt }, {wNFt } and {wAOt } be the sequences of updates obtained by running SGD for objective function (8) under CF, NF and AO respectively, and let {vCFt }, {vNFt } and {vAOt } be the sequences of updates under CF, NF and AO with calls to GC and GN replaced by calls to G\u2032C and G\u2032N. Let T = |DC|+ |DN|.\n1. If the learning rate \u03b7t = c/t where c \u2208 (0, 1/\u03bb), then E [ \u2016vCFT+1 \u2212 wCFT+1\u20162 ] \u2264 E [ \u2016vAOT+1 \u2212 wAOT+1\u20162 ] .\n2. If the learning rate \u03b7t = c/t where c > 1/\u03bb, then E [ \u2016vNFT+1 \u2212 wNFT+1\u20162 ] \u2264 E [ \u2016vAOT+1 \u2212 wAOT+1\u20162 ] .\nProof. Let the superscripts CF, NF and AO indicate the iterates for the CF, NF and AO algorithms. Let w1 denote the initial point of the optimization. Let (x O t , y O t ) be the data used under order O = CF,NF or AO to update w at time t, ZOi be the noise added to the exact gradient by GC or GN, depending on which oracle is used by O at t and wOt be the w obtained under order O at time t. Then by expanding the expression for wt in terms of the gradients, we have\nwOT+1 =w1 T\u220f i=1 (1\u2212 \u03b7t\u03bb)\u2212 T\u2211 t=1 \u03b7t\n( T\u220f\ns=t+1\n(1\u2212 \u03b7s\u03bb) ) (yOt x O t + Z O t ). (9)\nSimilarly, if v1 = w1, we have\nvOT+1 = w1 T\u220f i=1 (1\u2212 \u03b7t\u03bb)\u2212 T\u2211 t=1 \u03b7t\n( T\u220f\ns=t+1\n(1\u2212 \u03b7s\u03bb) ) yOt x O t . (10)\nDefine\n\u2206t = \u03b7t >\u220f s=t+1 (1\u2212 \u03b7s\u03bb).\nTaking the expected squared difference between (9) from (10), we obtain\nE [ \u2016vOT+1 \u2212 wOT+1\u20162 ] = E \u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 \u03b7t ( T\u220f s=t+1 (1\u2212 \u03b7s\u03bb) ) ZOt \u2225\u2225\u2225\u2225\u2225 2 \n= E \u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 \u2206tZ O t \u2225\u2225\u2225\u2225\u2225 2  = T\u2211 t=1 \u22062tE [ \u2016ZOt \u20162 ] , (11)\nwhere the second step follows because the ZOi \u2019s are independent. If \u03b7t = c/t, then\n\u2206t = c\nt >\u220f s=t+1 ( 1\u2212 c\u03bb s ) .\nTherefore\n\u22062t+1 \u22062t =\n c t+ 1 \u220f> s=t+2 ( 1\u2212 c\u03bb s ) c\nt\n\u220f> s=t+1 ( 1\u2212 c\u03bb\ns\n)  2 =  t (t+ 1) ( 1\u2212 c\u03bb\nt+ 1\n)  2 =  1 1 +\n1\u2212 c\u03bb t\n 2 ,\nwhich is smaller than 1 if c < 1/\u03bb, equal to 1 if c = 1/\u03bb, and greater than 1 if c > 1/\u03bb. Therefore \u2206t is decreasing if c < 1/\u03bb and is increasing if c > 1/\u03bb. If \u2206t is decreasing, then (11) is minimized if E [ \u2016ZOt \u20162 ] is increasing; if \u2206t is increasing, then (11)\nis minimized if E [ \u2016ZOt \u20162 ] is decreasing; and if \u2206t is constant, then (11) is the same under any\norder of E [ \u2016ZOt \u20162 ] . Therefore for c < 1/\u03bb,\nE [\u2225\u2225\u2225vCFT+1 \u2212 wCFT+1\u2225\u2225\u22252] \u2264 E [\u2225\u2225\u2225vAOT+1 \u2212 wAOT+1\u2225\u2225\u22252] \u2264 E [\u2225\u2225\u2225vNFT+1 \u2212 wNFT+1\u2225\u2225\u22252] .\nFor c = 1/\u03bb, E [\u2225\u2225\u2225vCFT+1 \u2212 wCFT+1\u2225\u2225\u22252] = E [\u2225\u2225\u2225vAOT+1 \u2212 wAOT+1\u2225\u2225\u22252] = E [\u2225\u2225\u2225vNFT+1 \u2212 wNFT+1\u2225\u2225\u22252] .\nFor c > 1/\u03bb, E [\u2225\u2225\u2225vCFT+1 \u2212 wCFT+1\u2225\u2225\u22252] \u2265 E [\u2225\u2225\u2225vAOT+1 \u2212 wAOT+1\u2225\u2225\u22252] \u2265 E [\u2225\u2225\u2225vNFT+1 \u2212 wNFT+1\u2225\u2225\u22252] .\nThis result says that arbitrary ordering of the data is worse than sequentially processing one data set after the other except in the special case where c = 1/\u03bb. If the step size is small (c < 1/\u03bb), the SGD should use the clean data first to more aggressively proceed towards the optimum. If the step size is larger (c > 1/\u03bb), then SGD should reserve the clean data for refining the initial estimates given by processing the noisy data."}, {"heading": "4 Adapting the learning rate to the noise level", "text": "We now investigate whether the performance of SGD can be improved by using different learning rates for oracles with different noise levels. Suppose we have oracles G1 and G2 with noise levels \u03931 and \u03932 that are implemented based on two datasets D1 and D2. Unlike the previous section, we do not assume any relation between \u03931 and \u03932 \u2013 we analyze the error for using oracle G1 followed by G2 in terms of \u03931 and \u03932 to choose a data order. Let T = |D1| + |D2|. Let \u03b21 = |D1|T and \u03b22 = 1\u2212 \u03b21 = |D2|T be the fraction of the data coming from G1 and G2, respectively. We adapt the gradient updates in (4) to heterogeneous noise by choosing the learning rate \u03b7t as a function of the noise level. Algorithm 1 shows a modified SGD for heterogeneous learning rates.\nAlgorithm 1 SGD with varying learning rate\n1: Inputs: Oracles G1,G2 implemented by data sets D1, D2. Learning rates c1 and c2. 2: Set w1 = 0. 3: for t = 1, 2, . . . , |D1| do 4: wt+1 = \u03a0W ( wt \u2212 c1t G1(wt)\n) 5: end for 6: for t = |D1|+ 1, |D1|+ 2, . . . , |D1|+ |D2| do 7: wt+1 = \u03a0W ( wt \u2212 c2t G2(wt)\n) 8: end for 9: return w|D1|+|D2|+1.\nConsider SGD with learning rate \u03b7t = c1/t while querying G1 and with \u03b7t = c2/t while querying G2 in the update (4). We must choose an order in which to query G1 and G2 as well as the constants\nc1 and c2 to get the best performance. We do this by minimizing an upper bound on the distance between the final iterate wT+1 and the optimal solution w\n\u2217 to E[f(w)] where f is defined in (1), and the expectation is with respect to the data distribution and the gradient noise; the upper bound we choose is based on Rakhlin et al. (2012). Note that for smooth functions f , a bound on the distance \u2016wT+1 \u2212 w\u2217\u2016 automatically translates to a bound on the regret f(wT+1)\u2212 f(w\u2217).\nTheorem 3 generalizes the results of Rakhlin et al. (2012) to our heterogeneous noise setting; the proof is in the supplement.\nTheorem 3. If 2\u03bbc1 > 1 and if 2\u03bbc2 6= 1, and if we query G1 before G2 with learning rates c1/t and c2/t respectively, then the SGD algorithm satisfies\nE [ \u2016wT+1 \u2212 w\u2217\u20162 ] \u2264 4\u0393 2 1\nT \u00b7 \u03b2\n2\u03bbc2\u22121 1 c 2 1 2\u03bbc1 \u2212 1\n+ 4\u039322 T \u00b7 (1\u2212 \u03b2 2\u03bbc2\u22121 1 )c 2 2 2\u03bbc2 \u2212 1 +O\n( 1\nTmin(2,2\u03bbc1)\n) . (12)\nProof. (Sketch) Let g(w) be the true gradient \u2207f(w) and g\u0302(w) be the unbiased noisy gradient provided by the oracle G1 or G2, whichever is queried. By strong convexity of f , we have\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 (1\u2212 2\u03bb\u03b7t)E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] + \u03b72t \u03b3 2 t .\nSolving it inductively, with \u03b3t = \u03931, \u03b7t = c1/t for t \u2264 \u03b21T and \u03b3t = \u03932, \u03b7t = c2/t for t > \u03b21T , we have\nE1,...,T [ \u2016wT+1 \u2212 w\u2217\u20162 ] \u2264 \u03b21T\u220f i=i0 ( 1\u2212 2\u03bbc1 i ) T\u220f i=\u03b21T+1 ( 1\u2212 2\u03bbc2 i ) E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162 ] + \u039321\nT\u220f i=\u03b21T+1 ( 1\u2212 2\u03bbc2 i ) \u03b21T\u2211 i=i0 c21 i2 \u03b21T\u220f j=i+1 ( 1\u2212 2\u03bbc1 j )\n+ \u039322 T\u2211 i=\u03b21T+1 c22 i2 T\u220f j=i+1 ( 1\u2212 2\u03bbc2 j ) ,\nwhere i0 is the smallest positive integer such that 2\u03bb\u03b7i0 < 1, i.e, i0 = d2c1\u03bbe. Then using 1\u2212 x \u2264 e\u2212x and upper bounding each term using integrals we get the (12).\nTwo remarks are in order. First, observe that the first two terms in the right hand side dominate the other term. Second, our proof techniques for Theorem 3, adapted from Rakhlin et al. (2012), require that 2\u03bbc1 > 1 in order to get a O(1/T ) rate of convergence; without this condition, the dependence on T is \u2126(1/T )."}, {"heading": "4.1 Algorithm description", "text": "Our algorithm for selecting c1 and c2 is motivated by Theorem 3. We propose an algorithm that selects c1 and c2 by minimizing the quantity B(c1, c2) which represents the highest order terms in Theorem 3:\nB(c1, c2) = 4\u039321\u03b2 2\u03bbc2\u22121 1 c 2 1\nT (2\u03bbc1 \u2212 1) +\n4\u039322(1\u2212 \u03b2 2\u03bbc2\u22121 1 )c 2 2\nT (2\u03bbc2 \u2212 1) . (13)\nGiven \u03bb, \u03931, \u03932 and \u03b21, we use c \u2217 1 and c \u2217 2 to denote the values of c1 and c2 that minimize B(c1, c2). We can optimize for fixed c2 with respect to c1 by minimizing c21\n2\u03bbc1\u22121 ; this gives c \u2217 1 = 1/\u03bb, and\nc\u221721 2\u03bbc\u22171\u22121 = 1/\u03bb2, which is independent of \u03b21 or the noise levels \u03931 and \u03932. Minimizing B(c \u2217 1, c2) with respect to c2 can be now performed numerically to yield c \u2217 2 = argminc2 B(c \u2217 1, c2). This yields optimal values of c1 and c2. Now suppose we have two oracles GC, GN with noise levels \u0393C and \u0393N that are implemented based on datasets DC and DN respectively. Let \u0393C < \u0393N, and let \u03b2C = |DC|\n|DC|+|DN| and \u03b2N = |DN|\n|DC|+|DN| be the fraction of the total data in each data set. Define the following functions:\nHCN(c) = 4\u03932C\u03b2 2\u03bbc\u22121 C\n\u03bb2 +\n4\u03932N(1\u2212 \u03b2 2\u03bbc\u22121 C )c 2\n2\u03bbc\u2212 1 ,\nHNC(c) = 4\u03932N\u03b2 2\u03bbc\u22121 N\n\u03bb2 +\n4\u03932C(1\u2212 \u03b2 2\u03bbc\u22121 N )c 2\n2\u03bbc\u2212 1 .\nThese represent the constant of the leading term in the upper bound in Theorem 3 for (G1,G2) = (GC,GN) and (G1,G2) = (GN,GC), respectively. Algorithm 2 repeats the process of choosing optimal c1, c2 with two orderings of the data \u2013 GC first and GN first \u2013 and selects the solution which provides the best bounds (according to the higher order terms of Theorem 3).\nAlgorithm 2 Selecting the Learning Rates\n1: Inputs: Data sets DC and DN accessed through oracles GC and GN with noise levels \u0393C and \u0393N. 2: Let \u03b2C = |DC|\n|DC|+|DN| and \u03b2N = |DN| |DC|+|DN| .\n3: Calculate cCN = argmincHCN(c) and cNC = argmincHNC(c). 4: if HCN(cCN) \u2264 HNC(cNC) then 5: Run Algorithm 1 using oracles (GC,GN), learning rates c1 = 1\u03bb and c2 = cCN. 6: else 7: Run Algorithm 1 using oracles (GN,GC), learning rates c1 = 1\u03bb and c2 = cNC. 8: end if"}, {"heading": "4.2 Regret Bounds", "text": "To provide a regret bound on the performance of SGD with two learning rates, we need to plug the optimal values of c1 and c2 into the right hand side of (13). Observe that as c1 = c2 and c2 = 0 are feasible inputs to (13), our algorithm by construction has a superior regret bound than using a single learning rate only, or using clean data only.\nUnfortunately, the value of c2 that minimizes (13) does not have a closed form solution, and as such it is difficult to provide a general simplified regret bound that holds for all \u03931, \u03932 and \u03b21. In this section, we consider two cases of interest, and derive simplified versions of the regret bound for SGD with two learning rates for these cases.\nWe consider the two data orders (\u03931,\u03932) = (\u0393N,\u0393C) and (\u03931,\u03932) = (\u0393C,\u0393N) in a scenario where \u0393N/\u0393C 1 and both \u03b2N and \u03b2C are bounded away from 0 and 1. That is, the noisy data is much noisier. The following two lemmas provide upper and lower bounds on B(c\u22171, c \u2217 2) in this setting.\nLemma 1. Suppose (\u03931,\u03932) = (\u0393N,\u0393C) and 0 < \u03b2N < 1. Then for sufficiently large \u0393N/\u0393C, the optimal solution c\u22172 to (13) satisfies\n2c\u22172\u03bb \u2208\n[ 1 + 2 log(\u0393N/\u0393C) + log log(1/\u03b2N)\nlog(1/\u03b2N) ,\n1 + 2 log(4\u0393N/\u0393C) + log log(1/\u03b2N)\nlog(1/\u03b2N)\n] .\nMoreover, B(c\u22171, c \u2217 2) satisfies:\nB(c\u22171, c \u2217 2) \u2265\n4\u03932C(log( \u0393N \u0393C ) + 12 log log 1 \u03b2N )\n\u03bb2T log( 1\u03b2N )\nB(c\u22171, c \u2217 2) \u2264 4\u03932C \u03bb2T\n( 4 + 4 + 2 log(\u0393N\u0393C ) + log log( 1 \u03b2N )\nlog( 1\u03b2N )\n) .\nProof. (Sketch) We prove that for any 2\u03bbc2 \u2264 1 + 2 log(\u0393N/\u0393C)+log log(1/\u03b2N)log(1/\u03b2N) , B(c \u2217 1, c2) is decreasing with respect to c2 when \u0393N/\u0393C is sufficiently large; and for any 2\u03bbc2 \u2265 1 + 2 log(4\u0393N/\u0393C)+log log(1/\u03b2N)log(1/\u03b2N) , B(c\u22171, c2) is increasing when \u0393N/\u0393C is sufficiently large. Therefore the minimum of B(c \u2217 1, c2) is achieved when 2\u03bbc2 is in between.\nObserve that the regret bound grows logarithmically with \u0393N/\u0393C. Moreover, if we only used\nthe cleaner data, then the regret bound would be 4\u03932C \u03bb2\u03b2CT\n, which is better, especially for large \u0393N/\u0393C. This means that using two learning rates with the noisy data first gives poor results at high noise levels.\nOur second bound takes the opposite data order, processing the clean data first.\nLemma 2. Suppose (\u03931,\u03932) = (\u0393C,\u0393N) and 0 < \u03b2C < 1. Let \u03c3 = (\u0393N/\u0393C) \u22122. Then for sufficiently large \u0393N/\u0393C, the optimal solution c \u2217 2 to (13) satisfies: 2c \u2217 2\u03bb \u2208 [ \u03c3, 8\u03b2C\u03c3 ] . Moreover, B(c\u22171, c \u2217 2) satisfies:\nB(c\u22171, c \u2217 2) \u2265 4\u03932C \u03bb2\u03b2CT \u03b2 8\u03c3/\u03b2C C\nB(c\u22171, c \u2217 2) \u2264 4\u03932C \u03bb2\u03b2CT \u03b2\u03c3C\n( 1 + \u03c3 log(1/\u03b2C)\n4\n) .\nProof. (Sketch) Similar as the proof of Lemma 1, we prove that for any 2\u03bbc2 \u2264 \u03c3, B(c\u22171, c2) is decreasing with respect to c2; and for any 2\u03bbc2 \u2265 8\u03b2C\u03c3, B(c \u2217 1, c2) is increasing when \u0393N/\u0393C is sufficiently large. Therefore the minimum of B(c\u22171, c2) is achieved when 2\u03bbc2 is in between.\nIf we only used the clean dataset, then the regret bound would be 4\u03932C \u03bb2\u03b2CT , so Lemma 2 yields an\nimprovement by a factor of \u03b2 (\u0393N/\u0393C)\n\u22122\nC\n( 1 + ( \u0393N \u0393C )\u22122 log(1/\u03b2C) 4 ) . As \u03b2C < 1, observe that this factor\nis always less than 1, and tends to 1 as \u0393N/\u0393C tends to infinity; therefore the difference between the regret bounds narrows as the noisy data grows noisier. We conclude that using two learning rates with clean data first gives a better regret bound than using only clean data or using two learning rates with noisy data first."}, {"heading": "5 Experiments", "text": "We next illustrate our theoretical results through experiments on real data. We consider the task of training a regularized logistic regression classifier for binary classification under local differential privacy. For our experiments, we consider two real datasets \u2013 MNIST (with the task 1 vs. Rest) and Covertype (Type 2 vs. Rest). The former consists of 60, 000 samples in 784 dimensions, while the latter consists of 500, 000 samples in 54-dimensions. We reduce the dimension of the MNIST dataset to 25 via random projections.\nTo investigate the effect of heterogeneous noise, we divide the training data into subsets (DC, DN) to be accessed through oracles (GC,GN) with privacy parameters ( C, N) respectively. We pick C > N, so GN is noisier than GC. To simulate typical practical situations where cleaner data is rare, we set the size of DC to be \u03b2C = 10% of the total data size. We set the regularization parameter \u03bb = 10\u22123, \u0393C and \u0393N according to Theorem 1 and use SGD with mini-batching (batch size 50).\nDoes Data Order Change Performance? Our first task is to investigate the effect of data order on performance. For this purpose, we compare three methods \u2013 CleanFirst, where all of DC is used before DN, NoisyFirst, where all of DN is used before DC, and Arbitrary, where data from DN \u222aDC is presented to the algorithm in a random order.\nThe results are in Figures 1(a) and 1(d). We use C = 10, N = 3. For each algorithm, we plot |f(wT+1)\u2212 f(vT+1)| as a function of the constant c in the learning rate. Here f(wT+1) is the function value obtained after T rounds of SGD, and f(vT+1) is the function value obtained after T rounds of SGD if we iterate over the data in the same order, but add no extra noise to the gradient. (See Theorem 2 for more details.) As predicted by Theorem 2, the results show that for c < 1\u03bb ,\nCleanFirst has the best performance, while for c > 1\u03bb , NoisyFirst performs best. Arbitrary performs close to NoisyFirst for a range of values of c, which we expect as only 10% of the data belongs to DC.\nAre Two Learning Rates Better than One? We next investigate whether using two learning rates in SGD can improve performance. We compare five approaches. Optimal is the gold standard where we access the raw data without any intervening noisy oracle. CleanOnly uses only DC with learning rate with the optimal value of c obtained from Section 4. SameClean and SameNoisy use a single value of the constant c in the learning rate for DN \u222aDC, where c is obtained by optimizing (13)1 under the constraint that c1 = c2. SameClean uses all of DC before using DN, while SameNoisy uses all of DN before using DC. In Algorithm2, we use Algorithm 2 to set the two learning rates and the data order (DC first or DN first). In each case, we set C = 10, vary N from 1 to 10, and plot the function value obtained at the end of the optimization.\nThe results are plotted in Figures 1(b) and 1(e). Each plotted point is an average of 100 runs. It is clear that Algorithm2, which uses two learning rates, performs better than both SameNoisy and SameClean. As expected, the performance difference diminishes as N increases (that is, the noisy data gets cleaner). For moderate and high N, Algorithm2 performs best, while for low N (very noisy DN), CleanOnly has slightly better performance. We therefore conclude that using two learning rates is better than using a single learning rate with both datasets, and that Algorithm2 performs best for moderate to low noise levels.\nDoes Noisy Data Always Help? A natural question to ask is whether using noisy data always helps performance, or if there is some threshold noise level beyond which we should not use noisy data. Lemma 2 shows that in theory, we obtain a better upper bound on performance when we use noisy data; in contrast, Figures 1(b) and 1(e) show that for low N (high noise), Algorithm2 performs worse than CleanOnly. How do we explain this apparent contradiction?\nTo understand this effect, in Figures 1(c) and 1(f) we plot the performance of SGD using two learning rates (with c1 = 1 \u03bb) against CleanOnly as a function of the second learning rate c2. The figures show that the best performance is attained at a value of c2 which is different from the value predicted by Algorithm2, and this best performance is better than CleanOnly. Thus, noisy data always improves performance; however, the improvement may not be achieved at the learning rate predicted by our algorithm.\nWhy does our algorithm perform suboptimally? We believe this happens because the values of \u0393N and \u0393C used by our algorithm are fairly loose upper bounds. For local differential privacy, an easy lower bound on \u0393 is \u221a\n4(d2+d) 2b , where b is the mini-batch size; let c2(L) (resp. c2(U)) be the\nvalue of c2 obtained by plugging in these lower bounds (resp. upper bounds from Theorem 1) to Algorithm 1. Our experiments show that the optimal value of c2 always lies between c2(L) and c2(U), which indicates that the suboptimal performance may be due to the looseness in the bounds.\nWe thus find that even in these high noise cases, theoretical analysis often allows us to identify an interval containing the optimal value of c2. In practice, we recommend running Algorithm 2 twice \u2013 once with upper, and once with lower bounds to obtain an interval containing c2, and then performing a line search to find the optimal c2.\n1Note that we plug in separate noise rates for GC and GN in the learning rate calculations."}, {"heading": "6 Conclusion", "text": "In this paper we propose a model for learning from heterogeneous noise that is appropriate for studying stochastic gradient approaches to learning. In our model, data from different sites are accessed through different oracles which provide noisy versions of the gradient. Learning under local differential privacy and random classification noise are both instances of our model. We show that for two sites with different noise levels, processing data from one site followed by the other is better than randomly sampling the data, and the optimal data order depends on the learning rate. We then provide a method for choosing learning rates that depends on the noise levels and showed that these choices achieve lower regret than using a common learning rate. We validate these findings through experiments on two standard data sets and show that our method for choosing learning rates often yields improvements when the noise levels are moderate. In the case where one data set is much noisier than the other, we provide a different heuristic to choose a learning rate that improves the regret.\nThere are several different directions towards generalizing the work here. Firstly, extending the results to multiple sites and multiple noise levels will give more insights as to how to leverage large numbers of data sources. This leads naturally to cost and budgeting questions: how much should we pay for additional noisy data? Our results for data order do not depend on the actual noise levels, but rather their relative level. However, we use the noise levels to tune the learning rates for different sites. If bounds on the noise levels are available, we can still apply our heuristic. Adaptive approaches for estimating the noise levels while learning are also an interesting approach for future study.\nAcknowledgements. The work of K. Chaudhuri and S. Song was sponsored by NIH under U54 HL108460 and the NSF under IIS 1253942."}, {"heading": "A Appendix", "text": ""}, {"heading": "A.1 Mathematical miscellany", "text": "In many cases we would like to bound a summation using an integral.\nLemma 3. For x \u2265 0, we have\nb\u2211 i=a ix \u2264 \u222b b+1 a ixdi = (b+ 1)x+1 \u2212 ax+1 x+ 1 (14) b\u2211 i=a ix \u2265 \u222b b a\u22121 ixdi = bx+1 \u2212 (a\u2212 1)x+1 x+ 1 (15)\nFor x < 0 and x 6= \u22121, we have\nb\u2211 i=a ix \u2264 \u222b b a\u22121 ixdi = bx+1 \u2212 (a\u2212 1)x+1 x+ 1 (16) b\u2211 i=a ix \u2265 \u222b b+1 a ixdi = (b+ 1)x+1 \u2212 ax+1 x+ 1 (17)\nFor x = \u22121, we have b\u2211 i=a ix \u2264 \u222b b a\u22121 ixdi = log b a\u2212 1 (18)\nb\u2211 i=a ix \u2265 \u222b b+1 a ixdi = log b+ 1 a (19)\nThe sequence {ix} is increasing when x > 0 and is decreasing when x < 0. The proof follows directly from applying standard technique of bounding summation with integral."}, {"heading": "A.2 Details from Section 2", "text": "Proof. (Of Theorem 1) Consider an oracle G implemented based on a dataset D of size T . Given any sequence w1, w2, . . . , wT , the disguised version of D output by G is the sequence of gradients G(w1), . . . ,G(wT ). Suppose that the oracle accesses the data in a (random) order specified by a permutation \u03c0; for any t, any x, x\u2032 \u2208 X , y, y\u2032 \u2208 {1,\u22121}, we have\n\u03c1(G(wt) = g|(x\u03c0(t), y\u03c0(t)) = (x, y)) \u03c1(G(wt) = g|(x\u03c0(t), y\u03c0(t)) = (x\u2032, y\u2032)) = \u03c1(Zt = g \u2212 \u03bbw \u2212\u2207`(w, x, y)) \u03c1(Zt = g \u2212 \u03bbw \u2212\u2207`(w, x\u2032, y\u2032))\n= e\u2212( /2)\u2016g\u2212\u03bbw\u2212\u2207`(w,x,y)\u2016\ne\u2212( /2)\u2016g\u2212\u03bbw\u2212\u2207`(w,x\u2032,y\u2032)\u2016 \u2264 exp ( ( /2)(\u2016\u2207`(w, x, y)\u2016+ \u2016\u2207`(w, x\u2032, y\u2032)\u2016) ) \u2264 exp ( ) .\nThe first inequality follows from the triangle inequality, and the last step follows from the fact that \u2016\u2207`(w, x, y)\u2016 \u2264 1. The privacy proof follows.\nFor the rest of the theorem, we consider a slightly generalized version of SGD that includes mini-batch updates. Suppose the batch size is b; for standard SGD, b = 1. For a given t, we call G(wt) b successive times to obtain noisy gradient estimates g1(wt), . . . , gb(wt); these are gradient estimates at wt but are based on separate (private) samples. The SGD update rule is:\nwt+1 = \u03a0W ( wt \u2212\n\u03b7t b\n(g1(wt) + . . .+ gb(wt)) ) .\nFor any i, E[gi(wt)] = \u03bbw+E[\u2207`(w, x, y)], where the first expectation is with respect to the data distribution and the noise, and the second is with respect to the data distribution; the unbiasedness result follows.\nWe now bound the norm of the noisy gradient calculated from a batch. Suppose that the oracle accesses the dataset D in an order \u03c0. Then, gi(wt) = \u03bbw+\u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i)) + Z(t\u22121)b+i. Expanding on the expression for the expected squared norm of the gradient, we have\nE [\u2225\u2225\u2225\u22251b (g1(wt) + . . .+ gb(wt)) \u2225\u2225\u2225\u22252 ] =E \u2225\u2225\u2225\u2225\u2225\u03bbw + 1b b\u2211 i=1 \u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i)) \u2225\u2225\u2225\u2225\u2225 2 \n+ 2\nb E\n[( \u03bbw + 1\nb b\u2211 i=1 \u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i))\n) \u00b7 ( b\u2211 i=1 Z(t\u22121)b+i )]\n+ 1\nb2 E \u2225\u2225\u2225\u2225\u2225 b\u2211 i=1 Z(t\u22121)b+i \u2225\u2225\u2225\u2225\u2225 2  (20)\nWe now look at the three terms in (20) separately. The first term can be further expanded to:\nE [ \u2016\u03bbw\u20162 ] + E \u2225\u2225\u2225\u2225\u2225 1b2 b\u2211 i=1 \u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i)) \u2225\u2225\u2225\u2225\u2225 2 \n+ 2\u03bbw \u00b7 ( b\u2211 i=1 E [ \u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i)) ]) (21)\nThe first term in (21) is at most \u03bb2 maxw\u2208W \u2016w\u20162, which is at most 1. The second term is at most maxw \u03bb\u2016w\u2016 \u00b7 maxw,x,y \u2016\u2207`(w, x, y)\u2016 \u2264 1, and the third term is at most 2. Thus, the first term in (20) is at most 4. Notice that this upper bound can be pretty loose compare to the average\u2225\u2225\u2225\u2225\u03bbw + 1b\u2211bi=1\u2207`(wt, x\u03c0((t\u22121)b+i), y\u03c0((t\u22121)b+i))\n\u2225\u2225\u2225\u22252 values seen in experiment. This leads to a loose estimation of the noise level for oracle GDP. To bound the second term in (20), observe that for all i, Z(t\u22121)b+i is independent of any Z(t\u22121)b+i\u2032 when i 6= i\u2032, as well as of the dataset. Combining this with the fact that E [Z\u03c4 ] = 0 for any \u03c4 , we get that this term is 0. To bound the third term in (20), we have:\n1 b2 E \u2225\u2225\u2225\u2225\u2225\u2211 t\u2208B Zt \u2225\u2225\u2225\u2225\u2225 2\n2\n = 1 b2 E \u2211 t\u2208B \u2016Zt\u201622 + \u2211 t\u2208B,s\u2208B,t 6=s Zt \u00b7 Zs  = 1\nb2 \u2211 t\u2208B E [ \u2016Zt\u201622 ] + 1 b2 \u2211 t\u2208B,s\u2208B,t 6=s E [Zt] \u00b7 E [Zs]\n= 1\nb2 \u2211 t\u2208B E [ \u2016Zt\u201622 ] ,\nwhere the first equality is from the linearity of expectation and the last two equalities is from the fact that Zi is independently drawn zeros mean vector. Because Zt follows \u03c1(Zt = z) \u221d e\u2212( /2)\u2016z\u2016, we have\n\u03c1(\u2016Zt\u2016 = x) \u221d xd\u22121e\u2212( /2)x,\nwhich is a Gamma distribution. For X \u223c Gamma(k, \u03b8), E [X] = k\u03b8 and Var (X) = k\u03b82. Also, by property of expectation, E [ X2 ] = (E [X])2 + Var (X). We then have E [ \u2016Zt\u201622 ] = 4(d2 + d) 2 and the whole term equals to 4(d2 + d)\n2b .\nCombining the three bounds together, we have a final bound of 4 + 4(d2 + d)\n2b . The lemma follows."}, {"heading": "A.3 Proofs from Section 4", "text": "Recall that we have oracles G1,G2 based on data sets D1 and D2. The fractions of data in each data set are \u03b21 =\n|D1| |D1|+|D2| and \u03b22 = |D2| |D1|+|D2| , respectively."}, {"heading": "A.3.1 Proof of Theorem 3", "text": "Theorem 3 is a corollary of the following Lemma.\nLemma 4. Consider the SGD algorithm that follows Algorithm 1. Suppose the objective function is \u03bb-strongly convex, and define W = {w : \u2016w\u2016 \u2264 B}. If 2\u03bbc1 > 1 and i0 = d2c1\u03bbe, then we have the following two cases:\n1. If 2\u03bbc2 6= 1,\nE [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 ( 4\u039321 \u03b22\u03bbc2\u221211 c 2 1\n2\u03bbc1 \u2212 1 + 4\u039322\nc22(1\u2212 \u03b2 2\u03bbc2\u22121 1 )\n2\u03bbc2 \u2212 1\n) \u00b7 1 T +O (\n1\nTmin(2\u03bbc1,2)\n)\n2. If 2\u03bbc2 = 1,\nE [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 ( 4\u039321 \u03b22\u03bbc2\u221211 c 2 1\n2\u03bbc1 \u2212 1 + 4\u039322c 2 2 log\n1\n\u03b21\n) \u00b7 1 T +O (\n1\nTmin(2\u03bbc1,2)\n)\nWe first begin with a lemma which follows from arguments very similar to those made in Rakhlin et al. (2012).\nLemma 5. Let w\u2217 be the optimal solution to E[f(w)]. Then,\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 (1\u2212 2\u03bb\u03b7t)E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] + \u03b72t \u03b3 2 t .\nwhere the expectation is taken wrt the oracle as well as sampling from the data distribution.\nProof. (Of Lemma 5) By strong convexity of f , we have\nf(w\u2032) \u2265 f(w) + g(w)>(w\u2032 \u2212 w) + \u03bb 2 \u2016w \u2212 w\u2032\u20162. (22)\nThen by taking w = wt, w \u2032 = w\u2217 we have\ng(wt) >(wt \u2212 w\u2217) \u2265 f(wt)\u2212 f(w\u2217) +\n\u03bb 2 \u2016wt \u2212 w\u2217\u20162. (23)\nAnd similarly by taking w\u2032 = wt, w = w \u2217, we have\nf(wt)\u2212 f(w\u2217) \u2265 \u03bb 2 \u2016wt \u2212 w\u2217\u20162. (24)\nBy the update rule and convexity of W, we have\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] = E1,...,t [ \u2016\u03a0W (wt \u2212 \u03b7tg\u0302(wt))\u2212 w\u2217\u20162 ] \u2264 E1,...,t [ \u2016wt \u2212 \u03b7tg\u0302(wt)\u2212 w\u2217\u20162\n] = E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] \u2212 2\u03b7tE1,...,t [ g\u0302(wt) >(wt \u2212 w\u2217) ] \u03b72tE1,...,t [ \u2016g\u0302(wt)\u20162 ] .\nConsider the term E1,...,t [ g\u0302(wt) >(wt \u2212 w\u2217) ] , where the expectation is taken over the randomness from time 1 to t. Since wt is a function of the samples used from time 1 to t\u2212 1, it is independent\nof the sample used at t. So we have\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 E1,...,t [ g\u0302(wt) >(wt \u2212 w\u2217) ]\n= E1,...,t\u22121 [ Et[g\u0302(wt)>(wt \u2212 w\u2217)|wt] ] = E1,...,t\u22121 [ Et[g\u0302(wt)>|wt](wt \u2212 w\u2217)\n] = E1,...,t\u22121 [ g(wt) >(wt \u2212 w\u2217) ] .\nWe have the following upper bound:\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] \u2212 2\u03b7tE1,...,t\u22121 [ g(wt) >(wt \u2212 w\u2217) ]\n+ \u03b72tE1,...,t [ \u2016g\u0302(wt)\u20162 ] .\nBy (23) and the bound E [ \u2016g\u0302(wt)\u20162 ] \u2264 \u03b32t , we have E1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] \u2212 2\u03b7tE1,...,t\u22121 [ f(wt)\u2212 f(w\u2217) + \u03bb\n2 \u2016wt \u2212 w\u2217\u20162\n] + \u03b72t \u03b3 2 t .\nThen by (24) and the fact that wt is independent of the sample used in time t, we have the following recursion:\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 (1\u2212 2\u03bb\u03b7t)E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] + \u03b72t \u03b3 2 t .\nProof. (Of Lemma 4) Let g(w) be the true gradient \u2207f(w) and g\u0302(w) be the unbiased noisy gradient provided by the oracle G1 or G2, whichever is queried. From Lemma 5, we have the following recursion:\nE1,...,t [ \u2016wt+1 \u2212 w\u2217\u20162 ] \u2264 (1\u2212 2\u03bb\u03b7t)E1,...,t [ \u2016wt \u2212 w\u2217\u20162 ] + \u03b72t \u03b3 2 t .\nLet i0 be the smallest positive integer such that 2\u03bb\u03b7i0 < 1, i.e, i0 = d2c1\u03bbe. Notice that for fixed step size constant c and \u03bb, i0 would be a fixed constant. Therefore we assume that i0 < \u03b2T . Using the above inequality inductively, and substituting \u03b3t = \u03931 for t \u2264 \u03b21T and \u03b3t = \u03932 for t > \u03b21T , we have\nE1,...,T [ \u2016wT+1 \u2212 w\u2217\u20162 ] \u2264 \u03b21T\u220f i=i0 (1\u2212 2\u03bb\u03b7i) T\u220f i=\u03b21T+1 (1\u2212 2\u03bb\u03b7i)E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162 ] + \u039321\nT\u220f i=\u03b21T+1 (1\u2212 2\u03bb\u03b7i) \u03b21T\u2211 i=i0 \u03b72i \u03b21T\u220f j=i+1 (1\u2212 2\u03bb\u03b7j)\n+ \u039322 T\u2211 i=\u03b21T+1 \u03b72i T\u220f j=i+1 (1\u2212 2\u03bb\u03b7j).\nBy substituting \u03b7t = c1 t for D1 and \u03b7t = c2 t for D2, we have\nE1,...,T [ \u2016wT+1 \u2212 w\u2217\u20162 ] \u2264 \u03b21T\u220f i=i0 ( 1\u2212 2\u03bbc1 i ) T\u220f i=\u03b21T+1 ( 1\u2212 2\u03bbc2 i ) E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162 ] + \u039321\nT\u220f i=\u03b21T+1 ( 1\u2212 2\u03bbc2 i ) \u03b21T\u2211 i=i0 c21 i2 \u03b21T\u220f j=i+1 ( 1\u2212 2\u03bbc1 j )\n+ \u039322 T\u2211 i=\u03b21T+1 c22 i2 T\u220f j=i+1 ( 1\u2212 2\u03bbc2 j ) .\nApplying the inequality 1\u2212 x \u2264 e\u2212x to each of the terms in the products, and simplifying, we get:\nE1,...,T [ \u2016wT+1 \u2212 w\u2217\u20162 ] \u2264 e\u22122\u03bbc1 \u2211\u03b21T i=i0 1 i e \u22122\u03bbc2 \u2211> i=\u03b21T+1 1 i E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162 ] + \u039321e \u22122\u03bbc2 \u2211> i=\u03b21T+1 1 i\n\u03b21T\u2211 i=i0 c21 i2 e \u22122\u03bbc1 \u2211\u03b21T j=i+1 1 j\n+ \u039322 >\u2211 i=\u03b21T+1 c22 i2 e \u22122\u03bbc2 \u2211> j=i+1 1 j . (25)\nWe would like to bound (25) term by term. A bound we will use later is:\ne2\u03bbc2/\u03b21T = 1 + 2\u03bbc2 \u03b21T e2\u03bbc2/\u03b21T \u2032 \u2264 1 + 2\u03bbc2 \u03b21T e2\u03bbc2/\u03b21 , (26)\nwhere the equality is obtained using Taylor\u2019s theorem, and the inequality follows because T \u2032 is in the range [1,\u221e). Now we can bound the three terms in (25) separately.\nThe first term in (25): We bound this as follows:\ne \u22122\u03bbc1 \u2211\u03b21T i=i0 1 i e \u22122\u03bbc2 \u2211> i=\u03b21T+1 1 i E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162 ] \u2264 e\u22122\u03bbc1 log \u03b21T i0 e \u22122\u03bbc2(log 1\u03b21\u2212 1 \u03b21T )E1,...,T [ \u2016wi0 \u2212 w\u2217\u20162\n] \u2264 ( i0 T )2\u03bbc1 \u03b2 2\u03bb(c2\u2212c1) 1 e 2\u03bbc2/\u03b21T (4B2)\n\u2264 ( i0 T )2\u03bbc1 \u03b2 2\u03bb(c2\u2212c1) 1 ( 1 + 2\u03bbc2 \u03b21T e2\u03bbc2/\u03b21 ) 4B2\n= 4B2i0 2\u03bbc1\u03b2 2\u03bb(c2\u2212c1) 1\n1\nT 2\u03bbc1 +O\n( 1\nT 2\u03bbc1+1\n) ,\nwhere the first equality follows from (17).The second inequality follows from \u2016w\u2016 \u2264 B, \u2016w\u2212w\u2032\u2016 \u2264 \u2016w\u2016+ \u2016w\u2032\u2016 \u2264 2B, and bounding expectation using maximum. The third follows from (26).\nThe second term in (25): We bound this as follows:\n\u039321e \u22122\u03bbc2\n\u2211> i=\u03b21T+1 1 i \u03b21T\u2211 i=i0 c21 i2 e \u22122\u03bbc1 \u2211\u03b21T j=i+1 1 j \u2264 \u039321e \u22122\u03bbc2(log 1\u03b21\u2212 1 \u03b21T ) \u03b21T\u2211 i=i0 c21 i2 e\u22122\u03bbc1 log \u03b21T i+1\n= \u039321\u03b2 2\u03bbc2 1 e 2\u03bbc2/\u03b21T \u03b21T\u2211 i=i0 c21 i2 ( i+ 1 \u03b21T )2\u03bbc1\n= \u039321\u03b2 2\u03bb(c2\u2212c1) 1 e 2\u03bbc2/\u03b21T c21T \u22122\u03bbc1 \u03b21T\u2211 i=i0 (i+ 1)2\u03bbc1 i2\n\u2264\u039321\u03b2 2\u03bb(c2\u2212c1) 1 e 2\u03bbc2/\u03b21T c21T \u22122\u03bbc1 \u03b21T\u2211 i=i0 4(i+ 1)2\u03bbc1\u22122\n\u22644\u039321\u03b2 2\u03bb(c2\u2212c1) 1\n( 1 +\n2\u03bbc2 \u03b21T\ne2\u03bbc2/\u03b21 ) c21T \u22122\u03bbc1 \u03b21T+1\u2211 i=i0+1 i2\u03bbc1\u22122,\n(27)\nwhere the first inequality follows from (17), the second inequality follows from (1+ 1i ) 2 \u2264 (1+ 11) 2 = 4, and the last inequality follows from (26).\nBounding summation using integral following (16) and (14) of Lemma 3, if 2\u03bbc1 > 1, the term on the right hand side would be in the order of O(1/T ); if 2\u03bbc1 = 1, it would be O(log T/T ); if 2\u03bbc1 < 1, it would be O(1/T 2\u03bbc1). Therefore to minimize the bound in terms of order, we would choose c1 such that 2\u03bbc1 > 1. To get an upper bound of the summation in (27), using (16) of Lemma 3, for 2\u03bbc1 < 2,\n\u03b21T+1\u2211 j=i0+1 i2\u03bbc1\u22122 = \u03b21T\u2211 j=i0+1 i2\u03bbc1\u22122 + (\u03b21T + 1) 2\u03bbc1\u22122 \u2264 (\u03b21T ) 2\u03bbc1\u22121 2\u03bbc1 \u2212 1 +O(T 2\u03bbc1\u22122).\nFor 2\u03bbc1 > 2, using (14) of Lemma 3,\n\u03b21T+1\u2211 j=i0+1 i2\u03bbc1\u22122 = \u03b21T\u22121\u2211 j=i0+1 i2\u03bbc1\u22122 + (\u03b21T ) 2\u03bbc1\u22122 + (\u03b21T + 1) 2\u03bbc1\u22122 \u2264 (\u03b21T ) 2\u03bbc1\u22121 2\u03bbc1 \u2212 1 +O(T 2\u03bbc1\u22122).\nFinally, for 2\u03bbc1 = 2,\n\u03b21T+1\u2211 j=i0+1 i2\u03bbc1\u22122 = (\u03b21T + 1)\u2212 (i0 + 1) + 1 = \u03b21T +O(1).\nCombining the three cases together, we have\n\u03b21T+1\u2211 j=i0+1 i2\u03bbc1\u22122 \u2264 (\u03b21T ) 2\u03bbc1\u22121 2\u03bbc1 \u2212 1 +O ( T 2\u03bbc1\u22122 ) .\nThis allows us to further upper bound (27):\n4\u039321\u03b2 2\u03bb(c2\u2212c1) 1\n( 1 +\n2\u03bbc2 \u03b21T\ne2\u03bbc2/\u03b21 ) c21T \u22122\u03bbc1 \u03b21T+1\u2211 i=i0+1 i2\u03bbc1\u22122\n\u2264 4\u039321\u03b2 2\u03bb(c2\u2212c1) 1\n( 1 +\n2\u03bbc2 \u03b21T\ne2\u03bbc2/\u03b21 ) c21T \u22122\u03bbc1 ( (\u03b21T ) 2\u03bbc1\u22121\n2\u03bbc1 \u2212 1 +O\n( T 2\u03bbc1\u22122 )) = 4\u039321c 2 1\u03b2 2\u03bbc2\u22121 1\n2\u03bbc1 \u2212 1 \u00b7 1 T\n+O ( 1\nT 2\n) +O ( 1\nT 3\n) .\nThe last term in (25): We bound this as follows:\n\u039322 >\u2211 i=\u03b21T+1 c22 i2 e \u22122\u03bbc2 \u2211> j=i+1 1 j \u2264 \u039322 >\u2211 i=\u03b21T+1 c22 i2 e\u22122\u03bbc2 log T i+1\n= \u039322c 2 2T \u22122\u03bbc2 >\u2211 i=\u03b21T+1 (i+ 1)2\u03bbc2 i2 \u2264 4\u039322c22T\u22122\u03bbc2 >\u2211 i=\u03b21T+1 (i+ 1)2\u03bbc2 (i+ 1)2\n= 4\u039322c 2 2T \u22122\u03bbc2 >+1\u2211 i=\u03b21T+2 i2\u03bbc2\u22122, (28)\nwhere the first inequality follows from (17) and the last inequality from (1 + 1i ) 2 \u2264 4. If 2\u03bbc2 6= 1 and 2\u03bbc2 \u2264 2, using (16) from Lemma 3,\nT+1\u2211 j=\u03b21T+2 i2\u03bbc2\u22122 \u2264 1\u2212 \u03b2 2\u03bbc2\u22121 1 2\u03bbc2 \u2212 1 T 2\u03bbc2\u22121.\nIf 2\u03bbc2 > 2, using (14) from Lemma 3,\nT+1\u2211 j=\u03b21T+2 i2\u03bbc2\u22122 = T\u22121\u2211 j=\u03b21T i2\u03bbc2\u22122 + T 2\u03bbc2\u22122 + (T + 1)2\u03bbc2\u22122 \u2212 (\u03b21T + 1)2\u03bbc2\u22122 \u2212 (\u03b21T )2\u03bbc2\u22122\n= 1\u2212 \u03b22\u03bbc2\u221211\n2\u03bbc2 \u2212 1 T 2\u03bbc2\u22121 +O\n( T 2\u03bbc2\u22122 ) .\nIf 2\u03bbc2 = 2,\nT+1\u2211 j=\u03b21T+2 i2\u03bbc2\u22122 = T+1\u2211 j=\u03b21T+2 1 = (1\u2212 \u03b21)T.\nIn all three cases we have\nT+1\u2211 j=\u03b21T+2 i2\u03bbc2\u22122 \u2264 1\u2212 \u03b2 2\u03bbc2\u22121 1 2\u03bbc2 \u2212 1 T 2\u03bbc2\u22121 +O ( T 2\u03bbc2\u22122 ) .\nThen (28) can be further upper bounded for 2\u03bbc2 6= 1\n4\u039322c 2 2T \u22122\u03bbc2 >+1\u2211 i=\u03b21T+2 i2\u03bbc2\u22122 \u2264 4\u039322 c22(1\u2212 \u03b2 2\u03bbc2\u22121 1 ) 2\u03bbc2 \u2212 1 \u00b7 1 T +O ( 1 T 2 ) . (29)\nIf 2\u03bbc2 = 1, we have\nT+1\u2211 j=\u03b21T+2 i2\u03bbc2\u22122 = T\u2211 j=\u03b21T+1 i\u22121 \u2212 (\u03b21T + 1)\u22121 + (T + 1)\u22121 \u2264 log 1 \u03b21 ,\nand then\n4\u039322c 2 2T \u22122\u03bbc2 >+1\u2211 i=\u03b21T+2 i2\u03bbc2\u22122 \u2264 4\u039322c22 log 1 \u03b21 \u00b7 1 T .\nwhich is basically taking the limit as 2\u03bbc2 \u2192 1 of the highest order term of (29). Therefore the summation of the three terms is of order O( 1T ) (from the second and third terms), and the constant in the front of the highest order term takes on one of two values:\n1. If 2\u03bbc2 6= 1,\n4\u039321 c21\u03b2 2\u03bbc2\u22121 1\n2\u03bbc1 \u2212 1 + 4\u039322\nc22(1\u2212 \u03b2 2\u03bbc2\u22121 1 )\n2\u03bbc2 \u2212 1 .\n2. If 2\u03bbc2 = 1,\n4\u039321 c21\u03b2 2\u03bbc2\u22121 1\n2\u03bbc1 \u2212 1 + 4\u039322c 2 2 log\n1\n\u03b21 ."}, {"heading": "A.3.2 Proof of Lemma 1", "text": "Proof. (Of Lemma 1) Omitting the constant terms and setting k1 = 2\u03bbc1, k2 = 2\u03bbc2, we can re-write (13) as 1/T times\nQ(k1, k2) =\u0393 2 1\n\u03b2k2\u221211 k 2 1\nk1 \u2212 1 + \u039322 (1\u2212 \u03b2k2\u221211 )k22 k2 \u2212 1 , (30)\nwith k\u22171 = 2\u03bbc \u2217 1 = 2. Observe that in this case, k\u22172 \u2265 2. Let x = k2 \u2212 1; then x \u2265 1. Plugging in k\u22171 = 2, we can re-write (30) as\nQ(x) = 4\u039321\u03b2 x 1 + \u0393 2 2(1\u2212 \u03b2x1 )\n( x+ 1\nx + 2\n) . (31)\nTaking the derivative, we see that Q\u2032(x) =\u2212 4\u039321\u03b2x1 log(1/\u03b21) + \u039322(1\u2212 \u03b2x1 ) (\n1\u2212 1 x2\n) + \u039322 ( x+ 1\nx + 2\n) \u03b2x1 log(1/\u03b21). (32)\nSuppose\nl = 2 log(\u03931/\u03932) + log log(1/\u03b21)\nlog(1/\u03b21) .\nObserve that \u03b2l1 log(1/\u03b21) = \u039322 \u039321 . Plugging x = l in to (32), the first term is \u22124\u039322, the second term is at most \u039322, and the third term is at most \u039342 \u039321 (l+ 1l + 2). Observe that for any fixed \u03b21, for large enough \u03931/\u03932, l \u2265 1. Thus, the right hand side of (32) is at most: \u22124\u039322 + \u039322 + \u039342 \u039321 (l + 3). For fixed \u03b21, l grows logarithmically in \u03931/\u03932, and hence, for large enough \u03931/\u03932, \u039322(l+3)\n\u039321 will become\narbitrarily small. Therefore, for large enough \u03931/\u03932, Q \u2032(l) < 0. Suppose\nu = 2 log(4\u03931/\u03932) + log log(1/\u03b21)\nlog(1/\u03b21) .\nObserve that \u03b2u1 log(1/\u03b21) = \u039322\n16\u039321 . Plugging in x = u to (32), the first term reduces to \u221214\u0393 2 2, the\nsecond term is \u039322(1\u2212 \u03b2u1 )(1\u2212 1u2 ), and the third term is \u2265 0. Observe that as \u03931/\u03932 \u2192\u221e with \u03b21 fixed, \u03b2u1 \u2192 0 and 1/u2 \u2192 0. Thus, for large enough \u03931/\u03932, \u039322(1\u2212\u03b2u1 )(1\u2212 1u2 )\u2192 \u0393 2 2, and therefore Q\u2032(u) > 0. Thus, Q\u2032(x) = 0 somewhere between l and u and the first part of the lemma follows. Consider\nx = 2 log(m\u03931/\u03932) + log log(1/\u03b21)\nlog(1/\u03b21)\nwith 1 \u2264 m \u2264 4. The first term of (31) is always positive. As for the second term, x+ 1x +2 \u2265 x for positive x and \u03b2x1 =\n\u039322 m2\u039321 1 log(1/\u03b21) is small when \u03931/\u03932 is sufficiently large. Therefore for sufficiently\nlarge \u03931/\u03932, we have \u0393 2 2(1 \u2212 \u03b2x1 )(x + 1x + 2) \u2265 \u039322 2 x, and thus Q(x) \u2265 \u039322 2 x, which gives the lower bound. And plugging in x = l gives the upper bound."}, {"heading": "A.3.3 Proof of Lemma 2", "text": "Proof. (Of Lemma 2) Let k2 = ; then \u2265 0. Plugging in k\u22171 = 2, we can re-write (30) as\nQ( ) = 4\u039321\u03b2 \u22121 1 + \u0393 2 2(1\u2212 \u03b2 \u221211 )\n( \u22121 + + 1\n\u22121 + + 2\n) . (33)\nTaking the derivative, we obtain the following:\nQ\u2032( ) =\u2212 4\u039321\u03b2 \u221211 log(1/\u03b21) + \u0393 2 2(1\u2212 \u03b2 \u221211 )(1\u2212\n1 (1\u2212 )2 )\u2212 \u0393\n2 2 2\n1\u2212 \u03b2 \u221211 log(1/\u03b21)\n=\u2212 \u03b2 \u221211 log(1/\u03b21) ( 4\u039321 + \u039322 2\n1\u2212\n) + \u039322(\u03b2 \u22121 1 \u2212 1) ( 1 (1\u2212 )2 \u2212 1 )\n=\u2212 \u03b2 \u221211 log(1/\u03b21) ( 4\u039321 + \u039322 2\n1\u2212\n) + \u039322(\u03b2 \u22121 1 \u2212 1) (2\u2212 ) (1\u2212 )2 . (34)\nFor = \u039321 \u039322 \u2264 1, using 1\u2212 \u03b21\u2212 1 \u2264 (1\u2212 ) log(1/\u03b21) and \u03b2 \u22121 1 \u2212 1 = (1\u2212 \u03b2 1\u2212 1 )\u03b2 \u22121, this is at most:\n\u2212\u03b2 \u221211 log(1/\u03b21) ( 4\u039321 + \u039322 2\n1\u2212 \u2212 \u0393 2 2 (2\u2212 ) 1\u2212 ) = \u22122\u039321\u03b2 \u221211 log(1/\u03b21).\nThus, at l = \u039321 \u039322 , Q\u2032(l) < 0. Moreover, for \u2208 [0, 12 ], 1\u2212 \u03b2 1\u2212 1 \u2265 \u03b21(1\u2212 ) log(1/\u03b21). Therefore, Q\u2032( ) is at least:\nQ\u2032( ) \u2265 \u2212\u03b2 \u221211 log(1/\u03b21) ( 4\u039321 + \u039322 2\n1\u2212\n) + \u039322\u03b2 1 log(1/\u03b21)\n(2\u2212 ) 1\u2212\n\u2265 \u03b2 \u221211 log(1/\u03b21) ( \u039322\u03b2 (2\u2212 ) 1\u2212 \u2212 4\u039321 \u2212 \u039322 2 1\u2212 ) .\nLet u = 8\u039321 \u03b21\u039322 ; suppose that \u03932/\u03931 is large enough such that u \u2264 \u03b21/4. Then, u(2\u2212u)\u03b21\u2212u2 \u2265 15u\u03b2116 , and\n\u039322(u(2\u2212 u)\u03b21 \u2212 u2) 1\u2212 u \u2265 15\u0393 2 2u\u03b21 16(1\u2212 \u03b21) \u2265 15\u0393 2 1 2(1\u2212 \u03b21) \u2265 5\u039321.\nTherefore, Q\u2032(u) > 0, and thus Q( ) is minimized at some \u2208 [l, u]. For the second part of the lemma, the upper bound is obtained by plugging in = \u03931\u03932 . For the lower bound, observe that for any \u2208 [l, u], Q( ) \u2265 4\u039321\u03b2 u\u22121 1 \u2265 4\u039321\u03b2 \u039322/\u03b2\u0393 2 1\u22121 1 ."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of convex optimization", "author": ["A. Agarwal", "P.L. Bartlett", "P. Ravikumar", "M.J. Wainwright"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2009}, {"title": "Private empirical risk minimization, revisited", "author": ["R. Bassily", "A. Thakurta", "A. Smith"], "venue": "In Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "Scaling up Machine Learning, Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "Bekkerman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bekkerman et al\\.", "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Learning from data of variable quality", "author": ["K. Crammer", "M. Kearns", "J. Wortman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Privacy aware learning", "author": ["J. Duchi", "M. Jordan", "M. Wainwright"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["M. Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "Kearns.,? \\Q2008\\E", "shortCiteRegEx": "Kearns.", "year": 2008}, {"title": "Making gradient descent optimal for strongly convex", "author": ["O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridaran"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2012}, {"title": "Stochastic gradient descent with differen", "author": ["K. Chaudhuri", "Anand D. Sarwate"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "A statistical framework for differential privacy", "author": ["S. Zhou"], "venue": "Journal of the American", "citeRegEx": "Wasserman and Zhou.,? \\Q2013\\E", "shortCiteRegEx": "Wasserman and Zhou.", "year": 2013}, {"title": "Dual averaging methods for regularized stochastic learning and online", "author": [], "venue": null, "citeRegEx": "Xiao.,? \\Q2009\\E", "shortCiteRegEx": "Xiao.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011).", "startOffset": 158, "endOffset": 196}, {"referenceID": 2, "context": "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011).", "startOffset": 158, "endOffset": 196}, {"referenceID": 3, "context": "To our knowledge, Crammer et al. (2006) were the first to provide a theoretical study of how to learn classifiers from data of variable quality.", "startOffset": 18, "endOffset": 40}, {"referenceID": 2, "context": "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011). We assume that the training data are accessed through an oracle which provides an unbiased but noisy estimate of the gradient of the objective. The noise comes from two sources: the random sampling of a data point, and additional noise due to the data quality. Our two motivating applications \u2013 learning with local differential privacy and learning from data of variable quality \u2013 can both be modeled as solving a regularized convex optimization problem using SGD. Learning from data with heterogeneous noise in this framework thus reduces to running SGD with noisy gradient estimates, where the magnitude of the added noise varies across iterations. Main results. In this paper we study noisy stochastic gradient methods when learning from multiple data sets with different noise levels. For simplicity we consider the case where there are two data sets, which we call Clean and Noisy. We process these data sets sequentially using SGD with learning rate O(1/t). In a future full version of this work we also analyze averaged gradient descent (AGD) with learning rate O(1/ \u221a t). We address some basic questions in this setup: In what order should we process the data? Suppose we use standard SGD on the union of Clean and Noisy. We show theoretically and empirically that the order in which we should process the datasets to get good performance depends on the learning rate of the algorithm: in some cases we should use the order (Clean,Noisy) and in others (Noisy,Clean). Can we use knowledge of the noise rates? We show that using separate learning rates that depend on the noise levels for the clean and noisy datasets improves the performance of SGD. We provide a heuristic for choosing these rates by optimizing an upper bound on the error for SGD that depends on the ratio of the noise levels. We analytically quantify the performance of our algorithm in two regimes of interest. For moderate noise levels, we demonstrate empirically that our algorithm outperforms using a single learning rate and using clean data only. Does using noisy data always help? The work of Crammer et al. (2006) suggests that if the noise level of noisy data is above some threshold, then noisy data will not help.", "startOffset": 173, "endOffset": 2296}, {"referenceID": 0, "context": "When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives.", "startOffset": 92, "endOffset": 189}, {"referenceID": 6, "context": "For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009).", "startOffset": 126, "endOffset": 241}, {"referenceID": 4, "context": "There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al.", "startOffset": 124, "endOffset": 193}, {"referenceID": 7, "context": "In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy.", "startOffset": 74, "endOffset": 120}, {"referenceID": 2, "context": "For simplicity we, like previous work Crammer et al. (2006), assume that the algorithms know the noise levels exactly.", "startOffset": 38, "endOffset": 60}, {"referenceID": 0, "context": "When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al.", "startOffset": 121, "endOffset": 943}, {"referenceID": 0, "context": "When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD.", "startOffset": 121, "endOffset": 1039}, {"referenceID": 0, "context": "When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy.", "startOffset": 121, "endOffset": 1187}, {"referenceID": 0, "context": "When the objective function is \u03bb-strongly convex, the learning rate used for SGD is O(1/\u03bbt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/\u03bb2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/\u03bbt) followed by some form of averaging of the iterates achieves O(1/\u03bbt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy. Our work is an extension of these papers to heterogeneous privacy requirements. Crammer et al. (2006) study classification when the labels in each data set are corrupted by RCN of different rates.", "startOffset": 121, "endOffset": 1350}, {"referenceID": 7, "context": "1 Local Differential Privacy Local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012; Kasiviswanathan et al., 2008) is a strong notion of privacy motivated by differential privacy (Dwork et al.", "startOffset": 56, "endOffset": 132}, {"referenceID": 7, "context": "Duchi et al. (2012) showed that this mechanism provides -local privacy assuming analytic conditions on the loss function, bounded data, and that the oracle generates a fresh random sample at each invocation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "2 Random Classification Noise In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, \u1ef91), .", "startOffset": 74, "endOffset": 88}, {"referenceID": 8, "context": "2 Random Classification Noise In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, \u1ef91), . . . , (xT , \u1ef9T ), where each \u1ef9i \u2208 {\u22121, 1} has been obtained by independently flipping the true label yi with some probability \u03c3. Natarajan et al. (2013) showed that solving", "startOffset": 74, "endOffset": 313}, {"referenceID": 5, "context": "In practice, we may wish to learn classifiers from multiple datasets with different amounts of classification noise (Crammer et al., 2006); for example, we may have a small dataset D1 labeled by domain experts, and a larger noisier dataset D2, labeled via crowdsourcing, with flip probabilities \u03c31 and \u03c32.", "startOffset": 116, "endOffset": 138}], "year": 2014, "abstractText": "We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogenous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate.", "creator": "LaTeX with hyperref package"}}}