{"id": "1412.5721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "An Algorithm for Online K-Means Clustering", "abstract": "In this model, the algorithm gets vectors v1,..., vn in any order. For each vector vi, the algorithm outputs a cluster detection before it gets vi + 1. Our online algorithm generates ~ O (k) clusters whose k mean cost is ~ O (W *), where W * is the optimal k mean cost using k clusters and ~ O suppresses poly-logarithmic factors. We also show that it is experimentally not much worse than k mean + + while operating in a strictly restrictive mathematical model.", "histories": [["v1", "Thu, 18 Dec 2014 05:09:32 GMT  (216kb,D)", "https://arxiv.org/abs/1412.5721v1", null], ["v2", "Mon, 23 Feb 2015 17:30:23 GMT  (220kb,D)", "http://arxiv.org/abs/1412.5721v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["edo liberty", "ram sriharsha", "maxim sviridenko"], "accepted": false, "id": "1412.5721"}, "pdf": {"name": "1412.5721.pdf", "metadata": {"source": "CRF", "title": "An Algorithm for Online K-Means Clustering", "authors": ["Edo Liberty", "Ram Sriharsha", "Maxim Sviridenko"], "emails": ["edo@yahoo-inc.com,", "harshars@yahoo-inc.com,", "sviri@yahoo-inc.com,Yahoo"], "sections": [{"heading": "1 Introduction", "text": "One of the most basic and well-studied optimization models in unsupervised Machine Learning is k-means clustering. In this problem we are given the set V of n points (or vectors) in Euclidian space. The goal is to partition V into k sets called clusters S1, . . . , Sk and choose one cluster center ci for each cluster Si to minimize\nk\u2211 i=1 \u2211 v\u2208Si ||v \u2212 ci||22.\nIn the standard offline setting, the set of input points is known in advance and the data access model is unrestricted. Even so, obtaining provably good solutions to this problem is difficult. See Section 1.2.\nIn the streaming model the algorithm must consume the data in one pass and is allowed to keep only a small (typically constant or poly-logarithmic in n) amount of information. Nevertheless, it must output its final decisions \u2217edo@yahoo-inc.com, Yahoo Labs, New York, NY \u2020harshars@yahoo-inc.com, Yahoo Labs, Sunnyvale, CA \u2021sviri@yahoo-inc.com,Yahoo Labs, New York, NY 1The notation O\u0303(\u00b7) suppresses poly-logarithmic factors.\nwhen the stream has ended. For example, the location of the centers for k-means. This severely restricted data access model requires new algorithmic ideas. See Section 1.2 for prior art. Notice that, in the streaming model, the assignment of individual points to clusters may become available only in hindsight.\nIn contrast, the online model of computation does not allow to postpone clustering decisions. In this setting, an a priori unknown number of points arrive one by one in an arbitrary order. When a new point arrives the algorithm must either put it in one of the existing clusters or open a new cluster (consisting of a single point). Note that this problem is conceptually non trivial even if one could afford unbounded computational power at every iteration. This is because the quality of current choices depend on the unknown (yet unseen) remainder of the stream.\nIn this paper, we consider the very restricted setting in the intersection of these two models. We require the algorithm outputs a single cluster identifier for each point online while using space and time at most poly-logarithmic in the length of the stream. This setting is harder than the streaming model. On the one hand, any space efficient online algorithm is trivially convertible to a streaming algorithm. One could trivially keep sufficient statistics for each cluster such that the centers of mass could be computed at the end of the stream. The computational and space overhead are independent of the length of the stream. On the other hand, the online problem resists approximation even in one dimension and k = 2.\nConsider the stream where v1 = 0 and v2 = 1 (acting as one dimensional vectors). Any online clustering algorithm must assign them to different clusters. Otherwise, the algorithm cost is 1/2 and the optimal is cost is trivially 0. If the the algorithm assigns v1 and v2 to different clusters, the third point might be v3 = c for some c 1. At this point, the algorithm is forced to assign v3 to one of the existing clusters incurring cost of \u2126(c) which\nar X\niv :1\n41 2.\n57 21\nv2 [\ncs .D\nS] 2\n3 Fe\nb 20\n15\nis arbitrarily larger than the optimal solution of cost 1/2. This example also proves that any online algorithm with a bounded approximation factor (such as ours) must create strictly more than k clusters.\nIn this work we provide algorithms for both online k-means and semi-online k-means. In the semi-online model we assume having a lower bound, w\u2217, for the total optimal cost of k-means, W \u2217, as well as an estimate for n, the length of the stream. Algorithm 1 creates at most\nO(k log n log(W \u2217/w\u2217))\nclusters in expectation and has an expected objective value of O(W \u2217). From a practical viewpoint, it is reasonable to assume having rough estimates for w\u2217 and n. Since the dependence on both estimates is logarithmic, the performance of the semi-online algorithm will degrade significantly only if our estimates are wrong by many orders of magnitude. In the fully online model we do not assume any prior knowledge. Algorithm 2 operates in that setting and opens a comparable number of clusters to Algorithm 1. But, its approximation factor guarantee degrades by a log n-factor."}, {"heading": "1.1 Motivation", "text": "In the context of machine learning, the results of k-means were shown to provide powerful unsupervised features [11] on par, sometimes, with neural nets for example. This is often referred to as (unsupervised) feature learning. Intuitively, if the clustering captures most of the variability in the data, assigning a single label to an entire cluster should be pretty accurate. It is not surprising therefore that cluster labels are powerful features for classification. In the case of online machine learning, these cluster labels must also be assigned online. The importance of such an online k-means model was already recognized in machine learning community [10, 12].\nFor information retrieval, [9] investigated the incremental k-centers problem. They argue that clustering algorithms, in practice, are often required to be online. We observe the same at Yahoo. For example, when suggesting news stories to users, we want to avoid suggesting those that are close variants of those they already read. Or, conversely, we want to suggest stories which are a part of a story-line the user is following. In either scenario, when\nYahoo receives a news item, it must immediately decide what cluster it belongs to and act accordingly."}, {"heading": "1.2 Prior Art", "text": "In the offline setting where the set of all points is known in advance, Lloyd\u2019s algorithm [20] provides popular heuristics. It is so popular that practitioners often simply refer to it as k-means. Yet, only recently some theoretical guaranties were proven for its performance on \u201cwell clusterable\u201d inputs [23]. The k-means++ [5] algorithm provides an expected O(log(k)) approximation or an efficient seeding algorithm. A well known theoretical algorithm is due to Kanungo et al. [18]. It gives a constant approximation ratio and is based on local search ideas popular in the related area of design and analysis of algorithms for facility location problems, e.g., [6]. Recently, [2] improved the analysis of [5] and gave an adaptive sampling based algorithm with constant factor approximation to the optimal cost. In an effort to make adaptive sampling techniques more scalable, [8] introduced k-means\u2016 which reduces the number of passes needed over the data and enables improved parallelization.\nThe streaming model was considered by [3] and [22] and later by [1]. They build upon adaptive sampling ideas from [5, 8] and branch-and-bound techniques from [17].\nThe first (to our knowledge) result in online clustering dates back the k-centers result of [9]. For k-means an Expectation Maximization (EM) approach was investigated by [19]. Their focus was on online EM as a whole but their techniques include online clustering. They offer very encouraging results, especially in the context of machine learning. To the best of our understanding, however, their techniques do not extend to arbitrary input sequences. In contrast, the result of [10] provides provable results for the online setting in the presence of base-k-means algorithm as experts.\nA closely related family of optimization problems is known as facility location problems. Two standard variants are the uncapacitated facility location problem (or the simple plant location problem in the Operations Research jargon) and the k-median problem. These problems are well-studied both from computational and theoretic viewpoints (a book [13] and a survey [24] provide the background on some of the aspects in this area). Meyerson [21] suggested a simple and elegant algorithm\nfor the online uncapacitated facility location with competitive ratio of O(log n). Fotakis [15] suggested a primal-dual algorithm with better performance guarantee of O(log n/ log log n). Anagnostopoulos et al. [4] considered a different set of algorithms based on hierarchical partitioning of the space and obtained similar competitive ratios. The survey [16] summarizes the results in this area. As a remark, [9] already considered connections between facility location problems and clustering. Interestingly, their algorithm is often referred to as \u201cthe doubling algorithm\u201d since the cluster diameters double as the algorithm receives more points. In our work the facility location cost is doubled which is technically different but intuitively related.\n2 Semi-Online k-means Algorithm\nWe begin with presenting the semi-online algorithm. It assumes knowing the number of vectors n and some lower bound w\u2217 for the value of the optimal solution. These assumptions make the algorithm slightly simpler and the result slightly tighter. Nevertheless, the semi-online online already faces most of the challenges faced by the fully online version. In fact, proving the correctness of the online algorithm (Section 3) would require only minor adjustments to the proofs in this section.\nThe algorithm uses ideas from the online facility location algorithm of Meyerson [21]. The intuition is as follows; think about k-means and a facility location problem where the service costs are squared Euclidean distances. For the facility cost, start with f1 which is known to be too low. By doing that the algorithm is \u201cencouraged\u201d to open many facilities (centers) which keeps the service costs low. If the algorithm detect that too many facilities were opened, it can conclude that the current facility cost is too low. It therefore doubles the facility cost of opening future facilities (centers). It is easy to see that the facility cost cannot be doubled many times without making opening new clusters prohibitively expensive. In Algorithm 2 we denote the distance of a point v to a set C asD(v, C) = minc\u2208C \u2016v\u2212c\u2016. As a convention, if C = \u2205 then D(v, C) =\u221e for any v.\nConsider some optimal solution consisting of clusters\nAlgorithm 1 semi-online k-means algorithm input: V , k, w\u2217, n C \u2190 \u2205 r \u2190 1; q1 \u2190 0; f1 \u2190 w\u2217/k log(n) for v \u2208 V do\nwith probability p = min(D2(v, C)/fr, 1) C \u2190 C \u222a {v}; qr \u2190 qr + 1 if qr \u2265 3k(1 + log(n)) then r \u2190 r + 1; qr \u2190 0; fr \u2190 2 \u00b7 fr\u22121 end if yield: c = arg minc\u2208C \u2016v \u2212 c\u20162\nend for\nS\u22171 , . . . , S \u2217 k with cluster centers c \u2217 1, . . . , c \u2217 k. Let\nW \u2217i = \u2211 v\u2208S\u2217i ||v \u2212 c\u2217i ||22\nbe the cost of the i-th cluster in the optimal solution and W \u2217 = \u2211k i=1W \u2217 i be the value of the optimal solution. Let A\u2217i be the average squared distance to the cluster center from a vectors in the i-th optimal cluster.\nA\u2217i = 1 |S\u2217i | \u2211 v\u2208C\u2217i \u2016v \u2212 c\u2217i \u201622 = W \u2217i |S\u2217i | .\nWe define a partition of the cluster S\u2217i into subsets that we call rings:\nS\u2217i,0 = {v \u2208 S\u2217i : ||v \u2212 c\u2217i ||22 \u2264 A\u2217i }\nand for 1 \u2264 \u03c4 \u2264 log n S\u2217i,\u03c4 = { v \u2208 S\u2217i : ||v \u2212 c\u2217i ||22 \u2208 (2\u03c4\u22121A\u2217i , 2\u03c4A\u2217i ] } .\nNote that we consider only values of \u03c4 \u2264 log n since S\u2217i,\u03c4 = \u2205 for \u03c4 > log(|S\u2217i |). To verify assume the contrary and compute A\u2217i .\nTheorem 1. Let C be the set of clusters defined by Algorithm 1. Then\nE[|C|] = O ( k log n log W \u2217\nw\u2217\n) .\nProof. Consider the phase r\u2032 of the algorithm where, for the first time\nfr\u2032 \u2265 W \u2217\nk log n .\nThe initial facility cost f1 is doubled at every phase during each of which the algorithm creates 3k(1+log n) clusters. The total number of clusters opened before phase r\u2032 is trivially upper bounded by 3k(1 + log n) log fr\u2032f1 . Which is, in turn, O(k log n log W \u2217\nw\u2217 ) by the choice of f1. Bounding the number of centers opened during and after phase r\u2032 is more complicated. Denote by S\u2217i,\u03c4,r the set of points in the ring S\u2217i,\u03c4 that our algorithm encounters during phase r. The expected number of clusters initiated by vectors in the ring S\u2217i,\u03c4 during phases r\n\u2032, . . . , R is at most\n1 + \u2211 r\u2265r\u2032 4 \u00b7 2\u03c4A\u2217i fr |S\u2217i,\u03c4,r| .\nThis is because once we open the first cluster with a center at some v \u2208 S\u2217i,\u03c4 the probability of opening a cluster for each subsequent vector v\u2032 \u2208 S\u2217i,\u03c4 is upper bounded by\n||v \u2212 v\u2032||22 fr \u2264 2||v \u2212 c \u2217 i ||22 + 2||v\u2032 \u2212 c\u2217i ||22 fr \u2264 4 \u00b7 2 \u03c4A\u2217i fr\nby the (squared) triangle inequality for v, v\u2032 \u2208 S\u2217i,\u03c4 . Therefore the expected number of clusters chosen from S\u2217i during and after phase r \u2032 is at most\n\u2211 \u03c4\u22650 1 + \u2211 r\u2265r\u2032 4 \u00b7 2\u03c4A\u2217i fr |S\u2217i,\u03c4,r|  \u2264 1 + log n+\n\u2211 \u03c4\u22650 4 \u00b7 2\u03c4A\u2217i \u2211 r\u2265r\u2032 |S\u2217i,\u03c4,r| fr\n\u2264 1 + log n+ 4 fr\u2032 \u2211 \u03c4\u22650 2\u03c4A\u2217i |S\u2217i,\u03c4 |\n\u2264 1 + log n+ 4 fr\u2032 A\u2217i |S\u2217i,0|+ 8 fr\u2032 \u2211 \u03c4\u22651 2\u03c4\u22121A\u2217i |S\u2217i,\u03c4 |\n\u2264 1 + log n+ 4 fr\u2032 A\u2217i |S\u2217i |+ 8 fr\u2032 \u2211 \u03c4\u22651 \u2211 v\u2208S\u2217i,\u03c4 \u2016v \u2212 c\u2217i \u20162\n\u2264 1 + log n+ 4 fr\u2032 W \u2217i + 8 fr\u2032 W \u2217i \u2264 1 + log n+ 12W \u2217 i\nfr\u2032 . Summing up over all i = 1, . . . , k using \u2211 iW \u2217 i = W \u2217 we obtain that the expected number of centers chosen during phases r\u2032, . . . , R is at most\nk(1 + log n) + 12W \u2217/fr\u2032 . (1)\nSubstituting fr\u2032 \u2265W \u2217/k log n completes the proof of the theorem.\nBefore we estimate the expected cost of clusters opened by our online algorithm we prove the following technical lemma.\nLemma 1. We are given a sequence X1, . . . , Xn of n independent experiments. Each experiment succeeds with probability pi \u2265 min{Ai/B, 1} where B \u2265 0 and Ai \u2265 0 for i = 1, . . . , n. Let t be the (random) number of sequential unsuccessful experiments, then:\nE [ t\u2211 i=1 Ai ] \u2264 B.\nProof. Let n\u2032 be the maximal index for which pi < 1 for all i \u2264 n\u2032.\nE [ t\u2211 i=1 Ai ] = n\u2032\u2211 i=1 Ai Pr[t \u2265 i]\n\u2264 n\u2032\u2211 i=1 Ai i\u220f j=1 ( 1\u2212 Aj B ) \u2264 B n\u2032\u2211 i=1 Ai B i\u22121\u220f j=1 ( 1\u2212 Aj B\n) \u2264 B.\nThe last inequality uses Ai/B \u2264 pi < 1 for i \u2264 n\u2032.\nTheorem 2. Let W be the cost of the online assignments of Algorithm 1 and W \u2217 the optimal k-means clustering cost. Then\nE[W ] = O(W \u2217) .\nProof. Consider the service cost of vectors in each ring S\u2217i,\u03c4 in two separate stages. Before a vector from the ring is chosen to start a new cluster and after. Before a center from S\u2217i,\u03c4 is chosen each vector v \u2208 S\u2217i,\u03c4 is chosen with probability p \u2265 min{d2(v, C)/fR, 1}. Here, C is the set of centers already chosen by the algorithm before encountering v. If v is not chosen the algorithm incurs a cost of d2(v, C). By Lemma 1 the expected sum of these costs is bounded by fR. Summing over all the rings we get a contribution of O(fRk log n).\nAfter a vector v \u2208 S\u2217i,\u03c4 is chosen to start a new cluster, the service cost of each additional vector v\u2032 is at most \u2016v\u2212 v\u2032\u20162 \u2264 4 \u00b7 2\u03c4A\u2217i . Summing up over all vectors and rings, this stage contributes are most 4 \u2211 i \u2211 \u03c4 \u00b72\u03c4A\u2217i \u00b7 |S\u2217i,\u03c4 | \u2264 12W \u2217 to the cost of our solution. All in all, the expected online k-means cost is bounded by\nE[W ] = O(fRk log n+W \u2217) .\nWe now turn to estimating E[fR]. Consider the first phase r\u2032\u2032 of the algorithm such that\nfr\u2032\u2032 \u2265 36W \u2217\nk(1 + log n) .\nBy Equation 1 the expected number of clusters opened during and after phase r\u2032\u2032 is at most k(1 + log n) + 12W \u2217/fr\u2032\u2032 \u2264 43k(1 + log n). By Markov\u2019s inequality the probability of opening more than 3k(1 + log n) clusters is at most 4/9. Therefore, with probability at least 5/9 the algorithm will conclude while at phase r\u2032\u2032.\nLet p be the probability that our algorithm terminates before round r\u2032\u2032. Since the probability of concluding the execution at each of the rounds after r\u2032\u2032 is at least 5/9 we derive an upper bound\nE[fR] \u2264 pfr\u2032\u2032\u22121 + (1\u2212 p) +\u221e\u2211 r=r\u2032\u2032 fr \u00b7 5 9 \u00b7 ( 4 9 )r\u2212r\u2032\u2032\n< fr\u2032\u2032 + fr\u2032\u2032 \u00b7 5\n9 +\u221e\u2211 i=0 2i \u00b7 ( 4 9 )i = O(fr\u2032\u2032)\nCombining E[fR] = O(fr\u2032\u2032) with our choice of fr\u2032\u2032 = O( W \u2217\nk(1+log n) ) and our previous observation that E[W ] = O(fRk log n+W \u2217) completes the proof.\n3 Fully Online k-means Algorithm Algorithm 2 is fully online yet it defers from Algorithm 1 in only a few aspects. First, since n is unknown, the initial facility cost and the doubling condition cannot depend on it. Second, it must generate its own lower boundw\u2217 based on a short prefix of points in the stream. Note that w\u2217 is trivially smaller that W \u2217. Any clustering of k + 1 points must put at least two points in one cluster, incurring a cost of \u2016v \u2212 v\u2032\u20162/2 \u2265 minv,v\u2032 \u2016v \u2212 v\u2032\u20162/2.\nAlgorithm 2 Online k-means algorithm input: V , k C \u2190 the first k+1 distinct vectors in V ; and n = k+1\n(For each of these yield itself as its center) w\u2217 \u2190 minv,v\u2032\u2208C \u2016v \u2212 v\u2032\u20162/2 r \u2190 1; q1 \u2190 0; f1 = w\u2217/k for v \u2208 the remainder of V do n\u2190 n+ 1 with probability p = min(D2(v, C)/fr, 1)\nC \u2190 C \u222a {v}; qr \u2190 qr + 1 if qr \u2265 3k(1 + log(n)) then r \u2190 r + 1; qr \u2190 0; fr \u2190 2 \u00b7 fr\u22121 end if yield: c = arg minc\u2208C \u2016v \u2212 c\u20162\nend for\nTheorem 3. Let C be the set of clusters defined by Algorithm 2. Then\nE[|C|] = O ( k log n log W \u2217\nw\u2217\n) = O (k log n log \u03b3n) .\nHere \u03b3 = maxv,v\u2032 \u2016v\u2212v \u2032\u2016\nminv,v\u2032 \u2016v\u2212v\u2032\u2016 is the dataset \u201caspect ratio\u201d.\nProof. Intuitively, for the same lower bound w\u2217 Algorithm 2 should create fewer centers than Algorithm 1 since its initial facility cost is higher and it is doubled more frequently. This intuition is made concrete by retracing the proof of Theorem 3 to show\nE[|C|] = O ( k log n log W \u2217\nw\u2217\n) .\nTo get a handle on the value of W \u2217/w\u2217, observe that W \u2217 \u2264 nmaxv,v\u2032 \u2016v \u2212 v\u2032\u20162. Combining this with the definition of \u03b3 we get log(W \u2217/w\u2217) = O(log \u03b3n).\nTheorem 4. Let W be the cost of the online assignments of Algorithm 2 and W \u2217 the optimal k-means clustering cost. Then\nE[W ] = O(W \u2217 log n) .\nProof. We start by following the argument of the proof of Theorem 2 verbatim. We arrive at the conclusion that\nE[W ] = O(fRk log n+W \u2217)\nwhere fR is the final facility cost of the algorithm and R is its last phase. Showing that E[fR] = O(W \u2217/k) will therefore complete the proof.\nConsider any phase r \u2265 r\u2032\u2032 of the algorithm where r\u2032\u2032 is the smallest index such that\nfr\u2032\u2032 \u2265 36W \u2217\nk .\nLet nr be the number of points from the input the algorithm went through by the end of phase r. Let qr be the number of clusters opened during phase r and q\u2032r the number those who are not the first in their ring.\nqr \u2264 k log(1 + log nr) + q\u2032r\nThe term k log(1+log nr) is an upper bound on the number of rings at the end of stage r. We pessimistically count at most one (the first) cluster from each such ring. Following the argument in the proof of Theorem 1 that lead us to Equation (1) we conclude E[q\u2032r] \u2264 12W \u2217/fr.\nAlgorithm 2 only advances to the next phase if qr \u2265 3 log(1 + log nr) which requires q\u2032r \u2265 2k(1 + log nr). By Markov\u2019s inequality and the fact that E[q\u2032r] \u2264 12W \u2217/fr \u2264 k/3 the probability of reaching the next phase is at most 1/6.\nWe now estimate E[fR]. Let p be the probability that our algorithm finishes before round r\u2032\u2032. We have\nE[fR] \u2264 pfr\u2032\u2032\u22121 + (1\u2212 p) +\u221e\u2211 r=r\u2032\u2032 fr \u00b7 5 6 \u00b7 ( 1 6 )r\u2212r\u2032\u2032\n\u2264 fr\u2032\u2032 + fr\u2032\u2032 \u00b7 5\n6 +\u221e\u2211 i=0 2i \u00b7 ( 1 6 )i = O(fr\u2032\u2032)\nSince fr\u2032\u2032 = O(W \u2217/k) the proof is complete."}, {"heading": "4 Experimental Analysis of the Algorithm", "text": ""}, {"heading": "4.1 Practical modifications to the algorithm", "text": "While experimenting with the algorithm, we discovered that some log factors were, in fact, too pessimistic in practice. We also had to make some pragmatic decisions about, for example, how to set the initial facility\ncost. As another practical adjustment we introduce the notion of ktarget and kactual. The value of ktarget is the number of clusters we would like the algorithm to output while kactual is the actual number of clusters generated. Internally, the algorithm operates with a value of k = d(ktarget \u2212 15)/5e. This is a heuristic (entirely adhoc) conversion that compensates for the kactual being larger than k by design.\nAlgorithm 3 Online k-means algorithm input: V , ktarget k = d(ktarget \u2212 15)/5e C \u2190 the first k + 10 vectors in V (For each of these yield itself as its center) w\u2217 \u2190 half the sum of the 10 smallest squared distances of points in C to their closest neighbor. r \u2190 1; q1 \u2190 0; f1 \u2190 w\u2217 for v \u2208 the remainder of V do\nwith probability p = min(D2(v, C)/fr, 1) C \u2190 C \u222a {v}; qr \u2190 qr + 1 if qr \u2265 k then r \u2190 r + 1; qr \u2190 0; fr \u2190 10 \u00b7 fr\u22121 end if yield: c = arg minc\u2208C \u2016v \u2212 c\u20162\nend for kactual \u2190 |C|"}, {"heading": "4.2 Datasets", "text": "To evaluate our algorithm we executed it on 12 different datasets. All the datasets that we used are conveniently aggregated on the LibSvm website [14] and on the UCI dataset collection [7]. Some basic information about each dataset is given in Table 1.\nFeature engineering for the sake of online learning is one of the motivations for this work. For that reason, we apply standard stochastic gradient descent linear learning with the squared loss on these data. Once with the raw features and once with the k-means features added. In some cases we see a small decrease in accuracy due to slower convergence of the learning on a larger feature set. This effect should theoretically be nullified in the presence of more data. In other cases, however, we see a significant uptick in classification accuracy. This is in agree-\nment with prior observations [11]."}, {"heading": "4.3 The number of online clusters", "text": "One of the artifacts of applying our online k-means algorithm is that the number of clusters is not exactly known a priory. But as we see in Figure 1, the number of resulting clusters is rather predictable and controllable. Figure 1 gives the ratio between the number of clusters output by the algorithm, kactual, and the specified target ktarget. The results reported are mean values of 3 runs for every parameter setting. The observed standard deviation of kactual is typically in the range [0, 3] and never exceeded 0.1 \u00b7 ktarget in any experiment. Figure 1 clearly shows that the ratio kactual/ktarget is roughly constant and close 1.0. Interestingly, the main differentiator is the choice of dataset."}, {"heading": "4.4 Online clustering cost", "text": "Throughout this section, we measure the online k-means clustering cost with respect to different baselines. We report averages of at least 3 different independent execu-\ntions for every parameter setting. In Figure 2 the reader can see the online k-means clustering cost for the set of centers chosen online by our algorithm for different values of ktarget and different datasets. For normalization, each cost is divided by f0, the sum of squares of all vector norms in the dataset (akin to the theoretical k-means cost of having one center at the origin). Note that some datasets are inherently unclusterable. Even using many cluster centers, the k-means objective does not decrease substantially. Nevertheless, as expected, the k-means cost obtained by the online algorithm, fonline, decreases as a function of ktarget.\nThe monotonicity of fonline with respect to ktarget is unsurprising. In Figure 3 we plot the ratio fonline/frandom as a function of ktarget. Here, frandom is the sum of squared distances of input points to kactual input points chosen uniformly at random (as centers). Note that in each experiment the number of clusters used by the random solution and online k-means is identical, namely, kactual. Figure 3 illustrates something surprising. The ratio between the costs remains relatively fixed per dataset and almost independent to ktarget. Put differently, even when the k-means cost is significantly lower than picking k random centers, they improve in similar rates as k grows.\nThe next experiment compares online k-means to k-\nmeans++. For every value of ktarget we ran online kmeans to obtain both fonline and kactual. Then, we invoke k-means++ using kactual clusters and computed its cost, fkmpp. This experiment was repeated 3 times for each dataset and each value of ktarget. The mean results are reported in Figure 4. Unsurprisingly, k-means++ is usually better in terms of cost. But, the reader should keep in mind that k-means++ is an offline algorithm that requires k passes over the data compared with the online computational model of our algorithm."}, {"heading": "5 Aknowledgements", "text": "We would like to thank Anna Choromanska and Sergei Vassilvitskii for very helpful suggestions and to Dean Foster for helping us with the proof of the Lemma 1."}], "references": [{"title": "Streamkm++: A clustering algorithm for data streams", "author": ["Marcel R. Ackermann", "Marcus M\u00e4rtens", "Christoph Raupach", "Kamil Swierkot", "Christiane Lammersen", "Christian Sohler"], "venue": "Journal of Experimental Algorithmics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 12th International Workshop", "author": ["Ankit Aggarwal", "Amit Deshpande", "Ravi Kannan"], "venue": "APPROX 2009, and 13th International Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["Nir Ailon", "Ragesh Jaiswal", "Claire Monteleoni"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A simple and deterministic competitive algorithm for online facility location", "author": ["Aris Anagnostopoulos", "Russell Bent", "Eli Upfal", "Pascal Van Hentenryck"], "venue": "Inf. Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM J. Comput.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Incremental clustering and dynamic information retrieval", "author": ["Moses Charikar", "Chandra Chekuri", "Tom\u00e1s Feder", "Rajeev Motwani"], "venue": "In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Online clustering with experts", "author": ["Anna Choromanska", "Claire Monteleoni"], "venue": "In Proceedings of the 9  Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "venue": "JMLR Proceedings,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Topics in unsupervised learning", "author": ["Sanjoy Dasgupta"], "venue": "Class Notes CSE 291,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Facility location - applications and theory", "author": ["Zvi Drezner", "Horst W. Hamacher"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Libsvm data: Classification, regression, and multi-label", "author": ["Rong-En Fan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "On the competitive ratio for online facility", "author": ["Dimitris Fotakis"], "venue": "location. Algorithmica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Online and incremental algorithms for facility location", "author": ["Dimitris Fotakis"], "venue": "SIGACT News,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Clustering data streams: Theory and practice", "author": ["Sudipto Guha", "Adam Meyerson", "Nina Mishra", "Rajeev Motwani", "Liadan O\u2019Callaghan"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "In Symposium on Computational Geometry,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Online EM for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1982}, {"title": "Online facility location. In FOCS, pages 426\u2013431", "author": ["Adam Meyerson"], "venue": "IEEE Computer Society,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fast and accurate k-means for large datasets", "author": ["Adam Meyerson", "Michael Shindler", "Alex Wong"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Approximation algorithms for facility location problems", "author": ["Jens Vygen"], "venue": "Lecture Notes, Technical Report No", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "In the context of machine learning, the results of k-means were shown to provide powerful unsupervised features [11] on par, sometimes, with neural nets for example.", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "The importance of such an online k-means model was already recognized in machine learning community [10, 12].", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "The importance of such an online k-means model was already recognized in machine learning community [10, 12].", "startOffset": 100, "endOffset": 108}, {"referenceID": 5, "context": "For information retrieval, [9] investigated the incremental k-centers problem.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "In the offline setting where the set of all points is known in advance, Lloyd\u2019s algorithm [20] provides popular heuristics.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Yet, only recently some theoretical guaranties were proven for its performance on \u201cwell clusterable\u201d inputs [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Recently, [2] improved the analysis of [5] and gave an adaptive sampling based algorithm with constant factor approximation to the optimal cost.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "They build upon adaptive sampling ideas from [5, 8] and branch-and-bound techniques from [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "The first (to our knowledge) result in online clustering dates back the k-centers result of [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "For k-means an Expectation Maximization (EM) approach was investigated by [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "In contrast, the result of [10] provides provable results for the online setting in the presence of base-k-means algorithm as experts.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "These problems are well-studied both from computational and theoretic viewpoints (a book [13] and a survey [24] provide the background on some of the aspects in this area).", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "These problems are well-studied both from computational and theoretic viewpoints (a book [13] and a survey [24] provide the background on some of the aspects in this area).", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "Meyerson [21] suggested a simple and elegant algorithm", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Fotakis [15] suggested a primal-dual algorithm with better performance guarantee of O(log n/ log log n).", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "[4] considered a different set of algorithms based on hierarchical partitioning of the space and obtained similar competitive ratios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The survey [16] summarizes the results in this area.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "As a remark, [9] already considered connections between facility location problems and clustering.", "startOffset": 13, "endOffset": 16}, {"referenceID": 17, "context": "The algorithm uses ideas from the online facility location algorithm of Meyerson [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "All the datasets that we used are conveniently aggregated on the LibSvm website [14] and on the UCI dataset collection [7].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Much more information is provided on LibSvm website [14] and in the UCI dataset collection [7].", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "ment with prior observations [11].", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "The observed standard deviation of kactual is typically in the range [0, 3] and never exceeded 0.", "startOffset": 69, "endOffset": 75}, {"referenceID": 7, "context": "Table 2: Corroborating the observations of [11] we report that adding k-means feature, particularly to low dimensional datasets, is very beneficial for improving classification accuracy.", "startOffset": 43, "endOffset": 47}], "year": 2015, "abstractText": "This paper shows that one can be competitive with the kmeans objective while operating online. In this model, the algorithm receives vectors v1, . . . , vn one by one in an arbitrary order. For each vector vt the algorithm outputs a cluster identifier before receiving vt+1. Our online algorithm generates \u00d5(k) clusters whose k-means cost is \u00d5(W \u2217) where W \u2217 is the optimal k-means cost using k clusters.1 We also show that, experimentally, it is not much worse than k-means++ while operating in a strictly more constrained computational model.", "creator": "LaTeX with hyperref package"}}}