{"id": "1412.5732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data", "abstract": "Online Multiple Output Regression is an important machine learning method for modelling, predicting and compressing multidimensional correlated data streams. In this paper, we propose a novel online multiple output regression method called MORES for streaming data. MORES can dynamically learn the structure of regression coefficients to facilitate the continuous refinement of the model. We observe that the limited expressiveness of the regression model, especially in the pre-stage of online updating, often results in the variables being dependent on the residual errors. In light of this point, MORES intends to dynamically learn and use the structure of residual errors to improve the prediction counter-accuracy. Furthermore, we define three statistical variables to represent\\ emph {precisely} all samples seen for\\ emph {step-by-step}, the predicted pre-loss of pre-generated data can be pre-loaded to the effective updating factor of each online updating round of the training sample, which can be pre-loaded to the effective updating factor.", "histories": [["v1", "Thu, 18 Dec 2014 06:37:50 GMT  (437kb,D)", "http://arxiv.org/abs/1412.5732v1", null], ["v2", "Tue, 8 Sep 2015 03:00:55 GMT  (1878kb,D)", "http://arxiv.org/abs/1412.5732v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "fan wei", "weishan dong", "qingshan liu", "xiangfeng wang", "xin zhang"], "accepted": false, "id": "1412.5732"}, "pdf": {"name": "1412.5732.pdf", "metadata": {"source": "CRF", "title": "MORES: Online Incremental Multiple-Output Regression for Data Streams", "authors": ["Changsheng Li", "Weishan Dong", "Qingshan Liu", "Xin Zhang"], "emails": ["zxin}@cn.ibm.com", "qsliu@nuist.edu.cn"], "sections": [{"heading": null, "text": "Index Terms\u2014Online multiple-output regression, dynamic relationship learning, forgetting factor, lossless compression\nI. INTRODUCTION\nData streams arise in many scenarios, such as online transactions in the financial market, Internet traffic and so on [1]. Unlike traditional datasets in batch mode, a data stream should be viewed as a potentially infinite process collecting data with varying update rates, as well as continuously evolving over time. In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc., have been extensively studied over the last decade, little attention is paid to multiple-output regression. However, multiple-output regression also has a great variety of potential applications on data streams, including weather forecast, air quality prediction, etc.\nIn batch data processing, the purpose of multiple-output regression is to learn a mapping \u03a6 from an input space Rd to an output space Rm on the whole training dataset. Based on the learned \u03a6, we can simultaneously predict multiple output variables y \u2208 Rm to a given new input vector x \u2208 Rd by: y = \u03a6(x). According to the property of\nC. Li, W. Dong, and X. Zhang are with IBM Research-China, Beijing 100093. E-mail: {lcsheng, dongweis, zxin}@cn.ibm.com\nQ. Liu is with the B-DAT Laboratory at the school of Information and Control, Nanjing University of Information Science and Technology, Nanjing 210014, China. E-mail: qsliu@nuist.edu.cn\nthe learned mapping \u03a6, multiple-output regression techniques can be divided into linear and nonlinear ones. Since linear methods usually have low complexities and reliable prediction performance, they have attracted more attention in the past. Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24]. However, in streaming environments, the data is not stored in a batch mode, but it arrives sequentially and continuously. If using these batch methods to re-train the models for streaming data, the computational complexity and memory complexity will increase sharply. Moreover, when the size of the arrived data becomes large, it is also impractical to load all the historical data into memory for model training. Therefore, it is necessary to develop an online regression algorithm for simultaneously predicting multiple outputs.\nSo far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27]. The representative method is online passive-aggressive (PA) algorithm [26]. PA is a margin based online learning algorithm, and it has an analytical solution to update model parameters as the new data sample(s) arrives. Since there are often correlations among outputs, mining the correlation relationships can improve the prediction accuracy of the model [19]. However, PA only treats each of multiple outputs as an independent task, and thus can not capture any correlations among outputs. Recently, McWilliams and Montana take advantage of partial least squares (PLS) to build a recursive regression model for online predicting multiple outputs, called iS-PLS [28]. iSPLS aims at finding a low-dimensional subspace to make the correlation between inputs and outputs maximized. iS-PLS focuses on the correlation between inputs and outputs, while it does not consider the correlations among outputs.\nIn this paper, we propose a novel Multiple-Output REgression method for Streaming data, named as MORES. MORES works in an incremental fashion. It aims at dynamically learning the structures of both the regression coefficients and the residual errors to continuously update the model. Specifically, when a new training sample arrives, we transform the update of the regression coefficients into an optimization problem. In the objective function, we highlight the following three aspects:\nFirst, we take advantage of the matrix Mahalanobis norm to measure the divergence between updated regression coefficient matrix and current regression coefficient matrix, which can learn the structure of the regression coefficients to facilitate model\u2019s update.\nSecond, due to limited expressive power of the regression coefficient matrix, especially in the early stage of online\nar X\niv :1\n41 2.\n57 32\nv1 [\ncs .L\nG ]\n1 8\nD ec\n2 01\n4\n2 update, there are often correlations between the residual errors. MORES thus utilizes the Mahalanobis distance instead of the Euclidean distance to measure the prediction error between the true values and the predicted values, which can learn the structure of the residual errors to improve the prediction accuracy.\nThird, we define three statistical variables to store necessary information of both the historical data and the current data for exactly measuring the prediction error, such that MORES can avoid loading all the data into memory and visiting data multiple times. This also effectively prevents the updated regression coefficient matrix gradually deviating from the true coefficient matrix due to noise and outliers in the data streams. Meanwhile, we introduce a forgetting factor to set different weights on samples for adapting to data streams\u2019 evolvement.\nExtensive experiments are conducted on three real-world datasets, and the experimental results demonstrate the effectiveness and efficiency of MORES."}, {"heading": "II. ONLINE MULTIPLE-OUTPUT REGRESSION FOR STREAMING DATA", "text": "Following the general setting of online learning [26], we assume that the learner first observes an instance xt \u2208 Rd on the t-th round, and it simultaneously predicts multiple outputs y\u0302t \u2208 Rm based on the current model Pt\u22121 \u2208 Rm\u00d7d. After that, the learner receives the true responses yt \u2208 Rm for this instance. Finally, the learner updates the current model Pt\u22121 based on the new data point (xt,yt). In this paper, our goal is to online update Pt\u22121, such that the updated Pt can predict the outputs for the incoming instance xt+1 as accurately as possible. The prediction can be expressed in the following form:\nyt+1 = Ptxt+1 + t+1, (1)\nwhere Pt = [pt,1, . . . ,pt,m]T denotes the learned regression coefficient matrix on the t-th round, and pt,i is the regression coefficient vector of the i-th output. t+1 = [ t+1,1, . . . , t+1,m]\nT is a vector consisting of m residual errors."}, {"heading": "A. Objective Function", "text": "In order to obtain Pt on the t-th round, we first propose a simple formulation as:\nPt = arg min P\u2208Rm\u00d7d \u2016P\u2212Pt\u22121\u20162F s.t. \u2016yt \u2212Pxt\u201622 \u2264 \u03be,\n(2)\nwhere \u03be is a positive parameter that controls the sensitivity to the prediction error. \u2016 \u00b7\u2016F denotes the matrix Frobenius norm, and \u2016 \u00b7 \u20162 denotes the l2 norm of a vector.\nThe core idea of objective function (2) is as follows: On one hand, it intends to minimize the distance between Pt and Pt\u22121 to make Pt close to Pt\u22121 as much as possible, which can retain the information learned on previous rounds. On the other hand, it requires Pt to meet the condition: The total prediction error on the current data point (xt,yt) is less than or equal to \u03be.\nFollowing [26], the optimization problem defined by (2) can be easily solved by the Lagrange multiplier method.\nAlthough (2) has some merits for online regression prediction of streaming data, it still has the following limitations:\n(i) Because of \u2016P \u2212 Pt\u22121\u20162F = tr((P \u2212 Pt\u22121)(P \u2212 Pt\u22121) T ) = \u2211m i=1((pi \u2212 pt\u22121,i)T (pi \u2212 pt\u22121,i)), we can see that the objective function in (2) treats the update of regression coefficients as m independent tasks. However, in streaming environments, outputs are often dependent, i.e., there are some positive or negative correlations among outputs, so updating regression coefficient vectors for all the outputs should not be regarded as completely independent tasks.\n(ii) In order to acquire the updater Pt, (2) imposes a constraint on P, i.e., \u2016yt \u2212 Pxt\u201622 \u2264 \u03be. This constraint just refers to the current data point (xt,yt) but ignores the historical data points St\u22121 = {(xi,yi)}t\u22121i=1 . This may lead to the updated coefficient matrix gradually deviating from the true coefficient matrix because of noise and outliers in many practical streaming data applications.\n(iii) The constraint of (2) takes advantage of the l2 norm to measure the total prediction error. As we know, the l2 norm of a vector assumes that all the variables in the vector are independent. However, due to limited expressive power of Pt, especially when the round t is small, there are often correlations between the residual errors. Therefore, the l2 norm is often the suboptimal choice for measuring the total prediction error.\nIn light of above three limitations, we propose to minimize the following objective function, in order to update the model on round t:\nmin J(P,\u2126,\u0393) =\u2016P\u2212Pt\u22121\u20162\u2126 + \u03b1`(P,\u0393;St) + \u03b2\u2206\u03c6(\u2126,\u2126t\u22121)\u2212 \u03b7 log |\u0393| (3)\ns.t. \u2126 0, \u0393 0,\nwhere \u03b1, \u03b2, and \u03b7 are three trade-off parameters. \u2016 \u00b7 \u2016\u2126 denotes the matrix Mahalanobis norm. `(P,\u0393;St) is the total prediction error on the data points St = {(xi,yi)}ti=1. \u2206\u03c6(\u2126,\u2126t\u22121) denotes the Bregman divergence [29] that measures the distance between the matrix \u2126 and the matrix \u2126t\u22121. \u2126 0 and \u0393 0 means that they are positive semi-definite.\nIn the objective function (3), the first term aims to learn the structure \u2126 of the regression coefficient matrix, and leverage \u2126 to measure the divergence between the updated matrix P and the current matrix Pt\u22121 on round t. The second term intends to mine the underlying structure \u0393 of the residual errors, and take advantage of \u0393 to measure the total prediction error on the current data and the historical data. Here we define three statistical variables to store necessary information of data for lowering memory complexity, and introduce a forgetting factor to adapt to the evolving data streams (See (5), (6) for details). The third term is a regularization term, which aims at keeping \u2126 updated in a conservative strategy to reduce the influence of noise. The last term is used to control the complexity of \u0393. Next, we will respectively explain the first three terms in detail. The first term: In order to capture the structure of the regression coefficient matrix, we introduce Mahalanobis norm\n3 of the matrix (P\u2212Pt\u22121) to measure the divergence between P and Pt\u22121. The Mahalanobis norm is expressed as:\n\u2016P\u2212Pt\u22121\u2016\u2126 = \u221a tr((P\u2212Pt\u22121)T\u2126(P\u2212Pt\u22121))\n= \u221a\u2211d i=1 (P(i)\u2212Pt\u22121(i))T\u2126(P(i)\u2212Pt\u22121(i)), (4)\nwhere P(i) denotes the i-th column of P. When \u2126 is set to the identity matrix, the Mahalanobis norm of the matrix is reduced to the Frobenius norm. In (4), the term (P(i) \u2212 Pt\u22121(i))T\u2126(P(i) \u2212 Pt\u22121(i)) is actually the Mahalanobis distance between P(i) and Pt\u22121(i), where \u2126 encodes the correlations between the variables of the i-th column of the regression coefficient matrix on round t [30]. Therefore, \u2016P\u2212Pt\u22121\u20162\u2126 can be viewed as a summation of d Mahalanobis distances, of which each measures the distance between the corresponding column vectors of P and Pt\u22121. The second term: The loss function `(P,\u0393;S) measures the prediction error on S , which is defined as:\n`(P,\u0393;S) = \u2211t\ni=1 \u00b5t\u2212i(yi \u2212Pxi)T\u0393(yi \u2212Pxi), (5)\nwhere 0 \u2264 \u00b5 \u2264 1 is a forgetting factor1. When \u00b5 = 0, the prediction loss is only measured on the current sample without any historical samples. When \u00b5 = 1, all the samples have equal weights to contribute to the prediction loss. When 0 < \u00b5 < 1, all the samples have different contributions to the prediction loss. As a matter of fact, the function of \u00b5 is similar to a new form of time window on samples. The farther the historical sample is from the current sample in the time domain, the lower its importance is, which will fit in the evolving characteristic of data streams well. The matrix \u0393 embeds the correlation relationships among the residual errors on round t. The term (yi \u2212 Pxi)T\u0393(yi \u2212 Pxi) measures the Mahalnobis distance between the true value yi and the predicted value Pxi, which can remove the influence of the residual errors\u2019 correlations on distance calculation [31].\nIn streaming environments, it is impractical to load all the historical data into memory or scan a sample multiple times. An effective way to handle this issue is to define some statistical variables to store necessary information of the samples. In this paper, we introduce three statistical variables to realize lossless compression of the data. To do this, we will make use of the following property and lemma.\nProperty 1: Given a set of arbitrary sequence vectors, x1,. . .,xt, and a constant \u00b5, if Ct = \u2211t i=1\u00b5 t\u2212ixix T i , then Ct=\u00b5Ct\u22121+xtx T t , where t is the timestamp.\nLemma 1: The loss function (5) can be expressed as\n` = tr(\u0393Ct,YY) + tr(P T\u0393PCt,XX)\u2212 2tr(\u0393PCt,XY).\n(6)\nwhere Ct,YY, Ct,XY, and Ct,XX are three variables, which are respectively defined as:\nCt,YY = \u00b5Ct\u22121,YY + yty T t . (7) Ct,XY = \u00b5Ct\u22121,XY + xty T t . (8) Ct,XX = \u00b5Ct\u22121,XX + xtx T t . (9)\n1When \u00b5 = 0, and t\u2212 i = 0, we define \u00b5t\u2212i = 1 to ensure consistency.\nProof: Based on (5), we have `(P,\u0393;S) = \u2211t\ni=1 \u00b5t\u2212i(yi \u2212Pxi)T\u0393(yi \u2212Pxi) = \u2211t\ni=1 \u00b5t\u2212i(tr(yTi \u0393yi) + tr(x T i P T\u0393Pxi)) \u2212 2 \u2211t\ni=1 \u00b5t\u2212itr(yTi \u0393Pxi) = \u2211t\ni=1 \u00b5t\u2212i(tr(\u0393yiy T i ) + tr(P T\u0393Pxix T i )) \u2212 2 \u2211t\ni=1 \u00b5t\u2212itr(\u0393Pxiy T i ) =tr(\u0393 \u2211t\ni=1 \u00b5t\u2212iyiy T i ) + tr(P\nT\u0393P \u2211t\ni=1 \u00b5t\u2212ixix T i ) \u2212 2tr(\u0393P \u2211t\ni=1 \u00b5t\u2212ixiy T i ). (10)\nDefining the matrix variables Ct,YY,Ct,XY, and Ct,XX as:\nCt,YY = \u2211t\ni=1 \u00b5t\u2212iyiy T i (11) Ct,XY = \u2211t\ni=1 \u00b5t\u2212ixiy T i (12) Ct,XX = \u2211t\ni=1 \u00b5t\u2212ixix T i (13)\nSubstituting Ct,YY,Ct,XY, and Ct,XX into (10), the loss function (5) becomes (6). Based on the Property 1, (11), (12), and (13) can be expressed by (7), (8), and (9), respectively.\nWhen a new data point (xt+1,yt+1) arrives on round t+1, we first update the statistical variables Ct+1,YY, Ct+1,XY, Ct+1,XX based on (7), (8), and (9) respectively, whose memory complexity is a constant: O(m2 + md + d2). After that, the prediction loss ` can be measured by (6), so it is no longer necessary to load all the training samples into memory or visit a training sample multiple times.\nIn summary, the loss function (5) has the following merits: First, it can dynamically learn the structure of the residual errors as the samples continuously arrive, and leverage the structure information to effectively measure the true distance between the predicted value and the ground truth. Second, the loss is measured based on all the seen samples not just the current sample, which can cut down on the influence of noise on model\u2019s update. Third, on each round, instead of loading all the samples into memory, the loss can be measured just relying on three defined statistical variables without information loss, as expressed in (6). Furthermore, by introducing the factor \u00b5 to weight the samples, MORES can fit in streaming data\u2019s evolvement well. The third term: In order to restrain \u03c6 fluctuating drastically on each round, we hope that the divergence \u2206\u03c6(\u2126,\u2126t\u22121) is as small as possible. \u2206\u03c6(\u2126,\u2126t\u22121) is defined as:\n\u2206\u03c6(\u2126,\u2126t\u22121) = \u03c6(\u2126)\u2212 \u03c6(\u2126t\u22121)\u2212 tr(g(\u2126t\u22121)(\u2126\u2212 \u2126t\u22121)),\nwhere \u03c6 is a real-valued strictly convex differentiable function on the parameter domain Rm\u00d7m. g(\u2126t\u22121) = \u2207\u2126\u03c6(\u2126)|\u2126t\u22121 . In this paper, we employ two matrix divergence metrics, quantum relative entropy and LogDet divergence, because of their good properties stated in [32].\n(i) When \u03c6(\u2126) = tr(\u2126log\u2126 \u2212 \u2126), the quantum relative entropy is defined as:\n\u2206\u03c6(\u2126,\u2126t\u22121) = tr(\u2126log\u2126\u2212 \u2126log\u2126t\u22121 \u2212 \u2126 + \u2126t\u22121),\n4 where log\u2126 = V(log\u039b)VT , and (log\u039b)i,i = log(\u039bi,i). \u2126 is a strictly positive definite matrix, and \u2126 = V\u039bVT .\n(ii) When \u03c6(\u2126) = \u2212logdet(\u2126), LogDet is defined as:\n\u2206\u03c6(\u2126,\u2126t\u22121) = log det(\u2126t\u22121)\ndet(\u2126) + tr(\u2126\u22121t\u22121\u2126)\u2212m,\nwhere det(\u00b7) denotes the determinant of a matrix."}, {"heading": "B. Optimization Procedure", "text": "The objective function (3) is not convex with respect to all variables, but it is convex with each variable when others are fixed. We adopt an alternating optimization strategy to solve (3), which can find local minima.\nOptimizing P, given \u2126 and \u0393: When \u2126 and \u0393 are fixed, (3) is then unconstrained and convex. P can be obtained by minimizing the following objective function:\nmin J1(P) = \u2016P\u2212Pt\u22121\u20162\u2126 + \u03b1`(P,\u0393;St) (14)\nThe necessary condition for the optimality is:\n\u2202J1(P)\n\u2202P = 0\n\u21d2 \u2126P + \u03b1\u0393PCt,XX = \u2126Pt\u22121 + \u03b1\u0393CTt,XY \u21d2 vec(\u2126P + \u03b1\u0393PCt,XX) = vec(\u2126Pt\u22121 + \u03b1\u0393CTt,XY)\n(15)\nwhere vec(\u00b7) is an operator that reshapes a matrix of size m\u00d7n into a vector of size mn\u00d7 1.\nNoticing that vec(ABC) = (CT \u2297 A)vec(B) and vec(A + B)=vec(A)+vec(B), (15) can become:\n(I\u2297 \u2126 + \u03b1Ct,XX \u2297 \u0393)vec(P) = vec(\u2126Pt\u22121 + \u03b1\u0393CTt,XY) \u21d2 vec(P)=(I\u2297 \u2126+\u03b1Ct,XX \u2297 \u0393)\u2020vec(\u2126Pt\u22121+\u03b1\u0393CTt,XY)\nwhere \u2297 denotes Kronecker product. (I\u2297 \u2126 + \u03b1Ct,XX \u2297 \u0393)\u2020 is the Moore-Penrose pseudoinverse of the matrix (I\u2297 \u2126 + \u03b1Ct,XX \u2297 \u0393). We know that when a matrix is invertible, its pseudoinverse is equal to its inverse. After obtaining vec(P), P can be easily found by the operator unvec(\u00b7) that reshapes a vector into a matrix.\nOptimizing \u2126, given P and \u0393: When P and \u0393 are fixed, solving \u2126 becomes a convex optimization problem as:\nmin \u2126 0\nJ2(\u2126) = \u2016P\u2212Pt\u22121\u20162\u2126 + \u03b2\u2206\u03c6(\u2126,\u2126t\u22121). (16)\n(i) When \u2206\u03c6(\u00b7) is quantum relative entropy, (16) becomes\nmin \u2126 0\nJ2(\u2126) = tr((P\u2212Pt\u22121)T\u2126(P\u2212Pt\u22121))\n+\u03b2tr(\u2126log\u2126\u2212 \u2126log\u2126t\u22121 \u2212 \u2126 + \u2126t\u22121). (17)\nTaking the derivative of J2(\u2126) with respect to \u2126, and setting it to zero, we can obtain a closed form of \u2126:\n\u2126 = exp(log\u2126t\u22121 \u2212 1\n\u03b2 (P\u2212Pt\u22121)(P\u2212Pt\u22121)T ),\nwhere exp(\u00b7) is the matrix exponential. According to [32], it is easily inferred that \u2126 is positive definite.\nAlgorithm 1 Multiple-Output Regression for Streaming Data(MORES) Input: Data streams {(x1,y1), (x2,y2), \u00b7 \u00b7 \u00b7 } that arrive one sample each time; Parameters: \u03b1, \u03b2, \u03b7, and the forgetting factor \u00b5; Initialize P0 = 0d\u00d7m, C0,XX = 0d\u00d7d, C0,XY = 0d\u00d7m, C0,YY = 0m\u00d7m, and \u21260 = \u03930 = 1m \u00d7 Im\u00d7m; Method 1. for t = 1, 2, \u00b7 \u00b7 \u00b7 2. Ct,YY = \u00b5Ct\u22121,YY + ytyTt ; 3. Ct,XY = \u00b5Ct\u22121,XY + xtyTt ; 4. Ct,XX = \u00b5Ct\u22121,XX + xtxTt ; 5. if t < Tmin 6. Find the optimal P\u2217 by solving (14); 7. Pt \u2190 P\u2217; \u2126t \u2190 \u21260; \u0393t \u2190 \u03930; 8. else 9. Find the local optimal P\u2217, \u2126\u2217, and \u0393\u2217 by solving (14), (17) or (18), and (19), respectively; 10. Pt \u2190 P\u2217; \u2126t \u2190 \u2126\u2217; \u0393t \u2190 \u0393\u2217; 11. end 12. end end Method Output: Regression coefficient matrix Pt \u2208 Rm\u00d7d.\n(ii) When using LogDet to represent \u2206\u03c6(\u00b7), (16) becomes\nmin \u2126 0\nJ3(\u2126) = tr((P\u2212Pt\u22121)T\u2126(P\u2212Pt\u22121))\n+\u03b2(log det(\u2126t\u22121)\ndet(\u2126) + tr(\u2126\u22121t\u22121\u2126)\u2212m). (18)\nSimilarly, solving \u2202J3(\u2126)\u2202(\u2126) = 0, we have\n\u2126 = (\u2126\u22121t\u22121 + 1 \u03b2 (P\u2212Pt\u22121)(P\u2212Pt\u22121)T )\u22121.\nFor simplicity, we use the matrix M to represent (P \u2212 Pt\u22121)(P \u2212 Pt\u22121)T . To guarantee \u2126 to be positive semidefinite, the matrix M should be positive semi-definite. We adopt the following strategy to make M positive semi-definite: first, we calculate its spectral decomposition: M = U\u039bUT . Then M is updated by thresholding the corresponding eigenvalues as: M = Umax(\u039b, 0)UT .\nOptimizing \u0393, given P and \u2126: When P and \u2126 are fixed, \u0393 can be obtained by solving the following convex optimization problem:\nmin \u0393 0\nJ4(\u0393) = \u03b1`(\u0393;P,S)\u2212 \u03b7 log |\u0393|. (19)\nThe necessary condition for the optimality is \u2202J4(\u0393)\u2202\u0393 = 0. Therefore, we obtain the following closed form solution:\n\u0393= \u03b7\n\u03b1 (Ct,YY\u2212CTt,XYPT \u2212PCt,XY + PCt,XXPT )\u22121.\nAfter obtaining the solution \u0393, we employ the same strategy to make \u0393 positive semi-definite as in (18).\nFinally, we summarize the procedure of MORES in Algorithm 1. During the preliminary stage of online update, the regression coefficient matrix P are not well formed, and poor initial estimations of P may result in poor estimations of \u2126 and \u0393. To avoid this case, we do not update \u2126 or \u0393 until the round t is greater than or equal to a given threshold Tmin.\n5\nTABLE I MAES OF DIFFERENT METHODS ON THE BARRETT WAM DATASET. THE LAST COLUMN IS THE AVERAGE MAE. BEST RESULTS ARE HIGHLIGHTED IN BOLD FONTS.\nMethod 1st DOF 2nd DOF 3rd DOF 4th DOF 5th DOF 6th DOF 7th DOF Average PA-I 1.196 0.753 0.350 0.382 0.101 0.089 0.045 0.417 PA-II 1.179 0.772 0.332 0.391 0.095 0.084 0.043 0.414 iS-PLS 1.026 6.510 0.398 2.162 0.087 0.089 0.036 1.473 SOMOR 1.176 0.756 0.339 0.384 0.098 0.086 0.044 0.412\nMORES-LD 0.547 0.375 0.176 0.184 0.047 0.047 0.025 0.200 MORES-QE 0.526 0.391 0.182 0.180 0.044 0.049 0.026 0.200"}, {"heading": "C. Time Complexity Analysis", "text": "In Algorithm 1, the most time-consuming part of MORES is to update Pt, \u2126t, and \u0393t, and the time cost of other parts can be ignored. Here we focus on analyzing the complexity of the case where t is greater than or equal to Tmin. For updating Pt, the complexity is O(m2d2 +d3m3) = O(d3m3). Updating \u0393 needs O(m3 + d2m). In order to update \u2126t, we utilize two kinds of divergence metric(quantum relative entropy and LogDet divergence) in this paper. When using quantum relative entropy as the divergence metric, updating \u2126t involves the spectral decomposition, whose complexity is the same as the eigen-decomposition, typically, O(m3) in practice. Therefore, updating \u2126t needs O(m3 + dm2). When using LogDet, it also costs O(m3 + dm2) to update \u2126t. Therefore, the total time complexity of MORES is of order O(d3m3 + m3 + d2m + m3 + dm2) = O(d3m3), which is dominated by the update of Pt."}, {"heading": "III. EXPERIMENTS", "text": "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34]."}, {"heading": "A. Experimental Setups", "text": "We compare our method with iS-PLS [19] that is the most relevant work to ours. We also compare with two variants of PA algorithm in [26] called PA-I and PA-II, which are two classical online learning approaches for single regression tasks. In the experiment, we use PA to train a regression model for each output. We name our simple formulation of online multiple-output regression proposed at the beginning of Sect. 2 as SOMOR for short. In addition, our proposed MORES has two variants: one using quantum relative entropy to measure the divergence of matrices, which is named as MORES-QE. The other using LogDet is called MORES-LD.\nThere are some parameters to be set in advance. In order to lower the cost of parameter tuning, we tune the parameters \u03b1, \u03b2, and \u03b7 in our algorithms by a grid-search strategy from {10\u22123, 10\u22122, . . . , 102, 103} on the Concrete Slump dataset [35], and choose the optimal parameters to directly apply to the above three datasets. The threshold Tmin is set to 50, and the forgetting factor \u00b5 is set to 0.8 throughout the experiments, unless otherwise stated. To fairly conduct experimental comparisons, the parameters in the other methods are also searched from {10\u22123, 10\u22122, . . . , 102, 103}.\nIn the experimental study, we focus on the accuracy of online regression prediction to evaluate our models\u2019 quality. The popular metric, Mean Absolute Error (MAE), is used to measure the prediction quality. MAE is defined as: MAE = 1t \u2211t i=1 |yi \u2212 y\u0302i|, where y\u0302i denotes the estimated values of the i-th instance, and yi is the true response values."}, {"heading": "B. Robot Inverse Dynamics", "text": "We first study the problem of online learning the inverse dynamics of a 7 degrees of freedom of robotic arms on the Barrett WAM dataset. This dataset consists of a total of 16,200 samples, where each sample is represented by 21 input features, corresponding to seven joint positions, seven joint velocities and seven joint accelerations. Seven joint torques for the seven degrees of freedom (DOF) are used as the outputs.\nWe summarize the results of different methods in Table I. For each output, MORES-QE and MORES-LD attain better prediction performances than all the other methods. Meanwhile, they both achieve 51.5% relative error deduction in terms of the average MAE over SOMOR. The performance of MORES-QE is comparable to that of MORES-LD. It indicates that the quantum relative entropy divergence metric has similar effect on the prediction performance with the LogDet divergence metric. In addition, iS-PLS has a poor performance on the second and the fourth outputs. The reason may be that iS-PLS tries to find a low-dimensional subspace, where the covariance between the inputs and the outputs is maximized, while the found subspace does not preserve the structures of both the second and the fourth outputs well.\nIn our method, there are three main components: dynamically learning the structures of both the regression coefficient matrix and the residual error vector, and introducing a forgetting factor to measure the prediction error in an incremental fashion. We verify the effectiveness of the three components\nin terms of the average MAE of all the outputs on this dataset. The experimental setting is as follows: When both \u2126 and \u0393 are set to the identity matrix I in (3), the model is updated Without Relationship Learning on each round. We name it WRL for short. When \u2126 is set to I and \u0393 is updated on each round, it indicates that we only Dynamically learn the Relationships of the Residual errors in the process of model\u2019s update. We call it DRR. When \u2126 is updated on each round and \u0393 is set to I, we only Dynamically learn the Relationships of the regression Coefficient matrix. Because we utilize quantum relative entropy and LogDet in updating \u2126, we call them DRC-QE and DRC-LD, respectively. The results are shown in Fig. 1. Taking Fig. 1(a) as an example, both DRR and DRC-LD are better than WRL. It shows that dynamically learning the structures of the regression coefficient matrix and the residual errors are both beneficial to online regression. MORES-LD achieves the best performance. This indicates that the combination of the two components is effective for online multiple-output regression.\nIn order to verify the effectiveness of the forgetting factor \u00b5, we conduct the experiments with different values of \u00b5. Table II lists the results. Based on previous analysis in Sect. 2, we know no historical data is utilized to update the model on each\nround if \u00b5 = 0. When \u00b5 = 1, all the samples have the same weight for calculating the prediction loss. As can be seen in Table II, when 0.6 \u2264 \u00b5 \u2264 0.9 , the performance is improved compared to that of \u00b5 = 0. This shows that taking advantage of the historical samples is good for online regression. Moreover, we observe that when \u00b5 = 0.7 and \u00b5 = 0.9, MORES-LD and MORES-QE respectively achieve the best performances. It implies that the data in this dataset is indeed evolving. By introducing \u00b5 to set higher weights on the newer training samples, the model can adapt to the data stream\u2019s evolvement, and the prediction accuracy can be improved."}, {"heading": "C. Stock Price Prediction", "text": "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction. We choose the daily stock price data of five companies including IBM, Yahoo, Microsoft, Apple, and Oracle in the period from 2010 to 2013. The learned model can predict the stock prices in the future by using the stock prices in the past as inputs. We use the autoregressive 1, aka AR(1) model yt+1 = Ptyt+ t, where yt+1 represents the real stock prices of the five companies at time t+1, and Pt denotes the learned regression coefficient matrix at time t.\nThe experimental results are reported in Table III. MORESLD and MORES-QE achieve better performances compared to the other methods. Taking MORES-LD and PA-II as an example, MORES-LD gains 33.6%, 13.1%, 22.3%, 16.6%, and 17.0% relative accuracy improvement over PA-II for IBM, Yahoo, Microsoft, Apple, and Oracle, respectively. Meanwhile, MORES-LD obtains 20.6% relative improvement in terms\n7\nof the average MAE over PA-II. These results show that dynamically learning the structures of both the regression coefficient matrix and the residual error vector, as well as utilizing the historical data in an appropriate way, is good for online multiple-output regression.\nWe also investigate the performances of different methods as a function of sequence length (t). At the end of each online round, we calculate the MAE for each output attained so far. Fig. 2 shows the results. The performances of MORES-LD and MORES-QE are superior to those of the other methods, especially when the sequence length t is larger. In addition, the MAE curves of Fig. 2 (a), (b), and (d) rise after falling as t increases. This is because the stock price is severely evolving at the inflection point, such that the current model can not predict the future price well. Although the data is evolving, our algorithms are still better than the other methods under this circumstance. From Fig. 2 (a), we can see MORES-LD and MORES-QE can quickly adjust the model to fit in the data\u2019s evolvement."}, {"heading": "D. Weather Forecast", "text": "We also evaluate our algorithms on the weather dataset for weather forecast [34]. This dataset consists of wind speed, wind direction, barometric pressure, water depth, maximum gust, maximum wave height, air temperature, water temperature and average wave height, which is collected every five minutes by a sensor network located on the south coast of England. One and a half years\u2019 data containing 143,034 samples are used in the experiments. The first five variables are used as the predictors, and the rest are the response variables.\nThe experimental results are reported in Table IV. MORESLD and MORES-QE obtain better prediction performances than the other methods for all the response variables. Meanwhile, the results of our algorithms are superior to those of the other methods in terms of the average MAE.\nWe further test all the methods with different model update frequencies. The experimental setting is as follows: The model is updated when accumulatively receiving N (=1, . . . , 10) training data points, while the test is still performed on all the data points. As reported in Table V, the prediction accuracies of all the methods are gradually reduced as N increases. Moreover, for various values of N , MORES-LD and MORESQE perform better than the other methods because of dynamic learning of the output structures and the utilization of the historical data. In addition, we can see that the performance of MORES-LD with N = 10 is comparable to that of PA-II with N = 1."}, {"heading": "E. Sensitivity Analysis", "text": "We also study the sensitivity of parameters \u03b1, \u03b2, and \u03b7 in our algorithm on the larger dataset, the weather dataset. As shown in Fig. 3, with the fixed \u03b7, our method is not sensitive to \u03b1 and \u03b2 with wide ranges. As for parameter \u03b7, when \u03b1 and \u03b2 are fixed, the performance is gradually improved as \u03b7 increases. When \u03b7 > 0.1, the performance is gradually degraded with \u03b7 increasing. When \u03b7 is set to 0.1, the performance is the best."}, {"heading": "F. Efficiency", "text": "We test the update speeds of our algorithms on the above three datasets. The experiments are conducted on a desktop with Inter(R) Core(TM) i7-3770 CPU, and MORES are implemented using MATLAB R2012b 64bit edition without parallel operation. The update speeds of our algorithms are reported in Table VI. On the weather dataset, MORES-QE achieves 3566 updates per second. And on the Inverse Dynamics dataset having the largest dimensions, our algorithms perform more than 1000 updates per second. If we apply some parallel implementations or use more efficient programming language, the update speed of MORES can be further improved."}, {"heading": "IV. RELATED WORK", "text": "In this section, we review the related works from two aspects: online single-output regression and batch multipleoutput regression.\n8 0.11 10 100 1000 0.001 0.01 0.1 1 10 0 0.1 0.2 0.3 0.4 M A E\n(a) MORES-LD (\u03b7 = 0.1)\n0.11 10 100\n1000 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(b) MORES-QE (\u03b7 = 0.1)\n0.010.1 1 10\n100 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(c) MORES-LD (\u03b2 = 10)\n0.010.1 1 10\n100 0.001 0.01 0.1\n1 10\n0 0.1 0.2 0.3 0.4 M A E\n(d) MORES-QE (\u03b2 = 10)\n0.010.1 1 10\n100 0.1\n1 10 100 1000\n0 0.1 0.2 0.3 0.4 M A E\n(e) MORES-LD (\u03b1 = 1)\n0.010.1 1 10\n100 0.1\n1 10 100 1000\n0 0.1 0.2 0.3 0.4 M A E\n(f) MORES-QE (\u03b1 = 1)\nFig. 3. MORES-LD and MORES-QE with different \u03b1, \u03b2, and \u03b7 on the weather dataset.\nOnline single-output regression: [25] presented an online version of support vector regression algorithm, called (AOSVR). AOSVR classified all training samples into three distinct auxiliary sets according to the KKT conditions that define the optimal solution. After that, the rules for recursively updating the optimal solution were devised based on this classification. [26] proposed a margin based online regression algorithm, called passive-aggressive (PA). PA incrementally updated the model by formalizing the trade-off between the amount of progress made on each round and the amount of information retained from previous rounds. [27] proposed an incremental support vector regression algorithm, which evolved a pool of online SVR experts and learned to trade by dynamically weighting the experts\u2019 opinions.\nBatch multiple-output regression: Many batch multipleout-put regression algorithms have been proposed, which tried to mine the structure among outputs. Rothman et al. [19] presented MRCE, which jointly learned the output structure in the form of the noise covariance matrix and the regression coefficients for predicting each output. Sohn and Kim [20] designed an algorithm to simultaneously estimate the regression coefficient vector for each output along with the covariance structure of the outputs with a shared sparsity assumption on\nthe regression coefficient vectors. Rai et al. [21] proposed an approach that leveraged the covariance structure of the regression coefficient matrix and the conditional covariance structure of the outputs for learning the model parameters. [22] proposed a tree-guided group lasso, or tree lasso, that directly combined statistical strength across multiple related outputs. They estimated the structured sparsity under multioutput regression by employing a novel penalty function constructed from the tree. Since these methods are trained in the batch mode, they are not suitable for online multiple-output prediction."}, {"heading": "V. CONCLUSIONS", "text": "In this paper, we proposed a novel online multiple-output regression method for streaming data. The proposed method can simultaneously and dynamically learn the structures of both the regression coefficients and the residual errors, and leverage the learned structure information to continuously update the model. Meanwhile, we accumulated the prediction error on all the seen samples in an incremental way without information loss, and introduced a forgetting factor to weight the samples so as to fit in data streams\u2019 evolvement. The experiments were conducted on three real-world datasets, and the experimental results demonstrated the effectiveness and efficiency of the proposed method."}], "references": [{"title": "Data Mining: Concepts and Techniques", "author": ["J. Han", "M. Kamber"], "venue": "Second Edition: Diane Cerra,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2000, pp. 71\u201380.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A framework for on-demand classification of evolving data streams", "author": ["C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 18, no. 5, pp. 577\u2013589, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Learn ++.NC: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes", "author": ["M.D. Muhlbaier", "A. Topalis", "R. Polikar"], "venue": "IEEE Trans. on Neural Networks, vol. 20, no. 1, pp. 152\u2013168, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for clustering evolving data streams", "author": ["C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu"], "venue": "International Conference on Very Large Data Bases (VLDB), 2003, pp. 81\u201392.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Density-based clustering for real-time stream data", "author": ["Y. Chen", "L. Tu"], "venue": "ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 2007, pp. 133\u2013142.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Svstream: A support vector-based algorithm for clustering data streams", "author": ["C.-D. Wang", "J.-H. Lai", "D. Huang", "W.-S. Zheng"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1410\u20131424, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning from data streams", "author": ["X. Zhu", "P. Zhang", "X. Lin", "Y. Shi"], "venue": "IEEE International Conference on Data Mining (ICDM), 2007, pp. 757\u2013762.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Unbiased online active learning in data streams", "author": ["W. Chu", "M. Zinkevich", "L. Li", "A. Thomas", "B. Tseng"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2011, pp. 195\u2013203.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning with drifting streaming data.", "author": ["I. Zliobaite", "A. Bifet", "B. Pfahringer", "G. Holmes"], "venue": "IEEE Trans. on Neural Networks and Learning Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Online feature selection with streaming features", "author": ["X. Wu", "K. Yu", "W. Ding", "H. Wang", "X. Zhu"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 5, pp. 1178\u20131192, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Online feature selection and its applications", "author": ["J. Wang", "P. Zhao", "S. Hoi", "R. Jin"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 3, pp. 698\u2013710, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning of multiple tasks and their relationships", "author": ["A. Saha", "P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 643\u2013651.  9", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning of multiple tasks with a shared loss.", "author": ["O. Dekel", "P.M. Long", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A martingale framework for detecting changes in data streams by testing exchangeability", "author": ["S.-S. Ho", "H. Wechsler"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, no. 12, pp. 2113\u2013 2127, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Change-point detection with feature selection in high-dimensional time-series data", "author": ["M. Yamada", "A. Kimura", "F. Naya", "H. Sawada"], "venue": "International joint conference on Artificial Intelligence (AAAI). AAAI Press, 2013, pp. 1827\u20131833.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A multivariate regression approach to association analysis of a quantitative trait network", "author": ["S. Kim", "K.-A. Sohn", "E.P. Xing"], "venue": "Bioinformatics, vol. 25, no. 12, pp. i204\u2013i212, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Svm multiregression for nonlinear channel estimation in multiple-input multiple-output systems", "author": ["M. S\u00e1nchez-Fern\u00e1ndez", "M. de Prado-Cumplido", "J. Arenas-Garc\u0131\u0301a", "F. P\u00e9rez-Cruz"], "venue": "IEEE Trans. on Signal Processing, vol. 52, no. 8, pp. 2298\u20132307, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse multivariate regression with covariance estimation", "author": ["A.J. Rothman", "E. Levina", "J. Zhu"], "venue": "Journal of Computational and Graphical Statistics, vol. 19, no. 4, pp. 947\u2013962, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint estimation of structured sparsity and output structure in multiple-output regression via inverse-covariance regularization", "author": ["K.-A. Sohn", "S. Kim"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), 2012, pp. 1081\u20131089.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping", "author": ["S. Kim", "E.P. Xing"], "venue": "The Annals of Applied Statistics, vol. 6, no. 3, pp. 1095\u20131117, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized bayesian matrix factorization", "author": ["M. Gonen", "S. Kaski"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 2047\u20132060, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Multivariate regression with calibration", "author": ["H. Liu", "L. Wang", "T. Zhao"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 127\u2013135.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Accurate on-line support vector regression", "author": ["J. Ma", "J. Theiler", "S. Perkins"], "venue": "Neural Computation, vol. 15, no. 11, pp. 2683\u20132703, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 551\u2013585, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to trade with incremental support vector regression experts", "author": ["G. Montana", "F. Parrella"], "venue": "Hybrid Artificial Intelligence Systems. Springer, 2008, pp. 591\u2013598.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse partial least squares regression for on-line variable selection with multivariate data streams", "author": ["B. McWilliams", "G. Montana"], "venue": "Statistical Analysis and Data Mining, vol. 3, no. 3, pp. 170\u2013193, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation, vol. 132, no. 1, pp. 1\u201363, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "The Conference on Uncertainty in Artificial Intelligence (UAI), 2010, pp. 733\u2013742.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2005, pp. 1473\u20131480.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Matrix exponentiated gradient updates for on-line learning and bregman projection.", "author": ["K. Tsuda", "G. R\u00e4tsch", "M.K. Warmuth", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Model learning with local gaussian process regression", "author": ["D. Nguyen-Tuong", "M. Seeger", "J. Peters"], "venue": "no. 15, pp. 2015\u20132034, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse convolved gaussian processes for multi-output regression.", "author": ["M.A. Alvarez", "N.D. Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Modeling slump flow of concrete using second-order regressions and artificial neural networks", "author": ["I. Yeh"], "venue": "Cement and Concrete Composites, vol. 29, no. 6, pp. 474\u2013480, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust sparse estimation of multiresponse regression and inverse covariance matrix via the l2 distance", "author": ["A.C. Lozano", "H. Jiang", "X. Deng"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2013, pp. 293\u2013301.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Data streams arise in many scenarios, such as online transactions in the financial market, Internet traffic and so on [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 190, "endOffset": 194}, {"referenceID": 12, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 251, "endOffset": 255}, {"referenceID": 15, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 257, "endOffset": 261}, {"referenceID": 16, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "The representative method is online passive-aggressive (PA) algorithm [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Since there are often correlations among outputs, mining the correlation relationships can improve the prediction accuracy of the model [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "Recently, McWilliams and Montana take advantage of partial least squares (PLS) to build a recursive regression model for online predicting multiple outputs, called iS-PLS [28].", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "Following the general setting of online learning [26], we assume that the learner first observes an instance xt \u2208 R on the t-th round, and it simultaneously predicts multiple outputs \u0177t \u2208 R based on the current model Pt\u22121 \u2208 Rm\u00d7d.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "Following [26], the optimization problem defined by (2) can be easily solved by the Lagrange multiplier method.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "\u2206\u03c6(\u03a9,\u03a9t\u22121) denotes the Bregman divergence [29] that measures the distance between the matrix \u03a9 and the matrix \u03a9t\u22121.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "In (4), the term (P(i) \u2212 Pt\u22121(i))\u03a9(P(i) \u2212 Pt\u22121(i)) is actually the Mahalanobis distance between P(i) and Pt\u22121(i), where \u03a9 encodes the correlations between the variables of the i-th column of the regression coefficient matrix on round t [30].", "startOffset": 236, "endOffset": 240}, {"referenceID": 29, "context": "The term (yi \u2212 Pxi)\u0393(yi \u2212 Pxi) measures the Mahalnobis distance between the true value yi and the predicted value Pxi, which can remove the influence of the residual errors\u2019 correlations on distance calculation [31].", "startOffset": 211, "endOffset": 215}, {"referenceID": 30, "context": "In this paper, we employ two matrix divergence metrics, quantum relative entropy and LogDet divergence, because of their good properties stated in [32].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "According to [32], it is easily inferred that \u03a9 is positive definite.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "We compare our method with iS-PLS [19] that is the most relevant work to ours.", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "We also compare with two variants of PA algorithm in [26] called PA-I and PA-II, which are two classical online learning approaches for single regression tasks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 33, "context": ", 10, 10} on the Concrete Slump dataset [35], and choose the optimal parameters to directly apply to the above three datasets.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "We also evaluate our algorithms on the weather dataset for weather forecast [34].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Online single-output regression: [25] presented an online version of support vector regression algorithm, called (AOSVR).", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "[26] proposed a margin based online regression algorithm, called passive-aggressive (PA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed an incremental support vector regression algorithm, which evolved a pool of online SVR experts and learned to trade by dynamically weighting the experts\u2019 opinions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] presented MRCE, which jointly learned the output structure in the form of the noise covariance matrix and the regression", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Sohn and Kim [20] designed an algorithm to simultaneously estimate the regression coefficient vector for each output along with the covariance structure of the outputs with a shared sparsity assumption on the regression coefficient vectors.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "[22] proposed a tree-guided group lasso, or tree lasso, that directly combined statistical strength across multiple related outputs.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for streaming data. MORES can dynamically learn the structure of the regression coefficients to facilitate the model\u2019s continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to dynamically learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to exactly represent all the seen samples for incrementally calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams\u2019 evolving characteristics quickly from the latest samples. Experiments on three real-world datasets validate the effectiveness and efficiency of the proposed method.", "creator": "LaTeX with hyperref package"}}}