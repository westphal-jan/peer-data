{"id": "1412.6093", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Learning Temporal Dependencies in Data Using a DBN-BLSTM", "abstract": "Since the advent of deep learning, it has been used to solve various problems using many different architectures. However, these architectures do not always sufficiently take into account the temporal dependencies of data. We therefore propose a new generic architecture called Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM), which models sequences by tracking the temporal information while allowing deep representations in the data. We demonstrate the generality of this new architecture by applying it to the generative task of music generation and achieving state-of-the-art results.", "histories": [["v1", "Thu, 18 Dec 2014 11:04:59 GMT  (251kb)", "https://arxiv.org/abs/1412.6093v1", "5 pages, 2 figures, 1 table, ICLR 2015 submission under review"], ["v2", "Tue, 23 Dec 2014 18:44:33 GMT  (400kb)", "http://arxiv.org/abs/1412.6093v2", "6 pages, 2 figures, 1 table, ICLR 2015 conference track submission under review"]], "COMMENTS": "5 pages, 2 figures, 1 table, ICLR 2015 submission under review", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kratarth goel", "raunaq vohra"], "accepted": false, "id": "1412.6093"}, "pdf": {"name": "1412.6093.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kratarthgoel@gmail.com", "ronvohra@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n60 93\nv2 [\ncs .L\nG ]\n2 3\nSince the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long ShortTerm Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results."}, {"heading": "1 INTRODUCTION", "text": "Deep architectures have been integral in creating a paradigm shift in the way we tackle most pattern recognition problems today. Deep belief networks (DBNs), for instance, are designed to maximize the variational lower bound of the log-likelihood, by hierarchical representation of data. They do not take into account the temporal information contained in speech signals, for example. The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)).\nGiven that music is an inherently dynamic, it seems natural to consider recurrent neural networks (RNNs) as a good baseline technique for music generation. However, these models do not perform as well as deep networks (Vinyals et al. (2012)). Another alternative can be to train RNNs \u2019end-toend\u2019 rather than combining them with Hidden Markov Models (HMMs). The results achieved in Graves et al. (2013) clearly support our claim about the importance of modeling temporal dependencies in sequential data.\nRNNs are inherently deep in time, and this property is used when we train them using Backpropogation Through Time (BPTT). We believe that an amalgamation of the ability of RNNs to learn temporal dependencies in the data with the depth provided by a DBN would lead to more expressive RNNs - or their more sophisticated counterparts, Long Short-Term Memory (LSTM) networks - which are capable of modeling long term temporal dependencies. Essentially, we need an architecture that models the temporal nature of auditory data but at the same time also takes advantage of the hierarchical representations that result from the use of deep belief networks. As such, we introduce a deep architecture - the DBN-BLSTM - which gives state of the art results on the generative task of polyphonic music generation."}, {"heading": "2 DBN", "text": "Restricted Boltzmann Machines (RBMs) are energy based models with their energy functionE(v, h) defined as:\nE(v, h) = \u2212b\u2032vv \u2212 b \u2032 hh\u2212 h \u2032Wv (1)\nwhere W represents the weights connecting the units of the visible (v) and hidden (h) layers and bv, bh are the biases of the visible and hidden layers respectively.\nSamples can be obtained from a RBM by performing block Gibbs sampling, where visible units are sampled simultaneously given fixed values of the hidden units. Similarly, hidden units are sampled simultaneously given the visible unit values. A single step in the Markov chain is thus taken as follows:\nh(n+1) = \u03c3(W \u2032v(n) + bh)\nv(n+1) = \u03c3(Wh(n+1) + bv), (2)\nwhere \u03c3 represents the sigmoid function acting on the activations of the (n+1)th hidden and visible units. Several algorithms have been devised for RBMs in order to efficiently sample from p(v, h) during the learning process, the most effective being the well-known contrastive divergence (CD-k) algorithm (Hinton (2002)).\nRBMs can be stacked and trained greedily to form Deep Belief Networks (DBNs). DBNs are graphical models which learn to extract a deep hierarchical representation of the training data (Hinton & Osindero (2006)). They model the joint distribution between the observed vector v and the \u2113 hidden layers hk as follows:\nP (v, h1, . . . , h\u2113) =\n(\n\u2113\u22122 \u220f\nk=0\nP (hk|hk+1)\n)\nP (h\u2113\u22121, h\u2113) (3)\nwhere v = h0, and P (hk\u22121|hk) is a conditional distribution for the visible units conditioned on the hidden units of the RBM at level k, and P (h\u2113\u22121, h\u2113) is the visible-hidden joint distribution in the top-level RBM."}, {"heading": "3 RNN AND BLSTM", "text": "A Recurrent Neural Network (RNN) is different from a standard network in that it takes a sequence v = (v1, v2, ..., vT ) as input, and iterates over it from t = 1 to T , to produce the following:\nqt = \u03a6(bqt + \u2212\u2212\u2192 Wvqvt + \u2212\u2212\u2192 Wqqqt\u22121) (4)\nwhere q = (q1, q2, ..., qT ) is a vector representing the hidden unit. The b terms are bias vectors (eg. bq represents bias of the hidden layer). The function \u03a6 is usually the application of elementwise sigmoid (\u03c3), in the case of a general RNN as can be seen from Eq. (5). However, when a LSTM is being used, we define \u03a6 as follows:\nut = \u03c3(bu +Wvuvt +Wuuut\u22121) (5)\nit = \u03c3(bi +Wcict\u22121 +Wqiqt\u22121 +Wuiut) (6)\nft = \u03c3(bf +Wcfct\u22121 +Wqf qt\u22121 +Wufut) (7)\nct = ftct\u22121 + it\u03c3(Wucut +Wqcqt\u22121 + bc) (8)\not = \u03c3(bo +Wcoct +Wqoqt\u2212 1 +Wuout) (9)\nqt = ottanh(ct) (10)\nwhere the W terms denote the weight martrices (eg. Wvq is the weight matrix between the visible and the hidden unit). Further, u represents the input of the LSTM memory cell and i, f , o and c are the input gate, forget gate, output gate and cell activation vectors respectively, all of which are of the same size as the hidden vector q. LSTM memory cells are good at finding and exploiting long range context. Figure 1 illustrates a single LSTM memory cell.\nThe basic idea of the Bidirectional LSTM (BLSTM) is to present each training sequence forwards and backwards to two separate LSTMs, both of which are connected to the same output layer. This means that for every point in a given sequence, the BLSTM has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size.\nThe forward hidden sequence is calculated as specified in Eq. (5). The backward hidden sequence \u2190\u2212qt of the bidirectional LSTM r is calculated by iterating over v from t = T to 1, as follows:\n\u2190\u2212qt = \u03a6(b\u2190\u2212qt + \u2190\u2212\u2212 Wvqvt + \u2190\u2212\u2212 Wqq \u2190\u2212\u2212qt\u22121) (11)"}, {"heading": "4 THE DBN-BLSTM", "text": ""}, {"heading": "4.1 THE ARCHITECTURE", "text": "The DBN-BLSTM is an extension of the generative model (RNN-DBN) proposed by Goel et al. (2014). There are a few significant improvements made to this model, most notably the replacement of the RNN with a LSTM, which is a more powerful neural architecture capable of modeling temporal dependencies across large time steps. This ensures the model retains information about\nthe sequence generated for a longer time duration. Since we are discussing generative models, this property lends itself exceptionally well to modeling creativity, for instance to music generation, where choosing an LSTM over a RNN would result in less repetition and more varied music due to better generalization, since the improved memory of the LSTM would possess more information about previously generated music in the sequence as compared to a RNN. The network is illustrated in Figure 2.\nThere are essentially two points for interaction between the DBN and the BLSTM. The input to the BLSTMs are calculated as shown in Eq. (4) and (11) (for the forward and the backward LSTMs respectively) where the v is the visible layer of the DBN. The second point of interaction between the two architectures is where the biases of the visible and hidden units of the DBN are calculated as follows:\nbvt = bv0 +Wuvut\u22121 +Wqvqt\u22121 (12)\nb h (n) t = bh +Wuh(n)ut\u22121 +Wqh(n)qt\u22121 (13)\nwhere b h (n) t represents the bias vector for the nth hidden layer, and bvt the bias of the visible layer at the tth time step of the recurrence for the DBN. Thus the temporal dependencies learned by the BLSTM strongly influences the hierarchical representations that are learned by the DBN to construct the joint probability distribution P (v, h1, . . . , h\u2113) given by Eq. (3)."}, {"heading": "4.2 TRAINING THE NETWORK", "text": "At each time step t, the input vt is provided to the DBN. The corresponding input ut to the BLSTM is provided using Eq. (5). The biases to the layers of the DBN are calculated as given by Eq. (12) and (13), using the \u03a6 function of the BSLTM given by Eq. (4) and (11). These biases are then used\nfor calculating the joint probability distribution between the observed and hidden layers of the DBN given in Eq. (3). For sampling from the DBN we use Eq. (2).\nIt must be noted here that the input vt to the DBN is a binary vector at each time step t, and the model for the music generation task is thus composed of only binary layers. However, this is not a limitation of our technique, and the model can easily be extended to handle real valued data as well. One such possible extension could have the visible layer of the DBN composed of Gaussian units and the subsequent layers could be ReLU units with dropout incorporated per layer."}, {"heading": "5 EXPERIMENT AND RESULTS", "text": "We demonstrate our technique by applying it to the task of polyphonic music generation. We used a DBN-BLSTM with 3 hidden DBN layers - each having 150 binary units - and 150 binary units in the BLSTM. The visible layer has 88 binary units, corresponding to the full range of the piano from A0 to C8. Dropout is incorporated in each layer. We used our technique on four datasets - JSB Chorales , MuseData1, Nottingham2 and Piano-midi.de. Only raw MIDI data has been given as input to the DBN-BLSTM. We evaluate our models qualitatively by generating sample sequences and quantitatively by using the log-likelihood (LL) as a performance measure. Results (some of which are reproduced from Boulanger-Lewandowski et al. (2012)) are presented in Table 1.\nThe results clearly indicate that our technique performs significantly better than the current state-ofthe-art."}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "We have proposed a generic technique called DBN-BLSTM for modeling sequences and have demonstrated its successful application to polyphonic music generation. We have used four datasets for evaluating our technique and have obtained state-of-the-art results. In the future, we look to work on other powerful architectures that help learning temporal dependencies in data without compromising on the powerful hierarchical representations that DBNs provide."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "We would like to thank Yoshua Bengio for helpful discussions. We would also like to thank the developers of Theano (Bastien et al. (2012); Bergstra et al. (2010)), which we have used for all our experiments."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Deep convex network: A scalable architecture for speech pattern classification", "author": ["Deng", "Li", "Yu", "Dong"], "venue": "In Interspeech. International Speech Communication Association,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Polyphonic music generation by modeling temporal dependencies using a RNN-DBN", "author": ["Goel", "Kratarth", "Vohra", "Raunaq", "J.K. Sahoo"], "venue": "In ICANN,", "citeRegEx": "Goel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey E", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N", "Kingsbury", "Brian"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Revisiting recurrent neural networks for robust ASR", "author": ["Vinyals", "Oriol", "Ravuri", "Suman", "Povey", "Daniel"], "venue": "IEEE International Confrence on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Vinyals et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)).", "startOffset": 125, "endOffset": 146}, {"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)).", "startOffset": 125, "endOffset": 164}, {"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)). Given that music is an inherently dynamic, it seems natural to consider recurrent neural networks (RNNs) as a good baseline technique for music generation. However, these models do not perform as well as deep networks (Vinyals et al. (2012)).", "startOffset": 125, "endOffset": 407}, {"referenceID": 4, "context": "The results achieved in Graves et al. (2013) clearly support our claim about the importance of modeling temporal dependencies in sequential data.", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "1 THE ARCHITECTURE The DBN-BLSTM is an extension of the generative model (RNN-DBN) proposed by Goel et al. (2014). There are a few significant improvements made to this model, most notably the replacement of the RNN with a LSTM, which is a more powerful neural architecture capable of modeling temporal dependencies across large time steps.", "startOffset": 95, "endOffset": 114}], "year": 2014, "abstractText": "Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network Bidirectional Long ShortTerm Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.", "creator": "LaTeX with hyperref package"}}}