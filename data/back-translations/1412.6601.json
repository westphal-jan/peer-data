{"id": "1412.6601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Using Neural Networks for Click Prediction of Sponsored Search", "abstract": "Sponsored search is a multi-billion dollar industry and represents an important source of revenue for search engines (SE). Click rates (CTR) play a critical role in ad selection and greatly influence SE revenue, ad traffic and user experience. We propose a novel architecture to solve the CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First, we compare ANN with other popular machine learning models used for this task. Then, we combine ANN with MatrixNet (proprietary implementation of enhanced trees) and evaluate the performance of the system as a whole. Results show that our approach offers significant improvements over existing models.", "histories": [["v1", "Sat, 20 Dec 2014 04:44:00 GMT  (55kb,D)", "http://arxiv.org/abs/1412.6601v1", "6 pages, 3 figures, for submission to workshop proceedings of ICLR 2015"], ["v2", "Sat, 28 Feb 2015 08:15:40 GMT  (57kb,D)", "http://arxiv.org/abs/1412.6601v2", "7 pages, 3 figures, for submission to workshop proceedings of ICLR 2015 Added more info about training the model in section 3.3 Updated Tables 2 and 3 Added some brief clarifications in section 3.1 and 4.2"], ["v3", "Sat, 19 Sep 2015 23:30:51 GMT  (75kb,D)", "http://arxiv.org/abs/1412.6601v3", "updated typos, and removed conference header"]], "COMMENTS": "6 pages, 3 figures, for submission to workshop proceedings of ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["afroze ibrahim baqapuri", "ilya trofimov"], "accepted": false, "id": "1412.6601"}, "pdf": {"name": "1412.6601.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SPONSORED SEARCH", "Afroze I. Baqapuri"], "emails": ["afroze.baqapuri@epfl.ch", "trofim@yandex-team.ru"], "sections": [{"heading": "1 INTRODUCTION", "text": "Sponsored search results are the small textual advertisements displayed on the search engine. Before displaying the ads are ranked by the SE according to two aspects: ad\u2019s relevance to the context so that more users will click on the ad, and some measure of the expected payment received from the advertiser. We limit ourselves to the most common cost-per-click model (CPC): advertiser is charged the bid price each time a user clicks the ad. Therefore, ad\u2019s CTR multiplied with the advertiser\u2019s bid is recognized as the estimated revenue. That is why CTR estimation plays an integral role in sponsored search and SE revenue.\nIn this study we use ANNs to model the click through rate of sponsored search. We propose a two-stage click-prediction system, which incorporates ANN into the existing framework of decision trees currently used at Yandex. Up to our knowledge, only Zhang et al. (2014) have used ANNs in the domain of click prediction. They use recurrent neural networks to model dependency on users sequential behavior in sponsored search. Compared to them our approach uses feed forward neural networks and models ID-based input features relating to the user, keyword, query, and advertisement.\nModern search engines use machine learning approaches to predict the CTR. Popular models include logistic regression (LR) (Richardson et al., 2007; McMahan et al., 2013; Chapelle et al., 2013) and boosted decision trees (Dembczynski et al., 2008; Trofimov et al., 2012). ANNs have advantage over LR because they are able to capture non-linear relationship between the input features and their \u201ddeeper\u201d architecture has inherently greater modeling strength. On the other hand decision trees - albeit popular in this domain - face additional challenges with with high-dimensional and sparse\n\u2217Most of the work was done while the author was an intern at Yandex.\nar X\niv :1\n41 2.\n66 01\nv1 [\ncs .L\nG ]\n2 0\nID-based features. We consider ANNs to be promising models because they already show state-ofthe-art results in various other domains including computer vision (Krizhevsky et al., 2012), natural language processing (Collobert et al., 2011b) and speech recognition (Mohamed et al., 2011).\nThe rest of this paper is organized as follows: In section 2 we briefly describe the existing search advertisement framework at Yandex. In Section 3 we discuss the experimental setting including the input features and the proposed model. In section 4 we present and discuss our results, and in section 5 we conclude our paper and propose some future work."}, {"heading": "2 SPONSORED SEARCH FRAMEWORK IN YANDEX", "text": "Mechanism of sponsored search is based on keyword auction: advertiser bids on a selected set of keywords. When a user types a query, the SE matches it with all keywords and selects appropriate ads to display. A simplified algorithm for selecting ads is described as follows. First all ads which match the user\u2019s query are selected and sorted in descending order according to expected revenue. Then the leading ads - at most three - are picked and sorted according to their bids.\nCTR is essential to this process as it is used in calculating the expected revenue. At Yandex MatrixNet is the machine learning algorithm used for estimating the CTR. MatrixNet is a proprietary implementation of boosted decision trees and it is successfully applied to numerous numerous classification and regression problems at the company. For sponsored search we use MatrixNet with real-valued features derived from click data logs. The input features describe statistics relating to the following groups: user, context, query, keyword, advertisement and advertiser. For more details about these input features refer to the paper by Trofimov et al. (2012)."}, {"heading": "3 EXPERIMENTAL SETTING", "text": ""}, {"heading": "3.1 INPUT FEATURES", "text": "We use click-through logs of Yandex search engine as our data set comprising of about 6.6 million training examples. The data was collected during one week between July 1 and July 7, 2014. For this study we focus only on ID-based features belonging to 14 namespaces, which can be divided into following categories:\n\u2022 User: User ID, domain ID, region ID.\n\u2022 Ad: Ad ID, campaign ID, words of ad title, words ad body, ad position, ad keywords.\n\u2022 Query: Words of user query\nThese features are encoded in 1-of-c encoding which results in very high dimensionality and extremely sparse features space. Since it would be infeasible to directly input the data directly into the neural network, we first attempt to reduce its dimensionality. This is done in two steps:\n1. Infrequent feature removal: If a feature occurs less than a certain threshold, we simply discard it. We found 10 to be a good threshold, which effectively reduces number of unique features from 8.5 million to 1.1 million.\n2. Hashing trick: We use a hash function to map the remaining features into a lower dimensional space, resulting in a compressed representation of the original feature space. There are bound to be some collisions, but Chapelle et al. (2013) empirically showed that it is not a major concern. We fix the hashing space dimensionality to 100,000.\nFollowing this we randomly divide the data into training (70%), validation (20%) and testing (10%) sets. An important point to note is that data has highly imbalanced classes (roughly 90% of the ad impressions resulting in no clicks). We cannot treat it as a standard classification problem because of high unfair bias towards negative class. Instead, we treat it as a stochastic process where the output of the model gives the probability of the ad being clicked."}, {"heading": "3.2 PROPOSED MODEL", "text": "We design the click prediction system as a two-stage process. In the first stage ANN models the sparse high-dimensional ID-based features. The second stage is MatrixNet which models the realvalued features. For each ad impression the ANN outputs a real-valued probability of that ad being clicked. This value serves as one of the input features of the MatrixNet. The other inputs are the ones being currently used as input features at Yandex and are described by Trofimov et al. (2012). The output of MatrixNet gives the final CTR of the ad impression which can be used to estimate the expected revenue. This two stage system is used for two reasons. Firstly, it provides a way of efficiently combining the two kinds of features (real-valued and ID-based) for CTR prediction. Secondly, since Yandex already uses MatrixNet for the task, this is the easiest way to incorporate the ANN model into the existing framework without any major overhaul.\nWe train feed-forward neural network with stochastic gradient descent (SGD) and mini-batches of size 100. We do a grid search along several meta parameters including l2 regularization coefficient, learning rate, learning rate decay and non-linearity function. Using the held-out validation set we select these meta parameters as follows: l2 coefficient = 3e-4; learning rate = 0.1; learning rate decay = 2e-4 per million training instances; non-linearity function = rectified linear units. We used torch7 for building and training the ANN models (Collobert et al., 2011a).\nFor comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al. (2007). A review of different optimization methods is presented by Minka (2003) wich shows BFGS to be fast and perform well in practice on logistic regression. Again we do a grid search ands select the best settings of the meta parameters. We used Vowpal Wabbit (Weinberger et al., 2009) to build and train these linear models.\nNegative log likelihood (NLL) is used as the error criterion for training the models, while NLL and and area under precision / recall curve (auPRC) are used for evaluation on the test set."}, {"heading": "4 RESULTS", "text": ""}, {"heading": "4.1 INDIVIDUAL ANN", "text": "First we evaluate the ANN independently, not considering the MatrixNet stage. The softmax output of ANN can be interpreted as CTR which is used for evaluating the neural network performance. We select the best six ANN architectures using early stopping on validation set. Table 1 shows the changes in NLL and auPRC of these models over the baseline LR performance. It is clear that replacing LR with ANN considerably improves performance, with the best network architecture showing 0.88% improvement in log likelihood and 5.57% improvement in auPRC metric.\nExperimentation also revealed that using rectified linear units (relu) was instrumental for achieving this performance improvement. Using relu resulted in up to 0.87% improvement in log likelihood and up to 5.7% improvement in auPRC. Similarly, using dropout functionality in the hidden units of the network enhanced the performance."}, {"heading": "4.2 ENSEMBLE OF ANNS", "text": "In ensemble methods results of many models are averaged to give performance improvement. The principle is that if different models settle down to very different local optimals then they will give incorrect predictions for different sets of instances. So if we combine and average the predictions of these models before comparing them to the target of the test instance, it may result in better performance as compared to the results of any individual model. We experiment with this method and use outputs of the best six ANN models in our ensemble.\nFigure 1(a) shows that performance improves steadily as we keep on increasing the number of ANNs in our ensemble. ANN models are added to the ensemble in the same order as they are listed in Table 1 (moving from top to bottom). In Table 2 we compare the performance of the ensemble and the best performing individual ANN with the LR results. We can observe that using ensemble improves the log likelihood by 0.24% and auPRC by 1.15% over the individual ANN. Figure 1(b) plots the precision-recall curves for the baseline LR and ensemble model. It clearly shows a visible improvement in auPRC for the ensemble."}, {"heading": "4.3 SIZE OF TRAINING SET", "text": "We wanted to determine the effect of training data size on the performance. For this purpose we train the ANN models on subsets of our training data (not changing the test and validation sets). Figure 2 shows the result of this experiment, where different models are evaluated and compared using auPRC metric. As expected, increasing the training size improves performance of all models. However, we notice that when the training data is small LR outperforms ANN. But as the training data increases, ANN catches up to LR, and in fact the performance gap keeps increasing steadily in favour of ANN. This means that true potential of ANN is exploited when using large data sets. From the plots we can infer that if we use even more training data, ANN would perform even better than LR as compared to current results."}, {"heading": "4.4 MATRIXNET RESULTS", "text": "Now we will evaluate the complete click-prediction system at Yandex as a whole. As stated before in Section 3, this consists of ANN followed by MatrixNet. For comparison we will also evaluate the existing framework: MatrixNet alone with baseline features sans ANN ooutput. The comparison results in Table 3 show that using ANN output as additional input to MatrixNet causes up to 0.22% improvement in log likelihood, and 0.48% improvement in auPRC. All the results are calculated on a separate test set. From our experience we know that results with a difference in log likelihood of 0.1% is small but important, and should not be neglected (Trofimov et al., 2012). Due to this observation we consider that using neural networks in the click prediction system at Yandex results in significant improvement."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this paper we proposed using ANNs for modeling ID-based features for CTR prediction task. First we showed that using non-linear models like ANN improves performance over linear model like LR. We went on to show that using an ensemble of ANNs improves performance even more and also using more data further increases the performance gap between . Finally we did a comparison and stated improvements of using ANN in combination with existing click prediction framework MatrixNet.\nFurther research can include testing on real-time data, and see the performance effects on a real-time ad selection system. However, more work would need to be done on improving time efficiency of the ANN system with extremely sparse input. As the results clearly show gap in performance improving with large data size, it would be interesting to see the effect of using much larger training data. Moreover, since many of the ID-based features are in forms of words it may be useful to initialize the neural network as an RBM trained with unsupervised contrastive divergence on a large volume of unlabeled examples. And then fine tune it as a discriminative model with back propagation."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank the Machine Learning group and the Computer Vision group at Yandex for helpful discussions."}], "references": [{"title": "A simple and scalable response prediction for display advertising", "author": ["Chapelle", "Olivier"], "venue": null, "citeRegEx": "Chapelle and Olivier,? \\Q2013\\E", "shortCiteRegEx": "Chapelle and Olivier", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Predicting ads click-through rate with decision rules", "author": ["Dembczynski", "Krzysztof", "W Kotlowski", "Weiss", "Dawid"], "venue": "In Workshop on Targeting and Ranking in Online Advertising,", "citeRegEx": "Dembczynski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ad click prediction: a view from the trenches", "author": ["McMahan", "H Brendan", "Holt", "Gary", "Sculley", "David", "Young", "Michael", "Ebner", "Dietmar", "Grady", "Julian", "Nie", "Lan", "Phillips", "Todd", "Davydov", "Eugene", "Golovin", "Daniel"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "McMahan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2013}, {"title": "A comparison of numerical optimizers for logistic regression", "author": ["Minka", "Thomas P"], "venue": "Unpublished draft,", "citeRegEx": "Minka and P.,? \\Q2003\\E", "shortCiteRegEx": "Minka and P.", "year": 2003}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Mohamed", "Abdel-rahman", "Sainath", "Tara N", "Dahl", "George", "Ramabhadran", "Bhuvana", "Hinton", "Geoffrey E", "Picheny", "Michael A"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["Nocedal", "Jorge"], "venue": "Mathematics of computation,", "citeRegEx": "Nocedal and Jorge.,? \\Q1980\\E", "shortCiteRegEx": "Nocedal and Jorge.", "year": 1980}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Richardson", "Matthew", "Dominowska", "Ewa", "Ragno", "Robert"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Using boosted trees for click-through rate prediction for sponsored search", "author": ["Trofimov", "Ilya", "Kornetova", "Anna", "Topinskiy", "Valery"], "venue": "In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy,", "citeRegEx": "Trofimov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Trofimov et al\\.", "year": 2012}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Zhang", "Yuyu", "Dai", "Hanjun", "Xu", "Chang", "Feng", "Jun", "Wang", "Taifeng", "Bian", "Jiang", "Bin", "Liu", "Tie-Yan"], "venue": "arXiv preprint arXiv:1404.5772,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Popular models include logistic regression (LR) (Richardson et al., 2007; McMahan et al., 2013; Chapelle et al., 2013) and boosted decision trees (Dembczynski et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 5, "context": "Popular models include logistic regression (LR) (Richardson et al., 2007; McMahan et al., 2013; Chapelle et al., 2013) and boosted decision trees (Dembczynski et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 3, "context": ", 2013) and boosted decision trees (Dembczynski et al., 2008; Trofimov et al., 2012).", "startOffset": 35, "endOffset": 84}, {"referenceID": 10, "context": ", 2013) and boosted decision trees (Dembczynski et al., 2008; Trofimov et al., 2012).", "startOffset": 35, "endOffset": 84}, {"referenceID": 8, "context": "Up to our knowledge, only Zhang et al. (2014) have used ANNs in the domain of click prediction.", "startOffset": 26, "endOffset": 46}, {"referenceID": 4, "context": "We consider ANNs to be promising models because they already show state-ofthe-art results in various other domains including computer vision (Krizhevsky et al., 2012), natural language processing (Collobert et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": ", 2011b) and speech recognition (Mohamed et al., 2011).", "startOffset": 32, "endOffset": 54}, {"referenceID": 10, "context": "For more details about these input features refer to the paper by Trofimov et al. (2012).", "startOffset": 66, "endOffset": 89}, {"referenceID": 11, "context": "We used Vowpal Wabbit (Weinberger et al., 2009) to build and train these linear models.", "startOffset": 22, "endOffset": 47}, {"referenceID": 7, "context": "The other inputs are the ones being currently used as input features at Yandex and are described by Trofimov et al. (2012). The output of MatrixNet gives the final CTR of the ad impression which can be used to estimate the expected revenue.", "startOffset": 100, "endOffset": 123}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al.", "startOffset": 57, "endOffset": 310}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al. (2007). A review of different optimization methods is presented by Minka (2003) wich shows BFGS to be fast and perform well in practice on logistic regression.", "startOffset": 57, "endOffset": 339}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al. (2007). A review of different optimization methods is presented by Minka (2003) wich shows BFGS to be fast and perform well in practice on logistic regression.", "startOffset": 57, "endOffset": 412}, {"referenceID": 10, "context": "1% is small but important, and should not be neglected (Trofimov et al., 2012).", "startOffset": 55, "endOffset": 78}], "year": 2014, "abstractText": "Sponsored search is a multi-billion dollar industry and makes up a major source of revenue for search engines (SE). click-through-rate (CTR) estimation plays a crucial role for ads selection, and greatly affects the SE revenue, advertiser traffic and user experience. We propose a novel architecture for solving CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First we compare ANN with respect to other popular machine learning models being used for this task. Then we go on to combine ANN with MatrixNet (proprietary implementation of boosted trees) and evaluate the performance of the system as a whole. The results show that our approach provides significant improvement over existing models.", "creator": "LaTeX with hyperref package"}}}