{"id": "1412.6885", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Half-CNN: A General Framework for Whole-Image Regression", "abstract": "In this article, we propose a holistic CNN regression model by removing the entire connectivity layer and training the network with continuous function boards, which is a generic regression framework that is suitable for many applications.We demonstrate this method by two tasks: simultaneous face recognition & amp; segmentation and scene out prediction. The result is comparable to other models in the respective areas, using only a small-scale network.Since the regression model is trained on corresponding image function boards, unlike the classification model, there are no requirements for uniform input size. Our framework avoids classification design, a process that could introduce too many manual interventions into model development.Nevertheless, it is strongly correlated to the classification network and offers some in-depth checks of CNN structures, unlike the classification model.", "histories": [["v1", "Mon, 22 Dec 2014 06:43:58 GMT  (925kb)", "http://arxiv.org/abs/1412.6885v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jun yuan", "bingbing ni", "ashraf a kassim"], "accepted": false, "id": "1412.6885"}, "pdf": {"name": "1412.6885.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures."}, {"heading": "1. Introduction", "text": "The image classification techniques have evolved vastly in recent years, from Bag-of-Visual-Words (BoVW) [1, 2], Fisher Vector (FV) Encoding [3], to the state-of-the-art Convolutional Neural Network (CNN) [4]. The BoVW and FV models typically consist of a pipeline, with cascading processes of feature description, dictionary building and feature encoding, pooling over image and classification. A linear SVM classifier is used at final stage, and the whole process can be considered as a nonlinear classification model on images. These techniques have been extensively studied in [5, 6]. The CNN classification model consists of several convolution blocks followed by fully connected layers. Each convolution block consists of a convolution layer with a non-linear activation, a pooling layer, and sometimes a local normalization layer [7]. The fully connected layer can be considered as a final pooling layer, which feeds the image feature to the classifier. The CNN model typically uses a\nsoftmax classifier at the training stage. After training, the softmax classifier is often replaced by an SVM classifier on the final image representations, for improved classification performance. Sometimes the SVM is directly used in the network training process.\nThe key advance of CNN lies in its ability to learn adaptive features, as opposed to the hand-crafted features like SIFT [8] in BoVW and FV models. These handcrafted features are intuitive for human perception; they can also be considered as a shallow network. The former convolution blocks in CNN perform similar tasks as these feature detectors; however the learned features are more optimized for the given task. Similar statements hold for the encoding process. The linear SVM classifier at output is also a shallow network. Thus the classification pipeline can be treated as a (relatively) shallow network consisting of hand-crafted function blocks, whereas CNN is a deep\nnetwork consisting of optimized blocks. This explains the great success of CNN in classification tasks. The CNN classification model achieves state-of-the-art performance on classification datasets such as ImageNet ILSVRC [9] and PASCAL VOC challenges [10]. A comprehensive study can be found in [11]. The pre-trained ImageNet model [12, 13, 14] is also successful in generalizing to other image classification datasets.\nThe CNN is also widely applied on non-classification tasks, such as object detection [15, 16] and segmentation [17, 16]. The model can be either applied on image level, or patch level with information aggregation. The ImageNet pre-trained model is also widely used in these applications.\nHowever, it seems that CNN is not so widely used in regression tasks, which is also a fundamental problem in machine learning. Regression and classification problems are highly correlated [18], and can be transferred to the other in many scenarios. For example, the softmax classifier is related to the softmax regression [19], and the SVM classifier is based on geometric regression. It is natural to develop a deep CNN model for regression tasks as a counterpart of classification.\nIn this paper, we propose a simple but generic 2D CNN regression model. By removing the fully connected layer(s) in the classification network, we yield a network output that is locally correlated with network input (due to the locality of convolution operation); this is also the key characteristic of local regression models. The final output feature map is generated through a linear combination of convolution channels, which can be considered as a partially connected layer. Thus we can get a 2D local regression network out of the classification network. We also propose an up-sampling layer to reduce the downsampling effects from pooling operations. The relationship between the classification and regression networks is discussed in detail in section 5.\nThe ground truth to train the regression network are feature maps generated from input images. The size of the maps is related with input images, either identical or with a pre-defined down-sampling factor. These two scenarios correspond to the convolution and pooling operations in the neural network. Since there is no fully connected layer, our model does not pose a requirement on identical input size, which can be a problem in the CNN classification model.\nWe apply the above framework to two applications: simultaneous face detection & segmentation and saliency prediction. In the former task, the ground truth feature maps are generated by fitting a 2D Gaussian density function inside the detection window. The trained network can not only recover the position of faces, but also output the segmented face regions on the feature map. The saliency prediction is a natural application of our network, since the ground truth is already given in real-value\nfeature maps. We demonstrate the effectiveness of our work through a small scale regression network."}, {"heading": "2. Related Work", "text": "Face detection is an important task in computer vision. There are various models from the appearance to learning based models [20, 21, 22, 23]. The classical models are extensively studied in [24]. The CNN models are recently developed into this field [25]. This paper uses convolution to generate feature maps, and a SVM classifier to detect face windows. This approach is similar to the part-based model object detection [26]. Some other face detection algorithms are implemented through segmentation [27, 28]; the idea is close to our framework. However, our network does not need to include any specific knowledge for facial features, e.g. skin color, to the detection or segmentation task. The features are fully learned without manual crafting in our network.\nFace detection can also be combined with simultaneous face landmark localization [29, 30, 31]. Part of our work is based on Y. Sun et al. [31], where a CNN model is applied to detect face landmarks. A three-level cascaded network is used to predict face landmark through a coarse-to-fine localization process. Their network also uses a regression model, but different from ours. In their approach, the network outputs several locations as a fixed-size set of real numbers, which can be considered as a mapping from the 2D real domain to the 1D real domain, i.e. a 2D-1D mapping. The CNN classification model maps images to a fixed finite countable set which can be considered as a 2D0D mapping. A fully connected layer is needed whenever there is dimension reduction from the input to the output. As a comparison, our CNN regression model is a 2D-2D mapping between images and feature maps, where the full connection is avoided. The work [31] can be considered as a bridge between the classification CNN model and our model.\nWe also test the face landmark localization in our framework. However, this task heavily depends on the knowledge of spatial relationship between face landmark points, and a pure texture approach like ours to give independent predictions of landmarks is not ideal. The CNN localization model in Y. Sun et al. [31] also utilizes the spatial relationship of landmarks implicitly through a global-to-local refinement. We shall briefly discuss this point in section 4.\nSaliency prediction is another important study field, serving as a bridge between computer vision and human perception. The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37]. An extensive study of classical models can be found in [38]. A comparison of these models can be found in [39].\nThere are two types of saliency models: bottom-up and up-to-down saliency model. The former is typically more focused on human perception process and low level image information, whereas the latter is more related to scene knowledge on interest objects. The latter is also highly related to object detection and segmentation; some models explicitly use detection and segmentation techniques to give saliency predictions [34, 35]. The recent CNN models also offer competitive performance in this field. The eDN [37] uses a large number of randomly initialized CNNs, picking those with good performance and aggregates their output feature maps to give predictions. The latter Deep Gaze I [36] uses ImageNet pre-trained network without the full connection layer, and learns a weight on linear combination of convolution channels. This model offers the state-of-the-art performance. Our model has a structure similar to the Deep Gaze model, but trained in a different way without using classification information.\nSince saliency information is provided as feature maps, this is a natural application fits into our framework. We use the CNN regression model to train the saliency model and give comparative studies in section 4. There is also a detailed discussion in section 5."}, {"heading": "3. Implementation Details", "text": "Our network consists of several convolution blocks and an output layer. The convolution blocks follow structure of [7], and the output layer combines all the feature channels through a linear combination.\nConvolution blocks. Each convolution block includes a convolution layer, a ReLU activation, and a pooling layer with a down-sampling factor of 2. Max-pooling is used in as the pooling function. A normalization layer introduced in [7] can also be used after the pooling layer, with the local normalization function below:\n\ud835\udc4f\ud835\udc65,\ud835\udc66 = \ud835\udc4e\ud835\udc65,\ud835\udc66/ (\ud835\udc58 + \ud835\udefc \u2211 (\ud835\udc4e\ud835\udc65,\ud835\udc66 2 )\n\ud835\udc57\u2208\ud835\udc41(\ud835\udc57)\n)\n\ud835\udefd\nThe above function computes the feature response at location (\ud835\udc65, \ud835\udc66) by normalizing the responses at the same location in its \ud835\udc41 neighboring channels. In practice, the normalization layer can offer limited performance gain, and is used only in the first two convolution blocks in our experiments.\nThere is a limit on the amount of pooling layers that can be used in our network, due to the size limitation of the ground truth feature maps. For example, if the feature map is 4\u00d7 times smaller than the original image, we can only use two max-pooling layers with a down-sampling factor 2. There are several ways to get around this: We can either to use pooling with a stride of 1, or use convolution without pooling in the following blocks. However, these two methods reduce the non-linearity of the network, which is not a desired property.\nIn this paper, we propose an up-sampling layer, to maintain the size of intermediate features in the network. The up-sampling layer follows the pooling layer and restores the size of pooled features, while retaining the non-linearity of the network. The up-sampling function is defined as follows:\n\ud835\udc34\ud835\udc56+1(\ud835\udc5d\ud835\udc65 \u2212 \ud835\udc5d + 1: \ud835\udc5d\ud835\udc65, \ud835\udc5d\ud835\udc66 \u2212 \ud835\udc5d + 1: \ud835\udc5d\ud835\udc66) = \ud835\udc34\ud835\udc56(\ud835\udc65, \ud835\udc66)\nwhere \ud835\udc5d is the down-sampling factor used in the pooling layer, typically of the value 2. The above function copies a value from the pooled features to a \ud835\udc5d\u00d7\ud835\udc5d block in the following up-sampling layer. The back-propagation rule of this layer is just the average-pooling layer in the reverse direction, with a scale of \ud835\udc5d2. Since each block in the upsampling layer consists of the same value, the backpropagation principal of this layer can be simplified as:\n\ud835\udc51\ud835\udc34\ud835\udc56(\ud835\udc65, \ud835\udc66) = \ud835\udc5d 2 \ud835\udc51\ud835\udc34\ud835\udc56+1(\ud835\udc5d\ud835\udc65, \ud835\udc5d\ud835\udc66)\nOutput Layer. The output layer combines the feature channels from the last convolution block through a linear combination. Then a sigmoid activation function is applied to produce the final output. The layer forward and back propagation rules are shown below:\n\ud835\udc34\ud835\udc5c = sigm (\u2211 \ud835\udc64\ud835\udc56\ud835\udc34\ud835\udc56 \ud835\udc56 + \ud835\udc4f) \u225c sigm(\ud835\udc4d) \ud835\udc51\ud835\udc4d = (\ud835\udc40 \u2218 \ud835\udc51\ud835\udc34\ud835\udc5c) \u2218 \ud835\udc34\ud835\udc5c \u2218 (1 \u2212 \ud835\udc34\ud835\udc5c) =\n\ud835\udc51\ud835\udc64\ud835\udc56 = \u2211 \ud835\udc34\ud835\udc56,\ud835\udc65,\ud835\udc66\ud835\udc51\ud835\udc4d\ud835\udc65,\ud835\udc66 \ud835\udc65,\ud835\udc66 , \ud835\udc51\ud835\udc4f = \u2211 \ud835\udc51\ud835\udc4d\ud835\udc65,\ud835\udc66 \ud835\udc65,\ud835\udc66 \ud835\udc51\ud835\udc34\ud835\udc56 = \ud835\udc64\ud835\udc56\ud835\udc51\ud835\udc4d\nwhere \ud835\udc34\ud835\udc56 is the convoluted feature channels, \ud835\udc64 and \ud835\udc4f are the weights and bias of the linear combination. \ud835\udc40 is a mask indicating the content in images and feature maps, which is introduced in section 3.1.\nAlso, it is possible to use other type of combinations, such as a max-pooling function among all the convolution channels. However, this function seems to underperform the linear combination in our experiments.\nSince the feature maps consist of real continuous values (typically ranging from 0 to 1), the network performs a 2D-2D regression task. The structure of our network is shown in Figure 2."}, {"heading": "3.1. Size considerations in practice", "text": "The size of ground truth feature map is typically related to the input image. Because there are no fully connected layers in the regression framework, we can bypass the uniform input size requirement in typical classification CNNs. If the input sizes are different in the classification model, the images have to be warped or cropped to the same size. Different image aspect ratio is also a problem. However, our regression model can fundamentally avoid these headaches.\nThe convolution operation does not depend on image size; however the layer input needs to be properly padded according to the filter size, to ensure the correct downsampling factor after pooling. The input and output size of convolution blocks differ by a factor of 2, if pooling without up-sampling is used. When an up-sampling layer is present, the input and output sizes of convolution blocks are identical.\nIn theory, the network can accept input images of all different sizes. The batch gradient can be calculated by averaging all the individual gradients produced by single images in the batch. However, for fast computation, it is recommended that all the images within a batch are identical in size to utilize the advanced data structure in typical CNN implementations.\nShould the input sizes differ vastly, there is another convenient way to bypass the uniform-size requirement. All the input images in a batch can be padded to the same size, with a mask recording the content of each input. The ground truth feature maps are padded in accordance with corresponding images. The mask is then applied to the\nderivative of the output feature map to ensure correct gradient computation. This technique cannot be applied on the classification network, since the fully connected layer requires proper arrangement of features. Applying masks will interrupt the pre-defined feature sequences.\nThe above padding-masking scheme can also be applied on filters, if a convolution layer includes filters of different sizes. However, this seems to be a relatively rare setup in most of the CNN applications."}, {"heading": "4. Experiments", "text": "We use the above framework for two applications. One is simultaneous face detection & segmentation on the LFW face dataset [40], the other is saliency prediction on the MIT dataset (1003 images version) [41]. The two networks are both trained on the MatConvNet platform [42], a Matlab toolbox based on the famous CAFFE [13] CNN implementation."}, {"heading": "4.1. Face Detection & Segmentation", "text": "We use 5590 face images from the LFW dataset, with different viewing angles. All images are of identical size 250\u00d7250. We use 4151 images for training and the rest for testing. The detection window provided in [31] is used as the ground truth.\nWe use a simple technique to generate the ground truth feature maps: a 2D Gaussian density function is fit at the center of the detection window, with the diameter (6\ud835\udf0e\ud835\udc65 and 6\ud835\udf0e\ud835\udc66) being the window width and height. The size of input images is padded to 256\u00d7256 as stated in section 3, and a 4\u00d7 down-sampling is applied to the generated feature maps.\nWe use 3 convolution blocks in this experiment. The first two blocks consist of 5 filters of size 11\u00d711 and 7\u00d77 respectively, with max-pooling and normalization. The following block consists of 5 filters of size 5\u00d75, with upsampling layer after max-pooling.\nFor numerical stability, the ground truth feature maps are re-normalized to [0.1, 0.9] for the sigmoid activation at the output layer. Also, there is a small \ud835\udc3f2 penalty on filter weights and biases of each layer for regularization. This penalty terms can prevent the network from over-fitting and improves generalization ability.\nSome sample outputs are shown in Figure 3. The output feature map is overlayed on input images for visualization. It can be seen that the output feature maps correspond to the segmentation of faces in the input. The neck area is sometimes included in the feature map; this is natural because of the color similarity between face and neck areas. Increase the network complexity might relieve this problem.\nThe detection window can be retrieved by analyzing the feature maps: get the center and standard deviation of the response, and fit the window as the reverse process of generating Gaussian density functions. The retrieval rate of detection window is greater than 95% in our experiments.\nThough we train the network with a simple Gaussian density function in the detection window, the network surprisingly provides the ability of both detection and segmentation, with only a small scale network. Moreover, the network can detect and segment multiple faces as shown in Figure 1 and 3. This indicates the generalization\nability of our regression network.\nWe also tested our regression model on face landmark localization experiments, based on the work of [31]. The ground truth feature maps are generated by fitting small 2D Gaussian density functions to the face landmark positions. However, our network does not perform well on this application, for that it is a more textural approach, without incorporating any spatial relationship between landmarks. This relationship is crucial in this application, and detecting the landmarks independently through our framework is not ideal. Some sample detection results are shown in Figure 4.\nThe landmark localization network is able to detect more structural regions like eyes, and fails in the less structural regions on noses. This is expected because of the color similarity in nose regions. The structural contents in eyes are most, and moderate in mouths; noses offer the least information in textures."}, {"heading": "4.2. Saliency Prediction", "text": "The MIT saliency dataset consists of 1003 indoor and outdoor scene images. The longest dimension of each image is 1024 pixels, and the other edge ranges between 405 and 1024 pixels. The aspect ratio is typically around 4:3. The ground truth of saliency prediction is already provided as feature maps, thus this is an application that naturally fits our regression framework. We pad the images to 256\u00d7256, and use a down-sampling factor of 4 for feature maps. We use the same network regularization parameters as in the former face detection & segmentation experiments.\nWe use 4 convolution blocks in this experiment. The first two blocks consist of 10 filters of size 7\u00d77, with maxpooling and local normalization. The following two blocks\nconsist of 10 filters of size 5\u00d75, with max-pooling and upsampling.\nSome experiment results are shown in Figure 5. We can see that the network output is highly correlated with the ground truth saliency map. Some quantitative evaluation and comparison with other methods are shown in Figure 6.\nOur model outperforms most of the classical saliency prediction methods, while not incorporating any specific knowledge to saliency studies. We have yet to compare our model to the latest work, due to the difference in database and evaluation methods.\nAdditional experiment results can be found at Figure 7\nand 8 for both applications.\nComputation Time. Since our regression network is small in scale, the L-BFGS [43] algorithm is used for fast convergence. The network can be trained in 2~3 hours and 5~6 hours respectively for the face detection & segmentation and saliency prediction, with an NVidia K40 graphics card."}, {"heading": "5. Discussion", "text": "Generality. Our CNN regression model is a general framework that can be applied on a variety of applications, where the output is a feature map correlated with input images. For example, other than our simple face detection & segmentation, we can generate feature maps on more complicated segmentation tasks such as the PASCAL VOC challenge [10] and neuron membrane segmentation [44] in medical images, with the segmented object as the ground truth. This is part of our future study on more advanced detection & segmentation applications.\nAlso, our framework provides another view of classical features, such as SIFT and HoG [45]. The output of SIFT and HoG can be re-interpreted as feature maps to fit our regression network. Our network is also a part of the CNN\nclassification model, where the former convolution blocks perform similar feature detection tasks.\nThe regression model can also be applied on non-image applications. The convolution operation is local, and shares similar structure with local regression models, such as the famous non-linear kernel regression. Thus many regression works can be reviewed by the CNN regression model.\nLimitations. A limitation of our framework is, the network requires a very large training set to perform well; this is the same as other CNN models. For example, the Deep Gaze I [36] uses ImageNet pre-trained models on saliency prediction, and achieves best performance. This amounts to using a large training set in this application. Nevertheless, getting the ground truth feature maps is not as easy as getting classification labels, which aggravates the difficulty in generating training sets.\nPrevent Over-fitting. Over-fitting is a very common problem in regression. Adding regularization terms on model parameters will prevent over-fitting and achieve better generalization ability on testing sets. In our CNN regression model, a simple \ud835\udc3f2 penalty on network is applied as regularization terms. Other types of penalties, such as the \ud835\udc3f1 norm, can offer some desired properties like sparsity [46, 47, 48, 49]. However, network with these penalties can be more complex to optimize. Adding regularization terms alleviates over-fitting by reducing the effective degrees of freedom (DoF) of the network [18]. For example, a large network will over-fit on simple tasks like our face detection and segmentation problem; adding penalties can reduce the effective parameters and make the network behave like a small-scale network. Determining the proper DoF or network scale for different applications is very difficult, and often based on test-and-trial. The drop-out technique [50] is a famous way to prevent overfitting in neural networks; it is proved to be an adaptive \ud835\udc3f2 regularization [51] on networks, which alleviates the test-\nand-trial process on penalty selection.\nAnother way to prevent over-fitting is the data approach. Providing huge training data to the network can also be considered as a regularization process: the training data itself covers the whole space; a network trained on the whole space data will have better generalization ability than those trained on a (small) subspace, since the test example will always lie in the space expanded by training samples. In other words, the testing phase will always be an interpolation process rather than extrapolation process; the latter is known to be much more difficult than the former. The ImageNet pre-trained model is the big data approach, and is proved to have good generalization ability in many other classification tasks. Acquiring large data is sometimes not possible; data augmentation is often applied [11] to alleviate the requirement for training data. Typical data augmentations are geometric transforms on the image dataset. This offers enlarged space (though still limited) spanned by the training images.\nRelationship with the CNN classification model. The convolution blocks in our network are identical to those in the CNN classification model. The linear combination on convolution channels can be considered as a partially connected layer, compared to the fully connected layer in the classification model. Thus by adding another level of connection over the output feature map, we yield the CNN classification model.\nThe feature maps often correspond to the interested objects, which are also crucial to the classification task. We can consider the CNN classification model as an implicit feature map generating process using image class labels, whereas the regression model uses explicit feature maps generated by other techniques. The convoluted features before fully connected layers can be considered as implicit feature maps corresponding to objects. It might explain why the ImageNet pre-trained classification model is also successful in many non-classification applications."}, {"heading": "6. Conclusion", "text": "We propose a CNN-based regression model trained on continuous feature maps. The regression network does not require fully connection layer, and is insensitive to input sizes. We introduce an up-sampling technique for size compatibility, and generate output feature maps through a linear combination on convolution channels. We apply this framework to face detection & segmentation and saliency prediction, and demonstrate its generalization ability in these tasks. Also, this general framework is highly related to the classification model and has the potential to be applied on variety of applications."}], "references": [{"title": "Video Google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "in Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Visual categorization with bags of keypoints", "author": ["C.B.G. Csurka", "C. Dance", "L. Fan"], "venue": "Workshop on Statistical Learning in Computer Vision, ECCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Improving the fisher kernel for large-scale image classification, in Computer Vision\u2013ECCV", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y LeCun"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K Chatfield"], "venue": "British Machine Vision Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Feature coding in image classification: a comprehensive study", "author": ["Y Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks, in Advances in Neural Information", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Distinctive Image Features from Scale- Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J Deng"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M Everingham"], "venue": "International journal of computer vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "author": ["Razavian", "A.S"], "venue": "arXiv preprint arXiv:1403.6382,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "author": ["Chatfield", "K.K.a.S", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y Jia"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Pedestrian detection with convolutional neural networks", "author": ["M Szarvas"], "venue": "in Intelligent Vehicles Symposium,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R Girshick"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "New object-oriented segmentation algorithm based on the CNN paradigm", "author": ["G Grassi"], "venue": "Circuits and Systems II: Express Briefs, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The elements of statistical learning: data mining, inference, and prediction", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Pattern recognition and machine learning. Vol. 1. 2006: springer New York", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "Training support vector machines: an application to face detection", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Example-based learning for view-based human face detection", "author": ["Sung", "K.-K", "T. Poggio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Rapid object detection using a boosted cascade of simple features. in Computer Vision and Pattern Recognition", "author": ["P. Viola", "M. Jones"], "venue": "Proceedings of the 2001 IEEE Computer Society Conference on (Volume:1", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "A CNN-Based Face Detector with a Simple Feature Map and a Coarse-to-fine Classifier", "author": ["C Yin-Nong"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Object Detection with Discriminatively Trained Part-Based Models. IEEE Figure 7: Additional experiment results on simultaneous face detection & segmentation Figure 8: Additional experiment results on scene saliency prediction", "author": ["Felzenszwalb", "P.F"], "venue": "Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Face segmentation using skincolor map in videophone applications. Circuits and Systems for Video Technology", "author": ["D. Chai", "K.N. Ngan"], "venue": "IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "Multi-source Multi-scale Counting in Extremely Dense Crowd Images", "author": ["H Idrees"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Active appearance models", "author": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X. Zhu", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Deep Convolutional Network Cascade for Facial Point Detection", "author": ["S. Yi", "W. Xiaogang", "T. Xiaoou"], "venue": "in Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Computational modelling of visual attention", "author": ["L. Itti", "C. Koch"], "venue": "Nature reviews neuroscience,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Graph-based visual saliency. in Advances in neural information processing systems", "author": ["J. Harel", "C. Koch", "P. Perona"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Learning visual saliency by combining feature maps in a nonlinear manner using AdaBoost", "author": ["Q. Zhao", "C. Koch"], "venue": "Journal of Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Predicting human gaze beyond pixels", "author": ["J Xu"], "venue": "Journal of vision,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet", "author": ["M. K\u00fcmmerer", "L. Theis", "M. Bethge"], "venue": "arXiv preprint arXiv:1411.1045,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images. in Computer Vision and Pattern Recognition", "author": ["E. Vig", "M. Dorr", "D. Cox"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Saliency in images and video: a brief survey", "author": ["K. Duncan", "S. Sarkar"], "venue": "Computer Vision, IET,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. in Workshop on Faces in'Real-Life'Images: Detection, Alignment, and Recognition", "author": ["Huang", "G.B"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Learning to predict where humans look", "author": ["T Judd"], "venue": "Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1989}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images. in Advances in neural information processing", "author": ["D Ciresan"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Histograms of oriented gradients for human detection. in Computer Vision and Pattern Recognition", "author": ["N. Dalal", "B. Triggs"], "venue": "Proceedings of the 2005IEEE Computer Society Conference on (Volume:1", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Regression Shrinkage and Selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society.Series B (Methodological),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1996}, {"title": "Regularization and Variable Selection via the Elastic Net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society.Series B (Statistical Methodology),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "A sparse sampling model for 3D face recognition", "author": ["Y. Jun", "A.A. Kassim"], "venue": "Image Processing (ICIP),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "G.E"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "Dropout Training as Adaptive Regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "ArXiv e-prints,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The image classification techniques have evolved vastly in recent years, from Bag-of-Visual-Words (BoVW) [1, 2], Fisher Vector (FV) Encoding [3], to the state-of-the-art Convolutional Neural Network (CNN) [4].", "startOffset": 105, "endOffset": 111}, {"referenceID": 1, "context": "The image classification techniques have evolved vastly in recent years, from Bag-of-Visual-Words (BoVW) [1, 2], Fisher Vector (FV) Encoding [3], to the state-of-the-art Convolutional Neural Network (CNN) [4].", "startOffset": 105, "endOffset": 111}, {"referenceID": 2, "context": "The image classification techniques have evolved vastly in recent years, from Bag-of-Visual-Words (BoVW) [1, 2], Fisher Vector (FV) Encoding [3], to the state-of-the-art Convolutional Neural Network (CNN) [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "The image classification techniques have evolved vastly in recent years, from Bag-of-Visual-Words (BoVW) [1, 2], Fisher Vector (FV) Encoding [3], to the state-of-the-art Convolutional Neural Network (CNN) [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "These techniques have been extensively studied in [5, 6].", "startOffset": 50, "endOffset": 56}, {"referenceID": 5, "context": "These techniques have been extensively studied in [5, 6].", "startOffset": 50, "endOffset": 56}, {"referenceID": 6, "context": "Each convolution block consists of a convolution layer with a non-linear activation, a pooling layer, and sometimes a local normalization layer [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "The key advance of CNN lies in its ability to learn adaptive features, as opposed to the hand-crafted features like SIFT [8] in BoVW and FV models.", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "The CNN classification model achieves state-of-the-art performance on classification datasets such as ImageNet ILSVRC [9] and PASCAL VOC challenges [10].", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "The CNN classification model achieves state-of-the-art performance on classification datasets such as ImageNet ILSVRC [9] and PASCAL VOC challenges [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "A comprehensive study can be found in [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The pre-trained ImageNet model [12, 13, 14] is also successful in generalizing to other image classification datasets.", "startOffset": 31, "endOffset": 43}, {"referenceID": 12, "context": "The pre-trained ImageNet model [12, 13, 14] is also successful in generalizing to other image classification datasets.", "startOffset": 31, "endOffset": 43}, {"referenceID": 13, "context": "The pre-trained ImageNet model [12, 13, 14] is also successful in generalizing to other image classification datasets.", "startOffset": 31, "endOffset": 43}, {"referenceID": 14, "context": "The CNN is also widely applied on non-classification tasks, such as object detection [15, 16] and segmentation [17, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 15, "context": "The CNN is also widely applied on non-classification tasks, such as object detection [15, 16] and segmentation [17, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 16, "context": "The CNN is also widely applied on non-classification tasks, such as object detection [15, 16] and segmentation [17, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "The CNN is also widely applied on non-classification tasks, such as object detection [15, 16] and segmentation [17, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 17, "context": "Regression and classification problems are highly correlated [18], and can be transferred to the other in many scenarios.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "For example, the softmax classifier is related to the softmax regression [19], and the SVM classifier is based on geometric regression.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "There are various models from the appearance to learning based models [20, 21, 22, 23].", "startOffset": 70, "endOffset": 86}, {"referenceID": 20, "context": "There are various models from the appearance to learning based models [20, 21, 22, 23].", "startOffset": 70, "endOffset": 86}, {"referenceID": 21, "context": "There are various models from the appearance to learning based models [20, 21, 22, 23].", "startOffset": 70, "endOffset": 86}, {"referenceID": 22, "context": "There are various models from the appearance to learning based models [20, 21, 22, 23].", "startOffset": 70, "endOffset": 86}, {"referenceID": 23, "context": "The CNN models are recently developed into this field [25].", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "This approach is similar to the part-based model object detection [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "Some other face detection algorithms are implemented through segmentation [27, 28]; the idea is close to our framework.", "startOffset": 74, "endOffset": 82}, {"referenceID": 26, "context": "Some other face detection algorithms are implemented through segmentation [27, 28]; the idea is close to our framework.", "startOffset": 74, "endOffset": 82}, {"referenceID": 27, "context": "Face detection can also be combined with simultaneous face landmark localization [29, 30, 31].", "startOffset": 81, "endOffset": 93}, {"referenceID": 28, "context": "Face detection can also be combined with simultaneous face landmark localization [29, 30, 31].", "startOffset": 81, "endOffset": 93}, {"referenceID": 29, "context": "Face detection can also be combined with simultaneous face landmark localization [29, 30, 31].", "startOffset": 81, "endOffset": 93}, {"referenceID": 29, "context": "[31], where a CNN model is applied to detect face landmarks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The work [31] can be considered as a bridge between the classification CNN model and our model.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "[31] also utilizes the spatial relationship of landmarks implicitly through a global-to-local refinement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 60, "endOffset": 64}, {"referenceID": 32, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 98, "endOffset": 106}, {"referenceID": 33, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 98, "endOffset": 106}, {"referenceID": 34, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 144, "endOffset": 152}, {"referenceID": 35, "context": "The saliency prediction models evolved from Itti [32], GBVS [33], detection / segmentation models [34, 35], and to more recent CNN-based models [36, 37].", "startOffset": 144, "endOffset": 152}, {"referenceID": 36, "context": "An extensive study of classical models can be found in [38].", "startOffset": 55, "endOffset": 59}, {"referenceID": 32, "context": "The latter is also highly related to object detection and segmentation; some models explicitly use detection and segmentation techniques to give saliency predictions [34, 35].", "startOffset": 166, "endOffset": 174}, {"referenceID": 33, "context": "The latter is also highly related to object detection and segmentation; some models explicitly use detection and segmentation techniques to give saliency predictions [34, 35].", "startOffset": 166, "endOffset": 174}, {"referenceID": 35, "context": "The eDN [37] uses a large number of randomly initialized CNNs, picking those with good performance and aggregates their output feature maps to give predictions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "The latter Deep Gaze I [36] uses ImageNet pre-trained network without the full connection layer, and learns a weight on linear combination of convolution channels.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "The convolution blocks follow structure of [7], and the output layer combines all the feature channels through a linear combination.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "A normalization layer introduced in [7] can also be used after the pooling layer, with the local normalization function below:", "startOffset": 36, "endOffset": 39}, {"referenceID": 37, "context": "One is simultaneous face detection & segmentation on the LFW face dataset [40], the other is saliency prediction on the MIT dataset (1003 images version) [41].", "startOffset": 74, "endOffset": 78}, {"referenceID": 38, "context": "One is simultaneous face detection & segmentation on the LFW face dataset [40], the other is saliency prediction on the MIT dataset (1003 images version) [41].", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "networks are both trained on the MatConvNet platform [42], a Matlab toolbox based on the famous CAFFE [13] CNN implementation.", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "The detection window provided in [31] is used as the ground truth.", "startOffset": 33, "endOffset": 37}, {"referenceID": 29, "context": "We also tested our regression model on face landmark localization experiments, based on the work of [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "Since our regression network is small in scale, the L-BFGS [43] algorithm is used for fast convergence.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "complicated segmentation tasks such as the PASCAL VOC challenge [10] and neuron membrane segmentation [44] in medical images, with the segmented object as the ground truth.", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "complicated segmentation tasks such as the PASCAL VOC challenge [10] and neuron membrane segmentation [44] in medical images, with the segmented object as the ground truth.", "startOffset": 102, "endOffset": 106}, {"referenceID": 41, "context": "Also, our framework provides another view of classical features, such as SIFT and HoG [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "For example, the Deep Gaze I [36] uses ImageNet pre-trained models on", "startOffset": 29, "endOffset": 33}, {"referenceID": 42, "context": "such as the L1 norm, can offer some desired properties like sparsity [46, 47, 48, 49].", "startOffset": 69, "endOffset": 85}, {"referenceID": 43, "context": "such as the L1 norm, can offer some desired properties like sparsity [46, 47, 48, 49].", "startOffset": 69, "endOffset": 85}, {"referenceID": 44, "context": "such as the L1 norm, can offer some desired properties like sparsity [46, 47, 48, 49].", "startOffset": 69, "endOffset": 85}, {"referenceID": 17, "context": "effective degrees of freedom (DoF) of the network [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 45, "context": "The drop-out technique [50] is a famous way to prevent over-", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "fitting in neural networks; it is proved to be an adaptive L2 regularization [51] on networks, which alleviates the testFigure 6: Comparison with classical saliency models, under the AUC and sAUC performance metric.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Acquiring large data is sometimes not possible; data augmentation is often applied [11] to alleviate the requirement for training data.", "startOffset": 83, "endOffset": 87}], "year": 2014, "abstractText": "The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures.", "creator": "Microsoft\u00ae Word 2010"}}}