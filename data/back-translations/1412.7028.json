{"id": "1412.7028", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Joint RNN-Based Greedy Parsing and Word Composition", "abstract": "The greedy parser and the compositional method are trained together and are closely interdependent. The compositional procedure produces a vector representation that syntactically (parsing tags) and semantically (words) summarizes sub-trees. Composition and tagging are achieved over continuous (word or tag) representations and recurring neural networks. We achieve an equal performance with known existing parsers, while thanks to the greedy nature of the parser we have the advantage of speed.", "histories": [["v1", "Mon, 22 Dec 2014 15:40:31 GMT  (257kb,D)", "https://arxiv.org/abs/1412.7028v1", "Under review as a conference paper at ICLR 2015"], ["v2", "Thu, 25 Dec 2014 17:39:39 GMT  (260kb,D)", "http://arxiv.org/abs/1412.7028v2", "Under review as a conference paper at ICLR 2015"], ["v3", "Thu, 8 Jan 2015 15:04:34 GMT  (260kb,D)", "http://arxiv.org/abs/1412.7028v3", "Under review as a conference paper at ICLR 2015"], ["v4", "Fri, 10 Apr 2015 21:57:49 GMT  (273kb,D)", "http://arxiv.org/abs/1412.7028v4", "Published as a conference paper at ICLR 2015"]], "COMMENTS": "Under review as a conference paper at ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["jo\\\"el legrand", "ronan collobert"], "accepted": true, "id": "1412.7028"}, "pdf": {"name": "1412.7028.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jo\u00ebl Legrand", "Ronan Collobert"], "emails": ["joel.legrand@idiap.ch", "ronan@collobert.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In Natural Language Processing (NLP), the parsing task aims at analysing the underlying syntactic structure of a natural language sequence of words (a sentence). The analysis is expressed as a tree of syntactic relations between sub-constituents of the sentence. In the linguistic world, Chomsky (1956) first introduced formally the parsing task, by defining the natural language syntax as a set of context-free grammar rules (a particular type of formal grammar), combined with transformations rules. Automated syntactic parsing became rapidly a key task in computational linguistic. A parse tree not only carries syntax information, but might also embed some semantic information (in the sense that it can disambiguate different interpretations of a given sentence). In that respect, parsing it has been widely used as an input feature for several other NLP tasks such as machine translation (Zollmann & Venugopal, 2006), information retrieval (Alonso et al., 2002), or Semantic Role Labeling (Punyakanok et al., 2008).\nThis paper introduces a greedy parser which leverages a new composition approach to keep an history of what has been predicted so far. The composition performs a syntactic and semantic summary of the contents of a sub-tree in the form of a vector representation. The composition is performed along the tree: bottom tree node representations are obtained by composing continuous word vector representations, and produces vector representations which are in turn composed together in subsequent nodes of the tree. The composition operation as well as tree node tagging and predictions are achieved with a Recurrent Neural Network (RNN). Both the composition and node prediction are trained jointly.\nSection 2 presents several related approaches. Section 3 details our parsing architecture. An empirical evaluation of our models as well as our compositional vectors is given in Section 4.\n\u2217All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI Research.\n1The parser can be downloaded at joel-legrand.fr/parser.\nar X\niv :1\n41 2.\n70 28\nv4 [\ncs .L\nG ]\n1 0\nA pr"}, {"heading": "2 RELATED WORK", "text": "The first attempts to automatically parse natural language were mainly conducted using generative models. A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000). These types of parsers model the syntactic grammar by computing statistics of simple grammar rules (over parsing tags) occurring in a training corpus. However, many language ambiguities cannot be caught with simple tag-based PCFG rules. A key element in the success of PCFGs is to refine the rules with a word lexicon. This is usually achieved by attaching to PCFGs a lexical information called the head-word. Several head-word variants exist, but they all rely on a deterministic procedure which leverages clever linguistic knowledge. Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986). These methods face a classical learning dilemma: on one hand PCFG rules have to be refined enough to avoid any ambiguities in the prediction. On the other hand, too much refinement in these rules implies lower occurrences in the training set, and thus a possible generalization issue. PCFGs-based parsers are thus judiciously composed with carefully chosen PCFG rules and clever regularization tricks."}, {"heading": "2.1 STATE-OF-THE-ART", "text": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them. This method leverages an efficient coarse-to-fine procedure to speed up the decoding process. More recently, Finkel et al. (2008); Petrov & Klein (2008) proposed PCFG-based discriminative parsers reaching the performance of their generative counterparts. Conditional Random Fields (CRFs) are at the core of such approaches. Carreras et al. (2008) currently holds the state-of-the-art among the (non-reranking) discriminative parsers. Their parser leverages a global-linear model (instead of a CRF) with PCFGs, together with various new advanced features. Z. et al. (2010) showed that jointly using multiple self-trained grammars can achieve higher accuracy than an individual grammar.\nIn contrast to these existing approaches, our parser does not rely on PCFGs, nor on refined features like head-words. Tagging nodes is achieved in a greedy manner, using only raw words and part-ofspeech (POS) as features. Tree node history is maintained as a vector representation obtained in a recurrent fashion, by composing past node representations and tag predictions."}, {"heading": "2.2 GREEDY PARSING", "text": "Many discriminative parsers follows a greedy strategy because of the lack (or the intractability) of a global tree score for an entire derivation path which would combine independent node decisions. Adopting a greedy strategy that maximize local scores for individual decisions is then a solution worth investigating. One of the first successful discriminative parsers (Ratnaparkhi, 1999) was based on MaxEnt classifiers (trained over a large number of different features) and powered a greedy shiftreduce strategy.\nHenderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures.\nTurian & Melamed (2006) later proposed a bottom-up greedy algorithm following a left-to-right or a right-to left strategy and using using a feature boosting approach. In this approach, greedy decisions regarding the tree construction are made using decision tree classifiers. Their model was nevertheless limited to short length sentences.\nZhu et al. (2013) proposed a shift-reduce parser which achieves results comparable to their chartbased counterparts. This is done by leveraging several unsupervisely trained features (word Brown\nclustering, dependency relations, dependency language model) combined with a smart beam search strategy."}, {"heading": "2.3 PARSING WITH RECURRENT NEURAL NETWORKS", "text": "Recurrent Neural Networks (RNNs) were seen very early (Elman, 1991) as a way to tackle the problem of parsing, as they can naturally recur along the parse tree. A first practical application of RNN on syntactic parsing were proposed by Costa et al. (2002). Their approach was based on a leftto-right incremental parser, where a recursive neural network was used to re-rank possible phrase attachments. The goal of their contribution was, in their own terms, the assessment of a methodology rather than a fully functional system. They demonstrated that RNNs were able to capture enough information to make correct parsing decisions.\nCollobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged).\nSocher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition.\nLegrand & Collobert (2014) proposed a greedy RNN-based parser. The neural network was recurrent only in the sense it used previously predicted tags to produce next tree node tags. Contrary to Socher et al. (2013), it did not involve composing sub-tree representations. Instead, head-words were used as a key feature. Our approach shares some similarities with (Legrand & Collobert, 2014), as it is also a greedy parser based on RNNs. However, instead of relying on head-words (which could be seen as a simplistic representations of sub-trees), we leverage compositional sub-tree vector representations trained jointly with the parser. This approach leads to much better parsing accuracy, while relying only on a few simple features (words and POS tags). Our model has also the ability of producing phrase embeddings, which may represent a valuable feature for other NLP tasks.\nChen & Manning (2014) proposed a greedy transition-based dependency parser based on neural networks, fed with dense word and tag vector representations. In contrast to our approach, it does not integrate a compositional procedure over sentence sub-trees. The network is only involved in predicting correct transitions at each step of the parsing process."}, {"heading": "3 GREEDY RNN PARSING", "text": "Our parser is based on a neural network tagger, and perform parsing in a greedy recurrent way. Our approach is a bottom-up iterative procedure: the tree is constructed starting from the terminal nodes (sentence words), as shown in Figure 1. At each iteration,\n1. We look for all possible new tree nodes merging input constituents (i.e., heads of the trees predicted so far or leaves which have not been composed so far). For that purpose, we apply a neural network (see Figure 3) sliding window tagger over input constituents X1, . . . , XN . Considering an arbitrary rule\nA\u2192 Xi, Xi+1, . . . , Xj\ndefining a new node with tag A, the tagger will produce prefixed tags B-A, I-A, . . . E-A, respectively for constituents Xi, Xi+1, . . . , Xj , following a classical BIOES prefixing scheme2.\n2. A simple dynamic programming is performed, only to insure the coherence of the tag prediction (e.g., a B-A can be followed only by a I-A or a E-A).\n3. A (neural network) composition module computes vector representations of the new nodes, according to the representations of the merged constituents, as well as the tag predictions (see Figure 2).\n4. New predicted nodes become input constituents and we go back to 1 (see Figure 1).\nOur system is recurrent in two ways: newly predicted parsing node labels as well as vector representations obtained by composing these predicted nodes, are used in the next iteration of our algorithm.\nWe will detail our architecture in the following."}, {"heading": "3.1 WORD EMBEDDINGS", "text": "Following the work from Collobert & Weston (2008) on various NLP tasks, our parser relies on raw words. Each word in a finite dictionary W , is assigned a continuous vector representation. These representations as all parameters of our architecture are trained by back-propagation. More formally, given a sentence of N words, w1, w2, ..., wN , each word wn \u2208 W is first embedded in a D-dimensional vector space by applying a lookup-table operation:\nLTW (wn) =Wwn ,\nwhere the matrix W \u2208 RD\u00d7|W| represents the parameters to be trained in this lookup layer. Each column Wn \u2208 RD corresponds to the vector embedding of the nth word in our dictionaryW . In this work, two kind of features are used to feed the networks: words (or word compositions) and POS tags T (or parsing tags P). As for words, a lookup-table associates each tag t in the finite set of tag T \u222a P with a continuous vector representation of size T . The output vectors of the different lookup-tables are simply concatenated to form the input of the next layer.\n2Begin, Intermediate, Other, End, Single. This approach is very often used in NLP, when one wants to rewrite a chunk (here node) prediction problem into a word tagging problem.\nUsing continuous word vectors as input allows us to take advantage of unsupervisely pre-trained word embeddings. Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al. (2013). In this paper, we chose to use the representations from Lebret & Collobert (2014), obtained by a simple PCA on a matrix of word co-occurrences."}, {"heading": "3.2 WORD-TAG COMPOSITION", "text": "At each step of the parsing procedure, we represents each node of the tree as a vector representation, which summarizes both the syntax (predicted POS or parsing tags) and the semantic (words) of the sub-tree corresponding to the given node. As shown in Figure 2, the vector representation is obtained by a simple recurrent procedure, which involves several components:\n\u2022 Word vector representations for the leaves (coming out from a lookup table) (dimension D).\n\u2022 Tag (POS for the leaves, predicted tags otherwise) vector representations (also coming out for another lookup table, as explained in Section 3.1) (dimension T ).\n\u2022 Compositional networks Ck(). Each of them can compress the representation of a chunk of size k into a D-dimensional vector.\nCompositional networks take as input both the merged node representations and predicted tag representations. There is one different network Ck for each possible node with a number of k merged constituent. In practice most tree nodes do not merge more than a few constituents3. In our case, denoting z \u2208 R(D+T )\u00d7k the concatenation of the merged constituent representations (k vectors of tags and constituent representations), the compositional network is simply a matrix-vector operation followed by a non-linearity\nCk(z) = h(M kz) ,\nwhere Mk \u2208 RD\u00d7(k(D+T )) is a matrix of parameters to be trained, and h() is a simple non-linearity such as a pointwise hyperbolic tangent.\n3Taking 1 \u2264 k \u2264 5 covers already 98.6% of the nodes in the Wall Street Journal training corpus, and 1 \u2264 k \u2264 7 covers 99.8%.\nNote that node and word representations are embedded in the same space. This way, the compositional networks Ck can compress indifferently information coming from leaves or sub-trees. Implementation-wise, one can store new node representations into the word lookup-table as the tree is created, such that subsequent composition or tagging operations can be achieved in an efficient manner."}, {"heading": "3.3 SLIDING WINDOW BIOES TAGGER", "text": "The tagging module of our architecture (see Figure 3) is a two-layer neural network which applies a sliding window over the input constituent representations (as computed in Section 3.2), as well as the input constituent tag representations. Considering N input constituents X1, . . . , XN , if we assume their respective representations has been stored so far in lookup tables, the nth window is defined as\nun = [LT (Xn\u2212K\u221212 ), ..., LT (Xn), ..., LT (Xn+K\u221212 )] ,\nwhere K is the size of window. Denoting P\u0303 the set of BIOES-prefixed parsing tags from P , the module outputs a vector of scores s(un) = [s1, ..., s|P\u0303|] (where st is the score of the BIOESprefixed parsing tag t \u2208 P\u0303 for the constituent Xn). The constituent with indices exceeding the input boundaries (n \u2212 (K \u2212 1)/2 < 1 or n + (K \u2212 1)/2 > N ) are mapped to a special padding vector (which is also learned). As any classical two-layer neural network, our architecture performs several matrix-vector operations on its inputs, interleaved with some non-linear transfer function h(\u00b7),\ns(un) =M2 h(M1 un) ,\nwhere the matrices M1 \u2208 RH\u00d7K|D| and M1 \u2208 R|P\u0303|\u00d7H are the trained parameters of the network, and h() is a pointwise non-linear function such as the hyperbolic tangent. The number of hidden units H is a hyper-parameter to be tuned."}, {"heading": "3.4 COHERENT BIOES PREDICTIONS", "text": "The next module of our architecture aggregates the BIOES-prefixed parsing tags from our tagger module in a coherent manner. It is implemented as a Viterbi decoding algorithm over a constrained graph G, which encodes all the possible valid sequences of BIOES-prefixed tags over constituents: e.g. B-A tags can only be followed by I-A or E-A tags, for any parsing label A. Each node of the graph is assigned a score produced by the previous neural network module (score for each BIOES-prefixed tag, and for each word). The score S([t]N1 , [X] N 1 , \u03b8) for a sequence of tags [t] N 1 in the lattice G is simply obtained by summing scores along the path ([X]N1 being the input sequence of constituents and \u03b8 all the parameters of the model). We followed the exact same approach as in (Legrand & Collobert, 2014), except that transition scores (edges on the graph) were all set to zero. Indeed, we observed in empirical experiments that adding transitions scores does not improve F1score performance. This decoding is thus present only to insure coherence in the predicted sequence of tags."}, {"heading": "3.5 TRAINING PROCEDURE", "text": "Both the composition network and tagging networks are trained by maximizing a likelihood over the training data using stochastic gradient ascent.\nWe performed all possible iterations, over all training sentences, of the greedy procedure presented in Figure 1 constrained with the provided labeled parse tree. This leads to our training set of sequences of tree nodes. While this procedure is similar to (Legrand & Collobert, 2014), it is worth mentioning that it implies the system is only trained on correct sequences of tree nodes. In that respect, it is not trained to recover from past mistakes it could have made during the recurrent process. For every tree node, the sub-trees (structure and tags) were also extracted during this procedure.\nTraining the system consists in repeating the following steps:\n\u2022 Pick a random sequence of nodes extracted in the training set, as described above. Consider the associated sub-trees for each node which is not a leaf.\n\u2022 Perform a forward pass of the word-tag composer (see Section 3.2) along these sub-trees.\n\u2022 For all nodes in the sequence, perform a forward pass of the tagger according to word (or sub-tree) representations, as well as constituent tags. \u2022 Compute a likelihood of the right sequence of BIOS-prefixed tags (see below), given the\nscores of the tagger. \u2022 Backward gradient through the tagger up to the word (or sub-tree) and tag representations. \u2022 Backward gradient through the word-tag composer up to the word and tag representation. \u2022 Update all model parameters (from compositional networksCi, tagger network, and lookup\ntables) with a fixed learning rate.\nDetails about the training likelihood can be found in (Legrand & Collobert, 2014). The score S([t]N1 , [X] N 1 , \u03b8) of the true sequence of BIOS-prefixed tags [t] N 1 , given the input node sequence [X]N1 can be interpreted as a conditional probability by exponentiating this score (thus making it positive) and normalizing it with respect to all possible path scores. The log-probability of a sequence of tags [t]N1 for the input sequence of constituents [X] N 1 is given by:\nlogP ([t]N1 |[X]N1 , \u03b8) = S([t]N1 , [X]N1 , \u03b8) (1)\n\u2212 log\n  \u2211\n\u2200[t\u2032]N1\nexpS([t\u2032]N1 , [X] N 1 , \u03b8)\n  .\nThe second term of this equation (which correspond to the normalisation term) can be computed in linear time thanks to a recursion similar to the Viterbi algorithm (see Rabiner, 1989). Similar training procedures have been proposed in the past for structured data (Denker & Burges, 1995; Bottou et al., 1997; Lafferty et al., 2001)."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 CORPUS", "text": "Experiments were conducted using the standard English Penn Treebank data set (Marcus et al., 1993). We adopted the classical setup, with sections 02-21 for train, section 22 for validation, and section 23 for test. The validation corpus was used to select our hyper-parameters and best models.\nWe pre-processed the data only with a subset of operations which are achieved in standard parsers: (1) functional labels, traces were removed, (2) the PRT label was replaced by ADVP (Magerman, 1995). (3) We tackled the unary chain issue - non-terminals with a single non-terminal child - by merging the nodes together and assigning as tag the concatenation of the merged node tags. This was done in order to avoid looping issues in the parsing algorithm (e.g. a node being repetitively tagged with two different tags in our iterative process) and ensure the convergence of the parsing process. Only concatenated labels which occurred at least 30 times (corresponding to the lowest number of occurrences of the less common original parsing tag) were kept, leading to 11 additional parsing tags. Added to the original 26 parsing tags, this resulted in 161 tags produced by our parser. At test time, the inverse operation is performed: concatenated tag nodes are simply expanded into their original form."}, {"heading": "4.2 DETAILED SETUP", "text": "Our systems were trained using a stochastic gradient descent over the available training data until convergence on the validation set. Hyper-parameters were chosen according to the validation. Lookup-table sizes for the words and tags (part-of-speech and parsing) are 200 and 20, respectively. The window size for the tagger is K = 7 (3 neighbours from each side). The size of the tagger\u2019s hidden layer is H = 500. We used the word embeddings obtained from Lebret & Collobert (2014) to initialize the word lookup-table. These embeddings were then fine-tuned during the training process. We fixed the learning rate to \u03bb = 0.15 during the stochastic gradient procedure. As suggested in Plaut & Hinton (1987), the learning rate was divided by the size of the input vector of each layer. The part-of-speech tags were obtained using the freely available software SENNA4.\n4http://ml.nec-labs.com/senna"}, {"heading": "4.3 WORD EMBEDDING DROPOUT REGULARIZATION", "text": "We found that our system was easily subject to overfitting (training F1-score increasing while the validation curve was eventually decreasing as shown in Figure 4). As the capacity of our network mainly lies on the words and tag embeddings, we adopted a dropout regularization strategy (see Hinton et al., 2012) for the lookup tables. The key idea of the dropout regularization is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. In our case, during the training phase, a \u201cdropout mask\u201d is applied to the output of the lookup-tables: each element of the output is set to 0 with a probability 0.25. At test time, no patch is applied but the output is re-weighted, scaling it by 0.75. We observed a good improvement in F1-score performance, as shown in Figure 4."}, {"heading": "4.4 PERFORMANCE COMPARISON", "text": "F1 performance scores are reported in Table 1. Scores were obtained using the Evalb implementation5. We compared our system is compared with a range of different state-of-the-art parsers. In addition to the the four main generative parsers, we report the scores of well known re-ranking parsers (including the state-of-the-art from McClosky et al. (2006)) as well as for two major purely discriminative parsers. Detailed error analysis compared against a subset of these parsers is reported in Table 2, using the code provided by Kummerfeld et al. (2012). Performance with respect to sentence length is reported in Figure 5.\nWe included a voting procedure using several models trained starting from different random initializations. The voting procedure is achieved in the following way: at each iteration of the greedy parsing procedure, given the input sequence of constituents, (1) node representations are computed for each model by composing the sub-tree representations corresponding to the given model and using its own compositional network (2) each model computes tag scores using its own tagger network (3) tag scores are averaged (4) a coherent path of tag is predicted using the Viterbi algorithm.\nFinally, we report a brief quantitative evaluation of our compositional representations in Table 3. Random phrases were picked in the WSJ corpus, and closest neighbors (according to the Euclidean distance) with other phrases of the corpus are reported.\n5Available at http://nlp.cs.nyu.edu/evalb"}, {"heading": "5 CONCLUSION", "text": "In this paper, we introduced a new parsing architecture which leverages RNN-based compositional representation of parsing sub-trees, both encoding the syntactic (tags) and semantic (words) information. The parsing procedure is tightly integrated with the composition operation, and allows us to reach performance of very well-known parsers while (1) adopting a greedy and fast procedure (2) avoid standard refined features such as headwords."}, {"heading": "ACKNOWLEDGMENTS", "text": "Part of this work was supported by NEC Laboratories America."}], "references": [{"title": "On the usefulness of extracting syntactic dependencies for text indexing", "author": ["M.A. Alonso", "J. Vilares", "V.M. Darriba"], "venue": "In Artificial Intelligence and Cognitive Science", "citeRegEx": "Alonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2002}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. LeCun", "Y. Bengio"], "venue": "In Proc. of Computer Vision and Pattern Recognition,", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["X. Carreras", "M. Collins", "T. Koo"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "A maximum-entropy-inspired parser", "author": ["E. Charniak"], "venue": "In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference,", "citeRegEx": "Charniak,? \\Q2000\\E", "shortCiteRegEx": "Charniak", "year": 2000}, {"title": "Coarse-to-fine N-best parsing and MaxEnt discriminative reranking", "author": ["E. Charniak", "M. Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak and Johnson,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRE Transactions on Information Theory,", "citeRegEx": "Chomsky,? \\Q1956\\E", "shortCiteRegEx": "Chomsky", "year": 1956}, {"title": "Head-driven statistical models for natural language parsing", "author": ["M. Collins"], "venue": "Comput. Linguist.,", "citeRegEx": "Collins,? \\Q2003\\E", "shortCiteRegEx": "Collins", "year": 2003}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "In AISTATS,", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Towards incremental parsing of natural language using recursive neural networks", "author": ["F. Costa", "P. Frasconi", "V. Lombardo", "G. Soda"], "venue": null, "citeRegEx": "Costa et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Costa et al\\.", "year": 2002}, {"title": "Image segmentation and recognition", "author": ["J.S. Denker", "C.J.C. Burges"], "venue": "The Mathematics of Induction,", "citeRegEx": "Denker and Burges,? \\Q1995\\E", "shortCiteRegEx": "Denker and Burges", "year": 1995}, {"title": "An efficient context-free parsing algorithm", "author": ["J. Earley"], "venue": null, "citeRegEx": "Earley,? \\Q1970\\E", "shortCiteRegEx": "Earley", "year": 1970}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["J.L. Elman"], "venue": "Mach. Learn.,", "citeRegEx": "Elman,? \\Q1991\\E", "shortCiteRegEx": "Elman", "year": 1991}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["J.R. Finkel", "A. Kleeman", "C.D. Manning"], "venue": "In In Proc. ACL/HLT,", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["J. Henderson"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "Henderson,? \\Q2003\\E", "shortCiteRegEx": "Henderson", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser", "author": ["J. Henderson"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Henderson,? \\Q2004\\E", "shortCiteRegEx": "Henderson", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "G E", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An efficient recognition and syntax analysis algorithm for context-free languages", "author": ["T. Kasami"], "venue": "Technical report,", "citeRegEx": "Kasami,? \\Q1965\\E", "shortCiteRegEx": "Kasami", "year": 1965}, {"title": "Readings in natural language processing. chapter Algorithm Schemata and Data Structures in Syntactic Processing", "author": ["M. Kay"], "venue": null, "citeRegEx": "Kay,? \\Q1986\\E", "shortCiteRegEx": "Kay", "year": 1986}, {"title": "Parser showdown at the wall street corral: An empirical investigation of error types in parser output", "author": ["J.K. Kummerfeld", "D. Hall", "J.R. Curran", "D. Klein"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Kummerfeld et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Eighteenth International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Word embeddings through hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Recurrent greedy parsing with neural networks", "author": ["J. Legrand", "R. Collobert"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Legrand and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Legrand and Collobert", "year": 2014}, {"title": "Statistical decision-tree models for parsing", "author": ["D.M. Magerman"], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Magerman,? \\Q1995\\E", "shortCiteRegEx": "Magerman", "year": 1995}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "In that respect, parsing it has been widely used as an input feature for several other NLP tasks such as machine translation (Zollmann & Venugopal, 2006), information retrieval (Alonso et al., 2002), or Semantic Role Labeling (Punyakanok et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 5, "context": "In the linguistic world, Chomsky (1956) first introduced formally the parsing task, by defining the natural language syntax as a set of context-free grammar rules (a particular type of formal grammar), combined with transformations rules.", "startOffset": 25, "endOffset": 40}, {"referenceID": 24, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 7, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 3, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 18, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 12, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 19, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 12, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser.", "startOffset": 31, "endOffset": 48}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser.", "startOffset": 49, "endOffset": 75}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset.", "startOffset": 49, "endOffset": 328}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them.", "startOffset": 49, "endOffset": 610}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them. This method leverages an efficient coarse-to-fine procedure to speed up the decoding process. More recently, Finkel et al. (2008); Petrov & Klein (2008) proposed PCFG-based discriminative parsers reaching the performance of their generative counterparts.", "startOffset": 49, "endOffset": 824}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them. This method leverages an efficient coarse-to-fine procedure to speed up the decoding process. More recently, Finkel et al. (2008); Petrov & Klein (2008) proposed PCFG-based discriminative parsers reaching the performance of their generative counterparts.", "startOffset": 49, "endOffset": 847}, {"referenceID": 2, "context": "Carreras et al. (2008) currently holds the state-of-the-art among the (non-reranking) discriminative parsers.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Carreras et al. (2008) currently holds the state-of-the-art among the (non-reranking) discriminative parsers. Their parser leverages a global-linear model (instead of a CRF) with PCFGs, together with various new advanced features. Z. et al. (2010) showed that jointly using multiple self-trained grammars can achieve higher accuracy than an individual grammar.", "startOffset": 0, "endOffset": 248}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures. Turian & Melamed (2006) later proposed a bottom-up greedy algorithm following a left-to-right or a right-to left strategy and using using a feature boosting approach.", "startOffset": 0, "endOffset": 285}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures. Turian & Melamed (2006) later proposed a bottom-up greedy algorithm following a left-to-right or a right-to left strategy and using using a feature boosting approach. In this approach, greedy decisions regarding the tree construction are made using decision tree classifiers. Their model was nevertheless limited to short length sentences. Zhu et al. (2013) proposed a shift-reduce parser which achieves results comparable to their chartbased counterparts.", "startOffset": 0, "endOffset": 619}, {"referenceID": 13, "context": "Recurrent Neural Networks (RNNs) were seen very early (Elman, 1991) as a way to tackle the problem of parsing, as they can naturally recur along the parse tree.", "startOffset": 54, "endOffset": 67}, {"referenceID": 9, "context": "A first practical application of RNN on syntactic parsing were proposed by Costa et al. (2002). Their approach was based on a leftto-right incremental parser, where a recursive neural network was used to re-rank possible phrase attachments.", "startOffset": 75, "endOffset": 95}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner.", "startOffset": 0, "endOffset": 169}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner.", "startOffset": 0, "endOffset": 453}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner.", "startOffset": 0, "endOffset": 518}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN).", "startOffset": 0, "endOffset": 698}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser.", "startOffset": 0, "endOffset": 1520}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser. The neural network was recurrent only in the sense it used previously predicted tags to produce next tree node tags. Contrary to Socher et al. (2013), it did not involve composing sub-tree representations.", "startOffset": 0, "endOffset": 1706}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser. The neural network was recurrent only in the sense it used previously predicted tags to produce next tree node tags. Contrary to Socher et al. (2013), it did not involve composing sub-tree representations. Instead, head-words were used as a key feature. Our approach shares some similarities with (Legrand & Collobert, 2014), as it is also a greedy parser based on RNNs. However, instead of relying on head-words (which could be seen as a simplistic representations of sub-trees), we leverage compositional sub-tree vector representations trained jointly with the parser. This approach leads to much better parsing accuracy, while relying only on a few simple features (words and POS tags). Our model has also the ability of producing phrase embeddings, which may represent a valuable feature for other NLP tasks. Chen & Manning (2014) proposed a greedy transition-based dependency parser based on neural networks, fed with dense word and tag vector representations.", "startOffset": 0, "endOffset": 2392}, {"referenceID": 8, "context": "Following the work from Collobert & Weston (2008) on various NLP tasks, our parser relies on raw words.", "startOffset": 24, "endOffset": 50}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al. (2013). In this paper, we chose to use the representations from Lebret & Collobert (2014), obtained by a simple PCA on a matrix of word co-occurrences.", "startOffset": 67, "endOffset": 116}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al. (2013). In this paper, we chose to use the representations from Lebret & Collobert (2014), obtained by a simple PCA on a matrix of word co-occurrences.", "startOffset": 67, "endOffset": 199}, {"referenceID": 1, "context": "Similar training procedures have been proposed in the past for structured data (Denker & Burges, 1995; Bottou et al., 1997; Lafferty et al., 2001).", "startOffset": 79, "endOffset": 146}, {"referenceID": 21, "context": "Similar training procedures have been proposed in the past for structured data (Denker & Burges, 1995; Bottou et al., 1997; Lafferty et al., 2001).", "startOffset": 79, "endOffset": 146}, {"referenceID": 25, "context": "Experiments were conducted using the standard English Penn Treebank data set (Marcus et al., 1993).", "startOffset": 77, "endOffset": 98}, {"referenceID": 24, "context": "We pre-processed the data only with a subset of operations which are achieved in standard parsers: (1) functional labels, traces were removed, (2) the PRT label was replaced by ADVP (Magerman, 1995).", "startOffset": 182, "endOffset": 198}, {"referenceID": 8, "context": "We used the word embeddings obtained from Lebret & Collobert (2014) to initialize the word lookup-table.", "startOffset": 51, "endOffset": 68}, {"referenceID": 8, "context": "We used the word embeddings obtained from Lebret & Collobert (2014) to initialize the word lookup-table. These embeddings were then fine-tuned during the training process. We fixed the learning rate to \u03bb = 0.15 during the stochastic gradient procedure. As suggested in Plaut & Hinton (1987), the learning rate was divided by the size of the input vector of each layer.", "startOffset": 51, "endOffset": 291}, {"referenceID": 20, "context": "Detailed error analysis compared against a subset of these parsers is reported in Table 2, using the code provided by Kummerfeld et al. (2012). Performance with respect to sentence length is reported in Figure 5.", "startOffset": 118, "endOffset": 143}], "year": 2015, "abstractText": "This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which summarizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. We provide a fully functional implementation of the method described in this paper 1.", "creator": "LaTeX with hyperref package"}}}