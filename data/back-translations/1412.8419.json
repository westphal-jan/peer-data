{"id": "1412.8419", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2014", "title": "Simple Image Description Generator via a Linear Phrase-Based Approach", "abstract": "In this paper, we present a simple model that is capable of generating sentences from an example image. Our model learns to embed images (generated from a previously trained Convolutionary Neural Network) in a multimodal space common to the images and the phrases with which they are described. It is able to derive phrases from a given image sample. Based on the sentence descriptions, we propose a simple language model that is capable of generating relevant descriptions for a given test image based on the characteristic representation and the phrases derived from it. We achieve promising initial results on the recently published COCO dataset.", "histories": [["v1", "Mon, 29 Dec 2014 18:43:10 GMT  (1422kb,D)", "https://arxiv.org/abs/1412.8419v1", null], ["v2", "Wed, 18 Mar 2015 05:09:13 GMT  (742kb,D)", "http://arxiv.org/abs/1412.8419v2", "Under review as a workshop paper at ICLR 2015"], ["v3", "Sat, 11 Apr 2015 03:53:26 GMT  (1591kb,D)", "http://arxiv.org/abs/1412.8419v3", "Accepted as a workshop paper at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.NE", "authors": ["remi lebret", "pedro o pinheiro", "ronan collobert"], "accepted": true, "id": "1412.8419"}, "pdf": {"name": "1412.8419.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["R\u00e9mi Lebret", "Pedro O. Pinheiro"], "emails": ["remi@lebret.ch,", "pedro@opinheiro.com", "ronan@collobert.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Being able to automatically generate a description from an image is a fundamental problem in artificial intelligent, connecting computer vision and natural language processing. The problem is particularly challenging because it requires to correctly recognize different objects in images and also how they interact.\nConvolutional Neural Networks (CNN) have achieved state of the art results in different computer vision tasks in the last few years. More recently, different authors proposed automatic image sentence description approaches based on deep neural networks. All the solutions use the representation of images generated by CNN that was previously trained for object recognition tasks as start point.\nVinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose a encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Kiros et al. (2014) also consider a encoder/decoder pipeline, but uses a combination of CNN and LSTM networks for encoding and a language model for decoding. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional recurrent neural networks over sentences and a structured objective responsible for a multimodal embedding. They propose a second recurrent neural network architecture to generate new sentences. Similar to the previous works, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a deep recurrent neural network for sentences. The two networks interact with each other in a multimodal common layer.\n\u2217These two authors contributed equally to this work. \u2020All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI\nResearch.\nar X\niv :1\n41 2.\n84 19\nv3 [\ncs .C\nL ]\n1 1\nA pr\nFang et al. (2014) propose a different approach to the problem that does not rely on recurrent neural networks. Their solution can be divided into three steps: (i) visual detector for words that commonly occur are trained using multiple instance learning, (ii) a set of sentences are generated using a Maximum-Entropy language-model and (iii) the sentences are re-ranked using sentence-level features and a proposed deep multimodal similarity model.\nThis paper proposes a different approach to the problem. We propose a system that at the same time: (i) automatically generates a sentence describing a given scene and (ii) is relatively simpler than the recently proposed approaches. Our model shares some similarities with previously proposed deep approaches. For instance, we also use a pre-trained CNN to extract image features and we also consider a multimodal embedding. However, thanks to the phrase-based approach, we do not use any complex recurrent network for sentence generation.\nWe represent the ground-truth sentences as a collection of noun, verb and prepositional phrases. Each phrase is represented by the mean of the vector representation of the words that compose it. We then train a simple linear embedding model that transform an image representation into a multimodal space that is common to the image and the phrases that are used to describe them. To automatically generate sentences in inference time, we (i) infer the phrases that correspond to the sample image and (ii) use a simple language model based on the statistics of the ground-truth sentences present in the corpus."}, {"heading": "2 PHRASE-BASED MODEL FOR IMAGE DESCRIPTIONS", "text": ""}, {"heading": "2.1 UNDERSTANDING STRUCTURES OF IMAGE DESCRIPTIONS", "text": "The art of writing sentences can vary a lot according to the domain it is being applied. When reporting news or reviewing an item, not only the choice of the words might vary, but also the general structure of the sentence. Sentence structures used for describing images can therefore be identified.\nThey possess a very distinct structure, usually describing the different objects present on the scene and how they interact between each other. This interaction among objects is described as actions or relative position between different objects. The sentence can be short or long, but it generally respects this process. This statement is illustrated with the ground-truth sentence descriptions of the image in Figure 1.\nChunking-based approach All the key elements in a given image are usually described with a noun phrase (NP). Interactions between these elements can then be explained using prepositional phrases (PP) or verb phrases (VP). Describing an image is therefore just a matter of identifying these constituents to describe images. We propose to train a model which can predict the phrases which are likely to be in a given image.\nPhrase representations Noun phrases or verb phrases are often a combination of several words. Good word vector representations can be obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen. By leveraging the ability of these word vector representations to compose, representations for phrases are easily computed with an element-wise addition.\nFrom phrases to sentence After identifying the most likely constituents of the image, we propose to use a statistical language model to combine them and generate a proper description. A general framework is defined to reduce the total number of combination and thus speed up the process for generating sentences. The constrained language model used is illustrated in Figure 2. In general, a noun phrase is always followed by a verb phrase or a prepositional phrase, and both are then followed by another noun phrase. This process is repeated N times until reaching the end of a sentence (characterized by a period). This heuristic is based on the analysis of syntax if the sentences (see Section 3.1)."}, {"heading": "2.2 A MULTIMODAL REPRESENTATION", "text": "Image representations For the representation of images, we choose to use a Convolutional Neural Network. CNNs have been widely used for many different vision domains and are currently the state-of-the-art in many object recognition tasks. We consider a CNN that has been pre-trained for the task of object classification. We use a CNN solely to the purpose of feature extraction, that is, no learning is done in the CNN layers.\nLearning of a common space for image and phrase representations Let I be the set of training images, D the set of all sentence descriptions for I, C the set of all phrases occuring in D, and \u03b8 the trainable parameters of the model. Di is the set of sentences describing a given image i \u2208 I, and Cd is the set of phrases which compose a sentence description d \u2208 Di. The training objective is to find the phrases c that describe the images i by maximizing the log probability:\u2211\ni\u2208I \u2211 d\u2208Di \u2211 c\u2208Cd log p(c|i) (1)\nEach image i \u2208 I is represented by a vector xi \u2208 Rn thanks to a pre-trained CNN. Each phrase c is composed of K words w which are represented by a vector xw \u2208 Rm thanks to another pre-trained model for word representations. A vector representation zc for a phrase c = {w1, . . . , wK} is then calculated by averaging its word vector representations:\nzc = 1\nK K\u2211 k=1 xwk . (2)\nVector representations for all phrases c \u2208 C can thus be obtained to build a matrix V =[ zc1 , . . . , zc|C| ] \u2208 Rm\u00d7|C| . In general, m n. An encoding function is therefore defined to map image representations xi \u2208 Rn in the same vector space than phrase representations zc \u2208 Rm: g\u03b8(i) = xiU , (3) where U \u2208 Rn\u00d7m is initialized randomly and trained to encode images in the same vectorial space than the phrases used for their descriptions. Because representations of images and phrases are in a common vector space, similarities between a given image i and all phrases can be calculated:\nf\u03b8(i) = g\u03b8(i)V , (4)\nwhere V is fine-tuned to incorporate other features coming from the images. By denoting [f\u03b8(i)]j the score for the jth phrase, this score can be interpreted as the conditional probability p(c = cj |i, \u03b8) by applying a softmax operation over all the phrases:\np(c = cj |i, \u03b8) = e[f\u03b8(i)]j\u2211|C| k=1 e [f\u03b8(i)]k . (5)\nIn practice, this formulation is often impractical due to the large set of possible phrases C.\nTraining with negative sampling With \u03b8 = {U, V } and a negative sampling approach, we instead minimize the following logistic loss function with respect to \u03b8:\n\u03b8 7\u2192 \u2211 i\u2208I \u2211 d\u2208Di \u2211 cj\u2208Cd ( log ( 1 + e[f\u03b8(i)]j ) + N\u2211 k=1 log ( 1 + e\u2212[f\u03b8(i)]k )) . (6)\nThus the task is to distinguish the target phrase from draws from the noise distribution, where there areN negative samples for each data sample. The model is trained using stochastic gradient descent."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 EXPERIMENTAL SETUP", "text": "Dataset We validate our model on the recently proposed COCO dataset (Lin et al., 2014), which contains complex images with multiple objects. The dataset contains a total of 123,000 images, each of them with 5 human annotated sentences. The testing images has not yet been released. We thus use two sets of 5,000 images from the validation images for validation and test, as in Karpathy & Fei-Fei (2014)1. We measure the quality of of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002).\nFeature selection Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al., 2014). This model generates image representations of dimension 4096 form RGB input images. For sentence features, we extract phrases from the 576,737 training sentences with the SENNA software2. Statistics reported in Figure 3 confirm the hypothesis that image\n1Available at http://cs.stanford.edu/people/karpathy/deepimagesent/ 2Available at http://ml.nec-labs.com/senna/\ndescriptions have a simple syntactic structure. A large majority of sentences contain from two to four noun phrases. Two noun phrases then interact using a verb or prepositional phrase. Only phrases occuring at least ten times in the training set are considered. This results in 11,688 noun phrases, 3,969 verb phrases3 and 219 prepositional phrases. Phrase representations are then computed by averaging vector representations of their words. We obtained word vector representations from the Hellinger PCA of a word co-occurence matrix, following the method described in Lebret & Collobert (2014). The word co-occurence matrix is built over the entire English Wikipedia4, with a symmetric context window of ten words coming from the 10,000 most frequent words. Words, and therefore also phrases, are represented in 400-dimensional vectors.\nLearning multimodal representation The parameters \u03b8 are U \u2208 R4096\u00d7400 and V \u2208 R400\u00d715876. The latter is initialized with the phrase representations. They are trained with N = 15 negative samples and a learning rate set to 0.00025.\nGenerating sentences from the predicted phrases According to the statistics of ground-truth sentence structures, we set N = {2, 3, 4}. As nodes, we consider only the top twenty predicted noun phrases, the top ten predicted verb phrases and the top five predicted prepositional phrases. A trigram language model is used for the transition probabilities between two nodes. The probability of each lexical phrase is calculated using the previous phrases, p(cj |cj\u22122, cj\u22121), and the constraint described in Figure 2. In order to reduce the number of sentences generated, we just consider the transitions which are likely to happen (we discard any sentence which would have a trigram transition probability inferior to 0.01). This thresholding also helps to discard sentences that are semantically incorrect.\nRanking generated sentences Our final step consists on ranking the sentences generated and choosing the one with the highest score as the final output. For each test image i, we generate a set of M sentence candidates using the proposed language model. For each sentence sm (m \u2208 {1, ...,M}), we compute its vector representation zsm by averaging the representation of the phrases zc \u2208 V that make the sentence. The final score for each sentence sm is computed by doing a dot product between the sentence vector representation and the encoded representation of the sample image i:\nf\u03b8(i,m) = g\u03b8(i)zsm . (7)\nThe output of the system is the sentence which has the highest score. This ranking helps the system to chose the sentence which is closer to the sample image."}, {"heading": "3.2 EXPERIMENTAL RESULTS", "text": "Table 1 show our sentence generation results on the COCO dataset. BLEU scores are reported up to 4-grams. Human agreement scores are computed by comparing one of the ground-truth description against the others. For comparison, we include results from recently proposed models. Although we use the same test set as in Karpathy & Fei-Fei (2014), there are slight variations between the test sets chosen in other papers. Our model gives competitive results at all N-gram levels. It is interesting to note that our results are very close to the human agreement scores. Examples of full automatic generated sentences can be found in Figure 4."}, {"heading": "4 CONCLUSION AND FUTURE WORKS", "text": "In this paper, we propose a simple model that is able to automatically generate sentences from an image sample. Our model is considerably simpler than the current state of the art, which uses complex recurrent neural networks. We predict phrase components that are likely to describe a given image and use a simple statistical language model to generate sentences. Our model achieves promising first results. Future works include apply the model to different datasets (Flickr8k, Flickr30k and final COCO version for benchmarking), do image-sentence ranking experiments and improve the language model used.\n3Pre-verbal and post-verbal adverb phrases are merged with verb phrases. 4Available at http://download.wikimedia.org. We took the January 2014 version."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the HASLER foundation through the grant \u201cInformation and Communication Technology for a Better World 2020\u201d (SmartWorld)."}], "references": [{"title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue", "Jeff", "Hendricks", "Lisa Anne", "Guadarrama", "Sergio", "Rohrbach", "Marcus", "Venugopalan", "Subhashini", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "CoRR, abs/1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Neural Language Models. volume abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Rehabilitation of Count-based Models for Word Vector Representations", "author": ["Lebret", "Remi", "Collobert", "Ronan"], "venue": "CoRR, abs/1412.4930,", "citeRegEx": "Lebret et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan L"], "venue": "CoRR, abs/1410.1090,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "Dean", "Jeff"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "Kavukcuoglu", "Koray"], "venue": "In NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "Socher", "Richard", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Kiros et al. (2014) also consider a encoder/decoder pipeline, but uses a combination of CNN and LSTM networks for encoding and a language model for decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Kiros et al. (2014) also consider a encoder/decoder pipeline, but uses a combination of CNN and LSTM networks for encoding and a language model for decoding. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional recurrent neural networks over sentences and a structured objective responsible for a multimodal embedding.", "startOffset": 0, "endOffset": 184}, {"referenceID": 2, "context": "Kiros et al. (2014) also consider a encoder/decoder pipeline, but uses a combination of CNN and LSTM networks for encoding and a language model for decoding. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional recurrent neural networks over sentences and a structured objective responsible for a multimodal embedding. They propose a second recurrent neural network architecture to generate new sentences. Similar to the previous works, Mao et al. (2014) and Donahue et al.", "startOffset": 0, "endOffset": 492}, {"referenceID": 1, "context": "(2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a deep recurrent neural network for sentences.", "startOffset": 11, "endOffset": 33}, {"referenceID": 11, "context": "Good word vector representations can be obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014).", "startOffset": 100, "endOffset": 200}, {"referenceID": 7, "context": "Good word vector representations can be obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen.", "startOffset": 101, "endOffset": 225}, {"referenceID": 5, "context": "1 EXPERIMENTAL SETUP Dataset We validate our model on the recently proposed COCO dataset (Lin et al., 2014), which contains complex images with multiple objects.", "startOffset": 89, "endOffset": 107}, {"referenceID": 10, "context": "We measure the quality of of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002).", "startOffset": 102, "endOffset": 125}, {"referenceID": 0, "context": "Feature selection Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al., 2014).", "startOffset": 102, "endOffset": 126}, {"referenceID": 4, "context": "1 EXPERIMENTAL SETUP Dataset We validate our model on the recently proposed COCO dataset (Lin et al., 2014), which contains complex images with multiple objects. The dataset contains a total of 123,000 images, each of them with 5 human annotated sentences. The testing images has not yet been released. We thus use two sets of 5,000 images from the validation images for validation and test, as in Karpathy & Fei-Fei (2014)1.", "startOffset": 90, "endOffset": 424}, {"referenceID": 4, "context": "1 EXPERIMENTAL SETUP Dataset We validate our model on the recently proposed COCO dataset (Lin et al., 2014), which contains complex images with multiple objects. The dataset contains a total of 123,000 images, each of them with 5 human annotated sentences. The testing images has not yet been released. We thus use two sets of 5,000 images from the validation images for validation and test, as in Karpathy & Fei-Fei (2014)1. We measure the quality of of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002). Feature selection Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al.", "startOffset": 90, "endOffset": 607}, {"referenceID": 11, "context": "19 Vinyals et al. (2014) 0.", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "67 - Donahue et al. (2014) 0.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "67 - Donahue et al. (2014) 0.63 0.44 0.30 0.21 Fang et al. (2014) - - - 0.", "startOffset": 5, "endOffset": 66}], "year": 2015, "abstractText": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-theart models, achieves comparable results on the recently release Microsoft COCO dataset.", "creator": "LaTeX with hyperref package"}}}