{"id": "1412.8534", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2014", "title": "Disjunctive Normal Networks", "abstract": "Artificial neural networks are powerful pattern classifiers, but are surpassed by methods such as support vector machines and random forests, which are also easier to use and faster to train. Backpropagation, which is used to form artificial neural networks, suffers from the herd effect problem, which leads to long training times and limited classification accuracy. We use the disjunctive normal form and approach the Boolean connection operations with products to construct a novel network architecture. The proposed model can be trained by minimizing error function and allows an effective and intuitive initialization that solves the herd effect problem associated with back propagation, resulting in state-of-the-art classification accuracy and fast training times. In addition, our model can be optimized in a uniform structure together with revolutionary features, resulting in state-of-the-art results in computer vision problems with fast convergence rates.", "histories": [["v1", "Tue, 30 Dec 2014 02:17:30 GMT  (611kb,D)", "http://arxiv.org/abs/1412.8534v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["mehdi sajjadi", "mojtaba seyedhosseini", "tolga tasdizen"], "accepted": false, "id": "1412.8534"}, "pdf": {"name": "1412.8534.pdf", "metadata": {"source": "CRF", "title": "Disjunctive Normal Networks", "authors": ["Mehdi Sajjadi", "Mojtaba Seyedhosseini"], "emails": ["mehdi@sci.utah.edu"], "sections": [{"heading": null, "text": "1 Disjunctive Normal Networks Mehdi Sajjadi, Mojtaba Seyedhosseini and Tolga Tasdizen, Senior Member, IEEE,\nAbstract\u2014Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herdeffect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available\nI. INTRODUCTION\nAN artificial neural network (ANN) consisting of onehidden layer of squashing functions is an universal approximator for continuous functions defined on the unit hypercube [1], [2]. However, until the introduction of the backpropagation algorithm [3], training such multilayer perceptron (MLP) networks was not possible in practice. The backpropagation algorithm propelled MLPs to be the method of choice for many classification and regression applications. However, eventually MLPs were replaced by more recent techniques such as support vector machines (SVM) [4] and random forests (RF) [5]. In addition to being surpassed in accuracy by these modern techniques, an important drawback of MLPs has been the high computational cost of training emphasized by growing data set sizes and dimensionality. An underlying reason for the limited accuracy and high computational cost of training is the herd-effect problem [6]. During backpropagation each hidden unit tries to evolve into a useful feature detector from a random initialization; however, this task is complicated by the fact that all units are changing at the same time without any direct communication between them. Consequently, hidden units can not effectively subdivide the necessary computational tasks among themselves leading to a complex dance which can take a long time to settle down.\nIn this paper, we introduce a new network architecture that overcomes the difficulties associated with MLPs and backpropagation for supervised learning. Our network consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We call the proposed network architecture Logistic Disjunctive\nThe authors are with the Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, 84112 USA e-mail: mehdi@sci.utah.edu\nNormal Network (LDNN). Unlike MLPs, LDNNs allow for a simple and intuitive initialization of the network weights which avoids the herd-effect. Furthermore, due to the single adaptive layer, it allows larger step sizes in minimizing the error function. We also propose a deep learning structure which consists of automatic convolutional feature extractors and LDNNs as efficient classifiers. The proposed structure performs automatic feature extraction and classification simultaneously and in an unified structure. Finally, we present results of experiments on LDNN for general classification and image classification using proposed deep structure. For general classification, we conducted experiments on 10 binary and 6 multi-class classification problems. LDNNs outperformed MLPs in every case both in terms of accuracy and computational speed. LDNNs produced the best accuracy in 11 out of the 16 classification problems in comparison to SVMs and RFs. For image classification, we tested our deep structure on 5 popular datasets. Our model was able to achieve state-ofthe-art performance on 2 out of 5 datasets and competitive results on the rest."}, {"heading": "II. RELATED WORK", "text": "Extensive research has been performed on variants of the backpropagation algorithm including batch vs. stochastic learning [7], [8], squared error vs. cross-entropy [9] and optimal learning rates [10], [11]. Many other practical choices including normalization of inputs, initialization of weights, stopping criteria, activation functions, target output values that will not saturate the activation functions, shuffling training examples, momentum terms in optimization, and optimization techniques that make use of the second-order derivatives of the error are summarized in [12]. More recently, Hinton et al. proposed a Dropout scheme for backpropagation which helps prevent co-adaptation of feature detectors [13]. Despite the extensive effort devoted to making learning MLPs as efficient as possible, the fundamental problems outlined in Section I remain because they arise from the architecture of MLPs. Contrastive divergence [14], [15] can be used to pre-train networks in an unsupervised manner prior to backpropagation such that the herd-effect problem is alleviated. Contrastive divergence has been used successfully to train deep networks. The LDNN model proposed in this paper can be seen as an architectural alternative for supervised learning of ANNs.\nThe idea of representing classification functions in disjunctive form has been previously explored in the literature. Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space. The most important drawback of this model is its limitation to axis aligned decision boundaries which can significantly increase the number of conjunctions necessary\nar X\niv :1\n41 2.\n85 34\nv1 [\ncs .L\nG ]\n3 0\nD ec\n2 01\n4\n2 for a good approximation. We construct a significantly more efficient approximation by using an union of convex polytopes. Furthermore, fuzzy min-max neural networks employ an adhoc expansion-contraction scheme for learning, whereas we formulate learning as an energy minimization problem. Lu et al. [19] proposed a multi-sieving network that decomposes learning tasks. Lee et al. [20] proposed a disjunctive fuzzy network which is based on prototypes; however, it lacks an objective function and is based on an adhoc training procedure. Similarly, the modular network proposed by Lu and Ito [21] removes the axis aligned hypercube restriction from fuzzy min-max networks; however, their network can not be learned by minimizing a single energy function. Our LDNN model uses differentiable activation functions which makes it possible to optimize the network parameters in an unified manner by minimizing a single energy function. We show that unified training of our classifier results in very significant accuracy advantages over the modular network. Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques. A closely related approach to ours is adaptive mixtures of local experts which uses a gating network to stochastically select the output from a set of feedforward networks [26]. The reader is referred to [27] for a survey of mixture of expert methods. The products of experts approach models complex probability distributions by multiplying simpler distributions is also related [28].\nBesides the network approaches discussed in the previous paragraph, the idea of partitioning the decision space and learning simpler decision functions in each partition has been explored. Mixture discriminant analysis treats each class as a mixture of Gaussians and learns discriminants between the Gaussians [29]. Subclass discriminant analysis also relies on modeling classes as mixtures of Gaussians prior to learning discriminant [30]. Local linear discriminant analysis clusters the data and learns a linear discriminant in each cluster [31]. In these approaches partitioning of the space is treated as a step independent from the supervised learning step. Wang and Saligrama, proposed a more recent approach that unifies space partitioning and supervised learning [32]. While this method is related in concept to our disjunctive learning, in Section IV-C we show that LDNNs outperform space partitioning by a large margin. Dai et al. proposed an approach which places local classifiers close to the global decision boundary [33]. Toussaint and Vijayakumar propose a products-of-sigmoids model for discontinuously switching between local models [34]. Another approach greedily builds a piecewise linear classifier by adding classifiers in regions of error clusters [35]. Local versions of SVMs have also been explored [36], [37]. A specific type of local classification is based on the idea of pairwise coupling between positive and negative examples or clusters is conceptually close to the initialization we propose for our LDNN model. These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise\nclassifiers into a single decision [38], [39], [40], [41], [42], [43], [44]. The modular network [21] discussed previously also falls into this category.\nDespite the limitations mentioned earlier, artificial neural networks are the basis for highly successful Convolutional Neural Networks [45], [46] (ConvNet). ConvNets are special types of neural networks based on two properties: local connectivity and weight sharing. In general, a ConvNet consists of a few convolutional layers followed by one or more fully connected layers. The training process is done using error back-propagation all the way to the first layer. ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51]. They work best when large labeled data is available for training. For example, the state-ofthe-art results for large 1000-category ImageNet [52] dataset was significantly improved using ConvNets [53]. The main reason for this success is that ConvNets are strong feature learners for images. However, the classifier part in a ConvNet consists of one or a few layers of fully connected neural networks. These fully connected layers exhibit the limitations of MLPs including the herd-effect problem discussed earlier. There is a whole body of literature on the general task of classification in the past two decades that ConvNets simply cannot directly exploit in an unified structure because the classifier of choice is usually incompatible with learning by back-propagation and training the feature extractor and classifier independently does not lead to a structure with optimum performance. Usually, joint optimization of deep structures leads to a better solution or at least improves the result of layer-wise learning [14].\nIn this paper, we proposed a deep model which replaces fully connected layers with LDNN. We compared this model with state-of-the-art methods. These models are based on ConvNet. A notable and successful example is MCDNN proposed by Ciresan et al. [49]. In this model, they train multiple networks with slightly distorted and different inputs. The final classification is obtained by a voting scheme over probabilities of different networks.\nThe state-of-the-art results on many image classification datasets are achieved by DropConnect proposed by Wan et al. [50]. The idea of DropConnect is inspired by Dropout [13]. They randomly drop the connections between the nodes instead of dropping output nodes of intermediate layers. We compared our model to ConvNets that use DropConnect in their fully connected layers. We show that our method is able to achieve competitive results with fewer epochs and smaller training parameters in most of the cases. Another recent example proposed by Goodfellow et al. is Maxout Networks [51]. Instead of using an activation function over the output of a single node, they take the maximum output of a group of hidden nodes as the output. Here, the Max operator acts as an activation function. They also use a similar approach for convolutional layers. We provide comparisons with Maxout networks. But in general, they require significantly large networks because the output of a node or a convolutional map is determined by maximum of several input nodes or maps.\n3"}, {"heading": "III. METHODS", "text": ""}, {"heading": "A. Network Architecture", "text": "Consider the binary classification problem f : Rn \u2192 B where B = {0, 1}. Let \u2126+ = {x \u2208 Rn : f(x) = 1}. Lets approximate \u2126+ as the union of N convex polytopes \u2126\u0303+ = \u222aNi=1Pi where the i\u2019th polytope is the intersection Pi = \u2229Mij=1Hij of Mi half-spaces Hij = {x \u2208 Rn : hij(x) > 0}. We can replace Mi with M = maxiMi without loss of generality. Hij is defined in terms of its indicator function\nhij(x) =\n{ 1, \u2211n k=1 wijkxk + bij \u2265 0\n0, otherwise , (1)\nwhere wijk and bij are the weights and the bias term. Any Boolean function b : Bn \u2192 B can be written as a disjunction of conjunctions, also known as the disjunctive normal form [54]. Hence, we can construct the function\nf\u0303(x) = N\u2228 i=1  M\u2227 j=1 hij(x)  \ufe38 \ufe37\ufe37 \ufe38\nbi(x)\n(2)\nsuch that \u2126\u0303+ = {x \u2208 Rn : f\u0303(x) = 1}. Since \u2126\u0303+ is an approximation to \u2126+, it follows that f\u0303 is an approximation to f . Our next step is to provide a differentiable approximation to this disjunctive normal form. First, the conjunction of binary variables\n\u2227M j=1 hij(x) can be replaced by the product\u220fM\nj=1 hij(x). Then, using De Morgan\u2019s laws [54] we can replace the disjunction of the binary variables \u2228N i=1 bi(x)\nwith \u00ac \u2227N\ni=1 \u00acbi(x), which in turn can be replaced by the expression 1\u2212 \u220fN i=1(1\u2212 bi(x)). Finally, we can approximate the perceptrons hij(x) with the logistic sigmoid functions\n\u03c3ij(x) = 1 1 + e\u2212 \u2211n k=1 wijkxk+bij . (3)\nThis yields the differentiable approximation to f\u0303\nf\u0302(x) = 1\u2212 N\u220f i=1 (1\u2212 M\u220f j=1\n\u03c3ij(x)\ufe38 \ufe37\ufe37 \ufe38 gi(x) ), (4)\nwhich can also be visualized as a network (Figure 1). We refer to the proposed network architecture as LDNN. The only adaptive parameters of the LDNN are the weights and biases of the first layer of logistic sigmoid functions. The second layer consists of N soft NAND gates which implement the logical negations of the conjunctions gi(x) using products. The output layer is a single soft NAND gate which implements the disjunction using De Morgan\u2019s law. We will refer to a LDNN classifier which has N NAND gates in the second layer and M discriminants per NAND gate as a N \u00d7M LDNN. Note that other variations of disjunctive normal networks can be constructed by using any classifier that is differentiable with respect to its parameters in place of the logistic sigmoid functions."}, {"heading": "B. Model Initialization", "text": "Consider a set of training examples \u0393 = {(x, y(x))} where y(x) denotes the desired binary class corresponding to x. Let \u0393+ and \u0393\u2212 be the subsets of \u0393 for which y = 1 and y = 0, respectively. The disjunctive normal form permits a very simple and intuitive initialization of the network weights. To initialize a N \u00d7M LDNN, we first partition \u0393+ and \u0393\u2212 into N and M clusters, respectively. Let vij = c+i \u2212 c \u2212 j where c + i and c\u2212j are the centroids of the i\u2019th positive and j\u2019th negative clusters, respectively. We initialize the weight vectors as wij = vij/|vij|. Finally, we initialize the bias terms bij such that the logistic sigmoid functions \u03c3ij(x) take the value 0.5 at the midpoints of the lines connecting the positive and negative cluster centroids. In other words, let bij = \u3008wij , 0.5(c+i +c \u2212 j )\u3009 where \u3008a,b\u3009 denotes the inner product of the vectors a and b. This procedure initilizes gi(x), the i\u2019th conjunction in the second hidden layer of the LDNN, to a convex polytope which aims to separate the training instances in the i\u2019th cluster of \u0393+ from all training instances in \u0393\u2212. We give an intuitive description of LDNN initialization in the context of the two moons dataset. An illustration of this dataset and three clusters for each of the two classes are shown in (Figure 2a). Initial discriminants for the positive clusters taken one at a time are shown in (Figure 2b-d). The conjunction of these discriminants form convex polytopes for the positive clusters (Figure 2e-g). The disjunction of these conjunctions before and after weight optimization (Section III-C) are illustrated in (Figure 2h). This initialization procedure is similar to the modular neural network proposed by Lu and Ito (12) as well as to locally linear classifica-\n4 tion by pairwise coupling (20) in general. Each module in Lu and Ito\u2019s modular network independently learns a linear classifier between a pair of positive and negative training data clusters. The key difference of our classifier from Lu and Ito\u2019s network, as well as from locally linear classification by pairwise coupling in general, is that we learn all the linear discriminants simultaneously by minimizing a single error function. When each module is trained independently, the success of the initial clustering can strongly influence\nthe outcome. In Section IV, we show, using both real and artificial datasets, that this important disadvantage can create very significant differences in classification accuracy between modular networks and LDNNs."}, {"heading": "C. Model Optimization", "text": "The LDNN model can be trained by choosing the network weights and biases that minimize the quadratic error\nE(W,\u0393) = \u2211\n(x,y)\u2208\u0393\n(y \u2212 f(x))2 , (5)\nwhere f is determined by the set of network weights and biases W . Starting from an initialization as described in Section III-B, we minimize (5) using gradient descent. To derive the update equations we need to find the partial derivatives of the error with respect to the network weights and biases. Using the fact that \u2202\u03c3ij/\u2202wpqk is non-zero only when i = p and j = q, the derivatives of the error function with respect to the network weights is obtained using the chain rule\n\u2202E \u2202wijk = \u2202E \u2202f \u2202f \u2202gi \u2202gi \u2202\u03c3ij \u2202\u03c3ij \u2202wijk\n= \u22122(y \u2212 f(x)) \u220f r 6=i (1\u2212 gr(x)) \u00d7 \u220f\nl 6=j\n\u03c3il(x)  (\u03c3ij(x)(1\u2212 \u03c3ij(x))xk) = 2(f(x)\u2212 y)\n\u220f r 6=i (1\u2212 gr(x))  gi(x) (1\u2212 \u03c3ij(x))xk (6)\nSimilarly, we obtain the derivative of the error function with respect to the network biases as\n\u2202E \u2202bij = 2(f(x)\u2212 y) \u220f r 6=i (1\u2212 gr(x))  gi(x) (1\u2212 \u03c3ij(x)) (7)\nWe perform stochastic gradient descent after randomly permuting the order of the instances in \u0393 and updating the model weights and biases according to wnewijk = wijk\u2212\u03b1 \u2202E\u2202wijk , and bnewij = bij \u2212 \u03b1 \u2202E\u2202bij , respectively. The constant \u03b1 is the step size. This constitutes one epoch of training. Multiple epochs are performed until convergence as determined using a separate validation set. Notice that it is possible to achieve 0 training error for any finite training set \u0393 by letting each positive training instance and each negative training instance\na \u00a0 c \u00a0 d \u00a0\nh \u00a0\nb \u00a0\ng \u00a0 f \u00a0 e \u00a0\n1\n1\n1 1\n1\n1\n1\n1\n1\n0\n0 0 0\n0 0\n0\n0 0\n1 0\n1 0\n1 0\nFig. 2. A binary classification problem: (a) positive and negative training examples partitioned into three clusters each; linear discriminants from each negative cluster to (b) the first positive cluster, (c) the second positive cluster and (d) the third positive cluster; the conjunction of the discriminants for (g) the first positive cluster, (f) the second positive cluster and (e) the third positive cluster; (h) the disjunction of the conjunctions before (blue) and after (red) gradient descent. The 1/0 pair on the sides of the discriminants represent the direction of the discriminant.\nrepresent a positive and negative cluster centroid, respectively. However, in practice, this is expected to lead to overfitting and poor generalization and typically a much smaller number of clusters than training instances is used."}, {"heading": "D. Deep learning with LDNN", "text": "1) Convolutional feature learning: We use X \u2217H for 2D convolution of X and H , and X ?H for 2D cross-correlation. We have the following equation for the forward pass of a convolutional layer:\nX lj = \u03c3( \u2211 i\u2208mlj X l\u22121i ? H l ij + b l j\n\ufe38 \ufe37\ufe37 \ufe38 Slj\n) (8)\nIn (8), l is the index of the convolutional layers and i and j are the indices of the layer maps. X l\u22121i are the maps of the layer l\u2212 1 and X lj are the maps of the layer l, mlj is the subset of maps in the layer l\u22121 that are connected to map j of layer l via filters H lij . Finally, \u03c3 is the activation function (e.g., logistic, ReLU). For the backward pass, we have the following sensitivity equations for the same convolutional layer:\n\u2202E \u2202Slj = \u2202E \u2202X lj \u25e6 \u03c3\u2032(Slj), \u2202E \u2202H lij = X l\u22121i ? \u2202E \u2202Slj ,\n\u2202E \u2202bj = \u2211 u,v \u2202E \u2202Slj ,\n\u2202E \u2202X l\u22121j = \u2202E \u2202Slj \u2217H lj (9)\nE is the error that we want to minimize. Here, the derivative of E with respect to matrices X , H and S is a matrix consisting of derivatives of E with respect to each element of that matrix. In the above equations \u25e6 is defined to be element-wise multiplication.\n2) proposed structure: As mentioned earlier, LDNN consists of one layer of learnable weights and two layers of fixed soft gates. The error is back-propagated through soft gates to update the learnable weights. It is also possible to calculate\n5 the sensitivities for the input vector:\n\u2202E \u2202xk = 2(f(x)\u2212 y) N\u2211 i=1 \u220f r 6=i (1\u2212 gr(x))gi(x)\u00d7\nM\u2211 j=1 (1\u2212 \u03c3ij(x))wijk, k = 1, . . . , n (10)\nThis allows us to train a convolutional feature extractor by back-propagating the error. In other words, we can seamlessly replace the fully-connected layers in ConvNet with LDNN. This combination is compatible because both ConvNet feature extractor and LDNN classifier are being trained using backpropagation via the chain rule. Our proposed structure is shown in Figure 3.\nFor the case of multiclass classification, we use multiple LDNNs (one for every class). In this setup, the backpropagated errors for all the LDNNs will be summed together to form the sensitivities for convolutional layers. Assuming that C is the number of classes:\n\u2202E \u2202xk = C\u2211 c=1 2(fc(x)\u2212 yc) N\u2211 i=1 \u220f r 6=i (1\u2212 gcr(x))gci(x)\nM\u2211 j=1 (1\u2212 \u03c3cij(x))wcijk, k = 1, . . . , n (11)\nfc(x) is output of the LDNN corresponding to class c. yc (for c = 1, . . . , C) is the label vector for datapoint x. In the above equations, it is assumed that we are minimizing quadratic error. We can also minimize the cross-entropy loss function: E(W,\u0393) = \u2212 \u2211 (x,y)\u2208\u0393 y log f(x)+(1\u2212y) log(1\u2212 f(x)). It must be noted that using multiple LDNNs does not make the algorithm noticeably slower compared to a similarly configured ConvNet.\nAssuming that we have L convolutional layers, the forward pass for this structure starts from (8). Beginning from input data, we use this equation recursively until we get the output maps of the last convolutional layer, which is XLj . The input for LDNN is formed by reshaping and concatenating the maps of the last convolutional layer into a vector:\n(a) (b)\n(c) (d)\nFig. 4. Two moons test set: (a)-(c) the 3 conjunctions in the second layer of the network evaluated individually, and (d) the output of the 3\u00d73 LDNN. +/o symbols denote the two classes.\nx = [xL1 ,x L 2 , . . . ,x L p(L)] T . p(L) is the number of maps in the last convolutional layer and xLj is the matrix X L j reshaped into a vector. Then we can perform the forward pass of LDNN using (4) and perform the backward pass using (6) and (7) to update LDNN weights and biases. Then we can use (11) to get sensitivities with respect to LDNN inputs: [\u2202E/\u2202x1, \u2202E/\u2202x2, . . . , \u2202E\u2202xn]. By reshaping this vector into 2D maps, we can get the sensitivities for maps in the last convolutional layer (i.e., \u2202E/\u2202XLj ). Having these sensitivities, we can back-propagate the error to convolutional layers using (9)."}, {"heading": "IV. EXPERIMENTS ON GENERAL CLASSIFICATION", "text": ""}, {"heading": "A. Artificial datasets", "text": "We first experimented with the two moons artificial dataset to evaluate the LDNN algorithm with and without the proposed clustering initialization. We also compare the LDNN model with the modular neural networks(ModN)) [21]. To construct the two moons dataset, we start by generating random radius and angle pairs (r, \u03b8). For both moons, r is an uniform random variable between R \u2212W/2 and R + W/2 where R and W are parameters that determine the radius and the width of the moons, respectively. For the top moon, \u03b8 is an uniformly distributed random variable between 0 and \u03c0. For the bottom moon, \u03b8 is an uniformly distributed random variable between \u03c0 and 2\u03c0. The Cartesian coordinates for data points on the top and bottom moons are then generated as (R cos \u03b8,R sin \u03b8) and (R cos \u03b8\u2212W/2, R sin \u03b8\u2212\u03b1), respectively. The parameter \u03b1 determines the vertical separation between the two moons. We generated a training and a testing dataset by using the parameters R = 1, W = 0.7, \u03b1 = \u22120.7 which generates slightly overlapping classes. Both datasets contained 1000 instances on the top moon and 1000 instances on the bottom moon. Then, for each n \u2208 [1, 7], we trained 50 n \u00d7 n LDNNs starting from random parameter initializations, 50 n \u00d7 n LDNNs initialized from k-means clustering with n clusters per moon and 50 n \u00d7 n ModNs initialized from kmeans clustering with n clusters per moon. For ModNs, the n2 linear discriminants are trained independently using data from the n2 pairs of positive (top moon) and negative (bottom moon) clusters and then combined using min/ma functions. We used stochastic gradient descent with a step size of 0.3,\n6 n LDNN random init LDNN cluster init ModN cluster init Av. Range Av. Range Av. Range 1 15.6 [15.2, 18.6] 15.6 [15.2, 20.2] 15.5 [15.2, 16.3] 2 6.6 [3.0, 15.8] 3.3 [2.9, 3.7] 4.2 [3.6, 5.4] 3 4.1 [1.1, 15.6] 2.3 [1.2, 3.5] 2.7 [1.2, 4.8] 4 3.6 [1.2, 15.6] 2.2 [1.3, 3.5] 3.0 [1.8, 5.2] 5 3.4 [1.2, 15.4] 2.2 [1.2, 4.2] 2.8 [1.4, 5.7]\nTABLE I AVERAGE, MIN. AND MAX. TESTING ERROR PERCENTAGES OVER 50 REPETITIONS FOR LDNN INITIALIZED WITH RANDOM PARAMETERS, INITIALIZED WITH CLUSTERING AND MODN [21] INITIALIZED WITH CLUSTERING FOR DIFFERENT MODEL SIZES.\na momentum term weight of 0.1 and 500 epochs for training all models. Testing accuracies were computed over the second dataset which was not used in training. Table I shows the mean, minimum and maximum testing error over the 50 trials for each of the models. We observe that training the LDNN model starting from a random initialization is successful in general; however, the range of testing error rates varies by a larger amount compared to when a cluster initialization is used resulting in a slightly worse mean testing error. We also note that the LDNN model performs better both on average and when comparing the maximum error rates over the 50 trials than the ModN model. Figure 4 illustrates the output of the LDNN model for n = 3, which appears to be an appropriate choice based on Table I. The outputs of the 3 conjunctions are also shown separately to give further intuition into the behavior of the LDNN model. Notice the similarity to Figures 2(e-h)).\nThe two-spirals dataset is an extremely difficult dataset for the MLP architecture trained with the backpropagation algorithm [6]. The original dataset consists of 194 (x, y) pairs arranged in two interlocking spirals that orbit the origin three times. The classification task is to determine which spiral any given (x, y) point belongs to. We used the farthest distance clustering algorithm [55] for initialization of both models. The k-means clustering algorithm places most centroids near the origin where the data points are denser and fewer centroids on the spiral arms further from the origin where the data is sparser. On the other hand, the farthest distance clustering algorithm provides more uniformly distributed centroids which leads to better classification results with fewer clusters. We performed clustering with maximum distance thresholds 2.2, 2.0 and 1.5 resulting in 18, 21 and 27 clusters per class, respectively. For each of these, we trained a LDNN and a ModN. Note that the number of parameters in both models is the same for the same number of clusters. We used stochastic gradient descent with a step size of 0.3, a momentum term weight of 0.1 and 2, 000 epochs for training all models. LDNN achieved 0 percent training error in each of these cases while the ModN\u2019s training error was 0.232, 0.062 and 0 percent, respectively. These results suggest that the unified learning framework of LDNN is able to capture the spiral dataset with many fewer parameters than independent, pairwise learning of discriminants as in [21]. Furthermore, it can be seen from Figure 5 that LDNN creates a much smoother approximation to the spirals than pairwise learning. Finally, we note that LDNN initialized randomly was not able to find a satisfactory\nlocal minimum of the error function via gradient descent. This is similar to the failure of the standard MLP architecture for this dataset. This observation underlines the importance of the existence of an intuitive initialization for the LDNN architecture."}, {"heading": "B. Two-class problems", "text": "We experimented with 10 different binary classification datasets from the UCI Machine Learning Repository [56] and the LIBSVM Tools webpage [57]. For each dataset, we trained the LDNN, ModN, MLP, SVM and RF classifiers."}, {"heading": "1) Dataset normalization, training/testing set split:", "text": "Datasets were normalized as follows: For LDNN, ModN and MLP training, we applied a whitening transform [55] to datasets with a large number of training instances (Forest cover type and Webspam) since the covariance matrix could be estimated reliably. All other datasets were normalized by centering each dimension of the feature vector at the origin by subtracting its mean and then scaling by dividing it with its standard deviation. For SVM training, each dimension of the feature vector was linearly scaled to the range [0, 1]. For RF training, no normalization is necessary.\nThe IJCNN and COD RNA binary datasets had previously determined training and testing sets. For the rest of the datasets, we randomly picked 2/3\u2019s of the instances for training and the rest for testing. For LDNN, MLP, MLP-m and Mod-N experiments, the training set was further randomly split into a training (%90) and cross-validation (%10) set for determining the number of epochs to use in training.\n2) Model and classifier training parameter selection: For LDNN classifiers we need to choose the number of NAND gates (N) and the number of discriminants per group (M). These parameters translate into the number of positive and negative clusters, respectively in the initialization. Various combinations, up to 40 clusters per class, were tried to find the selection that gives the best testing accuracy. For any given number of clusters, the k-means algorithm was repeated 50 times and the clustering result with the lowest sum of\n7 square distances to nearest cluster centroid was selected to initialize the LDNN weights. We also fine tuned the step size for gradient descent. The number of epochs for training was selected using the cross-validation set except for the IJCNN dataset. For the IJCNN dataset cross-validation set was also used in training as in [58] and the number of epochs was fixed at 20.\nFor MLP training, there are two main parameters. The first one is the number of hidden nodes which was varied from 2 to 40 to find the best test set accuracy. This was followed by fine tuning the step size for backpropagation. The number of epochs was chosen using the cross-validation set. We also trained a second MLP classifier (MLP-m) for which the number of hidden nodes was chosen as N \u00d7M to match the total number of logistic sigmoid functions in the LDNN classifier. This was done to compare LDNN to a MLP with approximately the same degrees of freedom. It was not feasible to train the MLP-m classifier for 4 of the datasets due to extremely long training times. Similarly, a modular network, which we refer to as Mod-N, with the same number of conjunctions and disjunctions as the LDNN classifier was trained to control for the degrees of freedom.\nThere are three main parameters involved in RF training. The first one is the number of trees. We choose a sufficiently large number of trees to ensure that the out of bag error rate converges. The second parameter is the number of features that will be considered in every node of the tree. We tried a range of numbers around the square root of the number of features [5]. The last parameter is the fraction of total samples that will be used in the construction of each tree. We tried 2/3, 1/2, 1/3, 1/4 and 1/5 as possible values for this parameter.\nFor SVM training, a RBF kernel was used for all datasets except for the MNIST dataset for which a 9th degree polynomial kernel was used [14]. For all datasets except MNIST, we used the grid search tool provided by the Libsvm guide [57] to set the parameters of the RBF kernel.\nThe training and model parameters selected for all models are listed in Table II.\n3) Results: All of the classifiers we consider, with the exception of SVM, are stochastic. Therefore, each experiment with the exception of SVM was repeated 50 times to obtain mean, minimum and maximum testing errors which are reported in Table II for all classifiers. The LDNN classifier outperformed MLPs for all datasets. Furthermore, LDNNs also outperform MLP-m in all datasets for which the MLPm classifier was trained. In 8 out of 10 datasets, the mean LDNN error was smaller than the minimum MLP error, and, in 5 out of 6 datasets, the mean LDNN error was smaller than the minimum MLP-m error. All algorithms were run on an Intel i7-3770 3.4 Ghz CPU. In all datasets LDNNs were significantly faster to train than the MLPs. These results signify that the LDNN network architecture and training offers a faster to train and more accurate alternative to MLPs and backpropagation. The LDNN classifier also outperformed the Mod-N classifier in all datasets including several datasets such as Forest cover type and Wisconsin breast cancer where the accuracy difference was very large. This emphasizes the importance of training the entire network in an unified manner.\nConsidering all of the classifiers tested, LDNNs had the lowest testing error in 7 out of 10 datasets. LDNNs outperformed SVMs in 8 out of 10 cases and RFs in 7 out of 10 cases. In 5 out of 10 cases the mean LDNN error was lower than the minimum RF error. The RF mean error was lower than the LDNN minimum error in only 2 out of 10 cases. Finally, LDNNs never severely over fit the data, whereas RFs has significant accuracy differences between training and testing sets for several datasets including Adult, PIMA Indian diabetes, German credit and Forest cover type."}, {"heading": "C. Multi-class problems", "text": "We also experimented with 6 multi-class datasets from the UCI Machine Learning Repository [56]. Each dataset was first normalized in the same way as described in Section IV-B1. For each dataset we trained the LDNN, RF and SVM classifiers with the exception of the MNIST dataset for which the SVM results are reported from [14]. In that paper, a SVM is trained on a feature set generated by a deep belief network. The model and classifier training parameters were chosen as described in Section IV-B2 and are reported in Table III. LDNN and RF experiments were repeated 20 times to obtain mean, minimum and maximum testing errors which are reported in Table III. The LDNN classifier is also related to the idea of space partitioning [32] which combines partitioning of the space and learning a local classifier for each partition into a global objective function for supervised learning. All space partitioning classifier results are reported from [32]. LDNNs had the best accuracy in 4 out of 6 datasets. Note that the minimum and maximum testing errors for LDNNs were equal for MNIST."}, {"heading": "V. EXPERIMENTS ON IMAGE CLASSIFICATION", "text": "In this section, we evaluate the performance of our proposed deep structure through different experiments. We incorporated our LDNN into a GPU implementation of ConvNet that is publicly available at [59]. Most of the experiments described here are done using this GPU implementation. In all the experiments, we found good parameters using cross-validation on a small portion of training data and repeat the training on all training data. For all the datasets, we repeated the experiments a number of times and report two numbers here. One is the average over error rates of different experiments and the other is the voting result of different experiments. For obtaining the voting result, we average the predicted probabilities of different experiments before calculating the error rate. Basically the only difference between multiple runs of the experiments for every dataset is the random initialization of weights."}, {"heading": "A. MNIST", "text": "MNIST [46] is probably the most popular dataset in the area of digit classification. It contains 60000 training and 10000 test samples of size 28\u00d728 pixels. We conducted two sets of experiments on this dataset: the first on unmodified MNIST data and the second on the MNIST with the augmented\n8 training set. For the first task, we used 2 convolutional layers as feature detectors. The first layer uses 7\u00d77 filters and produces 20 maps. The second layer also uses 7\u00d77 filters but produces 15 maps. Ten LDNNs perform the classification part (one per class). Every LDNN consists of 5 groups and 5 discriminants per group. No preprocessing was performed on this dataset. We trained 4 networks with this setting and used voting for the final classification. We did not use annealing, momentum or any type of regularization for this case. However, the convergence is very fast and on average it requires less than 20 epochs, significantly faster than any other neural networkbased method, which require hundreds of epochs to train\n[50], [60]. Furthermore, the number of trainable parameters in our case is less than in most other ConvNet-based methods. Using this setting, we obtained 0.38% test error rate on the unmodified MNIST dataset. Table IV compares this error rate to other algorithms. The solutions based on maxout Networks [51] (0.45%) and stochastic pooling [60] (0.47%) also use convolutional feature learning but require more trainable parameters or training epochs. For example, stochastic pooling uses 3 convolutional layers with 64 maps in each layer and 280 training epochs. Results of Maxout Networks are obtained by using 3 convolutional layers and 96 maps in each of them. In a matched experiment, we compared the convergence properties\nof our method to conventional ConvNet. The number of nodes in fully-connected layer of ConvNet is set to be the same as the number of linear discriminants in LDNN. For ConvNet, another layer of weights was added to connect the fullyconnected layer to ten outputs. Everything else is exactly the same for both structures. The convergence plot is shown in Figure 6. We can see that our structure performs better in terms of accuracy and convergence. It must be noted that a conventional ConvNet is significantly faster to converge compared to structures with Dropout or DropConnect.\nWe obtained the results of the first experiment using a simple MATLAB implementation, mainly because the GPU implementation does not allow for any arbitrary choice for the number of maps per layer. However, for the second task of MNIST discussed below and the rest of experiments we used the GPU implementation. In addition, for the first task of MNIST we used quadratic error. For the rest of the experiments, however, the cross-entropy error was minimized.\nFor the second task, we used data translation, rotation and scaling. Image translation was achieved by cropping the\n10\noriginal images to 24\u00d724 patches picked at random locations. Random rotations of maximum 15 degrees and random scaling of maximum 15% of the original size were also used during training. For this task, we opted for a network with two larger convolutional layers. For the first layer, 5\u00d75 filters were used to produce 64 maps, and 4\u00d74 filters were used in the second one to produce 128 maps. LDNNs with 4 groups and 4 discriminants per group perform the classification part. We did not use any preprocessing for this task. We trained 5 networks for 100, 15 and 15 epochs with corresponding learning rates of 1e-3, 1e-4 and 1e-5. An error rate of 0.19% was achieved by voting the results of these 5 networks. Table V compares this result to other methods. The result of DropConnect [50] (0.21%) is obtained after 1000 epochs of training, which is nearly 10 times more than our training epochs. Our best results over different runs of the algorithm was 0.22% for a single model network. For comparison, the 0.23% error rate reported in [49] obtained by voting of 35 single networks."}, {"heading": "B. CIFAR10", "text": "CIFAR10 [62] is a collection of 60000 tiny 32\u00d732 images of 10 categories (50000 for training and 10000 for test). Our setup for this dataset consists of 2 convolutional layers followed by two locally connected layers. There are 64 maps in each convolutional layer and 32 maps in each locally connected layer. Filters are 5\u00d75 in convolutional layers and 3\u00d73 in locally connected layers (the same as \u2019layers-conv-local-13pct.cfg\u2019 of [59]). LDNNs with 7 groups and 7 discriminants per group were used for classification. Training data was also augmented with image translations, which is done by taking 24\u00d724 cropped versions of the original images at random locations. A common preprocessing for this dataset is to subtract the per pixel mean of the training set from every image [13]. We trained 5 networks for 500, 20 and 20 epochs with corresponding learning rates of 1e-3, 1e-4 and 1e-5 for convolutional layers and 1e-2, 1e-3 and 1e-4 for LDNN layer. We obtained 9.39% error rate by voting these 5 single networks. The state-of-the-art result for this task is 9.32% reported by Wan et al. [50] and obtained by voting of 12 networks. They also used the same GPU implementation of [59] to obtain this number. Their model, however, is based on \u2019layers-conv-local-11pct.cfg\u2019 setting, which is a slightly more complex model. This setting contains two extra response normalization layers and the first locally connected layer contains 64 maps (vs. 32 in our setting). Another notable difference is that they trained their network for over 1000 epochs. In comparison, our network converged in 540 epochs, which is nearly half of their number of epochs. It must\nbe noted that our method achieves better average error rate compared to other models that use multiple networks. The result of Maxout networks [51] obtained by a much bigger network which has 3 convolutional layers with 192, 384 and 384 maps respectively. The classification layer also has 2500 nodes (500 maxout hidden nodes with 5 linear units for each of them). They perform global contrast normalization and ZCA whitening before training their model."}, {"heading": "C. NORB", "text": "NORB [63] is a collection of stereo images in 6 classes. The training set contains 10 folds of 29160 images. It is common practice to use only first two folds for training. The test set contains 2 folds totalizing 58320. The original images are 108\u00d7108. However, we scaled them down to 48\u00d748 similar to [49]. The layer configuration for this task is similar to CIFAR10. LDNNs also have 7 groups and 7 discriminants per group. We trained this structure for 75 and 20 epochs with learning rates of 1e-3 and 1e-4. Data translation, rotation and scaling were also used during training. Image translation was obtained by randomly cropping the training images to 44\u00d744. We trained 4 networks and used voting for final classification. We obtained 3.09% error rate on this task. The state-of-theart for this task is 3.03% reported by Wan et al. [50] as shown in Table VII. They did not use image translation as they found that it hurts the performance. In addition, their model is slightly more complex and requires more training epochs (150). Here again, our method achieves the best average error rate."}, {"heading": "D. SVHN", "text": "SVHN [64] is another digit classification task similar to MNIST. This dataset contains 604388 images for training and validation. The test set contains 26032 images, which are RGB images of size 32 \u00d7 32. Generally, SVHN is a more difficult task than MNIST because of the large variations in the images.\n11\nIt is common to apply local contrast normalization to each 3 RGB channel of the input image in order to reduce the effect of variations of the images [65]. We did not perform any kind of preprocessing for this dataset. We simply converted the color images to grayscale by removing hue and saturation information. The feature extractor is similar to CIFAR10. We used locally connected layers with 64 maps for this case. LDNNs with 4 groups and 4 discriminants per group perform the classification. We did a careful annealing in this case and trained the model for 200, 20, 20 and 10 epochs with learning rates of 1e-3, 1e-4, 1e-5 and 5e-6. Image translation, rotation and scaling were also applied during training. Images were cropped to 28\u00d728 at random locations for image translation. For the last 10 epochs, we turned off image rotation and scaling. Four networks were trained using this setting and we obtained a 1.92% error rate after voting. Table VIII compares this number to other results. The 1.94% reported by Wan et al. [50] is obtained after 150 epochs. This faster convergence is probably because of their preprocessing scheme. However, our model still achieves the best average error rate."}, {"heading": "E. CIFAR100", "text": "CIFAR100 [62] is similar to CIFAR10, but it contains tiny images of 100 classes. We used the same setup as CIFAR10. The only difference is that we used LDNNs with 4 groups and 4 discriminants per group instead of 7 and 7. Per pixel mean subtraction and image translation were also applied. Four networks were trained and an error rate of 36.17% was obtained after voting, which shows that our structure is able to handle datasets with many classes."}, {"heading": "VI. CONCLUSION", "text": "We believe that the LDNN network architecture and training can become a favorable alternative to MLPs for supervised learning with artificial neural networks. The LDNN classifier has several advantages over MLPs: First, LDNNs allow for a simple and intuitive initialization of the network weights before supervised learning that avoids the herd-effect. Second, due to the single adaptive layer, learning can use larger step-sizes in gradient descent. We demonstrated empirically that LDNNs are significantly faster to train and are more accurate than MLPs. Similar to MLPs, the LDNN classifier also requires the choice of model complexity. The number of conjunctions (number of positive training clusters) and the number of logistic sigmoid functions per conjunction (number of negative training clusters) need to be chosen. However, the complexity of the model could be chosen automatically by either using a validation set, as commonly done for\nSVM training, or by initializing the LDNN in different ways. For instance, sequential covering algorithms can be used to generate a set of rules [66]. Each rule is a conjunction and the final classification is a disjunction of these conjunctions which can easily be converted to a LDNN classifier and fine tuned using gradient descent.\nWhile LDNNs are similar in architecture to modular neural networks [21], they are significantly more accurate owing to the unified training of the network that we introduced. LDNNs outperformed RFs in 13 of the 16 datasets and outperformed SVMs in 12 of the 16 datasets. Based on these results and observations, we believe that LDNNs should be considered as a state-of-the art classifier that provides a viable alternative to RFs and SVMs. Further improvements in accuracy can be possible by using cross-entropy instead of the square error criterion or by using adaptive step sizes for training LDNNs. Another possibility is to use more powerful nonlinear discriminants such as conic sections in (3).\nFinally, our deep structure is a novel combination of a powerful automatic feature learner and LDNN as an efficient classifier. The whole structure is jointly optimized using backpropagation via the chain rule. We demonstrated its reliability through different experiments on MNIST, CIFAR10, NORB, SVHN and CIFAR100 datasets. We showed that it is possible to achieve state-of-the-art or near state-of-the-art results on these datasets using the proposed structure. Furthermore, in most of the cases, the average error rate of our structure is better than state-of-the-art methods."}], "references": [{"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks, vol. 2, pp. 359\u2013366, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Math. Control Systems, vol. 2, no. 303\u2013314, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": "Nature, 1986.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "The cascade-correlation learning architecture", "author": ["S.E. Fahlman", "C. Lebiere"], "venue": "Advances in Neural Information Processing Systems 2. Morgan Kaufmann, 1990, pp. 524\u2013532.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Dynamics and algorithms for stochastic learning", "author": ["G. Orr"], "venue": "Ph.D. dissertation, Oregon Graduate Institute, 1995.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Speeding up backpropagation algorithms by using cross-entropy combined with pattern normalization", "author": ["M. Joost", "W. Schiffmann"], "venue": "Int. Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 2, 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Exact solution for on-line learning in multilayer neural networks", "author": ["D. Saad", "S. Solla"], "venue": "Physical Review Letters, vol. 74, pp. 4337\u20134340, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive on-line learning in changing environments", "author": ["N. Murata", "K. Muller", "A. Ziehe", "S. Amari"], "venue": "Advances in Neural Information Processing Systems, vol. 9, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "http://arxiv.org/abs/1207.0580.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1207}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.  12", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy min-max neural networks - part 1: Classification", "author": ["P. Simpson"], "venue": "IEEE Trans Neural Networks, vol. 3, no. 5, pp. 776\u2013786, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive membership function fusion and annihilation in if-then rules", "author": ["B. Song", "R.M. II", "S. Oh", "P. Arabshahi", "T. Caudell", "J. Choi"], "venue": "Proc. Int. Conf. Fuzzy Syst., vol. II, 1993, pp. 961\u2013967.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "A fuzzy min-max neural network classifier with compensatory neuron architecture", "author": ["A. Nandedkar", "P. Biswas"], "venue": "Int. Conf. on Pattern Recognition, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "A multi-sieving neural network architecture that decomposes learning tasks automatically", "author": ["B.-L. Lu", "H. Kita", "Y. Nishikawa"], "venue": "Proc. Int. Conf. on Neural Networks, vol. 3, 1994, pp. 1319\u20131324.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "A neural network classifier with disjunctive fuzzy information", "author": ["H.-M. Lee", "K.-H. Chen", "I.-F. Jiang"], "venue": "Neural Networks, vol. 11, pp. 1113\u2013 1125, 1998.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Task decomposition and module combination based on class relations: A modular neural network for pattern classification", "author": ["B.-L. Lu", "M. Ito"], "venue": "IEEE Trans Neural Networks, vol. 10, no. 5, pp. 1244\u20131256, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Steepest descent adaptations of min-max fuzzy if-then rules", "author": ["R.M. II", "S. Oh", "P. Arabshahi", "T. Caudell", "J. Choi", "B. Song"], "venue": "Proc Int. Joint Conf. on Neural Networks, vol. III, 1992, pp. 471\u2013477.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "A learning method of fuzzy inference by descent method", "author": ["H. Normura", "I. Hayashi", "N. Wakami"], "venue": "Proc. Int. Conf. Fuzzy Syst., 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "The delta rule and learning for min-max neural networks", "author": ["X. Zhang", "C.-C. Hang", "S. Tan", "P.-Z. Wang"], "venue": "Proc. Int. Conf. Neural Networks, 1994, pp. 38\u201343.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "The min-max function differentiation and training of fuzzy neural networks", "author": ["\u2014\u2014"], "venue": "IEEE Trans Neural Networks, vol. 7, no. 5, pp. 1139\u20131150, 1996.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Adaptive mixtures of local experts", "author": ["R. Jacobs", "M. Jordan", "S. Nowlan", "G. Hinton"], "venue": "Neural Computation, vol. 3, no. 79\u201387, 1991.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Twenty years of mixture of experts", "author": ["S. Yuksel", "J. Wilson", "P. Gader"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, vol. 23, no. 8, pp. 1177\u20131193, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminant analysis by gaussian mixtures", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, vol. 58, pp. 155\u2013176, 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Subclass discriminant analysis", "author": ["M. Zhu", "A. Martinez"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 28, no. 8, pp. 1274\u2013 1286, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Locally linear discriminant analysis for multimodally distributed classes for face recognition with a single model image", "author": ["T.-K. Kim", "J. Kittler"], "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence, vol. 27, pp. 318\u2013327, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "V. Saligrame"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally adaptive classification piloted by uncertainty", "author": ["J. Dai", "S. Yan", "X. Tang", "J. Kwok"], "venue": "Proc. Int. Conf. on Machine Learning, 2006, pp. 225\u2013232.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning discontinuities with products-of-sigmoids for switching between local models", "author": ["M. Toussaint", "S. Vijayakumar"], "venue": "Proc. Int. Conf. on Machine Learning, 2005, pp. 904\u2013911.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "There is a hole in my data space: Piecewise predictors for heterogenous learning problems", "author": ["O. Dekel", "O. Shamir"], "venue": "Proc. Int. Conf. on Artificial Intelligence and Machine Learninf, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Localized support vector machine and its efficient algorithm", "author": ["H. Cheng", "P. Tang", "R. Jin"], "venue": "Proc. SIAM Int. Conf. on Data Mining, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "The elements of statistical learning: Data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2001}, {"title": "Dipol - a hybrid piecewise linear classifier", "author": ["B. Schulmeister", "F. Wysotzki"], "venue": "ch. Machine Learning and Statistics: the Interface,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 26, no. 1, pp. 451\u2013471, 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "A part-versus-part method  for massively parallel training of support vector machines", "author": ["B. Lu", "K. Wang", "M. Utiyama", "H. Isahara"], "venue": "Proc. of Int. Joint. Conf. on Neural Networks, 2004, pp. 735\u2013740.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "Probability estimates for multi-class classification by pairwise coupling", "author": ["T. Wu", "C. Lin", "R. Weng"], "venue": "Journal of Machine Learning Research, pp. 975\u20131005, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Local decomposition for rare class analysis", "author": ["J. Wu", "H. Hui", "W. Peng", "J. Chen"], "venue": "Proc. ACM Int. Conf. on Knowledge discovery and data mining, 2007, pp. 814\u2013823.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "On locally linear classification by pairwise coupling", "author": ["F. Chen", "C.-T. Lu", "A. Boedihardjo"], "venue": "Data Mining, 2008. ICDM \u201908. Eighth IEEE International Conference on, dec. 2008, pp. 749 \u2013754.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained classifier: a novel approach to nonlinear classification", "author": ["H. Abbassi", "R. Monsefi", "H. Sadoghi Yazdi"], "venue": "Neural Computing and Applications, pp. 1\u201311, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Advances in Neural Information Processing Systems (NIPS 1989), vol. 2, Denver, CO, 1990.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, November 1998.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, August 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "International Conference on Learning Representations (ICLR2014), April 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition, 2012, pp. 3642\u20133649.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proc. International Conference on Machine learning (ICML\u201913), 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Large scale visual recognition challenge 2010.", "author": ["A. Berg", "J. Deng", "L. Fei-Fei"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Ijcnn 2001 challenge: generalization ability and text decoding", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "International Joint Conference on Neural Networks, vol. 2, 2001, pp. 1031\u20136.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1872\u20131886, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1872}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep, 2009.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F. Huang", "L. Bottou"], "venue": "Computer Vision and Pattern Recognition (CVPR). IEEE Press, 2004.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2004}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, 2011.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR), 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "AN artificial neural network (ANN) consisting of one hidden layer of squashing functions is an universal approximator for continuous functions defined on the unit hypercube [1], [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "AN artificial neural network (ANN) consisting of one hidden layer of squashing functions is an universal approximator for continuous functions defined on the unit hypercube [1], [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "However, until the introduction of the backpropagation algorithm [3], training such multilayer perceptron (MLP) networks was not possible in practice.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "However, eventually MLPs were replaced by more recent techniques such as support vector machines (SVM) [4] and random forests (RF) [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "However, eventually MLPs were replaced by more recent techniques such as support vector machines (SVM) [4] and random forests (RF) [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "An underlying reason for the limited accuracy and high computational cost of training is the herd-effect problem [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "stochastic learning [7], [8], squared error vs.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "proposed a Dropout scheme for backpropagation which helps prevent co-adaptation of feature detectors [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Contrastive divergence [14], [15] can be used to pre-train networks in an unsupervised manner prior to backpropagation such that the herd-effect problem is alleviated.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Contrastive divergence [14], [15] can be used to pre-train networks in an unsupervised manner prior to backpropagation such that the herd-effect problem is alleviated.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "[19] proposed a multi-sieving network that decomposes learning tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] proposed a disjunctive fuzzy network which is based on prototypes; however, it lacks an objective function and is based on an adhoc training procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Similarly, the modular network proposed by Lu and Ito [21] removes the axis aligned hypercube restriction from fuzzy min-max networks; however, their network can not be learned by minimizing a single energy function.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "A closely related approach to ours is adaptive mixtures of local experts which uses a gating network to stochastically select the output from a set of feedforward networks [26].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "The reader is referred to [27] for a survey of mixture of expert methods.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "The products of experts approach models complex probability distributions by multiplying simpler distributions is also related [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Mixture discriminant analysis treats each class as a mixture of Gaussians and learns discriminants between the Gaussians [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Subclass discriminant analysis also relies on modeling classes as mixtures of Gaussians prior to learning discriminant [30].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Local linear discriminant analysis clusters the data and learns a linear discriminant in each cluster [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "partitioning and supervised learning [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "proposed an approach which places local classifiers close to the global decision boundary [33].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "Toussaint and Vijayakumar propose a products-of-sigmoids model for discontinuously switching between local models [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Another approach greedily builds a piecewise linear classifier by adding classifiers in regions of error clusters [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 33, "context": "Local versions of SVMs have also been explored [36], [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "Local versions of SVMs have also been explored [36], [37].", "startOffset": 53, "endOffset": 57}, {"referenceID": 35, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 262, "endOffset": 266}, {"referenceID": 36, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 268, "endOffset": 272}, {"referenceID": 37, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 274, "endOffset": 278}, {"referenceID": 38, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 280, "endOffset": 284}, {"referenceID": 39, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 286, "endOffset": 290}, {"referenceID": 40, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 292, "endOffset": 296}, {"referenceID": 41, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 298, "endOffset": 302}, {"referenceID": 18, "context": "The modular network [21] discussed previously also falls into this category.", "startOffset": 20, "endOffset": 24}, {"referenceID": 42, "context": "Neural Networks [45], [46] (ConvNet).", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "Neural Networks [45], [46] (ConvNet).", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 148, "endOffset": 152}, {"referenceID": 45, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 154, "endOffset": 158}, {"referenceID": 46, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 160, "endOffset": 164}, {"referenceID": 47, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 166, "endOffset": 170}, {"referenceID": 48, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 172, "endOffset": 176}, {"referenceID": 49, "context": "For example, the state-ofthe-art results for large 1000-category ImageNet [52] dataset was significantly improved using ConvNets [53].", "startOffset": 74, "endOffset": 78}, {"referenceID": 50, "context": "For example, the state-ofthe-art results for large 1000-category ImageNet [52] dataset was significantly improved using ConvNets [53].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Usually, joint optimization of deep structures leads to a better solution or at least improves the result of layer-wise learning [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 46, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The idea of DropConnect is inspired by Dropout [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 48, "context": "is Maxout Networks [51].", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "We also compare the LDNN model with the modular neural networks(ModN)) [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Then, for each n \u2208 [1, 7], we trained 50 n \u00d7 n LDNNs starting from random parameter initializations, 50 n \u00d7 n LDNNs initialized from k-means clustering with n clusters per moon and 50 n \u00d7 n ModNs initialized from k-", "startOffset": 19, "endOffset": 25}, {"referenceID": 18, "context": "TESTING ERROR PERCENTAGES OVER 50 REPETITIONS FOR LDNN INITIALIZED WITH RANDOM PARAMETERS, INITIALIZED WITH CLUSTERING AND MODN [21] INITIALIZED WITH CLUSTERING FOR DIFFERENT MODEL SIZES.", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "The two-spirals dataset is an extremely difficult dataset for the MLP architecture trained with the backpropagation algorithm [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 18, "context": "many fewer parameters than independent, pairwise learning of discriminants as in [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "For SVM training, each dimension of the feature vector was linearly scaled to the range [0, 1].", "startOffset": 88, "endOffset": 94}, {"referenceID": 51, "context": "For the IJCNN dataset cross-validation set was also used in training as in [58] and the number of epochs was fixed at 20.", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "We tried a range of numbers around the square root of the number of features [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "For SVM training, a RBF kernel was used for all datasets except for the MNIST dataset for which a 9th degree polynomial kernel was used [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "For each dataset we trained the LDNN, RF and SVM classifiers with the exception of the MNIST dataset for which the SVM results are reported from [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "The LDNN classifier is also related to the idea of space partitioning [32] which combines partitioning of the space and learning a local classifier for each partition into a global objective function for supervised learning.", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "All space partitioning classifier results are reported from [32].", "startOffset": 60, "endOffset": 64}, {"referenceID": 43, "context": "MNIST [46] is probably the most popular dataset in the area of digit classification.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "58 \u2014 Results taken from [32] Landsat LDNN 2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "95 \u2014 Results taken from [32] Letter LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "08 \u2014 Results taken from [32] Optdigit LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "23 \u2014 Results taken from [32] Pendigit LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "32 \u2014 Results taken from [32] MNIST LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "40 \u2014 Results taken from [14]", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "THE SPACE PARTITIONING (SP) RESULTS ARE FROM [32].", "startOffset": 45, "endOffset": 49}, {"referenceID": 47, "context": "[50], [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[50], [60].", "startOffset": 6, "endOffset": 10}, {"referenceID": 48, "context": "The solutions based on maxout Networks [51] (0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 52, "context": "45%) and stochastic pooling [60] (0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 47, "context": "027) DropConnect [50] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "035) Dropout [50] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 53, "context": "039) Scattering Networks [61] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 48, "context": "43 (single model) Maxout Networks [51] 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 52, "context": "45 (single model) Stochastic Pooling [60] 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": "The result of DropConnect [50] (0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 46, "context": "23% error rate reported in [49] obtained by voting of 35 single networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 47, "context": "036) DropConnect [50] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "032) Dropout [50] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 46, "context": "016) MC-DNN [49] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 54, "context": "CIFAR10 [62] is a collection of 60000 tiny 32\u00d732 images of 10 categories (50000 for training and 10000 for test).", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "cessing for this dataset is to subtract the per pixel mean of the training set from every image [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "[50] and obtained by voting of 12 networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The result of Maxout networks [51] obtained by a much bigger network which has 3 convolutional layers with 192, 384 and 384 maps respectively.", "startOffset": 30, "endOffset": 34}, {"referenceID": 47, "context": "18) DropConnect [50] (5 nets) 9.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "13) DropConnect [50] (12 nets) 9.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "32 (voting error) Dropout [50] 9.", "startOffset": 26, "endOffset": 30}, {"referenceID": 48, "context": "18) Maxout Networks [51] 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 46, "context": "38 (single model) MC-DNN [49] 11.", "startOffset": 25, "endOffset": 29}, {"referenceID": 55, "context": "NORB [63] is a collection of stereo images in 6 classes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "However, we scaled them down to 48\u00d748 similar to [49].", "startOffset": 49, "endOffset": 53}, {"referenceID": 47, "context": "[50] as shown in Table VII.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "13) DropConnect [50] 3.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "06) Dropout [50] 3.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "16) Multi-column DNN [49] 3.", "startOffset": 21, "endOffset": 25}, {"referenceID": 56, "context": "SVHN [64] is another digit classification task similar to MNIST.", "startOffset": 5, "endOffset": 9}, {"referenceID": 57, "context": "RGB channel of the input image in order to reduce the effect of variations of the images [65].", "startOffset": 89, "endOffset": 93}, {"referenceID": 47, "context": "[50] is obtained after 150 epochs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "037) DropConnect [50] 1.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "039) Dropout [50] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "CIFAR100 [62] is similar to CIFAR10, but it contains tiny images of 100 classes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "While LDNNs are similar in architecture to modular neural networks [21], they are significantly more accurate owing to the unified training of the network that we introduced.", "startOffset": 67, "endOffset": 71}], "year": 2014, "abstractText": "Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herdeffect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available", "creator": "LaTeX with hyperref package"}}}