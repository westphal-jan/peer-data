{"id": "1501.01348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2015", "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data", "abstract": "High-content screening uses large collections of unlabeled cell image data to think about genetics or cell biology. Two important tasks are to identify those cells that exhibit interesting phenotypes, and to identify subpopulations enriched with these phenotypes. Explorative data analysis typically involves dimensionality reduction followed by clusters in the hope that clusters represent a phenotype. We suggest the use of stacked denoising auto-encoders to perform dimensionality reduction for high-content screening.", "histories": [["v1", "Wed, 7 Jan 2015 02:13:09 GMT  (1278kb,D)", "http://arxiv.org/abs/1501.01348v1", "5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine Learning in Computational Biology)"]], "COMMENTS": "5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine Learning in Computational Biology)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lee zamparo", "zhaolei zhang"], "accepted": false, "id": "1501.01348"}, "pdf": {"name": "1501.01348.pdf", "metadata": {"source": "CRF", "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data", "authors": ["Lee Zamparo", "Zhaolei Zhang"], "emails": ["zamparo@cs.toronto.edu", "zhaolei.zhang@utoronto.ca"], "sections": [{"heading": null, "text": "High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap."}, {"heading": "1 Introduction", "text": "The use of machine learning methods to apply phenotype labels to cells has proven an effective way to uncover novel roles of genes in model organisms [Vizeacoumar et al., 2010] as well as in human [Fuchs et al., 2010]. In an unsupervised model, characterizing all cells split across distinct populations presents a clustering problem over many millions of high dimensional data points. This complicates the clustering problem, and it is preferable to transform the data into a much lower dimensional space. How to best perform the dimensionality reduction is far from clear. Particularly important qualities in this use case are the ability to scale to millions of data points, and the flexibility to model non-linear relationships between covariates in the map from higher to lower dimensional space. Standard algorithms for dimensionality reduction fail in one or both of these criteria.\n\u2022 PCA is fast once the covariance matrix is formed, but is not able to model any non-linear interactions.\n\u2022 Kernel PCA [Ham et al., 2004] can model more flexible relationships, but is impractical for large data sets due to the growth of the kernel matrix.\n\u2022 Local Linear Embedding (LLE) [Roweis, 2000] and Isomap [De Silva et al., 2003] are impractical for large data sets as they require an large matrix decomposition that scales cubically in the number of data points.\nNeural networks structured as auto-encoders ([Hinton and Salakhutdinov, 2006]) can model nonlinear interactions, and scale well to large data sets. They can also be trained with unlabeled data, which in the case of high-content screening is plentiful. We investigated how well a class of autoencoders ([Vincent et al., 2008]) could be composed to create an efficient and flexible dimensionality reduction algorithm.\n\u2217http://www.cs.toronto.edu/\u02dczamparo/\nar X\niv :1\n50 1.\n01 34\n8v 1\n[ cs\n.L G\n] 7\nJ an"}, {"heading": "2 Stacked De-noising Autoencoders", "text": "The idea of composing simpler models in layers to form more complex ones has been successful with a variety of basis models, stacked de-noising autoencoders (abbrv. SdA) being one example [Hinton and Salakhutdinov, 2006, Ranzato et al., 2008, Vincent et al., 2010]. These models were frequently employed as unsupervised pre-training; a layer-wise scheme for initializing the parameters of a multi-layer perceptron, which is subsequently trained by minimizing an appropriate loss function over real versus model-predicted labels of the data. Where labeled data is plentiful, supervised training now reigns supreme, with pre-training overlooked in favour of randomly initialized models coupled with powerful new regularization techniques [Srivastava, 2013, Goodfellow et al., 2013]. These methods are a poor fit for high-content screening, where biological expertise and time-intensive sampling makes labels scarce. Unlabeled data remains plentiful, so pre-training is well-suited to this scenario. We chose de-noising autoencoders since they are simple to train without incurring any sacrifice in competitive performance [Vincent et al., 2010]."}, {"heading": "2.1 De-noising Autoencoders", "text": "The de-noising autoencoder [Vincent et al., 2008] takes input data x \u2208 <d, and maps it to a hidden representation y \u2208 [0, 1]n, n d through a corrupted noisy mapping y = f\u03b8(x\u0303) = sigmoid(Wx\u0303+ b), parametrized by \u03b8 = {W, b}. W is a n \u00d7 d weight matrix and b is a bias vector. The input x is corrupted into x\u0303 by randomly setting a certain proportion of the values of x to zero. The latent representation y is then mapped back to a reconstructed vector z \u2208 <d via z = g\u03b8\u2032(y) = W \u2032y + b\u2032 with \u03b8\u2032 = {W \u2032, b\u2032}. The parameters W,d are set to minimize the reconstruction loss L(x, z). See the schematic in figure 1"}, {"heading": "2.2 Training", "text": "To find the best architecture of layer sizes, we performed another grid search over both layer sizes as well as layers. Hinton and Salakhutdinov [Hinton and Salakhutdinov, 2006] used a four layer auto-encoder for MNIST, but we began with 3 layer networks and tried both 4 and 5 layer networks. Both 3 and 4 layer networks achieved better results measured by reconstruction error on a validation set. For a given number of layers, each candidate model was a point on a grid with a step size of 100: 700 . . . 1000\u00d7 500 . . . 900\u00d7 100 . . . 400\u00d7 50 . . . 10. Each dA layer in every model was pre-trained using mini-batch stochastic gradient descent for 50 epochs, in batches of 100 samples. The minimum mean reconstruction error for each layer was recorded. After selecting the top 5 performing models for both 3 and 4 layer networks, we performed grid searches over each of the tunable hyperparameters momentum, noise rate, learning rate, and weight decay. We used Adagrad to adjust the learning rate appropriately for each dimension [Duchi et al., 2010]. Tuning the initial value of the base learning rate had the largest effect on performance. The other parameters showed much smaller effects. Both the architecture and hyper-parameter searches were performed in parallel on a cluster with GPU capable nodes. All GPU code was written in python using Theano [Bergstra et al., 2010]."}, {"heading": "3 Experiments", "text": "We used data derived from images of yeast cell strains transformed to express a GFP fusion protein acting as a marker for DNA damage foci. Each well in a 384 well plate contained an homozygous population of yeast bearing some set of genetic deletions. Four field of view images per well were captured, each containing several hundred cells. Post capture, the images were run through an image processing pipeline using CellProfiler, to segment the cells in each field, and represent each as a vector of intensity, shape and texture features [Kamentsky et al., 2011]. The cell phenotypes in this study consisted of three classes: dna-damage foci, non-round nuclei, or wild-type1. A validation set of approximately 10000 labeled cells was generated by first manually labeling images for each phenotype, and then training an SVM in a one vs all manner to label the remaining cells. The predicted labels were manually validated.\nThe data for the validation set was scaled to have mean 0 and variance 1. The scaled data was reduced using one of the candidate algorithms, followed by Gaussian mixture clustering using scikit-learn [Pedregosa et al., 2011]. The number of mixture components was chosen as the number of labels in the validation set. All hyperparameters were chosen by cross-validation. For each model and embedding of the data, a Gaussian Mixture Model was run with randomly initialized parameters for 10 iterations. Each of these runs was repeated 10 times. The parameters from the best performing run were used to initialize a GMM which was run until convergence, and the homogeneity was measured and recorded. This process was repeated five times for each model, resulting in five homogeneity measurements. We report the average homogeneity (fit by loess) as well as the standard deviation 2."}, {"heading": "4 Discussion", "text": "Examination of figure 2 shows that SdA models were consistently ranked as the best models for dimensionality reduction in preparation for phenotype based clustering. There is a loss of homogeneity incurred by using an algorithm that cannot learn non-linear transformations, as shown in the gap between Isomap and PCA. To try and understand what accounts for the gap between SdA models and the other algorithms, we randomly sampled output from each algorithm and computed estimates\n1Yeast cells that displayed neither DNA-damage foci nor a non-round nucleus phenotype\nof the distribution over both the inter-label and intra-label distances between points. Shown in figure 3 is an example of the estimated distance distributions between kernel PCA and a sample three layer SdA model. While the points sampled from kernel PCA have a smaller intra-label mean distance2, the points sampled from the SdA model have a larger inter-label distance3. Therefore, a clustering algorithm applied to SdA reduced data was more reliably able to find assignments that reflected the phenotype labels.\nWe have introduced deep autoencoder models for dimensionality reduction of high content screening data. Mini-batch stochastic gradient descent allowed us to train using millions of data points, and the nature of the model allowed us to apply the resulting models to unseen data, circumventing the limitations of other comparable dimensionality reduction algorithms. We also demonstrated that SdA models produced output that was more easily assigned to clusters that reflected biologically meaningful phenotypes."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra et al", "J. 2010] Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Global versus local methods in nonlinear dimensionality reduction", "author": ["De Silva et al", "V. 2003] De Silva", "J.J.B. Tenenbaum", "V.D. Silva"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al", "J. 2010] Duchi", "E. Hazan", "Y. Singer"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Clustering phenotype populations by genome-wide RNAi and multiparametric imaging", "author": ["Fuchs et al", "F. 2010] Fuchs", "G. Pau", "D. Kranz", "O. Sklyar", "C. Budjan", "S. Steinbrink", "T. Horn", "A. Pedal", "W. Huber", "M. Boutros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Ham et al", "J. 2004] Ham", "D.D. Lee", "S. Mika", "B. Sch\u00f6lkopf"], "venue": "In Twenty-first international conference on Machine learning - ICML", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Salakhutdinov", "G.E. 2006] Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improved structure, function and compatibility for CellProfiler: modular high-throughput image analysis", "author": ["Kamentsky et al", "L. 2011] Kamentsky", "T.R. Jones", "A. Fraser", "Bray", "M.-A", "D.J. Logan", "K.L. Madden", "V. Ljosa", "C. Rueden", "K.W. Eliceiri", "A.E. Carpenter"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Sparse Feature Learning for Deep Belief Networks", "author": ["Ranzato et al", "M. 2008] Ranzato", "Boureau", "Y.-l", "Y.L. Cun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent et al", "P. 2008] Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In Proceedings of the 25th international conference on Machine learning - ICML", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "P. 2010] Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Integrating high-throughput genetic interaction mapping and high-content screening to explore yeast spindle morphogenesis", "author": ["Vizeacoumar et al", "F.J. 2010] Vizeacoumar", "N. van Dyk", "F. S Vizeacoumar", "V. Cheung", "J. Li", "Y. Sydorskyy", "N. Case", "Z. Li", "A. Datti", "C. Nislow", "B. Raught", "Z. Zhang", "B. Frey", "K. Bloom", "C. Boone", "B.J. Andrews"], "venue": "The Journal of cell biology,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}], "referenceMentions": [], "year": 2015, "abstractText": "High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.", "creator": "LaTeX with hyperref package"}}}