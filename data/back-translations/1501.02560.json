{"id": "1501.02560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Belief Hierarchical Clustering", "abstract": "In the field of data mining, many clustering methods have been proposed, but standard versions do not take into account insecure databases. This paper is about a new approach to clustering insecure data by defining hierarchical clustering within the framework of belief function. The main goal of hierarchical clustering is to allow an object to belong to one or more clusters. A certain amount of belief is assigned to each affiliation, and clusters are combined on the basis of pignistic characteristics. Experiments with truly insecure data show that our proposed method can be considered a suitable tool.", "histories": [["v1", "Mon, 12 Jan 2015 07:55:41 GMT  (229kb,D)", "http://arxiv.org/abs/1501.02560v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["wiem maalel", "kuang zhou", "arnaud martin", "zied elouedi"], "accepted": false, "id": "1501.02560"}, "pdf": {"name": "1501.02560.pdf", "metadata": {"source": "CRF", "title": "Belief Hierarchical Clustering", "authors": ["Wiem Maalel", "Kuang Zhou", "Arnaud Martin", "Zied Elouedi"], "emails": ["Wiem.Maalel@gmail.com", "kzhoumath@163.com", "Arnaud.Martin@univ-rennes1.fr", "Zied.Elouedi@gmx.fr"], "sections": [{"heading": null, "text": "Keywords: Clustering, Hierarchical clustering, Belief function, Belief clustering"}, {"heading": "1 Introduction", "text": "Due to the increase of imperfect data, the process of decision making is becoming harder. In order to face this, the data analysis is being applied in various fields.\nClustering is mostly used in data mining and aims at grouping a set of similar objects into clusters. In this context, many clustering algorithms exist and are categorized into two main families: The first family involves the partitioning methods based on density such as kmeans algorithm [6] that is widely used thanks to its convergence speed. It partitions the data into k clusters represented by their centers. The second family includes the hierarchical clustering methods such as the top-down and the Hierarchical Ascendant Clustering (HAC) [5]. This latter consists on constructing clusters recursively by partitioning the objects in a bottom-up way. This process leads to good result visualizations. Nevertheless, it has a non-linear complexity.\nAll these standard methods deal with certain and precise data. Thus, in order to facilitate the decision making, it would be more appropriate to handle uncertain data. Here, we need a soft clustering process that will take into account the possibility that objects belong to more than one cluster.\nIn such a case, several methods have been established. Among them, the Fuzzy C-Means [1] which consists in assigning a membership to each data point\nar X\niv :1\n50 1.\n02 56\n0v 1\n[ cs\n.A I]\n1 2\nJa n\n20 15\ncorresponding to the cluster center, and the weights minimizing the total weighted mean-square error. This method constantly converges. Patently, Evidential cMeans (ECM) [3], [7] is deemed to be a very fateful method. It enhances the FCM and generates a credal partition from attribute data. This method deals with the clustering of object data. Accordingly, the belief k-Modes method [4] is a popular method, which builds K groups characterized by uncertain attribute values and provides a classification of new instances. Schubert has also found a clustering algorithm [8] which uses the mass on the empty set to build a classifier.\nOur objective in this paper is to develop a belief hierarchical clustering method, in order to ensure the membership of objects in several clusters, and to handle the uncertainty in data under the belief function framework.\nThis remainder is organized as follows: in the next section we review the ascendant hierarchical clustering, its concepts and its characteristics. In section 3, we recall some of the basic concepts of belief function theory. Our method is described in section 4 and we evaluate its performance on a real data set in section 5. Finally, Section 6 is a conclusion for the whole paper."}, {"heading": "2 Ascendant hierarchical clustering", "text": "This method consists on agglomerating the close clusters in order to have finally one cluster containing all the objects xj (where j = 1, .., N). Let\u2019s consider PK = {C1, ..., CK} the set of clusters. IfK = N , C1 = x1, ..., CN = xN . Thereafter, throughout all the steps of clustering, we will move from a partition PK to a partition PK\u22121. The result generated is described by a hierarchical clustering tree (dendrogram), where the nodes represent the successive fusions and the height of the nodes represents the value of the distance between two objects which gives a concrete meaning to the level of nodes conscripted as \u201dindexed hierarchy\u201d. This latter is usually indexed by the values of the distances (or dissimilarity) for each aggregation step. The indexed hierarchy can be seen as a set with an ultrametric distance d which satisfies these properties: i) x = y \u21d0\u21d2 d(x, y) = 0. ii) d(x, y) = d(y, x). iii) d(x, y) \u2264 d(x, z) + d(y, z),\u2200x, y, z \u2208 IR.\nThe algorithm is as follows:\n\u2013 Initialisation: the initial clusters are the N-singletons. We compute their dissimilarity matrix. \u2013 Iterate these two steps until the aggregation turns into a single cluster: \u2022 Combine the two most similar (closest) elements (clusters) from the se-\nlected groups according to some distance rules. \u2022 Update the matrix distance by replacing the two grouped elements by\nthe new one and calculate its distance from each of the other classes.\nOnce all these steps completed, we do not recover a partition of K clusters, but a partition of K \u2212 1 clusters. Hence, we had to point out the aggregation\ncriterion (distance rules) between two points and between two clusters. We can use the Euclidian distance between N objects x defined in a space IR. Different distances can be considered between two clusters: we can consider the minimum as follows:\nd(Cij , C i j\u2032) = min\nxk\u2208Cij ,xk\u2032\u2208Cij\u2032 d(xk, xk\u2032) (1)\nwith j, j\u2032 = 1, ..., i. The maximum can also be considered, however, the minimum and maximum distances create compact clusters but sensitive to \u201doutliers\u201d. The average can also be used, but the most used method is Ward\u2019s method, using Huygens formula to compute this:\n\u2206Iinter(Cij ,Cij\u2032 ) =\nmCjmCj\u2032\nmCj +mCj\u2032 d2(Cij,C i j\u2032) (2)\nwhere mCj and mCj\u2032 are numbers of elements of Cj and Cj\u2032 respectively and Cij,C i j\u2032 the centers. Then, we had to find the couple of clusters minimizing the distance: (Ckl , C k l\u2032) = d(C i l , C i l\u2032) = min\nCij ,C i l\u2032\u2208C\ni d(Cij , C i j\u2032) (3)"}, {"heading": "3 Basis on the theory of belief functions", "text": "In this Section, we briefly review the main concepts that will be used in our method that underlies the theory of belief functions [9] as interpreted in the Transferable Belief Model (TBM) [10]. Let\u2019s suppose that the frame of discernment is \u2126 = {\u03c91, \u03c92, ..., \u03c93}. \u2126 is a finite set that reflects a state of partial knowledge that can be represented by a basis belief assignment defined as:\nm : 2\u2126 \u2192 [0, 1]\u2211 A\u2286\u2126 m(A) = 1 (4)\nThe value m(A) is named a basic belief mass (bbm) of A. The subset A \u2208 2\u2126 is called focal element if m(A) > 0. One of the important rules in the belief theory is the conjunctive rule which consists on combining two basic belief assignments m1 and m2 induced from two distinct and reliable information sources defined as:\nm1 \u2229\u00a9m2(C) = \u2211\nA\u2229B=C m1(A) \u00b7m2(B), \u2200C \u2286 \u2126 (5)\nThe Dempster rule is the normalized conjunctive rule:\nm1 \u2295m2(C) = m1 \u2229\u00a9m2(C)\n1\u2212m1 \u2229\u00a9m2(\u2205) , \u2200C \u2286 \u2126 (6)\nIn order to ensure the decision making, beliefs are transformed into probability measures recorded BetP, and defined as follows [10]:\nBetP(A) = \u2211 B\u2286\u2126 |A \u2229B | | B | m(B) (1\u2212m(\u2205)) ,\u2200A \u2208 \u2126 (7)"}, {"heading": "4 Belief hierarchical clustering", "text": "In order to set down a way to develop a belief hierarchical clustering, we choose to work on different levels: on one hand, the object level, on the other hand, the cluster level. At the beginning, for N objects we have, the frame of discernment is \u2126 = {x1, ..., xN} and for each object belonging to one cluster, a degree of belief is assigned. Let PN be the partition of N objects. Hence, we define a mass function for each object xi, inspired from the k-nearest neighbors [2] method which is defined as follows:\nm\u2126ii (xj) = \u03b1e \u2212\u03b3d2(xi,xj) m\u2126ii (\u2126i) = 1\u2212 \u2211 m\u2126ii (xj)\n(8)\nwhere i 6= j, \u03b1 and \u03b3 are two parameters we can optimize [11], d can be considered as the Euclidean distance, and the frame of discernment is given by \u2126i = {x1, ..., xN} \\ {xi}.\nIn order to move from the partition of N objects to a partition of N \u2212 1 objects we have to find both nearest objects (xi, xj) to form a cluster. Eventually, the partition of N \u2212 1 clusters will be given by PN\u22121 = {(xi, xj), xk} where k = 1, ..., N\\ {i, j}. The nearest objects are found considering the pignistic probability, defined on the frame \u2126i, of each object xi, where we proceed the comparison by pairs, by computing firstly the pignistic for each object, and then we continue the process using arg max. The nearest objects are chosen using the maximum of the pignistic values between pairs of objects, and we will compute the product pair one by one.\n(xi, xj) = arg max xi,xj\u2208PN\nBetP\u2126ii (xj) \u2217 BetP \u2126j j (xi) (9)\nThen, this first couple of objects is a cluster. Now consider that we have a partition PK of K clusters {C1, . . . , CK}. In order to find the best partition PK\u22121 of K\u22121 clusters, we have to find the best couple of clusters to be merged. First, if we consider one of the classical distances d (single link, complete link, average, etc), presented in section 2, between the clusters, we delineate a mass function, defined within the frame \u2126i for each cluster Ci \u2208 PK with Ci 6= Cj by:\nm\u2126ii (Cj) = \u03b1e \u2212\u03b3d2(Ci,Cj) (10) m\u2126ii (\u2126i) = 1\u2212 \u2211 m\u2126ii (Cj) (11)\nwhere \u2126i = {C1, . . . , CK} \\ {Ci}. Then, both clusters to merge are given by:\n(Ci, Cj) = arg max Ci,Cj\u2208PK\nBetP\u2126i(Cj) \u2217 BetP\u2126j (Ci) (12)\nand the partition PK\u22121 is made from the new cluster (Ci, Cj) and all the other clusters of PK . The point by doing so is to prove that if we maximize the\ndegree of probability we will have the couple of clusters to combine. Of course, this approach will give exactly the same partitions than the classical ascendant hierarchical clustering, but the dendrogram can be built from BetP and the best partition (i.e. the number of clusters) can be preferred to find. The indexed hierarchy will be indexed by the sum of BetP which will lead to more precise and specific results according to the dissimilarity between objects and therefore will facilitate our process.\nHereafter, we define another way to build the partition PK\u22121. For each initial object xi to classify, it exists a cluster of PK such as xi \u2208 Ck. We consider the frame of discernment \u2126i = {C1, . . . , CK} \\ {Ck}, m, which describes the degree that the two clusters could be merged, can be noted m\u2126and we define the mass function:\nm\u2126ii (Ckj ) = \u220f\nxj\u2208Ckj\n\u03b1e\u2212\u03b3d 2(xi,xj) (13)\nm\u2126ii (\u2126i) = 1\u2212 \u2211\nxj\u2208Ckj\nm\u2126ii (Ckj ) (14)\nIn order to find a mass function for each cluster Ci of PK , we combine all the mass functions given by all objects of Ci by a combination rule such as the Dempster rule of combination given by equation (6). Then, to merge both clusters we use the equation (12) as before. The sum of the pignisitic probabilities will be the index of the dendrogram, called BetP index."}, {"heading": "5 Experimentations", "text": "Experiments were first applied on diamond data set composed of twelve objects as describe in Figure 1.a and analyzed in [7]. The dendrograms for both classical and Belief Hierarchical Clustering (BHC) are represented by Figures 1.b and 1.c. The object 12 is well considered as an outlier with both approaches. With the belief hierarchical clustering, this object is clearly different, thanks to the pignistic probability. For HAC, the distance between object 12 and other objects is small, however, for BHC, there is a big gap between object 12 and others. This points out that our method is better for detecting outliers. If the objects 5 and 6 are associated to 1, 2, 3 and 4 with the classical hierarchical clustering, with BHC these points are more identified as different. This synthetic data set is special because of the equidistance of the points and there is no uncertainty.\nWe continue our experiments with a well-known data set, Iris data set, which is composed of flowers from four types of species of Iris described by sepal length, sepal width, petal length, and petal width. The data set contains three clusters known to have a significant overlap.\nIn order to reduce the complexity and present distinctly the dendrogram, we first used the k-means method to get initial few clusters for our algorithm. It is not necessary to apply this method if the number of objects is not high.\nSeveral experiments have been used with several number of clusters. We present in Figure 2 the obtained dendrograms for 10 and 13 clusters. We notice different combinations between the nearest clusters for both classical and belief hierarchical clustering. The best situation for BHC is obtained with the pignistic equal to 0.5 because it indicates that the data set is composed of three significant clusters which reflects the real situation. For the classical hierarchical clustering\nthe results are not so obvious. Indeed, for HAC, it is difficult to decide for the optimum cluster number because of the use of the euclidean distance and as seen in Figure 2.c it indistinguishable in terms of the y-value. However, for BHC, it is more easy to do this due to the use of the pignistic probability.\nIn order to evaluate the performance of our method, we use some of the most popular measures: precision, recall and Rand Index (RI). The results for both BHC and HAC are summarized in Table 1. The first three columns are for BHC, while the others are for HAC. In fact, we suppose that Fc represents the final number of clusters and we start with Fc = 2 until Fc = 6. We fixed the value of kinit at 13. We note that for Fc = 2 the precision is low while the recall is of high value, and that when we have a high cluster number (Fc = 5 or 6), the precision will be high but the recall will be relatively low. Thus, we note that for the same number of final clusters (e.g. Fc = 4), our method is better in terms of precision, recall and RI.\nTests are also performed to a third data base, Congressional Voting Records Data Set. The results presented in Figure 3 show that the pignistic probability value increased at each level, having thereby, a more homogeneous partition. We notice different combinations, between the nearest clusters, that are not the same within the two methods compared. For example, cluster 9 is associated to cluster 10 and then to 6 with HAC, but, with BHC it is associated to cluster 4 and then to 10. Although, throughout the BHC dendrograms shown in Figure 3.c and Figure 3.d, the best situation indicating the optimum number of clusters can be clearly obtained. This easy way is due to the use of the pignistic probability.\nFor this data set, we notice that for Fc = 2 and 3, the precision is low while the recall is high. However, with the increasing of our cluster number, we notice that BHC provides a better results. In fact, for Fc = 3, 4, 5 and 6 the precision and RI values relative to BHC are higher then the precision and RI values relative to HAC, which confirmed the efficiency of our approach which is better in terms of precision and RI."}, {"heading": "6 Conclusion", "text": "Ultimately, we have introduced a new clustering method using the hierarchical paradigm in order to implement uncertainty in the belief function framework. This method puts the emphasis on the fact that one object may belong to several clusters. It seeks to merge clusters based on its pignistic probability. Our method was proved on data sets and the corresponding results have clearly shown its efficiency. The algorithm complexity has revealed itself as the usual problem of the belief function theory. Our future work will be devoted to focus on this peculiar problem."}], "references": [{"title": "Fcm: The fuzzy c-means clustering algorithm", "author": ["J.C. Bezdek", "R. Ehrlich", "W. Fulls"], "venue": "Computers and Geosciences 10(2-3), 191\u2013203", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "A k-Nearest Neighbor Classification Rule Based on Dempster-Shafer Theory", "author": ["T. Den\u0153ux"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 25(5), 804\u2013813", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "EVCLUS: Evidential Clustering of Proximity Data", "author": ["T. Den\u0153ux", "M. Masson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 34(1), 95\u2013 109", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering approach using belief function theory", "author": ["S. ben Hariz", "Z. Elouedi", "K. Mellouli"], "venue": "AIMSA. pp. 162\u2013171", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "The elements of statistical learning; data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman", "J. Franklin"], "venue": "Springer Verlag, New York", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability 11", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1967}, {"title": "Clustering interval-valued proximity data using belief functions", "author": ["M. Masson", "T. Den\u0153ux"], "venue": "Pattern Recognition Letters 25, 163\u2013171", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering belief functions based on attracting and conflicting metalevel evidence", "author": ["J. Schubert"], "venue": "Bouchon-Meunier, B., Foulloy, L., Yager, R. (eds.) Intelligent Systems for Information Processing: From Representation to Applications. Elsevier Science", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Mathematical Theory of evidence", "author": ["G. Shafer"], "venue": "Princeton Univ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1976}, {"title": "The Transferable Belief Model", "author": ["P. Smets", "R. Kennes"], "venue": "Artificial Intelligent 66, 191\u2013234", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "An Evidence-Theoric k-NN Rule with Parameter Optimization", "author": ["L.M. Zouhal", "T. Den\u0153ux"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications and Reviews 28(2), 263\u2013271", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 5, "context": "In this context, many clustering algorithms exist and are categorized into two main families: The first family involves the partitioning methods based on density such as kmeans algorithm [6] that is widely used thanks to its convergence speed.", "startOffset": 187, "endOffset": 190}, {"referenceID": 4, "context": "The second family includes the hierarchical clustering methods such as the top-down and the Hierarchical Ascendant Clustering (HAC) [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "Among them, the Fuzzy C-Means [1] which consists in assigning a membership to each data point ar X iv :1 50 1.", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "Patently, Evidential cMeans (ECM) [3], [7] is deemed to be a very fateful method.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Patently, Evidential cMeans (ECM) [3], [7] is deemed to be a very fateful method.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Accordingly, the belief k-Modes method [4] is a popular method, which builds K groups characterized by uncertain attribute values and provides a classification of new instances.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "Schubert has also found a clustering algorithm [8] which uses the mass on the empty set to build a classifier.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "In this Section, we briefly review the main concepts that will be used in our method that underlies the theory of belief functions [9] as interpreted in the Transferable Belief Model (TBM) [10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "In this Section, we briefly review the main concepts that will be used in our method that underlies the theory of belief functions [9] as interpreted in the Transferable Belief Model (TBM) [10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "\u03a9 is a finite set that reflects a state of partial knowledge that can be represented by a basis belief assignment defined as: m : 2 \u2192 [0, 1] \u2211", "startOffset": 134, "endOffset": 140}, {"referenceID": 9, "context": "In order to ensure the decision making, beliefs are transformed into probability measures recorded BetP, and defined as follows [10]:", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "Hence, we define a mass function for each object xi, inspired from the k-nearest neighbors [2] method which is defined as follows:", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "where i 6= j, \u03b1 and \u03b3 are two parameters we can optimize [11], d can be considered as the Euclidean distance, and the frame of discernment is given by \u03a9i = {x1, .", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "a and analyzed in [7].", "startOffset": 18, "endOffset": 21}], "year": 2015, "abstractText": "In the data mining field many clustering methods have been proposed, yet standard versions do not take into account uncertain databases. This paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework. The main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters. To each belonging, a degree of belief is associated, and clusters are combined based on the pignistic properties. Experiments with real uncertain data show that our proposed method can be considered as a propitious tool.", "creator": "LaTeX with hyperref package"}}}