{"id": "1501.02592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Photonic Delay Systems as Machine Learning Implementations", "abstract": "Nonlinear photonic delay systems are interesting implementation platforms for machine learning. They can be extremely fast, have a high degree of parallelism, and potentially consume much less power than digital processors. So far, they have been successfully used for signal processing according to the reservoir computing paradigm. In this work, we show that their scope can be considerably expanded if we apply a gradient descent with backpropagation over time to a model of the system for optimizing the input coding of such systems. We conduct physical experiments that show that the input coding obtained works well in reality, and we show that optimized systems work much better than the traditional reservoir computing approach. The results presented here show that conventional machine learning sequence techniques can indeed be applied to physical neuro-inspired analog computers.", "histories": [["v1", "Mon, 12 Jan 2015 10:25:31 GMT  (773kb,D)", "http://arxiv.org/abs/1501.02592v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["michiel hermans", "miguel soriano", "joni dambre", "peter bienstman", "ingo fischer"], "accepted": false, "id": "1501.02592"}, "pdf": {"name": "1501.02592.pdf", "metadata": {"source": "CRF", "title": "Photonic Delay Systems as Machine Learning Implementations", "authors": ["Michiel Hermans", "Miguel C. Soriano", "Joni Dambre", "Peter Bienstman", "Ingo Fischer"], "emails": ["(michiel.hermans@ulb.ac.be)"], "sections": [{"heading": null, "text": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.\n\u2217OPERA Photonique, Universit/\u2019e Libre de Bruxelles, Avenue F. Roosevelt 50, 1050 Brussels (michiel.hermans@ulb.ac.be) \u2020Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain \u2021ELIS departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00a7INTEC departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00b6Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain\nar X\niv :1\n50 1.\n02 59\n2v 1\n[ cs\n.N E"}, {"heading": "1 Introduction", "text": "Applied research in neural networks is currently strongly influenced by available computer architectures. Most strikingly, the increasing availability of general-purpose graphical processing unit (GPGPU) programming has sped up the computations required for training (deep) neural networks by an order of magnitude. This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on stateof-the-art performances on tasks such as computer vision ([17, 6]). One class of neural models which has only seen limited effects of the boost in speed from GPUs are recurrent models. Recurrent neural networks (RNNs) are very interesting for processing time series, as they can take into account an arbitrarily long context of their input history. This has important implications in tasks such as natural language processing, where the desired output of the system may depend on context that has been presented to the network a relatively long time ago. In common feedforward networks such dependencies are very hard to include without scaling up the model to an impractically large size. Recurrent networks, however, can\u2013at least in principle\u2013carry along relevant context as they are being updated. In practice, recurrent models suffer from two important drawbacks. First of all, where feedforward networks fully benefit from massively parallel architectures in terms of scalability, recurrent networks, with their inherently sequential nature do not fit so well into this framework. Even though GPUs have been used to speed up training RNNs ([30, 12]), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks. The second issue is that training RNNs is a notoriously slow process due to problems associated with fading gradients, which is especially cumbersome if the network needs to learn long-term dependencies within the input time series. Recent attempts to solve this problem using the Hessian-free approach have proved promising ([22]). Other attempts using stochastic gradient descent combined with more heuristic ideas have been described in [2]. In this paper we will consider a radical alternative to common, digitally implemented RNNs. A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]). Despite its simplicity, RC has several important advantages over traditional gradient descent training methods. First of all, the training process is extremely fast. Only\noutput weights are trained, and this is performed by solving a single linear system of equations. Second, and of great importance, the RC concept is applicable to any non-linear dynamical system, as long as it exhibits consistent responses, a high-dimensional state space, and fading memory. This has opened lines of research that go beyond common digital implementations and into analog physical implementations. The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]). As opposed to digital implementations, physical systems can offer great speed-ups, inherent massive parallelism, and great reductions in power consumption. In this sense, physical dynamical systems as machine learning implementation platforms may one day break important barriers in terms of scalability. In the near future, especially optical computing devices might find applications in several tasks where fast processing is essential, such as in optical header recognition, optical signal recovery, or fast control loops. The RC paradigm, despite its notable successes, still suffers from an important drawback. Its inherently unoptimized nature makes it relatively inefficient for many important machine learning problems. When the dimensionality of the input time series is low, the expansion into a high-dimensional nonlinear space offered by the reservoir will provide a sufficiently diverse set of features to approximate the desired output. If the input dimensionality becomes larger, however, relying on random features becomes increasingly difficult as the space of possible features becomes so massive. Here, optimization with gradient descent still has an important edge over the RC concept: it can shape the necessary nonlinear features automatically from the data. In this paper we aim to integrate the concept of gradient descent in neural networks with physically implemented analog machine learning models. Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]). In order to use such a system as a reservoir, an input time series is encoded into a continuous time signal and subsequently used to drive the dynamics of the physical setup. The response of the device is recorded and converted to a high-dimensional feature set, which in turn is used with linear regression in the common RC setup. In this particular case, the randomness of RC is incorporated in the input encoding. This encoding is performed offline on a computer, but is usually completely random. Even though efforts have been performed to improve this encoding in a generic\nway (by ensuring a high diversity in the network\u2019s response ([24, 1])), a way to create task-specific input encodings is still lacking. In [13], the possibility to use backpropagation through time (BPTT) ([26]) as a generic optimization tool for physical dynamical systems was addressed. It was found that BPTT can be used to find remarkably intricate solutions to complicated problems in dynamical system design. In [11] simulated results of BPTT used as an optimization method for input encoding in the physical system described above were presented. In this paper we go beyond this work and show for the first time experimental evidence that model-based BPTT is a viable training strategy for physical dynamical systems. We choose two often-used high-dimensional datasets for validation, and we show that input encoding that is optimized using BPTT in a common machine learning approach, provides a significant boost in performance for these tasks when compared to random input encodings. This not only demonstrates that machine learning approaches are more broadly applicable than is generally assumed, but also that physical analog computers can in fact be considered as parametrizable machine learning models, and may play a significant role in the next generation of signal processing hardware. This paper is structured as follows: first of all we discuss the physical system and its corresponding model in detail. We explain how we convert the continuous-time dynamics of the system into a discrete-time update equation which we use as model in our simulation. Next, we present and analyze the results on the tasks we considered and compare experimental and simulated results."}, {"heading": "2 Physical System", "text": "In this section we will explain the details of the physical system. We will start by formally introducing its delay dynamics operating in continuous time. Next, we will explain how the feedback delay can be used for realizing a high-dimensional state space encoded in time, and we demonstrate that\u2013 combined with special input and output encoding\u2013the setup can be seen as a special case of RNN. Finally we explain how we discretize the system\u2019s input and output encoding, which enables us to approximate the dynamics of the system by a discrete-time update equation. The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda-type dynamics ([18, 34]). We provide a schematic depiction of the physical setup in Figure 1. It consists of a laser source, a Mach-Zehnder modulator, a long optical fiber (\u2248 4 km) which acts as a physical delay line,\nand an electronic circuit which transforms the optical beam intensity in the fiber into a voltage. This voltage is amplified and low-pass filtered and can be measured to serve as the system output. Moreover, it is added to an external input voltage signal, and then serves as the driving signal for the Mach-Zehnder modulator. The measured output signal is well described by the following differential equation ([19]):\nT a\u0307(t) = \u2212a(t) + \u03b2 [ sin2(a(t\u2212D) + z(t) + \u03c6)\u2212 1/2 ] . (1)\nHere, the signal a(t) corresponds to a measured voltage signal (down to a constant scaling and bias factor). The factor T is the time scale of the lowpass filtering operation in the electronic circuit, equal to 0.241 \u00b5s, \u03b2 is the total amplification in the loop, which in the experiments can be varied by changing the power of the laser source. D is the delay of the system, which has been chosen as 20.82 \u00b5s. z(t) is the external input signal, and \u03c6 is a constant offset phase (which can be controlled by setting a bias voltage), which we set at \u03c0/4 for all results presented in this paper. For ease of notation we will call the system a delay-coupled Mach-Zehnder, which we abbreviate as DCMZ. Note that the parameters \u03b2 and \u03c6, together with the global scaling of the input signal z(t), control the global dynamical behavior of the system ([19]). Indeed, previous research in the RC context have identified the role of these parameters in connection with task performance. They found that good performance is usually found when the parameters put the system in an asymptotically stable regime. For instance, if we keep \u03c6 = \u03c0/4, and \u03b2 < 1, the system state will always fall back to zero in the absence of input. In the case of \u03b2 > 1, the state of the system will spontaneously start to oscillate, which has a detrimental effect on task performance. In this paper we will simply use values for \u03b2 and \u03c6 that were found to generally work well in the reservoir setup."}, {"heading": "2.1 Input and Output Encoding", "text": "Delay-coupled systems have\u2013in principle\u2013an infinite-dimensional state space, as these systems directly depend on their full history covering an interval of one delay time. This property has been the initial motivation for using delay-coupled systems in the RC paradigm in the past years. Suppose we have a multivariate input time series, which we will denote by si, for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , S}, S being the total number of instances (the length of the input sequence). Each si is a column vector of size Nin \u00d7 1, with Nin the number\nof input dimensions. We wish to construct an accompanying output time series yi. We convert each data point si to a continuous-time segment zi(t) as follows:\nzi(t) = m0(t) + m T(t)si,\nwhere m0(t) and m(t) are masking signals, which are defined for t \u2208 [0 \u00b7 \u00b7 \u00b7P ], with P the masking period. The signal m0(t) is scalar, and constitutes a bias signal, and m(t) is a column vector of size Nin \u00d7 1. The total input signal z(t) is then constructed by time-concatenation of the segments zi(t):\nz(t) = zi(t mod P ) for t \u2208 {(i\u2212 1)P \u00b7 \u00b7 \u00b7 iP}.\nSimilarly, we define an output mask u(t). We divide the state variable time traces a(t) in segments ai(t) of duration P such that\na(t) = ai(t mod P ) for t \u2208 {(i\u2212 1)P \u00b7 \u00b7 \u00b7 iP}.\nThe output time series yi is then defined as\nyi = y0 + \u222b P 0 dt ai(t)u(t). (2)\nIt is possible to see the delay-coupled dynamical system combined with the masking principle as a special case of an infinite-dimensional discrete-time recurrent neural network, as illustrated in Figure 2. The recurrent weights, connecting the hidden states over time, are fixed, and manifested by the delayed feedback connection. The input and output weights correspond to the input and output masks. The role of the parameters D and P is important to consider. If they are equal to each other the recurrent network analogy, as shown in Figure 2b, reduces to a network where all nodes have self-connections, and interaction between different nodes between different tasking periods is due to a combination of the low-pass filtering effect and the self-connection. If the difference between D and P is small, there will be direct time-interaction between different nodes. In fact, using a difference of one masking step between D and P has been the basis for opto-electronic systems that do not have a low-pass filter ([23]). If the difference between D and P becomes significant it is difficult to anticipate how performance will be affected. If D P , most interactions will happen within a single masking period, such that there will be little useful interaction between the nodes at different time steps. If D P , the nodes interact over connections that bridge several time steps. We found that, for small differences of D and P , there is little\nto no noticeable effect on performance, such that we kept D = P , as was used in previous publications. In practice, we cannot measure the state trajectory with infinite time resolution, nor can we produce signals with an arbitrary time dependency, as there will always be constraints that limit the maximum bandwidth of the generated signals. Therefore, we assume that m0(t), m(t) and u(t) all consist of piecewise constant signals1, which are segmented in Nm parts, Nm being the number of masking steps:\nm0(t) = m0k for t \u2208 {(k \u2212 1)Pm \u00b7 \u00b7 \u00b7 kPm},\nm(t) = mk for t \u2208 {(k \u2212 1)Pm \u00b7 \u00b7 \u00b7 kPm},\nu(t) = uk for t \u2208 {(k \u2212 1)Pm \u00b7 \u00b7 \u00b7 kPm}, (3)\nwhere the length of each step is given by Pm = P/Nm. This means that we now have a finite number of parameters that fully determine m0(t), m(t) and u(t). Note that, due to our choice of P = D, Pm will by definition be an integer number of times the delay length D. This is convenient for the next section, where we will make a discrete-time approximation of the system, but it is not a necessary requirement of the system to perform well."}, {"heading": "2.2 Converting the System to a Trainable Machine Learning Model", "text": "In [13] it was shown that BPTT can be applied to models of continuous-time dynamical systems. Indeed, it is perfectly possible to simulate the system\n1Note that with a finite frequency bandwidth we cannot produce immediate jumps from one constant level to the next. Therefore, we make sure that the duration of each constant part is much longer than the transient in between, and we can safely ignore it.\nusing differential equation solvers and consequently compute parameter gradients. One issue, however, is the significant computational cost. Note that, in a common discrete-time RNN, a single state update corresponds to a single matrix-vector multiplication and the application of a nonlinearity. In our case it involves the sequential computation of the full time trace of ai(t). This is considerably more costly to compute, especially given the fact that\u2013 as in most gradient descent algorithms\u2013we may need to compute it on large amounts of data and this for multiple thousands of iterations. Due to the piecewise constant definition of u(t) we can make a good approximation of a(t). First we combine Equations 2 and 3. This gives us:\nyi = Nm\u2211 k=1 \u222b kPm (k\u22121)Pm dt ukai(t) = Nm\u2211 k=1 uka\u0304ik,\nwhere a\u0304ik = \u222b kPm (k\u22121)Pm dt ai(t). This means that we can represent ai(t) by a finite set of variables a\u0304ik. To represent the full time trace of a(t) we adopt a simplified notation as follows2: a\u0304j = a\u0304ik, where j = (i\u2212 1)Nm + k. Now we make the following approximation: we assume that for the duration of a single masking step, we can replace the term a(t\u2212D) by a\u0304i\u2212Nm , that is, we consider it to be constant. With this assumption, we can solve Equation 1 for the duration of one masking step:\na(t) = \u03b3i + (a\u0302i \u2212 \u03b3i) exp ( \u2212 t T ) for t \u2208 {0 \u00b7 \u00b7 \u00b7Pm}, (4)\nwith \u03b3i = \u03b2 [ sin2(a\u0304i\u2212Nm + z(t) + \u03c6)\u2212 1/2 ] ,\nand a\u0302i the value of a(t) at the start of the interval. Integrating over the interval t = {0 \u00b7 \u00b7 \u00b7Pm} we find:\na\u0304i = (a\u0302i \u2212 \u03b3i)\u03ba+ Pm\u03b3i,\nwith \u03ba = 1\u2212 e\u2212Pm/T . We can eliminate a\u0302i as follows. First we derive from Equation 4 that a\u0302i+1 = (a\u0302i \u2212 \u03b3i)e\u2212Pm/T + \u03b3i. If we combine this expression with the following two:\na\u0304i = (a\u0302i \u2212 \u03b3i)\u03ba+ Pm\u03b3i,\na\u0304i+1 = (a\u0302i+1 \u2212 \u03b3i+1)\u03ba+ Pm\u03b3i+1, 2Please do not confuse with the index i in ai(t). Here the index indicates single masking\nsteps, rather than full mask periods.\nwe can eliminate a\u0302i, and we end up with the following update equation for a\u0304i:\na\u0304i+1 = \u03c1oa\u0304i + \u03c11\u03b3i + \u03c12\u03b3i+1,\nwith \u03c10 = e \u2212Pm/T , \u03c11 = T\u03ba \u2212 Pme\u2212Pm/T , and \u03c12 = Pm \u2212 T\u03ba. This leads to a relatively quick-to-compute update equation to simulate the system. BPTT can also be readily applied on this formula, as it is a simple update equation just like for a common RNN. This is the simulation model we used for training the input and output masks of the system. We verified the accuracy of this approximation both on measured data of the DCMZ and on a highly accurate simulation of the system. For the parameters used in the DCMZ we got very good correspondence with the model (obtaining a correlation coefficient between simulated and measured signals of 99.6%)."}, {"heading": "2.3 Hybrid Training Approach", "text": "One challenge we faced when trying to match the model with the experimentally measured data was that we obtained a sufficiently good correspondence only when we very carefully fitted the values for \u03b2 and \u03c6. We can physically control these parameters, but exactly setting their numerical values turned out not to be trivial in the experiments, especially since they tend to show slight drifting behavior over longer periods of time (in the order of hours). As a consequence, it turned out to be a challenge to train parameters in simulation, and simply apply them directly on the DCMZ. Therefore, we applied a hybrid approach between gradient descent and the RC approach. We train both the input and output masks in simulations. Next, we only use the input masks for the physical setup. After recording all the data, we retrained the output weights using gradient descent, this time on the measured data itself. The idea is that the input encoding will produce highly useful features for the system even when it is trained on a model that may show small, systematic differences with the physical setup."}, {"heading": "2.4 Input Limitations", "text": "One additional physical constraint is the fact that the voltages that can be generated by the electronic part of the system are limited within a range set by its supply voltage. The output voltage of the electronic part serves as the input of the Mach-Zehnder interferometer, and corresponds to the term a(t \u2212 D) + z(t) in the argument of the squared sine in Equation 1 (the offset phase \u03c6 is controlled by a separate voltage source). The voltage\nrange we were able to cover before the amplifiers started to saturate, roughly corresponded to a range of [\u2212\u03c0/2 \u00b7 \u00b7 \u00b7\u03c0/2] in Equation 1: one full wavelength. Instead of accounting for the saturation of the amplifiers in our simulations, we made sure that when the input argument z(t) went outside of this range, we mapped it back into this range by adding or subtracting \u03c0. Note that this has no effect on Equation 1 due to the periodicity of the squared sine. Due to the addition of the input signal with the delayed feedback a(t\u2212D), there is still a chance that the total argument falls out of the range [\u2212\u03c0/2 \u00b7 \u00b7 \u00b7\u03c0/2], but in practice such occurrences turned out to be rare, and could safely be ignored."}, {"heading": "3 Experiments", "text": "We tested the use of BPTT for training the input masks both in simulation and in experiment on two benchmark tasks. First, we considered the oftenused MNIST written digit recognition dataset, where we use the dynamics of the system indirectly. Next, we applied it on the TIMIT phoneme dataset. For the MNIST experiment we used Nm = 400 masking steps. For TIMIT we used Nm = 600."}, {"heading": "3.1 MNIST", "text": "To classify static images using a dynamical system, we follow an approach similar to the one introduced in [25]. Essentially, we repeat the same input segment several times until the state vector ai(t) of the DCMZ no longer changes. Next we choose the final instance of ai(t) to classify the image. In practice we used 10 iterations for each image in the MNIST dataset (i.e., each input digit is repeated for 10 masking periods). This sufficed for ai(t) to no longer depend on its initial conditions, and in practice this meant that we were able to present all digits to the network right after each other. Input masks were trained using 106 training iterations, where for each iteration the gradient was determined on 500 randomly sampled digits. For training we used Nesterov momentum ([29]), with momentum coefficient 0.9, and a learning rate of 0.01 which linearly decayed to zero over the duration of the training. As regularization we only performed 1-pixel shifts for the digits. Note that these 1-pixel shifts were used for training the input masks, but we did not include them when retraining the output weights, as we only presented the DCMZ with the original 60,000 training examples. After training the input weights, we gathered both physical and simulated data for the 4 experiments as described below, and retrained the output\nweights to obtain a final score. Output weights are trained using the crossentropy loss function over 106 training iterations, where for each iteration the gradient was determined on 1000 randomly sampled digits. We again used Nesterov momentum, with momentum coefficient 0.9. The learning rate was chosen at 0.002 and linearly decayed to zero. Meta-parameter optimization was performed using 10,000 randomly selected examples from the training set. We performed 4 tests on MNIST. First of all we directly compared performances between the simulated and experimental data. When we visualized the features that the trained input masks generated, we noticed that they seemed ordered (see Figure 3). Indeed, for each masking step, a single set of weights mk, which can be seen as a receptive field, is applied to the input image, and the resulting signals from the receptive fields are injected into the physical setup one after each other. Apparently, the trained input masks have similar features grouped together in time. To confirm that this ordering in time is a purposeful property, we shuffled the features mk over a single masking period to obtain a new input mask without a natural ordering in the features. Next we tested (in simulation) how much the performance degraded when using these masks. Finally, we also tested classification employing masks with completely random elements, where only the scaling of\nthe weights was optimized (which is the RC approach). Results are presented in the middle column of Table 1. The difference between experimental and simulation results is very small. The time shuffled features do indeed cause a notable increase in the classification error rate, indicating that the optimized input masks actively make use of the internal dynamics of the system, and not just offer a generically good feature set. For the sake of comparison we have added the current state-of-the-art result on MNIST. For a comprehensive overview of results on MNIST please consult http://yann.lecun.com/exdb/mnist/. Our result are comparable to the best results obtained using neural networks with a single hidden layer (denoted as a 2-layer NN on the previously mentioned website)."}, {"heading": "3.2 TIMIT", "text": "We applied frame-wise phoneme recognition to the TIMIT dataset ([9]). The data was pre-processed to 39-dimensional feature vectors using Mel Frequency Cepstral Coefficients (MFCCs). The data consists of the log energy, and the first 12 MFCC coefficients, enhanced with their first and second derivative (the so-called delta and delta-delta features). The phonemes were clustered into a set of 39 classes, as is the common approach. Note that we did not include the full processing pipeline to include segmentation of the labels and arrive at a phoneme error rate. Here, we wish to illustrate the potential of our approach and demonstrate how realizations of physical computers can be extended to further concepts, rather than to claim state-of-the-art performance. Given that, in addition, the input masks are trained to perform frame-wise phoneme classification, including the whole processing pipeline would not be informative. Input masks are trained using 50, 000 training iterations, where for each iteration the gradient was determined on 200 randomly sampled sequences\nof a length of 50 frames. For training we again used Nesterov momentum, with momentum coefficient 0.9, and a learning rate of 0.2 which linearly decayed to zero over the duration of the training. As we were in a regime far from overfitting, we simply chose the training error for meta-parameter optimization. We have depicted the optimized input mask in Figure 4. Note that the training process strongly rescaled the masking weights for different input channels, putting more emphasis on the delta and delta-delta features (respectively channels 14 to 26 and 27 to 39 ). We repeated the four scenarios previously discussed: using optimized masks in simulation and experiment, using time-shuffled masks, and using random masks. The resulting frame error rates are presented in the right column of Table 1. The simulated and experimental data differ by 1.5%, a relatively small difference, indicating that input masks optimized in simulation are useful in practice, even in the presence of unavoidable discrepancies between the used model and the DCMZ. Results for random masks are significantly worse than those with optimized input masks. Comparison to literature is not straightforward as most publications do not mention frame error rate, but rather the error rate after segmentation. We included the lowest frame error rate mentioned in literature to our knowledge, though it should be stated that other works may have even lower values, even when they are not explicitly mentioned. For an overview of other results on frame error rate please check [16]. The decrease in performance when using time-shuffled masks is quite modest, suggesting that in this case, most of the improvement over random masks is due directly from the features themselves, and the precise details of the dynamics of the system are less crucial than was the case in the MNIST task 3. Although further testing is needed, we suggest two possible reasons for this. First of all, the TIMIT dataset we used contained the first and second derivatives of the first thirteen channels, which already provides information on the preceding and future values and acts as an effective time window. Indeed as can be seen from Figure 4, the input features amplify these derivatives. Therefore, a lot of temporal context is already embedded in a single input frame, reducing the need for recurrent connections. Secondly, the lack of control over the way information is mixed over time may still pose an important obstacle to effectively use the recurrence in the system. Currently, input features are trained to perform two tasks at once:\n3Note that, when the features are shuffled in time over a single masking period, this indirectly also affects the way information is passed on between different masking periods as the communication between specific nodes between masking periods is a combined effect of the low-pass filter and the self connection.\nprovide a good representation of the current input, and at the same time design the features in such a way that they can make use of the (fixed) dynamics present within the system. It may prove the case that the input masks do not have enough modeling power to fulfill both tasks at once, or that the way temporal mixing occurs in the network cannot be effectively put to use for this particular task."}, {"heading": "4 Discussion and Future Work", "text": "In this paper we presented an experimental survey of the use of backpropagation through time on a physical delay-coupled electro-optical dynamical system, in order to use it as a machine learning model. We have shown that such a physical setup can be approached as a special case of recurrent neural network, and consequently can be trained with gradient descent using backpropagation. Specifically, we have shown that both the input and output encodings (input and output masks) for such a system can be fully optimized in this way, and that the encodings can be successfully applied to the real physical setup. Previous research in the usage of electro-optical dynamical systems for signal processing used random input encodings, which are quite inefficient in scenarios where the input dimensionality is high. We focused on two tasks with a relatively high input dimensionality: the MNIST written digit recognition dataset and the TIMIT phoneme recognition dataset. We showed that in both cases, optimizing the input encoding provides a significant performance boost over random masks. We also showed that the input encoding for the MNIST dataset seems to directly utilize the inherent dynamics of the system, and hence does more than simply provide a useful feature set. Note that the comparison with Reservoir Computing is based on the constraints by a given physical setup and a given set of resources. We note that the Reservoir Computing setup could give good results on the proposed tasks too, if we were greatly scaling up its effective dimensionality. This has been evidenced in, for example, [31], where good results on the TIMIT dataset were achieved by using Echo State Networks (a particular kind of Reservoir Computing) of up to 20,000 nodes. In our setup this would be achieved by increasing the number of masking steps Nm within one masking period. In reality, however, we will face two practical problems. First of all, there are bandwidth limitations in signal generation and measurement. Parts of the signal that fluctuate rapidly would be lost when reducing the duration of a single masking step. If one would scale up by keeping the length of the\nmask steps fixed but use a longer physical delay, for instance a fiber of tens or hundreds of kilometers, the potential gain in performance comes at the cost of one of the systems important advantages: its speed. Also it is hard to foresee how other optical effects in such long fibers such as dispersion and attenuation, would affect performance. This would be an interesting research topic for future investigations. At the current stage we did not quantify how much the results in this paper hinge on the ability to model the system mathematically. This particular system can be modeled rather precisely, but it is unclear how fast the usefulness of the presented approach would degrade when the model becomes less acute. Several directions for future improvements are apparent. The most obvious one is that we could greatly simplify the training process by putting the DCMZ measurements directly in the training loop: instead of optimizing input masks in simulations, we could just as well directly use real, measured data. The Jacobians required for the backpropagation phase can be computed from the measured data. A training iteration would then consist of the following steps: sample data, produce the corresponding input to the DCMZ with the current input mask, measure the output, perform backpropagation in simulation, and update the parameters. The benefit would be that we directly end up with functional input and output masks, without the need for retraining. On top of that, data collection would be much faster. The only additional requirement for this setup would be the need for a single computer controlling both signal generation and signal measurement. The next direction for improvement would be to rethink the design of the system from a machine learning perspective. The current physical setup on which we applied backpropagation finds its origins in reservoir computing research. As we argue in Section 2, the system can be considered as a special case of recurrent network with a fixed, specific connection matrix between the hidden states at different time steps. In the reservoir computing paradigm, one always uses fixed dynamical systems that remain largely unoptimized, such that in the past this fact was not particularly restrictive. However, given the possibility of fully optimizing the system that was demonstrated in this paper, the question on how to redesign this system such that we can assert more control over the recurrent connection matrix, and hence the dynamics of the system itself, becomes far more relevant. Currently we have a fixed dynamical system of which we optimize the input signal to accommodate a certain signal processing task. As explained at the end of Section 3.2, it appears that backpropagation can currently only leverage the recurrence of the system to a limited degree, when using\na single delay loop. Therefore it would be more desirable to optimize both the input signal and the internal dynamics of the system to accommodate a certain task. Alternatively, the configuration can be easily extended to multiple delay loops, allowing for a richer recurrent connectivity. The most significant result of this paper is that we have shown experimentally that the backpropagation algorithm, a highly abstract machine learning algorithm, can be used as a tool in designing analog hardware to perform signal processing. This means that we may be able to vastly broaden the scope of research into physical and analog realizations of neural architectures. In the end this may result in systems that combine the best of both worlds: powerful processing capabilities at a tremendous speed and with a very low power consumption."}, {"heading": "Acknowledgements", "text": "P.B., M.H. and J.D. acknowledge support by the interuniversity attraction pole (IAP) Photonics@be of the Belgian Science Policy Office, the ERC NaResCo Starting grant and the European Union Seventh Framework Programme under grant agreement no. 604102 (Human Brain Project). M.C.S. and I.F. acknowledge support by MINECO (Spain), Comunitat Auto\u0300noma de les Illes Balears, FEDER, and the European Commission under Projects TEC2012-36335 (TRIPHOP), and Grups Competitius. M.H. and I.F. acknowledge support from the Universitat de les Illes Balears for an Invited Young Researcher Grant. In addition, we acknowledge Prof. L. Larger for developing the optoelectronic delay setup."}], "references": [{"title": "Constructing optimized binary masks for reservoir computing with delay systems", "author": ["Lennert Appeltant", "Guy Van der Sande", "Jan Danckaert", "Ingo Fischer"], "venue": "Scientific reports,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Parallel photonic information processing at gigabyte per second data rates using transient states", "author": ["Daniel Brunner", "Miguel C Soriano", "Claudio R Mirasso", "Ingo Fischer"], "venue": "Nature communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Locomotion without a brain: physical reservoir computing in tensegrity structures", "author": ["Ken Caluwaerts", "Michiel D\u2019Haene", "David Verstraeten", "Benjamin Schrauwen"], "venue": "Artificial life,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A fast online algorithm for large margin training of online continuous density hidden markov models", "author": ["Chih-Chieh Cheng", "Fei Sha", "Lawrence Saul"], "venue": "In Interspeech", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["Dan Cire\u015fan", "Ueli Meier", "Luca Maria Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Cire\u015fan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Pattern recognition in a bucket", "author": ["Chrisantha Fernando", "Sampsa Sojakka"], "venue": "In Proceedings of the 7th European Conference on Artificial Life,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "National Institute of Standards, Technology (US, Linguistic Data Consortium, Information Science, Technology Office, United States, and Defense Advanced Research Projects Agency)", "author": ["John Garofolo"], "venue": "TIMIT Acoustic-phonetic Continuous Speech Corpus. Linguistic Data Consortium,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Towards a theoretical foundation for morphological computation with compliant bodies", "author": ["Helmut Hauser", "Auke J Ijspeert", "Rudolf M F\u00fcchslin", "Rolf Pfeifer", "Wolfgang Maass"], "venue": "Biological cybernetics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Optoelectronic systems trained with backpropagation through time", "author": ["Michiel Hermans", "Joni Dambre", "Peter Bienstman"], "venue": "IEEE Transactions in Neural Networks and Learning Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Automated design of complex dynamic systems", "author": ["Michiel Hermans", "Benjamin Schrauwen", "Peter Bienstman", "Joni Dambre"], "venue": "PloS One,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Short term memory in echo state networks", "author": ["Herbert Jaeger"], "venue": "Technical Report GMD Report 152,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless telecommunication", "author": ["Herbert Jaeger", "Harald Haas"], "venue": "Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Pac-bayesian approach for minimization of phoneme error rate", "author": ["Joseph Keshet", "David McAllester", "Tamir Hazan"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Ikeda-based nonlinear delayed dynamics for application to secure optical transmission systems using chaos", "author": ["Laurent Larger", "Jean-Pierre Goedgebuer", "Vladimir Udaltsov"], "venue": "Comptes Rendus Physique,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Photonic information processing beyond turing: an optoelectronic implementation of reservoir computing", "author": ["Laurent Larger", "Miguel C Soriano", "Daniel Brunner", "Lennert Appeltant", "Jose M Guti\u00e9rrez", "Luis Pesquera", "Claudio R Mirasso", "Ingo Fischer"], "venue": "Optics express,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["Mantas Lukosevicius", "Herbert Jaeger"], "venue": "Computer Science Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Real-time computing without stable states: A new framework for neural computation based on perturbations", "author": ["Wolfgang Maass", "Thomas Natschl\u00e4ger", "Henri Markram"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Optoelectronic reservoir computing", "author": ["Yvan Paquot", "Francois Duport", "Antoneo Smerieri", "Joni Dambre", "Benjamin Schrauwen", "Marc Haelterman", "Serge Massar"], "venue": "Scientific Reports,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Minimum complexity echo state network", "author": ["Ali Rodan", "Peter Tino"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Discriminative recurrent sparse auto-encoders", "author": ["Jason Tyler Rolfe", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3775,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Learning internal representations by error propagation", "author": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1986}, {"title": "Opto-electronic reservoir computing: tackling noise-induced performance degradation", "author": ["Miguel C Soriano", "Silvia Ort\u0301\u0131n", "Daniel Brunner", "Laurent Larger", "Claudio Mirasso", "Ingo Fischer", "L\u00fa\u0131s Pesquera"], "venue": "Optics express,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity", "author": ["Jochen Steil"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Phoneme recognition with large hierarchical reservoirs", "author": ["Fabian Triefenbach", "Azaraksh Jalalvand", "Benjamin Schrauwen", "Jean-Pierre Martens"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Toward optical signal processing using photonic reservoir computing", "author": ["Kristof Vandoorne", "Wouter Dierckx", "Benjamin Schrauwen", "David Verstraeten", "Roel Baets", "Peter Bienstman", "Jan van Campenhout"], "venue": "Optics Express,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Experimental demonstration of reservoir computing on a silicon photonics chip", "author": ["Kristof Vandoorne", "Pauline Mechet", "Thomas Van Vaerenbergh", "Martin Fiers", "Geert Morthier", "David Verstraeten", "Benjamin Schrauwen", "Joni Dambre", "Peter Bienstman"], "venue": "Nature communications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Strongly asymmetric square waves in a time-delayed system", "author": ["Lionel Weicker", "Thomas Erneux", "Otti DHuys", "Jan Danckaert", "Maxime Jacquot", "Yanne Chembo", "Laurent Larger"], "venue": "Physical Review E,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on stateof-the-art performances on tasks such as computer vision ([17, 6]).", "startOffset": 184, "endOffset": 191}, {"referenceID": 5, "context": "This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on stateof-the-art performances on tasks such as computer vision ([17, 6]).", "startOffset": 184, "endOffset": 191}, {"referenceID": 29, "context": "Even though GPUs have been used to speed up training RNNs ([30, 12]), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks.", "startOffset": 59, "endOffset": 67}, {"referenceID": 11, "context": "Even though GPUs have been used to speed up training RNNs ([30, 12]), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks.", "startOffset": 59, "endOffset": 67}, {"referenceID": 21, "context": "Recent attempts to solve this problem using the Hessian-free approach have proved promising ([22]).", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "Other attempts using stochastic gradient descent combined with more heuristic ideas have been described in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 13, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 14, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 20, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 27, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 19, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 7, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 9, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 18, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 22, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 2, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 243, "endOffset": 246}, {"referenceID": 31, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 275, "endOffset": 283}, {"referenceID": 32, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 275, "endOffset": 283}, {"referenceID": 18, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 22, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 26, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 23, "context": "way (by ensuring a high diversity in the network\u2019s response ([24, 1])), a way to create task-specific input encodings is still lacking.", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "way (by ensuring a high diversity in the network\u2019s response ([24, 1])), a way to create task-specific input encodings is still lacking.", "startOffset": 61, "endOffset": 68}, {"referenceID": 12, "context": "In [13], the possibility to use backpropagation through time (BPTT) ([26]) as a generic optimization tool for physical dynamical systems was addressed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [13], the possibility to use backpropagation through time (BPTT) ([26]) as a generic optimization tool for physical dynamical systems was addressed.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "In [11] simulated results of BPTT used as an optimization method for input encoding in the physical system described above were presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda-type dynamics ([18, 34]).", "startOffset": 105, "endOffset": 113}, {"referenceID": 33, "context": "The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda-type dynamics ([18, 34]).", "startOffset": 105, "endOffset": 113}, {"referenceID": 18, "context": "The measured output signal is well described by the following differential equation ([19]):", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Note that the parameters \u03b2 and \u03c6, together with the global scaling of the input signal z(t), control the global dynamical behavior of the system ([19]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 22, "context": "In fact, using a difference of one masking step between D and P has been the basis for opto-electronic systems that do not have a low-pass filter ([23]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "In [13] it was shown that BPTT can be applied to models of continuous-time dynamical systems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "To classify static images using a dynamical system, we follow an approach similar to the one introduced in [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "For training we used Nesterov momentum ([29]), with momentum coefficient 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "0% [7] [5]", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "0% [7] [5]", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "We applied frame-wise phoneme recognition to the TIMIT dataset ([9]).", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "For an overview of other results on frame error rate please check [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "This has been evidenced in, for example, [31], where good results on the TIMIT dataset were achieved by using Echo State Networks (a particular kind of Reservoir Computing) of up to 20,000 nodes.", "startOffset": 41, "endOffset": 45}], "year": 2015, "abstractText": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers. \u2217OPERA Photonique, Universit/\u2019e Libre de Bruxelles, Avenue F. Roosevelt 50, 1050 Brussels (michiel.hermans@ulb.ac.be) \u2020Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain \u2021ELIS departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00a7INTEC departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00b6Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain 1 ar X iv :1 50 1. 02 59 2v 1 [ cs .N E ] 1 2 Ja n 20 15", "creator": "LaTeX with hyperref package"}}}