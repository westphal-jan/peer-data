{"id": "1501.04324", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2015", "title": "Phrase Based Language Model For Statistical Machine Translation", "abstract": "We look at Phrase-based Language Models (LMs), which generalize common word-level models. Similar concepts for Phrase-based LMs exist in speech recognition, which is more specialized and therefore less suitable for Machine Translation (MT). In contrast to the dependence on LM, we first introduce comprehensive LMs tailored to the use of phrases. Initial experimental results show that our approach surpasses word-based LMs in terms of perplexity and translation quality.", "histories": [["v1", "Sun, 18 Jan 2015 16:37:53 GMT  (20kb)", "http://arxiv.org/abs/1501.04324v1", "5 pages. This version of the paper was submitted for review to EMNLP 2013. The title, the idea and the content of this paper was presented by the first author in the machine translation group meeting at the MSRA-NLC lab (Microsoft Research Asia, Natural Language Computing) on July 16, 2013"]], "COMMENTS": "5 pages. This version of the paper was submitted for review to EMNLP 2013. The title, the idea and the content of this paper was presented by the first author in the machine translation group meeting at the MSRA-NLC lab (Microsoft Research Asia, Natural Language Computing) on July 16, 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jia xu", "geliang chen"], "accepted": false, "id": "1501.04324"}, "pdf": {"name": "1501.04324.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xu@tsinghua.edu.cn,", "cglcgl200208@126.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n04 32\n4v 1\n[ cs\n.C L\n] 1\n8 Ja\nn 20\nWe consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality."}, {"heading": "1 Introduction", "text": "Statistical language models estimating the distribution of various natural language phenomena are crucial for many applications. In machine translation, it measures the fluency and well-formness of a translation, and therefore is important for the translation quality, see (Och, 2002) and (Koehn, Och and Marcu, 2003) etc.\nCommon applications of LMs include estimating the distribution based on N-gram coverage of words, to predict word and word orders, as in (Stolcke, 2002) and (Lafferty et. al., 2001). The independence assumption for each word is one of the simplifying method widely adopted. However, it does not hold in textual data, and\n* This version of the paper was submitted for review to EMNLP 2013. The title, the idea and the content of this paper was presented by the first author in the machine translation group meeting at the MSRA-NLC lab (Microsoft Research Asia, Natural Language Computing) on July 16, 2013.\nunderlying content structures need to be investigated as discussed in (Gao et. al., 2004).\nWe model the prediction of phrase and phrase orders. By considering all word sequences as phrases, the dependency inside a phrase is preserved, and the phrase level structure of a sentence can be learned from observations. This can be considered as an n-gram model on the n-gram of words, therefore word based LM is a special case of phrase based LM if only singleword phrases are considered. Intuitively our approach has the following advantages:\n1) Long distance dependency : The phrase based LM can capture the long distance relationship easily. To capture the sentence level dependency, e.g. between the first and last word of the sentence in Table 1, we need a 7-gram word based LM, but only a 3-gram phrase based LM, if we take \u201cplayed the basketball\u201d and \u201cthe day before yesterday\u201d as phrases.\n2) Consistent translation unit with phrase based MT : Some words may acquire meaning only in context, such as \u201cday\u201d, or \u201cthe\u201d in \u201cthe day before yesterday\u201d in Table 1. Considering the frequent phrases as single units will reduce the entropy of the language model. More importantly, current MT is performed on phrases, which is taken as the translation unit. The translation task is to predict the next phrase, which corresponds to the phrased based LM.\n3) Fewer independence assumptions in statistical models: The sentence probability is computed as the product of the single word probabilities in the word based n-gram LM and the product of the phrase probabilities in the phrase based n-gram LM, given their histories. The less\nwords/phrases in a sentence, the fewer mistakes the LM may contain due to less independence assumption on words/phrases. Once the phrase segmentation is fixed, the number of elements via phrase based LM is much less than that via the word based LM. Therefore, our approach is less likely to obtain errors due to assumptions.\n4) Phrase boundaries as additional information: We consider different segmentation of phrases in one sentence as a hidden variable, which provides additional constraints to align phrases in translation. Therefore, the constraint alignment in the blocks of words can provide more information than the word based LM.\nComparison to Previous Work In the dependency or structured LM, phrases corresponding to the grammars are considered, and dependencies are extracted, such as in (Gao et. al., 2004) and in (Shen et. al., 2008). However, in the phrase based SMT, even phrases violating the grammar structure may help as a translation unit. For instance, the partial phrase \u201cthe day before\u201d may appear both in \u201cthe day before yesterday\u201d and \u201cthe day before Spring\u201d. Most importantly, the phrase candidates in our phrase based LM are same as that in the phrase based translation, therefore are more consistent in the whole translation process, as mentioned in item 2 in Section 1.\nSome researchers have proposed their phrase based LM for speech recognition. In (Kuo and Reichl, 1999) and (Tang, 2002), new phrases are added to the lexicon with different measure function. In (Heeman and Damnati, 1997), a different LM was proposed which derived the phrase probabilities from a language model built at\nthe lexical level. Nonetheless, these methods do not consider the dependency between phrases and the re-ordering problem, and therefore are not suitable for the MT application."}, {"heading": "2 Phrase Based LM", "text": "We are given a sentence as a sequence of words wI1 = w1w2 \u00b7 \u00b7 \u00b7wi \u00b7 \u00b7 \u00b7wI(i \u2208 1, 2, \u00b7 \u00b7 \u00b7 , I), where I is the sentence length.\nIn the word based LM (Stolcke, 2002), the probability of a sentence Pr(wI1)\n1is defined as the product of the probabilities of each word given its previous n\u2212 1 words:\nP(wI1) =\nI \u220f\ni=1\nP(wi|w i\u22121 i\u2212n+1) (1)\nThe positions of phrase boundaries on a word sequence wI1 is indicated by k0 \u2261 0 and K = kJ1 = k1k2 \u00b7 \u00b7 \u00b7 kj \u00b7 \u00b7 \u00b7 kJ (j \u2208 1, 2, \u00b7 \u00b7 \u00b7 , J), where kj \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , I}, kj\u22121 < kj , kJ \u2261 I, and J is the number of phrases in the sentence. We use kj to indicate that the j-th phrase segmentation is placed after the word wkj and in front of word wkj+1, where 1 \u2264 j \u2264 J . k0 is a boundary on the left side of the first word w1, which is defined as 0, and kJ is always placed after the last word wI and therefore equals I.\nAn example is illustrated in Table 1. The English sentence (wI1) contains seven words (I = 7), where w1 denotes \u201cJohn\u201d, etc. The first phrase segmentation boundary is placed after the first word, and the second boundary is after the third word (k = 3) and so on. The phrase sequence pJ1 in this sentence have a different order\n1The notational convention will be as follows: we use the symbol Pr to denote general probability distributions with (almost) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol P(\u2206).\nthan that in its translation, on the phrase level. Hence, the phrase based LM advances the word based LM in learning the phrase re-ordering.\n(1) Model description Given a sequence of words wI1 and its phrase segmentation boundaries kJ1 , a sentence can also be represented in the form of a sequence of phrases pJ1 = p1p2 \u00b7 \u00b7 \u00b7 pj \u00b7 \u00b7 \u00b7 pJ(j \u2208 1, 2, \u00b7 \u00b7 \u00b7 , J), and each individual phrase pj is defined as\npj = wkj\u22121+1 \u00b7 \u00b7 \u00b7wkj = w kj kj\u22121+1\nIn phrase based LM, we consider the phrase segmentation kJ1 as hidden variable and the Equation 1 can be extended as follows:\nPr(wI1) = \u2211\nK\nPr(wI1 ,K)\n= \u2211\nkJ 1 ,J\nPr(pJ1 |k J 1 ) \u00b7Pr(k J 1 ) (2)\n(2) Sentence probability For the segmentation prior probability, we assume a uniform distribution for simplicity, i.e. P(kJ1 ) = 1/|K|, where the number of different K, i.e. |K| = 2I if not considering the maximum phrase or phrase n-gram length; To compute the Pr(wm1 ), we consider either two approaches:\n\u2022 Sum Model (Baum-Welch)\nWe consider all 2I segmentation candidates. Equation 2 is defined as\nPrsum(w I 1) \u2248\n\u2211\nkJ 1 ,J\nJ \u220f\nj=1\nP(pj |p j\u22121 j\u2212n+1) \u00b7P(k J 1 ),\n\u2022 Max Model (Viterbi)\nThe sentence probability formula of the second model is defined as\nPmax(w I 1) \u2248 max\nkJ 1 ,J\nJ \u220f\nj=1\nP(pj |p j\u22121 j\u2212n+1) \u00b7P(k J 1 ).\nIn practice we select the segmentation that maximizes the perplexity of the sentence instead of the probability to consider the length normalization.\n(3) Perplexity Sentence perplexity and text perplexity in the sum model use the same definition as that in the word based LM. Sentence perplexity in the max model is defined as\nPPL(wI1) = argmin kJ 1 ,J [P(wI1 , k J 1 )] \u22121/J\n. (4) Parameter estimation We apply maximum likelihood to estimate probabilities in both sum model and max model :\nP(pi|p i\u22121 i\u2212n+1) =\nC(pi)\nC(pi\u22121i\u2212n+1) , (3)\nwhere C(\u00b7) is the frequency of a phrase. The unigram phrase probability is P(p) = C(p)C , and C is the frequency of all single phrases, in the training text. Since we generate exponential number of phrases to the sentence length, the number of parameters is huge. Therefore, we set the maximum n-gram length on the phrase level (note not the phrase length) as N = 3 in experiments.\n(5) Smoothing For the unseen events, we perform Good-Turing smoothing as commonly done in word based LMs. Moreover, we interpolate between the phrase probability and the product of single word probabilities in a phrase using a convex optimization:\nP\u2217(pj |p j\u22121 j\u2212n+1) =\n\u03bbP(pj |p j\u22121 j\u2212n+1) + (1\u2212 \u03bb)\n\u220fj\u2032\ni=1P(wi) ( \u2211\nw P(w) )j\u2032\nwhere phrase pj is made up of j \u2032 words wj\n\u2032\n1 . The idea of this interpolation is to make the probability of a phrase consisting of of j\u2032 words smooth with a j\u2032-word unigram probability after normalization. In our experiments, we set \u03bb = 0.4 for convenience.\n(6) Algorithm of calculating phrase ngram counts The training task is to calculate n-gram counts on the phrase level in Equation 3.\nGiven a training corpus W S1 , where there are S sentences Ws (s = 1, 2, \u00b7 \u00b7 \u00b7 , S), our goal is to to compute C(\u00b7), for all phrase n-grams that the number of phrases is no greater than N . Therefore, for each sentence wI1, we should find out every n-gram phrases that 0 < n < N .\nWe do Dynamic Programming to collect the phrase n-grams in one sentence wI1 :\nQ(1, d;wI1) = {p = w d b ,\u22001 \u2264 b \u2264 d \u2264 I} Q(n, d;wI1) =\n\u222ab Q(n\u2212 1, b\u2212 1;w I 1)\u2295 p = w d b , \u2200n \u2264 b \u2264 d \u2264 I,\nwhere Q(\u00b7) is the auxiliary function denoting the multiset of all phrase n-grams or unigram ending at position d (1 < n \u2264 N). b denotes the starting word position of the last phrase in the multiset. The {\u00b7} is a multiset, and \u2295 means to append the element to each element in the multiset. \u222ab denotes the union of multisets. After appending p, we consider all b that is no less than n and no greater than d.\nThe phrase counts C(\u00b7) is the sum of all phrase n-grams from all sentences W S1 , with each sentenceWs = w I 1, and |\u00b7| is the number of elements in a multiset:\nC(pn1 ) = S \u2211\ns=1\n|pn1 \u2208 \u222a |Ws| d=nQ(n, d;Ws)|"}, {"heading": "3 Experiments", "text": "This is an ongoing work, and we performed preliminary experiments on the IWSLT (IWSLT, 2011) task, then evaluated the LM performance by measuring the LM perplexity and the MT translation performance.\nBecause of the computational requirement, we only employed sentences which contain no more than 15 words in the training corpus and no more than 10 words in the test corpora (Dev2010, on Tst2010 and on Tst2011), as shown in Table 2.\nWe took word based LM in Equation 1 as the baseline method (Base). We calculated the perplexities of Tst2011 with different n-gram orders using both sum model and max model, with and without smoothing (S.) as in Section 2. Table 3 shows that perplexities in our approaches are all lower than those in the baseline.\nFor MT, we selected the single best translation output based on the LM perplexity of the 100-best translation candiates, using different LMs as shown in Table 4. Max model along with smoothing outperforms the baseline method under all three test sets with the BLEU score (Papineni et. al., 2002) increase of 0.3% on Dev2010, 0.45% on Tst2010, and 0.22% on Tst2011, respectively.\nTable 5 shows two examples from the Tst2010, where we can see that our max model generates better selection results than the baseline method in these cases."}, {"heading": "4 Conclusion", "text": "We showed the preliminary results that a phrase based LM can improve the performance of MT systems and the LM perplexity. We presented two phrase based models which consider phrases as the basic components of a sentence and perform exhaustive search. Our future work will focus on the efficiency for a larger data track as well as the improvements on the smoothing methods."}], "references": [{"title": "The Theory of Parsing, Translation and Compiling, volume", "author": ["Aho", "Ullman1972] Alfred V. Aho", "Jeffrey D. Ullman"], "venue": null, "citeRegEx": "Aho et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Aho et al\\.", "year": 1972}, {"title": "Alternation. Journal of the Association for Computing Machinery, 28(1):114\u2013133", "author": ["Dexter C. Kozen", "Larry J. Stockmeyer"], "venue": null, "citeRegEx": "Chandra et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Chandra et al\\.", "year": 1981}, {"title": "Algorithms on Strings, Trees and Sequences", "author": ["Dan Gusfield"], "venue": null, "citeRegEx": "Gusfield.,? \\Q1997\\E", "shortCiteRegEx": "Gusfield.", "year": 1997}, {"title": "Phrase-Based Language Models for Speech Recognition", "author": ["Kuo", "Reichl1999] Hong-Kwang Jeff Kuo", "Wolfgang Reichl"], "venue": "In EUROSPEECH", "citeRegEx": "Kuo et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kuo et al\\.", "year": 1999}, {"title": "Building Phrase Based Language Model from Large Corpus", "author": ["Haijiang Tang"], "venue": null, "citeRegEx": "Tang.,? \\Q2002\\E", "shortCiteRegEx": "Tang.", "year": 2002}, {"title": "Statistical Machine Translation: From Single Word Models to Alignment Templates", "author": ["F. Och"], "venue": "Ph.D. thesis, RWTH Aachen,", "citeRegEx": "Och.,? \\Q2002\\E", "shortCiteRegEx": "Och.", "year": 2002}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni et"], "venue": "In Proc. of ACL,", "citeRegEx": "et.,? \\Q2002\\E", "shortCiteRegEx": "et.", "year": 2002}, {"title": "Statistical phrasebased translation", "author": ["Koehn", "Och", "Marcu2003] P. Koehn", "F. Och", "D. Marcu"], "venue": "In Proc. of HLT-NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Deriving Phrase-based Language Models", "author": ["Heeman", "Damnati1997] Peter A. Heeman", "Geraldine Damnati"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Heeman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Heeman et al\\.", "year": 1997}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown", "S. Della Pietra", "V. Della Pietra", "R. Mercer1993] P. Brown", "R. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Dependence language model for information retrieval", "author": ["Gao et"], "venue": "In Proc. of ACM,", "citeRegEx": "et.,? \\Q2004\\E", "shortCiteRegEx": "et.", "year": 2004}, {"title": "A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language", "author": ["Shen et"], "venue": null, "citeRegEx": "et.,? \\Q2008\\E", "shortCiteRegEx": "et.", "year": 2008}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["R. Rosenfeld"], "venue": "Proc. of IEEE,", "citeRegEx": "Rosenfeld.,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld.", "year": 2000}, {"title": "SRILM-an extensible language modeling toolkit. In INTERSPEECH", "author": ["A. Stolcke"], "venue": null, "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty et"], "venue": "In Intl. Conf. on Machine Learning,", "citeRegEx": "et.,? \\Q2001\\E", "shortCiteRegEx": "et.", "year": 2001}, {"title": "Discriminative language modeling with conditional random fields and the perceptron algorithm", "author": ["Roark", "Saraclar", "Collins", "Johnson2004] B. Roark", "M. Saraclar", "M. Collins", "M. Johnson"], "venue": "In Proc. of ACL,", "citeRegEx": "Roark et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2004}, {"title": "A structured language model", "author": ["C. Chelba"], "venue": "In Proc. of ACL,", "citeRegEx": "Chelba.,? \\Q1997\\E", "shortCiteRegEx": "Chelba.", "year": 1997}], "referenceMentions": [{"referenceID": 5, "context": "In machine translation, it measures the fluency and well-formness of a translation, and therefore is important for the translation quality, see (Och, 2002) and (Koehn, Och and Marcu, 2003) etc.", "startOffset": 144, "endOffset": 155}, {"referenceID": 13, "context": "Common applications of LMs include estimating the distribution based on N-gram coverage of words, to predict word and word orders, as in (Stolcke, 2002) and (Lafferty et.", "startOffset": 137, "endOffset": 152}, {"referenceID": 4, "context": "In (Kuo and Reichl, 1999) and (Tang, 2002), new phrases are added to the lexicon with different measure function.", "startOffset": 30, "endOffset": 42}, {"referenceID": 13, "context": "In the word based LM (Stolcke, 2002), the probability of a sentence Pr(wI 1) 1is defined as the product of the probabilities of each word given its previous n\u2212 1 words:", "startOffset": 21, "endOffset": 36}], "year": 2015, "abstractText": "We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality.", "creator": "LaTeX with hyperref package"}}}