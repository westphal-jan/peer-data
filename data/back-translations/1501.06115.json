{"id": "1501.06115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2015", "title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "abstract": "Extreme Learning Machine (ELM) is an extremely fast learning method and performs well in pattern recognition tasks demonstrated by tremendous researchers and engineers. However, its good generalization ability is based on a large number of hidden neurons, which is not advantageous for real-time response during the test process. In this paper, we proposed new ways, so-called \"confined extreme learning machines\" (CELMs), to randomly select hidden neurons based on sample distribution. Compared to the completely random selection of hidden nodes in the ELM, the CELMs randomly select hidden nodes from the limited vector space, which contains some basic combinations of original sample vectors. Experimental results show that the CELMs have better generalization capability than traditional ELM, SVM and some other related methods.", "histories": [["v1", "Sun, 25 Jan 2015 05:11:34 GMT  (623kb)", "http://arxiv.org/abs/1501.06115v1", "14 pages, 6 figure, journel"], ["v2", "Wed, 4 Feb 2015 11:42:01 GMT  (589kb)", "http://arxiv.org/abs/1501.06115v2", "14 pages, 6 figure, journel"]], "COMMENTS": "14 pages, 6 figure, journel", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["wentao zhu", "jun miao", "laiyun qing"], "accepted": false, "id": "1501.06115"}, "pdf": {"name": "1501.06115.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wentaozhu1991@gmail.com)."], "sections": [{"heading": null, "text": "Index Terms\u2014Feedforward neural networks, extreme learning machine, sample based learning, discriminative feature mapping.\nfast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers. However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process. In this paper, we proposed new ways, named \u201cconstrained extreme learning machines\u201d (CELMs), to randomly select hidden neurons based on sample distribution. Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors. The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods. Additionally, the CELMs have a similar fast learning speed as ELM.\nI. INTRODUCTION ANY neural network architectures have been developed over the past several decades. The feedforward neural networks are especially the most popular ones studied all\nthe times. It has been proven that the learning capacity of a multilayer feedforward neural network with non-polynomial activation functions can approximate any continuous function [1]. Single hidden layer feedforward neural networks (SLFNs) was studied and applied extensively by researches because of their model simplicity and relatively high learning and responding speed. The learning capacity of SLFNs is not inferior to that of multilayer feedforward neural networks, as proved in [2, 3]. There are mainly three different ways to train the SLFNs:\n1. Gradient based learning methods. The most famous gradient based learning method is back-propagation algorithm [4]. However, these methods may face quite slow learning speed and stack into local minimal. Although many assistant methods were proposed to solve such problems, such as Levenberg-Marquardt\nThis work was supported in part by Natural Science Foundation of China\n(Nos. 61175115 and 61272320) and President Fund of Graduate University of Chinese Academy of Sciences (No. Y35101CY00).\nW. Zhu and J. Miao are with the Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China (e-mail: wentaozhu1991@gmail.com).\nL. Qing is with School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing 100049, China.\n(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.\n2. Optimization based learning methods. One of the most popular optimization based SLFNs is Support Vector Machine (SVM) [9]. The objective function of SVM is to optimize the weights for maximum margin corresponding to structural risk minimization. The solution of SVM can be obtained by convex optimization methods in the dual problem space and is the global optimal solution. SVM is a very popular method attracting many researchers [10].\n3. Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13]. These methods have quite fast learning speed due to the essence of \u201cNo-Prop\u201d and fast learning speed of LMS.\nIn recent years, Huang et al. [14] proposed a novel extremely fast learning model of SLFNs, called Extreme Learning Machine (ELM). One of its salient essences that the weights from the input layer to the hidden layer can be randomly generated was firstly shown by Tamura et al. [2]. Huang et al. [14] further completely proved the random feature mapping theory rigorously.\nAfter the random nonlinear feature mapping in the hidden layer, the rest of ELM can be considered as a linear system [15]. Therefore, ELM has a closed form of solution due to the simple network structure and random hidden layer weights. The essence of the linear system used by ELM is to minimize the training error and the norm of connection weights from the hidden layer to the output layer at the same time [16]. Hence ELM has a good generalization performance according to the feedforward neural network theory [12, 17]. As a consequence, ELM has some desirable features, such as that hidden layer parameters need not be tuned, fast learning speed and good generalization performance. Additionally, ELM has a unified framework for classification, regression, semi-supervised, supervised and unsupervised tasks [16, 18]. These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].\nHowever, the random selection of hidden layer parameters makes quite inefficient use of hidden nodes [23]. ELM usually has to randomly generate a great number of hidden nodes to achieve desirable performance. This leads to time consuming in test process, which is not helpful in real applications. Large numbers of hidden nodes also easily make the trained model\nM\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n2\nstack into over fitting. There are mainly three ways to solve the problem for a more compact model:\n1. Use online incremental learning methods to add hidden layer nodes dynamically [23, 24]. These methods randomly generate parts or all of the hidden layer nodes, and then select the candidate hidden nodes one by one or chunk by chunk with fixed or varying chunk size. Whether the hidden layer node is added or not is usually depending on the objective function of the output layer. 2. Use pruning methods to select the candidate hidden layer nodes [25, 26]. These methods start with a large neural network using the traditional ELM, and then apply some metrics, such as statistical criteria and multi-response sparse regression, to rank these hidden nodes. Finally, eliminate those low relevance hidden nodes to form a more compact neural network structure. 3. Use gradient based methods to update the weights from the input layer to the hidden layer in ELM [27]. After randomly initialize the weights from the input layer to the hidden layer and use a close-form least square solution to calculate the weights from the hidden layer to the output layer, these methods use the gradient descending method to update the weights from the input layer to the hidden layer in ELM iteratively.\nThe above methods can overcome the drawbacks of the traditional ELM to some degree. However, they do not solve the problem directly from the essence of hidden nodes. Besides, these methods are somewhat time-consuming.\nThe essence of hidden layer functions is to map the data into a feature space, where the output layer can use a linear classifier to separate the data perfectly. Therefore, the hidden layer should extract discriminative features or some other data representations for classification tasks.\nLDA [28] is probably the most commonly used method to extract discriminative features. However, traditional LDA has some drawbacks, such as that the number of feature-mapped dimensions is less than the number of classes, \u201cSmall Sample Size\u201d (SSS) problem and Gaussian distribution assumption of equal covariance and different means. Su et al. [29] proposed a projection pursuit based LDA method to overcome these problems. The method showed that the difference vectors of between-class samples have a strong discriminative property for classification tasks, but this method is rather complex with many embedded trivial tricks.\nIn this work, to balance the high discriminative feature learning and the fast training speed of the ELM, we propose a novel model, called Constrained difference Extreme Learning Machine (CDELM), which utilizes a random subset of difference vectors of between-class samples to replace the completely random connection weights from the input layer to the hidden layer in ELM. More generally, the linear combination of sample vectors, such as sum vectors of within-class samples, sample vectors of all classes, sum vectors of randomly selected sample vectors and the mixed vectors including difference vectors of between-class samples and sum vectors of within-class samples, as connection weights from the input layer to the hidden layer are validated. We proposed Constrained Sum Extreme Learning Machine (CSELM),\nSample Extreme Learning Machine (SELM), Random Sum Extreme Learning Machine (RSELM) and Constrained Mixed Extreme Learning Machine (CMELM) based on these data-driven hidden layer features mapping ways. Experimental results show that, CELMs has better generalization ability than ELM related methods, SVM related methods and the BP neural network, whilst retaining the fast learning characteristics of ELM. We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32]. The results show that the CELM algorithms outperform these methods. The remaining part of the paper is organized as follows: in section \u2161, we review the traditional ELM algorithm, and then propose the CELMs in section \u2162. Experiments are presented in section \u2163. Conclusion and discussion are given in section \u2164."}, {"heading": "II. REVIEW OF EXTREME LEARNING MACHINE", "text": "ELM is a type of SLFNs. The hidden layer parameters, i.e.,\nthe connection weights from the input layer to the hidden nodes, are randomly generated in ELM. The output layer is a linear system, where the connection weights from the hidden layer to the output layer are learned by computing the Moore-Penrose generalized inverse [14]. The ELM network has an extreme high learning speed due to the simple network structure and its closed form solution. Additionally, the randomness makes ELM not necessarily tune these hidden layer parameters iteratively.\nGiven the training samples and class labels\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n3\n={( , )| , , 1, , }n mi i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the number of hidden nodes L and activation function ( , , )G ba x , where n\u2208x R is the input vector, n\u2208a R is the associated connection weight vector and b \u2208 R is the bias, the algorithm of ELM network can be summarized as the following three steps:\nStep 1: Assign the parameters {( , ) | 1, , }j jb j L=a  of hidden nodes with randomly generated values.\nStep 2: Calculate the hidden layer output matrix H for all the training samples:\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n , where ( , , )i i jG ba x is the activation function of ith hidden node for jth sample.\nStep 3: Calculate the hidden layer\u2019s output connection\nweights \u03b2 by solving the least squares problem: \u2020=\u03b2 H T\n, where \u2020H is the generalized inverse matrix of the matrix H ,\nand 1 =\nT\nT N N m\u00d7          t T t  .\nHowever, the condition number of the random projected matrix H may be very large and the above traditional ELM model may encounter ill-posed problems [33]. In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36]. The solution of regularized ELM can be obtained as\n1= ( + )T T\u03b2 \u03bb\n\u2212IH HH T\nor 1=( + )T T\u03b2 \u03bb\n\u2212I H H H T\n, where I is the identity matrix and \u03bb is the regularization factor which can be obtained by cross validation in the training process.\nAs analyzed in theory and further verified by the simulation results in [34], ELM for classification tends to achieve better generalization performance than traditional SVM. ELM can also overcome the local minimal problem that BP neural nets faced, due to its convex model structure. The learning speed of ELM is extremely fast at the same time."}, {"heading": "III. CONSTRAINED EXTREME LEARNING MACHINES", "text": "In this section, we will introduce the CELMs with the idea of using simple linear combination of sample vectors to generate hidden nodes in the traditional ELM network structure."}, {"heading": "A. Constrained Difference Extreme Learning Machine", "text": "The Constrained Difference Extreme Learning Machine (SCELM) attempts to extract discriminative features in the hidden layer. The completely random parameters in the hidden layer of ELM do not always represent discriminative features. Such unconstrained random parameters may make ELM has to\ngenerate a great number of hidden nodes to meet desirable generalization performances. More hidden nodes mean more processing time and more easily over fitting. These problems in ELM should be solved.\nAlthough the method [29] is rather complex with many embedded trivial tricks, it shows that the difference vectors of between-class samples are effective to classification tasks. Considering the simplicity and the extreme high learning speed of the ELM, we extend the ELM model to Constrained Difference Extreme Learning Machine (CDELM) by constraining the weight vector parameters { | 1, , }j j L=a  of ELM to be randomly drawn from a closed set of difference vectors of between-class samples instead of from the open set of arbitrary vectors to tackle the problem of generation of discriminative hidden nodes. We use a simple case to illustrate the idea of difference vectors of between-class samples.\nThe essence of the weight vectors from the input layer to the neurons in the hidden layer is to map the original samples into a discriminative feature space spanned by these vectors, where the samples can be classified. The weight vectors are helpful for classification if the directions of the weight vectors are from\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n4\nclass 1 to class 2 or reversely, as illustrated in Fig. 1(a). The blocks in the figure represent the samples in class 1, and the circles represent the samples in class 2. As a comparison, the weight vectors from the input layer to the hidden nodes in ELM are completely random without constraints, as illustrated in Fig. 1(b). It can be inferred the weight vectors which do not follow the direction from class 1 to class 2 are less discriminative for classification tasks. This is the reason why not all the hidden nodes in ELM are efficient or discriminative for classification tasks.\nWe randomly generate the weight vectors from the input layer to the hidden layer with the differences of between-class samples as illustrated in Fig. 1(a). The difference vectors of between-class samples can map the samples to a higher\ndiscriminative feature space than ELM. The weight vectors from the input layer to the hidden layer in CDELM are illustrated in Fig. 1(c). The directions of these weight vectors are close to the direction from class 1 to class 2, which are more discriminative for the classification tasks intuitively.\nWe normalize the difference vectors as the weights from the input layer to the hidden layer. The reason why normalize these weights and how to normalize will be introduced in the following discussion. In our originally Constrained Difference Extreme Learning Machine [37], we deleted the difference vectors of small norms and too relevant difference vectors. These pre-processing operations are somewhat time-consuming especially when the number of hidden nodes is large. Although the two processes can improve the performance [38], the improvement is very little in our experiments.\nIn CDELM, the prior information of samples\u2019 class distribution is utilized to generate the weights from the input layer to the hidden layer. The aim is to split different classes\u2019 samples into different areas in the feature space. The ideal case is that, for example, class 1 is mapped into negative semi axis and class 2 is mapped into positive semi axis in the feature space. Hence the bias must be set as the middle point of the two selected samples from the geometric sense intuitively. As a result, the biases to the hidden neurons can be determined by assuming that the samples from one class are mapped to -1 and the samples from another class are mapped 1 respectively. Denote 1cx and 2cx as the samples drawn from two classes. Then the weight vector w from the input layer to one hidden neuron can be generated with 2 1( )c c\u03b1 \u2212x x , where \u03b1 is the normalized factor. The original data x is transformed to\n2 1( ) T T c cb b\u03b1+ = \u2212 +x w x x x by feature mapping, where b is the bias with respect to the weight vector w in ELM model. The assumption that 1cx and 2cx are mapped to -1 and 1 can be written as\n1 2 1( ) 1 T c c c b\u03b1 \u2212 + = \u2212x x x , and\n2 2 1( ) 1 T c c c b\u03b1 \u2212 + =x x x .\nWe can obtain that the normalization factor 2 2 2 1\n2\nc c L\n\u03b1 = \u2212x x\nand the corresponding bias 2 1 2 1 2 2 2 1 ( ) ( )Tc c c c c c L b + \u2212 = \u2212 x x x x x x by solving the above two equation constraints. The commonly used activation function for hidden neurons\nis sigmoid function 1( ) 1 x f x e\u2212 = + . The output layer in\nCDELM is a simple linear system as same as that of ELM. From the above discussion, the training algorithm for CDELM can be concluded in the Algorithm 1. The essence of CDELM is to constrain the hidden neuron\u2019s input connection weights to be consistent with the directions from one class to another class. So the random weights are constrained to be chosen from the set that is composed of the difference vectors of between-class samples.\nAlgorithm 1: Training of the Constrained Difference Extreme Learning Machine (CDELM)\nInput: the training\nsamples ={( , )| , , 1, , } n m i i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the hidden node number L and the activation\nfunction ( , , )G bw x . Output: the model parameters of CELM, i.e., the\nweight matrix n L\u00d7W and the bias vector 1 L\u00d7b from the input layer to the hidden layer, the weight matrix\nL m\u03b2 \u00d7 from the hidden layer to the output layer. 1) While the number of chosen difference vectors is\nless than L\na) Randomly draw training samples 1cx and 2cx from any two different classes respectively and generate the\ndifference vector 2 1c c\u2212x x ; b) Normalize the difference vector by\n2\n2 1 2\n2 1 2( )c c c c L \u2212 = \u2212 x x w\nx x , and calculate the corresponding bias\n2\n1 2 1 2 2\n2 1 ( ) ( )Tc c c c c c L b + \u2212 = \u2212 x x x x x x\n. c) Use the vector w and bias b to construct the\nweight matrix n L\u00d7W and bias vector 1 L\u00d7b . 2) Calculate the hidden layer output matrix H as\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n . 3) Calculate the hidden layer\u2019s output weight matrix\nL m\u03b2 \u00d7 by solving the least squares problem: \u2020=\u03b2 H T\n, where \u2020H is the generalized inverse matrix\nand\n1\n=\nT\nT N N m\u00d7          t T t \n.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n5"}, {"heading": "B. Sample Extreme Learning Machine", "text": "The Sample Extreme Learning Machine (SELM) utilizes\nsample vectors that are randomly drawn from the training set to construct the weights from the input layer to the hidden layer. The SELM firstly randomly selects a group of sample vectors,\nthen normalizes these vectors as 2 2 i i L\nx x\n, where ix is the\nchosen sample vector. After the normalization, the sample itself can be transformed of norm 1 to serve the linear classification in the output layer. The normalized sample vectors are assigned as the weights from the input layer to the hidden layer. The biases used in SELM are randomly generated from uniform distribution as same as that in ELM.\nThe SELM model is a little like kernel based methods. If the activation function is sigmoid function, we can formula the ith hidden node in SELM as\n1( , , ) ( , , ) 1 exp[ ( )]i i i i i i G b G b b = = + \u2212 + a x x x x x\n, where x is one input vector, ix is the ith hidden node weight, which is selected randomly from sample vectors. Actually,\n( , )i i iK b= +x x x x is a polynomial kernel function, which can tackle some linearly inseparable cases, such as data sets of quadratic curve. The difference between the kernel used here and SVM kernel is that, the samples in the SVM kernel function is the support vectors. The difference between kernel ELM [16] and our SELM is that ELM kernel uses all the training samples. The sigmoidal activation function used is to stretch the kernel mapped data and helps the linear classification in the output layer.\nFrom the above discussion, we can write the SELM algorithm as Algorithm 2. The essence of SELM is to constrain the hidden neuron\u2019s input connection weights to be consistent with the directions of sample vectors. So the original random weights are constrained to be chosen from the training set.\nAlgorithm 2: Training of the Sample Extreme Learning Machine (SELM)\nInput: the training\nsamples ={( , )| , , 1, , } n m i i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the hidden node number L and the activation\nfunction ( , , )G bw x . Output: the model parameters of SELM, i.e., the weight\nmatrix n L\u00d7W and the bias vector 1 L\u00d7b from the input layer to the hidden layer, the weight matrix L m\u03b2 \u00d7 from the hidden layer to the output layer.\n1) While the number of chosen sample vectors is less than L\na) Randomly draw samples ix from the training data;\nb) Normalize the sample vector by 2 2 i i L\n= x\nw x\n, and\ndraw the corresponding bias b randomly from [0,1]\nuniform distribution. c) Use the vector w and bias b to construct the weight\nmatrix n L\u00d7W and bias vector 1 L\u00d7b . 2) Calculate the hidden layer output matrix H as\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n . 3) Calculate the hidden layer\u2019s output weight matrix\nL m\u03b2 \u00d7 by solving the least squares problem: \u2020=\u03b2 H T\n, where \u2020H is the generalized inverse matrix\nand\n1\n=\nT\nT N N m\u00d7          t T t \n."}, {"heading": "C. Constrained Sum Extreme Learning Machine", "text": "The Constrained Sum Extreme Learning Machine (CSELM)\nutilizes sum vectors of random chosen within-class sample vectors to construct the weights from the input layer to hidden layer. The CSELM firstly randomly selects any two\nwithin-class sample vectors c\u2032x and c\u2032\u2032x , calculate the sum of\nthe two vectors c c\u2032 \u2032\u2032+x x , then normalizes the sum vector as\n2\n2 c c\nc c L\n\u2032 \u2032\u2032+\n\u2032 \u2032\u2032+\nx x\nx x . The normalized sum sample vectors are\nassigned as the weights from the input layer to the hidden layer. The biases used in CSELM are also randomly generated from the uniform distribution as same as that in ELM. The constrained sum vectors used here were firstly inspired by the difference vectors of between-class samples. The constrained sum vectors also can be considered as some derivative samples, which can somewhat weaken the affection of noise samples in"}, {"heading": "CSELM.", "text": "From the above discussion, we can design the SELM algorithm as Algorithm 3. The essence of CSELM is to constrain the hidden neuron\u2019s input connection weights to be consistent with the directions of derivative robust sample vectors. So the random weights are constrained to be chosen from the set that is composed of the sum vectors of within-class sample vectors. Algorithm 3: Training of the Constrained Sum Extreme Learning Machine (CSELM)\nInput: the training\nsamples ={( , )| , , 1, , } n m i i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the hidden node number L and the activation\nfunction ( , , )G bw x . Output: the model parameters of CSELM, i.e., the\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n6\nweight matrix n L\u00d7W and the bias vector 1 L\u00d7b from the input layer to the hidden layer, the weight matrix\nL m\u03b2 \u00d7 from the hidden layer to the output layer. 1) While the number of chosen constrained sum\nvectors is less than L a) Randomly draw training samples c\u2032x and c\u2032\u2032x from\nthe same class respectively and generate the sum vector\nc c \u2032 \u2032\u2032+x x ;\nb) Normalize the sum vector by\n2\n2 c c\nc c L\n\u2032 \u2032\u2032+ =\n\u2032 \u2032\u2032+\nx x w\nx x , and\ndraw the corresponding bias b randomly from [0,1] uniform distribution.\nc) Use the vector w and bias b to construct the\nweight matrix n L\u00d7W and bias vector 1 L\u00d7b . 2) Calculate the hidden layer output matrix H as\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n . 3) Calculate the hidden layer\u2019s output weight matrix\nL m\u03b2 \u00d7 by solving the least squares problem: \u2020=\u03b2 H T\n, where \u2020H is the generalized inverse matrix\nand\n1\n=\nT\nT N N m\u00d7          t T t \n."}, {"heading": "D. Random Sum Extreme Learning Machine", "text": "The Random Sum Extreme Learning Machine (RSELM)\nutilizes sum vectors of random sample vectors regardless of classes to construct the weights from the input layer to the hidden layer. The RSELM firstly randomly selects any two\nsample vectors \u2032x and \u2032\u2032x , calculate the sum of the two vectors\n\u2032 \u2032\u2032+x x , then normalizes the sum vector as\n2\n2\nL\n\u2032 \u2032\u2032+\n\u2032 \u2032\u2032+\nx x\nx x . The\nnormalized sum vectors are assigned as the weights from input layer to the hidden layer. The biases used in RSELM are also randomly generated from the uniform distribution as same as that in ELM. The sum vectors of random samples used here is to accelerate the speed of hidden layer weights generation.\nFrom the above discussion, we can design the RSELM algorithm as Algorithm 4. The essence of RSELM is to constrain the hidden neuron\u2019s input connection weights to be consistent with the sum vectors of random samples.\nAlgorithm 4: Training of the Random Sample Extreme Learning Machine (RSELM) Input: the training\nsamples ={( , )| , , 1, , } n m i i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the hidden node number L and the activation\nfunction ( , , )G bw x . Output: the model parameters of CELM, i.e., the weight\nmatrix n L\u00d7W and the bias vector 1 L\u00d7b from the input layer to the hidden layer, the weight matrix L m\u03b2 \u00d7 from the hidden layer to the output layer. 1) While the number of chosen random sum vectors is less than L a) Randomly draw training samples \u2032x and \u2032\u2032x from data samples;\nb) Normalize the difference vector by\n2\n2\nL\n\u2032 \u2032\u2032+ =\n\u2032 \u2032\u2032+ x xw x x\n, and\ndraw the corresponding bias b randomly from [0,1] uniform distribution. c) Use the vector w and bias b to construct the weight\nmatrix n L\u00d7W and bias vector 1 L\u00d7b . 2) Calculate the hidden layer output matrix H as\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n .\n3) Calculate the hidden layer\u2019s output weight matrix L m\u03b2 \u00d7 by solving the least squares problem:\n\u2020=\u03b2 H T , where \u2020H is the generalized inverse matrix\nand\n1\n=\nT\nT N N m\u00d7          t T t \n."}, {"heading": "E. Constrained Mixed Extreme Learning Machine", "text": "The Constrained Mixed Extreme Learning Machine\n(CMELM) utilizes mixed vectors, containing class-constrained difference vectors and class-constrained sum vectors, to construct the weights from the input layer to the hidden layer. The CMELM firstly generates half numbers of hidden nodes whose weights and biases are constructed with constrained sum vectors, then generates the others whose weights and biases are constructed with constrained difference vectors. The constrained sum vectors are normalized as same as that of CSELM, and the constrained difference vectors are normalized as same as that of CDELM. The normalized sum sample vectors are assigned as the weights from the input layer to the hidden layer. The constrained mixed vectors can be considered\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n7\nas model average (the average model of CDELM and CSELM) because of the linear property of the output layer.\nFrom the above discussion, we can design the CMELM algorithm as Algorithm 5. Algorithm 5: Training of the Constrained Mixed Extreme Learning Machine (CMELM) Input: the training\nsamples ={( , )| , , 1, , } n m i i i i i N\u2135 \u2208 \u2208 = \u2026x t x R t R , the hidden node number L and the activation\nfunction ( , , )G bw x . Output: the model parameters of CELM, i.e., the weight\nmatrix n L\u00d7W and the bias vector 1 L\u00d7b from the input layer to the hidden layer, the weight matrix L m\u03b2 \u00d7 from the hidden layer to the output layer. 1) While the number of constrained sum vectors is less than\n/2L  \na) Randomly draw training samples c\u2032x and c\u2032\u2032x from\nthe same class and generate the sum vector c c\u2032 \u2032\u2032+x x ;\nb) Normalize the sum vector by\n2\n2 c c\nc c L\n\u2032 \u2032\u2032+ =\n\u2032 \u2032\u2032+\nx x w\nx x , and\ndraw the corresponding bias b randomly from [0,1] uniform distribution.\nc) Use the vector w and bias b to construct the weight matrix ( /2 )n L\u00d7   W and bias vector 1 ( /2 )L\u00d7   b . 2) While the number of chose difference vectors is less than\n/2L L \u2212  \na) Randomly draw training samples 1cx and 2cx from any two different classes respectively and generate the\ndifference vector 2 1c c\u2212x x ; b) Normalize the difference vector by\n2\n2 1 2\n2 1 2( )c c c c L \u2212 = \u2212 x x w\nx x , and calculate the corresponding bias\n2\n1 2 1 2 2\n2 1 ( ) ( )Tc c c c c c L b + \u2212 = \u2212 x x x x x x\n. c) Use the vector w and bias b to construct the\nweight matrix ( /2 )n L L\u00d7 \u2212  W and bias vector 1 ( /2 )L L\u00d7 \u2212  b . 2) Concatenate the above ( /2 )n L\u00d7   W and ( /2 )n L L\u00d7 \u2212  W to form the hidden layer nodes weights, and concatenate the\n1 ( /2 )L\u00d7    b and 1 ( /2 )L L\u00d7 \u2212  b to form the hidden layer nodes\nbiases. Calculate the hidden layer output matrix H as\n1 1 1 1 1\n1 1\n( ) ( , , ) ( , , )\n( ) ( , , ) ( , , )\nL L\nN N L L N N L\nG b G b\nG b G b \u00d7        = =           h x a x a x H h x a x a x     \n .\n3) Calculate the hidden layer\u2019s output weight matrix L m\u03b2 \u00d7 by solving the least squares problem:\n\u2020=\u03b2 H T , where \u2020H is the generalized inverse matrix\nand\n1\n=\nT\nT N N m\u00d7          t T t \n."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "In this section, we evaluate the proposed CELMs and compare it with some classifiers, such as ELM, SVM and some related deep learning methods, on both synthetic and real-world datasets. Ten rounds of experiments are conducted for each data set. In each experiment, the training set and the test set are randomly generated using the samples from synthetic datasets and UCI database [39]. The samples from UCI database are normalized to be of zero mean and unit variance. The performances are recorded with the means and the standard deviations of classification accuracies.\nIn these experiments, we also compare the CELMs with the orthogonal ELM [40], which makes weight vectors orthogonal to each other and biases orthogonal to each other. The aim of this comparison is to compare CELMs with other ELM related methods appeared in literature sufficiently. The code of ELM used in the experiments was downloaded from [14]. In the following figures, the red solid performance curve stands for ELM performance, the green solid curve stands for orthogonal ELM performance, the blue solid curve stands for CDELM performance, the blue dashed curve with triangle markers stands for SELM performance, the brilliant blue solid curve stands for CSELM performance, the brilliant blue dashed curve with triangle markers stands for RSELM performance and the black solid curve stands for CMELM performance. The software used in the experiments is MATLAB R2010a with the Microsoft Windows Serve 2003 operation system. The configuration of hardware is Intel(R) Xeon(R) CPU E5440 @2.83GHz. The total RAM of the server is 32.0 GB, but the experiments cannot take up too much due to other users\u2019 usage."}, {"heading": "A. Experiments on Synthetic Dataset", "text": "We first evaluate our CELM algorithms on the synthetic dataset of the spiral data. It is illustrated in Fig. 3. To retain the symmetrical shape of the spiral, we normalize the samples into the range [-1, 1] as same as that in [14]. The total number of such generated spiral data is 5000. Two thirds of data samples are used as training set and the rest are used as test set. These data samples are randomly drawn from original spiral data samples. The two sets are randomly generated in each one of the total ten rounds of experiments.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n8\nWe compare the performances of CELMs with ELM and orthogonal ELM. The number of hidden nodes is selected from 10 to 150 at a step 10. The performances of these models are illustrated in Fig. 2.\nAs shown in Fig. 2, CELMs have a perfect performance when the number of hidden nodes reaches a slight larger than 50, while orthogonal ELM drops when the number of hidden nodes is larger than 50. The drop of orthogonal ELM is probably because the orthogonalization in two vectors of 50 dimensions is of no help, even degrades the information in the random weights. The test accuracy of ELM only reaches 0.8 even when the number of hidden nodes is 150. The test accuracies of CELMs are above those of orthogonal ELM and ELM all the time. The result shows that CELMs have the better generalization abilities than orthogonal ELM and ELM. Besides, the variances of these CELMs are becoming quite small with the increase of hidden nodes number and there is little difference among the performances of these CELMs. Therefore, the difference vectors and sample vectors really work from the performance comparison between CELMs and orthogonal ELM and ELM.\n0 10 20 30 40 50 60 70 0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\nThe number of hidden nodes\nTe st\nin g\nac cu\nra cy\nWDBC\nELM Orthogonal ELM CDELM CSELM SELM RSELM CMELM\n0 10 20 30 40 50 60 70 80 90 100\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\nThe number of hidden nodes\nTe st\nin g\nac cu\nra cy\nMDD-fac\nELM Orthogonal ELM CDELM CSELM SELM RSELM CMELM\n0 10 20 30 40 50 60\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nThe number of hidden nodes\nTe st\nin g\nac cu\nra cy\nMDD-fou\nELM Orthogonal ELM CDELM CSELM SELM RSELM CMELM\n0 10 20 30 40 50 60 70 80 0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nThe number of hidden nodes\nTe st\nin g\nac cu\nra cy\nMDD-kar\nELM Orthogonal ELM CDELM CSELM SELM RSELM CMELM\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n9"}, {"heading": "B. Experiments on UCI Datasets", "text": "Six datasets from UCI database [39], including Wisconsin diagnostic breast cancer dataset (WDBC) and digit datasets in five different features (MDD-fac, fou, kar, pix, zer), are used for evaluating the proposed CELMs. CELMs are compared with ELM, orthogonal ELM.\nIn the first experiment, the number of training samples is 2/3 size of the total samples, and the rest is used as test samples. The training and test sets are randomly generated. The comparison of ELM, CELMs and orthogonal ELM in the first experiment is illustrated in Fig. 3. The performances of these models are displayed sufficiently from the trends of the curves in the figure. It can be seen that the test accuracy curves of CELMs are above those of other two methods in the experiment and so the generalization ability of the proposed CELMs is better than other two models on these real world datasets. The samples\u2019 distribution prior we introduced makes the efficient use of hidden nodes in CELMs and really helps the classification tasks.\nIn the second experiment, a benchmark performance evaluation is conducted on the six datasets. The size of the training set is also 2/3 of the total samples and the remaining\ndata is used as the test set. We compare our methods with the regularized ELM, regularized orthogonal ELM, BP network and linear SVM methods. It should be mentioned that the optimization objectives of ELM, orthogonal ELM and CELMs used in the benchmark evaluation are all added a regularized term. Three-fold cross validation is used to select the best regularization factor in log10 space from -8 to 8 with the step 1. The number of hidden nodes used in ELM, orthogonal ELM, CELMs and BP network is from 5 to 200 with the step 5 in five MDD data sets. And in WDBC data set, the number of hidden nodes used is from 5 to 100 with the step 5. The selection of hidden nodes number is based on the Fig. 3. Although it is unfair for those methods that are not converged completely, the aim of the proposed methods is to improve the efficiency of hidden neurons. Due to no kernels used in ELM, the SVM used here is linear SVM. The cost factor used in SVM is selected as same as the regularization factor in ELM. The performances of ten rounds of experiments are recorded. The mean of test accuracies and training times are recorded in Table I. The best test accuracies are represented in bold face.\nThe training method of BP neural network is RPROP [41] due to time and memory problems in the experiments. The liner SVM code used is the MATLAB code obtained from [43].\nIt can be seen from the Table I that the performances of most CELMs outperform those of other methods. The CELMs improve the performance significantly, whilst retaining the extremely high learning speed of ELM.\nC. Experiments on Large Scale Datasets We also evaluate the CELMs and ELM on two large size datasets, i.e., MNIST [42] and CIFAR-10 [32]. The MNIST database of handwritten digits contains a training set of 60,000 samples, and a test set of 10,000 samples. It consists of binary images of ten classes and the size of these digits is 28\u00d728 pixels. The samples are normalized before input to the CELMs and ELM. The CIFAR-10 dataset contains 60,000 color images in 10 classes, with 6000 image per class. The training set and the test set consist of 50,000 images and 10,000 images respectively. In CIFAR-10 dataset, the standard evaluation pipeline defined in [43] is adopted. First, extract dense 6 6\u00d7 local patches with ZCA whitening and the stride is 1. Second, use threshold coding with =0.25\u03b1 to encode. The codebook is trained with OMP-1 [44] and the codebook size is 50 in the experiment. Third, average-pool the features on a 2 2\u00d7 grid to form the global image representation.\nThe performances of CELMs, ELM and orthogonal ELM are illustrated in Fig. 4. In this experiment, these methods are implemented without regularized terms for the sake of evaluating the efficiency of hidden neurons sufficiently.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n10\nFrom Fig. 4, it can be learned the performances of CELMs are about 8 percentages higher than those of ELM averagely on the two datasets. The efficiency of the sample vectors based weighting adopted by SELM can be evaluated as follows. When the same numbers of vectors are used, the method with higher test accuracy means more efficient. In Fig. 4, the performances of CELMs are higher than that of orthogonal ELM when they are not converged. The gaps between the performances of CELMs and orthogonal ELM show that the sample vector based weighting is effective. The curve of\nCELMs\u2019 performance is always above those of orthogonal ELM and ELM in all these datasets, which shows that sample vectors based weighting really helps the efficient use of hidden nodes. Experiments are conducted on MNIST data set to compare CELMs with ELM, orthogonal ELM and Multi-Layer Extreme Learning Machine (ML-ELM) [40]. The ML-ELM is a Stacked Denoising Auto Encoder (SDAE) model [46] based on ELM. The random feature mapping is used as encoder in ELM and the linear system is used as decoder. The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46]. In our experiment, the network structure of ML-ELM is set to 700-700-1000 due to the limit of our machine capacity. And the number of hidden nodes used in ELM, orthogonal ELM and CELMs is set to 1000 and 2000 for evaluation. In the experiments, regularization term is used for ELM, orthogonal ELM and CELMs. The parameter selection is the same as before. The average performance is recorded in Table \u2161. From Table \u2161 , it can be learned when the number of CELMs\u2019 hidden nodes is 1000, the test accuracies of CELMs are comparable to ML-ELM, but CELMs have much high learning speed. When the number of CELMs\u2019 hidden nodes is 2000, the CELMs outperform the ML-ELM of our implemented version. But the learning speed of CELMs is slower due to the computation complexity of matrix inversion. From the significant performance gap between ELM and CELMs and the similar learning speed of ELM and CELMs, we can conclude the sample based features used in CELMs really help the improvement, whilst retaining fast learning property of\nDATASETS WDBC MDD-fac MDD-fou MDD-kar MDD-pix MDD-zer\nELM Test accuracy 0.974 0.958 0.805 0.923 0.934 0.809\nTraining time(s) 0.01 0.03 0.03 0.03 0.03 0.04\nOrthogonal ELM Test accuracy 0.969 0.976 0.807 0.937 0.942 0.827\nTraining time(s) 0.01 0.07 0.04 0.05 0.08 0.04\nConstrained Difference ELM Test accuracy 0.973 0.975 0.839 0.953 0.961 0.833 Training time(s) 0.02 0.05 0.04 0.04 0.05 0.04\nConstrained Sum ELM Test accuracy 0.975 0.977 0.84 0.964 0.973 0.838 Training time(s) 0.02 0.07 0.07 0.08 0.08 0.07\nSample ELM Test accuracy 0.973 0.978 0.84 0.964 0.970 0.841 Training time(s) 0.02 0.04 0.03 0.03 0.04 0.03 Random Sum ELM Test accuracy 0.974 0.977 0.84 0.963 0.970 0.836 Training time(s) 0.01 0.04 0.03 0.03 0.04 0.03 Constrained Mixed ELM Test accuracy 0.974 0.977 0.84 0.963 0.971 0.836 Training time(s) 0.03 0.07 0.07 0.07 0.08 0.06\nBP Test accuracy 0.972 0.616 0.428 0.416 0.525 0.378 Training time(s) 0.97 3.69 3.80 4.27 5.33 4.98 Linear SVM Test accuracy 0.628 0.965 0.827 0.950 0.965 0.831 Training time(s) 0.73 12.37 0.43 0.151 2.88 1.41\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n11"}, {"heading": "ELM.", "text": "The CELMs are also compared with SVM related methods, e.g., Linear SVM and R2SVM [30], and deep ELM related method, e.g., DCN [31], on CIFAR-10 dataset. The R2SVM is a deep learning model and its building block is a linear SVM model. The outputs of the previous layers are transformed by a random matrix, and then the transformed outputs are added to the original features. The modified features are input into the next layer after transformed by a sigmoid function. The DCN is also a deep learning model, but its building block is an ELM based model, in which parts of the hidden nodes are built with random projection and the other part of hidden nodes are built with RBM weights [31, 45]. Instead of the way that adds the output of previous layer as a bias to the next layer, the outputs of the previous layers are concatenated with the original features sequentially and are input into the next layer in DCN model.\nIn this experiment, all the 50,000 training samples are used to train the model, and all the 10,000 test samples are used to evaluate the performance. Table \u2162 shows the performances of ELM, orthogonal ELM, CELMs, linear-SVM, R2SVM [30] and DCN [31] methods. Note the performances of linear SVM, R2SVM and DCN are cited from [30]. And the experimental conditions of ELM, orthogonal ELM and CELMs, such as the features and the number of used training and test sets, are the same with [30]."}, {"heading": "TABLE \u2161", "text": ""}, {"heading": "PERFORMANCE ON MNIST DATASET", "text": ""}, {"heading": "TABLE \u2162", "text": ""}, {"heading": "PERFORMANCE ON CIFAR-10 DATASET", "text": "Sample ELM 7000 nodes 0.733 Random Sum ELM 7000 nodes 0.727 Constrained Mixed ELM 7000 nodes 0.723\nFrom the Table \u2162, the CELMs can be found to have the better performances than that of linear-SVM, DCN and R2SVM. The CELMs have the test accuracies of at least 8 percentages higher than that of linear SVM. In [30], the R2SVM has 60 layers, and each layer is a linear SVM after random projection and sigmoid transformation. Although R2SVM and DCN have many layers, the CELMs of one hidden layer have the test accuracy of at least 3 percentages higher than that of these discriminative deep learning methods. Besides, the CELMs can train a model at a case of 50,000 training data well, which suggests that the proposed CELMs can tackle large scale data effectively.\nTo further understand the feature mapping of ELM, orthogonal ELM and the CELMs, the last experiment is conducted for the visualization of the hidden layer\u2019s feature mapping of these methods on MNIST data set. The visualization method, t-SNE [49], is used. The t-SNE is a very ideal visualization tool due to preserving local structure and overcoming the \u201ccrowding\u201d problem of mapped data. The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set. In the experiment, the whole 60,000 training data are used as training set and 2000 data are randomly selected from 10,000 test data to be used as the test set in t-SNE. The number of hidden nodes in ELM, orthogonal ELM and CELMs is 10,000. The visualization is illustrated in Fig. 5.\nSeen from the figure, our CELMs can retain the data structure well with the pre-assigned iteration number. The hidden layer\u2019s feature mapping in CELMs can separate the data of different classes well. However, after the transformation of 10,000 hidden neurons in ELM and orthogonal ELM, the input data do not converge after many iterations in t-SNE visualization. The phenomenon probably reveals the essence that the constrained random mapping hidden neurons may outperform the completely random hidden neurons in traditional ELMs. It also provides new insights into ELM related research that fast generation of meaningful hidden neurons could boost ELM\u2019s performance greatly.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n12\n-60 -40 -20 0 20 40 60 80\n-80\n-60\n-40\n-20\n0\n20\n40\n60\nMNIST Test Data\n0 1 2 3 4 5 6 7 8 9\n`\n-1000 -800 -600 -400 -200 0 200 400 600 800 1000 -1000\n-800\n-600\n-400\n-200\n0\n200\n400\n600\n800\n1000\nMNIST Test Data in Orthogonal ELM Feature Space\n0 1 2 3 4 5 6 7 8 9\n-60 -40 -20 0 20 40 60 -80\n-60\n-40\n-20\n0\n20\n40\n60\n80 MNIST Test Data in Constrained Difference Vectors Feature Space\n0 1 2 3 4 5 6 7 8 9\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n13"}, {"heading": "V. DISCUSSION", "text": "We have compared CELMs with several ELM based methods, such as normalized ELM [37], orthogonal ELM, and ELM. The main contributions of the CELMs to the ELM study are, 1) we introduce a constrained hidden weights based on sample distributions, 2) we normalized the hidden weights by the square of their l2 norms, other than l2 norm (The normalized ELM is taken the strategy.). Several work have validated the effectiveness of CELMs [57]. From the experiments of CELMs, we observe that when the number of hidden nodes is small, the CELMs are outperform normalized ELM, orthogonal ELM and ELM greatly. However, when the number of hidden nodes is large, the margins between the ELM and normalized ELM are not that big. From the observation, the constrained weights and normalized strategy both work for the success of CELMs. On one hand, when the number of hidden nodes is small, the hidden layer is equivalent to dimension reduction. Thus the directions of hidden weights are quite import since they represent the directions to be retained after the reduction. The weights based on sample distribution work well for the dimension reduction [52]. On the other hand, the normalization of hidden weights is important due to the property of sigmoid activation function. The effective response area of sigmoid function is near the zero. When the weights are not normalized and the dimension of inputs is high, the absolute values of many elements in WX+b are very big. Thus the normalization is quite import. And we strongly recommend the operator will be added in the research of ELM in the future."}, {"heading": "VI. CONCLUSION", "text": "To address the inefficient use of hidden nodes in ELM, this paper proposed the novel learning models, CELMs. The CELMs constrain its random weights\u2019 generation from a smaller space compared to that of the ELM, i.e., replacing the completely random weight vectors with ones that are randomly drawn from the set of simple linear combination of sample vectors. The main contribution of CELMs is that it introduces sample distribution prior into the construction of the hidden layer to make a better feature mapping and benefit the next layer\u2019s linear classification. The effective feature mapping greatly contributes the efficient use of hidden nodes in ELM. Extensive comparisons between CELMs and some related methods on both synthetic and real-world datasets showed that CELMs have better performances in almost all the cases.\nHowever, the CELMs still have some problems that typical ELM owned. One is that CELMs face the over fitting problem when the number of hidden nodes is very large, although CELMs improve the effective use of discriminative hidden nodes. To our relief, the methods in [34, 35, 51] can tackle the problem effectively. Another problem is that the solving of the weights from the hidden layer to the output layer is time-consuming when the number of hidden nodes is very large. This case is much common in large scale applications. Some\ngradient based solving methods for linear system can tackle the problem iteratively.\nThe further research will include the study of invariant feature generating for the improvement of CELMs and the experimental verification on CELMs\u2019 application to regression problems. The analyses on what kinds of problems that the CELMs will work with and such related theories are also expected to be studied in the future."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Dr. L. L. C. Kasun, Dr. H.\nZhou, Prof. G.-B. Huang from Nanyang Technological University, Singapore and Prof. C. M. Vong from University of Macau, Macau for their kindly help with Multi-Layer Extreme Learning Machine (ML-ELM)."}], "references": [{"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V. Ya. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks, vol. 6(6), pp. 861\u2013867, 1993.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Capabilities of a four-layered feedforward neural network: four layers versus three", "author": ["S. Tamura", "M. Tateishi"], "venue": "IEEE Trans. Neural Networks, vol. 8(2), pp. 251\u2013255, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions", "author": ["G.-B. Huang", "H.A. Babri"], "venue": "IEEE Trans. Neural Networks, vol. 9(1), pp. 224\u2013229, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Beyond regression: new tools for prediction and analysis in the behavioral sciences", "author": ["P. Werbos"], "venue": "Ph.D. Thesis Harvard University, Cambridge, MA, 1974.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1974}, {"title": "The Levenberg-Marquardt algorithm: implementation and theory", "author": ["J.J. Mor\u00e9"], "venue": "Numerical analysis, Springer Berlin Heidelberg, pp. 105-116, 1978.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1978}, {"title": "A simple procedure for pruning back-propagation trained neural networks", "author": ["E.D. Karnin"], "venue": "IEEE Trans. Neural Networks, vol. 1(2), pp. 239-242, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Evolving artificial neural networks", "author": ["X. Yao"], "venue": "Proceedings of the IEEE, vol. 87(9), pp. 1423-1447, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast generic selection of features for neural network classifiers", "author": ["F.Z. Brill", "D.E. Brown", "W.N. Martin"], "venue": "IEEE Trans. Neural Networks, vol. 3(2), pp. 324-328, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20(3), pp. 273-297, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A one-layer recurrent neural network for support vector machine learning", "author": ["Y. Xia", "J. Wang"], "venue": "IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 34, no. 2, pp. 1261\u20131269, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive radial basis function nonlinearities, and the problem of generalization", "author": ["D. Lowe"], "venue": "First IEE International Conference on Artificial Neural Networks (Conf. Publ. No. 313), IET, pp. 171-175, October, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "The No-Prop algorithm: A new learning algorithm for multilayer neural networks", "author": ["B. Widrow", "A. Greenblatt", "Y. Kim", "D. Park"], "venue": "Neural Networks, vol. 37, pp. 182-188, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive switching circuits", "author": ["B. Widrow", "M.E. Hoff"], "venue": "IRE WESCON Convention Record, 1960.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1960}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70 (1\u20133), pp. 489\u2013501, Dec. 2006, [Code: http://www.ntu.edu.sg/home/egbhuang/elm_ random_hidden_nodes.html].", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2004), vol. 2, (Budapest, Hungary), pp. 985\u2013990, 25-29 Jul., 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 14 on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42(2), pp. 513-529, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Trans. Inf. Theory, vol. 44(2), pp. 525\u2013536, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-supervised and unsupervised extreme learning machines", "author": ["G. Huang", "S. Song", "J.N. Gupta", "C. Wu"], "venue": "IEEE Trans Cybern, 2014, in press.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Online sequential fuzzy extreme learning machine for function approximation and classification problems", "author": ["H.-J. Rong", "G.-B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39(4), pp. 1067-1072, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A neuro-fuzzy inference system through integration of fuzzy logic and extreme learning machines", "author": ["Z.-L. Sun", "K.-F. Au", "T.-M. Choi"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 37(5), pp. 1321-1331, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Vehicle detection in driving simulation using extreme learning machine", "author": ["W. Zhu", "J. Miao", "J. Hu", "L. Qing"], "venue": "Neurocomputing, vol. 128, pp. 160-165, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse Extreme Learning Machine for Classification", "author": ["Z. Bai", "G.-B. Huang", "D. Wang", "H. Wang", "M.B. Westover"], "venue": "IEEE Transactions on Cybernetics, 2014, in press.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolutionary extreme learning machine", "author": ["Q.-Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.-B. Huang"], "venue": "Pattern Recognition, vol. 38(10), pp. 1759\u2013 1763, Oct. 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "A constructive enhancement for online sequential extreme learning machine", "author": ["L. Yuan", "Y.C. Soh", "G.-B. Huang"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2009), pp. 1708\u20131713, 14-19 Jun., 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "OP-ELM: optimally pruned extreme learning machine", "author": ["Y. Miche", "A. Sorjamaa", "P. Bas", "O. Simula", "C. Jutten", "A.A. Lendasse"], "venue": "Neural Networks, IEEE Transactions on, vol. 21(1), pp. 158-162, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast pruned-extreme learning machine for classification problem", "author": ["H.J. Rong", "Y.S. Ong", "A.H. Tan", "Z. Zhu"], "venue": "Neurocomputing, vol. 72(1), pp. 359-366, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and effective algorithms for training single-hidden-layer neural networks", "author": ["D. Yu", "L. Deng"], "venue": "Pattern Recognition Letters, vol. 33(5), pp. 554\u2013558, 1 Apr. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "The MIT Press, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifiability-based discriminatory projection pursuit", "author": ["Y. Su", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE Trans. Neural Networks, vol. 22(12), pp. 2050-2061, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with recursive perceptual representations", "author": ["O. Vinyals", "Y. Jia", "L. Deng", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems, pp. 2834-2842, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["L. Deng", "D. Yu"], "venue": "Proceedings of the Interspeech 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master's thesis, Department of Computer Science, University of Toronto, 2009, [http://www.cs.toronto.edu/~kriz/ cifar.html].", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17(6), pp. 1411-1423, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimization method based extreme learning machine for classification", "author": ["G.-B. Huang", "X. Ding", "H. Zhou"], "venue": "Neurocomputing, vol. 74(1), pp. 155-163, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust regression with extreme support vectors", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "pattern recognition letters, vol. 45, pp. 205-210, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme support vector machine classifier", "author": ["Q. Liu", "Q. He", "Z. Shi"], "venue": "Advances in Knowledge Discovery and Data Mining, Springer Berlin Heidelberg, pp. 222-233, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained Extreme Learning Machine: a Novel Highly Discriminative Random Feedforward Neural Network", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2014), pp. 800-807, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop", "N.M. Nasrabadi"], "venue": "New York: springer,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "UCI Repository of machine learning databases", "author": ["C.L. Blake", "C.J. Merz"], "venue": "[http://www.ics.uci.edu/~mlearn/MLRepository. html]. Irvine, CA: University of California. Department of Information and Computer Science,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1998}, {"title": "Representational Learning with Extreme Learning Machine for Big Data", "author": ["L.L.C. Kasun", "H. Zhou", "G.-B. Huang", "C.M. Vong"], "venue": "IEEE Intelligent Systems, vol. 28, no. 6, pp. 31-34, December 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks, pp. 586-591, 1993.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1993}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86(11), pp. 2278-2324, 1998, [Online]. Available: http://yann.lecun.com/exdb/mnist.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 921-928), 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "On the difference between orthogonal matching pursuit and orthogonal least squares", "author": ["T. Blumensath", "M.E. Davies"], "venue": "unpublished manuscript, 2007, [http://www.see.ed.ac.uk/~tblumens/papers/BD OMPvsOLS07.pdf].", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation, vol. 18(7), pp. 1527-1554, 2006.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371-3408, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R.R.R. Salakhutdinov"], "venue": "Science, vol. 313(5786), pp. 504-507, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient learning of deep Boltzmann machines", "author": ["R. Salakhutdinov", "H. Larochelle"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 693-700, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["L. Maaten"], "venue": "International Conference on Artificial Intelligence and Statistics,  pp.  384-391,  2009, [Code: http://homepage.tudelft.nl/19j49/t-SNE.html].", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290(5500), pp. 2323-2326, 2000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme support vector regression", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "Proceedings of International Conference on Extreme Learning Machines (ELM2013), Beijing, China, Spring-Verlag, 15-17 Oct. 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Classifiability-based discriminatory projection pursuit", "author": ["Y. Su", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE Trans. Neural Networks, vol. 22(12), pp. 2050-2061, 2011.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights", "author": ["M.D. McDonnell", "M.D. Tissera", "A. van Schaik"], "venue": "arXiv preprint arXiv:1412.8307, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been proven that the learning capacity of a multilayer feedforward neural network with non-polynomial activation functions can approximate any continuous function [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "The learning capacity of SLFNs is not inferior to that of multilayer feedforward neural networks, as proved in [2, 3].", "startOffset": 111, "endOffset": 117}, {"referenceID": 2, "context": "The learning capacity of SLFNs is not inferior to that of multilayer feedforward neural networks, as proved in [2, 3].", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "The most famous gradient based learning method is back-propagation algorithm [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "One of the most popular optimization based SLFNs is Support Vector Machine (SVM) [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "SVM is a very popular method attracting many researchers [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "[14] proposed a novel extremely fast learning model of SLFNs, called Extreme Learning Machine (ELM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] further completely proved the random feature mapping theory rigorously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "After the random nonlinear feature mapping in the hidden layer, the rest of ELM can be considered as a linear system [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "The essence of the linear system used by ELM is to minimize the training error and the norm of connection weights from the hidden layer to the output layer at the same time [16].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "Hence ELM has a good generalization performance according to the feedforward neural network theory [12, 17].", "startOffset": 99, "endOffset": 107}, {"referenceID": 16, "context": "Hence ELM has a good generalization performance according to the feedforward neural network theory [12, 17].", "startOffset": 99, "endOffset": 107}, {"referenceID": 15, "context": "Additionally, ELM has a unified framework for classification, regression, semi-supervised, supervised and unsupervised tasks [16, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "Additionally, ELM has a unified framework for classification, regression, semi-supervised, supervised and unsupervised tasks [16, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 18, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 19, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 20, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 21, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 22, "context": "However, the random selection of hidden layer parameters makes quite inefficient use of hidden nodes [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Use online incremental learning methods to add hidden layer nodes dynamically [23, 24].", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Use online incremental learning methods to add hidden layer nodes dynamically [23, 24].", "startOffset": 78, "endOffset": 86}, {"referenceID": 24, "context": "Use pruning methods to select the candidate hidden layer nodes [25, 26].", "startOffset": 63, "endOffset": 71}, {"referenceID": 25, "context": "Use pruning methods to select the candidate hidden layer nodes [25, 26].", "startOffset": 63, "endOffset": 71}, {"referenceID": 26, "context": "Use gradient based methods to update the weights from the input layer to the hidden layer in ELM [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "LDA [28] is probably the most commonly used method to extract discriminative features.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "[29] proposed a projection pursuit based LDA method to overcome these problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 73, "endOffset": 81}, {"referenceID": 30, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 73, "endOffset": 81}, {"referenceID": 31, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "The output layer is a linear system, where the connection weights from the hidden layer to the output layer are learned by computing the Moore-Penrose generalized inverse [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 32, "context": "However, the condition number of the random projected matrix H may be very large and the above traditional ELM model may encounter ill-posed problems [33].", "startOffset": 150, "endOffset": 154}, {"referenceID": 33, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 34, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 35, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 33, "context": "As analyzed in theory and further verified by the simulation results in [34], ELM for classification tends to achieve better generalization performance than traditional SVM.", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "Although the method [29] is rather complex with many embedded trivial tricks, it shows that the difference vectors of between-class samples are effective to classification tasks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "In our originally Constrained Difference Extreme Learning Machine [37], we deleted the difference vectors of small norms and too relevant difference vectors.", "startOffset": 66, "endOffset": 70}, {"referenceID": 37, "context": "Although the two processes can improve the performance [38], the improvement is very little in our experiments.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "The difference between kernel ELM [16] and our SELM is that ELM kernel uses all the training samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 38, "context": "In each experiment, the training set and the test set are randomly generated using the samples from synthetic datasets and UCI database [39].", "startOffset": 136, "endOffset": 140}, {"referenceID": 39, "context": "In these experiments, we also compare the CELMs with the orthogonal ELM [40], which makes weight vectors orthogonal to each other and biases orthogonal to each other.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "The code of ELM used in the experiments was downloaded from [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "To retain the symmetrical shape of the spiral, we normalize the samples into the range [-1, 1] as same as that in [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "To retain the symmetrical shape of the spiral, we normalize the samples into the range [-1, 1] as same as that in [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 38, "context": "Experiments on UCI Datasets Six datasets from UCI database [39], including Wisconsin diagnostic breast cancer dataset (WDBC) and digit datasets in five different features (MDD-fac, fou, kar, pix, zer), are used for evaluating the proposed CELMs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 40, "context": "The training method of BP neural network is RPROP [41] due to time and memory problems in the experiments.", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "The liner SVM code used is the MATLAB code obtained from [43].", "startOffset": 57, "endOffset": 61}, {"referenceID": 41, "context": ", MNIST [42] and CIFAR-10 [32].", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": ", MNIST [42] and CIFAR-10 [32].", "startOffset": 26, "endOffset": 30}, {"referenceID": 42, "context": "In CIFAR-10 dataset, the standard evaluation pipeline defined in [43] is adopted.", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "The codebook is trained with OMP-1 [44] and the codebook size is 50 in the experiment.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "Experiments are conducted on MNIST data set to compare CELMs with ELM, orthogonal ELM and Multi-Layer Extreme Learning Machine (ML-ELM) [40].", "startOffset": 136, "endOffset": 140}, {"referenceID": 45, "context": "The ML-ELM is a Stacked Denoising Auto Encoder (SDAE) model [46] based on ELM.", "startOffset": 60, "endOffset": 64}, {"referenceID": 47, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 146, "endOffset": 150}, {"referenceID": 47, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 181, "endOffset": 185}, {"referenceID": 45, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 223, "endOffset": 227}, {"referenceID": 29, "context": ", Linear SVM and RSVM [30], and deep ELM related method, e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 30, "context": ", DCN [31], on CIFAR-10 dataset.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The DCN is also a deep learning model, but its building block is an ELM based model, in which parts of the hidden nodes are built with random projection and the other part of hidden nodes are built with RBM weights [31, 45].", "startOffset": 215, "endOffset": 223}, {"referenceID": 44, "context": "The DCN is also a deep learning model, but its building block is an ELM based model, in which parts of the hidden nodes are built with random projection and the other part of hidden nodes are built with RBM weights [31, 45].", "startOffset": 215, "endOffset": 223}, {"referenceID": 29, "context": "Table III shows the performances of ELM, orthogonal ELM, CELMs, linear-SVM, RSVM [30] and DCN [31] methods.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "Table III shows the performances of ELM, orthogonal ELM, CELMs, linear-SVM, RSVM [30] and DCN [31] methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Note the performances of linear SVM, RSVM and DCN are cited from [30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "And the experimental conditions of ELM, orthogonal ELM and CELMs, such as the features and the number of used training and test sets, are the same with [30].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "In [30], the RSVM has 60 layers, and each layer is a linear SVM after random projection and sigmoid transformation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "The visualization method, t-SNE [49], is used.", "startOffset": 32, "endOffset": 36}, {"referenceID": 37, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 81, "endOffset": 85}, {"referenceID": 49, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 91, "endOffset": 95}, {"referenceID": 46, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 113, "endOffset": 117}, {"referenceID": 36, "context": "We have compared CELMs with several ELM based methods, such as normalized ELM [37], orthogonal ELM, and ELM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 51, "context": "The weights based on sample distribution work well for the dimension reduction [52].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}, {"referenceID": 34, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}, {"referenceID": 50, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}], "year": 2015, "abstractText": null, "creator": "Acrobat PDFMaker 9.0 Word \u7248"}}}