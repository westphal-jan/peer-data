{"id": "1501.07315", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2015", "title": "Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation", "abstract": "Applications with dictionary learning, non-negative matrix factorization, sub-space clusters, and parallel factor splitting problems well motivate algorithms for pro-block convex and non-smooth optimization problems. By utilizing the stochastic approximation paradigm and first-order acceleration schemes, this work develops an online and modular learning algorithm for a large class of nonconvex data models, in which convexity is manifested only per block of variables when the rest of them is fixed. The advocated algorithm requires computational complexity that is scaled linearly with the number of unknowns. Under minimal assumptions about the cost functions of the composite optimization task, with no limitations on the optimization variables or any explicit information about the limits of the Lipschitz coefficients, the expected costs measured online on the resulting items are demonstrably correlated per quadrata per point to the accumulated accumulated accumulated accumulation of one-half of the accumulation of the accumulation.", "histories": [["v1", "Thu, 29 Jan 2015 00:15:28 GMT  (610kb)", "http://arxiv.org/abs/1501.07315v1", "Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4-9, 2014"], ["v2", "Mon, 12 Dec 2016 22:18:49 GMT  (0kb,I)", "http://arxiv.org/abs/1501.07315v2", "The paper has been withdrawn by the author due to the need for a careful and very long revision process"], ["v3", "Thu, 26 Jan 2017 15:49:01 GMT  (0kb,I)", "http://arxiv.org/abs/1501.07315v3", "The paper has been withdrawn by the author due to the need for a careful and very long revision process"]], "COMMENTS": "Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4-9, 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantinos slavakis", "georgios b giannakis"], "accepted": false, "id": "1501.07315"}, "pdf": {"name": "1501.07315.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Konstantinos Slavakis", "Georgios B. Giannakis"], "emails": ["kslavaki@umn.edu", "georgios@umn.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n07 31\n5v 1\n[ cs\n.L G\n] 2\n9 Ja\nn 20"}, {"heading": "1 Introduction", "text": "Aiming at succinct representations of large-scale data, models relying on non-convex functions have\nemerged as a prominent tool to learn low-dimensional structure from (possibly high-dimensional) data.\nAreas of interest span signal processing and machine learning applications including dictionary learning\n(DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor\n(PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.\nConsider DL for specificity, where a given M \u00d71 vector yt is modeled as the product of an unknown over-complete dictionary D := [d1, . . . ,dQ], Q \u2265 M , times an unknown sparse coefficient vector st [2, 3]. With Yt := [y1, . . . ,yt], and likewise for S, DL solves\nmin (S,D)\n1\n2t \u2016Yt \u2212DS\u20162F + \u03bbs\u2016S\u20161 + \u03b9D(D) (1)\n\u2217Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4\u20139, 2014. This work was supported by the NSF grant Eager 1343860.\n\u2020The authors are with the Dept. of ECE and the Digital Technology Center, Univ. of Minnesota, 117 Pleasant St. SE, Minneapolis, MN 55455, USA. Tel: (612) 625-0763; Emails: {kslavaki,georgios}@umn.edu\nwhere \u2016\u00b7\u2016 F denotes the Frobenius norm; the scale \u03bbs > 0 controls the sparsity effected by the \u21131-norm \u2016S\u20161 := \u2211\nq,t|sq,t|; the set indicator is \u03b9D(D) = 0 if D \u2208 D, and \u03b9D(D) = +\u221e otherwise, where the set D := {D \u2208 RM\u00d7Q | \u2016dq\u2016 \u2264 1, q \u2208 {1, . . . , Q}} confines the dictionary to have bounded-norm columns. This constraint fixes the inherent scale ambiguity of the bilinear fit DS, and also ensures that the\nsolution of (1) remains bounded. Sparsity on the other hand, renders DL representations identifiable\neven when yt has missing entries [8], due to e.g., malfunctioning, privacy reasons, or, high cost of data gathering.\nDue to the bilinear term DS, the three-summand cost Ft(S,D;Ot) := ft(S,D;Ot)+ g1(S)+ g2(D) in (1) is non-convex (set Ot := {y1, . . . ,yt} collects observations up to t.) However, Ft is clearly \u201cperblock-convex,\u201d as it is convex in either S or D, if the other one is fixed. Related multilinear forms emerge\nalso with NMF, SSC, PARAFAC, and TLS models. Mainly for offline optimization, block coordinate\ndescent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity\nof the cost functions involved [3, 9\u201319]. For online DL, BCDMs alternate between two iterations to\nupdate current estimates (St\u22121 := [s1, . . . , st\u22121],Dt\u22121) as follows [3]\nst \u2208 argmins Ft ( [St\u22121, s],Dt\u22121;Ot ) (2a) Dt \u2208 argminD Ft(St,D;Ot) . (2b)\nGiven Ot, each step in (2) is a convex optimization task: Basis pursuit [20] in (2a), and constrained leastsquares (LS) in (2b). However, the per-block minimizations in BCD may not be affordable by todays big\ndata applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22]. Further, as data are streaming, analytics must often be performed in real time, without a chance to\nrevisit past entries \u2013 a feature common to stochastic approximation (SA) setups [23].\nIn the spirit of SA, the present paper deals with minimizing expected value costs of the form\nminx EO{Ft(x;Ot)} (3)\nwhere x comprises all blocks of variables, e.g., x := (S,D) in (1), and expectation E is over the random\nOt, whose probability density function (pdf) is unknown. The goal is to develop a modular algorithmic framework for solving (3) that: i) Leverages per-block-convexity of Ft as in BCDMs; ii) it operates online with streaming data Ot of unknown pdf; iii) relies only on first-order (sub)gradient information of Ft, bypassing the need for (almost) exact minimizers per block as in (2); iv) it incurs affordable complexity per iteration, at most linear with respect to (w.r.t.) the number of unknowns; and v)\niterations converge quadratically to a solution of (3), which is optimal among first-order methods in\nthe sense of [24].\nTo place our contributions i)-v) in context, related first-order online BCDMs include the proximal\nstochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex\nproblems, on top of being challenged by step-size choices. A relevant stochastic algorithm is the SA-\nbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly\nconvergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.\nOn the other hand, accelerated first-order quadratically convergent iterations are available for off-line\nconvex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs. Even\nthough [42] deals with non-convex costs, it requires bounds on the (primal) variables, knowledge of a\nbound on the Lipschitz coefficient of the gradient operator for the algorithm to operate, and does not\nexploit the modularity offered by per-block-convexity.\nOur work markedly broadens the offline acceleration technique introduced for convex costs in [39],\nto per-block-convex and to online SA setups. Unless the per-block optimization task can be solved in\nsimple closed-form, there is no need for exact minimizers per block. Without knowing the data pdf and\nby relying only on first-order information of the instantaneous cost Ft, at linear complexity per iteration, we prove that the expected cost converges quadratically. Neither bounds on the block variables nor\nknowledge of bounds on Lipschitz coefficients are required. Under minimal assumptions and without\nimposing any block-wise strong convexity on the cost, performance analysis is carried out both for the\ncost values and the block variables of task (3). Specifically, we establish that the expected limit cost is\nan accumulation point of (per-block) minima, and that subgradients of the expected cost asymptotically\nvanish in the mean-squared sense. The analytical results are tested on two instances of broad practical\ninterest: (i) Online, robust and sparsity-aware linear TLS regression, using synthetic data; and (ii)\nonline semi-supervised DL for network-wide link load tracking and imputation of real data. Numerical\ntests corroborate our analytical claims, and demonstrate that under a linear computational complexity\nfootprint the proposed algorithm outperforms BCDMs and the computationally heavier ADMM-based\nalternatives [8].\nThe rest of the manuscript is organized as follows. Preliminaries are given in Sec. 2, while the\nproposed algorithm is developed in Sec. 3. Performance analysis is the subject of Sec. 4, with proofs\ndelegated to Appendix A. Two examples of principal practical interest are provided in Sec. 5. Numerical\ntests both on synthetic and real data are presented in Sec. 6, while the manuscript is concluded in Sec. 7.\nPreliminary results were presented in [43], and outlined in [21]."}, {"heading": "2 Preliminaries", "text": "A first-order algorithm for the off-line minimization of a convex cost \u03d5(x) := f(x) + g(x), x \u2208 M, was studied in [39] (presented for convenience in Table 1), where M is a linear vector space; f is convex as\nwell as L-Lipschitz continuously differentiable; and g is convex but possibly non-smooth, e.g., the \u21131norm. Auxiliary variables {\u03c8i, \u03b6i} \u2282 M are utilized to generate a sequence (\u03d5(xi))i\u2208Z\u22650 that converges as i \u2192 +\u221e to the (global) minimum of \u03d5 with quadratic rate. The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Prox\u03b2ig : M \u2192 M : x 7\u2192 argmin\u03be\u2208M\u2016x \u2212 \u03be\u20162/2 + \u03b2ig(\u03be) for any \u03b2i \u2208 R>0 [44]. The FB iteration splits operation on \u03d5 into two concatenated stages: Firstly on the differentiable f through the classical steepest-descent operator (Id\u2212\u03b2i\u2207f), and secondly on g via Prox\u03b2ig, which usually obtains closed-forms for the majority of regularizers g, e.g., Prox\u2016\u00b7\u20161 boils down to the soft-thresholding operator [45]. If the FB iteration were performed with \u03c8i+1 taking the place of \u03b6i in line 5, then (\u03c8i)i\u2208Z\u22650 would converge to a minimizer of \u03d5 [44], but with no claims on quadratic rate of convergence. Towards establishing such claims, \u03b6i of line 5 is convexly combined with xi\u22121 to form (1\u2212 \u03bbi)xi\u22121 + \u03bbi\u03b6i, which, together with line 6, guarantee that the values of \u03d5 are monotonically non-increasing: \u03d5(xi) \u2264 \u03d5(xi\u22121). Parameters {\u03b7i+1, \u03bbi+1} in line 2 are used to define stepsize \u03b2i+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1\u2212 \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L, (1+ \u221a\n1\u2212 \u03b7i+1\u03bbi+1L)/L] per iteration, as opposed to the rigid \u03b2i+1 = 1/L in [37, 38]. Instrumental in establishing quadratic rate of convergence\nis the sequence of positive coefficients (\u00b5i)i\u2208Z>0 of line 4 (cf. Fact 1 in Appendix A.3 where limi\u2192\u221e \u00b5i = +\u221e). Finally, line 7 links variables {xi\u22121, xi, \u03c8i, \u03c8i+1, \u03b6i} together, and, as it will be shown later on, it facilitates performance analysis via helpful telescoping terms. Under proper parameter selection\n(\u03b7i = \u03b2i = 1/L, \u03bbi = 1), the algorithm of Table 1 boils down to [38]. Moreover, with its guaranteed monotonically non-increasing behavior of cost values through line 6, and the flexibility offered by the\nvariable step-sizes (\u03b2i)i\u2208Z>0 in line 3, the algorithm in Table 1 has merits over [37]. Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}. The purpose of this study is to extend the merits of the algorithm in Table 1 to the much more general\nsetting where not only the underlying cost is time-varying, but it is per-block-convex, and several of its\nparameters are of stochastic nature.\nTo this end, and with reference to (3), consider a sequence of functions Ft of composite structure\nFt(x ;Ot) := ft(x ;Ot) + \u2211B b=1 gb(x (b)), where x gathers all unknowns, split in B blocks of variables x := (x(1), . . . , x(B)), with x(b) belonging to a finite-dimensional linear space Mb with inner-product \u3008\u00b7 | \u00b7\u3009 Mb ; ft exhibits per-block-convexity, i.e., ft is convex w.r.t. each of the blocks x (b) whenever the rest of them are fixed; and gb is a convex function used to regularize and account for prior information on each x(b). Symbol M stands for the Cartesian product M := \u00d7Bb=1Mb. An inner product on M is defined as \u3008x1 | x2\u3009M := \u2211B b=1\u3008x (b) 1 | x (b) 2 \u3009Mb , \u2200(x1,x2) \u2208 M 2. Whenever clear from the context, subscripts Mb will be dropped from the inner-product symbols for notational convenience.\nFor any function \u03a6 on M, notation \u03a6(x(b) | x(\u2212b)) stresses the dependence of \u03a6 onto the bth block of variables x(b), whenever the rest of them are fixed, with x(\u2212b) denoting all but the bth blocks contained in x. Term \u03a6(x | x(\u2212b)) serves also the previous purpose, but with superscript (b) dropped from x(b) to avoid overloading notation.\nLet \u03930(Mb) denote all proper, convex, and lower-semicontinuous (l.s.c.) functions defined on Mb with values in R\u222a{+\u221e} [44]. For any \u03d5 \u2208 \u03930(Mb), the subdifferential \u2202\u03d5(x(b)) is defined as the set of all subgradients \u03d5\u2032(x(b)) of \u03d5 at x(b): \u2202\u03d5(x(b)) := {\u03d5\u2032(x(b)) \u2208 Mb | \u3008\u03d5\u2032(x(b)) | \u03be\u2212x(b)\u3009+\u03d5(x(b)) \u2264 \u03d5(\u03be),\u2200\u03be \u2208\nMb} [44]. If \u03d5 is differentiable at x(b), \u2202\u03d5(x(b)) = {\u2207b\u03d5(x(b))}, with \u2207b denoting the gradient operator w.r.t. the bth block in x. In this context, per realization of the r.vs. in Ot, ft(\u00b7 | x(\u2212b) ;Ot) \u2208 \u03930(Mb) is assumed [L\n(b) t := L (b) t (x (\u2212b),Ot)]-Lipschitz continuously differentiable. Moreover, gb \u2208 \u03930(Mb). Data Ot are considered to be r.vs. defined on a probability space (\u2126,A,Pr) with E{\u00b7} := EA{\u00b7} denoting expectation w.r.t. the \u03c3-algebra A [46]. IfO is the \u03c3-subalgebra of A including all events related to data {Ot}t\u2208Z>0 , EO{\u00b7} stands for expectation w.r.t. O. Whenever block x(b) is viewed as an r.v., it is assumed to have finite second-order moment. In this context, Hb := {x : \u2126 \u2192 Mb | E{\u2016x\u20162Mb} < +\u221e} turns out to be a Hilbert space with inner-product E{\u3008x1 | x2\u3009Mb}, \u2200(x1, x2) \u2208 H 2 b [44, Example 2.6]. To generalize, H := \u00d7Bb=1Hb is also a Hilbert space with inner-product E{\u3008x1 | x2\u3009M}, \u2200(x1,x2) \u2208 H2. If X denotes a \u03c3-subalgebra of A which includes all events related to all blocks of variables, then EX{\u00b7} stands for expectation w.r.t. X [46]. Moreover, EO|X{\u00b7} denotes conditional expectation w.r.t. O, conditioned on X [46]. Hereafter, it is assumed that X \u222aO = A, so that E{\u00b7} = EA{\u00b7} = EX,O{\u00b7}."}, {"heading": "3 Algorithm", "text": "The algorithm of this section incorporates the acceleration module of Table 1 into the online learning setup of (3). Given that variables are split in blocks x = (x(1), . . . , x(b), . . . , x(B)), the proposed\nalgorithm takes advantage of the per-block convexity of the cost Ft and visits blocks of variables in a Gauss-Seidel or successive fashion. The basic principles of this modular algorithm are depicted\nin the block diagram of Fig. 1. Per iteration (time slot) t and given observed data Ot, the algorithm visits all blocks of variables successively to solve the per-block b convex minimization task minx(b)\u2208Mb ft(x (b) | x(\u2212b)t )+ gb(x(b)). Symbol Ot is dropped from ft(x(b) | x (\u2212b) t ;Ot) for notational convenience, and x (\u2212b) t := (x (1) t , . . . , x (b\u22121) t , x (b+1) t\u22121 , . . . , x (B) t\u22121) comprises all updated blocks up to the (b\u2212 1)st one, as well as the {b + 1, . . . , B} unvisited ones. If the solution to the previous minimization task\nis affordable both w.r.t. time and computational resources, then block b is updated by the obtained\nminimizer; otherwise, the acceleration module of Table 1 is run only for a finite number of iterations Rb, i.e., i \u2208 {1, . . . , Rb} in the context of Table 1, and not infinitely often (i \u2192 +\u221e) as in the batch and off-line mode of [39]. Having the bth block updated, effort is put on the next (b+ 1)st one. Once\nall blocks have been updated, the previous procedure is repeated for the (t+ 1)st time instant, and so\non.\nSince the acceleration module of Table 1 is allowed to be employed for Rb times per (b, t), the time index \u03c4 (b) := (t\u2212 1)Rb + rb, with rb \u2208 {1, . . . , Rb}, is introduced here to account for this \u201coverclocking\u201d or finer -time-scaling of the original t-axis. Because data are originally observed according to the \u201ct-\nclicks,\u201d to abide by the \u03c4 -click notation, define O\u03c4 (b) = O(t\u22121)Rb+rb := Ot, as well as the induced functions F\u03c4 (b) = F(t\u22121)Rb+rb := Ft and f\u03c4 (b) = f(t\u22121)Rb+rb := ft.\nA more detailed version of the block diagram of Fig. 1, equipped with the \u03c4 -time notation, is given in Table 2. The acceleration module of Table 1, called here Accel(\u00b7), has been revised in Table 2b to abide by the notation which pertains to per-block b operation. Overclocking and acceleration can be\nseen in lines 7\u201313 of Table 2a. Symbol x (\u2212b) \u03c4 in line 9 (the (b) superscript from \u03c4 (b) is omitted for notational convenience) stands for all but the bth blocks of variables, where only blocks {1, . . . , b\u2212 1} have been updated. On the other hand, x (b)\n\u03c4 (b) in line 12 collects x\n(\u2212b) \u03c4 (b) and the recently updated bth\nblock of variables. Symbol I\u03c4 (b) stands for the input arguments of the acceleration module Accel(I\u03c4 (b)) in line 11, which is expanded in Table 2b. These input arguments consist of only those parts of the cost which are affected by the bth block update; the value of x (b)\n\u03c4 (b)\u22121 at the previous time instant \u03c4 (b) \u2212 1; as\nwell as the Accel(\u00b7)\u2019s intrinsic variables {\u03c8(b) \u03c4 (b) , \u03b7 (b) \u03c4 (b) }. Finally, xt in line 16 collects all B updated blocks at time t.\nImplementing only the first-order information of ft(\u00b7 | x(\u2212b)\u03c4 ) in (M5) equips the algorithm in Table 2 with a low computational footprint which scales linearly w.r.t. the number of unknown parameters. For\nspecificity, the computational complexities on two practical examples will be provided in Sec. 5."}, {"heading": "4 Main Result", "text": "The following assumptions will be instrumental in the subsequent discussion.\n[As0] (Stationarity.) Expectation F (x) := EO{Ft(x ;Ot)} is time-invariant.\n[As1] (Boundedness from below.) Functions Ft(x ;Ot) are bounded from below almost surely (a.s.).\n[As2] (Coercivity.) If limk\u2192\u221e E{\u2016\u03bek\u20162} = +\u221e for any (\u03bek)k\u2208Z\u22650 \u2282 H, then limk\u2192\u221e E{F (\u03bek)} = +\u221e [44].\n[As3] With (x\u03c4 )\u03c4\u2208Z\u22650 standing for the sequence of estimates of the algorithm in Table 2, and F\u2217\ndenoting the limit which appears in Thm. 1.1, \u2203x(b)\u2217 \u2208 Hb s.t. lim sup\u03c4\u2192\u221e E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 F\u2217.\n[As4] Given the sequence of Lipschitz coefficients (L (b) \u03c4 )\u03c4\u2208Z\u22650 produced by the algorithm in Table 2,\nthere exists L\u0302 \u2208 R\u22650 s.t. L(b)\u03c4 \u2264 L\u0302 a.s. \u2200(\u03c4, b). Moreover, there exist \u03c4 (b)\u2217 \u2208 Z\u22650 and a sufficiently small \u03b4(b) \u2208 R>0 s.t. \u03b7(b)\u03c4 \u03bb(b)\u03c4 L\u0302 \u2264 1\u2212 \u03b4(b), \u2200\u03c4 \u2265 \u03c4 (b)\u2217 .\n[As5] If w (b) \u03c4 \u2208 argminx(b)\u2208Hb E{F (x(b) | x (\u2212b) \u03c4 )}, then (E{\u2016w(b)\u03c4 \u20162})\u03c4\u2208Z\u22650 is bounded.\nComments on the previous assumptions are in order. First, As0 can be recognized as one of the principal hypotheses in SA. As1 will be used to prevent cost values from sinking to \u2212\u221e, and it is usually met in practice, e.g., any quadratic data-fit term as well as any vector-norm satisfy As1 due to non-\nnegativity. As2 will be used to prevent the proposed algorithm from generating unbounded sequences of\nestimates, without any a-priori enforcement of hard bounds on the variables, as in [42]. Examples where coercivity is introduced via {gb}Bb=1 will be given shortly in Sec. 5. With regard to As3, it will be shown in Lemma 3 that it is a necessary condition for properties related to (weak/strong sequential) cluster\npoints of (x\u03c4 )\u03c4\u2208Z\u22650 , as well as to the boundedness of (sub)gradients of the expected cost. The existence of a sufficiently large L\u0302, which upper-bounds the data-dependent Lipschitz coefficients in As4, is well-\nmotivated by the coercivity assumption As2 that promotes bounded sequences of iterates (cf. Thm. 1.2).\nThe clarification of the previous statement will be given through concrete examples in Remark 2, where\nthe boundedness of the resultant iterates, as well as an assumption on the boundedness of the moments\nof the observed data, justify the existence of L\u0302. As4 and the related performance analysis suggest also strategies for selecting {\u03b7(b)\u03c4 , \u03bb(b)\u03c4 } (cf. Remark 1). It is important to stress here that the stepsize \u03b2 (b) \u03c4 of the forward-backward iteration relies on the \u201clocal\u201d Lipschitz coefficient L (b) \u03c4 and not on L\u0302, e.g., \u03b2 (b) \u03c4 := 1/L (b) \u03c4 . Finally, As5 imposes a uniform bound, across time, on the second-order moment of (per-block) minimizers of the expected cost. This is the case if moments of the observed data (Ot)t\u2208Z>0 and block variables are bounded. In this sense, As5 is necessary here since the present framework does\nnot enforce hard bounds on block variables and follows the more relaxed coercivity assumption of As2.\nIt is also important to stress here that As5 is a condition on existence; there is no need of constructing\nsuch minimizers for the algorithm to operate.\nThe following lemma gathers a few helpful properties on the function E{F (\u00b7 | x(\u2212b))}.\nLemma 1. Per block b and for any realization of x(\u2212b), function x(b) 7\u2192 E{F (x(b) | x(\u2212b))} is convex on Hb. Moreover, EO|X{F \u2032t (x(b);Ot | x(\u2212b))} \u2208 \u2202 E{F (x(b) | x(\u2212b))}. In other words, EO|X{F \u2032t (x(b);Ot | x(\u2212b))} is a subgradient of E{F (\u00b7 | x(\u2212b))} at x(b). Further, under As0 and As1, E{F (\u00b7)} is bounded from below on H.\nProof. See Appendix A.1.\nThe following lemma sheds light on the connection between the selection of {\u03b7(b)\u03c4 , \u03bb(b)\u03c4 } and the negativity of the quadratic polynomial in (M3).\nLemma 2. Under As4 there exists a sequence of stepsizes (\u03b2 (b) \u03c4 )\u03c4\u2208Z>0 and a \u03b4\u030c (b) \u2208 R>0 s.t.\nL(b)\u03c4 \u03b2 (b) \u03c4 2 \u2212 2\u03b2(b)\u03c4 + \u03b7(b)\u03c4 \u03bb(b)\u03c4 \u2264 \u2212\u03b4\u030c(b) , \u2200\u03c4 \u2265 \u03c4\u2217 .\nProof. See Appendix A.2.\nThe main results of this paper are summarized in the following theorem.\nTheorem 1.\n1. Under As0 and As1, there exists F\u2217 to which (E{F (xt)})t\u2208Z>0 converges. In other words, \u2203F\u2217 \u2208 R s.t. F\u2217 = limt\u2192\u221e E{F (xt)} = lim\u03c4\u2192\u221e E{F (x(b)\u03c4 )}, \u2200b \u2208 {1, . . . , B}, where x(b)\u03c4 is defined in line 12 of Table 2a.\n2. Under As0, As1, and As2, sequences (xt)t\u2208Z>0 and (x (b) \u03c4 (b) )\u03c4 (b)\u2208Z\u22650 inH, as well as (x (b) \u03c4 (b) )\u03c4 (b)\u2208Z\u22650 in Hb\nare bounded. Consequently, the sets of weak sequential cluster points W{(xt)t\u2208Z>0}, W{(x (b) \u03c4 (b) )\u03c4 (b)\u2208Z\u22650}, and W{(x(b) \u03c4 (b) )\u03c4 (b)\u2208Z\u22650} are non-empty [44, Lem. 2.37]. Moreover, according to (M6) define\nT (b) \u03b6 :=\n{ \u03c4 (b) \u2208 Z\u22650 \u2223 \u2223 \u2223 x (b)\n\u03c4 (b) 6= x(b) \u03c4 (b)\u22121\n}\n.\nThen, sequence (\u03b6 (b)\n\u03c4 (b) ) \u03c4 (b)\u2208T\n(b) \u03b6 \u2282 Hb is bounded and W{(\u03b6(b)\u03c4 (b))\u03c4 (b)\u2208T(b) \u03b6 } 6= \u2205.\n3. Under As0\u2013As3, E{F (x(b)\u03c4 )} enjoys a quadratic rate of convergence to F\u2217. More precisely, for any arbitrarily fixed \u01eb \u2208 R>0, there exists \u03c4 \u20320 \u2208 Z>0 s.t. \u2200\u03c4 > \u03c4 \u20320,\nE{F (x(b)\u03c4 )} \u2212 F\u2217\n\u2264 4 \u03bb (b) 1 2\u03bb\u030c(b)2\u03b7\u030c(b)(1 + \u03c4)2\n[\n\u03b7(b)\u03c40 \u00b5 (b) \u03c40 2 ( E{F (x(b)\u03c40 )} \u2212 F\u2217 )\n+ 1\n2 E {\u2225 \u2225 \u2225 \u2225 \u2225 \u03b7 (b) \u03c40 \u03bb (b) \u03c40 \u00b5 (b) \u03c40\n\u03b2 (b) \u03c40\n\u03b6(b)\u03c40 + v (b) \u03c40 \u2212 \u03bb (b) 1 x (b) \u2217 \u2225 \u2225 \u2225 \u2225 \u2225 2}] + \u01eb .\n4. Under As0, As1, and As4, \u2211\u221e \u03c4=0 E{\u2016\u03b6 (b) \u03c4 \u2212\u03c8(b)\u03c4 \u20162} < +\u221e. Necessarily, lim\u03c4\u2192\u221e E{\u2016\u03b6(b)\u03c4 \u2212\u03c8(b)\u03c4 \u20162} = 0.\n5. Under As0, As1, and As4,\nlim \u03c4\u2192\u221e\nE { \u2016EO|X{F \u2032\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ;O\u03c4 )}\u20162 } = 0.\nIn other words, according to Lemma 1, the subgradient EO|X{F \u2032t (\u03b6 (b) \u03c4 | x(\u2212b)\u03c4 ;O\u03c4 )} of E{F (\u00b7 | x(\u2212b)\u03c4 )} at \u03b6 (b) \u03c4 converges to 0 in the mean-squared sense.\n6. Under As0\u2013As2, and As4\u2013As5,\nF\u2217 = lim \u03c4\u2208T\n(b) \u03b6\nE{F (\u03b6(b)\u03c4 | x(\u2212b)\u03c4 )}\n= lim sup \u03c4\u2192\u221e min x(b)\u2208Hb\nE{F (x(b) | x(\u2212b)\u03c4 )} .\nThe last equation implies that there exists a subsequence (x (b) \u03c4k )k\u2208Z\u22650 that satisfies the following property: For any arbitrarily small \u01eb \u2208 R>0, there exists a k0 s.t. \u2200k \u2265 k0,\nE{F (x(b)\u03c4k | x (\u2212b) \u03c4k )} \u2212 min x(b)\u2208Hb E{F (x(b) | x(\u2212b)\u03c4k )} \u2264 \u01eb .\nIn other words, there exists (x (b) \u03c4k )k\u2208Z\u22650 onto which E{F (x (b) \u03c4k | x (\u2212b) \u03c4k )} approximate arbitrarily close the per-block minima minx(b)\u2208Hb E{F (x (b) | x(\u2212b)\u03c4k )}.\nProof. The proof is given in Appendix A.3.\nTo show that As3 is a rather weak assumption, the following Lemma 3 demonstrates that As3 is\nnecessary to the more \u201cconventional\u201d As6a and As6b1 on weak and strong sequential cluster points of\nthe sequence of r.vs. (xt)t\u2208Z>0 . More specifically, As6b assumes existence of a strong sequential cluster point and bounds a sequence of subgradients of the expected cost, similarly to the bound on gradients\nintroduced in [47].\nLemma 3. Under As0\u2013As2, if E{F (\u00b7)} is also l.s.c. on H, then As3 is necessary to As6a. Moreover, under As0\u2013As2, As3 is also necessary to As6b.\n1Both As6a and As6b are placed in Appendix A.4 for not disrupting the flow of the present discussion.\nProof. See Appendix A.4.\nRemark 1. Setting \u03bb (b) \u03c4 := 1 for simplicity, As4 suggests that \u03b7 (b) \u03c4 should be sufficiently small for \u03b7 (b) \u03c4 \u2264 (1 \u2212 \u03b4(b))/L\u0302, given \u03b4(b) \u2208 (0, 1) and assuming that L\u0302 is available. Without having knowledge of L\u0302, selection rules for \u03b7 (b) \u03c4 are (i) \u03b7 (b) \u03c4 := \u03b7\u030c(b), and (ii) \u03b7 (b) \u03c4 := \u03b7\u030c(b)+1/\u03c4 , after choosing a sufficiently small \u03b7\u030c(b) > 0 [cf. (M2)]. Notice that the previous rules abide by the monotonicity of \u03b7 (b) \u03c4 , i.e., \u03b7 (b) \u03c4+1 \u2264 \u03b7 (b) \u03c4 , in (M2). In practice, and in the numerical tests of Sec. 6, the following rule is adopted:\n\u03bb(b)\u03c4 := 1, \u03b7 (b) \u03c4 := min\n{\n\u03b7 (b) \u03c4\u22121,\n1\u2212 \u03b4(b)\nL (b) \u03c4\n, \u03b7\u030c(b) + 1\n\u03c4\n}\n. (4)\nIt is important to stress here that the selection of stepsize \u03b2 (b) \u03c4 , in the forward-backward iteration of (M3), is based on the \u201clocal\u201d coefficient L (b) \u03c4 and not on L\u0302. Further discussion on L\u0302 is provided in Remark 2."}, {"heading": "5 Examples", "text": "Two concrete examples of practical interest follow."}, {"heading": "5.1 Total least-squares", "text": "Data (yt)t\u2208Z>0 are generated by yt = u \u22a4 \u2217ts\u2217+vt, where (u\u2217t, s\u2217) \u2208 RQ\u00d7RQ; the unknown s\u2217 is sparse; \u22a4 denotes transposition; and vt stands for noise. Observed data are Ot := {yt,ut}, where ut := u\u2217t\u2212et is a noisy version of u\u2217t, and no statistical information on the process (et)t\u2208Z>0 is available. Motivated by the TLS criterion and the resultant errors-in-variables (EIV) modeling approach [7, 16], the following\nsequence of per-block-convex costs is considered:\nFt(s,e;Ot) := 1\n2\n[ yt \u2212 (ut + e)\u22a4s ]2 + \u03bbs2 2 \u2016s\u20162 \ufe38 \ufe37\ufe37 \ufe38\n=: ft(s,e;Ot)\n+ \u03bbs1\u2016s\u20161 \ufe38 \ufe37\ufe37 \ufe38\n=: g1(s)\n+ \u03bbe 2 \u2016e\u20162\n\ufe38 \ufe37\ufe37 \ufe38\n=: g2(e)\n, (5)\nwhere the first quadratic term in (5) quantifies fitness to the observed data, with e modeling EIV; \u2016 \u00b7 \u20161 promotes sparsity on s; \u2016e\u20162 penalizes large entries of e; and \u2016s\u20162 is used to regularize the cost in (5) by imposing coercivity (cf. As2). To draw connections with Sec. 2, M1 := M2 := R Q, and x(1) := s, x(2) := e.\nIf {yt,ut} are (jointly) wide sense stationary, then As0 holds with F (s,e) := EO{Ft(s,e;Ot)}. Due to the non-negativity of all terms in (5), it can be readily verified that As1 is also satisfied.\nIn the case where both s and e are considered as r.vs., x := (s,e) \u2208 H, \u03bbs2 6= 0, \u03bbe 6= 0, and (5) suggest that Ft(x;Ot) \u2265 \u03bbs2\u2016s\u20162/2 + \u03bbe\u2016e\u20162/2, and under As0, E{F (x)} = EX{EO|X{Ft(x;Ot)}} \u2265 \u03bbs2 E{\u2016s\u20162}/2 + \u03bbe E{\u2016e\u20162}/2; hence, As2 holds.\nMoreover, it can be verified by standard algebraic manipulations that a Lipschitz coefficient of \u2207sft is L\n(s) t (e,ut) = \u2016(ut+ e)(ut+ e)\u22a4 +\u03bbs2IQ\u2016 \u2264 \u2016(ut+ e)(ut+ e)\u22a4\u2016F +\u03bbs2\u2016IQ\u2016F = \u2016ut+ e\u20162+\u03bbs2\n\u221a Q,\nwhere \u2016A\u2016 denotes the spectral norm of a matrix A. Moreover, a Lipschitz coefficient of \u2207eft is L (e) t (s) = \u2016ss\u22a4\u2016 \u2264 \u2016ss\u22a4\u2016F = \u2016s\u20162."}, {"heading": "5.2 Semi-supervised dictionary learning", "text": "Following [8], consider an undirected graph G(V, E), where V denotes the set of all vertices or nodes,\nwith cardinality V, and E is the set of all edges. Connectivity and edge strengths of G are described by the adjacency matrix W \u2208 RV\u00d7V, where [W ]ij > 0 if nodes \u03bdi and \u03bdj are connected, while [W ]ij = 0 otherwise. Per t and node \u03bd, r.v. \u03c7t\u03bd : \u2126 \u2192 R describes a network-wide dynamical process of interest, e.g., traffic load. All r.vs. are collected in \u03c7t := [\u03c7t1, . . . , \u03c7tV]. A succinct representation of the process over G models \u03c7t as a superposition of \u201cfew\u201d atoms in a dictionary D \u2208 RV\u00d7Q, Q \u2265 V: \u03c7t = Dst, where st \u2208 RQ is sparse. Further, only a few entries of \u03c7t are observed. Such a missing-entries scenario is conceivable in cases where not all of {\u03c7t\u03bd}V\u03bd=1 are observable due to privacy constraints, severely corrupted measurements, node failures, or, data collection costs. To this end, let the random masking matrix Mt \u2208 RM\u00d7V, M < V, whose mth row is the transpose of a canonical basis vector for RV; in other words, Mt\u03c7t selects M out of V entries of \u03c7t. To summarize, yt = MtDst + vt, with observed data Ot := {yt,Mt} and vt denoting noise. To enable imputation of missing entries, the topology of G is utilized. Spatial correlation of the network is captured by the Laplacian matrix L := diag(W1V)\u2212W , where diag(a) defines the diagonal matrix whose main diagonal entries are those of vector a, and 1V \u2208 RV is the all-one vector. Given a \u201cforgetting factor\u201d \u03b4 \u2208 (0, 1] to gradually diminish the effect of past data, define the per-block-convex cost\nFt(s,D)\n:=\n=: ft(s,D;Ot) \ufe37 \ufe38\ufe38 \ufe37 t\u2211\n\u03c4=1\n\u03b4t\u2212\u03c4\u2016y\u03c4 \u2212M\u03c4Ds\u20162 2\u2206t + \u03bbL 2 s\u22a4D\u22a4LDs+ \u03bbs2 2 \u2016s\u20162\n+ \u03bbs1\u2016s\u20161 \ufe38 \ufe37\ufe37 \ufe38\n=: g1(s)\n+ \u03b9D(D) \ufe38 \ufe37\ufe37 \ufe38\n=: g2(D)\n, (6)\nwhere \u2206t := \u2211t \u03c4=1 \u03b4 t\u2212\u03c4 ; \u2016s\u20162 and \u2016s\u20161 are as in (5), while the term including L quantifies prior knowledge on the topology of G, promotes \u201csmooth\u201d solutions over strongly connected nodes of G, and\nis instrumental in imputing missing entries [8].\nTo establish links with the introductory discussion, M1 := R Q (x(1) := s), with \u3008\u00b7 | \u00b7\u3009\nM1 being the\ndot-vector product, and M2 := R V\u00d7Q (x(2) := D), with \u3008D1 | D2\u3009M2 := trace(D\u22a41 D2), \u2200(D1,D2) \u2208 M22 . If expectations in (6) are invariant w.r.t. t, then As0 holds with F (s,D) := EO{Ft(s,D;Ot)}. The non-negativity of all terms in (6) guarantees that As1 is also satisfied. Further, by following a similar argument as in Sec. 5.1, for any \u03bbs2 6= 0, E{F (\u00b7)} satisfies As2. Standard algebra suggests that a Lipschitz coefficient of \u2207sft is L(s)t (D,At) = \u2016D\u22a4AtD+\u03bbs2IQ\u2016 \u2264\n\u2016D\u22a4AtD+\u03bbs2IQ\u2016F \u2264 \u2016D\u20162F\u2016At\u2016F+\u03bbs2 \u221a Q, whereAt := \u2211t \u03c4=1 \u03b4\nt\u2212\u03c4M\u22a4\u03c4 M\u03c4/\u2206t+\u03bbLL. By\u2207Dft(D) = AtDss \u22a4\u2212\u2211t\u03c4=1 \u03b4t\u2212\u03c4M\u22a4\u03c4 y\u03c4s\u22a4/\u2206t, it can be also verified that a Lipschitz constant of\u2207Dft is L (D) t (s,At) = \u2016s\u20162\u2016At\u2016F.\nThe computational complexities of the algorithm in Table 2 on examples of Secs. 5.1 and 5.2,\nincluding computations of Lipschitz constants and function evaluations, are linear w.r.t. to the number\nof unknown variables, and more specifically, in the order of O[(R1 +R2)Q] and O[(R1 +R2)(Q+V)V] per t, respectively.\nRemark 2. With regard to the selection of L\u0302 in As4, recall that Markov\u2019s inequality dictates that Pr(L (b) \u03c4 \u2265 L\u0302) \u2264 E{L(b)\u03c4 }/L\u0302, for any L\u0302 [46]. Provided that there exists \u039b\u0302 s.t. E{L(b)\u03c4 } < \u039b\u0302, one can arbitrarily decrease the measure of the event {\u03c9 \u2208 \u2126 | L(b)\u03c4 (\u03c9) \u2265 L\u0302} by choosing a sufficiently large L\u0302. Examples for which E{L(b)\u03c4 } < \u039b\u0302 can be found in this section; regarding Sec. 5.1, it is straightforward to verify that E{L(s)\u03c4 (e\u03c4 ,u\u03c4 )} \u2264 2E{\u2016u\u03c4\u20162} + 2E{\u2016e\u03c4\u20162} + \u03bbs2 \u221a Q. Hence, under Thm. 1.2 and the assumption that E{\u2016u\u03c4\u20162} < +\u221e, there exists \u039b\u0302 < \u221e s.t. E{L(s)\u03c4 (e\u03c4 ,u\u03c4 )} \u2264 \u039b\u0302, \u2200\u03c4 . It can be also verified that the previous discussion carries over to the Lipschitz coefficients of Sec. 5.2 in a similar way."}, {"heading": "6 Numerical Tests", "text": ""}, {"heading": "6.1 Synthetic data", "text": "To validate the algorithm of Table 2 on the example of Sec. 5.1, entries of (u\u2217t := u\u2217, s\u2217) are drawn independently from a zero-mean, unit-variance Gaussian r.v. To make s\u2217 sparse, its nonzero entries are placed randomly in s\u2217 following a uniform distribution under two scenarios, a low-dimensional one corresponding to (Q, \u2016s\u2217\u20160) = (100, 10), and a high-dimensional one with (Q, \u2016s\u2217\u20160) = (103, 100), tagged \u201clow-d\u201d and \u201chigh-d\u201d in Figs. 2a, 2b, and 2c, respectively. Noise vt is considered to be zero-mean and i.i.d. Gaussian, with variance 10\u22122. Regressor vectors (ut)t\u2208Z>0 are observed after an i.i.d. zeromean Gaussian process with variance 10\u22124 is added to u\u2217. Parameters (\u03bbs1, \u03bbs2, \u03bbe) = (10 \u22125, 10\u22121, 1) are used in (5), common to all employed methods. Minimization w.r.t. block e accepts a closed-form solution; given s, the minimizer of (5) w.r.t. e is e\u0302 = (yt \u2212 u\u22a4t s)(ss\u22a4 + \u03bbeIQ)\u22121s, where inversion is facilitated by the matrix inversion lemma as (ss\u22a4 + \u03bbeIQ)\n\u22121 = [IQ \u2212 ss\u22a4/(\u03bbe + \u2016s\u20162)]/\u03bbe. It is worth noticing here that R1 = 3 for the inner loop in Table 2 (lines 7\u201313). Parameters {\u03b7(1)\u03c4 , \u03bb(1)\u03c4 } follow (4).\nThe algorithm in Table 2 is tested against a block-version of the classical online (sub)gradient\ndescent method [25], tagged as BOGD in Fig. 2. BOGD adopts the Gauss-Seidel strategy of visiting\nblocks; per t, the standard subgradient descent step is applied first w.r.t. s with constant step size 10\u22123, followed by a minimization step w.r.t. block e which is given also here by the closed-form solution\nemployed in the proposed scheme. Moreover, a block coordinate descent (BCD) strategy is validated,\nwhere (5) is \u201cmaximally\u201d separated in scalar-valued blocks w.r.t. s. More specifically, following the Gauss-Seidel scheme and letting index b \u2208 {1, . . . , Q} visit the bth entry of s successively, having fixed e and the entries {sj}j 6=b of s, minimization of (5) w.r.t. sb amounts to the scalar-valued optimization task s\u0302b := argminsb [yt \u2212 \u2211\nj 6=b(utj + ej)sj \u2212 (utb + eb)sb]2/2 + \u03bbs2s2b + \u03bbs1|sb|, which can be solved in closed form using the soft-thresholding operator as\ns\u0302b =\n{ (utb+eb)\u03b8b\u2212sgn[(utb+eb)\u03b8b]\u03bbs1\n(utb+eb)2+\u03bbs2 , |(utb + eb)\u03b8b \u2212 \u03bbs1| > 0\n0, otherwise .\nNotice that \u03b8b := yt\u2212 \u2211 j 6=b(utj + ej)sj , while utj denotes the jth entry of ut. After all entries of s are updated, the closed form solution e\u0302, leveraged in the proposed and BOGD techniques, updates block\n(a) Cost function values vs. time.\n(b) Normalized deviation on the complement of supp(s\u2217).\ne to conclude step t of BCD. For fairness, both BOGD and BCD run three consecutive iterations per t\nto meet the computational load of the proposed scheme where R1 = 3.\nFigs. 2a, 2b, and 2c illustrate the performance of the employed methods. Fig. 2a depicts the cost\nfunction values (5) across time; Fig. 2b shows the per entry deviation [ \u2211\nj /\u2208supp(s\u2217) (stj \u2212 s\u2217j)2]1/2/(Q\u2212\n\u2016s\u2217\u20160) across t, where supp(s\u2217) stands for the support of s\u2217, i.e., all those indexes j s.t. s\u2217j 6= 0; and Fig. 2c plots the time-variations of the subgradient norm of the cost. Curves in Figs. 2a, 2b, and 2c\nare obtained after averaging uniformly 100 realizations, and are illustrated in log-log scale for easily\nidentifying the rate of convergence. Numerical results corroborate Thm. 1.3 which states that there\nexists a time instant after which the proposed algorithm converges with quadratic rate to an arbitrarily\nsmall neighborhood around F\u2217. The behavior of BOGD confirms the fact that (sub)gradient techniques are in general slow convergent."}, {"heading": "6.2 Real data", "text": "In the context of Sec. 5.2, the advocated algorithm is validated on estimating and tracking network-wide\nlink loads taken from the Internet2 measurement archive [48]. Analyzing the Internet2 backbone network\nyields a graph Gwith V = 54 number of vertices. Using the network topology and routing information, network-wide link loads (\u03c7t) 30,000 t=1 \u2282 RV become available (in Gbps). Per time slot t, only M = 30 of the \u03c7t components, chosen randomly via Mt \u2208 RM\u00d7V, are observed in yt \u2208 RM . The cardinality of\nthe time-varying dictionaries is set constant to Q = 80. To cope with pronounced temporal variations\nof the Internet2 link loads, the forgetting factor \u03b4 in Sec. 5.2 is set equal to 0.95. Initial values for both\n(s,D) are randomly drawn from the feasibility regions seen in Sec. 5.2. Parameters in (6) are defined as (\u03bbL, \u03bbs1, \u03bbs2) = (10 \u22123, 10\u22123, 10\u22123). Moreover, as in Sec. 6.1, {\u03b7(b)\u03c4 , \u03bb(b)\u03c4 } follow (4).\nThe advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a Gauss-\nSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related\nto (6) w.r.t. s, with the same parameters (\u03bbL, \u03bbs1, \u03bbs2) as in (6), and (ii) BCD iterations requiring matrix inversions are leveraged to optimize the associated loss w.r.t. D. Fig. 2d depicts the normalized squared estimation error between the true \u03c7t and the inferred \u03c7\u0302t, namely \u2016\u03c7t\u2212 \u03c7\u0302t\u20162/\u2016\u03c7t\u20162, versus time t for a randomly chosen network link. For visualization reasons, only a small portion of the data is shown in\nFig. 2d. To obtain computationally light recursions, the number of inner loops in Table 2 w.r.t. s is set\nequal to R1 = 2, while R2 = 5 w.r.t. D. It is worth noticing here that ADMM in [8] requires multiple iterations to achieve a prescribed estimation accuracy, and that no matrix inversion was incorporated in\nthe realization of Table 2. The proposed method and [8] perform similarly, scoring mean (normalized)\nestimation errors of 0.1166 and 0.1161 on the entire dataset of cardinality 30, 000, respectively."}, {"heading": "7 Conclusions", "text": "This manuscript presented a modular online learning algorithm which extended arguments, originally\ndeveloped for accelerating first-order methods in batch convex optimization tasks, to the per-block-\nconvex and stochastic approximation context. The proposed framework showed a computational com-\nplexity that scales linearly w.r.t. the number of unknowns. Assuming no knowledge of the underlying\ndata statistics, the convergence rate of the expected loss on the resultant iterates was proved to be\nquadratic. Rigorous theoretical analysis was performed in the Hilbert space of r.vs. of finite second-order\nmoments. The framework was tested on two instances of broad practical interest: (i) Sparsity-aware\nregression based on the TLS criterion; and (ii) semi-supervised DL for network-wide link load tracking\nand imputation. Numerical tests on synthetic and real data demonstrated that the proposed algorithm\nperforms better than BCDMs and comparably to state-of-the-art but computationally heavier ADMM-\nbased methods. Future directions include the extension of the proposed framework from Gauss-Seidel\nstrategies of visiting blocks of variables to parallel and random ones. Moreover, to study the effect\nof data non-stationarities, a regret analysis on the per-block convex loss will be presented in a future\nsubmission."}, {"heading": "A Appendices", "text": "A.1 Proof of Lemma 1\nSince x(b) 7\u2192 Ft(x(b) | x(\u2212b);Ot) is convex on Mb for any realizations of x(\u2212b) and Ot, then \u2200\u03bb \u2208 [0, 1], and \u2200(x(b)1 , x (b) 2 ) \u2208 M2b , Ft(\u03bbx (b) 1 + (1 \u2212 \u03bb)x (b) 2 | x(\u2212b);Ot) \u2264 \u03bbFt(x (b) 1 | x(\u2212b);Ot) + (1 \u2212 \u03bb)Ft(x (b) 2 | x(\u2212b);Ot). Applying EX{EO|X{\u00b7}} to both sides of the previous inequality yields the convexity of E{F (\u00b7 | x(\u2212b))} on Hb.\nThe convexity of Ft(\u00b7 | x(\u2212b);Ot) implies that for any x(b) \u2208 Mb, Ft(x(b) | x(\u2212b);Ot) +\u3008F \u2032t(x(b) | x(\u2212b);Ot) | \u03be \u2212 x(b)\u3009 \u2264 Ft(\u03be | x(\u2212b);Ot), \u2200\u03be \u2208 Mb. Application of EO|X{\u00b7} to both sides of\nthe previous inequality results in F (x(b) | x(\u2212b)) + \u3008EO|X{F \u2032t (x(b) | x(\u2212b);Ot)} | \u03be \u2212 x(b)\u3009 \u2264 F (\u03be | x(\u2212b)). An additional application of E{\u00b7} yields E{F (x(b) | x(\u2212b))}+E{\u3008EO|X{F \u2032t (x(b) | x(\u2212b);Ot)} | \u03be\u2212x(b)\u3009} \u2264 E{F (\u03be | x(\u2212b))}, or, EO|X{F \u2032t (x(b) | x(\u2212b);Ot) \u2208 \u2202 E{F (x(b) | x(\u2212b))}. Notice finally that it can be trivially verified by As1 that E{F (\u00b7)} is bounded from below on H.\nA.2 Proof of Lemma 2\nIt can be verified that min\u03b2\u2208R{L(b)\u03c4 \u03b22 \u2212 2\u03b2 + \u03b7(b)\u03c4 \u03bb(b)\u03c4 } = (\u03b7(b)\u03c4 \u03bb(b)\u03c4 L(b)\u03c4 \u2212 1)/L(b)\u03c4 , which is attained at \u03b2\u2217 = 1/L (b) \u03c4 . Due to As4, (\u03b7 (b) \u03c4 \u03bb (b) \u03c4 L (b) \u03c4 \u2212 1)/L(b)\u03c4 \u2264 (\u03b7(b)\u03c4 \u03bb(b)\u03c4 L\u0302\u2212 1)/L(b)\u03c4 \u2264 (\u03b7(b)\u03c4 \u03bb(b)\u03c4 L\u0302\u2212 1)/L\u0302 \u2264 \u2212\u03b4(b)/L\u0302. Hence, the choices of \u03b2 (b) \u03c4 := 1/L (b) \u03c4 and \u03b4\u030c(b) := \u03b4(b)/L\u0302 are sufficient to establish the claim of Lemma 2. Moreover, due to the continuity of the function L (b) \u03c4 \u03b22 \u2212 2\u03b2 + \u03b7(b)\u03c4 \u03bb(b)\u03c4 w.r.t. \u03b2, one can always find a neighborhood of 1/L (b) \u03c4 onto which the claim of Lemma 2 also holds for, let\u2019s say, \u03b4\u030c(b)/2.\nA.3 Proof of Theorem 1\nFirst, a fact is in order.\nFact 1 ([39, Lem. 1]).\n1. Let \u03d5 := f + g, where (f, g) \u2208 \u03930(M)2, and f is Lf -Lipschitz continuously differentiable. For any \u03b2 \u2208 R>0, define \u03b6\u03c8 := Prox\u03b2g[\u03c8 \u2212 \u03b2\u2207f(\u03c8)], \u2200\u03c8 \u2208 M. Assume that \u2203(\u03c8, \u03be, w) \u2208 M3, \u2203\u03b2 \u2208 R>0, and \u2203\u03bb \u2208 [0, 1] s.t. \u03d5(\u03be) \u2264 \u03d5((1 \u2212 \u03bb)w + \u03bb\u03b6\u03c8). Then, \u2200x \u2208 M, \u2200L \u2265 Lf ,\n\u03d5(\u03be) \u2264 (1\u2212 \u03bb)\u03d5(w) + \u03bb\u03d5(x)\u2212 \u03bb \u03b2 \u3008\u03b6\u03c8 \u2212 \u03c8 | \u03c8 \u2212 x\u3009\n+ \u03bb (L 2 \u2212 1 \u03b2 ) \u2016\u03b6\u03c8 \u2212 \u03c8\u20162 .\n2. Given (\u03bbi)i\u2208Z>0 \u2282 (0, 1], the sequence (\u00b5i)i\u2208Z>0 defined in line 4 of Table 1 satisfies \u00b5i+1 > \u00b5i and \u00b5i \u2265 \u03bb1(1 + \u2211i i\u2032=1 \u03bbi\u2032)/2, \u2200i \u2208 Z>0.\nRemark 3. Fact 1.1 holds for any over-estimate L of the Lipschitz coefficient Lf . This flexibility is inherited by the subsequent performance analysis, and it facilitates computations in Table 2 in cases\nwhere computing Lf requires considerable effort; cf. Sec. 5 where the smallest Lipschitz coefficients requires computation of the spectral norm of a matrix, while an over-estimate is provided by the\nmanageable Frobenius norm. Under these considerations, it will be assumed that in all of the subsequent discussion there exists a sufficiently small L\u030c \u2208 R>0 that stands as a lower bound on all employed Lipschitz coefficients.\nThe proof of Thm. 1 now follows. Symbol Ot is omitted to avoid overloading notations. For the\nsame reason, superscript (b) is often omitted from \u03c4 (b). 1) By (M6), and the definition of x (\u2212b) \u03c4 given in Table 2a (line 9),\nFt ( x(b)\u03c4 | x(\u2212b)\u03c4 ) \u2264 Ft ( x (b) \u03c4\u22121 | x(\u2212b)\u03c4 ) = Ft ( x (b) \u03c4\u22121 | x (\u2212b) \u03c4\u22121 ) . (7)\nFor \u03c4 \u2208 {(t\u22121)Rb+2, . . . , tRb}, the previous inequality yields Ft(x(b)tRb | x (\u2212b) tRb ) \u2264 Ft(x(b)(t\u22121)Rb+1 | x (\u2212b) (t\u22121)Rb+1 ). Moreover, by Table 2a (line 11), Ft(x (b) (t\u22121)Rb+1 | x(\u2212b)(t\u22121)Rb+1) \u2264 Ft(x (b) (t\u22121)Rb | x(\u2212b)(t\u22121)Rb+1). Notice now by\nthe definition in Table 2a (line 12), that if {x(b)(t\u22121)Rb} is combined with x (\u2212b) (t\u22121)Rb+1 , then x (b\u22121) tRb\u22121 is obtained. Hence, the previous arguments summarize to Ft(x (b) tRb ) = Ft(x (b) tRb | x(\u2212b)tRb ) \u2264 Ft(x (b\u22121) tRb\u22121 ). If b assumes all consecutive values in {1, . . . , B}, then the previous inequality yields Ft(xt) = Ft(x(B)tRB ) \u2264 Ft(x (0) tR0 ) = Ft(xt\u22121), where x (0) tR0 := xt\u22121 was used in the last equality. Consequently, by As0, E{F (xt)} = EX EO|X{Ft(xt)} \u2264 EX EO|X{Ft(xt\u22121)} = E{F (xt\u22121)}. This suggests that the boundedfrom-below sequence (E{F (xt)})t\u2208Z>0 is non-increasing; thus convergent. In other words, \u2203F\u2217 \u2208 R s.t. F\u2217 = limt\u2192\u221e E{F (xt)}.\nWith reference to (7) and the preceding arguments, \u2200\u03c4 \u2208 {(t\u2212 1)Rb + rb | rb \u2208 {1, . . . , Rb}},\nE{F (xt)} \u2264 E { F ( x(b)\u03c4 | x(\u2212b)\u03c4 )} = E { F ( x(b)\u03c4 )}\n\u2264 E { F ( x (b) \u03c4\u22121 | x (\u2212b) \u03c4\u22121 )} = E { F ( x (b) \u03c4\u22121 )} \u2264 E{F (xt\u22121)} .\nHence, since (E{F (xt)})t\u2208Z>0 converges to F\u2217, so does also the non-increasing (E{F (x (b) \u03c4 )})\u03c4\u2208Z\u22650 . 2) Due to As2 and the existence of F\u2217 by Thm. 1.1, the sequences (xt)t\u2208Z>0 and (x (b)\n\u03c4 (b) )\u03c4 (b) are necessarily\nbounded. Moreover, due to the definition of H, boundedness of (x (b)\n\u03c4 (b) )\u03c4 (b) implies also boundedness of\nthe block-sequence (x (b) \u03c4 (b) )\u03c4 (b) \u2282 Hb, \u2200i. Now, \u2200\u03c4 \u2208 T (b) \u03b6 ,\nE { \u2016\u03b6(b)\u03c4 \u20162 }\n= E\n \n \u2225 \u2225 \u2225 \u2225 \u2225 1\n\u03bb (b) \u03c4\n( x(b)\u03c4 \u2212 (1\u2212 \u03bb(b)\u03c4 )x(b)\u03c4\u22121 ) \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n\n\n\u2264 2 (\u03bb (b) \u03c4 )2 E { \u2016x(b)\u03c4 \u20162 } +\n2(1 \u2212 \u03bb(b)\u03c4 )2\n(\u03bb (b) \u03c4 )2\nE { \u2016x(b)\u03c4\u22121\u20162 }\n\u2264 2\u2206\u0302 (\u03bb (b) \u03c4 )2 \u2264 2\u2206\u0302 (\u03bb\u030c(b))2 ,\nwhere \u2206\u0302 \u2265 sup\u03c4\u2208Z\u22650 E{\u2016x (b) \u03c4 \u20162}. As such, (\u03b6(b)\u03c4 )\u03c4\u2208T(b)\n\u03b6 is bounded, and, consequently, W{(\u03b6(b)\u03c4 )\u03c4\u2208T(b) \u03b6 } 6= \u2205 [44, Lem. 2.37]. 3) The following proof is based on the one developed in [39] for the off-line, convex analytic case. The\nsubsequent one offers a generalization in the context of the present online/stochastic setup.\nBy (M6) in Table 2, F\u03c4+1(x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2264 F\u03c4+1((1 \u2212 \u03bb\u03c4+1)x\u03c4 + \u03bb\u03c4+1\u03b6\u03c4+1 | x (\u2212b) \u03c4+1 ). Fact 1.1 will be applied here with the convex F\u03c4+1(\u00b7 | x(\u2212b)\u03c4+1 ) taking the place of \u03d5, and (\u03c8\u03c4+1, x\u03c4+1, x\u03c4 , \u03b6\u03c4+1) that of (\u03c8, \u03be, w, \u03b6\u03c8). Let also x := x\u03c4 and an arbitrarily fixed x\u0304 (b) \u2208 Mb in Fact 1.1 to obtain\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )\n\u2264 F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )\n\u2212 \u03bb\u03c4+1 \u03b2\u03c4+1 \u3008\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1 |\u03c8\u03c4+1 \u2212 x\u03c4 \u3009\n+ \u03bb\u03c4+1 (L\u03c4+1 2 \u2212 1 \u03b2\u03c4+1 ) \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162. (8)\nAnother application of Fact 1.1 with x = x\u0304(b) yields\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )\n\u2264 (1\u2212 \u03bb\u03c4+1) [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 \u03bb\u03c4+1 \u03b2\u03c4+1\n\u2329 \u03b6\u03c4+1 \u2212 \u03c8\u03c4+1 \u2223 \u2223 \u2223\u03c8\u03c4+1 \u2212 x\u0304(b) \u232a\n+ \u03bb\u03c4+1 (L\u03c4+1 2 \u2212 1 \u03b2\u03c4+1 ) \u2016\u03b6\u03c4+1 \u2212 y\u03c4+1\u20162 . (9)\nMultiplying (8) by \u00b5\u03c4+1(\u00b5\u03c4+1 \u2212 \u03bb1) \u2265 0 and (9) by \u00b5\u03c4+1\u03bb1 \u2265 0, and adding the resultant inequalities,\n\u00b52\u03c4+1\n[\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2264 \u00b5\u03c4+1(\u00b5\u03c4+1 \u2212 \u03bb1\u03bb\u03c4+1) \u00d7 [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 \u2329 \u03bb\u03c4+1\u00b5\u03c4+1\n\u03b2\u03c4+1\n( \u03b6\u03c4+1 \u2212 \u03c8\u03c4+1 )\n\ufe38 \ufe37\ufe37 \ufe38 =: a\u03c4\n\u2223 \u2223 \u2223 \u2223\n(\u00b5\u03c4+1 \u2212 \u03bb1) ( \u03c8\u03c4+1 \u2212 x\u03c4 ) + \u03bb1 ( \u03c8\u03c4+1 \u2212 x\u0304(b) ) \ufe38 \ufe37\ufe37 \ufe38\n=: b\u03c4\n\u232a\n+ \u00b52\u03c4+1\u03bb\u03c4+1 (L\u03c4+1 2 \u2212 1 \u03b2\u03c4+1 ) \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 .\nApplication of \u3008a\u03c4 | b\u03c4 \u3009 = (\u2016\u03b7a\u03c4 + b\u03c4\u20162 \u2212 \u2016b\u03c4\u20162 \u2212 \u2016\u03b7a\u03c4\u20162)/(2\u03b7), \u2200\u03b7 \u2208 R>0, to the previous inequality and v\u03c4 := \u00b5\u03c4 (1\u2212 \u03b7\u03c4\u03bb\u03c4/\u03b2\u03c4 )\u03c8\u03c4 \u2212 (\u00b5\u03c4 \u2212 \u03bb1)x\u03c4\u22121 yield\n\u00b52\u03c4+1\n[\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2264 \u00b5\u03c4+1(\u00b5\u03c4+1 \u2212 \u03bb1\u03bb\u03c4+1) \u00d7 [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 1 2\u03b7\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\u03b7\u03c4+1\n\u2225 \u2225 \u2225\u00b5\u03c4+1\u03c8\u03c4+1 \u2212 (\u00b5\u03c4+1 \u2212 \u03bb1)x\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 2\n+ 1\n2\u03b7\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 ( \u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\n) \u2225 \u2225 \u2225 \u2225 2\n+ \u00b52\u03c4+1\u03bb\u03c4+1 (L\u03c4+1 2 \u2212 1 \u03b2\u03c4+1 ) \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 . (10)"}, {"heading": "It can be verified by (M7) that \u00b5\u03c4+1\u03c8\u03c4+1 \u2212 (\u00b5\u03c4+1 \u2212 \u03bb1)x\u03c4 = \u03b7\u03c4\u03bb\u03c4\u00b5\u03c4\u03b6\u03c4/\u03b2\u03c4 + \u00b5\u03c4 (1 \u2212 \u03b7\u03c4\u03bb\u03c4/\u03b2\u03c4 )\u03c8\u03c4 \u2212", "text": "(\u00b5\u03c4 \u2212 \u03bb1)x\u03c4\u22121. Notice also that (M4) is equivalent to \u00b52\u03c4 = \u00b5\u03c4+1(\u00b5\u03c4+1 \u2212 \u03bb1\u03bb\u03c4+1). Incorporating these\narguments and (M3) into (10),\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 1 2 \u2225 \u2225 \u2225 \u2225 \u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ \u03b7\u03c4+1\u00b5\n2 \u03c4+1\u03bb\u03c4+1\n2\u03b22\u03c4+1 (L\u03c4+1\u03b2\n2 \u03c4+1 \u2212 2\u03b2\u03c4+1 + \u03b7\u03c4+1\u03bb\u03c4+1)\n\u00d7 \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 (11) \u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 1 2 \u2225 \u2225 \u2225 \u2225 \u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2 .\nThe previous inequality suggests that\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[\nEO|X { F\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 )} \u2212 EO|X { F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )}]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1\n+ v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ EO|X { F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 )}\n\u2212 EO|X { F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)}]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\nwhich, according to As0, results in\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[\nF ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u2217 + F\u2217 \u2212 F ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1\n+ v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u2217 + F\u2217 \u2212 F ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2} .\nAfter some elementary algebra,\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[\nF ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u2217\n]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1\n+ v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u2217 ]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b7\u03c4+1(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nF ( x\u0304(b) | x(\u2212b)\u03c4+1 ) \u2212 F\u2217\n]\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u2217 ]\n+ 1\n2 EO|X\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nF ( x\u0304(b) | x(\u2212b)\u03c4+1 ) \u2212 F\u2217\n]\n.\nConsequently,\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[ E{F (x(b)\u03c4+1)} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ E { F ( x(b)\u03c4 | x(\u2212b)\u03c4+1 )} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nE { F ( x\u0304(b) | x(\u2212b)\u03c4+1 )} \u2212 F\u2217\n]\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ E{F (x(b\u22121)\u03c4+1 )} \u2212 F\u2217 ]\n(12a)\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nE { F ( x\u0304(b) | x(\u2212b)\u03c4+1 )} \u2212 F\u2217\n]\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ E{F (x(b)\u03c4 )} \u2212 F\u2217 ]\n(12b)\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nE { F ( x\u0304(b) | x(\u2212b)\u03c4+1 )} \u2212 F\u2217\n]\n\u2264 \u03b7\u03c4\u00b52\u03c4 [ E{F (x(b)\u03c4 )} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2}\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\nE { F ( x\u0304(b) | x(\u2212b)\u03c4+1 )} \u2212 F\u2217\n]\n, (12c)\nwhere Table 2a (line 12) and the monotonicity in Thm. 1.1 were used in (12a) and (12b), while (M2)\nwas utilized in (12c).\nLet now \u03d1(x (b) \u2217 ) := lim sup\u03c4\u2192\u221e E{F (x(b)\u2217 | x(\u2212b)\u03c4 )}, where x(b)\u2217 was defined in As3. Since (12c) holds for any x\u0304(b), by x\u0304(b) := x (b) \u2217 and the definition of lim sup, it is straightforward to verify that \u2200\u01eb > 0, there exists \u03c4 \u20320 \u2208 Z\u22650 s.t. \u2200\u03c4 \u2265 \u03c4 \u20320,\n\u03b7\u03c4+1\u00b5 2 \u03c4+1\n[ E{F (x(b)\u03c4+1)} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x(b)\u2217 \u2225 \u2225 \u2225 \u2225\n2 }\n\u2264 \u03b7\u03c4\u00b52\u03c4 [ E{F (x(b)\u03c4 )} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x(b)\u2217 \u2225 \u2225 \u2225 \u2225\n2 }\n+ \u03b70(\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c4 )\n[\n\u03d1 ( x (b) \u2217 ) \u2212 F\u2217 + \u01eb\u2032\n]\n,\nwhere \u01eb\u2032 := \u01eb\u03b7\u030c/\u03b70. Hence, by As3,\n0 \u2264 \u03b7\u03c4+1\u00b52\u03c4+1 [ E{F (x(b)\u03c4+1)} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n\u2264 \u03b7\u03c40\u00b52\u03c40 [ E{F (x(b)\u03c40 )} \u2212 F\u2217 ]\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c40\u03bb\u03c40\u00b5\u03c40 \u03b2\u03c40\n\u03b6\u03c40 + v\u03c40 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n+ \u03b70\u01eb \u2032\n\u03c4\u2211\n\u03c4 \u2032=\u03c4 \u20320\n( \u00b52\u03c4 \u2032+1 \u2212 \u00b52\u03c4 \u2032 ) ,\nand\n0 \u2264 [ E{F ( x (b) \u03c4+1 ) } \u2212 F\u2217 ]\n+ 1\n2\u03b7\u03c4+1\u00b52\u03c4+1 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1\n+ v\u03c4+1 \u2212 \u03bb1x(b)\u2217 \u2225 \u2225 \u2225 \u2225\n2 }\n\u2264 1 \u03b7\u03c4+1\u00b52\u03c4+1\n[\n\u03b7\u03c40\u00b5 2 \u03c40 ( E{F (x(b)\u03c40 )} \u2212 F\u2217 )\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c40\u03bb\u03c40\u00b5\u03c40 \u03b2\u03c40 \u03b6\u03c40 + v\u03c40 \u2212 \u03bb1x (b) \u2217\n\u2225 \u2225 \u2225 \u2225\n2 }]\n+ \u03b70\u01eb \u2032\u00b5 2 \u03c4+1 \u2212 \u00b52\u03c40 \u03b7\u03c4+1\u00b52\u03c4+1\n\u2264 4 \u03bb21\u03b7\u030c ( 1 + \u2211\u03c4+1 \u03c4 \u2032=1 \u03bb\u03c4 \u2032 )2\n[\n\u03b7\u03c40\u00b5 2 \u03c40 ( E{F (x(b)\u03c40 )} \u2212 F\u2217 ) (13a)\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c40\u03bb\u03c40\u00b5\u03c40 \u03b2\u03c40 \u03b6\u03c40 + v\u03c40 \u2212 \u03bb1x (b) \u2217\n\u2225 \u2225 \u2225 \u2225\n2 }]\n+ \u03b70 \u03b7\u030c \u01eb\u2032 ( 1\u2212 \u00b5 2 \u03c40\n\u00b52t+1\n)\n\u2264 4 \u03bb21\u03b7\u030c ( 1 + \u03bb\u030c(1 + \u03c4) )2\n[\n\u03b7\u03c40\u00b5 2 \u03c40 ( E{F (x(b)\u03c40 )} \u2212 F\u2217 )\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c40\u03bb\u03c40\u00b5\u03c40 \u03b2\u03c40 \u03b6\u03c40 + v\u03c40 \u2212 \u03bb1x (b) \u2217\n\u2225 \u2225 \u2225 \u2225\n2 }]\n+ \u03b70 \u03b7\u030c \u01eb\u2032\n\u2264 4 \u03bb21\u03bb\u030c 2\u03b7\u030c(1 + \u03c4)2\n[\n\u03b7\u03c40\u00b5 2 \u03c40 ( E{F (x(b)\u03c40 )} \u2212 F\u2217 )\n+ 1\n2 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c40\u03bb\u03c40\u00b5\u03c40 \u03b2\u03c40 \u03b6\u03c40 + v\u03c40 \u2212 \u03bb1x (b) \u2217\n\u2225 \u2225 \u2225 \u2225\n2 }]\n+ \u03b70 \u03b7\u030c \u01eb\u2032 , (13b)\nwhere Fact 1.2 was utilized in (13a). The previous inequality establishes the claim of Thm. 1.3.\n4) By (M3),\n1\u2212 \u221a\n1\u2212 \u03b7\u03c4\u03bb\u03c4L(b)\u03c4 L (b) \u03c4 \u2264 \u03b2\u03c4 \u2264 1 +\n\u221a\n1\u2212 \u03b7\u03c4\u03bb\u03c4L(b)\u03c4 L (b) \u03c4 . (14)\nMoreover, since the preceding discussion holds for any over-estimate L (b) \u03c4 of the underlying Lipschitz constants (cf. Remark 3), one can always set a sufficiently small L\u030c \u2208 R>0 as a lower-bound on all L(b)\u03c4 , i.e., L (b) \u03c4 \u2265 L\u030c, \u2200(\u03c4, i). Accordingly, the right hand side of (14) suggests that \u03b2\u03c4 \u2264 2/L\u030c. Given also that \u03b7\u03c4 \u2265 \u03b7\u030c, \u03bb\u03c4 \u2265 \u03bb\u030c, and As4, then there exists \u03b4 > 0 s.t. \u2212\u03b7\u03c4\u03bb\u03c4 (L(b)\u03c4 \u03b22\u03c4 \u2212 2\u03b2\u03c4 + \u03b7\u03c4\u03bb\u03c4 )/(2\u03b22\u03c4 ) \u2265 \u03b4, \u2200\u03c4 . As a result, (11) implies that for any x\u0304(b),\n\u00b52\u03c4+1\u03b4\u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162\n\u2264 \u2212 \u03b7\u03c4+1\u00b5 2 \u03c4+1\u03bb\u03c4+1\n2\u03b22\u03c4+1\n( L (b) \u03c4+1\u03b2 2 \u03c4+1 \u2212 2\u03b2\u03c4+1 + \u03b7\u03c4+1\u03bb\u03c4+1 )\n\u00d7 \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162\n\u2264 \u03b7\u03c4+1\u00b52\u03c4 [ F\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 \u03b7\u03c4+1\u00b52\u03c4+1 [ F\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1 )]\n\u2212 1 2 \u2225 \u2225 \u2225 \u2225 \u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1 \u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2 ,\nand\n\u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162\n\u2264\u03b7\u03c4+1\u00b5 2 \u03c4\n\u03b4\u00b52\u03c4+1\n[\nF\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2212 \u03b7\u03c4+1 \u03b4\n[\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2212 1 2\u03b4\u00b52\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\u03b4\u00b52\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n\u2264\u03b7\u03c4+1 \u03b4\n[\nF\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2212 \u03b7\u03c4+1 \u03b4\n[\nF\u03c4+1 ( x\u03c4+1 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u0304(b) | x(\u2212b)\u03c4+1\n)]\n\u2212 1 2\u03b4\u00b52\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\u03b4\u00b52\u03c4\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n= \u03b7\u03c4+1 \u03b4\n[\nF\u03c4+1 ( x\u03c4 | x(\u2212b)\u03c4+1 ) \u2212 F\u03c4+1 ( x\u03c4+1 | x(b)\u03c4+1\n)]\n\u2212 1 2\u03b4\u00b52\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\u03b4\u00b52\u03c4\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n\u2264\u03b7\u03c4+1 \u03b4\n[\nF\u03c4+1 ( x (b\u22121) \u03c4+1 ) \u2212 F\u03c4+1 ( x (b) \u03c4+1\n)]\n\u2212 1 2\u03b4\u00b52\u03c4+1\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2\n+ 1\n2\u03b4\u00b52\u03c4\n\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225 2 .\nApplying expectations to the previous inequality yields\nE { \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 } \u2264 \u03b70 \u03b4 [ E { F ( x (b\u22121) \u03c4+1 )} \u2212 E { F ( x (b) \u03c4+1 )}]\n\u2212 1 2\u03b4\u00b52\u03c4+1 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n+ 1\n2\u03b4\u00b52\u03c4 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n\u2264 \u03b70 \u03b4\n[\nE { F ( x(b)\u03c4 )} \u2212 E { F ( x (b) \u03c4+1\n)}]\n\u2212 1 2\u03b4\u00b52\u03c4+1 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4+1\u03bb\u03c4+1\u00b5\u03c4+1 \u03b2\u03c4+1\n\u03b6\u03c4+1 + v\u03c4+1 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n+ 1\n2\u03b4\u00b52\u03c4 E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u03bb\u03c4\u00b5\u03c4 \u03b2\u03c4\n\u03b6\u03c4 + v\u03c4 \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n.\nFor arbitrarily fixed (\u03c4\u030c , \u03c4\u0302) \u2208 Z2\u22650 s.t. \u03c4\u030c \u2264 \u03c4\u0302 , adding the previous inequality for \u03c4 \u2208 {\u03c4\u030c , \u03c4\u030c + 1, . . . , \u03c4\u0302} results in\n\u03c4\u0302\u2211\n\u03c4=\u03c4\u030c\nE\n{ \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 }\n\u2264 \u03b70 \u03b4\n[\nE { F ( x (b) \u03c4\u030c )} \u2212 E { F ( x (b) \u03c4\u0302+1\n)}]\n+ 1\n2\u03b4\u00b52\u03c4\u030c E\n{\u2225 \u2225 \u2225 \u2225\n\u03b7\u03c4\u030c\u03bb\u03c4\u030c\u00b5\u03c4\u030c \u03b2\u03c4\u030c\n\u03b6\u03c4\u030c + v\u03c4\u030c \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 \u2225\n2 }\n.\nHence, by applying lim\u03c4\u0302\u2192\u221e to the previous inequality,\n\u221e\u2211\n\u03c4=\u03c4\u030c\nE\n{ \u2016\u03b6\u03c4+1 \u2212 \u03c8\u03c4+1\u20162 }\n\u2264 \u03b70 \u03b4\n[\nE { F ( x (b) \u03c4\u030c )} \u2212 F\u2217\n]\n+\nE {\u2225 \u2225 \u2225 \u03b7\u03c4\u030c\u03bb\u03c4\u030c\u00b5\u03c4\u030c \u03b2\u03c4\u030c \u03b6\u03c4\u030c + v\u03c4\u030c \u2212 \u03bb1x\u0304(b) \u2225 \u2225 \u2225 2 }\n2\u03b4\u00b52\u03c4\u030c ."}, {"heading": "5) By the definition of Prox\u03b2\u03c4gb(h) as the (unique) minimizer Prox\u03b2\u03c4gb(h) = argmin\u03be[\u2016h \u2212 \u03be\u20162/2 +", "text": "\u03b2\u03c4gb(\u03be)], there exists g \u2032 b(Prox\u03b2\u03c4gb(h)) \u2208 \u2202gb(Prox\u03b2\u03c4gb(h)) s.t. Prox\u03b2\u03c4gb(h) \u2212 h + \u03b2\u03c4g\u2032b(Prox\u03b2\u03c4gb(h)) = 0. Following the notation of (M5), a rearrangement of the previous equality yields that there exists g\u2032b(\u03b6\u03c4 ) \u2208 \u2202gb(\u03b6\u03c4 ) s.t. \u03c8\u03c4 \u2212 \u03b6\u03c4 = \u03b2\u03c4 [\u2207bf\u03c4 ( \u03c8\u03c4 | x(\u2212b)\u03c4 ) + g\u2032b(\u03b6\u03c4 )] . (15)\nBy definition, f\u03c4 (\u00b7 | x(\u2212b)\u03c4 )\u2019s range is included in R; hence, the relative interior [44, p. 91] of its domain ri dom f\u03c4 (\u00b7 | x(\u2212b)\u03c4 ) = Mb. Consequently, the definition F\u03c4 = f\u03c4 + \u2211B b=1 gb, and the elementary ri dom[f\u03c4 (\u00b7 | x(\u2212b)\u03c4 )]\u2229 ri dom gb(\u00b7) = ri dom gb 6= \u2205, suggest that \u2202bF\u03c4 (\u00b7 | x(\u2212b)\u03c4 ) = \u2207bf\u03c4 (\u00b7 | x(\u2212b)\u03c4 )+\u2202gb(\u00b7) [44, Cor. 16.38.iv]. As a result, the existence of g\u2032b(\u03b6\u03c4 ) and (15) establish that for any (\u03c4, b), there exists F \u2032\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2208 \u2202bF\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ) s.t. \u03c8\u03c4 \u2212 \u03b6\u03c4 = \u03b2\u03c4 [F \u2032\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ) +\u2207bf\u03c4 (\u03c8\u03c4 | x(\u2212b)\u03c4 )\u2212\u2207bf\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 )]. Hence,\n\u2225 \u2225 \u2225F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )\u2225\u2225 \u2225 2\n\u2264 2 \u03b22\u03c4 \u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162\n+ 2 \u2225 \u2225 \u2225\u2207bf\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2212\u2207bf\u03c4 ( \u03c8\u03c4 | x(\u2212b)\u03c4 ) \u2225 \u2225 \u2225 2\n\u2264 2 \u03b22\u03c4 \u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162 + 2L2\u03c4\u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162 \u2264 4L2\u03c4\u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162 \u2264 4L\u0302\u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162 , (16)\nwhere (16) utilized As4 and the fact that due to (14), \u03b2\u03c4 \u2265 (1\u2212 (1\u2212\u03b7\u03c4\u03bb\u03c4L(b)\u03c4 )1/2)/L(b)\u03c4 \u2265 1/L(b)\u03c4 \u2265 1/L\u0302. Assuming that F \u2032\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2208 Hb and applying expectations to (16) yields E{\u2016F \u2032\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 )\u20162} \u2264 4L\u0302E{\u2016\u03c8\u03c4 \u2212 \u03b6\u03c4\u20162}, which together with Thm. 1.4 establish\nlim \u03c4\u2192\u221e E\n{\u2225 \u2225 \u2225F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2225 \u2225 \u2225 2 } = 0 . (17)\nFurther, due to the convexity of \u2016 \u00b7 \u20162, Jensen\u2019s inequality for conditional expectations [46, \u00a7 IV.3] suggests that\nE {\u2225 \u2225 \u2225F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2225 \u2225 \u2225 2 }\n= EX\n{\nEO|X {\u2225 \u2225 \u2225F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 ) \u2225 \u2225 \u2225 2 }}\n\u2265 EX {\u2225 \u2225 \u2225EO|X { F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} \u2225 \u2225 \u2225 2 } .\nAccordingly, the previous inequality together with (17) establish Thm. 1.5. 6) By (M6), \u2200\u03c4 \u2208 T(b)\u03b6 , where T (b) \u03b6 is defined in Thm. 1.2, F\u03c4 (x\u03c4 | x (\u2212b) \u03c4 ) \u2264 \u03bb\u03c4F\u03c4 (\u03b6\u03c4 | x(\u2212b)\u03c4 ) + (1 \u2212 \u03bb\u03c4 )F\u03c4 (x\u03c4\u22121 | x(\u2212b)\u03c4 ). Hence, \u2200\u03c4 \u2208 T(b)\u03b6 ,\nE { F ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} = E { F\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} \u2265 1 \u03bb\u03c4 E { F\u03c4 ( x\u03c4 | x(\u2212b)\u03c4 )} \u2212 1\u2212 \u03bb\u03c4 \u03bb\u03c4 E { F\u03c4 ( x\u03c4\u22121 | x(\u2212b)\u03c4 )} = 1\n\u03bb\u03c4 E { F ( x(b)\u03c4\n)} \u2212 1\u2212 \u03bb\u03c4\n\u03bb\u03c4 E { F ( x\u03c4\u22121 | x(\u2212b)\u03c4 )}\n\u2265 1 \u03bb\u03c4 E { F ( x(b)\u03c4 )} \u2212 1\u2212 \u03bb\u03c4 \u03bb\u03c4 E { F ( x(b\u22121)\u03c4 )} (18a) = 1\n\u03bb\u03c4\n[\nE { F ( x(b)\u03c4 )} \u2212 E { F ( x(b\u22121)\u03c4\n)}]\n+ E { F ( x(b\u22121)\u03c4 )}\n\u2265 1 \u03bb\u030c\n[\nE { F ( x(b)\u03c4 )} \u2212 E { F ( x(b\u22121)\u03c4\n)}]\n+ E { F ( x(b\u22121)\u03c4 )} (18b)\n\u2265 1 \u03bb\u030c\n[\nE { F ( x(b)\u03c4 )} \u2212 E { F ( x(b\u22121)\u03c4\n)}]\n+ F\u2217 , (18c)\nwhere \u03bb\u030c \u2264 \u03bb\u03c4 , \u2200\u03c4 , and the monotonicity properties E{F (x(b\u22121)\u03c4 )} \u2264 E{F (x\u03c4\u22121 | x(\u2212b)\u03c4 )}, as well as F\u2217 \u2264 E{F (x(b)\u03c4 )} \u2264 E{F (x(b\u22121)\u03c4 )}, established in Thm. 1.1, were used in (18a), (18b), and (18c). Since lim\u03c4\u2192\u221e[E{F (x(b)\u03c4 )} \u2212 E{F (x(b\u22121)\u03c4 )}] = 0 by Thm. 1.1, given any \u01eb \u2208 R>0, there exists \u03c4 \u20320 \u2208 Z\u22650 s.t. \u2200\u03c4 \u2208 T(b)\u03b6 \u2229 [\u03c4 \u20320,+\u221e),\nE{F (\u03b6\u03c4 | x(\u2212b)\u03c4 )} \u2265 \u2212 1\n\u03bb\u030c\n\u03bb\u030c\u01eb\n2 + F\u2217 = F\u2217 \u2212\n\u01eb 2 . (19)\nDue to the elementary E{\u2016\u03b6(b)\u03c4 \u2212 w(b)\u03c4 \u20162} \u2264 2E{\u2016\u03b6(b)\u03c4 \u20162} + 2E{\u2016w(b)\u03c4 \u20162}, Thm. 1.2 and As5 suggest that there exists \u2206\u0302 s.t. E{\u2016\u03b6(b)\u03c4 \u2212 w(b)\u03c4 \u20162} \u2264 \u2206\u03022.\nNow, according to Lemma 1, the subgradient inequality for function E{F (\u00b7 | x(\u2212b)\u03c4 )} yields \u2200\u03c4 ,\nE { F ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} = E { F\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )}\n\u2264 E { F\u03c4 ( w\u03c4 | x(\u2212b)\u03c4 )}\n+ E {\u2329\nEO|X { F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} \u2223 \u2223 \u2223 \u03b6\u03c4 \u2212 w\u03c4 \u232a}\n\u2264 E { F\u03c4 ( w\u03c4 | x(\u2212b)\u03c4 )}\n+ [ E { \u2016\u03b6\u03c4 \u2212 w\u03c4\u20162 }] 1\n2 \u00d7 [\nE {\u2225 \u2225 \u2225EO|X { F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )}\u2225\u2225 \u2225 2 }] 1 2\n\u2264 E { F\u03c4 ( w\u03c4 | x(\u2212b)\u03c4 )}\n+ \u2206\u0302\n[\nE {\u2225 \u2225 \u2225EO|X { F \u2032\u03c4 ( \u03b6\u03c4 | x(\u2212b)\u03c4 )} \u2225 \u2225 \u2225 2 }] 1 2 , (20)\nwhere \u2206\u0302 bounds ([E{\u2016\u03b6(b)\u03c4 \u2212 w(b)\u03c4 \u20162}]1/2)\u03c4\u2208Z>0 . Since (20) holds for any subgradient, it holds also for the specific EO|X{F \u2032\u03c4 (\u03b6\u03c4 | x (\u2212b) \u03c4 )} involved in Thm. 1.5. Moreover, by the definition of lim sup, \u2203\u03c4 \u2032\u20320 \u2208 Z\u22650 \u2229 [\u03c4 \u20320,+\u221e) s.t. \u2200\u03c4 \u2208 T (b) \u03b6 \u2229 [\u03c4 \u2032\u20320 ,+\u221e),\nE { F ( \u03b6\u03c4 | x(\u2212b)\u03c4 )}\n\u2264 lim sup \u03c4\u2192\u221e\nE { F ( w\u03c4 | x(\u2212b)\u03c4 )} + \u01eb\n4 + \u2206\u0302\n\u01eb\n4\u2206\u0302\n= lim sup \u03c4\u2192\u221e min x\u2208Hb\nE { F ( x | x(\u2212b)\u03c4 )}\n\ufe38 \ufe37\ufe37 \ufe38\n=: F\u0304b\n+ \u01eb\n2\n\u2264 lim sup \u03c4\u2192\u221e\nE { F ( x(b)\u03c4 | x(\u2212b)\u03c4 )} + \u01eb\n2 = F\u2217 +\n\u01eb 2 . (21)\nTo summarize, (19) and (21) suggest that \u2200\u03c4 \u2208 T(b)\u03b6 \u2229 [\u03c4 \u2032\u20320 ,+\u221e), F\u2217 \u2264 E{F (\u03b6\u03c4 | x (\u2212b) \u03c4 )}+ \u01eb/2 \u2264 F\u0304b+ \u01eb \u2264 F\u2217 + \u01eb, and since \u01eb was chosen arbitrarily, F\u2217 = F\u0304b.\nThe previous result together with the definition of lim sup suggest that there exists a subsequence\n(x (b) \u03c4k )k\u2208Z\u22650 s.t. limk\u2192\u221eminx\u2208Hb E{F (x | x (\u2212b) \u03c4k )} = F\u2217. In other words, given any arbitrarily small \u01eb \u2208 R>0, there exists k \u2032 0 s.t. \u2200k \u2265 k\u20320, |F\u2217\u2212minx\u2208Hb E{F (x | x (\u2212b) \u03c4k )}| \u2264 \u01eb/2. Moreover, Thm. 1.1 has already demonstrated that F\u2217 = lim\u03c4\u2192\u221e E{F (x(b)\u03c4 | x(\u2212b)\u03c4 )}; hence, F\u2217 = limk\u2192\u221e E{F (x(b)\u03c4k | x (\u2212b) \u03c4k )}, and there exists k\u2032\u20320 s.t. \u2200k \u2265 k\u2032\u20320 , |F\u2217\u2212E{F (x (b) \u03c4k | x (\u2212b) \u03c4k )}| \u2264 \u01eb/2. Putting everything together, by choosing any k0 \u2265 max{k\u20320, k\u2032\u20320}, it can be readily deduced that \u2200k \u2265 k0, E{F (x (b) \u03c4k | x (\u2212b) \u03c4k )}\u2212minx\u2208Hb E{F (x | x (\u2212b) \u03c4k )} \u2264 \u01eb.\nA.4 Proof of Lemma 3\nBy Thm. 1.2, consider x (b) \u2217 \u2208 W{(x(b)\u03c4 )\u03c4\u2208Z\u22650} 6= \u2205. In other words, there exists a subsequence (\u03c4k)k\u2208Z\u22650 s.t. w-limk\u2192\u221e x (b) \u03c4k = x (b) \u2217 , where w-lim denotes the limit in the weak topology of Hb [44, \u00a72.4].\nFix now arbitrarily \u01eb \u2208 R>0. By the definition of lim sup, there exists \u03c4 \u20320 \u2208 Z\u22650 s.t. \u2200\u03c4 \u2265 \u03c4 \u20320, lim sup\u03c4\u2192\u221e E{F ( x (b) \u2217 | x(\u2212b)\u03c4 )} \u2264 E{F (x(b)\u2217 | x(\u2212b)\u03c4 )}+\u01eb/2. By assumption, E{F (\u00b7 | x(\u2212b)\u03c4 )} is l.s.c. on Hb, and, consequently, it is also weakly sequentially l.s.c. [44, Thm. 9.1]. Hence, there exists a neighborhood V\u03c4 (x (b) \u2217 ) of x (b) \u2217 in the weak topology of Hb s.t. \u2200x \u2208 V\u03c4 (x(b)\u2217 ), E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 E{F (x | x(\u2212b)\u03c4 )}+\u01eb/2 [44, Def. 1.21].\n[As6a] There exists an open neighborhood V(x (b) \u2217 ) of x (b) \u2217 in the weak topology of Hb and a \u03c4# \u2208 Z\u22650\ns.t. V(x (b) \u2217 ) \u2282 \u2229+\u221e\u03c4=\u03c4# V\u03c4 (x (b) \u2217 ).\nSince w-limk\u2192\u221e x (b) \u03c4k = x (b) \u2217 , there exists k \u2032 0 \u2208 Z\u22650 s.t. \u2200k \u2265 k\u20320, x\u03c4k \u2208 V(x (b) \u2217 ). The previous arguments and As6a suggest that there exists a sufficiently large k\u2032\u20320 \u2208 Z\u22650 s.t. \u03c4k\u2032\u20320 \u2265 max{\u03c4#, \u03c4 \u2032 0}, k\u2032\u20320 \u2265 k\u20320, and consequently \u2200k \u2265 k\u2032\u20320 ,\nlim sup \u03c4\u2192\u221e\nE { F ( x (b) \u2217 | x(\u2212b)\u03c4 )}\n\u2264 E { F ( x (b) \u2217 | x(\u2212b)\u03c4k )} + \u01eb\n2\n\u2264 E { F ( x(b)\u03c4k | x (\u2212b) \u03c4k )} + \u01eb\n2 +\n\u01eb\n2\n= E { F ( x(b)\u03c4k )} + \u01eb.\nTherefore,\nlim sup \u03c4\u2192\u221e\nE { F ( x (b) \u2217 | x(\u2212b)\u03c4 )}\n= lim k\u2192\u221e lim sup \u03c4\u2192\u221e\nE { F ( x (b) \u2217 | x(\u2212b)\u03c4 )}\n\u2264 lim k\u2192\u221e\nE { F ( x(b)\u03c4k )} + \u01eb = F\u2217 + \u01eb .\nThe previous inequality holds for any \u01eb; hence lim sup\u03c4\u2192\u221e E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 F\u2217.\n[As6b] There exists a strong sequential cluster point x (b) \u2217 \u2208 C{(x(b)\u03c4 )\u03c4\u2208Z\u22650}, a \u2206\u0302 \u2208 R>0, and a sequence\nof subgradients (EO|X{F \u2032t(x(b)\u2217 | x(\u2212b)\u03c4 )})\u03c4\u2208Z>0 \u2282 Hb (cf. Lemma 1) s.t.\nlim sup \u03c4\u2192\u221e E\n{\u2225 \u2225 \u2225EO|X { F \u2032t(x (b) \u2217 | x(\u2212b)\u03c4 ) } \u2225 \u2225 \u2225 2 } \u2264 \u2206\u0302.\nDue to x (b) \u2217 \u2208 C{(x(b)\u03c4 )\u03c4\u2208Z\u22650}, there exists a subsequence (x (b) \u03c4k )k\u2208Z\u22650 s.t. limk\u2192\u221e[E{\u2016x (b) \u2217 \u2212x(b)\u03c4k \u20162}]1/2 =\n0. According to Lemma 1, the subgradient inequality for function E{F (\u00b7 | x(\u2212b)\u03c4 )} yields E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 E{F (x(b)\u03c4 | x(\u2212b)\u03c4 )}+ E{\u3008EO|X{F \u2032t (x(b)\u2217 | x(\u2212b)\u03c4 )} | x(b)\u2217 \u2212 x(b)\u03c4 \u3009}. Similarly to a previous discussion, by the definition of lim sup, \u2203\u03c4 \u20320 \u2208 Z\u22650 s.t. \u2200\u03c4 \u2265 \u03c4 \u20320, lim sup\u03c4\u2192\u221e E{F ( x (b) \u2217 | x(\u2212b)\u03c4 )} \u2264 E{F (x(b)\u2217 | x(\u2212b)\u03c4 )}+ \u01eb/2.\nHence, \u2200\u03c4 \u2265 \u03c4 \u20320,\nlim sup \u03c4\u2192\u221e\nE { F ( x (b) \u2217 | x(\u2212b)\u03c4 )}\n\u2264 E { F ( x (b) \u2217 | x(\u2212b)\u03c4 )} + \u01eb \u2264 E { F ( x(b)\u03c4 | x(\u2212b)\u03c4 ) }+ \u01eb\n+ E {\u2329 EO|X{F \u2032t (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2223 \u2223 \u2223 x (b) \u2217 \u2212 x(b)\u03c4 \u232a}\n\u2264 E { F ( x(b)\u03c4 ) }+ \u01eb\n+\n[\nE {\u2225 \u2225 \u2225x (b) \u2217 \u2212 x(b)\u03c4 \u2225 \u2225 \u2225 2 }] 1 2\n\u00d7 [\nE {\u2225 \u2225 \u2225EO|X { F \u2032t ( x (b) \u2217 | x(\u2212b)\u03c4 )} \u2225 \u2225 \u2225 2 }] 1 2\n\u2264 E { F ( x(b)\u03c4 ) }+\n\u221a\n\u2206\u0302\n[\nE {\u2225 \u2225 \u2225x (b) \u2217 \u2212 x(b)\u03c4 \u2225 \u2225 \u2225 2 }] 1 2 + \u01eb ,\nand \u2200k \u2265 k\u2032\u20320 , for a sufficiently large k\u2032\u20320 \u2208 Z>0,\nlim sup \u03c4\u2192\u221e\nE { F ( x (b) \u2217 | x(\u2212b)\u03c4 )}\n\u2264 E { F ( x(b)\u03c4k ) }+\n\u221a\n\u2206\u0302\n[\nE {\u2225 \u2225 \u2225x\n(b) \u2217 \u2212 x(b)\u03c4k\n\u2225 \u2225 \u2225\n2 }] 1 2\n+ \u01eb .\nBy applying limk\u2192\u221e to both sides of the previous inequality, it can be verified that lim sup\u03c4\u2192\u221e E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 F\u2217+\u01eb, and since \u01eb was chosen arbitrarily, lim sup\u03c4\u2192\u221e E{F (x(b)\u2217 | x(\u2212b)\u03c4 )} \u2264 F\u2217."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Dr. P. Forero of SPAWAR for providing the code of the method in [8]."}], "references": [{"title": "Dictionary learning", "author": ["I. To\u0161i\u0107", "P. Frossard"], "venue": "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 27\u201338, Mar. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Machine Learn. Research, vol. 11, pp. 19\u201360, Mar. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, Oct. 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 52\u201368, Mar. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009. 27", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction of partially observed dynamical processes over networks via dictionary learning", "author": ["P. Forero", "K. Rajawat", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 13, pp. 3305\u20133320, July 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Convergence of block coordinate decent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, pp. 475\u2013494, June 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Math. Program., Ser. B, vol. 117, pp. 387\u2013423, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated block-coordinate relaxation for regularized optimization", "author": ["S.J. Wright"], "venue": "SIAM J. Optim., vol. 22, no. 1, pp. 159\u2013186, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM J. Optim., vol. 22, no. 2, pp. 341\u2013362, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM J. Imaging, vol. 6, no. 3, pp. 1758\u2013 1789, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Math. Program., Ser. A, pp. 1\u201338, Dec. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479\u20132493, July 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity-cognizant total least-squares for perturbed compressive sampling", "author": ["H. Zhu", "G. Leus", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 5, pp. 2002\u20132016, May 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM J. Optim., vol. 23, no. 2, pp. 1126\u20131153, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic coordinate descent methods for regularized smooth and nonsmooth losses", "author": ["Q. Tao", "K. Kong", "D. Chu", "G. Wu"], "venue": "Lecture Notes in Computer Science, ser. Machine Learning and Knowledge Discovery in Databases, P. A. Flach, T. de Bie, and N. Cristianini, Eds., vol. 7523. Springer, 2012, pp. 537\u2013552.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Decentralized sparsity-regularized rank minimization: Algorithms and applications", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 11, pp. 5374\u20135388, Nov. 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inform. Theory, vol. 52, pp. 1289\u20131306, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling and optimization for big data analytics", "author": ["K. Slavakis", "G.B. Giannakis", "G. Mateos"], "venue": "IEEE Signal Process. Magaz., vol. 31, no. 5, pp. 18\u201331, Sept. 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic approximation vis-\u00e0-vis online learning for big data analytics", "author": ["K. Slavakis", "S.-J. Kim", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Signal Process. Magaz., vol. 31, no. 6, pp. 124\u2013129, Nov. 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications, 2nd ed", "author": ["H.J. Kushner", "G.G. Yin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "A method for solving the convex programming problem with convergence rate O(1/k)", "author": ["Y. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR, vol. 269, pp. 543\u2013547, 1983, in Russian.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1983}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107\u2013194, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "J. Machine Learn. Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization", "author": ["S. Ghadimi", "G. Lan", "H. Zhang"], "venue": "Aug. 2013, arXiv:1308.6594. 28", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic successive minimization method for nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks", "author": ["M. Razaviyayn", "M. Sanjabi", "Z.-Q. Luo"], "venue": "Jul. 2013, arXiv:1307.4457.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Sur l\u2019approximation par \u00e9l\u00e9ments finis et la r\u00e9solution par p\u00e9nalisationdualit\u00e9 d\u2019une classe de probl\u00e8mes de Dirichlet non lin\u00e9aires", "author": ["R. Glowinski", "A. Marrocco"], "venue": "Rev. Francaise d\u2019Aut. Inf. Rech. Oper., vol. 9, no. 2, pp. 41\u201376, 1975.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1975}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finiteelement approximations", "author": ["D. Gabay", "B. Mercier"], "venue": "Comp. Math. Appl., vol. 2, pp. 17\u201340, 1976.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1976}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I.D. Schizas", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 6, pp. 2365\u20132381, June 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance analysis of the consensus-based distributed LMS algorithm", "author": ["G. Mateos", "I.D. Schizas", "G.B. Giannakis"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2009, Dec. 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L.Q. Tran", "A. Gray"], "venue": "Proc. ICML, Atlanta, Georgia: USA, June 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "On the O(1/t) convergence rate of the Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Numerical Analysis, vol. 50, no. 2, pp. 700\u2013709, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "On the o(1/k) convergence and parallelization of the alternating direction method of multipliers", "author": ["W. Deng", "M.-J. Lai", "W. Yin"], "venue": "2013, arXiv:1312.3040.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "2012, arXiv:1208.3922.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Image Process., vol. 18, pp. 2419\u20132439, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize", "author": ["M. Yamagishi", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 10, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Acceleration of adaptive proximal forward-backward splitting method and its application to sparse system identification", "author": ["M. Yamagishi", "M. Yukawa", "I. Yamada"], "venue": "Proc. ICASSP, Prague: Czech Republic, May 22\u201327 2011, pp. 4296\u20134299.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["C. Hu", "J.T. Kwok", "W. Pan"], "venue": "Proc. NIPS, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "Oct. 2013, arXiv:1310.3787.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Online dictionary learning from big data using accelerated stochastic approximation algorithms", "author": ["K. Slavakis", "G.B. Giannakis"], "venue": "Proc. ICASSP, Florence: Italy, May 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": "New York: Springer,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer-Verlag, 2011, pp. 185\u2013212.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Mathematical Foundations of the Calculus of Probability", "author": ["J. Neveu"], "venue": "San Francisco: Holden-Day,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1965}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditski", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim., vol. 19, no. 4, pp. 1574\u20131609, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 110, "endOffset": 115}, {"referenceID": 1, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 110, "endOffset": 115}, {"referenceID": 2, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 254, "endOffset": 257}, {"referenceID": 0, "context": ",dQ], Q \u2265 M , times an unknown sparse coefficient vector st [2, 3].", "startOffset": 60, "endOffset": 66}, {"referenceID": 1, "context": ",dQ], Q \u2265 M , times an unknown sparse coefficient vector st [2, 3].", "startOffset": 60, "endOffset": 66}, {"referenceID": 5, "context": "Sparsity on the other hand, renders DL representations identifiable even when yt has missing entries [8], due to e.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 6, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 7, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 8, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 9, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 10, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 11, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 12, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 13, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 14, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 15, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 16, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 1, "context": ", st\u22121],Dt\u22121) as follows [3] st \u2208 argmins Ft ( [St\u22121, s],Dt\u22121;Ot ) (2a) Dt \u2208 argminD Ft(St,D;Ot) .", "startOffset": 25, "endOffset": 28}, {"referenceID": 17, "context": "Given Ot, each step in (2) is a convex optimization task: Basis pursuit [20] in (2a), and constrained leastsquares (LS) in (2b).", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].", "startOffset": 174, "endOffset": 181}, {"referenceID": 19, "context": "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].", "startOffset": 174, "endOffset": 181}, {"referenceID": 20, "context": "Further, as data are streaming, analytics must often be performed in real time, without a chance to revisit past entries \u2013 a feature common to stochastic approximation (SA) setups [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": ") the number of unknowns; and v) iterations converge quadratically to a solution of (3), which is optimal among first-order methods in the sense of [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 22, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 24, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 25, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 26, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 27, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 28, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 29, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 30, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 31, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 32, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 33, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 21, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 34, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 35, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 36, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 37, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 38, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 148, "endOffset": 152}, {"referenceID": 39, "context": "Even though [42] deals with non-convex costs, it requires bounds on the (primal) variables, knowledge of a", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "Our work markedly broadens the offline acceleration technique introduced for convex costs in [39], to per-block-convex and to online SA setups.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "Numerical tests corroborate our analytical claims, and demonstrate that under a linear computational complexity footprint the proposed algorithm outperforms BCDMs and the computationally heavier ADMM-based alternatives [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 40, "context": "Preliminary results were presented in [43], and outlined in [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Preliminary results were presented in [43], and outlined in [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "A first-order algorithm for the off-line minimization of a convex cost \u03c6(x) := f(x) + g(x), x \u2208 M, was studied in [39] (presented for convenience in Table 1), where M is a linear vector space; f is convex as well as L-Lipschitz continuously differentiable; and g is convex but possibly non-smooth, e.", "startOffset": 114, "endOffset": 118}, {"referenceID": 41, "context": "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Prox\u03b2ig : M \u2192 M : x 7\u2192 argmin\u03be\u2208M\u2016x \u2212 \u03be\u2016/2 + \u03b2ig(\u03be) for any \u03b2i \u2208 R>0 [44].", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Prox\u03b2ig : M \u2192 M : x 7\u2192 argmin\u03be\u2208M\u2016x \u2212 \u03be\u2016/2 + \u03b2ig(\u03be) for any \u03b2i \u2208 R>0 [44].", "startOffset": 211, "endOffset": 215}, {"referenceID": 42, "context": ", Prox\u2016\u00b7\u20161 boils down to the soft-thresholding operator [45].", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "If the FB iteration were performed with \u03c8i+1 taking the place of \u03b6i in line 5, then (\u03c8i)i\u2208Z\u22650 would converge to a minimizer of \u03c6 [44], but with no claims on quadratic rate of convergence.", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "Parameters {\u03b7i+1, \u03bbi+1} in line 2 are used to define stepsize \u03b2i+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1\u2212 \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L, (1+ \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L] per iteration, as opposed to the rigid \u03b2i+1 = 1/L in [37, 38].", "startOffset": 249, "endOffset": 257}, {"referenceID": 35, "context": "Parameters {\u03b7i+1, \u03bbi+1} in line 2 are used to define stepsize \u03b2i+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1\u2212 \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L, (1+ \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L] per iteration, as opposed to the rigid \u03b2i+1 = 1/L in [37, 38].", "startOffset": 249, "endOffset": 257}, {"referenceID": 36, "context": "Table 1: Minimizing the convex cost \u03c6 := f + g [39] Require: \u03bb\u030c, \u03b7\u030c \u2208 R>0; \u03bc1 := \u03bb1 \u2208 [\u03bb\u030c, 1]; Lipschitz coeff.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "Under proper parameter selection (\u03b7i = \u03b2i = 1/L, \u03bbi = 1), the algorithm of Table 1 boils down to [38].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "Moreover, with its guaranteed monotonically non-increasing behavior of cost values through line 6, and the flexibility offered by the variable step-sizes (\u03b2i)i\u2208Z>0 in line 3, the algorithm in Table 1 has merits over [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 34, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 25, "endOffset": 33}, {"referenceID": 35, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 25, "endOffset": 33}, {"referenceID": 36, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": ") functions defined on Mb with values in R\u222a{+\u221e} [44].", "startOffset": 48, "endOffset": 52}, {"referenceID": 41, "context": "Mb} [44].", "startOffset": 4, "endOffset": 8}, {"referenceID": 43, "context": "the \u03c3-algebra A [46].", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "X [46].", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "O, conditioned on X [46].", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": ", Rb} in the context of Table 1, and not infinitely often (i \u2192 +\u221e) as in the batch and off-line mode of [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": ") If limk\u2192\u221e E{\u2016\u03bek\u2016} = +\u221e for any (\u03bek)k\u2208Z\u22650 \u2282 H, then limk\u2192\u221e E{F (\u03bek)} = +\u221e [44].", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "As2 will be used to prevent the proposed algorithm from generating unbounded sequences of estimates, without any a-priori enforcement of hard bounds on the variables, as in [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 44, "context": "More specifically, As6b assumes existence of a strong sequential cluster point and bounds a sequence of subgradients of the expected cost, similarly to the bound on gradients introduced in [47].", "startOffset": 189, "endOffset": 193}, {"referenceID": 13, "context": "Motivated by the TLS criterion and the resultant errors-in-variables (EIV) modeling approach [7, 16], the following sequence of per-block-convex costs is considered:", "startOffset": 93, "endOffset": 100}, {"referenceID": 5, "context": "Following [8], consider an undirected graph G(V, E), where V denotes the set of all vertices or nodes, with cardinality V, and E is the set of all edges.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "where \u2206t := \u2211t \u03c4=1 \u03b4 t\u2212\u03c4 ; \u2016s\u2016 and \u2016s\u20161 are as in (5), while the term including L quantifies prior knowledge on the topology of G, promotes \u201csmooth\u201d solutions over strongly connected nodes of G, and is instrumental in imputing missing entries [8].", "startOffset": 243, "endOffset": 246}, {"referenceID": 43, "context": "With regard to the selection of L\u0302 in As4, recall that Markov\u2019s inequality dictates that Pr(L (b) \u03c4 \u2265 L\u0302) \u2264 E{L \u03c4 }/L\u0302, for any L\u0302 [46].", "startOffset": 131, "endOffset": 135}, {"referenceID": 22, "context": "The algorithm in Table 2 is tested against a block-version of the classical online (sub)gradient descent method [25], tagged as BOGD in Fig.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 149, "endOffset": 156}, {"referenceID": 27, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 149, "endOffset": 156}, {"referenceID": 5, "context": "It is worth noticing here that ADMM in [8] requires multiple iterations to achieve a prescribed estimation accuracy, and that no matrix inversion was incorporated in the realization of Table 2.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The proposed method and [8] perform similarly, scoring mean (normalized) estimation errors of 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 36, "context": "3) The following proof is based on the one developed in [39] for the off-line, convex analytic case.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "Forero of SPAWAR for providing the code of the method in [8].", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (perblock) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}