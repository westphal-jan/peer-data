{"id": "1502.03536", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Speeding up Permutation Testing in Neuroimaging", "abstract": "In order to correct these phenomena, we need a reliable estimate of the Family-Wise Error Rate (FWER). While the well-known Bonferroni correction method is easy to use, it is relatively conservative and can significantly overwhelm a study because it ignores the dependencies between test statistics. Permutation tests, on the other hand, are an exact, non-parametric method of estimating the FWER for a certain $\\ alpha $threshold, but for acceptably low thresholds, the computing load can be prohibitive. In this paper, we show that permutation tests actually amount to populating the columns of a very large matrix ${bf P} $. By analyzing the spectrum of this matrix, we see that ${bf P} $has a low acceleration plus a small variance of the residual decomposition, which represents a high-grade attenuation of the generated neuromatrix, while achieving a high attenuation of 0.5%.", "histories": [["v1", "Thu, 12 Feb 2015 04:30:06 GMT  (534kb,D)", "http://arxiv.org/abs/1502.03536v1", "NIPS 13"]], "COMMENTS": "NIPS 13", "reviews": [], "SUBJECTS": "stat.CO cs.AI stat.ML", "authors": ["chris hinrichs", "vamsi k ithapu", "qinyuan sun", "sterling c johnson", "vikas singh"], "accepted": true, "id": "1502.03536"}, "pdf": {"name": "1502.03536.pdf", "metadata": {"source": "CRF", "title": "Speeding up Permutation Testing in Neuroimaging", "authors": ["Chris Hinrichs", "Vamsi K. Ithapu", "Qinyuan Sun", "Sterling C. Johnson", "Vikas Singh"], "emails": ["hinrichs@cs.wisc.edu", "vamsi@cs.wisc.edu", "qsun28@wisc.edu", "scj@medicine.wisc.edu", "vsingh@biostat.wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "Suppose we have completed a placebo-controlled clinical trial of a promising new drug for a neurodegenerative disorder such as Alzheimer\u2019s disease (AD) on a small sized cohort. The study is designed such that in addition to assessing improvements in standard cognitive outcomes (e.g., MMSE), the purported treatment effects will also be assessed using Neuroimaging data. The rationale here is that, even if the drug does induce variations in cognitive symptoms, the brain changes are observable much earlier in the imaging data. On the imaging front, this analysis checks for statistically significant differences between brain images of subjects assigned to the two trial arms: treatment and placebo. Alternatively, consider a second scenario where we have completed a neuroimaging research study of a particular controlled factor, such as genotype, and the interest is to evaluate group-wise differences in the brain images: to identify which regions are affected as a function of class membership. In either cases, the standard image processing workflow yields for each subject a 3-D image (or voxel-wise \u201cmap\u201d). Depending on the image modality acquired, these maps are of cerebral gray matter density, longitudinal deformation (local growth or contraction) or metabolism. It is assumed that these maps have been \u2018co-registered\u2019 across different subjects so that each voxel corresponds to approximately the same anatomical location. [1, 2].\nIn order to localize the effect under investigation (i.e., treatment or genotype), we then have to calculate a very large number (say, v) of univariate voxel-wise statistics \u2013 typically up to several\n\u2217Hinrichs and Ithapu are joint first authors and contributed equally to this work.\nar X\niv :1\n50 2.\n03 53\n6v 1\n[ st\nat .C\nO ]\n1 2\nFe b\nmillion voxels. For example, consider group-contrast t-statistics (here we will mainly consider tstatistics, however other test statistics are also applicable, such as the F statistic used in ANOVA testing, Pearson\u2019s correlation as used in functional imaging studies, or the \u03c72 test of dependence between variates, so long as certain conditions described in Section 2.3 are satisfied). In some voxels, it may turn out that a group-level effect has been indicated, but it is not clear right away what its true significance level should be, if any. As one might expect, given the number of hypotheses tests v, multiple testing issues in this setting are quite severe, making it difficult to assess the true FamilyWise Type I Error Rate (FWER) [3]. If we were to address this issue via Bonferroni correction [4], the enormous number of separate tests implies that certain weaker signals will almost certainly never be detected, even if they are real. This directly affects studies of neurodegenerative disorders in which atrophy proceeds at a very slow rate and the therapeutic effects of a drug is likely to be mild to moderate anyway. This is a critical bottleneck which makes localizing real, albeit slight, shortterm treatment effects problematic. Already, this restriction will prevent us from using a smaller sized study (fewer subjects), increasing the cost of pharmaceutical research. In the worst case, an otherwise real treatment effect of a drug may not survive correction, and the trial may be deemed a failure.\nBonferroni versus true FWER threshold. Observe that theoretically, there is a case in which the Bonferroni corrected threshold is close to the true FWER threshold: when point-wise statistics are i.i.d. If so, then the extremely low Bonferroni corrected \u03b1-threshold crossings effectively become mutually exclusive, which makes the Union Bound (on which Bonferroni correction is based) nearly tight. However, when variables are highly dependent \u2013 and indeed even without smoothing there are many sources of strong non-Gaussian dependencies between voxels, the true FWER threshold can be much more relaxed, and it is precisely this phenomenon which drives the search for alternatives to Bonferroni correction. Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].\nPermutation testing. A commonly used method of directly and non-parametrically estimating the FWER is Permutation testing [12, 13], which is a method of sampling from the Global (i.e., FamilyWise) Null distribution. Permutation testing ensures that any relevant dependencies present in the data carry through to the test statistics, giving an unbiased estimator of the FWER. If we want to choose a threshold sufficient to exclude all spurious results with probability 1\u2212\u03b1, we can construct a histogram of sample maxima taken from permutation samples, and choose a threshold giving the 1 \u2212 \u03b1/2 quantile. Unfortunately, reliable FWER estimates derived via permutation testing come at excessive (and often infeasible) computational cost \u2013 often tens of thousands or even millions of permutation samples are required, each of which requires a complete pass over the entire data set. This step alone can run from a few days up to many weeks and even longer [14, 15].\nObserve that the very same dependencies between voxels, that forced the usage of permutation testing, indicate that the overwhelming majority of work in computing so many highly correlated Null statistics is redundant. Note that regardless of their description, strong dependencies of almost any kind will tend to concentrate most of their co-variation into a low-rank subspace, leaving a high-rank, low-variance residual [5]. In fact, for Genome wide Association studies (GWAS), many strategies calculate the \u2018effective number\u2019 (Meff ) of independent tests corresponding to the rank of this subspace [16, 5]. This paper is based on the observation that such a low-rank structure must also appear in permutation test samples. Using ideas from online low-rank matrix completion [17] we can sample a few of the Null statistics and reconstruct the remainder as long as we properly account for the residual. This allows us to sub-sample at extremely low rates, generally< 1%. The contribution of our work is to significantly speed up permutation testing in neuroimaging, delivering running time improvements of up to 50\u00d7. In other words, our algorithm does the same job as permutation testing, but takes anywhere from a few minutes up to a few hours, rather than days or weeks. Further, based on recent work in random matrix theory, we provide an analysis which sheds additional light on the use of matrix completion methods in this context. To ensure that our conclusions are not an artifact of a specific dataset, we present strong empirical evidence via evaluations on four separate neuroimaging datasets of Alzheimer\u2019s disease (AD) and Mild Cognitive Impairment (MCI) patients as well as cognitively healthy age-matched controls (CN), showing that the proposed method can recover highly faithful Global Null distributions, while offering substantial speedups."}, {"heading": "2 The Proposed Algorithm", "text": "We first cover some basic concepts underlying permutation testing and low rank matrix completion in more detail, before presenting our algorithm and the associated analysis."}, {"heading": "2.1 Permutation testing", "text": "Randomly sampled permutation testing [18] is a methodology for drawing samples under the Global (Family-Wise) Null hypothesis. Recall that although point-wise test statistics have well characterized univariate Null distributions, the sample maximum usually has no analytic form due to the strong correlations across voxels. Permutation is particularly desirable in this setting because it is free of any distribution assumption whatsoever [12]. The basic idea of permutation testing is very simple, yet extremely powerful. Suppose we have a set of labeled high dimensional data points, and a univariate test statistic which measures some interaction between labeled groups for every dimension (or feature). If we randomly permute the labels and recalculate each test statistic, then by construction we get a sample from the Global Null distribution. The maximum over all of these statistics for every permutation sample is then used to construct a histogram, which therefore is a non-parametric estimate of the distribution of the sample maximum of Null statistics. For a test statistic derived from the real labels, the FWER corrected p-value is then equal to the fraction of permutation samples which were more extreme. Note that all of the permutation samples can be assembled into a matrix P \u2208 Rv\u00d7T where v is the number of comparisons (voxels for images), and T is the number of permutation samples.\nThere is a drawback to this approach, however. Observe that it is in the nature of random sampling methods that we get many samples from near the mode(s) of the distribution of interest, but fewer from the tails. Hence, to characterize the threshold for a small portion of the tail of this distribution, we must draw a very large number of samples just so that the estimate converges. Thus, if we want an \u03b1 = 0.01 threshold from the Null sample maximum distribution, we require many thousands of permutation samples \u2014 each requires randomizing the labels and recalculating all test statistics, a very computationally expensive procedure when v is large. To be certain, we would like to ensure an especially low FWER by first setting \u03b1 very low, and then getting a very precise estimate of the corresponding threshold. The smallest possible p-value we can derive this way is 1/T , so for very low p-values, T must be very large."}, {"heading": "2.2 Low-rank Matrix completion", "text": "Low-rank matrix completion [19] seeks to reconstruct missing entries from a matrix, given only a small fraction of its entries. The problem is ill-posed unless we assume this matrix has a low-rank column space. If so, then a much smaller number of observations, on the order of r log(v), where r is the column space\u2019s rank, and v is its ambient dimension [19] is sufficient to recover both an orthogonal basis for the row space as well as the expansion coefficients for each column, giving the recovery. By placing an `1-norm penalty on the eigenvalues of the recovered matrix via the nuclear norm [20, 21] we can ensure that the solution is as low rank as possible. Alternatively, we can specify a rank r ahead of time, and estimate an orthogonal basis of that rank by following a gradient along the Grassmannian manifold [22, 17]. Denoting the set of randomly subsampled entries as \u2126, the matrix completion problem is given as,\nmin P\u0303 \u2016P\u2126 \u2212 P\u0303\u2126\u20162F s.t. P\u0303 = UW; U is orthogonal (1)\nwhere U \u2208 Rv\u00d7r is the low-rank basis of P, \u2126 gives the measured entries, and W is the set of expansion coefficients which reconstructs P\u0303 in U. Two recent methods operate in an online setting, i.e., where rows of P arrive one at a time, and both U and W are updated accordingly [22, 17]."}, {"heading": "2.3 Low rank plus a long tail", "text": "Real-world data often have a dominant low-rank component. While the data may not be exactly characterized by a low-rank basis, the residual will not significantly alter the eigen-spectrum of the sample covariance in such cases. Having strong correlations is nearly synonymous with having a\nskewed eigen-spectrum, because the flatter the eigen-spectrum becomes, the sparser the resulting covariance matrix tends to be (the \u201cuncertainty principle\u201d between low-rank and sparse matrices [23]). This low-rank structure carries through for purely linear statistics (such as sample means). However, non-linearities in the test statistic calculation, e.g., normalizing by pooled variances, will contribute a long tail of eigenvalues, and so we require that this long tail will either decay rapidly, or that it does not overlap with the dominant eigenvalues. For t-statistics, the pooled variances are unlikely to change very much from one permutation sample to another (barring outliers) \u2014 hence we expect that the spectrum of P will resemble that of the data covariance, with the addition of a long, exponentially decaying tail. More generally, if the non-linearity does not de-correlate the test statistics too much, it will preserve the low-rank structure.\nIf this long tail is indeed dominated by the low-rank structure, then its contribution to P can be modeled as a low variance Gaussian i.i.d. residual. A Central Limit argument appeals to the number of independent eigenfunctions that contribute to this residual, and, the orthogonality of eigenfunctions implies that as more of them meaningfully contribute to each entry in the residual, the more independent those entries become. In other words, if this long tail begins at a low magnitude and decays slowly, then we can treat it as a Gaussian i.i.d. residual; and if it decays rapidly, then the residual will perhaps be less Gaussian, but also more negligible. Thus, our development in the next section makes no direct assumption about these eigenvalues themselves, but rather that the residual corresponds to a low-variance i.i.d. Gaussian random matrix \u2014 its contribution to the covariance of test statistics will be Wishart distributed, and from that we can characterize its eigenvalues."}, {"heading": "2.4 Our Method", "text": "It still remains to model the residual numerically. By sub-sampling we can reconstruct the low-rank portion of P via matrix completion, but in order to obtain the desired sample maximum distribution we must also recover the residual. Exact recovery of the residual is essentially impossible; fortunately, for our purposes we need only need its effect on the distribution of the maximum per permutation test. So, we estimate its variance, (its mean is zero by assumption,) and then randomly sample from that distribution to recover the unobserved remainder of the matrix.\nA large component in the running time of online subspace tracking algorithms is spent in updating the basis set U; yet, once a good estimate for U has been found this becomes superfluous. We therefore divide the entire process into two steps: training, and recovery. During the training phase we conduct a small number of fully sampled permutation tests (100 permutations in our experiments). From these permutation tests, we estimate U using sub-sampled matrix completion methods [22, 17], making multiple passes over the training set (with fixed sub-sampling rate), until convergence. In our evaluations, three passes sufficed. Then, we obtain a distribution of the residual S over the entire training set. Next is the recovery phase, in which we sub-sample a small fraction of the entries of each successive column t, solve for the reconstruction coefficients W(\u00b7, t) in the basis U by least-squares, and then add random residuals using parameters estimated during training. After that, we proceed exactly as in a normal permutation testing, to recover the statistics.\nBias-Variance tradeoff. By using a very sparse subsampling method, there is a bias-variance dilemma in estimating S. That is, if we use the entire matrix P to estimate U, W and S, we will obtain reliable estimates of S. But, there is an overfitting problem: the least-squares objective used in fitting W(\u00b7, t) to such a small sample of entries is likely to grossly underestimate the variance of S compared to where we use the entire matrix; (the sub-sampling problem is not nearly as over-constrained as for the whole matrix). This sampling artifact reduces the apparent variance of S, and induces a bias in the distribution of the sample maximum, because extreme values are found less frequently. This sampling artifact has the effect of \u2018shifting\u2019 the distribution of the sample maximum towards 0. We correct for this bias by estimating the amount of the shift during the training phase, and then shifting the recovered sample max distribution by this estimated amount."}, {"heading": "3 Analysis", "text": "We now discuss two results which show that as long as the variance of the residual is below a certain level, we can recover the distribution of the sample maximum. Recall from (1) that for low-rank matrix completion methods to be applied we must assume that the permutation matrix P can be\ndecomposed into a low-rank component plus a high-rank residual matrix S: P = UW + S, (2)\nwhere U is a v \u00d7 r orthogonal matrix that spans the r min(v, t) -dimensional column subspace of P, and W is the corresponding coefficient matrix. We can then treat the residual S as a random matrix whose entries are i.i.d. zero-mean Gaussian with variance \u03c32. We arrive at our first result by analyzing how the low-rank portion of P\u2019s singular spectrum interlaces with the contribution coming from the residual by treating P as a low-rank perturbation of a random matrix. If this low-rank perturbation is sufficient to dominate the eigenvalues of the random matrix, then P can be recovered with high fidelity at a low sampling rate [22, 17]. Consequently, we can estimate the distribution of the maximum as well, as shown by our second result.\nThe following development relies on the observation that the eigenvalues of PPT are the squared singular values of P. Thus, rather than analyzing the singular value spectrum of P directly, we can analyze the eigenvalues of PPT using a recent result from [24]. This is important because in order to ensure recovery of P, we require that its singular value spectrum will approximately retain the shape of UW\u2019s. More precisely, we require that for some 0 < \u03b4 < 1,\n|\u03c6\u0303i \u2212 \u03c6i| < \u03b4\u03c6i i = 1, . . . , r; \u03c6\u0303i < \u03b4\u03c6r i = r + 1, . . . , v (3) where \u03c6i and \u03c6\u0303i are the singular values of UW and P respectively. (Recall that in this analysis P is the perturbation of UW.) Thm. 3.1 relates the rate at which eigenvalues are perturbed, \u03b4, to the parameterization of S in terms of \u03c32. The theorem\u2019s principal assumption also relates \u03c32 inversely with the number of columns of P, which is just the number of trials t. Note however that the process may be split up between several matrices Pi, and the results can then be combined. For purposes of applying this result in practice we may then choose a number of columns t which gives the best bound. Theorem 3.1 also assumes that the number of trials t is greater than the number of voxels v, which is a difficult regime to explore empirically. Thus, our numerical evaluations cover the case where t < v, while Thm 3.1 covers the case where t is larger.\nFrom the definition of P in (2), we have, PPT = UWWTUT + SST + UWST + SWTUT . (4)\nWe first analyze the change in eigenvalue structure of SST when perturbed by UWWTUT , (which has r non-zero eigenvalues). The influence of the cross-terms (UWST and SWTUT ) is addressed later. Thus, we have the following theorem.\nTheorem 3.1. Denote that r non-zero eigenvalues of Q = UWWTUT \u2208 Rv\u00d7v by \u03bb1 \u2265 \u03bb2 \u2265 , . . . , \u03bbr > 0; and let S be a v \u00d7 t random matrix such that Si,j \u223c N (0, \u03c32), with unknown \u03c32. As v, t\u2192\u221e such that vt 1, the eigenvalues \u03bb\u0303i of the perturbed matrix Q + SS T will satisfy\n|\u03bb\u0303i \u2212 \u03bbi| < \u03b4\u03bbi i = 1, . . . , r; \u03bb\u0303i < \u03b4\u03bbr i = r + 1, . . . , v (?) for some 0 < \u03b4 < 1, whenever \u03c32 < \u03b4\u03bbrt\nProof. The first half of the proof emulates Theorem 2.1 from [24]. Consider the matrix X = \u221a tS. By the structure of S, each entry of X is i.i.d.Gaussian with zero\u2013mean and variance \u03c32t. Let Y = 1tXX\nT and denote its ordered eigenvalues as \u03b3i, i = 1, . . . , v (large to small). Consider the random spectral measure\n\u00b5v(A) = 1 v#{\u03b3i \u2208 A}, A \u2282 R\nThe Marchenko\u2013Pastur law [?] states that as v, t \u2192 \u221e such that vt \u2264 1, the random measure \u00b5v \u2192 \u00b5, where d\u00b5 is given by\nd\u00b5(a) = 12\u03c0\u03c32t\u03b3a \u221a (\u03b3+ \u2212 a)(a\u2212 \u03b3\u2212)1[\u03b3\u2212,\u03b3+]da\nwhere \u03b3 = vt . Here 1[\u03b3\u2212,\u03b3+] is an indicator function that is non\u2013zero on [\u03b3\u2212, \u03b3+]. \u03b3\u00b1 = \u03c3 2t(1 \u00b1\u221a\n\u03b3)2 are the extreme points of the support of \u00b5. It is well known that the extreme eigenvalues converge almost surely to \u03b3\u00b1 [?]. Since v, t \u2192 \u221e and \u03b3 = vt 1, the length of [\u03b3\u2212, \u03b3+] is much smaller than the values in it. Hence we have,\n\u03b3\u00b1 \u223c \u03c32t(1\u00b1 2 \u221a \u03b3) ; \u221a (\u03b3+ \u2212 a)(a\u2212 \u03b3\u2212) a\nand the new d\u00b5(a) is given by\nd\u00b5(a) =\n\u221a (\u03c32t(1 + 2 \u221a \u03b3)\u2212 a)(a\u2212 \u03c32t(1\u2212 2\u221a\u03b3))\n2\u03c0\u03b3\u03c34t2 1[\u03c32t(1\u22122\u221a\u03b3),\u03c32t(1+2\u221a\u03b3)]da\n= 1\n2\u03c0\u03b3\u03c34t2\n\u221a 4\u03b3\u03c34t2 \u2212 (a\u2212 \u03c32t)21[\u03c32t(1\u22122\u221a\u03b3),\u03c32t(1+2\u221a\u03b3)]da\nThe form we have derived for d\u00b5(a) shares some similarities with d\u00b5X(x) in Section 3.1 of [24]. The analysis in [24] takes into account the phase transition of extreme eigen values. This is done by imitating a time\u2013frequency type analysis on compact support of extreme spectral measure i.e. using Cauchy transform. For our case, the Cauchy transform of \u00b5(a) is\nG\u00b5(z) = 1\n2\u03b3\u03c34t2\n( z \u2212 \u03c32t\u2212 sgn(z) \u221a (z \u2212 \u03c32t)2 \u2212 4\u03b3\u03c34t2 ) for z \u2208 (\u221e, \u03c32t(1\u2212 2\u221a\u03b3)) \u222a (\u03c32t(1 + 2\u221a\u03b3),\u221e)\nSince we are interested in the asymptotic eigen values (and \u03b3 1), G\u00b5(\u03b3\u00b1) and the functional inverse G\u22121\u00b5 (\u03b8) are\nG\u00b5(\u03b3+) = 1 \u03c32t \u221a (\u03b3) ; G\u00b5(\u03b3\u2212) = \u2212 1 \u03c32t \u221a (\u03b3) ; G\u22121\u00b5 (\u03b8) = \u03c3 2t+ 1\u03b8 + \u03b3\u03c3 4t2\u03b8\nHence, the asymptotic behavior of the eigen values of perturbed matrix Q + SST is (observing that SST = Y and Q has r non\u2013zero positive eigen values)\n\u03bb\u0303i(i = 1, . . . , r) \u2248\n{ \u03bbi + \u03c3 2t+ \u03b3\u03c3 4t2\n\u03bbi for \u03bbi > \u03b3\u03c32t\n\u03b3\u03c32t else (\u2217)\n\u03bb\u0303i(i = r + 1, . . . , v) \u2248 \u03c32t(1\u2212 2 \u221a (\u03b3))\nWith \u03bb\u0303i, i = 1, . . . , v in hand, we now bound the unknown variance \u03c32 such that (?) is satisfied. We only have two cases to consider,\n(1) \u03bbi > \u03b3\u03c32t , i = 1, . . . , r (2) \u03bbi \u2264 \u03b3\u03c32t , i = k, . . . , r (for some k \u2265 1)\nWe constrain the unknown \u03c32 such that case (2) does not arise. Substituting for \u03bb\u0303i\u2018s from (\u2217) in (?), we get,\n\u03c32t+ \u03b3\u03c3 4t2\n\u03bbi < \u03b4\u03bbi ; \u03bbi > \u03b3\u03c32t ; \u03c32t(1\u2212 2\n\u221a (\u03b3)) < \u03b4\u03bbr\nThese inequalities will hold when \u03c32 < \u03b4\u03bbrt (since \u03b3 = 1, \u03b4 < 1 and \u03bb1 \u2265 \u03bb2 \u2265, . . . , \u03bbr).\nNote that the missing cross-terms would not change the result of Theorem 3.1 drastically, because UW has r non-zero singular values and hence UWST is a low-rank projection of a low-variance random matrix, and this will clearly be dominated by either of the other terms. Having justified the model in (2), the following thorem shows that the empirical distribution of the maximum Null statistic approximates the true distribution.\nTheorem 3.2. Let mt = maxiPi,t be the maximum observed test statistic at permutation trial t, and similarly let m\u0302t = maxi P\u0302i,t be the maximum reconstructed test statistic. Further, let the maximum reconstruction error be , such that |Pi,t \u2212 P\u0302i,t| \u2264 . Then, for any real number k > 0, we have,\nPr [ mt \u2212 m\u0302t \u2212 (b\u2212 b\u0302) > k ] < 1\nk2\nwhere b is the bias term described in Section 2, and b\u0302 is its estimate from the training phase.\nProof. Recall that there is a bias term in estimating the distribution of the maximum which must be corrected for this is because var(S\u0302) underestimates var(S) due to the bias/variance tradeoff. Let b be this difference:\nb = Et [ max i Pi,t ] \u2212 Et [ max i P\u0302i,t ] .\nFurther, recall that we estimate b by taking the difference of mean sample maxima between observed and reconstructed test statistics over the training set, giving b\u0302, which is an unbiased estimator of b \u2014 it is unbiased because a difference in sample means is an unbiased estimator of the difference of two expectations.\nLet \u03b4t = mt \u2212 m\u0302t. To show the result we must derive a concentration bound on \u03b4t, which we will do by applying Chebyshev\u2019s inequality. In order to do so, we require an expression for the mean and variance of \u03b4t. First, we derive an expression for the mean. Taking the expectation over t of mt \u2212 m\u0302t we have,\nEt [mt \u2212 m\u0302t] = Et [ max i Pi,t \u2212max i P\u0302i,t \u2212 b\u0302 ]\n= Et [ max i Pi,t ] \u2212 Et [ max i P\u0302i,t ] \u2212 b\u0302\n= b\u2212 b\u0302 where the second equality follows from the linearity of expectation.\nNext, we require an expression for the variance of \u03b4t. Let i be the index at which the maximum observed test statistic occurs for permutation trial t, and likewise let j be the index at which the maximum reconstructed test statistic occurs. Thus we have,\nPi,t \u2264 P\u0302i,t + \u2264 P\u0302j,t + Pi,t \u2265 Pj,t \u2265 P\u0302j,t \u2212 ,\nand so we have that |mt \u2212 m\u0302t| < 2\nand so var(mt \u2212 m\u0302t) \u2264 2.\nApplying Chebyshev\u2019s bound, Pr [ mt \u2212 m\u0302t \u2212 (b\u2212 b\u0302) > k ] < 1\nk2\nwhich completes the proof."}, {"heading": "4 Experimental evaluations", "text": "Our experimental evaluations include four separate neuroimaging datasets of Alzheimer\u2019s Disease (AD) patients, cognitively healthy age-matched controls (CN), and in some cases Mild Cognitive Impairment (MCI) patients. The first of these is the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset, a nation-wide multi-site study. ADNI is a landmark study sponsored by the NIH, major pharmaceuticals and others to determine the extent to which multimodal brain imaging can help predict onset, and monitor progression of AD. The others were collected as part of other studies of AD and MCI. We refer to these datasets as Dataset A\u2014D. Their demographic characteristics are as follows: Dataset A: 40 subjects, AD vs. CN, median age : 76; Dataset B: 50 subjects, AD vs. CN, median age : 68; Dataset C: 55 subjects, CN vs. MCI, median age : 65.16; Dataset D: 70 subjects, CN vs. MCI, median age : 66.24.\nOur evaluations focus on three main questions: (i) Can we recover an acceptable approximation of the maximum statistic Null distribution from an approximation of the permutation test matrix? (ii) What degree of computational speedup can we expect at various subsampling rates, and how does this affect the trade-off with approximation error? (iii) How sensitive is the estimated \u03b1-level threshold with respect to the recovered Null distribution? In all our experiments, the rank estimate for subspace tracking (to construct the low\u2013rank basis U) was taken as the number of subjects.\n4.1 Can we recover the Maximum Null?\nOur experiments suggest that our model can recover the maximum Null. We use Kullback\u2013Leibler (KL) divergence and Bhattacharya Distance (BD) to compare the estimated maximum Null from our model to the true one. We also construct a \u201cNaive\u2013Null\u201d, where the subsampled statistics are pooled and the Null distribution is constructed with no further processing (i.e., completion). Using this as a baseline, Fig. 5 shows the KL and BD values obtained from three datasets, at 20 different subsampling rates (ranging from 0.1% to 10%). Note that our model involves a training module where the approximate \u2018bias\u2019 of residuals is estimated. This estimation is prone to noise (for example, number of training frames). Hence Fig. 5 also shows the error bars pertaining to 5 realizations on the 20 sampling rates. The first observation from Fig. 5 is that both KL and BD measures of the recovered Null to the true distribution are < e\u22125 for sampling rates more than 0.4%. This\nsuggests that our model recovers both the shape (low BD) and position (low KL) of the null to high accuracy at extremely low sub-sampling. We also see that above a certain minimum subsampling rate (\u223c 0.3%), the KL and BD do not change drastically as the rate is increased. This is expected from the theory on matrix completion where after observing a minimum number of data samples, adding in new samples does not substantially increase information content. Further, the error bars (although very small in magnitude) of both KL and BD show that the recovery is noisy. We believe this is due to the approximate estimate of bias from training module."}, {"heading": "4.2 What is the computational speedup?", "text": "Our experiments suggest that the speedup is substantial. Figs. 6 and 2 compare the time taken to perform the complete permutation testing to that of our model. The three plots in Fig. 6 correspond to the datasets used in Fig. 5, in that order. Each plot contains 4 curves and represent the time taken by our model, the corresponding sampling and GRASTA [17] recovery (plus training) times and the total time to construct the entire matrix P (horizontal line). And Fig. 2 shows the scatter plot of computational speedup vs. KL divergence (over 3 repeated set of experiments on all the datasets and sampling rates). Our model achieved at least 30 times decrease in computation time in the low sampling regime (< 1%). Around 0.5% \u2212 0.6% sub-sampling (where the KL and BD are already < e\u22125), the computation speed-up factor averaged over all datasets was 45\u00d7. This shows that our model achieved good accuracy (low KL and BD) together with high computational speed up in tandem, especially, for 0.4% \u2212 0.7% sampling rates. However note from Fig. 2 that there is a trade\u2013off between the speedup factor and approximation error (KL or BD). Overall the highest computational speedup factor achieved at a recovery level of e\u22125 on KL and BD is around 50x (and this occured around 0.4% \u2212 0.5% sampling rate, refer to Fig. 2). It was observed that a speedup factor of upto 55\u00d7 was obtained for Datasets C and D at 0.3% subsampling, where the KL and BD were as low as e\u22125.5 (refer to Fig. 5 and the extended version of the paper)."}, {"heading": "4.3 How stable is the estimated \u03b1-threshold (clinical significance)?", "text": "Our experiments suggest that the threshold is stable. Fig. 4 and Table 1 summarize the clinical significance of our model. Fig. 4 show the error in estimating the true max threshold, at 1 \u2212 \u03b1 = 0.95 level of confidence. The x\u2013axis corresponds to the 20 different sampling rates used and y\u2013 axis shows the absolute difference of thresholds in log scale. Observe that for sampling rates higher than 3%, the mean and maximum differences was 0.04 and 0.18. Note that the binning\nresolution of max.statistic used for constructing the Null was 0.01. These results show that not only the global shape of the maximum Null distribution is estimated to high accuracy (see Section 4.1) but also the shape and area in the tail. To support this observation, we show the absolute differences of the estimated thresholds on all the datasets at 4 different \u03b1 levels in Table 1. The errors for 1\u2212 \u03b1 = 0.95, 0.99 are at most 0.16. The increase in error for 1\u2212 \u03b1 > 0.995 is a sampling artifact and is expected. Note that in a few cases, the error at 0.5% is slightly higher than that at 0.3% suggesting that the recovery is noisy (see Sec. 4.1 and the errorbars of Fig. 5). Overall the estimated \u03b1-thresholds are both faithful and stable."}, {"heading": "5 Conclusions and future directions", "text": "In this paper, we have proposed a novel method of efficiently approximating the permutation testing matrix by first estimating the major singular vectors, then filling in the missing values via matrix completion, and finally estimating the distribution of residual values. Experiments on four different neuroimaging datasets show that we can recover the distribution of the maximum Null statistic to a high degree of accuracy, while maintaining a computational speedup factor of roughly 50\u00d7. While our focus has been on neuroimaging problems, we note that multiple testing and False Discovery Rate (FDR) correction are important issues in genomic and RNA analyses, and our contribution may offer enhanced leverage to existing methodologies which use permutation testing in these settings[6].\nAcknowledgments: We thank Robert Nowak, Grace Wahba, Moo K. Chung and the anonymous reviewers for their helpful comments, and Jia Xu for helping with a preliminary implementation of the model. This work was supported in part by NIH R01 AG040396; NSF CAREER grant 1252725; NSF RI 1116584; Wisconsin Partnership Fund; UW ADRC P50 AG033514; UW ICTR 1UL1RR025011 and a Veterans Administration Merit Review Grant I01CX000165. Hinrichs is supported by a CIBM post-doctoral fellowship via NLM grant 2T15LM007359. The contents do not represent views of the Dept. of Veterans Affairs or the United States Government.\nFig 1. : All 4 datasets\nFig 2. : All 4 datasets"}], "references": [{"title": "Voxel-based morphometry\u2013the methods", "author": ["J. Ashburner", "K.J. Friston"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Why voxel-based morphometry should be used", "author": ["J. Ashburner", "K.J. Friston"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Resampling-based multiple testing: examples and methods for p-value adjustment, volume 279", "author": ["P.H. Westfall", "S.S. Young"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Multiple significance tests: the bonferroni method", "author": ["J.M. Bland", "D.G. Altman"], "venue": "British Medical Journal,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Adjusting multiple testing in multilocus analyses using the eigenvalues of a correlation", "author": ["J. Li", "L. Ji"], "venue": "matrix. Heredity,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Statistical significance for genomewide studies", "author": ["J. Storey", "R. Tibshirani"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Controlling the familywise error rate with plug-in estimator for the proportion of true null hypotheses", "author": ["H. Finner", "V. Gontscharuk"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A general framework for multiple testing dependence", "author": ["J.T. Leek", "J.D. Storey"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Robustness of multiple testing procedures against dependence", "author": ["S. Clarke", "P. Hall"], "venue": "The Annals of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis", "author": ["S. Garc\u0131\u0301a", "A. Fern\u00e1ndez", "J. Luengo", "F. Herrera"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Resampling-based multiple testing for microarray data analysis", "author": ["Y. Ge", "S. Dudoit", "T.P. Speed"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Controlling the familywise error rate in functional neuroimaging: a comparative review", "author": ["T. Nichols", "S. Hayasaka"], "venue": "Statistical Methods in Medical Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Group imaging of task-related changes in cortical synchronisation using nonparametric permutation testing", "author": ["K.D. Singh", "G.R. Barnes", "A. Hillebrand"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "A comparison of random field theory and permutation methods for the statistical analysis of meg data", "author": ["D. Pantazis", "T.E. Nichols", "S. Baillet", "R.M. Leahy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification", "author": ["B. Gaonkar", "C. Davatzikos"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A simple correction for multiple comparisons in interval mapping genome", "author": ["J.M. Cheverud"], "venue": "scans. Heredity,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "author": ["J. He", "L. Balzano", "A. Szlam"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Modified randomization tests for nonparametric hypotheses", "author": ["M. Dwass"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1957}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Rank minimization and applications in system theory", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "In American Control Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "Arxiv Preprint,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["L. Balzano", "R. Nowak", "B. Recht"], "venue": "Arxiv Preprint,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "Willsky A. S"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices", "author": ["F. Benaych-Georges", "R.R. Nadakuditi"], "venue": "Advances in Mathematics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2].", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2].", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "As one might expect, given the number of hypotheses tests v, multiple testing issues in this setting are quite severe, making it difficult to assess the true FamilyWise Type I Error Rate (FWER) [3].", "startOffset": 194, "endOffset": 197}, {"referenceID": 3, "context": "If we were to address this issue via Bonferroni correction [4], the enormous number of separate tests implies that certain weaker signals will almost certainly never be detected, even if they are real.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 107, "endOffset": 119}, {"referenceID": 5, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 107, "endOffset": 119}, {"referenceID": 6, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 107, "endOffset": 119}, {"referenceID": 7, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 107, "endOffset": 119}, {"referenceID": 8, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 214, "endOffset": 218}, {"referenceID": 11, "context": "Thus, many methods have been developed to more accurately and efficiently estimate or approximate the FWER [5, 6, 7, 8], which is a subject of much interest in statistics [9], machine learning [10], bioinformatics [11], and neuroimaging [12].", "startOffset": 237, "endOffset": 241}, {"referenceID": 11, "context": "A commonly used method of directly and non-parametrically estimating the FWER is Permutation testing [12, 13], which is a method of sampling from the Global (i.", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "A commonly used method of directly and non-parametrically estimating the FWER is Permutation testing [12, 13], which is a method of sampling from the Global (i.", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "This step alone can run from a few days up to many weeks and even longer [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 14, "context": "This step alone can run from a few days up to many weeks and even longer [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 4, "context": "Note that regardless of their description, strong dependencies of almost any kind will tend to concentrate most of their co-variation into a low-rank subspace, leaving a high-rank, low-variance residual [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 15, "context": "In fact, for Genome wide Association studies (GWAS), many strategies calculate the \u2018effective number\u2019 (Meff ) of independent tests corresponding to the rank of this subspace [16, 5].", "startOffset": 174, "endOffset": 181}, {"referenceID": 4, "context": "In fact, for Genome wide Association studies (GWAS), many strategies calculate the \u2018effective number\u2019 (Meff ) of independent tests corresponding to the rank of this subspace [16, 5].", "startOffset": 174, "endOffset": 181}, {"referenceID": 16, "context": "Using ideas from online low-rank matrix completion [17] we can sample a few of the Null statistics and reconstruct the remainder as long as we properly account for the residual.", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Randomly sampled permutation testing [18] is a methodology for drawing samples under the Global (Family-Wise) Null hypothesis.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Permutation is particularly desirable in this setting because it is free of any distribution assumption whatsoever [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 18, "context": "Low-rank matrix completion [19] seeks to reconstruct missing entries from a matrix, given only a small fraction of its entries.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "If so, then a much smaller number of observations, on the order of r log(v), where r is the column space\u2019s rank, and v is its ambient dimension [19] is sufficient to recover both an orthogonal basis for the row space as well as the expansion coefficients for each column, giving the recovery.", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "By placing an `1-norm penalty on the eigenvalues of the recovered matrix via the nuclear norm [20, 21] we can ensure that the solution is as low rank as possible.", "startOffset": 94, "endOffset": 102}, {"referenceID": 20, "context": "By placing an `1-norm penalty on the eigenvalues of the recovered matrix via the nuclear norm [20, 21] we can ensure that the solution is as low rank as possible.", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "Alternatively, we can specify a rank r ahead of time, and estimate an orthogonal basis of that rank by following a gradient along the Grassmannian manifold [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 16, "context": "Alternatively, we can specify a rank r ahead of time, and estimate an orthogonal basis of that rank by following a gradient along the Grassmannian manifold [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 21, "context": ", where rows of P arrive one at a time, and both U and W are updated accordingly [22, 17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 16, "context": ", where rows of P arrive one at a time, and both U and W are updated accordingly [22, 17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 22, "context": "skewed eigen-spectrum, because the flatter the eigen-spectrum becomes, the sparser the resulting covariance matrix tends to be (the \u201cuncertainty principle\u201d between low-rank and sparse matrices [23]).", "startOffset": 193, "endOffset": 197}, {"referenceID": 21, "context": "From these permutation tests, we estimate U using sub-sampled matrix completion methods [22, 17], making multiple passes over the training set (with fixed sub-sampling rate), until convergence.", "startOffset": 88, "endOffset": 96}, {"referenceID": 16, "context": "From these permutation tests, we estimate U using sub-sampled matrix completion methods [22, 17], making multiple passes over the training set (with fixed sub-sampling rate), until convergence.", "startOffset": 88, "endOffset": 96}, {"referenceID": 21, "context": "If this low-rank perturbation is sufficient to dominate the eigenvalues of the random matrix, then P can be recovered with high fidelity at a low sampling rate [22, 17].", "startOffset": 160, "endOffset": 168}, {"referenceID": 16, "context": "If this low-rank perturbation is sufficient to dominate the eigenvalues of the random matrix, then P can be recovered with high fidelity at a low sampling rate [22, 17].", "startOffset": 160, "endOffset": 168}, {"referenceID": 23, "context": "Thus, rather than analyzing the singular value spectrum of P directly, we can analyze the eigenvalues of PP using a recent result from [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "1 from [24].", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "1 of [24].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "The analysis in [24] takes into account the phase transition of extreme eigen values.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Each plot contains 4 curves and represent the time taken by our model, the corresponding sampling and GRASTA [17] recovery (plus training) times and the total time to construct the entire matrix P (horizontal line).", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "While our focus has been on neuroimaging problems, we note that multiple testing and False Discovery Rate (FDR) correction are important issues in genomic and RNA analyses, and our contribution may offer enhanced leverage to existing methodologies which use permutation testing in these settings[6].", "startOffset": 295, "endOffset": 298}], "year": 2015, "abstractText": "Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given \u03b1-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub\u2013sampled \u2014 on the order of 0.5% \u2014 matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly 50\u00d7 can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated \u03b1-threshold is also recovered faithfully, and is stable.", "creator": "LaTeX with hyperref package"}}}