{"id": "1502.03671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Phrase-based Image Captioning", "abstract": "In this paper, we present a simple model that is capable of generating descriptive sentences from a sample image, which focuses heavily on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutionary Neural Network) and phrases used to describe it. The system is then able to derive phrases from a given image sample. Based on syntax statistics, we propose a simple language model that can use the derived phrases to create relevant descriptions for a particular test image. Our approach, which is much simpler than modern models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "histories": [["v1", "Thu, 12 Feb 2015 14:17:15 GMT  (730kb,D)", "http://arxiv.org/abs/1502.03671v1", null], ["v2", "Thu, 9 Apr 2015 09:48:52 GMT  (572kb,D)", "http://arxiv.org/abs/1502.03671v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "pedro h o pinheiro", "ronan collobert"], "accepted": true, "id": "1502.03671"}, "pdf": {"name": "1502.03671.pdf", "metadata": {"source": "CRF", "title": "Phrase-based Image Captioning", "authors": ["R\u00e9mi Lebret", "Pedro O. Pinheiro", "Ronan Collobert"], "emails": ["REMI@LEBRET.CH,", "PEDRO@OPINHEIRO.COM", "RONAN@COLLOBERT.COM"], "sections": [{"heading": "1. Introduction", "text": "Being able to automatically generate a description from an image is a fundamental problem in artificial intelligence, connecting computer vision and natural language processing. The problem is particularly challenging because it requires to correctly recognize different objects in images and how they interact. Another challenge is that an image description generator needs to express these interactions in a natural language (e.g. English). Therefore, a language model is implicitly required in addition to visual understanding.\n1These two authors contributed equally to this work.\nRecently, this problem has been studied by many different authors. Most of the attempts are based on recurrent neural networks to generate sentences. These models leverage the power of neural networks to transform image and sentence representations into a common space (Mao et al., 2014; Karpathy & Fei-Fei, 2014; Vinyals et al., 2014; Donahue et al., 2014).\nIn this paper, we propose a different approach to the problem that does not rely on complex recurrent neural networks. An exploratory analysis of two large datasets of image descriptions reveals that their syntax is quite simple. The ground-truth descriptions can be represented as a collection of noun, verb and prepositional phrases. The different objects in a given image are described by the noun phrases, while the interactions between these objects are encoded by both the verb and the prepositional phrases. We thus train a model that predicts the set of phrases present in the sentences used to describe the images. By leveraging previous works on word vector representations, each phrase can be represented by the mean of the representations of the words that compose the phrase. Vector representations for images can also be easily obtained from some pre-trained convolutional neural networks. The model then learns a common embedding between phrase and image representations (see Figure 3).\nGiven a test image, a bilinear model is trained to predict a set of top-ranked phrases that best describe it. Several noun phrases, verb phrases and prepositional phrases are in this set. The objective is therefore to generate syntactically correct sentences from (possibly different) subsets of these phrases. We introduce a trigram constrained language model based on our knowledge about how the sentence descriptions are structured in the training set. With a very constrained decoding scheme, sentences are inferred with a beam search. Because these sentences are not conditioned to the given image (apart with the initial phrases selection), a re-ranking is used to pick the sentence that is closest to the sample image (according to the learned met-\nar X\niv :1\n50 2.\n03 67\n1v 1\n[ cs\n.C L\n] 1\n2 Fe\nb 20\nric). The quality of our sentence generation is evaluated on two very popular datasets for the task: Flickr30k (Hodosh et al., 2013) and the recently published COCO (Lin et al., 2014). Using the popular BLEU score (Papineni et al., 2002), our results are competitive with other recent works. Our generated sentences also achieve a similar performance as humans on the BLEU metric.\nThe paper is organized as follows. Section 2 presents related works. Section 3 presents the analysis we conducted to better understand the syntax of image descriptions. Section 4 describes the proposed phrase-based model. Section 5 introduces the sentence generation from the predicted phrases. Section 6 describes our experimental setup and the results on the two datasets. Section 7 concludes."}, {"heading": "2. Related Works", "text": "The classical approach to sentence generation is to pose the problem as a retrieval problem: a given test image will be described with the highest ranked annotation in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014). These matching methods may not generate proper descriptions for a new combination of objects. Due to this limitation, several generative approaches have been proposed. Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012). These approaches benefit from visual recognition systems to infer words or phrases, but in contrast to our work they do not leverage a multimodal metric between images and phrases.\nMore recently, automatic image sentence description approaches based on deep neural networks have emerged with the release of new large datasets. As starting point, these solutions use the rich representation of images generated by Convolutional Neural Networks (LeCun et al., 1998) (CNN) that were previously trained for object recognition tasks. These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding.\nVinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN,\nbidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a RNN for sentences. The two networks interact with each other in a multimodal common layer.\nOur model shares some similarities with these recent proposed approaches. We also use a pre-trained CNN to extract image features. However, thanks to the phrase-based approach, our model does not rely on complex recurrent networks for sentence generation, and we do not fine-tune the image features.\nAs our approach, Fang et al. (2014) proposes to not use recurrent networks for generating the sentences. Their solution can be divided into three steps: (i) a visual detector for words that commonly occur are trained using multiple instance learning, (ii) a set of sentences are generated using a Maximum-Entropy language model and (iii) the set of sentences is re-ranked using sentence-level features and a proposed deep multimodal similarity model. Our work differs from this approach in two different important ways: our model infers phrases present in the sentences instead of words and we use a considerably simpler language model."}, {"heading": "3. Syntax Analysis of Image Descriptions", "text": "The art of writing sentences can vary a lot according to the domain. When reporting news or reviewing an item, not only the choice of the words might vary, but also the general structure of the sentence. In this section, we wish to analysis the syntax of image descriptions to identify whether images have their own structures. We therefore proceed to a exploratory analysis of two recent datasets containing a large amount of images with descriptions: Flickr30k (Hodosh et al., 2013) and COCO (Lin et al., 2014)."}, {"heading": "3.1. Datasets", "text": "The Flickr30k dataset contains 31,014 images where 1,014 images are for validation, 1,000 for testing and the rest for training (i.e. 29,000 images). The COCO dataset contains 123,287 images, 82,783 training images and 40,504 validation images. The testing images has not yet been released. We thus use two sets of 5,000 images from the validation images for validation and test, as in Karpathy & Fei-Fei (2014)2. In both datasets, images are given with five (or six) sentence descriptions annotated using Amazon Mechanical Turk. This results in 559,113 sentences when combining both training datasets.\n2Available at http://cs.stanford.edu/people/ karpathy/deepimagesent/\n0 1 2 3 4 5 6+\nNP VP PP\nA pp\nar ea\nnc e\nfr eq\nue nc\nie s\n(% )\n0 10\n20 30\n40 50\n60\nFigure 1. Statistics on the number of phrases (NP, VP, PP) per ground-truth descriptions in Flickr30k and COCO training datasets."}, {"heading": "3.2. Chunking-based Approach", "text": "A quick overview over these sentence descriptions reveals that they all share a common structure, usually describing the different objects present in the image and how they interact between each other. This interaction among objects is described as actions or relative position between different objects. The sentence can be short or long, but it generally respects this process. To confirm this claim and better understand the description structures, we used a chunking (also called shallow parsing) approach which identifies the constituents of a sentence. These constituents are usually noun phrases (NP), verb phrases (VP) and prepositional phrases (PP). We extract them from the training sentences with the SENNA software3. Pre-verbal and post-verbal adverb phrases are merged with verb phrases to limit the number of phrase types.\nStatistics reported in Figure 1 and Figure 2 confirm that image descriptions possess a simple and distinct structure. These sentences do not have much variability. All the key elements in a given image are usually described with a noun phrase (NP). Interactions between these elements can then be explained using prepositional phrases (PP) or verb phrases (VP). A large majority of sentences contain from two to four noun phrases. Two noun phrases then interact using a verb or prepositional phrase. Describing an image is therefore just a matter of identifying these constituents. We thus propose to train a model which can predict the phrases which are likely to be in a given image.\n3Available at http://ml.nec-labs.com/senna/"}, {"heading": "4. Phrase-based Model for Image Descriptions", "text": "By leveraging previous works on word and image representations, we propose a simple model which can predict the phrases that best describe a given image. For this purpose, a metric between images and phrases is trained, as illustrated in Figure 3. The proposed architecture is then just a low-rank bilinear model UTV ."}, {"heading": "4.1. Image Representations", "text": "For the representation of images, we choose to use a Convolutional Neural Network. CNN have been widely used in different vision domains and are currently the state-of-theart in many object recognition tasks. We consider a CNN that has been pre-trained for the task of object classification (Chatfield et al., 2014). We use a CNN solely to the purpose of feature extraction, that is, no learning is done in the CNN layers."}, {"heading": "4.2. Learning a Common Space for Image and Phrase Representations", "text": "Let I be the set of training images, C the set of all phrases used to describe I, and \u03b8 the trainable parameters of the model. By representing each image i \u2208 I with a vector zi \u2208 Rn thanks to the pre-trained CNN, we define a metric between the image i and a phrase c as a bilinear operation:\nf\u03b8(c, i) = u T c V zi , (1)\nwithU = (uc1 , . . . ,uc|C|) \u2208 Rm\u00d7|C| and V \u2208 Rm\u00d7n being the trainable parameters \u03b8. Note that UTV could be a full matrix, but a low-rank setting eases the capacity control."}, {"heading": "4.3. Phrase Representations Initialization", "text": "Noun phrases or verb phrases are often a combination of several words. Good word vector representations can be obtained very efficiently with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen. By leveraging the ability of these word vector representations to compose by simple summation, representations for phrases are easily computed with an element-wise addition.\nEach phrase c composed of K words wk is therefore represented by a vector xwk \u2208 Rm thanks to a word representation model pre-trained on large unlabeled text corpora. A vector representation uc for a phrase c = {w1, . . . , wK} is then calculated by averaging its word vector representations:\nuc = 1\nK\nK\u2211\nk=1\nxwk . (2)\nVector representations for all phrases c \u2208 C can thus be obtained to initialized the matrix U \u2208 Rm\u00d7|C|. V \u2208 Rm\u00d7n is initialized randomly and trained to encode images in the same vector space than the phrases used for their descriptions."}, {"heading": "4.4. Training with Negative Sampling", "text": "Each image i is described by a multitude of possible phrases Ci \u2286 C. We consider |C| classifiers attributing a score for each phrase. We train our model to discriminate a target phrase cj from a set of negative phrases ck \u2208 C\u2212 \u2286 C, with ck 6= cj . With \u03b8 = {U, V }, we minimize the following logistic loss function with respect to \u03b8:\n\u03b8 7\u2192 \u2211\ni\u2208I\n\u2211\ncj\u2208Ci\n( log ( 1 + e +uTcj V zi )\n+ \u2211 ck\u2208C\u2212 log ( 1 + e\u2212u T ck V zi )) . (3)\nThe model is trained using stochastic gradient descent."}, {"heading": "5. From Phrases to Sentence", "text": "After identifying the L most likely constituents cj in the image i, we propose to generate sentences out of them. From this set, l \u2208 {1, . . . , L} phrases are used to compose a syntactically correct description."}, {"heading": "5.1. Sentence Generation", "text": "Using a statistical language framework, the likelihood of a certain sentence is given by:\nP (c1, c2, . . . , cl) =\nl\u220f\nj=1\nP (cj |c1, . . . , cj\u22121) (4)\nKeeping this system as simple as possible and using the second order Markov property, we approximate Equation 4\nwith a trigram language model:\nP (c1, c2, . . . , cl) \u2248 l\u220f\nj=1\nP (cj |cj\u22122, cj\u22121) . (5)\nThe best candidate corresponds to the sentence P (c1, c2, . . . , cl) which maximizes the likelihood of Equation 5 over all the possible sizes of sentence. Because we want to constrain the decoding algorithm to include prior knowledge on chunking tags t \u2208 {NP, V P, PP}, we rewrite Equation 5 as:\nl\u220f\nj=1\n\u2211\nt\nP (cj |tj = t, cj\u22122, cj\u22121)P (tj = t|cj\u22122, cj\u22121)\n= l\u220f\nj=1\nP (cj |tj , cj\u22122, cj\u22121)P (tj |cj\u22122, cj\u22121) . (6)\nBoth conditions P (cj |tj , cj\u22122, cj\u22121) and P (tj |cj\u22122, cj\u22121) are probabilities estimated by counting trigrams in the training datasets."}, {"heading": "5.2. Sentence Decoding", "text": "At decoding time, we prune the graph of all possible sentences made out of the top L phrases with a beam search, according to three heuristics: (i) we consider only the transitions which are likely to happen (we discard any sentence which would have a trigram transition probability inferior to 0.01). This thresholding helps to discard sentences that are semantically incorrect; (ii) each predicted phrases cj may appear only once4; (iii) we add syntactic constraints which are illustrated in Figure 4. The last heuristic is based on the analysis of syntax in Section 3. In Figure 2, we see that a noun phrase is, in general, always followed by a verb phrase or a prepositional phrase, and both are then followed by another noun phrase. A large majority of the sentences contain three noun phrases interleaved with verb phrases or prepositional phrases. According the statistics reported in Figure 1, sentences with two or four noun phrases are also common, but sentences with more than four noun phrases are marginal. We thus repeat this process N = {2, 3, 4} times until reaching the end of a sentence (characterized by a period)."}, {"heading": "5.3. Sentence Re-ranking", "text": "For each test image i, the proposed model will generate a set of M sentences. Sentence generation is not conditioned by the image, apart phrases which are selected beforehand. Some phrase sequences might be syntactically good, but have low match with the image. Consider, for instance, an\n4This is easy to implement with a beam search, but intractable with a full search.\nimage with a cat and a dog. Both sentences \u201ca cat sitting on a mat and a dog eating a bone\u201d and \u201ca cat sitting on a mat\u201d are correct, but the second is missing an important part of the image. A ranking of the generated sentences is therefore necessary to choose the one that has the best match with the image.\nBecause a generated sentence is composed from l phrases predicted by our system, we simply average the phrase scores given by Equation 1. For a generated sentence s composed of l phrases cj , a score between s and i is calculated as:\n1\nl\n\u2211 cj\u2208s f\u03b8(cj , i) . (7)\nThe best candidate is the sentence which has the highest score out of theM generated sentences. This ranking helps the system to chose the sentence which is closer to the sample image."}, {"heading": "6. Experiments", "text": ""}, {"heading": "6.1. Experimental Setup", "text": ""}, {"heading": "6.1.1. FEATURE SELECTION", "text": "Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al., 2014). This model generates image representations of dimension 4096 from RGB input images.\nFor each training set, only phrases occurring at least ten times are considered. This threshold is chosen to fulfil two objectives: (i) limit the number of phrases C and therefore the size of the matrix U and (ii) exclude rare phrases to better generalize the descriptions. Statistics on the number of phrases are reported in Table 1. For Flickr30k, this threshold covers about 81% of NP, 83% of VP and 99% of PP. For COCO, it covers about 73% of NP, 75% of VP and 99% of PP. Phrase representations are then computed by aver-\nFlickr30k COCO\nNoun Phrase (NP) 4818 8982\nVerb Phrase (VP) 2109 3083\nPrepositional Phrase (PP) 128 189\nTotal |C| 7055 12254\nTable 1. Statistics of phrases appearing at least ten times.\naging vector representations of their words. We obtained word representations from the Hellinger PCA of a word cooccurrence matrix, following the method described in Lebret & Collobert (2014). The word co-occurrence matrix is built over the entire English Wikipedia5, with a symmetric context window of ten words coming from the 10,000 most frequent words. Words, and therefore also phrases, are represented in 400-dimensional vectors."}, {"heading": "6.1.2. LEARNING THE MULTIMODAL METRIC", "text": "The parameters \u03b8 are V \u2208 R400\u00d74096 (initialized randomly) and U \u2208 R400\u00d7|C| (initialized with the phrase representations) which are tuned on the validation datasets. They are trained with 15 randomly chosen negative samples and a learning rate set to 0.00025."}, {"heading": "6.1.3. GENERATING SENTENCES FROM THE PREDICTED PHRASES", "text": "Transition probabilities for our constrained language model (see Figure 4) are calculated independently for each training set. No smoothing has been used in the experiments. Concerning the set of top-ranked phrases for a given test image, we select only the top five predicted verb phrases and the top five predicted prepositional phrases. Since the average number of noun phrases is higher than for the two other types of phrases (see Figure 1), more noun phrases are needed. The top twenty predicted noun phrases are thus selected."}, {"heading": "6.2. Experimental Results", "text": "As a first evaluation, we consider the task of retrieving the ground-truth phrases from test image descriptions. Results reported in Table 2 show that our system achieves a recall of around 50% on this task on the test set of both datasets, assuming the threshold considered for each type of phrase (see 6.1.3). Note that this task is extremely difficult, as semantically similar phrases (the women / women / the little girls) are classified separately. Despite the possible number\n5Available at http://download.wikimedia.org. We took the January 2014 version.\nof noun phrases being higher, results in Table 2 reveal that noun phrases are better retrieved than verb phrases. This shows that our system is able to detect different objects in the image. However, finding the right verb phrase seems to be more difficult. A possible explanation could be that there exists a wide choice of verb phrases to describe interactions between the noun phrases. For instance, we see in Figure 3 that two annotators have used the same noun phrases (a man, a skateboard and a (wooden) ramp) to describe the scene, but they have then chosen a different verb phrase to link them (riding versus is grinding). Therefore, we suspect that a low recall for verb phrases does not necessarily mean that the predictions are wrong. Finding the right prepositional phrase seems, on the contrary, much easier. The high recall for prepositional phrase can be explained by much lower variability of this type of phrase compared to the two others (see Table 1).\nAs a second evaluation, we consider the task of generating full descriptions. We measure the quality of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002). Table 3 shows our sentence generation results on the two datasets considered. BLEU scores are reported up to 4-gram. Human agreement scores are computed by comparing the first ground-truth description against the four others6. For comparison, we include results from recently proposed models. Our model, despite being simpler, achieves similar results to state of the art results. It is interesting to note that our results are very close to the human agreement scores.\nWe show examples of full automatic generated sentences in Figure 5. The simple language model used is able to generate sentences that are in general syntactically correct. Our model produces sensible descriptions with variable complexity for different test samples. Due to the generative aspect of the model, it can occur that the sentence generated is very different from the ground-truth and still provides a descent description. The last row of Figure 5 illustrates failure\n6For all models, BLEU scores are computed against five reference sentences which give a slight advantage compared to human scores.\nsamples. We can see in these failure samples that our system has however outputted relevant phrases. There is still room for improvement for generating the final description. We deliberately choose a simple language model to show that competitive results can be achieved with a simple approach. A more complex language model could probably avoid these failure samples by considering a larger context. The probability for a dog to stand on top of a wave is obviously very low, but this kind of mistake cannot be detected with a simple trigram language model."}, {"heading": "6.3. Diversity of Image Descriptions", "text": "In contrast to RNN-based models, our model is not trained to match a given image i with its ground-truth descriptions s, i.e., to give P (s|i). Because our model outputs instead a set of phrases, this is not really surprising that only 1% of our generated descriptions are in the training set for Flickr30k, and 9.7% for COCO. While a RNN-based model is generative, it might easily overfits a small training data. Vinyals et al. (2014) report, for instance, that the generated sentence is present in the training set 80% of the times. Our model therefore offers a good alternative with the possibility of producing unseen descriptions with a combination of phrases from the training set."}, {"heading": "6.4. Phrase Representation Fine-Tuning", "text": "Before training the model, the matrix U is initialized with phrase representations obtained from the whole English Wikipedia. This corpus of unlabeled text is well structured and large enough to provide good word vector representations, which can then produce good phrase representations. However, the content of Wikipedia is clearly different from the content of the image descriptions. Some words used for describing images might be used in different contexts in Wikipedia, which can lead to out-of-domain represen-\ntations for certain phrases. This becomes thus crucial to adapt these phrase representations by fine-tuning the matrix U during the training7. Some examples of noun phrases are reported in Table 4 with their nearest neighbors before and after the training. These confirm the importance of finetuning to incorporate visual features. In Wikipedia, cat seems to occur in the same context than dog or other animals. When looking at the nearest neighbors of a phrase such as a grey cat, other grey animals arise. After training on images, the word cat becomes the important feature of that phrase. And we see that the nearest neighbors are now cats with different colours. In some cases, averaging word vectors to represent phrases is not enough to capture\n7Experiments with a fixed U phrase representations matrix significantly hurt the general performance. We observe about a 50% decrease in both datasets with the BLEU metric. Since the number of trainable parameters is reduced, the capacity of V should be increased to guarantee a fair comparison.\nthe semantic meaning. Fine-tuning is thus also important to better learn specific phrases. Images related to baseball games, for example, have enabled the phrase home plate to be better defined. This is also true for the phrase a half pipe with images about skateboarding. This leads to interesting phrase representations, grounded in the visual world, which could be possibly used in natural language applications in future work."}, {"heading": "7. Conclusion", "text": "In this paper, we propose a simple model that is able to infer different phrases from image samples. From the phrases\npredicted, our model is able to automatically generate sentences using a statistical language model. We show that the problem of sentence generation can be effectively achieved without the use of complex recurrent networks. Our algorithm, despite being simpler than state-of-the-art models, achieves similar results on this task. Also, our model generate new sentences which are not generally present in training set. Future research directions will go towards leveraging unsupervised data and more complex language models to improve sentence generation. Another interest is assessing the impact of visually grounded phrase representations into existing natural language processing systems."}], "references": [{"title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Learning a Recurrent Visual Representation for Image Caption Generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654,", "citeRegEx": "Chen and Zitnick,? \\Q2014\\E", "shortCiteRegEx": "Chen and Zitnick", "year": 2014}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Framing image description as a ranking task: data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CoRR, abs/1412.2306,", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2014\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2014}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Neural Language Models. volume abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "In ACL,", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Rehabilitation of Countbased Models for Word Vector Representations", "author": ["R. Lebret", "R. Collobert"], "venue": "CoRR, abs/1412.4930,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating Image Descriptions from Computer Vision Detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "III Daum\u00e9"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "Kavukcuoglu", "Koray"], "venue": "In NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Multimodal Learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2014\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov", "year": 2014}, {"title": "Translating Videos to Natural Language", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "Using Deep Recurrent Neural Networks. CoRR,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "I2T: Image Parsing to Text Description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "These models leverage the power of neural networks to transform image and sentence representations into a common space (Mao et al., 2014; Karpathy & Fei-Fei, 2014; Vinyals et al., 2014; Donahue et al., 2014).", "startOffset": 119, "endOffset": 207}, {"referenceID": 3, "context": "The quality of our sentence generation is evaluated on two very popular datasets for the task: Flickr30k (Hodosh et al., 2013) and the recently published COCO (Lin et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 13, "context": "Using the popular BLEU score (Papineni et al., 2002), our results are competitive with other recent works.", "startOffset": 29, "endOffset": 52}, {"referenceID": 3, "context": "The classical approach to sentence generation is to pose the problem as a retrieval problem: a given test image will be described with the highest ranked annotation in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014).", "startOffset": 185, "endOffset": 261}, {"referenceID": 15, "context": "The classical approach to sentence generation is to pose the problem as a retrieval problem: a given test image will be described with the highest ranked annotation in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014).", "startOffset": 185, "endOffset": 261}, {"referenceID": 18, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 11, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 6, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 8, "context": "As starting point, these solutions use the rich representation of images generated by Convolutional Neural Networks (LeCun et al., 1998) (CNN) that were previously trained for object recognition tasks.", "startOffset": 116, "endOffset": 136}, {"referenceID": 2, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 17, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 5, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem.", "startOffset": 34, "endOffset": 393}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding.", "startOffset": 34, "endOffset": 659}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al.", "startOffset": 34, "endOffset": 909}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a RNN for sentences.", "startOffset": 34, "endOffset": 935}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a RNN for sentences. The two networks interact with each other in a multimodal common layer. Our model shares some similarities with these recent proposed approaches. We also use a pre-trained CNN to extract image features. However, thanks to the phrase-based approach, our model does not rely on complex recurrent networks for sentence generation, and we do not fine-tune the image features. As our approach, Fang et al. (2014) proposes to not use recurrent networks for generating the sentences.", "startOffset": 34, "endOffset": 1427}, {"referenceID": 3, "context": "We therefore proceed to a exploratory analysis of two recent datasets containing a large amount of images with descriptions: Flickr30k (Hodosh et al., 2013) and COCO (Lin et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 0, "context": "We consider a CNN that has been pre-trained for the task of object classification (Chatfield et al., 2014).", "startOffset": 82, "endOffset": 106}, {"referenceID": 14, "context": "Good word vector representations can be obtained very efficiently with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014).", "startOffset": 104, "endOffset": 204}, {"referenceID": 9, "context": "Good word vector representations can be obtained very efficiently with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king man + woman \u2248 queen.", "startOffset": 105, "endOffset": 229}, {"referenceID": 0, "context": "Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al., 2014).", "startOffset": 84, "endOffset": 108}, {"referenceID": 13, "context": "We measure the quality of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002).", "startOffset": 99, "endOffset": 122}, {"referenceID": 2, "context": "67 - - Donahue et al. (2014) 0.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "67 - - Donahue et al. (2014) 0.59 0.39 0.25 0.16 0.63 0.44 0.30 0.21 Fang et al. (2014) - - - - - - 0.", "startOffset": 7, "endOffset": 88}], "year": 2017, "abstractText": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "creator": "LaTeX with hyperref package"}}}