{"id": "1502.03879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "abstract": "We look at the general problem of using both labeled and unlabeled data to improve data rendering performance. A new semi-supervised learning framework is proposed by combing through diverse regulatory and data rendering methods such as non-negative matrix factorization and sparse encoding. We use unattended data rendering methods as learning machines, as they are not dependent on the wired data, which can improve the machine's ability to generate as much as possible. The proposed framework forms the laplac regulator by learning the affinity graph. We integrate the new laplac regulator into unattended data rendering to flatten the low-dimensional representation of data and utilize label information. Experimental results from several real benchmark data sets suggest that our semi-supervised learning framework achieves encouraging results compared to modern methods.", "histories": [["v1", "Fri, 13 Feb 2015 03:35:15 GMT  (518kb)", "http://arxiv.org/abs/1502.03879v1", "10 pages,2 Tables. Written in Aug,2013"]], "COMMENTS": "10 pages,2 Tables. Written in Aug,2013", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["weiya ren"], "accepted": false, "id": "1502.03879"}, "pdf": {"name": "1502.03879.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "authors": ["Weiya Ren"], "emails": ["weiyren.phd@gmail.com"], "sections": [{"heading": null, "text": "performance. A new semi-supervised learning framework is proposed by combing manifold regularization and data representation methods such as Non negative matrix factorization and sparse coding. We adopt unsupervised data representation methods as the learning machines because they do not depend on the labeled data, which can improve machine\u2019s generation ability as much as possible. The proposed framework forms the Laplacian regularizer through learning the affinity graph. We incorporate the new Laplacian regularizer into the unsupervised data representation to smooth the low dimensional representation of data and make use of label information. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results compared with state-of-art methods.\nIndex Terms\u2014Semi-supervised learning, sparse coding, clustering, manifold regularization, nonnegative matrix\nfactorization, metric learning.\nI. INTRODUCTION\nThe task of semi-supervised learning (SSL) algorithms is to utilize both labeled and unlabeled data to improve learning ability. A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency. Most SSL algorithms use label information to improve the learning performance. However, they may also face many problems such as over-fitting. In order to improve learning machine\u2019s generation ability as much as possible, we aim to propose a semi-supervised framework that learns the predictions that do not directly depend on the label samples.\nWe adopt unsupervised data representation methods, such as nonnegative matrix factorization (NMF) and sparse coding (SC) as the learning machines. Without considering the difference between labeled samples and unlabeled samples, these data representation methods always have a good generation ability. Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data. We make use of label information by learning an affinity graph, which contains label information, i.e., prior information is embedded in the manifold\nregularization to control the smoothness of the data representation. When such an affinity graph is obtained, the corresponding graph Laplacian regularizer is incorporated into the unsupervised data representation methods.\nDifferent from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.\nNotice that in GSSL, the sparsification of affinity matrix is important since it leads to improved efficiency in the label inference stage, better accuracy and robustness to noise. In fact, spurious connections between dissimilar nodes (which tend to be in different classes) are removed and each node connects to only a few nodes. Metric learning method, which learn the similarity and dissimilarity of pairs of the labeled samples, is a good way to lead the sparsification stage more accurate (delete more \u201cright\u201d spurious connections). In this way, the affinity graph is more suitable with considering the label information.\nIn this paper, we propose a novel semi-supervised framework. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results. We compared our proposed methods with state-of-art methods, include some excellent GSSL and unsupervised methods.\nThe rest of the paper is organized as follows: Section 2 gives a brief review of several basic algorithms. In Section 3, we introduce our proposed framework. Experimental results are presented in Section 4. Finally, we conclude in Section 5."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Graph-based Semi-supervised Learning (GSSL)", "text": "GSSL techniques start with computing a similarity score between all pairs of nodes using a similarity function or kernel. Then, an algorithm is selected for finding a sparse weighted subgraph from the fully connected similarity graph. There are two typical ways to build a sparse graph: neighborhood approaches including the k-nearest and neighbors algorithms, and matching approaches such as b-matching [10].\nOnce a graph has been sparsified, several procedures can then be used to update the graph weights such as Binary weighting and Gaussian kernel weighting. At last, given the final affinity graph and some initial label information, GSSL algorithms diffuse the labels on the known part of the graph to the unknown nodes.\nThe current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the\nlocal and global consistency method [5] and the alternating graph transduction method [13]."}, {"heading": "B. Metric learning", "text": "Metric learning expects that the learned metric makes distances between similar samples small and distances\nbetween dissimilar samples large. In general, a Mahalanobis distance metric measures the squared distance between\ntwo data points and :\n( ) ( ) ( ) (1)\nwhere is a positive semi-definite matrix and is a pair of samples .\nThe current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11],\nInformation theoretic metric learning (ITML) [12], and KISS metric learning [7].\nIII. PROPOSED FRAMEWORK"}, {"heading": "A. Framwork", "text": "We adopt unsupervised data representation methods as the learning machines. Traditional semi-supervised manner for unsupervised data representation usually use the Label Weight as the affinity [15]. Label Weight is constructed as follows:\n{\n(2)\nIn this way, the unlabeled data is totally ignored. In the proposed framework, prior information is embedded in the\nmanifold regularization to control the smoothness of the data representation.\nGiven independent and identically distributed samples of feature dimensions and\ninstances, and the first samples have labels. The learned weight matrix forms the following Laplacian regularizer which is used to measure the smoothness of the low dimensional representation:\n\u2211\n(3)\nwhere is the data representation of data , . denotes the trace of a matrix and\nis a diagonal matrix whose entries are column sums of . Then the loss function in the proposed framework is defined as follows:\n(4)\nwhere is the loss function of one unsupervised method, ( ) is the Laplacian regularizer, is the\nregularization parameter to measure the smoothness of the low dimensional representation."}, {"heading": "B. Affinity graph learning", "text": "We utilize KISS metric learning method [7] to produce a kernel matrix to measure the similarity between all pairs of samples. Then, the similarity scores between all pairs of nodes creates a full adjacency matrix , where\n( ) .\nSubsequently, -nearest neighbors algorithm or -matching algorithm can be applied to build a sparse graph\n( ). When the graph is sparsified, Gaussian kernel weighting is used to update the graph weights produce a final\naffinity graph. Therein, the edge weight between two connected samples and is computed as:\n(5)\nwhere is the kernel bandwidth parameter and we always set \u2211 \u2211 \u221a\n."}, {"heading": "C. Algorithms under the proposed framework", "text": "We adopt two powerful unsupervised data representation methods: Graph Regularized Nonnegative Matrix\nFactorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to display our framework.\n1) Semi-supervised Graph Regularized Nonnegative Matrix Factorization\nGNMF extend NMF by explicitly considering the manifold assumption [3] and the locally invariant idea , i.e., the\nnearby points are likely to have similar embeddings.\nGNMF is extended to a semi-supervised method by using the proposed affinity graph. Given a nonnegative data\nmatrix , let be the basis matrix, and\nbe the coefficient matrix. The\nloss function in GNMF is defined as follows:\n(6)\nwhere the regularization parameter .\nThe Euclidian distance based GNMF algorithms is:\n(7)\n2) Semi-supervised Graph Regularized Sparse Coding\nGraph regularized Sparse Coding (GSC) learns the sparse representations that explicitly take into account the local manifold structure of the data. By incorporating the new Laplacian regularizer into the original sparse coding, GSC is\nextended to its semi-supervised manner. Given a data matrix , let be the dictionary matrix,\nand be the coefficient matrix. The loss function in GSC is defined as follows:\n| |\n\u2211 | |\n(8)\nwhere , are the regularization parameter.\nFollowing the iteratively optimization method in [6], the GSC algorithm can learn the graph regularized sparse codes and the learning dictionary iteratively."}, {"heading": "IV. EXPERIMENTS", "text": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6]. We investigate the clustering performance of the proposed semi-supervised framework on three real world image data sets, i.e., the Yale database 1 ,the ORL database 2 , and the FEI database 3 .\nThe ORL face database consists of 10 different images for each of 40 distinct subjects, which are taken at different times, under different lighting condition, with different facial expression and with/without glasses. The Yale database contains 165 grayscale images of 15 individuals. There are 11 images per subject, one per different facial expression or configuration. The FEI face database is taken against a white homogenous background in an upright frontal position with profile rotation of up to about 180 degrees. Scale might vary about 10%. We select 50 individuals (part 1 in FEI) for clustering experiments, and there are 14 images per subject. All images in three databases are down sampled to a size of 24\u00d732, 32\u00d732, 32\u00d732 pixels with 256 gray levels per pixel, respectively. Pixel features are used for testing.\nWe compare the following algorithms for data clustering:\n The K-means clustering algorithm (K\u2013means).  Constrained Nonnegative Matrix Factorization (CNMF) [15].  The Local and Global Consistency method (LGC) [5].  Greedy Max\u2013Cut (GMC) [1].\n1 http://homepages.dcc.ufmg.br/~william/datasets.html. 2 http://cvc.yale.edu/projects/yalefaces/yalefaces.html. 3 http://www.uk.research.att.com/facedatabase.html.\n Graph regularized Nonnegative Matrix Factorization \uff08GNMF\uff09[4] +K\u2013means.\n Graph regularized Nonnegative Matrix Factorization using the Label Weight (2) (LGNMF) + K\u2013means  Graph regularized Nonnegative Matrix Factorization under the Proposed Framework (FGNMF)+ K\u2013means.  Graph regularized Sparse Coding (GSC) [6]+ K\u2013means.  Graph regularized Sparse Coding using the Label Weight (2) (LGSC) + K\u2013means.\n Graph regularized Sparse Coding under the Proposed Framework (FGSC) + K\u2013means.\nThe clustering result is evaluated by comparing the obtained label of each sample with that provided by the data set. GSSL methods such as LGC and GMC directly propagate label from labeled data to unlabeled data. For other methods, we use Accuracy Metric (AC) [14] to evaluate the clustering performance. The AC value is obtained by mapping the cluster to the corresponding predicted label, so it can be compared with the predicted accuracy in GSSL.\nAll algorithms expect GSSL methods obtain a new data representations of . We set the dimensionality of the new space to be the same as the number of clusters. We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6]. For each data set, the evaluations are conducted with different numbers of clusters. For all algorithms, we randomly choose categories from the normalized data set ( ), and mix the images of\nthese categories as the collection for clustering. For the fixed number of clusters , we randomly pick up two images from each subject as label information. Clustering performance on 10 algorithms are shown in Table 1. When\nlabeled samples of each subject increase from two to more, clustering performance on 7 semi-supervised algorithms are shown in Table 2.\nThese experiments reveal a number of interesting points:\n The unsupervised methods under our framework outperform themselves and their original semi- supervised\nmanner (using the Label Weigh).\n The unsupervised methods under our framework perform better than the best GSSL methods (LGC, GMC)\nespecially when number of clusters becomes larger.\n When labeled samples of each subject increase, the unsupervised methods under our framework outperform the\nother semi-supervised algorithms. It seems that some algorithms like LGC and LGNMF are not sensitive to label information.\n Regardless of the data sets, the Graph Sparse Coding method under our framework always performs good. This\nsuggests that the sparsity is important for data representation."}, {"heading": "V. CONCLUSIONS", "text": "We present a novel semi-supervised framework that explicitly considers the generation ability of learning machines and the prior information. By learning the affinity graph, the graph Laplacian regularizer is incorporated into the unsupervised data representation method. The experimental results on clustering have demonstrated that our proposed framework can have better discriminating power and significantly enhance the data representation performance."}, {"heading": "VI. ACKNOWLEDGMENTS", "text": "This paper is supported by College of Information System and Management, National University of Defense Technology and subsidized by National Natural Science Foundation of China Grant No. 61170158 and Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing Grant 2013A08."}, {"heading": "VII. REFERENCES", "text": "[1] Jun Wang, Tony Jebara, and Shih-Fu Chang, \u201cSemi-Supervised Learning Using Greedy Max -Cut,\u201d Journal of Machine Learning Research,\n2013.\n[2] Rui Zhao, Wanli Ouyang, and Xiaogang Wang, \u201cUnsupervised Salience Learning for Person Re-identification,\u201d Proc. IEEE Intern. Conf.\non Computer Vision and Pattern Recognition, 2013.\n[3] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani, \u201cManifold Regularization: A Geometric Framework for Learning from Labeled and\nUnlabeled Examples,\u201d Journal of Machine Learning Research, 2006.\n[4] D. Cai, X. He, X. Wu, and J. Han, \u201cGraph Regularized Non-negative Matrix Factorization for Data Representation,\u201d IEEE Trans. Patt.\nAnal. Mach. Intel, vol.33, no.8, 1548-1560, 2011.\n[5] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch o\u0308lkopf, \u201cLearning with local and global consistency,\u201d In S. Thrun, L. Saul, and B.\nSch o\u0308lkopf, editors, Advances in Neural Information Processing Systems, volume 16, pages 321\u2013328. 2004.\n[6] M. Zheng, J. Bu, C.A. Chen, C. Wang, L. Zhang, G. Qiu, D. Cai, \u201cGraph regularized sparse coding for image representation,\u201d IEEE Trans.\nImage Process., vol. 20, pp. 1327-1336, 2011.\n[7] M. Kostinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof \u201cLarge scale metric learning from equivalence constraints,\u201d in Proc.\nIEEE Conf. Comput. Vision Pattern Recogn., Jun. 2012, pp. 2288\u20132295.\n[8] Belkin, M., Niyogi, P., & Sindhwani, V, \u201cOn manifold regularization,\u201d Int. Workshop on Artificial Intelligence and Statistics, 2005. [9] B. Kveton, M. Valko, A. Rahimi, and L. Huang, \u201cSemi-supervised learning with max-margin graph cuts,\u201d In Proceedings of the 13th\nInternational Conference on Artificial Intelligence and Statistics, pages 421\u2013428, 2010.\n[10] Tony Jebara, Jun Wang, and Shih-Fu Chang, \u201cGraph Construction and b-Matching for Semi-Supervised Learning,\u201d Proceedings of the\n26th International Conference on Machine Learning, 2009.\n[11] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, \u201cInformation-theoretic metric learning,\u201d In Proc. IEEE Intern. Conf. on Machine\nLearning, 2007.\n[12] Davis J V, Kulis B, Jain P, et al. Information-theoretic metric learning[C]//Proceedings of the 24th international conference on Machine\nlearning. ACM, 2007: 209-216.\n[13] Wang, F., & Zhang, C. S., \u201cLabel propagation through linear neighborhoods,\u201d IEEE Trans. Knowl. Data Eng., 20, 55\u201367, 2008. [14] W. Xu, X. Liu, and Y. Gong, \u201cDocument Clustering Based on Non-Negative Matrix Factorization,\u201d Proc. Ann. ACM SIGIR Conf.\nResearch and Development in Information Retrieval, 2003.\n[15] Liu H, Wu Z, Li X, Cai D, Huang TS, \u201cConstrained nonnegative matrix factorization for image representation,\u201d IEEE Trans Pattern Anal\nMach Intell 34(7):1299\u20131311, 2012."}], "references": [{"title": "Semi-Supervised Learning Using Greedy Max -Cut", "author": ["Jun Wang", "Tony Jebara", "Shih-Fu Chang"], "venue": "Journal of Machine Learning Research, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised Salience Learning for Person Re-identification", "author": ["Rui Zhao", "Wanli Ouyang", "Xiaogang Wang"], "venue": "Proc. IEEE Intern. Conf. on Computer Vision and Pattern Recognition, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "Journal of Machine Learning Research, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Graph Regularized Non-negative Matrix Factorization for Data Representation", "author": ["D. Cai", "X. He", "X. Wu", "J. Han"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel, vol.33, no.8, 1548-1560, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch \u00f6lkopf"], "venue": "S. Thrun, L. Saul, and B. Sch \u00f6lkopf, editors, Advances in Neural Information Processing Systems, volume 16, pages 321\u2013328. 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph regularized sparse coding for image representation", "author": ["M. Zheng", "J. Bu", "C.A. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai"], "venue": "IEEE Trans. Image Process., vol. 20, pp. 1327-1336, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale metric learning from equivalence constraints,", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "in Proc. IEEE Conf. Comput. Vision Pattern Recogn.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "On manifold regularization", "author": ["M. Belkin", "P. Niyogi", "V Sindhwani"], "venue": "Int. Workshop on Artificial Intelligence and Statistics, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised learning with max-margin graph cuts", "author": ["B. Kveton", "M. Valko", "A. Rahimi", "L. Huang"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, pages 421\u2013428, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph Construction and b-Matching for Semi-Supervised Learning", "author": ["Tony Jebara", "Jun Wang", "Shih-Fu Chang"], "venue": "Proceedings of the 26th International Conference on Machine Learning, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proc. IEEE Intern. Conf. on Machine Learning, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Information-theoretic metric learning[C]//Proceedings of the 24th international conference on Machine learning", "author": ["V Davis J", "B Kulis", "P Jain"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Label propagation through linear neighborhoods", "author": ["F. Wang", "C.S. Zhang"], "venue": "IEEE Trans. Knowl. Data Eng., 20, 55\u201367, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Document Clustering Based on Non-Negative Matrix Factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proc. Ann. ACM SIGIR Conf. Research and Development in Information Retrieval, 2003. 10", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Constrained nonnegative matrix factorization for image representation", "author": ["H Liu", "Z Wu", "X Li", "D Cai", "TS Huang"], "venue": "IEEE Trans Pattern Anal Mach Intell 34(7):1299\u20131311, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 7, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 8, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 9, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 12, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 2, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 3, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 5, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 2, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 10, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 11, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 9, "context": "There are two typical ways to build a sparse graph: neighborhood approaches including the k-nearest and neighbors algorithms, and matching approaches such as b-matching [10].", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 150, "endOffset": 153}, {"referenceID": 12, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 10, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 14, "context": "Traditional semi-supervised manner for unsupervised data representation usually use the Label Weight as the affinity [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "We utilize KISS metric learning method [7] to produce a kernel matrix to measure the similarity between all pairs of samples.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "We adopt two powerful unsupervised data representation methods: Graph Regularized Nonnegative Matrix Factorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to display our framework.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "We adopt two powerful unsupervised data representation methods: Graph Regularized Nonnegative Matrix Factorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to display our framework.", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "1) Semi-supervised Graph Regularized Nonnegative Matrix Factorization GNMF extend NMF by explicitly considering the manifold assumption [3] and the locally invariant idea , i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Following the iteratively optimization method in [6], the GSC algorithm can learn the graph regularized sparse codes and the learning dictionary iteratively.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 14, "context": "\uf06c Constrained Nonnegative Matrix Factorization (CNMF) [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "\uf06c The Local and Global Consistency method (LGC) [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "\uf06c Greedy Max\u2013Cut (GMC) [1].", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "\uf06c Graph regularized Nonnegative Matrix Factorization (GNMF)[4] +K\u2013means.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "\uf06c Graph regularized Sparse Coding (GSC) [6]+ K\u2013means.", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "For other methods, we use Accuracy Metric (AC) [14] to evaluate the clustering performance.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 3, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 5, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 14, "context": "42 CNMF [15] 55.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "69 LGC [5] 41.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "54 GMC [1] 45.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "43 GNMF [4] 48.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "65 LGNMF [15] 48.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "99 GSC [6] 51.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "ORL, k=5 Yale, k=5 FEI, k=5 Number of labeled samples of each subject 2 5 8 2 5 8 2 5 8 10 Test Runs 100 100 100 100 100 100 100 100 100 100 CNMF [15] 74.", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "65 LGC [5] 75.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "53 GMC [1] 81.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "69 LGNMF [15] 67.", "startOffset": 9, "endOffset": 13}], "year": 2015, "abstractText": "We consider the general problem of utilizing both labeled and unlabeled data to improve data representation performance. A new semi-supervised learning framework is proposed by combing manifold regularization and data representation methods such as Non negative matrix factorization and sparse coding. We adopt unsupervised data representation methods as the learning machines because they do not depend on the labeled data, which can improve machine\u2019s generation ability as much as possible. The proposed framework forms the Laplacian regularizer through learning the affinity graph. We incorporate the new Laplacian regularizer into the unsupervised data representation to smooth the low dimensional representation of data and make use of label information. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results compared with state-of-art methods.", "creator": "Microsoft\u00ae Word 2010"}}}