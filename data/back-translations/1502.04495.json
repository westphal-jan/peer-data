{"id": "1502.04495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "A Generalization of Gustafson-Kessel Algorithm Using a New Constraint Parameter", "abstract": "This paper introduces a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters, which can be considered a generalization of the Gustafson Kessel algorithm for fuzzy clustering.", "histories": [["v1", "Mon, 16 Feb 2015 11:09:52 GMT  (444kb)", "http://arxiv.org/abs/1502.04495v1", "Proceedings of the Joint 4th Conference of the European Society for Fuzzy Logic and Technology and the 11th Rencontres Francophones sur la Logique Floue et ses Applications, pp. 1250-1255, Barcelona, Spain, September 7-9, 2005"]], "COMMENTS": "Proceedings of the Joint 4th Conference of the European Society for Fuzzy Logic and Technology and the 11th Rencontres Francophones sur la Logique Floue et ses Applications, pp. 1250-1255, Barcelona, Spain, September 7-9, 2005", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vasile patrascu"], "accepted": false, "id": "1502.04495"}, "pdf": {"name": "1502.04495.pdf", "metadata": {"source": "CRF", "title": "A Generalization of Gustafson-Kessel Algorithm Using a New Constraint Parameter", "authors": ["Vasile Patrascu"], "emails": ["vpatrascu@tarom.ro,", "vpatrascu@caramail.com"], "sections": [{"heading": null, "text": "In this paper one presents a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters. This algorithm can be considered a generalization of the Gustafson-Kessel algorithm for fuzzy clustering.\nKeywords: fuzzy clustering, GustafsonKessel algorithm, dissimilarity function, cluster density, cluster volume.\n1 Introduction\nThe Gustafson-Kessel [4] algorithm is an important algorithm for fuzzy clustering. Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3]. One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].\nIn this paper one presents an algorithm that can eliminate this insufficiency. The algorithm presented in this paper can be considered as a generalization of the Gustafson-Kessel algorithm.\nFurther the paper has the following structure: section 2 does a short presentation of the GustafsonKessel algorithm; section 3 presents the new algorithm; section 4 presents experimental results; section 5 presents some conclusions.\n2 The Gustafson-Kessel Algorithm\nThe Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2]. Let there be the objective function:\n)()(),( 11 ijij\nc\nj\nN\ni xDwMWJ \u03b1 == \u2211\u2211= (1)\nwhere { } ,1[| NiRxX ki \u2208\u2208= ] is a set containing unlabelled vectors; N\n{ } cjRm M kj ],1[| \u2208\u2208= is the set of centers of clusters; is the ][ ijwW = cN \u00d7 fuzzy c-partition matrix, containing the membership values of all in all clusters; is a dissimilarity measure between the vector and the center of a specific cluster defined by: ix )( ij xD ix\njm j\n( ) 2 1\n)det()( ijkjjij dCxD \u22c5\u22c5= \u03bb (2)\nwhere is the fuzzy covariance matrix of the cluster defined by: jC j\n( ) ( )\n\u03b1\n\u03b1\n)(\n)(\n1\n1\nij\nN\ni\nN\ni\nT jijiij\nj\nw\nmxmxw C\n\u2211\n\u2211\n=\n= \u2212\u22c5\u2212\u22c5\n= (3)\nijd is the Mahalanobis distance\n)()( 12 jij T jiij mxCmxd \u2212\u22c5\u22c5\u2212= \u2212 (4)\nand c\u03bb\u03bb\u03bb ,...,, 21 are positive constants. Usually c 1=j\u03bb ; ),1( \u221e\u2208\u03b1 is a control parameter of\nfuzziness. Usually 2=\u03b1 . The clustering problem can be defined as the minimization of under the following constraint: J\n1 1 =\u2211 =\nc\nj ijw (5)\nThe Gustafson-Kessel algorithm consists in the iteration of the following formulae:\n\u03b1\n\u03b1\n)(\n)(\n1\n1\nij\nN\ni\niij\nN\ni j\nw\nxw m\n\u2211\n\u2211\n=\n= \u22c5\n= (6)\nand\n\u2211 =\n\u2212\n\u239f\u239f \u23a0 \u239e \u239c\u239c \u239d \u239b\n= c\nt it\nij\nij\nxD xD\nw\n1\n1 1\n)( )(\n1\n\u03b1\n(7)\nThe major drawback is that Gustafson-Kessel algorithm is restricted to constant volume clusters due to the fixed a priori values j\u03bb .\n3 The New Fuzzy Clustering Algorithm\n3.1 The 3-parameter Dissimilarity Function\nLet there be the vector set where and let c be the number of clusters that\nmust be obtained. The cluster will be described by the parameters: the cluster center, the fuzzy covariance matrix, a parameter that shows how large or how small is the cluster in comparison to the other clusters, the fuzzy membership degree of the element . One denotes:\n{ }NxxxX ,...,, 21= k\ni Rx \u2208 j\njm jC\njf j\nijw\nix\n\u2211 =\n\u03b1= N\ni ijj wn 1 )( (8)\n)det( jj CV = (9)\nj\nj j V\nn =\u03c1 (10)\nj is very close to the cluster cardinality, can be considered as a measure for cluster volume and n jV\nj\u03c1 a measure for cluster density. In the framework of this algorithm the dissimilarity function of the element\nto the cluster is defined by the relation: ix j\n2\n2 k\u239e\nij j ij df D \u22c5\u239f \u239f \u23a0 \u239c \u239c \u239d\n= (11) j V\u239b\nwhere the distance is defined by relation (4).\nolve the problem there will be used the following objective function:\nNc\nijd\nIn order to s\n)()(),,( 1 ij i ij j xDwFMWJ \u2211\u2211 ==\n\u22c5= \u03b1 (12)\nwhere\n1\n{ } cjf F j ],1[|)1,0( \u2208\u2208= and M , W have by the Gustafson-Kess\nrs ve constraint:\n=j (13)\nIn addition here ar constraints for the membership degrees\nthe same definition used el algorithm. The paramete rify the following jf\n1=\u2211 j c\nf 1\nto this, t e the well-known ijw .\nNi ,...,2,1=\u2200 1 1 =\u2211 =\nter , due\ning formulae that are similar to those\nO ective Function J\nIn this section we will prove the following equivalent form for the objective function\nc\nj ijw (14)\nIn this paper there will be presented only the determination of formulae for the parame jf to the fact that the parameters jm and ijw are computed us used by the Gustafson-Kessel algorithm.\n3.2 An Equivalent Form for the bj\nJ .\nj kj c nk V J \u22c5\u22c5\u239f \u239e \u239c \u239b = \u2211 2\n(15) jj f \u239f\u23a0\u239c\u239d=1\nThis form will be used in the s this it is necessary to demonstrate the following\n=1\nection 3.3. To prove\nrelation:\njijij N nkdw \u22c5=\u22c5\u2211 2)( \u03b1 (16) i\nLet there be the or diagonalizes the covariance\nthe eigenvalues of the ma .\n\u22c5\u22c5= \u2212\u2212 11 (18)\njj CHS \u22c5\u22c5= (19)\nis the following diagonal matrix:\njH thogonal matrix that matrix jC and\n22 2 2 1 ,...,, jkjj \u03c3\u03c3\u03c3 trix jC\nThere will be: T jj HH = \u22121 (17)\njj T jj HSHC\nT jj H\nwhere jS\n( )22221 ,...,, jkjjj diagS \u03c3\u03c3\u03c3=\n(20)\nDenoting:\niji xHy \u22c5= (21)\njjj mH \u22c5=\u00b5 (22)\nit results:\n)( jijji mxHy \u2212\u22c5=\u2212 \u00b5 (23)\nFrom (4), (18) and (23) it results:\n(24)\nor\n)()( 12 jij T jiij ySyd \u00b5\u00b5 \u2212\u22c5\u22c5\u2212= \u2212\n( ) 2 2\n1 jtt \u03c3 2 jtit\nk ij y d \u00b5\u2212\n= \u2211 =\n(25)\nFrom (19) and (3) it results:\n\u03b1\n\u03b1\n)(\n))(()(\n1i=\n1\nij\nN\nN\ni\nT j T jijijij\nj\nw\nHmxmxHw S\n\u2211\n\u2211 = \u2212\u2212\u22c5 = (26)\nUsing (23) the relation (26) becomes:\n\u03b1\n\u03b1 \u00b5\u00b5\n)(\n)Tj()()( 1\nij\nN\nN\ni ijiij\nj\nw\nyyw S\n\u2211\n\u2211 = \u2212\u22c5\u2212\u22c5 = (27)\nFrom (27) and (20) it results for\n1i=\nkt ,...,2,1=\n\u03b1\n\u03b1 \u00b5 \u03c3\n)(\n)()(\n1\n2\n12\nij\nN\ni\njtitij N w\u2211\ni jt\nw\ny\n\u2211 =\n= \u2212\u22c5\n= (28)\nUsing (8) relation (28) becomes:\nj jt n=2\u03c3 (29)\njtitij\nN\ni yw \u2212\u22c5\u2211 =\n2 1 )()( \u00b5\u03b1\nFrom (25) it results:\n\u2211 \u2211\u2211 == \u239f\n\u239f \u239e\n\u239c \u239c \u239d\n\u239b \u2212\u22c5 \u22c5 k\nt\nN jtitij N\ni ij\nyw dw\n1 2\n2\n1\n2 )()()( \u03c3 \u00b5\u03b1\u03b1 (30) = \u23a0 = i jt ij 1\nUsing (29) relation (30) becomes:\nt demonstrated.\n3.3 The Calculus of the Parameters f\nIn this section it i parameters . There will be considered the\nj\nN\ni ijij nkdw \u22c5=\u22c5\u2211 =1\n2)( \u03b1 (31)\nnamely he relation (16) that might be\ns presented the determination of the jf\nobjective function:\n\u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b \u2212+\u22c5\u22c5\u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b = \u2211\u2211 == 1 1\n2\n1 j\nc\nj j\nk\nj\nj c\nj fnk f V J \u03c9 (32)\nwhere \u03c9 is the Lagrange multiplier.\nWe must solve the equation:\n0= \u2202 \u2202 jf J\n2) and (33) it results:\n(33)\nFrom (3\n012 2\n21 =+\u22c5\u22c5\u22c5\u2212 + \u03c9kjj\nk j\nVkn\nf k\n(34)\nFrom (34) it results:\n( ) 2 1\n222 ++ \u22c5\u22c5\u239f \u23a0 \u239e \u239c \u239d \u239b= kj k j\nk k\nj Vnf \u03c9 (35)\naking into account the relations (13) and (35) it results: T\n( ) 2 1 2\n1\n2 12 + =\u239f \u239e \u239c \u239b k\nk\n+\n= \u22c5\n\u23a0\u239d \u2211 ktkt c\nt Vn\n\u03c9 (36)\nand\n( )\n( ) 2 1 2 +\n= k c jf\n1= \u22c5\u2211 ktt t Vn\nFrom (10) and (37) it results the follo equivalent formula f\n2 1\n2 +\u22c5 kj k j Vn\n(37)\nwing or the parameter : jf\ntt\nc\nt\njj j\nV\nV f\nk k\nk k\n\u22c5\n\u22c5 =\n+\n+\n\u2211 =\n2\n2\n1 \u03c1\n\u03c1 (38)\nTh\nalgorithm, namely:\n3.4 The Calculus of the Parameters m\ne calculus formulae for the cluster centers jm are similar to those used by the Gustafson-Kessel\n\u03b1\n\u03b1\n)(\n)(\n1i \u2211 =\n1\nij\nN\niij\nN\ni j\nw\nxw m \u2211 = \u22c5 = (39)\n3.5 The Calculus of the Parameter w\nThe calculus formulae for the membership degrees the Gustafson-\ns\nijw are similar to those used by Kessel algorithm, namely:\n\u2211 =\n\u2212\u03b1\n\u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b\n= c\nt it\nij\nij\nxD\nxD w\n1\n1 1\n)(\n)(\n1 (40)\nBut from (38) it results:\nt k\nk\nj\nt c\ntj\nj V f V \u22c5\u239f\n\u239f \u23a0 \u239e \u239c \u239c \u239d \u239b = + = \u2211 2 1 \u03c1 \u03c1\n2\n2\n1\n2 ij\nk c\nt t\nk k\nj\nt ij dVD \u22c5 \u239f\u239f \u239f \u239f\n\u23a0\n\u239e\n\u239c\u239c \u239c \u239c \u239d\n\u239b\n\u22c5\u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b = \u2211 = + \u03c1 \u03c1 (42)\nFrom (40) and (42) one can obtain the following equivalent formulae for ijw .\n1\n1\n2 1\n12 2 \u2212\n= \u239f \u239f \u239f\n\u23a0\n\u239e\n\u239c\u239c \u239c \u239d\n\u239b \u22c5\n\u23a0\u239d=\n+\u2211 \u03b1\n\u03c1 it t\nc\nt\nij ij\nd\nw\nk\n(43)\n4 Experimental Results\nOne presents the using of the new algorithm for the two test sets shown in the figures 1 and 5. The first set was obtained by a random generation of some points inside two ellipses (figure 1). The second set was obtained by a random generating of some points inside three ellipses (figure 5). The obtained results using the new algorithm can be seen in the figures 3 and 7 respectively, those obtained using the Gustafson-Kessel algorithm in the figures 2 and 6, and those obtained using Gath-Geva alg\n1 1\n2\n2\n\u239f\u239c d\norithm in figures 4 and 8. One can see that while the lgorithm has generated clusters having approximately the same size ( figures 2 and\non-Kessel algorithm. There has been used a dissimilarity function having three parameters and a new constraint. In this way there it was eliminated one of the main insufficiency of the Gustafson-Kessel algorithm regarding to the obtaining of some clusters having very different sizes. The experimental results stand as proof of the new algorithm superiority in comparison to the Gustafson-Kessel one.\n12 \u2212 \u239f \u239f \u239e \u239c \u239c \u239b \u22c5 + \u03b1 \u03c1 j k\nGustafson-Kessel a\n6), the new algorithm (figures 3 and 7) has generated clusters that are very close to the ideal clustering (figures 1 and 5). The results obtained using the proposed algorithm are very close to those obtained using the Gath-Geva algorithm.\n5 Conclusions\nThis paper has presented an algorithm that can be considered a generalization of Gustafs\n(41)\nUsing (41) relation (11) becomes:\nclustering.\nclustering.\nReferences\n[1] J. C. Bezdek, Fuzzy mathematics in pattern classification, Ph.D. dissertation, Cornell Univ., Ithaca, NY, 1973. [2] J. C. Bezdek, Pattern recognition with fuzzy objective function algorithms, New York: Plenum Press, 1981. [3] I. I. Gath and A. B. Geva, \u201cUnsupervised optimal fuzzy clustering\u201d, IEEE Trans. Pattern And Machine Intell., Vol. 2, no. 7, pp. 773-781, 1989. [4] E. E. Gustafson and W. C. Kessel, \u201cFuzzy clustering with a fuzzy covariance matrix\u201d, Proc. IEEE CDC, San-Diego, CA, pp. 761-766, 1979.\n[5] R. A. Keller and F. Klawonn, \u201cAdaptation of\ncluster sizes in objective function based fuzzy clustering\u201d, In T.Leondes, editor. Intelligent Systems: Techniques and Applications \u2013 Database and Learning Systems, vol. 4, pp. 181-191. CRC Press, 2002. [6] R. Krishnapuram and J. Kim, \u201cClustering algorithms based on volume criteria\u201d. IEEE Transactions on Fuzzy Systems, 8(2), pp. 228- 236, 2000. [7] M. Setnes and U. Kaymak, \u201cExtended fuzzy cmeans with volume prototypes and cluster merging\u201d, In Proc. Of 6th European Congress on Intelligent Techniques and Soft Computing, pp. 1360-1364, 1998."}], "references": [{"title": "Fuzzy mathematics in pattern classification, Ph.D", "author": ["J.C. Bezdek"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1973}, {"title": "Pattern recognition with fuzzy objective function algorithms", "author": ["J.C. Bezdek"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1981}, {"title": "Unsupervised optimal fuzzy clustering", "author": ["I.I. Gath", "A.B. Geva"], "venue": "IEEE Trans. Pattern And Machine Intell.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Fuzzy clustering with a fuzzy covariance matrix", "author": ["E.E. Gustafson", "W.C. Kessel"], "venue": "Proc. IEEE CDC, San-Diego,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1979}, {"title": "Adaptation of cluster sizes in objective function based fuzzy clustering", "author": ["R.A. Keller", "F. Klawonn"], "venue": "In T.Leondes, editor. Intelligent Systems: Techniques and Applications \u2013 Database and Learning Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Clustering algorithms based on volume criteria", "author": ["R. Krishnapuram", "J. Kim"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Extended fuzzy cmeans with volume prototypes and cluster merging", "author": ["M. Setnes", "U. Kaymak"], "venue": "In Proc. Of 6 European Congress on Intelligent Techniques and Soft Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}], "referenceMentions": [{"referenceID": 3, "context": "1 Introduction The Gustafson-Kessel [4] algorithm is an important algorithm for fuzzy clustering.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 4, "context": "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].", "startOffset": 144, "endOffset": 150}, {"referenceID": 1, "context": "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].", "startOffset": 144, "endOffset": 150}], "year": 2005, "abstractText": "In this paper one presents a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters. This algorithm can be considered a generalization of the Gustafson-Kessel algorithm for fuzzy clustering.", "creator": "Acrobat PDFMaker 6.0 for Word"}}}