{"id": "1502.05957", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Web Similarity", "abstract": "Normalized Web Distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or another large electronic database, such as Wikipedia, and a search engine that provides reliable aggregate numbers of pages. For search terms, the NWD indicates a similarity on a scale from 0 (identical) to 1 (completely different). NWD approaches similarity according to all (upper) calculable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov's complexity.", "histories": [["v1", "Fri, 20 Feb 2015 17:55:58 GMT  (205kb,D)", "http://arxiv.org/abs/1502.05957v1", "LaTeX 25 pages, 3 tables. A precursor isarXiv:1308.3177"]], "COMMENTS": "LaTeX 25 pages, 3 tables. A precursor isarXiv:1308.3177", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.CV", "authors": ["andrew r cohen", "paul m b vitanyi"], "accepted": false, "id": "1502.05957"}, "pdf": {"name": "1502.05957.pdf", "metadata": {"source": "CRF", "title": "Web Similarity", "authors": ["Andrew R. Cohen", "M.B. Vit\u00e1nyi"], "emails": ["acohen@coe.drexel.edu", "Paul.Vitanyi@cwi.nl."], "sections": [{"heading": null, "text": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or any other large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity according to all (upper semi)computable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov complexity.\nIndex Terms\u2014 Normalized web distance, pattern recognition, data mining, similarity, classification,\nKolmogorov complexity,\nI. INTRODUCTION\nCommonly objects are computer files that carry all their properties in themselves. However, there are also objects that are given by name, such as \u2018red,\u2019 \u2018three,\u2019 \u2018Einstein,\u2019 or \u2018chair.\u2019 Such objects acquire their meaning from the common knowledge of mankind. We can give objects either as the object itself or as the name of that object, such as the literal text of the work \u201cMacbeth by Shakespeare\u201d or the name \u201cMacbeth by Shakespeare.\u201d We focus on the name case using the background information provided by the World Wide Web, or another data base such as Wikipedia, and a search engine that produces reliable aggregate page counts. The frequencies involved enable us to compute a distance for each set of names. The normalized form of this distance expresses similarity, that is, the search engine discovers the \u201cmeaning\u201d names have in common. Insofar as the meaning of names on the data base as discovered by this process approximates the meaning of those names in human society, the above distance expresses the\nAndrew Cohen is with the Department of Electrical and Computer Engineering, Drexel University. Address: A.R. Cohen, 3120\u201340 Market Street, Suite 313, Philadelphia, PA 19104, USA. Email: acohen@coe.drexel.edu\nPaul Vita\u0301nyi is with the national research center for mathematics and computer science in the Netherlands (CWI), and the University of Amsterdam. Address: CWI, Science Park 123, 1098XG Amsterdam, The Netherlands. Email: Paul.Vitanyi@cwi.nl.\nFebruary 23, 2015 DRAFT\nar X\niv :1\n50 2.\n05 95\n7v 1\n[ cs\n.I R\n] 2\n0 Fe\nb 20\n15\n2 common semantics of the names. The term \u201cname\u201d is used here synonymously with \u201cword\u201d \u201csearch term\u201d or \u201cquery.\u201d The normalized distance above is called the normalized web distance (NWD). We apply it in classification.\nExample I.1. Although Google gives notoriously unreliable counts it serves well enough for an example. On our scale of similarity, if NWD(X) = 0 then the search terms in the set X are identical, and if NWD(X) = 1 then the search terms in X are as different as can be. On 19 August 2014 searching for \u201cShakespeare\u201d gave 124,000,000 hits; searching for \u201cMacbeth\u201d gave 22,400,000 hits; searching for \u201cHamlet\u201d gave 51,300,000 hits; searching for \u201cShakespeare Macbeth\u201d gave 7,730,000 hits; searching for \u201cShakespeare Hamlet\u201d gave 18,500,000 hits; and searching for \u201cShakespeare Macbeth Hamlet\u201d gave 663,000 hits. The number of web pages returned by Google was estimated by searching for \u201cthe\u201d as 25,270,000,000. By (II.3) we have NWD({Shakespeare,Macbeth}) \u2248 0.395, NWD({Shakespeare,Hamlet}) \u2248 0.306 and NWD({Shakespeare,Macbeth,Hamlet}) \u2248 0.372. We conclude that Shakespeare and Hamlet have a lot in common, Shakespeare and Macbeth have a lot in common, and the commonality of Shakespeare, Hamlet, and Macbeth is intermediate between the two. \u2666\nTo develop the theory behind the NWD we consider the information in individual objects. These objects are finite and expressed as finite binary strings. The classic notion of Kolmogorov complexity [8] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [1]. There arises the question of the shared information between many objects instead of just a pair of objects."}, {"heading": "A. Related Work", "text": "The similarity or relative semantics between pairs of search terms was defined in [5] and demonstrated in practice by using the World Wide Web as database and Google as search engine. The proposed normalized Google distance (NGD) works for any search engine that gives an aggregate page count for search terms. See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.\nIn [11] the notion is introduced of the information required to go from any object in a finite multiset (a set where a member can occur more than once) of objects to any other object in the set. Let X denote a finite multiset of n finite binary strings defined by {x1, . . . , xn}, the constituting\nFebruary 23, 2015 DRAFT\n3 elements ordered length-increasing lexicographic. The information distance in X is defined by Emax(X) = min{|p| : U(xi, p, j) = xj for all xi, xj \u2208 X}. For instance, with X = {x, y} the quantity Emax(X) is the least number of bits in a program to transform x to y and y to x. In [18] the mathematical theory is developed further and the difficulty of normalization is shown."}, {"heading": "B. Results", "text": "The NWD is a similarity (a common semantics) between all search terms in a set. (We use set rather than multiset since a set is more appropriate in the context of search terms.) It can be thought of as a diameter of the set. For sets of cardinality two this diameter reduces to a distance between the two elements of the set. The NWD can be used for the classification of an unseen item into one of several classes (sets of names or phrases). This is simpler and computationally much easier that constructing the classes from the pairwise distances. In the latter solution inevitably information gets lost.\nThe basic concepts like the web events, web distribution, and web code are given in Section II. We determine the length of a single shortest binary program to compute from any web event of a single member in a set to the web event associated with the whole set (Theorem II.5). The mentioned length is an absolute information distance associated with the set. It is incomputable (Lemma II.4). However, for different sets it can be large while a set has similar members and small when a (different) set has dissimilar members. Therefore we normalize on a scale from 0 to 1 to express the information distance or similarity between members of the set. We approximate the incomputable normalized version with the computable NWD (Definition II.6). In Section III we present properties of the NWD such as the range from 0 to 1 (Lemma III.1), whether and how it changes under adding members (Lemma III.2), and that it does not satisfy the triangle inequality and hence is not metric (Lemma III.5). Theorem III.7 and Corollary III.8 show that the NWD approximates the common similarity of the queries in a set of search terms (that is, a common semantics). We subsequently apply the NWD to various data sets based on search results from Amazon, Wikipedia and the National Center for Biotechnology Information (NCBI) website from the U.S. National Institutes of Health in Section IV. We treat strings and self-delimiting strings in Appendix A, computability notions in Appendix B, Kolmogorov complexity in Appendix C, and metric of sets in Appendix D. The proofs are deferred to Appendix E.\nFebruary 23, 2015 DRAFT\n4"}, {"heading": "II. WEB DISTRIBUTION AND WEB CODE", "text": "We give a derivation that holds for idealized search engines that return reliable aggregate page counts from their data bases (here called the web consisting of web pages). Subsequently we apply the idealized theory to real problems using real search engines on real data bases."}, {"heading": "A. Web Event", "text": "The set of singleton search terms is denoted by S, a set of search terms is X = {x1, . . . , xn} with xi \u2208 S for 1 \u2264 i \u2264 n < \u221e, and X denotes the set of such X . Let the set of web pages indexed (possible of being returned) by the search engine be \u2126.\nDefinition II.1. We define the web event e(X) \u2286 \u2126 by the set of web pages returned by the search engine doing a search for X such that each web page in the set contains occurrences of all elements from X .\nIf x, y \u2208 S and e(x) = e(y) then x \u223c y and the equivalence class [x] = {y \u2208 S : y \u223c x}. Unless otherwise stated, we consider all singleton search terms that define the same web event as the same term. Hence we deal actually with equivalence classes [x] rather than x. However, for ease of notation we write x in the sequel and consider this to mean [x].\nIf X = {x1, . . . , xn}, then e(X) = e(x1) \u22c2 \u00b7 \u00b7 \u00b7 \u22c2 e(xn) and the frequency f(X) = |e(X)|. The web event e(X) embodies all direct context in which all elements from X simultaneously occur in these web pages. Therefore web events capture in the outlined sense all background knowledge about this combination of search terms on the web."}, {"heading": "B. The Web Code", "text": "It is natural to consider code words for events. We base those code words on the probability\nof the event. Consider the set\nTw,s = {(w, s) : w \u2208 \u2126, s \u2208 S, s occurs in w}.\nThen \u03b1 = \u2211\nw\u2208\u2126, s\u2208S |Tw,s|/|\u2126| is the average number of search terms per web page in \u2126. Define\nthe probability g(X) of X as g(X) = f(X)/N with N = \u03b1|\u2126|. This probability may change over time, but let us imagine that the probability holds in the sense of an instantaneous snapshot.\nFebruary 23, 2015 DRAFT\n5 A probability mass function on a known set allows us to define the associated prefix-code word length (information content) equal to unique decodable code word length [9], [13]. Such a prefix code is a code such that no code word is a proper prefix of any other code word. By the ubiquitous Kraft inequality [9], if l1, l2, . . . is a sequence of positive integers satisfying\u2211 i 2\u2212li \u2264 1, (II.1)\nthen there is a set of prefix-code words of length l1, l2, . . . . Conversely, if there is a set of prefix-code words of length l1, l2, . . . then these lengths satisfy the above displayed equation. By the fact that the probabilities of a discrete set sum to at most 1, every web event e(X) having probability g(X) can be encoded in a prefix-code word.\nDefinition II.2. The length G(X) of the web code word for X \u2208 X is\nG(X) = log 1/g(X), (II.2)\nor \u221e for g(X) = 0. The case |X| = 1 gives the length of the web code word for singleton search terms. The logarithms are throughout base 2.\nThe web code is a prefix code. The code word associated with X and therefore with the web event e(X) can be viewed as a compressed version of the set of web pages constituting e(X). That is, the search engine compresses the set of web pages that contain all elements from X into a code word of length G(X).\nDefinition II.3. Let p \u2208 {0, 1}\u2217 and X \u2208 X \\ S. The information EGmax(X) to compute event e(X) from event e(x) for any x \u2208 X is defined by EGmax(X) = minp{|p| : for all x \u2208 X we have U(e(x), p) = e(X)}.\nIn this way EGmax(X) corresponds to the length of a single shortest self-delimiting program to compute output e(X) from an input e(x) for all x \u2208 X . We use the notion of prefix Kolmogorov complexity K as in Appendix C.\nLemma II.4. The function EGmax is upper semicomputable but not computable.\nTheorem II.5. EGmax(X) = maxx\u2208X{K(e(X)|e(x))} up to an additive logarithmic term\nFebruary 23, 2015 DRAFT\n6 O(log maxx\u2208X{K(e(X)|e(x))}) which we ignore in the sequel.\nTo obtain the NWD we must normalize EGmax. Let us give some intuition first. Suppose X, Y \u2208 X \\S. If the web events e(x)\u2019s are more or less the same for all x \u2208 X then we consider the members of X very similar to each other. If the web events e(y)\u2019s are very different for different y \u2208 Y then we consider the members of Y to be very different from one another. Yet for certain X and Y depending on the cardinalities and the size of the web events of the members we can have EGmax(X) = EGmax(Y ). That is to say, the similarity is dependent on size. Therefore, to express similarity of the elements in a set X we need to normalize EGmax(X) using the cardinality of X and the events of its members. Expressing the normalized values on a scale of 0 to 1 allows us to express the degree in which all elements of a set are alike. Then we can compare truly different sets.\nUse the symmetry of information law (A.1) to rewrite EGmax(X) according to Theorem II.5 as K(e(X)) \u2212 minx\u2208X{K(e(x))} up to a logarithmic additive term which we ignore. Since G(X) is computable prefix code for e(X), while K(e(X)) is the shortest computable prefix code for e(X), it follows that K(e(X)) \u2264 G(X). Similarly K(e(x)) \u2264 G(x) for x \u2208 X . The search engine G returns frequency f(X) on query X (respectively frequency f(x) on query x). These frequencies are readily converted into G(X) (respectively G(x)) using (II.2). Replace K(e(X)) by G(X) and minx\u2208X{K(e(x))} by minx\u2208X{G(x)} in EGmax(X). Subsequently use as normalizing term maxx\u2208X{G(x)}(|X| \u2212 1). This yields the following.\nDefinition II.6. The normalized web distance (NWD) of X \u2208 X with G(X) <\u221e (equivalently f(X) > 0)) is\nNWD(X) = G(X)\u2212minx\u2208X{G(x)} maxx\u2208X{G(x)}(|X| \u2212 1)\n(II.3)\n= maxx\u2208X{log f(x)} \u2212 log f(X)\n(logN \u2212minx\u2208X{log f(x)})(|X| \u2212 1) ,\notherwise NWD(X) is undefined.\nThe second equality in (II.3), expressing the NWD in terms of frequencies, is seen as follows. We use (II.2). The numerator is rewritten by G(X) = log 1/g(X) = log(N/f(X)) = logN \u2212 log f(X) and minx\u2208X{G(x)} = minx\u2208X{log 1/g(x)} = logN \u2212 maxx\u2208X{log f(x)}.\nFebruary 23, 2015 DRAFT\n7 The denominator is rewritten as maxx\u2208X{G(x)}(|X| \u2212 1) = maxx\u2208X{log 1/g(x)}(|X| \u2212 1) = (logN \u2212minx\u2208X{log f(x)})(|X| \u2212 1).\nRemark II.7. By assumption f(X) > 0 which, since it has integer values, means f(X) \u2265 1. The case f(X) = 0 means that there is an x \u2208 X such that e(x) \u22c2 e(X \\ {x}) = \u2205. That is, query x is independent of the set of queries X \\ {x}, that is, x has nothing in common with X \\ {x} since there is no common web page. Hence the NWD is undefined. The other extreme is that e(x) = e(y) (x \u223c y) for all x, y \u2208 X . In this case the NWD(X) = 0. \u2666"}, {"heading": "III. THEORY", "text": "Let X = {x, y} \u2208 X . We can rewrite [5, Section 3.4 formula (6)] for the NGD distance between x and y as NWD(X) up to a constant. Hence the NGD and NWD coincide for pairs up to a constant. For arbitrary sets the following holds.\nLemma III.1. Let X \u2208 X \\ S. Then NWD(X) \u2208 [0, 1].\nWe determine bounds on how the NWD may change under addition of members to its argument. These bounds are necessary loose since the added members may be similar to existing ones or very different. In Lemma III.2 below we shall distinguish two cases for the relation between the minimum frequencies of members of X and Y with X \u2282 Y and the overall frequencies of X and Y . In the first case\nf(y1)f(X) f(x1)f(Y ) \u2265 ( f(x0) f(y0) )(|X|\u22121)NWD(X) , (III.1)\nwhere x0 = arg minx\u2208X{log f(x)}, y0 = arg miny\u2208Y {log f(y)}, x1 = arg maxx\u2208X{log f(x)}, and y1 = arg maxy\u2208Y {log f(y)}.\nWe give an example. Let |X| = 5, f(x0) = 1, 100, 000, f(y0) = 1, 000, 000, f(x1) = f(y1) = 2, 000, 000, f(X) = 500, f(Y ) = 100, and NWD(X) = 0.5. The righthand side of the inequality (III.1) is 1.12 = 1.21 while the lefthand side is 5. In the second case inequality (III.1) does not hold, that is, it holds with the \u2265 sign replaced by the < sign. We give an example. Let |X| = 5, f(x0) = 1, 100, 000, f(y0) = 1, 000, 000, f(x1) = f(y1) = 2, 000, 000, f(X) = 110, f(Y ) = 100, and NWD(X) = 0.5. The righthand side of the inequality (III.1) with \u2265 replaced by < is 1.12 = 1.21 while the lefthand side is 1.1.\nFebruary 23, 2015 DRAFT\n8 Lemma III.2. Let X,Z \u2286 Y , X, Y, Z \u2208 X \\ S, and minz\u2208Z{f(z)} = miny\u2208Y {f(y)}.\n(i) If f(y) \u2265 minx\u2208X{f(x)} for all y \u2208 Y then (|X| \u2212 1)NWD(X) \u2264 (|Y | \u2212 1)NWD(Y ). (ii) Let f(y) < minx\u2208X{f(x)} for some y \u2208 Y . If (III.1) holds then (|X| \u2212 1)NWD(X) \u2264 (|Y | \u2212 1)NWD(Y ). If (III.1) does not hold then (|X| \u2212 1)NWD(X) > (|Y | \u2212 1)NWD(Y ) \u2265 (|Z| \u2212 1)NWD(Z).\nExample III.3. Consider the Shakespeare\u2013Macbeth\u2013Hamlet Example I.1. Let X = {Shakespeare,Macbeth}, Y = {Shakespeare,Macbeth,Hamlet}, and Z = {Shakespeare,Hamlet}. Then inequality (III.1) for X versus Y gives (124, 000, 000\u00d7 7, 730, 000/(124, 000, 000\u00d7 663, 000) \u2265 (22, 400, 000/22, 400, 000)0.395 (that is 11.659 \u2265 1), and for Z versus Y gives 18, 500, 000/663, 000 \u2265 (51, 300, 000/22, 400, 000)0.306 (that is 27.903 \u2265 1.289). In the first case Lemma III.2 item (i) is applicable since the frequency minima of X and Y are the same. (In this case inequality (III.1) is not needed.) Therefore NWD(X)(|X| \u2212 1)/(|Y | \u2212 1) \u2264 NWD(Y ) which works out as 0.395/2 \u2264 0.372. In the second case Lemma III.2 item (ii) is applicable since the frequency minima of Z and Y are not the same. Since inequality (III.1) holds this gives NWD(Z)(|Z| \u2212 1)/(|Y | \u2212 1) \u2264 NWD(Y ) which works out as 0.306/2 \u2264 0.372. \u2666\nRemark III.4. To interpret Lemma III.2 we give the following intuition. Under addition of a member to a set there are two opposing tendencies on the NWD concerned. First, the range of the NWD stays fixed at a unit and (II.3) shows that addition of a member tends to decrease the NWD, that is, it moves closer to 0. Second, the common similarity of queries in a given set as measured by the NWD is based on the number of properties all members of a set have in common. By adding a member to the set clearly the number of common properties does not increase and generally decreases. This diminishing tends to cause the NWD to increase\u2014move closer to 1. The first effect is visible when (|X| \u2212 1)NWD(X) > (|Y | \u2212 1)NWD(Y ), which happens in the case of Lemma III.2 item (ii) for the case when the frequencies do not satisfy (A.2). The second effect is visible when (|X| \u2212 1)NWD(X) \u2264 (|Y | \u2212 1)NWD(Y ), which happens in Lemma III.2 item (i), and item (ii) with the frequencies satisfying (A.2). (Note that to keep NWD(X) \u2208 [0, 1] for all X we have the factor (|X| \u2212 1) in the denominator of NWD(X). Without this factor the resulting function of X has range [0, |X| \u2212 1] and in the inequalities in this remark and in the NWD formula (II.3) and all the previous theory properties\nFebruary 23, 2015 DRAFT\n9 the factors |X| \u2212 1 and |Y | \u2212 1 are replaced by 1.) \u2666\nFor every set X we have that the NWD(X) is invariant under permutation of X: it is symmetric. The NWD is also positive definite as in Appendix D (where equal members should be interpreted as saying that the set has only one member). However the NWD does not satisfy the triangle inequality and hence is not a metric. This is natural for a common similarity or semantics: The members of a set XY can be less similar (have greater NWD) then the similarity of the members of XZ plus the similarity of the members of ZY for some set Z.\nLemma III.5. The NWD violates the triangle inequality.\nIt remains to formally prove that the NWD expresses in the similarity of the search terms in the set. We define the notion of a distance on these sets using the web as side-information. We consider only distances that are upper semicomputable, that is, the distance can be computably approximated from above (Appendix B). A priori we allow asymmetric distances, but we exclude degenerate distances such as d(X) = 1/2 for all X \u2208 X containing a fixed element x. That is, for every d we want only finitely many sets X 3 x such that d(X) \u2264 d. Exactly how fast we want the number of sets we admit to go to \u221e is not important; it is only a matter of scaling.\nDefinition III.6. A web distance function (quantifying the common properties or common features) d : X \u2192 R+ is admissible if d(X) is (i) a nonnegative total real function and is 0 iff X \u2208 S; (ii) it is upper semicomputable from the e(x)\u2019s with x \u2208 X and e(X); and (iii) it satisfies the density requirement: for every x \u2208 S\u2211 X3x, |X|\u22652 2\u2212d(X) \u2264 1.\nWe give the gist of what we are about to prove. Let X = {x1, x2, . . . , xn}. A feature of a query is a property of the web event of that query. For example, the frequency in the web event of web pages containing an occurrence of the word \u201cred.\u201d We can compute this frequency for each e(xi) (1 \u2264 i \u2264 n). The minimum of those frequencies is the maximum of the number of web pages containing the word \u201cred\u201d which surely is contained in each web event e(x1), . . . , e(xn). One can identify this maximum with the inverse of a distance in X . There are many such distances in X . The shorter a web distance is, the more dominant is the feature it represents. We show\nFebruary 23, 2015 DRAFT\n10\nthat the minimum admissible distance is EGmax(X). It is the least admissible web distance and represents the shortest of all admissible web distances in members of X . Hence the closer the numerator of NWD(X) is to EGmax(X) the better it represents the dominant feature all members of X have in common.\nTheorem III.7. Let X \u2208 X . The function G(X)\u2212minx\u2208X{G(x)} is a computable upper bound on EGmax(X). The closer it is to EGmax(X), the better it approximates the shortest admissible distance in X . The normalized form of EGmax(X) is NWD(X).\nThe normalized least admissible distance in a set is the least admissible distance between its\nmembers which we call the common admissible similarity. Therefore we have:\nCorollary III.8. The function NWD(X) is the common admissible similarity among all search terms in X . This admissible similarity can be viewed as semantics that all search terms in X have in common."}, {"heading": "IV. APPLICATIONS", "text": "The application of the approach presented here requires the ability to query a database for the number of occurrences and co-occurrences of the elements in the set that we wish to analyze. One challenge is to find a database that has sufficient breadth as to contain a meaningful numbers of co-occurrences for related terms. As discussed previously, an example of one such database is the World Wide Web, with the page counts returned by Google search queries used as an estimate of co-occurrence frequency. There are two issues with using Google search page counts. The first issue is that Google limits the number of programmatic searches in a single day to a maximum of 100 queries, and charges for queries in excess of 100 at a rate of up to $50 per thousand. The second issue with using Google web search page counts is that the numbers are not exact, but are generated using an approximate algorithm that Google has not disclosed. For the questions considered previously [5] we found that these approximate measures were sufficient at that time to generate useful answers, especially in the absence of any a priori domain knowledge. It is possible to implement internet based searches without using search engine API\u2019s, and therefore not subject to daily limit. This can be accomplished by parsing the HTML returned by the search engine directly. The issue with google page counts in this study being approximate counts based\nFebruary 23, 2015 DRAFT\n11\non a non-public algorithm was more concerning as changes in the approximation algorithm can influence page count results in a way that may not reflect true changes to the underlying distributions. Since any internet search that returns a results count can be used in computing the NWD, we adopt the approach of using web sites that return exact rather than approximate page counts for a given query.\nHere we describe a comparison of the NWD using the set formulation based on web-site search result counts with the pairwise NWD formulation. The examples are based on search results from Amazon, Wikipedia and the National Center for Biotechnology Information (NCBI) website from the U.S. National Institutes of Health. The NCBI website exposes all of the NIH databases searchable from a single web portal. We consider example classification questions that involve partitioning a set of words into underlying categories. For the NCBI applications we compare various diseases using the loci identified by large genome wide association studies (GWAS). For the NWD set classification, we determine whether to assign element x to class A or class B by computing NWD(Ax)\u2212NWD(A) and NWD(Bx)\u2212NWD(B) and assigning element x to whichever class achieves the minimum.\nFor the pairwise formulation, we use the gap spectral clustering unsupervised approach developed in [4]. Gap spectral clustering uses the gap statistic as first proposed in [17] to estimate the number of clusters in a data set from an arbitrary clustering algorithm. In [4], it was shown that the gap statistic in conjunction with a spectral clustering [15] of the distance matrix obtained from pairwise NWD measurements is an estimate of randomness deficiency for clustering models. Randomness deficiency is a measure of the meaningful information that a model, here a clustering algorithm, captures in a particular set of data [12]. The approach is to select the number of clusters that minimizes the randomness deficiency as approximated by the gap value. In practice, this is achieved by picking the first number of clusters where the gap value achieves a maximum as described in [4].\nThe gap value is computed by comparing the intra-cluster dispersions of the pairwise NWD distance matrix to that of uniformly distributed randomly generated data on the same range. For each value of k, the number of clusters in the data, we apply a spectral clustering algorithm to partition the data, assigning each element in the data to one of k clusters. Next, we compute\nFebruary 23, 2015 DRAFT\n12\nDr, the sum of the distances between elements in each cluster Cr,\nDr = \u2211 i,j\u2208Cr di,j.\nThe average intra-cluster dispersion is calculated,\nWk = k\u2211\nr=1\n1\n2nr Dr,\nwhere nr is the number of points in cluster Cr. The gap statistic is then computed as the difference between the averages of the intra-cluster distances of our data and the intra-cluster distances of B randomly generated uniformly distributed data sets of the same dimension as our data,\nGap(k) = 1\nB B\u2211 b=1 log(Wkb)\u2212 log(Wk),\nwhere Wkb is the average intra-cluster dispersion obtained by running our clustering algorithm on each of the B randomly generated uniformly-distributed datasets. Following [4] we set B to 100. We compute the standard deviation of the gap value sk from \u03c3k, the standard deviation of the B uniformly distributed randomly generated data, adjusted to account for simulation error, as\nsk = \u03c3k \u221a 1 + 1/B.\nFinally, we choose the smallest value of k for which\nGap(k) \u2265 Gap(k + 1)\u2212 sk+1.\nWe now describe results from a number of sample applications. For all of these applications, we use a single implementation based on co-occurrence counts. For each search engine that we used, including Amazon, Wikipedia and NCBI a custom MATLAB script was developed to parse the search count results. We used the page counts returned using the built in search from each website for the frequencies, and following the approach in [5] choose N as the frequency for the search term \u2019the\u2019. The results described were not sensitive to the choice of search term used to establish N , for example identical classification results were obtained using the counts returned by the search term \u2019N\u2019 as the normalizing factor. Following each classification result below, we\nFebruary 23, 2015 DRAFT\n13\ninclude in parenthesis the 95% confidence interval for the result, computed as described in [19]\nThe first three classification questions we considered used the wikipedia search engine. These questions include classifying colors vs. animals, classifying colors vs. shapes and classifying presidential candidates by political party for the US 2008 U.S. presidential election. For colors vs animals and shapes, gap spectral clustering found two groups in the data and classified all of the elements 100% correctly. The NWD set formulation classified the terms perfectly (0.82,1.0). For the presidential candidate classification by party, the pairwise NWD formulation performed poorly, classifying 58% correctly (0.32,0.8), while the multiset formulation obtained 100% correct classification (0.76,1.0). Table I shows the data used for each question, together with the pairwise and set accuracy, the number of groups obtained by gap spectral clustering and the total number of website queries required for each method.\nThe next classification question considered used page counts returned by the Amazon website search engine to classify book titles by author. Table II summarizes the sets of novels associated with each author, and the classification results for each author as a confusion matrix. The Multiset NWD (top) misclassified one of the Tolstoy novels (\u2019War and Peace\u2019) to Stephen King, but\nFebruary 23, 2015 DRAFT\n14\nShakespeare = {Macbeth, The Tempest, Othello, King Lear, Hamlet, The Merchant of Venice, A Midsummer Nights Dream, Much Ado About Nothing, Taming of the Shrew, Twelfth Night} King = {Carrie, Salems Lot, The Shining, The Stand, The Dead Zone, Firestarter, Cujo} Twain = {Adventures of Huckleberry Finn, A Connecticut Yankee in King Arthurs Court, Life on the Mississippi, Puddnhead Wilson} Hemingway = {The Old Man and The Sea, The Sun Also Rises, For Whom the Bell Tolls, A Farewell To Arms} Tolstoy = {Anna Karenina, War and Peace, The Death of Ivan Ilyich}\ncorrectly classified all other novels correctly, 96% accurate (0.83,0.99). The pairwise NWD performed significantly more poorly, achieving only 79% accuracy (0.6,0.9).\nThe final application considered is to quantify similarities among diseases based on the results of genome wide association studies (GWAS). These studies scan the genomes from a large population of individuals to identify genetic variations occurring at fixed locations, or loci that can be associated with the given disease. Here we use the the NIH NCBI database to search for similarities among diseases, comparing loci identified by recent GWAS results for each disease. The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25]. The top of Table III lists the loci used for each disease. The middle panel of Table III shows at each\nFebruary 23, 2015 DRAFT\n15\nlocation (i, j) of the distance matrix the NWD computed for the combined counts for the loci of disease i concatenated with disease j. The diagonal elements (i, i) show the NWD for the loci of disease i. The bottom panel of Table III shows the NWD for each element with the diagonal subtracted, (i, j) \u2212 (i, i). This is equivalent to the NWD(Ax) \u2212 NWD(A) value used in the previous classification problems. The two minimum values in the bottom panel, showing the relationships between Parkinsons and Obesity, as well as between Schizophrenia and Leukemia were surprising. The hypothesis was that neurological disorders such as Parkinsons, ALS and Alzheimers, would be more similar to each other. After these findings we found that there actually have been recent findings of strong relationships between both Schizophrenia and Leukemia [29] as well as between Parkinsons and Obesity [30], relationships that have also been identified by clinical evidence not relating to GWAS approaches.\nFebruary 23, 2015 DRAFT"}, {"heading": "V. CONCLUSION", "text": "Consider queries to a search engine using a data base divided in chunks called web pages. On each query the search engine returns a set of web pages. We propose a method, the normalized web distance (NWD) for sets of queries that quantifies in a single number between 0 and 1 the way in which the queries in the set are similar: 0 means all queries in the set are the same (the set has cardinality one) and 1 means all queries in the set are maximally dissimilar to each other.\nFebruary 23, 2015 DRAFT\n17\nThe similarity among queries uses the frequency counts of web pages returned for each query and the set of queries. The method can be applied using any big data base and a search engine that returns reliable aggregate page counts. Since this method uses names for object, and not the objects themselves, we can view the common similarity of the names as a common semantics between those names (words or phrases). The common similarity between a finite nonempty set of queries can be viewed as a distance or diameter of this set. We show that this distance ranges in between 0 and 1, how it changes under adding members to the set, that it does not satisfy the triangle property, and that the NWD formally and provably expresses common similarity (common semantics).\nTo test the efficacy of the new method for classification we experimented with small data sets of queries based on search results from Wikipedia, Amazon, and the National Center for Biotechnology Information (NCBI) website from the U.S. National Institutes of Health. In particular we compared classification using pairwise NWDs with classification using set NWD. The last mentioned performed consistently equal or better, sometimes much better.\nAPPENDIX"}, {"heading": "A. Strings and the Self-Delimiting Property", "text": "We write string to mean a finite binary string, and denotes the empty string. (If the string is over a larger finite alphabet we recode it into binary.) The length of a string x (the number of bits in it) is denoted by |x|. Thus, | | = 0. The self-delimiting code for x of length n is x\u0304 = 1|x|0x of length 2n+ 1, or even shorter x\u2032 = 1x\u03040x of length n+ 2 log n+ 1 (see [12] for still shorter self-delimiting codes). Self-delimiting code words encode where they end. The advantage is that if many strings of varying lengths are encoded self-delimitingly using the same code, then their concatenation can be parsed in their constituent code words in one pass going from left to right. Self delimiting codes are computable prefix codes. A prefix code has the property that no code word is a proper prefix of any other code word. The code-word set is called prefix-free.\nWe identify strings with natural numbers by associating each string with its index in the length-increasing lexicographic ordering according to the scheme ( , 0), (0, 1), (1, 2), (00, 3), (01, 4), (10, 5), (11, 6), . . . . In this way the Kolmogorov complexity can be about finite binary strings or natural numbers.\nFebruary 23, 2015 DRAFT\n18"}, {"heading": "B. Computability Notions", "text": "A pair of integers such as (p, q) can be interpreted as the rational p/q. We assume the notion of a function with rational arguments and values. A function f(x) with x rational is upper semicomputable if it is defined by a rational-valued total computable function \u03c6(x, k) with x a rational number and k a nonnegative integer such that \u03c6(x, k + 1) \u2264 \u03c6(x, k) for every k and limk\u2192\u221e \u03c6(x, k) = f(x). This means that f can be computed from above (see [12], p. 35). A function f is lower semicomputable if \u2212f is semicomputable from above. If a function is both upper semicomputable and lower semicomputable then it is computable."}, {"heading": "C. Kolmogorov Complexity", "text": "The Kolmogorov complexity is the information in a single finite object [8]. Informally, the Kolmogorov complexity of a finite binary string is the length of the shortest string from which the original can be lossless reconstructed by an effective general-purpose computer such as a particular universal Turing machine. Hence it constitutes a lower bound on how far a lossless compression program can compress. For technical reasons we choose Turing machines with a separate read-only input tape that is scanned from left to right without backing up, a separate work tape on which the computation takes place, an auxiliary tape inscribed with the auxiliary information, and a separate output tape. All tapes are divided into squares and are semi-infinite. Initially, the input tape contains a semi-infinite binary string with one bit per square starting at the leftmost square, and all heads scan the leftmost squares on their tapes. Upon halting, the initial segment p of the input that has been scanned is called the input program and the contents of the output tape is called the output. By construction, the set of halting programs is prefix free (Appendix A), and this type of Turing machine is called a prefix Turing machine. A standard enumeration of prefix Turing machines T1, T2, . . . contains a universal machine U such that U(i, p, y) = Ti(p, y) for all indexes i, programs p, and auxiliary strings y. (Such universal machines are called \u201coptimal\u201d in contrast with universal machines like U \u2032 with U \u2032(i, pp, y) = Ti(p, y) for all i, p, y, and U \u2032(i, q, y) = 1 for q 6= pp for some p.) We call U the reference universal prefix Turing machine. This leads to the definition of prefix Kolmogorov complexity.\nFormally, the conditional prefix Kolmogorov complexity K(x|y) is the length of the shortest input z such that the reference universal prefix Turing machine U on input z with auxiliary information y outputs x. The unconditional Kolmogorov complexity K(x) is defined by K(x| )\nFebruary 23, 2015 DRAFT\n19\nwhere is the empty string. In these definitions both x and y can consist of strings into which finite sets of finite binary strings are encoded. Theory and applications are given in the textbook [12].\nFor a finite set of strings we assume that the strings are length-increasing lexicographic ordered. This allows us to assign a unique Kolmogorov complexity to a set. The conditional prefix Kolmogorov complexity K(X|x) of a set X given an element x is the length of a shortest program p for the reference universal Turing machine that with input x outputs the set X . The prefix Kolmogorov complexity K(X) of a set X is defined by K(X| ). One can also put set in the conditional such as K(x|X) or K(X|Y ). We will use the straightforward laws K(\u00b7|X, x) = K(\u00b7|X) and K(X|x) = K(X \u2032|x) up to an additive constant term, for x \u2208 X and X \u2032 equals the set X with the element x deleted.\nWe use the following notions from the theory of Kolmogorov complexity. The symmetry of\ninformation property [6] for strings x, y is\nK(x, y) = K(x) +K(y|x) = K(y) +K(x|y), (A.1)\nwith equalities up to an additive term O(log(K(x, y)))."}, {"heading": "D. Metricity", "text": "A distance function d on X is defined by d : X \u2192 R+ where R+ is the set of nonnegative real numbers. If X, Y, Z \u2208 X , then Z = XY if Z is the set consisting of the elements of the sets X and Y ordered length-increasing lexicographic. A distance function d is a metric if\n1) Positive definiteness: d(X) = 0 if all elements of X are equal and d(X) > 0 otherwise.\n(For sets equality of all members means |X| = 1.)\n2) Symmetry: d(X) is invariant under all permutations of X . 3) Triangle inequality: d(XY ) \u2264 d(XZ) + d(ZY )."}, {"heading": "E. Proofs", "text": "Proof: of Lemma II.4.\nWe can run all programs dovetailed fashion and at each time instant select a shortest program that with inputs e(x) for all x \u2208 X has terminated with the same output e(X).\nFebruary 23, 2015 DRAFT\n20\nThe lengths of these shortest programs gets shorter and shorter, and in for growing time eventually reaches EGmax(X) (but we do not know the time for which it does). Therefore EGmax(X) is upper semicomputable. It is not computable since for X = {x, y} we have EGmax(X) = max{K(e(x)|e(y)), K(e(y)|e(x))}+O(1), the information distance between e(x) and e(y) which is known to be incomputable [1].\nProof: of Theorem II.5.\n(\u2264) We use a modification of the proof of [11, Theorem 2]. According to Definition II.1 x = y iff e(x) = e(y). Let X = {x1, . . . , xn} and k = maxx\u2208X{K(e(X)|e(x)}. A set of cardinality n in S is for the purposes of this proof represented by an n-vector of which the entries consist of the lexicographic length-increasing sorted members of the set. For each 1 \u2264 i \u2264 n let Yi be the set of computably enumerated n-vectors Y = (y1, . . . , yn) with entries in S such that\nK(e(Y )|e(yi)) \u2264 k for each 1 \u2264 i \u2264 n. Define the set V = \u22c3n i=1 Yi. This V is the set of vertices of a graph G = (V,E). The set of edges E is defined by: two vertices u = (u1, . . . , un) and v = (v1, . . . , vn) are connected by an edge iff there is 1 \u2264 j \u2264 n such that uj = vj . There are at most 2k self-delimiting programs of length at most k computing from input e(uj) to different e(v)\u2019s with uj in vertex v as jth entry. Hence there can be at most 2k vertices v with uj as jth entry. Therefore, for every u \u2208 V and 1 \u2264 j \u2264 n there are at most 2k vertices v \u2208 V such that vj = uj . The vertex-degree of graph G is therefore bounded by n2k. Each graph can be vertex-colored by a number of colors equal to the maximal vertex-degree. This divides the set of\nvertices V into disjoint color classes V = V1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 VD with D \u2264 n2k. To compute e(X) from e(x) with x \u2208 X we only need the color class of which e(X) is a member and the position of x in n-vector X . Namely, by construction every vertex with the same element in the jth position is connected by an edge. Therefore there is at most a single vertex with x in the jth position in a color class. Let x be the jth entry of n-vector X . It suffices to have a program of length at most log(n2k) +O(log nk) = k +O(log nk) bits to compute e(X) from e(x). From n and k we can generate G and given log(n2k) bits we can identify the color class Vd of e(X). Using another log n bits we define the position of x in the n-vector X . To make such a program self-delimiting add a logarithmic term. In total k +O(log k) suffices since O(log k) = O(log n+ log nk).\n(\u2265) That EGmax(X) \u2265 maxx\u2208X{K(e(X)|e(x)} follows trivially from the definitions.\nProof: of Lemma III.1.\n(\u2265 0) Since f(X) \u2264 f(x) for all x \u2208 X the numerator of the right-hand side of (II.3) is\nFebruary 23, 2015 DRAFT\n21\nnonnegative. Since the denominator is also nonnegative we have NWD(X) \u2265 0. Example of the lower bound: if maxx\u2208X{log f(x)} = log f(X), then NWD(X) = 0.\n(\u2264 1) Intuitively the upper bound on g(X) is reached if the web events e(x) for x \u2208 X are mutually almost disjoint. We say \u201dalmost\u201d since if \u22c2\nx\u2208X e(x) = \u2205 then NWD(X) is undefined. Case 1 Let the web events e(x) satisfy | \u22c2 x\u2208X e(x)| = 1. Then g(X) = \u220f x\u2208X(g(x)\u22121/N) +\n1/N . By (II.2) therefore \u2211\nx\u2208X G(x)\u2212G(X) = for some very small positive .\nSubcase 1.a Let |e(x)| = |e(y)| for all x, y \u2208 X . Then G(X) \u2212 minx\u2208X{G(x)} = (X \u2212 1) maxx\u2208X{G(x)} \u2212 . By (II.3) we have NWD(X) = 1 \u2212 \u2032 where \u2032 = /((X \u2212 1) maxx\u2208X{G(x)}).\nSubcase 1.b Let |e(x)| 6= |e(y)| for some x, y \u2208 X . Then G(X) \u2212 minx\u2208X{G(x)} < (X \u2212\n1) maxx\u2208X{G(x)}. By (II.3) we have NWD(X) < 1\u2212 \u2032. Case 2 Let the web events e(x) satisfy | \u22c2 x\u2208X e(x)| > 1. Then g(X) > \u220f x\u2208X(g(x) \u2212\n1/N) + 1/N yielding \u2211\nx\u2208X G(x)\u2212G(X) < and therefore G(X)\u2212minx\u2208X{G(x)} < (X \u2212\n1) maxx\u2208X{G(x)}. By (II.3) we have NWD(X) < 1\u2212 \u2032.\nProof: of Lemma III.2.\n(i) Since X \u2286 Y and because of the condition of item (i) we have miny\u2208Y {log f(y)} = minx\u2208X{log f(x)}. From X \u2286 Y also follows maxy\u2208Y {log f(y)} \u2265 maxx\u2208X{log f(x)}, and log f(X) \u2265 log f(Y ). Therefore the numerator of NWD(Y ) is at least as great as that of NWD(X), and the denominator of NWD(Y ) equals (|Y |\u22121)/(|X|\u22121) times the denominator of NWD(X).\n(ii) We have minx\u2208Y log f(y) < minx\u2208X{log f(x)}. If NWD(X) = 1 then NWD(Y ) = 1 (in both cases there is no common similarity of the members of the set). Item (ii) follows vacuously in this case. Therefore assume that NWD(X) < 1. Write NWD(X) = a/b with a equal to the numerator of NWD(X) and b equal to the denominator. If c, d are real numbers satisfying c/d \u2265 a/b then bc \u2265 ad. Therefore ab + bc \u2265 ab + ad which rearranged yields (a+ c)/(b+ d) \u2265 a/b. If c/d < a/b then by similar reasoning (a+ c)/(b+ d) < a/b.\nAssume (III.1) holds. We take the logarithms of both sides of (III.1) and rearrange it to obtain log f(X)\u2212maxx\u2208X{log f(x)} \u2212 log f(Y ) + maxy\u2208Y {log f(y)} \u2265 (minx\u2208X{log f(x)} \u2212 miny\u2208Y {log f(y)})(|X| \u2212 1)NWD(X). Let the lefthand side of the inequality be c and the\nFebruary 23, 2015 DRAFT\n22\nrighthand side of the inequality be dNWD(X). Then\nNWD(X) = maxx\u2208X{log f(x)} \u2212 log f(X)\n(logN \u2212minx\u2208X{log f(x)})(|X| \u2212 1) (A.2)\n\u2264 maxy\u2208Y {log f(y)} \u2212 log f(Y ) (logN \u2212miny\u2208Y {log f(y)})(|X| \u2212 1) = |Y | \u2212 1 |X| \u2212 1 NWD(Y ).\nThe inequality holds by the rewritten (III.1) and the a, b, c, d argument above since c/d \u2265 NWD(X) = a/b.\nAssume (III.1) does not hold, that is, it holds with the \u2265 sign replaced by a < sign. We take logarithms of both sides of this last version and rewrite it to obtain log f(X)\u2212maxx\u2208X{log f(x)}\u2212 log f(Y ) + maxy\u2208Y {log f(y)} < (minx\u2208X{log f(x)} \u2212miny\u2208Y {log f(y)})(|X| \u2212 1)NWD(X). Let the lefthand side of the inequality be c and the righthand side dNWD(X). Since c/d < NWD(X) = a/b we have a/b > (a + c)/(b + d) by the a, b, c.d argument above. Hence (A.2) holds with the \u2264 sign switched to a > sign. It remains to prove that NWD(Y ) \u2265 NWD(Z)(|Z| \u2212 1)/(|Y | \u2212 1). This follows directly from item (i).\nProof: of Lemma III.5.\nThe following is a counterexample. Let X = {x1}, Y = {x2}, Z = {x3, x4}, maxx\u2208XY {log f(x)} = 10, maxx\u2208XZ{log f(x)} = 10, maxx\u2208ZY {log f(x)} = 5, log f(XY ) = log f(XZ) = log f(ZY ) = 3, minx\u2208XY {log f(x)} = minx\u2208XZ{f(x)} = minx\u2208ZY {log f(x)} = 4, and logN = 35. This arrangement can be realized for queries x1, x2, x3, x4. (As usual we assume that e(xi) 6= e(xj) for 1 \u2264 i, j \u2264 4 and i 6= j.) Computation shows NWD(XY ) > NWD(XZ) +NWD(ZY ) since 7/31 > 7/62 + 1/62.\nProof: of Theorem III.7.\nWe start with the following:\nClaim A.1. EGmax(X) is an admissible web distance function and EGmax(X) \u2264 D(X) for every computable admissible web distance function D.\nProof: Clearly EGmax(X) satisfies items (i) and (ii) of Definition III.6. To show it is an\nadmissible web distance it remains to establish the density requirement (iii). For fixed x consider the sets X 3 x and |X| \u2265 2. We have\u2211 X:X3x & |X|\u22652 2\u2212EGmax(X) \u2264 1,\nFebruary 23, 2015 DRAFT\n23\nsince for every x the set {EGmax(X) : X 3 x & EGmax(X) > 0} is the length set of a binary prefix code and therefore the summation above satisfies the Kraft inequality [9] given by (II.1). Hence EGmax is an admissible distance.\nIt remains to prove minorization. Let D be a computable admissible web distance, and the function f defined by f(X, x) = 2\u2212D(X) for x \u2208 X and 0 otherwise. Since D is computable the function f is computable. Given D, one can compute f and therefore K(f) \u2264 K(D) + O(1). Let m denote the universal distribution [12]. By [12, Theorem 4.3.2] cDm(X|x) \u2265 f(X, x) with cD = 2K(f) = 2K(D)+O(1), that is, cD is a positive constant depending on D only. By [12, Theorem 4.3.4] we have \u2212 logm(X|x) = K(X|x) + O(1). Altogether, for every X \u2208 X and for every x \u2208 X holds log 1/f(X, x) \u2265 K(X|x) + log 1/cD + O(1). Hence D(X) \u2265 EGmax(X) + log 1/cD +O(1).\nBy Lemma II.4 the function EGmax is upper semicomputable but not computable. The function G(X) \u2212 minx\u2208X{G(x)} is a computable and an admissible function as in Definition III.6. By Claim A.1 it is an upper bound on EGmax(X) and hence EGmax(X) < G(X)\u2212minx\u2208X{G(x)}. Every admissible property or feature that is common to all members of X is quantized as an upper bound on EGmax(X). Thus, the closer G(X) \u2212 minx\u2208X{G(x)} approximates EGmax(X), the better it approximates the common admissible properties among all search terms in X . This G(X) \u2212 minx\u2208X{G(x)} is the numerator of NWD(X). The denominator is maxx\u2208X{G(x)}(|X| \u2212 1), a normalizing factor suited to the numerator of NWD(X). It is chosen such that the quotient NWD(X) has a value in [0, 1] (Lemma III.1)."}], "references": [{"title": "Information distance", "author": ["C.H. Bennett", "P. G\u00e1cs", "M. Li", "P.M.B. Vit\u00e1nyi", "W. Zurek"], "venue": "IEEE Trans. Inform. Theory, 44:4", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Measuring semantic similarity between words using web search engines", "author": ["D. Bollegala", "M. Yutaka", "I. Mitsuru"], "venue": "Proc. WWW., Vol. 766", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic keyword prediction using Google similarity distance", "author": ["P.-I. Chen", "S.-J. Lin"], "venue": "Expert Systems with Applications, 37:3", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Summarization of Changes in Biological Image Sequences using Algorithmic Information Theory", "author": ["A.R. Cohen", "C. Bjornsson", "S. Temple", "G. Banker", "B. Roysam"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The Google similarity distance", "author": ["R.L. Cilibrasi", "P.M.B. Vit\u00e1nyi"], "venue": "IEEE Trans. Knowledge and Data Engineering, 19:3", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "On the symmetry of algorithmic information", "author": ["P. G\u00e1cs"], "venue": "Soviet Math. Doklady, 15:1477\u20131480, 1974. Correction, Ibid., 15", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1974}, {"title": "W", "author": ["R. Gligorov"], "venue": "ten Kate, Z. Aleksovski and F. van Harmelen, Using Google distance to weight approximate ontology matches, Proc. 16th Intl Conf. World Wide Web, ACM Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Three approaches to the quantitative definition of information", "author": ["A.N. Kolmogorov"], "venue": "Problems Inform. Transmission 1:1", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1965}, {"title": "A device for quantizing", "author": ["L.G. Kraft"], "venue": "grouping, and coding amplitude modulated pulses, MS Thesis, EE Dept., Massachusetts Institute of Technology, Cambridge. Mass., USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1949}, {"title": "Laws of information conservation (nongrowth) and aspects of the foundation of probability theory", "author": ["L.A. Levin"], "venue": "Probl. Inform. Transm., 10", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1974}, {"title": "Information shared by many objects", "author": ["C. Long", "X. Zhu", "M. Li", "B. Ma"], "venue": "Proc. 17th ACM Conf. Information and Knowledge Management", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P.M.B. Vit\u00e1nyi"], "venue": "Springer-Verlag, New York, Third edition", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Two inequalities implied by unique decipherability", "author": ["B. McMillan"], "venue": "IEEE Trans. Information Theory, 2:4", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1956}, {"title": "Quantitative Analysis of Culture", "author": ["J.-B. Michel", "Y.K. Shen", "A.P. Aiden", "A. Veres", "M.K. Gray", "T.G.B. Team"], "venue": "Using Millions of Digitized Books, Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "On Spectral Clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "The mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell System Tech. J., 27", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1948}, {"title": "Estimating the number of clusters in a dataset via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society 63:(2001)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Information distance in multiples", "author": ["P.M.B. Vit\u00e1nyi"], "venue": "IEEE Trans. Inform. Theory, 57:4", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "and E", "author": ["I.H. Witten"], "venue": "Frank, Data Mining: Practical Machine Learning Tools and Techniques", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Asymmetric information distances for automated taxonomy construction", "author": ["W.L. Woon", "S. Madnick"], "venue": "Knowl. Inf. Systems, 21", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation of the online tourism domain in search engines", "author": ["Z. Xian", "K. Weber", "D.R. Fesenmaier"], "venue": "J. Travel Research, 47:2", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Genome-wide association study of Alzheimer\u2019s disease.", "author": ["Kamboh", "M. I"], "venue": "Transl Psychiatry", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Post-GWAS Functional Characterization of Susceptibility Variants for Chronic Lymphocytic Leukemia.", "author": ["Sill", "F.C. M"], "venue": "PLoS One", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Chromosome 6p22 Locus Associated with Clinically Aggressive Neuroblastoma.", "author": ["Maris", "J. M"], "venue": "New England Journal of Medicine", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Two New Loci for Body-Weight Regulation Identified in a Joint Analysis of Genome-Wide Association Studies for Early-Onset Extreme Obesity in French and German Study Groups.", "author": ["A Scherag"], "venue": "PLoS Genet", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "GWAS risk factors in Parkinson\u2019s disease: LRRK2 coding variation and genetic interaction with PARK16.", "author": ["Soto-Ortolaza", "A. I"], "venue": "Am J Neurodegener Dis", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Prefrontal dysfunction in schizophrenia involves mixed-lineage leukemia 1-regulated histone methylation at GABAergic gene promoters.", "author": ["Huang", "H. S"], "venue": "J Neurosci", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Obesity and the risk of Parkinson\u2019s disease.", "author": ["H Chen"], "venue": "Am J Epidemiol", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "The classic notion of Kolmogorov complexity [8] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "The classic notion of Kolmogorov complexity [8] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "The similarity or relative semantics between pairs of search terms was defined in [5] and demonstrated in practice by using the World Wide Web as database and Google as search engine.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "In [11] the notion is introduced of the information required to go from any object in a finite multiset (a set where a member can occur more than once) of objects to any other object in the set.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18] the mathematical theory is developed further and the difficulty of normalization is shown.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "A probability mass function on a known set allows us to define the associated prefix-code word length (information content) equal to unique decodable code word length [9], [13].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "A probability mass function on a known set allows us to define the associated prefix-code word length (information content) equal to unique decodable code word length [9], [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "By the ubiquitous Kraft inequality [9], if l1, l2, .", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Then NWD(X) \u2208 [0, 1].", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "(Note that to keep NWD(X) \u2208 [0, 1] for all X we have the factor (|X| \u2212 1) in the denominator of NWD(X).", "startOffset": 28, "endOffset": 34}, {"referenceID": 4, "context": "For the questions considered previously [5] we found that these approximate measures were sufficient at that time to generate useful answers, especially in the absence of any a priori domain knowledge.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "For the pairwise formulation, we use the gap spectral clustering unsupervised approach developed in [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 16, "context": "Gap spectral clustering uses the gap statistic as first proposed in [17] to estimate the number of clusters in a data set from an arbitrary clustering algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "In [4], it was shown that the gap statistic in conjunction with a spectral clustering [15] of the distance matrix obtained from pairwise NWD measurements is an estimate of randomness deficiency for clustering models.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [4], it was shown that the gap statistic in conjunction with a spectral clustering [15] of the distance matrix obtained from pairwise NWD measurements is an estimate of randomness deficiency for clustering models.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Randomness deficiency is a measure of the meaningful information that a model, here a clustering algorithm, captures in a particular set of data [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "In practice, this is achieved by picking the first number of clusters where the gap value achieves a maximum as described in [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "Following [4] we set B to 100.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "We used the page counts returned using the built in search from each website for the frequencies, and following the approach in [5] choose N as the frequency for the search term \u2019the\u2019.", "startOffset": 128, "endOffset": 131}, {"referenceID": 18, "context": "include in parenthesis the 95% confidence interval for the result, computed as described in [19] The first three classification questions we considered used the wikipedia search engine.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 26, "context": "After these findings we found that there actually have been recent findings of strong relationships between both Schizophrenia and Leukemia [29] as well as between Parkinsons and Obesity [30], relationships that have also been identified by clinical evidence not relating to GWAS approaches.", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "After these findings we found that there actually have been recent findings of strong relationships between both Schizophrenia and Leukemia [29] as well as between Parkinsons and Obesity [30], relationships that have also been identified by clinical evidence not relating to GWAS approaches.", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The self-delimiting code for x of length n is x\u0304 = 1|x|0x of length 2n+ 1, or even shorter x\u2032 = 10x of length n+ 2 log n+ 1 (see [12] for still shorter self-delimiting codes).", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "This means that f can be computed from above (see [12], p.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "The Kolmogorov complexity is the information in a single finite object [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "Theory and applications are given in the textbook [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "The symmetry of information property [6] for strings x, y is", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "It is not computable since for X = {x, y} we have EGmax(X) = max{K(e(x)|e(y)), K(e(y)|e(x))}+O(1), the information distance between e(x) and e(y) which is known to be incomputable [1].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "since for every x the set {EGmax(X) : X 3 x & EGmax(X) > 0} is the length set of a binary prefix code and therefore the summation above satisfies the Kraft inequality [9] given by (II.", "startOffset": 167, "endOffset": 170}, {"referenceID": 11, "context": "Let m denote the universal distribution [12].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "It is chosen such that the quotient NWD(X) has a value in [0, 1] (Lemma III.", "startOffset": 58, "endOffset": 64}], "year": 2015, "abstractText": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or any other large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity according to all (upper semi)computable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov complexity.", "creator": "LaTeX with hyperref package"}}}